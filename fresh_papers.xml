<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 13 Dec 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SegFace: Face Segmentation of Long-Tail Classes</title><link>http://arxiv.org/abs/2412.08647v1</link><description>Face parsing refers to the semantic segmentation of human faces into keyfacial regions such as eyes, nose, hair, etc. It serves as a prerequisite forvarious advanced applications, including face editing, face swapping, andfacial makeup, which often require segmentation masks for classes likeeyeglasses, hats, earrings, and necklaces. These infrequently occurring classesare called long-tail classes, which are overshadowed by more frequentlyoccurring classes known as head classes. Existing methods, primarily CNN-based,tend to be dominated by head classes during training, resulting in suboptimalrepresentation for long-tail classes. Previous works have largely overlookedthe problem of poor segmentation performance of long-tail classes. To addressthis issue, we propose SegFace, a simple and efficient approach that uses alightweight transformer-based model which utilizes learnable class-specifictokens. The transformer decoder leverages class-specific tokens, allowing eachtoken to focus on its corresponding class, thereby enabling independentmodeling of each class. The proposed approach improves the performance oflong-tail classes, thereby boosting overall performance. To the best of ourknowledge, SegFace is the first work to employ transformer models for faceparsing. Moreover, our approach can be adapted for low-compute edge devices,achieving 95.96 FPS. We conduct extensive experiments demonstrating thatSegFace significantly outperforms previous state-of-the-art models, achieving amean F1 score of 88.96 (+2.82) on the CelebAMask-HQ dataset and 93.03 (+0.65)on the LaPa dataset. Code: https://github.com/Kartik-3004/SegFace</description><author>Kartik Narayan, Vibashan VS, Vishal M. Patel</author><pubDate>Wed, 11 Dec 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08647v1</guid></item><item><title>StreamChat: Chatting with Streaming Video</title><link>http://arxiv.org/abs/2412.08646v1</link><description>This paper presents StreamChat, a novel approach that enhances theinteraction capabilities of Large Multimodal Models (LMMs) with streaming videocontent. In streaming interaction scenarios, existing methods rely solely onvisual information available at the moment a question is posed, resulting insignificant delays as the model remains unaware of subsequent changes in thestreaming video. StreamChat addresses this limitation by innovatively updatingthe visual context at each decoding step, ensuring that the model utilizesup-to-date video content throughout the decoding process. Additionally, weintroduce a flexible and efficient crossattention-based architecture to processdynamic streaming inputs while maintaining inference efficiency for streaminginteractions. Furthermore, we construct a new dense instruction dataset tofacilitate the training of streaming interaction models, complemented by aparallel 3D-RoPE mechanism that encodes the relative temporal information ofvisual and text tokens. Experimental results demonstrate that StreamChatachieves competitive performance on established image and video benchmarks andexhibits superior capabilities in streaming interaction scenarios compared tostate-of-the-art video LMM.</description><author>Jihao Liu, Zhiding Yu, Shiyi Lan, Shihao Wang, Rongyao Fang, Jan Kautz, Hongsheng Li, Jose M. Alvare</author><pubDate>Wed, 11 Dec 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08646v1</guid></item><item><title>ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation</title><link>http://arxiv.org/abs/2412.08645v1</link><description>This paper introduces a tuning-free method for both object insertion andsubject-driven generation. The task involves composing an object, givenmultiple views, into a scene specified by either an image or text. Existingmethods struggle to fully meet the task's challenging objectives: (i)seamlessly composing the object into the scene with photorealistic pose andlighting, and (ii) preserving the object's identity. We hypothesize thatachieving these goals requires large scale supervision, but manually collectingsufficient data is simply too expensive. The key observation in this paper isthat many mass-produced objects recur across multiple images of large unlabeleddatasets, in different scenes, poses, and lighting conditions. We use thisobservation to create massive supervision by retrieving sets of diverse viewsof the same object. This powerful paired dataset enables us to train astraightforward text-to-image diffusion architecture to map the object andscene descriptions to the composited image. We compare our method, ObjectMate,with state-of-the-art methods for object insertion and subject-drivengeneration, using a single or multiple references. Empirically, ObjectMateachieves superior identity preservation and more photorealistic composition.Differently from many other multi-reference methods, ObjectMate does notrequire slow test-time tuning.</description><author>Daniel Winter, Asaf Shul, Matan Cohen, Dana Berman, Yael Pritch, Alex Rav-Acha, Yedid Hoshen</author><pubDate>Wed, 11 Dec 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08645v1</guid></item><item><title>GPD-1: Generative Pre-training for Driving</title><link>http://arxiv.org/abs/2412.08643v1</link><description>Modeling the evolutions of driving scenarios is important for the evaluationand decision-making of autonomous driving systems. Most existing methods focuson one aspect of scene evolution such as map generation, motion prediction, andtrajectory planning. In this paper, we propose a unified GenerativePre-training for Driving (GPD-1) model to accomplish all these tasks altogetherwithout additional fine-tuning. We represent each scene with ego, agent, andmap tokens and formulate autonomous driving as a unified token generationproblem. We adopt the autoregressive transformer architecture and use ascene-level attention mask to enable intra-scene bi-directional interactions.For the ego and agent tokens, we propose a hierarchical positional tokenizer toeffectively encode both 2D positions and headings. For the map tokens, we traina map vector-quantized autoencoder to efficiently compress ego-centric semanticmaps into discrete tokens. We pre-train our GPD-1 on the large-scale nuPlandataset and conduct extensive experiments to evaluate its effectiveness. Withdifferent prompts, our GPD-1 successfully generalizes to various tasks withoutfinetuning, including scene generation, traffic simulation, closed-loopsimulation, map prediction, and motion planning. Code:https://github.com/wzzheng/GPD.</description><author>Zixun Xie, Sicheng Zuo, Wenzhao Zheng, Yunpeng Zhang, Dalong Du, Jie Zhou, Jiwen Lu, Shanghang Zhang</author><pubDate>Wed, 11 Dec 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08643v1</guid></item><item><title>Generative Semantic Communication: Architectures, Technologies, and Applications</title><link>http://arxiv.org/abs/2412.08642v1</link><description>This paper delves into the applications of generative artificial intelligence(GAI) in semantic communication (SemCom) and presents a thorough study. Threepopular SemCom systems enabled by classical GAI models are first introduced,including variational autoencoders, generative adversarial networks, anddiffusion models. For each system, the fundamental concept of the GAI model,the corresponding SemCom architecture, and the associated literature review ofrecent efforts are elucidated. Then, a novel generative SemCom system isproposed by incorporating the cutting-edge GAI technology-large language models(LLMs). This system features two LLM-based AI agents at both the transmitterand receiver, serving as "brains" to enable powerful information understandingand content regeneration capabilities, respectively. This innovative designallows the receiver to directly generate the desired content, instead ofrecovering the bit stream, based on the coded semantic information conveyed bythe transmitter. Therefore, it shifts the communication mindset from"information recovery" to "information regeneration" and thus ushers in a newera of generative SemCom. A case study on point-to-point video retrieval ispresented to demonstrate the superiority of the proposed generative SemComsystem, showcasing a 99.98% reduction in communication overhead and a 53%improvement in retrieval accuracy compared to the traditional communicationsystem. Furthermore, four typical application scenarios for generative SemComare delineated, followed by a discussion of three open issues warranting futureinvestigation. In a nutshell, this paper provides a holistic set of guidelinesfor applying GAI in SemCom, paving the way for the efficient implementation ofgenerative SemCom in future wireless networks.</description><author>Jinke Ren, Yaping Sun, Hongyang Du, Weiwen Yuan, Chongjie Wang, Xianda Wang, Yingbin Zhou, Ziwei Zhu, Fangxin Wang, Shuguang Cui</author><pubDate>Wed, 11 Dec 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08642v1</guid></item><item><title>3D Mesh Editing using Masked LRMs</title><link>http://arxiv.org/abs/2412.08641v1</link><description>We present a novel approach to mesh shape editing, building on recentprogress in 3D reconstruction from multi-view images. We formulate shapeediting as a conditional reconstruction problem, where the model mustreconstruct the input shape with the exception of a specified 3D region, inwhich the geometry should be generated from the conditional signal. To thisend, we train a conditional Large Reconstruction Model (LRM) for maskedreconstruction, using multi-view consistent masks rendered from a randomlygenerated 3D occlusion, and using one clean viewpoint as the conditionalsignal. During inference, we manually define a 3D region to edit and provide anedited image from a canonical viewpoint to fill in that region. We demonstratethat, in just a single forward pass, our method not only preserves the inputgeometry in the unmasked region through reconstruction capabilities on par withSoTA, but is also expressive enough to perform a variety of mesh edits from asingle image guidance that past works struggle with, while being 10x fasterthan the top-performing competing prior work.</description><author>Will Gao, Dilin Wang, Yuchen Fan, Aljaz Bozic, Tuur Stuyck, Zhengqin Li, Zhao Dong, Rakesh Ranjan, Nikolaos Sarafianos</author><pubDate>Wed, 11 Dec 2024 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08641v1</guid></item><item><title>BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation</title><link>http://arxiv.org/abs/2412.08640v1</link><description>Single-image human mesh recovery is a challenging task due to the ill-posednature of simultaneous body shape, pose, and camera estimation. Existingestimators work well on images taken from afar, but they break down as theperson moves close to the camera. Moreover, current methods fail to achieveboth accurate 3D pose and 2D alignment at the same time. Error is mainlyintroduced by inaccurate perspective projection heuristically derived fromorthographic parameters. To resolve this long-standing challenge, we presentour method BLADE which accurately recovers perspective parameters from a singleimage without heuristic assumptions. We start from the inverse relationshipbetween perspective distortion and the person's Z-translation Tz, and we showthat Tz can be reliably estimated from the image. We then discuss the importantrole of Tz for accurate human mesh recovery estimated from close-range images.Finally, we show that, once Tz and the 3D human mesh are estimated, one canaccurately recover the focal length and full 3D translation. Extensiveexperiments on standard benchmarks and real-world close-range images show thatour method is the first to accurately recover projection parameters from asingle image, and consequently attain state-of-the-art accuracy on 3D poseestimation and 2D alignment for a wide range of images.https://research.nvidia.com/labs/amri/projects/blade/</description><author>Shengze Wang, Jiefeng Li, Tianye Li, Ye Yuan, Henry Fuchs, Koki Nagano, Shalini De Mello, Michael Stengel</author><pubDate>Wed, 11 Dec 2024 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08640v1</guid></item><item><title>Fast Prompt Alignment for Text-to-Image Generation</title><link>http://arxiv.org/abs/2412.08639v1</link><description>Text-to-image generation has advanced rapidly, yet aligning complex textualprompts with generated visuals remains challenging, especially with intricateobject relationships and fine-grained details. This paper introduces FastPrompt Alignment (FPA), a prompt optimization framework that leverages aone-pass approach, enhancing text-to-image alignment efficiency without theiterative overhead typical of current methods like OPT2I. FPA uses largelanguage models (LLMs) for single-iteration prompt paraphrasing, followed byfine-tuning or in-context learning with optimized prompts to enable real-timeinference, reducing computational demands while preserving alignment fidelity.Extensive evaluations on the COCO Captions and PartiPrompts datasetsdemonstrate that FPA achieves competitive text-image alignment scores at afraction of the processing time, as validated through both automated metrics(TIFA, VQA) and human evaluation. A human study with expert annotators furtherreveals a strong correlation between human alignment judgments and automatedscores, underscoring the robustness of FPA's improvements. The proposed methodshowcases a scalable, efficient alternative to iterative prompt optimization,enabling broader applicability in real-time, high-demand settings. The codebaseis provided to facilitate further research:https://github.com/tiktok/fast_prompt_alignment</description><author>Khalil Mrini, Hanlin Lu, Linjie Yang, Weilin Huang, Heng Wang</author><pubDate>Wed, 11 Dec 2024 18:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08639v1</guid></item><item><title>DMin: Scalable Training Data Influence Estimation for Diffusion Models</title><link>http://arxiv.org/abs/2412.08637v1</link><description>Identifying the training data samples that most influence a generated imageis a critical task in understanding diffusion models, yet existing influenceestimation methods are constrained to small-scale or LoRA-tuned models due tocomputational limitations. As diffusion models scale up, these methods becomeimpractical. To address this challenge, we propose DMin (Diffusion Modelinfluence), a scalable framework for estimating the influence of each trainingdata sample on a given generated image. By leveraging efficient gradientcompression and retrieval techniques, DMin reduces storage requirements from339.39 TB to only 726 MB and retrieves the top-k most influential trainingsamples in under 1 second, all while maintaining performance. Our empiricalresults demonstrate DMin is both effective in identifying influential trainingsamples and efficient in terms of computational and storage requirements.</description><author>Huawei Lin, Yingjie Lao, Weijie Zhao</author><pubDate>Wed, 11 Dec 2024 18:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08637v1</guid></item><item><title>Multimodal Latent Language Modeling with Next-Token Diffusion</title><link>http://arxiv.org/abs/2412.08635v1</link><description>Multimodal generative models require a unified approach to handle bothdiscrete data (e.g., text and code) and continuous data (e.g., image, audio,video). In this work, we propose Latent Language Modeling (LatentLM), whichseamlessly integrates continuous and discrete data using causal Transformers.Specifically, we employ a variational autoencoder (VAE) to represent continuousdata as latent vectors and introduce next-token diffusion for autoregressivegeneration of these vectors. Additionally, we develop $\sigma$-VAE to addressthe challenges of variance collapse, which is crucial for autoregressivemodeling. Extensive experiments demonstrate the effectiveness of LatentLMacross various modalities. In image generation, LatentLM surpasses DiffusionTransformers in both performance and scalability. When integrated intomultimodal large language models, LatentLM provides a general-purpose interfacethat unifies multimodal generation and understanding. Experimental results showthat LatentLM achieves favorable performance compared to Transfusion and vectorquantized models in the setting of scaling up training tokens. Intext-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2model in speaker similarity and robustness, while requiring 10x fewer decodingsteps. The results establish LatentLM as a highly effective and scalableapproach to advance large multimodal models.</description><author>Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei</author><pubDate>Wed, 11 Dec 2024 18:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08635v1</guid></item><item><title>MNIST-Fraction: Enhancing Math Education with AI-Driven Fraction Detection and Analysis</title><link>http://arxiv.org/abs/2412.08633v1</link><description>Mathematics education, a crucial and basic field, significantly influencesstudents' learning in related subjects and their future careers. Utilizingartificial intelligence to interpret and comprehend math problems in educationis not yet fully explored. This is due to the scarcity of quality datasets andthe intricacies of processing handwritten information. In this paper, wepresent a novel contribution to the field of mathematics education through thedevelopment of MNIST-Fraction, a dataset inspired by the renowned MNIST,specifically tailored for the recognition and understanding of handwritten mathfractions. Our approach is the utilization of deep learning, specificallyConvolutional Neural Networks (CNNs), for the recognition and understanding ofhandwritten math fractions to effectively detect and analyze fractions, alongwith their numerators and denominators. This capability is pivotal incalculating the value of fractions, a fundamental aspect of math learning. TheMNIST-Fraction dataset is designed to closely mimic real-world scenarios,providing a reliable and relevant resource for AI-driven educational tools.Furthermore, we conduct a comprehensive comparison of our dataset with theoriginal MNIST dataset using various classifiers, demonstrating theeffectiveness and versatility of MNIST-Fraction in both detection andclassification tasks. This comparative analysis not only validates thepractical utility of our dataset but also offers insights into its potentialapplications in math education. To foster collaboration and further researchwithin the computational and educational communities. Our work aims to bridgethe gap in high-quality educational resources for math learning, offering avaluable tool for both educators and researchers in the field.</description><author>Pegah Ahadian, Yunhe Feng, Karl Kosko, Richard Ferdig, Qiang Guan</author><pubDate>Wed, 11 Dec 2024 18:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08633v1</guid></item><item><title>Reducing Reasoning Costs -- The Path of Optimization for Chain of Thought via Sparse Attention Mechanism</title><link>http://arxiv.org/abs/2411.09111v4</link><description>In order to address the chain of thought in the large language modelinference cost surge, this research proposes to use a sparse attentionmechanism that only focuses on a few relevant tokens. The researcherconstructed a new attention mechanism and used GiantRabbit trained with customGPTs as an experimental tool. The experiment tested and compared the reasoningtime, correctness score and chain of thought length of this model and o1Preview in solving the linear algebra test questions of MIT OpenCourseWare. Theresults show that GiantRabbit's reasoning time and chain of thought length aresignificantly lower than o1 Preview. It verifies the feasibility of sparseattention mechanism for optimizing chain of thought reasoning. Detailedarchitectural details and experimental process have been uploaded to Github,the link is:https://github.com/brucewang123456789/GeniusTrail.git.</description><author>Libo Wang</author><pubDate>Wed, 11 Dec 2024 18:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09111v4</guid></item><item><title>FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models</title><link>http://arxiv.org/abs/2412.08629v1</link><description>Editing real images using a pre-trained text-to-image (T2I) diffusion/flowmodel often involves inverting the image into its corresponding noise map.However, inversion by itself is typically insufficient for obtainingsatisfactory results, and therefore many methods additionally intervene in thesampling process. Such methods achieve improved results but are not seamlesslytransferable between model architectures. Here, we introduce FlowEdit, atext-based editing method for pre-trained T2I flow models, which isinversion-free, optimization-free and model agnostic. Our method constructs anODE that directly maps between the source and target distributions(corresponding to the source and target text prompts) and achieves a lowertransport cost than the inversion approach. This leads to state-of-the-artresults, as we illustrate with Stable Diffusion 3 and FLUX. Code and examplesare available on the project's webpage.</description><author>Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli</author><pubDate>Wed, 11 Dec 2024 18:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08629v1</guid></item><item><title>Length Optimization in Conformal Prediction</title><link>http://arxiv.org/abs/2406.18814v3</link><description>Conditional validity and length efficiency are two crucial aspects ofconformal prediction (CP). Conditional validity ensures accurate uncertaintyquantification for data subpopulations, while proper length efficiency ensuresthat the prediction sets remain informative. Despite significant efforts toaddress each of these issues individually, a principled framework thatreconciles these two objectives has been missing in the CP literature. In thispaper, we develop Conformal Prediction with Length-Optimization (CPL) - a noveland practical framework that constructs prediction sets with (near-) optimallength while ensuring conditional validity under various classes of covariateshifts, including the key cases of marginal and group-conditional coverage. Inthe infinite sample regime, we provide strong duality results which indicatethat CPL achieves conditional validity and length optimality. In the finitesample regime, we show that CPL constructs conditionally valid prediction sets.Our extensive empirical evaluations demonstrate the superior prediction setsize performance of CPL compared to state-of-the-art methods across diversereal-world and synthetic datasets in classification, regression, and largelanguage model-based multiple choice question answering. An Implementation ofour algorithm can be accessed at the following link:https://github.com/shayankiyani98/CP.</description><author>Shayan Kiyani, George Pappas, Hamed Hassani</author><pubDate>Wed, 11 Dec 2024 18:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18814v3</guid></item><item><title>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation</title><link>http://arxiv.org/abs/2412.08628v1</link><description>Open-vocabulary panoptic segmentation aims to segment and classify everythingin diverse scenes across an unbounded vocabulary. Existing methods typicallyemploy two-stage or single-stage framework. The two-stage framework involvescropping the image multiple times using masks generated by a mask generator,followed by feature extraction, while the single-stage framework relies on aheavyweight mask decoder to make up for the lack of spatial positioninformation through self-attention and cross-attention in multiple stackedTransformer blocks. Both methods incur substantial computational overhead,thereby hindering the efficiency of model inference. To fill the gap inefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, andspatial-aware framework designed for open-vocabulary panoptic segmentation.Specifically, EOV-Seg innovates in two aspects. First, a Vocabulary-AwareSelection (VAS) module is proposed to improve the semantic comprehension ofvisual aggregated features and alleviate the feature interaction burden on themask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),which efficiently utilizes the spatial awareness capabilities of ViT-based CLIPbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabularypanoptic segmentation framework towards efficiency, which runs faster andachieves competitive performance compared with state-of-the-art methods.Specifically, with COCO training only, EOV-Seg achieves 24.2 PQ, 31.6 mIoU, and12.7 FPS on the ADE20K dataset for panoptic and semantic segmentation tasks andthe inference time of EOV-Seg is 4-21 times faster than state-of-the-artmethods. Especially, equipped with ResNet-50 backbone, EOV-Seg runs 25 FPS withonly 71M parameters on a single RTX 3090 GPU. Code is available at\url{https://github.com/nhw649/EOV-Seg}.</description><author>Hongwei Niu, Jie Hu, Jianghang Lin, Shengchuan Zhang</author><pubDate>Wed, 11 Dec 2024 18:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08628v1</guid></item><item><title>Synthetic Vision: Training Vision-Language Models to Understand Physics</title><link>http://arxiv.org/abs/2412.08619v1</link><description>Physical reasoning, which involves the interpretation, understanding, andprediction of object behavior in dynamic environments, remains a significantchallenge for current Vision-Language Models (VLMs). In this work, we proposetwo methods to enhance VLMs' physical reasoning capabilities using simulateddata. First, we fine-tune a pre-trained VLM using question-answer (QA) pairsgenerated from simulations relevant to physical reasoning tasks. Second, weintroduce Physics Context Builders (PCBs), specialized VLMs fine-tuned tocreate scene descriptions enriched with physical properties and processes.During physical reasoning tasks, these PCBs can be leveraged as context toassist a Large Language Model (LLM) to improve its performance. We evaluateboth of our approaches using multiple benchmarks, including a new stabilitydetection QA dataset called Falling Tower, which includes both simulated andreal-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLMcan significantly outperform larger state-of-the-art foundational models. Wealso show that integrating PCBs boosts the performance of foundational LLMs onphysical reasoning tasks. Using the real-world scenes from the Falling Towerdataset, we also validate the robustness of both approaches in Sim2Realtransfer. Our results highlight the utility that simulated data can have in thecreation of learning systems capable of advanced physical reasoning.</description><author>Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, Rahul G. Krishnan</author><pubDate>Wed, 11 Dec 2024 18:40:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08619v1</guid></item><item><title>Image Retrieval Methods in the Dissimilarity Space</title><link>http://arxiv.org/abs/2412.08618v1</link><description>Image retrieval methods rely on metric learning to train backbone featureextraction models that can extract discriminant queries and reference (gallery)feature representations for similarity matching. Although state-of-the-artaccuracy has improved considerably with the advent of deep learning (DL) modelstrained on large datasets, image retrieval remains challenging in manyreal-world video analytics and surveillance applications, e.g., personre-identification. Using the Euclidean space for matching limits theperformance in real-world applications due to the curse of dimensionality,overfitting, and sensitivity to noisy data. We argue that the feature dissimilarity space is more suitable for similaritymatching, and propose a dichotomy transformation to project query and referenceembeddings into a single embedding in the dissimilarity space. We also advocate for end-to-end training of a backbone and binaryclassification models for pair-wise matching. As opposed to comparing thedistance between queries and reference embeddings, we show the benefits ofclassifying the single dissimilarity space embedding (as similar ordissimilar), especially when trained end-to-end. We propose a method to trainthe max-margin classifier together with the backbone feature extractor byapplying constraints to the L2 norm of the classifier weights along with thehinge loss. Our extensive experiments on challenging image retrieval datasets and usingdiverse feature extraction backbones highlight the benefits of similaritymatching in the dissimilarity space. In particular, when jointly training thefeature extraction backbone and regularised classifier for matching, thedissimilarity space provides a higher level of accuracy.</description><author>Madhu Kiran, Kartikey Vishnu, Rafael M. O. Cruz, Eric Granger</author><pubDate>Wed, 11 Dec 2024 18:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08618v1</guid></item><item><title>Concept Bottleneck Language Models For protein design</title><link>http://arxiv.org/abs/2411.06090v2</link><description>We introduce Concept Bottleneck Protein Language Models (CB-pLM), agenerative masked language model with a layer where each neuron corresponds toan interpretable concept. Our architecture offers three key benefits: i)Control: We can intervene on concept values to precisely control the propertiesof generated proteins, achieving a 3 times larger change in desired conceptvalues compared to baselines. ii) Interpretability: A linear mapping betweenconcept values and predicted tokens allows transparent analysis of the model'sdecision-making process. iii) Debugging: This transparency facilitates easydebugging of trained models. Our models achieve pre-training perplexity anddownstream task performance comparable to traditional masked protein languagemodels, demonstrating that interpretability does not compromise performance.While adaptable to any language model, we focus on masked protein languagemodels due to their importance in drug discovery and the ability to validateour model's capabilities through real-world experiments and expert knowledge.We scale our CB-pLM from 24 million to 3 billion parameters, making them thelargest Concept Bottleneck Models trained and the first capable of generativelanguage modeling.</description><author>Aya Abdelsalam Ismail, Tuomas Oikarinen, Amy Wang, Julius Adebayo, Samuel Stanton, Taylor Joren, Joseph Kleinhenz, Allen Goodman, Héctor Corrada Bravo, Kyunghyun Cho, Nathan C. Frey</author><pubDate>Wed, 11 Dec 2024 18:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.06090v2</guid></item><item><title>Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models</title><link>http://arxiv.org/abs/2412.08615v1</link><description>Despite the advancements in training Large Language Models (LLMs) withalignment techniques to enhance the safety of generated content, these modelsremain susceptible to jailbreak, an adversarial attack method that exposessecurity vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG)method has demonstrated the ability to automatically generate adversarialsuffixes that jailbreak state-of-the-art LLMs. However, the optimizationprocess involved in GCG is highly time-consuming, rendering the jailbreakingpipeline inefficient. In this paper, we investigate the process of GCG andidentify an issue of Indirect Effect, the key bottleneck of the GCGoptimization. To this end, we propose the Model Attack Gradient Index GCG(MAGIC), that addresses the Indirect Effect by exploiting the gradientinformation of the suffix tokens, thereby accelerating the procedure by havingless computation and fewer iterations. Our experiments on AdvBench show thatMAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates(ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of74% on the Llama-2 and an ASR of 54% when conducting transfer attacks onGPT-3.5. Code is available at https://github.com/jiah-li/magic.</description><author>Jiahui Li, Yongchang Hao, Haoyu Xu, Xing Wang, Yu Hong</author><pubDate>Wed, 11 Dec 2024 18:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08615v1</guid></item><item><title>Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning</title><link>http://arxiv.org/abs/2412.08614v1</link><description>Generating detailed captions comprehending text-rich visual content in imageshas received growing attention for Large Vision-Language Models (LVLMs).However, few studies have developed benchmarks specifically tailored fordetailed captions to measure their accuracy and comprehensiveness. In thispaper, we introduce a detailed caption benchmark, termed as CompreCap, toevaluate the visual context from a directed scene graph view. Concretely, wefirst manually segment the image into semantically meaningful regions (i.e.,semantic segmentation mask) according to common-object vocabulary, while alsodistinguishing attributes of objects within all those regions. Then directionalrelation labels of these objects are annotated to compose a directed scenegraph that can well encode rich compositional information of the image. Basedon our directed scene graph, we develop a pipeline to assess the generateddetailed captions from LVLMs on multiple levels, including the object-levelcoverage, the accuracy of attribute descriptions, the score of keyrelationships, etc. Experimental results on the CompreCap dataset confirm thatour evaluation method aligns closely with human evaluation scores across LVLMs.</description><author>Fan Lu, Wei Wu, Kecheng Zheng, Shuailei Ma, Biao Gong, Jiawei Liu, Wei Zhai, Yang Cao, Yujun Shen, Zheng-Jun Zha</author><pubDate>Wed, 11 Dec 2024 18:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08614v1</guid></item><item><title>Fair Primal Dual Splitting Method for Image Inverse Problems</title><link>http://arxiv.org/abs/2412.08613v1</link><description>Image inverse problems have numerous applications, including imageprocessing, super-resolution, and computer vision, which are important areas inimage science. These application models can be seen as a three-functioncomposite optimization problem solvable by a variety of primal dual-typemethods. We propose a fair primal dual algorithmic framework that incorporatesthe smooth term not only into the primal subproblem but also into the dualsubproblem. We unify the global convergence and establish the convergence ratesof our proposed fair primal dual method. Experiments on image denoising andsuper-resolution reconstruction demonstrate the superiority of the proposedmethod over the current state-of-the-art.</description><author>Yunfei Qu, Deren Han</author><pubDate>Wed, 11 Dec 2024 18:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08613v1</guid></item><item><title>Competition and Diversity in Generative AI</title><link>http://arxiv.org/abs/2412.08610v1</link><description>Recent evidence suggests that the use of generative artificial intelligencereduces the diversity of content produced. In this work, we develop agame-theoretic model to explore the downstream consequences of contenthomogeneity when producers use generative AI to compete with one another. Atequilibrium, players indeed produce content that is less diverse than optimal.However, stronger competition mitigates homogeneity and induces more diverseproduction. Perhaps more surprisingly, we show that a generative AI model thatperforms well in isolation (i.e., according to a benchmark) may fail to do sowhen faced with competition, and vice versa. We validate our resultsempirically by using language models to play Scattergories, a word game inwhich players are rewarded for producing answers that are both correct andunique. We discuss how the interplay between competition and homogeneity hasimplications for the development, evaluation, and use of generative AI.</description><author>Manish Raghavan</author><pubDate>Wed, 11 Dec 2024 18:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08610v1</guid></item><item><title>Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data</title><link>http://arxiv.org/abs/2412.07762v2</link><description>The modern paradigm in machine learning involves pre-training on diversedata, followed by task-specific fine-tuning. In reinforcement learning (RL),this translates to learning via offline RL on a diverse historical dataset,followed by rapid online RL fine-tuning using interaction data. Most RLfine-tuning methods require continued training on offline data for stabilityand performance. However, this is undesirable because training on diverseoffline data is slow and expensive for large datasets, and in principle, alsolimit the performance improvement possible because of constraints or pessimismon offline data. In this paper, we show that retaining offline data isunnecessary as long as we use a properly-designed online RL approach forfine-tuning offline RL initializations. To build this approach, we start byanalyzing the role of retaining offline data in online fine-tuning. We findthat continued training on offline data is mostly useful for preventing asudden divergence in the value function at the onset of fine-tuning, caused bya distribution mismatch between the offline data and online rollouts. Thisdivergence typically results in unlearning and forgetting the benefits ofoffline pre-training. Our approach, Warm-start RL (WSRL), mitigates thecatastrophic forgetting of pre-trained initializations using a very simpleidea. WSRL employs a warmup phase that seeds the online RL run with a verysmall number of rollouts from the pre-trained policy to do fast online RL. Thedata collected during warmup helps ``recalibrate'' the offline Q-function tothe online distribution, allowing us to completely discard offline data withoutdestabilizing the online RL fine-tuning. We show that WSRL is able to fine-tunewithout retaining any offline data, and is able to learn faster and attainshigher performance than existing algorithms irrespective of whether they retainoffline data or not.</description><author>Zhiyuan Zhou, Andy Peng, Qiyang Li, Sergey Levine, Aviral Kumar</author><pubDate>Wed, 11 Dec 2024 18:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07762v2</guid></item><item><title>Counterfactual Explanations with Probabilistic Guarantees on their Robustness to Model Change</title><link>http://arxiv.org/abs/2408.04842v3</link><description>Counterfactual explanations (CFEs) guide users on how to adjust inputs tomachine learning models to achieve desired outputs. While existing researchprimarily addresses static scenarios, real-world applications often involvedata or model changes, potentially invalidating previously generated CFEs andrendering user-induced input changes ineffective. Current methods addressingthis issue often support only specific models or change types, requireextensive hyperparameter tuning, or fail to provide probabilistic guarantees onCFE robustness to model changes. This paper proposes a novel approach forgenerating CFEs that provides probabilistic guarantees for any model and changetype, while offering interpretable and easy-to-select hyperparameters. Weestablish a theoretical framework for probabilistically defining robustness tomodel change and demonstrate how our BetaRCE method directly stems from it.BetaRCE is a post-hoc method applied alongside a chosen base CFE generationmethod to enhance the quality of the explanation beyond robustness. Itfacilitates a transition from the base explanation to a more robust one withuser-adjusted probability bounds. Through experimental comparisons withbaselines, we show that BetaRCE yields robust, most plausible, and closest tobaseline counterfactual explanations.</description><author>Ignacy Stępka, Mateusz Lango, Jerzy Stefanowski</author><pubDate>Wed, 11 Dec 2024 18:32:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04842v3</guid></item><item><title>AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models</title><link>http://arxiv.org/abs/2412.08608v1</link><description>Recent advancements in large audio-language models (LALMs) have enabledspeech-based user interactions, significantly enhancing user experience andaccelerating the deployment of LALMs in real-world applications. However,ensuring the safety of LALMs is crucial to prevent risky outputs that may raisesocietal concerns or violate AI regulations. Despite the importance of thisissue, research on jailbreaking LALMs remains limited due to their recentemergence and the additional technical challenges they present compared toattacks on DNN-based audio models. Specifically, the audio encoders in LALMs,which involve discretization operations, often lead to gradient shattering,hindering the effectiveness of attacks relying on gradient-based optimizations.The behavioral variability of LALMs further complicates the identification ofeffective (adversarial) optimization targets. Moreover, enforcing stealthinessconstraints on adversarial audio waveforms introduces a reduced, non-convexfeasible solution space, further intensifying the challenges of theoptimization process. To overcome these challenges, we develop AdvWave, thefirst jailbreak framework against LALMs. We propose a dual-phase optimizationmethod that addresses gradient shattering, enabling effective end-to-endgradient-based optimization. Additionally, we develop an adaptive adversarialtarget search algorithm that dynamically adjusts the adversarial optimizationtarget based on the response patterns of LALMs for specific queries. To ensurethat adversarial audio remains perceptually natural to human listeners, wedesign a classifier-guided optimization approach that generates adversarialnoise resembling common urban sounds. Extensive evaluations on multipleadvanced LALMs demonstrate that AdvWave outperforms baseline methods, achievinga 40% higher average jailbreak attack success rate.</description><author>Mintong Kang, Chejian Xu, Bo Li</author><pubDate>Wed, 11 Dec 2024 18:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08608v1</guid></item><item><title>ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models</title><link>http://arxiv.org/abs/2412.07012v2</link><description>With the rise of multimodal applications, instruction data has becomecritical for training multimodal language models capable of understandingcomplex image-based queries. Existing practices rely on powerful but costlylarge language models (LLMs) or multimodal language models (MLMs) to produceinstruction data. These are often prone to hallucinations, licensing issues andthe generation process is often hard to scale and interpret. In this work, wepresent a programmatic approach that employs scene graphs as symbolicrepresentations of images and human-written programs to systematicallysynthesize vision-centric instruction data. Our approach ensures theinterpretability and controllability of the data generation process and scalesefficiently while maintaining factual accuracy. By implementing a suite of 24single-image, 14 multi-image instruction generators, and a scene graphgeneration pipeline, we build a scalable, cost-effective system: ProVisionwhich produces diverse question-answer pairs concerning objects, attributes,relations, depth, etc., for any given image. Applied to Visual Genome andDataComp datasets, we generate over 10 million instruction data points,ProVision-10M, and leverage them in both pretraining and instruction tuningstages of MLMs. When adopted in the instruction tuning stage, our single-imageinstruction data yields up to a 7% improvement on the 2D split and 8% on the 3Dsplit of CVBench, along with a 3% increase in performance on QBench2,RealWorldQA, and MMMU. Our multi-image instruction data leads to an 8%improvement on Mantis-Eval. Incorporation of our data in both pre-training andfine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6%across 11 benchmarks.</description><author>Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, silvio savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, Ran Xu</author><pubDate>Wed, 11 Dec 2024 18:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07012v2</guid></item><item><title>Preference Discerning with LLM-Enhanced Generative Retrieval</title><link>http://arxiv.org/abs/2412.08604v1</link><description>Sequential recommendation systems aim to provide personalized recommendationsfor users based on their interaction history. To achieve this, they oftenincorporate auxiliary information, such as textual descriptions of items andauxiliary tasks, like predicting user preferences and intent. Despite numerousefforts to enhance these models, they still suffer from limitedpersonalization. To address this issue, we propose a new paradigm, which weterm preference discerning. In preference dscerning, we explicitly condition agenerative sequential recommendation system on user preferences within itscontext. To this end, we generate user preferences using Large Language Models(LLMs) based on user reviews and item-specific data. To evaluate preferencediscerning capabilities of sequential recommendation systems, we introduce anovel benchmark that provides a holistic evaluation across various scenarios,including preference steering and sentiment following. We assess currentstate-of-the-art methods using our benchmark and show that they struggle toaccurately discern user preferences. Therefore, we propose a new method namedMender ($\textbf{M}$ultimodal Prefer$\textbf{en}$ce$\textbf{d}$iscern$\textbf{er}$), which improves upon existing methods andachieves state-of-the-art performance on our benchmark. Our results show thatMender can be effectively guided by human preferences even though they have notbeen observed during training, paving the way toward more personalizedsequential recommendation systems. We will open-source the code and benchmarksupon publication.</description><author>Fabian Paischer, Liu Yang, Linfeng Liu, Shuai Shao, Kaveh Hassani, Jiacheng Li, Ricky Chen, Zhang Gabriel Li, Xialo Gao, Wei Shao, Xue Feng, Nima Noorshams, Sem Park, Bo Long, Hamid Eghbalzadeh</author><pubDate>Wed, 11 Dec 2024 18:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08604v1</guid></item><item><title>Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis</title><link>http://arxiv.org/abs/2412.08603v1</link><description>Sewing patterns, the essential blueprints for fabric cutting and tailoring,act as a crucial bridge between design concepts and producible garments.However, existing uni-modal sewing pattern generation models struggle toeffectively encode complex design concepts with a multi-modal nature andcorrelate them with vectorized sewing patterns that possess precise geometricstructures and intricate sewing relations. In this work, we propose a novelsewing pattern generation approach Design2GarmentCode based on Large MultimodalModels (LMMs), to generate parametric pattern-making programs from multi-modaldesign concepts. LMM offers an intuitive interface for interpreting diversedesign inputs, while pattern-making programs could serve as well-structured andsemantically meaningful representations of sewing patterns, and act as a robustbridge connecting the cross-domain pattern-making knowledge embedded in LMMswith vectorized sewing patterns. Experimental results demonstrate that ourmethod can flexibly handle various complex design expressions such as images,textual descriptions, designer sketches, or their combinations, and convertthem into size-precise sewing patterns with correct stitches. Compared toprevious methods, our approach significantly enhances training efficiency,generation quality, and authoring flexibility. Our code and data will bepublicly available.</description><author>Feng Zhou, Ruiyang Liu, Chen Liu, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang</author><pubDate>Wed, 11 Dec 2024 18:26:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08603v1</guid></item><item><title>Der Effizienz- und Intelligenzbegriff in der Lexikographie und kuenstlichen Intelligenz: kann ChatGPT die lexikographische Textsorte nachbilden?</title><link>http://arxiv.org/abs/2412.08599v1</link><description>By means of pilot experiments for the language pair German and Galician, thispaper examines the concept of efficiency and intelligence in lexicography andartificial intelligence, AI. The aim of the experiments is to gain empiricallyand statistically based insights into the lexicographical text type,dictionaryarticle, in the responses of ChatGPT 3.5, as well as into the lexicographicaldata on which this chatbot was trained. Both quantitative and qualitativemethods are used for this purpose. The analysis is based on the evaluation ofthe outputs of several sessions with the same prompt in ChatGPT 3.5. On the onehand, the algorithmic performance of intelligent systems is evaluated incomparison with data from lexicographical works. On the other hand, the ChatGPTdata supplied is analysed using specific text passages of the aforementionedlexicographical text type. The results of this study not only help to evaluatethe efficiency of this chatbot regarding the creation of dictionary articles,but also to delve deeper into the concept of intelligence, the thoughtprocesses and the actions to be carried out in both disciplines.</description><author>Ivan Arias-Arias, Maria Jose Dominguez Vazquez, Carlos Valcarcel Riveiro</author><pubDate>Wed, 11 Dec 2024 18:18:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08599v1</guid></item><item><title>Numerical Analysis of HiPPO-LegS ODE for Deep State Space Models</title><link>http://arxiv.org/abs/2412.08595v1</link><description>In deep learning, the recently introduced state space models utilize HiPPO(High-order Polynomial Projection Operators) memory units to approximatecontinuous-time trajectories of input functions using ordinary differentialequations (ODEs), and these techniques have shown empirical success incapturing long-range dependencies in long input sequences. However, themathematical foundations of these ODEs, particularly the singular HiPPO-LegS(Legendre Scaled) ODE, and their corresponding numerical discretizations remainunexplored. In this work, we fill this gap by establishing that HiPPO-LegS ODEis well-posed despite its singularity, albeit without the freedom of arbitraryinitial conditions, and by establishing convergence of the associated numericaldiscretization schemes for Riemann-integrable input functions.</description><author>Jaesung R. Park, Jaewook J. Suh, Ernest K. Ryu</author><pubDate>Wed, 11 Dec 2024 18:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08595v1</guid></item><item><title>Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification</title><link>http://arxiv.org/abs/2403.12151v3</link><description>Domain-specific knowledge can significantly contribute to addressing a widevariety of vision tasks. However, the generation of such knowledge entailsconsiderable human labor and time costs. This study investigates the potentialof Large Language Models (LLMs) in generating and providing domain-specificinformation through semantic embeddings. To achieve this, an LLM is integratedinto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectorsin the context of the Vision-based Zero-shot Object State Classification task.We thoroughly examine the behavior of the LLM through an extensive ablationstudy. Our findings reveal that the integration of LLM-based embeddings, incombination with general-purpose pre-trained embeddings, leads to substantialperformance improvements. Drawing insights from this ablation study, we conducta comparative analysis against competing models, thereby highlighting thestate-of-the-art performance achieved by the proposed approach.</description><author>Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis, Theodore Patkos, Antonis Argyros, Dimitris Plexousakis</author><pubDate>Wed, 11 Dec 2024 18:12:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12151v3</guid></item><item><title>ASDnB: Merging Face with Body Cues For Robust Active Speaker Detection</title><link>http://arxiv.org/abs/2412.08594v1</link><description>State-of-the-art Active Speaker Detection (ASD) approaches mainly use audioand facial features as input. However, the main hypothesis in this paper isthat body dynamics is also highly correlated to "speaking" (and "listening")actions and should be particularly useful in wild conditions (e.g.,surveillance settings), where face cannot be reliably accessed. We proposeASDnB, a model that singularly integrates face with body information by mergingthe inputs at different steps of feature extraction. Our approach splits 3Dconvolution into 2D and 1D to reduce computation cost without loss ofperformance, and is trained with adaptive weight feature importance forimproved complement of face with body data. Our experiments show that ASDnBachieves state-of-the-art results in the benchmark dataset (AVA-ActiveSpeaker),in the challenging data of WASD, and in cross-domain settings using Columbia.This way, ASDnB can perform in multiple settings, which is positively regardedas a strong baseline for robust ASD models (code available athttps://github.com/Tiago-Roxo/ASDnB).</description><author>Tiago Roxo, Joana C. Costa, Pedro Inácio, Hugo Proença</author><pubDate>Wed, 11 Dec 2024 18:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08594v1</guid></item><item><title>Adaptive Principal Components Allocation with the $\ell_{2,g}$-regularized Gaussian Graphical Model for Efficient Fine-Tuning Large Models</title><link>http://arxiv.org/abs/2412.08592v1</link><description>In this work, we propose a novel Parameter-Efficient Fine-Tuning (PEFT)approach based on Gaussian Graphical Models (GGMs), marking the firstapplication of GGMs to PEFT tasks, to the best of our knowledge. The proposedmethod utilizes the $\ell_{2,g}$-norm to effectively select critical parametersand capture global dependencies. The resulting non-convex optimization problemis efficiently solved using a Block Coordinate Descent (BCD) algorithm.Experimental results on the GLUE benchmark [24] for fine-tuning RoBERTa-Base[18] demonstrate the effectiveness of the proposed approach, achievingcompetitive performance with significantly fewer trainable parameters. The codefor this work is available at: https://github.com/jzheng20/Course projects.git.</description><author>Jingjing Zheng, Yankai Cao</author><pubDate>Wed, 11 Dec 2024 18:11:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08592v1</guid></item><item><title>RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation</title><link>http://arxiv.org/abs/2412.08591v1</link><description>Vision-and-Language Navigation (VLN) suffers from the limited diversity andscale of training data, primarily constrained by the manual curation ofexisting simulators. To address this, we introduce RoomTour3D, avideo-instruction dataset derived from web-based room tour videos that capturereal-world indoor spaces and human walking demonstrations. Unlike existing VLNdatasets, RoomTour3D leverages the scale and diversity of online videos togenerate open-ended human walking trajectories and open-world navigableinstructions. To compensate for the lack of navigation data in online videos,we perform 3D reconstruction and obtain 3D trajectories of walking pathsaugmented with additional information on the room types, object locations and3D shape of surrounding scenes. Our dataset includes $\sim$100K open-endeddescription-enriched trajectories with $\sim$200K instructions, and 17Kaction-enriched trajectories from 1847 room tour environments. We demonstrateexperimentally that RoomTour3D enables significant improvements across multipleVLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3Dfacilitates the development of trainable zero-shot VLN agents, showcasing thepotential and challenges of advancing towards open-world navigation.</description><author>Mingfei Han, Liang Ma, Kamila Zhumakhanova, Ekaterina Radionova, Jingyi Zhang, Xiaojun Chang, Xiaodan Liang, Ivan Laptev</author><pubDate>Wed, 11 Dec 2024 18:10:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08591v1</guid></item><item><title>Preventing Conflicting Gradients in Neural Marked Temporal Point Processes</title><link>http://arxiv.org/abs/2412.08590v1</link><description>Neural Marked Temporal Point Processes (MTPP) are flexible models to capturecomplex temporal inter-dependencies between labeled events. These modelsinherently learn two predictive distributions: one for the arrival times ofevents and another for the types of events, also known as marks. In this study,we demonstrate that learning a MTPP model can be framed as a two-task learningproblem, where both tasks share a common set of trainable parameters that areoptimized jointly. We show that this often leads to the emergence ofconflicting gradients during training, where task-specific gradients arepointing in opposite directions. When such conflicts arise, following theaverage gradient can be detrimental to the learning of each individual tasks,resulting in overall degraded performance. To overcome this issue, we introducenovel parametrizations for neural MTPP models that allow for separate modelingand training of each task, effectively avoiding the problem of conflictinggradients. Through experiments on multiple real-world event sequence datasets,we demonstrate the benefits of our framework compared to the original modelformulations.</description><author>Tanguy Bosser, Souhaib Ben Taieb</author><pubDate>Wed, 11 Dec 2024 18:10:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08590v1</guid></item><item><title>SPACE-SUIT: An Artificial Intelligence based chromospheric feature extractor and classifier for SUIT</title><link>http://arxiv.org/abs/2412.08589v1</link><description>The Solar Ultraviolet Imaging Telescope(SUIT) onboard Aditya-L1 is an imagerthat observes the solar photosphere and chromosphere through observations inthe wavelength range of 200-400 nm. A comprehensive understanding of the plasmaand thermodynamic properties of chromospheric and photospheric morphologicalstructures requires a large sample statistical study, necessitating thedevelopment of automatic feature detection methods. To this end, we develop thefeature detection algorithm SPACE-SUIT: Solar Phenomena Analysis andClassification using Enhanced vision techniques for SUIT, to detect andclassify the solar chromospheric features to be observed from SUIT's Mg II kfilter. Specifically, we target plage regions, sunspots, filaments, andoff-limb structures. SPACE uses You Only Look Once(YOLO), a neuralnetwork-based model to identify regions of interest. We train and validateSPACE using mock-SUIT images developed from Interface Region ImagingSpectrometer(IRIS) full-disk mosaic images in Mg II k line, while we alsoperform detection on Level-1 SUIT data. SPACE achieves an approximate precisionof 0.788, recall 0.863 and MAP of 0.874 on the validation mock SUIT FITSdataset. Given the manual labeling of our dataset, we perform "self-validation"by applying statistical measures and Tamura features on the ground truth andpredicted bounding boxes. We find the distributions of entropy, contrast,dissimilarity, and energy to show differences in the features. Thesedifferences are qualitatively captured by the detected regions predicted bySPACE and validated with the observed SUIT images, even in the absence oflabeled ground truth. This work not only develops a chromospheric featureextractor but also demonstrates the effectiveness of statistical metrics andTamura features for distinguishing chromospheric features, offering independentvalidation for future detection schemes.</description><author>Pranava Seth, Vishal Upendran, Megha Anand, Janmejoy Sarkar, Soumya Roy, Priyadarshan Chaki, Pratyay Chowdhury, Borishan Ghosh, Durgesh Tripathi</author><pubDate>Wed, 11 Dec 2024 18:08:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08589v1</guid></item><item><title>Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts</title><link>http://arxiv.org/abs/2402.16822v3</link><description>As large language models (LLMs) become increasingly prevalent across manyreal-world applications, understanding and enhancing their robustness toadversarial attacks is of paramount importance. Existing methods foridentifying adversarial prompts tend to focus on specific domains, lackdiversity, or require extensive human annotations. To address theselimitations, we present Rainbow Teaming, a novel black-box approach forproducing a diverse collection of adversarial prompts. Rainbow Teaming castsadversarial prompt generation as a quality-diversity problem and usesopen-ended search to generate prompts that are both effective and diverse.Focusing on the safety domain, we use Rainbow Teaming to target variousstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approachreveals hundreds of effective adversarial prompts, with an attack success rateexceeding 90% across all tested models. Furthermore, we demonstrate thatprompts generated by Rainbow Teaming are highly transferable and thatfine-tuning models with synthetic data generated by our method significantlyenhances their safety without sacrificing general performance or helpfulness.We additionally explore the versatility of Rainbow Teaming by applying it toquestion answering and cybersecurity, showcasing its potential to drive robustopen-ended self-improvement in a wide range of applications.</description><author>Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, Roberta Raileanu</author><pubDate>Wed, 11 Dec 2024 18:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16822v3</guid></item><item><title>Advancing Single- and Multi-task Text Classification through Large Language Model Fine-tuning</title><link>http://arxiv.org/abs/2412.08587v1</link><description>Both encoder-only models (e.g., BERT, RoBERTa) and large language models(LLMs, e.g., Llama3) have been widely used for text classification tasks.However, there is a lack of systematic studies comparing the performance ofencoder-based models and LLMs in text classification, particularly whenfine-tuning is involved. This study employed a diverse range of models andmethods, varying in size and architecture, and including both fine-tuned andpre-trained approaches. We first assessed the performances of these LLMs on the20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-onlyRoBERTa models. Additionally, we explored the multi-task capabilities of bothmodel types by combining multiple classification tasks, including intentdetection and slot-filling, into a single model using data from both datasets.Our results indicate that fully fine-tuned Llama3-70B models outperformRoBERTa-large and other decoder LLMs across various classification tasks anddatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched theperformance of dual-model setups in both tasks across both datasets. Overall,our study provides a comprehensive benchmark of encoder-only and LLM models ontext classification tasks and demonstrates a method to combine two or morefully fine-tuned decoder LLMs for reduced latency and equivalent performance.</description><author>Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang</author><pubDate>Wed, 11 Dec 2024 18:06:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08587v1</guid></item><item><title>TURBOATTENTION: Efficient Attention Approximation For High Throughputs LLMs</title><link>http://arxiv.org/abs/2412.08585v1</link><description>Large language model (LLM) inference demands significant amount ofcomputation and memory, especially in the key attention mechanism. Whiletechniques, such as quantization and acceleration algorithms, likeFlashAttention, have improved efficiency of the overall inference, they addressdifferent aspects of the problem: quantization focuses on weight-activationoperations, while FlashAttention improves execution but requires high-precisionformats. Recent Key-value (KV) cache quantization reduces memory bandwidth butstill needs floating-point dequantization for attention operation. We present TurboAttention, a comprehensive approach to enable quantizedexecution of attention that simultaneously addresses both memory andcomputational efficiency. Our solution introduces two key innovations: FlashQ,a headwise attention quantization technique that enables both compression of KVcache and quantized execution of activation-activation multiplication, andSparsity-based Softmax Approximation (SAS), which eliminates the need fordequantization to FP32 during exponentiation operation in attention.Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedupin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37xmaximum throughput over the FP16 baseline while outperforming state-of-the-artquantization and compression techniques across various datasets and models.</description><author>Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan</author><pubDate>Wed, 11 Dec 2024 18:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08585v1</guid></item><item><title>Utilizing Multi-step Loss for Single Image Reflection Removal</title><link>http://arxiv.org/abs/2412.08582v1</link><description>Image reflection removal is crucial for restoring image quality. Distortedimages can negatively impact tasks like object detection and imagesegmentation. In this paper, we present a novel approach for image reflectionremoval using a single image. Instead of focusing on model architecture, weintroduce a new training technique that can be generalized to image-to-imageproblems, with input and output being similar in nature. This technique isembodied in our multi-step loss mechanism, which has proven effective in thereflection removal task. Additionally, we address the scarcity of reflectionremoval training data by synthesizing a high-quality, non-linear syntheticdataset called RefGAN using Pix2Pix GAN. This dataset significantly enhancesthe model's ability to learn better patterns for reflection removal. We alsoutilize a ranged depth map, extracted from the depth estimation of the ambientimage, as an auxiliary feature, leveraging its property of lacking depthestimations for reflections. Our approach demonstrates superior performance onthe SIR^2 benchmark and other real-world datasets, proving its effectiveness byoutperforming other state-of-the-art models.</description><author>Abdelrahman Elnenaey, Marwan Torki</author><pubDate>Wed, 11 Dec 2024 17:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08582v1</guid></item><item><title>LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations</title><link>http://arxiv.org/abs/2412.08580v1</link><description>Recent advances in text-to-image (T2I) generation have shown remarkablesuccess in producing high-quality images from text. However, existing T2Imodels show decayed performance in compositional image generation involvingmultiple objects and intricate relationships. We attribute this problem tolimitations in existing datasets of image-text pairs, which lack preciseinter-object relationship annotations with prompts only. To address thisproblem, we construct LAION-SG, a large-scale dataset with high-qualitystructural annotations of scene graphs (SG), which precisely describeattributes and relationships of multiple objects, effectively representing thesemantic structure in complex scenes. Based on LAION-SG, we train a newfoundation model SDXL-SG to incorporate structural annotation information intothe generation process. Extensive experiments show advanced models trained onour LAION-SG boast significant performance improvements in complex scenegeneration over models on existing datasets. We also introduce CompSG-Bench, abenchmark that evaluates models on compositional image generation, establishinga new standard for this domain.</description><author>Zejian Li, Chenye Meng, Yize Li, Ling Yang, Shengyuan Zhang, Jiarui Ma, Jiayi Li, Guang Yang, Changyuan Yang, Zhiyuan Yang, Jinxiong Chang, Lingyun Sun</author><pubDate>Wed, 11 Dec 2024 17:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08580v1</guid></item><item><title>Sparse Signature Coefficient Recovery via Kernels</title><link>http://arxiv.org/abs/2412.08579v1</link><description>Central to rough path theory is the signature transform of a path, aninfinite series of tensors given by the iterated integrals of the underlyingpath. The signature poses an effective way to capture sequentially orderedinformation, thanks both to its rich analytic and algebraic properties as wellas its universality when used as a basis to approximate functions on pathspace. Whilst a truncated version of the signature can be efficiently computedusing Chen's identity, there is a lack of efficient methods for computing asparse collection of iterated integrals contained in high levels of thesignature. We address this problem by leveraging signature kernels, defined asthe inner product of two signatures, and computable efficiently by means ofPDE-based methods. By forming a filter in signature space with which to takekernels, one can effectively isolate specific groups of signature coefficientsand, in particular, a singular coefficient at any depth of the transform. Weshow that such a filter can be expressed as a linear combination of suitablesignature transforms and demonstrate empirically the effectiveness of ourapproach. To conclude, we give an example use case for sparse collections ofsignature coefficients based on the construction of N-step Euler schemes forsparse CDEs.</description><author>Daniil Shmelev, Cristopher Salvi</author><pubDate>Wed, 11 Dec 2024 17:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08579v1</guid></item><item><title>Harnessing the Power of Vicinity-Informed Analysis for Classification under Covariate Shift</title><link>http://arxiv.org/abs/2405.16906v2</link><description>Transfer learning enhances prediction accuracy on a target distribution byleveraging data from a source distribution, demonstrating significant benefitsin various applications. This paper introduces a novel dissimilarity measurethat utilizes vicinity information, i.e., the local structure of data points,to analyze the excess error in classification under covariate shift, a transferlearning setting where marginal feature distributions differ but conditionallabel distributions remain the same. We characterize the excess error using theproposed measure and demonstrate faster or competitive convergence ratescompared to previous techniques. Notably, our approach is effective in thesupport non-containment assumption, which often appears in real-worldapplications, holds. Our theoretical analysis bridges the gap between currenttheoretical findings and empirical observations in transfer learning,particularly in scenarios with significant differences between source andtarget distributions.</description><author>Mitsuhiro Fujikawa, Yohei Akimoto, Jun Sakuma, Kazuto Fukuchi</author><pubDate>Wed, 11 Dec 2024 17:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16906v2</guid></item><item><title>Machine Learning Information Retrieval and Summarisation to Support Systematic Review on Outcomes Based Contracting</title><link>http://arxiv.org/abs/2412.08578v1</link><description>As academic literature proliferates, traditional review methods areincreasingly challenged by the sheer volume and diversity of availableresearch. This article presents a study that aims to address these challengesby enhancing the efficiency and scope of systematic reviews in the socialsciences through advanced machine learning (ML) and natural language processing(NLP) tools. In particular, we focus on automating stages within the systematicreviewing process that are time-intensive and repetitive for human annotatorsand which lend themselves to immediate scalability through tools such asinformation retrieval and summarisation guided by expert advice. The articleconcludes with a summary of lessons learnt regarding the integrated approachtowards systematic reviews and future directions for improvement, includingexplainability.</description><author>Iman Munire Bilal, Zheng Fang, Miguel Arana-Catania, Felix-Anselm van Lier, Juliana Outes Velarde, Harry Bregazzi, Eleanor Carter, Mara Airoldi, Rob Procter</author><pubDate>Wed, 11 Dec 2024 17:54:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08578v1</guid></item><item><title>Annotation-Efficient Task Guidance for Medical Segment Anything</title><link>http://arxiv.org/abs/2412.08575v1</link><description>Medical image segmentation is a key task in the imaging workflow, influencingmany image-based decisions. Traditional, fully-supervised segmentation modelsrely on large amounts of labeled training data, typically obtained throughmanual annotation, which can be an expensive, time-consuming, and error-proneprocess. This signals a need for accurate, automatic, and annotation-efficientmethods of training these models. We propose SAM-Mix, a novel multitasklearning framework for medical image segmentation that uses class activationmaps produced by an auxiliary classifier to guide the predictions of thesemi-supervised segmentation branch, which is based on the SAM framework.Experimental evaluations on the public LiTS dataset confirm the effectivenessof SAM-Mix for simultaneous classification and segmentation of the liver fromabdominal computed tomography (CT) scans. When trained for 90% fewer epochs ononly 50 labeled 2D slices, representing just 0.04% of the available labeledtraining data, SAM-Mix achieves a Dice improvement of 5.1% over the bestbaseline model. The generalization results for SAM-Mix are even moreimpressive, with the same model configuration yielding a 25.4% Dice improvementon a cross-domain segmentation task. Our code is available athttps://github.com/tbwa233/SAM-Mix.</description><author>Tyler Ward, Abdullah-Al-Zubaer Imran</author><pubDate>Wed, 11 Dec 2024 17:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08575v1</guid></item><item><title>Learning Sketch Decompositions in Planning via Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2412.08574v1</link><description>In planning and reinforcement learning, the identification of common subgoalstructures across problems is important when goals are to be achieved over longhorizons. Recently, it has been shown that such structures can be expressed asfeature-based rules, called sketches, over a number of classical planningdomains. These sketches split problems into subproblems which then becomesolvable in low polynomial time by a greedy sequence of IW$(k)$ searches.Methods for learning sketches using feature pools and min-SAT solvers have beendeveloped, yet they face two key limitations: scalability and expressivity. Inthis work, we address these limitations by formulating the problem of learningsketch decompositions as a deep reinforcement learning (DRL) task, wheregeneral policies are sought in a modified planning problem where the successorstates of a state s are defined as those reachable from s through an IW$(k)$search. The sketch decompositions obtained through this method areexperimentally evaluated across various domains, and problems are regarded assolved by the decomposition when the goal is reached through a greedy sequenceof IW$(k)$ searches. While our DRL approach for learning sketch decompositionsdoes not yield interpretable sketches in the form of rules, we demonstrate thatthe resulting decompositions can often be understood in a crisp manner.</description><author>Michael Aichmüller, Hector Geffner</author><pubDate>Wed, 11 Dec 2024 17:45:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08574v1</guid></item><item><title>MultiOrg: A Multi-rater Organoid-detection Dataset</title><link>http://arxiv.org/abs/2410.14612v2</link><description>High-throughput image analysis in the biomedical domain has gainedsignificant attention in recent years, driving advancements in drug discovery,disease prediction, and personalized medicine. Organoids, specifically, are anactive area of research, providing excellent models for human organs and theirfunctions. Automating the quantification of organoids in microscopy imageswould provide an effective solution to overcome substantial manualquantification bottlenecks, particularly in high-throughput image analysis.However, there is a notable lack of open biomedical datasets, in contrast toother domains, such as autonomous driving, and, notably, only few of them haveattempted to quantify annotation uncertainty. In this work, we present MultiOrga comprehensive organoid dataset tailored for object detection tasks withuncertainty quantification. This dataset comprises over 400 high-resolution 2dmicroscopy images and curated annotations of more than 60,000 organoids. Mostimportantly, it includes three label sets for the test data, independentlyannotated by two experts at distinct time points. We additionally provide abenchmark for organoid detection, and make the best model available through aneasily installable, interactive plugin for the popular image visualization toolNapari, to perform organoid quantification.</description><author>Christina Bukas, Harshavardhan Subramanian, Fenja See, Carina Steinchen, Ivan Ezhov, Gowtham Boosarpu, Sara Asgharpour, Gerald Burgstaller, Mareike Lehmann, Florian Kofler, Marie Piraud</author><pubDate>Wed, 11 Dec 2024 17:44:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14612v2</guid></item><item><title>TryOffAnyone: Tiled Cloth Generation from a Dressed Person</title><link>http://arxiv.org/abs/2412.08573v1</link><description>The fashion industry is increasingly leveraging computer vision and deeplearning technologies to enhance online shopping experiences and operationalefficiencies. In this paper, we address the challenge of generatinghigh-fidelity tiled garment images essential for personalized recommendations,outfit composition, and virtual try-on systems from photos of garments worn bymodels. Inspired by the success of Latent Diffusion Models (LDMs) inimage-to-image translation, we propose a novel approach utilizing a fine-tunedStableDiffusion model. Our method features a streamlined single-stage networkdesign, which integrates garmentspecific masks to isolate and process targetclothing items effectively. By simplifying the network architecture throughselective training of transformer blocks and removing unnecessarycrossattention layers, we significantly reduce computational complexity whileachieving state-of-the-art performance on benchmark datasets like VITON-HD.Experimental results demonstrate the effectiveness of our approach in producinghigh-quality tiled garment images for both full-body and half-body inputs. Codeand model are available at: https://github.com/ixarchakos/try-off-anyone</description><author>Ioannis Xarchakos, Theodoros Koukopoulos</author><pubDate>Wed, 11 Dec 2024 17:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08573v1</guid></item><item><title>Discretized Gaussian Representation for Tomographic Reconstruction</title><link>http://arxiv.org/abs/2411.04844v2</link><description>Computed Tomography (CT) is a widely used imaging technique that providesdetailed cross-sectional views of objects. Over the past decade, DeepLearning-based Reconstruction (DLR) methods have led efforts to enhance imagequality and reduce noise, yet they often require large amounts of data and arecomputationally intensive. Inspired by recent advancements in scenereconstruction, some approaches have adapted NeRF and 3D Gaussian Splatting(3DGS) techniques for CT reconstruction. However, these methods are not idealfor direct 3D volume reconstruction. In this paper, we reconsider therepresentation of CT reconstruction and propose a novel Discretized GaussianRepresentation (DGR) specifically designed for CT. Unlike the popular 3DGaussian Splatting, our representation directly reconstructs the 3D volumeusing a set of discretized Gaussian functions in an end-to-end manner.Additionally, we introduce a Fast Volume Reconstruction technique thatefficiently aggregates the contributions of these Gaussians into a discretizedvolume. Extensive experiments on both real-world and synthetic datasetsdemonstrate the effectiveness of our method in improving reconstruction qualityand computational efficiency. Our code has been provided for review purposesand will be made publicly available upon acceptance.</description><author>Shaokai Wu, Yuxiang Lu, Wei Ji, Suizhi Huang, Fengyu Yang, Shalayiding Sirejiding, Qichen He, Jing Tong, Yanbiao Ji, Yue Ding, Hongtao Lu</author><pubDate>Wed, 11 Dec 2024 17:40:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04844v2</guid></item><item><title>GenPlan: Generative sequence models as adaptive planners</title><link>http://arxiv.org/abs/2412.08565v1</link><description>Offline reinforcement learning has shown tremendous success in behavioralplanning by learning from previously collected demonstrations. However,decision-making in multitask missions still presents significant challenges.For instance, a mission might require an agent to explore an unknownenvironment, discover goals, and navigate to them, even if it involvesinteracting with obstacles along the way. Such behavioral planning problems aredifficult to solve due to: a) agents failing to adapt beyond the single tasklearned through their reward function, and b) the inability to generalize tonew environments not covered in the training demonstrations, e.g., environmentswhere all doors were unlocked in the demonstrations. Consequently,state-of-the-art decision making methods are limited to missions where therequired tasks are well-represented in the training demonstrations and can besolved within a short (temporal) planning horizon. To address this, we proposeGenPlan: a stochastic and adaptive planner that leverages discrete-flow modelsfor generative sequence modeling, enabling sample-efficient exploration andexploitation. This framework relies on an iterative denoising procedure togenerate a sequence of goals and actions. This approach captures multi-modalaction distributions and facilitates goal and task discovery, thereby enhancinggeneralization to out-of-distribution tasks and environments, i.e., missionsnot part of the training data. We demonstrate the effectiveness of our methodthrough multiple simulation environments. Notably, GenPlan outperforms thestate-of-the-art methods by over 10% on adaptive planning tasks, where theagent adapts to multi-task missions while leveraging demonstrations onsingle-goal-reaching tasks.</description><author>Akash Karthikeyan, Yash Vardhan Pant</author><pubDate>Wed, 11 Dec 2024 17:32:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08565v1</guid></item><item><title>Can We Generate Visual Programs Without Prompting LLMs?</title><link>http://arxiv.org/abs/2412.08564v1</link><description>Visual programming prompts LLMs (large language mod-els) to generateexecutable code for visual tasks like visual question answering (VQA).Prompt-based methods are difficult to improve while also being unreliable andcostly in both time and money. Our goal is to develop an efficient visualprogramming system without 1) using prompt-based LLMs at inference time and 2)a large set of program and answer annotations. We develop a synthetic dataaugmentation approach and alternative program generation method based ondecoupling programs into higher-level skills called templates and thecorresponding arguments. Our results show that with data augmentation,prompt-free smaller LLMs ($\approx$ 1B parameters) are competitive withstate-of-the art models with the added benefit of much faster inference</description><author>Michal Shlapentokh-Rothman, Yu-Xiong Wang, Derek Hoiem</author><pubDate>Wed, 11 Dec 2024 17:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08564v1</guid></item><item><title>Physics Based Differentiable Rendering for Inverse Problems and Beyond</title><link>http://arxiv.org/abs/2412.08563v1</link><description>Physics-based differentiable rendering (PBDR) has become an efficient methodin computer vision, graphics, and machine learning for addressing an array ofinverse problems. PBDR allows patterns to be generated from perceptions whichcan be applied to enhance object attributes like geometry, substances, andlighting by adding physical models of light propagation and materialsinteraction. Due to these capabilities, distinguished rendering has beenemployed in a wider range of sectors such as autonomous navigation, scenereconstruction, and material design. We provide an extensive overview of PBDRtechniques in this study, emphasizing their creation, effectiveness, andlimitations while managing inverse situations. We demonstrate modern techniquesand examine their value in everyday situations.</description><author>Preetish Kakkar, Srijani Mukherjee, Hariharan Ragothaman, Vishal Mehta</author><pubDate>Wed, 11 Dec 2024 17:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08563v1</guid></item><item><title>Continuous Multidimensional Scaling</title><link>http://arxiv.org/abs/2402.04436v3</link><description>Multidimensional scaling (MDS) is the act of embedding proximity informationabout a set of $n$ objects in $d$-dimensional Euclidean space. As originallyconceived by the psychometric community, MDS was concerned with embedding afixed set of proximities associated with a fixed set of objects. Modernconcerns, e.g., that arise in developing asymptotic theories for statisticalinference on random graphs, more typically involve studying the limitingbehavior of a sequence of proximities associated with an increasing set ofobjects. Here we are concerned with embedding dissimilarities by minimizingKruskal's (1964) raw stress criterion. Standard results from the theory ofpoint-to-set maps can be used to establish that, if $n$ is fixed and a sequenceof dissimilarity matrices converges, then the limit of their embeddedstructures is the embedded structure of the limiting dissimilarity matrix. Butwhat if $n$ increases? It then becomes necessary to reformulate MDS so that theentire sequence of embedding problems can be viewed as a sequence ofoptimization problems in a fixed space. We present such a reformulation, {\emcontinuous MDS}. Within the continuous MDS framework, we derive two $L^p$consistency results, one for embedding without constraints on theconfiguration, the other for embedding subject to {\em approximate Lipschitzconstraints}\/ that encourage smoothness of the embedding function. The latterapproach, {\em Approximate Lipschitz Embedding}\/ (ALE) is new. Finally, wedemonstrate that embedded structures produced by ALE can be interpolated in away that results in uniform convergence.</description><author>Michael W. Trosset, Carey E. Priebe</author><pubDate>Wed, 11 Dec 2024 17:22:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04436v3</guid></item><item><title>Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning</title><link>http://arxiv.org/abs/2412.08559v1</link><description>Large Language Models are trained on extensive datasets that often containsensitive, human-generated information, raising significant concerns aboutprivacy breaches. While certified unlearning approaches offer strong privacyguarantees, they rely on restrictive model assumptions that are not applicableto LLMs. As a result, various unlearning heuristics have been proposed, withthe associated privacy risks assessed only empirically. The standard evaluationpipelines typically randomly select data for removal from the training set,apply unlearning techniques, and use membership inference attacks to comparethe unlearned models against models retrained without the to-be-unlearned data.However, since every data point is subject to the right to be forgotten,unlearning should be considered in the worst-case scenario from the privacyperspective. Prior work shows that data outliers may exhibit highermemorization effects. Intuitively, they are harder to be unlearn and thus theprivacy risk of unlearning them is underestimated in the current evaluation. Inthis paper, we leverage minority data to identify such a critical flaw inpreviously widely adopted evaluations. We substantiate this claim throughcarefully designed experiments, including unlearning canaries related tominority groups, inspired by privacy auditing literature. Using personallyidentifiable information as a representative minority identifier, wedemonstrate that minority groups experience at least 20% more privacy leakagein most cases across six unlearning approaches, three MIAs, three benchmarkdatasets, and two LLMs of different scales. Given that the right to beforgotten should be upheld for every individual, we advocate for a morerigorous evaluation of LLM unlearning methods. Our minority-aware evaluationframework represents an initial step toward ensuring more equitable assessmentsof LLM unlearning efficacy.</description><author>Rongzhe Wei, Mufei Li, Mohsen Ghassemi, Eleonora Kreačić, Yifan Li, Xiang Yue, Bo Li, Vamsi K. Potluru, Pan Li, Eli Chien</author><pubDate>Wed, 11 Dec 2024 17:22:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08559v1</guid></item><item><title>Exact Algorithms for Multiagent Path Finding with Communication Constraints on Tree-Like Structures</title><link>http://arxiv.org/abs/2412.08556v1</link><description>Consider the scenario where multiple agents have to move in an optimal waythrough a network, each one towards their ending position while avoidingcollisions. By optimal, we mean as fast as possible, which is evaluated by ameasure known as the makespan of the proposed solution. This is the settingstudied in the Multiagent Path Finding problem. In this work, we additionallyprovide the agents with a way to communicate with each other. Due to sizeconstraints, it is reasonable to assume that the range of communication of eachagent will be limited. What should be the trajectories of the agents to,additionally, maintain a backbone of communication? In this work, we study theMultiagent Path Finding with Communication Constraint problem under theparameterized complexity framework. Our main contribution is three exact algorithms that are efficient whenconsidering particular structures for the input network. We provide suchalgorithms for the case when the communication range and the number of agents(the makespan resp.) are provided in the input and the network has a treetopology, or bounded maximum degree (has a tree-like topology, i.e., boundedtreewidth resp.). We complement these results by showing that it is highlyunlikely to construct efficient algorithms when considering the number ofagents as part of the input, even if the makespan is $3$ and the communicationrange is $1$.</description><author>Foivos Fioravantes, Dušan Knop, Jan Matyáš Křišťan, Nikolaos Melissinos, Michal Opler</author><pubDate>Wed, 11 Dec 2024 17:17:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08556v1</guid></item><item><title>Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks Defending against Poisoning Attacks</title><link>http://arxiv.org/abs/2412.08555v1</link><description>End-to-end training with global optimization have popularized graph neuralnetworks (GNNs) for node classification, yet inadvertently introducedvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploitthe inherent opened interfaces of GNNs' input and output, perturbing criticaledges and thus manipulating the classification results. Current defenses, dueto their persistent utilization of global-optimization-based end-to-endtraining schemes, inherently encapsulate the vulnerabilities of GNNs. This isspecifically evidenced in their inability to defend against targeted secondaryattacks. In this paper, we propose the Graph Agent Network (GAgN) to addressthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agentnetwork in which each node is designed as an 1-hop-view agent. Through thedecentralized interactions between agents, they can learn to infer globalperceptions to perform tasks including inferring embeddings, degrees andneighbor relationships for given nodes. This empowers nodes to filteringadversarial edges while carrying out classification tasks. Furthermore, agents'limited view prevents malicious messages from propagating globally in GAgN,thereby resisting global-optimization-based secondary attacks. We prove thatsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficientto achieve these functionalities. Experimental results show that GAgNeffectively implements all its intended capabilities and, compared tostate-of-the-art defenses, achieves optimal classification accuracy on theperturbed datasets.</description><author>Ao Liu, Wenshan Li, Beibei Li, Wengang Ma, Tao Li, Pan Zhou</author><pubDate>Wed, 11 Dec 2024 17:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08555v1</guid></item><item><title>Watermarking Training Data of Music Generation Models</title><link>http://arxiv.org/abs/2412.08549v1</link><description>Generative Artificial Intelligence (Gen-AI) models are increasingly used toproduce content across domains, including text, images, and audio. While thesemodels represent a major technical breakthrough, they gain their generativecapabilities from being trained on enormous amounts of human-generated content,which often includes copyrighted material. In this work, we investigate whetheraudio watermarking techniques can be used to detect an unauthorized usage ofcontent to train a music generation model. We compare outputs generated by amodel trained on watermarked data to a model trained on non-watermarked data.We study factors that impact the model's generation behaviour: the watermarkingtechnique, the proportion of watermarked samples in the training set, and therobustness of the watermarking technique against the model's tokenizer. Ourresults show that audio watermarking techniques, including some that areimperceptible to humans, can lead to noticeable shifts in the model's outputs.We also study the robustness of a state-of-the-art watermarking technique toremoval techniques.</description><author>Pascal Epple, Igor Shilov, Bozhidar Stevanovski, Yves-Alexandre de Montjoye</author><pubDate>Wed, 11 Dec 2024 17:10:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08549v1</guid></item><item><title>Enhanced Facial Feature Extraction and Recignation Using Optimal Fully Dispersed Haar-like Filters</title><link>http://arxiv.org/abs/2404.10476v2</link><description>Haar-like filters are renowned for their simplicity, speed, and accuracy invarious computer vision tasks. This paper proposes a novel algorithm toidentify optimal fully dispersed Haar-like filters for enhanced facial featureextraction and recognation. Unlike traditional Haar-like filters, these novelfilters allow pixels to move freely within images, enabling more effictivecapture of intricate local features...</description><author>Zeinab Sedaghatjoo, Hossein Hosseinzadeh, Ahmad shirzadi</author><pubDate>Wed, 11 Dec 2024 17:09:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10476v2</guid></item><item><title>Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations</title><link>http://arxiv.org/abs/2203.01517v2</link><description>Spurious correlations pose a major challenge for robust machine learning.Models trained with empirical risk minimization (ERM) may learn to rely oncorrelations between class labels and spurious attributes, leading to poorperformance on data groups without these correlations. This is particularlychallenging to address when spurious attribute labels are unavailable. Toimprove worst-group performance on spuriously correlated data without trainingattribute labels, we propose Correct-N-Contrast (CNC), a contrastive approachto directly learn representations robust to spurious correlations. As ERMmodels can be good spurious attribute predictors, CNC works by (1) using atrained ERM model's outputs to identify samples with the same class butdissimilar spurious features, and (2) training a robust model with contrastivelearning to learn similar representations for same-class samples. To supportCNC, we introduce new connections between worst-group error and arepresentation alignment loss that CNC aims to minimize. We empirically observethat worst-group error closely tracks with alignment loss, and prove that thealignment loss over a class helps upper-bound the class's worst-group vs.average error gap. On popular benchmarks, CNC reduces alignment lossdrastically, and achieves state-of-the-art worst-group accuracy by 3.6% averageabsolute lift. CNC is also competitive with oracle methods that require grouplabels.</description><author>Michael Zhang, Nimit S. Sohoni, Hongyang R. Zhang, Chelsea Finn, Christopher Ré</author><pubDate>Wed, 11 Dec 2024 17:06:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.01517v2</guid></item><item><title>Bilevel Joint Unsupervised and Supervised Training for Automatic Speech Recognition</title><link>http://arxiv.org/abs/2412.08548v1</link><description>In this paper, we propose a bilevel joint unsupervised and supervisedtraining (BL-JUST) framework for automatic speech recognition. Compared to theconventional pre-training and fine-tuning strategy which is a disconnectedtwo-stage process, BL-JUST tries to optimize an acoustic model such that itsimultaneously minimizes both the unsupervised and supervised loss functions.Because BL-JUST seeks matched local optima of both loss functions, acousticrepresentations learned by the acoustic model strike a good balance betweenbeing generic and task-specific. We solve the BL-JUST problem usingpenalty-based bilevel gradient descent and evaluate the trained deep neuralnetwork acoustic models on various datasets with a variety of architectures andloss functions. We show that BL-JUST can outperform the widely-usedpre-training and fine-tuning strategy and some other popular semi-supervisedtechniques.</description><author>Xiaodong Cui, A F M Saif, Songtao Lu, Lisha Chen, Tianyi Chen, Brian Kingsbury, George Saon</author><pubDate>Wed, 11 Dec 2024 17:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08548v1</guid></item><item><title>Improving Satellite Imagery Masking using Multi-task and Transfer Learning</title><link>http://arxiv.org/abs/2412.08545v1</link><description>Many remote sensing applications employ masking of pixels in satelliteimagery for subsequent measurements. For example, estimating water qualityvariables, such as Suspended Sediment Concentration (SSC) requires isolatingpixels depicting water bodies unaffected by clouds, their shadows, terrainshadows, and snow and ice formation. A significant bottleneck is the relianceon a variety of data products (e.g., satellite imagery, elevation maps), and alack of precision in individual steps affecting estimation accuracy. We proposeto improve both the accuracy and computational efficiency of masking bydeveloping a system that predicts all required masks from Harmonized Landsatand Sentinel (HLS) imagery. Our model employs multi-tasking to sharecomputation and enable higher accuracy across tasks. We experiment with recentadvances in deep network architectures and show that masking models can benefitfrom these, especially when combined with pre-training on large satelliteimagery datasets. We present a collection of models offering differentspeed/accuracy trade-offs for masking. MobileNet variants are the fastest, andperform competitively with larger architectures. Transformer-basedarchitectures are the slowest, but benefit the most from pre-training on largesatellite imagery datasets. Our models provide a 9% F1 score improvementcompared to previous work on water pixel identification. When integrated withan SSC estimation system, our models result in a 30x speedup while reducingestimation error by 2.64 mg/L, allowing for global-scale analysis. We alsoevaluate our model on a recently proposed cloud and cloud shadow estimationbenchmark, where we outperform the current state-of-the-art model by at least6% in F1 score.</description><author>Rangel Daroya, Luisa Vieira Lucchese, Travis Simmons, Punwath Prum, Tamlin Pavelsky, John Gardner, Colin J. Gleason, Subhransu Maji</author><pubDate>Wed, 11 Dec 2024 17:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08545v1</guid></item><item><title>Training Data Reconstruction: Privacy due to Uncertainty?</title><link>http://arxiv.org/abs/2412.08544v1</link><description>Being able to reconstruct training data from the parameters of a neuralnetwork is a major privacy concern. Previous works have shown thatreconstructing training data, under certain circumstances, is possible. In thiswork, we analyse such reconstructions empirically and propose a new formulationof the reconstruction as a solution to a bilevel optimisation problem. Wedemonstrate that our formulation as well as previous approaches highly dependon the initialisation of the training images $x$ to reconstruct. In particular,we show that a random initialisation of $x$ can lead to reconstructions thatresemble valid training samples while not being part of the actual trainingdataset. Thus, our experiments on affine and one-hidden layer networks suggestthat when reconstructing natural images, yet an adversary cannot identifywhether reconstructed images have indeed been part of the set of trainingsamples.</description><author>Christina Runkel, Kanchana Vaishnavi Gandikota, Jonas Geiping, Carola-Bibiane Schönlieb, Michael Moeller</author><pubDate>Wed, 11 Dec 2024 17:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08544v1</guid></item><item><title>Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning</title><link>http://arxiv.org/abs/2410.06140v2</link><description>QUIC, a new and increasingly used transport protocol, enhances TCP byoffering improved security, performance, and stream multiplexing. Thesefeatures, however, also impose challenges for network middle-boxes that need tomonitor and analyze web traffic. This paper proposes a novel method to estimatethe number of HTTP/3 responses in a given QUIC connection by an observer. Thisestimation reveals server behavior, client-server interactions, and datatransmission efficiency, which is crucial for various applications such asdesigning a load balancing solution and detecting HTTP/3 flood attacks. Theproposed scheme transforms QUIC connection traces into image sequences and usesmachine learning (ML) models, guided by a tailored loss function, to predictresponse counts. Evaluations on more than seven million images-derived from100,000 traces collected across 44,000 websites over four months-achieve up to97% accuracy in both known and unknown server settings and 92% accuracy onpreviously unseen complete QUIC traces.</description><author>Barak Gahtan, Robert J. Shahla, Reuven Cohen, Alex M. Bronstein</author><pubDate>Wed, 11 Dec 2024 16:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06140v2</guid></item><item><title>Robust Multi-Agent Control via Maximum Entropy Heterogeneous-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2306.10715v5</link><description>In multi-agent reinforcement learning, optimal control with robustnessguarantees are critical for its deployment in real world. However, existingmethods face challenges related to sample complexity, training instability,potential suboptimal Nash Equilibrium convergence and non-robustness tomultiple perturbations. In this paper, we propose a unified framework forlearning \emph{stochastic} policies to resolve these issues. We embedcooperative MARL problems into probabilistic graphical models, from which wederive the maximum entropy (MaxEnt) objective optimal for MARL. Based on theMaxEnt framework, we propose \emph{Heterogeneous-Agent Soft Actor-Critic}(HASAC) algorithm. Theoretically, we prove the monotonic improvement andconvergence to \emph{quantal response equilibrium} (QRE) properties of HASAC.Furthermore, HASAC is provably robust against a wide range of real-worlduncertainties, including perturbations in rewards, environment dynamics,states, and actions. Finally, we generalize a unified template for MaxEntalgorithmic design named \emph{Maximum Entropy Heterogeneous-Agent MirrorLearning} (MEHAML), which provides any induced method with the same guaranteesas HASAC. We evaluate HASAC on seven benchmarks: Bi-DexHands, Multi-AgentMuJoCo, Pursuit-Evade, StarCraft Multi-Agent Challenge, Google ResearchFootball, Multi-Agent Particle Environment, Light Aircraft Game. Results showthat HASAC consistently outperforms strong baselines in 34 out of 38 tasks,exhibiting improved training stability, better sample efficiency and sufficientexploration. The robustness of HASAC was further validated when encounteringuncertainties in rewards, dynamics, states, and actions of 14 magnitudes, andreal-world deployment in a multi-robot arena against these four types ofuncertainties. See our page at \url{https://sites.google.com/view/meharl}.</description><author>Simin Li, Yifan Zhong, Jiarong Liu, Jianing Guo, Siyuan Qi, Ruixiao Xu, Xin Yu, Siyi Hu, Haobo Fu, Qiang Fu, Xiaojun Chang, Yujing Hu, Bo An, Xianglong Liu, Yaodong Yang</author><pubDate>Wed, 11 Dec 2024 16:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10715v5</guid></item><item><title>MaestroMotif: Skill Design from Artificial Intelligence Feedback</title><link>http://arxiv.org/abs/2412.08542v1</link><description>Describing skills in natural language has the potential to provide anaccessible way to inject human knowledge about decision-making into an AIsystem. We present MaestroMotif, a method for AI-assisted skill design, whichyields high-performing and adaptable agents. MaestroMotif leverages thecapabilities of Large Language Models (LLMs) to effectively create and reuseskills. It first uses an LLM's feedback to automatically design rewardscorresponding to each skill, starting from their natural language description.Then, it employs an LLM's code generation abilities, together withreinforcement learning, for training the skills and combining them to implementcomplex behaviors specified in language. We evaluate MaestroMotif using a suiteof complex tasks in the NetHack Learning Environment (NLE), demonstrating thatit surpasses existing approaches in both performance and usability.</description><author>Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos C. Machado, Pierluca D'Oro</author><pubDate>Wed, 11 Dec 2024 16:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08542v1</guid></item><item><title>Euclidean Fast Attention: Machine Learning Global Atomic Representations at Linear Cost</title><link>http://arxiv.org/abs/2412.08541v1</link><description>Long-range correlations are essential across numerous machine learning tasks,especially for data embedded in Euclidean space, where the relative positionsand orientations of distant components are often critical for accuratepredictions. Self-attention offers a compelling mechanism for capturing theseglobal effects, but its quadratic complexity presents a significant practicallimitation. This problem is particularly pronounced in computational chemistry,where the stringent efficiency requirements of machine learning force fields(MLFFs) often preclude accurately modeling long-range interactions. To addressthis, we introduce Euclidean fast attention (EFA), a linear-scalingattention-like mechanism designed for Euclidean data, which can be easilyincorporated into existing model architectures. A core component of EFA arenovel Euclidean rotary positional encodings (ERoPE), which enable efficientencoding of spatial information while respecting essential physical symmetries.We empirically demonstrate that EFA effectively captures diverse long-rangeeffects, enabling EFA-equipped MLFFs to describe challenging chemicalinteractions for which conventional MLFFs yield incorrect results.</description><author>J. Thorben Frank, Stefan Chmiela, Klaus-Robert Müller, Oliver T. Unke</author><pubDate>Wed, 11 Dec 2024 16:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08541v1</guid></item><item><title>SenCLIP: Enhancing zero-shot land-use mapping for Sentinel-2 with ground-level prompting</title><link>http://arxiv.org/abs/2412.08536v1</link><description>Pre-trained vision-language models (VLMs), such as CLIP, demonstrateimpressive zero-shot classification capabilities with free-form prompts andeven show some generalization in specialized domains. However, theirperformance on satellite imagery is limited due to the underrepresentation ofsuch data in their training sets, which predominantly consist of ground-levelimages. Existing prompting techniques for satellite imagery are oftenrestricted to generic phrases like a satellite image of ..., limiting theireffectiveness for zero-shot land-use and land-cover (LULC) mapping. To addressthese challenges, we introduce SenCLIP, which transfers CLIPs representation toSentinel-2 imagery by leveraging a large dataset of Sentinel-2 images pairedwith geotagged ground-level photos from across Europe. We evaluate SenCLIPalongside other SOTA remote sensing VLMs on zero-shot LULC mapping tasks usingthe EuroSAT and BigEarthNet datasets with both aerial and ground-levelprompting styles. Our approach, which aligns ground-level representations withsatellite imagery, demonstrates significant improvements in classificationaccuracy across both prompt styles, opening new possibilities for applyingfree-form textual descriptions in zero-shot LULC mapping.</description><author>Pallavi Jain, Dino Ienco, Roberto Interdonato, Tristan Berchoux, Diego Marcos</author><pubDate>Wed, 11 Dec 2024 16:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08536v1</guid></item><item><title>The BrowserGym Ecosystem for Web Agent Research</title><link>http://arxiv.org/abs/2412.05467v3</link><description>The BrowserGym ecosystem addresses the growing need for efficient evaluationand benchmarking of web agents, particularly those leveraging automation andLarge Language Models (LLMs) for web interaction tasks. Many existingbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,making it challenging to achieve reliable comparisons and reproducible results.BrowserGym aims to solve this by providing a unified, gym-like environment withwell-defined observation and action spaces, facilitating standardizedevaluation across diverse benchmarks. Combined with AgentLab, a complementaryframework that aids in agent creation, testing, and analysis, BrowserGym offersflexibility for integrating new benchmarks while ensuring consistent evaluationand comprehensive experiment management. This standardized approach seeks toreduce the time and complexity of developing web agents, supporting morereliable comparisons and facilitating in-depth analysis of agent behaviors, andcould result in more adaptable, capable agents, ultimately acceleratinginnovation in LLM-driven automation. As a supporting evidence, we conduct thefirst large-scale, multi-benchmark web agent experiment and compare theperformance of 6 state-of-the-art LLMs across all benchmarks currentlyavailable in BrowserGym. Among other findings, our results highlight a largediscrepancy between OpenAI and Anthropic's latests models, withClaude-3.5-Sonnet leading the way on almost all benchmarks, except onvision-related tasks where GPT-4o is superior. Despite these advancements, ourresults emphasize that building robust and efficient web agents remains asignificant challenge, due to the inherent complexity of real-world webenvironments and the limitations of current models.</description><author>Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, Alexandre Lacoste</author><pubDate>Wed, 11 Dec 2024 16:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05467v3</guid></item><item><title>Protecting Confidentiality, Privacy and Integrity in Collaborative Learning</title><link>http://arxiv.org/abs/2412.08534v1</link><description>A collaboration between dataset owners and model owners is needed tofacilitate effective machine learning (ML) training. During this collaboration,however, dataset owners and model owners want to protect the confidentiality oftheir respective assets (i.e., datasets, models and training code), with thedataset owners also caring about the privacy of individual users whose data isin their datasets. Existing solutions either provide limited confidentialityfor models and training code, or suffer from privacy issues due to collusion. We present Citadel++, a scalable collaborative ML training system designed tosimultaneously protect the confidentiality of datasets, models and trainingcode, as well as the privacy of individual users. Citadel++ enhancesdifferential privacy techniques to safeguard the privacy of individual userdata while maintaining model utility. By employing Virtual Machine-levelTrusted Execution Environments (TEEs) and improved integrity protectiontechniques through various OS-level mechanisms, Citadel++ effectively preservesthe confidentiality of datasets, models and training code, and enforces ourprivacy mechanisms even when the models and training code have been maliciouslydesigned. Our experiments show that Citadel++ provides privacy, model utilityand performance while adhering to confidentiality and privacy requirements ofdataset owners and model owners, outperforming the state-of-the-artprivacy-preserving training systems by up to 543x on CPU and 113x on GPU TEEs.</description><author>Dong Chen, Alice Dethise, Istemi Ekin Akkus, Ivica Rimac, Klaus Satzke, Antti Koskela, Marco Canini, Wei Wang, Ruichuan Chen</author><pubDate>Wed, 11 Dec 2024 16:48:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08534v1</guid></item><item><title>TECO: Improving Multimodal Intent Recognition with Text Enhancement through Commonsense Knowledge Extraction</title><link>http://arxiv.org/abs/2412.08529v1</link><description>The objective of multimodal intent recognition (MIR) is to leverage variousmodalities-such as text, video, and audio-to detect user intentions, which iscrucial for understanding human language and context in dialogue systems.Despite advances in this field, two main challenges persist: (1) effectivelyextracting and utilizing semantic information from robust textual features; (2)aligning and fusing non-verbal modalities with verbal ones effectively. Thispaper proposes a Text Enhancement with CommOnsense Knowledge Extractor (TECO)to address these challenges. We begin by extracting relations from bothgenerated and retrieved knowledge to enrich the contextual information in thetext modality. Subsequently, we align and integrate visual and acousticrepresentations with these enhanced text features to form a cohesive multimodalrepresentation. Our experimental results show substantial improvements overexisting baseline methods.</description><author>Quynh-Mai Thi Nguyen, Lan-Nhi Thi Nguyen, Cam-Van Thi Nguyen</author><pubDate>Wed, 11 Dec 2024 16:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08529v1</guid></item><item><title>Continual Learning for Encoder-only Language Models via a Discrete Key-Value Bottleneck</title><link>http://arxiv.org/abs/2412.08528v1</link><description>Continual learning remains challenging across various natural languageunderstanding tasks. When models are updated with new training data, they riskcatastrophic forgetting of prior knowledge. In the present work, we introduce adiscrete key-value bottleneck for encoder-only language models, allowing forefficient continual learning by requiring only localized updates. Inspired bythe success of a discrete key-value bottleneck in vision, we address new andNLP-specific challenges. We experiment with different bottleneck architecturesto find the most suitable variants regarding language, and present a genericdiscrete key initialization technique for NLP that is task independent. Weevaluate the discrete key-value bottleneck in four continual learning NLPscenarios and demonstrate that it alleviates catastrophic forgetting. Weshowcase that it offers competitive performance to other popular continuallearning methods, with lower computational costs.</description><author>Andor Diera, Lukas Galke, Fabian Karl, Ansgar Scherp</author><pubDate>Wed, 11 Dec 2024 16:38:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08528v1</guid></item><item><title>Differentially Private Low-dimensional Synthetic Data from High-dimensional Datasets</title><link>http://arxiv.org/abs/2305.17148v3</link><description>Differentially private synthetic data provide a powerful mechanism to enabledata analysis while protecting sensitive information about individuals.However, when the data lie in a high-dimensional space, the accuracy of thesynthetic data suffers from the curse of dimensionality. In this paper, wepropose a differentially private algorithm to generate low-dimensionalsynthetic data efficiently from a high-dimensional dataset with a utilityguarantee with respect to the Wasserstein distance. A key step of our algorithmis a private principal component analysis (PCA) procedure with a near-optimalaccuracy bound that circumvents the curse of dimensionality. Unlike thestandard perturbation analysis, our analysis of private PCA works withoutassuming the spectral gap for the covariance matrix.</description><author>Yiyun He, Thomas Strohmer, Roman Vershynin, Yizhe Zhu</author><pubDate>Wed, 11 Dec 2024 16:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17148v3</guid></item><item><title>Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective</title><link>http://arxiv.org/abs/2411.14654v2</link><description>Large Language Models (LLMs) have revolutionized natural language processing(NLP) by delivering state-of-the-art performance across a variety of tasks.Among these, Transformer-based models like BERT and GPT rely on pooling layersto aggregate token-level embeddings into sentence-level representations. Commonpooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role inthis aggregation process. Despite their widespread use, the comparativeperformance of these strategies on different LLM architectures remainsunderexplored. To address this gap, this paper investigates the effects ofthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in thecontext of sentence-level sentiment analysis. Comprehensive experiments revealthat each pooling mechanism exhibits unique strengths and weaknesses dependingon the task's specific requirements. Our findings underline the importance ofselecting pooling methods tailored to the demands of particular applications,prompting a re-evaluation of common assumptions regarding pooling operations.By offering actionable insights, this study contributes to the optimization ofLLM-based models for downstream tasks.</description><author>Jinming Xing, Ruilin Xing, Yan Sun</author><pubDate>Wed, 11 Dec 2024 16:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14654v2</guid></item><item><title>Spend More to Save More (SM2): An Energy-Aware Implementation of Successive Halving for Sustainable Hyperparameter Optimization</title><link>http://arxiv.org/abs/2412.08526v1</link><description>A fundamental step in the development of machine learning models commonlyinvolves the tuning of hyperparameters, often leading to multiple modeltraining runs to work out the best-performing configuration. As machinelearning tasks and models grow in complexity, there is an escalating need forsolutions that not only improve performance but also address sustainabilityconcerns. Existing strategies predominantly focus on maximizing the performanceof the model without considering energy efficiency. To bridge this gap, in thispaper, we introduce Spend More to Save More (SM2), an energy-awarehyperparameter optimization implementation based on the widely adoptedsuccessive halving algorithm. Unlike conventional approaches includingenergy-intensive testing of individual hyperparameter configurations, SM2employs exploratory pretraining to identify inefficient configurations withminimal energy expenditure. Incorporating hardware characteristics andreal-time energy consumption tracking, SM2 identifies an optimal configurationthat not only maximizes the performance of the model but also enablesenergy-efficient training. Experimental validations across various datasets,models, and hardware setups confirm the efficacy of SM2 to prevent the waste ofenergy during the training of hyperparameter configurations.</description><author>Daniel Geissler, Bo Zhou, Sungho Suh, Paul Lukowicz</author><pubDate>Wed, 11 Dec 2024 16:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08526v1</guid></item><item><title>Learning to Decouple the Lights for 3D Face Texture Modeling</title><link>http://arxiv.org/abs/2412.08524v1</link><description>Existing research has made impressive strides in reconstructing human facialshapes and textures from images with well-illuminated faces and minimalexternal occlusions. Nevertheless, it remains challenging to recover accuratefacial textures from scenarios with complicated illumination affected byexternal occlusions, e.g. a face that is partially obscured by items such as ahat. Existing works based on the assumption of single and uniform illuminationcannot correctly process these data. In this work, we introduce a novelapproach to model 3D facial textures under such unnatural illumination. Insteadof assuming single illumination, our framework learns to imitate the unnaturalillumination as a composition of multiple separate light conditions combinedwith learned neural representations, named Light Decoupling. According toexperiments on both single images and video sequences, we demonstrate theeffectiveness of our approach in modeling facial textures under challengingillumination affected by occlusions. Please checkhttps://tianxinhuang.github.io/projects/Deface for our videos and codes.</description><author>Tianxin Huang, Zhenyu Zhang, Ying Tai, Gim Hee Lee</author><pubDate>Wed, 11 Dec 2024 16:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08524v1</guid></item><item><title>EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache Compression Based on Global-Local Importance</title><link>http://arxiv.org/abs/2412.08521v1</link><description>As large language models (LLMs) continue to advance, the demand for higherquality and faster processing of long contexts across various applications isgrowing. KV cache is widely adopted as it stores previously generated key andvalue tokens, effectively reducing redundant computations during inference.However, as memory overhead becomes a significant concern, efficientcompression of KV cache has gained increasing attention. Most existing methodsperform compression from two perspectives: identifying important tokens anddesigning compression strategies. However, these approaches often producebiased distributions of important tokens due to the influence of accumulatedattention scores or positional encoding. Furthermore, they overlook thesparsity and redundancy across different heads, which leads to difficulties inpreserving the most effective information at the head level. To this end, wepropose EMS to overcome these limitations, while achieving better KV cachecompression under extreme compression ratios. Specifically, we introduce aGlobal-Local score that combines accumulated attention scores from both globaland local KV tokens to better identify the token importance. For thecompression strategy, we design an adaptive and unified Evict-then-Mergeframework that accounts for the sparsity and redundancy of KV tokens acrossdifferent heads. Additionally, we implement the head-wise parallel compressionthrough a zero-class mechanism to enhance efficiency. Extensive experimentsdemonstrate our SOTA performance even under extreme compression ratios. EMSconsistently achieves the lowest perplexity, improves scores by over 1.28points across four LLMs on LongBench under a 256 cache budget, and preserves95% retrieval accuracy with a cache budget less than 2% of the context lengthin the Needle-in-a-Haystack task.</description><author>Yingxin Li, Ye Li, Yuan Meng, Xinzhu Ma, Zihan Geng, Shutao Xia, Zhi Wang</author><pubDate>Wed, 11 Dec 2024 16:35:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08521v1</guid></item><item><title>GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek</title><link>http://arxiv.org/abs/2412.08520v1</link><description>We present GR-NLP-TOOLKIT, an open-source natural language processing (NLP)toolkit developed specifically for modern Greek. The toolkit providesstate-of-the-art performance in five core NLP tasks, namely part-of-speechtagging, morphological tagging, dependency parsing, named entity recognition,and Greeklishto-Greek transliteration. The toolkit is based on pre-trainedTransformers, it is freely available, and can be easily installed in Python(pip install gr-nlp-toolkit). It is also accessible through a demonstrationplatform on HuggingFace, along with a publicly available API for non-commercialuse. We discuss the functionality provided for each task, the underlyingmethods, experiments against comparable open-source toolkits, and futurepossible enhancements. The toolkit is available at:https://github.com/nlpaueb/gr-nlp-toolkit</description><author>Lefteris Loukas, Nikolaos Smyrnioudis, Chrysa Dikonomaki, Spyros Barbakos, Anastasios Toumazatos, John Koutsikakis, Manolis Kyriakakis, Mary Georgiou, Stavros Vassos, John Pavlopoulos, Ion Androutsopoulos</author><pubDate>Wed, 11 Dec 2024 16:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08520v1</guid></item><item><title>Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2412.08519v1</link><description>The reranker and generator are two critical components in theRetrieval-Augmented Generation (i.e., RAG) pipeline, responsible for rankingrelevant documents and generating responses. However, due to differences inpre-training data and objectives, there is an inevitable gap between thedocuments ranked as relevant by the reranker and those required by thegenerator to support answering the query. To address this gap, we proposeRADIO, a novel and practical preference alignment framework with RAtionaleDIstillatiOn. Specifically, We first propose a rationale extraction method thatleverages the reasoning capabilities of Large Language Models (LLMs) to extractthe rationales necessary for answering the query. Subsequently, arationale-based alignment process is designed to rerank the documents based onthe extracted rationales, and fine-tune the reranker to align the preferences.We conduct extensive experiments on two tasks across three datasets todemonstrate the effectiveness of our approach compared to baseline methods. Ourcode is released online to ease reproduction.</description><author>Pengyue Jia, Derong Xu, Xiaopeng Li, Zhaocheng Du, Xiangyang Li, Xiangyu Zhao, Yichao Wang, Yuhao Wang, Huifeng Guo, Ruiming Tang</author><pubDate>Wed, 11 Dec 2024 16:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08519v1</guid></item><item><title>MathDSL: A Domain-Specific Language for Concise Mathematical Solutions Via Program Synthesis</title><link>http://arxiv.org/abs/2409.17490v3</link><description>We present MathDSL, a Domain-Specific Language (DSL) for mathematicalequation solving, which, when deployed in program synthesis models, outperformsstate-of-the-art reinforcement-learning-based methods. We also introduce aquantitative metric for measuring the conciseness of a mathematical solutionand demonstrate the improvement in the quality of generated solutions comparedto other methods. Our system demonstrates that a program synthesis system(DreamCoder) using MathDSL can generate programs that solve linear equationswith greater accuracy and conciseness than using reinforcement learningsystems. Additionally, we demonstrate that if we use the action spaces ofprevious reinforcement learning systems as DSLs, MathDSL outperforms theaction-space-DSLs. We use DreamCoder to store equation-solving strategies aslearned abstractions in its program library and demonstrate that by usingMathDSL, these can be converted into human-interpretable solution strategiesthat could have applications in mathematical education.</description><author>Sagnik Anupam, Maddy Bowers, Omar Costilla-Reyes, Armando Solar-Lezama</author><pubDate>Wed, 11 Dec 2024 16:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17490v3</guid></item><item><title>Vision-and-Language Pretraining</title><link>http://arxiv.org/abs/2207.01772v3</link><description>With the burgeoning amount of data of image-text pairs and diversity ofVision-and-Language (V\&amp;L) tasks, scholars have introduced an abundance of deeplearning models in this research domain. Furthermore, in recent years, transferlearning has also shown tremendous success in Computer Vision for tasks such asImage Classification, Object Detection, etc., and in Natural LanguageProcessing for Question Answering, Machine Translation, etc. Inheriting thespirit of Transfer Learning, research works in V\&amp;L have devised multiplepretraining techniques on large-scale datasets in order to enhance theperformance of downstream tasks. The aim of this article is to provide acomprehensive revision of contemporary V\&amp;L pretraining models. In particular,we categorize and delineate pretraining approaches, along with the summary ofstate-of-the-art vision-and-language pretrained models. Moreover, a list oftraining datasets and downstream tasks is supplied to further polish theperspective into V\&amp;L pretraining. Lastly, we decided to take a further step todiscuss numerous directions for future research.</description><author>Thong Nguyen, Cong-Duy Nguyen, Xiaobao Wu, See-Kiong Ng, Anh Tuan Luu</author><pubDate>Wed, 11 Dec 2024 16:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.01772v3</guid></item><item><title>Self-Interested Agents in Collaborative Learning: An Incentivized Adaptive Data-Centric Framework</title><link>http://arxiv.org/abs/2412.06597v2</link><description>We propose a framework for adaptive data-centric collaborative learning amongself-interested agents, coordinated by an arbiter. Designed to handle theincremental nature of real-world data, the framework operates in an onlinemanner: at each step, the arbiter collects a batch of data from agents, trainsa machine learning model, and provides each agent with a distinct modelreflecting its data contributions. This setup establishes a feedback loop whereshared data influence model updates, and the resulting models guide futuredata-sharing strategies. Agents evaluate and partition their data, selecting apartition to share using a stochastic parameterized policy optimized via policygradient methods to optimize the utility of the received model as defined byagent-specific evaluation functions. On the arbiter side, the expected lossfunction over the true data distribution is optimized, incorporatingagent-specific weights to account for distributional differences arising fromdiverse sources and selective sharing. A bilevel optimization algorithm jointlylearns the model parameters and agent-specific weights. Mean-zero noise,computed using a distortion function that adjusts these agent-specific weights,is introduced to generate distinct agent-specific models, promoting valuabledata sharing without requiring separate training. Our framework is underpinnedby non-asymptotic analyses, ensuring convergence of the agent-side policyoptimization to an approximate stationary point of the evaluation functions andconvergence of the arbiter-side optimization to an approximate stationary pointof the expected loss function.</description><author>Nithia Vijayan, Bryan Kian Hsiang Low</author><pubDate>Wed, 11 Dec 2024 16:28:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06597v2</guid></item><item><title>Open-Canopy: Towards Very High Resolution Forest Monitoring</title><link>http://arxiv.org/abs/2407.09392v4</link><description>Estimating canopy height and its changes at meter resolution from satelliteimagery is a significant challenge in computer vision with criticalenvironmental applications. However, the lack of open-access datasets at thisresolution hinders the reproducibility and evaluation of models. We introduceOpen-Canopy, the first open-access, country-scale benchmark for veryhigh-resolution (1.5 m) canopy height estimation, covering over 87,000 km$^2$across France with 1.5 m resolution satellite imagery and aerial LiDAR data.Additionally, we present Open-Canopy-$\Delta$, a benchmark for canopy heightchange detection between images from different years at tree level-achallenging task for current computer vision models. We evaluatestate-of-the-art architectures on these benchmarks, highlighting significantchallenges and opportunities for improvement. Our datasets and code arepublicly available at https://github.com/fajwel/Open-Canopy.</description><author>Fajwel Fogel, Yohann Perron, Nikola Besic, Laurent Saint-André, Agnès Pellissier-Tanon, Martin Schwartz, Thomas Boudras, Ibrahim Fayad, Alexandre d'Aspremont, Loic Landrieu, Philippe Ciais</author><pubDate>Wed, 11 Dec 2024 16:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09392v4</guid></item><item><title>INRetouch: Context Aware Implicit Neural Representation for Photography Retouching</title><link>http://arxiv.org/abs/2412.03848v2</link><description>Professional photo editing remains challenging, requiring extensive knowledgeof imaging pipelines and significant expertise. With the ubiquity of smartphonephotography, there is an increasing demand for accessible yet sophisticatedimage editing solutions. While recent deep learning approaches, particularlystyle transfer methods, have attempted to automate this process, they oftenstruggle with output fidelity, editing control, and complex retouchingcapabilities. We propose a novel retouch transfer approach that learns fromprofessional edits through before-after image pairs, enabling precisereplication of complex editing operations. To facilitate this researchdirection, we introduce a comprehensive Photo Retouching Dataset comprising100,000 high-quality images edited using over 170 professional Adobe Lightroompresets. We develop a context-aware Implicit Neural Representation that learnsto apply edits adaptively based on image content and context, requiring nopretraining and capable of learning from a single example. Our method extractsimplicit transformations from reference edits and adaptively applies them tonew images. Through extensive evaluation, we demonstrate that our approach notonly surpasses existing methods in photo retouching but also enhancesperformance in related image reconstruction tasks like Gamut Mapping and RawReconstruction. By bridging the gap between professional editing capabilitiesand automated solutions, our work presents a significant step toward makingsophisticated photo editing more accessible while maintaining high-fidelityresults. Check the Project Page at https://omaralezaby.github.io/inretouch formore Results and information about Code and Dataset availability.</description><author>Omar Elezabi, Marcos V. Conde, Zongwei Wu, Radu Timofte</author><pubDate>Wed, 11 Dec 2024 16:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03848v2</guid></item><item><title>Enhancing Interpretability Through Loss-Defined Classification Objective in Structured Latent Spaces</title><link>http://arxiv.org/abs/2412.08515v1</link><description>Supervised machine learning often operates on the data-driven paradigm,wherein internal model parameters are autonomously optimized to convergepredicted outputs with the ground truth, devoid of explicitly programming rulesor a priori assumptions. Although data-driven methods have yielded notablesuccesses across various benchmark datasets, they inherently treat models asopaque entities, thereby limiting their interpretability and yielding a lack ofexplanatory insights into their decision-making processes. In this work, weintroduce Latent Boost, a novel approach that integrates advanced distancemetric learning into supervised classification tasks, enhancing bothinterpretability and training efficiency. Thus during training, the model isnot only optimized for classification metrics of the discrete data points butalso adheres to the rule that the collective representation zones of each classshould be sharply clustered. By leveraging the rich structural insights ofintermediate model layer latent representations, Latent Boost improvesclassification interpretability, as demonstrated by higher Silhouette scores,while accelerating training convergence. These performance and latentstructural benefits are achieved with minimum additional cost, making itbroadly applicable across various datasets without requiring data-specificadjustments. Furthermore, Latent Boost introduces a new paradigm for aligningclassification performance with improved model transparency to address thechallenges of black-box models.</description><author>Daniel Geissler, Bo Zhou, Mengxi Liu, Paul Lukowicz</author><pubDate>Wed, 11 Dec 2024 16:25:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08515v1</guid></item><item><title>Image-Based Malware Classification Using QR and Aztec Codes</title><link>http://arxiv.org/abs/2412.08514v1</link><description>In recent years, the use of image-based techniques for malware detection hasgained prominence, with numerous studies demonstrating the efficacy of deeplearning approaches such as Convolutional Neural Networks (CNN) in classifyingimages derived from executable files. In this paper, we consider an innovativemethod that relies on an image conversion process that consists of transformingfeatures extracted from executable files into QR and Aztec codes. These codescapture structural patterns in a format that may enhance the learningcapabilities of CNNs. We design and implement CNN architectures tailored to theunique properties of these codes and apply them to a comprehensive analysisinvolving two extensive malware datasets, both of which include a significantcorpus of benign samples. Our results yield a split decision, with CNNs trainedon QR and Aztec codes outperforming the state of the art on one of thedatasets, but underperforming more typical techniques on the other dataset.These results indicate that the use of QR and Aztec codes as a form of featureengineering holds considerable promise in the malware domain, and thatadditional research is needed to better understand the relative strengths andweaknesses of such an approach.</description><author>Atharva Khadilkar, Mark Stamp</author><pubDate>Wed, 11 Dec 2024 16:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08514v1</guid></item><item><title>REPEAT: Improving Uncertainty Estimation in Representation Learning Explainability</title><link>http://arxiv.org/abs/2412.08513v1</link><description>Incorporating uncertainty is crucial to provide trustworthy explanations ofdeep learning models. Recent works have demonstrated how uncertainty modelingcan be particularly important in the unsupervised field of representationlearning explainable artificial intelligence (R-XAI). Current R-XAI methodsprovide uncertainty by measuring variability in the importance score. However,they fail to provide meaningful estimates of whether a pixel is certainlyimportant or not. In this work, we propose a new R-XAI method called REPEATthat addresses the key question of whether or not a pixel is \textit{certainly}important. REPEAT leverages the stochasticity of current R-XAI methods toproduce multiple estimates of importance, thus considering each pixel in animage as a Bernoulli random variable that is either important or unimportant.From these Bernoulli random variables we can directly estimate the importanceof a pixel and its associated certainty, thus enabling users to determinecertainty in pixel importance. Our extensive evaluation shows that REPEAT givescertainty estimates that are more intuitive, better at detectingout-of-distribution data, and more concise.</description><author>Kristoffer K. Wickstrøm, Thea Brüsch, Michael C. Kampffmeyer, Robert Jenssen</author><pubDate>Wed, 11 Dec 2024 16:24:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08513v1</guid></item><item><title>Combining Neural Fields and Deformation Models for Non-Rigid 3D Motion Reconstruction from Partial Data</title><link>http://arxiv.org/abs/2412.08511v1</link><description>We introduce a novel, data-driven approach for reconstructing temporallycoherent 3D motion from unstructured and potentially partial observations ofnon-rigidly deforming shapes. Our goal is to achieve high-fidelity motionreconstructions for shapes that undergo near-isometric deformations, such ashumans wearing loose clothing. The key novelty of our work lies in its abilityto combine implicit shape representations with explicit mesh-based deformationmodels, enabling detailed and temporally coherent motion reconstructionswithout relying on parametric shape models or decoupling shape and motion. Eachframe is represented as a neural field decoded from a feature space whereobservations over time are fused, hence preserving geometric details present inthe input data. Temporal coherence is enforced with a near-isometricdeformation constraint between adjacent frames that applies to the underlyingsurface in the neural field. Our method outperforms state-of-the-artapproaches, as demonstrated by its application to human and animal motionsequences reconstructed from monocular depth videos.</description><author>Aymen Merrouche, Stefanie Wuhrer, Edmond Boyer</author><pubDate>Wed, 11 Dec 2024 16:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08511v1</guid></item><item><title>Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information</title><link>http://arxiv.org/abs/2409.01179v2</link><description>With the advancement of large-scale language modeling techniques, largemultimodal models combining visual encoders with large language models havedemonstrated exceptional performance in various visual tasks. Most of thecurrent large-scale multimodal models achieve this by mapping visual featuresobtained from the visual encoder into a large language model and using them asinputs alongside text for downstream tasks. Therefore, the number of visualtokens directly affects the training and inference speed of the model. Therehas been significant work on token pruning for visual transformers, but forlarge multimodal models, only relying on visual information for token pruningor compression may lead to significant loss of important information. On theother hand, the textual input in the form of a question may contain valuableinformation that can aid in answering the question, providing additionalknowledge to the model. To address the potential oversimplification andexcessive pruning that can occur with most purely visual token pruning methods,we propose a text information-guided dynamic visual token recovery mechanismthat does not require training. This mechanism leverages the similarity betweenthe question text and visual tokens to recover visually meaningful tokens withimportant text information while merging other less important tokens.Experimental results demonstrate that our proposed method achieves comparableperformance to the original approach while compressing the visual tokens to anaverage of 10% of the original quantity. Our source code will be made publiclyavailable following acceptance.</description><author>Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu</author><pubDate>Wed, 11 Dec 2024 16:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01179v2</guid></item><item><title>Comparative Opinion Mining in Product Reviews: Multi-perspective Prompt-based Learning</title><link>http://arxiv.org/abs/2412.08508v1</link><description>Comparative reviews are pivotal in understanding consumer preferences andinfluencing purchasing decisions. Comparative Quintuple Extraction (COQE) aimsto identify five key components in text: the target entity, compared entities,compared aspects, opinions on these aspects, and polarity. Extracting precisecomparative information from product reviews is challenging due to nuancedlanguage and sequential task errors in traditional methods. To mitigate theseproblems, we propose MTP-COQE, an end-to-end model designed for COQE.Leveraging multi-perspective prompt-based learning, MTP-COQE effectively guidesthe generative model in comparative opinion mining tasks. Evaluation on theCamera-COQE (English) and VCOM (Vietnamese) datasets demonstrates MTP-COQE'sefficacy in automating COQE, achieving superior performance with a 1.41% higherF1 score than the previous baseline models on the English dataset.Additionally, we designed a strategy to limit the generative model's creativityto ensure the output meets expectations. We also performed data augmentation toaddress data imbalance and to prevent the model from becoming biased towardsdominant samples.</description><author>Hai-Yen Thi Nguyen, Cam-Van Thi Nguyen</author><pubDate>Wed, 11 Dec 2024 16:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08508v1</guid></item><item><title>Orchestrating the Symphony of Prompt Distribution Learning for Human-Object Interaction Detection</title><link>http://arxiv.org/abs/2412.08506v1</link><description>Human-object interaction (HOI) detectors with popular query-transformerarchitecture have achieved promising performance. However, accuratelyidentifying uncommon visual patterns and distinguishing between ambiguous HOIscontinue to be difficult for them. We observe that these difficulties may arisefrom the limited capacity of traditional detector queries in representingdiverse intra-category patterns and inter-category dependencies. To addressthis, we introduce the Interaction Prompt Distribution Learning (InterProDa)approach. InterProDa learns multiple sets of soft prompts and estimatescategory distributions from various prompts. It then incorporates HOI querieswith category distributions, making them capable of representing near-infiniteintra-category dynamics and universal cross-category relationships. OurInterProDa detector demonstrates competitive performance on HICO-DET and vcocobenchmarks. Additionally, our method can be integrated into mosttransformer-based HOI detectors, significantly enhancing their performance withminimal additional parameters.</description><author>Mingda Jia, Liming Zhao, Ge Li, Yun Zheng</author><pubDate>Wed, 11 Dec 2024 16:18:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08506v1</guid></item><item><title>Weighted Ensemble Models Are Strong Continual Learners</title><link>http://arxiv.org/abs/2312.08977v4</link><description>In this work, we study the problem of continual learning (CL) where the goalis to learn a model on a sequence of tasks, such that the data from theprevious tasks becomes unavailable while learning on the current task data. CLis essentially a balancing act between being able to learn on the new task(i.e., plasticity) and maintaining the performance on the previously learnedconcepts (i.e., stability). Intending to address the stability-plasticitytrade-off, we propose to perform weight-ensembling of the model parameters ofthe previous and current tasks. This weighted-ensembled model, which we callContinual Model Averaging (or CoMA), attains high accuracy on the current taskby leveraging plasticity, while not deviating too far from the previous weightconfiguration, ensuring stability. We also propose an improved variant of CoMA,named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectivelyweighs each parameter in the weights ensemble by leveraging the Fisherinformation of the weights of the model. Both variants are conceptually simple,easy to implement, and effective in attaining state-of-the-art performance onseveral standard CL benchmarks. Code is available at:https://github.com/IemProg/CoFiMA.</description><author>Imad Eddine Marouf, Subhankar Roy, Enzo Tartaglione, Stéphane Lathuilière</author><pubDate>Wed, 11 Dec 2024 16:18:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08977v4</guid></item><item><title>PointTalk: Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based Talking Head Synthesis</title><link>http://arxiv.org/abs/2412.08504v1</link><description>Talking head synthesis with arbitrary speech audio is a crucial challenge inthe field of digital humans. Recently, methods based on radiance fields havereceived increasing attention due to their ability to synthesize high-fidelityand identity-consistent talking heads from just a few minutes of trainingvideo. However, due to the limited scale of the training data, these methodsoften exhibit poor performance in audio-lip synchronization and visual quality.In this paper, we propose a novel 3D Gaussian-based method called PointTalk,which constructs a static 3D Gaussian field of the head and deforms it in syncwith the audio. It also incorporates an audio-driven dynamic lip point cloud asa critical component of the conditional information, thereby facilitating theeffective synthesis of talking heads. Specifically, the initial step involvesgenerating the corresponding lip point cloud from the audio signal andcapturing its topological structure. The design of the dynamic differenceencoder aims to capture the subtle nuances inherent in dynamic lip movementsmore effectively. Furthermore, we integrate the audio-point enhancement module,which not only ensures the synchronization of the audio signal with thecorresponding lip point cloud within the feature space, but also facilitates adeeper understanding of the interrelations among cross-modal conditionalfeatures. Extensive experiments demonstrate that our method achieves superiorhigh-fidelity and audio-lip synchronization in talking head synthesis comparedto previous methods.</description><author>Yifan Xie, Tao Feng, Xin Zhang, Xiangyang Luo, Zixuan Guo, Weijiang Yu, Heng Chang, Fei Ma, Fei Richard Yu</author><pubDate>Wed, 11 Dec 2024 16:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08504v1</guid></item><item><title>StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements</title><link>http://arxiv.org/abs/2412.08503v1</link><description>Text-driven style transfer aims to merge the style of a reference image withcontent described by a text prompt. Recent advancements in text-to-image modelshave improved the nuance of style transformations, yet significant challengesremain, particularly with overfitting to reference styles, limiting stylisticcontrol, and misaligning with textual content. In this paper, we propose threecomplementary strategies to address these issues. First, we introduce across-modal Adaptive Instance Normalization (AdaIN) mechanism for betterintegration of style and text features, enhancing alignment. Second, we developa Style-based Classifier-Free Guidance (SCFG) approach that enables selectivecontrol over stylistic elements, reducing irrelevant influences. Finally, weincorporate a teacher model during early generation stages to stabilize spatiallayouts and mitigate artifacts. Our extensive evaluations demonstratesignificant improvements in style transfer quality and alignment with textualprompts. Furthermore, our approach can be integrated into existing styletransfer frameworks without fine-tuning.</description><author>Mingkun Lei, Xue Song, Beier Zhu, Hao Wang, Chi Zhang</author><pubDate>Wed, 11 Dec 2024 16:13:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08503v1</guid></item><item><title>AI Can Enhance Creativity in Social Networks</title><link>http://arxiv.org/abs/2410.15264v3</link><description>Can peer recommendation engines elevate people's creative performances inself-organizing social networks? Answering this question requires resolvingchallenges in data collection (e.g., tracing inspiration links andpsycho-social attributes of nodes) and intervention design (e.g., balancingidea stimulation and redundancy in evolving information environments). Wetrained a model that predicts people's ideation performances using semantic andnetwork-structural features in an online platform. Using this model, we builtSocialMuse, which maximizes people's predicted performances to generate peerrecommendations for them. We found treatment networks leveraging SocialMuseoutperforming AI-agnostic control networks in several creativity measures. Thetreatment networks were more decentralized than the control, as SocialMuseincreasingly emphasized network-structural features at large network sizes.This decentralization spreads people's inspiration sources, helping inspiredideas stand out better. Our study provides actionable insights into buildingintelligent systems for elevating creativity.</description><author>Raiyan Abdul Baten, Ali Sarosh Bangash, Krish Veera, Gourab Ghoshal, Ehsan Hoque</author><pubDate>Wed, 11 Dec 2024 16:11:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15264v3</guid></item><item><title>GradStop: Exploring Training Dynamics in Unsupervised Outlier Detection through Gradient Cohesion</title><link>http://arxiv.org/abs/2412.08501v1</link><description>Unsupervised Outlier Detection (UOD) is a critical task in data mining andmachine learning, aiming to identify instances that significantly deviate fromthe majority. Without any label, deep UOD methods struggle with themisalignment between the model's direct optimization goal and the finalperformance goal of Outlier Detection (OD) task. Through the perspective oftraining dynamics, this paper proposes an early stopping algorithm to optimizethe training of deep UOD models, ensuring they perform optimally in OD ratherthan overfitting the entire contaminated dataset. Inspired by UOD mechanism and inlier priority phenomenon, where intuitivelymodels fit inliers more quickly than outliers, we propose GradStop, asampling-based label-free algorithm to estimate model's real-time performanceduring training. First, a sampling method generates two sets: one likelycontaining more outliers and the other more inliers, then a metric based ongradient cohesion is applied to probe into current training dynamics, whichreflects model's performance on OD task. Experimental results on 4 deep UOD algorithms and 47 real-world datasets andtheoretical proofs demonstrate the effectiveness of our proposed early stoppingalgorithm in enhancing the performance of deep UOD models. Auto Encoder (AE)enhanced by GradStop achieves better performance than itself, other SOTA UODmethods, and even ensemble AEs. Our method provides a robust and effectivesolution to the problem of performance degradation during training, enablingdeep UOD models to achieve better potential in anomaly detection tasks.</description><author>Yuang Zhang, Liping Wang, Yihong Huang, Yuanxing Zheng</author><pubDate>Wed, 11 Dec 2024 16:07:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08501v1</guid></item><item><title>AgentMixer: Multi-Agent Correlated Policy Factorization</title><link>http://arxiv.org/abs/2401.08728v2</link><description>In multi-agent reinforcement learning, centralized training withdecentralized execution (CTDE) methods typically assume that agents makedecisions based on their local observations independently, which may not leadto a correlated joint policy with coordination. Coordination can be explicitlyencouraged during training and individual policies can be trained to imitatethe correlated joint policy. However, this may lead to an \textit{asymmetriclearning failure} due to the observation mismatch between the joint andindividual policies. Inspired by the concept of correlated equilibrium, weintroduce a \textit{strategy modification} called AgentMixer that allows agentsto correlate their policies. AgentMixer combines individual partiallyobservable policies into a joint fully observable policy non-linearly. Toenable decentralized execution, we introduce\textit{Individual-Global-Consistency} to guarantee mode consistency duringjoint training of the centralized and decentralized policies and prove thatAgentMixer converges to an $\epsilon$-approximate Correlated Equilibrium. Inthe Multi-Agent MuJoCo, SMAC-v2, Matrix Game, and Predator-Prey benchmarks,AgentMixer outperforms or matches state-of-the-art methods.</description><author>Zhiyuan Li, Wenshuai Zhao, Lijun Wu, Joni Pajarinen</author><pubDate>Wed, 11 Dec 2024 16:06:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08728v2</guid></item><item><title>Diverse Similarity Encoder for Deep GAN Inversion</title><link>http://arxiv.org/abs/2108.10201v3</link><description>Current deep generative adversarial networks (GANs) can synthesizehigh-quality (HQ) images, so learning representation with GANs is favorable.GAN inversion is one of emerging approaches that study how to invert imagesinto latent space. Existing GAN encoders can invert images on StyleGAN, butcannot adapt to other deep GANs. We propose a novel approach to address thisissue. By evaluating diverse similarity in latent vectors and images, we designan adaptive encoder, named diverse similarity encoder (DSE), that can beexpanded to a variety of state-of-the-art GANs. DSE makes GANs reconstructhigher fidelity images from HQ images, no matter whether they are synthesizedor real images. DSE has unified convolutional blocks and adapts well tomainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN.</description><author>Cheng Yu, Wenmin Wang, Roberto Bugiolacchi</author><pubDate>Wed, 11 Dec 2024 16:03:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.10201v3</guid></item><item><title>Extreme AutoML: Analysis of Classification, Regression, and NLP Performance</title><link>http://arxiv.org/abs/2412.07000v2</link><description>Utilizing machine learning techniques has always required choosinghyperparameters. This is true whether one uses a classical technique such as aKNN or very modern neural networks such as Deep Learning. Though in manyapplications, hyperparameters are chosen by hand, automated methods have becomeincreasingly more common. These automated methods have become collectivelyknown as automated machine learning, or AutoML. Several automated selectionalgorithms have shown similar or improved performance over state-of-the-artmethods. This breakthrough has led to the development of cloud-based serviceslike Google AutoML, which is based on Deep Learning and is widely considered tobe the industry leader in AutoML services. Extreme Learning Machines (ELMs) usea fundamentally different type of neural architecture, producing better resultsat a significantly discounted computational cost. We benchmark the ExtremeAutoML technology against Google's AutoML using several popular classificationdata sets from the University of California at Irvine's (UCI) repository, andseveral other data sets, observing significant advantages for Extreme AutoML inaccuracy, Jaccard Indices, the variance of Jaccard Indices across classes (i.e.class variance) and training times.</description><author>Edward Ratner, Elliot Farmer, Brandon Warner, Christopher Douglas, Amaury Lendasse</author><pubDate>Wed, 11 Dec 2024 15:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07000v2</guid></item><item><title>SuperCode: Sustainability PER AI-driven CO-DEsign</title><link>http://arxiv.org/abs/2412.08490v1</link><description>Currently, data-intensive scientific applications require vast amounts ofcompute resources to deliver world-leading science. The climate emergency hasmade it clear that unlimited use of resources (e.g., energy) for scientificdiscovery is no longer acceptable. Future computing hardware promises to bemuch more energy efficient, but without better optimized software this cannotreach its full potential. In this vision paper, we propose a generic AI-drivenco-design methodology, using specialized Large Language Models (like ChatGPT),to effectively generate efficient code for emerging computing hardware. Wedescribe how we will validate our methodology with two radio astronomyapplications, with sustainability as the key performance indicator. This paperis a modified version of our accepted SuperCode project proposal. We present ithere in this form to introduce the vision behind this project and todisseminate the work in the spirit of Open Science and transparency. Anadditional aim is to collect feedback, invite potential collaboration partnersand use-cases to join the project.</description><author>P. Chris Broekema, Rob V. van Nieuwpoort</author><pubDate>Wed, 11 Dec 2024 15:54:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08490v1</guid></item><item><title>A Dual-Module Denoising Approach with Curriculum Learning for Enhancing Multimodal Aspect-Based Sentiment Analysis</title><link>http://arxiv.org/abs/2412.08489v1</link><description>Multimodal Aspect-Based Sentiment Analysis (MABSA) combines text and imagesto perform sentiment analysis but often struggles with irrelevant or misleadingvisual information. Existing methodologies typically address eithersentence-image denoising or aspect-image denoising but fail to comprehensivelytackle both types of noise. To address these limitations, we propose DualDe, anovel approach comprising two distinct components: the Hybrid CurriculumDenoising Module (HCD) and the Aspect-Enhance Denoising Module (AED). The HCDmodule enhances sentence-image denoising by incorporating a flexible curriculumlearning strategy that prioritizes training on clean data. Concurrently, theAED module mitigates aspect-image noise through an aspect-guided attentionmechanism that filters out noisy visual regions which unrelated to the specificaspects of interest. Our approach demonstrates effectiveness in addressing bothsentence-image and aspect-image noise, as evidenced by experimental evaluationson benchmark datasets.</description><author>Nguyen Van Doan, Dat Tran Nguyen, Cam-Van Thi Nguyen</author><pubDate>Wed, 11 Dec 2024 15:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08489v1</guid></item></channel></rss>