<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 16 Nov 2023 06:00:49 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Single-Image 3D Human Digitization with Shape-Guided Diffusion</title><link>http://arxiv.org/abs/2311.09221v1</link><description>We present an approach to generate a 360-degree view of a person with aconsistent, high-resolution appearance from a single input image. NeRF and itsvariants typically require videos or images from different viewpoints. Mostexisting approaches taking monocular input either rely on ground-truth 3D scansfor supervision or lack 3D consistency. While recent 3D generative models showpromise of 3D consistent human digitization, these approaches do not generalizewell to diverse clothing appearances, and the results lack photorealism. Unlikeexisting work, we utilize high-capacity 2D diffusion models pretrained forgeneral image synthesis tasks as an appearance prior of clothed humans. Toachieve better 3D consistency while retaining the input identity, weprogressively synthesize multiple views of the human in the input image byinpainting missing regions with shape-guided diffusion conditioned onsilhouette and surface normal. We then fuse these synthesized multi-view imagesvia inverse rendering to obtain a fully textured high-resolution 3D mesh of thegiven person. Experiments show that our approach outperforms prior methods andachieves photorealistic 360-degree synthesis of a wide range of clothed humanswith complex textures from a single image.</description><author>Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, Jia-Bin Huang</author><pubDate>Wed, 15 Nov 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09221v1</guid></item><item><title>DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model</title><link>http://arxiv.org/abs/2311.09217v1</link><description>We propose \textbf{DMV3D}, a novel 3D generation approach that uses atransformer-based 3D large reconstruction model to denoise multi-viewdiffusion. Our reconstruction model incorporates a triplane NeRF representationand can denoise noisy multi-view images via NeRF reconstruction and rendering,achieving single-stage 3D generation in $\sim$30s on single A100 GPU. We train\textbf{DMV3D} on large-scale multi-view image datasets of highly diverseobjects using only image reconstruction losses, without accessing 3D assets. Wedemonstrate state-of-the-art results for the single-image reconstructionproblem where probabilistic modeling of unseen object parts is required forgenerating diverse reconstructions with sharp textures. We also showhigh-quality text-to-3D generation results outperforming previous 3D diffusionmodels. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .</description><author>Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, Kai Zhang</author><pubDate>Wed, 15 Nov 2023 18:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09217v1</guid></item><item><title>Assessing Translation capabilities of Large Language Models involving English and Indian Languages</title><link>http://arxiv.org/abs/2311.09216v1</link><description>Generative Large Language Models (LLMs) have achieved remarkable advancementsin various NLP tasks. In this work, our aim is to explore the multilingualcapabilities of large language models by using machine translation as a taskinvolving English and 22 Indian languages. We first investigate the translationcapabilities of raw large language models, followed by exploring the in-contextlearning capabilities of the same raw models. We fine-tune these large languagemodels using parameter efficient fine-tuning methods such as LoRA andadditionally with full fine-tuning. Through our study, we have identified thebest performing large language model for the translation task involving LLMs,which is based on LLaMA. Our results demonstrate significant progress, with average BLEU scores of13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99,42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b forEnglish to Indian languages on IN22 (conversational), IN22 (general),flores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, forIndian languages to English, we achieved average BLEU scores of 14.03, 16.65,16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51,and 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational),IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets.Overall, our findings highlight the potential and strength of large languagemodels for machine translation capabilities, including for languages that arecurrently underrepresented in LLMs.</description><author>Vandan Mujadia, Ashok Urlana, Yash Bhaskar, Penumalla Aditya Pavani, Kukkapalli Shravya, Parameswari Krishnamurthy, Dipti Misra Sharma</author><pubDate>Wed, 15 Nov 2023 18:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09216v1</guid></item><item><title>Limitations of neural network training due to numerical instability of backpropagation</title><link>http://arxiv.org/abs/2210.00805v4</link><description>We study the training of deep neural networks by gradient descent wherefloating-point arithmetic is used to compute the gradients. In this frameworkand under realistic assumptions, we demonstrate that it is highly unlikely tofind ReLU neural networks that maintain, in the course of training withgradient descent, superlinearly many affine pieces with respect to their numberof layers. In virtually all approximation theoretical arguments that yieldhigh-order polynomial rates of approximation, sequences of ReLU neural networkswith exponentially many affine pieces compared to their numbers of layers areused. As a consequence, we conclude that approximating sequences of ReLU neuralnetworks resulting from gradient descent in practice differ substantially fromtheoretically constructed sequences. The assumptions and the theoreticalresults are compared to a numerical study, which yields concurring results.</description><author>Clemens Karner, Vladimir Kazeev, Philipp Christian Petersen</author><pubDate>Wed, 15 Nov 2023 18:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00805v4</guid></item><item><title>ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy</title><link>http://arxiv.org/abs/2311.09215v1</link><description>Modern computer vision offers a great variety of models to practitioners, andselecting a model from multiple options for specific applications can bechallenging. Conventionally, competing model architectures and trainingprotocols are compared by their classification accuracy on ImageNet. However,this single metric does not fully capture performance nuances critical forspecialized tasks. In this work, we conduct an in-depth comparative analysis ofmodel behaviors beyond ImageNet accuracy, for both ConvNet and VisionTransformer architectures, each across supervised and CLIP training paradigms.Although our selected models have similar ImageNet accuracies and computerequirements, we find that they differ in many other aspects: types ofmistakes, output calibration, transferability, and feature invariance, amongothers. This diversity in model characteristics, not captured by traditionalmetrics, highlights the need for more nuanced analysis when choosing amongdifferent models. Our code is available athttps://github.com/kirill-vish/Beyond-INet.</description><author>Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu</author><pubDate>Wed, 15 Nov 2023 18:56:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09215v1</guid></item><item><title>Schema-Driven Information Extraction from Heterogeneous Tables</title><link>http://arxiv.org/abs/2305.14336v2</link><description>In this paper, we explore the question of whether large language models cansupport cost-efficient information extraction from tables. We introduceschema-driven information extraction, a new task that transforms tabular datainto structured records following a human-authored schema. To assess variousLLM's capabilities on this task, we develop a benchmark composed of tables fromfour diverse domains: machine learning papers, chemistry literature, materialscience journals, and webpages. Alongside the benchmark, we present anextraction method based on instruction-tuned LLMs. Our approach showscompetitive performance without task-specific labels, achieving F1 scoresranging from 74.2 to 96.1, while maintaining great cost efficiency. Moreover,we validate the possibility of distilling compact table-extraction models toreduce API reliance, as well as extraction from image tables using multi-modalmodels. By developing a benchmark and demonstrating the feasibility of thistask using proprietary models, we aim to support future work on open-sourceschema-driven IE models.</description><author>Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, Alan Ritter</author><pubDate>Wed, 15 Nov 2023 18:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14336v2</guid></item><item><title>Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models</title><link>http://arxiv.org/abs/2311.09214v1</link><description>Large language models (LLMs) have achieved remarkable advancements in thefield of natural language processing. However, the sheer scale andcomputational demands of these models present formidable challenges whenconsidering their practical deployment in resource-constrained contexts. Whiletechniques such as chain-of-thought (CoT) distillation have displayed promisein distilling LLMs into small language models (SLMs), there is a risk thatdistilled SLMs may still carry over flawed reasoning or hallucinationsinherited from their LLM counterparts. To address these issues, we propose atwofold methodology: First, we introduce a novel method for distilling theself-evaluation capability inherent in LLMs into SLMs, which aims to mitigatethe adverse effects of erroneous reasoning and reduce hallucinations. Second,we advocate for a comprehensive distillation process that incorporates multipledistinct chain-of-thought and self-evaluation paradigms and ensures a moreholistic and robust knowledge transfer into SLMs. Experiments on three NLPbenchmarks demonstrate that our method significantly improves the performanceof distilled SLMs and sheds light on the path towards developing smaller modelsclosely aligned with human cognition.</description><author>Weize Liu, Guocong Li, Kai Zhang, Bang Du, Qiyuan Chen, Xuming Hu, Hongxia Xu, Jintai Chen, Jian Wu</author><pubDate>Wed, 15 Nov 2023 18:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09214v1</guid></item><item><title>GRIM: GRaph-based Interactive narrative visualization for gaMes</title><link>http://arxiv.org/abs/2311.09213v1</link><description>Dialogue-based Role Playing Games (RPGs) require powerful storytelling. Thenarratives of these may take years to write and typically involve a largecreative team. In this work, we demonstrate the potential of large generativetext models to assist this process. \textbf{GRIM}, a prototype\textbf{GR}aph-based \textbf{I}nteractive narrative visualization system forga\textbf{M}es, generates a rich narrative graph with branching storylines thatmatch a high-level narrative description and constraints provided by thedesigner. Game designers can interactively edit the graph by automaticallygenerating new sub-graphs that fit the edits within the original narrative andconstraints. We illustrate the use of \textbf{GRIM} in conjunction with GPT-4,generating branching narratives for four well-known stories with differentcontextual constraints.</description><author>Jorge Leandro, Sudha Rao, Michael Xu, Weijia Xu, Nebosja Jojic, Chris Brockett, Bill Dolan</author><pubDate>Wed, 15 Nov 2023 18:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09213v1</guid></item><item><title>Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects -- A Survey</title><link>http://arxiv.org/abs/2311.09212v1</link><description>Generic text summarization approaches often fail to address the specificintent and needs of individual users. Recently, scholarly attention has turnedto the development of summarization methods that are more closely tailored andcontrolled to align with specific objectives and user needs. While a growingcorpus of research is devoted towards a more controllable summarization, thereis no comprehensive survey available that thoroughly explores the diversecontrollable aspects or attributes employed in this context, delves into theassociated challenges, and investigates the existing solutions. In this survey,we formalize the Controllable Text Summarization (CTS) task, categorizecontrollable aspects according to their shared characteristics and objectives,and present a thorough examination of existing methods and datasets within eachcategory. Moreover, based on our findings, we uncover limitations and researchgaps, while also delving into potential solutions and future directions forCTS.</description><author>Ashok Urlana, Pruthwik Mishra, Tathagato Roy, Rahul Mishra</author><pubDate>Wed, 15 Nov 2023 18:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09212v1</guid></item><item><title>Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2311.09210v1</link><description>Retrieval-augmented language models (RALMs) represent a substantialadvancement in the capabilities of large language models, notably in reducingfactual hallucination by leveraging external knowledge sources. However, thereliability of the retrieved information is not always guaranteed. Theretrieval of irrelevant data can lead to misguided responses, and potentiallycausing the model to overlook its inherent knowledge, even when it possessesadequate information to address the query. Moreover, standard RALMs oftenstruggle to assess whether they possess adequate knowledge, both intrinsic andretrieved, to provide an accurate answer. In situations where knowledge islacking, these systems should ideally respond with "unknown" when the answer isunattainable. In response to these challenges, we introduces Chain-of-Noting(CoN), a novel approach aimed at improving the robustness of RALMs in facingnoisy, irrelevant documents and in handling unknown scenarios. The core idea ofCoN is to generate sequential reading notes for retrieved documents, enabling athorough evaluation of their relevance to the given question and integratingthis information to formulate the final answer. We employed ChatGPT to createtraining data for CoN, which was subsequently trained on an LLaMa-2 7B model.Our experiments across four open-domain QA benchmarks show that RALMs equippedwith CoN significantly outperform standard RALMs. Notably, CoN achieves anaverage improvement of +7.9 in EM score given entirely noisy retrieveddocuments and +10.5 in rejection rates for real-time questions that falloutside the pre-training knowledge scope.</description><author>Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu</author><pubDate>Wed, 15 Nov 2023 18:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09210v1</guid></item><item><title>TableLlama: Towards Open Large Generalist Models for Tables</title><link>http://arxiv.org/abs/2311.09206v1</link><description>Semi-structured tables are ubiquitous. There has been a variety of tasks thataim to automatically interpret, augment, and query tables. Current methodsoften require pretraining on tables or special model architecture design, arerestricted to specific table types, or have simplifying assumptions abouttables and tasks. This paper makes the first step towards developingopen-source large language models (LLMs) as generalists for a diversity oftable-based tasks. Towards that end, we construct TableInstruct, a new datasetwith a variety of realistic tables and tasks, for instruction tuning andevaluating LLMs. We further develop the first open-source generalist model fortables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address thelong context challenge. We experiment under both in-domain setting andout-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achievescomparable or better performance than the SOTA for each task, despite thelatter often has task-specific design. On 6 out-of-domain datasets, it achieves6-48 absolute point gains compared with the base model, showing that trainingon TableInstruct enhances the model's generalizability. We will open-source ourdataset and trained model to boost future work on developing open generalistmodels for tables.</description><author>Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun</author><pubDate>Wed, 15 Nov 2023 18:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09206v1</guid></item><item><title>When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages</title><link>http://arxiv.org/abs/2311.09205v1</link><description>Multilingual language models are widely used to extend NLP systems tolow-resource languages. However, concrete evidence for the effects ofmultilinguality on language modeling performance in individual languagesremains scarce. Here, we pre-train over 10,000 monolingual and multilinguallanguage models for over 250 languages, including multiple language familiesthat are under-studied in NLP. We assess how language modeling performance ineach language varies as a function of (1) monolingual dataset size, (2) addedmultilingual dataset size, (3) linguistic similarity of the added languages,and (4) model size (up to 45M parameters). We find that in moderation, addingmultilingual data improves low-resource language modeling performance, similarto increasing low-resource dataset sizes by up to 33%. Improvements depend onthe syntactic similarity of the added multilingual data, with marginaladditional effects of vocabulary overlap. However, high-resource languagesconsistently perform worse in multilingual pre-training scenarios. As datasetsizes increase, adding multilingual data begins to hurt performance for bothlow-resource and high-resource languages, likely due to limited model capacity(the "curse of multilinguality"). These results suggest that massivelymultilingual pre-training may not be optimal for any languages involved, butthat more targeted models can significantly improve performance.</description><author>Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen</author><pubDate>Wed, 15 Nov 2023 18:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09205v1</guid></item><item><title>Fusion-Eval: Integrating Evaluators with LLMs</title><link>http://arxiv.org/abs/2311.09204v1</link><description>Evaluating Large Language Models (LLMs) is a complex task, especiallyconsidering the intricacies of natural language understanding and theexpectations for high-level reasoning. Traditional evaluations typically leanon human-based, model-based, or automatic-metrics-based paradigms, each withits own advantages and shortcomings. We introduce "Fusion-Eval", a system thatemploys LLMs not solely for direct evaluations, but to skillfully integrateinsights from diverse evaluators. This gives Fusion-Eval flexibility, enablingit to work effectively across diverse tasks and make optimal use of multiplereferences. In testing on the SummEval dataset, Fusion-Eval achieved a Spearmancorrelation of 0.96, outperforming other evaluators. The success of Fusion-Evalunderscores the potential of LLMs to produce evaluations that closely alignhuman perspectives, setting a new standard in the field of LLM evaluation.</description><author>Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, Lei Meng</author><pubDate>Wed, 15 Nov 2023 18:46:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09204v1</guid></item><item><title>ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD</title><link>http://arxiv.org/abs/2311.09200v1</link><description>In this pioneering work we formulate ExpM+NF, a method for training machinelearning (ML) on private data with pre-specified differentially privacyguarantee $\varepsilon&gt;0, \delta=0$, by using the Exponential Mechanism (ExpM)and an auxiliary Normalizing Flow (NF). We articulate theoretical benefits ofExpM+NF over Differentially Private Stochastic Gradient Descent (DPSGD), thestate-of-the-art (SOTA) and de facto method for differentially private ML, andwe empirically test ExpM+NF against DPSGD using the SOTA implementation (Opacuswith PRV accounting) in multiple classification tasks on the Adult Dataset(census data) and MIMIC-III Dataset (electronic healthcare records) usingLogistic Regression and GRU-D, a deep learning recurrent neural network with~20K-100K parameters. In all experiments, ExpM+NF achieves greater than 93% ofthe non-private training accuracy (AUC) for $\varepsilon \in [1\mathrm{e}{-3},1]$, exhibiting greater accuracy (higher AUC) and privacy (lower $\varepsilon$with $\delta=0$) than DPSGD. Differentially private ML generally considers$\varepsilon \in [1,10]$ to maintain reasonable accuracy; hence, ExpM+NF'sability to provide strong accuracy for orders of magnitude better privacy(smaller $\varepsilon$) substantially pushes what is currently possible indifferentially private ML. Training time results are presented showing ExpM+NFis comparable to (slightly faster) than DPSGD. Code for these experiments willbe provided after review. Limitations and future directions are provided.</description><author>Robert A. Bridges, Vandy J. Tombs, Christopher B. Stanley</author><pubDate>Wed, 15 Nov 2023 18:43:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09200v1</guid></item><item><title>Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering</title><link>http://arxiv.org/abs/2311.09198v1</link><description>While large language models (LLMs) are equipped with longer text inputcapabilities than before, they are struggling to seek correct information inlong contexts. The "lost in the middle" problem challenges most LLMs, referringto the dramatic decline in accuracy when correct information is located in themiddle. To overcome this crucial issue, this paper proposes to enhance theinformation searching and reflection ability of LLMs in long contexts viaspecially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).Following these tasks, our model excels in focusing more precisely on thedesired information. Experimental results show substantial improvement inMulti-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%absolute gain in shuffled settings, by 21.5% in passage retrieval task. Werelease our model, Ziya-Reader to promote related research in the community.</description><author>He Junqing, Pan Kunhao, Dong Xiaoqun, Song Zhuoyang, Liu Yibo, Liang Yuxin, Wang Hao, Sun Qianguo, Zhang Songxin, Xie Zejian, Zhang Jiaxing</author><pubDate>Wed, 15 Nov 2023 18:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09198v1</guid></item><item><title>A Unified Approach to Learning Ising Models: Beyond Independence and Bounded Width</title><link>http://arxiv.org/abs/2311.09197v1</link><description>We revisit the problem of efficiently learning the underlying parameters ofIsing models from data. Current algorithmic approaches achieve essentiallyoptimal sample complexity when given i.i.d. samples from the stationary measureand the underlying model satisfies "width" bounds on the total $\ell_1$interaction involving each node. We show that a simple existing approach basedon node-wise logistic regression provably succeeds at recovering the underlyingmodel in several new settings where these assumptions are violated: (1) Given dynamically generated data from a wide variety of local Markovchains, like block or round-robin dynamics, logistic regression recovers theparameters with optimal sample complexity up to $\log\log n$ factors. Thisgeneralizes the specialized algorithm of Bresler, Gamarnik, and Shah [IEEETrans. Inf. Theory'18] for structure recovery in bounded degree graphs fromGlauber dynamics. (2) For the Sherrington-Kirkpatrick model of spin glasses, given$\mathsf{poly}(n)$ independent samples, logistic regression recovers theparameters in most of the known high-temperature regime via a simple reductionto weaker structural properties of the measure. This improves on recent work ofAnari, Jain, Koehler, Pham, and Vuong [ArXiv'23] which gives distributionlearning at higher temperature. (3) As a simple byproduct of our techniques, logistic regression achieves anexponential improvement in learning from samples in the M-regime of dataconsidered by Dutt, Lokhov, Vuffray, and Misra [ICML'21] as well as novelguarantees for learning from the adversarial Glauber dynamics of Chin, Moitra,Mossel, and Sandon [ArXiv'23]. Our approach thus significantly generalizes the elegant analysis of Wu,Sanghavi, and Dimakis [Neurips'19] without any algorithmic modification.</description><author>Jason Gaitonde, Elchanan Mossel</author><pubDate>Wed, 15 Nov 2023 18:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09197v1</guid></item><item><title>Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge</title><link>http://arxiv.org/abs/2311.09195v1</link><description>A significant bottleneck in applying current reinforcement learningalgorithms to real-world scenarios is the need to reset the environment betweenevery episode. This reset process demands substantial human intervention,making it difficult for the agent to learn continuously and autonomously.Several recent works have introduced autonomous reinforcement learning (ARL)algorithms that generate curricula for jointly training reset and forwardpolicies. While their curricula can reduce the number of required manual resetsby taking into account the agent's learning progress, they rely ontask-specific knowledge, such as predefined initial states or reset rewardfunctions. In this paper, we propose a novel ARL algorithm that can generate acurriculum adaptive to the agent's learning progress without task-specificknowledge. Our curriculum empowers the agent to autonomously reset to diverseand informative initial states. To achieve this, we introduce a successdiscriminator that estimates the success probability from each initial statewhen the agent follows the forward policy. The success discriminator is trainedwith relabeled transitions in a self-supervised manner. Our experimentalresults demonstrate that our ARL algorithm can generate an adaptive curriculumand enable the agent to efficiently bootstrap to solve sparse-reward mazenavigation tasks, outperforming baselines with significantly fewer manualresets.</description><author>Sang-Hyun Lee, Seung-Woo Seo</author><pubDate>Wed, 15 Nov 2023 18:40:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09195v1</guid></item><item><title>Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models</title><link>http://arxiv.org/abs/2311.09194v1</link><description>Abstract grammatical knowledge - of parts of speech and grammatical patterns- is key to the capacity for linguistic generalization in humans. But howabstract is grammatical knowledge in large language models? In the humanliterature, compelling evidence for grammatical abstraction comes fromstructural priming. A sentence that shares the same grammatical structure as apreceding sentence is processed and produced more readily. Because confoundsexist when using stimuli in a single language, evidence of abstraction is evenmore compelling from crosslingual structural priming, where use of a syntacticstructure in one language primes an analogous structure in another language. Wemeasure crosslingual structural priming in large language models, comparingmodel behavior to human experimental results from eight crosslingualexperiments covering six languages, and four monolingual structural primingexperiments in three non-English languages. We find evidence for abstractmonolingual and crosslingual grammatical representations in the models thatfunction similarly to those found in humans. These results demonstrate thatgrammatical representations in multilingual language models are not onlysimilar across languages, but they can causally influence text produced indifferent languages.</description><author>James A. Michaelov, Catherine Arnett, Tyler A. Chang, Benjamin K. Bergen</author><pubDate>Wed, 15 Nov 2023 18:39:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09194v1</guid></item><item><title>The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task</title><link>http://arxiv.org/abs/2311.09193v1</link><description>The study explores the effectiveness of the Chain-of-Thought approach, knownfor its proficiency in language tasks by breaking them down into sub-tasks andintermediate steps, in improving vision-language tasks that demandsophisticated perception and reasoning. We present the "Description thenDecision" strategy, which is inspired by how humans process signals. Thisstrategy significantly improves probing task performance by 50%, establishingthe groundwork for future research on reasoning paradigms in complexvision-language tasks.</description><author>Yifan Wu, Pengchuan Zhang, Wenhan Xiong, Barlas Oguz, James C. Gee, Yixin Nie</author><pubDate>Wed, 15 Nov 2023 18:39:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09193v1</guid></item><item><title>Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing</title><link>http://arxiv.org/abs/2304.02017v6</link><description>Large language models have revolutionized the field of artificialintelligence and have been used in various applications. Among these models,ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,it stands out as a powerful tool that has been widely adopted. ChatGPT has beensuccessfully applied in numerous areas, including chatbots, content generation,language translation, personalized recommendations, and even medical diagnosisand treatment. Its success in these applications can be attributed to itsability to generate human-like responses, understand natural language, andadapt to different contexts. Its versatility and accuracy make it a powerfultool for natural language processing (NLP). However, there are also limitationsto ChatGPT, such as its tendency to produce biased responses and its potentialto perpetuate harmful language patterns. This article provides a comprehensiveoverview of ChatGPT, its applications, advantages, and limitations.Additionally, the paper emphasizes the importance of ethical considerationswhen using this robust tool in real-world scenarios. Finally, This papercontributes to ongoing discussions surrounding artificial intelligence and itsimpact on vision and NLP domains by providing insights into prompt engineeringtechniques.</description><author>Walid Hariri</author><pubDate>Wed, 15 Nov 2023 18:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02017v6</guid></item><item><title>Domain Aligned CLIP for Few-shot Classification</title><link>http://arxiv.org/abs/2311.09191v1</link><description>Large vision-language representation learning models like CLIP havedemonstrated impressive performance for zero-shot transfer to downstream taskswhile largely benefiting from inter-modal (image-text) alignment viacontrastive objectives. This downstream performance can further be enhanced byfull-scale fine-tuning which is often compute intensive, requires largelabelled data, and can reduce out-of-distribution (OOD) robustness.Furthermore, sole reliance on inter-modal alignment might overlook the richinformation embedded within each individual modality. In this work, weintroduce a sample-efficient domain adaptation strategy for CLIP, termed DomainAligned CLIP (DAC), which improves both intra-modal (image-image) andinter-modal alignment on target distributions without fine-tuning the mainmodel. For intra-modal alignment, we introduce a lightweight adapter that isspecifically trained with an intra-modal contrastive objective. To improveinter-modal alignment, we introduce a simple framework to modulate theprecomputed class text embeddings. The proposed few-shot fine-tuning frameworkis computationally efficient, robust to distribution shifts, and does not alterCLIP's parameters. We study the effectiveness of DAC by benchmarking on 11widely used image classification tasks with consistent improvements in 16-shotclassification upon strong baselines by about 2.3% and demonstrate competitiveperformance on 4 OOD robustness benchmarks.</description><author>Muhammad Waleed Gondal, Jochen Gast, Inigo Alonso Ruiz, Richard Droste, Tommaso Macri, Suren Kumar, Luitpold Staudigl</author><pubDate>Wed, 15 Nov 2023 18:34:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09191v1</guid></item><item><title>On the Computation of the Gaussian Rate-Distortion-Perception Function</title><link>http://arxiv.org/abs/2311.09190v1</link><description>In this paper, we study the computation of the rate-distortion-perceptionfunction (RDPF) for a multivariate Gaussian source under mean squared error(MSE) distortion and, respectively, Kullback-Leibler divergence, geometricJensen-Shannon divergence, squared Hellinger distance, and squaredWasserstein-2 distance perception metrics. To this end, we first characterizethe analytical bounds of the scalar Gaussian RDPF for the aforementioneddivergence functions, also providing the RDPF-achieving forward "test-channel"realization. Focusing on the multivariate case, we establish that, fortensorizable distortion and perception metrics, the optimal solution resides onthe vector space spanned by the eigenvector of the source covariance matrix.Consequently, the multivariate optimization problem can be expressed as afunction of the scalar Gaussian RDPFs of the source marginals, constrained byglobal distortion and perception levels. Leveraging this characterization, wedesign an alternating minimization scheme based on the block nonlinearGauss-Seidel method, which optimally solves the problem while identifying theGaussian RDPF-achieving realization. Furthermore, the associated algorithmicembodiment is provided, as well as the convergence and the rate of convergencecharacterization. Lastly, for the "perfect realism" regime, the analyticalsolution for the multivariate Gaussian RDPF is obtained. We corroborate ourresults with numerical simulations and draw connections to existing results.</description><author>Giuseppe Serra, Photios A. Stavrou, Marios Kountouris</author><pubDate>Wed, 15 Nov 2023 18:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09190v1</guid></item><item><title>PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health</title><link>http://arxiv.org/abs/2311.09189v1</link><description>Recently, there has been a growing interest in utilizing large languagemodels (LLMs) in mental health research, with studies showcasing theirremarkable capabilities, such as disease detection. However, there is currentlya lack of a comprehensive benchmark for evaluating the capability of LLMs inthis domain. Therefore, we address this gap by introducing the firstcomprehensive benchmark tailored to the unique characteristics of the mentalhealth domain. This benchmark encompasses a total of six sub-tasks, coveringthree dimensions, to systematically assess the capabilities of LLMs in therealm of mental health. We have designed corresponding concise prompts for eachsub-task. And we comprehensively evaluate a total of eight advanced LLMs usingour benchmark. Experiment results not only demonstrate significant room forimprovement in current LLMs concerning mental health but also unveil potentialdirections for future model optimization.</description><author>Haoan Jin, Siyuan Chen, Mengyue Wu, Kenny Q. Zhu</author><pubDate>Wed, 15 Nov 2023 18:32:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09189v1</guid></item><item><title>Scheming AIs: Will AIs fake alignment during training in order to get power?</title><link>http://arxiv.org/abs/2311.08379v2</link><description>This report examines whether advanced AIs that perform well in training willbe doing so in order to gain power later -- a behavior I call "scheming" (alsosometimes called "deceptive alignment"). I conclude that scheming is adisturbingly plausible outcome of using baseline machine learning methods totrain goal-directed AIs sophisticated enough to scheme (my subjectiveprobability on such an outcome, given these conditions, is roughly 25%). Inparticular: if performing well in training is a good strategy for gaining power(as I think it might well be), then a very wide variety of goals would motivatescheming -- and hence, good training performance. This makes it plausible thattraining might either land on such a goal naturally and then reinforce it, oractively push a model's motivations towards such a goal as an easy way ofimproving performance. What's more, because schemers pretend to be aligned ontests designed to reveal their motivations, it may be quite difficult to tellwhether this has occurred. However, I also think there are reasons for comfort.In particular: scheming may not actually be such a good strategy for gainingpower; various selection pressures in training might work against schemer-likegoals (for example, relative to non-schemers, schemers need to engage in extrainstrumental reasoning, which might harm their training performance); and wemay be able to increase such pressures intentionally. The report discussesthese and a wide variety of other considerations in detail, and it suggests anarray of empirical research directions for probing the topic further.</description><author>Joe Carlsmith</author><pubDate>Wed, 15 Nov 2023 18:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08379v2</guid></item><item><title>Towards Verifiable Text Generation with Symbolic References</title><link>http://arxiv.org/abs/2311.09188v1</link><description>Large language models (LLMs) have demonstrated an impressive ability tosynthesize plausible and fluent text. However they remain vulnerable tohallucinations, and thus their outputs generally require manual humanverification for high-stakes applications, which can be time-consuming anddifficult. This paper proposes symbolically grounded generation (SymGen) as asimple approach for enabling easier validation of an LLM's output. SymGenprompts an LLM to interleave its regular output text with explicit symbolicreferences to fields present in some conditioning data (e.g., a table in JSONformat). The references can be used to display the provenance of differentspans of text in the generation, reducing the effort required for manualverification. Across data-to-text and question answering experiments, we findthat LLMs are able to directly output text that makes use of symbolicreferences while maintaining fluency and accuracy.</description><author>Lucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim</author><pubDate>Wed, 15 Nov 2023 18:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09188v1</guid></item><item><title>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</title><link>http://arxiv.org/abs/2311.09184v1</link><description>While large language models (LLMs) already achieve strong performance onstandard generic summarization benchmarks, their performance on more complexsummarization task settings is less studied. Therefore, we benchmark LLMs oninstruction controllable text summarization, where the model input consists ofboth a source article and a natural language requirement for the desiredsummary characteristics. To this end, we curate an evaluation-only dataset forthis task setting and conduct human evaluation on 5 LLM-based summarizationsystems. We then benchmark LLM-based automatic evaluation for this task with 4different evaluation protocols and 11 LLMs, resulting in 40 evaluation methodsin total. Our study reveals that instruction controllable text summarizationremains a challenging task for LLMs, since (1) all LLMs evaluated still makefactual and other types of errors in their summaries; (2) all LLM-basedevaluation methods cannot achieve a strong alignment with human annotators whenjudging the quality of candidate summaries; (3) different LLMs show largeperformance gaps in summary generation and evaluation. We make our collectedbenchmark, InstruSum, publicly available to facilitate future research in thisdirection.</description><author>Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</author><pubDate>Wed, 15 Nov 2023 18:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09184v1</guid></item><item><title>Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems</title><link>http://arxiv.org/abs/2307.08423v2</link><description>Advances in artificial intelligence (AI) are fueling a new paradigm ofdiscoveries in natural sciences. Today, AI has started to advance naturalsciences by improving, accelerating, and enabling our understanding of naturalphenomena at a wide range of spatial and temporal scales, giving rise to a newarea of research known as AI for science (AI4Science). Being an emergingresearch paradigm, AI4Science is unique in that it is an enormous and highlyinterdisciplinary area. Thus, a unified and technical treatment of this fieldis needed yet challenging. This work aims to provide a technically thoroughaccount of a subarea of AI4Science; namely, AI for quantum, atomistic, andcontinuum systems. These areas aim at understanding the physical world from thesubatomic (wavefunctions and electron density), atomic (molecules, proteins,materials, and interactions), to macro (fluids, climate, and subsurface) scalesand form an important subarea of AI4Science. A unique advantage of focusing onthese areas is that they largely share a common set of challenges, therebyallowing a unified and foundational treatment. A key common challenge is how tocapture physics first principles, especially symmetries, in natural systems bydeep learning methods. We provide an in-depth yet intuitive account oftechniques to achieve equivariance to symmetry transformations. We also discussother common technical challenges, including explainability,out-of-distribution generalization, knowledge transfer with foundation andlarge language models, and uncertainty quantification. To facilitate learningand education, we provide categorized lists of resources that we found to beuseful. We strive to be thorough and unified and hope this initial effort maytrigger more community interests and efforts to further advance AI4Science.</description><author>Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, YuQing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stärk, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Ladera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Alán Aspuru-Guzik, Erik Bekkers, Michael Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Liò, Rose Yu, Stephan Günnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi J</author><pubDate>Wed, 15 Nov 2023 18:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08423v2</guid></item><item><title>AutoMix: Automatically Mixing Language Models</title><link>http://arxiv.org/abs/2310.12963v2</link><description>Large language models (LLMs) are now available in various sizes andconfigurations from cloud API providers. While this diversity offers a broadspectrum of choices, effectively leveraging the options to optimizecomputational cost and performance remains challenging. In this work, wepresent AutoMix, an approach that strategically routes queries to larger LMs,based on the approximate correctness of outputs from a smaller LM. Central toAutoMix is a few-shot self-verification mechanism, which estimates thereliability of its own outputs without requiring training. Given thatverifications can be noisy, we employ a meta verifier in AutoMix to refine theaccuracy of these assessments. Our experiments using LLAMA2-13/70B, on fivecontext-grounded reasoning datasets demonstrate that AutoMix surpassesestablished baselines, improving the incremental benefit per cost by up to 89%.Our code and data are available at https://github.com/automix-llm/automix.</description><author>Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, Manaal Faruqui</author><pubDate>Wed, 15 Nov 2023 18:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12963v2</guid></item><item><title>ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models</title><link>http://arxiv.org/abs/2311.09182v1</link><description>In recent times, large language models (LLMs) have shown impressiveperformance on various document-level tasks such as document classification,summarization, and question-answering. However, research on understanding theircapabilities on the task of self-contradictions in long documents has been verylimited. In this work, we introduce ContraDoc, the first human-annotateddataset to study self-contradictions in long documents across multiple domains,varying document lengths, self-contradictions types, and scope. We then analyzethe current capabilities of four state-of-the-art open-source and commerciallyavailable LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4performs the best and can outperform humans on this task, we find that it isstill unreliable and struggles with self-contradictions that require morenuance and context. We release the dataset and all the code associated with theexperiments.</description><author>Jierui Li, Vipul Raheja, Dhruv Kumar</author><pubDate>Wed, 15 Nov 2023 18:23:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09182v1</guid></item><item><title>PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers</title><link>http://arxiv.org/abs/2311.09180v1</link><description>Powerful large language models have facilitated the development of writingassistants that promise to significantly improve the quality and efficiency ofcomposition and communication. However, a barrier to effective assistance isthe lack of personalization in LLM outputs to the author's communication styleand specialized knowledge. In this paper, we address this challenge byproposing PEARL, a retrieval-augmented LLM writing assistant personalized witha generation-calibrated retriever. Our retriever is trained to select historicuser-authored documents for prompt augmentation, such that they are likely tobest personalize LLM generations for a user request. We propose two keynovelties for training our retriever: 1) A training data selection method thatidentifies user requests likely to benefit from personalization and documentsthat provide that benefit; and 2) A scale-calibrating KL-divergence objectivethat ensures that our retriever closely tracks the benefit of a document forpersonalized generation. We demonstrate the effectiveness of PEARL ingenerating personalized workplace social media posts and Reddit comments.Finally, we showcase the potential of a generation-calibrated retriever todouble as a performance predictor and further improve low-quality generationsvia LLM chaining.</description><author>Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi</author><pubDate>Wed, 15 Nov 2023 18:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09180v1</guid></item><item><title>SiRA: Sparse Mixture of Low Rank Adaptation</title><link>http://arxiv.org/abs/2311.09179v1</link><description>Parameter Efficient Tuning has been an prominent approach to adapt the LargeLanguage Model to downstream tasks. Most previous works considers adding thedense trainable parameters, where all parameters are used to adapt certaintask. We found this less effective empirically using the example of LoRA thatintroducing more trainable parameters does not help. Motivated by this weinvestigate the importance of leveraging "sparse" computation and propose SiRA:sparse mixture of low rank adaption. SiRA leverages the Sparse Mixture ofExpert(SMoE) to boost the performance of LoRA. Specifically it enforces the top$k$ experts routing with a capacity limit restricting the maximum number oftokens each expert can process. We propose a novel and simple expert dropout ontop of gating network to reduce the over-fitting issue. Through extensiveexperiments, we verify SiRA performs better than LoRA and other mixture ofexpert approaches across different single tasks and multitask settings.</description><author>Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Canoee Liu, Liangchen Luo, Jindong Chen, Lei Meng</author><pubDate>Wed, 15 Nov 2023 18:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09179v1</guid></item><item><title>RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution</title><link>http://arxiv.org/abs/2311.09178v1</link><description>Recently, video super resolution (VSR) has become a very impactful task inthe area of Computer Vision due to its various applications. In this paper, wepropose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) forVSR in an attempt to generate temporally coherent solutions while preservingspatial details. RBPGAN integrates two state-of-the-art models to get the bestin both worlds without compromising the accuracy of produced video. Thegenerator of the model is inspired by RBPN system, while the discriminator isinspired by TecoGAN. We also utilize Ping-Pong loss to increase temporalconsistency over time. Our contribution together results in a model thatoutperforms earlier work in terms of temporally consistent details, as we willdemonstrate qualitatively and quantitatively using different datasets.</description><author>Dareen Hussein, Hesham Eraqi, Israa Fahmy, Marwah Sulaiman, Mohammed Barakat, Mohammed El-Naggar, Moustafa Youssef, Zahraa Shehabeldin</author><pubDate>Wed, 15 Nov 2023 18:15:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09178v1</guid></item><item><title>Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers</title><link>http://arxiv.org/abs/2311.09175v1</link><description>Query expansion has been proved to be effective in improving recall andprecision of first-stage retrievers, and yet its influence on a complicated,state-of-the-art cross-encoder ranker remains under-explored. We first showthat directly applying the expansion techniques in the current literature tostate-of-the-art neural rankers can result in deteriorated zero-shotperformance. To this end, we propose GFF, a pipeline that includes a largelanguage model and a neural ranker, to Generate, Filter, and Fuse queryexpansions more effectively in order to improve the zero-shot ranking metricssuch as nDCG@10. Specifically, GFF first calls an instruction-followinglanguage model to generate query-related keywords through a reasoning chain.Leveraging self-consistency and reciprocal rank weighting, GFF further filtersand combines the ranking results of each expanded query dynamically. Byutilizing this pipeline, we show that GFF can improve the zero-shot nDCG@10 onBEIR and TREC DL 2019/2020. We also analyze different modelling choices in theGFF pipeline and shed light on the future directions in query expansion forzero-shot neural rankers.</description><author>Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, Michael Bendersky</author><pubDate>Wed, 15 Nov 2023 18:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09175v1</guid></item><item><title>AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph</title><link>http://arxiv.org/abs/2311.09174v1</link><description>Cognitive research indicates that abstraction ability is essential in humanintelligence, which remains under-explored in language models. In this paper,we present AbsPyramid, a unified entailment graph of 221K textual descriptionsof abstraction knowledge. While existing resources only touch nouns or verbswithin simplified events or specific domains, AbsPyramid collects abstractknowledge for three components of diverse events to comprehensively evaluatethe abstraction ability of language models in the open domain. Experimentalresults demonstrate that current LLMs face challenges comprehending abstractionknowledge in zero-shot and few-shot settings. By training on our richabstraction knowledge, we find LLMs can acquire basic abstraction abilities andgeneralize to unseen events. In the meantime, we empirically show that ourbenchmark is comprehensive to enhance LLMs across two previous abstractiontasks.</description><author>Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing Fang, Hongming Zhang, Sehyun Choi, Xin Liu, Yangqiu Song</author><pubDate>Wed, 15 Nov 2023 18:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09174v1</guid></item><item><title>Masked Event Modeling: Self-Supervised Pretraining for Event Cameras</title><link>http://arxiv.org/abs/2212.10368v2</link><description>Event cameras offer the capacity to asynchronously capture brightness changeswith low latency, high temporal resolution, and high dynamic range. Deployingdeep learning methods for classification or other tasks to these sensorstypically requires large labeled datasets. Since the amount of labeled eventdata is tiny compared to the bulk of labeled RGB imagery, the progress ofevent-based vision has remained limited. To reduce the dependency on labeledevent data, we introduce Masked Event Modeling (MEM), a self-supervisedpretraining framework for events. Our method pretrains a neural network onunlabeled events, which can originate from any event camera recording.Subsequently, the pretrained model is finetuned on a downstream task leading toan overall better performance while requiring fewer labels. Our methodoutperforms the state-of-the-art on N-ImageNet, N-Cars, and N-Caltech101,increasing the object classification accuracy on N-ImageNet by 7.96%. Wedemonstrate that Masked Event Modeling is superior to RGB-based pretraining ona real world dataset.</description><author>Simon Klenk, David Bonello, Lukas Koestler, Nikita Araslanov, Daniel Cremers</author><pubDate>Wed, 15 Nov 2023 18:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10368v2</guid></item><item><title>Approaching adverse event detection utilizing transformers on clinical time-series</title><link>http://arxiv.org/abs/2311.09165v1</link><description>Patients being admitted to a hospital will most often be associated with acertain clinical development during their stay. However, there is always a riskof patients being subject to the wrong diagnosis or to a certain treatment notpertaining to the desired effect, potentially leading to adverse events. Ourresearch aims to develop an anomaly detection system for identifying deviationsfrom expected clinical trajectories. To address this goal we analyzed 16 monthsof vital sign recordings obtained from the Nordland Hospital Trust (NHT). Weemployed an self-supervised framework based on the STraTS transformerarchitecture to represent the time series data in a latent space. Theserepresentations were then subjected to various clustering techniques to explorepotential patient phenotypes based on their clinical progress. While ourpreliminary results from this ongoing research are promising, they underscorethe importance of enhancing the dataset with additional demographic informationfrom patients. This additional data will be crucial for a more comprehensiveevaluation of the method's performance.</description><author>Helge Fredriksen, Per Joel Burman, Ashenafi Woldaregay, Karl Øyvind Mikalsen, Ståle Nymo</author><pubDate>Wed, 15 Nov 2023 18:05:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09165v1</guid></item><item><title>The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models</title><link>http://arxiv.org/abs/2310.02457v2</link><description>In this paper, we address the concept of "alignment" in large language models(LLMs) through the lens of post-structuralist socio-political theory,specifically examining its parallels to empty signifiers. To establish a sharedvocabulary around how abstract concepts of alignment are operationalised inempirical datasets, we propose a framework that demarcates: 1) which dimensionsof model behaviour are considered important, then 2) how meanings anddefinitions are ascribed to these dimensions, and by whom. We situate existingempirical literature and provide guidance on deciding which paradigm to follow.Through this framework, we aim to foster a culture of transparency and criticalevaluation, aiding the community in navigating the complexities of aligningLLMs with human populations.</description><author>Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Scott A. Hale</author><pubDate>Wed, 15 Nov 2023 18:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02457v2</guid></item><item><title>CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models</title><link>http://arxiv.org/abs/2311.09154v1</link><description>We are currently in an era of fierce competition among various large languagemodels (LLMs) continuously pushing the boundaries of benchmark performance.However, genuinely assessing the capabilities of these LLMs has become achallenging and critical issue due to potential data contamination, and itwastes dozens of time and effort for researchers and engineers to download andtry those contaminated models. To save our precious time, we propose a noveland useful method, Clean-Eval, which mitigates the issue of data contaminationand evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM toparaphrase and back-translate the contaminated data into a candidate set,generating expressions with the same meaning but in different surface forms. Asemantic detector is then used to filter the generated low-quality samples tonarrow down this candidate set. The best candidate is finally selected fromthis set based on the BLEURT score. According to human assessment, this bestcandidate is semantically similar to the original contamination data butexpressed differently. All candidates can form a new benchmark to evaluate themodel. Our experiments illustrate that Clean-Eval substantially restores theactual evaluation results on contaminated LLMs under both few-shot learning andfine-tuning scenarios.</description><author>Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang, Hongyuan Lu</author><pubDate>Wed, 15 Nov 2023 17:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09154v1</guid></item><item><title>Temporal Knowledge Question Answering via Abstract Reasoning Induction</title><link>http://arxiv.org/abs/2311.09149v1</link><description>In this paper, we tackle the significant challenge of temporal knowledgereasoning in Large Language Models (LLMs), an area where such models frequentlyencounter difficulties. These difficulties often result in the generation ofmisleading or incorrect information, primarily due to their limited capacity toprocess evolving factual knowledge and complex temporal logic. In response, wepropose a novel, constructivism-based approach that advocates for a paradigmshift in LLM learning towards an active, ongoing process of knowledge synthesisand customization. At the heart of our proposal is the Abstract ReasoningInduction ARI framework, which divides temporal reasoning into two distinctphases: Knowledge-agnostic and Knowledge-based. This division aims to reduceinstances of hallucinations and improve LLMs' capacity for integrating abstractmethodologies derived from historical data. Our approach achieves remarkableimprovements, with relative gains of 29.7\% and 9.27\% on two temporal QAdatasets, underscoring its efficacy in advancing temporal reasoning in LLMs.The code will be released at https://github.com/czy1999/ARI.</description><author>Ziyang Chen, Dongfang Li, Xiang Zhao, Baotian Hu, Min Zhang</author><pubDate>Wed, 15 Nov 2023 17:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09149v1</guid></item><item><title>Model Agnostic Explainable Selective Regression via Uncertainty Estimation</title><link>http://arxiv.org/abs/2311.09145v1</link><description>With the wide adoption of machine learning techniques, requirements haveevolved beyond sheer high performance, often requiring models to betrustworthy. A common approach to increase the trustworthiness of such systemsis to allow them to refrain from predicting. Such a framework is known asselective prediction. While selective prediction for classification tasks hasbeen widely analyzed, the problem of selective regression is understudied. Thispaper presents a novel approach to selective regression that utilizesmodel-agnostic non-parametric uncertainty estimation. Our proposed frameworkshowcases superior performance compared to state-of-the-art selectiveregressors, as demonstrated through comprehensive benchmarking on 69 datasets.Finally, we use explainable AI techniques to gain an understanding of thedrivers behind selective regression. We implement our selective regressionmethod in the open-source Python package doubt and release the code used toreproduce our experiments.</description><author>Andrea Pugnana, Carlos Mougan, Dan Saattrup Nielsen</author><pubDate>Wed, 15 Nov 2023 17:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09145v1</guid></item><item><title>Grounding or Guesswork? Large Language Models are Presumptive Grounders</title><link>http://arxiv.org/abs/2311.09144v1</link><description>Effective conversation requires common ground: a shared understanding betweenthe participants. Common ground, however, does not emerge spontaneously inconversation. Speakers and listeners work together to both identify andconstruct a shared basis while avoiding misunderstanding. To accomplishgrounding, humans rely on a range of dialogue acts, like clarification (What doyou mean?) and acknowledgment (I understand.). In domains like teaching andemotional support, carefully constructing grounding prevents misunderstanding.However, it is unclear whether large language models (LLMs) leverage thesedialogue acts in constructing common ground. To this end, we curate a set ofgrounding acts and propose corresponding metrics that quantify attemptedgrounding. We study whether LLMs use these grounding acts, simulating themtaking turns from several dialogue datasets, and comparing the results tohumans. We find that current LLMs are presumptive grounders, biased towardsassuming common ground without using grounding acts. To understand the roots ofthis behavior, we examine the role of instruction tuning and reinforcementlearning with human feedback (RLHF), finding that RLHF leads to less grounding.Altogether, our work highlights the need for more research investigatinggrounding in human-AI interaction.</description><author>Omar Shaikh, Kristina Gligorić, Ashna Khetan, Matthias Gerstgrasser, Diyi Yang, Dan Jurafsky</author><pubDate>Wed, 15 Nov 2023 17:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09144v1</guid></item><item><title>Machine-learning parameter tracking with partial state observation</title><link>http://arxiv.org/abs/2311.09142v1</link><description>Complex and nonlinear dynamical systems often involve parameters that changewith time, accurate tracking of which is essential to tasks such as stateestimation, prediction, and control. Existing machine-learning methods requirefull state observation of the underlying system and tacitly assume adiabaticchanges in the parameter. Formulating an inverse problem and exploitingreservoir computing, we develop a model-free and fully data-driven framework toaccurately track time-varying parameters from partial state observation in realtime. In particular, with training data from a subset of the dynamicalvariables of the system for a small number of known parameter values, theframework is able to accurately predict the parameter variations in time. Low-and high-dimensional, Markovian and non-Markovian nonlinear dynamical systemsare used to demonstrate the power of the machine-learning basedparameter-tracking framework. Pertinent issues affecting the trackingperformance are addressed.</description><author>Zheng-Meng Zhai, Mohammadamin Moradi, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai</author><pubDate>Wed, 15 Nov 2023 17:39:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09142v1</guid></item><item><title>KTRL+F: Knowledge-Augmented In-Document Search</title><link>http://arxiv.org/abs/2311.08329v2</link><description>We introduce a new problem KTRL+F, a knowledge-augmented in-document searchtask that necessitates real-time identification of all semantic targets withina document with the awareness of external sources through a single naturalquery. This task addresses following unique challenges for in-document search:1) utilizing knowledge outside the document for extended use of additionalinformation about targets to bridge the semantic gap between the query and thetargets, and 2) balancing between real-time applicability with the performance.We analyze various baselines in KTRL+F and find there are limitations ofexisting models, such as hallucinations, low latency, or difficulties inleveraging external knowledge. Therefore we propose a Knowledge-AugmentedPhrase Retrieval model that shows a promising balance between speed andperformance by simply augmenting external knowledge embedding in phraseembedding. Additionally, we conduct a user study to verify whether solvingKTRL+F can enhance search experience of users. It demonstrates that even withour simple model users can reduce the time for searching with less queries andreduced extra visits to other sources for collecting evidence. We encourage theresearch community to work on KTRL+F to enhance more efficient in-documentinformation access.</description><author>Hanseok Oh, Haebin Shin, Miyoung Ko, Hyunji Lee, Minjoon Seo</author><pubDate>Wed, 15 Nov 2023 17:36:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08329v2</guid></item><item><title>NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge</title><link>http://arxiv.org/abs/2305.04978v2</link><description>Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) isan essential component of our world knowledge, yet understudied in priorliterature. In this paper, we study the task of comparative knowledgeacquisition, motivated by the dramatic improvements in the capabilities ofextreme-scale language models like GPT-4, which have fueled efforts towardsharvesting their knowledge into knowledge bases. While acquisition of suchcomparative knowledge is much easier from models like GPT-4, compared to theirconsiderably smaller and weaker counterparts such as GPT-2, not even the mostpowerful models are exempt from making errors. We thus ask: to what extent aremodels at different scales able to generate valid and diverse comparativeknowledge? We introduce NeuroComparatives, a novel framework for comparative knowledgedistillation overgenerated from language models such as GPT-variants and Llama,followed by stringent filtering of the generated knowledge. Our frameworkacquires comparative knowledge between everyday objects, producing a corpus ofup to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% morediverse than existing resources. Moreover, human evaluations show thatNeuroComparatives outperform existing resources (up to 32% absoluteimprovement). We also demonstrate the utility of our distilledNeuroComparatives on three downstream tasks. Our results show thatneuro-symbolic manipulation of smaller models offer complementary benefits tothe currently dominant practice of prompting extreme-scale language models forknowledge distillation.</description><author>Phillip Howard, Junlin Wang, Vasudev Lal, Gadi Singer, Yejin Choi, Swabha Swayamdipta</author><pubDate>Wed, 15 Nov 2023 17:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04978v2</guid></item><item><title>Automatic Textual Normalization for Hate Speech Detection</title><link>http://arxiv.org/abs/2311.06851v2</link><description>Social media data is a valuable resource for research, yet it contains a widerange of non-standard words (NSW). These irregularities hinder the effectiveoperation of NLP tools. Current state-of-the-art methods for the Vietnameselanguage address this issue as a problem of lexical normalization, involvingthe creation of manual rules or the implementation of multi-staged deeplearning frameworks, which necessitate extensive efforts to craft intricaterules. In contrast, our approach is straightforward, employing solely asequence-to-sequence (Seq2Seq) model. In this research, we provide a datasetfor textual normalization, comprising 2,181 human-annotated comments with aninter-annotator agreement of 0.9014. By leveraging the Seq2Seq model fortextual normalization, our results reveal that the accuracy achieved fallsslightly short of 70%. Nevertheless, textual normalization enhances theaccuracy of the Hate Speech Detection (HSD) task by approximately 2%,demonstrating its potential to improve the performance of complex NLP tasks.Our dataset is accessible for research purposes.</description><author>Anh Thi-Hoang Nguyen, Dung Ha Nguyen, Nguyet Thi Nguyen, Khanh Thanh-Duy Ho, Kiet Van Nguyen</author><pubDate>Wed, 15 Nov 2023 17:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06851v2</guid></item><item><title>Causal prediction models for medication safety monitoring: The diagnosis of vancomycin-induced acute kidney injury</title><link>http://arxiv.org/abs/2311.09137v1</link><description>The current best practice approach for the retrospective diagnosis of adversedrug events (ADEs) in hospitalized patients relies on a full patient chartreview and a formal causality assessment by multiple medical experts. Thisevaluation serves to qualitatively estimate the probability of causation (PC);the probability that a drug was a necessary cause of an adverse event. Thispractice is manual, resource intensive and prone to human biases, and may thusbenefit from data-driven decision support. Here, we pioneer a causal modelingapproach using observational data to estimate a lower bound of the PC(PC$_{low}$). This method includes two key causal inference components: (1) thetarget trial emulation framework and (2) estimation of individualized treatmenteffects using machine learning. We apply our method to the clinically relevantuse-case of vancomycin-induced acute kidney injury in intensive care patients,and compare our causal model-based PC$_{low}$ estimates to qualitativeestimates of the PC provided by a medical expert. Important limitations andpotential improvements are discussed, and we conclude that future improvedcausal models could provide essential data-driven support for medication safetymonitoring in hospitalized patients.</description><author>Izak Yasrebi-de Kom, Joanna Klopotowska, Dave Dongelmans, Nicolette De Keizer, Kitty Jager, Ameen Abu-Hanna, Giovanni Cinà</author><pubDate>Wed, 15 Nov 2023 17:29:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09137v1</guid></item><item><title>Othello is Solved</title><link>http://arxiv.org/abs/2310.19387v2</link><description>The game of Othello is one of the world's most complex and popular games thathas yet to be computationally solved. Othello has roughly ten octodecillion (10to the 58th power) possible game records and ten octillion (10 to the 28thpower) possible game positions. The challenge of solving Othello, determiningthe outcome of a game with no mistake made by either player, has long been agrand challenge in computer science. This paper announces a significantmilestone: Othello is now solved. It is computationally proved that perfectplay by both players lead to a draw. Strong Othello software has long beenbuilt using heuristically designed search techniques. Solving a game provides asolution that enables the software to play the game perfectly.</description><author>Hiroki Takizawa</author><pubDate>Wed, 15 Nov 2023 17:27:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19387v2</guid></item><item><title>RRescue: Ranking LLM Responses to Enhance Reasoning Over Context</title><link>http://arxiv.org/abs/2311.09136v1</link><description>Effectively using a given context is paramount for large language models. Acontext window can include task specifications, retrieved documents, previousconversations, and even model self-reflections, functioning similarly toepisodic memory. While efforts are being made to expand the context window,studies indicate that LLMs do not use their context optimally for responsegeneration. In this paper, we present a novel approach to optimize LLMs usingranking metrics, which teaches LLMs to rank a collection ofcontextually-grounded candidate responses. Rather than a traditional fullordering, we advocate for a partial ordering. This is because achievingconsensus on the perfect order for system responses can be challenging. Ourpartial ordering is more robust, less sensitive to noise, and can be acquiredthrough human labelers, heuristic functions, or model distillation. We test oursystem's improved contextual understanding using the latest benchmarks,including a new multi-document question answering dataset. We conduct ablationstudies to understand crucial factors, such as how to gather candidateresponses, determine their most suitable order, and balance supervisedfine-tuning with ranking metrics. Our approach, named RRescue, suggests apromising avenue for enhancing LLMs' contextual understanding via responseranking.</description><author>Yikun Wang, Rui Zheng, Haoming Li, Qi Zhang, Tao Gui, Fei Liu</author><pubDate>Wed, 15 Nov 2023 17:27:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09136v1</guid></item><item><title>A Critical Survey on Fairness Benefits of XAI</title><link>http://arxiv.org/abs/2310.13007v2</link><description>In this critical survey, we analyze typical claims on the relationshipbetween explainable AI (XAI) and fairness to disentangle the multidimensionalrelationship between these two concepts. Based on a systematic literaturereview and a subsequent qualitative content analysis, we identify sevenarchetypal claims from 175 papers on the alleged fairness benefits of XAI. Wepresent crucial caveats with respect to these claims and provide an entry pointfor future discussions around the potentials and limitations of XAI forspecific fairness desiderata. While the literature often suggests XAI to be anenabler for several fairness desiderata, we notice a divide between thesedesiderata and the capabilities of XAI. We encourage to conceive XAI as one ofmany tools to approach the multidimensional, sociotechnical challenge ofalgorithmic fairness and to be more specific about how exactly what kind of XAImethod enables whom to address which fairness desideratum.</description><author>Luca Deck, Jakob Schoeffer, Maria De-Arteaga, Niklas Kühl</author><pubDate>Wed, 15 Nov 2023 17:23:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13007v2</guid></item><item><title>Aligning Neural Machine Translation Models: Human Feedback in Training and Inference</title><link>http://arxiv.org/abs/2311.09132v1</link><description>Reinforcement learning from human feedback (RLHF) is a recent technique toimprove the quality of the text generated by a language model, making it closerto what humans would generate. A core ingredient in RLHF's success in aligningand improving large language models (LLMs) is its reward model, trained usinghuman feedback on model outputs. In machine translation (MT), where metricstrained from human annotations can readily be used as reward models, recentmethods using minimum Bayes risk decoding and reranking have succeeded inimproving the final quality of translation. In this study, we comprehensivelyexplore and compare techniques for integrating quality metrics as reward modelsinto the MT pipeline. This includes using the reward model for data filtering,during the training phase through RL, and at inference time by employingreranking techniques, and we assess the effects of combining these in a unifiedapproach. Our experimental results, conducted across multiple translationtasks, underscore the crucial role of effective data filtering, based onestimated quality, in harnessing the full potential of RL in enhancing MTquality. Furthermore, our findings demonstrate the effectiveness of combiningRL training with reranking techniques, showcasing substantial improvements intranslation quality.</description><author>Miguel Moura Ramos, Patrick Fernandes, António Farinhas, André F. T. Martins</author><pubDate>Wed, 15 Nov 2023 17:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09132v1</guid></item><item><title>Social Meme-ing: Measuring Linguistic Variation in Memes</title><link>http://arxiv.org/abs/2311.09130v1</link><description>Much work in the space of NLP has used computational methods to exploresociolinguistic variation in text. In this paper, we argue that memes, asmultimodal forms of language comprised of visual templates and text, alsoexhibit meaningful social variation. We construct a computational pipeline tocluster individual instances of memes into templates and semantic variables,taking advantage of their multimodal structure in doing so. We apply thismethod to a large collection of meme images from Reddit and make available theresulting \textsc{SemanticMemes} dataset of 3.8M images clustered by theirsemantic function. We use these clusters to analyze linguistic variation inmemes, discovering not only that socially meaningful variation in meme usageexists between subreddits, but that patterns of meme innovation andacculturation within these communities align with previous findings on writtenlanguage.</description><author>Naitian Zhou, David Jurgens, David Bamman</author><pubDate>Wed, 15 Nov 2023 17:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09130v1</guid></item><item><title>Explaining black box text modules in natural language with language models</title><link>http://arxiv.org/abs/2305.09863v2</link><description>Large language models (LLMs) have demonstrated remarkable predictionperformance for a growing array of tasks. However, their rapid proliferationand increasing opaqueness have created a growing need for interpretability.Here, we ask whether we can automatically obtain natural language explanationsfor black box text modules. A "text module" is any function that maps text to ascalar continuous value, such as a submodule within an LLM or a fitted model ofa brain region. "Black box" indicates that we only have access to the module'sinputs/outputs. We introduce Summarize and Score (SASC), a method that takes in a text moduleand returns a natural language explanation of the module's selectivity alongwith a score for how reliable the explanation is. We study SASC in 3 contexts.First, we evaluate SASC on synthetic modules and find that it often recoversground truth explanations. Second, we use SASC to explain modules found withina pre-trained BERT model, enabling inspection of the model's internals.Finally, we show that SASC can generate explanations for the response ofindividual fMRI voxels to language stimuli, with potential applications tofine-grained brain mapping. All code for using SASC and reproducing results ismade available on Github.</description><author>Chandan Singh, Aliyah R. Hsu, Richard Antonello, Shailee Jain, Alexander G. Huth, Bin Yu, Jianfeng Gao</author><pubDate>Wed, 15 Nov 2023 17:19:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09863v2</guid></item><item><title>Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion</title><link>http://arxiv.org/abs/2311.09128v1</link><description>Machine learning has been successfully used to study phase transitions. Oneof the most popular approaches to identifying critical points from data withoutprior knowledge of the underlying phases is the learning-by-confusion scheme.As input, it requires system samples drawn from a grid of the parameter whosechange is associated with potential phase transitions. Up to now, the schemerequired training a distinct binary classifier for each possible splitting ofthe grid into two sides, resulting in a computational cost that scales linearlywith the number of grid points. In this work, we propose and showcase analternative implementation that only requires the training of a singlemulti-class classifier. Ideally, such multi-task learning eliminates thescaling with respect to the number of grid points. In applications to the Isingmodel and an image dataset generated with Stable Diffusion, we find significantspeedups that closely correspond to the ideal case, with only minor deviations.</description><author>Julian Arnold, Frank Schäfer, Niels Lörch</author><pubDate>Wed, 15 Nov 2023 17:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09128v1</guid></item><item><title>Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts</title><link>http://arxiv.org/abs/2311.09127v1</link><description>Existing work on jailbreak Multimodal Large Language Models (MLLMs) hasfocused primarily on adversarial examples in model inputs, with less attentionto vulnerabilities in model APIs. To fill the research gap, we carry out thefollowing work: 1) We discover a system prompt leakage vulnerability in GPT-4V.Through carefully designed dialogue, we successfully steal the internal systemprompts of GPT-4V. This finding indicates potential exploitable security risksin MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLMjailbreaking attack method termed SASP (Self-Adversarial Attack via SystemPrompt). By employing GPT-4 as a red teaming tool against itself, we aim tosearch for potential jailbreak prompts leveraging stolen system prompts.Furthermore, in pursuit of better performance, we also add human modificationbased on GPT-4's analysis, which further improves the attack success rate to98.7\%; 3) We evaluated the effect of modifying system prompts to defendagainst jailbreaking attacks. Results show that appropriately designed systemprompts can significantly reduce jailbreak success rates. Overall, our workprovides new insights into enhancing MLLM security, demonstrating the importantrole of system prompts in jailbreaking, which could be leveraged to greatlyfacilitate jailbreak success rates while also holding the potential fordefending against jailbreaks.</description><author>Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun</author><pubDate>Wed, 15 Nov 2023 17:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09127v1</guid></item><item><title>Evaluating and Modeling Attribution for Cross-Lingual Question Answering</title><link>http://arxiv.org/abs/2305.14332v2</link><description>Trustworthy answer content is abundant in many high-resource languages and isinstantly accessible through question answering systems, yet this content canbe hard to access for those that do not speak these languages. The leap forwardin cross-lingual modeling quality offered by generative language models offersmuch promise, yet their raw generations often fall short in factuality. Toimprove trustworthiness in these systems, a promising direction is to attributethe answer to a retrieved source, possibly in a content-rich language differentfrom the query. Our work is the first to study attribution for cross-lingualquestion answering. First, we collect data in 5 languages to assess theattribution level of a state-of-the-art cross-lingual QA system. To oursurprise, we find that a substantial portion of the answers is not attributableto any retrieved passages (up to 50% of answers exactly matching a goldreference) despite the system being able to attend directly to the retrievedtext. Second, to address this poor attribution level, we experiment with a widerange of attribution detection techniques. We find that Natural LanguageInference models and PaLM 2 fine-tuned on a very small amount of attributiondata can accurately detect attribution. Based on these models, we improve theattribution level of a cross-lingual question-answering system. Overall, weshow that current academic generative cross-lingual QA systems have substantialshortcomings in attribution and we build tooling to mitigate these issues.</description><author>Benjamin Muller, John Wieting, Jonathan H. Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Baldini Soares, Roee Aharoni, Jonathan Herzig, Xinyi Wang</author><pubDate>Wed, 15 Nov 2023 17:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14332v2</guid></item><item><title>Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark</title><link>http://arxiv.org/abs/2311.09122v1</link><description>We introduce Universal NER (UNER), an open, community-driven project todevelop gold-standard NER benchmarks in many languages. The overarching goal ofUNER is to provide high-quality, cross-lingually consistent annotations tofacilitate and standardize multilingual NER research. UNER v1 contains 18datasets annotated with named entities in a cross-lingual consistent schemaacross 12 diverse languages. In this paper, we detail the dataset creation andcomposition of UNER; we also provide initial modeling baselines on bothin-language and cross-lingual learning settings. We release the data, code, andfitted models to the public.</description><author>Stephen Mayhew, Terra Blevins, Shuheng Liu, Marek Šuppa, Hila Gonen, Joseph Marvin Imperial, Börje F. Karlsson, Peiqin Lin, Nikola Ljubešić, LJ Miranda, Barbara Plank, Arij Riabi, Yuval Pinter</author><pubDate>Wed, 15 Nov 2023 17:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09122v1</guid></item><item><title>WildlifeDatasets: An open-source toolkit for animal re-identification</title><link>http://arxiv.org/abs/2311.09118v1</link><description>In this paper, we present WildlifeDatasets(https://github.com/WildlifeDatasets/wildlife-datasets) - an open-sourcetoolkit intended primarily for ecologists and computer-vision /machine-learning researchers. The WildlifeDatasets is written in Python, allowsstraightforward access to publicly available wildlife datasets, and provides awide variety of methods for dataset pre-processing, performance analysis, andmodel fine-tuning. We showcase the toolkit in various scenarios and baselineexperiments, including, to the best of our knowledge, the most comprehensiveexperimental comparison of datasets and methods for wildlife re-identification,including both local descriptors and deep learning approaches. Furthermore, weprovide the first-ever foundation model for individual re-identification withina wide range of species - MegaDescriptor - that provides state-of-the-artperformance on animal re-identification datasets and outperforms otherpre-trained models such as CLIP and DINOv2 by a significant margin. To make themodel available to the general public and to allow easy integration with anyexisting wildlife monitoring applications, we provide multiple MegaDescriptorflavors (i.e., Small, Medium, and Large) through the HuggingFace hub(https://huggingface.co/BVRA).</description><author>Vojtěch Čermák, Lukas Picek, Lukáš Adam, Kostas Papafitsoros</author><pubDate>Wed, 15 Nov 2023 17:08:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09118v1</guid></item><item><title>R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces</title><link>http://arxiv.org/abs/2311.09117v1</link><description>This paper introduces Robust Spin (R-Spin), a data-efficient self-supervisedfine-tuning framework for speaker and noise-invariant speech representations bylearning discrete acoustic units with speaker-invariant clustering (Spin).R-Spin resolves Spin's issues and enhances content representations by learningto predict acoustic pieces. R-Spin offers a 12X reduction in computationalresources compared to previous state-of-the-art methods while outperformingthem in severely distorted speech scenarios. This paper provides detailedanalyses to show how discrete units contribute to speech encoder training andimproving robustness in diverse acoustic environments.</description><author>Heng-Jui Chang, James Glass</author><pubDate>Wed, 15 Nov 2023 17:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09117v1</guid></item><item><title>PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning</title><link>http://arxiv.org/abs/2306.12370v2</link><description>Hyperparameters of Deep Learning (DL) pipelines are crucial for theirdownstream performance. While a large number of methods for HyperparameterOptimization (HPO) have been developed, their incurred costs are oftenuntenable for modern DL. Consequently, manual experimentation is still the mostprevalent approach to optimize hyperparameters, relying on the researcher'sintuition, domain knowledge, and cheap preliminary explorations. To resolvethis misalignment between HPO algorithms and DL researchers, we proposePriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefsand cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiencyacross a range of DL benchmarks and show its gains under informative expertinput and robustness against poor expert beliefs</description><author>Neeratyoy Mallik, Edward Bergman, Carl Hvarfner, Danny Stoll, Maciej Janowski, Marius Lindauer, Luigi Nardi, Frank Hutter</author><pubDate>Wed, 15 Nov 2023 17:06:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12370v2</guid></item><item><title>HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data</title><link>http://arxiv.org/abs/2311.09115v1</link><description>Technological advances in medical data collection such as high-resolutionhistopathology and high-throughput genomic sequencing have contributed to therising requirement for multi-modal biomedical modelling, specifically forimage, tabular, and graph data. Most multi-modal deep learning approaches usemodality-specific architectures that are trained separately and cannot capturethe crucial cross-modal information that motivates the integration of differentdata sources. This paper presents the Hybrid Early-fusion Attention LearningNetwork (HEALNet): a flexible multi-modal fusion architecture, which a)preserves modality-specific structural information, b) captures the cross-modalinteractions and structural information in a shared latent space, c) caneffectively handle missing modalities during training and inference, and d)enables intuitive model inspection by learning on the raw data input instead ofopaque embeddings. We conduct multi-modal survival analysis on Whole SlideImages and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas(TCGA). HEALNet achieves state-of-the-art performance, substantially improvingover both uni-modal and recent multi-modal baselines, whilst being robust inscenarios with missing modalities.</description><author>Konstantin Hemker, Nikola Smidjievski, Mateja Jamnik</author><pubDate>Wed, 15 Nov 2023 17:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09115v1</guid></item><item><title>Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification</title><link>http://arxiv.org/abs/2311.09114v1</link><description>Large Language Models (LLMs) have demonstrated remarkable proficiency ingenerating fluent text. However, they often encounter the challenge ofgenerating inaccurate or hallucinated content. This issue is common in bothnon-retrieval-based generation and retrieval-augmented generation approaches,and existing post-hoc rectification methods may not address the accumulatedhallucination errors that may be caused by the "snowballing" issue, especiallyin reasoning tasks. To tackle these challenges, we introduce a novel approachcalled Real-time Verification and Rectification (Ever). Instead of waitinguntil the end of the generation process to rectify hallucinations, Ever employsa real-time, step-wise generation and hallucination rectification strategy. Theprimary objective is to detect and rectify hallucinations as they occur duringthe text generation process. When compared to both retrieval-based andnon-retrieval-based baselines, Ever demonstrates a significant improvement ingenerating trustworthy and factually accurate text across a diverse range oftasks, including short-form QA, biography generation, and multi-hop reasoning.</description><author>Haoqiang Kang, Juntong Ni, Huaxiu Yao</author><pubDate>Wed, 15 Nov 2023 17:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09114v1</guid></item><item><title>CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models</title><link>http://arxiv.org/abs/2307.07705v2</link><description>Parameter-efficient tuning (PET) has been widely explored in recent yearsbecause it tunes much fewer parameters (PET modules) than full-parameterfine-tuning (FT) while still stimulating sufficient knowledge from largelanguage models (LLMs) for downstream tasks. Moreover, when PET is employed toserve multiple tasks, different task-specific PET modules can be built on afrozen LLM, avoiding redundant LLM deployments. Although PET significantlyreduces the cost of tuning and deploying LLMs, its inference still suffers fromthe computational bottleneck of LLMs. To address the above issue, we propose aneffective PET framework based on compressed LLMs, named "CPET". In CPET, weevaluate the impact of mainstream LLM compression techniques on PET performanceand then introduce knowledge inheritance and recovery strategies to restore theknowledge loss caused by these compression techniques. Our experimental resultsdemonstrate that, owing to the restoring strategies of CPET, collaboratingtask-specific PET modules with a compressed LLM can achieve comparableperformance to collaborating PET modules with the original version of thecompressed LLM and outperform directly applying vanilla PET methods to thecompressed LLM.</description><author>Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, Maosong Sun</author><pubDate>Wed, 15 Nov 2023 17:02:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07705v2</guid></item><item><title>Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?</title><link>http://arxiv.org/abs/2311.09109v1</link><description>Knowledge graphs (KGs) consist of links that describe relationships betweenentities. Due to the difficulty of manually enumerating all relationshipsbetween entities, automatically completing them is essential for KGs. KnowledgeGraph Completion (KGC) is a task that infers unseen relationships betweenentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links usingonly the knowledge from training data. In contrast, the recent Pre-trainedLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.Therefore, PLM-based KGC can estimate missing links between entities by reusingmemorized knowledge from pre-training without inference. This approach isproblematic because building KGC models aims to infer unseen links betweenentities. However, conventional evaluations in KGC do not consider inferenceand memorization abilities separately. Thus, a PLM-based KGC method, whichachieves high performance in current KGC evaluations, may be ineffective inpractical applications. To address this issue, we analyze whether PLM-based KGCmethods make inferences or merely access memorized knowledge. For this purpose,we propose a method for constructing synthetic datasets specified in thisanalysis and conclude that PLMs acquire the inference abilities required forKGC through pre-training, even though the performance improvements mostly comefrom textual information of entities and relations.</description><author>Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe</author><pubDate>Wed, 15 Nov 2023 16:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09109v1</guid></item><item><title>"We Demand Justice!": Towards Grounding Political Text in Social Context</title><link>http://arxiv.org/abs/2311.09106v1</link><description>Social media discourse from US politicians frequently consists of 'seeminglysimilar language used by opposing sides of the political spectrum'. But often,it translates to starkly contrasting real-world actions. For instance, "We needto keep our students safe from mass shootings" may signal either "armingteachers to stop the shooter" or "banning guns to reduce mass shootings"depending on who says it and their political stance on the issue. In thispaper, we define and characterize the context that is required to fullyunderstand such ambiguous statements in a computational setting and ground themin real-world entities, actions, and attitudes. To that end, we propose twochallenging datasets that require an understanding of the real-world context ofthe text to be solved effectively. We benchmark these datasets againstbaselines built upon large pre-trained models such as BERT, RoBERTa, GPT-3,etc. Additionally, we develop and benchmark more structured baselines buildingupon existing 'Discourse Contextualization Framework' and 'Political ActorRepresentation' models. We perform analysis of the datasets and baselinepredictions to obtain further insights into the pragmatic languageunderstanding challenges posed by the proposed social grounding tasks.</description><author>Rajkumar Pujari, Chengfei Wu, Dan Goldwasser</author><pubDate>Wed, 15 Nov 2023 16:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09106v1</guid></item><item><title>MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation</title><link>http://arxiv.org/abs/2311.09105v1</link><description>Understanding events in texts is a core objective of natural languageunderstanding, which requires detecting event occurrences, extracting eventarguments, and analyzing inter-event relationships. However, due to theannotation challenges brought by task complexity, a large-scale datasetcovering the full process of event understanding has long been absent. In thispaper, we introduce MAVEN-Arg, which augments MAVEN datasets with eventargument annotations, making the first all-in-one dataset supporting eventdetection, event argument extraction (EAE), and event relation extraction. Asan EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensiveschema covering 162 event types and 612 argument roles, all with expert-writtendefinitions and examples; (2) a large data scale, containing 98,591 events and290,613 arguments obtained with laborious human annotation; (3) the exhaustiveannotation supporting all task variants of EAE, which annotates both entity andnon-entity event arguments in document level. Experiments indicate thatMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietarylarge language models (LLMs). Furthermore, to demonstrate the benefits of anall-in-one dataset, we preliminarily explore a potential application, futureevent prediction, with LLMs. MAVEN-Arg and our code can be obtained fromhttps://github.com/THU-KEG/MAVEN-Argument.</description><author>Xiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng, Jianhui Chen, Lei Hou, Xu Han, Yankai Lin, Zhiyuan Liu, Ruobing Xie, Jie Zhou, Juanzi Li</author><pubDate>Wed, 15 Nov 2023 16:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09105v1</guid></item><item><title>Cross-view and Cross-pose Completion for 3D Human Understanding</title><link>http://arxiv.org/abs/2311.09104v1</link><description>Human perception and understanding is a major domain of computer visionwhich, like many other vision subdomains recently, stands to gain from the useof large models pre-trained on large datasets. We hypothesize that the mostcommon pre-training strategy of relying on general purpose, object-centricimage datasets such as ImageNet, is limited by an important domain shift. Onthe other hand, collecting domain specific ground truth such as 2D or 3D labelsdoes not scale well. Therefore, we propose a pre-training approach based onself-supervised learning that works on human-centric data using only images.Our method uses pairs of images of humans: the first is partially masked andthe model is trained to reconstruct the masked parts given the visible ones anda second image. It relies on both stereoscopic (cross-view) pairs, and temporal(cross-pose) pairs taken from videos, in order to learn priors about 3D as wellas human motion. We pre-train a model for body-centric tasks and one forhand-centric tasks. With a generic transformer architecture, these modelsoutperform existing self-supervised pre-training methods on a wide set ofhuman-centric downstream tasks, and obtain state-of-the-art performance forinstance when fine-tuning for model-based and model-free human mesh recovery.</description><author>Matthieu Armando, Salma Galaaoui, Fabien Baradel, Thomas Lucas, Vincent Leroy, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez</author><pubDate>Wed, 15 Nov 2023 16:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09104v1</guid></item><item><title>Guided Scale Space Radon Transform for linear structures detection</title><link>http://arxiv.org/abs/2311.09103v1</link><description>Using integral transforms to the end of lines detection in images withcomplex background, makes the detection a hard task needing additionalprocessing to manage the detection. As an integral transform, the Scale SpaceRadon Transform (SSRT) suffers from such drawbacks, even with its greatabilities for thick lines detection. In this work, we propose a method toaddress this issue for automatic detection of thick linear structures in grayscale and binary images using the SSRT, whatever the image background content.This method involves the calculated Hessian orientations of the investigatedimage while computing its SSRT, in such a way that linear structures areemphasized in the SSRT space. As a consequence, the subsequent maxima detectionin the SSRT space is done on a modified transform space freed from unwantedparts and, consequently, from irrelevant peaks that usually drown the peaksrepresenting lines. Besides, highlighting the linear structure in the SSRTspace permitting, thus, to efficiently detect lines of different thickness insynthetic and real images, the experiments show also the method robustnessagainst noise and complex background.</description><author>Aicha Baya Goumeidane, Djemel Ziou, Nafaa Nacereddine</author><pubDate>Wed, 15 Nov 2023 16:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09103v1</guid></item><item><title>OVeNet: Offset Vector Network for Semantic Segmentation</title><link>http://arxiv.org/abs/2303.14516v2</link><description>Semantic segmentation is a fundamental task in visual scene understanding. Wefocus on the supervised setting, where ground-truth semantic annotations areavailable. Based on knowledge about the high regularity of real-world scenes,we propose a method for improving class predictions by learning to selectivelyexploit information from neighboring pixels. In particular, our method is basedon the prior that for each pixel, there is a seed pixel in its closeneighborhood sharing the same prediction with the former. Motivated by thisprior, we design a novel two-head network, named Offset Vector Network(OVeNet), which generates both standard semantic predictions and a dense 2Doffset vector field indicating the offset from each pixel to the respectiveseed pixel, which is used to compute an alternative, seed-based semanticprediction. The two predictions are adaptively fused at each pixel using alearnt dense confidence map for the predicted offset vector field. We superviseoffset vectors indirectly via optimizing the seed-based prediction and via anovel loss on the confidence map. Compared to the baseline state-of-the-artarchitectures HRNet and HRNet+OCR on which OVeNet is built, the latter achievessignificant performance gains on three prominent benchmarks for semanticsegmentation, namely Cityscapes, ACDC and ADE20K. Code is available athttps://github.com/stamatisalex/OVeNet</description><author>Stamatis Alexandropoulos, Christos Sakaridis, Petros Maragos</author><pubDate>Wed, 15 Nov 2023 16:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14516v2</guid></item><item><title>Towards A Unified View of Answer Calibration for Multi-Step Reasoning</title><link>http://arxiv.org/abs/2311.09101v1</link><description>Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting havebroadened the scope for improving multi-step reasoning capabilities. Usually,answer calibration strategies such as step-level or path-level calibration playa vital role in multi-step reasoning. While effective, there remains asignificant gap in our understanding of the key factors that drive theirsuccess. In this paper, we break down the design of recent answer calibrationstrategies and present a unified view which establishes connections betweenthem. We then conduct a thorough evaluation on these strategies from a unifiedview, systematically scrutinizing step-level and path-level answer calibrationacross multiple paths. Our study holds the potential to illuminate key insightsfor optimizing multi-step reasoning with answer calibration.</description><author>Shumin Deng, Ningyu Zhang, Nay Oo, Bryan Hooi</author><pubDate>Wed, 15 Nov 2023 16:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09101v1</guid></item><item><title>Do prompt positions really matter?</title><link>http://arxiv.org/abs/2305.14493v3</link><description>Prompt-based models have gathered a lot of attention from researchers due totheir remarkable advancements in the fields of zero-shot and few-shot learning.Developing an effective prompt template plays a critical role. However, priorstudies have mainly focused on prompt vocabulary selection or embeddinginitialization within a predefined template with the prompt position fixed. Inthis empirical study, we conduct the most comprehensive analysis to date ofprompt position for diverse natural language process tasks. Our findingsquantify the substantial impact prompt position has on model performance. Weobserve that the prompt position used in prior studies is often sub-optimal.These findings suggest prompt position optimisation as a valuable researchdirection to fill the gap in existing prompt engineering methodologies.</description><author>Junyu Mao, Stuart E. Middleton, Mahesan Niranjan</author><pubDate>Wed, 15 Nov 2023 16:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14493v3</guid></item><item><title>Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization</title><link>http://arxiv.org/abs/2311.09096v1</link><description>Large Language Models (LLMs) continue to advance in their capabilities, yetthis progress is accompanied by a growing array of safety risks. Whilesignificant attention has been dedicated to exploiting weaknesses in LLMsthrough jailbreaking attacks, there remains a paucity of exploration intodefending against these attacks. We point out a pivotal factor contributing tothe success of jailbreaks: the inherent conflict between the goals of beinghelpful and ensuring safety. To counter jailbreaking attacks, we propose tointegrate goal prioritization at both training and inference stages.Implementing goal prioritization during inference substantially diminishes theAttack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromisinggeneral performance. Furthermore, integrating the concept of goalprioritization into the training phase reduces the ASR from 71.0% to 6.6% forLLama2-13B. Remarkably, even in scenarios where no jailbreaking samples areincluded during training, our approach slashes the ASR by half, decreasing itfrom 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMsface greater safety risks, they also possess a greater capacity to be steeredtowards defending against such attacks. We hope our work could contribute tothe comprehension of jailbreaking attacks and defenses, and shed light on therelationship between LLMs' capability and safety. Our code will be available at\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.</description><author>Zhexin Zhang, Junxiao Yang, Pei Ke, Minlie Huang</author><pubDate>Wed, 15 Nov 2023 16:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09096v1</guid></item><item><title>Can MusicGen Create Training Data for MIR Tasks?</title><link>http://arxiv.org/abs/2311.09094v1</link><description>We are investigating the broader concept of using AI-based generative musicsystems to generate training data for Music Information Retrieval (MIR) tasks.To kick off this line of work, we ran an initial experiment in which we traineda genre classifier on a fully artificial music dataset created with MusicGen.We constructed over 50 000 genre- conditioned textual descriptions andgenerated a collection of music excerpts that covers five musical genres. Ourpreliminary results show that the proposed model can learn genre-specificcharacteristics from artificial music tracks that generalise well to real-worldmusic recordings.</description><author>Nadine Kroher, Helena Cuesta, Aggelos Pikrakis</author><pubDate>Wed, 15 Nov 2023 16:41:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09094v1</guid></item><item><title>Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions</title><link>http://arxiv.org/abs/2311.09093v1</link><description>Autonomous vehicle refers to a vehicle capable of perceiving its surroundingenvironment and driving with little or no human driver input. The perceptionsystem is a fundamental component which enables the autonomous vehicle tocollect data and extract relevant information from the environment to drivesafely. Benefit from the recent advances in computer vision, the perceptiontask can be achieved by using sensors, such as camera, LiDAR, radar, andultrasonic sensor. This paper reviews publications on computer vision andautonomous driving that are published during the last ten years. In particular,we first investigate the development of autonomous driving systems andsummarize these systems that are developed by the major automotivemanufacturers from different countries. Second, we investigate the sensors andbenchmark data sets that are commonly utilized for autonomous driving. Then, acomprehensive overview of computer vision applications for autonomous drivingsuch as depth estimation, object detection, lane detection, and traffic signrecognition are discussed. Additionally, we review public opinions and concernson autonomous vehicles. Based on the discussion, we analyze the currenttechnological challenges that autonomous vehicles meet with. Finally, wepresent our insights and point out some promising directions for futureresearch. This paper will help the reader to understand autonomous vehiclesfrom the perspectives of academia and industry.</description><author>Xingshuai Dong, Massimiliano L. Cappuccio</author><pubDate>Wed, 15 Nov 2023 16:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09093v1</guid></item><item><title>ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning</title><link>http://arxiv.org/abs/2311.08385v2</link><description>Aligning language models (LMs) with human opinion is challenging yet vital toenhance their grasp of human values, preferences, and beliefs. We presentChOiRe, a four-step solution framework to predict human opinion thatdifferentiates between the user explicit personae (i.e. demographic orideological attributes) that are manually declared and implicit personaeinferred from user historical opinions. Specifically, it consists of (i) an LManalyzing the user explicit personae to filter out irrelevant attributes; (ii)the LM ranking the implicit persona opinions into a preferential list; (iii)Chain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes theexplicit personae and the most relevant implicit personae to perform opinionprediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times withincreasingly larger lists of implicit personae to overcome insufficientpersonae information to infer a final result. ChOiRe achieves newstate-of-the-art effectiveness with limited inference calls, improving previousLLM-based techniques significantly by 3.22%.</description><author>Xuan Long Do, Kenji Kawaguchi, Min-Yen Kan, Nancy F. Chen</author><pubDate>Wed, 15 Nov 2023 16:40:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08385v2</guid></item><item><title>Social Bias Probing: Fairness Benchmarking for Language Models</title><link>http://arxiv.org/abs/2311.09090v1</link><description>Large language models have been shown to encode a variety of social biases,which carries the risk of downstream harms. While the impact of these biaseshas been recognized, prior methods for bias evaluation have been limited tobinary association tests on small datasets, offering a constrained view of thenature of societal biases within language models. In this paper, we propose anoriginal framework for probing language models for societal biases. We collecta probing dataset to analyze language models' general associations, as well asalong the axes of societal categories, identities, and stereotypes. To thisend, we leverage a novel perplexity-based fairness score. We curate alarge-scale benchmarking dataset addressing drawbacks and limitations ofexisting fairness collections, expanding to a variety of different identitiesand stereotypes. When comparing our methodology with prior work, we demonstratethat biases within language models are more nuanced than previouslyacknowledged. In agreement with recent findings, we find that larger modelvariants exhibit a higher degree of bias. Moreover, we expose how identitiesexpressing different religions lead to the most pronounced disparate treatmentsacross all models.</description><author>Marta Marchiori Manerba, Karolina Stańczak, Riccardo Guidotti, Isabelle Augenstein</author><pubDate>Wed, 15 Nov 2023 16:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09090v1</guid></item><item><title>The Uli Dataset: An Exercise in Experience Led Annotation of oGBV</title><link>http://arxiv.org/abs/2311.09086v1</link><description>Online gender based violence has grown concomitantly with adoption of theinternet and social media. Its effects are worse in the Global majority wheremany users use social media in languages other than English. The scale andvolume of conversations on the internet has necessitated the need for automateddetection of hate speech, and more specifically gendered abuse. There is,however, a lack of language specific and contextual data to build suchautomated tools. In this paper we present a dataset on gendered abuse in threelanguages- Hindi, Tamil and Indian English. The dataset comprises of tweetsannotated along three questions pertaining to the experience of gender abuse,by experts who identify as women or a member of the LGBTQIA community in SouthAsia. Through this dataset we demonstrate a participatory approach to creatingdatasets that drive AI systems.</description><author>Arnav Arora, Maha Jinadoss, Cheshta Arora, Denny George, Brindaalakshmi, Haseena Dawood Khan, Kirti Rawat, Div, Ritash, Seema Mathur, Shivani Yadav, Shehla Rashid Shora, Rie Raut, Sumit Pawar, Apurva Paithane, Sonia, Vivek, Dharini Priscilla, Khairunnisha, Grace Banu, Ambika Tandon, Rishav Thakker, Rahul Dev Korra, Aatman Vaidya, Tarunima Prabhakar</author><pubDate>Wed, 15 Nov 2023 16:30:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09086v1</guid></item><item><title>Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search</title><link>http://arxiv.org/abs/2311.09084v1</link><description>Given a descriptive text query, text-based person search (TBPS) aims toretrieve the best-matched target person from an image gallery. Such across-modal retrieval task is quite challenging due to significant modalitygap, fine-grained differences and insufficiency of annotated data. To betteralign the two modalities, most existing works focus on introducingsophisticated network structures and auxiliary tasks, which are complex andhard to implement. In this paper, we propose a simple yet effective dualTransformer model for text-based person search. By exploiting a hardness-awarecontrastive learning strategy, our model achieves state-of-the-art performancewithout any special design for local feature alignment or side information.Moreover, we propose a proximity data generation (PDG) module to automaticallyproduce more diverse data for cross-modal training. The PDG module firstintroduces an automatic generation algorithm based on a text-to-image diffusionmodel, which generates new text-image pair samples in the proximity space oforiginal ones. Then it combines approximate text generation and feature-levelmixup during training to further strengthen the data diversity. The PDG modulecan largely guarantee the reasonability of the generated samples that aredirectly used for training without any human inspection for noise rejection. Itimproves the performance of our model significantly, providing a feasiblesolution to the data insufficiency problem faced by such fine-grainedvisual-linguistic tasks. Extensive experiments on two popular datasets of theTBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approachoutperforms state-of-the-art approaches evidently, e.g., improving by 3.88%,4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will beavailable at https://github.com/HCPLab-SYSU/PersonSearch-CTLG</description><author>Hefeng Wu, Weifeng Chen, Zhibin Liu, Tianshui Chen, Zhiguang Chen, Liang Lin</author><pubDate>Wed, 15 Nov 2023 16:26:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09084v1</guid></item><item><title>Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces</title><link>http://arxiv.org/abs/2211.14400v4</link><description>Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study theproblem of how efficiently, in terms of the number of parameters, deep neuralnetworks with the ReLU activation function can approximate functions in theSobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, witherror measured in the $L_p(\Omega)$ norm. This problem is important whenstudying the application of neural networks in a variety of fields, includingscientific computing and signal processing, and has previously been solved onlywhen $p=q=\infty$. Our contribution is to provide a complete solution for all$1\leq p,q\leq \infty$ and $s &gt; 0$ for which the corresponding Sobolev or Besovspace compactly embeds into $L_p$. The key technical tool is a novelbit-extraction technique which gives an optimal encoding of sparse vectors.This enables us to obtain sharp upper bounds in the non-linear regime where $p&gt; q$. We also provide a novel method for deriving $L_p$-approximation lowerbounds based upon VC-dimension when $p &lt; \infty$. Our results show that verydeep ReLU networks significantly outperform classical methods of approximationin terms of the number of parameters, but that this comes at the cost ofparameters which are not encodable.</description><author>Jonathan W. Siegel</author><pubDate>Wed, 15 Nov 2023 16:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14400v4</guid></item><item><title>Deanthropomorphising NLP: Can a Language Model Be Conscious?</title><link>http://arxiv.org/abs/2211.11483v4</link><description>This work is intended as a voice in the discussion over previous claims thata pretrained large language model (LLM) based on the Transformer modelarchitecture can be sentient. Such claims have been made concerning the LaMDAmodel and also concerning the current wave of LLM-powered chatbots, such asChatGPT. This claim, if confirmed, would have serious ramifications in theNatural Language Processing (NLP) community due to wide-spread use of similarmodels. However, here we take the position that such a large language modelcannot be sentient, or conscious, and that LaMDA in particular exhibits noadvances over other similar models that would qualify it. We justify this byanalysing the Transformer architecture through Integrated Information Theory ofconsciousness. We see the claims of sentience as part of a wider tendency touse anthropomorphic language in NLP reporting. Regardless of the veracity ofthe claims, we consider this an opportune moment to take stock of progress inlanguage modelling and consider the ethical implications of the task. In orderto make this work helpful for readers outside the NLP community, we alsopresent the necessary background in language modelling.</description><author>Matthew Shardlow, Piotr Przybyła</author><pubDate>Wed, 15 Nov 2023 16:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11483v4</guid></item><item><title>Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation</title><link>http://arxiv.org/abs/2311.09077v1</link><description>A crucial reason for the success of existing NeRF-based methods is to build aneural density field for the geometry representation via multiple perceptronlayers (MLPs). MLPs are continuous functions, however, real geometry or densityfield is frequently discontinuous at the interface between the air and thesurface. Such a contrary brings the problem of unfaithful geometryrepresentation. To this end, this paper proposes spiking NeRF, which leveragesspiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking NeuralNetwork (SNN) framework to build a discontinuous density field for faithfulgeometry representation. Specifically, we first demonstrate the reason whycontinuous density fields will bring inaccuracy. Then, we propose to use thespiking neurons to build a discontinuous density field. We conductcomprehensive analysis for the problem of existing spiking neuron models andthen provide the numerical relationship between the parameter of spiking neuronand the theoretical accuracy of geometry, Based on this, we propose a boundedspiking neuron to build the discontinuous density field. Our results achieveSOTA performance. Our code and data will be released to the public.</description><author>Zhanfeng Liao, Qian Zheng, Yan Liu, Gang Pan</author><pubDate>Wed, 15 Nov 2023 16:19:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09077v1</guid></item><item><title>Semi-Supervised Learning in the Few-Shot Zero-Shot Scenario</title><link>http://arxiv.org/abs/2308.14119v2</link><description>Semi-Supervised Learning (SSL) is a framework that utilizes both labeled andunlabeled data to enhance model performance. Conventional SSL methods operateunder the assumption that labeled and unlabeled data share the same labelspace. However, in practical real-world scenarios, especially when the labeledtraining dataset is limited in size, some classes may be totally absent fromthe labeled set. To address this broader context, we propose a general approachto augment existing SSL methods, enabling them to effectively handle situationswhere certain classes are missing. This is achieved by introducing anadditional term into their objective function, which penalizes theKL-divergence between the probability vectors of the true class frequencies andthe inferred class frequencies. Our experimental results reveal significantimprovements in accuracy when compared to state-of-the-art SSL, open-set SSL,and open-world SSL methods. We conducted these experiments on two benchmarkimage classification datasets, CIFAR-100 and STL-10, with the most remarkableimprovements observed when the labeled data is severely limited, with only afew labeled examples per class</description><author>Noam Fluss, Guy Hacohen, Daphna Weinshall</author><pubDate>Wed, 15 Nov 2023 16:15:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14119v2</guid></item><item><title>How Multilingual is Multilingual LLM?</title><link>http://arxiv.org/abs/2311.09071v1</link><description>Large Language Models (LLMs), trained predominantly on extensive Englishdata, often exhibit limitations when applied to other languages. Currentresearch is primarily focused on enhancing the multilingual capabilities ofthese models by employing various tuning strategies. Despite theireffectiveness in certain languages, the understanding of the multilingualabilities of LLMs remains incomplete. This study endeavors to evaluate themultilingual capacity of LLMs by conducting an exhaustive analysis across 101languages, and classifies languages with similar characteristics into fourdistinct quadrants. By delving into each quadrant, we shed light on therationale behind their categorization and offer actionable guidelines fortuning these languages. Extensive experiments reveal that existing LLMs possessmultilingual capabilities that surpass our expectations, and we cansignificantly improve the multilingual performance of LLMs by focusing on thesedistinct attributes present in each quadrant.</description><author>Fei Yuan, Shuai Yuan, Zhiyong Wu, Lei Li</author><pubDate>Wed, 15 Nov 2023 16:13:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09071v1</guid></item><item><title>How Well Do Large Language Models Truly Ground?</title><link>http://arxiv.org/abs/2311.09069v1</link><description>Reliance on the inherent knowledge of Large Language Models (LLMs) can causeissues such as hallucinations, lack of control, and difficulties in integratingvariable knowledge. To mitigate this, LLMs can be probed to generate responsesby grounding on external context, often given as input (knowledge-augmentedmodels). Yet, previous research is often confined to a narrow view of the term"grounding", often only focusing on whether the response contains the correctanswer or not, which does not ensure the reliability of the entire response. Toaddress this limitation, we introduce a strict definition of grounding: a modelis considered truly grounded when its responses (1) fully utilize necessaryknowledge from the provided context, and (2) don't exceed the knowledge withinthe contexts. We introduce a new dataset and a grounding metric to assess thisnew definition and perform experiments across 13 LLMs of different sizes andtraining methods to provide insights into the factors that influence groundingperformance. Our findings contribute to a better understanding of how toimprove grounding capabilities and suggest an area of improvement toward morereliable and controllable LLM applications.</description><author>Hyunji Lee, Sejune Joo, Chaeeun Kim, Joel Jang, Doyoung Kim, Kyoung-Woon On, Minjoon Seo</author><pubDate>Wed, 15 Nov 2023 16:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09069v1</guid></item><item><title>Learning Fair Division from Bandit Feedback</title><link>http://arxiv.org/abs/2311.09068v1</link><description>This work addresses learning online fair division under uncertainty, where acentral planner sequentially allocates items without precise knowledge ofagents' values or utilities. Departing from conventional online algorithm, theplanner here relies on noisy, estimated values obtained after allocating items.We introduce wrapper algorithms utilizing \textit{dual averaging}, enablinggradual learning of both the type distribution of arriving items and agents'values through bandit feedback. This approach enables the algorithms toasymptotically achieve optimal Nash social welfare in linear Fisher marketswith agents having additive utilities. We establish regret bounds in Nashsocial welfare and empirically validate the superior performance of ourproposed algorithms across synthetic and empirical datasets.</description><author>Hakuei Yamada, Junpei Komiyama, Kenshi Abe, Atsushi Iwasaki</author><pubDate>Wed, 15 Nov 2023 16:10:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09068v1</guid></item><item><title>Latent SDEs on Homogeneous Spaces</title><link>http://arxiv.org/abs/2306.16248v2</link><description>We consider the problem of variational Bayesian inference in a latentvariable model where a (possibly complex) observed stochastic process isgoverned by the solution of a latent stochastic differential equation (SDE).Motivated by the challenges that arise when trying to learn an (almostarbitrary) latent neural SDE from large-scale data, such as efficient gradientcomputation, we take a step back and study a specific subclass instead. In ourcase, the SDE evolves on a homogeneous latent space and is induced bystochastic dynamics of the corresponding (matrix) Lie group. In learningproblems, SDEs on the unit $n$-sphere are arguably the most relevantincarnation of this setup. Notably, for variational inference, the sphere notonly facilitates using a truly uninformative prior SDE, but we also obtain aparticularly simple and intuitive expression for the Kullback-Leiblerdivergence between the approximate posterior and prior process in the evidencelower bound. Experiments demonstrate that a latent SDE of the proposed type canbe learned efficiently by means of an existing one-step geometricEuler-Maruyama scheme. Despite restricting ourselves to a less diverse class ofSDEs, we achieve competitive or even state-of-the-art performance on varioustime series interpolation and classification benchmarks.</description><author>Sebastian Zeng, Florian Graf, Roland Kwitt</author><pubDate>Wed, 15 Nov 2023 16:07:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16248v2</guid></item><item><title>Attention-based Multi-task Learning for Base Editor Outcome Prediction</title><link>http://arxiv.org/abs/2311.07636v2</link><description>Human genetic diseases often arise from point mutations, emphasizing thecritical need for precise genome editing techniques. Among these, base editingstands out as it allows targeted alterations at the single nucleotide level.However, its clinical application is hindered by low editing efficiency andunintended mutations, necessitating extensive trial-and-error experimentationin the laboratory. To speed up this process, we present an attention-basedtwo-stage machine learning model that learns to predict the likelihood of allpossible editing outcomes for a given genomic target sequence. We furtherpropose a multi-task learning schema to jointly learn multiple base editors(i.e. variants) at once. Our model's predictions consistently demonstrated astrong correlation with the actual experimental results on multiple datasetsand base editor variants. These results provide further validation for themodels' capacity to enhance and accelerate the process of refining base editingdesigns.</description><author>Amina Mollaysa, Ahmed Allam, Michael Krauthammer</author><pubDate>Wed, 15 Nov 2023 16:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07636v2</guid></item><item><title>Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts</title><link>http://arxiv.org/abs/2311.09066v1</link><description>In the last decade, the United States has lost more than 500,000 people froman overdose involving prescription and illicit opioids(https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a nationalpublic health emergency (USDHHS, 2017). To more effectively preventunintentional opioid overdoses, medical practitioners require robust and timelytools that can effectively identify at-risk patients. Community-based socialmedia platforms such as Reddit allow self-disclosure for users to discussotherwise sensitive drug-related behaviors, often acting as indicators foropioid use disorder. Towards this, we present a moderate size corpus of 2500opioid-related posts from various subreddits spanning 6 different phases ofopioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. Forevery post, we annotate span-level extractive explanations and crucially studytheir role both in annotation quality and model development. We evaluateseveral state-of-the-art models in a supervised, few-shot, or zero-shotsetting. Experimental results and error analysis show that identifying thephases of opioid use disorder is highly contextual and challenging. However, wefind that using explanations during modeling leads to a significant boost inclassification accuracy demonstrating their beneficial role in a high-stakesdomain such as studying the opioid use disorder continuum. The dataset will bemade available for research on Github in the formal version.</description><author>Chenghao Yang, Tuhin Chakrabarty, Karli R Hochstatter, Melissa N Slavin, Nabila El-Bassel, Smaranda Muresan</author><pubDate>Wed, 15 Nov 2023 16:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09066v1</guid></item><item><title>Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints</title><link>http://arxiv.org/abs/2311.09065v1</link><description>We give a damped proximal augmented Lagrangian method (DPALM) for solvingproblems with a weakly-convex objective and convex linear/nonlinearconstraints. Instead of taking a full stepsize, DPALM adopts a damped dualstepsize to ensure the boundedness of dual iterates. We show that DPALM canproduce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterationsif each DPALM subproblem is solved to a proper accuracy. In addition, weestablish overall iteration complexity of DPALM when the objective is either aregularized smooth function or in a regularized compositional form. For theformer case, DPALM achieves the complexity of$\widetilde{\mathcal{O}}\left(\varepsilon^{-2.5} \right)$ to produce an$\varepsilon$-KKT point by applying an accelerated proximal gradient (APG)method to each DPALM subproblem. For the latter case, the complexity of DPALMis $\widetilde{\mathcal{O}}\left(\varepsilon^{-3} \right)$ to produce a near$\varepsilon$-KKT point by using an APG to solve a Moreau-envelope smoothedversion of each subproblem. Our outer iteration complexity and the overallcomplexity either generalize existing best ones from unconstrained orlinear-constrained problems to convex-constrained ones, or improve over thebest-known results on solving the same-structured problems. Furthermore,numerical experiments on linearly/quadratically constrained non-convexquadratic programs and linear-constrained robust nonlinear least squares areconducted to demonstrate the empirical efficiency of the proposed DPALM overseveral state-of-the art methods.</description><author>Hari Dahal, Wei Liu, Yangyang Xu</author><pubDate>Wed, 15 Nov 2023 16:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09065v1</guid></item><item><title>Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models</title><link>http://arxiv.org/abs/2311.09064v1</link><description>Systematic compositionality, or the ability to adapt to novel situations bycreating a mental model of the world using reusable pieces of knowledge,remains a significant challenge in machine learning. While there has beenconsiderable progress in the language domain, efforts towards systematic visualimagination, or envisioning the dynamical implications of a visual observation,are in their infancy. We introduce the Systematic Visual Imagination Benchmark(SVIB), the first benchmark designed to address this problem head-on. SVIBoffers a novel framework for a minimal world modeling problem, where models areevaluated based on their ability to generate one-step image-to-imagetransformations under a latent world dynamics. The framework provides benefitssuch as the possibility to jointly optimize for systematic perception andimagination, a range of difficulty levels, and the ability to control thefraction of possible factor combinations used during training. We provide acomprehensive evaluation of various baseline models on SVIB, offering insightinto the current state-of-the-art in systematic visual imagination. We hopethat this benchmark will help advance visual systematic compositionality.</description><author>Yeongbin Kim, Gautam Singh, Junyeong Park, Caglar Gulcehre, Sungjin Ahn</author><pubDate>Wed, 15 Nov 2023 16:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09064v1</guid></item><item><title>Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation</title><link>http://arxiv.org/abs/2311.00684v2</link><description>An ideal length-extrapolatable Transformer language model can handlesequences longer than the training length without any fine-tuning. Suchlong-context utilization capability relies heavily on a flexible positionalembedding design. Upon investigating the flexibility of existing largepre-trained Transformer language models, we find that the T5 family deserves acloser look, as its positional embeddings capture rich and flexible attentionpatterns. However, T5 suffers from the dispersed attention issue: the longerthe input sequence, the flatter the attention distribution. To alleviate theissue, we propose two attention alignment strategies via temperature scaling.Our findings show improvement on the long-context utilization capability of T5on language modeling, retrieval, multi-document question answering, and codecompletion tasks without any fine-tuning. This suggests that a flexiblepositional embedding design and attention alignment can go a long way towardTransformer length extrapolation.</description><author>Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky</author><pubDate>Wed, 15 Nov 2023 15:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00684v2</guid></item><item><title>Do Localization Methods Actually Localize Memorized Data in LLMs?</title><link>http://arxiv.org/abs/2311.09060v1</link><description>Large language models (LLMs) can memorize many pretrained sequences verbatim.This paper studies if we can locate a small set of neurons in LLMs responsiblefor memorizing a given sequence. While the concept of localization is oftenmentioned in prior work, methods for localization have never beensystematically and directly evaluated; we address this with two benchmarkingapproaches. In our INJ Benchmark, we actively inject a piece of new informationinto a small subset of LLM weights and measure whether localization methods canidentify these "ground truth" weights. In the DEL Benchmark, we studylocalization of pretrained data that LLMs have already memorized; while thissetting lacks ground truth, we can still evaluate localization by measuringwhether dropping out located neurons erases a memorized sequence from themodel. We evaluate five localization methods on our two benchmarks, and bothshow similar rankings. All methods exhibit promising localization ability,especially for pruning-based methods, though the neurons they identify are notnecessarily specific to a single memorized sequence.</description><author>Ting-Yun Chang, Jesse Thomason, Robin Jia</author><pubDate>Wed, 15 Nov 2023 15:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09060v1</guid></item><item><title>New Horizons in Parameter Regularization: A Constraint Approach</title><link>http://arxiv.org/abs/2311.09058v1</link><description>This work presents constrained parameter regularization (CPR), an alternativeto traditional weight decay. Instead of applying a constant penalty uniformlyto all parameters, we enforce an upper bound on a statistical measure (e.g.,the L$_2$-norm) of individual parameter groups. This reformulates learning as aconstrained optimization problem. To solve this, we utilize an adaptation ofthe augmented Lagrangian method. Our approach allows for varying regularizationstrengths across different parameter groups, removing the need for explicitpenalty coefficients in the regularization terms. CPR only requires twohyperparameters and introduces no measurable runtime overhead. We offerempirical evidence of CPR's effectiveness through experiments in the "grokking"phenomenon, image classification, and language modeling. Our findings show thatCPR can counteract the effects of grokking, and it consistently matches orsurpasses the performance of traditional weight decay.</description><author>Jörg K. H. Franke, Michael Hefenbrock, Gregor Koehler, Frank Hutter</author><pubDate>Wed, 15 Nov 2023 15:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09058v1</guid></item><item><title>ReadMe++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment</title><link>http://arxiv.org/abs/2305.14463v2</link><description>We present a systematic study and comprehensive evaluation of large languagemodels for automatic multilingual readability assessment. In particular, weconstruct ReadMe++, a multilingual multi-domain dataset with human annotationsof 9757 sentences in Arabic, English, French, Hindi, and Russian collected from112 different data sources. ReadMe++ offers more domain and language diversitythan existing readability datasets, making it ideal for benchmarkingmultilingual and non-English language models (including mBERT, XLM-R, mT5,Llama-2, GPT-4, etc.) in the supervised, unsupervised, and few-shot promptingsettings. Our experiments reveal that models fine-tuned on ReadMe++ outperformthose trained on single-domain datasets, showcasing superior performance onmulti-domain readability assessment and cross-lingual transfer capabilities. Wealso compare to traditional readability metrics (such as Flesch-Kincaid GradeLevel and Open Source Metric for Measuring Arabic Narratives), as well as thestate-of-the-art unsupervised metric RSRS (Martinc et al., 2021). We will makeour data and code publicly available at: https://github.com/tareknaous/readme.</description><author>Tarek Naous, Michael J. Ryan, Anton Lavrouk, Mohit Chandra, Wei Xu</author><pubDate>Wed, 15 Nov 2023 15:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14463v2</guid></item><item><title>Assessing Knowledge Editing in Language Models via Relation Perspective</title><link>http://arxiv.org/abs/2311.09053v1</link><description>Knowledge Editing (KE) for modifying factual knowledge in Large LanguageModels (LLMs) has been receiving increasing attention. However, existingknowledge editing methods are entity-centric, and it is unclear whether thisapproach is suitable for a relation-centric perspective. To address this gap,this paper constructs a new benchmark named RaKE, which focuses on Relationbased Knowledge Editing. In this paper, we establish a suite of innovativemetrics for evaluation and conduct comprehensive experiments involving variousknowledge editing baselines. We notice that existing knowledge editing methodsexhibit the potential difficulty in their ability to edit relations. Therefore,we further explore the role of relations in factual triplets within thetransformer. Our research results confirm that knowledge related to relationsis not only stored in the FFN network but also in the attention layers. Thisprovides experimental support for future relation-based knowledge editingmethods.</description><author>Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu Lei, Yixuan Weng, Ran Song, Kang Liu</author><pubDate>Wed, 15 Nov 2023 15:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09053v1</guid></item><item><title>Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications</title><link>http://arxiv.org/abs/2302.05763v4</link><description>Human-robot interaction (HRI) research is progressively addressingmulti-party scenarios, where a robot interacts with more than one human user atthe same time. Conversely, research is still at an early stage for human-robotcollaboration. The use of machine learning techniques to handle such type ofcollaboration requires data that are less feasible to produce than in a typicalHRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRCapplications. Based upon these concepts, this study also proposes analternative way of gathering data regarding multi-user activity, by collectingdata related to single users and merging them in post-processing, to reduce theeffort involved in producing recordings of pair settings. To validate thisstatement, 3D skeleton poses of activity of single users were collected andmerged in pairs. After this, such datapoints were used to separately train along short-term memory (LSTM) network and a variational autoencoder (VAE)composed of spatio-temporal graph convolutional networks (STGCN) to recognisethe joint activities of the pairs of people. The results showed that it ispossible to make use of data collected in this way for pair HRC settings andget similar performances compared to using training data regarding groups ofusers recorded under the same settings, relieving from the technicaldifficulties involved in producing these data. The related code and collected data are publicly available.</description><author>Francesco Semeraro, Jon Carberry, Angelo Cangelosi</author><pubDate>Wed, 15 Nov 2023 15:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05763v4</guid></item><item><title>Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts</title><link>http://arxiv.org/abs/2311.09050v1</link><description>Zero-shot Visual Question Answering (VQA) is a prominent vision-language taskthat examines both the visual and textual understanding capability of systemsin the absence of training data. Recently, by converting the images intocaptions, information across multi-modalities is bridged and Large LanguageModels (LLMs) can apply their strong zero-shot generalization capability tounseen questions. To design ideal prompts for solving VQA via LLMs, severalstudies have explored different strategies to select or generatequestion-answer pairs as the exemplar prompts, which guide LLMs to answer thecurrent questions effectively. However, they totally ignore the role ofquestion prompts. The original questions in VQA tasks usually encounterellipses and ambiguity which require intermediate reasoning. To this end, wepresent Reasoning Question Prompts for VQA tasks, which can further activatethe potential of LLMs in zero-shot scenarios. Specifically, for each question,we first generate self-contained questions as reasoning question prompts via anunsupervised question edition module considering sentence fluency, semanticintegrity and syntactic invariance. Each reasoning question prompt clearlyindicates the intent of the original question. This results in a set ofcandidate answers. Then, the candidate answers associated with their confidencescores acting as answer heuristics are fed into LLMs and produce the finalanswer. We evaluate reasoning question prompts on three VQA challenges,experimental results demonstrate that they can significantly improve theresults of LLMs on zero-shot setting and outperform existing state-of-the-artzero-shot methods on three out of four data sets. Our source code is publiclyreleased at \url{https://github.com/ECNU-DASE-NLP/RQP}.</description><author>Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin, Weining Qian</author><pubDate>Wed, 15 Nov 2023 15:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09050v1</guid></item><item><title>GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models</title><link>http://arxiv.org/abs/2311.09048v1</link><description>This paper presents GRASP, a novel benchmark to evaluate the languagegrounding and physical understanding capabilities of video-based multimodallarge language models (LLMs). This evaluation is accomplished via a two-tierapproach leveraging Unity simulations. The initial level tests for languagegrounding by assessing a model's ability to relate simple textual descriptionswith visual information. The second level evaluates the model's understandingof 'Intuitive Physics' principles, such as object permanence and continuity. Inaddition to releasing the benchmark, we use it to evaluate severalstate-of-the-art multimodal LLMs. Our evaluation reveals significantshortcomings in current models' language grounding and intuitive physics. Theseidentified limitations underline the importance of benchmarks like GRASP tomonitor the progress of future models in developing these competencies.</description><author>Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, Elia Bruni</author><pubDate>Wed, 15 Nov 2023 15:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09048v1</guid></item><item><title>In-context Learning and Gradient Descent Revisited</title><link>http://arxiv.org/abs/2311.07772v2</link><description>In-context learning (ICL) has shown impressive results in few-shot learningtasks, yet its underlying mechanism is still not fully understood. Recent workssuggest that ICL can be thought of as a gradient descent (GD) basedoptimization process. While promising, these results mainly focus on simplifiedsettings of ICL and provide only a preliminary evaluation of the similaritiesbetween the two methods. In this work, we revisit the comparison between ICLand GD-based finetuning and study what properties of ICL an equivalent processmust follow. We highlight a major difference in the flow of information betweenICL and standard finetuning. Namely, ICL can only rely on information fromlower layers at every point, while finetuning depends on loss gradients fromdeeper layers. We refer to this discrepancy as Layer Causality and show that alayer causal variant of the finetuning process aligns with ICL on par withvanilla finetuning and is even better in most cases across relevant metrics. Tothe best of our knowledge, this is the first work to discuss this discrepancyexplicitly and suggest a solution that tackles this problem with minimalchanges.</description><author>Tomer Bar Natan, Gilad Deutch, Nadav Magar, Guy Dar</author><pubDate>Wed, 15 Nov 2023 15:29:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07772v2</guid></item><item><title>MELA: Multilingual Evaluation of Linguistic Acceptability</title><link>http://arxiv.org/abs/2311.09033v1</link><description>Recent benchmarks for Large Language Models (LLMs) have mostly focused onapplication-driven tasks such as complex reasoning and code generation, andthis has led to a scarcity in purely linguistic evaluation of LLMs. Againstthis background, we introduce Multilingual Evaluation of LinguisticAcceptability -- MELA, the first multilingual benchmark on linguisticacceptability with 48K samples covering 10 languages from a diverse set oflanguage families. We establish baselines of commonly used LLMs along withsupervised models, and conduct cross-lingual transfer and multi-task learningexperiments with XLM-R. In pursuit of multilingual interpretability, we analyzethe weights of fine-tuned XLM-R to explore the possibility of identifyingtransfer difficulty between languages. Our results show that ChatGPT benefitsmuch from in-context examples but still lags behind fine-tuned XLM-R, while theperformance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting.Cross-lingual and multi-task learning experiments show that unlike semantictasks, in-language training data is crucial in acceptability judgements.Results in layerwise probing indicate that the upper layers of XLM-R become atask-specific but language-agnostic region for multilingual acceptabilityjudgment. We also introduce the concept of conflicting weight, which could be apotential indicator for the difficulty of cross-lingual transfer betweenlanguages. Our data will be available at https://github.com/sjtu-compling/MELA.</description><author>Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, Rui Wang, Hai Hu</author><pubDate>Wed, 15 Nov 2023 15:25:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09033v1</guid></item><item><title>Self-Annotated 3D Geometric Learning for Smeared Points Removal</title><link>http://arxiv.org/abs/2311.09029v1</link><description>There has been significant progress in improving the accuracy and quality ofconsumer-level dense depth sensors. Nevertheless, there remains a common depthpixel artifact which we call smeared points. These are points not on any 3Dsurface and typically occur as interpolations between foreground and backgroundobjects. As they cause fictitious surfaces, these points have the potential toharm applications dependent on the depth maps. Statistical outlier removalmethods fare poorly in removing these points as they tend also to remove actualsurface points. Trained network-based point removal faces difficulty inobtaining sufficient annotated data. To address this, we propose a fullyself-annotated method to train a smeared point removal classifier. Our approachrelies on gathering 3D geometric evidence from multiple perspectives toautomatically detect and annotate smeared points and valid points. To validatethe effectiveness of our method, we present a new benchmark dataset: the RealAzure-Kinect dataset. Experimental results and ablation studies show that ourmethod outperforms traditional filters and other self-annotated methods. Ourwork is publicly available athttps://github.com/wangmiaowei/wacv2024_smearedremover.git.</description><author>Miaowei Wang, Daniel Morris</author><pubDate>Wed, 15 Nov 2023 15:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09029v1</guid></item></channel></rss>