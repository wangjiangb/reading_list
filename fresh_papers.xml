<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 27 Nov 2024 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Generative Omnimatte: Learning to Decompose Video into Layers</title><link>http://arxiv.org/abs/2411.16683v1</link><description>Given a video and a set of input object masks, an omnimatte method aims todecompose the video into semantically meaningful layers containing individualobjects along with their associated effects, such as shadows and reflections.Existing omnimatte methods assume a static background or accurate pose anddepth estimation and produce poor decompositions when these assumptions areviolated. Furthermore, due to the lack of generative prior on natural videos,existing methods cannot complete dynamic occluded regions. We present a novelgenerative layered video decomposition framework to address the omnimatteproblem. Our method does not assume a stationary scene or require camera poseor depth information and produces clean, complete layers, including convincingcompletions of occluded dynamic regions. Our core idea is to train a videodiffusion model to identify and remove scene effects caused by a specificobject. We show that this model can be finetuned from an existing videoinpainting model with a small, carefully curated dataset, and demonstratehigh-quality decompositions and editing results for a wide range of casuallycaptured videos containing soft shadows, glossy reflections, splashing water,and more.</description><author>Yao-Chih Lee, Erika Lu, Sarah Rumbley, Michal Geyer, Jia-Bin Huang, Tali Dekel, Forrester Cole</author><pubDate>Mon, 25 Nov 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16683v1</guid></item><item><title>Factorized Visual Tokenization and Generation</title><link>http://arxiv.org/abs/2411.16681v1</link><description>Visual tokenizers are fundamental to image generation. They convert visualdata into discrete tokens, enabling transformer-based models to excel at imagegeneration. Despite their success, VQ-based tokenizers like VQGAN facesignificant limitations due to constrained vocabulary sizes. Simply expandingthe codebook often leads to training instability and diminishing performancegains, making scalability a critical challenge. In this work, we introduceFactorized Quantization (FQ), a novel approach that revitalizes VQ-basedtokenizers by decomposing a large codebook into multiple independentsub-codebooks. This factorization reduces the lookup complexity of largecodebooks, enabling more efficient and scalable visual tokenization. To ensureeach sub-codebook captures distinct and complementary information, we propose adisentanglement regularization that explicitly reduces redundancy, promotingdiversity across the sub-codebooks. Furthermore, we integrate representationlearning into the training process, leveraging pretrained vision models likeCLIP and DINO to infuse semantic richness into the learned representations.This design ensures our tokenizer captures diverse semantic levels, leading tomore expressive and disentangled representations. Experiments show that theproposed FQGAN model substantially improves the reconstruction quality ofvisual tokenizers, achieving state-of-the-art performance. We furtherdemonstrate that this tokenizer can be effectively adapted into auto-regressiveimage generation. https://showlab.github.io/FQGAN</description><author>Zechen Bai, Jianxiong Gao, Ziteng Gao, Pichao Wang, Zheng Zhang, Tong He, Mike Zheng Shou</author><pubDate>Mon, 25 Nov 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16681v1</guid></item><item><title>Quark: Real-time, High-resolution, and General Neural View Synthesis</title><link>http://arxiv.org/abs/2411.16680v1</link><description>We present a novel neural algorithm for performing high-quality,high-resolution, real-time novel view synthesis. From a sparse set of input RGBimages or videos streams, our network both reconstructs the 3D scene andrenders novel views at 1080p resolution at 30fps on an NVIDIA A100. Ourfeed-forward network generalizes across a wide variety of datasets and scenesand produces state-of-the-art quality for a real-time method. Our qualityapproaches, and in some cases surpasses, the quality of some of the top offlinemethods. In order to achieve these results we use a novel combination ofseveral key concepts, and tie them together into a cohesive and effectivealgorithm. We build on previous works that represent the scene usingsemi-transparent layers and use an iterative learned render-and-refine approachto improve those layers. Instead of flat layers, our method reconstructslayered depth maps (LDMs) that efficiently represent scenes with complex depthand occlusions. The iterative update steps are embedded in a multi-scale,UNet-style architecture to perform as much compute as possible at reducedresolution. Within each update step, to better aggregate the information frommultiple input views, we use a specialized Transformer-based network component.This allows the majority of the per-input image processing to be performed inthe input image space, as opposed to layer space, further increasingefficiency. Finally, due to the real-time nature of our reconstruction andrendering, we dynamically create and discard the internal 3D geometry for eachframe, generating the LDM for each view. Taken together, this produces a noveland effective algorithm for view synthesis. Through extensive evaluation, wedemonstrate that we achieve state-of-the-art quality at real-time rates.Project page: https://quark-3d.github.io/</description><author>John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Cl√©ment Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, Supreeth Achar, Kira Prabhu, Tiancheng Sun, Lynn Tsai, Ryan Overbeck</author><pubDate>Mon, 25 Nov 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16680v1</guid></item><item><title>Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?</title><link>http://arxiv.org/abs/2411.16679v1</link><description>We evaluate how well Large Language Models (LLMs) latently recall and composefacts to answer multi-hop queries like "In the year Scarlett Johansson wasborn, the Summer Olympics were hosted in the country of". One major challengein evaluating this ability is that LLMs may have developed shortcuts byencounters of the head entity "Scarlett Johansson" and the answer entity"United States" in the same training sequences or merely guess the answer basedon frequency-based priors. To prevent shortcuts, we exclude test queries wherethe head and answer entities co-appear in pretraining corpora. Through carefulselection of relations and facts and systematic removal of cases where modelsmight guess answers or exploit partial matches, we construct an evaluationdataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMsdemonstrate promising latent multi-hop reasoning abilities without exploitingshortcuts, but only for certain types of queries. For queries requiring latentrecall of countries as the intermediate answer, the best models achieve 80%latent composability, but this drops to just 5% for the recall of years.Comparisons with Chain-of-Thought composability highlight a significant gapbetween the ability of models to reason latently versus explicitly. Analysisreveals that latent representations of the intermediate answer are constructedmore often in queries with higher latent composability, and shows the emergenceof latent multi-hop reasoning during pretraining.</description><author>Sohee Yang, Nora Kassner, Elena Gribovskaya, Sebastian Riedel, Mor Geva</author><pubDate>Mon, 25 Nov 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16679v1</guid></item><item><title>Reconstructing Hand-Held Objects in 3D from Images and Videos</title><link>http://arxiv.org/abs/2404.06507v3</link><description>Objects manipulated by the hand (i.e., manipulanda) are particularlychallenging to reconstruct from Internet videos. Not only does the hand occludemuch of the object, but also the object is often only visible in a small numberof image pixels. At the same time, two strong anchors emerge in this setting:(1) estimated 3D hands help disambiguate the location and scale of the object,and (2) the set of manipulanda is small relative to all possible objects. Withthese insights in mind, we present a scalable paradigm for hand-held objectreconstruction that builds on recent breakthroughs in large language/visionmodels and 3D object datasets. Given a monocular RGB video, we aim toreconstruct hand-held object geometry in 3D, over time. In order to obtain thebest performing single frame model, we first present MCC-Hand-Object (MCC-HO),which jointly reconstructs hand and object geometry given a single RGB imageand inferred 3D hand as inputs. Subsequently, we prompt a text-to-3D generativemodel using GPT-4(V) to retrieve a 3D object model that matches the object inthe image(s); we call this alignment Retrieval-Augmented Reconstruction (RAR).RAR provides unified object geometry across all frames, and the result isrigidly aligned with both the input images and 3D MCC-HO observations in atemporally consistent manner. Experiments demonstrate that our approachachieves state-of-the-art performance on lab and Internet image/video datasets.We make our code and models available on the project website:https://janehwu.github.io/mcc-ho</description><author>Jane Wu, Georgios Pavlakos, Georgia Gkioxari, Jitendra Malik</author><pubDate>Mon, 25 Nov 2024 18:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06507v3</guid></item><item><title>Motion Code: Robust Time Series Classification and Forecasting via Sparse Variational Multi-Stochastic Processes Learning</title><link>http://arxiv.org/abs/2402.14081v3</link><description>Despite extensive research, time series classification and forecasting onnoisy data remain highly challenging. The main difficulties lie in findingsuitable mathematical concepts to describe time series and effectively separatenoise from the true signals. Unlike traditional methods treating time series asstatic vectors or fixed sequences, we propose a novel framework that views eachtime series, regardless of length, as a realization of a continuous-timestochastic process. This mathematical approach captures dependencies acrosstimestamps and detects hidden, time-varying signals within the noise. However,real-world data often involves multiple distinct dynamics, making itinsufficient to model the entire process with a single stochastic model. Toaddress this, we assign each dynamic a unique signature vector and introducethe concept of "most informative timestamps" to infer a sparse approximation ofthe individual dynamics from these vectors. The resulting model, called MotionCode, includes parameters that fully capture diverse underlying dynamics in anintegrated manner, enabling simultaneous classification and forecasting of timeseries. Extensive experiments on noisy datasets, including real-worldParkinson's disease sensor tracking, demonstrate Motion Code's strongperformance against established benchmarks for time series classification andforecasting.</description><author>Chandrajit Bajaj, Minh Nguyen</author><pubDate>Mon, 25 Nov 2024 18:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14081v3</guid></item><item><title>Is Hyper-Parameter Optimization Different for Software Analytics?</title><link>http://arxiv.org/abs/2401.09622v3</link><description>Yes. SE data can have "smoother" boundaries between classes (compared totraditional AI data sets). To be more precise, the magnitude of the secondderivative of the loss function found in SE data is typically much smaller. Anew hyper-parameter optimizer, called SMOOTHIE, can exploit this idiosyncrasyof SE data. We compare SMOOTHIE and a state-of-the-art AI hyper-parameteroptimizer on three tasks: (a) GitHub issue lifetime prediction (b) detectingstatic code warnings false alarm; (c) defect prediction. For completeness, wealso show experiments on some standard AI datasets. SMOOTHIE runs faster andpredicts better on the SE data--but ties on non-SE data with the AI tool. Hencewe conclude that SE data can be different to other kinds of data; and thosedifferences mean that we should use different kinds of algorithms for our data.To support open science and other researchers working in this area, all ourscripts and datasets are available on-line athttps://github.com/yrahul3910/smoothness-hpo/.</description><author>Rahul Yedida, Tim Menzies</author><pubDate>Mon, 25 Nov 2024 18:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09622v3</guid></item><item><title>Diffusion Features for Zero-Shot 6DoF Object Pose Estimation</title><link>http://arxiv.org/abs/2411.16668v1</link><description>Zero-shot object pose estimation enables the retrieval of object poses fromimages without necessitating object-specific training. In recent approachesthis is facilitated by vision foundation models (VFM), which are pre-trainedmodels that are effectively general-purpose feature extractors. Thecharacteristics exhibited by these VFMs vary depending on the training data,network architecture, and training paradigm. The prevailing choice in thisfield are self-supervised Vision Transformers (ViT). This study assesses theinfluence of Latent Diffusion Model (LDM) backbones on zero-shot poseestimation. In order to facilitate a comparison between the two families ofmodels on a common ground we adopt and modify a recent approach. Therefore, atemplate-based multi-staged method for estimating poses in a zero-shot fashionusing LDMs is presented. The efficacy of the proposed approach is empiricallyevaluated on three standard datasets for object-specific 6DoF pose estimation.The experiments demonstrate an Average Recall improvement of up to 27% over theViT baseline. The source code is available at: https://github.com/BvG1993/DZOP.</description><author>Bernd Von Gimborn, Philipp Ausserlechner, Markus Vincze, Stefan Thalhammer</author><pubDate>Mon, 25 Nov 2024 18:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16668v1</guid></item><item><title>OPMOS: Ordered Parallel Multi-Objective Shortest-Path</title><link>http://arxiv.org/abs/2411.16667v1</link><description>The Multi-Objective Shortest-Path (MOS) problem finds a set of Pareto-optimalsolutions from a start node to a destination node in a multi-attribute graph.To solve the NP-hard MOS problem, the literature explores heuristicmulti-objective A*-style algorithmic approaches. A generalized MOS algorithmmaintains a "frontier" of partial paths at each node and performs orderedprocessing to ensure that Pareto-optimal paths are generated to reach the goalnode. The algorithm becomes computationally intractable as the number ofobjectives increases due to a rapid increase in the non-dominated paths, andthe concomitantly large increase in Pareto-optimal solutions. While prior workshave focused on algorithmic methods to reduce the complexity, we tackle thischallenge by exploiting parallelism using an algorithm-architecture approach.The key insight is that MOS algorithms rely on the ordered execution of partialpaths to maintain high work efficiency. The OPMOS framework, proposed herein,unlocks ordered parallelism and efficiently exploits the concurrent executionof multiple paths in MOS. Experimental evaluation using the NVIDIA GH200Superchip shows the performance scaling potential of OPMOS on work efficiencyand parallelism using a real-world application to ship routing.</description><author>Leo Gold, Adam Bienkowski, David Sidoti, Krishna Pattipati, Omer Khan</author><pubDate>Mon, 25 Nov 2024 18:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16667v1</guid></item><item><title>CatNet: Effective FDR Control in LSTM with Gaussian Mirrors and SHAP Feature Importance</title><link>http://arxiv.org/abs/2411.16666v1</link><description>We introduce CatNet, an algorithm that effectively controls False DiscoveryRate (FDR) and selects significant features in LSTM with the Gaussian Mirror(GM) method. To evaluate the feature importance of LSTM in time series, weintroduce a vector of the derivative of the SHapley Additive exPlanations(SHAP) to measure feature importance. We also propose a new kernel-baseddependence measure to avoid multicollinearity in the GM algorithm, to make arobust feature selection with controlled FDR. We use simulated data to evaluateCatNet's performance in both linear models and LSTM models with different linkfunctions. The algorithm effectively controls the FDR while maintaining a highstatistical power in all cases. We also evaluate the algorithm's performance indifferent low-dimensional and high-dimensional cases, demonstrating itsrobustness in various input dimensions. To evaluate CatNet's performance inreal world applications, we construct a multi-factor investment portfolio toforecast the prices of S\&amp;P 500 index components. The results demonstrate thatour model achieves superior predictive accuracy compared to traditional LSTMmodels without feature selection and FDR control. Additionally, CatNeteffectively captures common market-driving features, which helps informeddecision-making in financial markets by enhancing the interpretability ofpredictions. Our study integrates of the Gaussian Mirror algorithm with LSTMmodels for the first time, and introduces SHAP values as a new featureimportance metric for FDR control methods, marking a significant advancement infeature selection and error control for neural networks.</description><author>Jiaan Han, Junxiao Chen, Yanzhe Fu</author><pubDate>Mon, 25 Nov 2024 18:53:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16666v1</guid></item><item><title>Edge Weight Prediction For Category-Agnostic Pose Estimation</title><link>http://arxiv.org/abs/2411.16665v1</link><description>Category-Agnostic Pose Estimation (CAPE) localizes keypoints across diverseobject categories with a single model, using one or a few annotated supportimages. Recent works have shown that using a pose graph (i.e., treatingkeypoints as nodes in a graph rather than isolated points) helps handleocclusions and break symmetry. However, these methods assume a static posegraph with equal-weight edges, leading to suboptimal results. We introduceEdgeCape, a novel framework that overcomes these limitations by predicting thegraph's edge weights which optimizes localization. To further leveragestructural priors, we propose integrating Markovian Structural Bias, whichmodulates the self-attention interaction between nodes based on the number ofhops between them. We show that this improves the model's ability to captureglobal spatial dependencies. Evaluated on the MP-100 benchmark, which includes100 categories and over 20K images, EdgeCape achieves state-of-the-art resultsin the 1-shot setting and leads among similar-sized methods in the 5-shotsetting, significantly improving keypoint localization accuracy. Our code ispublicly available.</description><author>Or Hirschorn, Shai Avidan</author><pubDate>Mon, 25 Nov 2024 18:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16665v1</guid></item><item><title>Gaussian Process Priors for Boundary Value Problems of Linear Partial Differential Equations</title><link>http://arxiv.org/abs/2411.16663v1</link><description>Solving systems of partial differential equations (PDEs) is a fundamentaltask in computational science, traditionally addressed by numerical solvers.Recent advancements have introduced neural operators and physics-informedneural networks (PINNs) to tackle PDEs, achieving reduced computational costsat the expense of solution quality and accuracy. Gaussian processes (GPs) havealso been applied to linear PDEs, with the advantage of always yielding precisesolutions. In this work, we propose Boundary Ehrenpreis-Palamodov GaussianProcesses (B-EPGPs), a novel framework for constructing GP priors that satisfyboth general systems of linear PDEs with constant coefficients and linearboundary conditions. We explicitly construct GP priors for representative PDEsystems with practical boundary conditions. Formal proofs of correctness areprovided and empirical results demonstrating significant accuracy improvementsover state-of-the-art neural operator approaches.</description><author>Jianlei Huang, Marc H√§rk√∂nen, Markus Lange-Hegermann, Bogdan Rai≈£ƒÉ</author><pubDate>Mon, 25 Nov 2024 18:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16663v1</guid></item><item><title>Fast training of large kernel models with delayed projections</title><link>http://arxiv.org/abs/2411.16658v1</link><description>Classical kernel machines have historically faced significant challenges inscaling to large datasets and model sizes--a key ingredient that has driven thesuccess of neural networks. In this paper, we present a new methodology forbuilding kernel machines that can scale efficiently with both data size andmodel size. Our algorithm introduces delayed projections to PreconditionedStochastic Gradient Descent (PSGD) allowing the training of much larger modelsthan was previously feasible, pushing the practical limits of kernel-basedlearning. We validate our algorithm, EigenPro4, across multiple datasets,demonstrating drastic training speed up over the existing methods whilemaintaining comparable or better classification accuracy.</description><author>Amirhesam Abedsoltan, Siyuan Ma, Parthe Pandit, Mikhail Belkin</author><pubDate>Mon, 25 Nov 2024 18:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16658v1</guid></item><item><title>DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation</title><link>http://arxiv.org/abs/2411.16657v1</link><description>Storytelling video generation (SVG) has recently emerged as a task to createlong, multi-motion, multi-scene videos that consistently represent the storydescribed in the input text script. SVG holds great potential for diversecontent creation in media and entertainment; however, it also presentssignificant challenges: (1) objects must exhibit a range of fine-grained,complex motions, (2) multiple objects need to appear consistently acrossscenes, and (3) subjects may require multiple motions with seamless transitionswithin a single scene. To address these challenges, we propose DreamRunner, anovel story-to-video generation method: First, we structure the input scriptusing a large language model (LLM) to facilitate both coarse-grained sceneplanning as well as fine-grained object-level layout and motion planning. Next,DreamRunner presents retrieval-augmented test-time adaptation to capture targetmotion priors for objects in each scene, supporting diverse motioncustomization based on retrieved videos, thus facilitating the generation ofnew videos with complex, scripted motions. Lastly, we propose a novelspatial-temporal region-based 3D attention and prior injection module SR3AI forfine-grained object-motion binding and frame-by-frame semantic control. Wecompare DreamRunner with various SVG baselines, demonstrating state-of-the-artperformance in character consistency, text alignment, and smooth transitions.Additionally, DreamRunner exhibits strong fine-grained condition-followingability in compositional text-to-video generation, significantly outperformingbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability togenerate multi-object interactions with qualitative examples.</description><author>Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, Mohit Bansal</author><pubDate>Mon, 25 Nov 2024 18:41:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16657v1</guid></item><item><title>Enhancing Multimodal Medical Image Classification using Cross-Graph Modal Contrastive Learning</title><link>http://arxiv.org/abs/2410.17494v2</link><description>The classification of medical images is a pivotal aspect of diseasediagnosis, often enhanced by deep learning techniques. However, traditionalapproaches typically focus on unimodal medical image data, neglecting theintegration of diverse non-image patient data. This paper proposes a novelCross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal medicalimage classification. The model effectively integrates both image and non-imagedata by constructing cross-modality graphs and leveraging contrastive learningto align multimodal features in a shared latent space. An inter-modalityfeature scaling module further optimizes the representation learning process byreducing the gap between heterogeneous modalities. The proposed approach isevaluated on two datasets: a Parkinson's disease (PD) dataset and a publicmelanoma dataset. Results demonstrate that CGMCL outperforms conventionalunimodal methods in accuracy, interpretability, and early disease prediction.Additionally, the method shows superior performance in multi-class melanomaclassification. The CGMCL framework provides valuable insights into medicalimage classification while offering improved disease interpretability andpredictive capabilities.</description><author>Jun-En Ding, Chien-Chin Hsu, Feng Liu</author><pubDate>Mon, 25 Nov 2024 18:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17494v2</guid></item><item><title>Self-Generated Critiques Boost Reward Modeling for Language Models</title><link>http://arxiv.org/abs/2411.16646v1</link><description>Reward modeling is crucial for aligning large language models (LLMs) withhuman preferences, especially in reinforcement learning from human feedback(RLHF). However, current reward models mainly produce scalar scores andstruggle to incorporate critiques in a natural language format. We hypothesizethat predicting both critiques and the scalar reward would improve rewardmodeling ability. Motivated by this, we propose Critic-RM, a framework thatimproves reward models using self-generated critiques without extrasupervision. Critic-RM employs a two-stage process: generating and filteringhigh-quality critiques, followed by joint fine-tuning on reward prediction andcritique generation. Experiments across benchmarks show that Critic-RM improvesreward modeling accuracy by 3.7%-7.3% compared to standard reward models andLLM judges, demonstrating strong performance and data efficiency. Additionalstudies further validate the effectiveness of generated critiques in rectifyingflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.</description><author>Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou</author><pubDate>Mon, 25 Nov 2024 18:28:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16646v1</guid></item><item><title>Recommender Systems for Good (RS4Good): Survey of Use Cases and a Call to Action for Research that Matters</title><link>http://arxiv.org/abs/2411.16645v1</link><description>In the area of recommender systems, the vast majority of research efforts isspent on developing increasingly sophisticated recommendation models, alsousing increasingly more computational resources. Unfortunately, most of theseresearch efforts target a very small set of application domains, mostlye-commerce and media recommendation. Furthermore, many of these models arenever evaluated with users, let alone put into practice. The scientific,economic and societal value of much of these efforts by scholars thereforeremains largely unclear. To achieve a stronger positive impact resulting fromthese efforts, we posit that we as a research community should more oftenaddress use cases where recommender systems contribute to societal good(RS4Good). In this opinion piece, we first discuss a number of examples wherethe use of recommender systems for problems of societal concern has beensuccessfully explored in the literature. We then proceed by outlining aparadigmatic shift that is needed to conduct successful RS4Good research, wherethe key ingredients are interdisciplinary collaborations and longitudinalevaluation approaches with humans in the loop.</description><author>Dietmar Jannach, Alan Said, Marko Tkalƒçiƒç, Markus Zanker</author><pubDate>Mon, 25 Nov 2024 18:27:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16645v1</guid></item><item><title>Exploring Discrete Flow Matching for 3D De Novo Molecule Generation</title><link>http://arxiv.org/abs/2411.16644v1</link><description>Deep generative models that produce novel molecular structures have thepotential to facilitate chemical discovery. Flow matching is a recentlyproposed generative modeling framework that has achieved impressive performanceon a variety of tasks including those on biomolecular structures. The seminalflow matching framework was developed only for continuous data. However, denovo molecular design tasks require generating discrete data such as atomicelements or sequences of amino acid residues. Several discrete flow matchingmethods have been proposed recently to address this gap. In this work webenchmark the performance of existing discrete flow matching methods for 3D denovo small molecule generation and provide explanations of their differingbehavior. As a result we present FlowMol-CTMC, an open-source model thatachieves state of the art performance for 3D de novo design with fewerlearnable parameters than existing methods. Additionally, we propose the use ofmetrics that capture molecule quality beyond local chemical valency constraintsand towards higher-order structural motifs. These metrics show that even thoughbasic constraints are satisfied, the models tend to produce unusual andpotentially problematic functional groups outside of the training datadistribution. Code and trained models for reproducing this work are availableat \url{https://github.com/dunni3/FlowMol}.</description><author>Ian Dunn, David R. Koes</author><pubDate>Mon, 25 Nov 2024 18:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16644v1</guid></item><item><title>Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective</title><link>http://arxiv.org/abs/2411.16642v1</link><description>Jailbreak prompts pose a significant threat in AI and cybersecurity, as theyare crafted to bypass ethical safeguards in large language models, potentiallyenabling misuse by cybercriminals. This paper analyzes jailbreak prompts from acyber defense perspective, exploring techniques like prompt injection andcontext manipulation that allow harmful content generation, content filterevasion, and sensitive information extraction. We assess the impact ofsuccessful jailbreaks, from misinformation and automated social engineering tohazardous content creation, including bioweapons and explosives. To addressthese threats, we propose strategies involving advanced prompt analysis,dynamic safety protocols, and continuous model fine-tuning to strengthen AIresilience. Additionally, we highlight the need for collaboration among AIresearchers, cybersecurity experts, and policymakers to set standards forprotecting AI systems. Through case studies, we illustrate these cyber defenseapproaches, promoting responsible AI practices to maintain system integrity andpublic trust. \textbf{\color{red}Warning: This paper contains content which thereader may find offensive.}</description><author>Jean Marie Tshimula, Xavier Ndona, D'Jeff K. Nkashama, Pierre-Martin Tardif, Froduald Kabanza, Marc Frappier, Shengrui Wang</author><pubDate>Mon, 25 Nov 2024 18:23:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16642v1</guid></item><item><title>DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding</title><link>http://arxiv.org/abs/2311.11810v4</link><description>This work presents DocPedia, a novel large multimodal model (LMM) forversatile OCR-free document understanding, capable of parsing images up to2,560$\times$2,560 resolution. Unlike existing work either struggle withhigh-resolution documents or give up the large language model thus vision orlanguage ability constrained, our DocPedia directly processes visual input inthe frequency domain rather than the pixel space. The unique characteristicenables DocPedia to capture a greater amount of visual and textual informationusing a limited number of visual tokens. To consistently enhance bothperception and comprehension abilities of our model, we develop a dual-stagetraining strategy and enrich instructions/annotations of all training taskscovering multiple document types. Extensive quantitative and qualitativeexperiments conducted on various publicly available benchmarks confirm themutual benefits of jointly learning perception and comprehension tasks. Theresults provide further evidence of the effectiveness and superior performanceof our DocPedia over other methods.</description><author>Hao Feng, Qi Liu, Hao Liu, Jingqun Tang, Wengang Zhou, Houqiang Li, Can Huang</author><pubDate>Mon, 25 Nov 2024 18:17:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11810v4</guid></item><item><title>Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation</title><link>http://arxiv.org/abs/2411.16638v1</link><description>Modern LLMs can now produce highly readable abstractive summaries, to thepoint where traditional automated metrics for evaluating summary quality, suchas ROUGE, have become saturated. However, LLMs still sometimes introduceunwanted content into summaries, i.e., information inconsistent with orunsupported by their source. Measuring the occurrence of these often subtle``hallucinations'' automatically has proved to be challenging. This in turn hasmotivated development of a variety of metrics intended to measure the factualconsistency of generated summaries against their source. But are theseapproaches measuring what they purport to do? In this work, we stress-testautomatic factuality metrics. Specifically, we investigate whether and to whatdegree superficial attributes of summary texts suffice to predict``factuality'', finding that a (supervised) model using only such shallowfeatures is reasonably competitive with SOTA factuality scoring methods. Wethen evaluate how factuality metrics respond to factual corrections ininconsistent summaries and find that only a few show meaningful improvements.In contrast, some metrics are more sensitive to benign, non-factual edits.Motivated by these insights, we show that one can ``game'' (most) automaticfactuality metrics, i.e., reliably inflate ``factuality'' scores by appendinginnocuous sentences to generated summaries.Taken together, our results raisequestions about the degree to which we should rely on existing automatedfactuality metrics and what exactly we want ``factuality metrics'' to measure.</description><author>Sanjana Ramprasad, Byron C. Wallace</author><pubDate>Mon, 25 Nov 2024 18:15:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16638v1</guid></item><item><title>Word4Per: Zero-shot Composed Person Retrieval</title><link>http://arxiv.org/abs/2311.16515v3</link><description>Searching for specific person has great social benefits and security value,and it often involves a combination of visual and textual information.Conventional person retrieval methods, whether image-based or text-based,usually fall short in effectively harnessing both types of information, leadingto the loss of accuracy. In this paper, a whole new task called Composed PersonRetrieval (CPR) is proposed to jointly utilize both image and text informationfor target person retrieval. However, the supervised CPR requires very costlymanual annotation dataset, while there are currently no available resources. Tomitigate this issue, we firstly introduce the Zero-shot Composed PersonRetrieval (ZS-CPR), which leverages existing domain-related data to resolve theCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, wepropose a two-stage learning framework, Word4Per, where a lightweight TextualInversion Network (TINet) and a text-based person retrieval model based onfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learnedwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text ComposedPerson Retrieval (ITCPR) dataset is built as the benchmark to assess theperformance of the proposed Word4Per framework. Extensive experiments underboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPRtask, surpassing the comparative methods by over 10\%. The code and ITCPRdataset will be publicly available athttps://github.com/Delong-liu-bupt/Word4Per.</description><author>Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong</author><pubDate>Mon, 25 Nov 2024 18:11:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16515v3</guid></item><item><title>LegoPET: Hierarchical Feature Guided Conditional Diffusion for PET Image Reconstruction</title><link>http://arxiv.org/abs/2411.16629v1</link><description>Positron emission tomography (PET) is widely utilized for cancer detectiondue to its ability to visualize functional and biological processes in vivo.PET images are usually reconstructed from histogrammed raw data (sinograms)using traditional iterative techniques (e.g., OSEM, MLEM). Recently, deeplearning (DL) methods have shown promise by directly mapping raw sinogram datato PET images. However, DL approaches that are regression-based or GAN-basedoften produce overly smoothed images or introduce various artifactsrespectively. Image-conditioned diffusion probabilistic models (cDPMs) areanother class of likelihood-based DL techniques capable of generating highlyrealistic and controllable images. While cDPMs have notable strengths, theystill face challenges such as maintain correspondence and consistency betweeninput and output images when they are from different domains (e.g., sinogramvs. image domain) as well as slow convergence rates. To address theselimitations, we introduce LegoPET, a hierarchical feature guided conditionaldiffusion model for high-perceptual quality PET image reconstruction fromsinograms. We conducted several experiments demonstrating that LegoPET not onlyimproves the performance of cDPMs but also surpasses recent DL-based PET imagereconstruction techniques in terms of visual quality and pixel-level PSNR/SSIMmetrics. Our code is available at https://github.com/yransun/LegoPET.</description><author>Yiran Sun, Osama Mawlawi</author><pubDate>Mon, 25 Nov 2024 18:05:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16629v1</guid></item><item><title>Inference-Time Policy Steering through Human Interactions</title><link>http://arxiv.org/abs/2411.16627v1</link><description>Generative policies trained with human demonstrations can autonomouslyaccomplish multimodal, long-horizon tasks. However, during inference, humansare often removed from the policy execution loop, limiting the ability to guidea pre-trained policy towards a specific sub-goal or trajectory shape amongmultiple predictions. Naive human intervention may inadvertently exacerbatedistribution shift, leading to constraint violations or execution failures. Tobetter align policy output with human intent without inducingout-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS)framework that leverages human interactions to bias the generative samplingprocess, rather than fine-tuning the policy on interaction data. We evaluateITPS across three simulated and real-world benchmarks, testing three forms ofhuman interaction and associated alignment distance metrics. Among six samplingstrategies, our proposed stochastic sampling with diffusion policy achieves thebest trade-off between alignment and distribution shift. Videos are availableat https://yanweiw.github.io/itps/.</description><author>Yanwei Wang, Lirui Wang, Yilun Du, Balakumar Sundaralingam, Xuning Yang, Yu-Wei Chao, Claudia Perez-D'Arpino, Dieter Fox, Julie Shah</author><pubDate>Mon, 25 Nov 2024 18:03:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16627v1</guid></item><item><title>Multi-Modal Deep Learning for Credit Rating Prediction Using Text and Numerical Data Streams</title><link>http://arxiv.org/abs/2304.10740v3</link><description>Knowing which factors are significant in credit rating assignment leads tobetter decision-making. However, the focus of the literature thus far has beenmostly on structured data, and fewer studies have addressed unstructured ormulti-modal datasets. In this paper, we present an analysis of the mosteffective architectures for the fusion of deep learning models for theprediction of company credit rating classes, by using structured andunstructured datasets of different types. In these models, we tested differentcombinations of fusion strategies with different deep learning models,including CNN, LSTM, GRU, and BERT. We studied data fusion strategies in termsof level (including early and intermediate fusion) and techniques (includingconcatenation and cross-attention). Our results show that a CNN-basedmulti-modal model with two fusion strategies outperformed other multi-modaltechniques. In addition, by comparing simple architectures with more complexones, we found that more sophisticated deep learning models do not necessarilyproduce the highest performance; however, if attention-based models areproducing the best results, cross-attention is necessary as a fusion strategy.Finally, our comparison of rating agencies on short-, medium-, and long-termperformance shows that Moody's credit ratings outperform those of otheragencies like Standard &amp; Poor's and Fitch Ratings.</description><author>Mahsa Tavakoli, Rohitash Chandra, Fengrui Tian, Cristi√°n Bravo</author><pubDate>Mon, 25 Nov 2024 18:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10740v3</guid></item><item><title>Imperceptible Adversarial Examples in the Physical World</title><link>http://arxiv.org/abs/2411.16622v1</link><description>Adversarial examples in the digital domain against deep learning-basedcomputer vision models allow for perturbations that are imperceptible to humaneyes. However, producing similar adversarial examples in the physical world hasbeen difficult due to the non-differentiable image distortion functions invisual sensing systems. The existing algorithms for generating physicallyrealizable adversarial examples often loosen their definition of adversarialexamples by allowing unbounded perturbations, resulting in obvious or evenstrange visual patterns. In this work, we make adversarial examplesimperceptible in the physical world using a straight-through estimator (STE,a.k.a. BPDA). We employ STE to overcome the non-differentiability -- applyingexact, non-differentiable distortions in the forward pass of thebackpropagation step, and using the identity function in the backward pass. Ourdifferentiable rendering extension to STE also enables imperceptibleadversarial patches in the physical world. Using printout photos, andexperiments in the CARLA simulator, we show that STE enables fast generation of$\ell_\infty$ bounded adversarial examples despite the non-differentiabledistortions. To the best of our knowledge, this is the first work demonstratingimperceptible adversarial examples bounded by small $\ell_\infty$ norms in thephysical world that force zero classification accuracy in the globalperturbation threat model and cause near-zero ($4.22\%$) AP50 in objectdetection in the patch perturbation threat model. We urge the community tore-evaluate the threat of adversarial examples in the physical world.</description><author>Weilin Xu, Sebastian Szyller, Cory Cornelius, Luis Murillo Rojas, Marius Arvinte, Alvaro Velasquez, Jason Martin, Nageen Himayat</author><pubDate>Mon, 25 Nov 2024 18:02:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16622v1</guid></item><item><title>Text-guided Image Restoration and Semantic Enhancement for Text-to-Image Person Retrieval</title><link>http://arxiv.org/abs/2307.09059v2</link><description>The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specificperson images according to the given textual descriptions. A primary challengein this task is bridging the substantial representational gap between visualand textual modalities. The prevailing methods map texts and images intounified embedding space for matching, while the intricate semanticcorrespondences between texts and images are still not effectively constructed.To address this issue, we propose a novel TIPR framework to build fine-grainedinteractions and alignment between person images and the corresponding texts.Specifically, via fine-tuning the Contrastive Language-Image Pre-training(CLIP) model, a visual-textual dual encoder is firstly constructed, topreliminarily align the image and text features. Secondly, a Text-guided ImageRestoration (TIR) auxiliary task is proposed to map abstract textual entitiesto specific image regions, improving the alignment between local textual andvisual embeddings. Additionally, a cross-modal triplet loss is presented tohandle hard samples, and further enhance the model's discriminability for minordifferences. Moreover, a pruning-based text data augmentation approach isproposed to enhance focus on essential elements in descriptions, therebyavoiding excessive model attention to less significant information. Theexperimental results show our proposed method outperforms state-of-the-artmethods on three popular benchmark datasets, and the code will be made publiclyavailable at https://github.com/Delong-liu-bupt/SEN.</description><author>Delong Liu, Haiwen Li, Zhicheng Zhao, Yuan Dong, Nikolaos V. Boulgouris</author><pubDate>Mon, 25 Nov 2024 18:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09059v2</guid></item><item><title>Human-Activity AGV Quality Assessment: A Benchmark Dataset and an Objective Evaluation Metric</title><link>http://arxiv.org/abs/2411.16619v1</link><description>AI-driven video generation techniques have made significant progress inrecent years. However, AI-generated videos (AGVs) involving human activitiesoften exhibit substantial visual and semantic distortions, hindering thepractical application of video generation technologies in real-world scenarios.To address this challenge, we conduct a pioneering study on human activity AGVquality assessment, focusing on visual quality evaluation and theidentification of semantic distortions. First, we construct the AI-GeneratedHuman activity Video Quality Assessment (Human-AGVQA) dataset, consisting of3,200 AGVs derived from 8 popular text-to-video (T2V) models using 400 textprompts that describe diverse human activities. We conduct a subjective studyto evaluate the human appearance quality, action continuity quality, andoverall video quality of AGVs, and identify semantic issues of human bodyparts. Based on Human-AGVQA, we benchmark the performance of T2V models andanalyze their strengths and weaknesses in generating different categories ofhuman activities. Second, we develop an objective evaluation metric, namedAI-Generated Human activity Video Quality metric (GHVQ), to automaticallyanalyze the quality of human activity AGVs. GHVQ systematically extractshuman-focused quality features, AI-generated content-aware quality features,and temporal continuity features, making it a comprehensive and explainablequality metric for human activity AGVs. The extensive experimental results showthat GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by alarge margin, demonstrating its efficacy in assessing the quality of humanactivity AGVs. The Human-AGVQA dataset and GHVQ metric will be released inpublic at https://github.com/zczhang-sjtu/GHVQ.git</description><author>Zhichao Zhang, Wei Sun, Xinyue Li, Yunhao Li, Qihang Ge, Jun Jia, Zicheng Zhang, Zhongpeng Ji, Fengyu Sun, Shangling Jui, Xiongkuo Min, Guangtao Zhai</author><pubDate>Mon, 25 Nov 2024 17:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16619v1</guid></item><item><title>Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions</title><link>http://arxiv.org/abs/2411.14405v2</link><description>Currently OpenAI o1 sparks a surge of interest in the study of largereasoning models (LRM). Building on this momentum, Marco-o1 not only focuses ondisciplines with standard answers, such as mathematics, physics, and coding --which are well-suited for reinforcement learning (RL) -- but also placesgreater emphasis on open-ended resolutions. We aim to address the question:''Can the o1 model effectively generalize to broader domains where clearstandards are absent and rewards are challenging to quantify?'' Marco-o1 ispowered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS),reflection mechanisms, and innovative reasoning strategies -- optimized forcomplex real-world problem-solving tasks.</description><author>Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang</author><pubDate>Mon, 25 Nov 2024 17:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14405v2</guid></item><item><title>StructFormer: Document Structure-based Masked Attention and its Impact on Language Model Pre-Training</title><link>http://arxiv.org/abs/2411.16618v1</link><description>Most state-of-the-art techniques for Language Models (LMs) today rely ontransformer-based architectures and their ubiquitous attention mechanism.However, the exponential growth in computational requirements with longer inputsequences confines Transformers to handling short passages. Recent efforts haveaimed to address this limitation by introducing selective attention mechanisms,notably local and global attention. While sparse attention mechanisms, akin tofull attention in being Turing-complete, have been theoretically established,their practical impact on pre-training remains unexplored. This study focuseson empirically assessing the influence of global attention on BERTpre-training. The primary steps involve creating an extensive corpus ofstructure-aware text through arXiv data, alongside a text-only counterpart. Wecarry out pre-training on these two datasets, investigate shifts in attentionpatterns, and assess their implications for downstream tasks. Our analysisunderscores the significance of incorporating document structure into LMmodels, demonstrating their capacity to excel in more abstract tasks, such asdocument understanding.</description><author>Kaustubh Ponkshe, Venkatapathy Subramanian, Natwar Modani, Ganesh Ramakrishnan</author><pubDate>Mon, 25 Nov 2024 17:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16618v1</guid></item><item><title>GeoFormer: A Multi-Polygon Segmentation Transformer</title><link>http://arxiv.org/abs/2411.16616v1</link><description>In remote sensing there exists a common need for learning scale invariantshapes of objects like buildings. Prior works relies on tweaking multiple lossfunctions to convert segmentation maps into the final scale invariantrepresentation, necessitating arduous design and optimization. For this purposewe introduce the GeoFormer, a novel architecture which presents a remedy to thesaid challenges, learning to generate multipolygons end-to-end. By modelingkeypoints as spatially dependent tokens in an auto-regressive manner, theGeoFormer outperforms existing works in delineating building objects fromsatellite imagery. We evaluate the robustness of the GeoFormer against formermethods through a variety of parameter ablations and highlight the advantagesof optimizing a single likelihood function. Our study presents the firstsuccessful application of auto-regressive transformer models for multi-polygonpredictions in remote sensing, suggesting a promising methodologicalalternative for building vectorization.</description><author>Maxim Khomiakov, Michael Riis Andersen, Jes Frellsen</author><pubDate>Mon, 25 Nov 2024 17:54:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16616v1</guid></item><item><title>Graph Pooling with Local Cluster Selection</title><link>http://arxiv.org/abs/2411.16615v1</link><description>Graph poolings in GNNs are a family of operations which take graphs as inputsand produce coarsened graphs as output. Modern graph poolings are trainable andclosely related to GNNs, which learn to pool graphs under differentassumptions. Though there are various assumptions, the procedure of generatingpooled graphs is relatively similar and limited. This work formalizes a novelprocedure of pooling graphs, along with a graph pooling approach for averagesituations.</description><author>Yizhu Chen</author><pubDate>Mon, 25 Nov 2024 17:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16615v1</guid></item><item><title>Efficient Biological Data Acquisition through Inference Set Design</title><link>http://arxiv.org/abs/2410.19631v2</link><description>In drug discovery, highly automated high-throughput laboratories are used toscreen a large number of compounds in search of effective drugs. Theseexperiments are expensive, so one might hope to reduce their cost byexperimenting on a subset of the compounds, and predicting the outcomes of theremaining experiments. In this work, we model this scenario as a sequentialsubset selection problem: we aim to select the smallest set of candidates inorder to achieve some desired level of accuracy for the system as a whole. Ourkey observation is that, if there is heterogeneity in the difficulty of theprediction problem across the input space, selectively obtaining the labels forthe hardest examples in the acquisition pool will leave only the relativelyeasy examples to remain in the inference set, leading to better overall systemperformance. We call this mechanism inference set design, and propose the useof a confidence-based active learning solution to prune out these challengingexamples. Our algorithm includes an explicit stopping criterion that stopsrunning the experiments when it is sufficiently confident that the system hasreached the target performance. Our empirical studies on image and moleculardatasets, as well as a real-world large-scale biological assay, show thatactive learning for inference set design leads to significant reduction inexperimental cost while retaining high system performance.</description><author>Ihor Neporozhnii, Julien Roy, Emmanuel Bengio, Jason Hartford</author><pubDate>Mon, 25 Nov 2024 17:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19631v2</guid></item><item><title>Recent Trends in Linear Text Segmentation: a Survey</title><link>http://arxiv.org/abs/2411.16613v1</link><description>Linear Text Segmentation is the task of automatically tagging text documentswith topic shifts, i.e. the places in the text where the topics change. Awell-established area of research in Natural Language Processing, drawing fromwell-understood concepts in linguistic and computational linguistic research,the field has recently seen a lot of interest as a result of the surge of text,video, and audio available on the web, which in turn require ways ofsummarising and categorizing the mole of content for which linear textsegmentation is a fundamental step. In this survey, we provide an extensiveoverview of current advances in linear text segmentation, describing the stateof the art in terms of resources and approaches for the task. Finally, wehighlight the limitations of available resources and of the task itself, whileindicating ways forward based on the most recent literature and under-exploredresearch directions.</description><author>Iacopo Ghinassi, Lin Wang, Chris Newell, Matthew Purver</author><pubDate>Mon, 25 Nov 2024 17:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16613v1</guid></item><item><title>OminiControl: Minimal and Universal Control for Diffusion Transformer</title><link>http://arxiv.org/abs/2411.15098v2</link><description>In this paper, we introduce OminiControl, a highly versatile andparameter-efficient framework that integrates image conditions into pre-trainedDiffusion Transformer (DiT) models. At its core, OminiControl leverages aparameter reuse mechanism, enabling the DiT to encode image conditions usingitself as a powerful backbone and process them with its flexible multi-modalattention processors. Unlike existing methods, which rely heavily on additionalencoder modules with complex architectures, OminiControl (1) effectively andefficiently incorporates injected image conditions with only ~0.1% additionalparameters, and (2) addresses a wide range of image conditioning tasks in aunified manner, including subject-driven generation and spatially-alignedconditions such as edges, depth, and more. Remarkably, these capabilities areachieved by training on images generated by the DiT itself, which isparticularly beneficial for subject-driven generation. Extensive evaluationsdemonstrate that OminiControl outperforms existing UNet-based and DiT-adaptedmodels in both subject-driven and spatially-aligned conditional generation.Additionally, we release our training dataset, Subjects200K, a diversecollection of over 200,000 identity-consistent images, along with an efficientdata synthesis pipeline to advance research in subject-consistent generation.</description><author>Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang</author><pubDate>Mon, 25 Nov 2024 17:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15098v2</guid></item><item><title>Selecting informative conformal prediction sets with false coverage rate control</title><link>http://arxiv.org/abs/2403.12295v3</link><description>In supervised learning, including regression and classification, conformalmethods provide prediction sets for the outcome/label with finite samplecoverage for any machine learning predictor. We consider here the case wheresuch prediction sets come after a selection process. The selection processrequires that the selected prediction sets be `informative' in a well definedsense. We consider both the classification and regression settings where theanalyst may consider as informative only the sample with prediction sets smallenough, excluding null values, or obeying other appropriate `monotone'constraints. We develop a unified framework for building such informativeconformal prediction sets while controlling the false coverage rate (FCR) onthe selected sample. While conformal prediction sets after selection have beenthe focus of much recent literature in the field, the new introducedprocedures, called InfoSP and InfoSCOP, are to our knowledge the first onesproviding FCR control for informative prediction sets. We show the usefulnessof our resulting procedures on real and simulated data.</description><author>Ulysse Gazin, Ruth Heller, Ariane Marandon, Etienne Roquain</author><pubDate>Mon, 25 Nov 2024 17:46:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12295v3</guid></item><item><title>F -- A Model of Events based on the Foundational Ontology DOLCE+DnS Ultralite</title><link>http://arxiv.org/abs/2411.16609v1</link><description>The lack of a formal model of events hinders interoperability in distributedevent-based systems. In this paper, we present a formal model of events, calledEvent-Model-F. The model is based on the foundational ontology DOLCE+DnSUltralite (DUL) and provides comprehensive support to represent time and space,objects and persons, as well as mereological, causal, and correlativerelationships between events. In addition, the Event-Model-F provides aflexible means for event composition, modeling event causality and eventcorrelation, and representing different interpretations of the same event. TheEvent-Model-F is developed following the pattern-oriented approach of DUL, ismodularized in different ontologies, and can be easily extended by domainspecific ontologies.</description><author>Ansgar Scherp, Thomas Franz, Carsten Saathoff, Steffen Staab</author><pubDate>Mon, 25 Nov 2024 17:45:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16609v1</guid></item><item><title>Uncertainty in Supply Chain Digital Twins: A Quantum-Classical Hybrid Approach</title><link>http://arxiv.org/abs/2411.10254v2</link><description>This study investigates uncertainty quantification (UQ) usingquantum-classical hybrid machine learning (ML) models for applications incomplex and dynamic fields, such as attaining resiliency in supply chaindigital twins and financial risk assessment. Although quantum featuretransformations have been integrated into ML models for complex data tasks, agap exists in determining their impact on UQ within their hybrid architectures(quantum-classical approach). This work applies existing UQ techniques fordifferent models within a hybrid framework, examining how quantum featuretransformation affects uncertainty propagation. Increasing qubits from 4 to 16shows varied model responsiveness to outlier detection (OD) samples, which is acritical factor for resilient decision-making in dynamic environments. Thiswork shows how quantum computing techniques can transform data features for UQ,particularly when combined with traditional methods.</description><author>Abdullah Abdullah, Fannya Ratana Sandjaja, Ayesha Abdul Majeed, Gyan Wickremasinghe, Karen Rafferty, Vishal Sharma</author><pubDate>Mon, 25 Nov 2024 17:36:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.10254v2</guid></item><item><title>Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction</title><link>http://arxiv.org/abs/2404.08509v2</link><description>Large language models (LLMs) have been driving a new wave of interactive AIapplications across numerous domains. However, efficiently serving LLMinference requests is challenging due to their unpredictable execution timesoriginating from the autoregressive nature of generative models. Existing LLMserving systems exploit first-come-first-serve (FCFS) scheduling, sufferingfrom head-of-line blocking issues. To address the non-deterministic nature ofLLMs and enable efficient interactive LLM serving, we present a speculativeshortest-job-first (SSJF) scheduler that uses a light proxy model to predictLLM output sequence lengths. Our open-source SSJF implementation does notrequire changes to memory management or batching strategies. Evaluations onreal-world datasets and production workload traces show that SSJF reducesaverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6xcompared to FCFS schedulers, across no batching, dynamic batching, andcontinuous batching settings.</description><author>Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbigniew T. Kalbarczyk, Tamer Ba≈üar, Ravishankar K. Iyer</author><pubDate>Mon, 25 Nov 2024 17:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08509v2</guid></item><item><title>Chat2SVG: Vector Graphics Generation with Large Language Models and Image Diffusion Models</title><link>http://arxiv.org/abs/2411.16602v1</link><description>Scalable Vector Graphics (SVG) has become the de facto standard for vectorgraphics in digital design, offering resolution independence and precisecontrol over individual elements. Despite their advantages, creatinghigh-quality SVG content remains challenging, as it demands technical expertisewith professional editing software and a considerable time investment to craftcomplex shapes. Recent text-to-SVG generation methods aim to make vectorgraphics creation more accessible, but they still encounter limitations inshape regularity, generalization ability, and expressiveness. To address thesechallenges, we introduce Chat2SVG, a hybrid framework that combines thestrengths of Large Language Models (LLMs) and image diffusion models fortext-to-SVG generation. Our approach first uses an LLM to generate semanticallymeaningful SVG templates from basic geometric primitives. Guided by imagediffusion models, a dual-stage optimization pipeline refines paths in latentspace and adjusts point coordinates to enhance geometric complexity. Extensiveexperiments show that Chat2SVG outperforms existing methods in visual fidelity,path regularity, and semantic alignment. Additionally, our system enablesintuitive editing through natural language instructions, making professionalvector graphics creation accessible to all users.</description><author>Ronghuan Wu, Wanchao Su, Jing Liao</author><pubDate>Mon, 25 Nov 2024 17:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16602v1</guid></item><item><title>Approximation Algorithms for Combinatorial Optimization with Predictions</title><link>http://arxiv.org/abs/2411.16600v1</link><description>We initiate a systematic study of utilizing predictions to improve overapproximation guarantees of classic algorithms, without increasing the runningtime. We propose a systematic method for a wide class of optimization problemsthat ask to select a feasible subset of input items of minimal (or maximal)total weight. This gives simple (near-)linear time algorithms for, e.g., VertexCover, Steiner Tree, Min-Weight Perfect Matching, Knapsack, and Clique. Ouralgorithms produce optimal solutions when provided with perfect predictions andtheir approximation ratios smoothly degrade with increasing prediction error.With small enough prediction error we achieve approximation guarantees that arebeyond reach without predictions in the given time bounds, as exemplified bythe NP-hardness and APX-hardness of many of the above problems. Although weshow our approach to be optimal for this class of problems as a whole, there isa potential for exploiting specific structural properties of individualproblems to obtain improved bounds; we demonstrate this on the Steiner Treeproblem. We conclude with an empirical evaluation of our approach.</description><author>Antonios Antoniadis, Marek Eli√°≈°, Adam Polak, Moritz Venzin</author><pubDate>Mon, 25 Nov 2024 17:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16600v1</guid></item><item><title>Unlocking The Potential of Adaptive Attacks on Diffusion-Based Purification</title><link>http://arxiv.org/abs/2411.16598v1</link><description>Diffusion-based purification (DBP) is a defense against adversarial examples(AEs), amassing popularity for its ability to protect classifiers in anattack-oblivious manner and resistance to strong adversaries with access to thedefense. Its robustness has been claimed to ensue from the reliance ondiffusion models (DMs) that project the AEs onto the natural distribution. Werevisit this claim, focusing on gradient-based strategies that back-propagatethe loss gradients through the defense, commonly referred to as ``adaptiveattacks". Analytically, we show that such an optimization method invalidatesDBP's core foundations, effectively targeting the DM rather than the classifierand restricting the purified outputs to a distribution over malicious samplesinstead. Thus, we reassess the reported empirical robustness, uncoveringimplementation flaws in the gradient back-propagation techniques used thus farfor DBP. We fix these issues, providing the first reliable gradient library forDBP and demonstrating how adaptive attacks drastically degrade its robustness.We then study a less efficient yet stricter majority-vote setting where theclassifier evaluates multiple purified copies of the input to make itsdecision. Here, DBP's stochasticity enables it to remain partially robustagainst traditional norm-bounded AEs. We propose a novel adaptation of a recentoptimization method against deepfake watermarking that crafts systemicmalicious perturbations while ensuring imperceptibility. When integrated withthe adaptive attack, it completely defeats DBP, even in the majority-votesetup. Our findings prove that DBP, in its current state, is not a viabledefense against AEs.</description><author>Andre Kassis, Urs Hengartner, Yaoliang Yu</author><pubDate>Mon, 25 Nov 2024 17:30:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16598v1</guid></item><item><title>From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge</title><link>http://arxiv.org/abs/2411.16594v1</link><description>Assessment and evaluation have long been critical challenges in artificialintelligence (AI) and natural language processing (NLP). However, traditionalmethods, whether matching-based or embedding-based, often fall short of judgingsubtle attributes and delivering satisfactory results. Recent advancements inLarge Language Models (LLMs) inspire the "LLM-as-a-judge" paradigm, where LLMsare leveraged to perform scoring, ranking, or selection across various tasksand applications. This paper provides a comprehensive survey of LLM-basedjudgment and assessment, offering an in-depth overview to advance this emergingfield. We begin by giving detailed definitions from both input and outputperspectives. Then we introduce a comprehensive taxonomy to exploreLLM-as-a-judge from three dimensions: what to judge, how to judge and where tojudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge andhighlight key challenges and promising directions, aiming to provide valuableinsights and inspire future research in this promising research area. Paperlist and more resources about LLM-as-a-judge can be found at\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\url{https://llm-as-a-judge.github.io}.</description><author>Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, Huan Liu</author><pubDate>Mon, 25 Nov 2024 17:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16594v1</guid></item><item><title>Adversarial Attacks for Drift Detection</title><link>http://arxiv.org/abs/2411.16591v1</link><description>Concept drift refers to the change of data distributions over time. Whiledrift poses a challenge for learning models, requiring their continualadaption, it is also relevant in system monitoring to detect malfunctions,system failures, and unexpected behavior. In the latter case, the robust andreliable detection of drifts is imperative. This work studies the shortcomingsof commonly used drift detection schemes. We show how to construct data streamsthat are drifting without being detected. We refer to those as driftadversarials. In particular, we compute all possible adversairals for commondetection schemes and underpin our theoretical findings with empiricalevaluations.</description><author>Fabian Hinder, Valerie Vaquet, Barbara Hammer</author><pubDate>Mon, 25 Nov 2024 17:25:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16591v1</guid></item><item><title>Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process</title><link>http://arxiv.org/abs/2403.10842v4</link><description>Fault detection and diagnosis (FDD) is a crucial task for ensuring the safetyand efficiency of industrial processes. We propose a novel FDD methodology forthe Tennessee Eastman Process (TEP), a widely used benchmark for chemicalprocess control. The model employs two separate Transformer branches, enablingindependent processing of input data and potential extraction of diverseinformation. A novel attention mechanism, Gated Dynamic Learnable Attention(GDLAttention), is introduced which integrates a gating mechanism and dynamiclearning capabilities. The gating mechanism modulates the attention weights,allowing the model to focus on the most relevant parts of the input. Thedynamic learning approach adapts the attention strategy during training,potentially leading to improved performance. The attention mechanism uses abilinear similarity function, providing greater flexibility in capturingcomplex relationships between query and key vectors. In order to assess theeffectiveness of our approach, we tested it against 21 and 18 distinct faultscenarios in TEP, and compared its performance with several established FDDtechniques. The outcomes indicate that the method outperforms others in termsof accuracy, false alarm rate, and misclassification rate. This underscores therobustness and efficacy of the approach for FDD in intricate industrialprocesses.</description><author>Mohammad Ali Labbaf-Khaniki, Mohammad Manthouri, Hanieh Ajami</author><pubDate>Mon, 25 Nov 2024 17:22:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10842v4</guid></item><item><title>Alpha Entropy Search for New Information-based Bayesian Optimization</title><link>http://arxiv.org/abs/2411.16586v1</link><description>Bayesian optimization (BO) methods based on information theory have obtainedstate-of-the-art results in several tasks. These techniques heavily rely on theKullback-Leibler (KL) divergence to compute the acquisition function. In thiswork, we introduce a novel information-based class of acquisition functions forBO called Alpha Entropy Search (AES). AES is based on the {\alpha}-divergence,that generalizes the KL divergence. Iteratively, AES selects the nextevaluation point as the one whose associated target value has the highest levelof the dependency with respect to the location and associated value of theglobal maximum of the optimization problem. Dependency is measured in terms ofthe {\alpha}-divergence, as an alternative to the KL divergence. Intuitively,this favors the evaluation of the objective function at the most informativepoints about the global maximum. The {\alpha}-divergence has a free parameter{\alpha}, which determines the behavior of the divergence, trading-offevaluating differences between distributions at a single mode, and evaluatingdifferences globally. Therefore, different values of {\alpha} result indifferent acquisition functions. AES acquisition lacks a closed-formexpression. However, we propose an efficient and accurate approximation using atruncated Gaussian distribution. In practice, the value of {\alpha} can bechosen by the practitioner, but here we suggest to use a combination ofacquisition functions obtained by simultaneously considering a range of valuesof {\alpha}. We provide an implementation of AES in BOTorch and we evaluate itsperformance in both synthetic, benchmark and real-world experiments involvingthe tuning of the hyper-parameters of a deep neural network. These experimentsshow that the performance of AES is competitive with respect to otherinformation-based acquisition functions such as JES, MES or PES.</description><author>Daniel Fern√°ndez-S√°nchez, Eduardo C. Garrido-Merch√°n, Daniel Hern√°ndez-Lobato</author><pubDate>Mon, 25 Nov 2024 17:19:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16586v1</guid></item><item><title>CoHD: A Counting-Aware Hierarchical Decoding Framework for Generalized Referring Expression Segmentation</title><link>http://arxiv.org/abs/2405.15658v2</link><description>The newly proposed Generalized Referring Expression Segmentation (GRES)amplifies the formulation of classic RES by involving complexmultiple/non-target scenarios. Recent approaches address GRES by directlyextending the well-adopted RES frameworks with object-existence identification.However, these approaches tend to encode multi-granularity object informationinto a single representation, which makes it difficult to precisely representcomprehensive objects of different granularity. Moreover, the simple binaryobject-existence identification across all referent scenarios fails to specifytheir inherent differences, incurring ambiguity in object understanding. Totackle the above issues, we propose a \textbf{Co}unting-Aware\textbf{H}ierarchical \textbf{D}ecoding framework (CoHD) for GRES. Bydecoupling the intricate referring semantics into different granularity with avisual-linguistic hierarchy, and dynamic aggregating it with intra- andinter-selection, CoHD boosts multi-granularity comprehension with thereciprocal benefit of the hierarchical nature. Furthermore, we incorporate thecounting ability by embodying multiple/single/non-target scenarios into count-and category-level supervision, facilitating comprehensive object perception.Experimental results on gRefCOCO, Ref-ZOM, R-RefCOCO, and RefCOCO benchmarksdemonstrate the effectiveness and rationality of CoHD which outperformsstate-of-the-art GRES methods by a remarkable margin. Code is available at\href{https://github.com/RobertLuo1/CoHD}{here}.</description><author>Zhuoyan Luo, Yinghao Wu, Tianheng Cheng, Yong Liu, Yicheng Xiao, Hongfa Wang, Xiao-Ping Zhang, Yujiu Yang</author><pubDate>Mon, 25 Nov 2024 17:14:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15658v2</guid></item><item><title>Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision</title><link>http://arxiv.org/abs/2411.16579v1</link><description>Training large language models (LLMs) to spend more time thinking andreflection before responding is crucial for effectively solving complexreasoning tasks in fields such as science, coding, and mathematics. However,the effectiveness of mechanisms like self-reflection and self-correctiondepends on the model's capacity to accurately assess its own performance, whichcan be limited by factors such as initial accuracy, question difficulty, andthe lack of external feedback. In this paper, we delve into a two-playerparadigm that separates the roles of reasoning and critique models, where thecritique model provides step-level feedback to supervise the reasoning (actor)model during both test-time and train-time. We first propose AutoMathCritique,an automated and scalable framework for collecting critique data, resulting ina dataset of $76,321$ responses paired with step-level feedback. Fine-tuninglanguage models with this dataset enables them to generate natural languagefeedback for mathematical reasoning. We demonstrate that the critique modelsconsistently improve the actor's performance on difficult queries at test-time,especially when scaling up inference-time computation. Motivated by thesefindings, we introduce the critique-based supervision to the actor'sself-training process, and propose a critique-in-the-loop self-improvementmethod. Experiments show that the method improves the actor's explorationefficiency and solution diversity, especially on challenging queries, leadingto a stronger reasoning model. Lastly, we take the preliminary step to exploretraining self-talk reasoning models via critique supervision and showcase itspotential. Our code and datasets are at\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}.</description><author>Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, Xiao Wang, Rui Zheng, Tao Ji, Xiaowei Shi, Yitao Zhai, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Zuxuan Wu, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Yu-Gang Jiang</author><pubDate>Mon, 25 Nov 2024 17:11:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16579v1</guid></item><item><title>CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features</title><link>http://arxiv.org/abs/2410.07610v2</link><description>Multimodal encoders like CLIP excel in tasks such as zero-shot imageclassification and cross-modal retrieval. However, they require excessivetraining data. We propose canonical similarity analysis (CSA), which uses twounimodal encoders to replicate multimodal encoders using limited data. CSA mapsunimodal features into a multimodal space, using a new similarity score toretain only the multimodal information. CSA only involves the inference ofunimodal encoders and a cubic-complexity matrix decomposition, eliminating theneed for extensive GPU-based model training. Experiments show that CSAoutperforms CLIP while requiring $300,000\times$ fewer multimodal data pairsand $6\times$ fewer unimodal data for ImageNet classification andmisinformative news captions detection. CSA surpasses the state-of-the-artmethod to map unimodal features to multimodal features. We also demonstrate theability of CSA with modalities beyond image and text, paving the way for futuremodality pairs with limited paired multimodal data but abundant unpairedunimodal data, such as lidar and text.</description><author>Po-han Li, Sandeep P. Chinchali, Ufuk Topcu</author><pubDate>Mon, 25 Nov 2024 17:01:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07610v2</guid></item><item><title>Rethinking Diffusion for Text-Driven Human Motion Generation</title><link>http://arxiv.org/abs/2411.16575v1</link><description>Since 2023, Vector Quantization (VQ)-based discrete generation methods haverapidly dominated human motion generation, primarily surpassing diffusion-basedcontinuous generation methods in standard performance metrics. However,VQ-based methods have inherent limitations. Representing continuous motion dataas limited discrete tokens leads to inevitable information loss, reduces thediversity of generated motions, and restricts their ability to functioneffectively as motion priors or generation guidance. In contrast, thecontinuous space generation nature of diffusion-based methods makes themwell-suited to address these limitations and with even potential for modelscalability. In this work, we systematically investigate why current VQ-basedmethods perform well and explore the limitations of existing diffusion-basedmethods from the perspective of motion data representation and distribution.Drawing on these insights, we preserve the inherent strengths of adiffusion-based human motion generation model and gradually optimize it withinspiration from VQ-based approaches. Our approach introduces a human motiondiffusion model enabled to perform bidirectional masked autoregression,optimized with a reformed data representation and distribution. Additionally,we also propose more robust evaluation methods to fairly assess different-basedmethods. Extensive experiments on benchmark human motion generation datasetsdemonstrate that our method excels previous methods and achievesstate-of-the-art performances.</description><author>Zichong Meng, Yiming Xie, Xiaogang Peng, Zeyu Han, Huaizu Jiang</author><pubDate>Mon, 25 Nov 2024 16:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16575v1</guid></item><item><title>Naive Algorithmic Collusion: When Do Bandit Learners Cooperate and When Do They Compete?</title><link>http://arxiv.org/abs/2411.16574v1</link><description>Algorithmic agents are used in a variety of competitive decision settings,notably in making pricing decisions in contexts that range from online retailto residential home rentals. Business managers, algorithm designers, legalscholars, and regulators alike are all starting to consider the ramificationsof "algorithmic collusion." We study the emergent behavior of multi-armedbandit machine learning algorithms used in situations where agents arecompeting, but they have no information about the strategic interaction theyare engaged in. Using a general-form repeated Prisoner's Dilemma game, agentsengage in online learning with no prior model of game structure and noknowledge of competitors' states or actions (e.g., no observation of competingprices). We show that these context-free bandits, with no knowledge ofopponents' choices or outcomes, still will consistently learn collusivebehavior - what we call "naive collusion." We primarily study this systemthrough an analytical model and examine perturbations to the model throughsimulations. Our findings have several notable implications for regulators. First, callsto limit algorithms from conditioning on competitors' prices are insufficientto prevent algorithmic collusion. This is a direct result of collusion arisingeven in the naive setting. Second, symmetry in algorithms can increasecollusion potential. This highlights a new, simple mechanism for"hub-and-spoke" algorithmic collusion. A central distributor need not imbue itsalgorithm with supra-competitive tendencies for apparent collusion to arise; itcan simply arise by using certain (common) machine learning algorithms.Finally, we highlight that collusive outcomes depend starkly on the specificalgorithm being used, and we highlight market and algorithmic conditions underwhich it will be unknown a priori whether collusion occurs.</description><author>Connor Douglas, Foster Provost, Arun Sundararajan</author><pubDate>Mon, 25 Nov 2024 16:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16574v1</guid></item><item><title>A Review of Mechanistic Models of Event Comprehension</title><link>http://arxiv.org/abs/2409.18992v2</link><description>This review examines theoretical assumptions and computational models ofevent comprehension, tracing the evolution from discourse comprehensiontheories to contemporary event cognition frameworks. The review covers keydiscourse comprehension accounts, including Construction-Integration, EventIndexing, Causal Network, and Resonance models, highlighting theircontributions to understanding cognitive processes in comprehension. I thendiscuss contemporary theoretical frameworks of event comprehension, includingEvent Segmentation Theory (Zacks et al., 2007), the Event Horizon Model(Radvansky &amp; Zacks, 2014), and Hierarchical Generative Framework (Kuperberg,2021), which emphasize prediction, causality, and multilevel representations inevent understanding. Building on these theories, I evaluate five computationalmodels of event comprehension: REPRISE (Butz et al., 2019), Structured EventMemory (SEM; Franklin et al., 2020), the Lu model (Lu et al., 2022), theGumbsch model (Gumbsch et al., 2022), and the Elman and McRae model (2019). Theanalysis focuses on their approaches to hierarchical processing, predictionmechanisms, and representation learning. Key themes that emerge include the useof hierarchical structures as inductive biases, the importance of prediction incomprehension, and diverse strategies for learning event dynamics. The reviewidentifies critical areas for future research, including the need for moresophisticated approaches to learning structured representations, integratingepisodic memory mechanisms, and developing adaptive updating algorithms forworking event models. By synthesizing insights from both theoretical frameworksand computational implementations, this review aims to advance ourunderstanding of human event comprehension and guide future modeling efforts incognitive science.</description><author>Tan T. Nguyen</author><pubDate>Mon, 25 Nov 2024 16:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18992v2</guid></item><item><title>A Survey of Event Causality Identification: Principles, Taxonomy, Challenges, and Assessment</title><link>http://arxiv.org/abs/2411.10371v2</link><description>Event Causality Identification (ECI) has become a crucial task in NaturalLanguage Processing (NLP), aimed at automatically extracting causalities fromtextual data. In this survey, we systematically address the foundationalprinciples, technical frameworks, and challenges of ECI, offering acomprehensive taxonomy to categorize and clarify current researchmethodologies, as well as a quantitative assessment of existing models. Wefirst establish a conceptual framework for ECI, outlining key definitions,problem formulations, and evaluation standards. Our taxonomy classifies ECImethods according to the two primary tasks of sentence-level (SECI) anddocument-level (DECI) event causality identification. For SECI, we examinefeature pattern-based matching, deep semantic encoding, causal knowledgepre-training and prompt-based fine-tuning, and external knowledge enhancementmethods. For DECI, we highlight approaches focused on event graph reasoning andprompt-based techniques to address the complexity of cross-sentence causalinference. Additionally, we analyze the strengths, limitations, and openchallenges of each approach. We further conduct an extensive quantitativeevaluation of various ECI methods on two benchmark datasets. Finally, weexplore future research directions, highlighting promising pathways to overcomecurrent limitations and broaden ECI applications.</description><author>Qing Cheng, Zefan Zeng, Xingchen Hu, Yuehang Si, Zhong Liu</author><pubDate>Mon, 25 Nov 2024 16:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.10371v2</guid></item><item><title>BenchMARL: Benchmarking Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2312.01472v3</link><description>The field of Multi-Agent Reinforcement Learning (MARL) is currently facing areproducibility crisis. While solutions for standardized reporting have beenproposed to address the issue, we still lack a benchmarking tool that enablesstandardization and reproducibility, while leveraging cutting-edgeReinforcement Learning (RL) implementations. In this paper, we introduceBenchMARL, the first MARL training library created to enable standardizedbenchmarking across different algorithms, models, and environments. BenchMARLuses TorchRL as its backend, granting it high performance and maintainedstate-of-the-art implementations while addressing the broad community of MARLPyTorch users. Its design enables systematic configuration and reporting, thusallowing users to create and run complex benchmarks from simple one-lineinputs. BenchMARL is open-sourced on GitHub:https://github.com/facebookresearch/BenchMARL</description><author>Matteo Bettini, Amanda Prorok, Vincent Moens</author><pubDate>Mon, 25 Nov 2024 16:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01472v3</guid></item><item><title>J-CaPA : Joint Channel and Pyramid Attention Improves Medical Image Segmentation</title><link>http://arxiv.org/abs/2411.16568v1</link><description>Medical image segmentation is crucial for diagnosis and treatment planning.Traditional CNN-based models, like U-Net, have shown promising results butstruggle to capture long-range dependencies and global context. To addressthese limitations, we propose a transformer-based architecture that jointlyapplies Channel Attention and Pyramid Attention mechanisms to improvemulti-scale feature extraction and enhance segmentation performance for medicalimages. Increasing model complexity requires more training data, and we furtherimprove model generalization with CutMix data augmentation. Our approach isevaluated on the Synapse multi-organ segmentation dataset, achieving a 6.9%improvement in Mean Dice score and a 39.9% improvement in Hausdorff Distance(HD95) over an implementation without our enhancements. Our proposed modeldemonstrates improved segmentation accuracy for complex anatomical structures,outperforming existing state-of-the-art methods.</description><author>Marzia Binta Nizam, Marian Zlateva, James Davis</author><pubDate>Mon, 25 Nov 2024 16:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16568v1</guid></item><item><title>Enhancing Few-Shot Learning with Integrated Data and GAN Model Approaches</title><link>http://arxiv.org/abs/2411.16567v1</link><description>This paper presents an innovative approach to enhancing few-shot learning byintegrating data augmentation with model fine-tuning in a framework designed totackle the challenges posed by small-sample data. Recognizing the criticallimitations of traditional machine learning models that require largedatasets-especially in fields such as drug discovery, target recognition, andmalicious traffic detection-this study proposes a novel strategy that leveragesGenerative Adversarial Networks (GANs) and advanced optimization techniques toimprove model performance with limited data. Specifically, the paper addressesthe noise and bias issues introduced by data augmentation methods, contrastingthem with model-based approaches, such as fine-tuning and metric learning,which rely heavily on related datasets. By combining Markov Chain Monte Carlo(MCMC) sampling and discriminative model ensemble strategies within a GANframework, the proposed model adjusts generative and discriminativedistributions to simulate a broader range of relevant data. Furthermore, itemploys MHLoss and a reparameterized GAN ensemble to enhance stability andaccelerate convergence, ultimately leading to improved classificationperformance on small-sample images and structured datasets. Results confirmthat the MhERGAN algorithm developed in this research is highly effective forfew-shot learning, offering a practical solution that bridges data scarcitywith high-performing model adaptability and generalization.</description><author>Yinqiu Feng, Aoran Shen, Jiacheng Hu, Yingbin Liang, Shiru Wang, Junliang Du</author><pubDate>Mon, 25 Nov 2024 16:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16567v1</guid></item><item><title>EnStack: An Ensemble Stacking Framework of Large Language Models for Enhanced Vulnerability Detection in Source Code</title><link>http://arxiv.org/abs/2411.16561v1</link><description>Automated detection of software vulnerabilities is critical for enhancingsecurity, yet existing methods often struggle with the complexity and diversityof modern codebases. In this paper, we introduce EnStack, a novel ensemblestacking framework that enhances vulnerability detection using natural languageprocessing (NLP) techniques. Our approach synergizes multiple pre-trained largelanguage models (LLMs) specialized in code understanding CodeBERT for semanticanalysis, GraphCodeBERT for structural representation, and UniXcoder forcross-modal capabilities. By fine-tuning these models on the Draper VDISCdataset and integrating their outputs through meta-classifiers such as LogisticRegression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStackeffectively captures intricate code patterns and vulnerabilities thatindividual models may overlook. The meta-classifiers consolidate the strengthsof each LLM, resulting in a comprehensive model that excels in detecting subtleand complex vulnerabilities across diverse programming contexts. Experimentalresults demonstrate that EnStack significantly outperforms existing methods,achieving notable improvements in accuracy, precision, recall, and F1-score.This work highlights the potential of ensemble LLM approaches in code analysistasks and offers valuable insights into applying NLP techniques for advancingautomated vulnerability detection.</description><author>Shahriyar Zaman Ridoy, Md. Shazzad Hossain Shaon, Alfredo Cuzzocrea, Mst Shapna Akter</author><pubDate>Mon, 25 Nov 2024 16:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16561v1</guid></item><item><title>Quantum Circuit Training with Growth-Based Architectures</title><link>http://arxiv.org/abs/2411.16560v1</link><description>This study introduces growth-based training strategies that incrementallyincrease parameterized quantum circuit (PQC) depth during training, mitigatingoverfitting and managing model complexity dynamically. We develop threedistinct methods: Block Growth, Sequential Feature Map Growth, and InterleaveFeature Map Growth, which add reuploader blocks to PQCs adaptively, expandingthe accessible frequency spectrum of the model in response to training needs.This approach enables PQCs to achieve more stable convergence andgeneralization, even in noisy settings. We evaluate our methods on regressiontasks and the 2D Laplace equation, demonstrating that dynamic growth methodsoutperform traditional, fixed-depth approaches, achieving lower final lossesand reduced variance between runs. These findings underscore the potential ofgrowth-based PQCs for quantum scientific machine learning (QSciML)applications, where balancing expressivity and stability is essential.</description><author>Callum Duffy, Smit Chaudhary, Gergana V. Velikova</author><pubDate>Mon, 25 Nov 2024 16:46:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16560v1</guid></item><item><title>Accelerating Task Generalisation with Multi-Level Hierarchical Options</title><link>http://arxiv.org/abs/2411.02998v2</link><description>Creating reinforcement learning agents that generalise effectively to newtasks is a key challenge in AI research. This paper introduces Fracture ClusterOptions (FraCOs), a multi-level hierarchical reinforcement learning method thatachieves state-of-the-art performance on difficult generalisation tasks. FraCOsidentifies patterns in agent behaviour and forms options based on the expectedfuture usefulness of those patterns, enabling rapid adaptation to new tasks. Intabular settings, FraCOs demonstrates effective transfer and improvesperformance as it grows in hierarchical depth. We evaluate FraCOs againststate-of-the-art deep reinforcement learning algorithms in several complexprocedurally generated environments. Our results show that FraCOs achieveshigher in-distribution and out-of-distribution performance than competitors.</description><author>Thomas P Cannon, √ñzg√ºr Simsek</author><pubDate>Mon, 25 Nov 2024 16:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02998v2</guid></item><item><title>Scalable and Efficient Temporal Graph Representation Learning via Forward Recent Sampling</title><link>http://arxiv.org/abs/2402.01964v2</link><description>Temporal graph representation learning (TGRL) is essential for modelingdynamic systems in real-world networks. However, traditional TGRL methods,despite their effectiveness, often face significant computational challengesand inference delays due to the inefficient sampling of temporal neighbors.Conventional sampling methods typically involve backtracking through theinteraction history of each node. In this paper, we propose a novel TGRLframework, No-Looking-Back (NLB), which overcomes these challenges byintroducing a forward recent sampling strategy. This strategy eliminates theneed to backtrack through historical interactions by utilizing aGPU-executable, size-constrained hash table for each node. The hash tablerecords a down-sampled set of recent interactions, enabling rapid queryresponses with minimal inference latency. The maintenance of this hash table ishighly efficient, operating with $O(1)$ complexity. Fully compatible with GPUprocessing, NLB maximizes programmability, parallelism, and power efficiency.Empirical evaluations demonstrate that NLB not only matches or surpassesstate-of-the-art methods in accuracy for tasks like link prediction and nodeclassification across six real-world datasets but also achieves 1.32-4.40xfaster training, 1.2-7.94x greater energy efficiency, and 1.63-12.95x lowerinference latency compared to competitive baselines. The link to the code:https://github.com/Graph-COM/NLB.</description><author>Yuhong Luo, Pan Li</author><pubDate>Mon, 25 Nov 2024 16:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01964v2</guid></item><item><title>Anomaly Detection and RFI Classification with Unsupervised Learning in Narrowband Radio Technosignature Searches</title><link>http://arxiv.org/abs/2411.16556v1</link><description>The search for radio technosignatures is an anomaly detection problem:candidate signals represent needles of interest in the proverbial haystack ofradio-frequency interference (RFI). Current search frameworks find an enormityof false-positive signals, especially in large surveys, requiring manualfollow-up to a sometimes prohibitive degree. Unsupervised learning provides analgorithmic way to winnow the most anomalous signals from the chaff, as well asgroup together RFI signals that bear morphological similarities. We presentGLOBULAR (Grouping Low-frequency Observations By Unsupervised Learning AfterReduction) clustering, a signal processing method that uses HDBSCAN to reducethe false-positive rate and isolate outlier signals for further analysis. Whencombined with a standard narrowband signal detection and spatial filteringpipeline, such as turboSETI, GLOBULAR clustering offers significantimprovements in the false-positive rate over the standard pipeline alone,suggesting dramatic potential for the amelioration of manual follow-uprequirements for future large surveys. By removing RFI signals in regions ofhigh spectral occupancy, GLOBULAR clustering may also enable the detection ofsignals missed by the standard pipeline. We benchmark our method against theChoza et al. (2024) turboSETI-only search of 97 nearby galaxies at L-band,demonstrating a false-positive hit reduction rate of 93.1% and a false-positiveevent reduction rate of 99.3%.</description><author>Ben Jacobson-Bell, Steve Croft, Carmen Choza, Alex Andersson, Daniel Bautista, Vishal Gajjar, Matthew Lebofsky, David H. E. MacMahon, Caleb Painter, Andrew P. V. Siemion</author><pubDate>Mon, 25 Nov 2024 16:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16556v1</guid></item><item><title>Regression-based Physics Informed Neural Networks (Reg-PINNs) for Magnetopause Tracking</title><link>http://arxiv.org/abs/2306.09621v5</link><description>Previous research in the scientific field has utilized statistical empiricalmodels and machine learning to address fitting challenges. While empiricalmodels have the advantage of numerical generalization, they often sacrificeaccuracy. However, conventional machine learning methods can achieve highprecision but may lack the desired generalization. The article introduces aRegression-based Physics-Informed Neural Networks (Reg-PINNs), which embedsphysics-inspired empirical models into the neural network's loss function,thereby combining the benefits of generalization and high accuracy. The studyvalidates the proposed method using the magnetopause boundary location as thetarget and explores the feasibility of methods including Shue et al. [1998], adata overfitting model, a fully-connected networks, Reg-PINNs with Shue'smodel, and Reg-PINNs with the overfitting model. Compared to Shue's model, thistechnique achieves approximately a 30% reduction in RMSE, presenting aproof-of-concept improved solution for the scientific community.</description><author>Po-Han Hou, Sung-Chi Hsieh</author><pubDate>Mon, 25 Nov 2024 16:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09621v5</guid></item><item><title>Generating Out-Of-Distribution Scenarios Using Language Models</title><link>http://arxiv.org/abs/2411.16554v1</link><description>The deployment of autonomous vehicles controlled by machine learningtechniques requires extensive testing in diverse real-world environments,robust handling of edge cases and out-of-distribution scenarios, andcomprehensive safety validation to ensure that these systems can navigatesafely and effectively under unpredictable conditions. AddressingOut-Of-Distribution (OOD) driving scenarios is essential for enhancing safety,as OOD scenarios help validate the reliability of the models within thevehicle's autonomy stack. However, generating OOD scenarios is challenging dueto their long-tailed distribution and rarity in urban driving dataset.Recently, Large Language Models (LLMs) have shown promise in autonomousdriving, particularly for their zero-shot generalization and common-sensereasoning capabilities. In this paper, we leverage these LLM strengths tointroduce a framework for generating diverse OOD driving scenarios. Ourapproach uses LLMs to construct a branching tree, where each branch representsa unique OOD scenario. These scenarios are then simulated in the CARLAsimulator using an automated framework that aligns scene augmentation with thecorresponding textual descriptions. We evaluate our framework through extensivesimulations, and assess its performance via a diversity metric that measuresthe richness of the scenarios. Additionally, we introduce a new "OOD-ness"metric, which quantifies how much the generated scenarios deviate from typicalurban driving conditions. Furthermore, we explore the capacity of modernVision-Language Models (VLMs) to interpret and safely navigate through thesimulated OOD scenarios. Our findings offer valuable insights into thereliability of language models in addressing OOD scenarios within the contextof urban driving.</description><author>Erfan Aasi, Phat Nguyen, Shiva Sreeram, Guy Rosman, Sertac Karaman, Daniela Rus</author><pubDate>Mon, 25 Nov 2024 16:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16554v1</guid></item><item><title>Representation Collapsing Problems in Vector Quantization</title><link>http://arxiv.org/abs/2411.16550v1</link><description>Vector quantization is a technique in machine learning that discretizescontinuous representations into a set of discrete vectors. It is widelyemployed in tokenizing data representations for large language models,diffusion models, and other generative models. Despite its prevalence, thecharacteristics and behaviors of vector quantization in generative modelsremain largely underexplored. In this study, we investigate representationcollapse in vector quantization - a critical degradation where codebook tokensor latent embeddings lose their discriminative power by converging to a limitedsubset of values. This collapse fundamentally compromises the model's abilityto capture diverse data patterns. By leveraging both synthetic and realdatasets, we identify the severity of each type of collapses and triggeringconditions. Our analysis reveals that restricted initialization and limitedencoder capacity result in tokens collapse and embeddings collapse. Building onthese findings, we propose potential solutions aimed at mitigating eachcollapse. To the best of our knowledge, this is the first comprehensive studyexamining representation collapsing problems in vector quantization.</description><author>Wenhao Zhao, Qiran Zou, Rushi Shah, Dianbo Liu</author><pubDate>Mon, 25 Nov 2024 16:32:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16550v1</guid></item><item><title>Transformers are Deep Optimizers: Provable In-Context Learning for Deep Model Training</title><link>http://arxiv.org/abs/2411.16549v1</link><description>We investigate the transformer's capability for in-context learning (ICL) tosimulate the training process of deep models. Our key contribution is providinga positive example of using a transformer to train a deep neural network bygradient descent in an implicit fashion via ICL. Specifically, we provide anexplicit construction of a $(2N+4)L$-layer transformer capable of simulating$L$ gradient descent steps of an $N$-layer ReLU network through ICL. We alsogive the theoretical guarantees for the approximation within any given errorand the convergence of the ICL gradient descent. Additionally, we extend ouranalysis to the more practical setting using Softmax-based transformers. Wevalidate our findings on synthetic datasets for 3-layer, 4-layer, and 6-layerneural networks. The results show that ICL performance matches that of directtraining.</description><author>Weimin Wu, Maojiang Su, Jerry Yao-Chieh Hu, Zhao Song, Han Liu</author><pubDate>Mon, 25 Nov 2024 16:32:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16549v1</guid></item><item><title>GSE: Group-wise Sparse and Explainable Adversarial Attacks</title><link>http://arxiv.org/abs/2311.17434v3</link><description>Sparse adversarial attacks fool deep neural networks (DNNs) through minimalpixel perturbations, often regularized by the $\ell_0$ norm. Recent effortshave replaced this norm with a structural sparsity regularizer, such as thenuclear group norm, to craft group-wise sparse adversarial attacks. Theresulting perturbations are thus explainable and hold significant practicalrelevance, shedding light on an even greater vulnerability of DNNs. However,crafting such attacks poses an optimization challenge, as it involves computingnorms for groups of pixels within a non-convex objective. We address this bypresenting a two-phase algorithm that generates group-wise sparse attackswithin semantically meaningful areas of an image. Initially, we optimize aquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailoredfor non-convex programming. Subsequently, the algorithm transitions to aprojected Nesterov's accelerated gradient descent with $2-$norm regularizationapplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 andImageNet datasets demonstrate a remarkable increase in group-wise sparsity,e.g., $50.9\%$ on CIFAR-10 and $38.4\%$ on ImageNet (average case, targetedattack). This performance improvement is accompanied by significantly fastercomputation times, improved explainability, and a $100\%$ attack success rate.</description><author>Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta</author><pubDate>Mon, 25 Nov 2024 16:23:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17434v3</guid></item><item><title>RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics</title><link>http://arxiv.org/abs/2411.16537v1</link><description>Spatial understanding is a crucial capability for robots to make groundeddecisions based on their environment. This foundational skill enables robotsnot only to perceive their surroundings but also to reason about and interactmeaningfully within the world. In modern robotics, these capabilities are takenon by visual language models, and they face significant challenges when appliedto spatial reasoning context due to their training data sources. These sourcesutilize general-purpose image datasets, and they often lack sophisticatedspatial scene understanding capabilities. For example, the datasets do notaddress reference frame comprehension - spatial relationships require clearcontextual understanding, whether from an ego-centric, object-centric, orworld-centric perspective, which allow for effective real-world interaction. Toaddress this issue, we introduce RoboSpatial, a large-scale spatialunderstanding dataset consisting of real indoor and tabletop scenes captured as3D scans and egocentric images, annotated with rich spatial informationrelevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3Mannotated spatial relationships, with paired 2D egocentric images and 3D scansto make it both 2D and 3D ready. Our experiments show that models trained withRoboSpatial outperform baselines on downstream tasks such as spatial affordanceprediction, spatial relationship prediction, and robotics manipulation.</description><author>Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield</author><pubDate>Mon, 25 Nov 2024 16:21:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16537v1</guid></item><item><title>CliMB: An AI-enabled Partner for Clinical Predictive Modeling</title><link>http://arxiv.org/abs/2410.03736v2</link><description>Despite its significant promise and continuous technical advances, real-worldapplications of artificial intelligence (AI) remain limited. We attribute thisto the "domain expert-AI-conundrum": while domain experts, such as clinicianscientists, should be able to build predictive models such as risk scores, theyface substantial barriers in accessing state-of-the-art (SOTA) tools. Whileautomated machine learning (AutoML) has been proposed as a partner in clinicalpredictive modeling, many additional requirements need to be fulfilled to makemachine learning accessible for clinician scientists. To address this gap, we introduce CliMB, a no-code AI-enabled partnerdesigned to empower clinician scientists to create predictive models usingnatural language. CliMB guides clinician scientists through the entire medicaldata science pipeline, thus empowering them to create predictive models fromreal-world data in just one conversation. CliMB also creates structured reportsand interpretable visuals. In evaluations involving clinician scientists andsystematic comparisons against a baseline GPT-4, CliMB consistentlydemonstrated superior performance in key areas such as planning, errorprevention, code execution, and model performance. Moreover, in blindedassessments involving 45 clinicians from diverse specialties and career stages,more than 80% preferred CliMB over GPT-4. Overall, by providing a no-codeinterface with clear guidance and access to SOTA methods in the fields ofdata-centric AI, AutoML, and interpretable ML, CliMB empowers clinicianscientists to build robust predictive models. The proof-of-concept version of CliMB is available as open-source software onGitHub: https://github.com/vanderschaarlab/climb.</description><author>Evgeny Saveliev, Tim Schubert, Thomas Pouplin, Vasilis Kosmoliaptsis, Mihaela van der Schaar</author><pubDate>Mon, 25 Nov 2024 16:21:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03736v2</guid></item><item><title>Continual Deep Reinforcement Learning with Task-Agnostic Policy Distillation</title><link>http://arxiv.org/abs/2411.16532v1</link><description>Central to the development of universal learning systems is the ability tosolve multiple tasks without retraining from scratch when new data arrives.This is crucial because each task requires significant training time.Addressing the problem of continual learning necessitates various methods dueto the complexity of the problem space. This problem space includes: (1)addressing catastrophic forgetting to retain previously learned tasks, (2)demonstrating positive forward transfer for faster learning, (3) ensuringscalability across numerous tasks, and (4) facilitating learning withoutrequiring task labels, even in the absence of clear task boundaries. In thispaper, the Task-Agnostic Policy Distillation (TAPD) framework is introduced.This framework alleviates problems (1)-(4) by incorporating a task-agnosticphase, where an agent explores its environment without any external goal andmaximizes only its intrinsic motivation. The knowledge gained during this phaseis later distilled for further exploration. Therefore, the agent acts in aself-supervised manner by systematically seeking novel states. By utilizingtask-agnostic distilled knowledge, the agent can solve downstream tasks moreefficiently, leading to improved sample efficiency. Our code is available atthe repository: https://github.com/wabbajack1/TAPD.</description><author>Muhammad Burhan Hafez, Kerim Erekmen</author><pubDate>Mon, 25 Nov 2024 16:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16532v1</guid></item><item><title>Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word Embeddings</title><link>http://arxiv.org/abs/2411.16527v1</link><description>Large language models (LLMs) are the foundation of the current successes ofartificial intelligence (AI), however, they are unavoidably biased. Toeffectively communicate the risks and encourage mitigation efforts these modelsneed adequate and intuitive descriptions of their discriminatory properties,appropriate for all audiences of AI. We suggest bias profiles with respect tostereotype dimensions based on dictionaries from social psychology research.Along these dimensions we investigate gender bias in contextual embeddings,across contexts and layers, and generate stereotype profiles for twelvedifferent LLMs, demonstrating their intuition and use case for exposing andvisualizing bias.</description><author>Carolin M. Schuster, Maria-Alexandra Dinisor, Shashwat Ghatiwala, Georg Groh</author><pubDate>Mon, 25 Nov 2024 16:14:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16527v1</guid></item><item><title>Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency</title><link>http://arxiv.org/abs/2411.16525v1</link><description>We investigate the statistical and computational limits of prompt tuning fortransformer-based foundation models. Our key contributions are prompt tuning on\textit{single-head} transformers with only a \textit{single} self-attentionlayer: (i) is universal, and (ii) supports efficient (even almost-linear time)algorithms under the Strong Exponential Time Hypothesis (SETH). Statistically,we prove that prompt tuning on such simplest possible transformers areuniversal approximators for sequence-to-sequence Lipschitz functions. Inaddition, we provide an exponential-in-$dL$ and -in-$(1/\epsilon)$ lower boundon the required soft-prompt tokens for prompt tuning to memorize any datasetwith 1-layer, 1-head transformers. Computationally, we identify a phasetransition in the efficiency of prompt tuning, determined by the norm of the\textit{soft-prompt-induced} keys and queries, and provide an upper boundcriterion. Beyond this criterion, no sub-quadratic (efficient) algorithm forprompt tuning exists under SETH. Within this criterion, we showcase our theoryby proving the existence of almost-linear time prompt tuning inferencealgorithms. These fundamental limits provide important necessary conditions fordesigning expressive and efficient prompt tuning methods for practitioners.</description><author>Jerry Yao-Chieh Hu, Wei-Po Wang, Ammar Gilani, Chenyang Li, Zhao Song, Han Liu</author><pubDate>Mon, 25 Nov 2024 16:12:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16525v1</guid></item><item><title>LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology Report Generation</title><link>http://arxiv.org/abs/2411.16523v1</link><description>In the current paradigm of image captioning, deep learning models are trainedto generate text from image embeddings of latent features. We challenge theassumption that these latent features ought to be high-dimensional vectorswhich require model fine tuning to handle. Here we propose Label BoostedRetrieval Augmented Generation (LaB-RAG), a text-based approach to imagecaptioning that leverages image descriptors in the form of categorical labelsto boost standard retrieval augmented generation (RAG) with pretrained largelanguage models (LLMs). We study our method in the context of radiology reportgeneration (RRG), where the task is to generate a clinician's report detailingtheir observations from a set of radiological images, such as X-rays. We arguethat simple linear classifiers over extracted image embeddings can effectivelytransform X-rays into text-space as radiology-specific labels. In combinationwith standard RAG, we show that these derived text labels can be used withgeneral-domain LLMs to generate radiology reports. Without ever training ourgenerative language model or image feature encoder models, and without everdirectly "showing" the LLM an X-ray, we demonstrate that LaB-RAG achievesbetter results across natural language and radiology language metrics comparedwith other retrieval-based RRG methods, while attaining competitive resultscompared to other fine-tuned vision-language RRG models. We further presentresults of our experiments with various components of LaB-RAG to betterunderstand our method. Finally, we critique the use of a popular RRG metric,arguing it is possible to artificially inflate its results without truedata-leakage.</description><author>Steven Song, Anirudh Subramanyam, Irene Madejski, Robert L. Grossman</author><pubDate>Mon, 25 Nov 2024 16:10:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16523v1</guid></item><item><title>Enhancing Diagnostic Precision in Gastric Bleeding through Automated Lesion Segmentation: A Deep DuS-KFCM Approach</title><link>http://arxiv.org/abs/2411.14385v2</link><description>Timely and precise classification and segmentation of gastric bleeding inendoscopic imagery are pivotal for the rapid diagnosis and intervention ofgastric complications, which is critical in life-saving medical procedures.Traditional methods grapple with the challenge posed by the indistinguishableintensity values of bleeding tissues adjacent to other gastric structures. Ourstudy seeks to revolutionize this domain by introducing a novel deep learningmodel, the Dual Spatial Kernelized Constrained Fuzzy C-Means (Deep DuS-KFCM)clustering algorithm. This Hybrid Neuro-Fuzzy system synergizes Neural Networkswith Fuzzy Logic to offer a highly precise and efficient identification ofbleeding regions. Implementing a two-fold coarse-to-fine strategy forsegmentation, this model initially employs the Spatial Kernelized Fuzzy C-Means(SKFCM) algorithm enhanced with spatial intensity profiles and subsequentlyharnesses the state-of-the-art DeepLabv3+ with ResNet50 architecture to refinethe segmentation output. Through extensive experiments across mainstreamgastric bleeding and red spots datasets, our Deep DuS-KFCM model demonstratedunprecedented accuracy rates of 87.95%, coupled with a specificity of 96.33%,outperforming contemporary segmentation methods. The findings underscore themodel's robustness against noise and its outstanding segmentation capabilities,particularly for identifying subtle bleeding symptoms, thereby presenting asignificant leap forward in medical image processing.</description><author>Xian-Xian Liu, Mingkun Xu, Yuanyuan Wei, Huafeng Qin, Qun Song, Simon Fong, Feng Tien, Wei Luo, Juntao Gao, Zhihua Zhang, Shirley Siu</author><pubDate>Mon, 25 Nov 2024 16:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14385v2</guid></item><item><title>Learning deep illumination-robust features from multispectral filter array images</title><link>http://arxiv.org/abs/2407.15472v3</link><description>Multispectral (MS) snapshot cameras equipped with a MS filter array (MSFA),capture multiple spectral bands in a single shot, resulting in a raw mosaicimage where each pixel holds only one channel value. The fully-defined MS imageis estimated from the raw one through \textit{demosaicing}, which inevitablyintroduces spatio-spectral artifacts. Moreover, training on fully-defined MSimages can be computationally intensive, particularly with deep neural networks(DNNs), and may result in features lacking discrimination power due tosuboptimal learning of spatio-spectral interactions. Furthermore, outdoor MSimage acquisition occurs under varying lighting conditions, leading toillumination-dependent features. This paper presents an original approach tolearn discriminant and illumination-robust features directly from raw images.It involves: \textit{raw spectral constancy} to mitigate the impact ofillumination, \textit{MSFA-preserving} transformations suited for raw imageaugmentation to train DNNs on diverse raw textures, and \textit{raw-mixing} tocapture discriminant spatio-spectral interactions in raw images. Experiments onMS image classification show that our approach outperforms both handcrafted andrecent deep learning-based methods, while also requiring significantly lesscomputational effort. The source code is available athttps://github.com/AnisAmziane/RawTexture.</description><author>Anis Amziane</author><pubDate>Mon, 25 Nov 2024 16:06:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15472v3</guid></item><item><title>PriorPath: Coarse-To-Fine Approach for Controlled De-Novo Pathology Semantic Masks Generation</title><link>http://arxiv.org/abs/2411.16515v1</link><description>Incorporating artificial intelligence (AI) into digital pathology offerspromising prospects for automating and enhancing tasks such as image analysisand diagnostic processes. However, the diversity of tissue samples and thenecessity for meticulous image labeling often result in biased datasets,constraining the applicability of algorithms trained on them. To harnesssynthetic histopathological images to cope with this challenge, it is essentialnot only to produce photorealistic images but also to be able to exert controlover the cellular characteristics they depict. Previous studies used methods togenerate, from random noise, semantic masks that captured the spatialdistribution of the tissue. These masks were then used as a prior forconditional generative approaches to produce photorealistic histopathologicalimages. However, as with many other generative models, this solution exhibitsmode collapse as the model fails to capture the full diversity of theunderlying data distribution. In this work, we present a pipeline, coinedPriorPath, that generates detailed, realistic, semantic masks derived fromcoarse-grained images delineating tissue regions. This approach enables controlover the spatial arrangement of the generated masks and, consequently, theresulting synthetic images. We demonstrated the efficacy of our method acrossthree cancer types, skin, prostate, and lung, showcasing PriorPath's capabilityto cover the semantic mask space and to provide better similarity to real maskscompared to previous methods. Our approach allows for specifying desired tissuedistributions and obtaining both photorealistic masks and images within asingle platform, thus providing a state-of-the-art, controllable solution forgenerating histopathological images to facilitate AI for computationalpathology.</description><author>Nati Daniel, May Nathan, Eden Azeroual, Yael Fisher, Yonatan Savir</author><pubDate>Mon, 25 Nov 2024 15:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16515v1</guid></item><item><title>Guarding the Gate: ConceptGuard Battles Concept-Level Backdoors in Concept Bottleneck Models</title><link>http://arxiv.org/abs/2411.16512v1</link><description>The increasing complexity of AI models, especially in deep learning, hasraised concerns about transparency and accountability, particularly inhigh-stakes applications like medical diagnostics, where opaque models canundermine trust. Explainable Artificial Intelligence (XAI) aims to addressthese issues by providing clear, interpretable models. Among XAI techniques,Concept Bottleneck Models (CBMs) enhance transparency by using high-levelsemantic concepts. However, CBMs are vulnerable to concept-level backdoorattacks, which inject hidden triggers into these concepts, leading toundetectable anomalous behavior. To address this critical security gap, weintroduce ConceptGuard, a novel defense framework specifically designed toprotect CBMs from concept-level backdoor attacks. ConceptGuard employs amulti-stage approach, including concept clustering based on text distancemeasurements and a voting mechanism among classifiers trained on differentconcept subgroups, to isolate and mitigate potential triggers. Ourcontributions are threefold: (i) we present ConceptGuard as the first defensemechanism tailored for concept-level backdoor attacks in CBMs; (ii) we providetheoretical guarantees that ConceptGuard can effectively defend against suchattacks within a certain trigger size threshold, ensuring robustness; and (iii)we demonstrate that ConceptGuard maintains the high performance andinterpretability of CBMs, crucial for trustworthiness. Through comprehensiveexperiments and theoretical proofs, we show that ConceptGuard significantlyenhances the security and trustworthiness of CBMs, paving the way for theirsecure deployment in critical applications.</description><author>Songning Lai, Yu Huang, Jiayu Yang, Gaoxiang Huang, Wenshuo Chen, Yutao Yue</author><pubDate>Mon, 25 Nov 2024 15:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16512v1</guid></item><item><title>LATUP-Net: A Lightweight 3D Attention U-Net with Parallel Convolutions for Brain Tumor Segmentation</title><link>http://arxiv.org/abs/2404.05911v2</link><description>Early-stage 3D brain tumor segmentation from magnetic resonance imaging (MRI)scans is crucial for prompt and effective treatment. However, this processfaces the challenge of precise delineation due to the tumors' complexheterogeneity. Moreover, energy sustainability targets and resourcelimitations, especially in developing countries, require efficient andaccessible medical imaging solutions. The proposed architecture, a Lightweight3D ATtention U-Net with Parallel convolutions, LATUP-Net, addresses theseissues. It is specifically designed to reduce computational requirementssignificantly while maintaining high segmentation performance. By incorporatingparallel convolutions, it enhances feature representation by capturingmulti-scale information. It further integrates an attention mechanism to refinesegmentation through selective feature recalibration. LATUP-Net achievespromising segmentation performance: the average Dice scores for the wholetumor, tumor core, and enhancing tumor on the BraTS 2020 dataset are 88.41%,83.82%, and 73.67%, and on the BraTS 2021 dataset, they are 90.29%, 89.54%, and83.92%, respectively. Hausdorff distance metrics further indicate its improvedability to delineate tumor boundaries. With its significantly reducedcomputational demand using only 3.07M parameters, about 59 times fewer thanother state-of-the-art models, and running on a single NVIDIA GeForce RTX306012GB GPU, LATUP-Net requires just 15.79 GFLOPs. This makes it a promisingsolution for real-world clinical applications, particularly in settings withlimited resources. Investigations into the model's interpretability, utilizinggradient-weighted class activation mapping and confusion matrices, reveal thatwhile attention mechanisms enhance the segmentation of small regions, theirimpact is nuanced. Achieving the most [...]. The code is available athttps://qyber.black/ca/code-bca.</description><author>Ebtihal J. Alwadee, Xianfang Sun, Yipeng Qin, Frank C. Langbein</author><pubDate>Mon, 25 Nov 2024 15:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05911v2</guid></item><item><title>A Unified Framework for Center-based Clustering of Distributed Data</title><link>http://arxiv.org/abs/2402.01302v2</link><description>We develop a family of distributed center-based clustering algorithms thatwork over networks of users. In the proposed scenario, users contain a localdataset and communicate only with their immediate neighbours, with the aim offinding a clustering of the full, joint data. The proposed family, termedDistributed Gradient Clustering (DGC-$\mathcal{F}_\rho$), is parametrized by$\rho \geq 1$, controling the proximity of users' center estimates, with$\mathcal{F}$ determining the clustering loss. Our framework allows for a broadclass of smooth convex loss functions, including popular clustering losses like$K$-means and Huber loss. Specialized to popular clustering losses like$K$-means and Huber loss, DGC-$\mathcal{F}_\rho$ gives rise to noveldistributed clustering algorithms DGC-KM$_\rho$ and DGC-HL$_\rho$, while novelclustering losses based on Logistic and Fair functions lead to DGC-LL$_\rho$and DGC-FL$_\rho$. We provide a unified analysis and establish several strongresults, under mild assumptions. First, we show that the sequence of centersgenerated by the methods converges to a well-defined notion of fixed point,under any center initialization and value of $\rho$. Second, we prove that, as$\rho$ increases, the family of fixed points produced by DGC-$\mathcal{F}_\rho$converges to a notion of consensus fixed points. We show that consensus fixedpoints of DGC-$\mathcal{F}_{\rho}$ are equivalent to fixed points of gradientclustering over the full data, guaranteeing a clustering of the full data isproduced. For the special case of Bregman losses, we show that our fixed pointsconverge to the set of Lloyd points. Extensive numerical experiments onsynthetic and real data confirm our theoretical findings, show strongperformance of our methods and demonstrate the usefulness and wide range ofpotential applications of our general framework, such as outlier detection.</description><author>Aleksandar Armacki, Dragana Bajoviƒá, Du≈°an Jakovetiƒá, Soummya Kar</author><pubDate>Mon, 25 Nov 2024 15:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01302v2</guid></item><item><title>Jaya R Package -- A Parameter-Free Solution for Advanced Single and Multi-Objective Optimization</title><link>http://arxiv.org/abs/2411.16509v1</link><description>The Jaya R package offers a robust and versatile implementation of theparameter-free Jaya optimization algorithm, suitable for solving bothsingle-objective and multi-objective optimization problems. By integratingadvanced features such as constraint handling, adaptive population management,Pareto front tracking for multi-objective trade-offs, and parallel processingfor computational efficiency, the package caters to a wide range ofoptimization challenges. Its intuitive design and flexibility allow users tosolve complex, real-world problems across various domains. To demonstrate itspractical utility, a case study on energy modeling explores the optimization ofrenewable energy shares, showcasing the package's ability to minimize carbonemissions and costs while enhancing system reliability. The Jaya R package isan invaluable tool for researchers and practitioners seeking efficient andadaptive optimization solutions.</description><author>Neeraj Dhanraj Bokde</author><pubDate>Mon, 25 Nov 2024 15:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16509v1</guid></item><item><title>All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages</title><link>http://arxiv.org/abs/2411.16508v1</link><description>Existing Large Multimodal Models (LMMs) generally focus on only a few regionsand languages. As LMMs continue to improve, it is increasingly important toensure they understand cultural contexts, respect local sensitivities, andsupport low-resource languages, all while effectively integrating correspondingvisual cues. In pursuit of culturally diverse global multimodal models, ourproposed All Languages Matter Benchmark (ALM-bench) represents the largest andmost comprehensive effort to date for evaluating LMMs across 100 languages.ALM-bench challenges existing models by testing their ability to understand andreason about culturally diverse images paired with text in various languages,including many low-resource languages traditionally underrepresented in LMMresearch. The benchmark offers a robust and nuanced evaluation frameworkfeaturing various question formats, including true/false, multiple choice, andopen-ended questions, which are further divided into short and long-answercategories. ALM-bench design ensures a comprehensive assessment of a model'sability to handle varied levels of difficulty in visual and linguisticreasoning. To capture the rich tapestry of global cultures, ALM-bench carefullycurates content from 13 distinct cultural aspects, ranging from traditions andrituals to famous personalities and celebrations. Through this, ALM-bench notonly provides a rigorous testing ground for state-of-the-art open andclosed-source LMMs but also highlights the importance of cultural andlinguistic inclusivity, encouraging the development of models that can servediverse global populations effectively. Our benchmark is publicly available.</description><author>Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kuckreja, Mykola Maslych, Wafa Al Ghallabi, Mihail Mihaylov, Chao Qin, Abdelrahman M Shaker, Mike Zhang, Mahardika Krisna Ihsani, Amiel Esplana, Monil Gokani, Shachar Mirkin, Harsh Singh, Ashay Srivastava, Endre Hamerlik, Fathinah Asma Izzati, Fadillah Adamsyah Maani, Sebastian Cavada, Jenny Chim, Rohit Gupta, Sanjay Manjunath, Kamila Zhumakhanova, Feno Heriniaina Rabevohitra, Azril Amirudin, Muhammad Ridzuan, Daniya Kareem, Ketan More, Kunyang Li, Pramesh Shakya, Muhammad Saad, Amirpouya Ghasemaghaei, Amirbek Djanibekov, Dilshod Azizov, Branislava Jankovic, Naman Bhatia, Alvaro Cabrera, Johan Obando-Ceron, Olympiah Otieno, Fabi</author><pubDate>Mon, 25 Nov 2024 15:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16508v1</guid></item><item><title>Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image Synthesis</title><link>http://arxiv.org/abs/2411.16503v1</link><description>Diffusion models have achieved impressive success in generatingphotorealistic images, but challenges remain in ensuring precise semanticalignment with input prompts. Optimizing the initial noisy latent offers a moreefficient alternative to modifying model architectures or prompt engineeringfor improving semantic alignment. A latest approach, InitNo, refines theinitial noisy latent by leveraging attention maps; however, these maps captureonly limited information, and the effectiveness of InitNo is highly dependenton the initial starting point, as it tends to converge on a local optimum nearthis point. To this end, this paper proposes leveraging the languagecomprehension capabilities of large vision-language models (LVLMs) to guide theoptimization of the initial noisy latent, and introduces the Noise Diffusionprocess, which updates the noisy latent to generate semantically faithfulimages while preserving distribution consistency. Furthermore, we provide atheoretical analysis of the condition under which the update improves semanticfaithfulness. Experimental results demonstrate the effectiveness andadaptability of our framework, consistently enhancing semantic alignment acrossvarious diffusion models. The code is available athttps://github.com/Bomingmiao/NoiseDiffusion.</description><author>Boming Miao, Chunxiao Li, Xiaoxiao Wang, Andi Zhang, Rui Sun, Zizhe Wang, Yao Zhu</author><pubDate>Mon, 25 Nov 2024 15:40:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16503v1</guid></item><item><title>Multimodal Foundation Models Exploit Text to Make Medical Image Predictions</title><link>http://arxiv.org/abs/2311.05591v2</link><description>Multimodal foundation models have shown compelling but conflictingperformance in medical image interpretation. However, the mechanisms by whichthese models integrate and prioritize different data modalities, includingimages and text, remain poorly understood. Here, using a diverse collection of1014 multimodal medical cases, we evaluate the unimodal and multimodal imageinterpretation abilities of proprietary (GPT-4, Gemini Pro 1.0) and open-source(Llama-3.2-90B, LLaVA-Med-v1.5) multimodal foundational models with and withoutthe use of text descriptions. Across all models, image predictions were largelydriven by exploiting text, with accuracy increasing monotonically with theamount of informative text. By contrast, human performance on medical imageinterpretation did not improve with informative text. Exploitation of text is adouble-edged sword; we show that even mild suggestions of an incorrectdiagnosis in text diminishes image-based classification, reducing performancedramatically in cases the model could previously answer with images alone.Finally, we conducted a physician evaluation of model performance on long-formmedical cases, finding that the provision of images either reduced or had noeffect on model performance when text is already highly informative. Ourresults suggest that multimodal AI models may be useful in medical diagnosticreasoning but that their accuracy is largely driven, for better and worse, bytheir exploitation of text.</description><author>Thomas Buckley, James A. Diao, Pranav Rajpurkar, Adam Rodman, Arjun K. Manrai</author><pubDate>Mon, 25 Nov 2024 15:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05591v2</guid></item><item><title>Enhancing Autonomous Driving Safety through World Model-Based Predictive Navigation and Adaptive Learning Algorithms for 5G Wireless Applications</title><link>http://arxiv.org/abs/2411.15042v2</link><description>Addressing the challenge of ensuring safety in ever-changing andunpredictable environments, particularly in the swiftly advancing realm ofautonomous driving in today's 5G wireless communication world, we presentNavigation Secure (NavSecure). This vision-based navigation framework mergesthe strengths of world models with crucial safety-focused decision-makingcapabilities, enabling autonomous vehicles to navigate real-world complexitiessecurely. Our approach anticipates potential threats and formulates saferroutes by harnessing the predictive capabilities of world models, thussignificantly reducing the need for extensive real-world trial-and-errorlearning. Additionally, our method empowers vehicles to autonomously learn anddevelop through continuous practice, ensuring the system evolves and adapts tonew challenges. Incorporating radio frequency technology, NavSecure leverages5G networks to enhance real-time data exchange, improving communication andresponsiveness. Validated through rigorous experiments under simulation-to-realdriving conditions, NavSecure has shown exceptional performance insafety-critical scenarios, such as sudden obstacle avoidance. Results indicatethat NavSecure excels in key safety metrics, including collision prevention andrisk reduction, surpassing other end-to-end methodologies. This framework notonly advances autonomous driving safety but also demonstrates how world modelscan enhance decision-making in critical applications. NavSecure sets a newstandard for developing more robust and trustworthy autonomous driving systems,capable of handling the inherent dynamics and uncertainties of real-worldenvironments.</description><author>Hong Ding, Ziming Wang, Yi Ding, Hongjie Lin, SuYang Xi, Chia Chao Kang</author><pubDate>Mon, 25 Nov 2024 15:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15042v2</guid></item><item><title>Interpreting Language Reward Models via Contrastive Explanations</title><link>http://arxiv.org/abs/2411.16502v1</link><description>Reward models (RMs) are a crucial component in the alignment of largelanguage models' (LLMs) outputs with human values. RMs approximate humanpreferences over possible LLM responses to the same prompt by predicting andcomparing reward scores. However, as they are typically modified versions ofLLMs with scalar output heads, RMs are large black boxes whose predictions arenot explainable. More transparent RMs would enable improved trust in thealignment of LLMs. In this work, we propose to use contrastive explanations toexplain any binary response comparison made by an RM. Specifically, we generatea diverse set of new comparisons similar to the original one to characterisethe RM's local behaviour. The perturbed responses forming the new comparisonsare generated to explicitly modify manually specified high-level evaluationattributes, on which analyses of RM behaviour are grounded. In quantitativeexperiments, we validate the effectiveness of our method for findinghigh-quality contrastive explanations. We then showcase the qualitativeusefulness of our method for investigating global sensitivity of RMs to eachevaluation attribute, and demonstrate how representative examples can beautomatically extracted to explain and compare behaviours of different RMs. Wesee our method as a flexible framework for RM explanation, providing a basisfor more interpretable and trustworthy LLM alignment.</description><author>Junqi Jiang, Tom Bewley, Saumitra Mishra, Freddy Lecue, Manuela Veloso</author><pubDate>Mon, 25 Nov 2024 15:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16502v1</guid></item><item><title>@Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology</title><link>http://arxiv.org/abs/2409.14215v2</link><description>As Vision-Language Models (VLMs) advance, human-centered AssistiveTechnologies (ATs) for helping People with Visual Impairments (PVIs) areevolving into generalists, capable of performing multiple tasks simultaneously.However, benchmarking VLMs for ATs remains under-explored. To bridge this gap,we first create a novel AT benchmark (@Bench). Guided by a pre-design userstudy with PVIs, our benchmark includes the five most crucial vision-languagetasks: Panoptic Segmentation, Depth Estimation, Optical Character Recognition(OCR), Image Captioning, and Visual Question Answering (VQA). Besides, wepropose a novel AT model (@Model) that addresses all tasks simultaneously andcan be expanded to more assistive functions for helping PVIs. Our frameworkexhibits outstanding performance across tasks by integrating multi-modalinformation, and it offers PVIs a more comprehensive assistance. Extensiveexperiments prove the effectiveness and generalizability of our framework.</description><author>Xin Jiang, Junwei Zheng, Ruiping Liu, Jiahang Li, Jiaming Zhang, Sven Matthiesen, Rainer Stiefelhagen</author><pubDate>Mon, 25 Nov 2024 15:36:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14215v2</guid></item><item><title>Multi-Resolution Generative Modeling of Human Motion from Limited Data</title><link>http://arxiv.org/abs/2411.16498v1</link><description>We present a generative model that learns to synthesize human motion fromlimited training sequences. Our framework provides conditional generation andblending across multiple temporal resolutions. The model adeptly captures humanmotion patterns by integrating skeletal convolution layers and a multi-scalearchitecture. Our model contains a set of generative and adversarial networks,along with embedding modules, each tailored for generating motions at specificframe rates while exerting control over their content and details. Notably, ourapproach also extends to the synthesis of co-speech gestures, demonstrating itsability to generate synchronized gestures from speech inputs, even with limitedpaired data. Through direct synthesis of SMPL pose parameters, our approachavoids test-time adjustments to fit human body meshes. Experimental resultsshowcase our model's ability to achieve extensive coverage of trainingexamples, while generating diverse motions, as indicated by local and globaldiversity metrics.</description><author>David Eduardo Moreno-Villamar√≠n, Anna Hilsmann, Peter Eisert</author><pubDate>Mon, 25 Nov 2024 15:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16498v1</guid></item><item><title>AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning</title><link>http://arxiv.org/abs/2411.16495v1</link><description>Recent advancements in large language models (LLMs) have led to significantimprovements in various natural language processing tasks, but it is stillchallenging for LLMs to perform knowledge-intensive complex question answeringdue to LLMs' inefficacy in reasoning planning and the hallucination problem. Atypical solution is to employ retrieval-augmented generation (RAG) coupled withchain-of-thought (CoT) reasoning, which decomposes complex questions intochain-like sub-questions and applies iterative RAG at each sub-question.However, prior works exhibit sub-optimal reasoning planning and overlookdynamic knowledge retrieval from heterogeneous sources. In this paper, wepropose AtomR, a novel heterogeneous knowledge reasoning framework thatconducts multi-source reasoning at the atomic level. Drawing inspiration fromthe graph modeling of knowledge, AtomR leverages large language models (LLMs)to decompose complex questions into combinations of three atomic knowledgeoperators, significantly enhancing the reasoning process at both the planningand execution stages. We also introduce BlendQA, a novel evaluation benchmarktailored to assess complex heterogeneous knowledge reasoning. Experiments showthat AtomR significantly outperforms state-of-the-art baselines across threesingle-source and two multi-source reasoning benchmarks, with notableperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.</description><author>Amy Xin, Jinxin Liu, Zijun Yao, Zhicheng Li, Shulin Cao, Lei Hou, Juanzi Li</author><pubDate>Mon, 25 Nov 2024 15:35:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16495v1</guid></item><item><title>O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?</title><link>http://arxiv.org/abs/2411.16489v1</link><description>This paper presents a critical examination of current approaches toreplicating OpenAI's O1 model capabilities, with particular focus on thewidespread but often undisclosed use of knowledge distillation techniques.While our previous work explored the fundamental technical path to O1replication, this study reveals how simple distillation from O1's API, combinedwith supervised fine-tuning, can achieve superior performance on complexmathematical reasoning tasks. Through extensive experiments, we show that abase model fine-tuned on simply tens of thousands of samples O1-distilledlong-thought chains outperforms O1-preview on the American InvitationalMathematics Examination (AIME) with minimal technical complexity. Moreover, ourinvestigation extends beyond mathematical reasoning to explore thegeneralization capabilities of O1-distilled models across diverse tasks:hallucination, safety and open-domain QA. Notably, despite training only onmathematical problem-solving data, our models demonstrated stronggeneralization to open-ended QA tasks and became significantly less susceptibleto sycophancy after fine-tuning. We deliberately make this finding public topromote transparency in AI research and to challenge the current trend ofobscured technical claims in the field. Our work includes: (1) A detailedtechnical exposition of the distillation process and its effectiveness, (2) Acomprehensive benchmark framework for evaluating and categorizing O1replication attempts based on their technical transparency and reproducibility,(3) A critical discussion of the limitations and potential risks ofover-relying on distillation approaches, our analysis culminates in a crucialbitter lesson: while the pursuit of more capable AI systems is important, thedevelopment of researchers grounded in first-principles thinking is paramount.</description><author>Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, Pengfei Liu</author><pubDate>Mon, 25 Nov 2024 15:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16489v1</guid></item><item><title>When Babies Teach Babies: Can student knowledge sharing outperform Teacher-Guided Distillation on small datasets?</title><link>http://arxiv.org/abs/2411.16487v1</link><description>We present our submission to the BabyLM challenge, aiming to push theboundaries of data-efficient language model pretraining. Our method builds upondeep mutual learning, introducing a student model search for diverseinitialization. We address the limitation of treating students equally byformulating weighted mutual learning as a bi-level optimization problem. Theinner loop learns compact students through online distillation, while the outerloop optimizes weights for better knowledge distillation from diverse students.This dynamic weighting strategy eliminates the need for a teacher model,reducing computational requirements. Our evaluations show that teacher-lessmethods can match or surpass teacher-supervised approaches.</description><author>Srikrishna Iyer</author><pubDate>Mon, 25 Nov 2024 15:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16487v1</guid></item><item><title>Graph Transformer Networks for Accurate Band Structure Prediction: An End-to-End Approach</title><link>http://arxiv.org/abs/2411.16483v1</link><description>Predicting electronic band structures from crystal structures is crucial forunderstanding structure-property correlations in materials science.First-principles approaches are accurate but computationally intensive. Recentyears, machine learning (ML) has been extensively applied to this field, whileexisting ML models predominantly focus on band gap predictions or indirect bandstructure estimation via solving predicted Hamiltonians. An end-to-end model topredict band structure accurately and efficiently is still lacking. Here, weintroduce a graph Transformer-based end-to-end approach that directly predictsband structures from crystal structures with high accuracy. Our methodleverages the continuity of the k-path and treat continuous bands as asequence. We demonstrate that our model not only provides accurate bandstructure predictions but also can derive other properties (such as band gap,band center, and band dispersion) with high accuracy. We verify the modelperformance on large and diverse datasets.</description><author>Weiyi Gong, Tao Sun, Hexin Bai, Jeng-Yuan Tsai, Haibin Ling, Qimin Yan</author><pubDate>Mon, 25 Nov 2024 15:22:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16483v1</guid></item><item><title>Deformable Mamba for Wide Field of View Segmentation</title><link>http://arxiv.org/abs/2411.16481v1</link><description>Wide-FoV cameras, like fisheye and panoramic setups, are essential forbroader perception but introduce significant distortions in 180{\deg} and360{\deg} images, complicating dense prediction tasks. For instance, existingMAMBA models lacking distortion-aware capacity cannot perform well in panoramicsemantic segmentation. To address this problem, this work presents DeformableMamba, a unified framework specifically designed to address imaging distortionswithin the context of panoramic and fisheye semantic segmentation. At the coreis a decoder constructed with a series of Deformable Mamba Fusion (DMF) blocks,making the whole framework more deformable, efficient, and accurate, whenhandling extreme distortions. Extensive evaluations across five datasetsdemonstrate that our method consistently improves segmentation accuracycompared to the previous state-of-the-art methods tailored for specific FoVs.Notably, Deformable Mamba achieves a +2.5% performance improvement on the360{\deg} Stanford2D3D dataset, and shows better results across FoVs from60{\deg} to 360{\deg}.</description><author>Jie Hu, Junwei Zheng, Jiale Wei, Jiaming Zhang, Rainer Stiefelhagen</author><pubDate>Mon, 25 Nov 2024 15:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16481v1</guid></item><item><title>Distributed, communication-efficient, and differentially private estimation of KL divergence</title><link>http://arxiv.org/abs/2411.16478v1</link><description>A key task in managing distributed, sensitive data is to measure the extentto which a distribution changes. Understanding this drift can effectivelysupport a variety of federated learning and analytics tasks. However, in manypractical settings sharing such information can be undesirable (e.g., forprivacy concerns) or infeasible (e.g., for high communication costs). In thiswork, we describe novel algorithmic approaches for estimating the KL divergenceof data across federated models of computation, under differential privacy. Weanalyze their theoretical properties and present an empirical study of theirperformance. We explore parameter settings that optimize the accuracy of thealgorithm catering to each of the settings; these provide sub-variations thatare applicable to real-world tasks, addressing different context- andapplication-specific trust level requirements. Our experimental results confirmthat our private estimators achieve accuracy comparable to a baseline algorithmwithout differential privacy guarantees.</description><author>Mary Scott, Sayan Biswas, Graham Cormode, Carsten Maple</author><pubDate>Mon, 25 Nov 2024 15:20:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16478v1</guid></item><item><title>Distributed Online Optimization with Stochastic Agent Availability</title><link>http://arxiv.org/abs/2411.16477v1</link><description>Motivated by practical federated learning settings where clients may not bealways available, we investigate a variant of distributed online optimizationwhere agents are active with a known probability $p$ at each time step, andcommunication between neighboring agents can only take place if they are bothactive. We introduce a distributed variant of the FTRL algorithm and analyzeits network regret, defined through the average of the instantaneous regret ofthe active agents. Our analysis shows that, for any connected communicationgraph $G$ over $N$ agents, the expected network regret of our FTRL variantafter $T$ steps is at most of order$(\kappa/p^2)\min\big\{\sqrt{N},N^{1/4}/\sqrt{p}\big\}\sqrt{T}$, where $\kappa$is the condition number of the Laplacian of $G$. We then show that similarregret bounds also hold with high probability. Moreover, we show that ournotion of regret (average-case over the agents) is essentially equivalent tothe standard notion of regret (worst-case over agents), implying that ourbounds are not significantly improvable when $p=1$. Our theoretical results aresupported by experiments on synthetic datasets.</description><author>Juliette Achddou, Nicol√≤ Cesa-Bianchi, Hao Qiu</author><pubDate>Mon, 25 Nov 2024 15:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16477v1</guid></item><item><title>NonSysId: A nonlinear system identification package with improved model term selection for NARMAX models</title><link>http://arxiv.org/abs/2411.16475v1</link><description>System identification involves constructing mathematical models of dynamicsystems using input-output data, enabling analysis and prediction of systembehaviour in both time and frequency domains. This approach can model theentire system or capture specific dynamics within it. For meaningful analysis,it is essential for the model to accurately reflect the underlying system'sbehaviour. This paper introduces NonSysId, an open-sourced MATLAB softwarepackage designed for nonlinear system identification, specifically focusing onNARMAX models. The software incorporates an advanced term selection methodologythat prioritises on simulation (free-run) accuracy while preserving modelparsimony. A key feature is the integration of iterative Orthogonal ForwardRegression (iOFR) with Predicted Residual Sum of Squares (PRESS)statistic-based term selection, facilitating robust model generalisationwithout the need for a separate validation dataset. Furthermore, techniques forreducing computational overheads are implemented. These features make NonSysIdparticularly suitable for real-time applications such as structural healthmonitoring, fault diagnosis, and biomedical signal processing, where it is achallenge to capture the signals under consistent conditions, resulting inlimited or no validation data.</description><author>Rajintha Gunawardena, Zi-Qiang Lang, Fei He</author><pubDate>Mon, 25 Nov 2024 15:19:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16475v1</guid></item><item><title>OffLight: An Offline Multi-Agent Reinforcement Learning Framework for Traffic Signal Control</title><link>http://arxiv.org/abs/2411.06601v2</link><description>Efficient traffic control (TSC) is essential for urban mobility, buttraditional systems struggle to handle the complexity of real-world traffic.Multi-agent Reinforcement Learning (MARL) offers adaptive solutions, but onlineMARL requires extensive interactions with the environment, making it costly andimpractical. Offline MARL mitigates these challenges by using historicaltraffic data for training but faces significant difficulties with heterogeneousbehavior policies in real-world datasets, where mixed-quality data complicateslearning. We introduce OffLight, a novel offline MARL framework designed tohandle heterogeneous behavior policies in TSC datasets. To improve learningefficiency, OffLight incorporates Importance Sampling (IS) to correct fordistributional shifts and Return-Based Prioritized Sampling (RBPS) to focus onhigh-quality experiences. OffLight utilizes a Gaussian Mixture VariationalGraph Autoencoder (GMM-VGAE) to capture the diverse distribution of behaviorpolicies from local observations. Extensive experiments across real-world urbantraffic scenarios show that OffLight outperforms existing offline RL methods,achieving up to a 7.8% reduction in average travel time and 11.2% decrease inqueue length. Ablation studies confirm the effectiveness of OffLight'scomponents in handling heterogeneous data and improving policy performance.These results highlight OffLight's scalability and potential to improve urbantraffic management without the risks of online learning.</description><author>Rohit Bokade, Xiaoning Jin</author><pubDate>Mon, 25 Nov 2024 15:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.06601v2</guid></item><item><title>Efficient Video Face Enhancement with Enhanced Spatial-Temporal Consistency</title><link>http://arxiv.org/abs/2411.16468v1</link><description>As a very common type of video, face videos often appear in movies, talkshows, live broadcasts, and other scenes. Real-world online videos are oftenplagued by degradations such as blurring and quantization noise, due to thehigh compression ratio caused by high communication costs and limitedtransmission bandwidth. These degradations have a particularly serious impacton face videos because the human visual system is highly sensitive to facialdetails. Despite the significant advancement in video face enhancement, currentmethods still suffer from $i)$ long processing time and $ii)$ inconsistentspatial-temporal visual effects (e.g., flickering). This study proposes a noveland efficient blind video face enhancement method to overcome the above twochallenges, restoring high-quality videos from their compressed low-qualityversions with an effective de-flickering mechanism. In particular, the proposedmethod develops upon a 3D-VQGAN backbone associated with spatial-temporalcodebooks recording high-quality portrait features and residual-based temporalinformation. We develop a two-stage learning framework for the model. In Stage\Rmnum{1}, we learn the model with a regularizer mitigating the codebookcollapse problem. In Stage \Rmnum{2}, we learn two transformers to lookup codefrom the codebooks and further update the encoder of low-quality videos.Experiments conducted on the VFHQ-Test dataset demonstrate that our methodsurpasses the current state-of-the-art blind face video restoration andde-flickering methods on both efficiency and effectiveness. Code is availableat \url{https://github.com/Dixin-Lab/BFVR-STC}.</description><author>Yutong Wang, Jiajie Teng, Jiajiong Cao, Yuming Li, Chenguang Ma, Hongteng Xu, Dixin Luo</author><pubDate>Mon, 25 Nov 2024 15:14:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16468v1</guid></item><item><title>No Identity, no problem: Motion through detection for people tracking</title><link>http://arxiv.org/abs/2411.16466v1</link><description>Tracking-by-detection has become the de facto standard approach to peopletracking. To increase robustness, some approaches incorporate re-identificationusing appearance models and regressing motion offset, which requires costlyidentity annotations. In this paper, we propose exploiting motion clues whileproviding supervision only for the detections, which is much easier to do. Ouralgorithm predicts detection heatmaps at two different times, along with a 2Dmotion estimate between the two images. It then warps one heatmap using themotion estimate and enforces consistency with the other one. This provides therequired supervisory signal on the motion without the need for any motionannotations. In this manner, we couple the information obtained from differentimages during training and increase accuracy, especially in crowded scenes andwhen using low frame-rate sequences. We show that our approach deliversstate-of-the-art results for single- and multi-view multi-target tracking onthe MOT17 and WILDTRACK datasets.</description><author>Martin Engilberge, F. Wilke Grosche, Pascal Fua</author><pubDate>Mon, 25 Nov 2024 15:13:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16466v1</guid></item><item><title>VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?</title><link>http://arxiv.org/abs/2411.10979v3</link><description>The advancement of Multimodal Large Language Models (MLLMs) has enabledsignificant progress in multimodal understanding, expanding their capacity toanalyze video content. However, existing evaluation benchmarks for MLLMsprimarily focus on abstract video comprehension, lacking a detailed assessmentof their ability to understand video compositions, the nuanced interpretationof how visual elements combine and interact within highly compiled videocontexts. We introduce VidComposition, a new benchmark specifically designed toevaluate the video composition understanding capabilities of MLLMs usingcarefully curated compiled videos and cinematic-level annotations.VidComposition includes 982 videos with 1706 multiple-choice questions,covering various compositional aspects such as camera movement, angle, shotsize, narrative structure, character actions and emotions, etc. Ourcomprehensive evaluation of 33 open-source and proprietary MLLMs reveals asignificant performance gap between human and model capabilities. Thishighlights the limitations of current MLLMs in understanding complex, compiledvideo compositions and offers insights into areas for further improvement. Theleaderboard and evaluation code are available athttps://yunlong10.github.io/VidComposition/.</description><author>Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, Pooyan Fazli, Chenliang Xu</author><pubDate>Mon, 25 Nov 2024 15:12:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.10979v3</guid></item><item><title>Tuning Synaptic Connections instead of Weights by Genetic Algorithm in Spiking Policy Network</title><link>http://arxiv.org/abs/2301.10292v2</link><description>Learning from interaction is the primary way that biological agents acquireknowledge about their environment and themselves. Modern deep reinforcementlearning (DRL) explores a computational approach to learning from interactionand has made significant progress in solving various tasks. However, despiteits power, DRL still falls short of biological agents in terms of energyefficiency. Although the underlying mechanisms are not fully understood, webelieve that the integration of spiking communication between neurons andbiologically-plausible synaptic plasticity plays a prominent role in achievinggreater energy efficiency. Following this biological intuition, we optimized aspiking policy network (SPN) using a genetic algorithm as an energy-efficientalternative to DRL. Our SPN mimics the sensorimotor neuron pathway of insectsand communicates through event-based spikes. Inspired by biological researchshowing that the brain forms memories by creating new synaptic connections andrewiring these connections based on new experiences, we tuned the synapticconnections instead of weights in the SPN to solve given tasks. Experimentalresults on several robotic control tasks demonstrate that our method canachieve the same level of performance as mainstream DRL methods whileexhibiting significantly higher energy efficiency.</description><author>Duzhen Zhang, Tielin Zhang, Shuncheng Jia, Qingyu Wang, Bo Xu</author><pubDate>Mon, 25 Nov 2024 15:11:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10292v2</guid></item><item><title>A Gaussian Process Model for Ordinal Data with Applications to Chemoinformatics</title><link>http://arxiv.org/abs/2405.09989v2</link><description>With the proliferation of screening tools for chemical testing, it is nowpossible to create vast databases of chemicals easily. However, rigorousstatistical methodologies employed to analyse these databases are in theirinfancy, and further development to facilitate chemical discovery isimperative. In this paper, we present conditional Gaussian process models topredict ordinal outcomes from chemical experiments, where the inputs arechemical compounds. We implement the Tanimoto distance, a metric on thechemical space, within the covariance of the Gaussian processes to capturecorrelated effects in the chemical space. A novel aspect of our model is thatthe kernel contains a scaling parameter, a feature not previously examined inthe literature, that controls the strength of the correlation between elementsof the chemical space. Using molecular fingerprints, a numerical representationof a compound's location within the chemical space, we find that accounting forcorrelation amongst chemical compounds improves predictive performance over theuncorrelated model, where effects are assumed to be independent. Moreover, wepresent a genetic algorithm for the facilitation of chemical discovery andidentification of important features to the compound's efficacy, based on twocriteria derived from the proposed model. Simulation studies are conducted todemonstrate the suitability of the proposed methods. Our model is demonstratedon a hazard classification problem of organic solvents.</description><author>Arron Gosnell, Evangelos Evangelou</author><pubDate>Mon, 25 Nov 2024 15:10:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09989v2</guid></item></channel></rss>