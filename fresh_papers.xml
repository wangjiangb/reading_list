<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 03 Sep 2023 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Karhunen-Lo√®ve Data Imputation in High Contrast Imaging</title><link>http://arxiv.org/abs/2308.16912v1</link><description>Detection and characterization of extended structures is a crucial goal inhigh contrast imaging. However, these structures face challenges in datareduction, leading to over-subtraction from speckles and self-subtraction withmost existing methods. Iterative post-processing methods offer promisingresults, but their integration into existing pipelines is hindered by selectivealgorithms, high computational cost, and algorithmic regularization. To addressthis for reference differential imaging (RDI), here we propose the dataimputation concept to Karhunen-Lo\`eve transform (DIKL) by modifying two stepsin the standard Karhunen-Lo\`eve image projection (KLIP) method. Specifically,we partition an image to two matrices: an anchor matrix which focuses only onthe speckles to obtain the DIKL coefficients, and a boat matrix which focuseson the regions of astrophysical interest for speckle removal using DIKLcomponents. As an analytical approach, DIKL achieves high-quality results withsignificantly reduced computational cost (~3 orders of magnitude less thaniterative methods). Being a derivative method of KLIP, DIKL is seamlesslyintegrable into high contrast imaging pipelines for RDI observations.</description><author>Bin B. Ren</author><pubDate>Thu, 31 Aug 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16912v1</guid></item><item><title>Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2307.04726v2</link><description>Offline Reinforcement Learning (RL) methods leverage previous experiences tolearn better policies than the behavior policy used for data collection. Incontrast to behavior cloning, which assumes the data is collected from expertdemonstrations, offline RL can work with non-expert data and multimodalbehavior policies. However, offline RL algorithms face challenges in handlingdistribution shifts and effectively representing policies due to the lack ofonline interaction during training. Prior work on offline RL uses conditionaldiffusion models to represent multimodal behavior in the dataset. Nevertheless,these methods are not tailored toward alleviating the out-of-distribution stategeneralization. We introduce a novel method, named State Reconstruction forDiffusion Policies (SRDP), incorporating state reconstruction feature learningin the recent class of diffusion policies to address the out-of-distributiongeneralization problem. State reconstruction loss promotes more descriptiverepresentation learning of states to alleviate the distribution shift incurredby the out-of-distribution (OOD) states. We design a novel 2D MultimodalContextual Bandit environment to illustrate the OOD generalization of SRDPcompared to prior algorithms. In addition, we assess the performance of ourmodel on D4RL continuous control benchmarks, namely the navigation of an 8-DoFant and forward locomotion of half-cheetah, hopper, and walker2d, achievingstate-of-the-art results.</description><author>Suzan Ece Ada, Erhan Oztop, Emre Ugur</author><pubDate>Thu, 31 Aug 2023 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04726v2</guid></item><item><title>PointLLM: Empowering Large Language Models to Understand Point Clouds</title><link>http://arxiv.org/abs/2308.16911v1</link><description>The unprecedented advancements in Large Language Models (LLMs) have created aprofound impact on natural language processing but are yet to fully embrace therealm of 3D understanding. This paper introduces PointLLM, a preliminary effortto fill this gap, thereby enabling LLMs to understand point clouds and offeringa new avenue beyond 2D visual data. PointLLM processes colored object pointclouds with human instructions and generates contextually appropriateresponses, illustrating its grasp of point clouds and common sense.Specifically, it leverages a point cloud encoder with a powerful LLM toeffectively fuse geometric, appearance, and linguistic information. We collecta novel dataset comprising 660K simple and 70K complex point-text instructionpairs to enable a two-stage training strategy: initially aligning latent spacesand subsequently instruction-tuning the unified model. To rigorously evaluateour model's perceptual abilities and its generalization capabilities, weestablish two benchmarks: Generative 3D Object Classification and 3D ObjectCaptioning, assessed through three different methods, including humanevaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experimentresults show that PointLLM demonstrates superior performance over existing 2Dbaselines. Remarkably, in human-evaluated object captioning tasks, PointLLMoutperforms human annotators in over 50% of the samples. Codes, datasets, andbenchmarks are available at https://github.com/OpenRobotLab/PointLLM .</description><author>Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin</author><pubDate>Thu, 31 Aug 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16911v1</guid></item><item><title>StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation</title><link>http://arxiv.org/abs/2308.16909v1</link><description>Unconditional video generation is a challenging task that involvessynthesizing high-quality videos that are both coherent and of extendedduration. To address this challenge, researchers have used pretrained StyleGANimage generators for high-quality frame synthesis and focused on motiongenerator design. The motion generator is trained in an autoregressive mannerusing heavy 3D convolutional discriminators to ensure motion coherence duringvideo generation. In this paper, we introduce a novel motion generator designthat uses a learning-based inversion network for GAN. The encoder in our methodcaptures rich and smooth priors from encoding images to latents, and given thelatent of an initially generated frame as guidance, our method can generatesmooth future latent by modulating the inversion encoder temporally. Our methodenjoys the advantage of sparse training and naturally constrains the generationspace of our motion generator with the inversion network guided by the initialframe, eliminating the need for heavy discriminators. Moreover, our methodsupports style transfer with simple fine-tuning when the encoder is paired witha pretrained StyleGAN generator. Extensive experiments conducted on variousbenchmarks demonstrate the superiority of our method in generating long andhigh-resolution videos with decent single-frame quality and temporalconsistency.</description><author>Yuhan Wang, Liming Jiang, Chen Change Loy</author><pubDate>Thu, 31 Aug 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16909v1</guid></item><item><title>Fine-Grained Cross-View Geo-Localization Using a Correlation-Aware Homography Estimator</title><link>http://arxiv.org/abs/2308.16906v1</link><description>In this paper, we introduce a novel approach to fine-grained cross-viewgeo-localization. Our method aligns a warped ground image with a correspondingGPS-tagged satellite image covering the same area using homography estimation.We first employ a differentiable spherical transform, adhering to geometricprinciples, to accurately align the perspective of the ground image with thesatellite map. This transformation effectively places ground and aerial imagesin the same view and on the same plane, reducing the task to an image alignmentproblem. To address challenges such as occlusion, small overlapping range, andseasonal variations, we propose a robust correlation-aware homography estimatorto align similar parts of the transformed ground image with the satelliteimage. Our method achieves sub-pixel resolution and meter-level GPS accuracy bymapping the center point of the transformed ground image to the satellite imageusing a homography matrix and determining the orientation of the ground camerausing a point above the central axis. Operating at a speed of 30 FPS, ourmethod outperforms state-of-the-art techniques, reducing the mean metriclocalization error by 21.3% and 32.4% in same-area and cross-areageneralization tasks on the VIGOR benchmark, respectively, and by 34.4% on theKITTI benchmark in same-area evaluation.</description><author>Xiaolong Wang, Runsen Xu, Zuofan Cui, Zeyu Wan, Yu Zhang</author><pubDate>Thu, 31 Aug 2023 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16906v1</guid></item><item><title>InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion</title><link>http://arxiv.org/abs/2308.16905v1</link><description>This paper addresses a novel task of anticipating 3D human-objectinteractions (HOIs). Most existing research on HOI synthesis lackscomprehensive whole-body interactions with dynamic objects, e.g., often limitedto manipulating small or static objects. Our task is significantly morechallenging, as it requires modeling dynamic objects with various shapes,capturing whole-body motion, and ensuring physically valid interactions. Tothis end, we propose InterDiff, a framework comprising two key steps: (i)interaction diffusion, where we leverage a diffusion model to encode thedistribution of future human-object interactions; (ii) interaction correction,where we introduce a physics-informed predictor to correct denoised HOIs in adiffusion step. Our key insight is to inject prior knowledge that theinteractions under reference with respect to contact points follow a simplepattern and are easily predictable. Experiments on multiple human-objectinteraction datasets demonstrate the effectiveness of our method for this task,capable of producing realistic, vivid, and remarkably long-term 3D HOIpredictions.</description><author>Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, Liang-Yan Gui</author><pubDate>Thu, 31 Aug 2023 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16905v1</guid></item><item><title>A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems</title><link>http://arxiv.org/abs/2308.16904v1</link><description>Large-scale linear systems, $Ax=b$, frequently arise in practice and demandeffective iterative solvers. Often, these systems are noisy due to operationalerrors or faulty data-collection processes. In the past decade, the randomizedKaczmarz (RK) algorithm has been studied extensively as an efficient iterativesolver for such systems. However, the convergence study of RK in the noisyregime is limited and considers measurement noise in the right-hand sidevector, $b$. Unfortunately, in practice, that is not always the case; thecoefficient matrix $A$ can also be noisy. In this paper, we analyze theconvergence of RK for noisy linear systems when the coefficient matrix, $A$, iscorrupted with both additive and multiplicative noise, along with the noisyvector, $b$. In our analyses, the quantity $\tilde R=\| \tilde A^{\dagger}\|_2^2 \|\tilde A \|_F^2$ influences the convergence of RK, where $\tilde A$represents a noisy version of $A$. We claim that our analysis is robust andrealistically applicable, as we do not require information about the noiselesscoefficient matrix, $A$, and considering different conditions on noise, we cancontrol the convergence of RK. We substantiate our theoretical findings byperforming comprehensive numerical experiments.</description><author>El Houcine Bergou, Soumia Boucherouite, Aritra Dutta, Xin Li, Anna Ma</author><pubDate>Thu, 31 Aug 2023 18:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16904v1</guid></item><item><title>Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement</title><link>http://arxiv.org/abs/2303.12059v3</link><description>Machine learning models for camera-based physiological measurement can haveweak generalization due to a lack of representative training data. Body motionis one of the most significant sources of noise when attempting to recover thesubtle cardiac pulse from a video. We explore motion transfer as a form of dataaugmentation to introduce motion variation while preserving physiologicalchanges of interest. We adapt a neural video synthesis approach to augmentvideos for the task of remote photoplethysmography (rPPG) and study the effectsof motion augmentation with respect to 1) the magnitude and 2) the type ofmotion. After training on motion-augmented versions of publicly availabledatasets, we demonstrate a 47% improvement over existing inter-dataset resultsusing various state-of-the-art methods on the PURE dataset. We also presentinter-dataset results on five benchmark datasets to show improvements of up to79% using TS-CAN, a neural rPPG estimation method. Our findings illustrate theusefulness of motion transfer as a data augmentation technique for improvingthe generalization of models for camera-based physiological sensing. We releaseour code for using motion transfer as a data augmentation technique on threepublicly available datasets, UBFC-rPPG, PURE, and SCAMPS, and modelspre-trained on motion-augmented data here: https://motion-matters.github.io/</description><author>Akshay Paruchuri, Xin Liu, Yulu Pan, Shwetak Patel, Daniel McDuff, Soumyadip Sengupta</author><pubDate>Thu, 31 Aug 2023 18:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12059v3</guid></item><item><title>Learning to Taste: A Multimodal Wine Dataset</title><link>http://arxiv.org/abs/2308.16900v1</link><description>We present WineSensed, a large multimodal wine dataset for studying therelations between visual perception, language, and flavor. The datasetencompasses 897k images of wine labels and 824k reviews of wines curated fromthe Vivino platform. It has over 350k unique vintages, annotated with year,region, rating, alcohol percentage, price, and grape composition. We obtainedfine-grained flavor annotations on a subset by conducting a wine-tastingexperiment with 256 participants who were asked to rank wines based on theirsimilarity in flavor, resulting in more than 5k pairwise flavor distances. Wepropose a low-dimensional concept embedding algorithm that combines humanexperience with automatic machine similarity kernels. We demonstrate that thisshared concept embedding space improves upon separate embedding spaces forcoarse flavor classification (alcohol percentage, country, grape, price,rating) and aligns with the intricate human perception of flavor.</description><author>Thoranna Bender, Simon M√∏e S√∏rensen, Alireza Kashani, K. Eldjarn Hjorleifsson, Grethe Hyldig, S√∏ren Hauberg, Serge Belongie, Frederik Warburg</author><pubDate>Thu, 31 Aug 2023 18:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16900v1</guid></item><item><title>Transformers as Support Vector Machines</title><link>http://arxiv.org/abs/2308.16898v1</link><description>Since its inception in "Attention Is All You Need", transformer architecturehas led to revolutionary advancements in NLP. The attention layer within thetransformer admits a sequence of input tokens $X$ and makes them interactthrough pairwise similarities computed as softmax$(XQK^\top X^\top)$, where$(K,Q)$ are the trainable key-query parameters. In this work, we establish aformal equivalence between the optimization geometry of self-attention and ahard-margin SVM problem that separates optimal input tokens from non-optimaltokens using linear constraints on the outer-products of token pairs. Thisformalism allows us to characterize the implicit bias of 1-layer transformersoptimized with gradient descent: (1) Optimizing the attention layer withvanishing regularization, parameterized by $(K,Q)$, converges in direction toan SVM solution minimizing the nuclear norm of the combined parameter$W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius normobjective. We characterize this convergence, highlighting that it can occurtoward locally-optimal directions rather than global ones. (2) Complementingthis, we prove the local/global directional convergence of gradient descentunder suitable geometric conditions. Importantly, we show thatover-parameterization catalyzes global convergence by ensuring the feasibilityof the SVM problem and by guaranteeing a benign optimization landscape devoidof stationary points. (3) While our theory applies primarily to linearprediction heads, we propose a more general SVM equivalence that predicts theimplicit bias with nonlinear heads. Our findings are applicable to arbitrarydatasets and their validity is verified via experiments. We also introduceseveral open problems and research directions. We believe these findingsinspire the interpretation of transformers as a hierarchy of SVMs thatseparates and selects optimal tokens.</description><author>Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, Samet Oymak</author><pubDate>Thu, 31 Aug 2023 18:57:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16898v1</guid></item><item><title>PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction</title><link>http://arxiv.org/abs/2308.16896v1</link><description>Semantic segmentation in autonomous driving has been undergoing an evolutionfrom sparse point segmentation to dense voxel segmentation, where the objectiveis to predict the semantic occupancy of each voxel in the concerned 3D space.The dense nature of the prediction space has rendered existing efficient2D-projection-based methods (e.g., bird's eye view, range view, etc.)ineffective, as they can only describe a subspace of the 3D scene. To addressthis, we propose a cylindrical tri-perspective view to represent point cloudseffectively and comprehensively and a PointOcc model to process themefficiently. Considering the distance distribution of LiDAR point clouds, weconstruct the tri-perspective view in the cylindrical coordinate system formore fine-grained modeling of nearer areas. We employ spatial group pooling tomaintain structural details during projection and adopt 2D backbones toefficiently process each TPV plane. Finally, we obtain the features of eachpoint by aggregating its projected features on each of the processed TPV planeswithout the need for any post-processing. Extensive experiments on both 3Doccupancy prediction and LiDAR segmentation benchmarks demonstrate that theproposed PointOcc achieves state-of-the-art performance with much faster speed.Specifically, despite only using LiDAR, PointOcc significantly outperforms allother methods, including multi-modal methods, with a large margin on theOpenOccupancy benchmark. Code: https://github.com/wzzheng/PointOcc.</description><author>Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 31 Aug 2023 18:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16896v1</guid></item><item><title>EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild</title><link>http://arxiv.org/abs/2308.16894v1</link><description>We present EMDB, the Electromagnetic Database of Global 3D Human Pose andShape in the Wild. EMDB is a novel dataset that contains high-quality 3D SMPLpose and shape parameters with global body and camera trajectories forin-the-wild videos. We use body-worn, wireless electromagnetic (EM) sensors anda hand-held iPhone to record a total of 58 minutes of motion data, distributedover 81 indoor and outdoor sequences and 10 participants. Together withaccurate body poses and shapes, we also provide global camera poses and bodyroot trajectories. To construct EMDB, we propose a multi-stage optimizationprocedure, which first fits SMPL to the 6-DoF EM measurements and then refinesthe poses via image observations. To achieve high-quality results, we leveragea neural implicit avatar model to reconstruct detailed human surface geometryand appearance, which allows for improved alignment and smoothness via a densepixel-level objective. Our evaluations, conducted with a multi-view volumetriccapture system, indicate that EMDB has an expected accuracy of 2.3 cmpositional and 10.6 degrees angular error, surpassing the accuracy of previousin-the-wild datasets. We evaluate existing state-of-the-art monocular RGBmethods for camera-relative and global pose estimation on EMDB. EMDB ispublicly available under https://ait.ethz.ch/emdb</description><author>Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan Zarate, Otmar Hilliges</author><pubDate>Thu, 31 Aug 2023 18:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16894v1</guid></item><item><title>Language-Conditioned Path Planning</title><link>http://arxiv.org/abs/2308.16893v1</link><description>Contact is at the core of robotic manipulation. At times, it is desired (e.g.manipulation and grasping), and at times, it is harmful (e.g. when avoidingobstacles). However, traditional path planning algorithms focus solely oncollision-free paths, limiting their applicability in contact-rich tasks. Toaddress this limitation, we propose the domain of Language-Conditioned PathPlanning, where contact-awareness is incorporated into the path planningproblem. As a first step in this domain, we propose Language-ConditionedCollision Functions (LACO) a novel approach that learns a collision functionusing only a single-view image, language prompt, and robot configuration. LACOpredicts collisions between the robot and the environment, enabling flexible,conditional path planning without the need for manual object annotations, pointcloud data, or ground-truth object meshes. In both simulation and the realworld, we demonstrate that LACO can facilitate complex, nuanced path plans thatallow for interaction with objects that are safe to collide, rather thanprohibiting any collision.</description><author>Amber Xie, Youngwoon Lee, Pieter Abbeel, Stephen James</author><pubDate>Thu, 31 Aug 2023 18:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16893v1</guid></item><item><title>ReZero: Region-customizable Sound Extraction</title><link>http://arxiv.org/abs/2308.16892v1</link><description>We introduce region-customizable sound extraction (ReZero), a general andflexible framework for the multi-channel region-wise sound extraction (R-SE)task. R-SE task aims at extracting all active target sounds (e.g., humanspeech) within a specific, user-defined spatial region, which is different fromconventional and existing tasks where a blind separation or a fixed, predefinedspatial region are typically assumed. The spatial region can be defined as anangular window, a sphere, a cone, or other geometric patterns. Being a solutionto the R-SE task, the proposed ReZero framework includes (1) definitions ofdifferent types of spatial regions, (2) methods for region feature extractionand aggregation, and (3) a multi-channel extension of the band-split RNN(BSRNN) model specified for the R-SE task. We design experiments for differentmicrophone array geometries, different types of spatial regions, andcomprehensive ablation studies on different system configurations. Experimentalresults on both simulated and real-recorded data demonstrate the effectivenessof ReZero. Demos are available at https://innerselfm.github.io/rezero/.</description><author>Rongzhi Gu, Yi Luo</author><pubDate>Thu, 31 Aug 2023 18:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16892v1</guid></item><item><title>GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields</title><link>http://arxiv.org/abs/2308.16891v1</link><description>It is a long-standing problem in robotics to develop agents capable ofexecuting diverse manipulation tasks from visual observations in unstructuredreal-world environments. To achieve this goal, the robot needs to have acomprehensive understanding of the 3D structure and semantics of the scene. Inthis work, we present $\textbf{GNFactor}$, a visual behavior cloning agent formulti-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$euralfeature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neuralfield (GNF) as a reconstruction module and a Perceiver Transformer as adecision-making module, leveraging a shared deep 3D voxel representation. Toincorporate semantics in 3D, the reconstruction module utilizes avision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distillrich semantic information into the deep 3D voxel. We evaluate GNFactor on 3real robot tasks and perform detailed ablations on 10 RLBench tasks with alimited number of demonstrations. We observe a substantial improvement ofGNFactor over current state-of-the-art methods in seen and unseen tasks,demonstrating the strong generalization ability of GNFactor. Our projectwebsite is https://yanjieze.com/GNFactor/ .</description><author>Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella Macaluso, Yuying Ge, Jianglong Ye, Nicklas Hansen, Li Erran Li, Xiaolong Wang</author><pubDate>Thu, 31 Aug 2023 18:52:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16891v1</guid></item><item><title>TouchStone: Evaluating Vision-Language Models by Language Models</title><link>http://arxiv.org/abs/2308.16890v1</link><description>Large vision-language models (LVLMs) have recently witnessed rapidadvancements, exhibiting a remarkable capacity for perceiving, understanding,and processing visual information by connecting visual receptor with largelanguage models (LLMs). However, current assessments mainly focus onrecognizing and reasoning abilities, lacking direct evaluation ofconversational skills and neglecting visual storytelling abilities. In thispaper, we propose an evaluation method that uses strong LLMs as judges tocomprehensively evaluate the various abilities of LVLMs. Firstly, we constructa comprehensive visual dialogue dataset TouchStone, consisting of open-worldimages and questions, covering five major categories of abilities and 27subtasks. This dataset not only covers fundamental recognition andcomprehension but also extends to literary creation. Secondly, by integratingdetailed image annotations we effectively transform the multimodal inputcontent into a form understandable by LLMs. This enables us to employ advancedLLMs for directly evaluating the quality of the multimodal dialogue withoutrequiring human intervention. Through validation, we demonstrate that powerfulLVLMs, such as GPT-4, can effectively score dialogue quality by leveragingtheir textual capabilities alone, aligning with human preferences. We hope ourwork can serve as a touchstone for LVLMs' evaluation and pave the way forbuilding stronger LVLMs. The evaluation code is available athttps://github.com/OFA-Sys/TouchStone.</description><author>Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, Jingren Zhou</author><pubDate>Thu, 31 Aug 2023 18:52:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16890v1</guid></item><item><title>StyleGAN as a Utility-Preserving Face De-identification Method</title><link>http://arxiv.org/abs/2212.02611v2</link><description>Face de-identification methods have been proposed to preserve users' privacyby obscuring their faces. These methods, however, can degrade the quality ofphotos, and they usually do not preserve the utility of faces, i.e., their age,gender, pose, and facial expression. Recently, GANs, such as StyleGAN, havebeen proposed, which generate realistic, high-quality imaginary faces. In thispaper, we investigate the use of StyleGAN in generating de-identified facesthrough style mixing. We examined this de-identification method for preservingutility and privacy by implementing several face detection, verification, andidentification attacks and conducting a user study. The results from ourextensive experiments, human evaluation, and comparison with twostate-of-the-art methods, i.e., CIAGAN and DeepPrivacy, show that StyleGANperforms on par or better than these methods, preserving users' privacy andimages' utility. In particular, the results of the machine learning-basedexperiments show that StyleGAN0-4 preserves utility better than CIAGAN andDeepPrivacy while preserving privacy at the same level. StyleGAN0-3 preservesutility at the same level while providing more privacy. In this paper, for thefirst time, we also performed a carefully designed user study to examine bothprivacy and utility-preserving properties of StyleGAN0-3, 0-4, and 0-5, as wellas CIAGAN and DeepPrivacy from the human observers' perspectives. Ourstatistical tests showed that participants tend to verify and identifyStyleGAN0-5 images more easily than DeepPrivacy images. All the methods butStyleGAN0-5 had significantly lower identification rates than CIAGAN. Regardingutility, as expected, StyleGAN0-5 performed significantly better in preservingsome attributes. Among all methods, on average, participants believe gender hasbeen preserved the most while naturalness has been preserved the least.</description><author>Seyyed Mohammad Sadegh Moosavi Khorzooghi, Shirin Nilizadeh</author><pubDate>Thu, 31 Aug 2023 18:51:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02611v2</guid></item><item><title>Federated Learning in UAV-Enhanced Networks: Joint Coverage and Convergence Time Optimization</title><link>http://arxiv.org/abs/2308.16889v1</link><description>Federated learning (FL) involves several devices that collaboratively train ashared model without transferring their local data. FL reduces thecommunication overhead, making it a promising learning method in UAV-enhancedwireless networks with scarce energy resources. Despite the potential,implementing FL in UAV-enhanced networks is challenging, as conventional UAVplacement methods that maximize coverage increase the FL delay significantly.Moreover, the uncertainty and lack of a priori information about crucialvariables, such as channel quality, exacerbate the problem. In this paper, wefirst analyze the statistical characteristics of a UAV-enhanced wireless sensornetwork (WSN) with energy harvesting. We then develop a model and solutionbased on the multi-objective multi-armed bandit theory to maximize the networkcoverage while minimizing the FL delay. Besides, we propose another solutionthat is particularly useful with large action sets and strict energyconstraints at the UAVs. Our proposal uses a scalarized best-arm identificationalgorithm to find the optimal arms that maximize the ratio of the expectedreward to the expected energy cost by sequentially eliminating one or more armsin each round. Then, we derive the upper bound on the error probability of ourmulti-objective and cost-aware algorithm. Numerical results show theeffectiveness of our approach.</description><author>Mariam Yahya, Setareh Maghsudi, Slawomir Stanczak</author><pubDate>Thu, 31 Aug 2023 18:50:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16889v1</guid></item><item><title>Prediction of Diblock Copolymer Morphology via Machine Learning</title><link>http://arxiv.org/abs/2308.16886v1</link><description>A machine learning approach is presented to accelerate the computation ofblock polymer morphology evolution for large domains over long timescales. Thestrategy exploits the separation of characteristic times between coarse-grainedparticle evolution on the monomer scale and slow morphological evolution overmesoscopic scales. In contrast to empirical continuum models, the proposedapproach learns stochastically driven defect annihilation processes directlyfrom particle-based simulations. A UNet architecture that respects differentboundary conditions is adopted, thereby allowing periodic and fixed substrateboundary conditions of arbitrary shape. Physical concepts are also introducedvia the loss function and symmetries are incorporated via data augmentation.The model is validated using three different use cases. Explainable artificialintelligence methods are applied to visualize the morphology evolution overtime. This approach enables the generation of large system sizes and longtrajectories to investigate defect densities and their evolution underdifferent types of confinement. As an application, we demonstrate theimportance of accessing late-stage morphologies for understanding particlediffusion inside a single block. This work has implications for directedself-assembly and materials design in micro-electronics, battery materials, andmembranes.</description><author>Hyun Park, Boyuan Yu, Juhae Park, Ge Sun, Emad Tajkhorshid, Juan J. de Pablo, Ludwig Schneider</author><pubDate>Thu, 31 Aug 2023 18:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16886v1</guid></item><item><title>The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants</title><link>http://arxiv.org/abs/2308.16884v1</link><description>We present Belebele, a multiple-choice machine reading comprehension (MRC)dataset spanning 122 language variants. Significantly expanding the languagecoverage of natural language understanding (NLU) benchmarks, this datasetenables the evaluation of text models in high-, medium-, and low-resourcelanguages. Each question is based on a short passage from the Flores-200dataset and has four multiple-choice answers. The questions were carefullycurated to discriminate between models with different levels of generallanguage comprehension. The English dataset on its own proves difficult enoughto challenge state-of-the-art language models. Being fully parallel, thisdataset enables direct comparison of model performance across all languages. Weuse this dataset to evaluate the capabilities of multilingual masked languagemodels (MLMs) and large language models (LLMs). We present extensive resultsand find that despite significant cross-lingual transfer in English-centricLLMs, much smaller MLMs pretrained on balanced multilingual data stillunderstand far more languages. We also observe that larger vocabulary size andconscious vocabulary construction correlate with better performance onlow-resource languages. Overall, Belebele opens up new avenues for evaluatingand analyzing the multilingual capabilities of NLP systems.</description><author>Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, Madian Khabsa</author><pubDate>Thu, 31 Aug 2023 18:43:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16884v1</guid></item><item><title>Seeking Interpretability and Explainability in Binary Activated Neural Networks</title><link>http://arxiv.org/abs/2209.03450v2</link><description>We study the use of binary activated neural networks as interpretable andexplainable predictors in the context of regression tasks on tabular data; morespecifically, we provide guarantees on their expressiveness, present anapproach based on the efficient computation of SHAP values for quantifying therelative importance of the features, hidden neurons and even weights. As themodel's simplicity is instrumental in achieving interpretability, we propose agreedy algorithm for building compact binary activated networks. This approachdoesn't need to fix an architecture for the network in advance: it is built onelayer at a time, one neuron at a time, leading to predictors that aren'tneedlessly complex for a given task.</description><author>Benjamin Leblanc, Pascal Germain</author><pubDate>Thu, 31 Aug 2023 18:38:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.03450v2</guid></item><item><title>Text2Scene: Text-driven Indoor Scene Stylization with Part-aware Details</title><link>http://arxiv.org/abs/2308.16880v1</link><description>We propose Text2Scene, a method to automatically create realistic texturesfor virtual scenes composed of multiple objects. Guided by a reference imageand text descriptions, our pipeline adds detailed texture on labeled 3Dgeometries in the room such that the generated colors respect the hierarchicalstructure or semantic parts that are often composed of similar materials.Instead of applying flat stylization on the entire scene at a single step, weobtain weak semantic cues from geometric segmentation, which are furtherclarified by assigning initial colors to segmented parts. Then we add texturedetails for individual objects such that their projections on image spaceexhibit feature embedding aligned with the embedding of the input. Thedecomposition makes the entire pipeline tractable to a moderate amount ofcomputation resources and memory. As our framework utilizes the existingresources of image and text embedding, it does not require dedicated datasetswith high-quality textures designed by skillful artists. To the best of ourknowledge, it is the first practical and scalable approach that can createdetailed and realistic textures of the desired style that maintain structuralcontext for scenes with multiple objects.</description><author>Inwoo Hwang, Hyeonwoo Kim, Young Min Kim</author><pubDate>Thu, 31 Aug 2023 18:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16880v1</guid></item><item><title>Adaptation Speed Analysis for Fairness-aware Causal Models</title><link>http://arxiv.org/abs/2308.16879v1</link><description>For example, in machine translation tasks, to achieve bidirectionaltranslation between two languages, the source corpus is often used as thetarget corpus, which involves the training of two models with oppositedirections. The question of which one can adapt most quickly to a domain shiftis of significant importance in many fields. Specifically, consider an originaldistribution p that changes due to an unknown intervention, resulting in amodified distribution p*. In aligning p with p*, several factors can affect theadaptation rate, including the causal dependencies between variables in p. Inreal-life scenarios, however, we have to consider the fairness of the trainingprocess, and it is particularly crucial to involve a sensitive variable (bias)present between a cause and an effect variable. To explore this scenario, weexamine a simple structural causal model (SCM) with a cause-bias-effectstructure, where variable A acts as a sensitive variable between cause (X) andeffect (Y). The two models, respectively, exhibit consistent and contrarycause-effect directions in the cause-bias-effect SCM. After conducting unknowninterventions on variables within the SCM, we can simulate some kinds of domainshifts for analysis. We then compare the adaptation speeds of two models acrossfour shift scenarios. Additionally, we prove the connection between theadaptation speeds of the two models across all interventions.</description><author>Yujie Lin, Chen Zhao, Minglai Shao, Xujiang Zhao, Haifeng Chen</author><pubDate>Thu, 31 Aug 2023 18:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16879v1</guid></item><item><title>SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation</title><link>http://arxiv.org/abs/2308.16876v1</link><description>Human-centric video frame interpolation has great potential for improvingpeople's entertainment experiences and finding commercial applications in thesports analysis industry, e.g., synthesizing slow-motion videos. Although thereare multiple benchmark datasets available in the community, none of them isdedicated for human-centric scenarios. To bridge this gap, we introduceSportsSloMo, a benchmark consisting of more than 130K video clips and 1M videoframes of high-resolution ($\geq$720p) slow-motion sports videos crawled fromYouTube. We re-train several state-of-the-art methods on our benchmark, and theresults show a decrease in their accuracy compared to other datasets. Ithighlights the difficulty of our benchmark and suggests that it posessignificant challenges even for the best-performing methods, as human bodiesare highly deformable and occlusions are frequent in sports videos. To improvethe accuracy, we introduce two loss terms considering the human-aware priors,where we add auxiliary supervision to panoptic segmentation and human keypointsdetection, respectively. The loss terms are model agnostic and can be easilyplugged into any video frame interpolation approaches. Experimental resultsvalidate the effectiveness of our proposed loss terms, leading to consistentperformance improvement over 5 existing models, which establish strong baselinemodels on our benchmark. The dataset and code can be found at:https://neu-vi.github.io/SportsSlomo/.</description><author>Jiaben Chen, Huaizu Jiang</author><pubDate>Thu, 31 Aug 2023 18:23:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16876v1</guid></item><item><title>Holistic Processing of Colour Images Using Novel Quaternion-Valued Wavelets on the Plane</title><link>http://arxiv.org/abs/2308.16875v1</link><description>We investigate the applicability of quaternion-valued wavelets on the planeto holistic colour image processing. We present a methodology for decomposingand reconstructing colour images using quaternionic wavelet filters associatedto recently developed quaternion-valued wavelets on the plane. We considercompression, enhancement, segmentation, and denoising techniques to demonstratequaternion-valued wavelets as a promising tool for holistic colour imageprocessing.</description><author>Neil D. Dizon, Jeffrey A. Hogan</author><pubDate>Thu, 31 Aug 2023 18:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16875v1</guid></item><item><title>The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages</title><link>http://arxiv.org/abs/2308.16871v1</link><description>Gender biases in language generation systems are challenging to mitigate. Onepossible source for these biases is gender representation disparities in thetraining and evaluation data. Despite recent progress in documenting thisproblem and many attempts at mitigating it, we still lack shared methodologyand tooling to report gender representation in large datasets. Suchquantitative reporting will enable further mitigation, e.g., via dataaugmentation. This paper describes the Gender-GAP Pipeline (for Gender-AwarePolyglot Pipeline), an automatic pipeline to characterize gender representationin large-scale datasets for 55 languages. The pipeline uses a multilinguallexicon of gendered person-nouns to quantify the gender representation in text.We showcase it to report gender representation in WMT training data anddevelopment data for the News task, confirming that current data is skewedtowards masculine representation. Having unbalanced datasets may indirectlyoptimize our systems towards outperforming one gender over the others. Wesuggest introducing our gender quantification pipeline in current datasets and,ideally, modifying them toward a balanced representation.</description><author>Benjamin Muller, Belen Alastruey, Prangthip Hansanti, Elahe Kalbassi, Christophe Ropers, Eric Michael Smith, Adina Williams, Luke Zettlemoyer, Pierre Andrews, Marta R. Costa-juss√†</author><pubDate>Thu, 31 Aug 2023 18:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16871v1</guid></item><item><title>Learning Driver Models for Automated Vehicles via Knowledge Sharing and Personalization</title><link>http://arxiv.org/abs/2308.16870v1</link><description>This paper describes a framework for learning Automated Vehicles (AVs) drivermodels via knowledge sharing between vehicles and personalization. The innatevariability in the transportation system makes it exceptionally challenging toexpose AVs to all possible driving scenarios during empirical experimentationor testing. Consequently, AVs could be blind to certain encounters that aredeemed detrimental to their safe and efficient operation. It is then criticalto share knowledge across AVs that increase exposure to driving scenariosoccurring in the real world. This paper explores a method to collaborativelytrain a driver model by sharing knowledge and borrowing strength acrossvehicles while retaining a personalized model tailored to the vehicle's uniqueconditions and properties. Our model brings a federated learning approach tocollaborate between multiple vehicles while circumventing the need to share rawdata between them. We showcase our method's performance in experimentalsimulations. Such an approach to learning finds several applications acrosstransportation engineering including intelligent transportation systems,traffic management, and vehicle-to-vehicle communication. Code and sampledataset are made available at the project page https://github.com/wissamkontar.</description><author>Wissam Kontar, Xinzhi Zhong, Soyoung Ahn</author><pubDate>Thu, 31 Aug 2023 18:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16870v1</guid></item><item><title>Dynamical systems' based neural networks</title><link>http://arxiv.org/abs/2210.02373v2</link><description>Neural networks have gained much interest because of their effectiveness inmany applications. However, their mathematical properties are generally notwell understood. If there is some underlying geometric structure inherent tothe data or to the function to approximate, it is often desirable to take thisinto account in the design of the neural network. In this work, we start with anon-autonomous ODE and build neural networks using a suitable,structure-preserving, numerical time-discretisation. The structure of theneural network is then inferred from the properties of the ODE vector field.Besides injecting more structure into the network architectures, this modellingprocedure allows a better theoretical understanding of their behaviour. Wepresent two universal approximation results and demonstrate how to impose someparticular properties on the neural networks. A particular focus is on1-Lipschitz architectures including layers that are not 1-Lipschitz. Thesenetworks are expressive and robust against adversarial attacks, as shown forthe CIFAR-10 and CIFAR-100 datasets.</description><author>Elena Celledoni, Davide Murari, Brynjulf Owren, Carola-Bibiane Sch√∂nlieb, Ferdia Sherry</author><pubDate>Thu, 31 Aug 2023 18:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.02373v2</guid></item><item><title>Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT</title><link>http://arxiv.org/abs/2307.11764v2</link><description>Large pre-trained language models have recently gained significant tractiondue to their improved performance on various down-stream tasks like textclassification and question answering, requiring only few epochs offine-tuning. However, their large model sizes often prohibit their applicationson resource-constrained edge devices. Existing solutions of yieldingparameter-efficient BERT models largely rely on compute-exhaustive training andfine-tuning. Moreover, they often rely on additional compute heavy models tomitigate the performance gap. In this paper, we present Sensi-BERT, asensitivity driven efficient fine-tuning of BERT models that can take anoff-the-shelf pre-trained BERT model and yield highly parameter-efficientmodels for downstream tasks. In particular, we perform sensitivity analysis torank each individual parameter tensor, that then is used to trim themaccordingly during fine-tuning for a given parameter or FLOPs budget. Ourexperiments show the efficacy of Sensi-BERT across different downstream tasksincluding MNLI, QQP, QNLI, SST-2 and SQuAD, showing better performance atsimilar or smaller parameter budget compared to various alternatives.</description><author>Souvik Kundu, Sharath Nittur Sridhar, Maciej Szankin, Sairam Sundaresan</author><pubDate>Thu, 31 Aug 2023 18:09:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11764v2</guid></item><item><title>Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images</title><link>http://arxiv.org/abs/2308.16863v1</link><description>Multiple Sclerosis (MS) is a severe neurological disease characterized byinflammatory lesions in the central nervous system. Hence, predictinginflammatory disease activity is crucial for disease assessment and treatment.However, MS lesions can occur throughout the brain and vary in shape, size andtotal count among patients. The high variance in lesion load and locationsmakes it challenging for machine learning methods to learn a globally effectiverepresentation of whole-brain MRI scans to assess and predict disease.Technically it is non-trivial to incorporate essential biomarkers such aslesion load or spatial proximity. Our work represents the first attempt toutilize graph neural networks (GNN) to aggregate these biomarkers for a novelglobal representation. We propose a two-stage MS inflammatory disease activityprediction approach. First, a 3D segmentation network detects lesions, and aself-supervised algorithm extracts their image features. Second, the detectedlesions are used to build a patient graph. The lesions act as nodes in thegraph and are initialized with image features extracted in the first stage.Finally, the lesions are connected based on their spatial proximity and theinflammatory disease activity prediction is formulated as a graphclassification task. Furthermore, we propose a self-pruning strategy toauto-select the most critical lesions for prediction. Our proposed methodoutperforms the existing baseline by a large margin (AUCs of 0.67 vs. 0.61 and0.66 vs. 0.60 for one-year and two-year inflammatory disease activity,respectively). Finally, our proposed method enjoys inherent explainability byassigning an importance score to each lesion for the overall prediction. Codeis available at https://github.com/chinmay5/ms_ida.git</description><author>Chinmay Prabhakar, Hongwei Bran Li, Johannes C. Paetzold, Timo Loehr, Chen Niu, Mark M√ºhlau, Daniel Rueckert, Benedikt Wiestler, Bjoern Menze</author><pubDate>Thu, 31 Aug 2023 18:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16863v1</guid></item><item><title>Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs</title><link>http://arxiv.org/abs/2308.16859v1</link><description>In this article, the optimal sample complexity of learning the underlyinginteraction/dependencies of a Linear Dynamical System (LDS) over a DirectedAcyclic Graph (DAG) is studied. The sample complexity of learning a DAG'sstructure is well-studied for static systems, where the samples of nodal statesare independent and identically distributed (i.i.d.). However, such a study isless explored for DAGs with dynamical systems, where the nodal states aretemporally correlated. We call such a DAG underlying an LDS as \emph{dynamical}DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics aredriven by unobserved exogenous noise sources that are wide-sense stationary(WSS) in time but are mutually uncorrelated, and have the same {power spectraldensity (PSD)}. Inspired by the static settings, a metric and an algorithmbased on the PSD matrix of the observed time series are proposed to reconstructthe DDAG. The equal noise PSD assumption can be relaxed such thatidentifiability conditions for DDAG reconstruction are not violated. For theLDS with WSS (sub) Gaussian exogenous noise sources, it is shown that theoptimal sample complexity (or length of state trajectory) needed to learn theDDAG is $n=\Theta(q\log(p/q))$, where $p$ is the number of nodes and $q$ is themaximum number of parents per node. To prove the sample complexity upper bound,a concentration bound for the PSD estimation is derived, under two differentsampling strategies. A matching min-max lower bound using generalized Fano'sinequality also is provided, thus showing the order optimality of the proposedalgorithm.</description><author>Mishfad Shaikh Veedu, Deepjyoti Deka, Murti V. Salapaka</author><pubDate>Thu, 31 Aug 2023 18:03:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16859v1</guid></item><item><title>Majorization-Minimization for sparse SVMs</title><link>http://arxiv.org/abs/2308.16858v1</link><description>Several decades ago, Support Vector Machines (SVMs) were introduced forperforming binary classification tasks, under a supervised framework. Nowadays,they often outperform other supervised methods and remain one of the mostpopular approaches in the machine learning arena. In this work, we investigatethe training of SVMs through a smooth sparse-promoting-regularized squaredhinge loss minimization. This choice paves the way to the application of quicktraining methods built on majorization-minimization approaches, benefiting fromthe Lipschitz differentiabililty of the loss function. Moreover, the proposedapproach allows us to handle sparsity-preserving regularizers promoting theselection of the most significant features, so enhancing the performance.Numerical tests and comparisons conducted on three different datasetsdemonstrate the good performance of the proposed methodology in terms ofqualitative metrics (accuracy, precision, recall, and F 1 score) as well ascomputational cost.</description><author>Alessandro Benfenati, Emilie Chouzenoux, Giorgia Franchini, Salla Latva-Aijo, Dominik Narnhofer, Jean-Christophe Pesquet, Sebastian J. Scott, Mahsa Yousefi</author><pubDate>Thu, 31 Aug 2023 18:03:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16858v1</guid></item><item><title>IoMT-Blockchain based Secured Remote Patient Monitoring Framework for Neuro-Stimulation Device</title><link>http://arxiv.org/abs/2308.16857v1</link><description>Biomedical Engineering's Internet of Medical Things (IoMT) is helping toimprove the accuracy, dependability, and productivity of electronic equipmentin the healthcare business. Real-time sensory data from patients may bedelivered and subsequently analyzed through rapid development of wearable IoMTdevices, such as neuro-stimulation devices with a range of functions. Data fromthe Internet of Things is gathered, analyzed, and stored in a single location.However, single-point failure, data manipulation, privacy difficulties, andother challenges might arise as a result of centralization. Due to itsdecentralized nature, blockchain (BC) can alleviate these issues. The viabilityof establishing a non-invasive remote neurostimulation system employingIoMT-based transcranial Direct Current Stimulation is investigated in this work(tDCS). A hardware-based prototype tDCS device has been developed that can beoperated over the internet using an android application. Our suggestedframework addresses the problems of IoMTBC-based systems, meets the criteria ofreal-time remote patient monitoring systems, and incorporates literature bestpractices in the relevant fields.</description><author>Md Sakib Ullah Sourav, Mohammad Sultan Mahmud, Md Simul Hasan Talukder, Rejwan Bin Sulaiman, Abdullah Yasin</author><pubDate>Thu, 31 Aug 2023 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16857v1</guid></item><item><title>Hypergraph Structure Inference From Data Under Smoothness Prior</title><link>http://arxiv.org/abs/2308.14172v2</link><description>Hypergraphs are important for processing data with higher-order relationshipsinvolving more than two entities. In scenarios where explicit hypergraphs arenot readily available, it is desirable to infer a meaningful hypergraphstructure from the node features to capture the intrinsic relations within thedata. However, existing methods either adopt simple pre-defined rules that failto precisely capture the distribution of the potential hypergraph structure, orlearn a mapping between hypergraph structures and node features but require alarge amount of labelled data, i.e., pre-existing hypergraph structures, fortraining. Both restrict their applications in practical scenarios. To fill thisgap, we propose a novel smoothness prior that enables us to design a method toinfer the probability for each potential hyperedge without labelled data assupervision. The proposed prior indicates features of nodes in a hyperedge arehighly correlated by the features of the hyperedge containing them. We use thisprior to derive the relation between the hypergraph structure and the nodefeatures via probabilistic modelling. This allows us to develop an unsupervisedinference method to estimate the probability for each potential hyperedge viasolving an optimisation problem that has an analytical solution. Experiments onboth synthetic and real-world data demonstrate that our method can learnmeaningful hypergraph structures from data more efficiently than existinghypergraph structure inference methods.</description><author>Bohan Tang, Siheng Chen, Xiaowen Dong</author><pubDate>Thu, 31 Aug 2023 17:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14172v2</guid></item><item><title>Humans in 4D: Reconstructing and Tracking Humans with Transformers</title><link>http://arxiv.org/abs/2305.20091v3</link><description>We present an approach to reconstruct humans and track them over time. At thecore of our approach, we propose a fully "transformerized" version of a networkfor human mesh recovery. This network, HMR 2.0, advances the state of the artand shows the capability to analyze unusual poses that have in the past beendifficult to reconstruct from single images. To analyze video, we use 3Dreconstructions from HMR 2.0 as input to a tracking system that operates in 3D.This enables us to deal with multiple people and maintain identities throughocclusion events. Our complete approach, 4DHumans, achieves state-of-the-artresults for tracking people from monocular video. Furthermore, we demonstratethe effectiveness of HMR 2.0 on the downstream task of action recognition,achieving significant improvements over previous pose-based action recognitionapproaches. Our code and models are available on the project website:https://shubham-goel.github.io/4dhumans/.</description><author>Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, Jitendra Malik</author><pubDate>Thu, 31 Aug 2023 17:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20091v3</guid></item><item><title>Speeding up Fourier Neural Operators via Mixed Precision</title><link>http://arxiv.org/abs/2307.15034v2</link><description>The Fourier neural operator (FNO) is a powerful technique for learningsurrogate maps for partial differential equation (PDE) solution operators. Formany real-world applications, which often require high-resolution data points,training time and memory usage are significant bottlenecks. While there aremixed-precision training techniques for standard neural networks, those workfor real-valued datatypes on finite dimensions and therefore cannot be directlyapplied to FNO, which crucially operates in the (complex-valued) Fourier domainand in function spaces. On the other hand, since the Fourier transform isalready an approximation (due to discretization error), we do not need toperform the operation at full precision. In this work, we (i) profile memoryand runtime for FNO with full and mixed-precision training, (ii) conduct astudy on the numerical stability of mixed-precision training of FNO, and (iii)devise a training routine which substantially decreases training time andmemory usage (up to 34%), with little or no reduction in accuracy, on theNavier-Stokes and Darcy flow equations. Combined with the recently proposedtensorized FNO (Kossaifi et al., 2023), the resulting model has far betterperformance while also being significantly faster than the original FNO.</description><author>Colin White, Renbo Tu, Jean Kossaifi, Gennady Pekhimenko, Kamyar Azizzadenesheli, Anima Anandkumar</author><pubDate>Thu, 31 Aug 2023 17:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15034v2</guid></item><item><title>Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications</title><link>http://arxiv.org/abs/2301.00752v3</link><description>This study demonstrates the feasibility of point cloud-based proactive linkquality prediction for millimeter-wave (mmWave) communications. Previousstudies have proposed machine learning-based methods to predict received signalstrength for future time periods using time series of depth images to mitigatethe line-of-sight (LOS) path blockage by pedestrians in mmWave communication.However, these image-based methods have limited applicability due to privacyconcerns as camera images may contain sensitive information. This studyproposes a point cloud-based method for mmWave link quality prediction anddemonstrates its feasibility through experiments. Point clouds representthree-dimensional (3D) spaces as a set of points and are sparser and lesslikely to contain sensitive information than camera images. Additionally, pointclouds provide 3D position and motion information, which is necessary forunderstanding the radio propagation environment involving pedestrians. Thisstudy designs the mmWave link quality prediction method and conducts realisticindoor experiments, where the link quality fluctuates significantly due tohuman blockage, using commercially available IEEE 802.11ad-based 60 GHzwireless LAN devices and Kinect v2 RGB-D camera and Velodyne VLP-16 lightdetection and ranging (LiDAR) for point cloud acquisition. The experimentalresults showed that our proposed method can predict future large attenuation ofmmWave received signal strength and throughput induced by the LOS path blockageby pedestrians with comparable or superior accuracy to image-based predictionmethods. Hence, our point cloud-based method can serve as a viable alternativeto image-based methods.</description><author>Shoki Ohta, Takayuki Nishio, Riichi Kudo, Kahoko Takahashi, Hisashi Nagata</author><pubDate>Thu, 31 Aug 2023 17:28:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00752v3</guid></item><item><title>Natural Quantum Monte Carlo Computation of Excited States</title><link>http://arxiv.org/abs/2308.16848v1</link><description>We present a variational Monte Carlo algorithm for estimating the lowestexcited states of a quantum system which is a natural generalization of theestimation of ground states. The method has no free parameters and requires noexplicit orthogonalization of the different states, instead transforming theproblem of finding excited states of a given system into that of finding theground state of an expanded system. Expected values of arbitrary observablescan be calculated, including off-diagonal expectations between different statessuch as the transition dipole moment. Although the method is entirely general,it works particularly well in conjunction with recent work on using neuralnetworks as variational Ansatze for many-electron systems, and we show that bycombining this method with the FermiNet and Psiformer Ansatze we can accuratelyrecover vertical excitation energies and oscillator strengths on molecules aslarge as benzene. Beyond the examples on molecules presented here, we expectthis technique will be of great interest for applications of variationalquantum Monte Carlo to atomic, nuclear and condensed matter physics.</description><author>David Pfau, Simon Axelrod, Halvard Sutterud, Ingrid von Glehn, James S. Spencer</author><pubDate>Thu, 31 Aug 2023 17:27:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16848v1</guid></item><item><title>Diffusion Models for Interferometric Satellite Aperture Radar</title><link>http://arxiv.org/abs/2308.16847v1</link><description>Probabilistic Diffusion Models (PDMs) have recently emerged as a verypromising class of generative models, achieving high performance in naturalimage generation. However, their performance relative to non-natural images,like radar-based satellite data, remains largely unknown. Generating largeamounts of synthetic (and especially labelled) satellite data is crucial toimplement deep-learning approaches for the processing and analysis of(interferometric) satellite aperture radar data. Here, we leverage PDMs togenerate several radar-based satellite image datasets. We show that PDMssucceed in generating images with complex and realistic structures, but thatsampling time remains an issue. Indeed, accelerated sampling strategies, whichwork well on simple image datasets like MNIST, fail on our radar datasets. Weprovide a simple and versatile open-sourcehttps://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample andevaluate PDMs using any dataset on a single GPU.</description><author>Alexandre Tuel, Thomas Kerdreux, Claudia Hulbert, Bertrand Rouet-Leduc</author><pubDate>Thu, 31 Aug 2023 17:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16847v1</guid></item><item><title>Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities</title><link>http://arxiv.org/abs/2302.08761v3</link><description>Traffic analysis is crucial for urban operations and planning, while theavailability of dense urban traffic data beyond loop detectors is still scarce.We present a large-scale floating vehicle dataset of per-street segment trafficinformation, Metropolitan Segment Traffic Speeds from Massive Floating Car Datain 10 Cities (MeTS-10), available for 10 global cities with a 15-minuteresolution for collection periods ranging between 108 and 361 days in 2019-2021and covering more than 1500 square kilometers per metropolitan area. MeTS-10features traffic speed information at all street levels from main arterials tolocal streets for Antwerp, Bangkok, Barcelona, Berlin, Chicago, Istanbul,London, Madrid, Melbourne and Moscow. The dataset leverages theindustrial-scale floating vehicle Traffic4cast data with speeds and vehiclecounts provided in a privacy-preserving spatio-temporal aggregation. We detailthe efficient matching approach mapping the data to the OpenStreetMap roadgraph. We evaluate the dataset by comparing it with publicly availablestationary vehicle detector data (for Berlin, London, and Madrid) and the Ubertraffic speed dataset (for Barcelona, Berlin, and London). The comparisonhighlights the differences across datasets in spatio-temporal coverage andvariations in the reported traffic caused by the binning method. MeTS-10enables novel, city-wide analysis of mobility and traffic patterns for tenmajor world cities, overcoming current limitations of spatially sparse vehicledetector data. The large spatial and temporal coverage offers an opportunityfor joining the MeTS-10 with other datasets, such as traffic surveys in trafficplanning studies or vehicle detector data in traffic control settings.</description><author>Moritz Neun, Christian Eichenberger, Yanan Xin, Cheng Fu, Nina Wiedemann, Henry Martin, Martin Tomko, Lukas Amb√ºhl, Luca Hermes, Michael Kopp</author><pubDate>Thu, 31 Aug 2023 17:21:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08761v3</guid></item><item><title>Neural Mixed Effects for Nonlinear Personalized Predictions</title><link>http://arxiv.org/abs/2306.08149v3</link><description>Personalized prediction is a machine learning approach that predicts aperson's future observations based on their past labeled observations and istypically used for sequential tasks, e.g., to predict daily mood ratings. Whenmaking personalized predictions, a model can combine two types of trends: (a)trends shared across people, i.e., person-generic trends, such as being happieron weekends, and (b) unique trends for each person, i.e., person-specifictrends, such as a stressful weekly meeting. Mixed effect models are popularstatistical models to study both trends by combining person-generic andperson-specific parameters. Though linear mixed effect models are gainingpopularity in machine learning by integrating them with neural networks, theseintegrations are currently limited to linear person-specific parameters: rulingout nonlinear person-specific trends. In this paper, we propose Neural MixedEffect (NME) models to optimize nonlinear person-specific parameters anywherein a neural network in a scalable manner. NME combines the efficiency of neuralnetwork optimization with nonlinear mixed effects modeling. Empirically, weobserve that NME improves performance across six unimodal and multimodaldatasets, including a smartphone dataset to predict daily mood and amother-adolescent dataset to predict affective state sequences where half themothers experience at least moderate symptoms of depression. Furthermore, weevaluate NME for two model architectures, including for neural conditionalrandom fields (CRF) to predict affective state sequences where the CRF learnsnonlinear person-specific temporal transitions between affective states.Analysis of these person-specific transitions on the mother-adolescent datasetshows interpretable trends related to the mother's depression symptoms.</description><author>Torsten W√∂rtwein, Nicholas Allen, Lisa B. Sheeber, Randy P. Auerbach, Jeffrey F. Cohn, Louis-Philippe Morency</author><pubDate>Thu, 31 Aug 2023 17:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08149v3</guid></item><item><title>Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information</title><link>http://arxiv.org/abs/2308.16836v1</link><description>This paper presents an end-to-end high-quality singing voice synthesis (SVS)system that uses bidirectional encoder representation from Transformers (BERT)derived semantic embeddings to improve the expressiveness of the synthesizedsinging voice. Based on the main architecture of recently proposed VISinger, weput forward several specific designs for expressive singing voice synthesis.First, different from the previous SVS models, we use text representation oflyrics extracted from pre-trained BERT as additional input to the model. Therepresentation contains information about semantics of the lyrics, which couldhelp SVS system produce more expressive and natural voice. Second, we furtherintroduce an energy predictor to stabilize the synthesized voice and model thewider range of energy variations that also contribute to the expressiveness ofsinging voice. Last but not the least, to attenuate the off-key issues, thepitch predictor is re-designed to predict the real to note pitch ratio. Bothobjective and subjective experimental results indicate that the proposed SVSsystem can produce singing voice with higher-quality outperforming VISinger.</description><author>Shaohuan Zhou, Shun Lei, Weiya You, Deyi Tuo, Yuren You, Zhiyong Wu, Shiyin Kang, Helen Meng</author><pubDate>Thu, 31 Aug 2023 17:12:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16836v1</guid></item><item><title>FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout</title><link>http://arxiv.org/abs/2308.16835v1</link><description>Federated Learning (FL) requires frequent exchange of model parameters, whichleads to long communication delay, especially when the network environments ofclients vary greatly. Moreover, the parameter server needs to wait for theslowest client (i.e., straggler, which may have the largest model size, lowestcomputing capability or worst network condition) to upload parameters, whichmay significantly degrade the communication efficiency. Commonly-used clientselection methods such as partial client selection would lead to the waste ofcomputing resources and weaken the generalization of the global model. Totackle this problem, along a different line, in this paper, we advocate theapproach of model parameter dropout instead of client selection, andaccordingly propose a novel framework of Federated learning scheme withDifferential parameter Dropout (FedDD). FedDD consists of two key modules:dropout rate allocation and uploaded parameter selection, which will optimizethe model parameter uploading ratios tailored to different clients'heterogeneous conditions and also select the proper set of important modelparameters for uploading subject to clients' dropout rate constraints.Specifically, the dropout rate allocation is formulated as a convexoptimization problem, taking system heterogeneity, data heterogeneity, andmodel heterogeneity among clients into consideration. The uploaded parameterselection strategy prioritizes on eliciting important parameters for uploadingto speedup convergence. Furthermore, we theoretically analyze the convergenceof the proposed FedDD scheme. Extensive performance evaluations demonstratethat the proposed FedDD scheme can achieve outstanding performances in bothcommunication efficiency and model convergence, and also possesses a stronggeneralization capability to data of rare classes.</description><author>Zhiying Feng, Xu Chen, Qiong Wu, Wen Wu, Xiaoxi Zhang, Qianyi Huang</author><pubDate>Thu, 31 Aug 2023 17:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16835v1</guid></item><item><title>Neural ShDF: Reviving an Efficient and Consistent Mesh Segmentation Method</title><link>http://arxiv.org/abs/2306.11737v2</link><description>Partitioning a polygonal mesh into meaningful parts can be challenging. Manyapplications require decomposing such structures for further processing incomputer graphics. In the last decade, several methods were proposed to tacklethis problem, at the cost of intensive computational times. Recently, machinelearning has proven to be effective for the segmentation task on 3D structures.Nevertheless, these state-of-the-art methods are often hardly generalizable andrequire dividing the learned model into several specific classes of objects toavoid overfitting. We present a data-driven approach leveraging deep learningto encode a mapping function prior to mesh segmentation for multipleapplications. Our network reproduces a neighborhood map using our knowledge ofthe \textsl{Shape Diameter Function} (SDF) method using similarities amongvertex neighborhoods. Our approach is resolution-agnostic as we downsample theinput meshes and query the full-resolution structure solely for neighborhoodcontributions. Using our predicted SDF values, we can inject the resultingstructure into a graph-cut algorithm to generate an efficient and robust meshsegmentation while considerably reducing the required computation times.</description><author>Bruno Roy</author><pubDate>Thu, 31 Aug 2023 16:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11737v2</guid></item><item><title>Coarse-to-Fine Amodal Segmentation with Shape Prior</title><link>http://arxiv.org/abs/2308.16825v1</link><description>Amodal object segmentation is a challenging task that involves segmentingboth visible and occluded parts of an object. In this paper, we propose a novelapproach, called Coarse-to-Fine Segmentation (C2F-Seg), that addresses thisproblem by progressively modeling the amodal segmentation. C2F-Seg initiallyreduces the learning space from the pixel-level image space to thevector-quantized latent space. This enables us to better handle long-rangedependencies and learn a coarse-grained amodal segment from visual features andvisible segments. However, this latent space lacks detailed information aboutthe object, which makes it difficult to provide a precise segmentationdirectly. To address this issue, we propose a convolution refine module toinject fine-grained information and provide a more precise amodal objectsegmentation based on visual features and coarse-predicted segmentation. Tohelp the studies of amodal object segmentation, we create a synthetic amodaldataset, named as MOViD-Amodal (MOViD-A), which can be used for both image andvideo amodal object segmentation. We extensively evaluate our model on twobenchmark datasets: KINS and COCO-A. Our empirical results demonstrate thesuperiority of C2F-Seg. Moreover, we exhibit the potential of our approach forvideo amodal object segmentation tasks on FISHBOWL and our proposed MOViD-A.Project page at: http://jianxgao.github.io/C2F-Seg.</description><author>Jianxiong Gao, Xuelin Qian, Yikai Wang, Tianjun Xiao, Tong He, Zheng Zhang, Yanwei Fu</author><pubDate>Thu, 31 Aug 2023 16:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16825v1</guid></item><item><title>Can Programming Languages Boost Each Other via Instruction Tuning?</title><link>http://arxiv.org/abs/2308.16824v1</link><description>When human programmers have mastered a programming language, it would beeasier when they learn a new programming language. In this report, we focus onexploring whether programming languages can boost each other during theinstruction fine-tuning phase of code large language models. We conductextensive experiments of 8 popular programming languages (Python, JavaScript,TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate thatprogramming languages can significantly improve each other. For example,CodeM-Python 15B trained on Python is able to increase Java by an absolute17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7Btrained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Ourtraining data is released at https://github.com/NL2Code/CodeM.</description><author>Daoguang Zan, Ailun Yu, Bo Shen, Jiaxin Zhang, Taihong Chen, Bing Geng, Bei Chen, Jichuan Ji, Yafen Yao, Yongji Wang, Qianxiang Wang</author><pubDate>Thu, 31 Aug 2023 16:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16824v1</guid></item><item><title>Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets</title><link>http://arxiv.org/abs/2308.16822v1</link><description>Multi-output Gaussian processes (MOGPs) have been introduced to deal withmultiple tasks by exploiting the correlations between different outputs.Generally, MOGPs models assume a flat correlation structure between theoutputs. However, such a formulation does not account for more elaboraterelationships, for instance, if several replicates were observed for eachoutput (which is a typical setting in biological experiments). This paperproposes an extension of MOGPs for hierarchical datasets (i.e. datasets forwhich the relationships between observations can be represented within a treestructure). Our model defines a tailored kernel function accounting forhierarchical structures in the data to capture different levels of correlationswhile leveraging the introduction of latent variables to express the underlyingdependencies between outputs through a dedicated kernel. This latter feature isexpected to significantly improve scalability as the number of tasks increases.An extensive experimental study involving both synthetic and real-world datafrom genomics and motion capture is proposed to support our claims.</description><author>Chunchao Ma, Arthur Leroy, Mauricio Alvarez</author><pubDate>Thu, 31 Aug 2023 16:52:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16822v1</guid></item><item><title>BTSeg: Barlow Twins Regularization for Domain Adaptation in Semantic Segmentation</title><link>http://arxiv.org/abs/2308.16819v1</link><description>Semantic image segmentation is a critical component in many computer visionsystems, such as autonomous driving. In such applications, adverse conditions(heavy rain, night time, snow, extreme lighting) on the one hand pose specificchallenges, yet are typically underrepresented in the available datasets.Generating more training data is cumbersome and expensive, and the processitself is error-prone due to the inherent aleatoric uncertainty. To addressthis challenging problem, we propose BTSeg, which exploits image-levelcorrespondences as weak supervision signal to learn a segmentation model thatis agnostic to adverse conditions. To this end, our approach uses the Barlowtwins loss from the field of unsupervised learning and treats images taken atthe same location but under different adverse conditions as "augmentations" ofthe same unknown underlying base image. This allows the training of asegmentation model that is robust to appearance changes introduced by differentadverse conditions. We evaluate our approach on ACDC and the new challengingACG benchmark to demonstrate its robustness and generalization capabilities.Our approach performs favorably when compared to the current state-of-the-artmethods, while also being simpler to implement and train. The code will bereleased upon acceptance.</description><author>Johannes K√ºnzel, Anna Hilsmann, Peter Eisert</author><pubDate>Thu, 31 Aug 2023 16:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16819v1</guid></item><item><title>Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network</title><link>http://arxiv.org/abs/2308.16818v1</link><description>Accurate traffic forecasting at intersections governed by intelligent trafficsignals is critical for the advancement of an effective intelligent trafficsignal control system. However, due to the irregular traffic time seriesproduced by intelligent intersections, the traffic forecasting task becomesmuch more intractable and imposes three major new challenges: 1) asynchronousspatial dependency, 2) irregular temporal dependency among traffic data, and 3)variable-length sequence to be predicted, which severely impede the performanceof current traffic forecasting methods. To this end, we propose an AsynchronousSpatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the trafficstates of the lanes entering intelligent intersections in a future time window.Specifically, by linking lanes via a traffic diffusion graph, we first proposean Asynchronous Graph Diffusion Network to model the asynchronous spatialdependency between the time-misaligned traffic state measurements of lanes.After that, to capture the temporal dependency within irregular traffic statesequence, a learnable personalized time encoding is devised to embed thecontinuous time for each lane. Then we propose a Transformable Time-awareConvolution Network that learns meta-filters to derive time-aware convolutionfilters with transformable filter sizes for efficient temporal convolution onthe irregular sequence. Furthermore, a Semi-Autoregressive Prediction Networkconsisting of a state evolution unit and a semiautoregressive predictor isdesigned to effectively and efficiently predict variable-length traffic statesequences. Extensive experiments on two real-world datasets demonstrate theeffectiveness of ASeer in six metrics.</description><author>Weijia Zhang, Le Zhang, Jindong Han, Hao Liu, Jingbo Zhou, Yu Mei, Hui Xiong</author><pubDate>Thu, 31 Aug 2023 16:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16818v1</guid></item><item><title>Approximate Bayesian inference from noisy likelihoods with Gaussian process emulated MCMC</title><link>http://arxiv.org/abs/2104.03942v2</link><description>We present a framework for approximate Bayesian inference when only a limitednumber of noisy log-likelihood evaluations can be obtained due to computationalconstraints, which is becoming increasingly common for applications of complexmodels. We model the log-likelihood function using a Gaussian process (GP) andthe main methodological innovation is to apply this model to emulate theprogression that an exact Metropolis-Hastings (MH) sampler would take if it wasapplicable. Informative log-likelihood evaluation locations are selected usinga sequential experimental design strategy until the MH accept/reject decisionis done accurately enough according to the GP model. The resulting approximatesampler is conceptually simple and sample-efficient. It is also more robust toviolations of GP modelling assumptions compared with earlier, related "Bayesianoptimisation-like" methods tailored for Bayesian inference. We discuss sometheoretical aspects and various interpretations of the resulting approximate MHsampler, and demonstrate its benefits in the context of Bayesian andgeneralised Bayesian likelihood-free inference for simulator-based statisticalmodels.</description><author>Marko J√§rvenp√§√§, Jukka Corander</author><pubDate>Thu, 31 Aug 2023 16:46:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.03942v2</guid></item><item><title>Deanthropomorphising NLP: Can a Language Model Be Conscious?</title><link>http://arxiv.org/abs/2211.11483v3</link><description>This work is intended as a voice in the discussion over previous claims thata pretrained large language model (LLM) based on the Transformer modelarchitecture can be sentient. Such claims have been made concerning the LaMDAmodel and also concerning the current wave of LLM-powered chatbots, such asChatGPT. This claim, if confirmed, would have serious ramifications in theNatural Language Processing (NLP) community due to wide-spread use of similarmodels. However, here we take the position that such a large language modelcannot be sentient, or conscious, and that LaMDA in particular exhibits noadvances over other similar models that would qualify it. We justify this byanalysing the Transformer architecture through Integrated Information Theory ofconsciousness. We see the claims of sentience as part of a wider tendency touse anthropomorphic language in NLP reporting. Regardless of the veracity ofthe claims, we consider this an opportune moment to take stock of progress inlanguage modelling and consider the ethical implications of the task. In orderto make this work helpful for readers outside the NLP community, we alsopresent the necessary background in language modelling.</description><author>Matthew Shardlow, Piotr Przyby≈Ça</author><pubDate>Thu, 31 Aug 2023 16:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11483v3</guid></item><item><title>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media</title><link>http://arxiv.org/abs/2307.09312v2</link><description>We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modalgraph-based transformer model for detecting hate speech in online socialnetworks, such as Reddit discussions. In contrast to traditional comment-onlymethods, our approach to labelling a comment as hate speech involves a holisticanalysis of text and images grounded in the discussion context. This is done byleveraging graph transformers to capture the contextual relationships in theentire discussion surrounding a comment and grounding the interwoven fusionlayers that combine individual comments' text and image embeddings instead ofprocessing modalities separately. We compare the performance of our model tobaselines that only process individual comments and conduct extensive ablationstudies. To evaluate our work, we present a new dataset, HatefulDiscussions,comprising complete multi-modal discussions from multiple online communities onReddit. We conclude with future work for multimodal solutions to deliver socialvalue in online contexts, arguing that capturing a holistic view of aconversation significantly advances the effort to detect anti-social behaviour.</description><author>Liam Hebert, Gaurav Sahu, Yuxuan Guo, Nanda Kishore Sreenivas, Lukasz Golab, Robin Cohen</author><pubDate>Thu, 31 Aug 2023 16:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09312v2</guid></item><item><title>RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation</title><link>http://arxiv.org/abs/2308.15975v2</link><description>For robots to be useful outside labs and specialized factories we need a wayto teach them new useful behaviors quickly. Current approaches lack either thegenerality to onboard new tasks without task-specific engineering, or else lackthe data-efficiency to do so in an amount of time that enables practical use.In this work we explore dense tracking as a representational vehicle to allowfaster and more general learning from demonstration. Our approach utilizesTrack-Any-Point (TAP) models to isolate the relevant motion in a demonstration,and parameterize a low-level controller to reproduce this motion across changesin the scene configuration. We show this results in robust robot policies thatcan solve complex object-arrangement tasks such as shape-matching, stacking,and even full path-following tasks such as applying glue and sticking objectstogether, all from demonstrations that can be collected in minutes.</description><author>Mel Vecerik, Carl Doersch, Yi Yang, Todor Davchev, Yusuf Aytar, Guangyao Zhou, Raia Hadsell, Lourdes Agapito, Jon Scholz</author><pubDate>Thu, 31 Aug 2023 16:29:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15975v2</guid></item><item><title>Multiscale Residual Learning of Graph Convolutional Sequence Chunks for Human Motion Prediction</title><link>http://arxiv.org/abs/2308.16801v1</link><description>A new method is proposed for human motion prediction by learning temporal andspatial dependencies. Recently, multiscale graphs have been developed to modelthe human body at higher abstraction levels, resulting in more stable motionprediction. Current methods however predetermine scale levels and combinespatially proximal joints to generate coarser scales based on human priors,even though movement patterns in different motion sequences vary and do notfully comply with a fixed graph of spatially connected joints. Another problemwith graph convolutional methods is mode collapse, in which predicted posesconverge around a mean pose with no discernible movements, particularly inlong-term predictions. To tackle these issues, we propose ResChunk, anend-to-end network which explores dynamically correlated body components basedon the pairwise relationships between all joints in individual sequences.ResChunk is trained to learn the residuals between target sequence chunks in anautoregressive manner to enforce the temporal connectivities betweenconsecutive chunks. It is hence a sequence-to-sequence prediction network whichconsiders dynamic spatio-temporal features of sequences at multiple levels. Ourexperiments on two challenging benchmark datasets, CMU Mocap and Human3.6M,demonstrate that our proposed method is able to effectively model the sequenceinformation for motion prediction and outperform other techniques to set a newstate-of-the-art. Our code is available athttps://github.com/MohsenZand/ResChunk.</description><author>Mohsen Zand, Ali Etemad, Michael Greenspan</author><pubDate>Thu, 31 Aug 2023 16:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16801v1</guid></item><item><title>Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks</title><link>http://arxiv.org/abs/2308.16800v1</link><description>Our study reveals new theoretical insights into over-smoothing and featureover-correlation in deep graph neural networks. We show the prevalence ofinvariant subspaces, demonstrating a fixed relative behavior that is unaffectedby feature transformations. Our work clarifies recent observations related toconvergence to a constant state and a potential over-separation of node states,as the amplification of subspaces only depends on the spectrum of theaggregation function. In linear scenarios, this leads to node representationsbeing dominated by a low-dimensional subspace with an asymptotic convergencerate independent of the feature transformations. This causes a rank collapse ofthe node representations, resulting in over-smoothing when smooth vectors spanthis subspace, and over-correlation even when over-smoothing is avoided. Guidedby our theory, we propose a sum of Kronecker products as a beneficial propertythat can provably prevent over-smoothing, over-correlation, and rank collapse.We empirically extend our insights to the non-linear case, demonstrating theinability of existing models to capture linearly independent features.</description><author>Andreas Roth, Thomas Liebig</author><pubDate>Thu, 31 Aug 2023 16:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16800v1</guid></item><item><title>Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation</title><link>http://arxiv.org/abs/2308.16797v1</link><description>Despite significant research effort in the development of automatic dialogueevaluation metrics, little thought is given to evaluating dialogues other thanin English. At the same time, ensuring metrics are invariant to semanticallysimilar responses is also an overlooked topic. In order to achieve the desiredproperties of robustness and multilinguality for dialogue evaluation metrics,we propose a novel framework that takes advantage of the strengths of currentevaluation models with the newly-established paradigm of prompting LargeLanguage Models (LLMs). Empirical results show our framework achieves state ofthe art results in terms of mean Spearman correlation scores across severalbenchmarks and ranks first place on both the Robust and Multilingual tasks ofthe DSTC11 Track 4 "Automatic Evaluation Metrics for Open-Domain DialogueSystems", proving the evaluation capabilities of prompted LLMs.</description><author>John Mendon√ßa, Patr√≠cia Pereira, Jo√£o Paulo Carvalho, Alon Lavie, Isabel Trancoso</author><pubDate>Thu, 31 Aug 2023 16:19:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16797v1</guid></item><item><title>Towards Multilingual Automatic Dialogue Evaluation</title><link>http://arxiv.org/abs/2308.16795v1</link><description>The main limiting factor in the development of robust multilingual dialogueevaluation metrics is the lack of multilingual data and the limitedavailability of open sourced multilingual dialogue systems. In this work, wepropose a workaround for this lack of data by leveraging a strong multilingualpretrained LLM and augmenting existing English dialogue data using MachineTranslation. We empirically show that the naive approach of finetuning apretrained multilingual encoder model with translated data is insufficient tooutperform the strong baseline of finetuning a multilingual model with onlysource data. Instead, the best approach consists in the careful curation oftranslated data using MT Quality Estimation metrics, excluding low qualitytranslations that hinder its performance.</description><author>John Mendon√ßa, Alon Lavie, Isabel Trancoso</author><pubDate>Thu, 31 Aug 2023 16:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16795v1</guid></item><item><title>Biclustering Methods via Sparse Penalty</title><link>http://arxiv.org/abs/2308.14388v2</link><description>In this paper, we first reviewed several biclustering methods that are usedto identify the most significant clusters in gene expression data. Here wemainly focused on the SSVD(sparse SVD) method and tried a new sparse penaltynamed "Prenet penalty" which has been used only in factor analysis to gainsparsity. Then in the simulation study, we tried different types of generateddatasets (with different sparsity and dimension) and tried 1-layerapproximation then for k-layers which shows the mixed Prenet penalty is veryeffective for non-overlapped data. Finally, we used some real gene expressiondata to show the behavior of our methods.</description><author>Jiqiang Wang</author><pubDate>Thu, 31 Aug 2023 16:13:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14388v2</guid></item><item><title>Combining Inductive and Deductive Reasoning for Query Answering over Incomplete Knowledge Graphs</title><link>http://arxiv.org/abs/2106.14052v2</link><description>Current methods for embedding-based query answering over incomplete KnowledgeGraphs (KGs) only focus on inductive reasoning, i.e., predicting answers bylearning patterns from the data, and lack the complementary ability to dodeductive reasoning, which requires the application of domain knowledge toinfer further information. To address this shortcoming, we investigate theproblem of incorporating ontologies into embedding-based query answering modelsby defining the task of embedding-based ontology-mediated query answering. Wepropose various integration strategies into prominent representatives ofembedding models that involve (1) different ontology-driven data augmentationtechniques and (2) adaptation of the loss function to enforce the ontologyaxioms. We design novel benchmarks for the considered task based on the LUBMand the NELL KGs and evaluate our methods on them. The achieved improvements inthe setting that requires both inductive and deductive reasoning are from 20%to 55% in HITS@3.</description><author>Medina Andresel, Trung-Kien Tran, Csaba Domokos, Pasquale Minervini, Daria Stepanova</author><pubDate>Thu, 31 Aug 2023 16:10:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.14052v2</guid></item><item><title>Joint Semantic-Native Communication and Inference via Minimal Simplicial Structures</title><link>http://arxiv.org/abs/2308.16789v1</link><description>In this work, we study the problem of semantic communication and inference,in which a student agent (i.e. mobile device) queries a teacher agent (i.e.cloud sever) to generate higher-order data semantics living in a simplicialcomplex. Specifically, the teacher first maps its data into a k-ordersimplicial complex and learns its high-order correlations. For effectivecommunication and inference, the teacher seeks minimally sufficient andinvariant semantic structures prior to conveying information. These minimalsimplicial structures are found via judiciously removing simplices selected bythe Hodge Laplacians without compromising the inference query accuracy.Subsequently, the student locally runs its own set of queries based on a maskedsimplicial convolutional autoencoder (SCAE) leveraging both local and remoteteacher's knowledge. Numerical results corroborate the effectiveness of theproposed approach in terms of improving inference query accuracy underdifferent channel conditions and simplicial structures. Experiments on acoauthorship dataset show that removing simplices by ranking the Laplacianvalues yields a 85% reduction in payload size without sacrificing accuracy.Joint semantic communication and inference by masked SCAE improves queryaccuracy by 25% compared to local student based query and 15% compared toremote teacher based query. Finally, incorporating channel semantics is shownto effectively improve inference accuracy, notably at low SNR values.</description><author>Qiyang Zhao, Hang Zou, Mehdi Bennis, Merouane Debbah, Ebtesam Almazrouei, Faouzi Bader</author><pubDate>Thu, 31 Aug 2023 16:04:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16789v1</guid></item><item><title>Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming</title><link>http://arxiv.org/abs/2308.16785v1</link><description>The rapid advancements in artificial intelligence (AI) have led to a growingtrend of human-AI teaming (HAT) in various fields. As machines continue toevolve from mere automation to a state of autonomy, they are increasinglyexhibiting unexpected behaviors and human-like cognitive/intelligentcapabilities, including situation awareness (SA). This shift has the potentialto enhance the performance of mixed human-AI teams over all-human teams,underscoring the need for a better understanding of the dynamic SA interactionsbetween humans and machines. To this end, we provide a review of leading SAtheoretical models and a new framework for SA in the HAT context based on thekey features and processes of HAT. The Agent Teaming Situation Awareness (ATSA)framework unifies human and AI behavior, and involves bidirectional, anddynamic interaction. The framework is based on the individual and team SAmodels and elaborates on the cognitive mechanisms for modeling HAT. Similarperceptual cycles are adopted for the individual (including both human and AI)and the whole team, which is tailored to the unique requirements of the HATcontext. ATSA emphasizes cohesive and effective HAT through structures andcomponents, including teaming understanding, teaming control, and the world, aswell as adhesive transactive part. We further propose several future researchdirections to expand on the distinctive contributions of ATSA and address thespecific and pressing next steps.</description><author>Qi Gao, Wei Xu, Mowei Shen, Zaifeng Gao</author><pubDate>Thu, 31 Aug 2023 16:02:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16785v1</guid></item><item><title>StratMed: Relevance Stratification for Low-resource Medication Recommendation</title><link>http://arxiv.org/abs/2308.16781v1</link><description>With the growing imbalance between limited medical resources and escalatingdemands, AI-based clinical tasks have become paramount. Medicationrecommendation, as a sub-domain, aims to amalgamate longitudinal patienthistory with medical knowledge, assisting physicians in prescribing safer andmore accurate medication combinations. Existing methods overlook the inherentlong-tail distribution in medical data, lacking balanced representation betweenhead and tail data, which leads to sub-optimal model performance. To addressthis challenge, we introduce StratMed, a model that incorporates an innovativerelevance stratification mechanism. It harmonizes discrepancies in datalong-tail distribution and strikes a balance between the safety and accuracy ofmedication combinations. Specifically, we first construct a pre-training methodusing deep learning networks to obtain entity representation. After that, wedesign a pyramid-like data stratification method to obtain more generalizedentity relationships by reinforcing the features of unpopular entities. Basedon this relationship, we designed two graph structures to express medicationprecision and safety at the same level to obtain visit representations.Finally, the patient's historical clinical information is fitted to generatemedication combinations for the current health condition. Experiments on theMIMIC-III dataset demonstrate that our method has outperformed currentstate-of-the-art methods in four evaluation metrics (including safety andaccuracy).</description><author>Xiang Li</author><pubDate>Thu, 31 Aug 2023 15:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16781v1</guid></item><item><title>Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models</title><link>http://arxiv.org/abs/2308.16777v1</link><description>Zero-shot referring image segmentation is a challenging task because it aimsto find an instance segmentation mask based on the given referringdescriptions, without training on this type of paired data. Current zero-shotmethods mainly focus on using pre-trained discriminative models (e.g., CLIP).However, we have observed that generative models (e.g., Stable Diffusion) havepotentially understood the relationships between various visual elements andtext descriptions, which are rarely investigated in this task. In this work, weintroduce a novel Referring Diffusional segmentor (Ref-Diff) for this task,which leverages the fine-grained multi-modal information from generativemodels. We demonstrate that without a proposal generator, a generative modelalone can achieve comparable performance to existing SOTA weakly-supervisedmodels. When we combine both generative and discriminative models, our Ref-Diffoutperforms these competing methods by a significant margin. This indicatesthat generative models are also beneficial for this task and can complementdiscriminative models for better referring segmentation. Our code is publiclyavailable at https://github.com/kodenii/Ref-Diff.</description><author>Minheng Ni, Yabo Zhang, Kailai Feng, Xiaoming Li, Yiwen Guo, Wangmeng Zuo</author><pubDate>Thu, 31 Aug 2023 15:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16777v1</guid></item><item><title>Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm</title><link>http://arxiv.org/abs/2308.16775v1</link><description>In prediction-based Neural Architecture Search (NAS), performance indicatorsderived from graph convolutional networks have shown significant success. Theseindicators, achieved by representing feed-forward structures as componentgraphs through one-hot encoding, face a limitation: their inability to evaluatearchitecture performance across varying search spaces. In contrast, handcraftedperformance indicators (zero-shot NAS), which use the same architecture withrandom initialization, can generalize across multiple search spaces. Addressingthis limitation, we propose a novel approach for zero-shot NAS using deeplearning. Our method employs Fourier sum of sines encoding for convolutionalkernels, enabling the construction of a computational feed-forward graph with astructure similar to the architecture under evaluation. These encodings arelearnable and offer a comprehensive view of the architecture's topologicalinformation. An accompanying multi-layer perceptron (MLP) then ranks thesearchitectures based on their encodings. Experimental results show that ourapproach surpasses previous methods using graph convolutional networks in termsof correlation on the NAS-Bench-201 dataset and exhibits a higher convergencerate. Moreover, our extracted feature representation trained on eachNAS-Benchmark is transferable to other NAS-Benchmarks, showing promisinggeneralizability across multiple search spaces. The code is available at:https://github.com/minh1409/DFT-NPZS-NAS</description><author>Minh Le, Nhan Nguyen, Ngoc Hoang Luong</author><pubDate>Thu, 31 Aug 2023 15:54:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16775v1</guid></item><item><title>Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings</title><link>http://arxiv.org/abs/2306.17670v2</link><description>Spiking Neural Networks (SNNs) are a promising research direction forbuilding power-efficient information processing systems, especially fortemporal tasks such as speech recognition. In SNNs, delays refer to the timeneeded for one spike to travel from one neuron to another. These delays matterbecause they influence the spike arrival times, and it is well-known thatspiking neurons respond more strongly to coincident input spikes. Moreformally, it has been shown theoretically that plastic delays greatly increasethe expressivity in SNNs. Yet, efficient algorithms to learn these delays havebeen lacking. Here, we propose a new discrete-time algorithm that addressesthis issue in deep feedforward SNNs using backpropagation, in an offlinemanner. To simulate delays between consecutive layers, we use 1D convolutionsacross time. The kernels contain only a few non-zero weights - one per synapse- whose positions correspond to the delays. These positions are learnedtogether with the weights using the recently proposed Dilated Convolution withLearnable Spacings (DCLS). We evaluated our method on three datasets: theSpiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC) and itsnon-spiking version Google Speech Commands v0.02 (GSC) benchmarks, whichrequire detecting temporal patterns. We used feedforward SNNs with two or threehidden fully connected layers, and vanilla leaky integrate-and fire neurons. Weshowed that fixed random delays help and that learning them helps even more.Furthermore, our method outperformed the state-of-the-art in the three datasetswithout using recurrent connections and with substantially fewer parameters.Our work demonstrates the potential of delay learning in developing accurateand precise models for temporal data processing. Our code is based on PyTorch /SpikingJelly and available at: https://github.com/Thvnvtos/SNN-delays</description><author>Ilyass Hammouamri, Ismail Khalfaoui-Hassani, Timoth√©e Masquelier</author><pubDate>Thu, 31 Aug 2023 15:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17670v2</guid></item><item><title>Enhancing PLM Performance on Labour Market Tasks via Instruction-based Finetuning and Prompt-tuning with Rules</title><link>http://arxiv.org/abs/2308.16770v1</link><description>The increased digitization of the labour market has given researchers,educators, and companies the means to analyze and better understand the labourmarket. However, labour market resources, although available in high volumes,tend to be unstructured, and as such, research towards methodologies for theidentification, linking, and extraction of entities becomes more and moreimportant. Against the backdrop of this quest for better labour marketrepresentations, resource constraints and the unavailability of large-scaleannotated data cause a reliance on human domain experts. We demonstrate theeffectiveness of prompt-based tuning of pre-trained language models (PLM) inlabour market specific applications. Our results indicate that cost-efficientmethods such as PTR and instruction tuning without exemplars can significantlyincrease the performance of PLMs on downstream labour market applicationswithout introducing additional model layers, manual annotations, and dataaugmentation.</description><author>Jarno Vrolijk, David Graus</author><pubDate>Thu, 31 Aug 2023 15:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16770v1</guid></item><item><title>Towards Low-Barrier Cybersecurity Research and Education for Industrial Control Systems</title><link>http://arxiv.org/abs/2308.16769v1</link><description>The protection of Industrial Control Systems (ICS) that are employed inpublic critical infrastructures is of utmost importance due to catastrophicphysical damages cyberattacks may cause. The research community requirestestbeds for validation and comparing various intrusion detection algorithms toprotect ICS. However, there exist high barriers to entry for research andeducation in the ICS cybersecurity domain due to expensive hardware, software,and inherent dangers of manipulating real-world systems. To close the gap,built upon recently developed 3D high-fidelity simulators, we further showcaseour integrated framework to automatically launch cyberattacks, collect data,train machine learning models, and evaluate for practical chemical andmanufacturing processes. On our testbed, we validate our proposed intrusiondetection model called Minimal Threshold and Window SVM (MinTWin SVM) thatutilizes unsupervised machine learning via a one-class SVM in combination witha sliding window and classification threshold. Results show that MinTWin SVMminimizes false positives and is responsive to physical process anomalies.Furthermore, we incorporate our framework with ICS cybersecurity education byusing our dataset in an undergraduate machine learning course where studentsgain hands-on experience in practicing machine learning theory with a practicalICS dataset. All of our implementations have been open-sourced.</description><author>Colman McGuan, Chansu Yu, Qin Lin</author><pubDate>Thu, 31 Aug 2023 15:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16769v1</guid></item><item><title>Transformers Meet Directed Graphs</title><link>http://arxiv.org/abs/2302.00049v3</link><description>Transformers were originally proposed as a sequence-to-sequence model fortext but have become vital for a wide range of modalities, including images,audio, video, and undirected graphs. However, transformers for directed graphsare a surprisingly underexplored topic, despite their applicability toubiquitous domains, including source code and logic circuits. In this work, wepropose two direction- and structure-aware positional encodings for directedgraphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-awaregeneralization of the combinatorial Laplacian; (2) directional random walkencodings. Empirically, we show that the extra directionality information isuseful in various downstream tasks, including correctness testing of sortingnetworks and source code understanding. Together with a data-flow-centric graphconstruction, our model outperforms the prior state of the art on the OpenGraph Benchmark Code2 relatively by 14.7%.</description><author>Simon Geisler, Yujia Li, Daniel Mankowitz, Ali Taylan Cemgil, Stephan G√ºnnemann, Cosmin Paduraru</author><pubDate>Thu, 31 Aug 2023 15:38:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00049v3</guid></item><item><title>Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection</title><link>http://arxiv.org/abs/2308.16763v1</link><description>Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities ofLarge Language Models (LLMs) through the generation of intermediate rationales.However, these enhancements predominantly benefit large-scale models, leavingsmall LMs without significant performance improvements when directly applyingCoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarilyon their pre-trained internal knowledge. The external knowledge that ispreviously unknown to the model remains unexploited. This omission becomespronounced in tasks such as stance detection, where the external backgroundknowledge plays a pivotal role. Additionally, the large-scale architecture ofLLMs inevitably present efficiency challenges during deployment. To addressthese challenges, we introduce the Ladder-of-Thought (LoT) for stancedetection. Grounded in a dual-phase Cascaded Optimization framework, LoTdirects the model to incorporate high-quality external knowledge, enhancing theintermediate rationales it generates. These bolstered rationales subsequentlyserve as the foundation for more precise predictions - akin to how a ladderfacilitates reaching elevated goals. LoT achieves a balance between efficiencyand accuracy, making it an adaptable and efficient framework for stancedetection. Our empirical evaluations underscore LoT's effectiveness, marking a16% improvement over ChatGPT and a 10% enhancement compared to ChatGPT withCoT.</description><author>Kairui Hu, Ming Yan, Joey Tianyi Zhou, Ivor W. Tsang, Wen Haw Chong, Yong Keong Yap</author><pubDate>Thu, 31 Aug 2023 15:31:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16763v1</guid></item><item><title>Constructing Indoor Region-based Radio Map without Location Labels</title><link>http://arxiv.org/abs/2308.16759v1</link><description>Radio map construction requires a large amount of radio measurement data withlocation labels, which imposes a high deployment cost. This paper develops aregion-based radio map from received signal strength (RSS) measurements withoutlocation labels. The construction is based on a set of blindly collected RSSmeasurement data from a device that visits each region in an indoor areaexactly once, where the footprints and timestamps are not recorded. The mainchallenge is to cluster the RSS data and match clusters with the physicalregions. Classical clustering algorithms fail to work as the RSS data naturallyappears as non-clustered due to multipaths and noise. In this paper, a signalsubspace model with a sequential prior is constructed for the RSS data, and anintegrated segmentation and clustering algorithm is developed, which is shownto find the globally optimal solution in a special case. Furthermore, theclustered data is matched with the physical regions using a graph-basedapproach. Based on real measurements from an office space, the proposed schemereduces the region localization error by roughly 50% compared to a weightedcentroid localization (WCL) baseline, and it even outperforms some supervisedlocalization schemes, including k-nearest neighbor (KNN), support vectormachine (SVM), and deep neural network (DNN), which require labeled data fortraining.</description><author>Zheng Xing, Junting Chen</author><pubDate>Thu, 31 Aug 2023 15:27:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16759v1</guid></item><item><title>Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation Using only Images</title><link>http://arxiv.org/abs/2308.16758v1</link><description>Generating 3D faces from textual descriptions has a multitude ofapplications, such as gaming, movie, and robotics. Recent progresses havedemonstrated the success of unconditional 3D face generation and text-to-3Dshape generation. However, due to the limited text-3D face data pairs,text-driven 3D face generation remains an open problem. In this paper, wepropose a text-guided 3D faces generation method, refer as TG-3DFace, forgenerating realistic 3D faces using text guidance. Specifically, we adopt anunconditional 3D face generation framework and equip it with text conditions,which learns the text-guided 3D face generation with only text-2D face data. Ontop of that, we propose two text-to-face cross-modal alignment techniques,including the global contrastive learning and the fine-grained alignmentmodule, to facilitate high semantic consistency between generated 3D faces andinput texts. Besides, we present directional classifier guidance during theinference process, which encourages creativity for out-of-domain generations.Compared to the existing methods, TG-3DFace creates more realistic andaesthetically pleasing 3D faces, boosting 9% multi-view consistency (MVIC) overLatent3D. The rendered face images generated by TG-3DFace achieve higher FIDand CLIP score than text-to-2D face/image generation models, demonstrating oursuperiority in generating realistic and semantic-consistent textures.</description><author>Cuican Yu, Guansong Lu, Yihan Zeng, Jian Sun, Xiaodan Liang, Huibin Li, Zongben Xu, Songcen Xu, Wei Zhang, Hang Xu</author><pubDate>Thu, 31 Aug 2023 15:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16758v1</guid></item><item><title>Training Neural Networks Using Reproducing Kernel Space Interpolation and Model Reduction</title><link>http://arxiv.org/abs/2308.16754v1</link><description>We introduce and study the theory of training neural networks usinginterpolation techniques from reproducing kernel Hilbert space theory. Wegeneralize the method to Krein spaces, and show that widely-used neural networkarchitectures are subsets of reproducing kernel Krein spaces (RKKS). We studythe concept of "associated Hilbert spaces" of RKKS and develop techniques toimprove upon the expressivity of various activation functions. Next, usingconcepts from the theory of functions of several complex variables, we prove acomputationally applicable, multidimensional generalization of the celebratedAdamjan- Arov-Krein (AAK) theorem. The theorem yields a novel class of neuralnetworks, called Prolongation Neural Networks (PNN). We demonstrate that, byapplying the multidimensional AAK theorem to gain a PNN, one can gainperformance superior to both our interpolatory methods and currentstate-of-the-art methods in noisy environments. We provide useful illustrationsof our methods in practice.</description><author>Eric Arthur Werneburg</author><pubDate>Thu, 31 Aug 2023 15:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16754v1</guid></item><item><title>Context Aware Query Rewriting for Text Rankers using LLM</title><link>http://arxiv.org/abs/2308.16753v1</link><description>Query rewriting refers to an established family of approaches that areapplied to underspecified and ambiguous queries to overcome the vocabularymismatch problem in document ranking. Queries are typically rewritten duringquery processing time for better query modelling for the downstream ranker.With the advent of large-language models (LLMs), there have been initialinvestigations into using generative approaches to generate pseudo documents totackle this inherent vocabulary gap. In this work, we analyze the utility ofLLMs for improved query rewriting for text ranking tasks. We find that thereare two inherent limitations of using LLMs as query re-writers -- concept driftwhen using only queries as prompts and large inference costs during queryprocessing. We adopt a simple, yet surprisingly effective, approach calledcontext aware query rewriting (CAR) to leverage the benefits of LLMs for queryunderstanding. Firstly, we rewrite ambiguous training queries by context-awareprompting of LLMs, where we use only relevant documents as context.Unlikeexisting approaches, we use LLM-based query rewriting only during the trainingphase. Eventually, a ranker is fine-tuned on the rewritten queries instead ofthe original queries during training. In our extensive experiments, we findthat fine-tuning a ranker using re-written queries offers a significantimprovement of up to 33% on the passage ranking task and up to 28% on thedocument ranking task when compared to the baseline performance of usingoriginal queries.</description><author>Abhijit Anand, Venktesh V, Vinay Setty, Avishek Anand</author><pubDate>Thu, 31 Aug 2023 15:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16753v1</guid></item><item><title>Moreau Envelope ADMM for Decentralized Weakly Convex Optimization</title><link>http://arxiv.org/abs/2308.16752v1</link><description>This paper proposes a proximal variant of the alternating direction method ofmultipliers (ADMM) for distributed optimization. Although the current versionsof ADMM algorithm provide promising numerical results in producing solutionsthat are close to optimal for many convex and non-convex optimization problems,it remains unclear if they can converge to a stationary point for weakly convexand locally non-smooth functions. Through our analysis using the Moreauenvelope function, we demonstrate that MADM can indeed converge to a stationarypoint under mild conditions. Our analysis also includes computing the bounds onthe amount of change in the dual variable update step by relating the gradientof the Moreau envelope function to the proximal function. Furthermore, theresults of our numerical experiments indicate that our method is faster andmore robust than widely-used approaches.</description><author>Reza Mirzaeifard, Naveen K. D. Venkategowda, Alexander Jung, Stefan Werner</author><pubDate>Thu, 31 Aug 2023 15:16:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16752v1</guid></item><item><title>6D Object Pose Estimation from Approximate 3D Models for Orbital Robotics</title><link>http://arxiv.org/abs/2303.13241v4</link><description>We present a novel technique to estimate the 6D pose of objects from singleimages where the 3D geometry of the object is only given approximately and notas a precise 3D model. To achieve this, we employ a dense 2D-to-3Dcorrespondence predictor that regresses 3D model coordinates for every pixel.In addition to the 3D coordinates, our model also estimates the pixel-wisecoordinate error to discard correspondences that are likely wrong. This allowsus to generate multiple 6D pose hypotheses of the object, which we then refineiteratively using a highly efficient region-based approach. We also introduce anovel pixel-wise posterior formulation by which we can estimate the probabilityfor each hypothesis and select the most likely one. As we show in experiments,our approach is capable of dealing with extreme visual conditions includingoverexposure, high contrast, or low signal-to-noise ratio. This makes it apowerful technique for the particularly challenging task of estimating the poseof tumbling satellites for in-orbit robotic applications. Our method achievesstate-of-the-art performance on the SPEED+ dataset and has won the SPEC2021post-mortem competition.</description><author>Maximilian Ulmer, Maximilian Durner, Martin Sundermeyer, Manuel Stoiber, Rudolph Triebel</author><pubDate>Thu, 31 Aug 2023 15:15:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13241v4</guid></item><item><title>DUFormer: Solving Power Line Detection Task in Aerial Images using Semantic Segmentation</title><link>http://arxiv.org/abs/2304.05821v2</link><description>Unmanned aerial vehicles (UAVs) are frequently used for inspecting powerlines and capturing high-resolution aerial images. However, detecting powerlines in aerial images is difficult,as the foreground data(i.e, power lines) issmall and the background information is abundant.To tackle this problem, weintroduce DUFormer, a semantic segmentation algorithm explicitly designed todetect power lines in aerial images. We presuppose that it is advantageous totrain an efficient Transformer model with sufficient feature extraction using aconvolutional neural network(CNN) with a strong inductive bias.With this goalin mind, we introduce a heavy token encoder that performs overlapping featureremodeling and tokenization. The encoder comprises a pyramid CNN featureextraction module and a power line feature enhancement module.After successfullocal feature extraction for power lines, feature fusion is conducted.Then,theTransformer block is used for global modeling. The final segmentation result isachieved by amalgamating local and global features in the decode head.Moreover,we demonstrate the importance of the joint multi-weight loss function in powerline segmentation. Our experimental results show that our proposed methodoutperforms all state-of-the-art methods in power line segmentation on thepublicly accessible TTPLA dataset.</description><author>Deyu An, Qiang Zhang, Jianshu Chao, Ting Li, Feng Qiao, Yong Deng, Zhenpeng Bian</author><pubDate>Thu, 31 Aug 2023 15:15:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05821v2</guid></item><item><title>"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models</title><link>http://arxiv.org/abs/2307.10811v2</link><description>Prewriting is the process of discovering and developing ideas before a firstdraft, which requires divergent thinking and often implies unstructuredstrategies such as diagramming, outlining, free-writing, etc. Although largelanguage models (LLMs) have been demonstrated to be useful for a variety oftasks including creative writing, little is known about how users wouldcollaborate with LLMs to support prewriting. The preferred collaborative roleand initiative of LLMs during such a creativity process is also unclear. Toinvestigate human-LLM collaboration patterns and dynamics during prewriting, weconducted a three-session qualitative study with 15 participants in twocreative tasks: story writing and slogan writing. The findings indicated thatduring collaborative prewriting, there appears to be a three-stage iterativeHuman-AI Co-creativity process that includes Ideation, Illumination, andImplementation stages. This collaborative process champions the human in adominant role, in addition to mixed and shifting levels of initiative thatexist between humans and LLMs. This research also reports on collaborationbreakdowns that occur during this process, user perceptions of using existingLLMs during Human-AI Co-creativity, and discusses design implications tosupport this co-creativity process.</description><author>Qian Wan, Siying Hu, Yu Zhang, Piaohong Wang, Bo Wen, Zhicong Lu</author><pubDate>Thu, 31 Aug 2023 15:13:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10811v2</guid></item><item><title>Unsupervised CT Metal Artifact Reduction by Plugging Diffusion Priors in Dual Domains</title><link>http://arxiv.org/abs/2308.16742v1</link><description>During the process of computed tomography (CT), metallic implants often causedisruptive artifacts in the reconstructed images, impeding accurate diagnosis.Several supervised deep learning-based approaches have been proposed forreducing metal artifacts (MAR). However, these methods heavily rely on trainingwith simulated data, as obtaining paired metal artifact CT and clean CT data inclinical settings is challenging. This limitation can lead to decreasedperformance when applying these methods in clinical practice. Existingunsupervised MAR methods, whether based on learning or not, typically operatewithin a single domain, either in the image domain or the sinogram domain. Inthis paper, we propose an unsupervised MAR method based on the diffusion model,a generative model with a high capacity to represent data distributions.Specifically, we first train a diffusion model using CT images without metalartifacts. Subsequently, we iteratively utilize the priors embedded within thepre-trained diffusion model in both the sinogram and image domains to restorethe degraded portions caused by metal artifacts. This dual-domain processingempowers our approach to outperform existing unsupervised MAR methods,including another MAR method based on the diffusion model, which we havequalitatively and quantitatively validated using synthetic datasets. Moreover,our method demonstrates superior visual results compared to both supervised andunsupervised methods on clinical datasets.</description><author>Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang</author><pubDate>Thu, 31 Aug 2023 15:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16742v1</guid></item><item><title>Socratis: Are large multimodal models emotionally aware?</title><link>http://arxiv.org/abs/2308.16741v1</link><description>Existing emotion prediction benchmarks contain coarse emotion labels which donot consider the diversity of emotions that an image and text can elicit inhumans due to various reasons. Learning diverse reactions to multimodal contentis important as intelligent machines take a central role in generating anddelivering content to society. To address this gap, we propose Socratis, a\underline{soc}ietal \underline{r}e\underline{a}c\underline{ti}on\underline{s}benchmark, where each image-caption (IC) pair is annotated with multipleemotions and the reasons for feeling them. Socratis contains 18K free-formreactions for 980 emotions on 2075 image-caption pairs from 5 widely-read newsand image-caption (IC) datasets. We benchmark the capability ofstate-of-the-art multimodal large language models to generate the reasons forfeeling an emotion given an IC pair. Based on a preliminary human study, weobserve that humans prefer human-written reasons over 2 times more often thanmachine-generated ones. This shows our task is harder than standard generationtasks because it starkly contrasts recent findings where humans cannot tellapart machine vs human-written news articles, for instance. We further see thatcurrent captioning metrics based on large vision-language models also fail tocorrelate with human preferences. We hope that these findings and our benchmarkwill inspire further research on training emotionally aware models.</description><author>Katherine Deng, Arijit Ray, Reuben Tan, Saadia Gabriel, Bryan A. Plummer, Kate Saenko</author><pubDate>Thu, 31 Aug 2023 14:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16741v1</guid></item><item><title>Parsing is All You Need for Accurate Gait Recognition in the Wild</title><link>http://arxiv.org/abs/2308.16739v1</link><description>Binary silhouettes and keypoint-based skeletons have dominated human gaitrecognition studies for decades since they are easy to extract from videoframes. Despite their success in gait recognition for in-the-lab environments,they usually fail in real-world scenarios due to their low information entropyfor gait representations. To achieve accurate gait recognition in the wild,this paper presents a novel gait representation, named Gait Parsing Sequence(GPS). GPSs are sequences of fine-grained human segmentation, i.e., humanparsing, extracted from video frames, so they have much higher informationentropy to encode the shapes and dynamics of fine-grained human parts duringwalking. Moreover, to effectively explore the capability of the GPSrepresentation, we propose a novel human parsing-based gait recognitionframework, named ParsingGait. ParsingGait contains a Convolutional NeuralNetwork (CNN)-based backbone and two light-weighted heads. The first headextracts global semantic features from GPSs, while the other one learns mutualinformation of part-level features through Graph Convolutional Networks tomodel the detailed dynamics of human walking. Furthermore, due to the lack ofsuitable datasets, we build the first parsing-based dataset for gaitrecognition in the wild, named Gait3D-Parsing, by extending the large-scale andchallenging Gait3D dataset. Based on Gait3D-Parsing, we comprehensivelyevaluate our method and existing gait recognition methods. The experimentalresults show a significant improvement in accuracy brought by the GPSrepresentation and the superiority of ParsingGait. The code and dataset areavailable at https://gait3d.github.io/gait3d-parsing-hp .</description><author>Jinkai Zheng, Xinchen Liu, Shuai Wang, Lihao Wang, Chenggang Yan, Wu Liu</author><pubDate>Thu, 31 Aug 2023 14:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16739v1</guid></item><item><title>US-SFNet: A Spatial-Frequency Domain-based Multi-branch Network for Cervical Lymph Node Lesions Diagnoses in Ultrasound Images</title><link>http://arxiv.org/abs/2308.16738v1</link><description>Ultrasound imaging serves as a pivotal tool for diagnosing cervical lymphnode lesions. However, the diagnoses of these images largely hinge on theexpertise of medical practitioners, rendering the process susceptible tomisdiagnoses. Although rapidly developing deep learning has substantiallyimproved the diagnoses of diverse ultrasound images, there remains aconspicuous research gap concerning cervical lymph nodes. The objective of ourwork is to accurately diagnose cervical lymph node lesions by leveraging a deeplearning model. To this end, we first collected 3392 images containing normallymph nodes, benign lymph node lesions, malignant primary lymph node lesions,and malignant metastatic lymph node lesions. Given that ultrasound images aregenerated by the reflection and scattering of sound waves across varied bodilytissues, we proposed the Conv-FFT Block. It integrates convolutional operationswith the fast Fourier transform to more astutely model the images. Buildingupon this foundation, we designed a novel architecture, named US-SFNet. Thisarchitecture not only discerns variances in ultrasound images from the spatialdomain but also adeptly captures microstructural alterations across variouslesions in the frequency domain. To ascertain the potential of US-SFNet, webenchmarked it against 12 popular architectures through five-foldcross-validation. The results show that US-SFNet is SOTA and can achieve 92.89%accuracy, 90.46% precision, 89.95% sensitivity and 97.49% specificity,respectively.</description><author>Yubiao Yue, Jun Xue, Haihua Liang, Bingchun Luo, Zhenzhang Li</author><pubDate>Thu, 31 Aug 2023 14:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16738v1</guid></item><item><title>Robust Networked Federated Learning for Localization</title><link>http://arxiv.org/abs/2308.16737v1</link><description>This paper addresses the problem of localization, which is inherentlynon-convex and non-smooth in a federated setting where the data is distributedacross a multitude of devices. Due to the decentralized nature of federatedenvironments, distributed learning becomes essential for scalability andadaptability. Moreover, these environments are often plagued by outlier data,which presents substantial challenges to conventional methods, particularly inmaintaining estimation accuracy and ensuring algorithm convergence. To mitigatethese challenges, we propose a method that adopts an $L_1$-norm robustformulation within a distributed sub-gradient framework, explicitly designed tohandle these obstacles. Our approach addresses the problem in its originalform, without resorting to iterative simplifications or approximations,resulting in enhanced computational efficiency and improved estimationaccuracy. We demonstrate that our method converges to a stationary point,highlighting its effectiveness and reliability. Through numerical simulations,we confirm the superior performance of our approach, notably in outlier-richenvironments, which surpasses existing state-of-the-art localization methods.</description><author>Reza Mirzaeifard, Naveen K. D. Venkategowda, Stefan Werner</author><pubDate>Thu, 31 Aug 2023 14:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16737v1</guid></item><item><title>Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment</title><link>http://arxiv.org/abs/2308.16735v1</link><description>Deployment of Deep Neural Networks in medical imaging is hindered bydistribution shift between training data and data processed after deployment,causing performance degradation. Post-Deployment Adaptation (PDA) addressesthis by tailoring a pre-trained, deployed model to the target data distributionusing limited labelled or entirely unlabelled target data, while assuming noaccess to source training data as they cannot be deployed with the model due toprivacy concerns and their large size. This makes reliable adaptationchallenging due to limited learning signal. This paper challenges thisassumption and introduces FedPDA, a novel adaptation framework that brings theutility of learning from remote data from Federated Learning into PDA. FedPDAenables a deployed model to obtain information from source data via remotegradient exchange, while aiming to optimize the model specifically for thetarget domain. Tailored for FedPDA, we introduce a novel optimization methodStarAlign (Source-Target Remote Gradient Alignment) that aligns gradientsbetween source-target domain pairs by maximizing their inner product, tofacilitate learning a target-specific model. We demonstrate the method'seffectiveness using multi-center databases for the tasks of cancer metastasesdetection and skin lesion classification, where our method compares favourablyto previous work. Code is available at: https://github.com/FelixWag/StarAlign</description><author>Felix Wagner, Zeju Li, Pramit Saha, Konstantinos Kamnitsas</author><pubDate>Thu, 31 Aug 2023 14:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16735v1</guid></item><item><title>WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model</title><link>http://arxiv.org/abs/2308.15962v2</link><description>Enabling robots to understand language instructions and react accordingly tovisual perception has been a long-standing goal in the robotics researchcommunity. Achieving this goal requires cutting-edge advances in naturallanguage processing, computer vision, and robotics engineering. Thus, thispaper mainly investigates the potential of integrating the most recent LargeLanguage Models (LLMs) and existing visual grounding and robotic graspingsystem to enhance the effectiveness of the human-robot interaction. Weintroduce the WALL-E (Embodied Robotic WAiter load lifting with Large Languagemodel) as an example of this integration. The system utilizes the LLM ofChatGPT to summarize the preference object of the users as a target instructionvia the multi-round interactive dialogue. The target instruction is thenforwarded to a visual grounding system for object pose and size estimation,following which the robot grasps the object accordingly. We deploy thisLLM-empowered system on the physical robot to provide a more user-friendlyinterface for the instruction-guided grasping task. The further experimentalresults on various real-world scenarios demonstrated the feasibility andefficacy of our proposed framework. See the project website at:https://star-uu-wang.github.io/WALL-E/</description><author>Tianyu Wang, Yifan Li, Haitao Lin, Xiangyang Xue, Yanwei Fu</author><pubDate>Thu, 31 Aug 2023 14:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15962v2</guid></item><item><title>Proof of Deep Learning: Approaches, Challenges, and Future Directions</title><link>http://arxiv.org/abs/2308.16730v1</link><description>The rise of computational power has led to unprecedented performance gainsfor deep learning models. As more data becomes available and modelarchitectures become more complex, the need for more computational powerincreases. On the other hand, since the introduction of Bitcoin as the firstcryptocurrency and the establishment of the concept of blockchain as adistributed ledger, many variants and approaches have been proposed. However,many of them have one thing in common, which is the Proof of Work (PoW)consensus mechanism. PoW is mainly used to support the process of new blockgeneration. While PoW has proven its robustness, its main drawback is that itrequires a significant amount of processing power to maintain the security andintegrity of the blockchain. This is due to applying brute force to solve ahashing puzzle. To utilize the computational power available in useful andmeaningful work while keeping the blockchain secure, many techniques have beenproposed, one of which is known as Proof of Deep Learning (PoDL). PoDL is aconsensus mechanism that uses the process of training a deep learning model asproof of work to add new blocks to the blockchain. In this paper, we survey thevarious approaches for PoDL. We discuss the different types of PoDL algorithms,their advantages and disadvantages, and their potential applications. We alsodiscuss the challenges of implementing PoDL and future research directions.</description><author>Mahmoud Salhab, Khaleel Mershad</author><pubDate>Thu, 31 Aug 2023 14:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16730v1</guid></item><item><title>Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery</title><link>http://arxiv.org/abs/2308.10283v2</link><description>We propose a new parameter-adaptive uncertainty-penalized Bayesianinformation criterion (UBIC) to prioritize the parsimonious partialdifferential equation (PDE) that sufficiently governs noisy spatial-temporalobserved data with few reliable terms. Since the naive use of the BIC for modelselection has been known to yield an undesirable overfitted PDE, the UBICpenalizes the found PDE not only by its complexity but also the quantifieduncertainty, derived from the model supports' coefficient of variation in aprobabilistic view. We also introduce physics-informed neural network learningas a simulation-based approach to further validate the selected PDE flexiblyagainst the other discovered PDE. Numerical results affirm the successfulapplication of the UBIC in identifying the true governing PDE. Additionally, wereveal an interesting effect of denoising the observed data on improving thetrade-off between the BIC score and model complexity. Code is available athttps://github.com/Pongpisit-Thanasutives/UBIC.</description><author>Pongpisit Thanasutives, Takashi Morita, Masayuki Numao, Ken-ichi Fukui</author><pubDate>Thu, 31 Aug 2023 14:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10283v2</guid></item><item><title>RBSR: Efficient and Flexible Recurrent Network for Burst Super-Resolution</title><link>http://arxiv.org/abs/2306.17595v2</link><description>Burst super-resolution (BurstSR) aims at reconstructing a high-resolution(HR) image from a sequence of low-resolution (LR) and noisy images, which isconducive to enhancing the imaging effects of smartphones with limited sensors.The main challenge of BurstSR is to effectively combine the complementaryinformation from input frames, while existing methods still struggle with it.In this paper, we suggest fusing cues frame-by-frame with an efficient andflexible recurrent network. In particular, we emphasize the role of thebase-frame and utilize it as a key prompt to guide the knowledge acquisitionfrom other frames in every recurrence. Moreover, we introduce an implicitweighting loss to improve the model's flexibility in facing input frames withvariable numbers. Extensive experiments on both synthetic and real-worlddatasets demonstrate that our method achieves better results thanstate-of-the-art ones. Codes and pre-trained models are available athttps://github.com/ZcsrenlongZ/RBSR.</description><author>Renlong Wu, Zhilu Zhang, Shuohao Zhang, Hongzhi Zhang, Wangmeng Zuo</author><pubDate>Thu, 31 Aug 2023 14:45:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17595v2</guid></item><item><title>ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models</title><link>http://arxiv.org/abs/2305.06566v4</link><description>Personalized content-based recommender systems have become indispensabletools for users to navigate through the vast amount of content available onplatforms like daily news websites and book recommendation services. However,existing recommenders face significant challenges in understanding the contentof items. Large language models (LLMs), which possess deep semanticcomprehension and extensive knowledge from pretraining, have proven to beeffective in various natural language processing tasks. In this study, weexplore the potential of leveraging both open- and closed-source LLMs toenhance content-based recommendation. With open-source LLMs, we utilize theirdeep layers as content encoders, enriching the representation of content at theembedding level. For closed-source LLMs, we employ prompting techniques toenrich the training data at the token level. Through comprehensive experiments,we demonstrate the high effectiveness of both types of LLMs and show thesynergistic relationship between them. Notably, we observed a significantrelative improvement of up to 19.32% compared to existing state-of-the-artrecommendation models. These findings highlight the immense potential of bothopen- and closed-source of LLMs in enhancing content-based recommendationsystems. We will make our code and LLM-generated data available for otherresearchers to reproduce our results.</description><author>Qijiong Liu, Nuo Chen, Tetsuya Sakai, Xiao-Ming Wu</author><pubDate>Thu, 31 Aug 2023 14:43:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06566v4</guid></item><item><title>Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance</title><link>http://arxiv.org/abs/2308.16725v1</link><description>Sketch-based terrain generation seeks to create realistic landscapes forvirtual environments in various applications such as computer games, animationand virtual reality. Recently, deep learning based terrain generation hasemerged, notably the ones based on generative adversarial networks (GAN).However, these methods often struggle to fulfill the requirements of flexibleuser control and maintain generative diversity for realistic terrain.Therefore, we propose a novel diffusion-based method, namely terrain diffusionnetwork (TDN), which actively incorporates user guidance for enhancedcontrollability, taking into account terrain features like rivers, ridges,basins, and peaks. Instead of adhering to a conventional monolithic denoisingprocess, which often compromises the fidelity of terrain details or thealignment with user control, a multi-level denoising scheme is proposed togenerate more realistic terrains by taking into account fine-grained details,particularly those related to climatic patterns influenced by erosion andtectonic activities. Specifically, three terrain synthesisers are designed forstructural, intermediate, and fine-grained level denoising purposes, whichallow each synthesiser concentrate on a distinct terrain aspect. Moreover, tomaximise the efficiency of our TDN, we further introduce terrain and sketchlatent spaces for the synthesizers with pre-trained terrain autoencoders.Comprehensive experiments on a new dataset constructed from NASA TopologyImages clearly demonstrate the effectiveness of our proposed method, achievingthe state-of-the-art performance. Our code and dataset will be publiclyavailable.</description><author>Zexin Hu, Kun Hu, Clinton Mo, Lei Pan, Zhiyong Wang</author><pubDate>Thu, 31 Aug 2023 14:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16725v1</guid></item><item><title>Robust Representation Learning for Unreliable Partial Label Learning</title><link>http://arxiv.org/abs/2308.16718v1</link><description>Partial Label Learning (PLL) is a type of weakly supervised learning whereeach training instance is assigned a set of candidate labels, but only onelabel is the ground-truth. However, this idealistic assumption may not alwayshold due to potential annotation inaccuracies, meaning the ground-truth may notbe present in the candidate label set. This is known as Unreliable PartialLabel Learning (UPLL) that introduces an additional complexity due to theinherent unreliability and ambiguity of partial labels, often resulting in asub-optimal performance with existing methods. To address this challenge, wepropose the Unreliability-Robust Representation Learning framework (URRL) thatleverages unreliability-robust contrastive learning to help the model fortifyagainst unreliable partial labels effectively. Concurrently, we propose a dualstrategy that combines KNN-based candidate label set correction andconsistency-regularization-based label disambiguation to refine label qualityand enhance the ability of representation learning within the URRL framework.Extensive experiments demonstrate that the proposed method outperformsstate-of-the-art PLL methods on various datasets with diverse degrees ofunreliability and ambiguity. Furthermore, we provide a theoretical analysis ofour approach from the perspective of the expectation maximization (EM)algorithm. Upon acceptance, we pledge to make the code publicly accessible.</description><author>Yu Shi, Dong-Dong Wu, Xin Geng, Min-Ling Zhang</author><pubDate>Thu, 31 Aug 2023 14:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16718v1</guid></item><item><title>When Deep Learning Meets Polyhedral Theory: A Survey</title><link>http://arxiv.org/abs/2305.00241v2</link><description>In the past decade, deep learning became the prevalent methodology forpredictive modeling thanks to the remarkable accuracy of deep neural networksin tasks such as computer vision and natural language processing. Meanwhile,the structure of neural networks converged back to simpler representationsbased on piecewise constant and piecewise linear functions such as theRectified Linear Unit (ReLU), which became the most commonly used type ofactivation function in neural networks. That made certain types of networkstructure $\unicode{x2014}$such as the typical fully-connected feedforwardneural network$\unicode{x2014}$ amenable to analysis through polyhedral theoryand to the application of methodologies such as Linear Programming (LP) andMixed-Integer Linear Programming (MILP) for a variety of purposes. In thispaper, we survey the main topics emerging from this fast-paced area of work,which bring a fresh perspective to understanding neural networks in more detailas well as to applying linear optimization techniques to train, verify, andreduce the size of such networks.</description><author>Joey Huchette, Gonzalo Mu√±oz, Thiago Serra, Calvin Tsay</author><pubDate>Thu, 31 Aug 2023 14:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00241v2</guid></item><item><title>Towards Vehicle-to-everything Autonomous Driving: A Survey on Collaborative Perception</title><link>http://arxiv.org/abs/2308.16714v1</link><description>Vehicle-to-everything (V2X) autonomous driving opens up a promising directionfor developing a new generation of intelligent transportation systems.Collaborative perception (CP) as an essential component to achieve V2X canovercome the inherent limitations of individual perception, including occlusionand long-range perception. In this survey, we provide a comprehensive review ofCP methods for V2X scenarios, bringing a profound and in-depth understanding tothe community. Specifically, we first introduce the architecture and workflowof typical V2X systems, which affords a broader perspective to understand theentire V2X system and the role of CP within it. Then, we thoroughly summarizeand analyze existing V2X perception datasets and CP methods. Particularly, weintroduce numerous CP methods from various crucial perspectives, includingcollaboration stages, roadside sensors placement, latency compensation,performance-bandwidth trade-off, attack/defense, pose alignment, etc. Moreover,we conduct extensive experimental analyses to compare and examine current CPmethods, revealing some essential and unexplored insights. Specifically, weanalyze the performance changes of different methods under differentbandwidths, providing a deep insight into the performance-bandwidth trade-offissue. Also, we examine methods under different LiDAR ranges. To study themodel robustness, we further investigate the effects of various simulatedreal-world noises on the performance of different CP methods, coveringcommunication latency, lossy communication, localization errors, and mixednoises. In addition, we look into the sim-to-real generalization ability ofexisting CP methods. At last, we thoroughly discuss issues and challenges,highlighting promising directions for future efforts. Our codes forexperimental analysis will be public athttps://github.com/memberRE/Collaborative-Perception.</description><author>Si Liu, Chen Gao, Yuan Chen, Xingyu Peng, Xianghao Kong, Kun Wang, Runsheng Xu, Wentao Jiang, Hao Xiang, Jiaqi Ma, Miao Wang</author><pubDate>Thu, 31 Aug 2023 14:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16714v1</guid></item><item><title>IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer</title><link>http://arxiv.org/abs/2307.14863v3</link><description>Advanced image tampering techniques are increasingly challenging thetrustworthiness of multimedia, leading to the development of Image ManipulationLocalization (IML). But what makes a good IML model? The answer lies in the wayto capture artifacts. Exploiting artifacts requires the model to extractnon-semantic discrepancies between manipulated and authentic regions,necessitating explicit comparisons between the two areas. With theself-attention mechanism, naturally, the Transformer should be a bettercandidate to capture artifacts. However, due to limited datasets, there iscurrently no pure ViT-based approach for IML to serve as a benchmark, and CNNsdominate the entire task. Nevertheless, CNNs suffer from weak long-range andnon-semantic modeling. To bridge this gap, based on the fact that artifacts aresensitive to image resolution, amplified under multi-scale features, andmassive at the manipulation border, we formulate the answer to the formerquestion as building a ViT with high-resolution capacity, multi-scale featureextraction capability, and manipulation edge supervision that could convergewith a small amount of data. We term this simple but effective ViT paradigmIML-ViT, which has significant potential to become a new benchmark for IML.Extensive experiments on five benchmark datasets verified our model outperformsthe state-of-the-art manipulation localization methods.Code and models areavailable at \url{https://github.com/SunnyHaze/IML-ViT}.</description><author>Xiaochen Ma, Bo Du, Zhuohang Jiang, Ahmed Y. Al Hammadi, Jizhe Zhou</author><pubDate>Thu, 31 Aug 2023 14:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14863v3</guid></item><item><title>CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset</title><link>http://arxiv.org/abs/2308.16705v1</link><description>English datasets predominantly reflect the perspectives of certainnationalities, which can lead to cultural biases in models and datasets. Thisis particularly problematic in tasks heavily influenced by subjectivity, suchas hate speech detection. To delve into how individuals from differentcountries perceive hate speech, we introduce CReHate, a cross-culturalre-annotation of the sampled SBIC dataset. This dataset includes annotationsfrom five distinct countries: Australia, Singapore, South Africa, the UnitedKingdom, and the United States. Our thorough statistical analysis highlightssignificant differences based on nationality, with only 59.4% of the samplesachieving consensus among all countries. We also introduce a culturallysensitive hate speech classifier via transfer learning, adept at capturingperspectives of different nationalities. These findings underscore the need tore-evaluate certain aspects of NLP research, especially with regard to thenuanced nature of hate speech in the English language.</description><author>Nayeon Lee, Chani Jung, Junho Myung, Jiho Jin, Juho Kim, Alice Oh</author><pubDate>Thu, 31 Aug 2023 14:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16705v1</guid></item><item><title>Transformer-based interpretable multi-modal data fusion for skin lesion classification</title><link>http://arxiv.org/abs/2304.14505v2</link><description>A lot of deep learning (DL) research these days is mainly focused onimproving quantitative metrics regardless of other factors. In human-centeredapplications, like skin lesion classification in dermatology, DL-drivenclinical decision support systems are still in their infancy due to the limitedtransparency of their decision-making process. Moreover, the lack of proceduresthat can explain the behavior of trained DL algorithms leads to almost no trustfrom clinical physicians. To diagnose skin lesions, dermatologists rely onvisual assessment of the disease and the data gathered from the patient'sanamnesis. Data-driven algorithms dealing with multi-modal data are limited bythe separation of feature-level and decision-level fusion procedures requiredby convolutional architectures. To address this issue, we enable single-stagemulti-modal data fusion via the attention mechanism of transformer-basedarchitectures to aid in diagnosing skin diseases. Our method beats otherstate-of-the-art single- and multi-modal DL architectures in image-rich andpatient-data-rich environments. Additionally, the choice of the architectureenables native interpretability support for the classification task both in theimage and metadata domain with no additional modifications necessary.</description><author>Theodor Cheslerean-Boghiu, Melia-Evelina Fleischmann, Theresa Willem, Tobias Lasser</author><pubDate>Thu, 31 Aug 2023 14:10:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14505v2</guid></item><item><title>Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models</title><link>http://arxiv.org/abs/2308.16703v1</link><description>Model extraction emerges as a critical security threat with attack vectorsexploiting both algorithmic and implementation-based approaches. The main goalof an attacker is to steal as much information as possible about a protectedvictim model, so that he can mimic it with a substitute model, even with alimited access to similar training data. Recently, physical attacks such asfault injection have shown worrying efficiency against the integrity andconfidentiality of embedded models. We focus on embedded deep neural networkmodels on 32-bit microcontrollers, a widespread family of hardware platforms inIoT, and the use of a standard fault injection strategy - Safe Error Attack(SEA) - to perform a model extraction attack with an adversary having a limitedaccess to training data. Since the attack strongly depends on the inputqueries, we propose a black-box approach to craft a successful attack set. Fora classical convolutional neural network, we successfully recover at least 90%of the most significant bits with about 1500 crafted inputs. These informationenable to efficiently train a substitute model, with only 8% of the trainingdataset, that reaches high fidelity and near identical accuracy level than thevictim model.</description><author>Kevin Hector, Pierre-Alain Moellic, Mathieu Dumont, Jean-Max Dutertre</author><pubDate>Thu, 31 Aug 2023 14:09:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16703v1</guid></item><item><title>Invertible normalizing flow neural networks by JKO scheme</title><link>http://arxiv.org/abs/2212.14424v2</link><description>Normalizing flow is a class of deep generative models for efficient samplingand density estimation. In practice, the flow often appears as a chain ofinvertible neural network blocks; to facilitate training, existing works haveregularized flow trajectories and designed special network architectures. Thecurrent paper develops a neural ODE flow network inspired by theJordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wisetraining of the residual blocks without sampling SDE trajectories or innerloops of score matching or variational learning. As the JKO scheme unfolds thedynamic of gradient flow, the proposed model naturally stacks residual networkblocks one by one, reducing the memory load and difficulty in performingend-to-end deep flow network training. We also develop adaptive timereparameterization of the flow network with a progressive refinement of thetrajectory in probability space, which improves the model training efficiencyand accuracy in practice. Using numerical experiments with synthetic and realdata, we show that the proposed JKO-iFlow model achieves similar or betterperformance in generating new samples compared with the existing flow anddiffusion models at a significantly reduced computational and memory cost.</description><author>Chen Xu, Xiuyuan Cheng, Yao Xie</author><pubDate>Thu, 31 Aug 2023 14:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14424v2</guid></item><item><title>USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2303.07806v2</link><description>Seed area generation is usually the starting point of weakly supervisedsemantic segmentation (WSSS). Computing the Class Activation Map (CAM) from amulti-label classification network is the de facto paradigm for seed areageneration, but CAMs generated from Convolutional Neural Networks (CNNs) andTransformers are prone to be under- and over-activated, respectively, whichmakes the strategies to refine CAMs for CNNs usually inappropriate forTransformers, and vice versa. In this paper, we propose a Unified optimizationparadigm for Seed Area GEneration (USAGE) for both types of networks, in whichthe objective function to be optimized consists of two terms: One is ageneration loss, which controls the shape of seed areas by a temperatureparameter following a deterministic principle for different types of networks;The other is a regularization loss, which ensures the consistency between theseed areas that are generated by self-adaptive network adjustment fromdifferent views, to overturn false activation in seed areas. Experimentalresults show that USAGE consistently improves seed area generation for bothCNNs and Transformers by large margins, e.g., outperforming state-of-the-artmethods by a mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE-generatedseed areas on Transformers, we achieve state-of-the-art WSSS results on bothPASCAL VOC and MS COCO.</description><author>Zelin Peng, Guanchun Wang, Lingxi Xie, Dongsheng Jiang, Wei Shen, Qi Tian</author><pubDate>Thu, 31 Aug 2023 14:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07806v2</guid></item><item><title>SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models</title><link>http://arxiv.org/abs/2308.16692v1</link><description>Current speech large language models build upon discrete speechrepresentations, which can be categorized into semantic tokens and acoustictokens. However, existing speech tokens are not specifically designed forspeech language modeling. To assess the suitability of speech tokens forbuilding speech language models, we established the first benchmark,SLMTokBench. Our results indicate that neither semantic nor acoustic tokens areideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speechtokenizer for speech large language models. SpeechTokenizer adopts theEncoder-Decoder architecture with residual vector quantization (RVQ). Unifyingsemantic and acoustic tokens, SpeechTokenizer disentangles different aspects ofspeech information hierarchically across different RVQ layers. Furthermore, Weconstruct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer.Experiments show that SpeechTokenizer performs comparably to EnCodec in speechreconstruction and demonstrates strong performance on the SLMTokBenchbenchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks.Code and models are available athttps://github.com/ZhangXInFD/SpeechTokenizer/.</description><author>Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, Xipeng Qiu</author><pubDate>Thu, 31 Aug 2023 13:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16692v1</guid></item><item><title>ViLTA: Enhancing Vision-Language Pre-training through Textual Augmentation</title><link>http://arxiv.org/abs/2308.16689v1</link><description>Vision-language pre-training (VLP) methods are blossoming recently, and itscrucial goal is to jointly learn visual and textual features via atransformer-based architecture, demonstrating promising improvements on avariety of vision-language tasks. Prior arts usually focus on how to alignvisual and textual features, but strategies for improving the robustness ofmodel and speeding up model convergence are left insufficiently explored. In this paper, we propose a novel method ViLTA, comprising of two componentsto further facilitate the model to learn fine-grained representations amongimage-text pairs. For Masked Language Modeling (MLM), we propose across-distillation method to generate soft labels to enhance the robustness ofmodel, which alleviates the problem of treating synonyms of masked words asnegative samples in one-hot labels. For Image-Text Matching (ITM), we leveragethe current language encoder to synthesize hard negatives based on the contextof language input, encouraging the model to learn high-quality representationsby increasing the difficulty of the ITM task. By leveraging the abovetechniques, our ViLTA can achieve better performance on various vision-languagetasks. Extensive experiments on benchmark datasets demonstrate that theeffectiveness of ViLTA and its promising potential for vision-languagepre-training.</description><author>Weihan Wang, Zhen Yang, Bin Xu, Juanzi Li, Yankui Sun</author><pubDate>Thu, 31 Aug 2023 13:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16689v1</guid></item></channel></rss>