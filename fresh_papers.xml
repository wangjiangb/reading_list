<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 26 Jan 2025 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass</title><link>http://arxiv.org/abs/2501.13928v1</link><description>Multi-view 3D reconstruction remains a core challenge in computer vision,particularly in applications requiring accurate and scalable representationsacross diverse perspectives. Current leading methods such as DUSt3R employ afundamentally pairwise approach, processing images in pairs and necessitatingcostly global alignment procedures to reconstruct from multiple views. In thiswork, we propose Fast 3D Reconstruction (Fast3R), a novel multi-viewgeneralization to DUSt3R that achieves efficient and scalable 3D reconstructionby processing many views in parallel. Fast3R's Transformer-based architectureforwards N images in a single forward pass, bypassing the need for iterativealignment. Through extensive experiments on camera pose estimation and 3Dreconstruction, Fast3R demonstrates state-of-the-art performance, withsignificant improvements in inference speed and reduced error accumulation.These results establish Fast3R as a robust alternative for multi-viewapplications, offering enhanced scalability without compromising reconstructionaccuracy.</description><author>Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, Matt Feiszli</author><pubDate>Thu, 23 Jan 2025 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13928v1</guid></item><item><title>CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation</title><link>http://arxiv.org/abs/2501.13927v1</link><description>Large language models (LLMs) have shown great potential in natural languageprocessing tasks, but their application to machine translation (MT) remainschallenging due to pretraining on English-centric data and the complexity ofreinforcement learning from human feedback (RLHF). Direct PreferenceOptimization (DPO) has emerged as a simpler and more efficient alternative, butits performance depends heavily on the quality of preference data. To addressthis, we propose Confidence-Reward driven Preference Optimization (CRPO), anovel method that combines reward scores with model confidence to improve dataselection for fine-tuning. CRPO selects challenging sentence pairs where themodel is uncertain or underperforms, leading to more effective learning. Whileprimarily designed for LLMs, CRPO also generalizes to encoder-decoder modelslike NLLB, demonstrating its versatility. Empirical results show that CRPOoutperforms existing methods such as RS-DPO, RSO and MBR score in bothtranslation accuracy and data efficiency.</description><author>Guofeng Cui, Pichao Wang, Yang Liu, Zemian Ke, Zhu Liu, Vimal Bhat</author><pubDate>Thu, 23 Jan 2025 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13927v1</guid></item><item><title>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</title><link>http://arxiv.org/abs/2501.13926v1</link><description>Chain-of-Thought (CoT) reasoning has been extensively explored in largemodels to tackle complex understanding tasks. However, it still remains an openquestion whether such strategies can be applied to verifying and reinforcingimage generation scenarios. In this paper, we provide the first comprehensiveinvestigation of the potential of CoT reasoning to enhance autoregressive imagegeneration. We focus on three techniques: scaling test-time computation forverification, aligning model preferences with Direct Preference Optimization(DPO), and integrating these techniques for complementary effects. Our resultsdemonstrate that these approaches can be effectively adapted and combined tosignificantly improve image generation performance. Furthermore, given thepivotal role of reward models in our findings, we propose the PotentialAssessment Reward Model (PARM) and PARM++, specialized for autoregressive imagegeneration. PARM adaptively assesses each generation step through a potentialassessment approach, merging the strengths of existing reward models, andPARM++ further introduces a reflection mechanism to self-correct the generatedunsatisfactory image. Using our investigated reasoning strategies, we enhance abaseline model, Show-o, to achieve superior results, with a significant +24%improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. Wehope our study provides unique insights and paves a new path for integratingCoT reasoning with autoregressive image generation. Code and models arereleased at https://github.com/ZiyuGuo99/Image-Generation-CoT</description><author>Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, Pheng-Ann Heng</author><pubDate>Thu, 23 Jan 2025 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13926v1</guid></item><item><title>Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization</title><link>http://arxiv.org/abs/2501.13924v1</link><description>Test-time adaptation (TTA) has demonstrated significant potential inaddressing distribution shifts between training and testing data. Open-settest-time adaptation (OSTTA) aims to adapt a source pre-trained model online toan unlabeled target domain that contains unknown classes. This task becomesmore challenging when multiple modalities are involved. Existing methods haveprimarily focused on unimodal OSTTA, often filtering out low-confidence sampleswithout addressing the complexities of multimodal data. In this work, wepresent Adaptive Entropy-aware Optimization (AEO), a novel frameworkspecifically designed to tackle Multimodal Open-set Test-time Adaptation(MM-OSTTA) for the first time. Our analysis shows that the entropy differencebetween known and unknown samples in the target domain strongly correlates withMM-OSTTA performance. To leverage this, we propose two key components:Unknown-aware Adaptive Entropy Optimization (UAE) and Adaptive ModalityPrediction Discrepancy Optimization (AMP). These components enhance the abilityof model to distinguish unknown class samples during online adaptation byamplifying the entropy difference between known and unknown samples. Tothoroughly evaluate our proposed methods in the MM-OSTTA setting, we establisha new benchmark derived from existing datasets. This benchmark includes twodownstream tasks and incorporates five modalities. Extensive experiments acrossvarious domain shift situations demonstrate the efficacy and versatility of theAEO framework. Additionally, we highlight the strong performance of AEO inlong-term and continual MM-OSTTA settings, both of which are challenging andhighly relevant to real-world applications. Our source code is available athttps://github.com/donghao51/AEO.</description><author>Hao Dong, Eleni Chatzi, Olga Fink</author><pubDate>Thu, 23 Jan 2025 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13924v1</guid></item><item><title>GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing</title><link>http://arxiv.org/abs/2501.13925v1</link><description>Recent advances in large multimodal models (LMMs) have recognizedfine-grained grounding as an imperative factor of visual understanding anddialogue. However, the benefits of such representation in LMMs are limited tothe natural image domain, and these models perform poorly for remote sensing(RS). The distinct overhead viewpoint, scale variation, and presence of smallobjects in high-resolution RS imagery present a unique challenge inregion-level comprehension. Moreover, the development of the groundingconversation capability of LMMs within RS is hindered by the lack of granular,RS domain-specific grounded data. Addressing these limitations, we proposeGeoPixel - the first end-to-end high resolution RS-LMM that supportspixel-level grounding. This capability allows fine-grained visual perception bygenerating interleaved masks in conversation. GeoPixel supports up to 4K HDresolution in any aspect ratio, ideal for high-precision RS image analysis. Tosupport the grounded conversation generation (GCG) in RS imagery, we curate avisually grounded dataset GeoPixelD through a semi-automated pipeline thatutilizes set-of-marks prompting and spatial priors tailored for RS data tomethodically control the data generation process. GeoPixel demonstratessuperior performance in pixel-level comprehension, surpassing existing LMMs inboth single-target and multi-target segmentation tasks. Our methodologicalablation studies validate the effectiveness of each component in the overallarchitecture. Our code and data will be publicly released.</description><author>Akashah Shabbir, Mohammed Zumri, Mohammed Bennamoun, Fahad S. Khan, Salman Khan</author><pubDate>Thu, 23 Jan 2025 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13925v1</guid></item><item><title>The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama with Vision-Aware and Function-Calling Capabilities</title><link>http://arxiv.org/abs/2501.13921v1</link><description>Breeze 2 is a suite of advanced multi-modal language models, available in 3Band 8B parameter configurations, specifically designed to enhance TraditionalChinese language representation. Building upon the Llama 3, Breeze 2 continuespretraining on an extensive corpus to enhance the linguistic and culturalheritage of Traditional Chinese. It incorporates vision-aware capabilitiesthrough a visual encoder and a bridge module, and supports function-calling viaprompt templates and post-training on function-calling data. The effectivenessof Breeze 2 is benchmarked across various tasks, including Taiwan generalknowledge, instruction-following, long context, function calling, and visionunderstanding. Furthermore, we showcase the capabilities of the its 3B model ina mobile application. We are publicly releasing all Breeze 2 models under theLlama 3 Community License.</description><author>Chan-Jan Hsu, Chia-Sheng Liu, Meng-Hsi Chen, Muxi Chen, Po-Chun Hsu, Yi-Chang Chen, Da-Shan Shiu</author><pubDate>Thu, 23 Jan 2025 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13921v1</guid></item><item><title>IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models</title><link>http://arxiv.org/abs/2501.13920v1</link><description>With the rapid development of diffusion models, text-to-image(T2I) modelshave made significant progress, showcasing impressive abilities in promptfollowing and image generation. Recently launched models such as FLUX.1 andIdeogram2.0, along with others like Dall-E3 and Stable Diffusion 3, havedemonstrated exceptional performance across various complex tasks, raisingquestions about whether T2I models are moving towards general-purposeapplicability. Beyond traditional image generation, these models exhibitcapabilities across a range of fields, including controllable generation, imageediting, video, audio, 3D, and motion generation, as well as computer visiontasks like semantic segmentation and depth estimation. However, currentevaluation frameworks are insufficient to comprehensively assess these models'performance across expanding domains. To thoroughly evaluate these models, wedeveloped the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0,Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is dividedinto five key domains: structured output generation, realism, and physicalconsistency, specific domain generation, challenging scenario generation, andmulti-style creation tasks. This comprehensive assessment highlights eachmodel's strengths and limitations, particularly the outstanding performance ofFLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoringthe expanding applications and potential of T2I models as foundational AItools. This study provides valuable insights into the current state and futuretrajectory of T2I models as they evolve towards general-purpose usability.Evaluation scripts will be released at https://github.com/jylei16/Imagine-e.</description><author>Jiayi Lei, Renrui Zhang, Xiangfei Hu, Weifeng Lin, Zhen Li, Wenjian Sun, Ruoyi Du, Le Zhuo, Zhongyu Li, Xinyue Li, Shitian Zhao, Ziyu Guo, Yiting Lu, Peng Gao, Hongsheng Li</author><pubDate>Thu, 23 Jan 2025 18:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13920v1</guid></item><item><title>Temporal Preference Optimization for Long-Form Video Understanding</title><link>http://arxiv.org/abs/2501.13919v1</link><description>Despite significant advancements in video large multimodal models(video-LMMs), achieving effective temporal grounding in long-form videosremains a challenge for existing models. To address this limitation, we proposeTemporal Preference Optimization (TPO), a novel post-training frameworkdesigned to enhance the temporal grounding capabilities of video-LMMs throughpreference learning. TPO adopts a self-training approach that enables models todifferentiate between well-grounded and less accurate temporal responses byleveraging curated preference datasets at two granularities: localized temporalgrounding, which focuses on specific video segments, and comprehensive temporalgrounding, which captures extended temporal dependencies across entire videosequences. By optimizing on these preference datasets, TPO significantlyenhances temporal understanding while reducing reliance on manually annotateddata. Extensive experiments on three long-form video understandingbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectivenessof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPOestablishes itself as the leading 7B model on the Video-MME benchmark,underscoring the potential of TPO as a scalable and efficient solution foradvancing temporal reasoning in long-form video understanding. Project page:https://ruili33.github.io/tpo_website.</description><author>Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy</author><pubDate>Thu, 23 Jan 2025 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13919v1</guid></item><item><title>Guaranteed Recovery of Unambiguous Clusters</title><link>http://arxiv.org/abs/2501.13093v2</link><description>Clustering is often a challenging problem because of the inherent ambiguityin what the "correct" clustering should be. Even when the number of clusters$K$ is known, this ambiguity often still exists, particularly when there isvariation in density among different clusters, and clusters have multiplerelatively separated regions of high density. In this paper we propose aninformation-theoretic characterization of when a $K$-clustering is ambiguous,and design an algorithm that recovers the clustering whenever it isunambiguous. This characterization formalizes the situation when two highdensity regions within a cluster are separable enough that they look more liketwo distinct clusters than two truly distinct clusters in the clustering. Thealgorithm first identifies $K$ partial clusters (or "seeds") using adensity-based approach, and then adds unclustered points to the initial $K$partial clusters in a greedy manner to form a complete clustering. We implementand test a version of the algorithm that is modified to effectively handleoverlapping clusters, and observe that it requires little parameter selectionand displays improved performance on many datasets compared to widely usedalgorithms for non-convex cluster recovery.</description><author>Kayvon Mazooji, Ilan Shomorony</author><pubDate>Thu, 23 Jan 2025 18:56:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13093v2</guid></item><item><title>Truncated Consistency Models</title><link>http://arxiv.org/abs/2410.14895v2</link><description>Consistency models have recently been introduced to accelerate sampling fromdiffusion models by directly predicting the solution (i.e., data) of theprobability flow ODE (PF ODE) from initial noise. However, the training ofconsistency models requires learning to map all intermediate points along PFODE trajectories to their corresponding endpoints. This task is much morechallenging than the ultimate objective of one-step generation, which onlyconcerns the PF ODE's noise-to-data mapping. We empirically find that thistraining paradigm limits the one-step generation performance of consistencymodels. To address this issue, we generalize consistency training to thetruncated time range, which allows the model to ignore denoising tasks atearlier time steps and focus its capacity on generation. We propose a newparameterization of the consistency function and a two-stage training procedurethat prevents the truncated-time training from collapsing to a trivialsolution. Experiments on CIFAR-10 and ImageNet $64\times64$ datasets show thatour method achieves better one-step and two-step FIDs than the state-of-the-artconsistency models such as iCT-deep, using more than 2$\times$ smallernetworks. Project page: https://truncated-cm.github.io/</description><author>Sangyun Lee, Yilun Xu, Tomas Geffner, Giulia Fanti, Karsten Kreis, Arash Vahdat, Weili Nie</author><pubDate>Thu, 23 Jan 2025 18:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14895v2</guid></item><item><title>SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts' QA Through Six-Dimensional Feature Analysis</title><link>http://arxiv.org/abs/2410.20651v2</link><description>Fact-checking is extensively studied in the context of misinformation anddisinformation, addressing objective inaccuracies. However, a softer form ofmisinformation involves responses that are factually correct but lack certainfeatures such as clarity and relevance. This challenge is prevalent in formalQuestion-Answer (QA) settings such as press conferences in finance, politics,sports, and other domains, where subjective answers can obscure transparency.Despite this, there is a lack of manually annotated datasets for subjectivefeatures across multiple dimensions. To address this gap, we introduceSubjECTive-QA, a human annotated dataset on Earnings Call Transcripts' (ECTs)QA sessions as the answers given by company representatives are often open tosubjective interpretations and scrutiny. The dataset includes 49,446annotations for long-form QA pairs across six features: Assertive, Cautious,Optimistic, Specific, Clear, and Relevant. These features are carefullyselected to encompass the key attributes that reflect the tone of the answersprovided during QA sessions across different domain. Our findings are that thebest-performing Pre-trained Language Model (PLM), RoBERTa-base, has similarweighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity,such as Relevant and Clear, with a mean difference of 2.17% in their weightedF1 scores. The models perform significantly better on features with highersubjectivity, such as Specific and Assertive, with a mean difference of 10.01%in their weighted F1 scores. Furthermore, testing SubjECTive-QA'sgeneralizability using QAs from White House Press Briefings and Gaggles yieldsan average weighted F1 score of 65.97% using our best models for each feature,demonstrating broader applicability beyond the financial domain. SubjECTive-QAis publicly available under the CC BY 4.0 license</description><author>Huzaifa Pardawala, Siddhant Sukhani, Agam Shah, Veer Kejriwal, Abhishek Pillai, Rohan Bhasin, Andrew DiBiasio, Tarun Mandapati, Dhruv Adha, Sudheer Chava</author><pubDate>Thu, 23 Jan 2025 18:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.20651v2</guid></item><item><title>Improving Video Generation with Human Feedback</title><link>http://arxiv.org/abs/2501.13918v1</link><description>Video generation has achieved significant advances through rectified flowtechniques, but issues like unsmooth motion and misalignment between videos andprompts persist. In this work, we develop a systematic pipeline that harnesseshuman feedback to mitigate these problems and refine the video generationmodel. Specifically, we begin by constructing a large-scale human preferencedataset focused on modern video generation models, incorporating pairwiseannotations across multi-dimensions. We then introduce VideoReward, amulti-dimensional video reward model, and examine how annotations and variousdesign choices impact its rewarding efficacy. From a unified reinforcementlearning perspective aimed at maximizing reward with KL regularization, weintroduce three alignment algorithms for flow-based models by extending thosefrom diffusion models. These include two training-time strategies: directpreference optimization for flow (Flow-DPO) and reward weighted regression forflow (Flow-RWR), and an inference-time technique, Flow-NRG, which appliesreward guidance directly to noisy videos. Experimental results indicate thatVideoReward significantly outperforms existing reward models, and Flow-DPOdemonstrates superior performance compared to both Flow-RWR and standardsupervised fine-tuning methods. Additionally, Flow-NRG lets users assign customweights to multiple objectives during inference, meeting personalized videoquality needs. Project page: https://gongyeliu.github.io/videoalign.</description><author>Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, Wanli Ouyang</author><pubDate>Thu, 23 Jan 2025 18:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13918v1</guid></item><item><title>PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy</title><link>http://arxiv.org/abs/2501.13916v1</link><description>We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),a communication-efficient Vertical Federated Learning algorithm withDifferential Privacy guarantees. PBM-VFL combines Secure Multi-PartyComputation with the recently introduced Poisson Binomial Mechanism to protectparties' private datasets during model training. We define the novel concept offeature privacy and analyze end-to-end feature and sample privacy of ouralgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. Wealso provide the first theoretical characterization of the relationship betweenprivacy budget, convergence error, and communication cost indifferentially-private VFL. Finally, we empirically show that our modelperforms well with high levels of privacy.</description><author>Linh Tran, Timothy Castiglia, Stacy Patterson, Ana Milanova</author><pubDate>Thu, 23 Jan 2025 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13916v1</guid></item><item><title>Binary Diffusion Probabilistic Model</title><link>http://arxiv.org/abs/2501.13915v1</link><description>We introduce the Binary Diffusion Probabilistic Model (BDPM), a novelgenerative model optimized for binary data representations. While denoisingdiffusion probabilistic models (DDPMs) have demonstrated notable success intasks like image synthesis and restoration, traditional DDPMs rely oncontinuous data representations and mean squared error (MSE) loss for training,applying Gaussian noise models that may not be optimal for discrete or binarydata structures. BDPM addresses this by decomposing images into bitplanes andemploying XOR-based noise transformations, with a denoising model trained usingbinary cross-entropy loss. This approach enables precise noise control andcomputationally efficient inference, significantly lowering computational costsand improving model convergence. When evaluated on image restoration tasks suchas image super-resolution, inpainting, and blind image restoration, BDPMoutperforms state-of-the-art methods on the FFHQ, CelebA, and CelebA-HQdatasets. Notably, BDPM requires fewer inference steps than traditional DDPMmodels to reach optimal results, showcasing enhanced inference efficiency.</description><author>Vitaliy Kinakh, Slava Voloshynovskiy</author><pubDate>Thu, 23 Jan 2025 18:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13915v1</guid></item><item><title>Analysis of Indic Language Capabilities in LLMs</title><link>http://arxiv.org/abs/2501.13912v1</link><description>This report evaluates the performance of text-in text-out Large LanguageModels (LLMs) to understand and generate Indic languages. This evaluation isused to identify and prioritize Indic languages suited for inclusion in safetybenchmarks. We conduct this study by reviewing existing evaluation studies anddatasets; and a set of twenty-eight LLMs that support Indic languages. Weanalyze the LLMs on the basis of the training data, license for model and data,type of access and model developers. We also compare Indic language performanceacross evaluation datasets and find that significant performance disparities inperformance across Indic languages. Hindi is the most widely representedlanguage in models. While model performance roughly correlates with number ofspeakers for the top five languages, the assessment after that varies.</description><author>Aatman Vaidya, Tarunima Prabhakar, Denny George, Swair Shah</author><pubDate>Thu, 23 Jan 2025 18:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13912v1</guid></item><item><title>NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls</title><link>http://arxiv.org/abs/2409.03797v2</link><description>The resurgence of autonomous agents built using large language models (LLMs)to solve complex real-world tasks has brought increased focus on LLMs'fundamental ability of tool or function calling. At the core of these agents,an LLM must plan, execute, and respond using external tools, APIs, and customfunctions. Research on tool calling has gathered momentum, but evaluationbenchmarks and datasets representing the complexity of the tasks have laggedbehind. In this work, we focus on one such complexity, nested sequencing, withthe goal of extending existing benchmarks and evaluation. Specifically, wepresent NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls,i.e., sequences where the output of one API call is passed as input to asubsequent call. NESTFUL contains 1800+ nested sequences where all the functioncalls are executable. Experimental results on multiple models and settings showthat the best-performing model on the dataset has a full sequence matchaccuracy of 25% and win-rate of 34% necessitating a large scope for improvementin the nested sequencing aspect of function calling. Our analysis of theseresults provides possible future research directions for the community, inaddition to a benchmark to track progress. We have released the NESTFUL datasetunder the Apache 2.0 license at https://github.com/IBM/NESTFUL.</description><author>Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, Xin Wang, Luis A. Lastras, Pavan Kapanipathi</author><pubDate>Thu, 23 Jan 2025 18:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03797v2</guid></item><item><title>On Learning Representations for Tabular Data Distillation</title><link>http://arxiv.org/abs/2501.13905v1</link><description>Dataset distillation generates a small set of information-rich instances froma large dataset, resulting in reduced storage requirements, privacy orcopyright risks, and computational costs for downstream modeling, though muchof the research has focused on the image data modality. We study tabular datadistillation, which brings in novel challenges such as the inherent featureheterogeneity and the common use of non-differentiable learning models (such asdecision tree ensembles and nearest-neighbor predictors). To mitigate thesechallenges, we present $\texttt{TDColER}$, a tabular data distillationframework via column embeddings-based representation learning. To evaluate thisframework, we also present a tabular data distillation benchmark, ${{\sf \smallTDBench}}$. Based on an elaborate evaluation on ${{\sf \small TDBench}}$,resulting in 226,890 distilled datasets and 548,880 models trained on them, wedemonstrate that $\texttt{TDColER}$ is able to boost the distilled data qualityof off-the-shelf distillation schemes by 0.5-143% across 7 different tabularlearning models.</description><author>Inwon Kang, Parikshit Ram, Yi Zhou, Horst Samulowitz, Oshani Seneviratne</author><pubDate>Thu, 23 Jan 2025 18:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13905v1</guid></item><item><title>Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2501.13904v1</link><description>Multimodal Large Language Models (LLMs) are pivotal in revolutionizingcustomer support and operations by integrating multiple modalities such astext, images, and audio. Federated Prompt Learning (FPL) is a recently proposedapproach that combines pre-trained multimodal LLMs such as vision-languagemodels with federated learning to create personalized, privacy-preserving AIsystems. However, balancing the competing goals of personalization,generalization, and privacy remains a significant challenge.Over-personalization can lead to overfitting, reducing generalizability, whilestringent privacy measures, such as differential privacy, can hinder bothpersonalization and generalization. In this paper, we propose a DifferentiallyPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge byleveraging a low-rank adaptation scheme to capture generalization whilemaintaining a residual term that preserves expressiveness for personalization.To ensure privacy, we introduce a novel method where we apply localdifferential privacy to the two low-rank components of the local prompt, andglobal differential privacy to the global prompt. Our approach mitigates theimpact of privacy noise on the model performance while balancing the tradeoffbetween personalization and generalization. Extensive experiments demonstratethe effectiveness of our approach over other benchmarks.</description><author>Linh Tran, Wei Sun, Stacy Patterson, Ana Milanova</author><pubDate>Thu, 23 Jan 2025 18:34:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13904v1</guid></item><item><title>PointOBB-v3: Expanding Performance Boundaries of Single Point-Supervised Oriented Object Detection</title><link>http://arxiv.org/abs/2501.13898v1</link><description>With the growing demand for oriented object detection (OOD), recent studieson point-supervised OOD have attracted significant interest. In this paper, wepropose PointOBB-v3, a stronger single point-supervised OOD framework. Comparedto existing methods, it generates pseudo rotated boxes without additionalpriors and incorporates support for the end-to-end paradigm. PointOBB-v3functions by integrating three unique image views: the original view, a resizedview, and a rotated/flipped (rot/flp) view. Based on the views, a scaleaugmentation module and an angle acquisition module are constructed. In thefirst module, a Scale-Sensitive Consistency (SSC) loss and a Scale-SensitiveFeature Fusion (SSFF) module are introduced to improve the model's ability toestimate object scale. To achieve precise angle predictions, the second moduleemploys symmetry-based self-supervised learning. Additionally, we introduce anend-to-end version that eliminates the pseudo-label generation process byintegrating a detector branch and introduces an Instance-Aware Weighting (IAW)strategy to focus on high-quality predictions. We conducted extensiveexperiments on the DIOR-R, DOTA-v1.0/v1.5/v2.0, FAIR1M, STAR, and RSARdatasets. Across all these datasets, our method achieves an average improvementin accuracy of 3.56% in comparison to previous state-of-the-art methods. Thecode will be available at https://github.com/ZpyWHU/PointOBB-v3.</description><author>Peiyuan Zhang, Junwei Luo, Xue Yang, Yi Yu, Qingyun Li, Yue Zhou, Xiaosong Jia, Xudong Lu, Jingdong Chen, Xiang Li, Junchi Yan, Yansheng Li</author><pubDate>Thu, 23 Jan 2025 18:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13898v1</guid></item><item><title>GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous Exploration</title><link>http://arxiv.org/abs/2501.13896v1</link><description>Graphical User Interface (GUI) action grounding is a critical step in GUIautomation that maps language instructions to actionable elements on GUIscreens. Most recent works of GUI action grounding leverage large GUI datasetsto fine-tune MLLMs. However, the fine-tuning data always covers limited GUIenvironments, and we find the performance of the resulting model deterioratesin novel environments. We argue that the GUI grounding models should be furtheraligned to the novel environments to reveal their full potential, when theinference is known to involve novel environments, i.e., environments not usedduring the previous fine-tuning. To realize this, we first propose GUI-Bee, anMLLM-based autonomous agent, to collect high-quality, environment-specific datathrough exploration and then continuously fine-tune GUI grounding models withthe collected data. Our agent leverages a novel Q-value-Incentive In-ContextReinforcement Learning (Q-ICRL) method to optimize exploration efficiency anddata quality. Additionally, we introduce NovelScreenSpot, a benchmark fortesting how well the data can help align GUI action grounding models to novelenvironments and demonstrate the effectiveness of data collected by GUI-Bee inthe experiments. Furthermore, we conduct an ablation study to validate theQ-ICRL method in enhancing the efficiency of GUI-Bee. Project page:https://gui-bee.github.io</description><author>Yue Fan, Handong Zhao, Ruiyi Zhang, Yu Shen, Xin Eric Wang, Gang Wu</author><pubDate>Thu, 23 Jan 2025 18:16:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13896v1</guid></item><item><title>Accelerate High-Quality Diffusion Models with Inner Loop Feedback</title><link>http://arxiv.org/abs/2501.13107v2</link><description>We propose Inner Loop Feedback (ILF), a novel approach to acceleratediffusion models' inference. ILF trains a lightweight module to predict futurefeatures in the denoising process by leveraging the outputs from a chosendiffusion backbone block at a given time step. This approach exploits two keyintuitions; (1) the outputs of a given block at adjacent time steps aresimilar, and (2) performing partial computations for a step imposes a lowerburden on the model than skipping the step entirely. Our method is highlyflexible, since we find that the feedback module itself can simply be a blockfrom the diffusion backbone, with all settings copied. Its influence on thediffusion forward can be tempered with a learnable scaling factor from zeroinitialization. We train this module using distillation losses; however, unlikesome prior work where a full diffusion backbone serves as the student, ourmodel freezes the backbone, training only the feedback module. While manyefforts to optimize diffusion models focus on achieving acceptable imagequality in extremely few steps (1-4 steps), our emphasis is on matching bestcase results (typically achieved in 20 steps) while significantly reducingruntime. ILF achieves this balance effectively, demonstrating strongperformance for both class-to-image generation with diffusion transformer (DiT)and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. Thequality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIPImage Quality Assessment, ImageReward, and qualitative comparisons. Projectinformation is available at https://mgwillia.github.io/ilf.</description><author>Matthew Gwilliam, Han Cai, Di Wu, Abhinav Shrivastava, Zhiyu Cheng</author><pubDate>Thu, 23 Jan 2025 18:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13107v2</guid></item><item><title>Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning</title><link>http://arxiv.org/abs/2501.13893v1</link><description>We present Pix2Cap-COCO, the first panoptic pixel-level caption datasetdesigned to advance fine-grained visual understanding. To achieve this, wecarefully design an automated annotation pipeline that prompts GPT-4V togenerate pixel-aligned, instance-specific captions for individual objectswithin images, enabling models to learn more granular relationships betweenobjects and their contexts. This approach results in 167,254 detailed captions,with an average of 22.94 words per caption. Building on Pix2Cap-COCO, weintroduce a novel task, panoptic segmentation-captioning, which challengesmodels to recognize instances in an image and provide detailed descriptions foreach simultaneously. To benchmark this task, we design a robust baseline basedon X-Decoder. The experimental results demonstrate that Pix2Cap-COCO is aparticularly challenging dataset, as it requires models to excel in bothfine-grained visual understanding and detailed language generation.Furthermore, we leverage Pix2Cap-COCO for Supervised Fine-Tuning (SFT) on largemultimodal models (LMMs) to enhance their performance. For example, trainingwith Pix2Cap-COCO significantly improves the performance of GPT4RoI, yieldinggains in CIDEr +1.4%, ROUGE +0.4%, and SPICE +0.5% on Visual Genome dataset,and strengthens its region understanding ability on the ViP-BENCH, with anoverall improvement of +5.1%, including notable increases in recognitionaccuracy +11.2% and language generation quality +22.2%.</description><author>Zuyao You, Junke Wang, Lingyu Kong, Bo He, Zuxuan Wu</author><pubDate>Thu, 23 Jan 2025 18:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13893v1</guid></item><item><title>OCMDP: Observation-Constrained Markov Decision Process</title><link>http://arxiv.org/abs/2411.07087v4</link><description>In many practical applications, decision-making processes must balance thecosts of acquiring information with the benefits it provides. Traditionalcontrol systems often assume full observability, an unrealistic assumption whenobservations are expensive. We tackle the challenge of simultaneously learningobservation and control strategies in such cost-sensitive environments byintroducing the Observation-Constrained Markov Decision Process (OCMDP), wherethe policy influences the observability of the true state. To manage thecomplexity arising from the combined observation and control actions, wedevelop an iterative, model-free deep reinforcement learning algorithm thatseparates the sensing and control components of the policy. This decompositionenables efficient learning in the expanded action space by focusing on when andwhat to observe, as well as determining optimal control actions, withoutrequiring knowledge of the environment's dynamics. We validate our approach ona simulated diagnostic task and a realistic healthcare environment usingHeartPole. Given both scenarios, the experimental results demonstrate that ourmodel achieves a substantial reduction in observation costs on average,significantly outperforming baseline methods by a notable margin in efficiency.</description><author>Taiyi Wang, Jianheng Liu, Bryan Lee, Zhihao Wu, Yu Wu</author><pubDate>Thu, 23 Jan 2025 18:08:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07087v4</guid></item><item><title>Federated Granger Causality Learning for Interdependent Clients with State Space Representation</title><link>http://arxiv.org/abs/2501.13890v1</link><description>Advanced sensors and IoT devices have improved the monitoring and control ofcomplex industrial enterprises. They have also created an interdependent fabricof geographically distributed process operations (clients) across theseenterprises. Granger causality is an effective approach to detect and quantifyinterdependencies by examining how one client's state affects others over time.Understanding these interdependencies captures how localized events, such asfaults and disruptions, can propagate throughout the system, possibly causingwidespread operational impacts. However, the large volume and complexity ofindustrial data pose challenges in modeling these interdependencies. This paperdevelops a federated approach to learning Granger causality. We utilize alinear state space system framework that leverages low-dimensional stateestimates to analyze interdependencies. This addresses bandwidth limitationsand the computational burden commonly associated with centralized dataprocessing. We propose augmenting the client models with the Granger causalityinformation learned by the server through a Machine Learning (ML) function. Weexamine the co-dependence between the augmented client and server models andreformulate the framework as a standalone ML algorithm providing conditions forits sublinear and linear convergence rates. We also study the convergence ofthe framework to a centralized oracle model. Moreover, we include adifferential privacy analysis to ensure data security while preserving causalinsights. Using synthetic data, we conduct comprehensive experiments todemonstrate the robustness of our approach to perturbations in causality, thescalability to the size of communication, number of clients, and the dimensionsof raw data. We also evaluate the performance on two real-world industrialcontrol system datasets by reporting the volume of data saved bydecentralization.</description><author>Ayush Mohanty, Nazal Mohamed, Paritosh Ramanan, Nagi Gebraeel</author><pubDate>Thu, 23 Jan 2025 18:04:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13890v1</guid></item><item><title>Generating Realistic Forehead-Creases for User Verification via Conditioned Piecewise Polynomial Curves</title><link>http://arxiv.org/abs/2501.13889v1</link><description>We propose a trait-specific image generation method that models foreheadcreases geometrically using B-spline and B\'ezier curves. This approach ensuresthe realistic generation of both principal creases and non-prominent creasepatterns, effectively constructing detailed and authentic forehead-creaseimages. These geometrically rendered images serve as visual prompts for adiffusion-based Edge-to-Image translation model, which generates correspondingmated samples. The resulting novel synthetic identities are then used to traina forehead-crease verification network. To enhance intra-subject diversity inthe generated samples, we employ two strategies: (a) perturbing the controlpoints of B-splines under defined constraints to maintain label consistency,and (b) applying image-level augmentations to the geometric visual prompts,such as dropout and elastic transformations, specifically tailored to creasepatterns. By integrating the proposed synthetic dataset with real-world data,our method significantly improves the performance of forehead-creaseverification systems under a cross-database verification protocol.</description><author>Abhishek Tandon, Geetanjali Sharma, Gaurav Jaswal, Aditya Nigam, Raghavendra Ramachandra</author><pubDate>Thu, 23 Jan 2025 18:01:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13889v1</guid></item><item><title>Multimodal Sensor Dataset for Monitoring Older Adults Post Lower-Limb Fractures in Community Settings</title><link>http://arxiv.org/abs/2501.13888v1</link><description>Lower-Limb Fractures (LLF) are a major health concern for older adults, oftenleading to reduced mobility and prolonged recovery, potentially impairing dailyactivities and independence. During recovery, older adults frequently facesocial isolation and functional decline, complicating rehabilitation andadversely affecting physical and mental health. Multi-modal sensor platformsthat continuously collect data and analyze it using machine-learning algorithmscan remotely monitor this population and infer health outcomes. They can alsoalert clinicians to individuals at risk of isolation and decline. This paperpresents a new publicly available multi-modal sensor dataset, MAISON-LLF,collected from older adults recovering from LLF in community settings. Thedataset includes data from smartphone and smartwatch sensors, motion detectors,sleep-tracking mattresses, and clinical questionnaires on isolation anddecline. The dataset was collected from ten older adults living alone at homefor eight weeks each, totaling 560 days of 24-hour sensor data. For technicalvalidation, supervised machine-learning and deep-learning models were developedusing the sensor and clinical questionnaire data, providing a foundationalcomparison for the research community.</description><author>Ali Abedi, Charlene H. Chu, Shehroz S. Khan</author><pubDate>Thu, 23 Jan 2025 18:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13888v1</guid></item><item><title>What Does an Audio Deepfake Detector Focus on? A Study in the Time Domain</title><link>http://arxiv.org/abs/2501.13887v1</link><description>Adding explanations to audio deepfake detection (ADD) models will boost theirreal-world application by providing insight on the decision making process. Inthis paper, we propose a relevancy-based explainable AI (XAI) method to analyzethe predictions of transformer-based ADD models. We compare against standardGrad-CAM and SHAP-based methods, using quantitative faithfulness metrics aswell as a partial spoof test, to comprehensively analyze the relativeimportance of different temporal regions in an audio. We consider largedatasets, unlike previous works where only limited utterances are studied, andfind that the XAI methods differ in their explanations. The proposedrelevancy-based XAI method performs the best overall on a variety of metrics.Further investigation on the relative importance of speech/non-speech, phoneticcontent, and voice onsets/offsets suggest that the XAI results obtained fromanalyzing limited utterances don't necessarily hold when evaluated on largedatasets.</description><author>Petr Grinberg, Ankur Kumar, Surya Koppisetti, Gaurav Bharaj</author><pubDate>Thu, 23 Jan 2025 18:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13887v1</guid></item><item><title>IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models</title><link>http://arxiv.org/abs/2406.03368v2</link><description>Despite the widespread adoption of Large language models (LLMs), theirremarkable capabilities remain limited to a few high-resource languages.Additionally, many low-resource languages (\eg African languages) are oftenevaluated only on basic text classification tasks due to the lack ofappropriate or comprehensive benchmarks outside of high-resource languages. Inthis paper, we introduce IrokoBench -- a human-translated benchmark dataset for17 typologically-diverse low-resource African languages covering three tasks:natural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), andmulti-choice knowledge-based question answering~(AfriMMLU). We use IrokoBenchto evaluate zero-shot, few-shot, and translate-test settings~(where test setsare translated into English) across 10 open and six proprietary LLMs. Ourevaluation reveals a significant performance gap between high-resourcelanguages~(such as English and French) and low-resource African languages. Weobserve a significant performance gap between open and proprietary models, withthe highest performing open model, Gemma 2 27B only at 63\% of thebest-performing proprietary model GPT-4o performance. In addition, machinetranslating the test set to English before evaluation helped to close the gapfor larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.170B. These findings suggest that more efforts are needed to develop and adaptLLMs for African languages.</description><author>David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda, Godson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, Tombekai Vangoni Sherman, Pontus Stenetorp</author><pubDate>Thu, 23 Jan 2025 17:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03368v2</guid></item><item><title>Exploring Finetuned Audio-LLM on Heart Murmur Features</title><link>http://arxiv.org/abs/2501.13884v1</link><description>Large language models (LLMs) for audio have excelled in recognizing andanalyzing human speech, music, and environmental sounds. However, theirpotential for understanding other types of sounds, particularly biomedicalsounds, remains largely underexplored despite significant scientific interest.In this study, we focus on diagnosing cardiovascular diseases usingphonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)paradigms are restricted to heart murmur classification (healthy vs unhealthy)and do not predict other acoustic features of the murmur such as timing,grading, harshness, pitch, and quality, which are important in helpingphysicians diagnose the underlying heart conditions. We propose to finetune anaudio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)dataset and evaluate its performance in classifying 11 expert-labeled murmurfeatures. Additionally, we aim to achieve more noise-robust and generalizablesystem by exploring a preprocessing segmentation algorithm using an audiorepresentation model, SSAMBA. Our results indicate that the LLM-based modeloutperforms state-of-the-art methods in 8 of the 11 features and performscomparably in the remaining 3. Moreover, the LLM successfully classifieslong-tail murmur features with limited training data, a task that all previousmethods have failed to classify. These findings underscore the potential ofaudio LLMs as assistants to human cardiologists in enhancing heart diseasediagnosis.</description><author>Adrian Florea, Xilin Jiang, Nima Mesgarani, Xiaofan Jiang</author><pubDate>Thu, 23 Jan 2025 17:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13884v1</guid></item><item><title>Utilizing Evolution Strategies to Train Transformers in Reinforcement Learning</title><link>http://arxiv.org/abs/2501.13883v1</link><description>We explore a capability of evolution strategies to train an agent with itspolicy based on a transformer architecture in a reinforcement learning setting.We performed experiments using OpenAI's highly parallelizable evolutionstrategy to train Decision Transformer in Humanoid locomotion environment andin the environment of Atari games, testing the ability of this black-boxoptimization technique to train even such relatively large and complicatedmodels (compared to those previously tested in the literature). We alsoproposed a method to aid the training by first pretraining the model beforeusing the OpenAI-ES to train it further, and tested its effectiveness. Theexamined evolution strategy proved to be, in general, capable of achievingstrong results and managed to obtain high-performing agents. Therefore, thepretraining was shown to be unnecessary; yet still, it helped us observe andformulate several further insights.</description><author>Matyáš Lorenc</author><pubDate>Thu, 23 Jan 2025 17:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13883v1</guid></item><item><title>A RAG-Based Institutional Assistant</title><link>http://arxiv.org/abs/2501.13880v1</link><description>Although large language models (LLMs) demonstrate strong text generationcapabilities, they struggle in scenarios requiring access to structuredknowledge bases or specific documents, limiting their effectiveness inknowledge-intensive tasks. To address this limitation, retrieval-augmentedgeneration (RAG) models have been developed, enabling generative models toincorporate relevant document fragments into their inputs. In this paper, wedesign and evaluate a RAG-based virtual assistant specifically tailored for theUniversity of S\~ao Paulo. Our system architecture comprises two key modules: aretriever and a generative model. We experiment with different types of modelsfor both components, adjusting hyperparameters such as chunk size and thenumber of retrieved documents. Our optimal retriever model achieves a Top-5accuracy of 30%, while our most effective generative model scores 22.04\%against ground truth answers. Notably, when the correct document chunks aresupplied to the LLMs, accuracy significantly improves to 54.02%, an increase ofover 30 percentage points. Conversely, without contextual input, performancedeclines to 13.68%. These findings highlight the critical role of databaseaccess in enhancing LLM performance. They also reveal the limitations ofcurrent semantic search methods in accurately identifying relevant documentsand underscore the ongoing challenges LLMs face in generating preciseresponses.</description><author>Gustavo Kuratomi, Paulo Pirozelli, Fabio G. Cozman, Sarajane M. Peres</author><pubDate>Thu, 23 Jan 2025 17:54:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13880v1</guid></item><item><title>Whether to trust: the ML leap of faith</title><link>http://arxiv.org/abs/2408.00786v2</link><description>Human trust is a prerequisite to trustworthy AI adoption, yet trust remainspoorly understood. Trust is often described as an attitude, but attitudescannot be reliably measured or managed. Additionally, humans frequentlyconflate trust in an AI system, its machine learning (ML) technology, and itsother component parts. Without fully understanding the 'leap of faith' involvedin trusting ML, users cannot develop intrinsic trust in these systems. A commonapproach to building trust is to explain a ML model's reasoning process.However, such explanations often fail to resonate with non-experts due to theinherent complexity of ML systems and explanations are disconnected from users'own (unarticulated) mental models. This work puts forward an innovative way ofdirectly building intrinsic trust in ML, by discerning and measuring the Leapof Faith (LoF) taken when a user decides to rely on ML. The LoF matrix capturesthe alignment between an ML model and a human expert's mental model. This matchis rigorously and practically identified by feeding the user's data andobjective function into both an ML agent and an expert-validated rules-basedagent: a verified point of reference that can be tested a priori against auser's own mental model. This represents a new class of neuro-symbolicarchitecture. The LoF matrix reveals to the user the distance that constitutesthe leap of faith between the rules-based and ML agents. For the first time, wepropose trust metrics that evaluate whether users demonstrate trust throughtheir actions rather than self-reported intent and whether such trust isdeserved based on outcomes. The significance of the contribution is that itenables empirical assessment and management of ML trust drivers, to supporttrustworthy ML adoption. The approach is illustrated through a long-termhigh-stakes field study: a 3-month pilot of a multi-agent sleep-improvementsystem.</description><author>Tory Frame, Julian Padget, George Stothart, Elizabeth Coulthard</author><pubDate>Thu, 23 Jan 2025 17:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00786v2</guid></item><item><title>Eye Gaze as a Signal for Conveying User Attention in Contextual AI Systems</title><link>http://arxiv.org/abs/2501.13878v1</link><description>Advanced multimodal AI agents can now collaborate with users to solvechallenges in the world. We explore eye tracking's role in such interaction toconvey a user's attention relative to the physical environment. We hypothesizethat this knowledge improves contextual understanding for AI agents. Byobserving hours of human-object interactions, we first measure the relationshipbetween an eye tracker's signal quality and its ability to reliably place gazeon nearby physical objects. We then conduct experiments which relay the user'sscanpath history as additional context querying multimodal agents. Our resultsshow that eye tracking provides high value as a user attention signal and canconvey information about the user's current task and interests to the agent.</description><author>Ethan Wilson, Naveen Sendhilnathan, Charlie S. Burlingham, Yusuf Mansour, Robert Cavin, Sai Deep Tetali, Ajoy Savio Fernandes, Michael J. Proulx</author><pubDate>Thu, 23 Jan 2025 17:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13878v1</guid></item><item><title>Sentence-level Aggregation of Lexical Metrics Correlates Stronger with Human Judgements than Corpus-level Aggregation</title><link>http://arxiv.org/abs/2407.12832v2</link><description>In this paper we show that corpus-level aggregation hinders considerably thecapability of lexical metrics to accurately evaluate machine translation (MT)systems. With empirical experiments we demonstrate that averaging individualsegment-level scores can make metrics such as BLEU and chrF correlate muchstronger with human judgements and make them behave considerably more similarto neural metrics such as COMET and BLEURT. We show that this difference existsbecause corpus- and segment-level aggregation differs considerably owing to theclassical average of ratio versus ratio of averages Mathematical problem.Moreover, as we also show, such difference affects considerably the statisticalrobustness of corpus-level aggregation. Considering that neural metricscurrently only cover a small set of sufficiently-resourced languages, theresults in this paper can help make the evaluation of MT systems forlow-resource languages more trustworthy.</description><author>Paulo Cavalin, Pedro Henrique Domingues, Claudio Pinhanez</author><pubDate>Thu, 23 Jan 2025 17:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12832v2</guid></item><item><title>Autoencoders for Anomaly Detection are Unreliable</title><link>http://arxiv.org/abs/2501.13864v1</link><description>Autoencoders are frequently used for anomaly detection, both in theunsupervised and semi-supervised settings. They rely on the assumption thatwhen trained using the reconstruction loss, they will be able to reconstructnormal data more accurately than anomalous data. Some recent works have positedthat this assumption may not always hold, but little has been done to study thevalidity of the assumption in theory. In this work we show that this assumptionindeed does not hold, and illustrate that anomalies, lying far away from normaldata, can be perfectly reconstructed in practice. We revisit the theory offailure of linear autoencoders for anomaly detection by showing how they canperfectly reconstruct out of bounds, or extrapolate undesirably, and note howthis can be dangerous in safety critical applications. We connect this tonon-linear autoencoders through experiments on both tabular data and real-worldimage data, the two primary application areas of autoencoders for anomalydetection.</description><author>Roel Bouman, Tom Heskes</author><pubDate>Thu, 23 Jan 2025 17:36:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13864v1</guid></item><item><title>RegMix: Data Mixture as Regression for Language Model Pre-training</title><link>http://arxiv.org/abs/2407.01492v2</link><description>The data mixture for large language model pre-training significantly impactsperformance, yet how to determine an effective mixture remains unclear. Wepropose RegMix to automatically identify a high-performing data mixture byformulating it as a regression task. RegMix trains many small models on diversedata mixtures, uses regression to predict performance of unseen mixtures, andapplies the best predicted mixture to train a large-scale model with orders ofmagnitude more compute. To empirically validate RegMix, we train 512 modelswith 1M parameters for 1B tokens to fit the regression model and predict thebest data mixture. Using this mixture we train a 1B parameter model for 25Btokens (i.e. 1000x larger and 25x longer) which we find performs best among 64candidate 1B parameter models with other mixtures. Furthermore, RegMixconsistently outperforms human selection in experiments involving models up to7B models trained on 100B tokens, while matching or exceeding DoReMi using just10% of the computational resources. Our experiments also show that (1) Datamixtures significantly impact performance; (2) Web corpora rather than dataperceived as high-quality like Wikipedia have the strongest positivecorrelation with downstream performance; (3) Domains interact in complex waysoften contradicting common sense, thus automatic approaches like RegMix areneeded; (4) Data mixture effects transcend scaling laws. Our code is availableat https://github.com/sail-sg/regmix.</description><author>Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, Min Lin</author><pubDate>Thu, 23 Jan 2025 17:35:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01492v2</guid></item><item><title>Dual-Modal Prototype Joint Learning for Compositional Zero-Shot Learning</title><link>http://arxiv.org/abs/2501.13859v1</link><description>Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositionsof attributes and objects by leveraging knowledge learned from seencompositions. Recent approaches have explored the use of Vision-Language Models(VLMs) to align textual and visual modalities. These methods typically employprompt engineering, parameter-tuning, and modality fusion to generate richtextual prototypes that serve as class prototypes for CZSL. However, themodality gap results in textual prototypes being unable to fully capture theoptimal representations of all class prototypes, particularly those withfine-grained features, which can be directly obtained from the visual modality.In this paper, we propose a novel Dual-Modal Prototype Joint Learning frameworkfor the CZSL task. Our approach, based on VLMs, introduces prototypes in boththe textual and visual modalities. The textual prototype is optimized tocapture broad conceptual information, aiding the model's generalization acrossunseen compositions. Meanwhile, the visual prototype is used to mitigate theclassification errors caused by the modality gap and capture fine-graineddetails to distinguish images with similar appearances. To effectively optimizethese prototypes, we design specialized decomposition modules and a jointlearning strategy that enrich the features from both modalities. Theseprototypes not only capture key category information during training but alsoserve as crucial reference targets during inference. Experimental resultsdemonstrate that our approach achieves state-of-the-art performance in theclosed-world setting and competitive performance in the open-world settingacross three publicly available CZSL benchmarks. These findings validate theeffectiveness of our method in advancing compositional generalization.</description><author>Shiyu Zhang, Cheng Yan, Yang Liu, Chenchen Jing, Lei Zhou, Wenjun Wang</author><pubDate>Thu, 23 Jan 2025 17:30:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13859v1</guid></item><item><title>First Lessons Learned of an Artificial Intelligence Robotic System for Autonomous Coarse Waste Recycling Using Multispectral Imaging-Based Methods</title><link>http://arxiv.org/abs/2501.13855v1</link><description>Current disposal facilities for coarse-grained waste perform manual sortingof materials with heavy machinery. Large quantities of recyclable materials arelost to coarse waste, so more effective sorting processes must be developed torecover them. Two key aspects to automate the sorting process are objectdetection with material classification in mixed piles of waste, and autonomouscontrol of hydraulic machinery. Because most objects in those accumulations ofwaste are damaged or destroyed, object detection alone is not feasible in themajority of cases. To address these challenges, we propose a classification ofmaterials with multispectral images of ultraviolet (UV), visual (VIS), nearinfrared (NIR), and short-wave infrared (SWIR) spectrums. Solution forautonomous control of hydraulic heavy machines for sorting of bulky waste isbeing investigated using cost-effective cameras and artificialintelligence-based controllers.</description><author>Timo Lange, Ajish Babu, Philipp Meyer, Matthis Keppner, Tim Tiedemann, Martin Wittmaier, Sebastian Wolff, Thomas Vögele</author><pubDate>Thu, 23 Jan 2025 17:24:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13855v1</guid></item><item><title>Cons-training tensor networks</title><link>http://arxiv.org/abs/2405.09005v3</link><description>In this study, we introduce a novel family of tensor networks, termed\textit{constrained matrix product states} (MPS), designed to incorporateexactly arbitrary discrete linear constraints, including inequalities, intosparse block structures. These tensor networks are particularly tailored formodeling distributions with support strictly over the feasible space, offeringbenefits such as reducing the search space in optimization problems,alleviating overfitting, improving training efficiency, and decreasing modelsize. Central to our approach is the concept of a quantum region, an extensionof quantum numbers traditionally used in U(1) symmetric tensor networks,adapted to capture any linear constraint, including the unconstrained scenario.We further develop a novel canonical form for these new MPS, which allow forthe merging and factorization of tensor blocks according to quantum regionfusion rules and permit optimal truncation schemes. Utilizing this canonicalform, we apply an unsupervised training strategy to optimize arbitraryobjective functions subject to discrete linear constraints. Our method'sefficacy is demonstrated by solving the quadratic knapsack problem, achievingsuperior performance compared to a leading nonlinear integer programmingsolver. Additionally, we analyze the complexity and scalability of ourapproach, demonstrating its potential in addressing complex constrainedcombinatorial optimization problems.</description><author>Javier Lopez-Piqueres, Jing Chen</author><pubDate>Thu, 23 Jan 2025 17:21:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09005v3</guid></item><item><title>Large Vision-Language Models for Knowledge-Grounded Data Annotation of Memes</title><link>http://arxiv.org/abs/2501.13851v1</link><description>Memes have emerged as a powerful form of communication, integrating visualand textual elements to convey humor, satire, and cultural messages. Existingresearch has focused primarily on aspects such as emotion classification, memegeneration, propagation, interpretation, figurative language, andsociolinguistics, but has often overlooked deeper meme comprehension andmeme-text retrieval. To address these gaps, this study introducesClassicMemes-50-templates (CM50), a large-scale dataset consisting of over33,000 memes, centered around 50 popular meme templates. We also present anautomated knowledge-grounded annotation pipeline leveraging largevision-language models to produce high-quality image captions, meme captions,and literary device labels overcoming the labor intensive demands of manualannotation. Additionally, we propose a meme-text retrieval CLIP model (mtrCLIP)that utilizes cross-modal embedding to enhance meme analysis, significantlyimproving retrieval performance. Our contributions include:(1) a novel datasetfor large-scale meme study, (2) a scalable meme annotation framework, and (3) afine-tuned CLIP for meme-text retrieval, all aimed at advancing theunderstanding and analysis of memes at scale.</description><author>Shiling Deng, Serge Belongie, Peter Ebert Christensen</author><pubDate>Thu, 23 Jan 2025 17:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13851v1</guid></item><item><title>Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth Estimation</title><link>http://arxiv.org/abs/2410.11610v4</link><description>Estimating depth from a single 2D image is a challenging task due to the lackof stereo or multi-view data, which are typically required for depthperception. In state-of-the-art architectures, the main challenge is toefficiently capture complex objects and fine-grained details, which are oftendifficult to predict. This paper introduces a novel deep learning-basedapproach using an enhanced encoder-decoder architecture, where theInception-ResNet-v2 model serves as the encoder. This is the first instance ofutilizing Inception-ResNet-v2 as an encoder for monocular depth estimation,demonstrating improved performance over previous models. It incorporatesmulti-scale feature extraction to enhance depth prediction accuracy acrossvarious object sizes and distances. We propose a composite loss functioncomprising depth loss, gradient edge loss, and Structural Similarity IndexMeasure (SSIM) loss, with fine-tuned weights to optimize the weighted sum,ensuring a balance across different aspects of depth estimation. Experimentalresults on the KITTI dataset show that our model achieves a significantlyfaster inference time of 0.019 seconds, outperforming vision transformers inefficiency while maintaining good accuracy. On the NYU Depth V2 dataset, themodel establishes state-of-the-art performance, with an Absolute Relative Error(ARE) of 0.064, a Root Mean Square Error (RMSE) of 0.228, and an accuracy of89.3% for $\delta$ &lt; 1.25. These metrics demonstrate that our model canaccurately and efficiently predict depth even in challenging scenarios,providing a practical solution for real-time applications.</description><author>Dabbrata Das, Argho Deb Das, Farhan Sadaf</author><pubDate>Thu, 23 Jan 2025 17:18:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11610v4</guid></item><item><title>Look Into the LITE in Deep Learning for Time Series Classification</title><link>http://arxiv.org/abs/2409.02869v2</link><description>Deep learning models have been shown to be a powerful solution for TimeSeries Classification (TSC). State-of-the-art architectures, while producingpromising results on the UCR and the UEA archives , present a high number oftrainable parameters. This can lead to long training with high CO2 emission,power consumption and possible increase in the number of FLoating-pointOperation Per Second (FLOPS). In this paper, we present a new architecture forTSC, the Light Inception with boosTing tEchnique (LITE) with only 2.34% of thenumber of parameters of the state-of-the-art InceptionTime model, whilepreserving performance. This architecture, with only 9, 814 trainableparameters due to the usage of DepthWise Separable Convolutions (DWSC), isboosted by three techniques: multiplexing, custom filters, and dilatedconvolution. The LITE architecture, trained on the UCR, is 2.78 times fasterthan InceptionTime and consumes 2.79 times less CO2 and power. To evaluate theperformance of the proposed architecture on multivariate time series data, weadapt LITE to handle multivariate time series, we call this version LITEMV. Tobring theory into application, we also conducted experiments using LITEMV onmultivariate time series representing human rehabilitation movements, showingthat LITEMV not only is the most efficient model but also the best performingfor this application on the Kimore dataset, a skeleton based humanrehabilitation exercises dataset. Moreover, to address the interpretability ofLITEMV, we present a study using Class Activation Maps to understand theclassification decision taken by the model during evaluation.</description><author>Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier</author><pubDate>Thu, 23 Jan 2025 17:15:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02869v2</guid></item><item><title>Where Do You Go? Pedestrian Trajectory Prediction using Scene Features</title><link>http://arxiv.org/abs/2501.13848v1</link><description>Accurate prediction of pedestrian trajectories is crucial for enhancing thesafety of autonomous vehicles and reducing traffic fatalities involvingpedestrians. While numerous studies have focused on modeling interactions amongpedestrians to forecast their movements, the influence of environmental factorsand scene-object placements has been comparatively underexplored. In thispaper, we present a novel trajectory prediction model that integrates bothpedestrian interactions and environmental context to improve predictionaccuracy. Our approach captures spatial and temporal interactions amongpedestrians within a sparse graph framework. To account for pedestrian-sceneinteractions, we employ advanced image enhancement and semantic segmentationtechniques to extract detailed scene features. These scene and interactionfeatures are then fused through a cross-attention mechanism, enabling the modelto prioritize relevant environmental factors that influence pedestrianmovements. Finally, a temporal convolutional network processes the fusedfeatures to predict future pedestrian trajectories. Experimental resultsdemonstrate that our method significantly outperforms existing state-of-the-artapproaches, achieving ADE and FDE values of 0.252 and 0.372 meters,respectively, underscoring the importance of incorporating both socialinteractions and environmental context in pedestrian trajectory prediction.</description><author>Mohammad Ali Rezaei, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi</author><pubDate>Thu, 23 Jan 2025 17:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13848v1</guid></item><item><title>DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation</title><link>http://arxiv.org/abs/2410.08159v2</link><description>Diffusion models have become the dominant approach for visual generation.They are trained by denoising a Markovian process which gradually adds noise tothe input. We argue that the Markovian property limits the model's ability tofully utilize the generation trajectory, leading to inefficiencies duringtraining and inference. In this paper, we propose DART, a transformer-basedmodel that unifies autoregressive (AR) and diffusion within a non-Markovianframework. DART iteratively denoises image patches spatially and spectrallyusing an AR model that has the same architecture as standard language models.DART does not rely on image quantization, which enables more effective imagemodeling while maintaining flexibility. Furthermore, DART seamlessly trainswith both text and image data in a unified model. Our approach demonstratescompetitive performance on class-conditioned and text-to-image generationtasks, offering a scalable, efficient alternative to traditional diffusionmodels. Through this unified framework, DART sets a new benchmark for scalable,high-quality image synthesis.</description><author>Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, Shuangfei Zhai</author><pubDate>Thu, 23 Jan 2025 17:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08159v2</guid></item><item><title>Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina</title><link>http://arxiv.org/abs/2410.19599v3</link><description>Recent studies suggest large language models (LLMs) can exhibit human-likereasoning, aligning with human behavior in economic experiments, surveys, andpolitical discourse. This has led many to propose that LLMs can be used assurrogates or simulations for humans in social science research. However, LLMsdiffer fundamentally from humans, relying on probabilistic patterns, absent theembodied experiences or survival objectives that shape human cognition. Weassess the reasoning depth of LLMs using the 11-20 money request game. Nearlyall advanced approaches fail to replicate human behavior distributions acrossmany models. Causes of failure are diverse and unpredictable, relating to inputlanguage, roles, and safeguarding. These results advise caution when using LLMsto study human behavior or as surrogates or simulations.</description><author>Yuan Gao, Dokyun Lee, Gordon Burtch, Sina Fazelpour</author><pubDate>Thu, 23 Jan 2025 17:05:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19599v3</guid></item><item><title>Ordered Momentum for Asynchronous SGD</title><link>http://arxiv.org/abs/2407.19234v3</link><description>Distributed learning is essential for training large-scale deep models.Asynchronous SGD (ASGD) and its variants are commonly used distributed learningmethods, particularly in scenarios where the computing capabilities of workersin the cluster are heterogeneous. Momentum has been acknowledged for itsbenefits in both optimization and generalization in deep model training.However, existing works have found that naively incorporating momentum intoASGD can impede the convergence. In this paper, we propose a novel methodcalled ordered momentum (OrMo) for ASGD. In OrMo, momentum is incorporated intoASGD by organizing the gradients in order based on their iteration indexes. Wetheoretically prove the convergence of OrMo with both constant anddelay-adaptive learning rates for non-convex problems. To the best of ourknowledge, this is the first work to establish the convergence analysis of ASGDwith momentum without dependence on the maximum delay. Empirical resultsdemonstrate that OrMo can achieve better convergence performance compared withASGD and other asynchronous methods with momentum.</description><author>Chang-Wei Shi, Yi-Rui Yang, Wu-Jun Li</author><pubDate>Thu, 23 Jan 2025 17:04:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19234v3</guid></item><item><title>Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages</title><link>http://arxiv.org/abs/2501.13836v1</link><description>Most social media users come from non-English speaking countries in theGlobal South. Despite the widespread prevalence of harmful content in theseregions, current moderation systems repeatedly struggle in low-resourcelanguages spoken there. In this work, we examine the challenges AI researchersand practitioners face when building moderation tools for low-resourcelanguages. We conducted semi-structured interviews with 22 AI researchers andpractitioners specializing in automatic detection of harmful content in fourdiverse low-resource languages from the Global South. These are: Tamil fromSouth Asia, Swahili from East Africa, Maghrebi Arabic from North Africa, andQuechua from South America. Our findings reveal that social media companies'restrictions on researchers' access to data exacerbate the historicalmarginalization of these languages, which have long lacked datasets forstudying online harms. Moreover, common preprocessing techniques and languagemodels, predominantly designed for data-rich English, fail to account for thelinguistic complexity of low-resource languages. This leads to critical errorswhen moderating content in Tamil, Swahili, Arabic, and Quechua, which aremorphologically richer than English. Based on our findings, we establish thatthe precarities in current moderation pipelines are rooted in deep systemicinequities and continue to reinforce historical power imbalances. We concludeby discussing multi-stakeholder approaches to improve moderation forlow-resource languages.</description><author>Farhana Shahid, Mona Elswah, Aditya Vashistha</author><pubDate>Thu, 23 Jan 2025 17:01:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13836v1</guid></item><item><title>On the Reasoning Capacity of AI Models and How to Quantify It</title><link>http://arxiv.org/abs/2501.13833v1</link><description>Recent advances in Large Language Models (LLMs) have intensified the debatesurrounding the fundamental nature of their reasoning capabilities. Whileachieving high performance on benchmarks such as GPQA and MMLU, these modelsexhibit limitations in more complex reasoning tasks, highlighting the need formore rigorous evaluation methodologies. We propose a novel phenomenologicalapproach that goes beyond traditional accuracy metrics to probe the underlyingmechanisms of model behavior, establishing a framework that could broadlyimpact how we analyze and understand AI systems. Using positional bias inmultiple-choice reasoning tasks as a case study, we demonstrate how systematicperturbations can reveal fundamental aspects of model decision-making. Toanalyze these behaviors, we develop two complementary phenomenological models:a Probabilistic Mixture Model (PMM) that decomposes model responses intoreasoning, memorization, and guessing components and an Information-TheoreticConsistency (ITC) analysis that quantifies the relationship between modelconfidence and strategy selection. Through controlled experiments on reasoningbenchmarks, we show that true reasoning remains challenging for current models,with apparent success often relying on sophisticated combinations ofmemorization and pattern matching rather than genuine logical deduction. Morefundamentally, we demonstrate that accuracy alone often overstates a model'sreasoning abilities, as model behavior can be characterized through underlyingmechanisms in the phase space of cognitive strategies, revealing how modelsdynamically balance different approaches when responding to queries. Thisframework enables quantitative criteria for real-world deployments, allowingapplications to specify reliability thresholds based on strategy distributionsrather than aggregate performance metrics.</description><author>Santosh Kumar Radha, Oktay Goktas</author><pubDate>Thu, 23 Jan 2025 16:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13833v1</guid></item><item><title>Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing</title><link>http://arxiv.org/abs/2501.13831v1</link><description>Large Language Models (LLMs) excel at rewriting tasks such as text styletransfer and grammatical error correction. While there is considerable overlapbetween the inputs and outputs in these tasks, the decoding cost stillincreases with output length, regardless of the amount of overlap. Byleveraging the overlap between the input and the output, Kaneko and Okazaki(2023) proposed model-agnostic edit span representations to compress therewrites to save computation. They reported an output length reduction rate ofnearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,we propose alternative edit phrase representations inspired by phrase-basedstatistical machine translation. We systematically compare our phrasalrepresentations with their span representations. We apply the LLM rewritingmodel to the task of Automatic Speech Recognition (ASR) post editing and showthat our target-phrase-only edit representation has the bestefficiency-accuracy trade-off. On the LibriSpeech test set, our method closes50-60% of the WER gap between the edit span model and the full rewrite modelwhile losing only 10-20% of the length reduction rate of the edit span model.</description><author>Hao Zhang, Felix Stahlberg, Shankar Kumar</author><pubDate>Thu, 23 Jan 2025 16:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13831v1</guid></item><item><title>A space-decoupling framework for optimization on bounded-rank matrices with orthogonally invariant constraints</title><link>http://arxiv.org/abs/2501.13830v1</link><description>Imposing additional constraints on low-rank optimization has garnered growinginterest. However, the geometry of coupled constraints hampers thewell-developed low-rank structure and makes the problem intricate. To this end,we propose a space-decoupling framework for optimization on bounded-rankmatrices with orthogonally invariant constraints. The ``space-decoupling" isreflected in several ways. We show that the tangent cone of coupled constraintsis the intersection of tangent cones of each constraint. Moreover, we decouplethe intertwined bounded-rank and orthogonally invariant constraints into twospaces, leading to optimization on a smooth manifold. Implementing Riemannianalgorithms on this manifold is painless as long as the geometry of additionalconstraints is known. In addition, we unveil the equivalence between thereformulated problem and the original problem. Numerical experiments onreal-world applications -- spherical data fitting, graph similarity measuring,low-rank SDP, model reduction of Markov processes, reinforcement learning, anddeep learning -- validate the superiority of the proposed framework.</description><author>Yan Yang, Bin Gao, Ya-xiang Yuan</author><pubDate>Thu, 23 Jan 2025 16:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13830v1</guid></item><item><title>MV-GMN: State Space Model for Multi-View Action Recognition</title><link>http://arxiv.org/abs/2501.13829v1</link><description>Recent advancements in multi-view action recognition have largely relied onTransformer-based models. While effective and adaptable, these models oftenrequire substantial computational resources, especially in scenarios withmultiple views and multiple temporal sequences. Addressing this limitation,this paper introduces the MV-GMN model, a state-space model specificallydesigned to efficiently aggregate multi-modal data (RGB and skeleton),multi-view perspectives, and multi-temporal information for action recognitionwith reduced computational complexity. The MV-GMN model employs an innovativeMulti-View Graph Mamba network comprising a series of MV-GMN blocks. Each blockincludes a proposed Bidirectional State Space Block and a GCN module. TheBidirectional State Space Block introduces four scanning strategies, includingview-prioritized and time-prioritized approaches. The GCN module leveragesrule-based and KNN-based methods to construct the graph network, effectivelyintegrating features from different viewpoints and temporal instances.Demonstrating its efficacy, MV-GMN outperforms the state-of-the-arts on severaldatasets, achieving notable accuracies of 97.3\% and 96.7\% on the NTU RGB+D120 dataset in cross-subject and cross-view scenarios, respectively. MV-GMNalso surpasses Transformer-based baselines while requiring only linearinference complexity, underscoring the model's ability to reduce computationalload and enhance the scalability and applicability of multi-view actionrecognition technologies.</description><author>Yuhui Lin, Jiaxuan Lu, Yue Yong, Jiahao Zhang</author><pubDate>Thu, 23 Jan 2025 16:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13829v1</guid></item><item><title>PhotoGAN: Generative Adversarial Neural Network Acceleration with Silicon Photonics</title><link>http://arxiv.org/abs/2501.13828v1</link><description>Generative Adversarial Networks (GANs) are at the forefront of AI innovation,driving advancements in areas such as image synthesis, medical imaging, anddata augmentation. However, the unique computational operations within GANs,such as transposed convolutions and instance normalization, introducesignificant inefficiencies when executed on traditional electronicaccelerators, resulting in high energy consumption and suboptimal performance.To address these challenges, we introduce PhotoGAN, the first silicon-photonicaccelerator designed to handle the specialized operations of GAN models. Byleveraging the inherent high throughput and energy efficiency of siliconphotonics, PhotoGAN offers an innovative, reconfigurable architecture capableof accelerating transposed convolutions and other GAN-specific layers. Theaccelerator also incorporates a sparse computation optimization technique toreduce redundant operations, improving computational efficiency. Ourexperimental results demonstrate that PhotoGAN achieves at least 4.4x higherGOPS and 2.18x lower energy-per-bit (EPB) compared to state-of-the-artaccelerators, including GPUs and TPUs. These findings showcase PhotoGAN as apromising solution for the next generation of GAN acceleration, providingsubstantial gains in both performance and energy efficiency.</description><author>Tharini Suresh, Salma Afifi, Sudeep Pasricha</author><pubDate>Thu, 23 Jan 2025 16:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13828v1</guid></item><item><title>Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos</title><link>http://arxiv.org/abs/2501.13826v1</link><description>Humans acquire knowledge through three cognitive stages: perceivinginformation, comprehending knowledge, and adapting knowledge to solve novelproblems. Videos serve as an effective medium for this learning process,facilitating a progression through these cognitive stages. However, existingvideo benchmarks fail to systematically evaluate the knowledge acquisitioncapabilities in Large Multimodal Models (LMMs). To address this gap, weintroduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed toassess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMUfeatures a curated collection of 300 expert-level videos and 900human-annotated questions across six disciplines, evaluating knowledgeacquisition through stage-aligned question-answer pairs: Perception,Comprehension, and Adaptation. A proposed knowledge gain metric,{\Delta}knowledge, quantifies improvement in performance after video viewing.Evaluation of LMMs reveals a steep decline in performance as cognitive demandsincrease and highlights a significant gap between human and model knowledgeacquisition, underscoring the need for methods to enhance LMMs' capability tolearn and adapt from videos.</description><author>Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, Ziwei Liu</author><pubDate>Thu, 23 Jan 2025 16:51:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13826v1</guid></item><item><title>Hallucinations Can Improve Large Language Models in Drug Discovery</title><link>http://arxiv.org/abs/2501.13824v1</link><description>Concerns about hallucinations in Large Language Models (LLMs) have beenraised by researchers, yet their potential in areas where creativity is vital,such as drug discovery, merits exploration. In this paper, we come up with thehypothesis that hallucinations can improve LLMs in drug discovery. To verifythis hypothesis, we use LLMs to describe the SMILES string of molecules innatural language and then incorporate these descriptions as part of the promptto address specific tasks in drug discovery. Evaluated on seven LLMs and fiveclassification tasks, our findings confirm the hypothesis: LLMs can achievebetter performance with text containing hallucinations. Notably, Llama-3.1-8Bachieves an 18.35% gain in ROC-AUC compared to the baseline withouthallucination. Furthermore, hallucinations generated by GPT-4o provide the mostconsistent improvements across models. Additionally, we conduct empiricalanalyses and a case study to investigate key factors affecting performance andthe underlying reasons. Our research sheds light on the potential use ofhallucinations for LLMs and offers new perspectives for future researchleveraging LLMs in drug discovery.</description><author>Shuzhou Yuan, Michael Färber</author><pubDate>Thu, 23 Jan 2025 16:45:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13824v1</guid></item><item><title>A Survey on Brain-Inspired Deep Learning via Predictive Coding</title><link>http://arxiv.org/abs/2308.07870v2</link><description>Artificial intelligence (AI) is rapidly becoming one of the key technologiesof this century. The majority of results in AI thus far have been achievedusing deep neural networks trained with the error backpropagation learningalgorithm. However, the ubiquitous adoption of this approach has highlightedsome important limitations such as substantial computational cost, difficultyin quantifying uncertainty, lack of robustness, unreliability, and biologicalimplausibility. It is possible that addressing these limitations may requireschemes that are inspired and guided by neuroscience theories. One such theory,called predictive coding (PC), has shown promising performance in machineintelligence tasks, exhibiting exciting properties that make it potentiallyvaluable for the machine learning community: PC can model informationprocessing in different brain areas, can be used in cognitive control androbotics, and has a solid mathematical grounding in variational inference,offering a powerful inversion scheme for a specific class of continuous-stategenerative models. With the hope of foregrounding research in this direction,we survey the literature that has contributed to this perspective, highlightingthe many ways that PC might play a role in the future of machine learning andcomputational intelligence at large.</description><author>Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl Friston, Alexander Ororbia</author><pubDate>Thu, 23 Jan 2025 16:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07870v2</guid></item><item><title>Consistent spectral clustering in sparse tensor block models</title><link>http://arxiv.org/abs/2501.13820v1</link><description>High-order clustering aims to classify objects in multiway datasets that areprevalent in various fields such as bioinformatics, social network analysis,and recommendation systems. These tasks often involve data that is sparse andhigh-dimensional, presenting significant statistical and computationalchallenges. This paper introduces a tensor block model specifically designedfor sparse integer-valued data tensors. We propose a simple spectral clusteringalgorithm augmented with a trimming step to mitigate noise fluctuations, andidentify a density threshold that ensures the algorithm's consistency. Ourapproach models sparsity using a sub-Poisson noise concentration framework,accommodating heavier than sub-Gaussian tails. Remarkably, this natural classof tensor block models is closed under aggregation across arbitrary modes.Consequently, we obtain a comprehensive framework for evaluating the tradeoffbetween signal loss and noise reduction during data aggregation. The analysisis based on a novel concentration bound for sparse random Gram matrices. Thetheoretical findings are illustrated through simulation experiments.</description><author>Ian Välimaa, Lasse Leskelä</author><pubDate>Thu, 23 Jan 2025 16:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13820v1</guid></item><item><title>Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data</title><link>http://arxiv.org/abs/2501.13818v1</link><description>Deep neural networks are increasingly employed in high-stakes medicalapplications, despite their tendency for shortcut learning in the presence ofspurious correlations, which can have potentially fatal consequences inpractice. Detecting and mitigating shortcut behavior is a challenging task thatoften requires significant labeling efforts from domain experts. To alleviatethis problem, we introduce a semi-automated framework for the identification ofspurious behavior from both data and model perspective by leveraging insightsfrom eXplainable Artificial Intelligence (XAI). This allows the retrieval ofspurious data points and the detection of model circuits that encode theassociated prediction rules. Moreover, we demonstrate how these shortcutencodings can be used for XAI-based sample- and pixel-level data annotation,providing valuable information for bias mitigation methods to unlearn theundesired shortcut behavior. We show the applicability of our framework usingfour medical datasets across two modalities, featuring controlled andreal-world spurious correlations caused by data artifacts. We successfullyidentify and mitigate these biases in VGG16, ResNet50, and contemporary VisionTransformer models, ultimately increasing their robustness and applicabilityfor real-world medical tasks.</description><author>Frederik Pahde, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek</author><pubDate>Thu, 23 Jan 2025 16:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13818v1</guid></item><item><title>By-Example Synthesis of Vector Textures</title><link>http://arxiv.org/abs/2501.13812v1</link><description>We propose a new method for synthesizing an arbitrarily sized novel vectortexture given a single raster exemplar. Our method first segments the exemplarto extract the primary textons, and then clusters them based on visualsimilarity. We then compute a descriptor to capture each texton's neighborhoodwhich contains the inter-category relationships that are used at synthesistime. Next, we use a simple procedure to both extract and place the secondarytextons behind the primary polygons. Finally, our method constructs a gradientfield for the background which is defined by a set of data points and colors.The color of the secondary polygons are also adjusted to better match thegradient field. To compare our work with other methods, we use a wide range ofperceptual-based metrics.</description><author>Christopher Palazzolo, Oliver van Kaick, David Mould</author><pubDate>Thu, 23 Jan 2025 16:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13812v1</guid></item><item><title>Learning to Help in Multi-Class Settings</title><link>http://arxiv.org/abs/2501.13810v1</link><description>Deploying complex machine learning models on resource-constrained devices ischallenging due to limited computational power, memory, and modelretrainability. To address these limitations, a hybrid system can beestablished by augmenting the local model with a server-side model, wheresamples are selectively deferred by a rejector and then sent to the server forprocessing. The hybrid system enables efficient use of computational resourceswhile minimizing the overhead associated with server usage. The recentlyproposed Learning to Help (L2H) model trains a server model given a fixed local(client) model, differing from the Learning to Defer (L2D) framework, whichtrains the client for a fixed (expert) server. In both L2D and L2H, thetraining includes learning a rejector at the client to determine when to querythe server. In this work, we extend the L2H model from binary to multi-classclassification problems and demonstrate its applicability in a number ofdifferent scenarios of practical interest in which access to the server may belimited by cost, availability, or policy. We derive a stage-switching surrogateloss function that is differentiable, convex, and consistent with the Bayesrule corresponding to the 0-1 loss for the L2H model. Experiments show that ourproposed methods offer an efficient and practical solution for multi-classclassification in resource-constrained environments.</description><author>Yu Wu, Yansong Li, Zeyu Dong, Nitya Sathyavageeswaran, Anand D. Sarwate</author><pubDate>Thu, 23 Jan 2025 16:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13810v1</guid></item><item><title>MuMA-ToM: Multi-modal Multi-Agent Theory of Mind</title><link>http://arxiv.org/abs/2408.12574v4</link><description>Understanding people's social interactions in complex real-world scenariosoften relies on intricate mental reasoning. To truly understand how and whypeople interact with one another, we must infer the underlying mental statesthat give rise to the social interactions, i.e., Theory of Mind reasoning inmulti-agent interactions. Additionally, social interactions are oftenmulti-modal -- we can watch people's actions, hear their conversations, and/orread about their past behaviors. For AI systems to successfully and safelyinteract with people in real-world environments, they also need to understandpeople's mental states as well as their inferences about each other's mentalstates based on multi-modal information about their interactions. For this, weintroduce MuMA-ToM, a Multi-modal Multi-Agent Theory of Mind benchmark.MuMA-ToM is the first multi-modal Theory of Mind benchmark that evaluatesmental reasoning in embodied multi-agent interactions. In MuMA-ToM, we providevideo and text descriptions of people's multi-modal behavior in realistichousehold environments. Based on the context, we then ask questions aboutpeople's goals, beliefs, and beliefs about others' goals. We validated MuMA-ToMin a human experiment and provided a human baseline. We also proposed a novelmulti-modal, multi-agent ToM model, LIMP (Language model-based InverseMulti-agent Planning). Our experimental results show that LIMP significantlyoutperforms state-of-the-art methods, including large multi-modal models (e.g.,GPT-4o, Gemini-1.5 Pro) and a recent multi-modal ToM model, BIP-ALM.</description><author>Haojun Shi, Suyu Ye, Xinyu Fang, Chuanyang Jin, Leyla Isik, Yen-Ling Kuo, Tianmin Shu</author><pubDate>Thu, 23 Jan 2025 16:31:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12574v4</guid></item><item><title>Generation of reusable learning objects from digital medical collections: An analysis based on the MASMDOA framework</title><link>http://arxiv.org/abs/2501.13806v1</link><description>Learning Objects represent a widespread approach to structuring instructionalmaterials in a large variety of educational contexts. The main aim of this workconsists of analyzing from a qualitative point of view the process ofgenerating reusable learning objects (RLOs) followed by Clavy, a tool that canbe used to retrieve data from multiple medical knowledge sources andreconfigure such sources in diverse multimedia-based structures andorganizations. From these organizations, Clavy is able to generate learningobjects which can be adapted to various instructional healthcare scenarios withseveral types of user profiles and distinct learning requirements. Moreover,Clavy provides the capability of exporting these learning objects througheducational standard specifications, which improves their reusability features.The analysis insights highlight the importance of having a tool able totransfer knowledge from the available digital medical collections to learningobjects that can be easily accessed by medical students and healthcarepractitioners through the most popular e-learning platforms.</description><author>Félix Buendía, Joaquín Gayoso-Cabada, José-Luis Sierra</author><pubDate>Thu, 23 Jan 2025 16:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13806v1</guid></item><item><title>EgoHand: Ego-centric Hand Pose Estimation and Gesture Recognition with Head-mounted Millimeter-wave Radar and IMUs</title><link>http://arxiv.org/abs/2501.13805v1</link><description>Recent advanced Virtual Reality (VR) headsets, such as the Apple Vision Pro,employ bottom-facing cameras to detect hand gestures and inputs, which offersusers significant convenience in VR interactions. However, these bottom-facingcameras can sometimes be inconvenient and pose a risk of unintentionallyexposing sensitive information, such as private body parts or personalsurroundings. To mitigate these issues, we introduce EgoHand. This systemprovides an alternative solution by integrating millimeter-wave radar and IMUsfor hand gesture recognition, thereby offering users an additional option forgesture interaction that enhances privacy protection. To accurately recognizehand gestures, we devise a two-stage skeleton-based gesture recognition scheme.In the first stage, a novel end-to-end Transformer architecture is employed toestimate the coordinates of hand joints. Subsequently, these estimated jointcoordinates are utilized for gesture recognition. Extensive experimentsinvolving 10 subjects show that EgoHand can detect hand gestures with 90.8%accuracy. Furthermore, EgoHand demonstrates robust performance across a varietyof cross-domain tests, including different users, dominant hands, bodypostures, and scenes.</description><author>Yizhe Lv, Tingting Zhang, Yunpeng Song, Han Ding, Jinsong Han, Fei Wang</author><pubDate>Thu, 23 Jan 2025 16:25:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13805v1</guid></item><item><title>3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2404.00409v2</link><description>In this paper, we present an implicit surface reconstruction method with 3DGaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3Dreconstruction with intricate details while inheriting the high efficiency andrendering quality of 3DGS. The key insight is incorporating an implicit signeddistance field (SDF) within 3D Gaussians to enable them to be aligned andjointly optimized. First, we introduce a differentiable SDF-to-opacitytransformation function that converts SDF values into corresponding Gaussians'opacities. This function connects the SDF and 3D Gaussians, allowing forunified optimization and enforcing surface constraints on the 3D Gaussians.During learning, optimizing the 3D Gaussians provides supervisory signals forSDF learning, enabling the reconstruction of intricate details. However, thisonly provides sparse supervisory signals to the SDF at locations occupied byGaussians, which is insufficient for learning a continuous SDF. Then, toaddress this limitation, we incorporate volumetric rendering and align therendered geometric attributes (depth, normal) with those derived from 3DGaussians. This consistency regularization introduces supervisory signals tolocations not covered by discrete 3D Gaussians, effectively eliminatingredundant surfaces outside the Gaussian sampling range. Our extensiveexperimental results demonstrate that our 3DGSR method enables high-quality 3Dsurface reconstruction while preserving the efficiency and rendering quality of3DGS. Besides, our method competes favorably with leading surfacereconstruction techniques while offering a more efficient learning process andmuch better rendering qualities. The code will be available athttps://github.com/CVMI-Lab/3DGSR.</description><author>Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi</author><pubDate>Thu, 23 Jan 2025 16:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00409v2</guid></item><item><title>PromptMono: Cross Prompting Attention for Self-Supervised Monocular Depth Estimation in Challenging Environments</title><link>http://arxiv.org/abs/2501.13796v1</link><description>Considerable efforts have been made to improve monocular depth estimationunder ideal conditions. However, in challenging environments, monocular depthestimation still faces difficulties. In this paper, we introduce visual promptlearning for predicting depth across different environments within a unifiedmodel, and present a self-supervised learning framework called PromptMono. Itemploys a set of learnable parameters as visual prompts to capturedomain-specific knowledge. To integrate prompting information into imagerepresentations, a novel gated cross prompting attention (GCPA) module isproposed, which enhances the depth estimation in diverse conditions. Weevaluate the proposed PromptMono on the Oxford Robotcar dataset and thenuScenes dataset. Experimental results demonstrate the superior performance ofthe proposed method.</description><author>Changhao Wang, Guanwen Zhang, Zhengyun Cheng, Wei Zhou</author><pubDate>Thu, 23 Jan 2025 16:14:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13796v1</guid></item><item><title>Training-Free Zero-Shot Temporal Action Detection with Vision-Language Models</title><link>http://arxiv.org/abs/2501.13795v1</link><description>Existing zero-shot temporal action detection (ZSTAD) methods predominantlyuse fully supervised or unsupervised strategies to recognize unseen activities.However, these training-based methods are prone to domain shifts and requirehigh computational costs, which hinder their practical applicability inreal-world scenarios. In this paper, unlike previous works, we propose atraining-Free Zero-shot temporal Action Detection (FreeZAD) method, leveragingexisting vision-language (ViL) models to directly classify and localize unseenactivities within untrimmed videos without any additional fine-tuning oradaptation. We mitigate the need for explicit temporal modeling and reliance onpseudo-label quality by designing the LOGarithmic decay weightedOuter-Inner-Contrastive Score (LogOIC) and frequency-based ActionnessCalibration. Furthermore, we introduce a test-time adaptation (TTA) strategyusing Prototype-Centric Sampling (PCS) to expand FreeZAD, enabling ViL modelsto adapt more effectively for ZSTAD. Extensive experiments on the THUMOS14 andActivityNet-1.3 datasets demonstrate that our training-free method outperformsstate-of-the-art unsupervised methods while requiring only 1/13 of the runtime.When equipped with TTA, the enhanced method further narrows the gap with fullysupervised methods.</description><author>Chaolei Han, Hongsong Wang, Jidong Kuang, Lei Zhang, Jie Gui</author><pubDate>Thu, 23 Jan 2025 16:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13795v1</guid></item><item><title>Unveiling the Power of Noise Priors: Enhancing Diffusion Models for Mobile Traffic Prediction</title><link>http://arxiv.org/abs/2501.13794v1</link><description>Accurate prediction of mobile traffic, \textit{i.e.,} network traffic fromcellular base stations, is crucial for optimizing network performance andsupporting urban development. However, the non-stationary nature of mobiletraffic, driven by human activity and environmental changes, leads to bothregular patterns and abrupt variations. Diffusion models excel in capturingsuch complex temporal dynamics due to their ability to capture the inherentuncertainties. Most existing approaches prioritize designing novel denoisingnetworks but often neglect the critical role of noise itself, potentiallyleading to sub-optimal performance. In this paper, we introduce a novelperspective by emphasizing the role of noise in the denoising process. Ouranalysis reveals that noise fundamentally shapes mobile traffic predictions,exhibiting distinct and consistent patterns. We propose NPDiff, a frameworkthat decomposes noise into \textit{prior} and \textit{residual} components,with the \textit{prior} derived from data dynamics, enhancing the model'sability to capture both regular and abrupt variations. NPDiff can seamlesslyintegrate with various diffusion-based prediction models, deliveringpredictions that are effective, efficient, and robust. Extensive experimentsdemonstrate that it achieves superior performance with an improvement over30\%, offering a new perspective on leveraging diffusion models in this domain.</description><author>Zhi Sheng, Yuan Yuan, Jingtao Ding, Yong Li</author><pubDate>Thu, 23 Jan 2025 16:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13794v1</guid></item><item><title>Reducing Reasoning Costs: The Path of Optimization for Chain of Thought via Sparse Attention Mechanism</title><link>http://arxiv.org/abs/2411.09111v5</link><description>In order to address the chain of thought in the large language modelinference cost surge, this research proposes to use a sparse attentionmechanism that only focuses on a few relevant tokens. The researcherconstructed a new attention mechanism and used GiantRabbit trained with customGPTs as an experimental tool. The experiment tested and compared the reasoningtime, correctness score and chain of thought length of this model and o1Preview in solving the linear algebra test questions of MIT OpenCourseWare. Theresults show that GiantRabbit's reasoning time and chain of thought length aresignificantly lower than o1 Preview. It verifies the feasibility of sparseattention mechanism for optimizing chain of thought reasoning. Detailedarchitectural details and experimental process have been uploaded to Github,the link is:https://github.com/brucewang123456789/GeniusTrail.git.</description><author>Libo Wang</author><pubDate>Thu, 23 Jan 2025 16:09:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09111v5</guid></item><item><title>Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression</title><link>http://arxiv.org/abs/2501.13790v1</link><description>We analyze two variants of Local Gradient Descent applied to distributedlogistic regression with heterogeneous, separable data and show convergence atthe rate $O(1/KR)$ for $K$ local steps and sufficiently large $R$ communicationrounds. In contrast, all existing convergence guarantees for Local GD appliedto any problem are at least $\Omega(1/R)$, meaning they fail to show thebenefit of local updates. The key to our improved guarantee is showing progresson the logistic regression objective when using a large stepsize $\eta \gg1/K$, whereas prior analysis depends on $\eta \leq 1/K$.</description><author>Michael Crawshaw, Blake Woodworth, Mingrui Liu</author><pubDate>Thu, 23 Jan 2025 16:09:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13790v1</guid></item><item><title>Parameter-Efficient Fine-Tuning for Foundation Models</title><link>http://arxiv.org/abs/2501.13787v1</link><description>This survey delves into the realm of Parameter-Efficient Fine-Tuning (PEFT)within the context of Foundation Models (FMs). PEFT, a cost-effectivefine-tuning technique, minimizes parameters and computational complexity whilestriving for optimal downstream task performance. FMs, like ChatGPT, DALL-E,and LLaVA specialize in language understanding, generative tasks, andmultimodal tasks, trained on diverse datasets spanning text, images, andvideos. The diversity of FMs guides various adaptation strategies for PEFT.Therefore, this survey aims to provide a comprehensive overview of PEFTtechniques applied to diverse FMs and address critical gaps in understandingthe techniques, trends, and applications. We start by providing a detaileddevelopment of FMs and PEFT. Subsequently, we systematically review the keycategories and core mechanisms of PEFT across diverse FMs to offer acomprehensive understanding of trends. We also explore the most recentapplications across various FMs to demonstrate the versatility of PEFT,shedding light on the integration of systematic PEFT methods with a range ofFMs. Furthermore, we identify potential research and development directions forimproving PEFTs in the future. This survey provides a valuable resource forboth newcomers and experts seeking to understand and use the power of PEFTacross FMs. All reviewed papers are listed at\url{https://github.com/THUDM/Awesome-Parameter-Efficient-Fine-Tuning-for-Foundation-Models}.</description><author>Dan Zhang, Tao Feng, Lilong Xue, Yuandong Wang, Yuxiao Dong, Jie Tang</author><pubDate>Thu, 23 Jan 2025 16:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13787v1</guid></item><item><title>Fast Iterative and Task-Specific Imputation with Online Learning</title><link>http://arxiv.org/abs/2501.13786v1</link><description>Missing feature values are a significant hurdle for downstreammachine-learning tasks such as classification and regression. However, they arepervasive in multiple real-life use cases, for instance, in drug discoveryresearch. Moreover, imputation methods might be time-consuming and offer fewguarantees on the imputation quality, especially for not-missing-at-randommechanisms. We propose an imputation approach named F3I based on the iterativeimprovement of a K-nearest neighbor imputation that learns the weights for eachneighbor of a data point, optimizing for the most likely distribution of pointsover data points. This algorithm can also be jointly trained with a downstreamtask on the imputed values. We provide a theoretical analysis of the imputationquality by F3I for several types of missing mechanisms. We also demonstrate theperformance of F3I on both synthetic data sets and real-life drug repurposingand handwritten-digit recognition data.</description><author>Rahul Bordoloi, Clémence Réda, Saptarshi Bej</author><pubDate>Thu, 23 Jan 2025 16:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13786v1</guid></item><item><title>How to Efficiently Annotate Images for Best-Performing Deep Learning Based Segmentation Models: An Empirical Study with Weak and Noisy Annotations and Segment Anything Model</title><link>http://arxiv.org/abs/2312.10600v3</link><description>Deep neural networks (DNNs) have demonstrated exceptional performance acrossvarious image segmentation tasks. However, the process of preparing datasetsfor training segmentation DNNs is both labor-intensive and costly, as ittypically requires pixel-level annotations for each object of interest. Tomitigate this challenge, alternative approaches such as using weak labels(e.g., bounding boxes or scribbles) or less precise (noisy) annotations can beemployed. Noisy and weak labels are significantly quicker to generate, allowingfor more annotated images within the same time frame. However, the potentialdecrease in annotation quality may adversely impact the segmentationperformance of the resulting model. In this study, we conducted a comprehensivecost-effectiveness evaluation on six variants of annotation strategies (9~10sub-variants in total) across 4 datasets and conclude that the common practiceof precisely outlining objects of interest is virtually never the optimalapproach when annotation budget is limited. Both noisy and weak annotationsshowed usage cases that yield similar performance to the perfectly annotatedcounterpart, yet had significantly better cost-effectiveness. We hope ourfindings will help researchers be aware of the different available options anduse their annotation budgets more efficiently, especially in cases whereaccurately acquiring labels for target objects is particularly costly. Our codewill be made available on https://github.com/yzluka/AnnotationEfficiency2D.</description><author>Yixin Zhang, Shen Zhao, Hanxue Gu, Maciej A. Mazurowski</author><pubDate>Thu, 23 Jan 2025 16:02:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10600v3</guid></item><item><title>Defending against Adversarial Malware Attacks on ML-based Android Malware Detection Systems</title><link>http://arxiv.org/abs/2501.13782v1</link><description>Android malware presents a persistent threat to users' privacy and dataintegrity. To combat this, researchers have proposed machine learning-based(ML-based) Android malware detection (AMD) systems. However, adversarialAndroid malware attacks compromise the detection integrity of the ML-based AMDsystems, raising significant concerns. Existing defenses against adversarialAndroid malware provide protections against feature space attacks whichgenerate adversarial feature vectors only, leaving protection against realisticthreats from problem space attacks which generate real adversarial malware anopen problem. In this paper, we address this gap by proposing ADD, a practicaladversarial Android malware defense framework designed as a plug-in to enhancethe adversarial robustness of the ML-based AMD systems against problem spaceattacks. Our extensive evaluation across various ML-based AMD systemsdemonstrates that ADD is effective against state-of-the-art problem spaceadversarial Android malware attacks. Additionally, ADD shows the defenseeffectiveness in enhancing the adversarial robustness of real-world antivirussolutions.</description><author>Ping He, Lorenzo Cavallaro, Shouling Ji</author><pubDate>Thu, 23 Jan 2025 15:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13782v1</guid></item><item><title>Matrix Completion in Group Testing: Bounds and Simulations</title><link>http://arxiv.org/abs/2501.13780v1</link><description>The main goal of group testing is to identify a small number of defectiveitems in a large population of items. A test on a subset of items is positiveif the subset contains at least one defective item and negative otherwise. Innon-adaptive design, all tests can be tested simultaneously and represented bya measurement matrix in which a row and a column represent a test and an item,respectively. An entry in row $i$ and column $j$ is 1 if item $j$ belongs tothe test $i$ and is 0 otherwise. Given an unknown set of defective items, theobjective is to design a measurement matrix such that, by observing itscorresponding outcome vector, the defective items can be recovered efficiently.The basic trait of this approach is that the measurement matrix has remainedunchanged throughout the course of generating the outcome vector and recoveringdefective items. In this paper, we study the case in which some entries in themeasurement matrix are erased, called \emph{the missing measurement matrix},before the recovery phase of the defective items, and our objective is to fullyrecover the measurement matrix from the missing measurement matrix. Inparticular, we show that some specific rows with erased entries provideinformation aiding the recovery while others do not. Given measurement matricesand erased entries follow the Bernoulli distribution, we show that before theerasing event happens, sampling sufficient sets of defective items and theircorresponding outcome vectors can help us recover the measurement matrix fromthe missing measurement matrix.</description><author>Trung-Khang Tran, Thach V. Bui</author><pubDate>Thu, 23 Jan 2025 15:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13780v1</guid></item><item><title>Not Every AI Problem is a Data Problem: We Should Be Intentional About Data Scaling</title><link>http://arxiv.org/abs/2501.13779v1</link><description>While Large Language Models require more and more data to train and scale,rather than looking for any data to acquire, we should consider what types oftasks are more likely to benefit from data scaling. We should be intentional inour data acquisition. We argue that the topology of data itself informs whichtasks to prioritize in data scaling, and shapes the development of the nextgeneration of compute paradigms for tasks where data scaling is inefficient, oreven insufficient.</description><author>Tanya Rodchenko, Natasha Noy, Nino Scherrer, Jennifer Prendki</author><pubDate>Thu, 23 Jan 2025 15:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13779v1</guid></item><item><title>Explainable XR: Understanding User Behaviors of XR Environments using LLM-assisted Analytics Framework</title><link>http://arxiv.org/abs/2501.13778v1</link><description>We present Explainable XR, an end-to-end framework for analyzing userbehavior in diverse eXtended Reality (XR) environments by leveraging LargeLanguage Models (LLMs) for data interpretation assistance. Existing XR useranalytics frameworks face challenges in handling cross-virtuality - AR, VR, MR- transitions, multi-user collaborative application scenarios, and thecomplexity of multimodal data. Explainable XR addresses these challenges byproviding a virtuality-agnostic solution for the collection, analysis, andvisualization of immersive sessions. We propose three main components in ourframework: (1) A novel user data recording schema, called User ActionDescriptor (UAD), that can capture the users' multimodal actions, along withtheir intents and the contexts; (2) a platform-agnostic XR session recorder,and (3) a visual analytics interface that offers LLM-assisted insights tailoredto the analysts' perspectives, facilitating the exploration and analysis of therecorded XR session data. We demonstrate the versatility of Explainable XR bydemonstrating five use-case scenarios, in both individual and collaborative XRapplications across virtualities. Our technical evaluation and user studiesshow that Explainable XR provides a highly usable analytics solution forunderstanding user actions and delivering multifaceted, actionable insightsinto user behaviors in immersive environments.</description><author>Yoonsang Kim, Zainab Aamir, Mithilesh Singh, Saeed Boorboor, Klaus Mueller, Arie E. Kaufman</author><pubDate>Thu, 23 Jan 2025 15:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13778v1</guid></item><item><title>Crossfire: An Elastic Defense Framework for Graph Neural Networks Under Bit Flip Attacks</title><link>http://arxiv.org/abs/2501.13776v1</link><description>Bit Flip Attacks (BFAs) are a well-established class of adversarial attacks,originally developed for Convolutional Neural Networks within the computervision domain. Most recently, these attacks have been extended to target GraphNeural Networks (GNNs), revealing significant vulnerabilities. This newdevelopment naturally raises questions about the best strategies to defend GNNsagainst BFAs, a challenge for which no solutions currently exist. Given theapplications of GNNs in critical fields, any defense mechanism must not onlymaintain network performance, but also verifiably restore the network to itspre-attack state. Verifiably restoring the network to its pre-attack state alsoeliminates the need for costly evaluations on test data to ensure networkquality. We offer first insights into the effectiveness of existing honeypot-and hashing-based defenses against BFAs adapted from the computer vision domainto GNNs, and characterize the shortcomings of these approaches. To overcometheir limitations, we propose Crossfire, a hybrid approach that exploits weightsparsity and combines hashing and honeypots with bit-level correction ofout-of-distribution weight elements to restore network integrity. Crossfire isretraining-free and does not require labeled data. Averaged over 2,160experiments on six benchmark datasets, Crossfire offers a 21.8% higherprobability than its competitors of reconstructing a GNN attacked by a BFA toits pre-attack state. These experiments cover up to 55 bit flips from variousattacks. Moreover, it improves post-repair prediction quality by 10.85%.Computational and storage overheads are negligible compared to the inherentcomplexity of even the simplest GNNs.</description><author>Lorenz Kummer, Samir Moustafa, Wilfried Gansterer, Nils Kriege</author><pubDate>Thu, 23 Jan 2025 15:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13776v1</guid></item><item><title>Do Large Language Models Truly Understand Geometric Structures?</title><link>http://arxiv.org/abs/2501.13773v1</link><description>Geometric ability is a significant challenge for large language models (LLMs)due to the need for advanced spatial comprehension and abstract thinking.Existing datasets primarily evaluate LLMs on their final answers, but theycannot truly measure their true understanding of geometric structures, as LLMscan arrive at correct answers by coincidence. To fill this gap, we introducethe GeomRel dataset, designed to evaluate LLMs' understanding of geometricstructures by isolating the core step of geometric relationship identificationin problem-solving. Using this benchmark, we conduct thorough evaluations ofdiverse LLMs and identify key limitations in understanding geometricstructures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,which enhances LLMs' ability to identify geometric relationships, resulting insignificant performance improvements.</description><author>Xiaofeng Wang, Yiming Wang, Wenhong Zhu, Rui Wang</author><pubDate>Thu, 23 Jan 2025 15:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13773v1</guid></item><item><title>Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak</title><link>http://arxiv.org/abs/2501.13772v1</link><description>Large Language Models (LLMs) demonstrate remarkable zero-shot performanceacross various natural language processing tasks. The integration of multimodalencoders extends their capabilities, enabling the development of MultimodalLarge Language Models that process vision, audio, and text. However, thesecapabilities also raise significant security concerns, as these models can bemanipulated to generate harmful or inappropriate content through jailbreak.While extensive research explores the impact of modality-specific input editson text-based LLMs and Large Vision-Language Models in jailbreak, the effectsof audio-specific edits on Large Audio-Language Models (LALMs) remainunderexplored. Hence, this paper addresses this gap by investigating howaudio-specific edits influence LALMs inference regarding jailbreak. Weintroduce the Audio Editing Toolbox (AET), which enables audio-modality editssuch as tone adjustment, word emphasis, and noise injection, and the EditedAudio Datasets (EADs), a comprehensive audio jailbreak benchmark. We alsoconduct extensive evaluations of state-of-the-art LALMs to assess theirrobustness under different audio edits. This work lays the groundwork forfuture explorations on audio-modality interactions in LALMs security.</description><author>Erjia Xiao, Hao Cheng, Jing Shao, Jinhao Duan, Kaidi Xu, Le Yang, Jindong Gu, Renjing Xu</author><pubDate>Thu, 23 Jan 2025 15:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13772v1</guid></item><item><title>Can LLMs Solve longer Math Word Problems Better?</title><link>http://arxiv.org/abs/2405.14804v2</link><description>Math Word Problems (MWPs) play a vital role in assessing the capabilities ofLarge Language Models (LLMs), yet current research primarily focuses onquestions with concise contexts. The impact of longer contexts on mathematicalreasoning remains under-explored. This study pioneers the investigation ofContext Length Generalizability (CoLeG), which refers to the ability of LLMs tosolve MWPs with extended narratives. We introduce Extended Grade-School Math(E-GSM), a collection of MWPs featuring lengthy narratives, and propose twonovel metrics to evaluate the efficacy and resilience of LLMs in tackling theseproblems. Our analysis of existing zero-shot prompting techniques withproprietary LLMs along with open-source LLMs reveals a general deficiency inCoLeG. To alleviate these issues, we propose tailored approaches for differentcategories of LLMs. For proprietary LLMs, we introduce a new instructionalprompt designed to mitigate the impact of long contexts. For open-source LLMs,we develop a novel auxiliary task for fine-tuning to enhance CoLeG. Ourcomprehensive results demonstrate the effectiveness of our proposed methods,showing improved performance on E-GSM. Additionally, we conduct an in-depthanalysis to differentiate the effects of semantic understanding and reasoningefficacy, showing that our methods improves the latter. We also establish thegeneralizability of our methods across several other MWP benchmarks. Ourfindings highlight the limitations of current LLMs and offer practicalsolutions correspondingly, paving the way for further exploration of modelgeneralizability and training methodologies.</description><author>Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, Yang Wang</author><pubDate>Thu, 23 Jan 2025 15:47:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14804v2</guid></item><item><title>An Efficient Diffusion-based Non-Autoregressive Solver for Traveling Salesman Problem</title><link>http://arxiv.org/abs/2501.13767v1</link><description>Recent advances in neural models have shown considerable promise in solvingTraveling Salesman Problems (TSPs) without relying on much hand-craftedengineering. However, while non-autoregressive (NAR) approaches benefit fromfaster inference through parallelism, they typically deliver solutions ofinferior quality compared to autoregressive ones. To enhance the solutionquality while maintaining fast inference, we propose DEITSP, a diffusion modelwith efficient iterations tailored for TSP that operates in a NAR manner.Firstly, we introduce a one-step diffusion model that integrates the controlleddiscrete noise addition process with self-consistency enhancement, enablingoptimal solution prediction through simultaneous denoising of multiplesolutions. Secondly, we design a dual-modality graph transformer to bolster theextraction and fusion of features from node and edge modalities, while furtheraccelerating the inference with fewer layers. Thirdly, we develop an efficientiterative strategy that alternates between adding and removing noise to improveexploration compared to previous diffusion methods. Additionally, we devise ascheduling framework to progressively refine the solution space by adjustingnoise levels, facilitating a smooth search for optimal solutions. Extensiveexperiments on real-world and large-scale TSP instances demonstrate that DEITSPperforms favorably against existing neural approaches in terms of solutionquality, inference latency, and generalization ability. Our code is availableat $\href{https://github.com/DEITSP/DEITSP}{https://github.com/DEITSP/DEITSP}$.</description><author>Mingzhao Wang, You Zhou, Zhiguang Cao, Yubin Xiao, Xuan Wu, Wei Pang, Yuan Jiang, Hui Yang, Peng Zhao, Yuanshu Li</author><pubDate>Thu, 23 Jan 2025 15:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13767v1</guid></item><item><title>UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level Mathematical Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2501.13766v1</link><description>Large Language Models (LLMs) have made significant strides in mathematicalreasoning, underscoring the need for a comprehensive and fair evaluation oftheir capabilities. However, existing benchmarks often fall short, eitherlacking extensive coverage of undergraduate-level mathematical problems orprobably suffering from test-set contamination. To address these issues, weintroduce UGMathBench, a diverse and dynamic benchmark specifically designedfor evaluating undergraduate-level mathematical reasoning with LLMs.UGMathBench comprises 5,062 problems across 16 subjects and 111 topics,featuring 10 distinct answer types. Each problem includes three randomizedversions, with additional versions planned for release as leading open-sourceLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:effective accuracy (EAcc), which measures the percentage of correctly solvedproblems across all three versions, and reasoning gap ($\Delta$), whichassesses reasoning robustness by calculating the difference between the averageaccuracy across all versions and EAcc. Our extensive evaluation of 23 leadingLLMs reveals that the highest EAcc achieved is 56.3\% by OpenAI-o1-mini, withlarge $\Delta$ values observed across different models. This highlights theneed for future research aimed at developing "large reasoning models" with highEAcc and $\Delta = 0$. We anticipate that the release of UGMathBench, alongwith its detailed evaluation codes, will serve as a valuable resource toadvance the development of LLMs in solving mathematical problems.</description><author>Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, Can Yang</author><pubDate>Thu, 23 Jan 2025 15:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13766v1</guid></item><item><title>Integrating Causality with Neurochaos Learning: Proposed Approach and Research Agenda</title><link>http://arxiv.org/abs/2501.13763v1</link><description>Deep learning implemented via neural networks, has revolutionized machinelearning by providing methods for complex tasks such as objectdetection/classification and prediction. However, architectures based on deepneural networks have started to yield diminishing returns, primarily due totheir statistical nature and inability to capture causal structure in thetraining data. Another issue with deep learning is its high energy consumption,which is not that desirable from a sustainability perspective. Therefore, alternative approaches are being considered to address theseissues, both of which are inspired by the functioning of the human brain. Oneapproach is causal learning, which takes into account causality among the itemsin the dataset on which the neural network is trained. It is expected that thiswill help minimize the spurious correlations that are prevalent in the learnedrepresentations of deep neural networks. The other approach is NeurochaosLearning, a recent development, which draws its inspiration from the nonlinearchaotic firing intrinsic to neurons in biological neural networks(brain/central nervous system). Both approaches have shown improved resultsover just deep learning alone. To that end, in this position paper, we investigate how causal and neurochaoslearning approaches can be integrated together to produce better results,especially in domains that contain linked data. We propose an approach for thisintegration to enhance classification, prediction and reinforcement learning.We also propose a set of research questions that need to be investigated inorder to make this integration a reality.</description><author>Nanjangud C. Narendra, Nithin Nagaraj</author><pubDate>Thu, 23 Jan 2025 15:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13763v1</guid></item><item><title>Evaluating LLMs for Quotation Attribution in Literary Texts: A Case Study of LLaMa3</title><link>http://arxiv.org/abs/2406.11380v2</link><description>Large Language Models (LLMs) have shown promising results in a variety ofliterary tasks, often using complex memorized details of narration andfictional characters. In this work, we evaluate the ability of Llama-3 atattributing utterances of direct-speech to their speaker in novels. The LLMshows impressive results on a corpus of 28 novels, surpassing published resultswith ChatGPT and encoder-based baselines by a large margin. We then validatethese results by assessing the impact of book memorization and annotationcontamination. We found that these types of memorization do not explain thelarge performance gain, making Llama-3 the new state-of-the-art for quotationattribution in English literature. We release publicly our code and data.</description><author>Gaspard Michel, Elena V. Epure, Romain Hennequin, Christophe Cerisara</author><pubDate>Thu, 23 Jan 2025 15:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11380v2</guid></item><item><title>Invariance Principle Meets Vicinal Risk Minimization</title><link>http://arxiv.org/abs/2407.05765v2</link><description>Deep learning models excel in computer vision tasks but often fail togeneralize to out-of-distribution (OOD) domains. Invariant Risk Minimization(IRM) aims to address OOD generalization by learning domain-invariant features.However, IRM struggles with datasets exhibiting significant diversity shifts.While data augmentation methods like Mixup and Semantic Data Augmentation (SDA)enhance diversity, they risk over-augmentation and label instability. Toaddress these challenges, we propose a domain-shared Semantic Data Augmentation(SDA) module, a novel implementation of Variance Risk Minimization (VRM)designed to enhance dataset diversity while maintaining label consistency. Wefurther provide a Rademacher complexity analysis, establishing a tightergeneralization error bound compared to baseline methods. Extensive evaluationson OOD benchmarks, including PACS, VLCS, OfficeHome, and TerraIncognita,demonstrate consistent performance improvements over state-of-the-art domaingeneralization methods.</description><author>Yaoyao Zhu, Xiuding Cai, Yingkai Wang, Dong Miao, Zhongliang Fu, Xu Luo</author><pubDate>Thu, 23 Jan 2025 15:42:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05765v2</guid></item><item><title>On Deciding the Data Complexity of Answering Linear Monadic Datalog Queries with LTL Operators(Extended Version)</title><link>http://arxiv.org/abs/2501.13762v1</link><description>Our concern is the data complexity of answering linear monadic datalogqueries whose atoms in the rule bodies can be prefixed by operators of lineartemporal logic LTL. We first observe that, for data complexity, answering anyconnected query with operators $\bigcirc/\bigcirc^-$ (at the next/previousmoment) is either in AC0, or in $ACC0\!\setminus\!AC0$, or $NC^1$-complete, orLogSpace-hard and in NLogSpace. Then we show that the problem of decidingLogSpace-hardness of answering such queries is PSpace-complete, while checkingmembership in the classes AC0 and ACC0 as well as $NC^1$-completeness can bedone in ExpSpace. Finally, we prove that membership in AC0 or in ACC0,$NC^1$-completeness, and LogSpace-hardness are undecidable for queries withoperators $\Diamond_f/\Diamond_p$ (sometime in the future/past) provided that$NC^1 \ne NLogSpace$, and $LogSpace \ne NLogSpace$.</description><author>Alessandro Artale, Anton Gnatenko, Vladislav Ryzhikov, Michael Zakharyaschev</author><pubDate>Thu, 23 Jan 2025 15:41:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13762v1</guid></item><item><title>2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings</title><link>http://arxiv.org/abs/2501.13758v1</link><description>Effective sentence embeddings that capture semantic nuances and generalizewell across diverse contexts are crucial for natural language processing tasks.We address this challenge by applying SimCSE (Simple Contrastive Learning ofSentence Embeddings) using contrastive learning to fine-tune the minBERT modelfor sentiment analysis, semantic textual similarity (STS), and paraphrasedetection. Our contributions include experimenting with three different dropouttechniques, namely standard dropout, curriculum dropout, and adaptive dropout,to tackle overfitting, proposing a novel 2-Tier SimCSE Fine-tuning Model thatcombines both unsupervised and supervised SimCSE on STS task, and exploringtransfer learning potential for Paraphrase and SST tasks. Our findingsdemonstrate the effectiveness of SimCSE, with the 2-Tier model achievingsuperior performance on the STS task, with an average test score of 0.742across all three downstream tasks. The results of error analysis revealschallenges in handling complex sentiments and reliance on lexical overlap forparaphrase detection, highlighting areas for future research. The ablationstudy revealed that removing Adaptive Dropout in the Single-Task UnsupervisedSimCSE Model led to improved performance on the STS task, indicatingoverfitting due to added parameters. Transfer learning from SimCSE models onParaphrase and SST tasks did not enhance performance, suggesting limitedtransferability of knowledge from the STS task.</description><author>Yumeng Wang, Ziran Zhou, Junjin Wang</author><pubDate>Thu, 23 Jan 2025 15:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13758v1</guid></item><item><title>Solving the long-tailed distribution problem by exploiting the synergies and balance of different techniques</title><link>http://arxiv.org/abs/2501.13756v1</link><description>In real-world data, long-tailed data distribution is common, making itchallenging for models trained on empirical risk minimisation to learn andclassify tail classes effectively. While many studies have sought to improvelong tail recognition by altering the data distribution in the feature spaceand adjusting model decision boundaries, research on the synergy and correctiveapproach among various methods is limited. Our study delves into threelong-tail recognition techniques: Supervised Contrastive Learning (SCL),Rare-Class Sample Generator (RSG), and Label-Distribution-Aware Margin Loss(LDAM). SCL enhances intra-class clusters based on feature similarity andpromotes clear inter-class separability but tends to favour dominant classesonly. When RSG is integrated into the model, we observed that the intra-classfeatures further cluster towards the class centre, which demonstrates asynergistic effect together with SCL's principle of enhancing intra-classclustering. RSG generates new tail features and compensates for the tailfeature space squeezed by SCL. Similarly, LDAM is known to introduce a largermargin specifically for tail classes; we demonstrate that LDAM further bolstersthe model's performance on tail classes when combined with the more explicitdecision boundaries achieved by SCL and RSG. Furthermore, SCL can compensatefor the dominant class accuracy sacrificed by RSG and LDAM. Our researchemphasises the synergy and balance among the three techniques, with eachamplifying the strengths of the others and mitigating their shortcomings. Ourexperiment on long-tailed distribution datasets, using an end-to-endarchitecture, yields competitive results by enhancing tail class accuracywithout compromising dominant class performance, achieving a balancedimprovement across all classes.</description><author>Ziheng Wang, Toni Lassila, Sharib Ali</author><pubDate>Thu, 23 Jan 2025 15:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13756v1</guid></item><item><title>Combining Multi-Objective Bayesian Optimization with Reinforcement Learning for TinyML</title><link>http://arxiv.org/abs/2305.14109v3</link><description>Deploying deep neural networks (DNNs) on microcontrollers (TinyML) is acommon trend to process the increasing amount of sensor data generated at theedge, but in practice, resource and latency constraints make it difficult tofind optimal DNN candidates. Neural architecture search (NAS) is an excellentapproach to automate this search and can easily be combined with DNNcompression techniques commonly used in TinyML. However, many NAS techniquesare not only computationally expensive, especially hyperparameter optimization(HPO), but also often focus on optimizing only a single objective, e.g.,maximizing accuracy, without considering additional objectives such as memoryrequirements or computational complexity of a DNN, which are key to makingdeployment at the edge feasible. In this paper, we propose a novel NAS strategyfor TinyML based on multi-objective Bayesian optimization (MOBOpt) and anensemble of competing parametric policies trained using Augmented Random Search(ARS) reinforcement learning (RL) agents. Our methodology aims at efficientlyfinding tradeoffs between a DNN's predictive accuracy, memory requirements on agiven target system, and computational complexity. Our experiments show that weconsistently outperform existing MOBOpt approaches on different datasets andarchitectures such as ResNet-18 and MobileNetV3.</description><author>Mark Deutel, Georgios Kontes, Christopher Mutschler, Jürgen Teich</author><pubDate>Thu, 23 Jan 2025 15:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14109v3</guid></item><item><title>On Disentangled Training for Nonlinear Transform in Learned Image Compression</title><link>http://arxiv.org/abs/2501.13751v1</link><description>Learned image compression (LIC) has demonstrated superior rate-distortion(R-D) performance compared to traditional codecs, but is challenged by traininginefficiency that could incur more than two weeks to train a state-of-the-artmodel from scratch. Existing LIC methods overlook the slow convergence causedby compacting energy in learning nonlinear transforms. In this paper, we firstreveal that such energy compaction consists of two components, i.e., featuredecorrelation and uneven energy modulation. On such basis, we propose a linearauxiliary transform (AuxT) to disentangle energy compaction in trainingnonlinear transforms. The proposed AuxT obtains coarse approximation to achieveefficient energy compaction such that distribution fitting with the nonlineartransforms can be simplified to fine details. We then develop wavelet-basedlinear shortcuts (WLSs) for AuxT that leverages wavelet-based downsampling andorthogonal linear projection for feature decorrelation and subband-awarescaling for uneven energy modulation. AuxT is lightweight and plug-and-play tobe integrated into diverse LIC models to address the slow convergence issue.Experimental results demonstrate that the proposed approach can acceleratetraining of LIC models by 2 times and simultaneously achieves an average 1\%BD-rate reduction. To our best knowledge, this is one of the first successfulattempt that can significantly improve the convergence of LIC with comparableor superior rate-distortion performance. Code will be released at\url{https://github.com/qingshi9974/AuxT}</description><author>Han Li, Shaohui Li, Wenrui Dai, Maida Cao, Nuowen Kan, Chenglin Li, Junni Zou, Hongkai Xiong</author><pubDate>Thu, 23 Jan 2025 15:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13751v1</guid></item><item><title>Exact Soft Analytical Side-Channel Attacks using Tractable Circuits</title><link>http://arxiv.org/abs/2501.13748v1</link><description>Detecting weaknesses in cryptographic algorithms is of utmost importance fordesigning secure information systems. The state-of-the-art soft analyticalside-channel attack (SASCA) uses physical leakage information to makeprobabilistic predictions about intermediate computations and combines these"guesses" with the known algorithmic logic to compute the posteriordistribution over the key. This attack is commonly performed via loopy beliefpropagation, which, however, lacks guarantees in terms of convergence andinference quality. In this paper, we develop a fast and exact inference methodfor SASCA, denoted as ExSASCA, by leveraging knowledge compilation andtractable probabilistic circuits. When attacking the Advanced EncryptionStandard (AES), the most widely used encryption algorithm to date, ExSASCAoutperforms SASCA by more than 31% top-1 success rate absolute. By leveragingsparse belief messages, this performance is achieved with little morecomputational cost than SASCA, and about 3 orders of magnitude less than exactinference via exhaustive enumeration. Even with dense belief messages, ExSASCAstill uses 6 times less computations than exhaustive inference.</description><author>Thomas Wedenig, Rishub Nagpal, Gaëtan Cassiers, Stefan Mangard, Robert Peharz</author><pubDate>Thu, 23 Jan 2025 15:25:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13748v1</guid></item><item><title>EICopilot: Search and Explore Enterprise Information over Large-scale Knowledge Graphs with LLM-driven Agents</title><link>http://arxiv.org/abs/2501.13746v1</link><description>The paper introduces EICopilot, an novel agent-based solution enhancingsearch and exploration of enterprise registration data within extensive onlineknowledge graphs like those detailing legal entities, registered capital, andmajor shareholders. Traditional methods necessitate text-based queries andmanual subgraph explorations, often resulting in time-consuming processes.EICopilot, deployed as a chatbot via Baidu Enterprise Search, improves thislandscape by utilizing Large Language Models (LLMs) to interpret naturallanguage queries. This solution automatically generates and executes Gremlinscripts, providing efficient summaries of complex enterprise relationships.Distinct feature a data pre-processing pipeline that compiles and annotatesrepresentative queries into a vector database of examples for In-contextlearning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thoughtwith ICL to enhance Gremlin script generation for knowledge graph search andexploration, and a novel query masking strategy that improves intentrecognition for heightened script accuracy. Empirical evaluations demonstratethe superior performance of EICopilot, including speed and accuracy, overbaseline methods, with the \emph{Full Mask} variant achieving a syntax errorrate reduction to as low as 10.00% and an execution correctness of up to82.14%. These components collectively contribute to superior queryingcapabilities and summarization of intricate datasets, positioning EICopilot asa groundbreaking tool in the exploration and exploitation of large-scaleknowledge graphs for enterprise information search.</description><author>Yuhui Yun, Huilong Ye, Xinru Li, Ruojia Li, Jingfeng Deng, Li Li, Haoyi Xiong</author><pubDate>Thu, 23 Jan 2025 15:22:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13746v1</guid></item><item><title>GPT-HTree: A Decision Tree Framework Integrating Hierarchical Clustering and Large Language Models for Explainable Classification</title><link>http://arxiv.org/abs/2501.13743v1</link><description>This paper introduces GPT-HTree, a framework combining hierarchicalclustering, decision trees, and large language models (LLMs) to address thischallenge. By leveraging hierarchical clustering to segment individuals basedon salient features, resampling techniques to balance class distributions, anddecision trees to tailor classification paths within each cluster, GPT-HTreeensures both accuracy and interpretability. LLMs enhance the framework bygenerating human-readable cluster descriptions, bridging quantitative analysiswith actionable insights.</description><author>Te Pei, Fuat Alican, Aaron Ontoyin Yin, Yigit Ihlamur</author><pubDate>Thu, 23 Jan 2025 15:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13743v1</guid></item><item><title>LOCUS: LOcalization with Channel Uncertainty and Sporadic Energy</title><link>http://arxiv.org/abs/2302.09409v2</link><description>Accurate sound source localization (SSL) requires consistent multichanneldata for reliable degree of arrival (DoA) estimation. However, intermittentlypowered batteryless systems often suffer from incomplete sensor data due to thestochastic nature of energy harvesting. Existing methods struggle with missingchannels, leading to significant performance degradation. In this paper, wepropose $\textit{LOCUS}$, a novel deep learning-based system designed torecover corrupted features for SSL in batteryless systems. $\textit{LOCUS}$addresses missing data by leveraging information entropy estimation andconditional interpolation, combining three modules: (1) Information-WeightedFocus (InFo), which identifies and quantifies corrupted data elements, (2)Latent Feature Synthesizer (LaFS), which synthesizes missing features, and (3)Guided Replacement (GRep), which intelligently replaces missing elements whilepreserving valid data. We demonstrate significant performance improvementsusing two datasets: DCASE and LargeSet, where $\textit{LOCUS}$ achieves up to$36.91\%$ lower DoA error compared to existing methods. Real-world evaluationsacross three environments with intermittent power sources show a$25.87-59.46\%$ improvement in performance when channels are stochasticallymissing. Additionally, we release a 50-hour multichannel dataset to supportfurther research in SSL.</description><author>Subrata Biswas, Mohammad Nur Hossain Khan, Alex Colwell, Jack Adiletta, Bashima Islam</author><pubDate>Thu, 23 Jan 2025 15:13:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09409v2</guid></item><item><title>A Study of the Plausibility of Attention between RNN Encoders in Natural Language Inference</title><link>http://arxiv.org/abs/2501.13735v1</link><description>Attention maps in neural models for NLP are appealing to explain the decisionmade by a model, hopefully emphasizing words that justify the decision. Whilemany empirical studies hint that attention maps can provide such justificationfrom the analysis of sound examples, only a few assess the plausibility ofexplanations based on attention maps, i.e., the usefulness of attention mapsfor humans to understand the decision. These studies furthermore focus on textclassification. In this paper, we report on a preliminary assessment ofattention maps in a sentence comparison task, namely natural languageinference. We compare the cross-attention weights between two RNN encoders withhuman-based and heuristic-based annotations on the eSNLI corpus. We show thatthe heuristic reasonably correlates with human annotations and can thusfacilitate evaluation of plausible explanations in sentence comparison tasks.Raw attention weights however remain only loosely related to a plausibleexplanation.</description><author>Duc Hau Nguyen, Duc Hau Nguyen, Pascale Sébillot</author><pubDate>Thu, 23 Jan 2025 15:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13735v1</guid></item><item><title>Sample complexity of data-driven tuning of model hyperparameters in neural networks with structured parameter-dependent dual function</title><link>http://arxiv.org/abs/2501.13734v1</link><description>Modern machine learning algorithms, especially deep learning basedtechniques, typically involve careful hyperparameter tuning to achieve the bestperformance. Despite the surge of intense interest in practical techniques likeBayesian optimization and random search based approaches to automating thislaborious and compute-intensive task, the fundamental learning theoreticcomplexity of tuning hyperparameters for deep neural networks is poorlyunderstood. Inspired by this glaring gap, we initiate the formal study ofhyperparameter tuning complexity in deep learning through a recently introduceddata driven setting. We assume that we have a series of deep learning tasks,and we have to tune hyperparameters to do well on average over the distributionof tasks. A major difficulty is that the utility function as a function of thehyperparameter is very volatile and furthermore, it is given implicitly by anoptimization problem over the model parameters. This is unlike previous work indata driven design, where one can typically explicitly model the algorithmicbehavior as a function of the hyperparameters. To tackle this challenge, weintroduce a new technique to characterize the discontinuities and oscillationsof the utility function on any fixed problem instance as we vary thehyperparameter, our analysis relies on subtle concepts including tools fromdifferential/algebraic geometry and constrained optimization. This can be usedto show that the learning theoretic complexity of the corresponding family ofutility functions is bounded. We instantiate our results and provide samplecomplexity bounds for concrete applications tuning a hyperparameter thatinterpolates neural activation functions and setting the kernel parameter ingraph neural networks.</description><author>Maria-Florina Balcan, Anh Tuan Nguyen, Dravyansh Sharma</author><pubDate>Thu, 23 Jan 2025 15:10:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13734v1</guid></item><item><title>Is Large-Scale Pretraining the Secret to Good Domain Generalization?</title><link>http://arxiv.org/abs/2412.02856v2</link><description>Multi-Source Domain Generalization (DG) is the task of training on multiplesource domains and achieving high classification performance on unseen targetdomains. Recent methods combine robust features from web-scale pretrainedbackbones with new features learned from source data, and this has dramaticallyimproved benchmark results. However, it remains unclear if DG finetuningmethods are becoming better over time, or if improved benchmark performance issimply an artifact of stronger pre-training. Prior studies have shown thatperceptual similarity to pre-training data correlates with zero-shotperformance, but we find the effect limited in the DG setting. Instead, weposit that having perceptually similar data in pretraining is not enough; andthat it is how well these data were learned that determines performance. Thisleads us to introduce the Alignment Hypothesis, which states that the final DGperformance will be high if and only if alignment of image and class label textembeddings is high. Our experiments confirm the Alignment Hypothesis is true,and we use it as an analysis tool of existing DG methods evaluated on DomainBeddatasets by splitting evaluation data into In-pretraining (IP) andOut-of-pretraining (OOP). We show that all evaluated DG methods struggle onDomainBed-OOP, while recent methods excel on DomainBed-IP. Put together, ourfindings highlight the need for DG methods which can generalize beyondpretraining alignment.</description><author>Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Bryan A. Plummer, Kate Saenko</author><pubDate>Thu, 23 Jan 2025 15:09:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02856v2</guid></item><item><title>Validating Deep Learning Weather Forecast Models on Recent High-Impact Extreme Events</title><link>http://arxiv.org/abs/2404.17652v2</link><description>The forecast accuracy of machine learning (ML) weather prediction models isimproving rapidly, leading many to speak of a "second revolution in weatherforecasting". With numerous methods being developed and limited physicalguarantees offered by ML models, there is a critical need for a comprehensiveevaluation of these emerging techniques. While this need has been partlyfulfilled by benchmark datasets, they provide little information on rare andimpactful extreme events or on compound impact metrics, for which modelaccuracy might degrade due to misrepresented dependencies between variables. Toaddress these issues, we compare ML weather prediction models (GraphCast,PanguWeather, and FourCastNet) and ECMWF's high-resolution forecast system(HRES) in three case studies: the 2021 Pacific Northwest heatwave, the 2023South Asian humid heatwave, and the North American winter storm in 2021. Wefind that ML weather prediction models locally achieve similar accuracy to HRESon the record-shattering Pacific Northwest heatwave but underperform whenaggregated over space and time. However, they forecast the compound winterstorm substantially better. We also highlight structural differences in how theerrors of HRES and the ML models build up to that event. The ML forecasts lackimportant variables for a detailed assessment of the health risks of the 2023humid heatwave. Using a possible substitute variable, prediction errors showspatial patterns with the highest danger levels over Bangladesh beingunderestimated by the ML models. Generally, case-study-driven, impact-centricevaluation can complement existing research, increase public trust, and aid indeveloping reliable ML weather prediction models.</description><author>Olivier C. Pasche, Jonathan Wider, Zhongwei Zhang, Jakob Zscheischler, Sebastian Engelke</author><pubDate>Thu, 23 Jan 2025 15:06:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17652v2</guid></item><item><title>A dimensionality reduction technique based on the Gromov-Wasserstein distance</title><link>http://arxiv.org/abs/2501.13732v1</link><description>Analyzing relationships between objects is a pivotal problem within datascience. In this context, Dimensionality reduction (DR) techniques are employedto generate smaller and more manageable data representations. This paperproposes a new method for dimensionality reduction, based on optimaltransportation theory and the Gromov-Wasserstein distance. We offer a newprobabilistic view of the classical Multidimensional Scaling (MDS) algorithmand the nonlinear dimensionality reduction algorithm, Isomap (Isometric Mappingor Isometric Feature Mapping) that extends the classical MDS, in which we usethe Gromov-Wasserstein distance between the probability measure ofhigh-dimensional data, and its low-dimensional representation. Through gradientdescent, our method embeds high-dimensional data into a lower-dimensionalspace, providing a robust and efficient solution for analyzing complexhigh-dimensional datasets.</description><author>Rafael P. Eufrazio, Eduardo Fernandes Montesuma, Charles C. Cavalcante</author><pubDate>Thu, 23 Jan 2025 15:05:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13732v1</guid></item><item><title>Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational Tasks</title><link>http://arxiv.org/abs/2501.13731v1</link><description>Graph computational tasks are inherently challenging and often demand thedevelopment of advanced algorithms for effective solutions. With the emergenceof large language models (LLMs), researchers have begun investigating theirpotential to address these tasks. However, existing approaches are constrainedby LLMs' limited capability to comprehend complex graph structures and theirhigh inference costs, rendering them impractical for handling large-scalegraphs. Inspired by human approaches to graph problems, we introduce a novelframework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for GraphComputational Tasks), which consists of three key steps: problem understanding,prompt design, and code generation. In this framework, LLMs are tasked withunderstanding the problem and extracting relevant information to generatecorrect code. The responsibility for analyzing the graph structure andexecuting the code is delegated to the interpreter. We inject task-relatedpseudocodes into the prompts to further assist the LLMs in generating efficientcode. We also employ cost-effective trial-and-error techniques to ensure thatthe LLM-generated code executes correctly. Unlike other methods that requireinvoking LLMs for each individual test case, PIE only calls the LLM during thecode generation phase, allowing the generated code to be reused andsignificantly reducing inference costs. Extensive experiments demonstrate thatPIE outperforms existing baselines in terms of both accuracy and computationalefficiency.</description><author>Chang Gong, Wanrui Bian, Zhijie Zhang, Weiguo Zheng</author><pubDate>Thu, 23 Jan 2025 15:04:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13731v1</guid></item><item><title>Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream Allocation in Feed</title><link>http://arxiv.org/abs/2412.10381v2</link><description>Reinforcement Learning (RL) has been widely applied in recommendation systemsto capture long-term user engagement, thus improving dwelling time andimproving user retention. In the context of a short video &amp; live stream mixedrecommendation scenario, the live stream recommendation system (RS) decideswhether to inject at most one live stream into the video feed for each userrequest. To maximize long-term user engagement, it is crucial to determine anoptimal live stream injection policy for accurate live stream allocation.However, traditional RL algorithms often face divergence and instabilityproblems, and these issues may cause too many live stream allocations, whichinterrupts the user's short-video interest and leads to a decrease in theuser's app usage duration. To address these challenges, we propose a novelSupervised Learning-enhanced Multi-Group Actor Critic algorithm (SL-MGAC).Specifically, we introduce a supervised learning-enhanced actor criticframework that incorporates variance reduction techniques, where multi-taskreward learning helps restrict bootstrapping error accumulation during criticlearning. Additionally, we design a multi-group state decomposition module forboth actor and critic networks to reduce prediction variance and improve modelstability. We also propose a novel reward function to prevent overly greedylive-stream allocation. Empirically, we evaluate the SL-MGAC algorithm usingoffline policy evaluation (OPE) and online A/B testing. Experimental resultsdemonstrate that the proposed method not only outperforms baseline methods butalso exhibits enhanced stability in online recommendation scenarios.</description><author>Jingxin Liu, Xiang Gao, Yisha Li, Xin Li, Haiyang Lu, Ben Wang</author><pubDate>Thu, 23 Jan 2025 15:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10381v2</guid></item></channel></rss>