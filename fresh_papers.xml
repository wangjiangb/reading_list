<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 11 Jun 2023 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Background Prompting for Improved Object Depth</title><link>http://arxiv.org/abs/2306.05428v1</link><description>Estimating the depth of objects from a single image is a valuable task formany vision, robotics, and graphics applications. However, current methodsoften fail to produce accurate depth for objects in diverse scenes. In thiswork, we propose a simple yet effective Background Prompting strategy thatadapts the input object image with a learned background. We learn thebackground prompts only using small-scale synthetic object datasets. To inferobject depth on a real image, we place the segmented object into the learnedbackground prompt and run off-the-shelf depth networks. Background Promptinghelps the depth networks focus on the foreground object, as they are madeinvariant to background variations. Moreover, Background Prompting minimizesthe domain gap between synthetic and real object images, leading to bettersim2real generalization than simple finetuning. Results on multiple syntheticand real datasets demonstrate consistent improvements in real object depths fora variety of existing depth networks. Code and optimized background prompts canbe found at: https://mbaradad.github.io/depth_prompt.</description><author>Manel Baradad, Yuanzhen Li, Forrester Cole, Michael Rubinstein, Antonio Torralba, William T. Freeman, Varun Jampani</author><pubDate>Thu, 08 Jun 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05428v1</guid></item><item><title>Grounded Text-to-Image Synthesis with Attention Refocusing</title><link>http://arxiv.org/abs/2306.05427v1</link><description>Driven by scalable diffusion models trained on large-scale paired text-imagedatasets, text-to-image synthesis methods have shown compelling results.However, these models still fail to precisely follow the text prompt whenmultiple objects, attributes, and spatial compositions are involved in theprompt. In this paper, we identify the potential reasons in both thecross-attention and self-attention layers of the diffusion model. We proposetwo novel losses to refocus the attention maps according to a given layoutduring the sampling process. We perform comprehensive experiments on theDrawBench and HRS benchmarks using layouts synthesized by Large LanguageModels, showing that our proposed losses can be integrated easily andeffectively into existing text-to-image methods and consistently improve theiralignment between the generated images and the text prompts.</description><author>Quynh Phung, Songwei Ge, Jia-Bin Huang</author><pubDate>Thu, 08 Jun 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05427v1</guid></item><item><title>SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking</title><link>http://arxiv.org/abs/2306.05426v1</link><description>In many domains, autoregressive models can achieve low log-likelihood on thetask of predicting the next observation. However, this maximum-likelihood (MLE)objective does not necessarily match a downstream use-case of autoregressivelygenerating high-quality sequences. The MLE objective weights sequencesproportionally to their frequency under the data distribution, with no guidancefor the model's behaviour out of distribution (OOD): leading to compoundingerror during autoregressive generation. In order to address this compoundingerror problem, we formulate sequence generation as an imitation learning (IL)problem. This allows us to minimize a variety of divergences between thedistribution of sequences generated by an autoregressive model and sequencesfrom a dataset, including divergences with weight on OOD generated sequences.The IL framework also allows us to incorporate backtracking by introducing abackspace action into the generation process. This further mitigates thecompounding error problem by allowing the model to revert a sampled token if ittakes the sequence OOD. Our resulting method, SequenceMatch, can be implementedwithout adversarial training or major architectural changes. We identify theSequenceMatch-$\chi^2$ divergence as a more suitable training objective forautoregressive models which are used for generation. We show that empirically,SequenceMatch training leads to improvements over MLE on text generation withlanguage models.</description><author>Chris Cundy, Stefano Ermon</author><pubDate>Thu, 08 Jun 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05426v1</guid></item><item><title>MIMIC-IT: Multi-Modal In-Context Instruction Tuning</title><link>http://arxiv.org/abs/2306.05425v1</link><description>High-quality instructions and responses are essential for the zero-shotperformance of large language models on interactive natural language tasks. Forinteractive vision-language tasks involving intricate visual scenes, a largequantity of diverse and creative instruction-response pairs should beimperative to tune vision-language models (VLMs). Nevertheless, the currentavailability of vision-language instruction-response pairs in terms ofquantity, diversity, and creativity remains limited, posing challenges to thegeneralization of interactive VLMs. Here we present MultI-Modal In-ContextInstruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodalinstruction-response pairs, with 2.2 million unique instructions derived fromimages and videos. Each pair is accompanied by multi-modal in-contextinformation, forming conversational contexts aimed at empowering VLMs inperception, reasoning, and planning. The instruction-response collectionprocess, dubbed as Syphus, is scaled using an automatic annotation pipelinethat combines human expertise with GPT's capabilities. Using the MIMIC-ITdataset, we train a large VLM named Otter. Based on extensive evaluationsconducted on vision-language benchmarks, it has been observed that Otterdemonstrates remarkable proficiency in multi-modal perception, reasoning, andin-context learning. Human evaluation reveals it effectively aligns with theuser's intentions. We release the MIMIC-IT dataset, instruction-responsecollection pipeline, benchmarks, and the Otter model.</description><author>Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, Ziwei Liu</author><pubDate>Thu, 08 Jun 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05425v1</guid></item><item><title>ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process</title><link>http://arxiv.org/abs/2306.05423v1</link><description>Image recognition and generation have long been developed independently ofeach other. With the recent trend towards general-purpose representationlearning, the development of general representations for both recognition andgeneration tasks is also promoted. However, preliminary attempts mainly focuson generation performance, but are still inferior on recognition tasks. Thesemethods are modeled in the vector-quantized (VQ) space, whereas leadingrecognition methods use pixels as inputs. Our key insights are twofold: (1)pixels as inputs are crucial for recognition tasks; (2) VQ tokens asreconstruction targets are beneficial for generation tasks. These observationsmotivate us to propose an Alternating Denoising Diffusion Process (ADDP) thatintegrates these two spaces within a single representation learning framework.In each denoising step, our method first decodes pixels from previous VQtokens, then generates new VQ tokens from the decoded pixels. The diffusionprocess gradually masks out a portion of VQ tokens to construct the trainingsamples. The learned representations can be used to generate diversehigh-fidelity images and also demonstrate excellent transfer performance onrecognition tasks. Extensive experiments show that our method achievescompetitive performance on unconditional generation, ImageNet classification,COCO detection, and ADE20k segmentation. Importantly, our method represents thefirst successful development of general representations applicable to bothgeneration and dense recognition tasks. Code shall be released.</description><author>Changyao Tian, Chenxin Tao, Jifeng Dai, Hao Li, Ziheng Li, Lewei Lu, Xiaogang Wang, Hongsheng Li, Gao Huang, Xizhou Zhu</author><pubDate>Thu, 08 Jun 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05423v1</guid></item><item><title>Stochastic Multi-Person 3D Motion Forecasting</title><link>http://arxiv.org/abs/2306.05421v1</link><description>This paper aims to deal with the ignored real-world complexities in priorwork on human motion forecasting, emphasizing the social properties ofmulti-person motion, the diversity of motion and social interactions, and thecomplexity of articulated motion. To this end, we introduce a novel task ofstochastic multi-person 3D motion forecasting. We propose a dual-levelgenerative modeling framework that separately models independent individualmotion at the local level and social interactions at the global level. Notably,this dual-level modeling mechanism can be achieved within a shared generativemodel, through introducing learnable latent codes that represent intents offuture motion and switching the codes' modes of operation at different levels.Our framework is general; we instantiate it with different generative models,including generative adversarial networks and diffusion models, and variousmulti-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,and SoMoF benchmarks show that our approach produces diverse and accuratemulti-person predictions, significantly outperforming the state of the art.</description><author>Sirui Xu, Yu-Xiong Wang, Liang-Yan Gui</author><pubDate>Thu, 08 Jun 2023 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05421v1</guid></item><item><title>Scaling Spherical CNNs</title><link>http://arxiv.org/abs/2306.05420v1</link><description>Spherical CNNs generalize CNNs to functions on the sphere, by using sphericalconvolutions as the main linear operation. The most accurate and efficient wayto compute spherical convolutions is in the spectral domain (via theconvolution theorem), which is still costlier than the usual planarconvolutions. For this reason, applications of spherical CNNs have so far beenlimited to small problems that can be approached with low model capacity. Inthis work, we show how spherical CNNs can be scaled for much larger problems.To achieve this, we make critical improvements including novel variants ofcommon model components, an implementation of core operations to exploithardware accelerator characteristics, and application-specific inputrepresentations that exploit the properties of our model. Experiments show ourlarger spherical CNNs reach state-of-the-art on several targets of the QM9molecular benchmark, which was previously dominated by equivariant graph neuralnetworks, and achieve competitive performance on multiple weather forecastingtasks. Our code is available athttps://github.com/google-research/spherical-cnn.</description><author>Carlos Esteves, Jean-Jacques Slotine, Ameesh Makadia</author><pubDate>Thu, 08 Jun 2023 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05420v1</guid></item><item><title>2D Supervised Monocular 3D Object Detection by Global-to-Local 3D Reconstruction</title><link>http://arxiv.org/abs/2306.05418v1</link><description>With the advent of the big model era, the demand for data has become moreimportant. Especially in monocular 3D object detection, expensive manualannotations potentially limit further developments. Existing works haveinvestigated weakly supervised algorithms with the help of LiDAR modality togenerate 3D pseudo labels, which cannot be applied to ordinary videos. In thispaper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the ideaof global-to-local 3D reconstruction for 2D supervised monocular 3D objectdetection. Specifically, we recover 3D structures from monocular videos byscene-level global reconstruction with global bundle adjustment (BA) and obtainobject clusters by the DoubleClustering algorithm. Learning from completelyreconstructed objects in global BA, GBA-Learner predicts pseudo labels foroccluded objects. Finally, we train an LBA-Learner with object-centric local BAto generalize the generated 3D pseudo labels to moving objects. Experiments onthe large-scale Waymo Open Dataset show that the performance of BA$^2$-Det ison par with the fully-supervised BA-Det trained with 10% videos and evenoutperforms some pioneer fully-supervised methods. We also show the greatpotential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.The code will be made available. Project page: https://ba2det.site .</description><author>Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang</author><pubDate>Thu, 08 Jun 2023 18:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05418v1</guid></item><item><title>TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture</title><link>http://arxiv.org/abs/2306.05419v1</link><description>Driving scene understanding task involves detecting static elements such aslanes, traffic signs, and traffic lights, and their relationships with eachother. To facilitate the development of comprehensive scene understandingsolutions using multiple camera views, a new dataset called Road Genome(OpenLane-V2) has been released. This dataset allows for the exploration ofcomplex road connections and situations where lane markings may be absent.Instead of using traditional lane markings, the lanes in this dataset arerepresented by centerlines, which offer a more suitable representation of lanesand their connections. In this study, we have introduced a new approach calledTopoMask for predicting centerlines in road topology. Unlike existingapproaches in the literature that rely on keypoints or parametric methods,TopoMask utilizes an instance-mask based formulation with a transformer-basedarchitecture and, in order to enrich the mask instances with flow information,a direction label representation is proposed. TopoMask have ranked 4th in theOpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline predictionin OpenLane Topology Challenge 2023. In comparison to the currentstate-of-the-art method, TopoNet, the proposed method has achieved similarperformance in Frechet-based lane detection and outperformed TopoNet inChamfer-based lane detection without utilizing its scene graph neural network.</description><author>M. Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel</author><pubDate>Thu, 08 Jun 2023 18:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05419v1</guid></item><item><title>Tracking Objects with 3D Representation from Videos</title><link>http://arxiv.org/abs/2306.05416v1</link><description>Data association is a knotty problem for 2D Multiple Object Tracking due tothe object occlusion. However, in 3D space, data association is not so hard.Only with a 3D Kalman Filter, the online object tracker can associate thedetections from LiDAR. In this paper, we rethink the data association in 2D MOTand utilize the 3D object representation to separate each object in the featurespace. Unlike the existing depth-based MOT methods, the 3D objectrepresentation can be jointly learned with the object association module.Besides, the object's 3D representation is learned from the video andsupervised by the 2D tracking labels without additional manual annotations fromLiDAR or pretrained depth estimator. With 3D object representation learningfrom Pseudo 3D object labels in monocular videos, we propose a new 2D MOTparadigm, called P3DTrack. Extensive experiments show the effectiveness of ourmethod. We achieve new state-of-the-art performance on the large-scale WaymoOpen Dataset.</description><author>Jiawei He, Lue Fan, Yuqi Wang, Yuntao Chen, Zehao Huang, Naiyan Wang, Zhaoxiang Zhang</author><pubDate>Thu, 08 Jun 2023 18:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05416v1</guid></item><item><title>Causal normalizing flows: from theory to practice</title><link>http://arxiv.org/abs/2306.05415v1</link><description>In this work, we deepen on the use of normalizing flows for causal reasoning.Specifically, we first leverage recent results on non-linear ICA to show thatcausal models are identifiable from observational data given a causal ordering,and thus can be recovered using autoregressive normalizing flows (NFs). Second,we analyze different design and learning choices for causal normalizing flowsto capture the underlying causal data-generating process. Third, we describehow to implement the do-operator in causal NFs, and thus, how to answerinterventional and counterfactual questions. Finally, in our experiments, wevalidate our design and training choices through a comprehensive ablationstudy; compare causal NFs to other approaches for approximating causal models;and empirically demonstrate that causal NFs can be used to address real-worldproblems, where the presence of mixed discrete-continuous data and partialknowledge on the causal graph is the norm. The code for this work can be foundat https://github.com/psanch21/causal-flows.</description><author>Adrián Javaloy, Pablo Sánchez-Martín, Isabel Valera</author><pubDate>Thu, 08 Jun 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05415v1</guid></item><item><title>Improving Negative-Prompt Inversion via Proximal Guidance</title><link>http://arxiv.org/abs/2306.05414v1</link><description>DDIM inversion has revealed the remarkable potential of real image editingwithin diffusion-based methods. However, the accuracy of DDIM reconstructiondegrades as larger classifier-free guidance (CFG) scales being used forenhanced editing. Null-text inversion (NTI) optimizes null embeddings to alignthe reconstruction and inversion trajectories with larger CFG scales, enablingreal image editing with cross-attention control. Negative-prompt inversion(NPI) further offers a training-free closed-form solution of NTI. However, itmay introduce artifacts and is still constrained by DDIM reconstructionquality. To overcome these limitations, we propose Proximal Negative-PromptInversion (ProxNPI), extending the concepts of NTI and NPI. We enhance NPI witha regularization term and reconstruction guidance, which reduces artifactswhile capitalizing on its training-free nature. Our method provides anefficient and straightforward approach, effectively addressing real imageediting tasks with minimal computational overhead.</description><author>Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu, Qilong Zhangli, Anastasis Stathopoulos, Jindong Jiang, Zhaoyang Xia, Akash Srivastava, Dimitris Metaxas</author><pubDate>Thu, 08 Jun 2023 18:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05414v1</guid></item><item><title>Offline Prioritized Experience Replay</title><link>http://arxiv.org/abs/2306.05412v1</link><description>Offline reinforcement learning (RL) is challenged by the distributional shiftproblem. To address this problem, existing works mainly focus on designingsophisticated policy constraints between the learned policy and the behaviorpolicy. However, these constraints are applied equally to well-performing andinferior actions through uniform sampling, which might negatively affect thelearned policy. To alleviate this issue, we propose Offline PrioritizedExperience Replay (OPER), featuring a class of priority functions designed toprioritize highly-rewarding transitions, making them more frequently visitedduring training. Through theoretical analysis, we show that this class ofpriority functions induce an improved behavior policy, and when constrained tothis improved policy, a policy-constrained offline RL algorithm is likely toyield a better solution. We develop two practical strategies to obtain priorityweights by estimating advantages based on a fitted value network (OPER-A) orutilizing trajectory returns (OPER-R) for quick computation. OPER is aplug-and-play component for offline RL algorithms. As case studies, we evaluateOPER on five different algorithms, including BC, TD3+BC, Onestep RL, CQL, andIQL. Extensive experiments demonstrate that both OPER-A and OPER-Rsignificantly improve the performance for all baseline methods. Codes andpriority weights are availiable at https://github.com/sail-sg/OPER.</description><author>Yang Yue, Bingyi Kang, Xiao Ma, Gao Huang, Shiji Song, Shuicheng Yan</author><pubDate>Thu, 08 Jun 2023 18:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05412v1</guid></item><item><title>R-MAE: Regions Meet Masked Autoencoders</title><link>http://arxiv.org/abs/2306.05411v1</link><description>Vision-specific concepts such as "region" have played a key role in extendinggeneral machine learning frameworks to tasks like object detection. Given thesuccess of region-based detectors for supervised learning and the progress ofintra-image methods for contrastive learning, we explore the use of regions forreconstructive pre-training. Starting from Masked Autoencoding (MAE) both as abaseline and an inspiration, we propose a parallel pre-text task tailored toaddress the one-to-many mapping between images and regions. Since such regionscan be generated in an unsupervised way, our approach (R-MAE) inherits the wideapplicability from MAE, while being more "region-aware". We conduct thoroughanalyses during the development of R-MAE, and converge on a variant that isboth effective and efficient (1.3% overhead over MAE). Moreover, it showsconsistent quantitative improvements when generalized to various pre-trainingdata and downstream detection and segmentation benchmarks. Finally, we provideextensive qualitative visualizations to enhance the understanding of R-MAE'sbehaviour and potential. Code will be made available athttps://github.com/facebookresearch/r-mae.</description><author>Duy-Kien Nguyen, Vaibhav Aggarwal, Yanghao Li, Martin R. Oswald, Alexander Kirillov, Cees G. M. Snoek, Xinlei Chen</author><pubDate>Thu, 08 Jun 2023 18:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05411v1</guid></item><item><title>LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</title><link>http://arxiv.org/abs/2306.05410v1</link><description>A critical obstacle preventing NeRF models from being deployed broadly in thewild is their reliance on accurate camera poses. Consequently, there is growinginterest in extending NeRF models to jointly optimize camera poses and scenerepresentation, which offers an alternative to off-the-shelf SfM pipelineswhich have well-understood failure modes. Existing approaches for unposed NeRFoperate under limited assumptions, such as a prior pose distribution or coarsepose initialization, making them less effective in a general setting. In thiswork, we propose a novel approach, LU-NeRF, that jointly estimates camera posesand neural radiance fields with relaxed assumptions on pose configuration. Ourapproach operates in a local-to-global manner, where we first optimize overlocal subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose andgeometry for this challenging few-shot task. The mini-scene poses are broughtinto a global reference frame through a robust pose synchronization step, wherea final global optimization of pose and scene can be performed. We show ourLU-NeRF pipeline outperforms prior attempts at unposed NeRF without makingrestrictive assumptions on the pose prior. This allows us to operate in thegeneral SE(3) pose setting, unlike the baselines. Our results also indicate ourmodel can be complementary to feature-based SfM pipelines as it comparesfavorably to COLMAP on low-texture and low-resolution images.</description><author>Zezhou Cheng, Carlos Esteves, Varun Jampani, Abhishek Kar, Subhransu Maji, Ameesh Makadia</author><pubDate>Thu, 08 Jun 2023 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05410v1</guid></item><item><title>SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding</title><link>http://arxiv.org/abs/2306.05407v1</link><description>Semantic 2D maps are commonly used by humans and machines for navigationpurposes, whether it's walking or driving. However, these maps havelimitations: they lack detail, often contain inaccuracies, and are difficult tocreate and maintain, especially in an automated fashion. Can we use raw imageryto automatically create better maps that can be easily interpreted by bothhumans and machines? We introduce SNAP, a deep network that learns rich neural2D maps from ground-level and overhead images. We train our model to alignneural maps estimated from different inputs, supervised only with camera posesover tens of millions of StreetView images. SNAP can resolve the location ofchallenging image queries beyond the reach of traditional methods,outperforming the state of the art in localization by a large margin. Moreover,our neural maps encode not only geometry and appearance but also high-levelsemantics, discovered without explicit supervision. This enables effectivepre-training for data-efficient semantic scene understanding, with thepotential to unlock cost-efficient creation of more detailed maps.</description><author>Paul-Edouard Sarlin, Eduard Trulls, Marc Pollefeys, Jan Hosang, Simon Lynen</author><pubDate>Thu, 08 Jun 2023 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05407v1</guid></item><item><title>Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories</title><link>http://arxiv.org/abs/2306.05406v1</link><description>Pre-trained language models (PLMs) demonstrate excellent abilities tounderstand texts in the generic domain while struggling in a specific domain.Although continued pre-training on a large domain-specific corpus is effective,it is costly to tune all the parameters on the domain. In this paper, weinvestigate whether we can adapt PLMs both effectively and efficiently by onlytuning a few parameters. Specifically, we decouple the feed-forward networks(FFNs) of the Transformer architecture into two parts: the original pre-trainedFFNs to maintain the old-domain knowledge and our novel domain-specificadapters to inject domain-specific knowledge in parallel. Then we adopt amixture-of-adapters gate to fuse the knowledge from different domain adaptersdynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs atwo-stage adapter-tuning strategy that leverages both unlabeled data andlabeled data to help the domain adaptation: i) domain-specific adapter onunlabeled data; followed by ii) the task-specific adapter on labeled data.MixDA can be seamlessly plugged into the pretraining-finetuning paradigm andour experiments demonstrate that MixDA achieves superior performance onin-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), andknowledge-intensive tasks (KILT). Further analyses demonstrate the reliability,scalability, and efficiency of our method. The code is available athttps://github.com/Amano-Aki/Mixture-of-Domain-Adapters.</description><author>Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang, Tong Zhang</author><pubDate>Thu, 08 Jun 2023 18:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05406v1</guid></item><item><title>RDumb: A simple approach that questions our progress in continual test-time adaptation</title><link>http://arxiv.org/abs/2306.05401v1</link><description>Test-Time Adaptation (TTA) allows to update pretrained models to changingdata distributions at deployment time. While early work tested these algorithmsfor individual fixed distribution shifts, recent work proposed and appliedmethods for continual adaptation over long timescales. To examine the reportedprogress in the field, we propose the Continuously Changing Corruptions (CCC)benchmark to measure asymptotic performance of TTA techniques. We find thateventually all but one state-of-the-art methods collapse and perform worse thana non-adapting model, including models specifically proposed to be robust toperformance collapse. In addition, we introduce a simple baseline, "RDumb",that periodically resets the model to its pretrained state. RDumb performsbetter or on par with the previously proposed state-of-the-art in allconsidered benchmarks. Our results show that previous TTA approaches areneither effective at regularizing adaptation to avoid collapse nor able tooutperform a simplistic resetting strategy.</description><author>Ori Press, Steffen Schneider, Matthias Kümmerer, Matthias Bethge</author><pubDate>Thu, 08 Jun 2023 18:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05401v1</guid></item><item><title>Matting Anything</title><link>http://arxiv.org/abs/2306.05399v1</link><description>In this paper, we propose the Matting Anything Model (MAM), an efficient andversatile framework for estimating the alpha matte of any instance in an imagewith flexible and interactive visual or linguistic user prompt guidance. MAMoffers several significant advantages over previous specialized image mattingnetworks: (i) MAM is capable of dealing with various types of image matting,including semantic, instance, and referring image matting with only a singlemodel; (ii) MAM leverages the feature maps from the Segment Anything Model(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alphamatte through iterative refinement, which has only 2.7 million trainableparameters. (iii) By incorporating SAM, MAM simplifies the user interventionrequired for the interactive use of image matting from the trimap to the box,point, or text prompt. We evaluate the performance of MAM on various imagematting benchmarks, and the experimental results demonstrate that MAM achievescomparable performance to the state-of-the-art specialized image matting modelsunder different metrics on each benchmark. Overall, MAM shows superiorgeneralization ability and can effectively handle various image matting taskswith fewer parameters, making it a practical solution for unified imagematting. Our code and models are open-sourced athttps://github.com/SHI-Labs/Matting-Anything.</description><author>Jiachen Li, Jitesh Jain, Humphrey Shi</author><pubDate>Thu, 08 Jun 2023 18:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05399v1</guid></item><item><title>When to Pre-Train Graph Neural Networks? From Data Generation Perspective!</title><link>http://arxiv.org/abs/2303.16458v4</link><description>In recent years, graph pre-training has gained significant attention,focusing on acquiring transferable knowledge from unlabeled graph data toimprove downstream performance. Despite these recent endeavors, the problem ofnegative transfer remains a major concern when utilizing graph pre-trainedmodels to downstream tasks. Previous studies made great efforts on the issue ofwhat to pre-train and how to pre-train by designing a variety of graphpre-training and fine-tuning strategies. However, there are cases where eventhe most advanced "pre-train and fine-tune" paradigms fail to yield distinctbenefits. This paper introduces a generic framework W2PGNN to answer thecrucial question of when to pre-train (i.e., in what situations could we takeadvantage of graph pre-training) before performing effortful pre-training orfine-tuning. We start from a new perspective to explore the complex generativemechanisms from the pre-training data to downstream data. In particular, W2PGNNfirst fits the pre-training data into graphon bases, each element of graphonbasis (i.e., a graphon) identifies a fundamental transferable pattern shared bya collection of pre-training graphs. All convex combinations of graphon basesgive rise to a generator space, from which graphs generated form the solutionspace for those downstream data that can benefit from pre-training. In thismanner, the feasibility of pre-training can be quantified as the generationprobability of the downstream data from any generator in the generator space.W2PGNN offers three broad applications: providing the application scope ofgraph pre-trained models, quantifying the feasibility of pre-training, andassistance in selecting pre-training data to enhance downstream performance. Weprovide a theoretically sound solution for the first application and extensiveempirical justifications for the latter two applications.</description><author>Yuxuan Cao, Jiarong Xu, Carl Yang, Jiaan Wang, Yunchao Zhang, Chunping Wang, Lei Chen, Yang Yang</author><pubDate>Thu, 08 Jun 2023 18:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16458v4</guid></item><item><title>Modular Visual Question Answering via Code Generation</title><link>http://arxiv.org/abs/2306.05392v1</link><description>We present a framework that formulates visual question answering as modularcode generation. In contrast to prior work on modular approaches to VQA, ourapproach requires no additional training and relies on pre-trained languagemodels (LMs), visual models pre-trained on image-caption pairs, and fifty VQAexamples used for in-context learning. The generated Python programs invoke andcompose the outputs of the visual models using arithmetic and conditionallogic. Our approach improves accuracy on the COVR dataset by at least 3% and onthe GQA dataset by roughly 2% compared to the few-shot baseline that does notemploy code generation.</description><author>Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, Dan Klein</author><pubDate>Thu, 08 Jun 2023 18:45:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05392v1</guid></item><item><title>HQ-50K: A Large-scale, High-quality Dataset for Image Restoration</title><link>http://arxiv.org/abs/2306.05390v1</link><description>This paper introduces a new large-scale image restoration dataset, calledHQ-50K, which contains 50,000 high-quality images with rich texture details andsemantic diversity. We analyze existing image restoration datasets from fivedifferent perspectives, including data scale, resolution, compression rates,texture details, and semantic coverage. However, we find that all of thesedatasets are deficient in some aspects. In contrast, HQ-50K considers all ofthese five aspects during the data curation process and meets all requirements.We also present a new Degradation-Aware Mixture of Expert (DAMoE) model, whichenables a single model to handle multiple corruption types and unknown levels.Our extensive experiments demonstrate that HQ-50K consistently improves theperformance on various image restoration tasks, such as super-resolution,denoising, dejpeg, and deraining. Furthermore, our proposed DAMoE, trained onour \dataset, outperforms existing state-of-the-art unified models designed formultiple restoration tasks and levels. The dataset and code are available at\url{https://github.com/littleYaang/HQ-50K}.</description><author>Qinhong Yang, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu, Jianmin Bao, Lu Yuan, Gang Hua, Nenghai Yu</author><pubDate>Thu, 08 Jun 2023 18:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05390v1</guid></item><item><title>Target-based Surrogates for Stochastic Optimization</title><link>http://arxiv.org/abs/2302.02607v2</link><description>We consider minimizing functions for which it is expensive to compute the(possibly stochastic) gradient. Such functions are prevalent in reinforcementlearning, imitation learning and adversarial training. Our target optimizationframework uses the (expensive) gradient computation to construct surrogatefunctions in a \emph{target space} (e.g. the logits output by a linear modelfor classification) that can be minimized efficiently. This allows for multipleparameter updates to the model, amortizing the cost of gradient computation. Inthe full-batch setting, we prove that our surrogate is a global upper-bound onthe loss, and can be (locally) minimized using a black-box optimizationalgorithm. We prove that the resulting majorization-minimization algorithmensures convergence to a stationary point of the loss. Next, we instantiate ourframework in the stochastic setting and propose the $SSO$ algorithm, which canbe viewed as projected stochastic gradient descent in the target space. Thisconnection enables us to prove theoretical guarantees for $SSO$ when minimizingconvex functions. Our framework allows the use of standard stochasticoptimization algorithms to construct surrogates which can be minimized by anydeterministic optimization method. To evaluate our framework, we consider asuite of supervised learning and imitation learning problems. Our experimentsindicate the benefits of target optimization and the effectiveness of $SSO$.</description><author>Jonathan Wilder Lavington, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, Nicolas Le Roux</author><pubDate>Thu, 08 Jun 2023 18:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02607v2</guid></item><item><title>Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC</title><link>http://arxiv.org/abs/2302.11552v3</link><description>Since their introduction, diffusion models have quickly become the prevailingapproach to generative modeling in many domains. They can be interpreted aslearning the gradients of a time-varying sequence of log-probability densityfunctions. This interpretation has motivated classifier-based andclassifier-free guidance as methods for post-hoc control of diffusion models.In this work, we build upon these ideas using the score-based interpretation ofdiffusion models, and explore alternative ways to condition, modify, and reusediffusion models for tasks involving compositional generation and guidance. Inparticular, we investigate why certain types of composition fail using currenttechniques and present a number of solutions. We conclude that the sampler (notthe model) is responsible for this failure and propose new samplers, inspiredby MCMC, which enable successful compositional generation. Further, we proposean energy-based parameterization of diffusion models which enables the use ofnew compositional operators and more sophisticated, Metropolis-correctedsamplers. Intriguingly we find these samplers lead to notable improvements incompositional generation across a wide set of problems such asclassifier-guided ImageNet modeling and compositional text-to-image generation.</description><author>Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl</author><pubDate>Thu, 08 Jun 2023 18:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11552v3</guid></item><item><title>Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across Age</title><link>http://arxiv.org/abs/2306.05387v1</link><description>Emerging psychopathology studies are showing that patterns of changes inemotional state -- emotion dynamics -- are associated with overall well-beingand mental health. More recently, there has been some work in tracking emotiondynamics through one's utterances, allowing for data to be collected on alarger scale across time and people. However, several questions about howemotion dynamics change with age, especially in children, and when determinedthrough children's writing, remain unanswered. In this work, we use both alexicon and a machine learning based approach to quantify characteristics ofemotion dynamics determined from poems written by children of various ages. Weshow that both approaches point to similar trends: consistent increasingintensities for some emotions (e.g., anger, fear, joy, sadness, arousal, anddominance) with age and a consistent decreasing valence with age. We also findincreasing emotional variability, rise rates (i.e., emotional reactivity), andrecovery rates (i.e., emotional regulation) with age. These results act as auseful baselines for further research in how patterns of emotions expressed bychildren change with age, and their association with mental health.</description><author>Daniela Teodorescu, Alona Fyshe, Saif M. Mohammad</author><pubDate>Thu, 08 Jun 2023 18:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05387v1</guid></item><item><title>Parallel Sampling of Diffusion Models</title><link>http://arxiv.org/abs/2305.16317v2</link><description>Diffusion models are powerful generative models but suffer from slowsampling, often taking 1000 sequential denoising steps for one sample. As aresult, considerable efforts have been directed toward reducing the number ofdenoising steps, but these methods hurt sample quality. Instead of reducing thenumber of denoising steps (trading quality for speed), in this paper we explorean orthogonal approach: can we run the denoising steps in parallel (tradingcompute for speed)? In spite of the sequential nature of the denoising steps,we show that surprisingly it is possible to parallelize sampling via Picarditerations, by guessing the solution of future denoising steps and iterativelyrefining until convergence. With this insight, we present ParaDiGMS, a novelmethod to accelerate the sampling of pretrained diffusion models by denoisingmultiple steps in parallel. ParaDiGMS is the first diffusion sampling methodthat enables trading compute for speed and is even compatible with existingfast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, weimprove sampling speed by 2-4x across a range of robotics and image generationmodels, giving state-of-the-art sampling speeds of 0.2s on 100-stepDiffusionPolicy and 16s on 1000-step StableDiffusion-v2 with no measurabledegradation of task reward, FID score, or CLIP score.</description><author>Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, Nima Anari</author><pubDate>Thu, 08 Jun 2023 18:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16317v2</guid></item><item><title>Robust Subtask Learning for Compositional Generalization</title><link>http://arxiv.org/abs/2302.02984v2</link><description>Compositional reinforcement learning is a promising approach for trainingpolicies to perform complex long-horizon tasks. Typically, a high-level task isdecomposed into a sequence of subtasks and a separate policy is trained toperform each subtask. In this paper, we focus on the problem of trainingsubtask policies in a way that they can be used to perform any task; here, atask is given by a sequence of subtasks. We aim to maximize the worst-caseperformance over all tasks as opposed to the average-case performance. Weformulate the problem as a two agent zero-sum game in which the adversary picksthe sequence of subtasks. We propose two RL algorithms to solve this game: oneis an adaptation of existing multi-agent RL algorithms to our setting and theother is an asynchronous version which enables parallel training of subtaskpolicies. We evaluate our approach on two multi-task environments withcontinuous states and actions and demonstrate that our algorithms outperformstate-of-the-art baselines.</description><author>Kishor Jothimurugan, Steve Hsu, Osbert Bastani, Rajeev Alur</author><pubDate>Thu, 08 Jun 2023 18:31:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02984v2</guid></item><item><title>Automatic Image Blending Algorithm Based on SAM and DINO</title><link>http://arxiv.org/abs/2306.05382v1</link><description>The field of image blending has gained significant popularity in recent yearsdue to its ability to create visually stunning content. The main objective ofimage blending is to merge an object from one image onto another seamlessly,with minor masking adjustments. With the recent development of SAM, which candetect and segment targets in images automatically. Our approach (1) combinessemantic object detection and segmentation with corresponding mask generationto automatically fuse images and (2) introduces the use of PAN for furtherquality enhancement during the fusion process. Our approach surpasses manyclassical visual fusion models in various performance indicators such as PSNR,SSIM, and Realism. Notably, our process is highly efficient and speedy, makingit widely applicable in industrial settings. This new process has the potentialto revolutionize visual content creation and improve productivity acrossvarious industries.</description><author>Haochen Xue, Mingyu Jin, Chong Zhang, Yuxuan Huang, Qian Weng, Xiaobo Jin</author><pubDate>Thu, 08 Jun 2023 18:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05382v1</guid></item><item><title>Do language models have coherent mental models of everyday things?</title><link>http://arxiv.org/abs/2212.10029v3</link><description>When people think of everyday things like an egg, they typically have amental image associated with it. This allows them to correctly judge, forexample, that "the yolk surrounds the shell" is a false statement. Do languagemodels similarly have a coherent picture of such everyday things? Toinvestigate this, we propose a benchmark dataset consisting of 100 everydaythings, their parts, and the relationships between these parts, expressed as11,720 "X relation Y?" true/false questions. Using these questions as probes,we observe that state-of-the-art pre-trained language models (LMs) like GPT-3and Macaw have fragments of knowledge about these everyday things, but do nothave fully coherent "parts mental models" (54-59% accurate, 19-43% conditionalconstraint violation). We propose an extension where we add a constraintsatisfaction layer on top of the LM's raw predictions to apply commonsenseconstraints. As well as removing inconsistencies, we find that this alsosignificantly improves accuracy (by 16-20%), suggesting how the incoherence ofthe LM's pictures of everyday things can be significantly reduced.</description><author>Yuling Gu, Bhavana Dalvi Mishra, Peter Clark</author><pubDate>Thu, 08 Jun 2023 18:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10029v3</guid></item><item><title>Leveraging Diffusion For Strong and High Quality Face Morphing Attacks</title><link>http://arxiv.org/abs/2301.04218v3</link><description>Face morphing attacks seek to deceive a Face Recognition (FR) system bypresenting a morphed image consisting of the biometric qualities from twodifferent identities with the aim of triggering a false acceptance with one ofthe two identities, thereby presenting a significant threat to biometricsystems. The success of a morphing attack is dependent on the ability of themorphed image to represent the biometric characteristics of both identitiesthat were used to create the image. We present a novel morphing attack thatuses a Diffusion-based architecture to improve the visual fidelity of the imageand the ability of the morphing attack to represent characteristics from bothidentities. We demonstrate the effectiveness of the proposed attack byevaluating its visual fidelity via the Frechet Inception Distance (FID). Also,extensive experiments are conducted to measure the vulnerability of FR systemsto the proposed attack. The ability of a morphing attack detector to detect theproposed attack is measured and compared against two state-of-the-art GAN-basedmorphing attacks along with two Landmark-based attacks. Additionally, a novelmetric to measure the relative strength between different morphing attacks isintroduced and evaluated.</description><author>Zander Blasingame, Chen Liu</author><pubDate>Thu, 08 Jun 2023 18:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04218v3</guid></item><item><title>DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference</title><link>http://arxiv.org/abs/2303.06171v2</link><description>Bayesian inference provides a principled framework for learning from complexdata and reasoning under uncertainty. It has been widely applied in machinelearning tasks such as medical diagnosis, drug design, and policymaking. Inthese common applications, data can be highly sensitive. Differential privacy(DP) offers data analysis tools with powerful worst-case privacy guarantees andhas been developed as the leading approach in privacy-preserving data analysis.In this paper, we study Metropolis-Hastings (MH), one of the most fundamentalMCMC methods, for large-scale Bayesian inference under differential privacy.While most existing private MCMC algorithms sacrifice accuracy and efficiencyto obtain privacy, we provide the first exact and fast DP MH algorithm, usingonly a minibatch of data in most iterations. We further reveal, for the firsttime, a three-way trade-off among privacy, scalability (i.e. the batch size),and efficiency (i.e. the convergence rate), theoretically characterizing howprivacy affects the utility and computational cost in Bayesian inference. Weempirically demonstrate the effectiveness and efficiency of our algorithm invarious experiments.</description><author>Wanrong Zhang, Ruqi Zhang</author><pubDate>Thu, 08 Jun 2023 18:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06171v2</guid></item><item><title>Causal Bandits without Graph Learning</title><link>http://arxiv.org/abs/2301.11401v2</link><description>We study the causal bandit problem when the causal graph is unknown anddevelop an efficient algorithm for finding the parent node of the reward nodeusing atomic interventions. We derive the exact equation for the expectednumber of interventions performed by the algorithm and show that under certaingraphical conditions it could perform either logarithmically fast or, undermore general assumptions, slower but still sublinearly in the number ofvariables. We formally show that our algorithm is optimal as it meets theuniversal lower bound we establish for any algorithm that performs atomicinterventions. Finally, we extend our algorithm to the case when the rewardnode has multiple parents. Using this algorithm together with a standardalgorithm from bandit literature leads to improved regret bounds.</description><author>Mikhail Konobeev, Jalal Etesami, Negar Kiyavash</author><pubDate>Thu, 08 Jun 2023 18:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11401v2</guid></item><item><title>Ordinal Potential-based Player Rating</title><link>http://arxiv.org/abs/2306.05366v1</link><description>A two-player symmetric zero-sum game is transitive if for any pure strategies$x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$is better than $z$. It was recently observed that the Elo rating fails atpreserving transitive relations among strategies and therefore cannot correctlyextract the transitive component of a game. Our first contribution is to showthat the Elo rating actually does preserve transitivity when computed in theright space. Precisely, using a suitable invertible mapping $\varphi$, we firstapply $\varphi$ to the game, then compute Elo ratings, then go back to theoriginal space by applying $\varphi^{-1}$. We provide a characterization oftransitive games as a weak variant of ordinal potential games with additivelyseparable potential functions. Leveraging this insight, we introduce theconcept of transitivity order, the minimum number of invertible mappingsrequired to transform the payoff of a transitive game into (differences of) itspotential function. The transitivity order is a tool to classify transitivegames, with Elo games being an example of transitive games of order one. Mostreal-world games have both transitive and non-transitive (cyclic) components,and we use our analysis of transitivity to extract the transitive (potential)component of an arbitrary game. We link transitivity to the known concept ofsign-rank: transitive games have sign-rank two; arbitrary games may have highersign-rank. Using a neural network-based architecture, we learn a decompositionof an arbitrary game into transitive and cyclic components that prioritisescapturing the sign pattern of the game. In particular, a transitive game alwayshas just one component in its decomposition, the potential component. Weprovide a comprehensive evaluation of our methodology using both toy examplesand empirical data from real-world games.</description><author>Nelson Vadori, Rahul Savani</author><pubDate>Thu, 08 Jun 2023 18:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05366v1</guid></item><item><title>Subject clustering by IF-PCA and several recent methods</title><link>http://arxiv.org/abs/2306.05363v1</link><description>Subject clustering (i.e., the use of measured features to cluster subjects,such as patients or cells, into multiple groups) is a problem of greatinterest. In recent years, many approaches were proposed, among whichunsupervised deep learning (UDL) has received a great deal of attention. Twointeresting questions are (a) how to combine the strengths of UDL and otherapproaches, and (b) how these approaches compare to one other. We combine Variational Auto-Encoder (VAE), a popular UDL approach, with therecent idea of Influential Feature PCA (IF-PCA), and propose IF-VAE as a newmethod for subject clustering. We study IF-VAE and compare it with severalother methods (including IF-PCA, VAE, Seurat, and SC3) on $10$ gene microarraydata sets and $8$ single-cell RNA-seq data sets. We find that IF-VAEsignificantly improves over VAE, but still underperforms IF-PCA. We also findthat IF-PCA is quite competitive, which slightly outperforms Seurat and SC3over the $8$ single-cell data sets. IF-PCA is conceptually simple and permitsdelicate analysis. We demonstrate that IF-PCA is capable of achieving the phasetransition in a Rare/Weak model. Comparatively, Seurat and SC3 are more complexand theoretically difficult to analyze (for these reasons, their optimalityremains unclear).</description><author>Dieyi Chen, Jiashun Jin, Zheng Tracy Ke</author><pubDate>Thu, 08 Jun 2023 18:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05363v1</guid></item><item><title>The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues</title><link>http://arxiv.org/abs/2306.05360v1</link><description>This paper presents the ADAIO team's system entry in the Building EducationalApplications (BEA) 2023 Shared Task on Generating AI Teacher Responses inEducational Dialogues. The task aims to assess the performance ofstate-of-the-art generative models as AI teachers in producing suitableresponses within a student-teacher dialogue. Our system comprises evaluatingvarious baseline models using OpenAI GPT-3 and designing diverse prompts toprompt the OpenAI models for teacher response generation. After the challenge,our system achieved second place by employing a few-shot prompt-based approachwith the OpenAI text-davinci-003 model. The results highlight the few-shotlearning capabilities of large-language models, particularly OpenAI's GPT-3, inthe role of AI teachers.</description><author>Adaeze Adigwe, Zheng Yuan</author><pubDate>Thu, 08 Jun 2023 18:05:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05360v1</guid></item><item><title>Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models</title><link>http://arxiv.org/abs/2306.05357v1</link><description>Text-to-image generative models have enabled high-resolution image synthesisacross different domains, but require users to specify the content they wish togenerate. In this paper, we consider the inverse problem -- given a collectionof different images, can we discover the generative concepts that representeach image? We present an unsupervised approach to discover generative conceptsfrom a collection of images, disentangling different art styles in paintings,objects, and lighting from kitchen scenes, and discovering image classes givenImageNet images. We show how such generative concepts can accurately representthe content of images, be recombined and composed to generate new artistic andhybrid images, and be further used as a representation for downstreamclassification tasks.</description><author>Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, Antonio Torralba</author><pubDate>Thu, 08 Jun 2023 18:02:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05357v1</guid></item><item><title>ReliableSwap: Boosting General Face Swapping Via Reliable Supervision</title><link>http://arxiv.org/abs/2306.05356v1</link><description>Almost all advanced face swapping approaches use reconstruction as the proxytask, i.e., supervision only exists when the target and source belong to thesame person. Otherwise, lacking pixel-level supervision, these methods strugglefor source identity preservation. This paper proposes to construct reliablesupervision, dubbed cycle triplets, which serves as the image-level guidancewhen the source identity differs from the target one during training.Specifically, we use face reenactment and blending techniques to synthesize theswapped face from real images in advance, where the synthetic face preservessource identity and target attributes. However, there may be some artifacts insuch a synthetic face. To avoid the potential artifacts and drive thedistribution of the network output close to the natural one, we reversely takesynthetic images as input while the real face as reliable supervision duringthe training stage of face swapping. Besides, we empirically find that theexisting methods tend to lose lower-face details like face shape and mouth fromthe source. This paper additionally designs a FixerNet, providingdiscriminative embeddings of lower faces as an enhancement. Our face swappingframework, named ReliableSwap, can boost the performance of any existing faceswapping network with negligible overhead. Extensive experiments demonstratethe efficacy of our ReliableSwap, especially in identity preservation. Theproject page is https://reliable-swap.github.io/.</description><author>Ge Yuan, Maomao Li, Yong Zhang, Huicheng Zheng</author><pubDate>Thu, 08 Jun 2023 18:01:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05356v1</guid></item><item><title>Negotiated Reasoning: On Provably Addressing Relative Over-Generalization</title><link>http://arxiv.org/abs/2306.05353v1</link><description>Over-generalization is a thorny issue in cognitive science, where people maybecome overly cautious due to past experiences. Agents in multi-agentreinforcement learning (MARL) also have been found to suffer relativeover-generalization (RO) as people do and stuck to sub-optimal cooperation.Recent methods have shown that assigning reasoning ability to agents canmitigate RO algorithmically and empirically, but there has been a lack oftheoretical understanding of RO, let alone designing provably RO-free methods.This paper first proves that RO can be avoided when the MARL method satisfies aconsistent reasoning requirement under certain conditions. Then we introduce anovel reasoning framework, called negotiated reasoning, that first builds theconnection between reasoning and RO with theoretical justifications. Afterthat, we propose an instantiated algorithm, Stein variational negotiatedreasoning (SVNR), which uses Stein variational gradient descent to derive anegotiation policy that provably avoids RO in MARL under maximum entropy policyiteration. The method is further parameterized with neural networks foramortized learning, making computation efficient. Numerical experiments on manyRO-challenged environments demonstrate the superiority and efficiency of SVNRcompared to state-of-the-art methods in addressing RO.</description><author>Junjie Sheng, Wenhao Li, Bo Jin, Hongyuan Zha, Jun Wang, Xiangfeng Wang</author><pubDate>Thu, 08 Jun 2023 17:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05353v1</guid></item><item><title>Multitask Learning and Bandits via Robust Statistics</title><link>http://arxiv.org/abs/2112.14233v3</link><description>Decision-makers often simultaneously face many related but heterogeneouslearning problems. For instance, a large retailer may wish to learn productdemand at different stores to solve pricing or inventory problems, making itdesirable to learn jointly for stores serving similar customers; alternatively,a hospital network may wish to learn patient risk at different providers toallocate personalized interventions, making it desirable to learn jointly forhospitals serving similar patient populations. Motivated by real datasets, westudy a natural setting where the unknown parameter in each learning instancecan be decomposed into a shared global parameter plus a sparseinstance-specific term. We propose a novel two-stage multitask learningestimator that exploits this structure in a sample-efficient way, using aunique combination of robust statistics (to learn across similar instances) andLASSO regression (to debias the results). Our estimator yields improved samplecomplexity bounds in the feature dimension $d$ relative to commonly-employedestimators; this improvement is exponential for "data-poor" instances, whichbenefit the most from multitask learning. We illustrate the utility of theseresults for online learning by embedding our multitask estimator withinsimultaneous contextual bandit algorithms. We specify a dynamic calibration ofour estimator to appropriately balance the bias-variance tradeoff over time,improving the resulting regret bounds in the context dimension $d$. Finally, weillustrate the value of our approach on synthetic and real datasets.</description><author>Kan Xu, Hamsa Bastani</author><pubDate>Thu, 08 Jun 2023 17:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.14233v3</guid></item><item><title>A Crystal-Specific Pre-Training Framework for Crystal Material Property Prediction</title><link>http://arxiv.org/abs/2306.05344v1</link><description>Crystal property prediction is a crucial aspect of developing novelmaterials. However, there are two technical challenges to be addressed forspeeding up the investigation of crystals. First, labeling crystal propertiesis intrinsically difficult due to the high cost and time involved in physicalsimulations or lab experiments. Second, crystals adhere to a specific quantumchemical principle known as periodic invariance, which is often not captured byexisting machine learning methods. To overcome these challenges, we propose thecrystal-specific pre-training framework for learning crystal representationswith self-supervision. The framework designs a mutex mask strategy forenhancing representation learning so as to alleviate the limited labelsavailable for crystal property prediction. Moreover, we take into account thespecific periodic invariance in crystal structures by developing a periodicinvariance multi-graph module and periodic attribute learning within ourframework. This framework has been tested on eight different tasks. Theexperimental results on these tasks show that the framework achieves promisingprediction performance and is able to outperform recent strong baselines.</description><author>Haomin Yu, Yanru Song, Jilin Hu, Chenjuan Guo, Bin Yang</author><pubDate>Thu, 08 Jun 2023 17:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05344v1</guid></item><item><title>Real-time GeoAI for High-resolution Mapping and Segmentation of Arctic Permafrost Features</title><link>http://arxiv.org/abs/2306.05341v1</link><description>This paper introduces a real-time GeoAI workflow for large-scale imageanalysis and the segmentation of Arctic permafrost features at afine-granularity. Very high-resolution (0.5m) commercial imagery is used inthis analysis. To achieve real-time prediction, our workflow employs alightweight, deep learning-based instance segmentation model, SparseInst, whichintroduces and uses Instance Activation Maps to accurately locate the positionof objects within the image scene. Experimental results show that the model canachieve better accuracy of prediction at a much faster inference speed than thepopular Mask-RCNN model.</description><author>Wenwen Li, Chia-Yu Hsu, Sizhe Wang, Chandi Witharana, Anna Liljedahl</author><pubDate>Thu, 08 Jun 2023 17:45:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05341v1</guid></item><item><title>Graph-based Time-Series Anomaly Detection: A Survey</title><link>http://arxiv.org/abs/2302.00058v2</link><description>With the recent advances in technology, a wide range of systems continue tocollect a large amount of data over time and thus generate time series.Time-Series Anomaly Detection (TSAD) is an important task in varioustime-series applications such as e-commerce, cybersecurity, vehiclemaintenance, and healthcare monitoring. However, this task is very challengingas it requires considering both the intra-variable dependency and theinter-variable dependency, where a variable can be defined as an observation intime series data. Recent graph-based approaches have made impressive progressin tackling the challenges of this field. In this survey, we conduct acomprehensive and up-to-date review of Graph-based TSAD (G-TSAD). First, weexplore the significant potential of graph representation learning fortime-series data. Then, we review state-of-the-art graph anomaly detectiontechniques in the context of time series and discuss their strengths anddrawbacks. Finally, we discuss the technical challenges and potential futuredirections for possible improvements in this research field.</description><author>Thi Kieu Khanh Ho, Ali Karami, Narges Armanfard</author><pubDate>Thu, 08 Jun 2023 17:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00058v2</guid></item><item><title>Deep Learning Meets Sparse Regularization: A Signal Processing Perspective</title><link>http://arxiv.org/abs/2301.09554v3</link><description>Deep learning has been wildly successful in practice and moststate-of-the-art machine learning methods are based on neural networks.Lacking, however, is a rigorous mathematical theory that adequately explainsthe amazing performance of deep neural networks. In this article, we present arelatively new mathematical framework that provides the beginning of a deeperunderstanding of deep learning. This framework precisely characterizes thefunctional properties of neural networks that are trained to fit to data. Thekey mathematical tools which support this framework include transform-domainsparse regularization, the Radon transform of computed tomography, andapproximation theory, which are all techniques deeply rooted in signalprocessing. This framework explains the effect of weight decay regularizationin neural network training, the use of skip connections and low-rank weightmatrices in network architectures, the role of sparsity in neural networks, andexplains why neural networks can perform well in high-dimensional problems.</description><author>Rahul Parhi, Robert D. Nowak</author><pubDate>Thu, 08 Jun 2023 17:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09554v3</guid></item><item><title>Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners</title><link>http://arxiv.org/abs/2305.14825v2</link><description>The emergent few-shot reasoning capabilities of Large Language Models (LLMs)have excited the natural language and machine learning community over recentyears. Despite of numerous successful applications, the underlying mechanism ofsuch in-context capabilities still remains unclear. In this work, wehypothesize that the learned \textit{semantics} of language tokens do the mostheavy lifting during the reasoning process. Different from human's symbolicreasoning process, the semantic representations of LLMs could create strongconnections among tokens, thus composing a superficial logical chain. To testour hypothesis, we decouple semantics from the language reasoning process andevaluate three kinds of reasoning abilities, i.e., deduction, induction andabduction. Our findings reveal that semantics play a vital role in LLMs'in-context reasoning -- LLMs perform significantly better when semantics areconsistent with commonsense but struggle to solve symbolic orcounter-commonsense reasoning tasks by leveraging in-context new knowledge. Thesurprising observations question whether modern LLMs have mastered theinductive, deductive and abductive reasoning abilities as in humanintelligence, and motivate research on unveiling the magic existing within theblack-box LLMs. On the whole, our analysis provides a novel perspective on therole of semantics in developing and evaluating language models' reasoningabilities. Code is available at {\url{https://github.com/XiaojuanTang/ICSR}}.</description><author>Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, Muhan Zhang</author><pubDate>Thu, 08 Jun 2023 17:38:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14825v2</guid></item><item><title>Actively learning a Bayesian matrix fusion model with deep side information</title><link>http://arxiv.org/abs/2306.05331v1</link><description>High-dimensional deep neural network representations of images and conceptscan be aligned to predict human annotations of diverse stimuli. However, suchalignment requires the costly collection of behavioral responses, such that, inpractice, the deep-feature spaces are only ever sparsely sampled. Here, wepropose an active learning approach to adaptively sampling experimental stimulito efficiently learn a Bayesian matrix factorization model with deep sideinformation. We observe a significant efficiency gain over a passive baseline.Furthermore, with a sequential batched sampling strategy, the algorithm isapplicable not only to small datasets collected from traditional laboratoryexperiments but also to settings where large-scale crowdsourced data collectionis needed to accurately align the high-dimensional deep feature representationsderived from pre-trained networks.</description><author>Yangyang Yu, Jordan W. Suchow</author><pubDate>Thu, 08 Jun 2023 17:31:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05331v1</guid></item><item><title>A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm under Smoothness and Isoperimetry</title><link>http://arxiv.org/abs/2304.04095v2</link><description>We study the mixing time of Metropolis-Adjusted Langevin algorithm (MALA) forsampling a target density on $\mathbb{R}^d$. We assume that the target densitysatisfies $\psi_\mu$-isoperimetry and that the operator norm and trace of itsHessian are bounded by $L$ and $\Upsilon$ respectively. Our main resultestablishes that, from a warm start, to achieve $\epsilon$-total variationdistance to the target density, MALA mixes in$O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2}\log\left(\frac{1}{\epsilon}\right)\right)$ iterations. Notably, this resultholds beyond the log-concave sampling setting and the mixing time depends ononly $\Upsilon$ rather than its upper bound $L d$. In the $m$-stronglylogconcave and $L$-log-smooth sampling setting, our bound recovers the previousminimax mixing bound of MALA~\cite{wu2021minimax}.</description><author>Yuansi Chen, Khashayar Gatmiry</author><pubDate>Thu, 08 Jun 2023 17:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04095v2</guid></item><item><title>When does Metropolized Hamiltonian Monte Carlo provably outperform Metropolis-adjusted Langevin algorithm?</title><link>http://arxiv.org/abs/2304.04724v2</link><description>We analyze the mixing time of Metropolized Hamiltonian Monte Carlo (HMC) withthe leapfrog integrator to sample from a distribution on $\mathbb{R}^d$ whoselog-density is smooth, has Lipschitz Hessian in Frobenius norm and satisfiesisoperimetry. We bound the gradient complexity to reach $\epsilon$ error intotal variation distance from a warm start by $\tildeO(d^{1/4}\text{polylog}(1/\epsilon))$ and demonstrate the benefit of choosingthe number of leapfrog steps to be larger than 1. To surpass previous analysison Metropolis-adjusted Langevin algorithm (MALA) that has$\tilde{O}(d^{1/2}\text{polylog}(1/\epsilon))$ dimension dependency in Wu etal. (2022), we reveal a key feature in our proof that the joint distribution ofthe location and velocity variables of the discretization of the continuous HMCdynamics stays approximately invariant. This key feature, when shown viainduction over the number of leapfrog steps, enables us to obtain estimates onmoments of various quantities that appear in the acceptance rate control ofMetropolized HMC. Moreover, to deal with another bottleneck on the HMC proposaldistribution overlap control in the literature, we provide a new approach toupper bound the Kullback-Leibler divergence between push-forwards of theGaussian distribution through HMC dynamics initialized at two different points.Notably, our analysis does not require log-concavity or independence of themarginals, and only relies on an isoperimetric inequality. To illustrate theapplicability of our result, several examples of natural functions that fallinto our framework are discussed.</description><author>Yuansi Chen, Khashayar Gatmiry</author><pubDate>Thu, 08 Jun 2023 17:26:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04724v2</guid></item><item><title>Cyclic Coordinate Dual Averaging with Extrapolation</title><link>http://arxiv.org/abs/2102.13244v4</link><description>Cyclic block coordinate methods are a fundamental class of optimizationmethods widely used in practice and implemented as part of standard softwarepackages for statistical learning. Nevertheless, their convergence is generallynot well understood and so far their good practical performance has not beenexplained by existing convergence analyses. In this work, we introduce a newblock coordinate method that applies to the general class of variationalinequality (VI) problems with monotone operators. This class includes compositeconvex optimization problems and convex-concave min-max optimization problemsas special cases and has not been addressed by the existing work. The resultingconvergence bounds match the optimal convergence bounds of full gradientmethods, but are provided in terms of a novel gradient Lipschitz conditionw.r.t.~a Mahalanobis norm. For $m$ coordinate blocks, the resulting gradientLipschitz constant in our bounds is never larger than a factor $\sqrt{m}$compared to the traditional Euclidean Lipschitz constant, while it is possiblefor it to be much smaller. Further, for the case when the operator in the VIhas finite-sum structure, we propose a variance reduced variant of our methodwhich further decreases the per-iteration cost and has better convergence ratesin certain regimes. To obtain these results, we use a gradient extrapolationstrategy that allows us to view a cyclic collection of block coordinate-wisegradients as one implicit gradient.</description><author>Chaobing Song, Jelena Diakonikolas</author><pubDate>Thu, 08 Jun 2023 17:24:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.13244v4</guid></item><item><title>Federated Learning under Covariate Shifts with Generalization Guarantees</title><link>http://arxiv.org/abs/2306.05325v1</link><description>This paper addresses intra-client and inter-client covariate shifts infederated learning (FL) with a focus on the overall generalization performance.To handle covariate shifts, we formulate a new global model training paradigmand propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM)along with improving density ratio matching methods without requiring perfectknowledge of the supremum over true ratios. We also propose thecommunication-efficient variant FITW-ERM with the same level of privacyguarantees as those of classical ERM in FL. We theoretically show that FTW-ERMachieves smaller generalization error than classical ERM under certainsettings. Experimental results demonstrate the superiority of FTW-ERM overexisting FL baselines in challenging imbalanced federated settings in terms ofdata distribution shifts across clients.</description><author>Ali Ramezani-Kebrya, Fanghui Liu, Thomas Pethick, Grigorios Chrysos, Volkan Cevher</author><pubDate>Thu, 08 Jun 2023 17:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05325v1</guid></item><item><title>Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application</title><link>http://arxiv.org/abs/2306.05323v1</link><description>The introduction of computerized medical records in hospitals has reducedburdensome operations like manual writing and information fetching. However,the data contained in medical records are still far underutilized, primarilybecause extracting them from unstructured textual medical records takes timeand effort. Information Extraction, a subfield of Natural Language Processing,can help clinical practitioners overcome this limitation, using automatedtext-mining pipelines. In this work, we created the first Italianneuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it todevelop a Large Language Model for this task. Moreover, we conducted severalexperiments with three external independent datasets to implement an effectivemulticenter model, with overall F1-score 84.77%, Precision 83.16%, Recall86.44%. The lessons learned are: (i) the crucial role of a consistentannotation process and (ii) a fine-tuning strategy that combines classicalmethods with a "few-shot" approach. This allowed us to establish methodologicalguidelines that pave the way for future implementations in this field and allowItalian hospitals to tap into important research opportunities.</description><author>Claudio Crema, Tommaso Mario Buonocore, Silvia Fostinelli, Enea Parimbelli, Federico Verde, Cira Fundarò, Marina Manera, Matteo Cotta Ramusino, Marco Capelli, Alfredo Costa, Giuliano Binetti, Riccardo Bellazzi, Alberto Redolfi</author><pubDate>Thu, 08 Jun 2023 17:15:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05323v1</guid></item><item><title>Real-time whole-heart electromechanical simulations using Latent Neural Ordinary Differential Equations</title><link>http://arxiv.org/abs/2306.05321v1</link><description>Cardiac digital twins provide a physics and physiology informed framework todeliver predictive and personalized medicine. However, high-fidelitymulti-scale cardiac models remain a barrier to adoption due to their extensivecomputational costs and the high number of model evaluations needed forpatient-specific personalization. Artificial Intelligence-based methods canmake the creation of fast and accurate whole-heart digital twins feasible. Inthis work, we use Latent Neural Ordinary Differential Equations (LNODEs) tolearn the temporal pressure-volume dynamics of a heart failure patient. Oursurrogate model based on LNODEs is trained from 400 3D-0D whole-heartclosed-loop electromechanical simulations while accounting for 43 modelparameters, describing single cell through to whole organ and cardiovascularhemodynamics. The trained LNODEs provides a compact and efficientrepresentation of the 3D-0D model in a latent space by means of a feedforwardfully-connected Artificial Neural Network that retains 3 hidden layers with 13neurons per layer and allows for 300x real-time numerical simulations of thecardiac function on a single processor of a standard laptop. This surrogatemodel is employed to perform global sensitivity analysis and robust parameterestimation with uncertainty quantification in 3 hours of computations, still ona single processor. We match pressure and volume time traces unseen by theLNODEs during the training phase and we calibrate 4 to 11 model parameterswhile also providing their posterior distribution. This paper introduces themost advanced surrogate model of cardiac function available in the literatureand opens new important venues for parameter calibration in cardiac digitaltwins.</description><author>Matteo Salvador, Marina Strocchi, Francesco Regazzoni, Luca Dede', Steven Niederer, Alfio Quarteroni</author><pubDate>Thu, 08 Jun 2023 17:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05321v1</guid></item><item><title>KIT's Multilingual Speech Translation System for IWSLT 2023</title><link>http://arxiv.org/abs/2306.05320v1</link><description>Many existing speech translation benchmarks focus on native-English speech inhigh-quality recording conditions, which often do not match the conditions inreal-life use-cases. In this paper, we describe our speech translation systemfor the multilingual track of IWSLT 2023, which focuses on the translation ofscientific conference talks. The test condition features accented input speechand terminology-dense contents. The tasks requires translation into 10languages of varying amounts of resources. In absence of training data from thetarget domain, we use a retrieval-based approach (kNN-MT) for effectiveadaptation (+0.8 BLEU for speech translation). We also use adapters to easilyintegrate incremental training data from data augmentation, and show that itmatches the performance of re-training. We observe that cascaded systems aremore easily adaptable towards specific target domains, due to their separatemodules. Our cascaded speech system substantially outperforms its end-to-endcounterpart on scientific talk translation, although their performance remainssimilar on TED talks.</description><author>Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues</author><pubDate>Thu, 08 Jun 2023 17:13:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05320v1</guid></item><item><title>RNN-Based GNSS Positioning using Satellite Measurement Features and Pseudorange Residuals</title><link>http://arxiv.org/abs/2306.05319v1</link><description>In the Global Navigation Satellite System (GNSS) context, the growing numberof available satellites has lead to many challenges when it comes to choosingthe most accurate pseudorange contributions, given the strong impact of biasedmeasurements on positioning accuracy, particularly in single-epoch scenarios.This work leverages the potential of machine learning in predicting link-wisemeasurement quality factors and, hence, optimize measurement weighting. Forthis purpose, we use a customized matrix composed of heterogeneous featuressuch as conditional pseudorange residuals and per-link satellite metrics (e.g.,carrier-to-noise power density ratio and its empirical statistics, satelliteelevation, carrier phase lock time). This matrix is then fed as an input to arecurrent neural network (RNN) (i.e., a long-short term memory (LSTM) network).Our experimental results on real data, obtained from extensive fieldmeasurements, demonstrate the high potential of our proposed solution beingable to outperform traditional measurements weighting and selection strategiesfrom state-of-the-art.</description><author>Ibrahim Sbeity, Christophe Villien, Benoît Denis, E. Veronica Belmega</author><pubDate>Thu, 08 Jun 2023 17:11:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05319v1</guid></item><item><title>CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models</title><link>http://arxiv.org/abs/2306.05317v1</link><description>In this paper, we consider the challenge of summarizing patients' medicalprogress notes in a limited data setting. For the Problem List Summarization(shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5fine-tuned to 765 medical clinic notes outperforms other extractive,abstractive and zero-shot baselines, yielding reasonable baseline systems formedical note summarization. Further, we introduce Hierarchical Ensemble ofSummarization Models (HESM), consisting of token-level ensembles of diversefine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding.Our HESM approach lead to a considerable summarization performance boost, andwhen evaluated on held-out challenge data achieved a ROUGE-L of 32.77, whichwas the best-performing system at the top of the shared task leaderboard.</description><author>Potsawee Manakul, Yassir Fathullah, Adian Liusie, Vyas Raina, Vatsal Raina, Mark Gales</author><pubDate>Thu, 08 Jun 2023 17:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05317v1</guid></item><item><title>Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization</title><link>http://arxiv.org/abs/2306.01103v2</link><description>We tackle the problem of graph out-of-distribution (OOD) generalization.Existing graph OOD algorithms either rely on restricted assumptions or fail toexploit environment information in training data. In this work, we propose tosimultaneously incorporate label and environment causal independence (LECI) tofully make use of label and environment information, thereby addressing thechallenges faced by prior methods on identifying causal and invariantsubgraphs. We further develop an adversarial training strategy to jointlyoptimize these two properties for causal subgraph discovery with theoreticalguarantees. Extensive experiments and analysis show that LECI significantlyoutperforms prior methods on both synthetic and real-world datasets,establishing LECI as a practical and effective solution for graph OODgeneralization.</description><author>Shurui Gui, Meng Liu, Xiner Li, Youzhi Luo, Shuiwang Ji</author><pubDate>Thu, 08 Jun 2023 17:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01103v2</guid></item><item><title>Predictive Modeling of Equine Activity Budgets Using a 3D Skeleton Reconstructed from Surveillance Recordings</title><link>http://arxiv.org/abs/2306.05311v1</link><description>In this work, we present a pipeline to reconstruct the 3D pose of a horsefrom 4 simultaneous surveillance camera recordings. Our environment posesinteresting challenges to tackle, such as limited field view of the cameras anda relatively closed and small environment. The pipeline consists of training a2D markerless pose estimation model to work on every viewpoint, then applyingit to the videos and performing triangulation. We present numerical evaluationof the results (error analysis), as well as show the utility of the achievedposes in downstream tasks of selected behavioral predictions. Our analysis ofthe predictive model for equine behavior showed a bias towards pain-inducedhorses, which aligns with our understanding of how behavior varies acrosspainful and healthy subjects.</description><author>Ernest Pokropek, Sofia Broomé, Pia Haubro Andersen, Hedvig Kjellström</author><pubDate>Thu, 08 Jun 2023 17:00:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05311v1</guid></item><item><title>A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments</title><link>http://arxiv.org/abs/2306.05310v1</link><description>While Deep Reinforcement Learning has been widely researched in medicalimaging, the training and deployment of these models usually require powerfulGPUs. Since imaging environments evolve rapidly and can be generated by edgedevices, the algorithm is required to continually learn and adapt to changingenvironments, and adjust to low-compute devices. To this end, we developedthree image coreset algorithms to compress and denoise medical images forselective experience replayed-based lifelong reinforcement learning. Weimplemented neighborhood averaging coreset, neighborhood sensitivity-basedsampling coreset, and maximum entropy coreset on full-body DIXON water andDIXON fat MRI images. All three coresets produced 27x compression withexcellent performance in localizing five anatomical landmarks: left knee, righttrochanter, left kidney, spleen, and lung across both imaging environments.Maximum entropy coreset obtained the best performance of $11.97\pm 12.02$average distance error, compared to the conventional lifelong learningframework's $19.24\pm 50.77$.</description><author>Guangyao Zheng, Shuhao Lai, Vladimir Braverman, Michael A. Jacobs, Vishwa S. Parekh</author><pubDate>Thu, 08 Jun 2023 16:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05310v1</guid></item><item><title>Are fairness metric scores enough to assess discrimination biases in machine learning?</title><link>http://arxiv.org/abs/2306.05307v1</link><description>This paper presents novel experiments shedding light on the shortcomings ofcurrent metrics for assessing biases of gender discrimination made by machinelearning algorithms on textual data. We focus on the Bios dataset, and ourlearning task is to predict the occupation of individuals, based on theirbiography. Such prediction tasks are common in commercial Natural LanguageProcessing (NLP) applications such as automatic job recommendations. We addressan important limitation of theoretical discussions dealing with group-wisefairness metrics: they focus on large datasets, although the norm in manyindustrial NLP applications is to use small to reasonably large linguisticdatasets for which the main practical constraint is to get a good predictionaccuracy. We then question how reliable are different popular measures of biaswhen the size of the training set is simply sufficient to learn reasonablyaccurate predictions. Our experiments sample the Bios dataset and learn morethan 200 models on different sample sizes. This allows us to statisticallystudy our results and to confirm that common gender bias indices providediverging and sometimes unreliable results when applied to relatively smalltraining and test samples. This highlights the crucial importance of variancecalculations for providing sound results in this field.</description><author>Fanny Jourdan, Laurent Risser, Jean-Michel Loubes, Nicholas Asher</author><pubDate>Thu, 08 Jun 2023 16:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05307v1</guid></item><item><title>Deploying clinical machine learning? Consider the following...</title><link>http://arxiv.org/abs/2109.06919v3</link><description>Despite the intense attention and considerable investment into clinicalmachine learning research, relatively few applications have been deployed at alarge-scale in a real-world clinical environment. While research is importantin advancing the state-of-the-art, translation is equally important in bringingthese techniques and technologies into a position to ultimately impacthealthcare. We believe a lack of appreciation for several considerations are amajor cause for this discrepancy between expectation and reality. To bettercharacterize a holistic perspective among researchers and practitioners, wesurvey several practitioners with commercial experience in developing CML forclinical deployment. Using these insights, we identify several main categoriesof challenges in order to better design and develop clinical machine learningapplications.</description><author>Charles Lu, Ken Chang, Praveer Singh, Stuart Pomerantz, Sean Doyle, Sujay Kakarmath, Christopher Bridge, Jayashree Kalpathy-Cramer</author><pubDate>Thu, 08 Jun 2023 16:53:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.06919v3</guid></item><item><title>Evaluating Self-Supervised Learning for Molecular Graph Embeddings</title><link>http://arxiv.org/abs/2206.08005v2</link><description>Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiringembeddings without expert labelling, a capability that carries profoundimplications for molecular graphs due to the staggering number of potentialmolecules and the high cost of obtaining labels. However, GSSL methods aredesigned not for optimisation within a specific domain but rather fortransferability across a variety of downstream tasks. This broad applicabilitycomplicates their evaluation. Addressing this challenge, we present "MolecularGraph Representation Evaluation" (MOLGRAPHEVAL), generating detailed profilesof molecular graph embeddings with interpretable and diversified attributes.MOLGRAPHEVAL offers a suite of probing tasks grouped into three categories: (i)generic graph, (ii) molecular substructure, and (iii) embedding spaceproperties. By leveraging MOLGRAPHEVAL to benchmark existing GSSL methodsagainst both current downstream datasets and our suite of tasks, we uncoversignificant inconsistencies between inferences drawn solely from existingdatasets and those derived from more nuanced probing. These findings suggestthat current evaluation methodologies fail to capture the entirety of thelandscape.</description><author>Hanchen Wang, Jean Kaddour, Shengchao Liu, Jian Tang, Joan Lasenby, Qi Liu</author><pubDate>Thu, 08 Jun 2023 16:52:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08005v2</guid></item><item><title>Bayesian Optimisation of Functions on Graphs</title><link>http://arxiv.org/abs/2306.05304v1</link><description>The increasing availability of graph-structured data motivates the task ofoptimising over functions defined on the node set of graphs. Traditional graphsearch algorithms can be applied in this case, but they may besample-inefficient and do not make use of information about the functionvalues; on the other hand, Bayesian optimisation is a class of promisingblack-box solvers with superior sample efficiency, but it has been scarcelybeen applied to such novel setups. To fill this gap, we propose a novelBayesian optimisation framework that optimises over functions defined ongeneric, large-scale and potentially unknown graphs. Through the learning ofsuitable kernels on graphs, our framework has the advantage of adapting to thebehaviour of the target function. The local modelling approach furtherguarantees the efficiency of our method. Extensive experiments on bothsynthetic and real-world graphs demonstrate the effectiveness of the proposedoptimisation framework.</description><author>Xingchen Wan, Pierre Osselin, Henry Kenlay, Binxin Ru, Michael A. Osborne, Xiaowen Dong</author><pubDate>Thu, 08 Jun 2023 16:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05304v1</guid></item><item><title>Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields</title><link>http://arxiv.org/abs/2306.05303v1</link><description>The quality of three-dimensional reconstruction is a key factor affecting theeffectiveness of its application in areas such as virtual reality (VR) andaugmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generaterealistic images from any viewpoint. It simultaneously reconstructs the shape,lighting, and materials of objects, and without surface defects, which breaksdown the barrier between virtuality and reality. The potential spatialcorrespondences displayed by NeRF between reconstructed scenes and real-worldscenes offer a wide range of practical applications possibilities. Despitesignificant progress in 3D reconstruction since NeRF were introduced, thereremains considerable room for exploration and experimentation. NeRF-basedmodels are susceptible to interference issues caused by colored "fog" noise.Additionally, they frequently encounter instabilities and failures whileattempting to reconstruct unbounded scenes. Moreover, the model takes asignificant amount of time to converge, making it even more challenging to usein such scenarios. Our approach, coined Enhance-NeRF, which adopts joint colorto balance low and high reflectivity objects display, utilizes a decodingarchitecture with prior knowledge to improve recognition, and employsmulti-layer performance evaluation mechanisms to enhance learning capacity. Itachieves reconstruction of outdoor scenes within one hour under single-cardcondition. Based on experimental results, Enhance-NeRF partially enhancesfitness capability and provides some support to outdoor scene reconstruction.The Enhance-NeRF method can be used as a plug-and-play component, making iteasy to integrate with other NeRF-based models. The code is available at:https://github.com/TANQIanQ/Enhance-NeRF</description><author>Qianqiu Tan, Tao Liu, Yinling Xie, Shuwan Yu, Baohua Zhang</author><pubDate>Thu, 08 Jun 2023 16:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05303v1</guid></item><item><title>ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases</title><link>http://arxiv.org/abs/2306.05301v1</link><description>Enabling large language models to effectively utilize real-world tools iscrucial for achieving embodied intelligence. Existing approaches to toollearning have primarily relied on either extremely large language models, suchas GPT-4, to attain generalized tool-use abilities in a zero-shot manner, orhave utilized supervised learning to train limited types of tools on compactmodels. However, it remains uncertain whether smaller language models canachieve generalized tool-use abilities without specific tool-specific training.To address this question, this paper introduces ToolAlpaca, a novel frameworkdesigned to automatically generate a tool-use corpus and learn generalizedtool-use abilities on compact language models with minimal human intervention.Specifically, ToolAlpaca first collects a comprehensive dataset by building amulti-agent simulation environment, which contains 3938 tool-use instances frommore than 400 real-world tool APIs spanning 50 distinct categories.Subsequently, the constructed corpus is employed to fine-tune compact languagemodels, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B,respectively. Finally, we evaluate the ability of these models to utilizepreviously unseen tools without specific training. Experimental resultsdemonstrate that ToolAlpaca achieves effective generalized tool-usecapabilities comparable to those of extremely large language models likeGPT-3.5. This validation supports the notion that learning generalized tool-useabilities is feasible for compact language models.</description><author>Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Le Sun</author><pubDate>Thu, 08 Jun 2023 16:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05301v1</guid></item><item><title>Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances</title><link>http://arxiv.org/abs/2306.05300v1</link><description>Stochastic gradient descent (SGD) has become a cornerstone of neural networkoptimization, yet the noise introduced by SGD is often assumed to beuncorrelated over time, despite the ubiquity of epoch-based training. In thiswork, we challenge this assumption and investigate the effects of epoch-basednoise correlations on the stationary distribution of discrete-time SGD withmomentum, limited to a quadratic loss. Our main contributions are twofold:first, we calculate the exact autocorrelation of the noise for training inepochs under the assumption that the noise is independent of small fluctuationsin the weight vector; second, we explore the influence of correlationsintroduced by the epoch-based learning scheme on SGD dynamics. We find that fordirections with a curvature greater than a hyperparameter-dependent crossovervalue, the results for uncorrelated noise are recovered. However, forrelatively flat directions, the weight variance is significantly reduced. Weprovide an intuitive explanation for these results based on a crossover betweencorrelation times, contributing to a deeper understanding of the dynamics ofSGD in the presence of epoch-based noise correlations.</description><author>Marcel Kühn, Bernd Rosenow</author><pubDate>Thu, 08 Jun 2023 16:45:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05300v1</guid></item><item><title>Habits of Mind: Reusing Action Sequences for Efficient Planning</title><link>http://arxiv.org/abs/2306.05298v1</link><description>When we exercise sequences of actions, their execution becomes more fluentand precise. Here, we consider the possibility that exercised action sequencescan also be used to make planning faster and more accurate by focusingexpansion of the search tree on paths that have been frequently used in thepast, and by reducing deep planning problems to shallow ones via multi-stepjumps in the tree. To capture such sequences, we use a flexible Bayesian actionchunking mechanism which finds and exploits statistically reliable structure atdifferent scales. This gives rise to shorter or longer routines that can beembedded into a Monte-Carlo tree search planner. We show the benefits of thisscheme using a physical construction task patterned after tangrams.</description><author>Noémi Éltető, Peter Dayan</author><pubDate>Thu, 08 Jun 2023 16:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05298v1</guid></item><item><title>One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks</title><link>http://arxiv.org/abs/2210.06379v2</link><description>Current multimodal models, aimed at solving Vision and Language (V+L) tasks,predominantly repurpose Vision Encoders (VE) as feature extractors. While manyVEs -- of different architectures, trained on different data and objectives --are publicly available, they are not designed for the downstream V+L tasks.Nonetheless, most current work assumes that a \textit{single} pre-trained VEcan serve as a general-purpose encoder. In this work, we focus on analysis andaim to understand whether the information stored within different VEs iscomplementary, i.e. if providing the model with features from multiple VEs canimprove the performance on a target task, and how they are combined. Weexhaustively experiment with three popular VEs on six downstream V+L tasks andanalyze the attention and VE-dropout patterns. Our analyses suggest thatdiverse VEs complement each other, resulting in improved downstream V+L taskperformance, where the improvements are not due to simple ensemble effects(i.e. the performance does not always improve when increasing the number ofencoders). We demonstrate that future VEs, which are not \textit{repurposed},but explicitly \textit{designed} for V+L tasks, have the potential of improvingperformance on the target V+L tasks.</description><author>Gregor Geigle, Chen Cecilia Liu, Jonas Pfeiffer, Iryna Gurevych</author><pubDate>Thu, 08 Jun 2023 16:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06379v2</guid></item><item><title>Connectional-Style-Guided Contextual Representation Learning for Brain Disease Diagnosis</title><link>http://arxiv.org/abs/2306.05297v1</link><description>Structural magnetic resonance imaging (sMRI) has shown great clinical valueand has been widely used in deep learning (DL) based computer-aided braindisease diagnosis. Previous approaches focused on local shapes and textures insMRI that may be significant only within a particular domain. The learnedrepresentations are likely to contain spurious information and have a poorgeneralization ability in other diseases and datasets. To facilitate capturingmeaningful and robust features, it is necessary to first comprehensivelyunderstand the intrinsic pattern of the brain that is not restricted within asingle data/task domain. Considering that the brain is a complex connectome ofinterlinked neurons, the connectional properties in the brain have strongbiological significance, which is shared across multiple domains and coversmost pathological information. In this work, we propose a connectional stylecontextual representation learning model (CS-CRL) to capture the intrinsicpattern of the brain, used for multiple brain disease diagnosis. Specifically,it has a vision transformer (ViT) encoder and leverages mask reconstruction asthe proxy task and Gram matrices to guide the representation of connectionalinformation. It facilitates the capture of global context and the aggregationof features with biological plausibility. The results indicate that CS-CRLachieves superior accuracy in multiple brain disease diagnosis tasks across sixdatasets and three diseases and outperforms state-of-the-art models.Furthermore, we demonstrate that CS-CRL captures more brain-network-likeproperties, better aggregates features, is easier to optimize and is morerobust to noise, which explains its superiority in theory. Our source code willbe released soon.</description><author>Gongshu Wang, Ning Jiang, Yunxiao Ma, Tiantian Liu, Duanduan Chen, Jinglong Wu, Guoqi Li, Dong Liang, Tianyi Yan</author><pubDate>Thu, 08 Jun 2023 16:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05297v1</guid></item><item><title>Safe Collaborative Filtering</title><link>http://arxiv.org/abs/2306.05292v1</link><description>Excellent tail performance is crucial for modern machine learning tasks, suchas algorithmic fairness, class imbalance, and risk-sensitive decision making,as it ensures the effective handling of challenging samples within a dataset.Tail performance is also a vital determinant of success for personalisedrecommender systems to reduce the risk of losing users with low satisfaction.This study introduces a "safe" collaborative filtering method that prioritisesrecommendation quality for less-satisfied users rather than focusing on theaverage performance. Our approach minimises the conditional value at risk(CVaR), which represents the average risk over the tails of users' loss. Toovercome computational challenges for web-scale recommender systems, we developa robust yet practical algorithm that extends the most scalable method,implicit alternating least squares (iALS). Empirical evaluation on real-worlddatasets demonstrates the excellent tail performance of our approach whilemaintaining competitive computational efficiency.</description><author>Riku Togashi, Tatsushi Oka, Naoto Ohsaka, Tetsuro Morimura</author><pubDate>Thu, 08 Jun 2023 16:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05292v1</guid></item><item><title>Stochastic noise can be helpful for variational quantum algorithms</title><link>http://arxiv.org/abs/2210.06723v2</link><description>Saddle points constitute a crucial challenge for first-order gradient descentalgorithms. In notions of classical machine learning, they are avoided forexample by means of stochastic gradient descent methods. In this work, weprovide evidence that the saddle points problem can be naturally avoided invariational quantum algorithms by exploiting the presence of stochasticity. Weprove convergence guarantees and present practical examples in numericalsimulations and on quantum hardware. We argue that the natural stochasticity ofvariational algorithms can be beneficial for avoiding strict saddle points,i.e., those saddle points with at least one negative Hessian eigenvalue. Thisinsight that some levels of shot noise could help is expected to add a newperspective to notions of near-term variational quantum algorithms.</description><author>Junyu Liu, Frederik Wilde, Antonio Anna Mele, Liang Jiang, Jens Eisert</author><pubDate>Thu, 08 Jun 2023 16:31:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06723v2</guid></item><item><title>Simple and Controllable Music Generation</title><link>http://arxiv.org/abs/2306.05284v1</link><description>We tackle the task of conditional music generation. We introduce MusicGen, asingle Language Model (LM) that operates over several streams of compresseddiscrete music representation, i.e., tokens. Unlike prior work, MusicGen iscomprised of a single-stage transformer LM together with efficient tokeninterleaving patterns, which eliminates the need for cascading several models,e.g., hierarchically or upsampling. Following this approach, we demonstrate howMusicGen can generate high-quality samples, while being conditioned on textualdescription or melodic features, allowing better controls over the generatedoutput. We conduct extensive empirical evaluation, considering both automaticand human studies, showing the proposed approach is superior to the evaluatedbaselines on a standard text-to-music benchmark. Through ablation studies, weshed light over the importance of each of the components comprising MusicGen.Music samples, code, and models are available athttps://github.com/facebookresearch/audiocraft.</description><author>Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez</author><pubDate>Thu, 08 Jun 2023 16:31:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05284v1</guid></item><item><title>Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training</title><link>http://arxiv.org/abs/2306.05278v1</link><description>We consider the task of few-shot intent detection, which involves training adeep learning model to classify utterances based on their underlying intentsusing only a small amount of labeled data. The current approach to address thisproblem is through continual pre-training, i.e., fine-tuning pre-trainedlanguage models (PLMs) on external resources (e.g., conversational corpora,public intent detection datasets, or natural language understanding datasets)before using them as utterance encoders for training an intent classifier. Inthis paper, we show that continual pre-training may not be essential, since theoverfitting problem of PLMs on this task may not be as serious as expected.Specifically, we find that directly fine-tuning PLMs on only a handful oflabeled examples already yields decent results compared to methods that employcontinual pre-training, and the performance gap diminishes rapidly as thenumber of labeled data increases. To maximize the utilization of the limitedavailable data, we propose a context augmentation method and leveragesequential self-distillation to boost performance. Comprehensive experiments onreal-world benchmarks show that given only two or more labeled samples perclass, direct fine-tuning outperforms many strong baselines that utilizeexternal data sources for continual pre-training. The code can be found athttps://github.com/hdzhang-code/DFTPlus.</description><author>Haode Zhang, Haowen Liang, Liming Zhan, Xiao-Ming Wu, Albert Y. S. Lam</author><pubDate>Thu, 08 Jun 2023 16:26:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05278v1</guid></item><item><title>Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction</title><link>http://arxiv.org/abs/2306.05276v1</link><description>Adverse Event (ADE) extraction is one of the core tasks in digitalpharmacovigilance, especially when applied to informal texts. This task hasbeen addressed by the Natural Language Processing community using largepre-trained language models, such as BERT. Despite the great number ofTransformer-based architectures used in the literature, it is unclear which ofthem has better performances and why. Therefore, in this paper we perform anextensive evaluation and analysis of 19 Transformer-based models for ADEextraction on informal texts. We compare the performance of all the consideredmodels on two datasets with increasing levels of informality (forums posts andtweets). We also combine the purely Transformer-based models with twocommonly-used additional processing layers (CRF and LSTM), and analyze theireffect on the models performance. Furthermore, we use a well-establishedfeature importance technique (SHAP) to correlate the performance of the modelswith a set of features that describe them: model category (AutoEncoding,AutoRegressive, Text-to-Text), pretraining domain, training from scratch, andmodel size in number of parameters. At the end of our analyses, we identify alist of take-home messages that can be derived from the experimental data.</description><author>Simone Scaboro, Beatrice Portellia, Emmanuele Chersoni, Enrico Santus, Giuseppe Serra</author><pubDate>Thu, 08 Jun 2023 16:25:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05276v1</guid></item><item><title>Federated Linear Contextual Bandits with User-level Differential Privacy</title><link>http://arxiv.org/abs/2306.05275v1</link><description>This paper studies federated linear contextual bandits under the notion ofuser-level differential privacy (DP). We first introduce a unified federatedbandits framework that can accommodate various definitions of DP in thesequential decision-making setting. We then formally introduce user-levelcentral DP (CDP) and local DP (LDP) in the federated bandits framework, andinvestigate the fundamental trade-offs between the learning regrets and thecorresponding DP guarantees in a federated linear contextual bandits model. ForCDP, we propose a federated algorithm termed as \robin and show that it isnear-optimal in terms of the number of clients $M$ and the privacy budget$\varepsilon$ by deriving nearly-matching upper and lower regret bounds whenuser-level DP is satisfied. For LDP, we obtain several lower bounds, indicatingthat learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regretblow-up factor at least {$\min\{1/\varepsilon,M\}$ or$\min\{1/\sqrt{\varepsilon},\sqrt{M}\}$} under different conditions.</description><author>Ruiquan Huang, Huanyu Zhang, Luca Melis, Milan Shen, Meisam Hajzinia, Jing Yang</author><pubDate>Thu, 08 Jun 2023 16:21:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05275v1</guid></item><item><title>Out-of-domain GAN inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation</title><link>http://arxiv.org/abs/2212.09262v2</link><description>The fidelity of Generative Adversarial Networks (GAN) inversion is impeded byOut-Of-Domain (OOD) areas (e.g., background, accessories) in the image.Detecting the OOD areas beyond the generation ability of the pre-trained modeland blending these regions with the input image can enhance fidelity. The"invertibility mask" figures out these OOD areas, and existing methods predictthe mask with the reconstruction error. However, the estimated mask is usuallyinaccurate due to the influence of the reconstruction error in the In-Domain(ID) area. In this paper, we propose a novel framework that enhances thefidelity of human face inversion by designing a new module to decompose theinput images to ID and OOD partitions with invertibility masks. Unlike previousworks, our invertibility detector is simultaneously learned with a spatialalignment module. We iteratively align the generated features to the inputgeometry and reduce the reconstruction error in the ID regions. Thus, the OODareas are more distinguishable and can be precisely predicted. Then, we improvethe fidelity of our results by blending the OOD areas from the input image withthe ID GAN inversion results. Our method produces photo-realistic results forreal-world human face image inversion and manipulation. Extensive experimentsdemonstrate our method's superiority over existing methods in the quality ofGAN inversion and attribute manipulation.</description><author>Xin Yang, Xiaogang Xu, Yingcong Chen</author><pubDate>Thu, 08 Jun 2023 16:20:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09262v2</guid></item><item><title>Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models</title><link>http://arxiv.org/abs/2306.05272v1</link><description>The advent of large pre-trained models has brought about a paradigm shift inboth visual representation learning and natural language processing. However,clustering unlabeled images, as a fundamental and classic machine learningproblem, still lacks effective solution, particularly for large-scale datasets.In this paper, we propose a novel image clustering pipeline that leverages thepowerful feature representation of large pre-trained models such as CLIP andcluster images effectively and efficiently at scale. We show that thepre-trained features are significantly more structured by further optimizingthe rate reduction objective. The resulting features may significantly improvethe clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore,by leveraging CLIP's image-text binding, we show how the new clustering methodleads to a simple yet effective self-labeling algorithm that successfully workson unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We willrelease the code in https://github.com/LeslieTrue/CPP.</description><author>Tianzhe Chu, Shengbang Tong, Tianjiao Ding, Xili Dai, Benjamin David Haeffele, Rene Vidal, Yi Ma</author><pubDate>Thu, 08 Jun 2023 16:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05272v1</guid></item><item><title>Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes</title><link>http://arxiv.org/abs/2306.05270v1</link><description>The BioNLP Workshop 2023 initiated the launch of a shared task on ProblemList Summarization (ProbSum) in January 2023. The aim of this shared task is toattract future research efforts in building NLP models for real-worlddiagnostic decision support applications, where a system generating relevantand accurate diagnoses will augment the healthcare providers decision-makingprocess and improve the quality of care for patients. The goal for participantsis to develop models that generated a list of diagnoses and problems usinginput from the daily care notes collected from the hospitalization ofcritically ill patients. Eight teams submitted their final systems to theshared task leaderboard. In this paper, we describe the tasks, datasets,evaluation metrics, and baseline systems. Additionally, the techniques andresults of the evaluation of the different approaches tried by theparticipating teams are summarized.</description><author>Yanjun Gao, Dmitriy Dligach, Timothy Miller, Matthew M. Churpek, Majid Afshar</author><pubDate>Thu, 08 Jun 2023 16:19:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05270v1</guid></item><item><title>Factorized Contrastive Learning: Going Beyond Multi-view Redundancy</title><link>http://arxiv.org/abs/2306.05268v1</link><description>In a wide range of multimodal tasks, contrastive learning has become aparticularly appealing approach since it can successfully learn representationsfrom abundant unlabeled data with only pairing information (e.g., image-captionor video-audio pairs). Underpinning these approaches is the assumption ofmulti-view redundancy - that shared information between modalities is necessaryand sufficient for downstream tasks. However, in many real-world settings,task-relevant information is also contained in modality-unique regions:information that is only present in one modality but still relevant to thetask. How can we learn self-supervised multimodal representations to captureboth shared and unique information relevant to downstream tasks? This paperproposes FactorCL, a new multimodal representation learning method to go beyondmulti-view redundancy. FactorCL is built from three new contributions: (1)factorizing task-relevant information into shared and unique representations,(2) capturing task-relevant information via maximizing MI lower bounds andremoving task-irrelevant information via minimizing MI upper bounds, and (3)multimodal data augmentations to approximate task relevance without labels. Onlarge-scale real-world datasets, FactorCL captures both shared and uniqueinformation and achieves state-of-the-art results on six benchmarks.</description><author>Paul Pu Liang, Zihao Deng, Martin Ma, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov</author><pubDate>Thu, 08 Jun 2023 16:17:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05268v1</guid></item><item><title>A fermion neural network with efficient optimization and quantum applicability</title><link>http://arxiv.org/abs/2211.05793v2</link><description>Classical artificial neural networks have witnessed widespread successes inmachine-learning applications. Here, we propose fermion neural networks (FNNs)whose physical properties, such as local density of states or conditionalconductance, serve as outputs, once the inputs are incorporated as an initiallayer. Comparable to back-propagation, we establish an efficient optimization,which entitles FNNs to competitive performance on challenging machine-learningbenchmarks. FNNs also directly apply to quantum systems, including hard oneswith interactions, and offer in-situ analysis without preprocessing orpresumption. Following machine learning, FNNs precisely determine topologicalphases and emergent charge orders. Their quantum nature also brings variousadvantages: quantum correlation entitles more general network connectivity andinsight into the vanishing gradient problem, quantum entanglement opens upnovel avenues for interpretable machine learning, etc.</description><author>Pei-Lin Zheng, Jia-Bao Wang, Yi Zhang</author><pubDate>Thu, 08 Jun 2023 16:16:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05793v2</guid></item><item><title>On the Hidden Mystery of OCR in Large Multimodal Models</title><link>http://arxiv.org/abs/2305.07895v3</link><description>Large models have recently played a dominant role in natural languageprocessing and multimodal vision-language learning. It remains less exploredabout their efficacy in text-related visual tasks. We conducted a comprehensivestudy of existing publicly available multimodal models, evaluating theirperformance in text recognition (document text, artistic text, handwrittentext, scene text), text-based visual question answering (document text, scenetext, and bilingual text), key information extraction (receipts, documents, andnutrition facts) and handwritten mathematical expression recognition. Ourfindings reveal strengths and weaknesses in these models, which primarily relyon semantic understanding for word recognition and exhibit inferior perceptionof individual character shapes. They also display indifference towards textlength and have limited capabilities in detecting finegrained features inimages. Consequently, these results demonstrate that even the current mostpowerful large multimodal models cannot match domain-specific methods intraditional text tasks and face greater challenges in more complex tasks. Mostimportantly, the baseline results showcased in this study could provide afoundational framework for the conception and assessment of innovativestrategies targeted at enhancing zero-shot multimodal techniques. Evaluationpipeline is available at https://github.com/Yuliang-Liu/MultimodalOCR.</description><author>Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Cheng-lin Liu, Lianwen Jin, Xiang Bai</author><pubDate>Thu, 08 Jun 2023 16:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07895v3</guid></item><item><title>A Lipschitz Bandits Approach for Continuous Hyperparameter Optimization</title><link>http://arxiv.org/abs/2302.01539v3</link><description>One of the most critical problems in machine learning is HyperParameterOptimization (HPO), since choice of hyperparameters has a significant impact onfinal model performance. Although there are many HPO algorithms, they eitherhave no theoretical guarantees or require strong assumptions. To this end, weintroduce BLiE -- a Lipschitz-bandit-based algorithm for HPO that only assumesLipschitz continuity of the objective function. BLiE exploits the landscape ofthe objective function to adaptively search over the hyperparameter space.Theoretically, we show that $(i)$ BLiE finds an $\epsilon$-optimalhyperparameter with $\mathcal{O} \left( \epsilon^{-(d_z + \beta)}\right)$ totalbudgets, where $d_z$ and $\beta$ are problem intrinsic; $(ii)$ BLiE is highlyparallelizable. Empirically, we demonstrate that BLiE outperforms thestate-of-the-art HPO algorithms on benchmark tasks. We also apply BLiE tosearch for noise schedule of diffusion models. Comparison with the defaultschedule shows that BLiE schedule greatly improves the sampling speed.</description><author>Yasong Feng, Weijian Luo, Yimin Huang, Tianyu Wang</author><pubDate>Thu, 08 Jun 2023 16:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01539v3</guid></item><item><title>EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object</title><link>http://arxiv.org/abs/2306.05262v1</link><description>Current robotic hand manipulation narrowly operates with objects inpredictable positions in limited environments. Thus, when the location of thetarget object deviates severely from the expected location, a robot sometimesresponds in an unexpected way, especially when it operates with a human. Forsafe robot operation, we propose the EXit-aware Object Tracker (EXOT) on arobot hand camera that recognizes an object's absence during manipulation. Therobot decides whether to proceed by examining the tracker's bounding box outputcontaining the target object. We adopt an out-of-distribution classifier formore accurate object recognition since trackers can mistrack a background as atarget object. To the best of our knowledge, our method is the first approachof applying an out-of-distribution classification technique to a trackeroutput. We evaluate our method on the first-person video benchmark dataset,TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5erobot. Then we test our tracker on the UR5e robot in real-time with aconveyor-belt sushi task, to examine the tracker's ability to track targetdishes and to determine the exit status. Our tracker shows 38% higherexit-aware performance than a baseline method. The dataset and the code will bereleased at https://github.com/hskAlena/EXOT.</description><author>Hyunseo Kim, Hye Jung Yoon, Minji Kim, Dong-Sig Han, Byoung-Tak Zhang</author><pubDate>Thu, 08 Jun 2023 16:03:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05262v1</guid></item><item><title>Representing and Learning Functions Invariant Under Crystallographic Groups</title><link>http://arxiv.org/abs/2306.05261v1</link><description>Crystallographic groups describe the symmetries of crystals and otherrepetitive structures encountered in nature and the sciences. These groupsinclude the wallpaper and space groups. We derive linear and nonlinearrepresentations of functions that are (1) smooth and (2) invariant under such agroup. The linear representation generalizes the Fourier basis tocrystallographically invariant basis functions. We show that such a basisexists for each crystallographic group, that it is orthonormal in the relevant$L_2$ space, and recover the standard Fourier basis as a special case for pureshift groups. The nonlinear representation embeds the orbit space of the groupinto a finite-dimensional Euclidean space. We show that such an embeddingexists for every crystallographic group, and that it factors functions througha generalization of a manifold called an orbifold. We describe algorithms that,given a standardized description of the group, compute the Fourier basis and anembedding map. As examples, we construct crystallographically invariant neuralnetworks, kernel machines, and Gaussian processes.</description><author>Ryan P. Adams, Peter Orbanz</author><pubDate>Thu, 08 Jun 2023 16:02:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05261v1</guid></item><item><title>Comprehensive evaluation of deep and graph learning on drug-drug interactions prediction</title><link>http://arxiv.org/abs/2306.05257v1</link><description>Recent advances and achievements of artificial intelligence (AI) as well asdeep and graph learning models have established their usefulness in biomedicalapplications, especially in drug-drug interactions (DDIs). DDIs refer to achange in the effect of one drug to the presence of another drug in the humanbody, which plays an essential role in drug discovery and clinical research.DDIs prediction through traditional clinical trials and experiments is anexpensive and time-consuming process. To correctly apply the advanced AI anddeep learning, the developer and user meet various challenges such as theavailability and encoding of data resources, and the design of computationalmethods. This review summarizes chemical structure based, network based, NLPbased and hybrid methods, providing an updated and accessible guide to thebroad researchers and development community with different domain knowledge. Weintroduce widely-used molecular representation and describe the theoreticalframeworks of graph neural network models for representing molecularstructures. We present the advantages and disadvantages of deep and graphlearning methods by performing comparative experiments. We discuss thepotential technical challenges and highlight future directions of deep andgraph learning models for accelerating DDIs prediction.</description><author>Xuan Lin, Lichang Dai, Yafang Zhou, Zu-Guo Yu, Wen Zhang, Jian-Yu Shi, Dong-Sheng Cao, Li Zeng, Haowen Chen, Bosheng Song, Philip S. Yu, Xiangxiang Zeng</author><pubDate>Thu, 08 Jun 2023 15:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05257v1</guid></item><item><title>Unscented Autoencoder</title><link>http://arxiv.org/abs/2306.05256v1</link><description>The Variational Autoencoder (VAE) is a seminal approach in deep generativemodeling with latent variables. Interpreting its reconstruction process as anonlinear transformation of samples from the latent posterior distribution, weapply the Unscented Transform (UT) -- a well-known distribution approximationused in the Unscented Kalman Filter (UKF) from the field of filtering. A finiteset of statistics called sigma points, sampled deterministically, provides amore informative and lower-variance posterior representation than theubiquitous noise-scaling of the reparameterization trick, while ensuringhigher-quality reconstruction. We further boost the performance by replacingthe Kullback-Leibler (KL) divergence with the Wasserstein distribution metricthat allows for a sharper posterior. Inspired by the two components, we derivea novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder(UAE), trained purely with regularization-like terms on the per-sampleposterior. We empirically show competitive performance in Fr\'echet InceptionDistance (FID) scores over closely-related models, in addition to a lowertraining variance than the VAE.</description><author>Faris Janjoš, Lars Rosenbaum, Maxim Dolgov, J. Marius Zöllner</author><pubDate>Thu, 08 Jun 2023 15:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05256v1</guid></item><item><title>Toward more accurate and generalizable brain deformation estimators for traumatic brain injury detection with unsupervised domain adaptation</title><link>http://arxiv.org/abs/2306.05255v1</link><description>Machine learning head models (MLHMs) are developed to estimate braindeformation for early detection of traumatic brain injury (TBI). However, theoverfitting to simulated impacts and the lack of generalizability caused bydistributional shift of different head impact datasets hinders the broadclinical applications of current MLHMs. We propose brain deformation estimatorsthat integrates unsupervised domain adaptation with a deep neural network topredict whole-brain maximum principal strain (MPS) and MPS rate (MPSR). With12,780 simulated head impacts, we performed unsupervised domain adaptation onon-field head impacts from 302 college football (CF) impacts and 457 mixedmartial arts (MMA) impacts using domain regularized component analysis (DRCA)and cycle-GAN-based methods. The new model improved the MPS/MPSR estimationaccuracy, with the DRCA method significantly outperforming other domainadaptation methods in prediction accuracy (p&lt;0.001): MPS RMSE: 0.027 (CF) and0.037 (MMA); MPSR RMSE: 7.159 (CF) and 13.022 (MMA). On another two hold-outtest sets with 195 college football impacts and 260 boxing impacts, the DRCAmodel significantly outperformed the baseline model without domain adaptationin MPS and MPSR estimation accuracy (p&lt;0.001). The DRCA domain adaptationreduces the MPS/MPSR estimation error to be well below TBI thresholds, enablingaccurate brain deformation estimation to detect TBI in future clinicalapplications.</description><author>Xianghao Zhan, Jiawei Sun, Yuzhe Liu, Nicholas J. Cecchi, Enora Le Flao, Olivier Gevaert, Michael M. Zeineh, David B. Camarillo</author><pubDate>Thu, 08 Jun 2023 15:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05255v1</guid></item><item><title>Supplementary Features of BiLSTM for Enhanced Sequence Labeling</title><link>http://arxiv.org/abs/2305.19928v3</link><description>Sequence labeling tasks require the computation of sentence representationsfor each word within a given sentence. With the rise of advanced pretrainedlanguage models; one common approach involves incorporating a BiLSTM layer toenhance the sequence structure information at the output level. Nevertheless,it has been empirically demonstrated (P.-H. Li, 2020) that BiLSTM's potentialfor generating sentence representations for sequence labeling tasks isconstrained, primarily due to the integration of fragments from past and futuresentence representations to form a complete sentence representation. In thisstudy, we observed that the entire sentence representation, found in both thefirst and last cells of BiLSTM, can supplement each cell's sentencerepresentation. Accordingly, we devised a global context mechanism to integrateentire future and past sentence representations into each cell's sentencerepresentation within BiLSTM, leading to a significant improvement in both F1score and accuracy. By embedding the BERT model within BiLSTM as ademonstration, and conducting exhaustive experiments on nine datasets forsequence labeling tasks, including named entity recognition (NER), part ofspeech (POS) tagging and End-to-End Aspect-Based sentiment analysis (E2E-ABSA).We noted significant improvements in F1 scores and accuracy across all examineddatasets.</description><author>Conglei Xu, Kun Shen, Hongguang Sun</author><pubDate>Thu, 08 Jun 2023 15:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19928v3</guid></item><item><title>Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation</title><link>http://arxiv.org/abs/2306.05254v1</link><description>Deep learning-based medical image segmentation models suffer from performancedegradation when deployed to a new healthcare center. To address this issue,unsupervised domain adaptation and multi-source domain generalization methodshave been proposed, which, however, are less favorable for clinical practicedue to the cost of acquiring target-domain data and the privacy concernsassociated with redistributing the data from multiple source domains. In thispaper, we propose a \textbf{C}hannel-level \textbf{C}ontrastive \textbf{S}ingle\textbf{D}omain \textbf{G}eneralization (\textbf{C$^2$SDG}) model for medicalimage segmentation. In C$^2$SDG, the shallower features of each image and itsstyle-augmented counterpart are extracted and used for contrastive training,resulting in the disentangled style representations and structurerepresentations. The segmentation is performed based solely on the structurerepresentations. Our method is novel in the contrastive perspective thatenables channel-wise feature disentanglement using a single source domain. Weevaluated C$^2$SDG against six SDG methods on a multi-domain joint optic cupand optic disc segmentation benchmark. Our results suggest the effectiveness ofeach module in C$^2$SDG and also indicate that C$^2$SDG outperforms thebaseline and all competing methods with a large margin. The code will beavailable at \url{https://github.com/ShishuaiHu/CCSDG}.</description><author>Shishuai Hu, Zehui Liao, Yong Xia</author><pubDate>Thu, 08 Jun 2023 15:49:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05254v1</guid></item><item><title>Window-Based Distribution Shift Detection for Deep Neural Networks</title><link>http://arxiv.org/abs/2210.10897v3</link><description>To deploy and operate deep neural models in production, the quality of theirpredictions, which might be contaminated benignly or manipulated maliciously byinput distributional deviations, must be monitored and assessed. Specifically,we study the case of monitoring the healthy operation of a deep neural network(DNN) receiving a stream of data, with the aim of detecting inputdistributional deviations over which the quality of the network's predictionsis potentially damaged. Using selective prediction principles, we propose adistribution deviation detection method for DNNs. The proposed method isderived from a tight coverage generalization bound computed over a sample ofinstances drawn from the true underlying distribution. Based on this bound, ourdetector continuously monitors the operation of the network out-of-sample overa test window and fires off an alarm whenever a deviation is detected. Ournovel detection method performs on-par or better than the state-of-the-art,while consuming substantially lower computation time (five orders of magnitudereduction) and space complexities. Unlike previous methods, which require atleast linear dependence on the size of the source distribution for eachdetection, rendering them inapplicable to ``Google-Scale'' datasets, ourapproach eliminates this dependence, making it suitable for real-worldapplications.</description><author>Guy Bar-Shalom, Yonatan Geifman, Ran El-Yaniv</author><pubDate>Thu, 08 Jun 2023 15:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10897v3</guid></item><item><title>Classification of Stress via Ambulatory ECG and GSR Data</title><link>http://arxiv.org/abs/2208.04705v2</link><description>In healthcare, detecting stress and enabling individuals to monitor theirmental health and wellbeing is challenging. Advancements in wearable technologynow enable continuous physiological data collection. This data can provideinsights into mental health and behavioural states through psychophysiologicalanalysis. However, automated analysis is required to provide timely results dueto the quantity of data collected. Machine learning has shown efficacy inproviding an automated classification of physiological data for healthapplications in controlled laboratory environments. Ambulatory uncontrolledenvironments, however, provide additional challenges requiring furthermodelling to overcome. This work empirically assesses several approachesutilising machine learning classifiers to detect stress using physiologicaldata recorded in an ambulatory setting with self-reported stress annotations. Asubset of the training portion SMILE dataset enables the evaluation ofapproaches before submission. The optimal stress detection approach achieves90.77% classification accuracy, 91.24 F1-Score, 90.42 Sensitivity and 91.08Specificity, utilising an ExtraTrees classifier and feature imputation methods.Meanwhile, accuracy on the challenge data is much lower at 59.23% (submission#54 from BEaTS-MTU, username ZacDair). The cause of the performance disparityis explored in this work.</description><author>Zachary Dair, Muhammad Muneeb Saad, Urja Pawar, Samantha Dockray, Ruairi O'Reilly</author><pubDate>Thu, 08 Jun 2023 15:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.04705v2</guid></item><item><title>Mesh-MLP: An all-MLP Architecture for Mesh Classification and Semantic Segmentation</title><link>http://arxiv.org/abs/2306.05246v1</link><description>With the rapid development of geometric deep learning techniques, manymesh-based convolutional operators have been proposed to bridge irregular meshstructures and popular backbone networks. In this paper, we show that whileconvolutions are helpful, a simple architecture based exclusively onmulti-layer perceptrons (MLPs) is competent enough to deal with meshclassification and semantic segmentation. Our new network architecture, namedMesh-MLP, takes mesh vertices equipped with the heat kernel signature (HKS) anddihedral angles as the input, replaces the convolution module of a ResNet withMulti-layer Perceptron (MLP), and utilizes layer normalization (LN) to performthe normalization of the layers. The all-MLP architecture operates in anend-to-end fashion and does not include a pooling module. Extensiveexperimental results on the mesh classification/segmentation tasks validate theeffectiveness of the all-MLP architecture.</description><author>Qiujie Dong, Rui Xu, Xiaoran Gong, Zixiong Wang, Shuangmin Chen, Shiqing Xin, Changhe Tu</author><pubDate>Thu, 08 Jun 2023 15:44:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05246v1</guid></item><item><title>Matching Latent Encoding for Audio-Text based Keyword Spotting</title><link>http://arxiv.org/abs/2306.05245v1</link><description>Using audio and text embeddings jointly for Keyword Spotting (KWS) has shownhigh-quality results, but the key challenge of how to semantically align twoembeddings for multi-word keywords of different sequence lengths remainslargely unsolved. In this paper, we propose an audio-text-based end-to-endmodel architecture for flexible keyword spotting (KWS), which builds uponlearned audio and text embeddings. Our architecture uses a novel dynamicprogramming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimallypartition the audio sequence into the same length as the word-based textsequence using the monotonic alignment of spoken content. Our proposed modelconsists of an encoder block to get audio and text embeddings, a projectorblock to project individual embeddings to a common latent space, and anaudio-text aligner containing a novel DSP algorithm, which aligns the audio andtext embeddings to determine if the spoken content is the same as the text.Experimental results show that our DSP is more effective than otherpartitioning schemes, and the proposed architecture outperformed thestate-of-the-art results on the public dataset in terms of Area Under the ROCCurve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively.</description><author>Kumari Nishu, Minsik Cho, Devang Naik</author><pubDate>Thu, 08 Jun 2023 15:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05245v1</guid></item><item><title>Efficient Multi-Task Scene Analysis with RGB-D Transformers</title><link>http://arxiv.org/abs/2306.05242v1</link><description>Scene analysis is essential for enabling autonomous systems, such as mobilerobots, to operate in real-world environments. However, obtaining acomprehensive understanding of the scene requires solving multiple tasks, suchas panoptic segmentation, instance orientation estimation, and sceneclassification. Solving these tasks given limited computing and batterycapabilities on mobile platforms is challenging. To address this challenge, weintroduce an efficient multi-task scene analysis approach, called EMSAFormer,that uses an RGB-D Transformer-based encoder to simultaneously perform theaforementioned tasks. Our approach builds upon the previously publishedEMSANet. However, we show that the dual CNN-based encoder of EMSANet can bereplaced with a single Transformer-based encoder. To achieve this, weinvestigate how information from both RGB and depth data can be effectivelyincorporated in a single encoder. To accelerate inference on robotic hardware,we provide a custom NVIDIA TensorRT extension enabling highly optimization forour EMSAFormer approach. Through extensive experiments on the commonly usedindoor datasets NYUv2, SUNRGB-D, and ScanNet, we show that our approachachieves state-of-the-art performance while still enabling inference with up to39.1 FPS on an NVIDIA Jetson AGX Orin 32 GB.</description><author>Söhnke Benedikt Fischedick, Daniel Seichter, Robin Schmidt, Leonard Rabes, Horst-Michael Gross</author><pubDate>Thu, 08 Jun 2023 15:41:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05242v1</guid></item><item><title>Dealing with Semantic Underspecification in Multimodal NLP</title><link>http://arxiv.org/abs/2306.05240v1</link><description>Intelligent systems that aim at mastering language as humans do must dealwith its semantic underspecification, namely, the possibility for a linguisticsignal to convey only part of the information needed for communication tosucceed. Consider the usages of the pronoun they, which can leave the genderand number of its referent(s) underspecified. Semantic underspecification isnot a bug but a crucial language feature that boosts its storage and processingefficiency. Indeed, human speakers can quickly and effortlessly integratesemantically-underspecified linguistic signals with a wide range ofnon-linguistic information, e.g., the multimodal context, social or culturalconventions, and shared knowledge. Standard NLP models have, in principle, noor limited access to such extra information, while multimodal systems groundinglanguage into other modalities, such as vision, are naturally equipped toaccount for this phenomenon. However, we show that they struggle with it, whichcould negatively affect their performance and lead to harmful consequences whenused for applications. In this position paper, we argue that our communityshould be aware of semantic underspecification if it aims to develop languagetechnology that can successfully interact with human users. We discuss someapplications where mastering it is crucial and outline a few directions towardachieving this goal.</description><author>Sandro Pezzelle</author><pubDate>Thu, 08 Jun 2023 15:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05240v1</guid></item><item><title>Point-Voxel Absorbing Graph Representation Learning for Event Stream based Recognition</title><link>http://arxiv.org/abs/2306.05239v1</link><description>Considering the balance of performance and efficiency, sampled point andvoxel methods are usually employed to down-sample dense events into sparseones. After that, one popular way is to leverage a graph model which treats thesparse points/voxels as nodes and adopts graph neural networks (GNNs) to learnthe representation for event data. Although good performance can be obtained,however, their results are still limited mainly due to two issues. (1) Existingevent GNNs generally adopt the additional max (or mean) pooling layer tosummarize all node embeddings into a single graph-level representation for thewhole event data representation. However, this approach fails to capture theimportance of graph nodes and also fails to be fully aware of the noderepresentations. (2) Existing methods generally employ either a sparse point orvoxel graph representation model which thus lacks consideration of thecomplementary between these two types of representation models. To addressthese issues, in this paper, we propose a novel dual point-voxel absorbinggraph representation learning for event stream data representation. To bespecific, given the input event stream, we first transform it into the sparseevent cloud and voxel grids and build dual absorbing graph models for themrespectively. Then, we design a novel absorbing graph convolutional network(AGCN) for our dual absorbing graph representation and learning. The key aspectof the proposed AGCN is its ability to effectively capture the importance ofnodes and thus be fully aware of node representations in summarizing all noderepresentations through the introduced absorbing nodes. Finally, the eventrepresentations of dual learning branches are concatenated together to extractthe complementary information of two cues. The output is then fed into a linearlayer for event data classification.</description><author>Bo Jiang, Chengguo Yuan, Xiao Wang, Zhimin Bao, Lin Zhu, Bin Luo</author><pubDate>Thu, 08 Jun 2023 15:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05239v1</guid></item><item><title>SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth</title><link>http://arxiv.org/abs/2306.05238v1</link><description>Exploring robust and efficient association methods has always been animportant issue in multiple-object tracking (MOT). Although existing trackingmethods have achieved impressive performance, congestion and frequentocclusions still pose challenging problems in multi-object tracking. We revealthat performing sparse decomposition on dense scenes is a crucial step toenhance the performance of associating occluded targets. To this end, wepropose a pseudo-depth estimation method for obtaining the relative depth oftargets from 2D images. Secondly, we design a depth cascading matching (DCM)algorithm, which can use the obtained depth information to convert a densetarget set into multiple sparse target subsets and perform data association onthese sparse target subsets in order from near to far. By integrating thepseudo-depth method and the DCM strategy into the data association process, wepropose a new tracker, called SparseTrack. SparseTrack provides a newperspective for solving the challenging crowded scene MOT problem. Only usingIoU matching, SparseTrack achieves comparable performance with thestate-of-the-art (SOTA) methods on the MOT17 and MOT20 benchmarks. Code andmodels are publicly available at \url{https://github.com/hustvl/SparseTrack}.</description><author>Zelin Liu, Xinggang Wang, Cheng Wang, Wenyu Liu, Xiang Bai</author><pubDate>Thu, 08 Jun 2023 15:36:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05238v1</guid></item><item><title>Population-Based Evolutionary Gaming for Unsupervised Person Re-identification</title><link>http://arxiv.org/abs/2306.05236v1</link><description>Unsupervised person re-identification has achieved great success through theself-improvement of individual neural networks. However, limited by the lack ofdiversity of discriminant information, a single network has difficulty learningsufficient discrimination ability by itself under unsupervised conditions. Toaddress this limit, we develop a population-based evolutionary gaming (PEG)framework in which a population of diverse neural networks is trainedconcurrently through selection, reproduction, mutation, and population mutuallearning iteratively. Specifically, the selection of networks to preserve ismodeled as a cooperative game and solved by the best-response dynamics, thenthe reproduction and mutation are implemented by cloning and fluctuatinghyper-parameters of networks to learn more diversity, and population mutuallearning improves the discrimination of networks by knowledge distillation fromeach other within the population. In addition, we propose a cross-referencescatter (CRS) to approximately evaluate re-ID models without labeled samplesand adopt it as the criterion of network selection in PEG. CRS measures amodel's performance by indirectly estimating the accuracy of its predictedpseudo-labels according to the cohesion and separation of the feature space.Extensive experiments demonstrate that (1) CRS approximately measures theperformance of models without labeled samples; (2) and PEG produces newstate-of-the-art accuracy for person re-identification, indicating the greatpotential of population-based network cooperative training for unsupervisedlearning.</description><author>Yunpeng Zhai, Peixi Peng, Mengxi Jia, Shiyong Li, Weiqiang Chen, Xuesong Gao, Yonghong Tian</author><pubDate>Thu, 08 Jun 2023 15:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05236v1</guid></item><item><title>Ownership Protection of Generative Adversarial Networks</title><link>http://arxiv.org/abs/2306.05233v1</link><description>Generative adversarial networks (GANs) have shown remarkable success in imagesynthesis, making GAN models themselves commercially valuable to legitimatemodel owners. Therefore, it is critical to technically protect the intellectualproperty of GANs. Prior works need to tamper with the training set or trainingprocess, and they are not robust to emerging model extraction attacks. In thispaper, we propose a new ownership protection method based on the commoncharacteristics of a target model and its stolen models. Our method can bedirectly applicable to all well-trained GANs as it does not require retrainingtarget models. Extensive experimental results show that our new method canachieve the best protection performance, compared to the state-of-the-artmethods. Finally, we demonstrate the effectiveness of our method with respectto the number of generations of model extraction attacks, the number ofgenerated samples, different datasets, as well as adaptive attacks.</description><author>Hailong Hu, Jun Pang</author><pubDate>Thu, 08 Jun 2023 15:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05233v1</guid></item><item><title>EquiMod: An Equivariance Module to Improve Self-Supervised Learning</title><link>http://arxiv.org/abs/2211.01244v2</link><description>Self-supervised visual representation methods are closing the gap withsupervised learning performance. These methods rely on maximizing thesimilarity between embeddings of related synthetic inputs created through dataaugmentations. This can be seen as a task that encourages embeddings to leaveout factors modified by these augmentations, i.e. to be invariant to them.However, this only considers one side of the trade-off in the choice of theaugmentations: they need to strongly modify the images to avoid simple solutionshortcut learning (e.g. using only color histograms), but on the other hand,augmentations-related information may be lacking in the representations forsome downstream tasks (e.g. color is important for birds and flowerclassification). Few recent works proposed to mitigate the problem of usingonly an invariance task by exploring some form of equivariance toaugmentations. This has been performed by learning additional embeddingsspace(s), where some augmentation(s) cause embeddings to differ, yet in anon-controlled way. In this work, we introduce EquiMod a generic equivariancemodule that structures the learned latent space, in the sense that our modulelearns to predict the displacement in the embedding space caused by theaugmentations. We show that applying that module to state-of-the-art invariancemodels, such as SimCLR and BYOL, increases the performances on CIFAR10 andImageNet datasets. Moreover, while our model could collapse to a trivialequivariance, i.e. invariance, we observe that it instead automatically learnsto keep some augmentations-related information beneficial to therepresentations.</description><author>Alexandre Devillers, Mathieu Lefort</author><pubDate>Thu, 08 Jun 2023 15:31:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01244v2</guid></item></channel></rss>