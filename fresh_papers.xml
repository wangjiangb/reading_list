<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 25 Jan 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Algebraic methods for solving recognition problems with non-crossing classes</title><link>http://arxiv.org/abs/2401.13666v1</link><description>In this paper, we propose to consider various models of pattern recognition.At the same time, it is proposed to consider models in the form of twooperators: a recognizing operator and a decision rule. Algebraic operations areintroduced on recognizing operators, and based on the application of theseoperators, a family of recognizing algorithms is created. An upper estimate isconstructed for the model, which guarantees the completeness of the extension.</description><author>Anvar Kabulov, Alimdzhan Babadzhanov, Islambek Saymanov</author><pubDate>Wed, 24 Jan 2024 18:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13666v1</guid></item><item><title>Entrywise Inference for Causal Panel Data: A Simple and Instance-Optimal Approach</title><link>http://arxiv.org/abs/2401.13665v1</link><description>In causal inference with panel data under staggered adoption, the goal is toestimate and derive confidence intervals for potential outcomes and treatmenteffects. We propose a computationally efficient procedure, involving onlysimple matrix algebra and singular value decomposition. We derivenon-asymptotic bounds on the entrywise error, establishing its proximity to asuitably scaled Gaussian variable. Despite its simplicity, our procedure turnsout to be instance-optimal, in that our theoretical scaling matches a localinstance-wise lower bound derived via a Bayesian Cram\'{e}r-Rao argument. Usingour insights, we develop a data-driven procedure for constructing entrywiseconfidence intervals with pre-specified coverage guarantees. Our analysis isbased on a general inferential toolbox for the SVD algorithm applied to thematrix denoising model, which might be of independent interest.</description><author>Yuling Yan, Martin J. Wainwright</author><pubDate>Wed, 24 Jan 2024 18:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13665v1</guid></item><item><title>The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations</title><link>http://arxiv.org/abs/2401.13662v1</link><description>In recent years, various powerful policy gradient algorithms have beenproposed in deep reinforcement learning. While all these algorithms build onthe Policy Gradient Theorem, the specific design choices differ significantlyacross algorithms. We provide a holistic overview of on-policy policy gradientalgorithms to facilitate the understanding of both their theoreticalfoundations and their practical implementations. In this overview, we include adetailed proof of the continuous version of the Policy Gradient Theorem,convergence results and a comprehensive discussion of practical algorithms. Wecompare the most prominent algorithms on continuous control environments andprovide insights on the benefits of regularization. All code is available athttps://github.com/Matt00n/PolicyGradientsJax.</description><author>Matthias Lehmann</author><pubDate>Wed, 24 Jan 2024 18:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13662v1</guid></item><item><title>MambaByte: Token-free Selective State Space Model</title><link>http://arxiv.org/abs/2401.13660v1</link><description>Token-free language models learn directly from raw bytes and remove the biasof subword tokenization. Operating on bytes, however, results in significantlylonger sequences, and standard autoregressive Transformers scale poorly in suchsettings. We experiment with MambaByte, a token-free adaptation of the Mambastate space model, trained autoregressively on byte sequences. Our experimentsindicate the computational efficiency of MambaByte compared to other byte-levelmodels. We also find MambaByte to be competitive with and even outperformstate-of-the-art subword Transformers. Furthermore, owing to linear scaling inlength, MambaByte benefits from fast inference compared to Transformers. Ourfindings establish the viability of MambaByte in enabling token-free languagemodeling.</description><author>Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M Rush</author><pubDate>Wed, 24 Jan 2024 18:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13660v1</guid></item><item><title>Inadequacy of common stochastic neural networks for reliable clinical decision support</title><link>http://arxiv.org/abs/2401.13657v1</link><description>Widespread adoption of AI for medical decision making is still hindered dueto ethical and safety-related concerns. For AI-based decision support systemsin healthcare settings it is paramount to be reliable and trustworthy. Commondeep learning approaches, however, have the tendency towards overconfidenceunder data shift. Such inappropriate extrapolation beyond evidence-basedscenarios may have dire consequences. This highlights the importance ofreliable estimation of local uncertainty and its communication to the end user.While stochastic neural networks have been heralded as a potential solution tothese issues, this study investigates their actual reliability in clinicalapplications. We centered our analysis on the exemplary use case of mortalityprediction for ICU hospitalizations using EHR from MIMIC3 study. Forpredictions on the EHR time series, Encoder-Only Transformer models wereemployed. Stochasticity of model functions was achieved by incorporating commonmethods such as Bayesian neural network layers and model ensembles. Our modelsachieve state of the art performance in terms of discrimination performance(AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortalityprediction benchmark. However, epistemic uncertainty is criticallyunderestimated by the selected stochastic deep learning methods. A heuristicproof for the responsible collapse of the posterior distribution is provided.Our findings reveal the inadequacy of commonly used stochastic deep learningapproaches to reliably recognize OoD samples. In both methods, unsubstantiatedmodel confidence is not prevented due to strongly biased functional posteriors,rendering them inappropriate for reliable clinical decision support. Thishighlights the need for approaches with more strictly enforced or inherentdistance-awareness to known data points, e.g., using kernel-based techniques.</description><author>Adrian Lindenmeyer, Malte Blattmann, Stefan Franke, Thomas Neumuth, Daniel Schneider</author><pubDate>Wed, 24 Jan 2024 18:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13657v1</guid></item><item><title>A fast horizon detector and a new annotated dataset for maritime video processing</title><link>http://arxiv.org/abs/2110.13694v4</link><description>Accurate and fast sea horizon detection is vital for tasks in autonomousnavigation and maritime security, such as video stabilization, target regionreduction, precise tracking, and obstacle avoidance. This paper introduces anovel sea horizon detector from RGB videos, focusing on rapid and effective seanoise suppression while preserving weak horizon edges. Line fitting methods aresubsequently employed on filtered edges for horizon detection. We address thefiltering problem by extracting line segments with a very low edge threshold,ensuring the detection of line segments even in low-contrast horizonconditions. We show that horizon line segments have simple and relevantproperties in RGB images, which we exploit to suppress noisy segments. Then weuse the surviving segments to construct a filtered edge map and infer thehorizon from the filtered edges. We propose a careful incorporation of temporalinformation for horizon inference and experimentally show its effectiveness. Weaddress the computational constraint by providing a vectorized implementationfor efficient CPU execution, and leveraging image downsizing with minimal lossof accuracy on the original size. Moreover, we contribute a public horizon linedataset to enrich existing data resources. Our algorithm's performance isrigorously evaluated against state-of-the-art methods, and its components arevalidated through ablation experiments. Source code and dataset files areavailable at:</description><author>Yassir Zardoua, Boulaala Mohammed, Mhamed El Mrabet, Astito Abdelali</author><pubDate>Wed, 24 Jan 2024 18:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.13694v4</guid></item><item><title>Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors</title><link>http://arxiv.org/abs/2401.13652v1</link><description>In this paper, we present a novel approach for detecting the discontinuityinterfaces of a discontinuous function. This approach leverages Graph-InformedNeural Networks (GINNs) and sparse grids to address discontinuity detectionalso in domains of dimension larger than 3. GINNs, trained to identify troubledpoints on sparse grids, exploit graph structures built on the grids to achieveefficient and accurate discontinuity detection performances. We also introducea recursive algorithm for general sparse grid-based detectors, characterized byconvergence properties and easy applicability. Numerical experiments onfunctions with dimensions n = 2 and n = 4 demonstrate the efficiency and robustgeneralization of GINNs in detecting discontinuity interfaces. Notably, thetrained GINNs offer portability and versatility, allowing integration intovarious algorithms and sharing among users.</description><author>Francesco Della Santa, Sandra Pieraccini</author><pubDate>Wed, 24 Jan 2024 18:44:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13652v1</guid></item><item><title>Tyche: Stochastic In-Context Learning for Medical Image Segmentation</title><link>http://arxiv.org/abs/2401.13650v1</link><description>Existing learning-based solutions to medical image segmentation have twoimportant shortcomings. First, for most new segmentation task, a new model hasto be trained or fine-tuned. This requires extensive resources and machinelearning expertise, and is therefore often infeasible for medical researchersand clinicians. Second, most existing segmentation methods produce a singledeterministic segmentation mask for a given image. In practice however, thereis often considerable uncertainty about what constitutes the correctsegmentation, and different expert annotators will often segment the same imagedifferently. We tackle both of these problems with Tyche, a model that uses acontext set to generate stochastic predictions for previously unseen taskswithout the need to retrain. Tyche differs from other in-context segmentationmethods in two important ways. (1) We introduce a novel convolution blockarchitecture that enables interactions among predictions. (2) We introducein-context test-time augmentation, a new mechanism to provide predictionstochasticity. When combined with appropriate model design and loss functions,Tyche can predict a set of plausible diverse segmentation candidates for new orunseen medical images and segmentation tasks without the need to retrain.</description><author>Marianne Rakic, Hallee E. Wong, Jose Javier Gonzalez Ortiz, Beth Cimini, John Guttag, Adrian V. Dalca</author><pubDate>Wed, 24 Jan 2024 18:35:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13650v1</guid></item><item><title>VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks</title><link>http://arxiv.org/abs/2401.13649v1</link><description>Autonomous agents capable of planning, reasoning, and executing actions onthe web offer a promising avenue for automating computer tasks. However, themajority of existing benchmarks primarily focus on text-based agents,neglecting many natural tasks that require visual information to effectivelysolve. Given that most computer interfaces cater to human perception, visualinformation often augments textual data in ways that text-only models struggleto harness effectively. To bridge this gap, we introduce VisualWebArena, abenchmark designed to assess the performance of multimodal web agents onrealistic \textit{visually grounded tasks}. VisualWebArena comprises of a setof diverse and complex web-based tasks that evaluate various capabilities ofautonomous multimodal agents. To perform on this benchmark, agents need toaccurately process image-text inputs, interpret natural language instructions,and execute actions on websites to accomplish user-defined objectives. Weconduct an extensive evaluation of state-of-the-art LLM-based autonomousagents, including several multimodal models. Through extensive quantitative andqualitative analysis, we identify several limitations of text-only LLM agents,and reveal gaps in the capabilities of state-of-the-art multimodal languageagents. VisualWebArena provides a framework for evaluating multimodalautonomous language agents, and offers insights towards building strongerautonomous agents for the web. Our code, baseline models, and data is publiclyavailable at https://jykoh.com/vwa.</description><author>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried</author><pubDate>Wed, 24 Jan 2024 18:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13649v1</guid></item><item><title>Reward-Free Curricula for Training Robust World Models</title><link>http://arxiv.org/abs/2306.09205v2</link><description>There has been a recent surge of interest in developing generally-capableagents that can adapt to new tasks without additional training in theenvironment. Learning world models from reward-free exploration is a promisingapproach, and enables policies to be trained using imagined experience for newtasks. However, achieving a general agent requires robustness across differentenvironments. In this work, we address the novel problem of generatingcurricula in the reward-free setting to train robust world models. We considerrobustness in terms of minimax regret over all environment instantiations andshow that the minimax regret can be connected to minimising the maximum errorin the world model across environment instances. This result informs ouralgorithm, WAKER: Weighted Acquisition of Knowledge across Environments forRobustness. WAKER selects environments for data collection based on theestimated error of the world model for each environment. Our experimentsdemonstrate that WAKER outperforms several baselines, resulting in improvedrobustness, efficiency, and generalisation.</description><author>Marc Rigter, Minqi Jiang, Ingmar Posner</author><pubDate>Wed, 24 Jan 2024 18:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09205v2</guid></item><item><title>Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization</title><link>http://arxiv.org/abs/2401.12946v2</link><description>We introduce Coverage Axis++, a novel and efficient approach to 3D shapeskeletonization. The current state-of-the-art approaches for this task oftenrely on the watertightness of the input or suffer from substantialcomputational costs, thereby limiting their practicality. To address thischallenge, Coverage Axis++ proposes a heuristic algorithm to select skeletalpoints, offering a high-accuracy approximation of the Medial Axis Transform(MAT) while significantly mitigating computational intensity for various shaperepresentations. We introduce a simple yet effective strategy that considersboth shape coverage and uniformity to derive skeletal points. The selectionprocedure enforces consistency with the shape structure while favoring thedominant medial balls, which thus introduces a compact underlying shaperepresentation in terms of MAT. As a result, Coverage Axis++ allows forskeletonization for various shape representations (e.g., water-tight meshes,triangle soups, point clouds), specification of the number of skeletal points,few hyperparameters, and highly efficient computation with improvedreconstruction accuracy. Extensive experiments across a wide range of 3D shapesvalidate the efficiency and effectiveness of Coverage Axis++. The code will bepublicly available once the paper is published.</description><author>Zimeng Wang, Zhiyang Dou, Rui Xu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Shiqing Xin, Lingjie Liu, Taku Komura, Xiaoming Yuan, Wenping Wang</author><pubDate>Wed, 24 Jan 2024 18:31:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12946v2</guid></item><item><title>Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering</title><link>http://arxiv.org/abs/2309.17249v2</link><description>Prompting and in-context learning (ICL) have become efficient learningparadigms for large language models (LLMs). However, LLMs suffer from promptbrittleness and various bias factors in the prompt, including but not limitedto the formatting, the choice verbalizers, and the ICL examples. To addressthis problem that results in unexpected performance degradation, calibrationmethods have been developed to mitigate the effects of these biases whilerecovering LLM performance. In this work, we first conduct a systematicanalysis of the existing calibration methods, where we both provide a unifiedview and reveal the failure cases. Inspired by these analyses, we propose BatchCalibration (BC), a simple yet intuitive method that controls the contextualbias from the batched input, unifies various prior approaches, and effectivelyaddresses the aforementioned issues. BC is zero-shot, inference-only, andincurs negligible additional costs. In the few-shot setup, we further extend BCto allow it to learn the contextual bias from labeled data. We validate theeffectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstratestate-of-the-art performance over previous calibration baselines across morethan 10 natural language understanding and image classification tasks.</description><author>Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy</author><pubDate>Wed, 24 Jan 2024 18:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17249v2</guid></item><item><title>Fine-tuning and Utilization Methods of Domain-specific LLMs</title><link>http://arxiv.org/abs/2401.02981v2</link><description>Recent releases of pre-trained Large Language Models (LLMs) have gainedconsiderable traction, yet research on fine-tuning and employingdomain-specific LLMs remains scarce. This study investigates approaches forfine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,foundational models, and methods for domain-specific pre-training. Focusing onthe financial sector, it details dataset selection, preprocessing, modelchoice, and considerations crucial for LLM fine-tuning in finance. Addressingthe unique characteristics of financial data, the study explores theconstruction of domain-specific vocabularies and considerations for securityand regulatory compliance. In the practical application of LLM fine-tuning, thestudy outlines the procedure and implementation for generating domain-specificLLMs in finance. Various financial cases, including stock price prediction,sentiment analysis of financial news, automated document processing, research,information extraction, and customer service enhancement, are exemplified. Thestudy explores the potential of LLMs in the financial domain, identifieslimitations, and proposes directions for improvement, contributing valuableinsights for future research. Ultimately, it advances natural languageprocessing technology in business, suggesting proactive LLM utilization infinancial services across industries.</description><author>Cheonsu Jeong</author><pubDate>Wed, 24 Jan 2024 18:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02981v2</guid></item><item><title>CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction</title><link>http://arxiv.org/abs/2310.01403v2</link><description>Open-vocabulary dense prediction tasks including object detection and imagesegmentation have been advanced by the success of Contrastive Language-ImagePre-training (CLIP). CLIP models, particularly those incorporating visiontransformers (ViTs), have exhibited remarkable generalization ability inzero-shot image classification. However, when transferring the vision-languagealignment of CLIP from global image representation to local regionrepresentation for the open-vocabulary dense prediction tasks, CLIP ViTs sufferfrom the domain shift from full images to local image regions. In this paper,we embark on an in-depth analysis of the region-language alignment in CLIPmodels, which is essential for downstream open-vocabulary dense predictiontasks. Subsequently, we propose an approach named CLIPSelf, which adapts theimage-level recognition ability of CLIP ViT to local image regions withoutneeding any region-text pairs. CLIPSelf empowers ViTs to distill itself byaligning a region representation extracted from its dense feature map with theimage-level representation of the corresponding image crop. With the enhancedCLIP ViTs, we achieve new state-of-the-art performance on open-vocabularyobject detection, semantic segmentation, and panoptic segmentation acrossvarious benchmarks. Models and code are released athttps://github.com/wusize/CLIPSelf.</description><author>Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, Chen Change Loy</author><pubDate>Wed, 24 Jan 2024 18:11:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01403v2</guid></item><item><title>How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability</title><link>http://arxiv.org/abs/2401.13641v1</link><description>Large Language Models (LLMs) such as GPT developed by OpenAI, have alreadyshown astonishing results, introducing quick changes in our society. This hasbeen intensified by the release of ChatGPT which allows anyone to interact in asimple conversational way with LLMs, without any experience in the fieldneeded. As a result, ChatGPT has been rapidly applied to many different taskssuch as code- and song-writer, education, virtual assistants, etc., showingimpressive results for tasks for which it was not trained (zero-shot learning). The present study aims to explore the ability of ChatGPT, based on the recentGPT-4 multimodal LLM, for the task of face biometrics. In particular, weanalyze the ability of ChatGPT to perform tasks such as face verification,soft-biometrics estimation, and explainability of the results. ChatGPT could bevery valuable to further increase the explainability and transparency of theautomatic decisions in human scenarios. Experiments are carried out in order toevaluate the performance and robustness of ChatGPT, using popular publicbenchmarks and comparing the results with state-of-the-art methods in thefield. The results achieved in this study show the potential of LLMs such asChatGPT for face biometrics, especially to enhance explainability. Forreproducibility reasons, we release all the code in GitHub.</description><author>Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia</author><pubDate>Wed, 24 Jan 2024 18:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13641v1</guid></item><item><title>Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach</title><link>http://arxiv.org/abs/2401.10451v3</link><description>Solving large-scale capacity expansion problems (CEPs) is central tocost-effective decarbonization of regional-scale energy systems. To ensure theintended outcomes of CEPs, modeling uncertainty due to weather-dependentvariable renewable energy (VRE) supply and energy demand becomes cruciallyimportant. However, the resulting stochastic optimization models are often lesscomputationally tractable than their deterministic counterparts. Here, wepropose a learning-assisted approximate solution method to tractably solvetwo-stage stochastic CEPs. Our method identifies low-cost planning decisions byconstructing and solving a sequence of tractable temporally aggregatedsurrogate problems. We adopt a Bayesian optimization approach to searching thespace of time series aggregation hyperparameters and compute approximatesolutions that minimize costs on a validation set of supply-demand projections.Importantly, we evaluate solved planning outcomes on a held-out set of testprojections. We apply our approach to generation and transmission expansionplanning for a joint power-gas system spanning New England. We show that ourapproach yields an estimated cost savings of up to 3.8% in comparison tobenchmark time series aggregation approaches.</description><author>Aron Brenner, Rahman Khorramfar, Dharik Mallapragada, Saurabh Amin</author><pubDate>Wed, 24 Jan 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10451v3</guid></item><item><title>Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild</title><link>http://arxiv.org/abs/2401.13627v1</link><description>We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking imagerestoration method that harnesses generative prior and the power of modelscaling up. Leveraging multi-modal techniques and advanced generative prior,SUPIR marks a significant advance in intelligent and realistic imagerestoration. As a pivotal catalyst within SUPIR, model scaling dramaticallyenhances its capabilities and demonstrates new potential for image restoration.We collect a dataset comprising 20 million high-resolution, high-quality imagesfor model training, each enriched with descriptive text annotations. SUPIRprovides the capability to restore images guided by textual prompts, broadeningits application scope and potential. Moreover, we introduce negative-qualityprompts to further improve perceptual quality. We also develop arestoration-guided sampling method to suppress the fidelity issue encounteredin generative-based restoration. Experiments demonstrate SUPIR's exceptionalrestoration effects and its novel capacity to manipulate restoration throughtextual prompts.</description><author>Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, Xintao Wang, Jingwen He, Yu Qiao, Chao Dong</author><pubDate>Wed, 24 Jan 2024 17:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13627v1</guid></item><item><title>Differentially Private Distributed Estimation and Learning</title><link>http://arxiv.org/abs/2306.15865v4</link><description>We study distributed estimation and learning problems in a networkedenvironment in which agents exchange information to estimate unknownstatistical properties of random variables from their privately observedsamples. The agents can collectively estimate the unknown quantities byexchanging information about their private observations, but they also faceprivacy risks. Our novel algorithms extend the existing distributed estimationliterature and enable the participating agents to estimate a completesufficient statistic from private signals acquired offline or online over timeand to preserve the privacy of their signals and network neighborhoods. This isachieved through linear aggregation schemes with adjusted randomization schemesthat add noise to the exchanged estimates subject to differential privacy (DP)constraints, both in an offline and online manner. We provide convergence rateanalysis and tight finite-time convergence bounds. We show that the noise thatminimizes the convergence time to the best estimates is the Laplace noise, withparameters corresponding to each agent's sensitivity to their signal andnetwork characteristics. Our algorithms are further amenable to dynamictopologies and balancing privacy and accuracy trade-offs. Finally, tosupplement and validate our theoretical results, we run experiments onreal-world data from the US Power Grid Network and electric consumption datafrom German Households to estimate the average power consumption of powerstations and households under all privacy regimes and show that our methodoutperforms existing first-order privacy-aware distributed optimizationmethods.</description><author>Marios Papachristou, M. Amin Rahimian</author><pubDate>Wed, 24 Jan 2024 17:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15865v4</guid></item><item><title>Can overfitted deep neural networks in adversarial training generalize? -- An approximation viewpoint</title><link>http://arxiv.org/abs/2401.13624v1</link><description>Adversarial training is a widely used method to improve the robustness ofdeep neural networks (DNNs) over adversarial perturbations. However, it isempirically observed that adversarial training on over-parameterized networksoften suffers from the \textit{robust overfitting}: it can achieve almost zeroadversarial training error while the robust generalization performance is notpromising. In this paper, we provide a theoretical understanding of thequestion of whether overfitted DNNs in adversarial training can generalize froman approximation viewpoint. Specifically, our main results are summarized intothree folds: i) For classification, we prove by construction the existence ofinfinitely many adversarial training classifiers on over-parameterized DNNsthat obtain arbitrarily small adversarial training error (overfitting), whereasachieving good robust generalization error under certain conditions concerningthe data quality, well separated, and perturbation level. ii) Linearover-parameterization (meaning that the number of parameters is only slightlylarger than the sample size) is enough to ensure such existence if the targetfunction is smooth enough. iii) For regression, our results demonstrate thatthere also exist infinitely many overfitted DNNs with linearover-parameterization in adversarial training that can achieve almost optimalrates of convergence for the standard generalization error. Overall, ouranalysis points out that robust overfitting can be avoided but the requiredmodel capacity will depend on the smoothness of the target function, while arobust generalization gap is inevitable. We hope our analysis will give abetter understanding of the mathematical foundations of robustness in DNNs froman approximation view.</description><author>Zhongjie Shi, Fanghui Liu, Yuan Cao, Johan A. K. Suykens</author><pubDate>Wed, 24 Jan 2024 17:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13624v1</guid></item><item><title>TrojanPuzzle: Covertly Poisoning Code-Suggestion Models</title><link>http://arxiv.org/abs/2301.02344v2</link><description>With tools like GitHub Copilot, automatic code suggestion is no longer adream in software engineering. These tools, based on large language models, aretypically trained on massive corpora of code mined from unvetted publicsources. As a result, these models are susceptible to data poisoning attackswhere an adversary manipulates the model's training by injecting maliciousdata. Poisoning attacks could be designed to influence the model's suggestionsat run time for chosen contexts, such as inducing the model into suggestinginsecure code payloads. To achieve this, prior attacks explicitly inject theinsecure code payload into the training data, making the poison data detectableby static analysis tools that can remove such malicious data from the trainingset. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE,that can bypass static analysis by planting malicious poison data inout-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE,goes one step further in generating less suspicious poison data by neverexplicitly including certain (suspicious) parts of the payload in the poisondata, while still inducing a model that suggests the entire payload whencompleting code (i.e., outside docstrings). This makes TROJANPUZZLE robustagainst signature-based dataset-cleansing methods that can filter outsuspicious sequences from the training data. Our evaluation against models oftwo sizes demonstrates that both COVERT and TROJANPUZZLE have significantimplications for practitioners when selecting code used to train or tunecode-suggestion models.</description><author>Hojjat Aghakhani, Wei Dai, Andre Manoel, Xavier Fernandes, Anant Kharkar, Christopher Kruegel, Giovanni Vigna, David Evans, Ben Zorn, Robert Sim</author><pubDate>Wed, 24 Jan 2024 17:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.02344v2</guid></item><item><title>DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning</title><link>http://arxiv.org/abs/2401.13621v1</link><description>Contrastive-learning-based methods have dominated sentence representationlearning. These methods regularize the representation space by pulling similarsentence representations closer and pushing away the dissimilar ones and havebeen proven effective in various NLP tasks, e.g., semantic textual similarity(STS) tasks. However, it is challenging for these methods to learn fine-grainedsemantics as they only learn from the inter-sentence perspective, i.e., theirsupervision signal comes from the relationship between data samples. In thiswork, we propose a novel denoising objective that inherits from anotherperspective, i.e., the intra-sentence perspective. By introducing both discreteand continuous noise, we generate noisy sentences and then train our model torestore them to their original form. Our empirical evaluations demonstrate thatthis approach delivers competitive results on both semantic textual similarity(STS) and a wide range of transfer tasks, standing up well in comparison tocontrastive-learning-based methods. Notably, the proposed intra-sentencedenoising objective complements existing inter-sentence contrastivemethodologies and can be integrated with them to further enhance performance.Our code is available at https://github.com/xinghaow99/DenoSent.</description><author>Xinghao Wang, Junliang He, Pengyu Wang, Yunhua Zhou, Tianxiang Sun, Xipeng Qiu</author><pubDate>Wed, 24 Jan 2024 17:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13621v1</guid></item><item><title>How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?</title><link>http://arxiv.org/abs/2309.08565v3</link><description>Customizing machine translation models to comply with desired attributes(e.g., formality or grammatical gender) is a well-studied topic. However, mostcurrent approaches rely on (semi-)supervised data with attribute annotations.This data scarcity bottlenecks democratizing such customization possibilitiesto a wider range of languages, particularly lower-resource ones. This gap isout of sync with recent progress in pretrained massively multilingualtranslation models. In response, we transfer the attribute controllingcapabilities to languages without attribute-annotated data with an NLLB-200model as a foundation. Inspired by techniques from controllable generation, weemploy a gradient-based inference-time controller to steer the pretrainedmodel. The controller transfers well to zero-shot conditions, as it operates onpretrained multilingual representations and is attribute -- rather thanlanguage-specific. With a comprehensive comparison to finetuning-based control,we demonstrate that, despite finetuning's clear dominance in supervisedsettings, the gap to inference-time control closes when moving to zero-shotconditions, especially with new and distant target languages. The latter alsoshows stronger domain robustness. We further show that our inference-timecontrol complements finetuning. A human evaluation on a real low-resourcelanguage, Bengali, confirms our findings. Our code ishttps://github.com/dannigt/attribute-controller-transfer</description><author>Danni Liu, Jan Niehues</author><pubDate>Wed, 24 Jan 2024 17:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08565v3</guid></item><item><title>FLLIC: Functionally Lossless Image Compression</title><link>http://arxiv.org/abs/2401.13616v1</link><description>Recently, DNN models for lossless image coding have surpassed theirtraditional counterparts in compression performance, reducing the bit rate byabout ten percent for natural color images. But even with these advances,mathematically lossless image compression (MLLIC) ratios for natural imagesstill fall short of the bandwidth and cost-effectiveness requirements of mostpractical imaging and vision systems at present and beyond. To break thebottleneck of MLLIC in compression performance, we question the necessity ofMLLIC, as almost all digital sensors inherently introduce acquisition noises,making mathematically lossless compression counterproductive. Therefore, incontrast to MLLIC, we propose a new paradigm of joint denoising and compressioncalled functionally lossless image compression (FLLIC), which performs losslesscompression of optimally denoised images (the optimality may be task-specific).Although not literally lossless with respect to the noisy input, FLLIC aims toachieve the best possible reconstruction of the latent noise-free originalimage. Extensive experiments show that FLLIC achieves state-of-the-artperformance in joint denoising and compression of noisy images and does so at alower computational cost.</description><author>Xi Zhang, Xiaolin Wu</author><pubDate>Wed, 24 Jan 2024 17:44:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13616v1</guid></item><item><title>Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode</title><link>http://arxiv.org/abs/2401.13613v1</link><description>Photo search, the task of retrieving images based on textual queries, haswitnessed significant advancements with the introduction of CLIP (ContrastiveLanguage-Image Pretraining) model. CLIP leverages a vision-language pretraining approach, wherein it learns a shared representation space for imagesand text, enabling cross-modal understanding. This model demonstrates thecapability to understand the semantic relationships between diverse image andtext pairs, allowing for efficient and accurate retrieval of images based onnatural language queries. By training on a large-scale dataset containingimages and their associated textual descriptions, CLIP achieves remarkablegeneralization, providing a powerful tool for tasks such as zero-shot learningand few-shot classification. This abstract summarizes the foundationalprinciples of CLIP and highlights its potential impact on advancing the fieldof photo search, fostering a seamless integration of natural languageunderstanding and computer vision for improved information retrieval inmultimedia applications</description><author>Naresh Kumar Lahajal, Harini S</author><pubDate>Wed, 24 Jan 2024 17:35:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13613v1</guid></item><item><title>Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models</title><link>http://arxiv.org/abs/2401.13611v1</link><description>Neural networks have been successfully used for non-intrusive speechintelligibility prediction. Recently, the use of feature representationssourced from intermediate layers of pre-trained self-supervised andweakly-supervised models has been found to be particularly useful for thistask. This work combines the use of Whisper ASR decoder layer representationsas neural network input features with an exemplar-based, psychologicallymotivated model of human memory to predict human intelligibility ratings forhearing-aid users. Substantial performance improvement over an establishedintrusive HASPI baseline system is found, including on enhancement systems andlisteners unseen in the training data, with a root mean squared error of 25.3compared with the baseline of 28.7.</description><author>Rhiannon Mogridge, George Close, Robert Sutherland, Thomas Hain, Jon Barker, Stefan Goetze, Anton Ragni</author><pubDate>Wed, 24 Jan 2024 17:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13611v1</guid></item><item><title>Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification</title><link>http://arxiv.org/abs/2401.09493v2</link><description>Cloud radiative feedback impacts early tropical cyclone (TC) intensification,but limitations in existing diagnostic frameworks make them unsuitable forstudying asymmetric or transient radiative heating. We propose a linearVariational Encoder-Decoder (VED) to learn the hidden relationship betweenradiation and the surface intensification of realistic simulated TCs. LimitingVED model inputs enables using its uncertainty to identify periods whenradiation has more importance for intensification. A close examination of theextracted 3D radiative structures suggests that longwave radiative forcing frominner core deep convection and shallow clouds both contribute tointensification, with the deep convection having the most impact overall. Wefind that deep convection downwind of the shallow clouds is critical to theintensification of Haiyan. Our work demonstrates that machine learning candiscover thermodynamic-kinematic relationships without relying on axisymmetricor deterministic assumptions, paving the way towards the objective discovery ofprocesses leading to TC intensification in realistic conditions.</description><author>Frederick Iat-Hin Tam, Tom Beucler, James H. Ruppert Jr</author><pubDate>Wed, 24 Jan 2024 17:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09493v2</guid></item><item><title>Stream-based perception for cognitive agents in mobile ecosystems</title><link>http://arxiv.org/abs/2401.13604v1</link><description>Cognitive agent abstractions can help to engineer intelligent systems acrossmobile devices. On smartphones, the data obtained from onboard sensors can givevaluable insights into the user's current situation. Unfortunately, today'scognitive agent frameworks cannot cope well with the challengingcharacteristics of sensor data. Sensor data is located on a low abstractionlevel and the individual data elements are not meaningful when observed inisolation. In contrast, cognitive agents operate on high-level percepts andlack the means to effectively detect complex spatio-temporal patterns insequences of multiple percepts. In this paper, we present a stream-basedperception approach that enables the agents to perceive meaningful situationsin low-level sensor data streams. We present a crowdshipping case study whereautonomous, self-interested agents collaborate to deliver parcels to theirdestinations. We show how situations derived from smartphone sensor data cantrigger and guide auctions, which the agents use to reach agreements.Experiments with real smartphone data demonstrate the benefits of stream-basedagent perception.</description><author>Jeremias Dötterl, Ralf Bruns, Jürgen Dunkel, Sascha Ossowski</author><pubDate>Wed, 24 Jan 2024 17:14:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13604v1</guid></item><item><title>DiConStruct: Causal Concept-based Explanations through Black-Box Distillation</title><link>http://arxiv.org/abs/2401.08534v2</link><description>Model interpretability plays a central role in human-AI decision-makingsystems. Ideally, explanations should be expressed using human-interpretablesemantic concepts. Moreover, the causal relations between these concepts shouldbe captured by the explainer to allow for reasoning about the explanations.Lastly, explanation methods should be efficient and not compromise theperformance of the predictive task. Despite the rapid advances in AIexplainability in recent years, as far as we know to date, no method fulfillsthese three properties. Indeed, mainstream methods for local conceptexplainability do not produce causal explanations and incur a trade-off betweenexplainability and prediction performance. We present DiConStruct, anexplanation method that is both concept-based and causal, with the goal ofcreating more interpretable local explanations in the form of structural causalmodels and concept attributions. Our explainer works as a distillation model toany black-box machine learning model by approximating its predictions whileproducing the respective explanations. Because of this, DiConStruct generatesexplanations efficiently while not impacting the black-box prediction task. Wevalidate our method on an image dataset and a tabular dataset, showing thatDiConStruct approximates the black-box models with higher fidelity than otherconcept explainability baselines, while providing explanations that include thecausal relations between the concepts.</description><author>Ricardo Moreira, Jacopo Bono, Mário Cardoso, Pedro Saleiro, Mário A. T. Figueiredo, Pedro Bizarro</author><pubDate>Wed, 24 Jan 2024 17:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08534v2</guid></item><item><title>Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine</title><link>http://arxiv.org/abs/2401.08396v2</link><description>Recent studies indicate that Generative Pre-trained Transformer 4 with Vision(GPT-4V) outperforms human physicians in medical challenge tasks. However,these evaluations primarily focused on the accuracy of multi-choice questionsalone. Our study extends the current scope by conducting a comprehensiveanalysis of GPT-4V's rationales of image comprehension, recall of medicalknowledge, and step-by-step multimodal reasoning when solving New EnglandJournal of Medicine (NEJM) Image Challenges - an imaging quiz designed to testthe knowledge and diagnostic capabilities of medical professionals. Evaluationresults confirmed that GPT-4V outperforms human physicians regardingmulti-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well incases where physicians incorrectly answer, with over 80% accuracy. However, wediscovered that GPT-4V frequently presents flawed rationales in cases where itmakes the correct final choices (27.3%), most prominent in image comprehension(21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, ourfindings emphasize the necessity for further in-depth evaluations of itsrationales before integrating such models into clinical workflows.</description><author>Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang Xu, Justin M. Cheung, Robert Chen, Ronald M. Summers, Justin F. Rousseau, Peiyun Ni, Marc J Landsman, Sally L. Baxter, Subhi J. Al'Aref, Yijia Li, Michael F. Chiang, Yifan Peng, Zhiyong Lu</author><pubDate>Wed, 24 Jan 2024 17:12:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08396v2</guid></item><item><title>MM-LLMs: Recent Advances in MultiModal Large Language Models</title><link>http://arxiv.org/abs/2401.13601v1</link><description>In the past year, MultiModal Large Language Models (MM-LLMs) have undergonesubstantial advancements, augmenting off-the-shelf LLMs to support MM inputs oroutputs via cost-effective training strategies. The resulting models not onlypreserve the inherent reasoning and decision-making capabilities of LLMs butalso empower a diverse range of MM tasks. In this paper, we provide acomprehensive survey aimed at facilitating further research of MM-LLMs.Specifically, we first outline general design formulations for modelarchitecture and training pipeline. Subsequently, we provide briefintroductions of $26$ existing MM-LLMs, each characterized by its specificformulations. Additionally, we review the performance of MM-LLMs on mainstreambenchmarks and summarize key training recipes to enhance the potency ofMM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrentlymaintaining a real-time tracking website for the latest developments in thefield. We hope that this survey contributes to the ongoing advancement of theMM-LLMs domain.</description><author>Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, Dong Yu</author><pubDate>Wed, 24 Jan 2024 17:10:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13601v1</guid></item><item><title>Deep Latent Force Models: ODE-based Process Convolutions for Bayesian Deep Learning</title><link>http://arxiv.org/abs/2311.14828v2</link><description>Modelling the behaviour of highly nonlinear dynamical systems with robustuncertainty quantification is a challenging task which typically requiresapproaches specifically designed to address the problem at hand. We introduce adomain-agnostic model to address this issue termed the deep latent force model(DLFM), a deep Gaussian process with physics-informed kernels at each layer,derived from ordinary differential equations using the framework of processconvolutions. Two distinct formulations of the DLFM are presented which utiliseweight-space and variational inducing points-based Gaussian processapproximations, both of which are amenable to doubly stochastic variationalinference. We present empirical evidence of the capability of the DLFM tocapture the dynamics present in highly nonlinear real-world multi-output timeseries data. Additionally, we find that the DLFM is capable of achievingcomparable performance to a range of non-physics-informed probabilistic modelson benchmark univariate regression tasks. We also empirically assess thenegative impact of the inducing points framework on the extrapolationcapabilities of LFM-based models.</description><author>Thomas Baldwin-McDonald, Mauricio A. Álvarez</author><pubDate>Wed, 24 Jan 2024 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14828v2</guid></item><item><title>Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction</title><link>http://arxiv.org/abs/2401.13598v1</link><description>Document-level Relation Triplet Extraction (DocRTE) is a fundamental task ininformation systems that aims to simultaneously extract entities with semanticrelations from a document. Existing methods heavily rely on a substantialamount of fully labeled data. However, collecting and annotating data for newlyemerging relations is time-consuming and labor-intensive. Recent advanced LargeLanguage Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-textgeneration capabilities, inspiring us to explore an alternative approach forobtaining auto-labeled documents with new relations. In this paper, we proposea Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework,which generates labeled data by retrieval and denoising knowledge from LLMs,called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guideChatGPT to generate labeled long-text data step by step. To improve the qualityof synthetic data, we propose a denoising strategy based on the consistency ofcross-document knowledge. Leveraging our denoised synthetic data, we proceed tofine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets.We perform experiments for both zero-shot document-level relation and tripletextraction on two public datasets. The experimental results illustrate that ourGenRDK framework outperforms strong baselines.</description><author>Qi Sun, Kun Huang, Xiaocui Yang, Rong Tong, Kun Zhang, Soujanya Poria</author><pubDate>Wed, 24 Jan 2024 17:04:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13598v1</guid></item><item><title>PLATE: A perception-latency aware estimator,</title><link>http://arxiv.org/abs/2401.13596v1</link><description>Target tracking is a popular problem with many potential applications. Therehas been a lot of effort on improving the quality of the detection of targetsusing cameras through different techniques. In general, with highercomputational effort applied, i.e., a longer perception-latency, a betterdetection accuracy is obtained. However, it is not always useful to apply thelongest perception-latency allowed, particularly when the environment doesn'trequire to and when the computational resources are shared between other tasks.In this work, we propose a new Perception-LATency aware Estimator (PLATE),which uses different perception configurations in different moments of time inorder to optimize a certain performance measure. This measure takes intoaccount a perception-latency and accuracy trade-off aiming for a goodcompromise between quality and resource usage. Compared to other heuristicframe-skipping techniques, PLATE comes with a formal complexity and optimalityanalysis. The advantages of PLATE are verified by several experiments includingan evaluation over a standard benchmark with real data and using state of theart deep learning object detection methods for the perception stage.</description><author>Rodrigo Aldana-López, Rosario Aragüés, Carlos Sagüés</author><pubDate>Wed, 24 Jan 2024 17:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13596v1</guid></item><item><title>Graph Guided Question Answer Generation for Procedural Question-Answering</title><link>http://arxiv.org/abs/2401.13594v1</link><description>In this paper, we focus on task-specific question answering (QA). To thisend, we introduce a method for generating exhaustive and high-quality trainingdata, which allows us to train compact (e.g., run on a mobile device),task-specific QA models that are competitive against GPT variants. The keytechnological enabler is a novel mechanism for automatic question-answergeneration from procedural text which can ingest large amounts of textualinstructions and produce exhaustive in-domain QA training data. While currentQA data generation methods can produce well-formed and varied data, theirnon-exhaustive nature is sub-optimal for training a QA model. In contrast, weleverage the highly structured aspect of procedural text and represent eachstep and the overall flow of the procedure as graphs. We then condition ongraph nodes to automatically generate QA pairs in an exhaustive andcontrollable manner. Comprehensive evaluations of our method show that: 1)small models trained with our data achieve excellent performance on the targetQA task, even exceeding that of GPT3 and ChatGPT despite being several ordersof magnitude smaller. 2) semantic coverage is the key indicator for downstreamQA performance. Crucially, while large language models excel at syntacticdiversity, this does not necessarily result in improvements on the end QAmodel. In contrast, the higher semantic coverage provided by our method iscritical for QA performance.</description><author>Hai X. Pham, Isma Hadji, Xinnuo Xu, Ziedune Degutyte, Jay Rainey, Evangelos Kazakos, Afsaneh Fazly, Georgios Tzimiropoulos, Brais Martinez</author><pubDate>Wed, 24 Jan 2024 17:01:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13594v1</guid></item><item><title>Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes</title><link>http://arxiv.org/abs/2401.13588v1</link><description>The field of healthcare has increasingly turned its focus towards LargeLanguage Models (LLMs) due to their remarkable performance. However, theirperformance in actual clinical applications has been underexplored. Traditionalevaluations based on question-answering tasks don't fully capture the nuancedcontexts. This gap highlights the need for more in-depth and practicalassessments of LLMs in real-world healthcare settings. Objective: We sought toevaluate the performance of LLMs in the complex clinical context of adultcritical care medicine using systematic and comprehensible analytic methods,including clinician annotation and adjudication. Methods: We investigated theperformance of three general LLMs in understanding and processing real-worldclinical notes. Concepts from 150 clinical notes were identified by MetaMap andthen labeled by 9 clinicians. Each LLM's proficiency was evaluated byidentifying the temporality and negation of these concepts using differentprompts for an in-depth analysis. Results: GPT-4 showed overall superiorperformance compared to other LLMs. In contrast, both GPT-3.5 andtext-davinci-003 exhibit enhanced performance when the appropriate promptingstrategies are employed. The GPT family models have demonstrated considerableefficiency, evidenced by their cost-effectiveness and time-saving capabilities.Conclusion: A comprehensive qualitative performance evaluation framework forLLMs is developed and operationalized. This framework goes beyond singularperformance aspects. With expert annotations, this methodology not onlyvalidates LLMs' capabilities in processing complex medical data but alsoestablishes a benchmark for future LLM evaluations across specialized domains.</description><author>Darren Liu, Cheng Ding, Delgersuren Bold, Monique Bouvier, Jiaying Lu, Benjamin Shickel, Craig S. Jabaley, Wenhui Zhang, Soojin Park, Michael J. Young, Mark S. Wainwright, Gilles Clermont, Parisa Rashidi, Eric S. Rosenthal, Laurie Dimisko, Ran Xiao, Joo Heung Yoon, Carl Yang, Xiao Hu</author><pubDate>Wed, 24 Jan 2024 16:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13588v1</guid></item><item><title>Prompt Weight Experiments for LLM Instruction Fine-Tuning</title><link>http://arxiv.org/abs/2401.13586v1</link><description>We present a small study analyzing how prompt token classification lossweighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned oninstruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1and LLaMA 2 using multiple instruction datasets. We found that modelsfine-tuned on our short-completion dataset have a negative quadraticrelationship with PLW while models fine-tuned on long-completion datasets wereunaffected by PLW.</description><author>Mathew Huerta-Enochian</author><pubDate>Wed, 24 Jan 2024 16:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13586v1</guid></item><item><title>Towards Efficient and Effective Deep Clustering with Dynamic Grouping and Prototype Aggregation</title><link>http://arxiv.org/abs/2401.13581v1</link><description>Previous contrastive deep clustering methods mostly focus on instance-levelinformation while overlooking the member relationship within groups/clusters,which may significantly undermine their representation learning and clusteringcapability. Recently, some group-contrastive methods have been developed,which, however, typically rely on the samples of the entire dataset to obtainpseudo labels and lack the ability to efficiently update the group assignmentsin a batch-wise manner. To tackle these critical issues, we present a novelend-to-end deep clustering framework with dynamic grouping and prototypeaggregation, termed as DigPro. Specifically, the proposed dynamic groupingextends contrastive learning from instance-level to group-level, which iseffective and efficient for timely updating groups. Meanwhile, we performcontrastive learning on prototypes in a spherical feature space, termed asprototype aggregation, which aims to maximize the inter-cluster distance.Notably, with an expectation-maximization framework, DigPro simultaneouslytakes advantage of compact intra-cluster connections, well-separated clusters,and efficient group updating during the self-supervised training. Extensiveexperiments on six image benchmarks demonstrate the superior performance of ourapproach over the state-of-the-art. Code is available athttps://github.com/Regan-Zhang/DigPro.</description><author>Haixin Zhang, Dong Huang</author><pubDate>Wed, 24 Jan 2024 16:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13581v1</guid></item><item><title>CNN architecture extraction on edge GPU</title><link>http://arxiv.org/abs/2401.13575v1</link><description>Neural networks have become popular due to their versatility andstate-of-the-art results in many applications, such as image classification,natural language processing, speech recognition, forecasting, etc. Theseapplications are also used in resource-constrained environments such asembedded devices. In this work, the susceptibility of neural networkimplementations to reverse engineering is explored on the NVIDIA Jetson Nanomicrocomputer via side-channel analysis. To this end, an architectureextraction attack is presented. In the attack, 15 popular convolutional neuralnetwork architectures (EfficientNets, MobileNets, NasNet, etc.) are implementedon the GPU of Jetson Nano and the electromagnetic radiation of the GPU isanalyzed during the inference operation of the neural networks. The results ofthe analysis show that neural network architectures are easily distinguishableusing deep learning-based side-channel analysis.</description><author>Peter Horvath, Lukasz Chmielewski, Leo Weissbart, Lejla Batina, Yuval Yarom</author><pubDate>Wed, 24 Jan 2024 16:40:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13575v1</guid></item><item><title>Surge Routing: Event-informed Multiagent Reinforcement Learning for Autonomous Rideshare</title><link>http://arxiv.org/abs/2307.02637v2</link><description>Large events such as conferences, concerts and sports games, often causesurges in demand for ride services that are not captured in average demandpatterns, posing unique challenges for routing algorithms. We propose alearning framework for an autonomous fleet of taxis that leverages event datafrom the internet to predict demand surges and generate cooperative routingpolicies. We achieve this through a combination of two major components: (i) ademand prediction framework that uses textual event information in the form ofevents' descriptions and reviews to predict event-driven demand surges overstreet intersections, and (ii) a scalable multiagent reinforcement learningframework that leverages demand predictions and uses one-agent-at-a-timerollout combined with limited sampling certainty equivalence to learnintersection-level routing policies. For our experimental results we considerreal NYC ride share data for the year 2022 and information for more than 2000events across 300 unique venues in Manhattan. We test our approach with a fleetof 100 taxis on a map with 2235 street intersections. Our experimental resultsdemonstrate that our method learns routing policies that reduce wait timeoverhead per serviced request by 25% to 75%, while picking up 1% to 4% morerequests than other model-based RL frameworks and classical methods inoperations research.</description><author>Daniel Garces, Stephanie Gil</author><pubDate>Wed, 24 Jan 2024 16:36:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02637v2</guid></item><item><title>Guided Diffusion for Fast Inverse Design of Density-based Mechanical Metamaterials</title><link>http://arxiv.org/abs/2401.13570v1</link><description>Mechanical metamaterial is a synthetic material that can possessextraordinary physical characteristics, such as abnormal elasticity, stiffness,and stability, by carefully designing its internal structure. To makemetamaterials contain delicate local structures with unique mechanicalproperties, it is a potential method to represent them through high-resolutionvoxels. However, it brings a substantial computational burden. To this end,this paper proposes a fast inverse design method, whose core is an advanceddeep generative AI algorithm, to generate voxel-based mechanical metamaterials.Specifically, we use the self-conditioned diffusion model, capable ofgenerating a microstructure with a resolution of $128^3$ to approach thespecified homogenized tensor matrix in just 3 seconds. Accordingly, this rapidreverse design tool facilitates the exploration of extreme metamaterials, thesequence interpolation in metamaterials, and the generation of diversemicrostructures for multi-scale design. This flexible and adaptive generativetool is of great value in structural engineering or other mechanical systemsand can stimulate more subsequent research.</description><author>Yanyan Yang, Lili Wang, Xiaoya Zhai, Kai Chen, Wenming Wu, Yunkai Zhao, Ligang Liu, Xiao-Ming Fu</author><pubDate>Wed, 24 Jan 2024 16:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13570v1</guid></item><item><title>Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns</title><link>http://arxiv.org/abs/2310.01749v2</link><description>Attention, specifically scaled dot-product attention, has proven effectivefor natural language, but it does not have a mechanism for handlinghierarchical patterns of arbitrary nesting depth, which limits its ability torecognize certain syntactic structures. To address this shortcoming, we proposestack attention: an attention operator that incorporates stacks, inspired bytheir theoretical connections to context-free languages (CFLs). We show thatstack attention is analogous to standard attention, but with a latent model ofsyntax that requires no syntactic supervision. We propose two variants: onerelated to deterministic pushdown automata (PDAs) and one based onnondeterministic PDAs, which allows transformers to recognize arbitrary CFLs.We show that transformers with stack attention are very effective at learningCFLs that standard transformers struggle on, achieving strong results on a CFLwith theoretically maximal parsing difficulty. We also show that stackattention is more effective at natural language modeling under a constrainedparameter budget, and we include results on machine translation.</description><author>Brian DuSell, David Chiang</author><pubDate>Wed, 24 Jan 2024 16:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01749v2</guid></item><item><title>MMD-Regularized Unbalanced Optimal Transport</title><link>http://arxiv.org/abs/2011.05001v9</link><description>We study the unbalanced optimal transport (UOT) problem, where the marginalconstraints are enforced using Maximum Mean Discrepancy (MMD) regularization.Our work is motivated by the observation that the literature on UOT is focusedon regularization based on $\phi$-divergence (e.g., KL divergence). Despite thepopularity of MMD, its role as a regularizer in the context of UOT seems lessunderstood. We begin by deriving a specific dual of MMD-regularized UOT(MMD-UOT), which helps us prove several useful properties. One interestingoutcome of this duality result is that MMD-UOT induces novel metrics, which notonly lift the ground metric like the Wasserstein but are also sample-wiseefficient to estimate like the MMD. Further, for real-world applicationsinvolving non-discrete measures, we present an estimator for the transport planthat is supported only on the given ($m$) samples. Under certain conditions, weprove that the estimation error with this finitely-supported transport plan isalso $\mathcal{O}(1/\sqrt{m})$. As far as we know, such error bounds that arefree from the curse of dimensionality are not known for $\phi$-divergenceregularized UOT. Finally, we discuss how the proposed estimator can be computedefficiently using accelerated gradient descent. Our experiments show thatMMD-UOT consistently outperforms popular baselines, including KL-regularizedUOT and MMD, in diverse machine learning applications. Our codes are publiclyavailable at https://github.com/Piyushi-0/MMD-reg-OT</description><author>Piyushi Manupriya, J. Saketha Nath, Pratik Jawanpuria</author><pubDate>Wed, 24 Jan 2024 16:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.05001v9</guid></item><item><title>The Power of Linear Recurrent Neural Networks</title><link>http://arxiv.org/abs/1802.03308v9</link><description>Recurrent neural networks are a powerful means to cope with time series. Weshow how autoregressive linear, i.e., linearly activated recurrent neuralnetworks (LRNNs) can approximate any time-dependent function f(t). Theapproximation can effectively be learned by simply solving a linear equationsystem; no backpropagation or similar methods are needed. Furthermore, and thisis the main contribution of this article, the size of an LRNN can be reducedsignificantly in one step after inspecting the spectrum of the networktransition matrix, i.e., its eigenvalues, by taking only the most relevantcomponents. Therefore, in contrast to other approaches, we do not only learnnetwork weights but also the network architecture. LRNNs have interestingproperties: They end up in ellipse trajectories in the long run and allow theprediction of further values and compact representations of functions. Wedemonstrate this by several experiments, among them multiple superimposedoscillators (MSO), robotic soccer (RoboCup), and stock price prediction. LRNNsoutperform the previous state-of-the-art for the MSO task with a minimal numberof units.</description><author>Frieder Stolzenburg, Sandra Litz, Olivia Michael, Oliver Obst</author><pubDate>Wed, 24 Jan 2024 16:22:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1802.03308v9</guid></item><item><title>Large Malaysian Language Model Based on Mistral for Enhanced Local Language Understanding</title><link>http://arxiv.org/abs/2401.13565v1</link><description>In this paper, we present significant advancements in the pretraining ofMistral 7B, a large-scale language model, using a dataset of 32.6 GB,equivalent to 1.1 billion tokens. We explore the impact of extending thecontext length, releasing models with context lengths of 4096 and 32768 tokens,and further refining performance with a specialized 16384 context lengthinstruction-tuned model, we called it Malaysian Mistral. Our experiments demonstrate the efficacy of continue pretraining and theinfluence of extended context lengths on Mistral 7B's language understandingcapabilities. Additionally, we release a model specifically tuned with a 16384context length instruction, showcasing its potential for capturing nuancedlanguage intricacies. Furthermore, our research contributes to the benchmarking of MalaysianMistral against prominent language models, including ChatGPT3.5 and Claude 2.We present compelling results indicating Malaysian Mistral's superiorperformance on Tatabahasa (Malay grammar) test set, particularly whenfine-tuned with instructions. All models released athttps://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c</description><author>Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan</author><pubDate>Wed, 24 Jan 2024 16:21:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13565v1</guid></item><item><title>SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation</title><link>http://arxiv.org/abs/2401.13560v1</link><description>The Transformer architecture has shown a remarkable ability in modelingglobal relationships. However, it poses a significant computational challengewhen processing high-dimensional medical images. This hinders its developmentand widespread adoption in this task. Mamba, as a State Space Model (SSM),recently emerged as a notable manner for long-range dependencies in sequentialmodeling, excelling in natural language processing filed with its remarkablememory efficiency and computational speed. Inspired by its success, weintroduce SegMamba, a novel 3D medical image \textbf{Seg}mentation\textbf{Mamba} model, designed to effectively capture long-range dependencieswithin whole volume features at every scale. Our SegMamba, in contrast toTransformer-based methods, excels in whole volume feature modeling from a statespace model standpoint, maintaining superior processing speed, even with volumefeatures at a resolution of {$64\times 64\times 64$}. Comprehensive experimentson the BraTS2023 dataset demonstrate the effectiveness and efficiency of ourSegMamba. The code for SegMamba is available at:https://github.com/ge-xing/SegMamba</description><author>Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, Lei Zhu</author><pubDate>Wed, 24 Jan 2024 16:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13560v1</guid></item><item><title>Task structure and nonlinearity jointly determine learned representational geometry</title><link>http://arxiv.org/abs/2401.13558v1</link><description>The utility of a learned neural representation depends on how well itsgeometry supports performance in downstream tasks. This geometry depends on thestructure of the inputs, the structure of the target outputs, and thearchitecture of the network. By studying the learning dynamics of networks withone hidden layer, we discovered that the network's activation function has anunexpectedly strong impact on the representational geometry: Tanh networks tendto learn representations that reflect the structure of the target outputs,while ReLU networks retain more information about the structure of the rawinputs. This difference is consistently observed across a broad class ofparameterized tasks in which we modulated the degree of alignment between thegeometry of the task inputs and that of the task labels. We analyzed thelearning dynamics in weight space and show how the differences between thenetworks with Tanh and ReLU nonlinearities arise from the asymmetric asymptoticbehavior of ReLU, which leads feature neurons to specialize for differentregions of input space. By contrast, feature neurons in Tanh networks tend toinherit the task label structure. Consequently, when the target outputs are lowdimensional, Tanh networks generate neural representations that are moredisentangled than those obtained with a ReLU nonlinearity. Our findings shedlight on the interplay between input-output geometry, nonlinearity, and learnedrepresentations in neural networks.</description><author>Matteo Alleman, Jack W Lindsey, Stefano Fusi</author><pubDate>Wed, 24 Jan 2024 16:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13558v1</guid></item><item><title>Visual inspection for illicit items in X-ray images using Deep Learning</title><link>http://arxiv.org/abs/2310.03658v2</link><description>Automated detection of contraband items in X-ray images can significantlyincrease public safety, by enhancing the productivity and alleviating themental load of security officers in airports, subways, customs/post offices,etc. The large volume and high throughput of passengers, mailed parcels, etc.,during rush hours practically make it a Big Data problem. Modern computervision algorithms relying on Deep Neural Networks (DNNs) have proven capable ofundertaking this task even under resource-constrained and embedded executionscenarios, e.g., as is the case with fast, single-stage object detectors.However, no comparative experimental assessment of the various relevant DNNcomponents/methods has been performed under a common evaluation protocol, whichmeans that reliable cross-method comparisons are missing. This paper presentsexactly such a comparative assessment, utilizing a public relevant dataset anda well-defined methodology for selecting the specific DNN components/modulesthat are being evaluated. The results indicate the superiority of Transformerdetectors, the obsolete nature of auxiliary neural modules that have beendeveloped in the past few years for security applications and the efficiency ofthe CSP-DarkNet backbone CNN.</description><author>Ioannis Mademlis, Georgios Batsis, Adamantia Anna Rebolledo Chrysochoou, Georgios Th. Papadopoulos</author><pubDate>Wed, 24 Jan 2024 16:13:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03658v2</guid></item><item><title>Benchmarking the Fairness of Image Upsampling Methods</title><link>http://arxiv.org/abs/2401.13555v1</link><description>Recent years have witnessed a rapid development of deep generative models forcreating synthetic media, such as images and videos. While the practicalapplications of these models in everyday tasks are enticing, it is crucial toassess the inherent risks regarding their fairness. In this work, we introducea comprehensive framework for benchmarking the performance and fairness ofconditional generative models. We develop a set ofmetrics$\unicode{x2013}$inspired by their supervised fairnesscounterparts$\unicode{x2013}$to evaluate the models on their fairness anddiversity. Focusing on the specific application of image upsampling, we createa benchmark covering a wide variety of modern upsampling methods. As part ofthe benchmark, we introduce UnfairFace, a subset of FairFace that replicatesthe racial distribution of common large-scale face datasets. Our empiricalstudy highlights the importance of using an unbiased training set and revealsvariations in how the algorithms respond to dataset imbalances. Alarmingly, wefind that none of the considered methods produces statistically fair anddiverse results.</description><author>Mike Laszkiewicz, Imant Daunhawer, Julia E. Vogt, Asja Fischer, Johannes Lederer</author><pubDate>Wed, 24 Jan 2024 16:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13555v1</guid></item><item><title>PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition</title><link>http://arxiv.org/abs/2401.13554v1</link><description>We present the PanAf20K dataset, the largest and most diverse open-accessannotated video dataset of great apes in their natural environment. Itcomprises more than 7 million frames across ~20,000 camera trap videos ofchimpanzees and gorillas collected at 18 field sites in tropical Africa as partof the Pan African Programme: The Cultured Chimpanzee. The footage isaccompanied by a rich set of annotations and benchmarks making it suitable fortraining and testing a variety of challenging and ecologically importantcomputer vision tasks including ape detection and behaviour recognition.Furthering AI analysis of camera trap information is critical given theInternational Union for Conservation of Nature now lists all species in thegreat ape family as either Endangered or Critically Endangered. We hope thedataset can form a solid basis for engagement of the AI community to improveperformance, efficiency, and result interpretation in order to supportassessments of great ape presence, abundance, distribution, and behaviour andthereby aid conservation efforts.</description><author>Otto Brookes, Majid Mirmehdi, Colleen Stephens, Samuel Angedakin, Katherine Corogenes, Dervla Dowd, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, Vera Leinert, Juan Lapuente, Maureen S. McCarthy, Amelia Meier, Mizuki Murai, Emmanuelle Normand, Virginie Vergnes, Erin G. Wessling, Roman M. Wittig, Kevin Langergraber, Nuria Maldonado, Xinyu Yang, Klaus Zuberbuhler, Christophe Boesch, Mimi Arandjelovic, Hjalmar Kuhl, Tilo Burghardt</author><pubDate>Wed, 24 Jan 2024 16:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13554v1</guid></item><item><title>Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection</title><link>http://arxiv.org/abs/2401.13551v1</link><description>Without human annotations, a typical Unsupervised Video Anomaly Detection(UVAD) method needs to train two models that generate pseudo labels for eachother. In previous work, the two models are closely entangled with each other,and it is not known how to upgrade their method without modifying theirtraining framework significantly. Second, previous work usually adopts fixedthresholding to obtain pseudo labels, however the user-specified threshold isnot reliable which inevitably introduces errors into the training process. Toalleviate these two problems, we propose a novel interleaved framework thatalternately trains a One-Class Classification (OCC) model and aWeakly-Supervised (WS) model for UVAD. The OCC or WS models in our method canbe easily replaced with other OCC or WS models, which facilitates our method toupgrade with the most recent developments in both fields. For handling thefixed thresholding problem, we break through the conventional cognitiveboundary and propose a weighted OCC model that can be trained on both normaland abnormal data. We also propose an adaptive mechanism for automaticallyfinding the optimal threshold for the WS model in a loose to strict manner.Experiments demonstrate that the proposed UVAD method outperforms previousapproaches.</description><author>Yongwei Nie, Hao Huang, Chengjiang Long, Qing Zhang, Pradipta Maji, Hongmin Cai</author><pubDate>Wed, 24 Jan 2024 16:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13551v1</guid></item><item><title>Anisotropy Is Inherent to Self-Attention in Transformers</title><link>http://arxiv.org/abs/2401.12143v2</link><description>The representation degeneration problem is a phenomenon that is widelyobserved among self-supervised learning methods based on Transformers. In NLP,it takes the form of anisotropy, a singular property of hidden representationswhich makes them unexpectedly close to each other in terms of angular distance(cosine-similarity). Some recent works tend to show that anisotropy is aconsequence of optimizing the cross-entropy loss on long-tailed distributionsof tokens. We show in this paper that anisotropy can also be observedempirically in language models with specific objectives that should not sufferdirectly from the same consequences. We also show that the anisotropy problemextends to Transformers trained on other modalities. Our observations suggestthat anisotropy is actually inherent to Transformers-based models.</description><author>Nathan Godey, Éric de la Clergerie, Benoît Sagot</author><pubDate>Wed, 24 Jan 2024 16:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12143v2</guid></item><item><title>Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?</title><link>http://arxiv.org/abs/2401.13544v1</link><description>Recently, interpretable machine learning has re-explored concept bottleneckmodels (CBM), comprising step-by-step prediction of the high-level conceptsfrom the raw features and the target variable from the predicted concepts. Acompelling advantage of this model class is the user's ability to intervene onthe predicted concept values, affecting the model's downstream output. In thiswork, we introduce a method to perform such concept-based interventions onalready-trained neural networks, which are not interpretable by design, givenan annotated validation set. Furthermore, we formalise the model'sintervenability as a measure of the effectiveness of concept-basedinterventions and leverage this definition to fine-tune black-box models.Empirically, we explore the intervenability of black-box classifiers onsynthetic tabular and natural image benchmarks. We demonstrate that fine-tuningimproves intervention effectiveness and often yields better-calibratedpredictions. To showcase the practical utility of the proposed techniques, weapply them to deep chest X-ray classifiers and show that fine-tuned black boxescan be as intervenable and more performant than CBMs.</description><author>Ričards Marcinkevičs, Sonia Laguna, Moritz Vandenhirtz, Julia E. Vogt</author><pubDate>Wed, 24 Jan 2024 16:02:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13544v1</guid></item><item><title>UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields</title><link>http://arxiv.org/abs/2311.05836v5</link><description>In the field of clinical medicine, computed tomography (CT) is an effectivemedical imaging modality for the diagnosis of various pathologies. Comparedwith X-ray images, CT images can provide more information, includingmulti-planar slices and three-dimensional structures for clinical diagnosis.However, CT imaging requires patients to be exposed to large doses of ionizingradiation for a long time, which may cause irreversible physical harm. In thispaper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based ongenerated radiation fields. The network can learn a continuous representationof CT projections from 2D X-ray images by obtaining the internal structure anddepth information and using adaptive loss weights to ensure the quality of thegenerated images. Our model is trained on publicly available knee and chestdatasets, and we show the results of CT projection rendering with a singleX-ray and compare our method with other methods based on generated radiationfields.</description><author>Jing Hu, Qinrui Fan, Shu Hu, Siwei Lyu, Xi Wu, Xin Wang</author><pubDate>Wed, 24 Jan 2024 15:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05836v5</guid></item><item><title>Boosting Multi-view Stereo with Late Cost Aggregation</title><link>http://arxiv.org/abs/2401.11751v2</link><description>Pairwise matching cost aggregation is a crucial step for modernlearning-based Multi-view Stereo (MVS). Prior works adopt an early aggregationscheme, which adds up pairwise costs into an intermediate cost. However, weanalyze that this process can degrade informative pairwise matchings, therebyblocking the depth network from fully utilizing the original geometric matchingcues. To address this challenge, we present a late aggregation approach thatallows for aggregating pairwise costs throughout the network feed-forwardprocess, achieving accurate estimations with only minor changes of the plainCasMVSNet. Instead of building an intermediate cost by weighted sum, lateaggregation preserves all pairwise costs along a distinct view channel. Thisenables the succeeding depth network to fully utilize the crucial geometriccues without loss of cost fidelity. Grounded in the new aggregation scheme, wepropose further techniques addressing view order dependence inside thepreserved cost, handling flexible testing views, and improving the depthfiltering process. Despite its technical simplicity, our method improvessignificantly upon the baseline cascade-based approach, achieving comparableresults with state-of-the-art methods with favorable computation overhead.</description><author>Jiang Wu, Rui Li, Yu Zhu, Wenxun Zhao, Jinqiu Sun, Yanning Zhang</author><pubDate>Wed, 24 Jan 2024 15:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11751v2</guid></item><item><title>Assessing Electricity Service Unfairness with Transfer Counterfactual Learning</title><link>http://arxiv.org/abs/2310.03258v2</link><description>Energy justice is a growing area of interest in interdisciplinary energyresearch. However, identifying systematic biases in the energy sector remainschallenging due to confounding variables, intricate heterogeneity incounterfactual effects, and limited data availability. First, this paperdemonstrates how one can evaluate counterfactual unfairness in a power systemby analyzing the average causal effect of a specific protected attribute.Subsequently, we use subgroup analysis to handle model heterogeneity andintroduce a novel method for estimating counterfactual unfairness based ontransfer learning, which helps to alleviate the data scarcity in each subgroup.In our numerical analysis, we apply our method to a unique large-scalecustomer-level power outage data set and investigate the counterfactual effectof demographic factors, such as income and age of the population, on poweroutage durations. Our results indicate that low-income and elderly-populatedareas consistently experience longer power outages under both daily andpost-disaster operations, and such discrimination is exacerbated under severeconditions. These findings suggest a widespread, systematic issue of injusticein the power service systems and emphasize the necessity for focusedinterventions in disadvantaged communities.</description><author>Song Wei, Xiangrui Kong, Alinson Santos Xavier, Shixiang Zhu, Yao Xie, Feng Qiu</author><pubDate>Wed, 24 Jan 2024 15:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03258v2</guid></item><item><title>Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models</title><link>http://arxiv.org/abs/2401.13537v1</link><description>We propose \textit{masked particle modeling} (MPM) as a self-supervisedmethod for learning generic, transferable, and reusable representations onunordered sets of inputs for use in high energy physics (HEP) scientific data.This work provides a novel scheme to perform masked modeling based pre-trainingto learn permutation invariant functions on sets. More generally, this workprovides a step towards building large foundation models for HEP that can begenerically pre-trained with self-supervised learning and later fine-tuned fora variety of down-stream tasks. In MPM, particles in a set are masked and thetraining objective is to recover their identity, as defined by a discretizedtoken representation of a pre-trained vector quantized variational autoencoder.We study the efficacy of the method in samples of high energy jets at colliderphysics experiments, including studies on the impact of discretization,permutation invariance, and ordering. We also study the fine-tuning capabilityof the model, showing that it can be adapted to tasks such as supervised andweakly supervised jet classification, and that the model can transferefficiently with small fine-tuning data sets to new classes and new datadomains.</description><author>Lukas Heinrich, Michael Kagan, Samuel Klein, Matthew Leigh, Tobias Golling, John Andrew Raine, Margarita Osadchy</author><pubDate>Wed, 24 Jan 2024 15:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13537v1</guid></item><item><title>Finetuning Foundation Models for Joint Analysis Optimization</title><link>http://arxiv.org/abs/2401.13536v1</link><description>In this work we demonstrate that significant gains in performance and dataefficiency can be achieved in High Energy Physics (HEP) by moving beyond thestandard paradigm of sequential optimization or reconstruction and analysiscomponents. We conceptually connect HEP reconstruction and analysis to modernmachine learning workflows such as pretraining, finetuning, domain adaptationand high-dimensional embedding spaces and quantify the gains in the exampleusecase of searches of heavy resonances decaying via an intermediate di-Higgssystem to four $b$-jets.</description><author>Matthias Vig, Nicole Hartman, Lukas Heinrich</author><pubDate>Wed, 24 Jan 2024 15:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13536v1</guid></item><item><title>QAGait: Revisit Gait Recognition from a Quality Perspective</title><link>http://arxiv.org/abs/2401.13531v1</link><description>Gait recognition is a promising biometric method that aims to identifypedestrians from their unique walking patterns. Silhouette modality, renownedfor its easy acquisition, simple structure, sparse representation, andconvenient modeling, has been widely employed in controlled in-the-labresearch. However, as gait recognition rapidly advances from in-the-lab toin-the-wild scenarios, various conditions raise significant challenges forsilhouette modality, including 1) unidentifiable low-quality silhouettes(abnormal segmentation, severe occlusion, or even non-human shape), and 2)identifiable but challenging silhouettes (background noise, non-standardposture, slight occlusion). To address these challenges, we revisit gaitrecognition pipeline and approach gait recognition from a quality perspective,namely QAGait. Specifically, we propose a series of cost-effective qualityassessment strategies, including Maxmial Connect Area and Template Match toeliminate background noises and unidentifiable silhouettes, Alignment strategyto handle non-standard postures. We also propose two quality-aware lossfunctions to integrate silhouette quality into optimization within theembedding space. Extensive experiments demonstrate our QAGait can guaranteeboth gait reliability and performance enhancement. Furthermore, our qualityassessment strategies can seamlessly integrate with existing gait datasets,showcasing our superiority. Code is available athttps://github.com/wzb-bupt/QAGait.</description><author>Zengbin Wang, Saihui Hou, Man Zhang, Xu Liu, Chunshui Cao, Yongzhen Huang, Peipei Li, Shibiao Xu</author><pubDate>Wed, 24 Jan 2024 15:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13531v1</guid></item><item><title>Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks</title><link>http://arxiv.org/abs/2309.07937v3</link><description>We propose a decoder-only language model, VoxtLM, that can perform fourtasks: speech recognition, speech synthesis, text generation, and speechcontinuation. VoxtLM integrates text vocabulary with discrete speech tokensfrom self-supervised speech features and uses special tokens to enablemultitask learning. Compared to a single-task model, VoxtLM exhibits asignificant improvement in speech synthesis, with improvements in both speechintelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90.VoxtLM also improves speech generation and speech recognition performance overthe single-task counterpart. Further, VoxtLM is trained with publicly availabledata and training recipes and model checkpoints are open-sourced to make fullyreproducible work.</description><author>Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe</author><pubDate>Wed, 24 Jan 2024 15:36:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07937v3</guid></item><item><title>Towards Understanding the Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space</title><link>http://arxiv.org/abs/2401.13530v1</link><description>Recently, optimization on the Riemannian manifold has provided new insightsto the optimization community. In this regard, the manifold taken as theprobability measure metric space equipped with the second-order Wassersteindistance is of particular interest, since optimization on it can be linked topractical sampling processes. In general, the oracle (continuous) optimizationmethod on Wasserstein space is Riemannian gradient flow (i.e., Langevindynamics when minimizing KL divergence). In this paper, we aim to enrich thecontinuous optimization methods in the Wasserstein space by extending thegradient flow into the stochastic gradient descent (SGD) flow and stochasticvariance reduction gradient (SVRG) flow. The two flows on Euclidean space arestandard stochastic optimization methods, while their Riemannian counterpartsare not explored yet. By leveraging the structures in Wasserstein space, weconstruct a stochastic differential equation (SDE) to approximate the discretedynamics of desired stochastic methods in the corresponded random vector space.Then, the flows of probability measures are naturally obtained by applyingFokker-Planck equation to such SDE. Furthermore, the convergence rates of theproposed Riemannian stochastic flows are proven, and they match the results inEuclidean space.</description><author>Mingyang Yi, Bohan Wang</author><pubDate>Wed, 24 Jan 2024 15:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13530v1</guid></item><item><title>Linear Log-Normal Attention with Unbiased Concentration</title><link>http://arxiv.org/abs/2311.13541v2</link><description>Transformer models have achieved remarkable results in a wide range ofapplications. However, their scalability is hampered by the quadratic time andmemory complexity of the self-attention mechanism concerning the sequencelength. This limitation poses a substantial obstacle when dealing with longdocuments or high-resolution images. In this work, we study the self-attentionmechanism by analyzing the distribution of the attention matrix and itsconcentration ability. Furthermore, we propose instruments to measure thesequantities and introduce a novel self-attention mechanism, Linear Log-NormalAttention, designed to emulate the distribution and concentration behavior ofthe original self-attention. Our experimental results on popular naturallanguage benchmarks reveal that our proposed Linear Log-Normal Attentionoutperforms other linearized attention alternatives, offering a promisingavenue for enhancing the scalability of transformer models. Our code isavailable in supplementary materials.</description><author>Yury Nahshan, Joseph Kampeas, Emir Haleva</author><pubDate>Wed, 24 Jan 2024 15:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13541v2</guid></item><item><title>OpenDPD: An Open-Source End-to-End Learning &amp; Benchmarking Framework for Wideband Power Amplifier Modeling and Digital Pre-Distortion</title><link>http://arxiv.org/abs/2401.08318v2</link><description>With the rise in communication capacity, deep neural networks (DNN) fordigital pre-distortion (DPD) to correct non-linearity in wideband poweramplifiers (PAs) have become prominent. Yet, there is a void in open-source andmeasurement-setup-independent platforms for fast DPD exploration and objectiveDPD model comparison. This paper presents an open-source framework, OpenDPD,crafted in PyTorch, with an associated dataset for PA modeling and DPDlearning. We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via anovel end-to-end learning architecture, outperforming previous DPD models on adigital PA (DPA) in the new digital transmitter (DTX) architecture withunconventional transfer characteristics compared to analog PAs. Measurementsshow our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dBfor 200 MHz OFDM signals. OpenDPD code, datasets, and documentation arepublicly available at https://github.com/lab-emi/OpenDPD.</description><author>Yizhuo Wu, Gagan Deep Singh, Mohammadreza Beikmirza, Leo C. N. de Vreede, Morteza Alavi, Chang Gao</author><pubDate>Wed, 24 Jan 2024 15:30:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08318v2</guid></item><item><title>SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation</title><link>http://arxiv.org/abs/2401.13527v1</link><description>Benefiting from effective speech modeling, current Speech Large LanguageModels (SLLMs) have demonstrated exceptional capabilities in in-context speechgeneration and efficient generalization to unseen speakers. However, theprevailing information modeling process is encumbered by certain redundancies,leading to inefficiencies in speech generation. We propose Chain-of-InformationGeneration (CoIG), a method for decoupling semantic and perceptual informationin large-scale speech generation. Building on this, we develop SpeechGPT-Gen,an 8-billion-parameter SLLM efficient in semantic and perceptual informationmodeling. It comprises an autoregressive model based on LLM for semanticinformation modeling and a non-autoregressive model employing flow matching forperceptual information modeling. Additionally, we introduce the novel approachof infusing semantic information into the prior distribution to enhance theefficiency of flow matching. Extensive experimental results demonstrate thatSpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voiceconversion, and speech-to-speech dialogue, underscoring CoIG's remarkableproficiency in capturing and modeling speech's semantic and perceptualdimensions. Code and models are available athttps://github.com/0nutation/SpeechGPT.</description><author>Dong Zhang, Xin Zhang, Jun Zhan, Shimin Li, Yaqian Zhou, Xipeng Qiu</author><pubDate>Wed, 24 Jan 2024 15:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13527v1</guid></item><item><title>Relative Policy-Transition Optimization for Fast Policy Transfer</title><link>http://arxiv.org/abs/2206.06009v3</link><description>We consider the problem of policy transfer between two Markov DecisionProcesses (MDPs). We introduce a lemma based on existing theoretical results inreinforcement learning to measure the relativity gap between two arbitraryMDPs, that is the difference between any two cumulative expected returnsdefined on different policies and environment dynamics. Based on this lemma, wepropose two new algorithms referred to as Relative Policy Optimization (RPO)and Relative Transition Optimization (RTO), which offer fast policy transferand dynamics modelling, respectively. RPO transfers the policy evaluated in oneenvironment to maximize the return in another, while RTO updates theparameterized dynamics model to reduce the gap between the dynamics of the twoenvironments. Integrating the two algorithms results in the complete RelativePolicy-Transition Optimization (RPTO) algorithm, in which the policy interactswith the two environments simultaneously, such that data collections from twoenvironments, policy and transition updates are completed in one closed loop toform a principled learning framework for policy transfer. We demonstrate theeffectiveness of RPTO on a set of MuJoCo continuous control tasks by creatingpolicy transfer problems via variant dynamics.</description><author>Jiawei Xu, Cheng Zhou, Yizheng Zhang, Baoxiang Wang, Lei Han</author><pubDate>Wed, 24 Jan 2024 15:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.06009v3</guid></item><item><title>Fast Algorithm for Constrained Linear Inverse Problems</title><link>http://arxiv.org/abs/2212.01068v6</link><description>We consider the constrained Linear Inverse Problem (LIP), where a certainatomic norm (like the $\ell_1 $ norm) is minimized subject to a quadraticconstraint. Typically, such cost functions are non-differentiable which makesthem not amenable to the fast optimization methods existing in practice. Wepropose two equivalent reformulations of the constrained LIP with improvedconvex regularity: (i) a smooth convex minimization problem, and (ii) astrongly convex min-max problem. These problems could be solved by applyingexisting acceleration-based convex optimization methods which provide better $O \left( \frac{1}{k^2} \right) $ theoretical convergence guarantee, improvingupon the current best rate of $ O \left( \frac{1}{k} \right) $. We also providea novel algorithm named the Fast Linear Inverse Problem Solver (FLIPS), whichis tailored to maximally exploit the structure of the reformulations. Wedemonstrate the performance of FLIPS on the classical problems of BinarySelection, Compressed Sensing, and Image Denoising. We also provide open source\texttt{MATLAB} package for these three examples, which can be easily adaptedto other LIPs.</description><author>Mohammed Rayyan Sheriff, Floor Fenne Redel, Peyman Mohajerin Esfahani</author><pubDate>Wed, 24 Jan 2024 15:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01068v6</guid></item><item><title>Delocate: Detection and Localization for Deepfake Videos with Randomly-Located Tampered Traces</title><link>http://arxiv.org/abs/2401.13516v1</link><description>Deepfake videos are becoming increasingly realistic, showing subtle tamperingtraces on facial areasthat vary between frames. Consequently, many existingDeepfake detection methods struggle to detect unknown domain Deepfake videoswhile accurately locating the tampered region. To address thislimitation, wepropose Delocate, a novel Deepfake detection model that can both recognizeandlocalize unknown domain Deepfake videos. Ourmethod consists of two stagesnamed recoveringand localization. In the recovering stage, the modelrandomlymasks regions of interest (ROIs) and reconstructs real faces without tamperingtraces, resulting in a relatively good recovery effect for realfaces and a poorrecovery effect for fake faces. Inthe localization stage, the output of therecoveryphase and the forgery ground truth mask serve assupervision to guidethe forgery localization process. This process strategically emphasizes therecovery phase of fake faces with poor recovery, facilitating the localizationof tampered regions. Ourextensive experiments on four widely used benchmarkdatasets demonstrate that Delocate not onlyexcels in localizing tampered areasbut also enhances cross-domain detection performance.</description><author>Juan Hu, Xin Liao, Difei Gao, Satoshi Tsutsui, Qian Wang, Zheng Qin, Mike Zheng Shou</author><pubDate>Wed, 24 Jan 2024 15:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13516v1</guid></item><item><title>Can GPT-3.5 Generate and Code Discharge Summaries?</title><link>http://arxiv.org/abs/2401.13512v1</link><description>Objective: To investigate GPT-3.5 in generating and coding medical documentswith ICD-10 codes for data augmentation on low-resources labels. Materials and Methods: Employing GPT-3.5 we generated and coded 9,606discharge summaries based on lists of ICD-10 code descriptions of patients withinfrequent (generation) codes within the MIMIC-IV dataset. Combined with thebaseline training set, this formed an augmented training set. Neural codingmodels were trained on baseline and augmented data and evaluated on a MIMIC-IVtest set. We report micro- and macro-F1 scores on the full codeset, generationcodes, and their families. Weak Hierarchical Confusion Matrices were employedto determine within-family and outside-of-family coding errors in the lattercodesets. The coding performance of GPT-3.5 was evaluated both on prompt-guidedself-generated data and real MIMIC-IV data. Clinical professionals evaluatedthe clinical acceptability of the generated documents. Results: Augmentation slightly hinders the overall performance of the modelsbut improves performance for the generation candidate codes and their families,including one unseen in the baseline training data. Augmented models displaylower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by theprompted descriptions, but performs poorly on real data. Evaluators note thecorrectness of generated concepts while suffering in variety, supportinginformation, and narrative. Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding.Augmentation positively affects generation code families but mainly benefitscodes with existing examples. Augmentation reduces out-of-family errors.Discharge summaries generated by GPT-3.5 state prompted concepts correctly butlack variety, and authenticity in narratives. They are unsuitable for clinicalpractice.</description><author>Matúš Falis, Aryo Pradipta Gema, Hang Dong, Luke Daines, Siddharth Basetti, Michael Holder, Rose S Penfold, Alexandra Birch, Beatrice Alex</author><pubDate>Wed, 24 Jan 2024 15:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13512v1</guid></item><item><title>VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View</title><link>http://arxiv.org/abs/2307.06082v2</link><description>Incremental decision making in real-world environments is one of the mostchallenging tasks in embodied artificial intelligence. One particularlydemanding scenario is Vision and Language Navigation~(VLN) which requiresvisual and natural language understanding as well as spatial and temporalreasoning capabilities. The embodied agent needs to ground its understanding ofnavigation instructions in observations of a real-world environment like StreetView. Despite the impressive results of LLMs in other research areas, it is anongoing problem of how to best connect them with an interactive visualenvironment. In this work, we propose VELMA, an embodied LLM agent that uses averbalization of the trajectory and of visual environment observations ascontextual prompt for the next action. Visual information is verbalized by apipeline that extracts landmarks from the human written navigation instructionsand uses CLIP to determine their visibility in the current panorama view. Weshow that VELMA is able to successfully follow navigation instructions inStreet View with only two in-context examples. We further finetune the LLMagent on a few thousand examples and achieve 25%-30% relative improvement intask completion over the previous state-of-the-art for two datasets.</description><author>Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, William Yang Wang</author><pubDate>Wed, 24 Jan 2024 15:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06082v2</guid></item><item><title>Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images</title><link>http://arxiv.org/abs/2401.13511v1</link><description>Tissue segmentation is a routine preprocessing step to reduce thecomputational cost of whole slide image (WSI) analysis by excluding backgroundregions. Traditional image processing techniques are commonly used for tissuesegmentation, but often require manual adjustments to parameter values foratypical cases, fail to exclude all slide and scanning artifacts from thebackground, and are unable to segment adipose tissue. Pen marking artifacts inparticular can be a potential source of bias for subsequent analyses if notremoved. In addition, several applications require the separation of individualcross-sections, which can be challenging due to tissue fragmentation andadjacent positioning. To address these problems, we develop a convolutionalneural network for tissue and pen marking segmentation using a dataset of 200H&amp;E stained WSIs. For separating tissue cross-sections, we propose a novelpost-processing method based on clustering predicted centroid locations of thecross-sections in a 2D histogram. On an independent test set, the modelachieved a mean Dice score of 0.981$\pm$0.033 for tissue segmentation and amean Dice score of 0.912$\pm$0.090 for pen marking segmentation. The meanabsolute difference between the number of annotated and separatedcross-sections was 0.075$\pm$0.350. Our results demonstrate that the proposedmodel can accurately segment H&amp;E stained tissue cross-sections and pen markingsin WSIs while being robust to many common slide and scanning artifacts. Themodel with trained model parameters and post-processing method are madepublicly available as a Python package called SlideSegmenter.</description><author>Ruben T. Lucassen, Willeke A. M. Blokx, Mitko Veta</author><pubDate>Wed, 24 Jan 2024 15:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13511v1</guid></item><item><title>Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers</title><link>http://arxiv.org/abs/2401.06461v2</link><description>Large language models have catalyzed an unprecedented wave in codegeneration. While achieving significant advances, they blur the distinctionsbetween machine-and human-authored source code, causing integrity andauthenticity issues of software artifacts. Previous methods such as DetectGPThave proven effective in discerning machine-generated texts, but they do notidentify and harness the unique patterns of machine-generated code. Thus, itsapplicability falters when applied to code. In this paper, we carefully studythe specific patterns that characterize machine and human-authored code.Through a rigorous analysis of code attributes such as length, lexicaldiversity, and naturalness, we expose unique pat-terns inherent to each source.We particularly notice that the structural segmentation of code is a criticalfactor in identifying its provenance. Based on our findings, we propose a novelmachine-generated code detection method called DetectCodeGPT, which improvesDetectGPT by capturing the distinct structural patterns of code. Diverging fromconventional techniques that depend on external LLMs for perturbations,DetectCodeGPT perturbs the code corpus by strategically inserting spaces andnewlines, ensuring both efficacy and efficiency. Experiment results show thatour approach significantly outperforms state-of-the-art techniques in detectingmachine-generated code.</description><author>Yuling Shi, Hongyu Zhang, Chengcheng Wan, Xiaodong Gu</author><pubDate>Wed, 24 Jan 2024 14:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06461v2</guid></item><item><title>MNL-Bandit with Knapsacks: a near-optimal algorithm</title><link>http://arxiv.org/abs/2106.01135v5</link><description>We consider a dynamic assortment selection problem where a seller has a fixedinventory of $N$ substitutable products and faces an unknown demand thatarrives sequentially over $T$ periods. In each period, the seller needs todecide on the assortment of products (satisfying certain constraints) to offerto the customers. The customer's response follows an unknown multinomial logitmodel (MNL) with parameter $\boldsymbol{v}$. If customer selects product $i \in[N]$, the seller receives revenue $r_i$. The goal of the seller is to maximizethe total expected revenue from the $T$ customers given the fixed initialinventory of $N$ products. We present MNLwK-UCB, a UCB-based algorithm andcharacterize its regret under different regimes of inventory size. We show thatwhen the inventory size grows quasi-linearly in time, MNLwK-UCB achieves a$\tilde{O}(N + \sqrt{NT})$ regret bound. We also show that for a smallerinventory (with growth $\sim T^{\alpha}$, $\alpha &lt; 1$), MNLwK-UCB achieves a$\tilde{O}(N(1 + T^{\frac{1 - \alpha}{2}}) + \sqrt{NT})$. In particular, over along time horizon $T$, the rate $\tilde{O}(\sqrt{NT})$ is always achievedregardless of the constraints and the size of the inventory.</description><author>Abdellah Aznag, Vineet Goyal, Noemie Perivier</author><pubDate>Wed, 24 Jan 2024 14:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.01135v5</guid></item><item><title>Generative Human Motion Stylization in Latent Space</title><link>http://arxiv.org/abs/2401.13505v1</link><description>Human motion stylization aims to revise the style of an input motion whilekeeping its content unaltered. Unlike existing works that operate directly inpose space, we leverage the latent space of pretrained autoencoders as a moreexpressive and robust representation for motion extraction and infusion.Building upon this, we present a novel generative model that produces diversestylization results of a single motion (latent) code. During training, a motioncode is decomposed into two coding components: a deterministic content code,and a probabilistic style code adhering to a prior distribution; then agenerator massages the random combination of content and style codes toreconstruct the corresponding motion codes. Our approach is versatile, allowingthe learning of probabilistic style space from either style labeled orunlabeled motions, providing notable flexibility in stylization as well. Ininference, users can opt to stylize a motion using style cues from a referencemotion or a label. Even in the absence of explicit style input, our modelfacilitates novel re-stylization by sampling from the unconditional style priordistribution. Experimental results show that our proposed stylization models,despite their lightweight design, outperform the state-of-the-arts in stylereeanactment, content preservation, and generalization across variousapplications and settings. Project Page: https://yxmu.foo/GenMoStyle</description><author>Chuan Guo, Yuxuan Mu, Xinxin Zuo, Peng Dai, Youliang Yan, Juwei Lu, Li Cheng</author><pubDate>Wed, 24 Jan 2024 14:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13505v1</guid></item><item><title>Research about the Ability of LLM in the Tamper-Detection Area</title><link>http://arxiv.org/abs/2401.13504v1</link><description>In recent years, particularly since the early 2020s, Large Language Models(LLMs) have emerged as the most powerful AI tools in addressing a diverse rangeof challenges, from natural language processing to complex problem-solving invarious domains. In the field of tamper detection, LLMs are capable ofidentifying basic tampering activities.To assess the capabilities of LLMs inmore specialized domains, we have collected five different LLMs developed byvarious companies: GPT-4, LLaMA, Bard, ERNIE Bot 4.0, and Tongyi Qianwen. Thisdiverse range of models allows for a comprehensive evaluation of theirperformance in detecting sophisticated tampering instances.We devised twodomains of detection: AI-Generated Content (AIGC) detection and manipulationdetection. AIGC detection aims to test the ability to distinguish whether animage is real or AI-generated. Manipulation detection, on the other hand,focuses on identifying tampered images. According to our experiments, most LLMscan identify composite pictures that are inconsistent with logic, and only morepowerful LLMs can distinguish logical, but visible signs of tampering to thehuman eye. All of the LLMs can't identify carefully forged images and veryrealistic images generated by AI. In the area of tamper detection, LLMs stillhave a long way to go, particularly in reliably identifying highlysophisticated forgeries and AI-generated images that closely mimic reality.</description><author>Xinyu Yang, Jizhe Zhou</author><pubDate>Wed, 24 Jan 2024 14:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13504v1</guid></item><item><title>Learning Representations for Clustering via Partial Information Discrimination and Cross-Level Interaction</title><link>http://arxiv.org/abs/2401.13503v1</link><description>In this paper, we present a novel deep image clustering approach termed PICI,which enforces the partial information discrimination and the cross-levelinteraction in a joint learning framework. In particular, we leverage aTransformer encoder as the backbone, through which the masked image modelingwith two paralleled augmented views is formulated. After deriving the classtokens from the masked images by the Transformer encoder, three partialinformation learning modules are further incorporated, including the PISDmodule for training the auto-encoder via masked image reconstruction, the PICDmodule for employing two levels of contrastive learning, and the CLI module formutual interaction between the instance-level and cluster-level subspaces.Extensive experiments have been conducted on six real-world image datasets,which demononstrate the superior clustering performance of the proposed PICIapproach over the state-of-the-art deep clustering approaches. The source codeis available at https://github.com/Regan-Zhang/PICI.</description><author>Hai-Xin Zhang, Dong Huang, Hua-Bao Ling, Guang-Yu Zhang, Wei-jun Sun, Zi-hao Wen</author><pubDate>Wed, 24 Jan 2024 14:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13503v1</guid></item><item><title>LDCA: Local Descriptors with Contextual Augmentation for Few-Shot Learning</title><link>http://arxiv.org/abs/2401.13499v1</link><description>Few-shot image classification has emerged as a key challenge in the field ofcomputer vision, highlighting the capability to rapidly adapt to new tasks withminimal labeled data. Existing methods predominantly rely on image-levelfeatures or local descriptors, often overlooking the holistic contextsurrounding these descriptors. In this work, we introduce a novel approachtermed "Local Descriptor with Contextual Augmentation (LDCA)". Specifically,this method bridges the gap between local and global understanding uniquely byleveraging an adaptive global contextual enhancement module. This moduleincorporates a visual transformer, endowing local descriptors with contextualawareness capabilities, ranging from broad global perspectives to intricatesurrounding nuances. By doing so, LDCA transcends traditional descriptor-basedapproaches, ensuring each local feature is interpreted within its larger visualnarrative. Extensive experiments underscore the efficacy of our method, showinga maximal absolute improvement of 20\% over the next-best on fine-grainedclassification datasets, thus demonstrating significant advancements infew-shot classification tasks.</description><author>Maofa Wang, Bingchen Yan</author><pubDate>Wed, 24 Jan 2024 14:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13499v1</guid></item><item><title>Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting</title><link>http://arxiv.org/abs/2401.13498v1</link><description>Synthesizing performing guitar sound is a highly challenging task due to thepolyphony and high variability in expression. Recently, deep generative modelshave shown promising results in synthesizing expressive polyphonic instrumentsounds from music scores, often using a generic MIDI input. In this work, wepropose an expressive acoustic guitar sound synthesis model with a customizedinput representation to the instrument, which we call guitarroll. We implementthe proposed approach using diffusion-based outpainting which can generateaudio with long-term consistency. To overcome the lack of MIDI/audio-paireddatasets, we used not only an existing guitar dataset but also collected datafrom a high quality sample-based guitar synthesizer. Through quantitative andqualitative evaluations, we show that our proposed model has higher audioquality than the baseline model and generates more realistic timbre sounds thanthe previous leading work.</description><author>Hounsu Kim, Soonbeom Choi, Juhan Nam</author><pubDate>Wed, 24 Jan 2024 14:44:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13498v1</guid></item><item><title>Separable Physics-Informed Neural Networks for the solution of elasticity problems</title><link>http://arxiv.org/abs/2401.13486v1</link><description>A method for solving elasticity problems based on separable physics-informedneural networks (SPINN) in conjunction with the deep energy method (DEM) ispresented. Numerical experiments have been carried out for a number of problemsshowing that this method has a significantly higher convergence rate andaccuracy than the vanilla physics-informed neural networks (PINN) and evenSPINN based on a system of partial differential equations (PDEs). In addition,using the SPINN in the framework of DEM approach it is possible to solveproblems of the linear theory of elasticity on complex geometries, which isunachievable with the help of PINNs in frames of partial differentialequations. Considered problems are very close to the industrial problems interms of geometry, loading, and material parameters.</description><author>Vasiliy A. Es'kin, Danil V. Davydov, Julia V. Gur'eva, Alexey O. Malkhanov, Mikhail E. Smorkalov</author><pubDate>Wed, 24 Jan 2024 14:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13486v1</guid></item><item><title>Vision Transformers increase efficiency of 3D cardiac CT multi-label segmentation</title><link>http://arxiv.org/abs/2310.09099v2</link><description>Accurate segmentation of the heart is essential for personalized blood flowsimulations and surgical intervention planning. Segmentations need to beaccurate in every spatial dimension, which is not ensured by segmenting dataslice by slice. Two cardiac computed tomography (CT) datasets consisting of 760volumes across the whole cardiac cycle from 39 patients, and of 60 volumes from60 patients respectively were used to train networks to simultaneously segmentmultiple regions representing the whole heart in 3D. The segmented regionsincluded the left and right atrium and ventricle, left ventricular myocardium,ascending aorta, pulmonary arteries, pulmonary veins, and left atrialappendage. The widely used 3D U-Net and the UNETR architecture were compared toour proposed method optimized for large volumetric inputs. The proposed networkarchitecture, termed Transformer Residual U-Net (TRUNet), maintains the cascadedownsampling encoder, cascade upsampling decoder and skip connections fromU-Net, while incorporating a Vision Transformer (ViT) block in the encoderalongside a modified ResNet50 block. TRUNet reached higher segmentationperformance for all structures within approximately half the training timeneeded for 3D U-Net and UNETR. The proposed method achieved more precise vesselboundary segmentations and better captured the heart's overall anatomicalstructure compared to the other methods. The fast training time and accuratedelineation of adjacent structures makes TRUNet a promising candidate formedical image segmentation tasks. The code for TRUNet is available atgithub.com/ljollans/TRUNet.</description><author>Lee Jollans, Mariana Bustamante, Lilian Henriksson, Anders Persson, Tino Ebbers</author><pubDate>Wed, 24 Jan 2024 14:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09099v2</guid></item><item><title>How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment</title><link>http://arxiv.org/abs/2401.13481v1</link><description>Exposure to large language model output is rapidly increasing. How willseeing AI-generated ideas affect human ideas? We conducted an experiment (800+participants, 40+ countries) where participants viewed creative ideas that werefrom ChatGPT or prior experimental participants and then brainstormed their ownidea. We varied the number of AI-generated examples (none, low, or highexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamicexperiment design -- ideas from prior participants in an experimental conditionare used as stimuli for future participants in the same experimental condition-- mimics the interdependent process of cultural creation: creative ideas arebuilt upon prior ideas. Hence, we capture the compounding effects of havingLLMs 'in the culture loop'. We find that high AI exposure (but not low AIexposure) did not affect the creativity of individual ideas but did increasethe average amount and rate of change of collective idea diversity. AI madeideas different, not better. There were no main effects of disclosure. We alsofound that self-reported creative people were less influenced by knowing anidea was from AI, and that participants were more likely to knowingly adopt AIideas when the task was difficult. Our findings suggest that introducing AIideas into society may increase collective diversity but not individualcreativity.</description><author>Joshua Ashkinaze, Julia Mendelsohn, Li Qiwei, Ceren Budak, Eric Gilbert</author><pubDate>Wed, 24 Jan 2024 14:29:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13481v1</guid></item><item><title>SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval</title><link>http://arxiv.org/abs/2401.13478v1</link><description>Multi-modal information retrieval (MMIR) is a rapidly evolving field, wheresignificant progress, particularly in image-text pairing, has been made throughadvanced representation learning and cross-modality alignment research.However, current benchmarks for evaluating MMIR performance in image-textpairing within the scientific domain show a notable gap, where chart and tableimages described in scholarly language usually do not play a significant role.To bridge this gap, we develop a specialised scientific MMIR (SciMMIR)benchmark by leveraging open-access paper collections to extract data relevantto the scientific domain. This benchmark comprises 530K meticulously curatedimage-text pairs, extracted from figures and tables with detailed captions inscientific documents. We further annotate the image-text pairs with two-levelsubset-subcategory hierarchy annotations to facilitate a more comprehensiveevaluation of the baselines. We conducted zero-shot and fine-tuning evaluationson prominent multi-modal image-captioning and visual language models, such asCLIP and BLIP. Our analysis offers critical insights for MMIR in the scientificdomain, including the impact of pre-training and fine-tuning settings and theinfluence of the visual and textual encoders. All our data and checkpoints arepublicly available at https://github.com/Wusiwei0410/SciMMIR.</description><author>Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, Wenhao Huang, Noura Al Moubayed, Jie Fu, Chenghua Lin</author><pubDate>Wed, 24 Jan 2024 14:23:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13478v1</guid></item><item><title>Segmenting Cardiac Muscle Z-disks with Deep Neural Networks</title><link>http://arxiv.org/abs/2401.13472v1</link><description>Z-disks are complex structures that delineate repeating sarcomeres instriated muscle. They play significant roles in cardiomyocytes such asproviding mechanical stability for the contracting sarcomere, cell signallingand autophagy. Changes in Z-disk architecture have been associated withimpaired cardiac function. Hence, there is a strong need to create tools tosegment Z-disks from microscopy images, that overcome traditional limitationssuch as variability in image brightness and staining technique. In this study,we apply deep learning based segmentation models to extract Z-disks in imagesof striated muscle tissue. We leverage a novel Airyscan confocal dataset, whichcomprises high resolution images of Z-disks of healthy heart tissue, stainedwith Affimers for specific Z-disk proteins. We employed an interactivelabelling tool, Ilastik to obtain ground truth segmentation masks and use theresulting data set to train and evaluate the performance of severalstate-of-the-art segmentation networks. On the test set, UNet++ achieves bestsegmentation performance for Z-disks in cardiomyocytes, with an average Dicescore of 0.91 and outperforms other established segmentation methods includingUNet, FPN, DeepLabv3+ and pix2pix. However, pix2pix demonstrates improvedgeneralisation, when tested on an additional dataset of cardiomyocytes with atitin mutation. This is the first study to demonstrate that automated machinelearning-based segmentation approaches may be used effectively to segmentZ-disks in confocal microscopy images. Automated segmentation approaches andpredicted segmentation masks could be used to derive morphological features ofZ-disks (e.g. width and orientation), and subsequently, to quantifydisease-related changes to cardiac microstructure.</description><author>Mihaela Croitor Ibrahim, Nishant Ravikumar, Alistair Curd, Joanna Leng, Oliver Umney, Michelle Peckham</author><pubDate>Wed, 24 Jan 2024 14:18:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13472v1</guid></item><item><title>Consistent Optimal Transport with Empirical Conditional Measures</title><link>http://arxiv.org/abs/2305.15901v4</link><description>Given samples from two joint distributions, we consider the problem ofOptimal Transportation (OT) between them when conditioned on a common variable.We focus on the general setting where the conditioned variable may becontinuous, and the marginals of this variable in the two joint distributionsmay not be the same. In such settings, standard OT variants cannot be employed,and novel estimation techniques are necessary. Since the main challenge is thatthe conditional distributions are not explicitly available, the key idea in ourOT formulation is to employ kernelized-least-squares terms computed over thejoint samples, which implicitly match the transport plan's marginals with theempirical conditionals. Under mild conditions, we prove that our estimatedtransport plans, as a function of the conditioned variable, are asymptoticallyoptimal. For finite samples, we show that the deviation in terms of ourregularized objective is bounded by $O(1/m^{1/4})$, where $m$ is the number ofsamples. We also discuss how the conditional transport plan could be modelledusing explicit probabilistic models as well as using implicit generative ones.We empirically verify the consistency of our estimator on synthetic datasets,where the optimal plan is analytically known. When employed in applicationslike prompt learning for few-shot classification and conditional-generation inthe context of predicting cell responses to treatment, our methodology improvesupon state-of-the-art methods.</description><author>Piyushi Manupriya, Rachit Keerti Das, Sayantan Biswas, Saketha Nath Jagarlapudi</author><pubDate>Wed, 24 Jan 2024 14:13:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15901v4</guid></item><item><title>SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering</title><link>http://arxiv.org/abs/2401.13463v1</link><description>Spoken Question Answering (SQA) is essential for machines to reply to user'squestion by finding the answer span within a given spoken passage. SQA has beenpreviously achieved without ASR to avoid recognition errors andOut-of-Vocabulary (OOV) problems. However, the real-world problem ofOpen-domain SQA (openSQA), in which the machine needs to first retrievepassages that possibly contain the answer from a spoken archive in addition,was never considered. This paper proposes the first known end-to-end framework,Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of theopenSQA problem. SpeechDPR learns a sentence-level semantic representation bydistilling knowledge from the cascading model of unsupervised ASR (UASR) andtext dense retriever (TDR). No manually transcribed speech data is needed.Initial experiments showed performance comparable to the cascading model ofUASR and TDR, and significantly better when UASR was poor, verifying thisapproach is more robust to speech recognition errors.</description><author>Chyi-Jiunn Lin, Guan-Ting Lin, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee</author><pubDate>Wed, 24 Jan 2024 14:08:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13463v1</guid></item><item><title>Growing from Exploration: A self-exploring framework for robots based on foundation models</title><link>http://arxiv.org/abs/2401.13462v1</link><description>Intelligent robot is the ultimate goal in the robotics field. Existing worksleverage learning-based or optimization-based methods to accomplishhuman-defined tasks. However, the challenge of enabling robots to explorevarious environments autonomously remains unresolved. In this work, we proposea framework named GExp, which enables robots to explore and learn autonomouslywithout human intervention. To achieve this goal, we devise modules includingself-exploration, knowledge-base-building, and close-loop feedback based onfoundation models. Inspired by the way that infants interact with the world,GExp encourages robots to understand and explore the environment with a seriesof self-generated tasks. During the process of exploration, the robot willacquire skills from beneficial experiences that are useful in the future. GExpprovides robots with the ability to solve complex tasks throughself-exploration. GExp work is independent of prior interactive knowledge andhuman intervention, allowing it to adapt directly to different scenarios,unlike previous studies that provided in-context examples as few-shot learning.In addition, we propose a workflow of deploying the real-world robot systemwith self-learned skills as an embodied assistant.</description><author>Shoujie Li, Ran Yu, Tong Wu, JunWen Zhong, Xiao-Ping Zhang, Wenbo Ding</author><pubDate>Wed, 24 Jan 2024 14:04:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13462v1</guid></item><item><title>Multi-Agent Diagnostics for Robustness via Illuminated Diversity</title><link>http://arxiv.org/abs/2401.13460v1</link><description>In the rapidly advancing field of multi-agent systems, ensuring robustness inunfamiliar and adversarial settings is crucial. Notwithstanding theiroutstanding performance in familiar environments, these systems often falter innew situations due to overfitting during the training phase. This is especiallypronounced in settings where both cooperative and competitive behaviours arepresent, encapsulating a dual nature of overfitting and generalisationchallenges. To address this issue, we present Multi-Agent Diagnostics forRobustness via Illuminated Diversity (MADRID), a novel approach for generatingdiverse adversarial scenarios that expose strategic vulnerabilities inpre-trained multi-agent policies. Leveraging the concepts from open-endedlearning, MADRID navigates the vast space of adversarial settings, employing atarget policy's regret to gauge the vulnerabilities of these settings. Weevaluate the effectiveness of MADRID on the 11vs11 version of Google ResearchFootball, one of the most complex environments for multi-agent reinforcementlearning. Specifically, we employ MADRID for generating a diverse array ofadversarial settings for TiZero, the state-of-the-art approach which "masters"the game through 45 days of training on a large-scale distributedinfrastructure. We expose key shortcomings in TiZero's tacticaldecision-making, underlining the crucial importance of rigorous evaluation inmulti-agent systems.</description><author>Mikayel Samvelyan, Davide Paglieri, Minqi Jiang, Jack Parker-Holder, Tim Rocktäschel</author><pubDate>Wed, 24 Jan 2024 14:02:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13460v1</guid></item><item><title>M2ORT: Many-To-One Regression Transformer for Spatial Transcriptomics Prediction from Histopathology Images</title><link>http://arxiv.org/abs/2401.10608v2</link><description>The advancement of Spatial Transcriptomics (ST) has facilitated thespatially-aware profiling of gene expressions based on histopathology images.Although ST data offers valuable insights into the micro-environment of tumors,its acquisition cost remains expensive. Therefore, directly predicting the STexpressions from digital pathology images is desired. Current methods usuallyadopt existing regression backbones for this task, which ignore the inherentmulti-scale hierarchical data structure of digital pathology images. To addressthis limit, we propose M2ORT, a many-to-one regression Transformer that canaccommodate the hierarchical structure of the pathology images through adecoupled multi-scale feature extractor. Different from traditional models thatare trained with one-to-one image-label pairs, M2ORT accepts multiple pathologyimages of different magnifications at a time to jointly predict the geneexpressions at their corresponding common ST spot, aiming at learning amany-to-one relationship through training. We have tested M2ORT on three publicST datasets and the experimental results show that M2ORT can achievestate-of-the-art performance with fewer parameters and floating-pointoperations (FLOPs). The code is available at:https://github.com/Dootmaan/M2ORT/.</description><author>Hongyi Wang, Xiuju Du, Jing Liu, Shuyi Ouyang, Yen-Wei Chen, Lanfen Lin</author><pubDate>Wed, 24 Jan 2024 13:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10608v2</guid></item><item><title>A New Sentence Extraction Strategy for Unsupervised Extractive Summarization Methods</title><link>http://arxiv.org/abs/2112.03203v5</link><description>In recent years, text summarization methods have attracted much attentionagain thanks to the researches on neural network models. Most of the currenttext summarization methods based on neural network models are supervisedmethods which need large-scale datasets. However, large-scale datasets aredifficult to obtain in practical applications. In this paper, we model the taskof extractive text summarization methods from the perspective of InformationTheory, and then describe the unsupervised extractive methods with a uniformframework. To improve the feature distribution and to decrease the mutualinformation of summarization sentences, we propose a new sentence extractionstrategy which can be applied to existing unsupervised extractive methods.Experiments are carried out on different datasets, and results show that ourstrategy is indeed effective and in line with expectations.</description><author>Dehao Tao, Yingzhu Xiong, Zhongliang Yang, Yongfeng Huang</author><pubDate>Wed, 24 Jan 2024 13:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.03203v5</guid></item><item><title>Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters</title><link>http://arxiv.org/abs/2302.13711v3</link><description>After the recent ground-breaking advances in protein structure prediction,one of the remaining challenges in protein machine learning is to reliablypredict distributions of structural states. Parametric models of fluctuationsare difficult to fit due to complex covariance structures between degrees offreedom in the protein chain, often causing models to either violate local orglobal structural constraints. In this paper, we present a new strategy formodelling protein densities in internal coordinates, which uses constraints in3D space to induce covariance structure between the internal degrees offreedom. We illustrate the potential of the procedure by constructing avariational autoencoder with full covariance output induced by the constraintsimplied by the conditional mean in 3D, and demonstrate that our approach makesit possible to scale density models of internal coordinates to full proteinbackbones in two settings: 1) a unimodal setting for proteins exhibiting smallfluctuations and limited amounts of available data, and 2) a multimodal settingfor larger conformational changes in a high data regime.</description><author>Marloes Arts, Jes Frellsen, Wouter Boomsma</author><pubDate>Wed, 24 Jan 2024 13:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13711v3</guid></item><item><title>Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription</title><link>http://arxiv.org/abs/2309.15717v2</link><description>In recent years, research on music transcription has focused mainly onarchitecture design and instrument-specific data acquisition. With the lack ofavailability of diverse datasets, progress is often limited to solo-instrumenttasks such as piano transcription. Several works have explored multi-instrumenttranscription as a means to bolster the performance of models on low-resourcetasks, but these methods face the same data availability issues. We proposeTimbre-Trap, a novel framework which unifies music transcription and audioreconstruction by exploiting the strong separability between pitch and timbre.We train a single autoencoder to simultaneously estimate pitch salience andreconstruct complex spectral coefficients, selecting between either outputduring the decoding stage via a simple switch mechanism. In this way, the modellearns to produce coefficients corresponding to timbre-less audio, which can beinterpreted as pitch salience. We demonstrate that the framework leads toperformance comparable to state-of-the-art instrument-agnostic transcriptionmethods, while only requiring a small amount of annotated data.</description><author>Frank Cwitkowitz, Kin Wai Cheuk, Woosung Choi, Marco A. Martínez-Ramírez, Keisuke Toyama, Wei-Hsiang Liao, Yuki Mitsufuji</author><pubDate>Wed, 24 Jan 2024 13:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15717v2</guid></item><item><title>Symbolic Equation Solving via Reinforcement Learning</title><link>http://arxiv.org/abs/2401.13447v1</link><description>Machine-learning methods are gradually being adopted in a great variety ofsocial, economic, and scientific contexts, yet they are notorious forstruggling with exact mathematics. A typical example is computer algebra, whichincludes tasks like simplifying mathematical terms, calculating formalderivatives, or finding exact solutions of algebraic equations. Traditionalsoftware packages for these purposes are commonly based on a huge database ofrules for how a specific operation (e.g., differentiation) transforms a certainterm (e.g., sine function) into another one (e.g., cosine function). Thus far,these rules have usually needed to be discovered and subsequently programmed byhumans. Focusing on the paradigmatic example of solving linear equations insymbolic form, we demonstrate how the process of finding elementarytransformation rules and step-by-step solutions can be automated usingreinforcement learning with deep neural networks.</description><author>Lennart Dabelow, Masahito Ueda</author><pubDate>Wed, 24 Jan 2024 13:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13447v1</guid></item><item><title>Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction</title><link>http://arxiv.org/abs/2309.03619v2</link><description>Self-supervised learning (SSL) has emerged as a promising paradigm forlearning flexible speech representations from unlabeled data. By designingpretext tasks that exploit statistical regularities, SSL models can captureuseful representations that are transferable to downstream tasks. This studyprovides an empirical analysis of Barlow Twins (BT), an SSL technique inspiredby theories of redundancy reduction in human perception. On downstream tasks,BT representations accelerated learning and transferred across domains.However, limitations exist in disentangling key explanatory factors, withredundancy reduction and invariance alone insufficient for factorization oflearned latents into modular, compact, and informative codes. Our ablationsstudy isolated gains from invariance constraints, but the gains werecontext-dependent. Overall, this work substantiates the potential of BarlowTwins for sample-efficient speech encoding. However, challenges remain inachieving fully hierarchical representations. The analysis methodology andinsights pave a path for extensions incorporating further inductive priors andperceptual principles to further enhance the BT self-supervision framework.</description><author>Yusuf Brima, Ulf Krumnack, Simone Pika, Gunther Heidemann</author><pubDate>Wed, 24 Jan 2024 13:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03619v2</guid></item><item><title>Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption</title><link>http://arxiv.org/abs/2401.13444v1</link><description>In recent times, large language models (LLMs) have showcased remarkablecapabilities. However, updating their knowledge poses challenges, potentiallyleading to inaccuracies when confronted with unfamiliar queries. Whileintegrating knowledge graphs with LLMs has been explored, existing approachestreat LLMs as primary decision-makers, imposing high demands on theircapabilities. This is particularly unsuitable for LLMs with lower computationalcosts and relatively poorer performance. In this paper, we introduce aClue-Guided Path Exploration framework (CGPE) that efficiently merges aknowledge base with an LLM, placing less stringent requirements on the model'scapabilities. Inspired by the method humans use to manually retrieve knowledge,CGPE employs information from the question as clues to systematically explorethe required knowledge path within the knowledge base. Experiments onopen-source datasets reveal that CGPE outperforms previous methods and ishighly applicable to LLMs with fewer parameters. In some instances, evenChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4.Furthermore, the results indicate a minimal invocation frequency of CGPE onLLMs, suggesting reduced computational overhead. For organizations andindividuals facing constraints in computational resources, our research offerssignificant practical value.</description><author>Dehao Tao, Feng Huang, Yongfeng Huang, Minghu Jiang</author><pubDate>Wed, 24 Jan 2024 13:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13444v1</guid></item><item><title>cito: An R package for training neural networks using torch</title><link>http://arxiv.org/abs/2303.09599v3</link><description>Deep Neural Networks (DNN) have become a central method in ecology. Mostcurrent deep learning (DL) applications rely on one of the major deep learningframeworks, in particular Torch or TensorFlow, to build and train DNN. Usingthese frameworks, however, requires substantially more experience and time thantypical regression functions in the R environment. Here, we present 'cito', auser-friendly R package for DL that allows specifying DNNs in the familiarformula syntax used by many R packages. To fit the models, 'cito' uses 'torch',taking advantage of the numerically optimized torch library, including theability to switch between training models on the CPU or the graphics processingunit (GPU) (which allows to efficiently train large DNN). Moreover, 'cito'includes many user-friendly functions for model plotting and analysis,including optional confidence intervals (CIs) based on bootstraps forpredictions and explainable AI (xAI) metrics for effect sizes and variableimportance with CIs and p-values. To showcase a typical analysis pipeline using'cito', including its built-in xAI features to explore the trained DNN, webuild a species distribution model of the African elephant. We hope that byproviding a user-friendly R framework to specify, deploy and interpret DNN,'cito' will make this interesting model class more accessible to ecologicaldata analysis. A stable version of 'cito' can be installed from thecomprehensive R archive network (CRAN).</description><author>Christian Amesoeder, Florian Hartig, Maximilian Pichler</author><pubDate>Wed, 24 Jan 2024 13:24:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09599v3</guid></item><item><title>PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection</title><link>http://arxiv.org/abs/2401.09793v3</link><description>Anomaly detection stands as a crucial aspect of time series analysis, aimingto identify abnormal events in time series samples. The central challenge ofthis task lies in effectively learning the representations of normal andabnormal patterns in a label-lacking scenario. Previous research mostly reliedon reconstruction-based approaches, restricting the representational abilitiesof the models. In addition, most of the current deep learning-based methods arenot lightweight enough, which prompts us to design a more efficient frameworkfor anomaly detection. In this study, we introduce PatchAD, a novel multi-scalepatch-based MLP-Mixer architecture that leverages contrastive learning forrepresentational extraction and anomaly detection. Specifically, PatchAD iscomposed of four distinct MLP Mixers, exclusively utilizing the MLParchitecture for high efficiency and lightweight architecture. Additionally, wealso innovatively crafted a dual project constraint module to mitigatepotential model degradation. Comprehensive experiments demonstrate that PatchADachieves state-of-the-art results across multiple real-world multivariate timeseries datasets. Our code is publicly availablehttps://github.com/EmorZz1G/PatchAD</description><author>Zhijie Zhong, Zhiwen Yu, Yiyuan Yang, Weizheng Wang, Kaixiang Yang</author><pubDate>Wed, 24 Jan 2024 13:23:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09793v3</guid></item><item><title>MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation</title><link>http://arxiv.org/abs/2401.11115v3</link><description>Controllable generation of 3D human motions becomes an important topic as theworld embraces digital transformation. Existing works, though making promisingprogress with the advent of diffusion models, heavily rely on meticulouslycaptured and annotated (e.g., text) high-quality motion corpus, aresource-intensive endeavor in the real world. This motivates our proposedMotionMix, a simple yet effective weakly-supervised diffusion model thatleverages both noisy and unannotated motion sequences. Specifically, weseparate the denoising objectives of a diffusion model into two stages:obtaining conditional rough motion approximations in the initial $T-T^*$ stepsby learning the noisy annotated motions, followed by the unconditionalrefinement of these preliminary motions during the last $T^*$ steps usingunannotated motions. Notably, though learning from two sources of imperfectdata, our model does not compromise motion generation quality compared to fullysupervised approaches that access gold data. Extensive experiments on severalbenchmarks demonstrate that our MotionMix, as a versatile framework,consistently achieves state-of-the-art performances on text-to-motion,action-to-motion, and music-to-dance tasks. Project page:https://nhathoang2002.github.io/MotionMix-page/</description><author>Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi</author><pubDate>Wed, 24 Jan 2024 13:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11115v3</guid></item><item><title>Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond</title><link>http://arxiv.org/abs/2401.13432v1</link><description>Thin-plate spline (TPS) is a principal warp that allows for representingelastic, nonlinear transformation with control point motions. With the increaseof control points, the warp becomes increasingly flexible but usuallyencounters a bottleneck caused by undesired issues, e.g., content distortion.In this paper, we explore generic applications of TPS in single-image-basedwarping tasks, such as rotation correction, rectangling, and portraitcorrection. To break this bottleneck, we propose the coupled thin-plate splinemodel (CoupledTPS), which iteratively couples multiple TPS with limited controlpoints into a more flexible and powerful transformation. Concretely, we firstdesign an iterative search to predict new control points according to thecurrent latent condition. Then, we present the warping flow as a bridge for thecoupling of different TPS transformations, effectively eliminatinginterpolation errors caused by multiple warps. Besides, in light of thelaborious annotation cost, we develop a semi-supervised learning scheme toimprove warping quality by exploiting unlabeled data. It is formulated throughdual transformation between the searched control points of unlabeled data andits graphic augmentation, yielding an implicit correction consistencyconstraint. Finally, we collect massive unlabeled data to exhibit the benefitof our semi-supervised scheme in rotation correction. Extensive experimentsdemonstrate the superiority and universality of CoupledTPS over the existingstate-of-the-art (SoTA) solutions for rotation correction and beyond. The codeand data will be available at https://github.com/nie-lang/CoupledTPS.</description><author>Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao</author><pubDate>Wed, 24 Jan 2024 13:03:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13432v1</guid></item><item><title>Detection of Correlated Random Vectors</title><link>http://arxiv.org/abs/2401.13429v1</link><description>In this paper, we investigate the problem of deciding whether two standardnormal random vectors $\mathsf{X}\in\mathbb{R}^{n}$ and$\mathsf{Y}\in\mathbb{R}^{n}$ are correlated or not. This is formulated as ahypothesis testing problem, where under the null hypothesis, these vectors arestatistically independent, while under the alternative, $\mathsf{X}$ and arandomly and uniformly permuted version of $\mathsf{Y}$, are correlated withcorrelation $\rho$. We analyze the thresholds at which optimal testing isinformation-theoretically impossible and possible, as a function of $n$ and$\rho$. To derive our information-theoretic lower bounds, we develop a noveltechnique for evaluating the second moment of the likelihood ratio using anorthogonal polynomials expansion, which among other things, reveals asurprising connection to integer partition functions. We also study amulti-dimensional generalization of the above setting, where rather than twovectors we observe two databases/matrices, and furthermore allow for partialcorrelations between these two.</description><author>Dor Elimelech, Wasim Huleihel</author><pubDate>Wed, 24 Jan 2024 12:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13429v1</guid></item><item><title>Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems</title><link>http://arxiv.org/abs/2211.12343v3</link><description>We consider the ubiquitous linear inverse problems with additive Gaussiannoise and propose an unsupervised sampling approach called diffusion modelbased posterior sampling (DMPS) to reconstruct the unknown signal from noisylinear measurements. Specifically, using one diffusion model (DM) as animplicit prior, the fundamental difficulty in performing posterior sampling isthat the noise-perturbed likelihood score, i.e., gradient of an annealedlikelihood function, is intractable. To circumvent this problem, we introduce asimple yet effective closed-form approximation using an uninformative priorassumption. Extensive experiments are conducted on a variety of noisy linearinverse problems such as noisy super-resolution, denoising, deblurring, andcolorization. In all tasks, the proposed DMPS demonstrates highly competitiveor even better performances on various tasks while being 3 times faster thanthe state-of-the-art competitor diffusion posterior sampling (DPS).</description><author>Xiangming Meng, Yoshiyuki Kabashima</author><pubDate>Wed, 24 Jan 2024 12:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12343v3</guid></item><item><title>The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting -- An Analytical Model</title><link>http://arxiv.org/abs/2401.12617v2</link><description>In continual learning, catastrophic forgetting is affected by multipleaspects of the tasks. Previous works have analyzed separately how forgetting isaffected by either task similarity or overparameterization. In contrast, ourpaper examines how task similarity and overparameterization jointly affectforgetting in an analyzable model. Specifically, we focus on two-task continuallinear regression, where the second task is a random orthogonal transformationof an arbitrary first task (an abstraction of random permutation tasks). Wederive an exact analytical expression for the expected forgetting - and uncovera nuanced pattern. In highly overparameterized models, intermediate tasksimilarity causes the most forgetting. However, near the interpolationthreshold, forgetting decreases monotonically with the expected tasksimilarity. We validate our findings with linear regression on synthetic data,and with neural networks on established permutation task benchmarks.</description><author>Daniel Goldfarb, Itay Evron, Nir Weinberger, Daniel Soudry, Paul Hand</author><pubDate>Wed, 24 Jan 2024 12:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12617v2</guid></item><item><title>Federated learning with distributed fixed design quantum chips and quantum channels</title><link>http://arxiv.org/abs/2401.13421v1</link><description>The privacy in classical federated learning can be breached through the useof local gradient results by using engineered queries from the clients.However, quantum communication channels are considered more secure because theuse of measurements in the data causes some loss of information, which can bedetected. Therefore, the quantum version of federated learning can be used toprovide more privacy. Additionally, sending an $N$ dimensional data vectorthrough a quantum channel requires sending $\log N$ entangled qubits, which canprovide exponential efficiency if the data vector is obtained as quantumstates. In this paper, we propose a quantum federated learning model where fixeddesign quantum chips are operated based on the quantum states sent by acentralized server. Based on the coming superposition states, the clientscompute and then send their local gradients as quantum states to the server,where they are aggregated to update parameters. Since the server does not sendmodel parameters, but instead sends the operator as a quantum state, theclients are not required to share the model. This allows for the creation ofasynchronous learning models. In addition, the model as a quantum state is fedinto client-side chips directly; therefore, it does not require measurements onthe upcoming quantum state to obtain model parameters in order to computegradients. This can provide efficiency over the models where parameter vectoris sent via classical or quantum channels and local gradients are obtainedthrough the obtained values of these parameters.</description><author>Ammar Daskin</author><pubDate>Wed, 24 Jan 2024 12:32:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13421v1</guid></item></channel></rss>