<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 01 Feb 2024 14:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators</title><link>http://arxiv.org/abs/2401.18085v1</link><description>Diffusion models are capable of generating impressive images conditioned ontext descriptions, and extensions of these models allow users to edit images ata relatively coarse scale. However, the ability to precisely edit the layout,position, pose, and shape of objects in images with diffusion models is stilldifficult. To this end, we propose motion guidance, a zero-shot technique thatallows a user to specify dense, complex motion fields that indicate where eachpixel in an image should move. Motion guidance works by steering the diffusionsampling process with the gradients through an off-the-shelf optical flownetwork. Specifically, we design a guidance loss that encourages the sample tohave the desired motion, as estimated by a flow network, while also beingvisually similar to the source image. By simultaneously sampling from adiffusion model and guiding the sample to have low guidance loss, we can obtaina motion-edited image. We demonstrate that our technique works on complexmotions and produces high quality edits of real and generated images.</description><author>Daniel Geng, Andrew Owens</author><pubDate>Wed, 31 Jan 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18085v1</guid></item><item><title>Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</title><link>http://arxiv.org/abs/2401.18084v1</link><description>The ability to associate touch with other modalities has huge implicationsfor humans and computational systems. However, multimodal learning with touchremains challenging due to the expensive data collection process andnon-standardized sensor outputs. We introduce UniTouch, a unified tactile modelfor vision-based touch sensors connected to multiple modalities, includingvision, language, and sound. We achieve this by aligning our UniTouchembeddings to pretrained image embeddings already associated with a variety ofother modalities. We further propose learnable sensor-specific tokens, allowingthe model to learn from a set of heterogeneous tactile sensors, all at the sametime. UniTouch is capable of conducting various touch sensing tasks in thezero-shot setting, from robot grasping prediction to touch image questionanswering. To the best of our knowledge, UniTouch is the first to demonstratesuch capabilities. Project page: https://cfeng16.github.io/UniTouch/</description><author>Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, Alex Wong</author><pubDate>Wed, 31 Jan 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18084v1</guid></item><item><title>Improved Scene Landmark Detection for Camera Localization</title><link>http://arxiv.org/abs/2401.18083v1</link><description>Camera localization methods based on retrieval, local feature matching, and3D structure-based pose estimation are accurate but require high storage, areslow, and are not privacy-preserving. A method based on scene landmarkdetection (SLD) was recently proposed to address these limitations. It involvestraining a convolutional neural network (CNN) to detect a few predetermined,salient, scene-specific 3D points or landmarks and computing camera pose fromthe associated 2D-3D correspondences. Although SLD outperformed existinglearning-based approaches, it was notably less accurate than 3D structure-basedmethods. In this paper, we show that the accuracy gap was due to insufficientmodel capacity and noisy labels during training. To mitigate the capacityissue, we propose to split the landmarks into subgroups and train a separatenetwork for each subgroup. To generate better training labels, we propose usingdense reconstructions to estimate visibility of scene landmarks. Finally, wepresent a compact architecture to improve memory efficiency. Accuracy wise, ourapproach is on par with state of the art structure based methods on theINDOOR-6 dataset but runs significantly faster and uses less storage. Code andmodels can be found at https://github.com/microsoft/SceneLandmarkLocalization.</description><author>Tien Do, Sudipta N. Sinha</author><pubDate>Wed, 31 Jan 2024 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18083v1</guid></item><item><title>KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</title><link>http://arxiv.org/abs/2401.18079v1</link><description>LLMs are seeing growing use for applications such as document analysis andsummarization which require large context windows, and with these large contextwindows KV cache activations surface as the dominant contributor to memoryconsumption during inference. Quantization is a promising approach forcompressing KV cache activations; however, existing solutions fail to representactivations accurately in ultra-low precisions, such as sub-4-bit. In thiswork, we present KVQuant, which addresses this problem by incorporating novelmethods for quantizing cached KV activations, including: (i) Per-Channel KeyQuantization, where we adjust the dimension along which we quantize the Keyactivations to better match the distribution; (ii) Pre-RoPE Key Quantization,where we quantize Key activations before the rotary positional embedding tomitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization,where we derive per-layer sensitivity-weighted non-uniform datatypes thatbetter represent the distributions; (iv) Per-Vector Dense-and-SparseQuantization, where we isolate outliers separately for each vector to minimizeskews in quantization ranges; and (v) Q-Norm, where we normalize quantizationcentroids in order to mitigate distribution shift, providing additionalbenefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2,and Mistral models, we achieve $&lt;0.1$ perplexity degradation with 3-bitquantization on both Wikitext-2 and C4, outperforming existing approaches. Ourmethod enables serving the LLaMA-7B model with a context length of up to 1million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.</description><author>Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami</author><pubDate>Wed, 31 Jan 2024 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18079v1</guid></item><item><title>CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting</title><link>http://arxiv.org/abs/2401.18075v1</link><description>We propose CARFF: Conditional Auto-encoded Radiance Field for 3D SceneForecasting, a method for predicting future 3D scenes given past observations,such as 2D ego-centric images. Our method maps an image to a distribution overplausible 3D latent scene configurations using a probabilistic encoder, andpredicts the evolution of the hypothesized scenes through time. Our latentscene representation conditions a global Neural Radiance Field (NeRF) torepresent a 3D scene model, which enables explainable predictions andstraightforward downstream applications. This approach extends beyond previousneural rendering work by considering complex scenarios of uncertainty inenvironmental states and dynamics. We employ a two-stage training ofPose-Conditional-VAE and NeRF to learn 3D representations. Additionally, weauto-regressively predict latent scene representations as a partiallyobservable Markov decision process, utilizing a mixture density network. Wedemonstrate the utility of our method in realistic scenarios using the CARLAdriving simulator, where CARFF can be used to enable efficient trajectory andcontingency planning in complex multi-agent autonomous driving scenariosinvolving visual occlusions.</description><author>Jiezhi Yang, Khushi Desai, Charles Packer, Harshil Bhatia, Nicholas Rhinehart, Rowan McAllister, Joseph Gonzalez</author><pubDate>Wed, 31 Jan 2024 18:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18075v1</guid></item><item><title>High-Quality Image Restoration Following Human Instructions</title><link>http://arxiv.org/abs/2401.16468v2</link><description>Image restoration is a fundamental problem that involves recovering ahigh-quality clean image from its degraded observation. All-In-One imagerestoration models can effectively restore images from various types and levelsof degradation using degradation-specific information as prompts to guide therestoration model. In this work, we present the first approach that useshuman-written instructions to guide the image restoration model. Given naturallanguage prompts, our model can recover high-quality images from their degradedcounterparts, considering multiple degradation types. Our method, InstructIR,achieves state-of-the-art results on several restoration tasks including imagedenoising, deraining, deblurring, dehazing, and (low-light) image enhancement.InstructIR improves +1dB over previous all-in-one restoration methods.Moreover, our dataset and results represent a novel benchmark for new researchon text-guided image restoration and enhancement. Our code, datasets and modelsare available at: https://github.com/mv-lab/InstructIR</description><author>Marcos V. Conde, Gregor Geigle, Radu Timofte</author><pubDate>Wed, 31 Jan 2024 18:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16468v2</guid></item><item><title>Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?</title><link>http://arxiv.org/abs/2401.18070v1</link><description>There is increasing interest in employing large language models (LLMs) ascognitive models. For such purposes, it is central to understand whichcognitive properties are well-modeled by LLMs, and which are not. In this work,we study the biases of LLMs in relation to those known in children when solvingarithmetic word problems. Surveying the learning science literature, we positthat the problem-solving process can be split into three distinct steps: textcomprehension, solution planning and solution execution. We construct tests foreach one in order to understand which parts of this process can be faithfullymodeled by current state-of-the-art LLMs. We generate a novel set of wordproblems for each of these tests, using a neuro-symbolic method that enablesfine-grained control over the problem features. We find evidence that LLMs,with and without instruction-tuning, exhibit human-like biases in both thetext-comprehension and the solution-planning steps of the solving process, butnot during the final step which relies on the problem's arithmetic expressions(solution execution).</description><author>Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan</author><pubDate>Wed, 31 Jan 2024 18:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18070v1</guid></item><item><title>Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models</title><link>http://arxiv.org/abs/2312.06712v2</link><description>Despite recent significant strides achieved by diffusion-based Text-to-Image(T2I) models, current systems are still less capable of ensuring decentcompositional generation aligned with text prompts, particularly for themulti-object generation. This work illuminates the fundamental reasons for suchmisalignment, pinpointing issues related to low attention activation scores andmask overlaps. While previous research efforts have individually tackled theseissues, we assert that a holistic approach is paramount. Thus, we propose twonovel objectives, the Separate loss and the Enhance loss, that reduce objectmask overlaps and maximize attention scores, respectively. Our method divergesfrom conventional test-time-adaptation techniques, focusing on finetuningcritical parameters, which enhances scalability and generalizability.Comprehensive evaluations demonstrate the superior performance of our model interms of image realism, text-image alignment, and adaptability, notablyoutperforming prominent baselines. Ultimately, this research paves the way forT2I diffusion models with enhanced compositional capacities and broaderapplicability.</description><author>Zhipeng Bao, Yijun Li, Krishna Kumar Singh, Yu-Xiong Wang, Martial Hebert</author><pubDate>Wed, 31 Jan 2024 18:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06712v2</guid></item><item><title>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</title><link>http://arxiv.org/abs/2401.18059v1</link><description>Retrieval-augmented language models can better adapt to changes in worldstate and incorporate long-tail knowledge. However, most existing methodsretrieve only short contiguous chunks from a retrieval corpus, limitingholistic understanding of the overall document context. We introduce the novelapproach of recursively embedding, clustering, and summarizing chunks of text,constructing a tree with differing levels of summarization from the bottom up.At inference time, our RAPTOR model retrieves from this tree, integratinginformation across lengthy documents at different levels of abstraction.Controlled experiments show that retrieval with recursive summaries offerssignificant improvements over traditional retrieval-augmented LMs on severaltasks. On question-answering tasks that involve complex, multi-step reasoning,we show state-of-the-art results; for example, by coupling RAPTOR retrievalwith the use of GPT-4, we can improve the best performance on the QuALITYbenchmark by 20% in absolute accuracy.</description><author>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning</author><pubDate>Wed, 31 Jan 2024 18:30:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18059v1</guid></item><item><title>LongAlign: A Recipe for Long Context Alignment of Large Language Models</title><link>http://arxiv.org/abs/2401.18058v1</link><description>Extending large language models to effectively handle long contexts requiresinstruction fine-tuning on input sequences of similar length. To address this,we present LongAlign -- a recipe of the instruction data, training, andevaluation for long context alignment. First, we construct a longinstruction-following dataset using Self-Instruct. To ensure the datadiversity, it covers a broad range of tasks from various long context sources.Second, we adopt the packing and sorted batching strategies to speed upsupervised fine-tuning on data with varied length distributions. Additionally,we develop a loss weighting method to balance the contribution to the lossacross different sequences during packing training. Third, we introduce theLongBench-Chat benchmark for evaluating instruction-following capabilities onqueries of 10k-100k in length. Experiments show that LongAlign outperformsexisting recipes for LLMs in long context tasks by up to 30\%, while alsomaintaining their proficiency in handling short, generic tasks. The code, data,and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.</description><author>Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li</author><pubDate>Wed, 31 Jan 2024 18:29:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18058v1</guid></item><item><title>Rank Supervised Contrastive Learning for Time Series Classification</title><link>http://arxiv.org/abs/2401.18057v1</link><description>Recently, various contrastive learning techniques have been developed tocategorize time series data and exhibit promising performance. A generalparadigm is to utilize appropriate augmentations and construct feasiblepositive samples such that the encoder can yield robust and discriminativerepresentations by mapping similar data points closer together in the featurespace while pushing dissimilar data points farther apart. Despite its efficacy,the fine-grained relative similarity (e.g., rank) information of positivesamples is largely ignored, especially when labeled samples are limited. Tothis end, we present Rank Supervised Contrastive Learning (RankSCL) to performtime series classification. Different from conventional contrastive learningframeworks, RankSCL augments raw data in a targeted way in the embedding spaceand adopts certain filtering rules to select more informative positive andnegative pairs of samples. Moreover, a novel rank loss is developed to assigndifferent weights for different levels of positive samples, enable the encoderto extract the fine-grained information of the same class, and produce a clearboundary among different classes. Thoroughly empirical studies on 128 UCRdatasets and 30 UEA datasets demonstrate that the proposed RankSCL can achievestate-of-the-art performance compared to existing baseline methods.</description><author>Qianying Ren, Dongsheng Luo, Dongjin Song</author><pubDate>Wed, 31 Jan 2024 18:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18057v1</guid></item><item><title>Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition</title><link>http://arxiv.org/abs/2401.18054v1</link><description>Continual learning (CL) is the research field that aims to build machinelearning models that can accumulate knowledge continuously over different taskswithout retraining from scratch. Previous studies have shown that pre-traininggraph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)after fine-tuning, a setting which is closely related to CL. Thus, we focus onstudying GNN in the continual graph learning (CGL) setting. We propose thefirst continual graph learning benchmark for spatio-temporal graphs and use itto benchmark well-known CGL methods in this novel setting. The benchmark isbased on the N-UCLA and NTU-RGB+D datasets for skeleton-based actionrecognition. Beyond benchmarking for standard performance metrics, we study theclass and task-order sensitivity of CGL methods, i.e., the impact of learningorder on each class/task's performance, and the architectural sensitivity ofCGL methods with backbone GNN at various widths and depths. We reveal thattask-order robust methods can still be class-order sensitive and observeresults that contradict previous empirical observations on architecturalsensitivity in CL.</description><author>Wei Wei, Tom De Schepper, Kevin Mets</author><pubDate>Wed, 31 Jan 2024 18:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18054v1</guid></item><item><title>Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery and Geographical Prior</title><link>http://arxiv.org/abs/2401.06550v2</link><description>Urban area-of-interest (AOI) refers to an integrated urban functional zonewith defined boundaries. The rapid development of urban commerce has resultedin an increased demand for more precise requirements in defining AOIs. However,existing research primarily concentrates on broad AOI mining for urban planningor regional economic analysis, failing to cater to the precise requirements ofmobile Internet online-to-offline businesses. These businesses necessitateaccuracy down to a specific community, school, or hospital. In this paper, wepropose an end-to-end multimodal deep learning algorithm for detecting AOIfence polygon using remote sensing images and multi-semantics referenceinformation. We then evaluate its timeliness through a cascaded module thatincorporates dynamic human mobility and logistics address information.Specifically, we begin by selecting a point-of-interest (POI) of specificcategory, and use it to recall corresponding remote sensing images, nearbyPOIs, road nodes, human mobility, and logistics addresses to build a multimodaldetection model based on transformer encoder-decoder architecture, titledAOITR. In the model, in addition to the remote sensing images, multi-semanticinformation including core POI and road nodes is embedded and reorganized asthe query content part for the transformer decoder to generate the AOI polygon.Meanwhile, relatively dynamic distribution features of human mobility, nearbyPOIs, and logistics addresses are used for AOI reliability evaluation through acascaded feedforward network. The experimental results demonstrate that ouralgorithm significantly outperforms two existing methods.</description><author>Chuanji Shi, Yingying Zhang, Jiaotuan Wang, Xin Guo, Qiqi Zhu</author><pubDate>Wed, 31 Jan 2024 18:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06550v2</guid></item><item><title>Epidemic Modeling using Hybrid of Time-varying SIRD, Particle Swarm Optimization, and Deep Learning</title><link>http://arxiv.org/abs/2401.18047v1</link><description>Epidemiological models are best suitable to model an epidemic if the spreadpattern is stationary. To deal with non-stationary patterns and multiple wavesof an epidemic, we develop a hybrid model encompassing epidemic modeling,particle swarm optimization, and deep learning. The model mainly caters tothree objectives for better prediction: 1. Periodic estimation of the modelparameters. 2. Incorporating impact of all the aspects using data fitting andparameter optimization 3. Deep learning based prediction of the modelparameters. In our model, we use a system of ordinary differential equations(ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling,Particle Swarm Optimization (PSO) for model parameter optimization, andstacked-LSTM for forecasting the model parameters. Initial or one timeestimation of model parameters is not able to model multiple waves of anepidemic. So, we estimate the model parameters periodically (weekly). We usePSO to identify the optimum values of the model parameters. We next train thestacked-LSTM on the optimized parameters, and perform forecasting of the modelparameters for upcoming four weeks. Further, we fed the LSTM forecastedparameters into the SIRD model to forecast the number of COVID-19 cases. Weevaluate the model for highly affected three countries namely; the USA, India,and the UK. The proposed hybrid model is able to deal with multiple waves, andhas outperformed existing methods on all the three datasets.</description><author>Naresh Kumar, Seba Susan</author><pubDate>Wed, 31 Jan 2024 18:08:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18047v1</guid></item><item><title>Multipath parsing in the brain</title><link>http://arxiv.org/abs/2401.18046v1</link><description>Humans understand sentences word-by-word, in the order that they hear them.This incrementality entails resolving temporary ambiguities about syntacticrelationships. We investigate how humans process these syntactic ambiguities bycorrelating predictions from incremental generative dependency parsers withtimecourse data from people undergoing functional neuroimaging while listeningto an audiobook. In particular, we compare competing hypotheses regarding thenumber of developing syntactic analyses in play during word-by-wordcomprehension: one vs more than one. This comparison involves evaluatingsyntactic surprisal from a state-of-the-art dependency parser with LLM-adaptedencodings against an existing fMRI dataset. In both English and Chinese data,we find evidence for multipath parsing. Brain regions associated with thismultipath effect include bilateral superior temporal gyrus.</description><author>Berta Franzluebbers, Donald Dunagan, Miloš Stanojević, Jan Buys, John T. Hale</author><pubDate>Wed, 31 Jan 2024 18:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18046v1</guid></item><item><title>SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition</title><link>http://arxiv.org/abs/2401.18045v1</link><description>Recent advancements in language models have significantly enhancedperformance in multiple speech-related tasks. Existing speech language modelstypically utilize task-dependent prompt tokens to unify various speech tasks ina single model. However, this design omits the intrinsic connections betweendifferent speech tasks, which can potentially boost the performance of eachtask. In this work, we propose a novel decoder-only speech language model,SpeechComposer, that can unify common speech tasks by composing a fixed set ofprompt tokens. Built upon four primary tasks -- speech synthesis, speechrecognition, speech language modeling, and text language modeling --SpeechComposer can easily extend to more speech tasks via compositions ofwell-designed prompt tokens, like voice conversion and speech enhancement. Theunification of prompt tokens also makes it possible for knowledge sharing amongdifferent speech tasks in a more structured manner. Experimental resultsdemonstrate that our proposed SpeechComposer can improve the performance ofboth primary tasks and composite tasks, showing the effectiveness of the sharedprompt tokens. Remarkably, the unified decoder-only model achieves a comparableand even better performance than the baselines which are expert models designedfor single tasks.</description><author>Yihan Wu, Soumi Maiti, Yifan Peng, Wangyou Zhang, Chenda Li, Yuyue Wang, Xihua Wang, Shinji Watanabe, Ruihua Song</author><pubDate>Wed, 31 Jan 2024 18:06:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18045v1</guid></item><item><title>Privacy Risks Analysis and Mitigation in Federated Learning for Medical Images</title><link>http://arxiv.org/abs/2311.06643v2</link><description>Federated learning (FL) is gaining increasing popularity in the medicaldomain for analyzing medical images, which is considered an effective techniqueto safeguard sensitive patient data and comply with privacy regulations.However, several recent studies have revealed that the default settings of FLmay leak private training data under privacy attacks. Thus, it is still unclearwhether and to what extent such privacy risks of FL exist in the medicaldomain, and if so, "how to mitigate such risks?". In this paper, first, wepropose a holistic framework for Medical data Privacy risk analysis andmitigation in Federated Learning (MedPFL) to analyze privacy risks and developeffective mitigation strategies in FL for protecting private medical data.Second, we demonstrate the substantial privacy risks of using FL to processmedical images, where adversaries can easily perform privacy attacks toreconstruct private medical images accurately. Third, we show that the defenseapproach of adding random noises may not always work effectively to protectmedical images against privacy attacks in FL, which poses unique and pressingchallenges associated with medical data for privacy protection.</description><author>Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu</author><pubDate>Wed, 31 Jan 2024 18:06:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06643v2</guid></item><item><title>Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability</title><link>http://arxiv.org/abs/2401.18040v1</link><description>End-to-end multi-task dialogue systems are usually designed with separatemodules for the dialogue pipeline. Among these, the policy module is essentialfor deciding what to do in response to user input. This policy is trained byreinforcement learning algorithms by taking advantage of an environment inwhich an agent receives feedback in the form of a reward signal. The currentdialogue systems, however, only provide meagre and simplistic rewards.Investigating intrinsic motivation reinforcement learning algorithms is thegoal of this study. Through this, the agent can quickly accelerate training andimprove its capacity to judge the quality of its actions by teaching it aninternal incentive system. In particular, we adapt techniques for randomnetwork distillation and curiosity-driven reinforcement learning to measure thefrequency of state visits and encourage exploration by using semanticsimilarity between utterances. Experimental results on MultiWOZ, aheterogeneous dataset, show that intrinsic motivation-based debate systemsoutperform policies that depend on extrinsic incentives. By adopting randomnetwork distillation, for example, which is trained using semantic similaritybetween user-system dialogues, an astounding average success rate of 73% isachieved. This is a significant improvement over the baseline Proximal PolicyOptimization (PPO), which has an average success rate of 60%. In addition,performance indicators such as booking rates and completion rates show a 10%rise over the baseline. Furthermore, these intrinsic incentive models helpimprove the system's policy's resilience in an increasing amount of domains.This implies that they could be useful in scaling up to settings that cover awider range of domains.</description><author>Navin Kamuni, Hardik Shah, Sathishkumar Chintala, Naveen Kunchakuri, Sujatha Alla Old Dominion</author><pubDate>Wed, 31 Jan 2024 18:03:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18040v1</guid></item><item><title>Variable selection for Naïve Bayes classification</title><link>http://arxiv.org/abs/2401.18039v1</link><description>The Na\"ive Bayes has proven to be a tractable and efficient method forclassification in multivariate analysis. However, features are usuallycorrelated, a fact that violates the Na\"ive Bayes' assumption of conditionalindependence, and may deteriorate the method's performance. Moreover, datasetsare often characterized by a large number of features, which may complicate theinterpretation of the results as well as slow down the method's execution. In this paper we propose a sparse version of the Na\"ive Bayes classifierthat is characterized by three properties. First, the sparsity is achievedtaking into account the correlation structure of the covariates. Second,different performance measures can be used to guide the selection of features.Third, performance constraints on groups of higher interest can be included.Our proposal leads to a smart search, which yields competitive running times,whereas the flexibility in terms of performance measure for classification isintegrated. Our findings show that, when compared against well-referencedfeature selection approaches, the proposed sparse Na\"ive Bayes obtainscompetitive results regarding accuracy, sparsity and running times for balanceddatasets. In the case of datasets with unbalanced (or with differentimportance) classes, a better compromise between classification rates for thedifferent classes is achieved.</description><author>Rafael Blanquero, Emilio Carrizosa, Pepa Ramírez-Cobo, M. Remedios Sillero-Denamiel</author><pubDate>Wed, 31 Jan 2024 18:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18039v1</guid></item><item><title>Optimizing contrastive learning for cortical folding pattern detection</title><link>http://arxiv.org/abs/2401.18035v1</link><description>The human cerebral cortex has many bumps and grooves called gyri and sulci.Even though there is a high inter-individual consistency for the main corticalfolds, this is not the case when we examine the exact shapes and details of thefolding patterns. Because of this complexity, characterizing the corticalfolding variability and relating them to subjects' behavioral characteristicsor pathologies is still an open scientific problem. Classical approachesinclude labeling a few specific patterns, either manually orsemi-automatically, based on geometric distances, but the recent availabilityof MRI image datasets of tens of thousands of subjects makes moderndeep-learning techniques particularly attractive. Here, we build aself-supervised deep-learning model to detect folding patterns in the cingulateregion. We train a contrastive self-supervised model (SimCLR) on both HumanConnectome Project (1101 subjects) and UKBioBank (21070 subjects) datasets withtopological-based augmentations on the cortical skeletons, which aretopological objects that capture the shape of the folds. We explore severalbackbone architectures (convolutional network, DenseNet, and PointNet) for theSimCLR. For evaluation and testing, we perform a linear classification task ona database manually labeled for the presence of the "double-parallel" foldingpattern in the cingulate region, which is related to schizophreniacharacteristics. The best model, giving a test AUC of 0.76, is a convolutionalnetwork with 6 layers, a 10-dimensional latent space, a linear projection head,and using the branch-clipping augmentation. This is the first time that aself-supervised deep learning model has been applied to cortical skeletons onsuch a large dataset and quantitatively evaluated. We can now envisage the nextstep: applying it to other brain regions to detect other biomarkers.</description><author>Aymeric Gaudin, Louise Guillon, Clara Fischer, Arnaud Cachia, Denis Rivière, Jean-François Mangin, Joël Chavas</author><pubDate>Wed, 31 Jan 2024 17:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18035v1</guid></item><item><title>Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models</title><link>http://arxiv.org/abs/2401.18034v1</link><description>We present Gyan AI Paramanu ("atom"), a family of novel language models forIndian languages. It is a collection of auto-regressive monolingual, bilingual,and multilingual Indic language models pretrained from scratch on a single GPUfor 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models arepretrained with a context size of 1024 on a single GPU. The models are veryefficient, small, fast, and powerful. We have also developed an efficient mostadvanced Indic tokenizer that can even tokenize unseen languages. In order toavoid the "curse of multi-linguality" in our multilingual mParamanu model, wepretrained on comparable corpora by typological grouping using the same script.We performed human evaluation of our pretrained models for open end textgeneration on grammar, coherence, creativity, and factuality metrics forBangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit modelsoutperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,GPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despitebeing smaller in size by 66 to 20 times compared to standard 7B LLMs. To runinference on our pretrained models, CPU is enough, and GPU is not needed. Wealso instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugumodels on 23k instructions in respective languages. Our pretrained andinstruction-tuned models which are first of its kind, most powerful efficientsmall generative language models ever developed for Indic languages, and thevarious results lead to the conclusion that high quality generative languagemodels are possible without high amount of compute power and humongous numberof parameters. We plan to release our models at https://www.bharatgpts.com.</description><author>Mitodru Niyogi, Arnab Bhattacharya</author><pubDate>Wed, 31 Jan 2024 17:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18034v1</guid></item><item><title>Deep Network Approximation: Beyond ReLU to Diverse Activation Functions</title><link>http://arxiv.org/abs/2307.06555v5</link><description>This paper explores the expressive power of deep neural networks for adiverse range of activation functions. An activation function set $\mathscr{A}$is defined to encompass the majority of commonly used activation functions,such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$,$\mathtt{ELU}$, $\mathtt{CELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$,$\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$,$\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$,$\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activationfunction $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ anddepth $L$ can be approximated to arbitrary precision by a $\varrho$-activatednetwork of width $3N$ and depth $2L$ on any bounded set. This finding enablesthe extension of most approximation results achieved with $\mathtt{ReLU}$networks to a wide variety of other activation functions, albeit with slightlyincreased constants. Significantly, we establish that the (width,$\,$depth)scaling factors can be further reduced from $(3,2)$ to $(1,1)$ if $\varrho$falls within a specific subset of $\mathscr{A}$. This subset includesactivation functions such as $\mathtt{ELU}$, $\mathtt{CELU}$, $\mathtt{SELU}$,$\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, and$\mathtt{Mish}$.</description><author>Shijun Zhang, Jianfeng Lu, Hongkai Zhao</author><pubDate>Wed, 31 Jan 2024 17:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06555v5</guid></item><item><title>DROP: Decouple Re-Identification and Human Parsing with Task-specific Features for Occluded Person Re-identification</title><link>http://arxiv.org/abs/2401.18032v1</link><description>The paper introduces the Decouple Re-identificatiOn and human Parsing (DROP)method for occluded person re-identification (ReID). Unlike mainstreamapproaches using global features for simultaneous multi-task learning of ReIDand human parsing, or relying on semantic information for attention guidance,DROP argues that the inferior performance of the former is due to distinctgranularity requirements for ReID and human parsing features. ReID focuses oninstance part-level differences between pedestrian parts, while human parsingcenters on semantic spatial context, reflecting the internal structure of thehuman body. To address this, DROP decouples features for ReID and humanparsing, proposing detail-preserving upsampling to combine varying resolutionfeature maps. Parsing-specific features for human parsing are decoupled, andhuman position information is exclusively added to the human parsing branch. Inthe ReID branch, a part-aware compactness loss is introduced to enhanceinstance-level part differences. Experimental results highlight the efficacy ofDROP, especially achieving a Rank-1 accuracy of 76.8% on Occluded-Duke,surpassing two mainstream methods. The codebase is accessible athttps://github.com/shuguang-52/DROP.</description><author>Shuguang Dou, Xiangyang Jiang, Yuanpeng Tu, Junyao Gao, Zefan Qu, Qingsong Zhao, Cairong Zhao</author><pubDate>Wed, 31 Jan 2024 17:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18032v1</guid></item><item><title>Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI</title><link>http://arxiv.org/abs/2401.18028v1</link><description>Anticipating the negative impacts of emerging AI technologies is a challenge,especially in the early stages of development. An understudied approach to suchanticipation is the use of LLMs to enhance and guide this process. Despiteadvancements in LLMs and evaluation metrics to account for biases in generatedtext, it is unclear how well these models perform in anticipatory tasks.Specifically, the use of LLMs to anticipate AI impacts raises questions aboutthe quality and range of categories of negative impacts these models arecapable of generating. In this paper we leverage news media, a diverse datasource that is rich with normative assessments of emerging technologies, toformulate a taxonomy of impacts to act as a baseline for comparing against. Bycomputationally analyzing thousands of news articles published by hundreds ofonline news domains around the world, we develop a taxonomy consisting of tencategories of AI impacts. We then evaluate both instruction-based (GPT-4 andMistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3)using a sample from this baseline. We find that the generated impacts usingMistral-7B, fine-tuned on impacts from the news media, tend to be qualitativelyon par with impacts generated using a larger scale model such as GPT-4.Moreover, we find that these LLMs generate impacts that largely reflect thetaxonomy of negative impacts identified in the news media, however the impactsproduced by instruction-based models had gaps in the production of certaincategories of impacts in comparison to fine-tuned models. This researchhighlights a potential bias in state-of-the-art LLMs when used for anticipatingimpacts and demonstrates the advantages of aligning smaller LLMs with a diverserange of impacts, such as those reflected in the news media, to better reflectsuch impacts during anticipatory exercises.</description><author>Mowafak Allaham, Nicholas Diakopoulos</author><pubDate>Wed, 31 Jan 2024 17:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18028v1</guid></item><item><title>Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization</title><link>http://arxiv.org/abs/2401.15496v2</link><description>Large language models (LLMs) like Llama, Baichuan and Bloom models showremarkable ability with instruction fine-tuning in many natural language tasks.Nevertheless, for the dialogue summarization task, which aims to generatesummaries for different roles in dialogue, most of the state-of-the-art methodsconduct on small models (e.g Bart and Bert). Existing methods try to add taskspecified optimization on small models like adding global-local centralityscore to models. In this paper, we propose an instruction fine-tuning model:Baichuan2-Sum, for role-oriented diaglouge summarization. By setting differentinstructions for different roles, the model can learn from the dialogueinteractions and output the expected summaries. Furthermore, we applied NEFTunetechnique to add suitable noise during training to improve the results. Theexperiments demonstrate that the proposed model achieves the newstate-of-the-art results on two public dialogue summarization datasets: CSDSand SAMSUM. We release our model and related codes to facilitate future studieson dialogue summarization task.</description><author>Jianfei Xiao, Yancan Chen, Yimin Ou, Hanyi Yu, Yiyong Xiao</author><pubDate>Wed, 31 Jan 2024 17:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15496v2</guid></item><item><title>A cost-sensitive constrained Lasso</title><link>http://arxiv.org/abs/2401.18023v1</link><description>The Lasso has become a benchmark data analysis procedure, and numerousvariants have been proposed in the literature. Although the Lasso formulationsare stated so that overall prediction error is optimized, no full control overthe accuracy prediction on certain individuals of interest is allowed. In thiswork we propose a novel version of the Lasso in which quadratic performanceconstraints are added to Lasso-based objective functions, in such a way thatthreshold values are set to bound the prediction errors in the different groupsof interest (not necessarily disjoint). As a result, a constrained sparseregression model is defined by a nonlinear optimization problem. Thiscost-sensitive constrained Lasso has a direct application in heterogeneoussamples where data are collected from distinct sources, as it is standard inmany biomedical contexts. Both theoretical properties and empirical studiesconcerning the new method are explored in this paper. In addition, twoillustrations of the method on biomedical and sociological contexts areconsidered.</description><author>Rafael Blanquero, Emilio Carrizosa, Pepa Ramírez-Cobo, M. Remedios Sillero-Denamiel</author><pubDate>Wed, 31 Jan 2024 17:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18023v1</guid></item><item><title>Domain-Generalizable Multiple-Domain Clustering</title><link>http://arxiv.org/abs/2301.13530v2</link><description>This work generalizes the problem of unsupervised domain generalization tothe case in which no labeled samples are available (completely unsupervised).We are given unlabeled samples from multiple source domains, and we aim tolearn a shared predictor that assigns examples to semantically relatedclusters. Evaluation is done by predicting cluster assignments in previouslyunseen domains. Towards this goal, we propose a two-stage training framework:(1) self-supervised pre-training for extracting domain invariant semanticfeatures. (2) multi-head cluster prediction with pseudo labels, which rely onboth the feature space and cluster head prediction, further leveraging a novelprediction-based label smoothing scheme. We demonstrate empirically that ourmodel is more accurate than baselines that require fine-tuning using samplesfrom the target domain or some level of supervision. Our code is available athttps://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering.</description><author>Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum</author><pubDate>Wed, 31 Jan 2024 17:29:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13530v2</guid></item><item><title>Prompt-Driven LLM Safeguarding via Directed Representation Optimization</title><link>http://arxiv.org/abs/2401.18018v1</link><description>Prepending model inputs with safety prompts is a common practice ofsafeguarding large language models (LLMs) from complying with queries thatcontain harmful intents. However, the working mechanisms of safety prompts havenot yet been fully understood, which hinders the potential for automaticallyoptimizing them for improved LLM safety. Motivated by this problem, weinvestigate the impact of safety prompts from the perspective of modelrepresentations. We find that in models' representation space, harmful andharmless queries can be largely distinguished, but this is not noticeablyenhanced by safety prompts. Instead, the queries' representations are moved bydifferent safety prompts in similar directions, where models become more proneto refusal (i.e., refusing to provide assistance) even when the queries areharmless. Inspired by these findings, we propose a method called DRO (DirectedRepresentation Optimization) for automatic safety prompt optimization. DROtreats safety prompts as continuous, trainable embeddings and learns to movethe representations of harmful/harmless queries along/opposite the direction inwhich the model's refusal probability increases. We demonstrate that DROremarkably improves the safeguarding performance of human-crafted safetyprompts and outperforms strong baselines, as evaluated on out-of-domainbenchmarks, without compromising the general model capability.</description><author>Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, Nanyun Peng</author><pubDate>Wed, 31 Jan 2024 17:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18018v1</guid></item><item><title>Causal Discovery by Kernel Deviance Measures with Heterogeneous Transforms</title><link>http://arxiv.org/abs/2401.18017v1</link><description>The discovery of causal relationships in a set of random variables is afundamental objective of science and has also recently been argued as being anessential component towards real machine intelligence. One class of causaldiscovery techniques are founded based on the argument that there are inherentstructural asymmetries between the causal and anti-causal direction which couldbe leveraged in determining the direction of causation. To go about capturingthese discrepancies between cause and effect remains to be a challenge and manycurrent state-of-the-art algorithms propose to compare the norms of the kernelmean embeddings of the conditional distributions. In this work, we argue thatsuch approaches based on RKHS embeddings are insufficient in capturingprincipal markers of cause-effect asymmetry involving higher-order structuralvariabilities of the conditional distributions. We propose Kernel IntrinsicInvariance Measure with Heterogeneous Transform (KIIM-HT) which introduces anovel score measure based on heterogeneous transformation of RKHS embeddings toextract relevant higher-order moments of the conditional densities for causaldiscovery. Inference is made via comparing the score of each hypotheticalcause-effect direction. Tests and comparisons on a synthetic dataset, atwo-dimensional synthetic dataset and the real-world benchmark datasetT\"ubingen Cause-Effect Pairs verify our approach. In addition, we conduct asensitivity analysis to the regularization parameter to faithfully compareprevious work to our method and an experiment with trials on variedhyperparameter values to showcase the robustness of our algorithm.</description><author>Tim Tse, Zhitang Chen, Shengyu Zhu, Yue Liu</author><pubDate>Wed, 31 Jan 2024 17:28:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18017v1</guid></item><item><title>Causal Coordinated Concurrent Reinforcement Learning</title><link>http://arxiv.org/abs/2401.18012v1</link><description>In this work, we propose a novel algorithmic framework for data sharing andcoordinated exploration for the purpose of learning more data-efficient andbetter performing policies under a concurrent reinforcement learning (CRL)setting. In contrast to other work which make the assumption that all agentsact under identical environments, we relax this restriction and insteadconsider the formulation where each agent acts within an environment whichshares a global structure but also exhibits individual variations. Ouralgorithm leverages a causal inference algorithm in the form of Additive NoiseModel - Mixture Model (ANM-MM) in extracting model parameters governingindividual differentials via independence enforcement. We propose a new datasharing scheme based on a similarity measure of the extracted model parametersand demonstrate superior learning speeds on a set of autoregressive, pendulumand cart-pole swing-up tasks and finally, we show the effectiveness of diverseaction selection between common agents under a sparse reward setting. To thebest of our knowledge, this is the first work in considering non-identicalenvironments in CRL and one of the few works which seek to integrate causalinference with reinforcement learning (RL).</description><author>Tim Tse, Isaac Chan, Zhitang Chen</author><pubDate>Wed, 31 Jan 2024 17:20:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18012v1</guid></item><item><title>EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation</title><link>http://arxiv.org/abs/2401.18006v1</link><description>In conventional machine learning (ML) approaches applied toelectroencephalography (EEG), this is often a limited focus, isolating specificbrain activities occurring across disparate temporal scales (from transientspikes in milliseconds to seizures lasting minutes) and spatial scales (fromlocalized high-frequency oscillations to global sleep activity). This siloedapproach limits the development EEG ML models that exhibit multi-scaleelectrophysiological understanding and classification capabilities. Moreover,typical ML EEG approaches utilize black-box approaches, limiting theirinterpretability and trustworthiness in clinical contexts. Thus, we proposeEEG-GPT, a unifying approach to EEG classification that leverages advances inlarge language models (LLM). EEG-GPT achieves excellent performance comparableto current state-of-the-art deep learning methods in classifying normal fromabnormal EEG in a few-shot learning paradigm utilizing only 2% of trainingdata. Furthermore, it offers the distinct advantages of providing intermediatereasoning steps and coordinating specialist EEG tools across multiple scales inits operation, offering transparent and interpretable step-by-stepverification, thereby promoting trustworthiness in clinical contexts.</description><author>Jonathan W. Kim, Ahmed Alaa, Danilo Bernardo</author><pubDate>Wed, 31 Jan 2024 17:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18006v1</guid></item><item><title>Desiderata for the Context Use of Question Answering Systems</title><link>http://arxiv.org/abs/2401.18001v1</link><description>Prior work has uncovered a set of common problems in state-of-the-artcontext-based question answering (QA) systems: a lack of attention to thecontext when the latter conflicts with a model's parametric knowledge, littlerobustness to noise, and a lack of consistency with their answers. However,most prior work focus on one or two of those problems in isolation, which makesit difficult to see trends across them. We aim to close this gap, by firstoutlining a set of -- previously discussed as well as novel -- desiderata forQA models. We then survey relevant analysis and methods papers to provide anoverview of the state of the field. The second part of our work presentsexperiments where we evaluate 15 QA systems on 5 datasets according to alldesiderata at once. We find many novel trends, including (1) systems that areless susceptible to noise are not necessarily more consistent with theiranswers when given irrelevant context; (2) most systems that are moresusceptible to noise are more likely to correctly answer according to a contextthat conflicts with their parametric knowledge; and (3) the combination ofconflicting knowledge and noise can reduce system performance by up to 96%. Assuch, our desiderata help increase our understanding of how these models workand reveal potential avenues for improvements.</description><author>Sagi Shaier, Lawrence E Hunter, Katharina von der Wense</author><pubDate>Wed, 31 Jan 2024 17:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.18001v1</guid></item><item><title>Multilinear Operator Networks</title><link>http://arxiv.org/abs/2401.17992v1</link><description>Despite the remarkable capabilities of deep neural networks in imagerecognition, the dependence on activation functions remains a largelyunexplored area and has yet to be eliminated. On the other hand, PolynomialNetworks is a class of models that does not require activation functions, buthave yet to perform on par with modern architectures. In this work, we aimclose this gap and propose MONet, which relies solely on multilinear operators.The core layer of MONet, called Mu-Layer, captures multiplicative interactionsof the elements of the input token. MONet captures high-degree interactions ofthe input elements and we demonstrate the efficacy of our approach on a seriesof image recognition and scientific computing benchmarks. The proposed modeloutperforms prior polynomial networks and performs on par with modernarchitectures. We believe that MONet can inspire further research on modelsthat use entirely multilinear operations.</description><author>Yixin Cheng, Grigorios G. Chrysos, Markos Georgopoulos, Volkan Cevher</author><pubDate>Wed, 31 Jan 2024 16:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17992v1</guid></item><item><title>A Deep Learning-based Global and Segmentation-based Semantic Feature Fusion Approach for Indoor Scene Classification</title><link>http://arxiv.org/abs/2302.06432v3</link><description>This work proposes a novel approach that uses a semantic segmentation mask toobtain a 2D spatial layout of the segmentation-categories across the scene,designated by segmentation-based semantic features (SSFs). These featuresrepresent, per segmentation-category, the pixel count, as well as the 2Daverage position and respective standard deviation values. Moreover, atwo-branch network, GS2F2App, that exploits CNN-based global features extractedfrom RGB images and the segmentation-based features extracted from the proposedSSFs, is also proposed. GS2F2App was evaluated in two indoor scene benchmarkdatasets: the SUN RGB-D and the NYU Depth V2, achieving state-of-the-artresults on both datasets.</description><author>Ricardo Pereira, Tiago Barros, Luis Garrote, Ana Lopes, Urbano J. Nunes</author><pubDate>Wed, 31 Jan 2024 16:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06432v3</guid></item><item><title>Shrub of a thousand faces: an individual segmentation from satellite images using deep learning</title><link>http://arxiv.org/abs/2401.17985v1</link><description>Monitoring the distribution and size structure of long-living shrubs, such asJuniperus communis, can be used to estimate the long-term effects of climatechange on high-mountain and high latitude ecosystems. Historical aerialvery-high resolution imagery offers a retrospective tool to monitor shrubgrowth and distribution at high precision. Currently, deep learning modelsprovide impressive results for detecting and delineating the contour of objectswith defined shapes. However, adapting these models to detect natural objectsthat express complex growth patterns, such as junipers, is still a challengingtask. This research presents a novel approach that leverages remotely sensed RGBimagery in conjunction with Mask R-CNN-based instance segmentation models toindividually delineate Juniperus shrubs above the treeline in Sierra Nevada(Spain). In this study, we propose a new data construction design that consistsin using photo interpreted (PI) and field work (FW) data to respectivelydevelop and externally validate the model. We also propose a new shrub-tailoredevaluation algorithm based on a new metric called Multiple Intersections overGround Truth Area (MIoGTA) to assess and optimize the model shrub delineationperformance. Finally, we deploy the developed model for the first time togenerate a wall-to-wall map of Juniperus individuals. The experimental results demonstrate the efficiency of our dual dataconstruction approach in overcoming the limitations associated with traditionalfield survey methods. They also highlight the robustness of MIoGTA metric inevaluating instance segmentation models on species with complex growth patternsshowing more resilience against data annotation uncertainty. Furthermore, theyshow the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone indelineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%,respectively.</description><author>Rohaifa Khaldi, Siham Tabik, Sergio Puertas-Ruiz, Julio Peñas de Giles, José Antonio Hódar Correa, Regino Zamora, Domingo Alcaraz Segura</author><pubDate>Wed, 31 Jan 2024 16:44:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17985v1</guid></item><item><title>Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study</title><link>http://arxiv.org/abs/2401.17981v1</link><description>Despite the impressive capabilities of Multimodal Large Language Models(MLLMs) in integrating text and image modalities, challenges remain inaccurately interpreting detailed visual elements. This paper presents anempirical study on enhancing MLLMs with state-of-the-art (SOTA) objectdetection and Optical Character Recognition models to improve fine-grainedimage understanding and reduce hallucination in responses. Our researchinvestigates the embedding-based infusion of detection information, the impactof such infusion on the MLLMs' original abilities, and the interchangeabilityof detection models. We conduct systematic experiments with models such asLLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refinesMLLMs' performance in specific visual tasks but also maintains their originalstrengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10benchmarks, achieving an improvement of up to 12.99% on the normalized averagescore, marking a notable advancement in multimodal understanding. We releaseour codes to facilitate further exploration into the fine-grained multimodaldialogue capabilities of MLLMs.</description><author>Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen</author><pubDate>Wed, 31 Jan 2024 16:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17981v1</guid></item><item><title>Entity Linking in the Job Market Domain</title><link>http://arxiv.org/abs/2401.17979v1</link><description>In Natural Language Processing, entity linking (EL) has centered aroundWikipedia, but yet remains underexplored for the job market domain.Disambiguating skill mentions can help us get insight into the current labormarket demands. In this work, we are the first to explore EL in this domain,specifically targeting the linkage of occupational skills to the ESCO taxonomy(le Vrang et al., 2014). Previous efforts linked coarse-grained (full)sentences to a corresponding ESCO skill. In this work, we link morefine-grained span-level mentions of skills. We tune two high-performing neuralEL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao etal., 2021), on a synthetically generated mention--skill pair dataset andevaluate them on a human-annotated skill-linking benchmark. Our findings revealthat both models are capable of linking implicit mentions of skills to theircorrect taxonomy counterparts. Empirically, BLINK outperforms GENRE in strictevaluation, but GENRE performs better in loose evaluation (accuracy@$k$).</description><author>Mike Zhang, Rob van der Goot, Barbara Plank</author><pubDate>Wed, 31 Jan 2024 16:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17979v1</guid></item><item><title>Circuit Partitioning for Multi-Core Quantum Architectures with Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2401.17976v1</link><description>Quantum computing holds immense potential for solving classically intractableproblems by leveraging the unique properties of quantum mechanics. Thescalability of quantum architectures remains a significant challenge.Multi-core quantum architectures are proposed to solve the scalability problem,arising a new set of challenges in hardware, communications and compilation,among others. One of these challenges is to adapt a quantum algorithm to fitwithin the different cores of the quantum computer. This paper presents a novelapproach for circuit partitioning using Deep Reinforcement Learning,contributing to the advancement of both quantum computing and graphpartitioning. This work is the first step in integrating Deep ReinforcementLearning techniques into Quantum Circuit Mapping, opening the door to a newparadigm of solutions to such problems.</description><author>Arnau Pastor, Pau Escofet, Sahar Ben Rached, Eduard Alarcón, Pere Barlet-Ros, Sergi Abadal</author><pubDate>Wed, 31 Jan 2024 16:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17976v1</guid></item><item><title>Understanding polysemanticity in neural networks through coding theory</title><link>http://arxiv.org/abs/2401.17975v1</link><description>Despite substantial efforts, neural network interpretability remains anelusive goal, with previous research failing to provide succinct explanationsof most single neurons' impact on the network output. This limitation is due tothe polysemantic nature of most neurons, whereby a given neuron is involved inmultiple unrelated network states, complicating the interpretation of thatneuron. In this paper, we apply tools developed in neuroscience and informationtheory to propose both a novel practical approach to network interpretabilityand theoretical insights into polysemanticity and the density of codes. Weinfer levels of redundancy in the network's code by inspecting theeigenspectrum of the activation's covariance matrix. Furthermore, we show howrandom projections can reveal whether a network exhibits a smooth ornon-differentiable code and hence how interpretable the code is. This sameframework explains the advantages of polysemantic neurons to learningperformance and explains trends found in recent results by Elhage etal.~(2022). Our approach advances the pursuit of interpretability in neuralnetworks, providing insights into their underlying structure and suggesting newavenues for circuit-level interpretability.</description><author>Simon C. Marshall, Jan H. Kirchner</author><pubDate>Wed, 31 Jan 2024 16:31:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17975v1</guid></item><item><title>GUMsley: Evaluating Entity Salience in Summarization for 12 English Genres</title><link>http://arxiv.org/abs/2401.17974v1</link><description>As NLP models become increasingly capable of understanding documents in termsof coherent entities rather than strings, obtaining the most salient entitiesfor each document is not only an important end task in itself but also vitalfor Information Retrieval (IR) and other downstream applications such ascontrollable summarization. In this paper, we present and evaluate GUMsley, thefirst entity salience dataset covering all named and non-named salient entitiesfor 12 genres of English text, aligned with entity types, Wikification linksand full coreference resolution annotations. We promote a strict definition ofsalience using human summaries and demonstrate high inter-annotator agreementfor salience based on whether a source entity is mentioned in the summary. Ourevaluation shows poor performance by pre-trained SOTA summarization models andzero-shot LLM prompting in capturing salient entities in generated summaries.We also show that predicting or providing salient entities to several modelarchitectures enhances performance and helps derive higher-quality summaries byalleviating the entity hallucination problem in existing abstractivesummarization.</description><author>Jessica Lin, Amir Zeldes</author><pubDate>Wed, 31 Jan 2024 16:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17974v1</guid></item><item><title>Extracting Dynamical Models from Data</title><link>http://arxiv.org/abs/2110.06917v6</link><description>The problem of determining the underlying dynamics of a system when onlygiven data of its state over time has challenged scientists for decades. Inthis paper, the approach of using machine learning to model the updates of thephase space variables is introduced; this is done as a function of the phasespace variables. (More generally, the modeling is done over functions of thejet space.) This approach (named FJet) allows one to accurately replicate thedynamics, and is demonstrated on the examples of the damped harmonicoscillator, the damped pendulum, and the Duffing oscillator; the underlyingdifferential equation is also accurately recovered for each example. Inaddition, the results in no way depend on how the data is sampled over time(i.e., regularly or irregularly). It is demonstrated that a regressionimplementation of FJet is similar to the model resulting from a Taylor seriesexpansion of the Runge-Kutta (RK) numerical integration scheme. Thisidentification confers the advantage of explicitly revealing the function spaceto use in the modeling, as well as the associated uncertainty quantificationfor the updates. Finally, it is shown in the undamped harmonic oscillatorexample that the stability of the updates is stable $10^9$ times longer thanwith $4$th-order RK (with time step $0.1$).</description><author>Michael F. Zimmer</author><pubDate>Wed, 31 Jan 2024 16:29:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.06917v6</guid></item><item><title>Intrinsic Gaussian Processes on Manifolds and Their Accelerations by Symmetry</title><link>http://arxiv.org/abs/2006.14266v2</link><description>Amidst the growing interest in nonparametric regression, we address asignificant challenge in Gaussian processes(GP) applied to manifold-basedpredictors. Existing methods primarily focus on low dimensional constraineddomains for heat kernel estimation, limiting their effectiveness inhigher-dimensional manifolds. Our research proposes an intrinsic approach forconstructing GP on general manifolds such as orthogonal groups, unitary groups,Stiefel manifolds and Grassmannian manifolds. Our methodology estimates theheat kernel by simulating Brownian motion sample paths using the exponentialmap, ensuring independence from the manifold's embedding. The introduction ofour strip algorithm, tailored for manifolds with extra symmetries, and the ballalgorithm, designed for arbitrary manifolds, constitutes our significantcontribution. Both algorithms are rigorously substantiated through theoreticalproofs and numerical testing, with the strip algorithm showcasing remarkableefficiency gains over traditional methods. This intrinsic approach deliversseveral key advantages, including applicability to high dimensional manifolds,eliminating the requirement for global parametrization or embedding. Wedemonstrate its practicality through regression case studies (torus knots andeight dimensional projective spaces) and by developing binary classifiers forreal world datasets (gorilla skulls planar images and diffusion tensor images).These classifiers outperform traditional methods, particularly in limited datascenarios.</description><author>Ke Ye, Mu Niu, Pokman Cheung, Zhenwen Dai, Yuan Liu</author><pubDate>Wed, 31 Jan 2024 16:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2006.14266v2</guid></item><item><title>MelNet: A Real-Time Deep Learning Algorithm for Object Detection</title><link>http://arxiv.org/abs/2401.17972v1</link><description>In this study, a novel deep learning algorithm for object detection, namedMelNet, was introduced. MelNet underwent training utilizing the KITTI datasetfor object detection. Following 300 training epochs, MelNet attained an mAP(mean average precision) score of 0.732. Additionally, three alternative models-YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTIdataset and juxtaposed with MelNet for object detection. The outcomes underscore the efficacy of employing transfer learning incertain instances. Notably, preexisting models trained on prominent datasets(e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another findingunderscores the viability of creating a new model tailored to a specificscenario and training it on a specific dataset. This investigation demonstratesthat training MelNet exclusively on the KITTI dataset also surpassesEfficientDet after 150 epochs. Consequently, post-training, MelNet'sperformance closely aligns with that of other pre-trained models.</description><author>Yashar Azadvatan, Murat Kurt</author><pubDate>Wed, 31 Jan 2024 16:27:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17972v1</guid></item><item><title>CONCORD: Towards a DSL for Configurable Graph Code Representation</title><link>http://arxiv.org/abs/2401.17967v1</link><description>Deep learning is widely used to uncover hidden patterns in large codecorpora. To achieve this, constructing a format that captures the relevantcharacteristics and features of source code is essential. Graph-basedrepresentations have gained attention for their ability to model structural andsemantic information. However, existing tools lack flexibility in constructinggraphs across different programming languages, limiting their use.Additionally, the output of these tools often lacks interoperability andresults in excessively large graphs, making graph-based neural networkstraining slower and less scalable. We introduce CONCORD, a domain-specific language to build customizable graphrepresentations. It implements reduction heuristics to reduce graphs' sizecomplexity. We demonstrate its effectiveness in code smell detection as anillustrative use case and show that: first, CONCORD can produce coderepresentations automatically per the specified configuration, and second, ourheuristics can achieve comparable performance with significantly reduced size.CONCORD will help researchers a) create and experiment with customizablegraph-based code representations for different software engineering tasksinvolving DL, b) reduce the engineering work to generate graph representations,c) address the issue of scalability in GNN models, and d) enhance thereproducibility of experiments in research through a standardized approach tocode representation and analysis.</description><author>Mootez Saad, Tushar Sharma</author><pubDate>Wed, 31 Jan 2024 16:16:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17967v1</guid></item><item><title>Combining Deep Learning and Street View Imagery to Map Smallholder Crop Types</title><link>http://arxiv.org/abs/2309.05930v2</link><description>Accurate crop type maps are an essential source of information for monitoringyield progress at scale, projecting global crop production, and planningeffective policies. To date, however, crop type maps remain challenging tocreate in low and middle-income countries due to a lack of ground truth labelsfor training machine learning models. Field surveys are the gold standard interms of accuracy but require an often-prohibitively large amount of time,money, and statistical capacity. In recent years, street-level imagery, such asGoogle Street View, KartaView, and Mapillary, has become available around theworld. Such imagery contains rich information about crop types grown atparticular locations and times. In this work, we develop an automated system togenerate crop type ground references using deep learning and Google Street Viewimagery. The method efficiently curates a set of street view images containingcrop fields, trains a model to predict crop type by utilizing weakly-labelledimages from disparate out-of-domain sources, and combines predicted labels withremote sensing time series to create a wall-to-wall crop type map. We showthat, in Thailand, the resulting country-wide map of rice, cassava, maize, andsugarcane achieves an accuracy of 93%. We publicly release the first-ever croptype map for all of Thailand 2022 at 10m-resolution with no gaps. To ourknowledge, this is the first time a 10m-resolution, multi-crop map has beencreated for any smallholder country. As the availability of roadside imageryexpands, our pipeline provides a way to map crop types at scale around theglobe, especially in underserved smallholder regions.</description><author>Jordi Laguarta Soler, Thomas Friedel, Sherrie Wang</author><pubDate>Wed, 31 Jan 2024 16:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05930v2</guid></item><item><title>Convergence Analysis for General Probability Flow ODEs of Diffusion Models in Wasserstein Distances</title><link>http://arxiv.org/abs/2401.17958v1</link><description>Score-based generative modeling with probability flow ordinary differentialequations (ODEs) has achieved remarkable success in a variety of applications.While various fast ODE-based samplers have been proposed in the literature andemployed in practice, the theoretical understandings about convergenceproperties of the probability flow ODE are still quite limited. In this paper,we provide the first non-asymptotic convergence analysis for a general class ofprobability flow ODE samplers in 2-Wasserstein distance, assuming accuratescore estimates. We then consider various examples and establish results on theiteration complexity of the corresponding ODE-based samplers.</description><author>Xuefeng Gao, Lingjiong Zhu</author><pubDate>Wed, 31 Jan 2024 16:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17958v1</guid></item><item><title>Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation</title><link>http://arxiv.org/abs/2303.14346v2</link><description>Object detection and multiple object tracking (MOT) are essential componentsof self-driving systems. Accurate detection and uncertainty quantification areboth critical for onboard modules, such as perception, prediction, andplanning, to improve the safety and robustness of autonomous vehicles.Collaborative object detection (COD) has been proposed to improve detectionaccuracy and reduce uncertainty by leveraging the viewpoints of multipleagents. However, little attention has been paid to how to leverage theuncertainty quantification from COD to enhance MOT performance. In this paper,as the first attempt to address this challenge, we design an uncertaintypropagation framework called MOT-CUP. Our framework first quantifies theuncertainty of COD through direct modeling and conformal prediction, andpropagates this uncertainty information into the motion prediction andassociation steps. MOT-CUP is designed to work with different collaborativeobject detectors and baseline MOT algorithms. We evaluate MOT-CUP on V2X-Sim, acomprehensive collaborative perception dataset, and demonstrate a 2%improvement in accuracy and a 2.67X reduction in uncertainty compared to thebaselines, e.g. SORT and ByteTrack. In scenarios characterized by highocclusion levels, our MOT-CUP demonstrates a noteworthy $4.01\%$ improvement inaccuracy. MOT-CUP demonstrates the importance of uncertainty quantification inboth COD and MOT, and provides the first attempt to improve the accuracy andreduce the uncertainty in MOT based on COD through uncertainty propagation. Ourcode is public on https://coperception.github.io/MOT-CUP/.</description><author>Sanbao Su, Songyang Han, Yiming Li, Zhili Zhang, Chen Feng, Caiwen Ding, Fei Miao</author><pubDate>Wed, 31 Jan 2024 16:00:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14346v2</guid></item><item><title>HyperZ$\cdot$Z$\cdot$W Operator Connects Slow-Fast Networks for Full Context Interaction</title><link>http://arxiv.org/abs/2401.17948v1</link><description>The self-attention mechanism utilizes large implicit weight matrices,programmed through dot product-based activations with very few trainableparameters, to enable long sequence modeling. In this paper, we investigate thepossibility of discarding residual learning by employing large implicit kernelsto achieve full context interaction at each layer of the network. To accomplishit, we introduce coordinate-based implicit MLPs as a slow network to generatehyper-kernels for another fast convolutional network. To get context-varyingweights for fast dynamic encoding, we propose a$\mathrm{Hyper}\mathcal{Z{\cdot}Z{\cdot}W}$ operator that connectshyper-kernels ($\mathcal{W}$) and hidden activations ($\mathcal{Z}$) throughsimple elementwise multiplication, followed by convolution of $\mathcal{Z}$using the context-dependent $\mathcal{W}$. Based on this design, we present anovel Terminator architecture that integrates hyper-kernels of different sizesto produce multi-branch hidden representations for enhancing the featureextraction capability of each layer. Additionally, a bottleneck layer isemployed to compress the concatenated channels, allowing only valuableinformation to propagate to the subsequent layers. Notably, our modelincorporates several innovative components and exhibits excellent properties,such as introducing local feedback error for updating the slow network, stablezero-mean features, faster training convergence, and fewer model parameters.Extensive experimental results on pixel-level 1D and 2D image classificationbenchmarks demonstrate the superior performance of our architecture.</description><author>Harvie Zhang</author><pubDate>Wed, 31 Jan 2024 15:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17948v1</guid></item><item><title>Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks</title><link>http://arxiv.org/abs/2307.05318v3</link><description>Aqueous solubility is a valuable yet challenging property to predict.Computing solubility using first-principles methods requires accounting for thecompeting effects of entropy and enthalpy, resulting in long computations forrelatively poor accuracy. Data-driven approaches, such as deep learning, offerimproved accuracy and computational efficiency but typically lack uncertaintyquantification. Additionally, ease of use remains a concern for anycomputational technique, resulting in the sustained popularity of group-basedcontribution methods. In this work, we addressed these problems with a deeplearning model with predictive uncertainty that runs on a static website(without a server). This approach moves computing needs onto the websitevisitor without requiring installation, removing the need to pay for andmaintain servers. Our model achieves satisfactory results in solubilityprediction. Furthermore, we demonstrate how to create molecular propertyprediction models that balance uncertainty and ease of use. The code isavailable at https://github.com/ur-whitelab/mol.dev, and the model is usable athttps://mol.dev.</description><author>Mayk Caldas Ramos, Andrew D. White</author><pubDate>Wed, 31 Jan 2024 15:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05318v3</guid></item><item><title>PanAf20K: A Large Video Dataset for Wild Ape Detection and Behaviour Recognition</title><link>http://arxiv.org/abs/2401.13554v2</link><description>We present the PanAf20K dataset, the largest and most diverse open-accessannotated video dataset of great apes in their natural environment. Itcomprises more than 7 million frames across ~20,000 camera trap videos ofchimpanzees and gorillas collected at 14 field sites in tropical Africa as partof the Pan African Programme: The Cultured Chimpanzee. The footage isaccompanied by a rich set of annotations and benchmarks making it suitable fortraining and testing a variety of challenging and ecologically importantcomputer vision tasks including ape detection and behaviour recognition.Furthering AI analysis of camera trap information is critical given theInternational Union for Conservation of Nature now lists all species in thegreat ape family as either Endangered or Critically Endangered. We hope thedataset can form a solid basis for engagement of the AI community to improveperformance, efficiency, and result interpretation in order to supportassessments of great ape presence, abundance, distribution, and behaviour andthereby aid conservation efforts.</description><author>Otto Brookes, Majid Mirmehdi, Colleen Stephens, Samuel Angedakin, Katherine Corogenes, Dervla Dowd, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, Vera Leinert, Juan Lapuente, Maureen S. McCarthy, Amelia Meier, Mizuki Murai, Emmanuelle Normand, Virginie Vergnes, Erin G. Wessling, Roman M. Wittig, Kevin Langergraber, Nuria Maldonado, Xinyu Yang, Klaus Zuberbuhler, Christophe Boesch, Mimi Arandjelovic, Hjalmar Kuhl, Tilo Burghardt</author><pubDate>Wed, 31 Jan 2024 15:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13554v2</guid></item><item><title>Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection</title><link>http://arxiv.org/abs/2308.12612v2</link><description>The rapid growth of deep learning (DL) has spurred interest in enhancinglog-based anomaly detection. This approach aims to extract meaning from logevents (log message templates) and develop advanced DL models for anomalydetection. However, these DL methods face challenges like heavy reliance ontraining data, labels, and computational resources due to model complexity. Incontrast, traditional machine learning and data mining techniques are lessdata-dependent and more efficient but less effective than DL. To make log-basedanomaly detection more practical, the goal is to enhance traditional techniquesto match DL's effectiveness. Previous research in a different domain (linkingquestions on Stack Overflow) suggests that optimized traditional techniques canrival state-of-the-art DL methods. Drawing inspiration from this concept, weconducted an empirical study. We optimized the unsupervised PCA (PrincipalComponent Analysis), a traditional technique, by incorporating lightweightsemantic-based log representation. This addresses the issue of unseen logevents in training data, enhancing log representation. Our study compared sevenlog-based anomaly detection methods, including four DL-based, two traditional,and the optimized PCA technique, using public and industrial datasets. Resultsindicate that the optimized unsupervised PCA technique achieves similareffectiveness to advanced supervised/semi-supervised DL methods while beingmore stable with limited training data and resource-efficient. Thisdemonstrates the adaptability and strength of traditional techniques throughsmall yet impactful adaptations.</description><author>Lin Yang, Junjie Chen, Shutao Gao, Zhihao Gong, Hongyu Zhang, Yue Kang, Huaan Li</author><pubDate>Wed, 31 Jan 2024 15:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12612v2</guid></item><item><title>Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting</title><link>http://arxiv.org/abs/2401.16416v2</link><description>In the realm of robot-assisted minimally invasive surgery, dynamic scenereconstruction can significantly enhance downstream tasks and improve surgicaloutcomes. Neural Radiance Fields (NeRF)-based methods have recently risen toprominence for their exceptional ability to reconstruct scenes. Nonetheless,these methods are hampered by slow inference, prolonged training, andsubstantial computational demands. Additionally, some rely on stereo depthestimation, which is often infeasible due to the high costs and logisticalchallenges associated with stereo cameras. Moreover, the monocularreconstruction quality for deformable scenes is currently inadequate. Toovercome these obstacles, we present Endo-4DGS, an innovative, real-timeendoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting(GS) and requires no ground truth depth data. This method extends 3D GS byincorporating a temporal component and leverages a lightweight MLP to capturetemporal Gaussian deformations. This effectively facilitates the reconstructionof dynamic surgical scenes with variable conditions. We also integrateDepth-Anything to generate pseudo-depth maps from monocular views, enhancingthe depth-guided reconstruction process. Our approach has been validated on twosurgical datasets, where it can effectively render in real-time, computeefficiently, and reconstruct with remarkable accuracy. These results underlinethe vast potential of Endo-4DGS to improve surgical assistance.</description><author>Yiming Huang, Beilei Cui, Long Bai, Ziqi Guo, Mengya Xu, Hongliang Ren</author><pubDate>Wed, 31 Jan 2024 15:51:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16416v2</guid></item><item><title>On the Generalizability of ECG-based Stress Detection Models</title><link>http://arxiv.org/abs/2210.06225v2</link><description>Stress is prevalent in many aspects of everyday life including work,healthcare, and social interactions. Many works have studied handcraftedfeatures from various bio-signals that are indicators of stress. Recently, deeplearning models have also been proposed to detect stress. Typically, stressmodels are trained and validated on the same dataset, often involving onestressful scenario. However, it is not practical to collect stress data forevery scenario. So, it is crucial to study the generalizability of these modelsand determine to what extent they can be used in other scenarios. In thispaper, we explore the generalization capabilities of Electrocardiogram(ECG)-based deep learning models and models based on handcrafted ECG features,i.e., Heart Rate Variability (HRV) features. To this end, we train three HRVmodels and two deep learning models that use ECG signals as input. We use ECGsignals from two popular stress datasets - WESAD and SWELL-KW - differing interms of stressors and recording devices. First, we evaluate the models usingleave-one-subject-out (LOSO) cross-validation using training and validationsamples from the same dataset. Next, we perform a cross-dataset validation ofthe models, that is, LOSO models trained on the WESAD dataset are validatedusing SWELL-KW samples and vice versa. While deep learning models achieve thebest results on the same dataset, models based on HRV features considerablyoutperform them on data from a different dataset. This trend is observed forall the models on both datasets. Therefore, HRV models are a better choice forstress recognition in applications that are different from the datasetscenario. To the best of our knowledge, this is the first work to compare thecross-dataset generalizability between ECG-based deep learning models and HRVmodels.</description><author>Pooja Prajod, Elisabeth André</author><pubDate>Wed, 31 Jan 2024 15:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06225v2</guid></item><item><title>An Empathetic AI Coach for Self-Attachment Therapy</title><link>http://arxiv.org/abs/2209.08316v2</link><description>In this work, we present a new dataset and a computational strategy for adigital coach that aims to guide users in practicing the protocols ofself-attachment therapy. Our framework augments a rule-based conversationalagent with a deep-learning classifier for identifying the underlying emotion ina user's text response, as well as a deep-learning assisted retrieval methodfor producing novel, fluent and empathetic utterances. We also craft a set ofhuman-like personas that users can choose to interact with. Our goal is toachieve a high level of engagement during virtual therapy sessions. We evaluatethe effectiveness of our framework in a non-clinical trial with N=16participants, all of whom have had at least four interactions with the agentover the course of five days. We find that our platform is consistently ratedhigher for empathy, user engagement and usefulness than the simple rule-basedframework. Finally, we provide guidelines to further improve the design andperformance of the application, in accordance with the feedback received.</description><author>Lisa Alazraki, Ali Ghachem, Neophytos Polydorou, Foaad Khosmood, Abbas Edalat</author><pubDate>Wed, 31 Jan 2024 15:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08316v2</guid></item><item><title>[Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference Annotation with LLMs</title><link>http://arxiv.org/abs/2401.17922v1</link><description>Coreference annotation and resolution is a vital component of computationalliterary studies. However, it has previously been difficult to build highquality systems for fiction. Coreference requires complicated structuredoutputs, and literary text involves subtle inferences and highly variedlanguage. New language-model-based seq2seq systems present the opportunity tosolve both these problems by learning to directly generate a copy of an inputsentence with markdown-like annotations. We create, evaluate, and releaseseveral trained models for coreference, as well as a workflow for training newmodels.</description><author>Rebecca M. M. Hicke, David Mimno</author><pubDate>Wed, 31 Jan 2024 15:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17922v1</guid></item><item><title>LOCOST: State-Space Models for Long Document Abstractive Summarization</title><link>http://arxiv.org/abs/2401.17919v1</link><description>State-space models are a low-complexity alternative to transformers forencoding long sequences and capturing long-term dependencies. We proposeLOCOST: an encoder-decoder architecture based on state-space models forconditional text generation with long context inputs. With a computationalcomplexity of $O(L \log L)$, this architecture can handle significantly longersequences than state-of-the-art models that are based on sparse attentionpatterns. We evaluate our model on a series of long document abstractivesummarization tasks. The model reaches a performance level that is 93-96%comparable to the top-performing sparse transformers of the same size whilesaving up to 50% memory during training and up to 87% during inference.Additionally, LOCOST effectively handles input texts exceeding 600K tokens atinference time, setting new state-of-the-art results on full-book summarizationand opening new perspectives for long input processing.</description><author>Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari</author><pubDate>Wed, 31 Jan 2024 15:33:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17919v1</guid></item><item><title>Source-free Domain Adaptive Object Detection in Remote Sensing Images</title><link>http://arxiv.org/abs/2401.17916v1</link><description>Recent studies have used unsupervised domain adaptive object detection(UDAOD) methods to bridge the domain gap in remote sensing (RS) images.However, UDAOD methods typically assume that the source domain data can beaccessed during the domain adaptation process. This setting is oftenimpractical in the real world due to RS data privacy and transmissiondifficulty. To address this challenge, we propose a practical source-freeobject detection (SFOD) setting for RS images, which aims to perform targetdomain adaptation using only the source pre-trained model. We propose a newSFOD method for RS images consisting of two parts: perturbed domain generationand alignment. The proposed multilevel perturbation constructs the perturbeddomain in a simple yet efficient form by perturbing the domain-variant featuresat the image level and feature level according to the color and style bias. Theproposed multilevel alignment calculates feature and label consistency betweenthe perturbed domain and the target domain across the teacher-student network,and introduces the distillation of feature prototype to mitigate the noise ofpseudo-labels. By requiring the detector to be consistent in the perturbeddomain and the target domain, the detector is forced to focus ondomaininvariant features. Extensive results of three synthetic-to-realexperiments and three cross-sensor experiments have validated the effectivenessof our method which does not require access to source domain RS images.Furthermore, experiments on computer vision datasets show that our method canbe extended to other fields as well. Our code will be available at:https://weixliu.github.io/ .</description><author>Weixing Liu, Jun Liu, Xin Su, Han Nie, Bin Luo</author><pubDate>Wed, 31 Jan 2024 15:32:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17916v1</guid></item><item><title>Attention Graph for Multi-Robot Social Navigation with Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2401.17914v1</link><description>Learning robot navigation strategies among pedestrian is crucial for domainbased applications. Combining perception, planning and prediction allows us tomodel the interactions between robots and pedestrians, resulting in impressiveoutcomes especially with recent approaches based on deep reinforcement learning(RL). However, these works do not consider multi-robot scenarios. In thispaper, we present MultiSoc, a new method for learning multi-agent sociallyaware navigation strategies using RL. Inspired by recent works on multi-agentdeep RL, our method leverages graph-based representation of agent interactions,combining the positions and fields of view of entities (pedestrians andagents). Each agent uses a model based on two Graph Neural Network combinedwith attention mechanisms. First an edge-selector produces a sparse graph, thena crowd coordinator applies node attention to produce a graph representing theinfluence of each entity on the others. This is incorporated into a model-freeRL framework to learn multi-agent policies. We evaluate our approach onsimulation and provide a series of experiments in a set of various conditions(number of agents / pedestrians). Empirical results show that our method learnsfaster than social navigation deep RL mono-agent techniques, and enablesefficient multi-agent implicit coordination in challenging crowd navigationwith multiple heterogeneous humans. Furthermore, by incorporating customizablemeta-parameters, we can adjust the neighborhood density to take into account inour navigation strategy.</description><author>Erwan Escudie, Laetitia Matignon, Jacques Saraydaryan</author><pubDate>Wed, 31 Jan 2024 15:24:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17914v1</guid></item><item><title>SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural Networks</title><link>http://arxiv.org/abs/2401.17911v1</link><description>As spiking neural networks receive more attention, we look towardapplications of this computing paradigm in fields other than computer visionand signal processing. One major field, underexplored in the neuromorphicsetting, is Natural Language Processing (NLP), where most state-of-the-artsolutions still heavily rely on resource-consuming and power-hungry traditionaldeep learning architectures. Therefore, it is compelling to design NLP modelsfor neuromorphic architectures due to their low energy requirements, with theadditional benefit of a more human-brain-like operating model for processinginformation. However, one of the biggest issues with bringing NLP to theneuromorphic setting is in properly encoding text into a spike train so that itcan be seamlessly handled by both current and future SNN architectures. In thispaper, we compare various methods of encoding text as spikes and assess eachmethod's performance in an associated SNN on a downstream NLP task, namely,sentiment analysis. Furthermore, we go on to propose a new method of encodingtext as spikes that outperforms a widely-used rate-coding technique, Poissonrate-coding, by around 13\% on our benchmark NLP tasks. Subsequently, wedemonstrate the energy efficiency of SNNs implemented in hardware for thesentiment analysis task compared to traditional deep neural networks, observingan energy efficiency increase of more than 32x during inference and 60x duringtraining while incurring the expected energy-performance tradeoff.</description><author>R. Alexander Knipper, Kaniz Mishty, Mehdi Sadi, Shubhra Kanti Karmaker Santu</author><pubDate>Wed, 31 Jan 2024 15:16:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17911v1</guid></item><item><title>Controllable Dense Captioner with Multimodal Embedding Bridging</title><link>http://arxiv.org/abs/2401.17910v1</link><description>In this paper, we propose a controllable dense captioner (ControlCap), whichaccommodates user's intention to dense captioning by introducing linguisticguidance. ControlCap is defined as a multimodal embedding bridgingarchitecture, which comprises multimodal embedding generation (MEG) module andbi-directional embedding bridging (BEB) module. While MEG module representsobjects/regions by combining embeddings of detailed information withcontext-aware ones, it also endows ControlCap the adaptability to specializedcontrols by utilizing them as linguistic guidance. BEB module aligns thelinguistic guidance with visual embeddings through borrowing/returning featuresfrom/to the visual domain and gathering such features to predict textdescriptions. Experiments on Visual Genome and VG-COCO datasets show thatControlCap respectively outperforms the state-of-the-art methods by 1.5% and3.7% (mAP). Last but not least, with the capability of convertingregion-category pairs to region-text pairs, ControlCap is able to act as apowerful data engine for dense captioning. Code is available athttps://github.com/callsys/ControlCap.</description><author>Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Qixiang Ye, Fang Wan</author><pubDate>Wed, 31 Jan 2024 15:15:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17910v1</guid></item><item><title>Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation</title><link>http://arxiv.org/abs/2401.17904v1</link><description>The Segment Anything Model (SAM), a profound vision foundation modelpre-trained on a large-scale dataset, breaks the boundaries of generalsegmentation and sparks various downstream applications. This paper introducesHi-SAM, a unified model leveraging SAM for hierarchical text segmentation.Hi-SAM excels in text segmentation across four hierarchies, including stroke,word, text-line, and paragraph, while realizing layout analysis as well.Specifically, we first turn SAM into a high-quality text stroke segmentation(TSS) model through a parameter-efficient fine-tuning approach. We use this TSSmodel to iteratively generate the text stroke labels in a semi-automaticalmanner, unifying labels across the four text hierarchies in the HierTextdataset. Subsequently, with these complete labels, we launch the end-to-endtrainable Hi-SAM based on the TSS architecture with a customized hierarchicalmask decoder. During inference, Hi-SAM offers both automatic mask generation(AMG) mode and promptable segmentation mode. In terms of the AMG mode, Hi-SAMsegments text stroke foreground masks initially, then samples foreground pointsfor hierarchical text mask generation and achieves layout analysis in passing.As for the promptable mode, Hi-SAM provides word, text-line, and paragraphmasks with a single point click. Experimental results show the state-of-the-artperformance of our TSS model: 84.86% fgIOU on Total-Text and 88.96% fgIOU onTextSeg for text stroke segmentation. Moreover, compared to the previousspecialist for joint hierarchical detection and layout analysis on HierText,Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on thetext-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout analysis,requiring 20x fewer training epochs. The code is available athttps://github.com/ymy-k/Hi-SAM.</description><author>Maoyuan Ye, Jing Zhang, Juhua Liu, Chenyu Liu, Baocai Yin, Cong Liu, Bo Du, Dacheng Tao</author><pubDate>Wed, 31 Jan 2024 15:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17904v1</guid></item><item><title>Revisiting speech segmentation and lexicon learning with better features</title><link>http://arxiv.org/abs/2401.17902v1</link><description>We revisit a self-supervised method that segments unlabelled speech intoword-like segments. We start from the two-stage duration-penalised dynamicprogramming method that performs zero-resource segmentation without learning anexplicit lexicon. In the first acoustic unit discovery stage, we replacecontrastive predictive coding features with HuBERT. After word segmentation inthe second stage, we get an acoustic word embedding for each segment byaveraging HuBERT features. These embeddings are clustered using K-means to geta lexicon. The result is good full-coverage segmentation with a lexicon thatachieves state-of-the-art performance on the ZeroSpeech benchmarks.</description><author>Herman Kamper, Benjamin van Niekerk</author><pubDate>Wed, 31 Jan 2024 15:06:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17902v1</guid></item><item><title>Multitask methods for predicting molecular properties from heterogeneous data</title><link>http://arxiv.org/abs/2401.17898v1</link><description>Data generation remains a bottleneck in training surrogate models to predictmolecular properties. We demonstrate that multitask Gaussian process regressionovercomes this limitation by leveraging both expensive and cheap data sources.In particular, we consider training sets constructed from coupled-cluster (CC)and density function theory (DFT) data. We report that multitask surrogates canpredict at CC level accuracy with a reduction to data generation cost by overan order of magnitude. Of note, our approach allows the training set to includeDFT data generated by a heterogeneous mix of exchange-correlation functionalswithout imposing any artificial hierarchy on functional accuracy. Moregenerally, the multitask framework can accommodate a wider range of trainingset structures -- including full disparity between the different levels offidelity -- than existing kernel approaches based on $\Delta$-learning, thoughwe show that the accuracy of the two approaches can be similar. Consequently,multitask regression can be a tool for reducing data generation costs evenfurther by opportunistically exploiting existing data sources.</description><author>Katharine Fisher, Michael Herbst, Youssef Marzouk</author><pubDate>Wed, 31 Jan 2024 15:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17898v1</guid></item><item><title>Employing Label Models on ChatGPT Answers Improves Legal Text Entailment Performance</title><link>http://arxiv.org/abs/2401.17897v1</link><description>The objective of legal text entailment is to ascertain whether the assertionsin a legal query logically follow from the information provided in one ormultiple legal articles. ChatGPT, a large language model, is robust in manynatural language processing tasks, including legal text entailment: when we setthe temperature = 0 (the ChatGPT answers are deterministic) and prompt themodel, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperformsthe previous SOTA of 67.89%. On the other hand, if the temperature is largerthan zero, ChatGPT answers are not deterministic, leading to inconsistentanswers and fluctuating results. We propose to leverage label models (afundamental component of weak supervision techniques) to integrate theprovisional answers by ChatGPT into consolidated labels. By that way, we treatChatGPT provisional answers as noisy predictions which can be consolidated bylabel models. The experimental results demonstrate that this approach canattain an accuracy of 76.15%, marking a significant improvement of 8.26% overthe prior state-of-the-art benchmark. Additionally, we perform an analysis ofthe instances where ChatGPT produces incorrect answers, then we classify theerrors, offering insights that could guide potential enhancements for futureresearch endeavors.</description><author>Chau Nguyen, Le-Minh Nguyen</author><pubDate>Wed, 31 Jan 2024 15:04:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17897v1</guid></item><item><title>ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural Radiance Fields</title><link>http://arxiv.org/abs/2401.17895v1</link><description>We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D sceneediting method that enables the replacement of specific objects within a scene.Given multi-view images of a scene, a text prompt describing the object toreplace, and a text prompt describing the new object, our Erase-and-Replaceapproach can effectively swap objects in the scene with newly generated contentwhile maintaining 3D consistency across multiple viewpoints. We demonstrate theversatility of ReplaceAnything3D by applying it to various realistic 3D scenes,showcasing results of modified foreground objects that are well-integrated withthe rest of the scene without affecting its overall integrity.</description><author>Edward Bartrum, Thu Nguyen-Phuoc, Chris Xie, Zhengqin Li, Numair Khan, Armen Avetisyan, Douglas Lanman, Lei Xiao</author><pubDate>Wed, 31 Jan 2024 15:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17895v1</guid></item><item><title>BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation</title><link>http://arxiv.org/abs/2401.17053v2</link><description>We present BlockFusion, a diffusion-based model that generates 3D scenes asunit blocks and seamlessly incorporates new blocks to extend the scene.BlockFusion is trained using datasets of 3D blocks that are randomly croppedfrom complete 3D scene meshes. Through per-block fitting, all training blocksare converted into the hybrid neural fields: with a tri-plane containing thegeometry features, followed by a Multi-layer Perceptron (MLP) for decoding thesigned distance values. A variational auto-encoder is employed to compress thetri-planes into the latent tri-plane space, on which the denoising diffusionprocess is performed. Diffusion applied to the latent representations allowsfor high-quality and diverse 3D scene generation. To expand a scene duringgeneration, one needs only to append empty blocks to overlap with the currentscene and extrapolate existing latent tri-planes to populate new blocks. Theextrapolation is done by conditioning the generation process with the featuresamples from the overlapping tri-planes during the denoising iterations. Latenttri-plane extrapolation produces semantically and geometrically meaningfultransitions that harmoniously blend with the existing scene. A 2D layoutconditioning mechanism is used to control the placement and arrangement ofscene elements. Experimental results indicate that BlockFusion is capable ofgenerating diverse, geometrically consistent and unbounded large 3D scenes withunprecedented high-quality shapes in both indoor and outdoor scenarios.</description><author>Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, Pan Ji</author><pubDate>Wed, 31 Jan 2024 14:53:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17053v2</guid></item><item><title>A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data</title><link>http://arxiv.org/abs/2308.13352v3</link><description>Anomaly detection (AD) tasks have been solved using machine learningalgorithms in various domains and applications. The great majority of thesealgorithms use normal data to train a residual-based model and assign anomalyscores to unseen samples based on their dissimilarity with the learned normalregime. The underlying assumption of these approaches is that anomaly-free datais available for training. This is, however, often not the case in real-worldoperational settings, where the training data may be contaminated with anunknown fraction of abnormal samples. Training with contaminated data, in turn,inevitably leads to a deteriorated AD performance of the residual-basedalgorithms. In this paper we introduce a framework for a fully unsupervised refinement ofcontaminated training data for AD tasks. The framework is generic and can beapplied to any residual-based machine learning model. We demonstrate theapplication of the framework to two public datasets of multivariate time seriesmachine data from different application fields. We show its clear superiorityover the naive approach of training with contaminated data without refinement.Moreover, we compare it to the ideal, unrealistic reference in whichanomaly-free data would be available for training. The method is based onevaluating the contribution of individual samples to the generalization abilityof a given model, and contrasting the contribution of anomalies with the one ofnormal samples. As a result, the proposed approach is comparable to, and oftenoutperforms training with normal samples only.</description><author>Markus Ulmer, Jannik Zgraggen, Lilach Goren Huber</author><pubDate>Wed, 31 Jan 2024 14:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13352v3</guid></item><item><title>Reimagining Reality: A Comprehensive Survey of Video Inpainting Techniques</title><link>http://arxiv.org/abs/2401.17883v1</link><description>This paper offers a comprehensive analysis of recent advancements in videoinpainting techniques, a critical subset of computer vision and artificialintelligence. As a process that restores or fills in missing or corruptedportions of video sequences with plausible content, video inpainting hasevolved significantly with the advent of deep learning methodologies. Despitethe plethora of existing methods and their swift development, the landscaperemains complex, posing challenges to both novices and established researchers.Our study deconstructs major techniques, their underpinning theories, and theireffective applications. Moreover, we conduct an exhaustive comparative study,centering on two often-overlooked dimensions: visual quality and computationalefficiency. We adopt a human-centric approach to assess visual quality,enlisting a panel of annotators to evaluate the output of different videoinpainting techniques. This provides a nuanced qualitative understanding thatcomplements traditional quantitative metrics. Concurrently, we delve into thecomputational aspects, comparing inference times and memory demands across astandardized hardware setup. This analysis underscores the balance betweenquality and efficiency: a critical consideration for practical applicationswhere resources may be constrained. By integrating human validation andcomputational resource comparison, this survey not only clarifies the presentlandscape of video inpainting techniques but also charts a course for futureexplorations in this vibrant and evolving field.</description><author>Shreyank N Gowda, Yash Thakre, Shashank Narayana Gowda, Xiaobo Jin</author><pubDate>Wed, 31 Jan 2024 14:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17883v1</guid></item><item><title>I Think, Therefore I am: Awareness in Large Language Models</title><link>http://arxiv.org/abs/2401.17882v1</link><description>Do large language models (LLMs) exhibit any forms of awareness similar tohumans? In this paper, we introduce the concept of awareness to LLMs, arguingthat awareness is an essential aspect of trustworthiness for LLMs to enhancetheir interaction with humans while ensuring ethical responses. We defineawareness in LLMs as the ability to perceive and understand themselves as AImodels and to exhibit social intelligence. We identify four key dimensions ofawareness: capability, mission, emotion, and perspective. To assess LLMs onthese dimensions, we introduce a specialized dataset, AwareLLM dataset. Ourfindings reveal that LLMs demonstrate a decent degree of awareness, though theystill lack substantial capability awareness.</description><author>Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, Lichao Sun</author><pubDate>Wed, 31 Jan 2024 14:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17882v1</guid></item><item><title>Efficiently Solving High-Order and Nonlinear ODEs with Rational Fraction Polynomial: the Ratio Net</title><link>http://arxiv.org/abs/2105.11309v2</link><description>Recent advances in solving ordinary differential equations (ODEs) with neuralnetworks have been remarkable. Neural networks excel at serving as trialfunctions and approximating solutions within functional spaces, aided bygradient backpropagation algorithms. However, challenges remain in solvingcomplex ODEs, including high-order and nonlinear cases, emphasizing the needfor improved efficiency and effectiveness. Traditional methods have typicallyrelied on established knowledge integration to improve problem-solvingefficiency. In contrast, this study takes a different approach by introducing anew neural network architecture for constructing trial functions, known asratio net. This architecture draws inspiration from rational fractionpolynomial approximation functions, specifically the Pade approximant. Throughempirical trials, it demonstrated that the proposed method exhibits higherefficiency compared to existing approaches, including polynomial-based andmultilayer perceptron (MLP) neural network-based methods. The ratio net holdspromise for advancing the efficiency and effectiveness of solving differentialequations.</description><author>Chenxin Qin, Ruhao Liu, Maocai Li, Shengyuan Li, Yi Liu, Chichun Zhou</author><pubDate>Wed, 31 Jan 2024 14:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2105.11309v2</guid></item><item><title>PVLR: Prompt-driven Visual-Linguistic Representation Learning for Multi-Label Image Recognition</title><link>http://arxiv.org/abs/2401.17881v1</link><description>Multi-label image recognition is a fundamental task in computer vision.Recently, vision-language models have made notable advancements in this area.However, previous methods often failed to effectively leverage the richknowledge within language models and instead incorporated label semantics intovisual features in a unidirectional manner. In this paper, we propose aPrompt-driven Visual-Linguistic Representation Learning (PVLR) framework tobetter leverage the capabilities of the linguistic modality. In PVLR, we firstintroduce a dual-prompting strategy comprising Knowledge-Aware Prompting (KAP)and Context-Aware Prompting (CAP). KAP utilizes fixed prompts to capture theintrinsic semantic knowledge and relationships across all labels, while CAPemploys learnable prompts to capture context-aware label semantics andrelationships. Later, we propose an Interaction and Fusion Module (IFM) tointeract and fuse the representations obtained from KAP and CAP. In contrast tothe unidirectional fusion in previous works, we introduce a Dual-ModalAttention (DMA) that enables bidirectional interaction between textual andvisual features, yielding context-aware label representations andsemantic-related visual representations, which are subsequently used tocalculate similarities and generate final predictions for all labels. Extensiveexperiments on three popular datasets including MS-COCO, Pascal VOC 2007, andNUS-WIDE demonstrate the superiority of PVLR.</description><author>Hao Tan, Zichang Tan, Jun Li, Jun Wan, Zhen Lei</author><pubDate>Wed, 31 Jan 2024 14:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17881v1</guid></item><item><title>Graph Attention-based Reinforcement Learning for Trajectory Design and Resource Assignment in Multi-UAV Assisted Communication</title><link>http://arxiv.org/abs/2401.17880v1</link><description>In the multiple unmanned aerial vehicle (UAV)- assisted downlinkcommunication, it is challenging for UAV base stations (UAV BSs) to realizetrajectory design and resource assignment in unknown environments. Thecooperation and competition between UAV BSs in the communication network leadsto a Markov game problem. Multi-agent reinforcement learning is a significantsolution for the above decision-making. However, there are still many commonissues, such as the instability of the system and low utilization of historicaldata, that limit its application. In this paper, a novel graph-attentionmulti-agent trust region (GA-MATR) reinforcement learning framework is proposedto solve the multi-UAV assisted communication problem. Graph recurrent networkis introduced to process and analyze complex topology of the communicationnetwork, so as to extract useful information and patterns from observationalinformation. The attention mechanism provides additional weighting for conveyedinformation, so that the critic network can accurately evaluate the value ofbehavior for UAV BSs. This provides more reliable feedback signals and helpsthe actor network update the strategy more effectively. Ablation simulationsindicate that the proposed approach attains improved convergence over thebaselines. UAV BSs learn the optimal communication strategies to achieve theirmaximum cumulative rewards. Additionally, multi-agent trust region method withmonotonic convergence provides an estimated Nash equilibrium for the multi-UAVassisted communication Markov game.</description><author>Zikai Feng, Di Wu, Mengxing Huang, Chau Yuen</author><pubDate>Wed, 31 Jan 2024 14:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17880v1</guid></item><item><title>AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error</title><link>http://arxiv.org/abs/2401.17879v1</link><description>With recent text-to-image models, anyone can generate deceptively realisticimages with arbitrary contents, fueling the growing threat of visualdisinformation. A key enabler for generating high-resolution images with lowcomputational cost has been the development of latent diffusion models (LDMs).In contrast to conventional diffusion models, LDMs perform the denoisingprocess in the low-dimensional latent space of a pre-trained autoencoder (AE)instead of the high-dimensional image space. Despite their relevance, theforensic analysis of LDMs is still in its infancy. In this work we proposeAEROBLADE, a novel detection method which exploits an inherent component ofLDMs: the AE used to transform images between image and latent space. We findthat generated images can be more accurately reconstructed by the AE than realimages, allowing for a simple detection approach based on the reconstructionerror. Most importantly, our method is easy to implement and does not requireany training, yet nearly matches the performance of detectors that rely onextensive training. We empirically demonstrate that AEROBLADE is effectiveagainst state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyonddetection, our approach allows for the qualitative analysis of images, whichcan be leveraged for identifying inpainted regions.</description><author>Jonas Ricker, Denis Lukovnikov, Asja Fischer</author><pubDate>Wed, 31 Jan 2024 14:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17879v1</guid></item><item><title>VR-based generation of photorealistic synthetic data for training hand-object tracking models</title><link>http://arxiv.org/abs/2401.17874v1</link><description>Supervised learning models for precise tracking of hand-object interactions(HOI) in 3D require large amounts of annotated data for training. Moreover, itis not intuitive for non-experts to label 3D ground truth (e.g. 6DoF objectpose) on 2D images. To address these issues, we present "blender-hoisynth", aninteractive synthetic data generator based on the Blender software.Blender-hoisynth can scalably generate and automatically annotate visual HOItraining data. Other competing approaches usually generate synthetic HOI datacompeletely without human input. While this may be beneficial in somescenarios, HOI applications inherently necessitate direct control over the HOIsas an expression of human intent. With blender-hoisynth, it is possible forusers to interact with objects via virtual hands using standard Virtual Realityhardware. The synthetically generated data are characterized by a high degreeof photorealism and contain visually plausible and physically realistic videosof hands grasping objects and moving them around in 3D. To demonstrate theefficacy of our data generation, we replace large parts of the training data inthe well-known DexYCB dataset with hoisynth data and train a state-of-the-artHOI reconstruction model with it. We show that there is no significantdegradation in the model performance despite the data replacement.</description><author>Chengyan Zhang, Rahul Chaudhari</author><pubDate>Wed, 31 Jan 2024 14:32:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17874v1</guid></item><item><title>CARPE-ID: Continuously Adaptable Re-identification for Personalized Robot Assistance</title><link>http://arxiv.org/abs/2310.19413v2</link><description>In today's Human-Robot Interaction (HRI) scenarios, a prevailing tendencyexists to assume that the robot shall cooperate with the closest individual orthat the scene involves merely a singular human actor. However, in realisticscenarios, such as shop floor operations, such an assumption may not hold andpersonalized target recognition by the robot in crowded environments isrequired. To fulfil this requirement, in this work, we propose a personre-identification module based on continual visual adaptation techniques thatensure the robot's seamless cooperation with the appropriate individual evensubject to varying visual appearances or partial or complete occlusions. Wetest the framework singularly using recorded videos in a laboratory environmentand an HRI scenario, i.e., a person-following task by a mobile robot. Thetargets are asked to change their appearance during tracking and to disappearfrom the camera field of view to test the challenging cases of occlusion andoutfit variations. We compare our framework with one of the state-of-the-artMulti-Object Tracking (MOT) methods and the results show that the CARPE-ID canaccurately track each selected target throughout the experiments in all thecases (except two limit cases). At the same time, the s-o-t-a MOT has a mean of4 tracking errors for each video.</description><author>Federico Rollo, Andrea Zunino, Nikolaos Tsagarakis, Enrico Mingo Hoffman, Arash Ajoudani</author><pubDate>Wed, 31 Jan 2024 14:31:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19413v2</guid></item><item><title>Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers</title><link>http://arxiv.org/abs/2401.17870v1</link><description>Subseasonal forecasting, which is pivotal for agriculture, water resourcemanagement, and early warning of disasters, faces challenges due to the chaoticnature of the atmosphere. Recent advances in machine learning (ML) haverevolutionized weather forecasting by achieving competitive predictive skillsto numerical models. However, training such foundation models requiresthousands of GPU days, which causes substantial carbon emissions and limitstheir broader applicability. Moreover, ML models tend to fool the pixel-wiseerror scores by producing smoothed results which lack physical consistency andmeteorological meaning. To deal with the aforementioned problems, we propose ateleconnection-informed transformer. Our architecture leverages the pretrainedPangu model to achieve good initial weights and integrates ateleconnection-informed temporal module to improve predictability in anextended temporal range. Remarkably, by adjusting 1.1% of the Pangu model'sparameters, our method enhances predictability on four surface and fiveupper-level atmospheric variables at a two-week lead time. Furthermore, theteleconnection-filtered features improve the spatial granularity of outputssignificantly, indicating their potential physical consistency. Our researchunderscores the importance of atmospheric and oceanic teleconnections indriving future weather conditions. Besides, it presents a resource-efficientpathway for researchers to leverage existing foundation models on versatiledownstream tasks.</description><author>Shan Zhao, Zhitong Xiong, Xiao Xiang Zhu</author><pubDate>Wed, 31 Jan 2024 14:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17870v1</guid></item><item><title>Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model</title><link>http://arxiv.org/abs/2401.17868v1</link><description>The Segment Anything Model (SAM) stands as a foundational framework for imagesegmentation. While it exhibits remarkable zero-shot generalization in typicalscenarios, its advantage diminishes when applied to specialized domains likemedical imagery and remote sensing. To address this limitation, this paperintroduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuningapproach. By integrating ultra-lightweight convolutional parameters intoLow-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biasesinto the plain ViT encoder, further reinforcing SAM's local prior assumption.Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledgebut also revives its capacity of learning high-level image semantics, which isconstrained by SAM's foreground-background segmentation pretraining.Comprehensive experimentation across diverse benchmarks spanning multipledomains underscores Conv-LoRA's superiority in adapting SAM to real-worldsemantic segmentation tasks.</description><author>Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, Chun Yuan</author><pubDate>Wed, 31 Jan 2024 14:27:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17868v1</guid></item><item><title>Leveraging Multi-lingual Positive Instances in Contrastive Learning to Improve Sentence Embedding</title><link>http://arxiv.org/abs/2309.08929v2</link><description>Learning multi-lingual sentence embeddings is a fundamental task in naturallanguage processing. Recent trends in learning both mono-lingual andmulti-lingual sentence embeddings are mainly based on contrastive learning (CL)among an anchor, one positive, and multiple negative instances. In this work,we argue that leveraging multiple positives should be considered formulti-lingual sentence embeddings because (1) positives in a diverse set oflanguages can benefit cross-lingual learning, and (2) transitive similarityacross multiple positives can provide reliable structural information forlearning. In order to investigate the impact of multiple positives in CL, wepropose a novel approach, named MPCL, to effectively utilize multiple positiveinstances to improve the learning of multi-lingual sentence embeddings.Experimental results on various backbone models and downstream tasksdemonstrate that MPCL leads to better retrieval, semantic similarity, andclassification performances compared to conventional CL. We also observe thatin unseen languages, sentence embedding models trained on multiple positivesshow better cross-lingual transfer performance than models trained on a singlepositive instance.</description><author>Kaiyan Zhao, Qiyu Wu, Xin-Qiang Cai, Yoshimasa Tsuruoka</author><pubDate>Wed, 31 Jan 2024 14:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08929v2</guid></item><item><title>Making Sense of Knowledge Intensive Processes: an Oil &amp; Gas Industry Scenario</title><link>http://arxiv.org/abs/2401.17866v1</link><description>Sensemaking is a constant and ongoing process by which people associatemeaning to experiences. It can be an individual process, known as abduction, ora group process by which people give meaning to collective experiences. Thesensemaking of a group is influenced by the abduction process of each personabout the experience. Every collaborative process needs some level ofsensemaking to show results. For a knowledge intensive process, sensemaking iscentral and related to most of its tasks. We present findings from a fieldworkexecuted in knowledge intensive process from the Oil and Gas industry. Ourfindings indicated that different types of knowledge can be combined to composethe result of a sensemaking process (e.g. decision, the need for morediscussion, etc.). This paper presents an initial set of knowledge types thatcan be combined to compose the result of the sensemaking of a collaborativedecision making process. We also discuss ideas for using systems powered byArtificial Intelligence to support sensemaking processes.</description><author>Juliana Jansen Ferreira, Vinícius Segura, Ana Fucs, Rogério de Paula</author><pubDate>Wed, 31 Jan 2024 14:25:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17866v1</guid></item><item><title>Manipulating Predictions over Discrete Inputs in Machine Teaching</title><link>http://arxiv.org/abs/2401.17865v1</link><description>Machine teaching often involves the creation of an optimal (typicallyminimal) dataset to help a model (referred to as the `student') achievespecific goals given by a teacher. While abundant in the continuous domain, thestudies on the effectiveness of machine teaching in the discrete domain arerelatively limited. This paper focuses on machine teaching in the discretedomain, specifically on manipulating student models' predictions based on thegoals of teachers via changing the training data efficiently. We formulate thistask as a combinatorial optimization problem and solve it by proposing aniterative searching algorithm. Our algorithm demonstrates significant numericalmerit in the scenarios where a teacher attempts at correcting erroneouspredictions to improve the student's models, or maliciously manipulating themodel to misclassify some specific samples to the target class aligned with hispersonal profits. Experimental results show that our proposed algorithm canhave superior performance in effectively and efficiently manipulating thepredictions of the model, surpassing conventional baselines.</description><author>Xiaodong Wu, Yufei Han, Hayssam Dahrouj, Jianbing Ni, Zhenwen Liang, Xiangliang Zhang</author><pubDate>Wed, 31 Jan 2024 14:23:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17865v1</guid></item><item><title>Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis</title><link>http://arxiv.org/abs/2401.17862v1</link><description>Multi-modal large language models (MLLMs) have demonstrated remarkablevision-language capabilities, primarily due to the exceptional in-contextunderstanding and multi-task learning strengths of large language models(LLMs). The advent of visual instruction tuning has further enhanced MLLMs'performance in vision-language understanding. However, while existing MLLMsadeptly recognize \textit{what} objects are in an image, they still facechallenges in effectively discerning \textit{where} these objects are,particularly along the distance (scene depth) axis. To overcome this limitationin MLLMs, we introduce Proximity Question Answering (Proximity QA), a novelframework designed to enable MLLMs to infer the proximity relationship betweenobjects in images. The framework operates in two phases: the first phasefocuses on guiding the models to understand the relative depth of objects, andthe second phase further encourages the models to infer the proximityrelationships between objects based on their depth perceptions. We also proposea VQA dataset called Proximity-110K, containing additional instructions thatincorporate depth information and the proximity relationships of objects. Wehave conducted extensive experiments to validate Proximity QA's superiorability in depth perception and proximity analysis, outperforming otherstate-of-the-art MLLMs. Code and dataset will be released at\textcolor{magenta}{https://github.com/NorthSummer/ProximityQA.git}.</description><author>Jianing Li, Xi Nan, Ming Lu, Li Du, Shanghang Zhang</author><pubDate>Wed, 31 Jan 2024 14:21:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17862v1</guid></item><item><title>Probing Language Models' Gesture Understanding for Enhanced Human-AI Interaction</title><link>http://arxiv.org/abs/2401.17858v1</link><description>The rise of Large Language Models (LLMs) has affected various disciplinesthat got beyond mere text generation. Going beyond their textual nature, thisproject proposal aims to investigate the interaction between LLMs andnon-verbal communication, specifically focusing on gestures. The proposal setsout a plan to examine the proficiency of LLMs in deciphering both explicit andimplicit non-verbal cues within textual prompts and their ability to associatethese gestures with various contextual factors. The research proposes to testestablished psycholinguistic study designs to construct a comprehensive datasetthat pairs textual prompts with detailed gesture descriptions, encompassingdiverse regional variations, and semantic labels. To assess LLMs' comprehensionof gestures, experiments are planned, evaluating their ability to simulatehuman behaviour in order to replicate psycholinguistic experiments. Theseexperiments consider cultural dimensions and measure the agreement betweenLLM-identified gestures and the dataset, shedding light on the models'contextual interpretation of non-verbal cues (e.g. gestures).</description><author>Philipp Wicke</author><pubDate>Wed, 31 Jan 2024 14:19:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17858v1</guid></item><item><title>Semantic Anything in 3D Gaussians</title><link>http://arxiv.org/abs/2401.17857v1</link><description>3D Gaussian Splatting has emerged as an alternative 3D representation ofNeural Radiance Fields (NeRFs), benefiting from its high-quality renderingresults and real-time rendering speed. Considering the 3D Gaussianrepresentation remains unparsed, it is necessary first to execute objectsegmentation within this domain. Subsequently, scene editing and collisiondetection can be performed, proving vital to a multitude of applications, suchas virtual reality (VR), augmented reality (AR), game/movie production, etc. Inthis paper, we propose a novel approach to achieve object segmentation in 3DGaussian via an interactive procedure without any training process and learnedparameters. We refer to the proposed method as SA-GS, for Segment Anything in3D Gaussians. Given a set of clicked points in a single input view, SA-GS cangeneralize SAM to achieve 3D consistent segmentation via the proposedmulti-view mask generation and view-wise label assignment methods. We alsopropose a cross-view label-voting approach to assign labels from differentviews. In addition, in order to address the boundary roughness issue ofsegmented objects resulting from the non-negligible spatial sizes of 3DGaussian located at the boundary, SA-GS incorporates the simple but effectiveGaussian Decomposition scheme. Extensive experiments demonstrate that SA-GSachieves high-quality 3D segmentation results, which can also be easily appliedfor scene editing and collision detection tasks. Codes will be released soon.</description><author>Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, Zhaoxiang Zhang</author><pubDate>Wed, 31 Jan 2024 14:19:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17857v1</guid></item><item><title>Instruction-Guided Scene Text Recognition</title><link>http://arxiv.org/abs/2401.17851v1</link><description>Multi-modal models have shown appealing performance in visual tasks recently,as instruction-guided training has evoked the ability to understandfine-grained visual content. However, current methods cannot be triviallyapplied to scene text recognition (STR) due to the gap between natural and textimages. In this paper, we introduce a novel paradigm that formulates STR as aninstruction learning problem, and propose instruction-guided scene textrecognition (IGTR) to achieve effective cross-modal learning. IGTR firstgenerates rich and diverse instruction triplets of &lt;condition,question,answer&gt;,serving as guidance for nuanced text image understanding. Then, we devise anarchitecture with dedicated cross-modal feature fusion module, and multi-taskanswer head to effectively fuse the required instruction and image features foranswering questions. Built upon these designs, IGTR facilitates accurate textrecognition by comprehending character attributes. Experiments on English andChinese benchmarks show that IGTR outperforms existing models by significantmargins. Furthermore, by adjusting the instructions, IGTR enables variousrecognition schemes. These include zero-shot prediction, where the model istrained based on instructions not explicitly targeting character recognition,and the recognition of rarely appearing and morphologically similar characters,which were previous challenges for existing models.</description><author>Yongkun Du, Zhineng Chen, Yuchen Su, Caiyan Jia, Yu-Gang Jiang</author><pubDate>Wed, 31 Jan 2024 14:13:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17851v1</guid></item><item><title>ConcatPlexer: Additional Dim1 Batching for Faster ViTs</title><link>http://arxiv.org/abs/2308.11199v2</link><description>Transformers have demonstrated tremendous success not only in the naturallanguage processing (NLP) domain but also the field of computer vision,igniting various creative approaches and applications. Yet, the superiorperformance and modeling flexibility of transformers came with a severeincrease in computation costs, and hence several works have proposed methods toreduce this burden. Inspired by a cost-cutting method originally proposed forlanguage models, Data Multiplexing (DataMUX), we propose a novel approach forefficient visual recognition that employs additional dim1 batching (i.e.,concatenation) that greatly improves the throughput with little compromise inthe accuracy. We first introduce a naive adaptation of DataMux for visionmodels, Image Multiplexer, and devise novel components to overcome itsweaknesses, rendering our final model, ConcatPlexer, at the sweet spot betweeninference speed and accuracy. The ConcatPlexer was trained on ImageNet1K andCIFAR100 dataset and it achieved 23.5% less GFLOPs than ViT-B/16 with 69.5% and83.4% validation accuracy, respectively.</description><author>Donghoon Han, Seunghyeon Seo, Donghyeon Jeon, Jiho Jang, Chaerin Kong, Nojun Kwak</author><pubDate>Wed, 31 Jan 2024 14:11:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11199v2</guid></item><item><title>Efficient Learning of Long-Range and Equivariant Quantum Systems</title><link>http://arxiv.org/abs/2312.17019v2</link><description>In this work, we consider a fundamental task in quantum many-body physics -finding and learning ground states of quantum Hamiltonians and theirproperties. Recent works have studied the task of predicting the ground stateexpectation value of sums of geometrically local observables by learning fromdata. For short-range gapped Hamiltonians, a sample complexity that islogarithmic in the number of qubits and quasipolynomial in the error wasobtained. Here we extend these results beyond the local requirements on bothHamiltonians and observables, motivated by the relevance of long-rangeinteractions in molecular and atomic systems. For interactions decaying as apower law with exponent greater than twice the dimension of the system, werecover the same efficient logarithmic scaling with respect to the number ofqubits, but the dependence on the error worsens to exponential. Further, weshow that learning algorithms equivariant under the automorphism group of theinteraction hypergraph achieve a sample complexity reduction, leading inparticular to a constant number of samples for learning sums of localobservables in systems with periodic boundary conditions. We demonstrate theefficient scaling in practice by learning from DMRG simulations of $1$Dlong-range and disordered systems with up to $128$ qubits. Finally, we providean analysis of the concentration of expectation values of global observablesstemming from the central limit theorem, resulting in increased predictionaccuracy.</description><author>Štěpán Šmíd, Roberto Bondesan</author><pubDate>Wed, 31 Jan 2024 14:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17019v2</guid></item><item><title>Fundamental Limits of Membership Inference Attacks on Machine Learning Models</title><link>http://arxiv.org/abs/2310.13786v3</link><description>Membership inference attacks (MIA) can reveal whether a particular data pointwas part of the training dataset, potentially exposing sensitive informationabout individuals. This article provides theoretical guarantees by exploringthe fundamental statistical limitations associated with MIAs on machinelearning models. More precisely, we first derive the statistical quantity thatgoverns the effectiveness and success of such attacks. We then deduce that in avery general regression setting with overfitting algorithms, attacks may have ahigh probability of success. Finally, we investigate several situations forwhich we provide bounds on this quantity of interest. Our results enable us todeduce the accuracy of potential attacks based on the number of samples andother structural parameters of learning models. In certain instances, theseparameters can be directly estimated from the dataset.</description><author>Eric Aubinais, Elisabeth Gassiat, Pablo Piantanida</author><pubDate>Wed, 31 Jan 2024 14:04:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13786v3</guid></item><item><title>Explainable Benchmarking for Iterative Optimization Heuristics</title><link>http://arxiv.org/abs/2401.17842v1</link><description>Benchmarking heuristic algorithms is vital to understand under whichconditions and on what kind of problems certain algorithms perform well. Inmost current research into heuristic optimization algorithms, only a verylimited number of scenarios, algorithm configurations and hyper-parametersettings are explored, leading to incomplete and often biased insights andresults. This paper presents a novel approach we call explainable benchmarking.Introducing the IOH-Xplainer software framework, for analyzing andunderstanding the performance of various optimization algorithms and the impactof their different components and hyper-parameters. We showcase the frameworkin the context of two modular optimization frameworks. Through this framework,we examine the impact of different algorithmic components and configurations,offering insights into their performance across diverse scenarios. We provide asystematic method for evaluating and interpreting the behaviour and efficiencyof iterative optimization heuristics in a more transparent and comprehensiblemanner, allowing for better benchmarking and algorithm design.</description><author>Niki van Stein, Diederick Vermetten, Anna V. Kononova, Thomas Bäck</author><pubDate>Wed, 31 Jan 2024 14:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17842v1</guid></item><item><title>Global-Liar: Factuality of LLMs over Time and Geographic Regions</title><link>http://arxiv.org/abs/2401.17839v1</link><description>The increasing reliance on AI-driven solutions, particularly Large LanguageModels (LLMs) like the GPT series, for information retrieval highlights thecritical need for their factuality and fairness, especially amidst the rampantspread of misinformation and disinformation online. Our study evaluates thefactual accuracy, stability, and biases in widely adopted GPT models, includingGPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediatedinformation dissemination. We introduce 'Global-Liar,' a dataset uniquely balanced in terms ofgeographic and temporal representation, facilitating a more nuanced evaluationof LLM biases. Our analysis reveals that newer iterations of GPT models do notalways equate to improved performance. Notably, the GPT-4 version from Marchdemonstrates higher factual accuracy than its subsequent June release.Furthermore, a concerning bias is observed, privileging statements from theGlobal North over the Global South, thus potentially exacerbating existinginformational inequities. Regions such as Africa and the Middle East are at adisadvantage, with much lower factual accuracy. The performance fluctuationsover time suggest that model updates may not consistently benefit all regionsequally. Our study also offers insights into the impact of various LLM configurationsettings, such as binary decision forcing, model re-runs and temperature, onmodel's factuality. Models constrained to binary (true/false) choices exhibitreduced factuality compared to those allowing an 'unclear' option. Singleinference at a low temperature setting matches the reliability of majorityvoting across various configurations. The insights gained highlight the needfor culturally diverse and geographically inclusive model training andevaluation. This approach is key to achieving global equity in technology,distributing AI benefits fairly worldwide.</description><author>Shujaat Mirza, Bruno Coelho, Yuyuan Cui, Christina Pöpper, Damon McCoy</author><pubDate>Wed, 31 Jan 2024 13:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17839v1</guid></item><item><title>A Cross-View Hierarchical Graph Learning Hypernetwork for Skill Demand-Supply Joint Prediction</title><link>http://arxiv.org/abs/2401.17838v1</link><description>The rapidly changing landscape of technology and industries leads to dynamicskill requirements, making it crucial for employees and employers to anticipatesuch shifts to maintain a competitive edge in the labor market. Existingefforts in this area either rely on domain-expert knowledge or regarding skillevolution as a simplified time series forecasting problem. However, bothapproaches overlook the sophisticated relationships among different skills andthe inner-connection between skill demand and supply variations. In this paper,we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH)framework for joint skill demand-supply prediction. Specifically, CHGH is anencoder-decoder network consisting of i) a cross-view graph encoder to capturethe interconnection between skill demand and supply, ii) a hierarchical graphencoder to model the co-evolution of skills from a cluster-wise perspective,and iii) a conditional hyper-decoder to jointly predict demand and supplyvariations by incorporating historical demand-supply gaps. Extensiveexperiments on three real-world datasets demonstrate the superiority of theproposed framework compared to seven baselines and the effectiveness of thethree modules.</description><author>Wenshuo Chao, Zhaopeng Qiu, Likang Wu, Zhuoning Guo, Zhi Zheng, Hengshu Zhu, Hao Liu</author><pubDate>Wed, 31 Jan 2024 13:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17838v1</guid></item><item><title>Predicting the Future with Simple World Models</title><link>http://arxiv.org/abs/2401.17835v1</link><description>World models can represent potentially high-dimensional pixel observations incompact latent spaces, making it tractable to model the dynamics of theenvironment. However, the latent dynamics inferred by these models may still behighly complex. Abstracting the dynamics of the environment with simple modelscan have several benefits. If the latent dynamics are simple, the model maygeneralize better to novel transitions, and discover useful latentrepresentations of environment states. We propose a regularization scheme thatsimplifies the world model's latent dynamics. Our model, the ParsimoniousLatent Space Model (PLSM), minimizes the mutual information between latentstates and the dynamics that arise between them. This makes the dynamics softlystate-invariant, and the effects of the agent's actions more predictable. Wecombine the PLSM with three different model classes used for i) future latentstate prediction, ii) video prediction, and iii) planning. We find that ourregularization improves accuracy, generalization, and performance in downstreamtasks.</description><author>Tankred Saanum, Peter Dayan, Eric Schulz</author><pubDate>Wed, 31 Jan 2024 13:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17835v1</guid></item><item><title>Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2401.17828v1</link><description>In recent years, weakly supervised semantic segmentation using image-levellabels as supervision has received significant attention in the field ofcomputer vision. Most existing methods have addressed the challenges arisingfrom the lack of spatial information in these labels by focusing onfacilitating supervised learning through the generation of pseudo-labels fromclass activation maps (CAMs). Due to the localized pattern detection ofConvolutional Neural Networks (CNNs), CAMs often emphasize only the mostdiscriminative parts of an object, making it challenging to accuratelydistinguish foreground objects from each other and the background. Recentstudies have shown that Vision Transformer (ViT) features, due to their globalview, are more effective in capturing the scene layout than CNNs. However, theuse of hierarchical ViTs has not been extensively explored in this field. Thiswork explores the use of Swin Transformer by proposing "SWTformer" to enhancethe accuracy of the initial seed CAMs by bringing local and global viewstogether. SWTformer-V1 generates class probabilities and CAMs using only thepatch tokens as features. SWTformer-V2 incorporates a multi-scale featurefusion mechanism to extract additional information and utilizes abackground-aware mechanism to generate more accurate localization maps withimproved cross-object discrimination. Based on experiments on the PascalVOC2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy,outperforming state-of-the-art models. It also yields comparable performance by0.82% mIoU on average higher than other methods in generating initiallocalization maps, depending only on the classification network. SWTformer-V2further improves the accuracy of the generated seed CAMs by 5.32% mIoU, furtherproving the effectiveness of the local-to-global view provided by the Swintransformer.</description><author>Rozhan Ahmadi, Shohreh Kasaei</author><pubDate>Wed, 31 Jan 2024 13:41:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17828v1</guid></item><item><title>Neural Machine Translation for Malayalam Paraphrase Generation</title><link>http://arxiv.org/abs/2401.17827v1</link><description>This study explores four methods of generating paraphrases in Malayalam,utilizing resources available for English paraphrasing and pre-trained NeuralMachine Translation (NMT) models. We evaluate the resulting paraphrases usingboth automated metrics, such as BLEU, METEOR, and cosine similarity, as well ashuman annotation. Our findings suggest that automated evaluation measures maynot be fully appropriate for Malayalam, as they do not consistently align withhuman judgment. This discrepancy underscores the need for more nuancedparaphrase evaluation approaches especially for highly agglutinative languages.</description><author>Christeena Varghese, Sergey Koshelev, Ivan P. Yamshchikov</author><pubDate>Wed, 31 Jan 2024 13:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17827v1</guid></item><item><title>A Survey of Pre-trained Language Models for Processing Scientific Text</title><link>http://arxiv.org/abs/2401.17824v1</link><description>The number of Language Models (LMs) dedicated to processing scientific textis on the rise. Keeping pace with the rapid growth of scientific LMs (SciLMs)has become a daunting task for researchers. To date, no comprehensive surveyson SciLMs have been undertaken, leaving this issue unaddressed. Given theconstant stream of new SciLMs, appraising the state-of-the-art and how theycompare to each other remain largely unknown. This work fills that gap andprovides a comprehensive review of SciLMs, including an extensive analysis oftheir effectiveness across different domains, tasks and datasets, and adiscussion on the challenges that lie ahead.</description><author>Xanh Ho, Anh Khoa Duong Nguyen, An Tuan Dao, Junfeng Jiang, Yuki Chida, Kaito Sugimoto, Huy Quoc To, Florian Boudin, Akiko Aizawa</author><pubDate>Wed, 31 Jan 2024 13:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17824v1</guid></item><item><title>$μ$PLAN: Summarizing using a Content Plan as Cross-Lingual Bridge</title><link>http://arxiv.org/abs/2305.14205v2</link><description>Cross-lingual summarization consists of generating a summary in one languagegiven an input document in a different language, allowing for the disseminationof relevant content across speakers of other languages. The task is challengingmainly due to the paucity of cross-lingual datasets and the compoundeddifficulty of summarizing and translating. This work presents $\mu$PLAN, anapproach to cross-lingual summarization that uses an intermediate planning stepas a cross-lingual bridge. We formulate the plan as a sequence of entitiescapturing the summary's content and the order in which it should becommunicated. Importantly, our plans abstract from surface form: using amultilingual knowledge base, we align entities to their canonical designationacross languages and generate the summary conditioned on this cross-lingualbridge and the input. Automatic and human evaluation on the XWikis dataset(across four language pairs) demonstrates that our planning objective achievesstate-of-the-art performance in terms of informativeness and faithfulness.Moreover, $\mu$PLAN models improve the zero-shot transfer to new cross-linguallanguage pairs compared to baselines without a planning component.</description><author>Fantine Huot, Joshua Maynez, Chris Alberti, Reinald Kim Amplayo, Priyanka Agrawal, Constanza Fierro, Shashi Narayan, Mirella Lapata</author><pubDate>Wed, 31 Jan 2024 13:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14205v2</guid></item><item><title>Privacy-preserving data release leveraging optimal transport and particle gradient descent</title><link>http://arxiv.org/abs/2401.17823v1</link><description>We present a novel approach for differentially private data synthesis ofprotected tabular datasets, a relevant task in highly sensitive domains such ashealthcare and government. Current state-of-the-art methods predominantly usemarginal-based approaches, where a dataset is generated from private estimatesof the marginals. In this paper, we introduce PrivPGD, a new generation methodfor marginal-based private data synthesis, leveraging tools from optimaltransport and particle gradient descent. Our algorithm outperforms existingmethods on a large range of datasets while being highly scalable and offeringthe flexibility to incorporate additional domain-specific constraints.</description><author>Konstantin Donhauser, Javier Abad, Neha Hulkund, Fanny Yang</author><pubDate>Wed, 31 Jan 2024 13:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17823v1</guid></item><item><title>Do Object Detection Localization Errors Affect Human Performance and Trust?</title><link>http://arxiv.org/abs/2401.17821v1</link><description>Bounding boxes are often used to communicate automatic object detectionresults to humans, aiding humans in a multitude of tasks. We investigate therelationship between bounding box localization errors and human taskperformance. We use observer performance studies on a visual multi-objectcounting task to measure both human trust and performance with different levelsof bounding box accuracy. The results show that localization errors have nosignificant impact on human accuracy or trust in the system. Recall andprecision errors impact both human performance and trust, suggesting thatoptimizing algorithms based on the F1 score is more beneficial inhuman-computer tasks. Lastly, the paper offers an improvement on bounding boxesin multi-object counting tasks with center dots, showing improved performanceand better resilience to localization inaccuracy.</description><author>Sven de Witte, Ombretta Strafforello, Jan van Gemert</author><pubDate>Wed, 31 Jan 2024 13:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17821v1</guid></item><item><title>Domain generalization across tumor types, laboratories, and species -- insights from the 2022 edition of the Mitosis Domain Generalization Challenge</title><link>http://arxiv.org/abs/2309.15589v2</link><description>Recognition of mitotic figures in histologic tumor specimens is highlyrelevant to patient outcome assessment. This task is challenging for algorithmsand human experts alike, with deterioration of algorithmic performance undershifts in image representations. Considerable covariate shifts occur whenassessment is performed on different tumor types, images are acquired usingdifferent digitization devices, or specimens are produced in differentlaboratories. This observation motivated the inception of the 2022 challenge onMItosis Domain Generalization (MIDOG 2022). The challenge provided annotatedhistologic tumor images from six different domains and evaluated thealgorithmic approaches for mitotic figure detection provided by nine challengeparticipants on ten independent domains. Ground truth for mitotic figuredetection was established in two ways: a three-expert consensus and anindependent, immunohistochemistry-assisted set of labels. This work representsan overview of the challenge tasks, the algorithmic strategies employed by theparticipants, and potential factors contributing to their success. With an$F_1$ score of 0.764 for the top-performing team, we summarize that domaingeneralization across various tumor domains is possible with today's deeplearning-based recognition pipelines. However, we also found that domaincharacteristics not present in the training set (feline as new species, spindlecell shape as new morphology and a new scanner) led to small but significantdecreases in performance. When assessed against theimmunohistochemistry-assisted reference standard, all methods resulted inreduced recall scores, but with only minor changes in the order of participantsin the ranking.</description><author>Marc Aubreville, Nikolas Stathonikos, Taryn A. Donovan, Robert Klopfleisch, Jonathan Ganz, Jonas Ammeling, Frauke Wilm, Mitko Veta, Samir Jabari, Markus Eckstein, Jonas Annuscheit, Christian Krumnow, Engin Bozaba, Sercan Cayir, Hongyan Gu, Xiang 'Anthony' Chen, Mostafa Jahanifar, Adam Shephard, Satoshi Kondo, Satoshi Kasai, Sujatha Kotte, VG Saipradeep, Maxime W. Lafarge, Viktor H. Koelzer, Ziyue Wang, Yongbing Zhang, Sen Yang, Xiyue Wang, Katharina Breininger, Christof A. Bertram</author><pubDate>Wed, 31 Jan 2024 13:19:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15589v2</guid></item><item><title>On-the-fly Denoising for Data Augmentation in Natural Language Understanding</title><link>http://arxiv.org/abs/2212.10558v2</link><description>Data Augmentation (DA) is frequently used to provide additional training datawithout extra human annotation automatically. However, data augmentation mayintroduce noisy data that impairs training. To guarantee the quality ofaugmented data, existing methods either assume no noise exists in the augmenteddata and adopt consistency training or use simple heuristics such as trainingloss and diversity constraints to filter out "noisy" data. However, thosefiltered examples may still contain useful information, and dropping themcompletely causes a loss of supervision signals. In this paper, based on theassumption that the original dataset is cleaner than the augmented data, wepropose an on-the-fly denoising technique for data augmentation that learnsfrom soft augmented labels provided by an organic teacher model trained on thecleaner original data. To further prevent overfitting on noisy labels, a simpleself-regularization module is applied to force the model prediction to beconsistent across two distinct dropouts. Our method can be applied to generalaugmentation techniques and consistently improve the performance on both textclassification and question-answering tasks.</description><author>Tianqing Fang, Wenxuan Zhou, Fangyu Liu, Hongming Zhang, Yangqiu Song, Muhao Chen</author><pubDate>Wed, 31 Jan 2024 13:14:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10558v2</guid></item><item><title>Deterministic Computing Power Networking: Architecture, Technologies and Prospects</title><link>http://arxiv.org/abs/2401.17812v1</link><description>With the development of new Internet services such as computation-intensiveand delay-sensitive tasks, the traditional "Best Effort" network transmissionmode has been greatly challenged. The network system is urgently required toprovide end-to-end transmission determinacy and computing determinacy for newapplications to ensure the safe and efficient operation of services. Based onthe research of the convergence of computing and networking, a new networkparadigm named deterministic computing power networking (Det-CPN) is proposed.In this article, we firstly introduce the research advance of computing powernetworking. And then the motivations and scenarios of Det-CPN are analyzed.Following that, we present the system architecture, technological capabilities,workflow as well as key technologies for Det-CPN. Finally, the challenges andfuture trends of Det-CPN are analyzed and discussed.</description><author>Qingmin Jia, Yujiao Hu, Xiaomao Zhou, Qianpiao Ma, Kai Guo, Huayu Zhang, Renchao Xie, Tao Huang, Yunjie Liu</author><pubDate>Wed, 31 Jan 2024 13:12:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17812v1</guid></item></channel></rss>