<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 01 Aug 2024 13:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey</title><link>http://arxiv.org/abs/2407.21794v1</link><description>Detecting out-of-distribution (OOD) samples is crucial for ensuring thesafety of machine learning systems and has shaped the field of OOD detection.Meanwhile, several other problems are closely related to OOD detection,including anomaly detection (AD), novelty detection (ND), open set recognition(OSR), and outlier detection (OD). To unify these problems, a generalized OODdetection framework was proposed, taxonomically categorizing these fiveproblems. However, Vision Language Models (VLMs) such as CLIP havesignificantly changed the paradigm and blurred the boundaries between thesefields, again confusing researchers. In this survey, we first present ageneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OODdetection, and OD in the VLM era. Our framework reveals that, with some fieldinactivity and integration, the demanding challenges have become OOD detectionand AD. In addition, we also highlight the significant shift in the definition,problem settings, and benchmarks; we thus feature a comprehensive review of themethodology for OOD detection, including the discussion over other relatedtasks to clarify their relationship to OOD detection. Finally, we explore theadvancements in the emerging Large Vision Language Model (LVLM) era, such asGPT-4V. We conclude this survey with open challenges and future directions.</description><author>Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin, Qing Yu, Go Irie, Shafiq Joty, Yixuan Li, Hai Li, Ziwei Liu, Toshihiko Yamasaki, Kiyoharu Aizawa</author><pubDate>Wed, 31 Jul 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21794v1</guid></item><item><title>Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?</title><link>http://arxiv.org/abs/2407.21792v1</link><description>As artificial intelligence systems grow more powerful, there has beenincreasing interest in "AI safety" research to address emerging and futurerisks. However, the field of AI safety remains poorly defined andinconsistently measured, leading to confusion about how researchers cancontribute. This lack of clarity is compounded by the unclear relationshipbetween AI safety benchmarks and upstream general capabilities (e.g., generalknowledge and reasoning). To address these issues, we conduct a comprehensivemeta-analysis of AI safety benchmarks, empirically analyzing their correlationwith general capabilities across dozens of models and providing a survey ofexisting directions in AI safety. Our findings reveal that many safetybenchmarks highly correlate with upstream model capabilities, potentiallyenabling "safetywashing" -- where capability improvements are misrepresented assafety advancements. Based on these findings, we propose an empiricalfoundation for developing more meaningful safety metrics and define AI safetyin a machine learning research context as a set of clearly delineated researchgoals that are empirically separable from generic capabilities advancements. Indoing so, we aim to provide a more rigorous framework for AI safety research,advancing the science of safety evaluations and clarifying the path towardsmeasurable progress.</description><author>Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks</author><pubDate>Wed, 31 Jul 2024 17:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21792v1</guid></item><item><title>Deep Learning for Options Trading: An End-To-End Approach</title><link>http://arxiv.org/abs/2407.21791v1</link><description>We introduce a novel approach to options trading strategies using a highlyscalable and data-driven machine learning algorithm. In contrast to traditionalapproaches that often require specifications of underlying market dynamics orassumptions on an option pricing model, our models depart fundamentally fromthe need for these prerequisites, directly learning non-trivial mappings frommarket data to optimal trading signals. Backtesting on more than a decade ofoption contracts for equities listed on the S&amp;P 100, we demonstrate that deeplearning models trained according to our end-to-end approach exhibitsignificant improvements in risk-adjusted performance over existing rules-basedtrading strategies. We find that incorporating turnover regularization into themodels leads to further performance enhancements at prohibitively high levelsof transaction costs.</description><author>Wee Ling Tan, Stephen Roberts, Stefan Zohren</author><pubDate>Wed, 31 Jul 2024 17:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21791v1</guid></item><item><title>PerAct2: Benchmarking and Learning for Robotic Bimanual Manipulation Tasks</title><link>http://arxiv.org/abs/2407.00278v2</link><description>Bimanual manipulation is challenging due to precise spatial and temporalcoordination required between two arms. While there exist several real-worldbimanual systems, there is a lack of simulated benchmarks with a large taskdiversity for systematically studying bimanual capabilities across a wide rangeof tabletop tasks. This paper addresses the gap by extending RLBench tobimanual manipulation. We open-source our code and benchmark comprising 13 newtasks with 23 unique task variations, each requiring a high degree ofcoordination and adaptability. To kickstart the benchmark, we extended severalstate-of-the art methods to bimanual manipulation and also present alanguage-conditioned behavioral cloning agent -- PerAct2, which enables thelearning and execution of bimanual 6-DoF manipulation tasks. Our novel networkarchitecture efficiently integrates language processing with action prediction,allowing robots to understand and perform complex bimanual tasks in response touser-specified goals. Project website with code is available at:http://bimanual.github.io</description><author>Markus Grotz, Mohit Shridhar, Tamim Asfour, Dieter Fox</author><pubDate>Wed, 31 Jul 2024 17:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00278v2</guid></item><item><title>Occam Gradient Descent</title><link>http://arxiv.org/abs/2405.20194v4</link><description>Deep learning neural network models must be large enough to adapt to theirproblem domain, while small enough to avoid overfitting training data duringgradient descent. To balance these competing demands, overprovisioned deeplearning models such as transformers are trained for a single epoch on largedata sets, and hence inefficient with both computing resources and trainingdata. In response to these inefficiencies, we exploit learning theory to deriveOccam Gradient Descent, an algorithm that interleaves adaptive reduction ofmodel size to minimize generalization error, with gradient descent on modelweights to minimize fitting error. In contrast, traditional gradient descentgreedily minimizes fitting error without regard to generalization error. Ouralgorithm simultaneously descends the space of weights and topological size ofany neural network without modification. With respect to loss, compute andmodel size, our experiments show (a) on image classification benchmarks, linearand convolutional neural networks trained with Occam Gradient Descentoutperform traditional gradient descent with or without post-train pruning; (b)on a range of tabular data classification tasks, neural networks trained withOccam Gradient Descent outperform traditional gradient descent, as well asRandom Forests; (c) on natural language transformers, Occam Gradient Descentoutperforms traditional gradient descent.</description><author>B. N. Kausik</author><pubDate>Wed, 31 Jul 2024 17:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20194v4</guid></item><item><title>Vision-Language Model Based Handwriting Verification</title><link>http://arxiv.org/abs/2407.21788v1</link><description>Handwriting Verification is a critical in document forensics. Deep learningbased approaches often face skepticism from forensic document examiners due totheir lack of explainability and reliance on extensive training data andhandcrafted features. This paper explores using Vision Language Models (VLMs),such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. Byleveraging their Visual Question Answering capabilities and 0-shotChain-of-Thought (CoT) reasoning, our goal is to provide clear,human-understandable explanations for model decisions. Our experiments on theCEDAR handwriting dataset demonstrate that VLMs offer enhancedinterpretability, reduce the need for large training datasets, and adapt betterto diverse handwriting styles. However, results show that the CNN-basedResNet-18 architecture outperforms the 0-shot CoT prompt engineering approachwith GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findingshighlight the potential of VLMs in generating human-interpretable decisionswhile underscoring the need for further advancements to match the performanceof specialized deep learning models.</description><author>Mihir Chauhan, Abhishek Satbhai, Mohammad Abuzar Hashemi, Mir Basheer Ali, Bina Ramamurthy, Mingchen Gao, Siwei Lyu, Sargur Srihari</author><pubDate>Wed, 31 Jul 2024 17:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21788v1</guid></item><item><title>Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</title><link>http://arxiv.org/abs/2407.21787v1</link><description>Scaling the amount of compute used to train language models has dramaticallyimproved their capabilities. However, when it comes to inference, we oftenlimit the amount of compute to only one attempt per problem. Here, we exploreinference compute as another axis for scaling by increasing the number ofgenerated samples. Across multiple tasks and models, we observe that coverage -the fraction of problems solved by any attempt - scales with the number ofsamples over four orders of magnitude. In domains like coding and formalproofs, where all answers can be automatically verified, these increases incoverage directly translate into improved performance. When we apply repeatedsampling to SWE-bench Lite, the fraction of issues solved withDeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250samples, outperforming the single-attempt state-of-the-art of 43% which usesmore capable frontier models. Moreover, using current API pricing, amplifyingthe cheaper DeepSeek model with five samples is more cost-effective and solvesmore issues than paying a premium for one sample from GPT-4o or Claude 3.5Sonnet. Interestingly, the relationship between coverage and the number ofsamples is often log-linear and can be modelled with an exponentiated powerlaw, suggesting the existence of inference-time scaling laws. Finally, we findthat identifying correct samples out of many generations remains an importantdirection for future research in domains without automatic verifiers. Whensolving math word problems from GSM8K and MATH, coverage with Llama-3 modelsgrows to over 95% with 10,000 samples. However, common methods to pick correctsolutions from a sample collection, such as majority voting or reward models,plateau beyond several hundred samples and fail to fully scale with the samplebudget.</description><author>Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, Azalia Mirhoseini</author><pubDate>Wed, 31 Jul 2024 17:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21787v1</guid></item><item><title>Can Editing LLMs Inject Harm?</title><link>http://arxiv.org/abs/2407.20224v2</link><description>Knowledge editing techniques have been increasingly adopted to efficientlycorrect the false or outdated knowledge in Large Language Models (LLMs), due tothe high cost of retraining from scratch. Meanwhile, one critical butunder-explored question is: can knowledge editing be used to inject harm intoLLMs? In this paper, we propose to reformulate knowledge editing as a new typeof safety threat for LLMs, namely Editing Attack, and conduct a systematicinvestigation with a newly constructed dataset EditAttack. Specifically, wefocus on two typical safety risks of Editing Attack including MisinformationInjection and Bias Injection. For the risk of misinformation injection, wefirst categorize it into commonsense misinformation injection and long-tailmisinformation injection. Then, we find that editing attacks can inject bothtypes of misinformation into LLMs, and the effectiveness is particularly highfor commonsense misinformation injection. For the risk of bias injection, wediscover that not only can biased sentences be injected into LLMs with higheffectiveness, but also one single biased sentence injection can cause a biasincrease in general outputs of LLMs, which are even highly irrelevant to theinjected sentence, indicating a catastrophic impact on the overall fairness ofLLMs. Then, we further illustrate the high stealthiness of editing attacks,measured by their impact on the general knowledge and reasoning capacities ofLLMs, and show the hardness of defending editing attacks with empiricalevidence. Our discoveries demonstrate the emerging misuse risks of knowledgeediting techniques on compromising the safety alignment of LLMs.</description><author>Canyu Chen, Baixiang Huang, Zekun Li, Zhaorun Chen, Shiyang Lai, Xiongxiao Xu, Jia-Chen Gu, Jindong Gu, Huaxiu Yao, Chaowei Xiao, Xifeng Yan, William Yang Wang, Philip Torr, Dawn Song, Kai Shu</author><pubDate>Wed, 31 Jul 2024 17:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20224v2</guid></item><item><title>MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning</title><link>http://arxiv.org/abs/2407.20999v2</link><description>Recently, large language models (LLMs) have demonstrated remarkablecapabilities in a wide range of tasks. Typically, an LLM is pre-trained onlarge corpora and subsequently fine-tuned on task-specific datasets. However,during fine-tuning, LLMs may forget the knowledge acquired in the pre-trainingstage, leading to a decline in general capabilities. To address this issue, wepropose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO).The key idea of MoFO is to iteratively select and update the model parameterswith the largest momentum magnitudes. Compared to full-parameter training, MoFOachieves similar fine-tuning performance while keeping parameters closer to thepre-trained model, thereby mitigating knowledge forgetting. Unlike mostexisting methods for forgetting mitigation, MoFO combines the following twoadvantages. First, MoFO does not require access to pre-training data. Thismakes MoFO particularly suitable for fine-tuning scenarios where pre-trainingdata is unavailable, such as fine-tuning checkpoint-only open-source LLMs.Second, MoFO does not alter the original loss function. This could avoidimpairing the model performance on the fine-tuning tasks. We validate MoFOthrough rigorous convergence analysis and extensive experiments, demonstratingits superiority over existing methods in mitigating forgetting and enhancingfine-tuning performance.</description><author>Yupeng Chen, Senmiao Wang, Zhihang Lin, Zeyu Qin, Yushun Zhang, Tian Ding, Ruoyu Sun</author><pubDate>Wed, 31 Jul 2024 17:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20999v2</guid></item><item><title>FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning</title><link>http://arxiv.org/abs/2402.13989v3</link><description>Federated learning (FL) is a promising framework for learning fromdistributed data while maintaining privacy. The development of efficient FLalgorithms encounters various challenges, including heterogeneous data andsystems, limited communication capacities, and constrained local computationalresources. Recently developed FedADMM methods show great resilience to bothdata and system heterogeneity. However, they still suffer from performancedeterioration if the hyperparameters are not carefully tuned. To address thisissue, we propose an inexact and self-adaptive FedADMM algorithm, termedFedADMM-InSa. First, we design an inexactness criterion for the clients' localupdates to eliminate the need for empirically setting the local trainingaccuracy. This inexactness criterion can be assessed by each clientindependently based on its unique condition, thereby reducing the localcomputational cost and mitigating the undesirable straggle effect. Theconvergence of the resulting inexact ADMM is proved under the assumption ofstrongly convex loss functions. Additionally, we present a self-adaptive schemethat dynamically adjusts each client's penalty parameter, enhancing algorithmrobustness by mitigating the need for empirical penalty parameter choices foreach client. Extensive numerical experiments on both synthetic and real-worlddatasets are conducted. As validated by some numerical tests, our proposedalgorithm can reduce the clients' local computational load significantly andalso accelerate the learning process compared to the vanilla FedADMM.</description><author>Yongcun Song, Ziqi Wang, Enrique Zuazua</author><pubDate>Wed, 31 Jul 2024 17:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13989v3</guid></item><item><title>The Llama 3 Herd of Models</title><link>http://arxiv.org/abs/2407.21783v1</link><description>Modern artificial intelligence (AI) systems are powered by foundation models.This paper presents a new set of foundation models, called Llama 3. It is aherd of language models that natively support multilinguality, coding,reasoning, and tool usage. Our largest model is a dense Transformer with 405Bparameters and a context window of up to 128K tokens. This paper presents anextensive empirical evaluation of Llama 3. We find that Llama 3 deliverscomparable quality to leading language models such as GPT-4 on a plethora oftasks. We publicly release Llama 3, including pre-trained and post-trainedversions of the 405B parameter language model and our Llama Guard 3 model forinput and output safety. The paper also presents the results of experiments inwhich we integrate image, video, and speech capabilities into Llama 3 via acompositional approach. We observe this approach performs competitively withthe state-of-the-art on image, video, and speech recognition tasks. Theresulting models are not yet being broadly released as they are still underdevelopment.</description><author>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michae</author><pubDate>Wed, 31 Jul 2024 17:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21783v1</guid></item><item><title>Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries</title><link>http://arxiv.org/abs/2407.21778v1</link><description>We introduce tulip agent, an architecture for autonomous LLM-based agentswith Create, Read, Update, and Delete access to a tool library containing apotentially large number of tools. In contrast to state-of-the-artimplementations, tulip agent does not encode the descriptions of all availabletools in the system prompt, which counts against the model's context window, orembed the entire prompt for retrieving suitable tools. Instead, the tulip agentcan recursively search for suitable tools in its extensible tool library,implemented exemplarily as a vector store. The tulip agent architecturesignificantly reduces inference costs, allows using even large tool libraries,and enables the agent to adapt and extend its set of tools. We evaluate thearchitecture with several ablation studies in a mathematics context anddemonstrate its generalizability with an application to robotics. A referenceimplementation and the benchmark are available atgithub.com/HRI-EU/tulip_agent.</description><author>Felix Ocker, Daniel Tanneberg, Julian Eggert, Michael Gienger</author><pubDate>Wed, 31 Jul 2024 17:50:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21778v1</guid></item><item><title>An Earth Rover dataset recorded at the ICRA@40 party</title><link>http://arxiv.org/abs/2407.05735v2</link><description>The ICRA conference is celebrating its $40^{th}$ anniversary in Rotterdam inSeptember 2024, with as highlight the Happy Birthday ICRA Party at the iconicHolland America Line Cruise Terminal. One month later the IROS conference willtake place, which will include the Earth Rover Challenge. In this challengeopen-world autonomous navigation models are studied truly open-world settings. As part of the Earth Rover Challenge several real-world navigation sets inseveral cities world-wide, like Auckland, Australia and Wuhan, China. The onlydataset recorded in the Netherlands is the small village Oudewater. Theproposal is to record a dataset with the robot used in the Earth RoverChallenge in Rotterdam, in front of the Holland America Line Cruise Terminal,before the festivities of the Happy Birthday ICRA Party start. See: https://github.com/SlamMate/vSLAM-on-FrodoBots-2K</description><author>Qi Zhang, Zhihao Lin, Arnoud Visser</author><pubDate>Wed, 31 Jul 2024 17:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05735v2</guid></item><item><title>RainMamba: Enhanced Locality Learning with State Space Models for Video Deraining</title><link>http://arxiv.org/abs/2407.21773v1</link><description>The outdoor vision systems are frequently contaminated by rain streaks andraindrops, which significantly degenerate the performance of visual tasks andmultimedia applications. The nature of videos exhibits redundant temporal cuesfor rain removal with higher stability. Traditional video deraining methodsheavily rely on optical flow estimation and kernel-based manners, which have alimited receptive field. Yet, transformer architectures, while enablinglong-term dependencies, bring about a significant increase in computationalcomplexity. Recently, the linear-complexity operator of the state space models(SSMs) has contrarily facilitated efficient long-term temporal modeling, whichis crucial for rain streaks and raindrops removal in videos. Unexpectedly, itsuni-dimensional sequential process on videos destroys the local correlationsacross the spatio-temporal dimension by distancing adjacent pixels. To addressthis, we present an improved SSMs-based video deraining network (RainMamba)with a novel Hilbert scanning mechanism to better capture sequence-level localinformation. We also introduce a difference-guided dynamic contrastive localitylearning strategy to enhance the patch-level self-similarity learning abilityof the proposed network. Extensive experiments on four synthesized videoderaining datasets and real-world rainy videos demonstrate the superiority ofour network in the removal of rain streaks and raindrops.</description><author>Hongtao Wu, Yijun Yang, Huihui Xu, Weiming Wang, Jinni Zhou, Lei Zhu</author><pubDate>Wed, 31 Jul 2024 17:48:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21773v1</guid></item><item><title>ShieldGemma: Generative AI Content Moderation Based on Gemma</title><link>http://arxiv.org/abs/2407.21772v1</link><description>We present ShieldGemma, a comprehensive suite of LLM-based safety contentmoderation models built upon Gemma2. These models provide robust,state-of-the-art predictions of safety risks across key harm types (sexuallyexplicit, dangerous content, harassment, hate speech) in both user input andLLM-generated output. By evaluating on both public and internal benchmarks, wedemonstrate superior performance compared to existing models, such as LlamaGuard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%).Additionally, we present a novel LLM-based data curation pipeline, adaptable toa variety of safety-related tasks and beyond. We have shown stronggeneralization performance for model trained mainly on synthetic data. Byreleasing ShieldGemma, we provide a valuable resource to the researchcommunity, advancing LLM safety and enabling the creation of more effectivecontent moderation solutions for developers.</description><author>Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, Olivia Sturman, Oscar Wahltinez</author><pubDate>Wed, 31 Jul 2024 17:48:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21772v1</guid></item><item><title>Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs</title><link>http://arxiv.org/abs/2407.21771v1</link><description>Existing Large Vision-Language Models (LVLMs) primarily align image featuresof vision encoder with Large Language Models (LLMs) to leverage their superiortext generation capabilities. However, the scale disparity between visionencoder and language model may led to LLMs assuming a predominant role inmulti-modal comprehension. This imbalance in LVLMs may result in the instancesof hallucinatory. Concretely, LVLMs may generate consistent descriptions withor without visual input, indicating that certain outputs are influenced solelyby context text. We refer to this phenomenon as "text inertia." To counteractthis issue, we introduce a training-free algorithm to find an equilibrium pointbetween image comprehension and language inference. Specifically, we adaptivelyinvolve adjusting and amplifying the attention weights assigned to imagetokens, thereby granting greater prominence to visual elements. Meanwhile, wesubtract the logits of multi-modal inputs from ones of pure text input, whichcan help LVLMs be not biased towards LLMs. By enhancing images tokens andreducing the stubborn output of LLM, we can let LVLM pay more attention toimages, towards alleviating text inertia and reducing the hallucination inLVLMs. Our extensive experiments shows that this method substantially reducesthe frequency of hallucinatory outputs in various LVLMs in terms of differentmetrics. Project page is available at https://lalbj.github.io/projects/PAI/.</description><author>Shi Liu, Kecheng Zheng, Wei Chen</author><pubDate>Wed, 31 Jul 2024 17:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21771v1</guid></item><item><title>MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts</title><link>http://arxiv.org/abs/2407.21770v1</link><description>We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)architecture designed for pre-training mixed-modal, early-fusion languagemodels. MoMa processes images and text in arbitrary sequences by dividingexpert modules into modality-specific groups. These groups exclusively processdesignated tokens while employing learned routing within each group to maintainsemantically informed adaptivity. Our empirical results reveal substantialpre-training efficiency gains through this modality-specific parameterallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,featuring 4 text experts and 4 image experts, achieves impressive FLOPssavings: 3.7x overall, with 2.6x for text and 5.2x for image processingcompared to a compute-equivalent dense baseline, measured by pre-training loss.This outperforms the standard expert-choice MoE with 8 mixed-modal experts,which achieves 3x overall FLOPs savings (3x for text, 2.8x for image).Combining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPssavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combinationhurts performance in causal inference due to increased sensitivity to routeraccuracy. These results demonstrate MoMa's potential to significantly advancethe efficiency of mixed-modal, early-fusion language model pre-training, pavingthe way for more resource-efficient and capable multimodal AI systems.</description><author>Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, Armen Aghajanyan</author><pubDate>Wed, 31 Jul 2024 17:46:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21770v1</guid></item><item><title>iMatching: Imperative Correspondence Learning</title><link>http://arxiv.org/abs/2312.02141v2</link><description>Learning feature correspondence is a foundational task in computer vision,holding immense importance for downstream applications such as visual odometryand 3D reconstruction. Despite recent progress in data-driven models, featurecorrespondence learning is still limited by the lack of accurate per-pixelcorrespondence labels. To overcome this difficulty, we introduce a newself-supervised scheme, imperative learning (IL), for training featurecorrespondence. It enables correspondence learning on arbitrary uninterruptedvideos without any camera pose or depth labels, heralding a new era forself-supervised correspondence learning. Specifically, we formulated theproblem of correspondence learning as a bilevel optimization, which takes thereprojection error from bundle adjustment as a supervisory signal for themodel. To avoid large memory and computation overhead, we leverage thestationary point to effectively back-propagate the implicit gradients throughbundle adjustment. Through extensive experiments, we demonstrate superiorperformance on tasks including feature matching and pose estimation, in whichwe obtained an average of 30% accuracy gain over the state-of-the-art matchingmodels. This preprint corresponds to the Accepted Manuscript in EuropeanConference on Computer Vision (ECCV) 2024.</description><author>Zitong Zhan, Dasong Gao, Yun-Jou Lin, Youjie Xia, Chen Wang</author><pubDate>Wed, 31 Jul 2024 17:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02141v2</guid></item><item><title>Learning Video Context as Interleaved Multimodal Sequences</title><link>http://arxiv.org/abs/2407.21757v1</link><description>Narrative videos, such as movies, pose significant challenges in videounderstanding due to their rich contexts (characters, dialogues, storylines)and diverse demands (identify who, relationship, and reason). In this paper, weintroduce MovieSeq, a multimodal language model developed to address the widerange of challenges in understanding video contexts. Our core idea is torepresent videos as interleaved multimodal sequences (including images, plots,videos, and subtitles), either by linking external knowledge databases or usingoffline models (such as whisper for subtitles). Through instruction-tuning,this approach empowers the language model to interact with videos usinginterleaved multimodal instructions. For example, instead of solely relying onvideo as input, we jointly provide character photos alongside their names anddialogues, allowing the model to associate these elements and generate morecomprehensive responses. To demonstrate its effectiveness, we validateMovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA)across five settings (video classification, audio description, video-textretrieval, video captioning, and video question-answering). The code will bepublic at https://github.com/showlab/MovieSeq.</description><author>Kevin Qinghong Lin, Pengchuan Zhang, Difei Gao, Xide Xia, Joya Chen, Ziteng Gao, Jinheng Xie, Xuhong Xiao, Mike Zheng Shou</author><pubDate>Wed, 31 Jul 2024 17:23:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21757v1</guid></item><item><title>Process Mining Embeddings: Learning Vector Representations for Petri Nets</title><link>http://arxiv.org/abs/2404.17129v3</link><description>Process Mining offers a powerful framework for uncovering, analyzing, andoptimizing real-world business processes. Petri nets provide a versatile meansof modeling process behavior. However, traditional methods often struggle toeffectively compare complex Petri nets, hindering their potential for processenhancement. To address this challenge, we introduce PetriNet2Vec, anunsupervised methodology inspired by Doc2Vec. This approach converts Petri netsinto embedding vectors, facilitating the comparison, clustering, andclassification of process models. We validated our approach using the PDCDataset, comprising 96 diverse Petri net models. The results demonstrate thatPetriNet2Vec effectively captures the structural properties of process models,enabling accurate process classification and efficient process retrieval.Specifically, our findings highlight the utility of the learned embeddings intwo key downstream tasks: process classification and process retrieval. Inprocess classification, the embeddings allowed for accurate categorization ofprocess models based on their structural properties. In process retrieval, theembeddings enabled efficient retrieval of similar process models using cosinedistance. These results demonstrate the potential of PetriNet2Vec tosignificantly enhance process mining capabilities.</description><author>Juan G. Colonna, Ahmed A. Fares, Márcio Duarte, Ricardo Sousa</author><pubDate>Wed, 31 Jul 2024 17:19:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17129v3</guid></item><item><title>Do Large Language Models Understand Conversational Implicature -- A case study with a chinese sitcom</title><link>http://arxiv.org/abs/2404.19509v2</link><description>Understanding the non-literal meaning of an utterance is critical for largelanguage models (LLMs) to become human-like social communicators. In this work,we introduce SwordsmanImp, the first Chinese multi-turn-dialogue-based datasetaimed at conversational implicature, sourced from dialogues in the Chinesesitcom $\textit{My Own Swordsman}$. It includes 200 carefully handcraftedquestions, all annotated on which Gricean maxims have been violated. We testeight close-source and open-source LLMs under two tasks: a multiple-choicequestion task and an implicature explanation task. Our results show that GPT-4attains human-level accuracy (94%) on multiple-choice questions. CausalLMdemonstrates a 78.5% accuracy following GPT-4. Other models, including GPT-3.5and several open-source models, demonstrate a lower accuracy ranging from 20%to 60% on multiple-choice questions. Human raters were asked to rate theexplanation of the implicatures generated by LLMs on their reasonability, logicand fluency. While all models generate largely fluent and self-consistent text,their explanations score low on reasonability except for GPT-4, suggesting thatmost LLMs cannot produce satisfactory explanations of the implicatures in theconversation. Moreover, we find LLMs' performance does not vary significantlyby Gricean maxims, suggesting that LLMs do not seem to process implicaturesderived from different maxims differently. Our data and code are available athttps://github.com/sjtu-compling/llm-pragmatics.</description><author>Shisen Yue, Siyuan Song, Xinyuan Cheng, Hai Hu</author><pubDate>Wed, 31 Jul 2024 17:08:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19509v2</guid></item><item><title>Diagnostic Runtime Monitoring with Martingales</title><link>http://arxiv.org/abs/2407.21748v1</link><description>Machine learning systems deployed in safety-critical robotics settings mustbe robust to distribution shifts. However, system designers must understand thecause of a distribution shift in order to implement the appropriateintervention or mitigation strategy and prevent system failure. In this paper,we present a novel framework for diagnosing distribution shifts in a streamingfashion by deploying multiple stochastic martingales simultaneously. We showthat knowledge of the underlying cause of a distribution shift can lead toproper interventions over the lifecycle of a deployed system. Our experimentalframework can easily be adapted to different types of distribution shifts,models, and datasets. We find that our method outperforms existing work ondiagnosing distribution shifts in terms of speed, accuracy, and flexibility,and validate the efficiency of our model in both simulated and live hardwaresettings.</description><author>Ali Hindy, Rachel Luo, Somrita Banerjee, Jonathan Kuck, Edward Schmerling, Marco Pavone</author><pubDate>Wed, 31 Jul 2024 17:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21748v1</guid></item><item><title>HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2407.21742v1</link><description>With the progressive advancements in deep graph learning, out-of-distribution(OOD) detection for graph data has emerged as a critical challenge. While theefficacy of auxiliary datasets in enhancing OOD detection has been extensivelystudied for image and text data, such approaches have not yet been explored forgraph data. Unlike Euclidean data, graph data exhibits greater diversity butlower robustness to perturbations, complicating the integration of outliers. Totackle these challenges, we propose the introduction of \textbf{H}ybridExternal and Internal \textbf{G}raph \textbf{O}utlier \textbf{E}xposure (HGOE)to improve graph OOD detection performance. Our framework involves usingrealistic external graph data from various domains and synthesizing internaloutliers within ID subgroups to address the poor robustness and presence of OODsamples within the ID class. Furthermore, we develop a boundary-aware OE lossthat adaptively assigns weights to outliers, maximizing the use of high-qualityOOD samples while minimizing the impact of low-quality ones. Our proposed HGOEframework is model-agnostic and designed to enhance the effectiveness ofexisting graph OOD detection models. Experimental results demonstrate that ourHGOE framework can significantly improve the performance of existing OODdetection models across all 8 real datasets.</description><author>Junwei He, Qianqian Xu, Yangbangyan Jiang, Zitai Wang, Yuchen Sun, Qingming Huang</author><pubDate>Wed, 31 Jul 2024 16:55:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21742v1</guid></item><item><title>Contrastive Factor Analysis</title><link>http://arxiv.org/abs/2407.21740v1</link><description>Factor analysis, often regarded as a Bayesian variant of matrixfactorization, offers superior capabilities in capturing uncertainty, modelingcomplex dependencies, and ensuring robustness. As the deep learning eraarrives, factor analysis is receiving less and less attention due to theirlimited expressive ability. On the contrary, contrastive learning has emergedas a potent technique with demonstrated efficacy in unsupervisedrepresentational learning. While the two methods are different paradigms,recent theoretical analysis has revealed the mathematical equivalence betweencontrastive learning and matrix factorization, providing a potentialpossibility for factor analysis combined with contrastive learning. Motivatedby the interconnectedness of contrastive learning, matrix factorization, andfactor analysis, this paper introduces a novel Contrastive Factor Analysisframework, aiming to leverage factor analysis's advantageous properties withinthe realm of contrastive learning. To further leverage the interpretabilityproperties of non-negative factor analysis, which can learn disentangledrepresentations, contrastive factor analysis is extended to a non-negativeversion. Finally, extensive experimental validation showcases the efficacy ofthe proposed contrastive (non-negative) factor analysis methodology acrossmultiple key properties, including expressiveness, robustness,interpretability, and accurate uncertainty estimation.</description><author>Zhibin Duan, Tiansheng Wen, Yifei Wang, Chen Zhu, Bo Chen, Mingyuan Zhou</author><pubDate>Wed, 31 Jul 2024 16:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21740v1</guid></item><item><title>A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation</title><link>http://arxiv.org/abs/2407.21739v1</link><description>Adapting foundation models for medical image analysis requires finetuningthem on a considerable amount of data because of extreme distribution shiftsbetween natural (source) data used for pretraining and medical (target) data.However, collecting task-specific medical data for such finetuning at a centrallocation raises many privacy concerns. Although Federated learning (FL)provides an effective means for training on private decentralized data,communication costs in federating large foundation models can quickly become asignificant bottleneck, impacting the solution's scalability. In this work, weaddress this problem of efficient communication while ensuring effectivelearning in FL by combining the strengths of Parameter-Efficient Fine-tuning(PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA)in a federated manner to adapt the Segment Anything Model (SAM) for 3D medicalimage segmentation. Unlike prior works that utilize LoRA and finetune theentire decoder, we critically analyze the contribution of each granularcomponent of SAM on finetuning performance. Thus, we identify specific layersto be federated that are very efficient in terms of communication cost whileproducing on-par accuracy. Our experiments show that retaining the parametersof the SAM model (including most of the decoder) in their original state duringadaptation is beneficial because fine-tuning on small datasets tends to distortthe inherent capabilities of the underlying foundation model. On Fed-KiTS, ourapproach decreases communication cost (~48x) compared to full fine-tuning whileincreasing performance (~6% Dice score) in 3D segmentation tasks. Our approachperforms similar to SAMed while achieving ~2.8x reduction in communication andparameters to be finetuned. We further validate our approach with experimentson Fed-IXI and Prostate MRI datasets.</description><author>Mothilal Asokan, Joseph Geo Benjamin, Mohammad Yaqub, Karthik Nandakumar</author><pubDate>Wed, 31 Jul 2024 16:48:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21739v1</guid></item><item><title>Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos</title><link>http://arxiv.org/abs/2407.21738v1</link><description>Self-supervised learning (SSL) methods are popular since they can addresssituations with limited annotated data by directly utilising the underlyingdata distribution. However, the adoption of such methods is not explored enoughin ultrasound (US) imaging, especially for fetal assessment. We investigate thepotential of dual-encoder SSL in utilizing unlabelled US video data to improvethe performance of challenging downstream Standard Fetal Cardiac Planes (SFCP)classification using limited labelled 2D US images. We study 7 SSL approachesbased on reconstruction, contrastive loss, distillation, and information theoryand evaluate them extensively on a large private US dataset. Our observationsand findings are consolidated from more than 500 downstream trainingexperiments under different settings. Our primary observation shows that forSSL training, the variance of the dataset is more crucial than its size becauseit allows the model to learn generalisable representations, which improve theperformance of downstream tasks. Overall, the BarlowTwins method shows robustperformance, irrespective of the training settings and data variations, whenused as an initialisation for downstream tasks. Notably, full fine-tuning with1% of labelled data outperforms ImageNet initialisation by 12% in F1-score andoutperforms other SSL initialisations by at least 4% in F1-score, thus makingit a promising candidate for transfer learning from US video to image data.</description><author>Joseph Geo Benjamin, Mothilal Asokan, Amna Alhosani, Hussain Alasmawi, Werner Gerhard Diehl, Leanne Bricker, Karthik Nandakumar, Mohammad Yaqub</author><pubDate>Wed, 31 Jul 2024 16:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21738v1</guid></item><item><title>Unifying Event-based Flow, Stereo and Depth Estimation via Feature Similarity Matching</title><link>http://arxiv.org/abs/2407.21735v1</link><description>As an emerging vision sensor, the event camera has gained popularity invarious vision tasks such as optical flow estimation, stereo matching, anddepth estimation due to its high-speed, sparse, and asynchronous event streams.Unlike traditional approaches that use specialized architectures for eachspecific task, we propose a unified framework, EventMatch, that reformulatesthese tasks as an event-based dense correspondence matching problem, allowingthem to be solved with a single model by directly comparing featuresimilarities. By utilizing a shared feature similarities module, whichintegrates knowledge from other event flows via temporal or spatialinteractions, and distinct task heads, our network can concurrently performoptical flow estimation from temporal inputs (e.g., two segments of eventstreams in the temporal domain) and stereo matching from spatial inputs (e.g.,two segments of event streams from different viewpoints in the spatial domain).Moreover, we further demonstrate that our unified model inherently supportscross-task transfer since the architecture and parameters are shared acrosstasks. Without the need for retraining on each task, our model can effectivelyhandle both optical flow and disparity estimation simultaneously. Theexperiment conducted on the DSEC benchmark demonstrates that our model exhibitssuperior performance in both optical flow and disparity estimation tasks,outperforming existing state-of-the-art methods. Our unified approach not onlyadvances event-based models but also opens new possibilities for cross-tasktransfer and inter-task fusion in both spatial and temporal dimensions. Ourcode will be available later.</description><author>Pengjie Zhang, Lin Zhu, Lizhi Wang, Hua Huang</author><pubDate>Wed, 31 Jul 2024 16:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21735v1</guid></item><item><title>ParLS-PBO: A Parallel Local Search Solver for Pseudo Boolean Optimization</title><link>http://arxiv.org/abs/2407.21729v1</link><description>As a broadly applied technique in numerous optimization problems, recently,local search has been employed to solve Pseudo-Boolean Optimization (PBO)problem. A representative local search solver for PBO is LSPBO. In this paper,firstly, we improve LSPBO by a dynamic scoring mechanism, which dynamicallystrikes a balance between score on hard constraints and score on the objectivefunction. Moreover, on top of this improved LSPBO , we develop the first parallel localsearch PBO solver. The main idea is to share good solutions among differentthreads to guide the search, by maintaining a pool of feasible solutions. Forevaluating solutions when updating the pool, we propose a function thatconsiders both the solution quality and the diversity of the pool. Furthermore,we calculate the polarity density in the pool to enhance the scoring functionof local search. Our empirical experiments show clear benefits of the proposedparallel approach, making it competitive with the parallel version of thefamous commercial solver Gurobi.</description><author>Zhihan Chen, Peng Lin, Hao Hu, Shaowei Cai</author><pubDate>Wed, 31 Jul 2024 16:30:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21729v1</guid></item><item><title>Artificial Intelligence Approaches for Energy Efficiency: A Review</title><link>http://arxiv.org/abs/2407.21726v1</link><description>United Nations set Sustainable Development Goals and this paper focuses on7th (Affordable and Clean Energy), 9th (Industries, Innovation andInfrastructure), and 13th (Climate Action) goals. Climate change is a majorconcern in our society; for this reason, a current global objective is toreduce energy waste. This work summarizes all main approaches towards energyefficiency using Artificial Intelligence with a particular focus on multi-agentsystems to create smart buildings. It mentions the tight relationship betweenAI, especially IoT, and Big Data. It explains the application of AI to anomalydetection in smart buildings and a possible classification of IntelligentEnergy Management Systems: Direct and Indirect. Finally, some drawbacks of AIapproaches and some possible future research focuses are proposed.</description><author>Alberto Pasqualetto, Lorenzo Serafini, Michele Sprocatti</author><pubDate>Wed, 31 Jul 2024 16:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21726v1</guid></item><item><title>Stable Audio Open</title><link>http://arxiv.org/abs/2407.14358v2</link><description>Open generative models are vitally important for the community, allowing forfine-tunes and serving as baselines when presenting new models. However, mostcurrent text-to-audio models are private and not accessible for artists andresearchers to build upon. Here we describe the architecture and trainingprocess of a new open-weights text-to-audio model trained with Creative Commonsdata. Our evaluation shows that the model's performance is competitive with thestate-of-the-art across various metrics. Notably, the reported FDopenl3 results(measuring the realism of the generations) showcase its potential forhigh-quality stereo sound synthesis at 44.1kHz.</description><author>Zach Evans, Julian D. Parker, CJ Carr, Zack Zukowski, Josiah Taylor, Jordi Pons</author><pubDate>Wed, 31 Jul 2024 16:22:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14358v2</guid></item><item><title>Optimal Decision Tree and Adaptive Submodular Ranking with Noisy Outcomes</title><link>http://arxiv.org/abs/2312.15357v2</link><description>In pool-based active learning, the learner is given an unlabeled data set andaims to efficiently learn the unknown hypothesis by querying the labels of thedata points. This can be formulated as the classical Optimal Decision Tree(ODT) problem: Given a set of tests, a set of hypotheses, and an outcome foreach pair of test and hypothesis, our objective is to find a low-cost testingprocedure (i.e., decision tree) that identifies the true hypothesis. Thisoptimization problem has been extensively studied under the assumption thateach test generates a deterministic outcome. However, in numerous applications,for example, clinical trials, the outcomes may be uncertain, which renders theideas from the deterministic setting invalid. In this work, we study afundamental variant of the ODT problem in which some test outcomes are noisy,even in the more general case where the noise is persistent, i.e., repeating atest gives the same noisy output. Our approximation algorithms provideguarantees that are nearly best possible and hold for the general case of alarge number of noisy outcomes per test or per hypothesis where the performancedegrades continuously with this number. We numerically evaluated our algorithmsfor identifying toxic chemicals and learning linear classifiers, and observedthat our algorithms have costs very close to the information-theoretic minimum.</description><author>Su Jia, Fatemeh Navidi, Viswanath Nagarajan, R. Ravi</author><pubDate>Wed, 31 Jul 2024 16:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15357v2</guid></item><item><title>Definition generation for lexical semantic change detection</title><link>http://arxiv.org/abs/2406.14167v2</link><description>We use contextualized word definitions generated by large language models assemantic representations in the task of diachronic lexical semantic changedetection (LSCD). In short, generated definitions are used as `senses', and thechange score of a target word is retrieved by comparing their distributions intwo time periods under comparison. On the material of five datasets and threelanguages, we show that generated definitions are indeed specific and generalenough to convey a signal sufficient to rank sets of words by the degree oftheir semantic change over time. Our approach is on par with or outperformsprior non-supervised sense-based LSCD methods. At the same time, it preservesinterpretability and allows to inspect the reasons behind a specific shift interms of discrete definitions-as-senses. This is another step in the directionof explainable semantic change modeling.</description><author>Mariia Fedorova, Andrey Kutuzov, Yves Scherrer</author><pubDate>Wed, 31 Jul 2024 16:20:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14167v2</guid></item><item><title>A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective</title><link>http://arxiv.org/abs/2403.16137v2</link><description>Graph self-supervised learning (SSL) is now a go-to method for pre-traininggraph foundation models (GFMs). There is a wide variety of knowledge patternsembedded in the graph data, such as node properties and clusters, which arecrucial to learning generalized representations for GFMs. However, existingsurveys of GFMs have several shortcomings: they lack comprehensivenessregarding the most recent progress, have unclear categorization ofself-supervised methods, and take a limited architecture-based perspective thatis restricted to only certain types of graph models. As the ultimate goal ofGFMs is to learn generalized graph knowledge, we provide a comprehensive surveyof self-supervised GFMs from a novel knowledge-based perspective. We propose aknowledge-based taxonomy, which categorizes self-supervised graph models by thespecific graph knowledge utilized. Our taxonomy consists of microscopic (nodes,links, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge(global structure, manifolds, etc.). It covers a total of 9 knowledgecategories and more than 25 pretext tasks for pre-training GFMs, as well asvarious downstream task generalization strategies. Such a knowledge-basedtaxonomy allows us to re-examine graph models based on new architectures moreclearly, such as graph language models, as well as provide more in-depthinsights for constructing GFMs.</description><author>Ziwen Zhao, Yixin Su, Yuhua Li, Yixiong Zou, Ruixuan Li, Rui Zhang</author><pubDate>Wed, 31 Jul 2024 16:16:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16137v2</guid></item><item><title>Open-Vocabulary Audio-Visual Semantic Segmentation</title><link>http://arxiv.org/abs/2407.21721v1</link><description>Audio-visual semantic segmentation (AVSS) aims to segment and classifysounding objects in videos with acoustic cues. However, most approaches operateon the close-set assumption and only identify pre-defined categories fromtraining data, lacking the generalization ability to detect novel categories inpractical applications. In this paper, we introduce a new task: open-vocabularyaudio-visual semantic segmentation, extending AVSS task to open-world scenariosbeyond the annotated label space. This is a more challenging task that requiresrecognizing all categories, even those that have never been seen nor heardduring training. Moreover, we propose the first open-vocabulary AVSS framework,OV-AVSS, which mainly consists of two parts: 1) a universal sound sourcelocalization module to perform audio-visual fusion and locate all potentialsounding objects and 2) an open-vocabulary classification module to predictcategories with the help of the prior knowledge from large-scale pre-trainedvision-language models. To properly evaluate the open-vocabulary AVSS, we splitzero-shot training and testing subsets based on the AVSBench-semanticbenchmark, namely AVSBench-OV. Extensive experiments demonstrate the strongsegmentation and zero-shot generalization ability of our model on allcategories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on basecategories and 29.14% mIoU on novel categories, exceeding the state-of-the-artzero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%.The code is available at https://github.com/ruohaoguo/ovavss.</description><author>Ruohao Guo, Liao Qu, Dantong Niu, Yanyu Qi, Wenzhen Yue, Ji Shi, Bowei Xing, Xianghua Ying</author><pubDate>Wed, 31 Jul 2024 16:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21721v1</guid></item><item><title>Detecting, Explaining, and Mitigating Memorization in Diffusion Models</title><link>http://arxiv.org/abs/2407.21720v1</link><description>Recent breakthroughs in diffusion models have exhibited exceptionalimage-generation capabilities. However, studies show that some outputs aremerely replications of training data. Such replications present potential legalchallenges for model owners, especially when the generated content containsproprietary information. In this work, we introduce a straightforward yeteffective method for detecting memorized prompts by inspecting the magnitude oftext-conditional predictions. Our proposed method seamlessly integrates withoutdisrupting sampling algorithms, and delivers high accuracy even at the firstgeneration step, with a single generation per prompt. Building on our detectionstrategy, we unveil an explainable approach that shows the contribution ofindividual words or tokens to memorization. This offers an interactive mediumfor users to adjust their prompts. Moreover, we propose two strategies i.e., tomitigate memorization by leveraging the magnitude of text-conditionalpredictions, either through minimization during inference or filtering duringtraining. These proposed strategies effectively counteract memorization whilemaintaining high-generation quality. Code is available athttps://github.com/YuxinWenRick/diffusion_memorization.</description><author>Yuxin Wen, Yuchen Liu, Chen Chen, Lingjuan Lyu</author><pubDate>Wed, 31 Jul 2024 16:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21720v1</guid></item><item><title>Assessing the State of AI Policy</title><link>http://arxiv.org/abs/2407.21717v1</link><description>The deployment of artificial intelligence (AI) applications has acceleratedrapidly. AI enabled technologies are facing the public in many ways includinginfrastructure, consumer products and home applications. Because many of thesetechnologies present risks either in the form of physical injury, or bias,potentially yielding unfair outcomes, policy makers must consider the need foroversight. Most policymakers, however, lack the technical knowledge to judgewhether an emerging AI technology is safe, effective, and requires oversight,therefore policy makers must depend on expert opinion. But policymakers arebetter served when, in addition to expert opinion, they have some generalunderstanding of existing guidelines and regulations. This work provides anoverview [the landscape] of AI legislation and directives at the international,U.S. state, city and federal levels. It also reviews relevant businessstandards, and technical society initiatives. Then an overlap and gap analysisare performed resulting in a reference guide that includes recommendations andguidance for future policy making.</description><author>Joanna F. DeFranco, Luke Biersmith</author><pubDate>Wed, 31 Jul 2024 16:09:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21717v1</guid></item><item><title>UMMAN: Unsupervised Multi-graph Merge Adversarial Network for Disease Prediction Based on Intestinal Flora</title><link>http://arxiv.org/abs/2407.21714v1</link><description>The abundance of intestinal flora is closely related to human diseases, butdiseases are not caused by a single gut microbe. Instead, they result from thecomplex interplay of numerous microbial entities. This intricate and implicitconnection among gut microbes poses a significant challenge for diseaseprediction using abundance information from OTU data. Recently, several methodshave shown potential in predicting corresponding diseases. However, thesemethods fail to learn the inner association among gut microbes from differenthosts, leading to unsatisfactory performance. In this paper, we present a novelarchitecture, Unsupervised Multi-graph Merge Adversarial Network (UMMAN). UMMANcan obtain the embeddings of nodes in the Multi-Graph in an unsupervisedscenario, so that it helps learn the multiplex association. Our method is thefirst to combine Graph Neural Network with the task of intestinal flora diseaseprediction. We employ complex relation-types to construct the Original-Graphand disrupt the relationships among nodes to generate correspondingShuffled-Graph. We introduce the Node Feature Global Integration (NFGI) moduleto represent the global features of the graph. Furthermore, we design a jointloss comprising adversarial loss and hybrid attention loss to ensure that thereal graph embedding aligns closely with the Original-Graph and diverges fromthe Shuffled-Graph. Comprehensive experiments on five classical OTU gutmicrobiome datasets demonstrate the effectiveness and stability of our method.(We will release our code soon.)</description><author>Dingkun Liu, Hongjie Zhou, Yilu Qu, Huimei Zhang, Yongdong Xu</author><pubDate>Wed, 31 Jul 2024 16:06:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21714v1</guid></item><item><title>Social Learning through Interactions with Other Agents: A Survey</title><link>http://arxiv.org/abs/2407.21713v1</link><description>Social learning plays an important role in the development of humanintelligence. As children, we imitate our parents' speech patterns until we areable to produce sounds; we learn from them praising us and scolding us; and asadults, we learn by working with others. In this work, we survey the degree towhich this paradigm -- social learning -- has been mirrored in machinelearning. In particular, since learning socially requires interacting withothers, we are interested in how embodied agents can and have utilised thesetechniques. This is especially in light of the degree to which recent advancesin natural language processing (NLP) enable us to perform new forms of sociallearning. We look at how behavioural cloning and next-token prediction mirrorhuman imitation, how learning from human feedback mirrors human education, andhow we can go further to enable fully communicative agents that learn from eachother. We find that while individual social learning techniques have been usedsuccessfully, there has been little unifying work showing how to bring themtogether into socially embodied agents.</description><author>Dylan hillier, Cheston Tan, Jing Jiang</author><pubDate>Wed, 31 Jul 2024 16:06:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21713v1</guid></item><item><title>Adaptive Retrieval-Augmented Generation for Conversational Systems</title><link>http://arxiv.org/abs/2407.21712v1</link><description>Despite the success of integrating large language models into the developmentof conversational systems, many studies have shown the effectiveness ofretrieving and augmenting external knowledge for informative responses. Hence,many existing studies commonly assume the always need for Retrieval AugmentedGeneration (RAG) in a conversational system without explicit control. Thisraises a research question about such a necessity. In this study, we propose toinvestigate the need for each turn of system response to be augmented withexternal knowledge. In particular, by leveraging human judgements on the binarychoice of adaptive augmentation, we develop RAGate, a gating model, whichmodels conversation context and relevant inputs to predict if a conversationalsystem requires RAG for improved responses. We conduct extensive experiments ondevising and applying RAGate to conversational models and well-rounded analysesof different conversational scenarios. Our experimental results and analysisindicate the effective application of RAGate in RAG-based conversationalsystems in identifying system responses for appropriate RAG with high-qualityresponses and a high generation confidence. This study also identifies thecorrelation between the generation's confidence level and the relevance of theaugmented knowledge.</description><author>Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz</author><pubDate>Wed, 31 Jul 2024 16:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21712v1</guid></item><item><title>PIPsUS: Self-Supervised Point Tracking in Ultrasound</title><link>http://arxiv.org/abs/2403.04969v2</link><description>Finding point-level correspondences is a fundamental problem in ultrasound(US), since it can enable US landmark tracking for intraoperative imageguidance in different surgeries, including head and neck. Most existing UStracking methods, e.g., those based on optical flow or feature matching, wereinitially designed for RGB images before being applied to US. Therefore domainshift can impact their performance. Training could be supervised byground-truth correspondences, but these are expensive to acquire in US. Tosolve these problems, we propose a self-supervised pixel-level tracking modelcalled PIPsUS. Our model can track an arbitrary number of points in one forwardpass and exploits temporal information by considering multiple, instead of justconsecutive, frames. We developed a new self-supervised training strategy thatutilizes a long-term point-tracking model trained for RGB images as a teacherto guide the model to learn realistic motions and use data augmentation toenforce tracking from US appearance. We evaluate our method on neck and oral USand echocardiography, showing higher point tracking accuracy when compared withfast normalized cross-correlation and tuned optical flow. Code will beavailable once the paper is accepted.</description><author>Wanwen Chen, Adam Schmidt, Eitan Prisman, Septimiu E Salcudean</author><pubDate>Wed, 31 Jul 2024 16:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04969v2</guid></item><item><title>Exact Fractional Inference via Re-Parametrization &amp; Interpolation between Tree-Re-Weighted- and Belief Propagation- Algorithms</title><link>http://arxiv.org/abs/2301.10369v3</link><description>The computational complexity of inference -- required to compute thepartition function, $Z$, of an Ising model over a graph of $N$''spins" -- ismost likely exponential in $N$. Efficient variational methods, such as BeliefPropagation (BP) and Tree Re-Weighted (TRW) algorithms, compute $Z$approximately by minimizing the respective (BP- or TRW-) free energy. Wegeneralize the variational scheme by building a $\lambda$-fractionalinterpolation, $Z^{(\lambda)}$, where $\lambda=0$ and $\lambda=1$ correspond toTRW- and BP-approximations, respectively. This fractional scheme -- coinedFractional Belief Propagation (FBP) -- guarantees that in the attractive(ferromagnetic) case $Z^{(TRW)} \geq Z^{(\lambda)} \geq Z^{(BP)}$, and thereexists a unique (``exact") $\lambda_*$ such that $Z=Z^{(\lambda_*)}$.Generalizing the re-parametrization approach of\citep{wainwright_tree-based_2002} and the loop series approach of\citep{chertkov_loop_2006}, we show how to express $Z$ as a product, $\forall\lambda:\ Z=Z^{(\lambda)}{\tilde Z}^{(\lambda)}$, where the multiplicativecorrection, ${\tilde Z}^{(\lambda)}$, is an expectation over a node-independentprobability distribution built from node-wise fractional marginals. Ourtheoretical analysis is complemented by extensive experiments with models fromIsing ensembles over planar and random graphs of medium- and large-sizes. Theempirical study yields a number of interesting observations, such as theability to estimate ${\tilde Z}^{(\lambda)}$ with $O(N^{2::4})$ fractionalsamples and suppression of $\lambda_*$ fluctuations with an increase in $N$ forinstances from a particular random Ising ensemble. We also verify and discussthe applicability of this approach to the problem of image de-noising.</description><author>Hamidreza Behjoo, Michael Chertkov</author><pubDate>Wed, 31 Jul 2024 16:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10369v3</guid></item><item><title>CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature</title><link>http://arxiv.org/abs/2407.21708v1</link><description>Ontologies are formal representations of knowledge in specific domains thatprovide a structured framework for organizing and understanding complexinformation. Creating ontologies, however, is a complex and time-consumingendeavor. ChEBI is a well-known ontology in the field of chemistry, whichprovides a comprehensive resource for defining chemical entities and theirproperties. However, it covers only a small fraction of the rapidly growingknowledge in chemistry and does not provide references to the scientificliterature. To address this, we propose a methodology that involves augmentingexisting annotated text corpora with knowledge from Chebi and fine-tuning alarge language model (LLM) to recognize chemical entities and their roles inscientific text. Our experiments demonstrate the effectiveness of our approach.By combining ontological knowledge and the language understanding capabilitiesof LLMs, we achieve high precision and recall rates in identifying both thechemical entities and roles in scientific literature. Furthermore, we extractthem from a set of 8,000 ChemRxiv articles, and apply a second LLM to create aknowledge graph (KG) of chemical entities and roles (CEAR), which providescomplementary information to ChEBI, and can help to extend it.</description><author>Stefan Langer, Fabian Neuhaus, Andreas Nürnberger</author><pubDate>Wed, 31 Jul 2024 15:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21708v1</guid></item><item><title>Tora: Trajectory-oriented Diffusion Transformer for Video Generation</title><link>http://arxiv.org/abs/2407.21705v1</link><description>Recent advancements in Diffusion Transformer (DiT) have demonstratedremarkable proficiency in producing high-quality video content. Nonetheless,the potential of transformer-based diffusion models for effectively generatingvideos with controllable motion remains an area of limited exploration. Thispaper introduces Tora, the first trajectory-oriented DiT framework thatintegrates textual, visual, and trajectory conditions concurrently for videogeneration. Specifically, Tora consists of a Trajectory Extractor~(TE), aSpatial-Temporal DiT, and a Motion-guidance Fuser~(MGF). The TE encodesarbitrary trajectories into hierarchical spacetime motion patches with a 3Dvideo compression network. The MGF integrates the motion patches into the DiTblocks to generate consistent videos following trajectories. Our design alignsseamlessly with DiT's scalability, allowing precise control of video content'sdynamics with diverse durations, aspect ratios, and resolutions. Extensiveexperiments demonstrate Tora's excellence in achieving high motion fidelity,while also meticulously simulating the movement of the physical world. Page canbe found at https://ali-videoai.github.io/tora_video.</description><author>Zhenghao Zhang, Junchao Liao, Menghao Li, Long Qin, Weizhi Wang</author><pubDate>Wed, 31 Jul 2024 15:53:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21705v1</guid></item><item><title>Hyper-parameter tuning for text guided image editing</title><link>http://arxiv.org/abs/2407.21703v1</link><description>The test-time finetuning text-guided image editing method, Forgedit, iscapable of tackling general and complex image editing problems given only theinput image itself and the target text prompt. During finetuning stage, usingthe same set of finetuning hyper-paramters every time for every given image,Forgedit remembers and understands the input image in 30 seconds. Duringediting stage, the workflow of Forgedit might seem complicated. However, infact, the editing process of Forgedit is not more complex than previous SOTAImagic, yet completely solves the overfitting problem of Imagic. In this paper,we will elaborate the workflow of Forgedit editing stage with examples. We willshow how to tune the hyper-parameters in an efficient way to obtain idealediting results.</description><author>Shiwen Zhang</author><pubDate>Wed, 31 Jul 2024 15:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21703v1</guid></item><item><title>TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities</title><link>http://arxiv.org/abs/2407.21693v1</link><description>Task-oriented dialogue (TOD) systems aim to efficiently handle task-orientedconversations, including information gathering. How to utilize ToD accurately,efficiently and effectively for information gathering has always been acritical and challenging task. Recent studies have demonstrated that LargeLanguage Models (LLMs) excel in dialogue, instruction generation, andreasoning, and can significantly enhance the performance of TOD throughfine-tuning. However, current datasets primarily cater to user-led systems andare limited to predefined specific scenarios and slots, thereby necessitatingimprovements in the proactiveness, diversity, and capabilities of TOD. In thisstudy, we present a detailed multi-domain task-oriented data constructionprocess for conversations, and a Chinese dialogue dataset generated based onthis process, \textbf{TransferTOD}, which authentically simulates human-machinedialogues in 30 popular life service scenarios. Leveraging this dataset, wetrained a \textbf{TransferTOD-7B} model using full-parameter fine-tuning,showcasing notable abilities in slot filling and questioning. Our work hasdemonstrated its strong generalization capabilities in various downstreamscenarios, significantly enhancing both data utilization efficiency and systemperformance. The data is released inhttps://github.com/KongLongGeFDU/TransferTOD.</description><author>Ming Zhang, Caishuang Huang, Yilong Wu, Shichun Liu, Huiyuan Zheng, Yurui Dong, Yujiong Shen, Shihan Dou, Jun Zhao, Junjie Ye, Qi Zhang, Tao Gui, Xuanjing Huang</author><pubDate>Wed, 31 Jul 2024 15:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21693v1</guid></item><item><title>Explainable Artificial Intelligence for Quantifying Interfering and High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom Environment Using Privacy-Preserving Video Analysis</title><link>http://arxiv.org/abs/2407.21691v1</link><description>Rapid identification and accurate documentation of interfering and high-riskbehaviors in ASD, such as aggression, self-injury, disruption, and restrictedrepetitive behaviors, are important in daily classroom environments fortracking intervention effectiveness and allocating appropriate resources tomanage care needs. However, having a staff dedicated solely to observing iscostly and uncommon in most educational settings. Recently, multiple researchstudies have explored developing automated, continuous, and objective toolsusing machine learning models to quantify behaviors in ASD. However, themajority of the work was conducted under a controlled environment and has notbeen validated for real-world conditions. In this work, we demonstrate that thelatest advances in video-based group activity recognition techniques canquantify behaviors in ASD in real-world activities in classroom environmentswhile preserving privacy. Our explainable model could detect the episode ofproblem behaviors with a 77% F1-score and capture distinctive behavior featuresin different types of behaviors in ASD. To the best of our knowledge, this isthe first work that shows the promise of objectively quantifying behaviors inASD in a real-world environment, which is an important step toward thedevelopment of a practical tool that can ease the burden of data collection forclassroom staff.</description><author>Barun Das, Conor Anderson, Tania Villavicencio, Johanna Lantz, Jenny Foster, Theresa Hamlin, Ali Bahrami Rad, Gari D. Clifford, Hyeokhyen Kwon</author><pubDate>Wed, 31 Jul 2024 15:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21691v1</guid></item><item><title>Posterior-Variance-Based Error Quantification for Inverse Problems in Imaging</title><link>http://arxiv.org/abs/2212.12499v2</link><description>In this work, a method for obtaining pixel-wise error bounds in Bayesianregularization of inverse imaging problems is introduced. The proposed methodemploys estimates of the posterior variance together with techniques fromconformal prediction in order to obtain coverage guarantees for the errorbounds, without making any assumption on the underlying data distribution. Itis generally applicable to Bayesian regularization approaches, independent,e.g., of the concrete choice of the prior. Furthermore, the coverage guaranteescan also be obtained in case only approximate sampling from the posterior ispossible. With this in particular, the proposed framework is able toincorporate any learned prior in a black-box manner. Guaranteed coveragewithout assumptions on the underlying distributions is only achievable sincethe magnitude of the error bounds is, in general, unknown in advance.Nevertheless, experiments with multiple regularization approaches presented inthe paper confirm that in practice, the obtained error bounds are rather tight.For realizing the numerical experiments, also a novel primal-dual Langevinalgorithm for sampling from non-smooth distributions is introduced in thiswork.</description><author>Dominik Narnhofer, Andreas Habring, Martin Holler, Thomas Pock</author><pubDate>Wed, 31 Jul 2024 15:36:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12499v2</guid></item><item><title>SpaER: Learning Spatio-temporal Equivariant Representations for Fetal Brain Motion Tracking</title><link>http://arxiv.org/abs/2407.20198v3</link><description>In this paper, we introduce SpaER, a pioneering method for fetal motiontracking that leverages equivariant filters and self-attention mechanisms toeffectively learn spatio-temporal representations. Different from conventionalapproaches that statically estimate fetal brain motions from pairs of images,our method dynamically tracks the rigid movement patterns of the fetal headacross temporal and spatial dimensions. Specifically, we first develop anequivariant neural network that efficiently learns rigid motion sequencesthrough low-dimensional spatial representations of images. Subsequently, welearn spatio-temporal representations by incorporating time encoding andself-attention neural network layers. This approach allows for the capture oflong-term dependencies of fetal brain motion and addresses alignment errors dueto contrast changes and severe motion artifacts. Our model also provides ageometric deformation estimation that properly addresses image distortionsamong all time frames. To the best of our knowledge, our approach is the firstto learn spatial-temporal representations via deep neural networks for fetalmotion tracking without data augmentation. We validated our model using realfetal echo-planar images with simulated and real motions. Our method carriessignificant potential value in accurately measuring, tracking, and correctingfetal motion in fetal MRI sequences.</description><author>Jian Wang, Razieh Faghihpirayesh, Polina Golland, Ali Gholipour</author><pubDate>Wed, 31 Jul 2024 15:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20198v3</guid></item><item><title>Dynamic Object Queries for Transformer-based Incremental Object Detection</title><link>http://arxiv.org/abs/2407.21687v1</link><description>Incremental object detection (IOD) aims to sequentially learn new classes,while maintaining the capability to locate and identify old ones. As thetraining data arrives with annotations only with new classes, IOD suffers fromcatastrophic forgetting. Prior methodologies mainly tackle the forgetting issuethrough knowledge distillation and exemplar replay, ignoring the conflictbetween limited model capacity and increasing knowledge. In this paper, weexplore \textit{dynamic object queries} for incremental object detection builton Transformer architecture. We propose the \textbf{Dy}namic object\textbf{Q}uery-based \textbf{DE}tection \textbf{TR}ansformer (DyQ-DETR), whichincrementally expands the model representation ability to achievestability-plasticity tradeoff. First, a new set of learnable object queries arefed into the decoder to represent new classes. These new object queries areaggregated with those from previous phases to adapt both old and new knowledgewell. Second, we propose the isolated bipartite matching for object queries indifferent phases, based on disentangled self-attention. The interaction amongthe object queries at different phases is eliminated to reduce inter-classconfusion. Thanks to the separate supervision and computation over objectqueries, we further present the risk-balanced partial calibration for effectiveexemplar replay. Extensive experiments demonstrate that DyQ-DETR significantlysurpasses the state-of-the-art methods, with limited parameter overhead. Codewill be made publicly available.</description><author>Jichuan Zhang, Wei Li, Shuang Cheng, Ya-Li Li, Shengjin Wang</author><pubDate>Wed, 31 Jul 2024 15:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21687v1</guid></item><item><title>Expressive Whole-Body 3D Gaussian Avatar</title><link>http://arxiv.org/abs/2407.21686v1</link><description>Facial expression and hand motions are necessary to express our emotions andinteract with the world. Nevertheless, most of the 3D human avatars modeledfrom a casually captured video only support body motions without facialexpressions and hand motions.In this work, we present ExAvatar, an expressivewhole-body 3D human avatar learned from a short monocular video. We designExAvatar as a combination of the whole-body parametric mesh model (SMPL-X) and3D Gaussian Splatting (3DGS). The main challenges are 1) a limited diversity offacial expressions and poses in the video and 2) the absence of 3Dobservations, such as 3D scans and RGBD images. The limited diversity in thevideo makes animations with novel facial expressions and poses non-trivial. Inaddition, the absence of 3D observations could cause significant ambiguity inhuman parts that are not observed in the video, which can result in noticeableartifacts under novel motions. To address them, we introduce our hybridrepresentation of the mesh and 3D Gaussians. Our hybrid representation treatseach 3D Gaussian as a vertex on the surface with pre-defined connectivityinformation (i.e., triangle faces) between them following the mesh topology ofSMPL-X. It makes our ExAvatar animatable with novel facial expressions bydriven by the facial expression space of SMPL-X. In addition, by usingconnectivity-based regularizers, we significantly reduce artifacts in novelfacial expressions and poses.</description><author>Gyeongsik Moon, Takaaki Shiratori, Shunsuke Saito</author><pubDate>Wed, 31 Jul 2024 15:29:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21686v1</guid></item><item><title>Pediatric Wrist Fracture Detection in X-rays via YOLOv10 Algorithm and Dual Label Assignment System</title><link>http://arxiv.org/abs/2407.15689v2</link><description>Wrist fractures are highly prevalent among children and can significantlyimpact their daily activities, such as attending school, participating insports, and performing basic self-care tasks. If not treated properly, thesefractures can result in chronic pain, reduced wrist functionality, and otherlong-term complications. Recently, advancements in object detection have shownpromise in enhancing fracture detection, with systems achieving accuracycomparable to, or even surpassing, that of human radiologists. The YOLO series,in particular, has demonstrated notable success in this domain. This study isthe first to provide a thorough evaluation of various YOLOv10 variants toassess their performance in detecting pediatric wrist fractures using theGRAZPEDWRI-DX dataset. It investigates how changes in model complexity, scalingthe architecture, and implementing a dual-label assignment strategy can enhancedetection performance. Experimental results indicate that our trained modelachieved mean average precision (mAP@50-95) of 51.9\% surpassing the currentYOLOv9 benchmark of 43.3\% on this dataset. This represents an improvement of8.6\%. The implementation code is publicly available athttps://github.com/ammarlodhi255/YOLOv10-Fracture-Detection</description><author>Ammar Ahmed, Abdul Manaf</author><pubDate>Wed, 31 Jul 2024 15:28:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15689v2</guid></item><item><title>Explainable Light-Weight Deep Learning Pipeline for Improved Drought Stress Identification</title><link>http://arxiv.org/abs/2404.10073v3</link><description>Early identification of drought stress in crops is vital for implementingeffective mitigation measures and reducing yield loss. Non-invasive imagingtechniques hold immense potential by capturing subtle physiological changes inplants under water deficit. Sensor based imaging data serves as a rich sourceof information for machine learning and deep learning algorithms, facilitatingfurther analysis aimed at identifying drought stress. While these approachesyield favorable results, real-time field applications requires algorithmsspecifically designed for the complexities of natural agricultural conditions.Our work proposes a novel deep learning framework for classifying droughtstress in potato crops captured by UAVs in natural settings. The novelty liesin the synergistic combination of a pre-trained network with carefully designedcustom layers. This architecture leverages feature extraction capabilities ofthe pre-trained network while the custom layers enable targeted dimensionalityreduction and enhanced regularization, ultimately leading to improvedperformance. A key innovation of our work involves the integration ofGradient-Class Activation Mapping (Grad-CAM), an explainability technique.Grad-CAM sheds light on the internal workings of the deep learning model,typically referred to as a black box. By visualizing the focus areas of themodel within the images, Grad-CAM fosters interpretability and builds trust inthe decision-making process of the model. Our proposed framework achievessuperior performance, particularly with the DenseNet121 pre-trained network,reaching a precision of 97% to identify the stressed class with an overallaccuracy of 91%. Comparative analysis of existing state-of-the-art objectdetection algorithms reveals the superiority of our approach in significantlyhigher precision and accuracy.</description><author>Aswini Kumar Patra, Lingaraj Sahoo</author><pubDate>Wed, 31 Jul 2024 15:22:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10073v3</guid></item><item><title>Automatic classification of prostate MR series type using image content and metadata</title><link>http://arxiv.org/abs/2404.10892v2</link><description>With the wealth of medical image data, efficient curation is essential.Assigning the sequence type to magnetic resonance images is necessary forscientific studies and artificial intelligence-based analysis. However,incomplete or missing metadata prevents effective automation. We thereforepropose a deep-learning method for classification of prostate cancer scanningsequences based on a combination of image data and DICOM metadata. Wedemonstrate superior results compared to metadata or image data alone, and makeour code publicly available athttps://github.com/deepakri201/DICOMScanClassification.</description><author>Deepa Krishnaswamy, Bálint Kovács, Stefan Denner, Steve Pieper, David Clunie, Christopher P. Bridge, Tina Kapur, Klaus H. Maier-Hein, Andrey Fedorov</author><pubDate>Wed, 31 Jul 2024 15:18:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10892v2</guid></item><item><title>Green Edge AI: A Contemporary Survey</title><link>http://arxiv.org/abs/2312.00333v2</link><description>Artificial intelligence (AI) technologies have emerged as pivotal enablersacross a multitude of industries largely due to their significant resurgenceover the past decade. The transformative power of AI is primarily derived fromthe utilization of deep neural networks (DNNs), which require extensive datafor training and substantial computational resources for processing.Consequently, DNN models are typically trained and deployed on resource-richcloud servers. However, due to potential latency issues associated with cloudcommunications, deep learning (DL) workflows are increasingly beingtransitioned to wireless edge networks in proximity to end-user devices (EUDs).This shift is designed to support latency-sensitive applications and has givenrise to a new paradigm of edge AI, which will play a critical role in upcomingsixth-generation (6G) networks to support ubiquitous AI applications. Despiteits considerable potential, edge AI faces substantial challenges, mostly due tothe dichotomy between the resource limitations of wireless edge networks andthe resource-intensive nature of DL. Specifically, the acquisition oflarge-scale data, as well as the training and inference processes of DNNs, canrapidly deplete the battery energy of EUDs. This necessitates anenergy-conscious approach to edge AI to ensure both optimal and sustainableperformance. In this paper, we present a contemporary survey on green edge AI.We commence by analyzing the principal energy consumption components of edge AIsystems to identify the fundamental design principles of green edge AI. Guidedby these principles, we then explore energy-efficient design methodologies forthe three critical tasks in edge AI systems, including training dataacquisition, edge training, and edge inference. Finally, we underscorepotential future research directions to further enhance the energy efficiencyof edge AI.</description><author>Yuyi Mao, Xianghao Yu, Kaibin Huang, Ying-Jun Angela Zhang, Jun Zhang</author><pubDate>Wed, 31 Jul 2024 15:17:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00333v2</guid></item><item><title>Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation</title><link>http://arxiv.org/abs/2407.21674v1</link><description>Synthetic data is becoming increasingly integral in data-scarce fields suchas medical imaging, serving as a substitute for real data. However, itsinherent statistical characteristics can significantly impact downstream tasks,potentially compromising deployment performance. In this study, we empiricallyinvestigate this issue and uncover a critical phenomenon: downstream neuralnetworks often exploit spurious distinctions between real and synthetic datawhen there is a strong correlation between the data source and the task label.This exploitation manifests as \textit{simplicity bias}, where models overlyrely on superficial features rather than genuine task-related complexities.Through principled experiments, we demonstrate that the source of data (realvs.\ synthetic) can introduce spurious correlating factors leading to poorperformance during deployment when the correlation is absent. We firstdemonstrate this vulnerability on a digit classification task, where the modelspuriously utilizes the source of data instead of the digit to provide aninference. We provide further evidence of this phenomenon in a medical imagingproblem related to cardiac view classification in echocardiograms, particularlydistinguishing between 2-chamber and 4-chamber views. Given the increasing roleof utilizing synthetic datasets, we hope that our experiments serve aseffective guidelines for the utilization of synthetic datasets in modeltraining.</description><author>Krishan Agyakari Raja Babu, Rachana Sathish, Mrunal Pattanaik, Rahul Venkataramani</author><pubDate>Wed, 31 Jul 2024 15:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21674v1</guid></item><item><title>Universal Approximation Theory: Foundations for Parallelism in Neural Networks</title><link>http://arxiv.org/abs/2407.21670v1</link><description>Neural networks are increasingly evolving towards training large models withbig data, a method that has demonstrated superior performance across manytasks. However, this approach introduces an urgent problem: current deeplearning models are predominantly serial, meaning that as the number of networklayers increases, so do the training and inference times. This is unacceptableif deep learning is to continue advancing. Therefore, this paper proposes adeep learning parallelization strategy based on the Universal ApproximationTheorem (UAT). From this foundation, we designed a parallel network calledPara-Former to test our theory. Unlike traditional serial models, the inferencetime of Para-Former does not increase with the number of layers, significantlyaccelerating the inference speed of multi-layer networks. Experimental resultsvalidate the effectiveness of this network.</description><author>Wei Wang, Qing Li</author><pubDate>Wed, 31 Jul 2024 15:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21670v1</guid></item><item><title>Synth-Empathy: Towards High-Quality Synthetic Empathy Data</title><link>http://arxiv.org/abs/2407.21669v1</link><description>In recent years, with the rapid advancements in large language models (LLMs),achieving excellent empathetic response capabilities has become a crucialprerequisite. Consequently, managing and understanding empathetic datasets havegained increasing significance. However, empathetic data are typicallyhuman-labeled, leading to insufficient datasets and wasted human labor. In thiswork, we present Synth-Empathy, an LLM-based data generation and quality anddiversity selection pipeline that automatically generates high-qualityempathetic data while discarding low-quality data. With the data generated froma low empathetic model, we are able to further improve empathetic responseperformance and achieve state-of-the-art (SoTA) results across multiplebenchmarks. Moreover, our model achieves SoTA performance on various humanevaluation benchmarks, demonstrating its effectiveness and robustness inreal-world applications. Furthermore, we show the trade-off between dataquantity and quality, providing insights into empathetic data generation andselection.</description><author>Hao Liang, Linzhuang Sun, Jingxuan Wei, Xijie Huang, Linkun Sun, Bihui Yu, Conghui He, Wentao Zhang</author><pubDate>Wed, 31 Jul 2024 15:12:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21669v1</guid></item><item><title>An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification</title><link>http://arxiv.org/abs/2407.21666v1</link><description>Early detection of drought stress is critical for taking timely measures forreducing crop loss before the drought impact becomes irreversible. The subtlephenotypical and physiological changes in response to drought stress arecaptured by non-invasive imaging techniques and these imaging data serve asvaluable resource for machine learning methods to identify drought stress.While convolutional neural networks (CNNs) are in wide use, vision transformers(ViTs) present a promising alternative in capturing long-range dependencies andintricate spatial relationships, thereby enhancing the detection of subtleindicators of drought stress. We propose an explainable deep learning pipelinethat leverages the power of ViTs for drought stress detection in potato cropsusing aerial imagery. We applied two distinct approaches: a synergisticcombination of ViT and support vector machine (SVM), where ViT extractsintricate spatial features from aerial images, and SVM classifies the crops asstressed or healthy and an end-to-end approach using a dedicated classificationlayer within ViT to directly detect drought stress. Our key findings explainthe ViT model's decision-making process by visualizing attention maps. Thesemaps highlight the specific spatial features within the aerial images that theViT model focuses as the drought stress signature. Our findings demonstratethat the proposed methods not only achieve high accuracy in drought stressidentification but also shedding light on the diverse subtle plant featuresassociated with drought stress. This offers a robust and interpretable solutionfor drought stress monitoring for farmers to undertake informed decisions forimproved crop management.</description><author>Aswini Kumar Patra, Ankit Varshney, Lingaraj Sahoo</author><pubDate>Wed, 31 Jul 2024 15:08:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21666v1</guid></item><item><title>A State-of-the-Art Review of Computational Models for Analyzing Longitudinal Wearable Sensor Data in Healthcare</title><link>http://arxiv.org/abs/2407.21665v1</link><description>Wearable devices are increasingly used as tools for biomedical research, asthe continuous stream of behavioral and physiological data they collect canprovide insights about our health in everyday contexts. Long-term tracking,defined in the timescale of months of year, can provide insights of patternsand changes as indicators of health changes. These insights can make medicineand healthcare more predictive, preventive, personalized, and participative(The 4P's). However, the challenges in modeling, understanding and processinglongitudinal data are a significant barrier to their adoption in researchstudies and clinical settings. In this paper, we review and discuss threemodels used to make sense of longitudinal data: routines, rhythms and stabilitymetrics. We present the challenges associated with the processing and analysisof longitudinal wearable sensor data, with a special focus on how to handle thedifferent temporal dynamics at various granularities. We then discuss currentlimitations and identify directions for future work. This review is essentialto the advancement of computational modeling and analysis of longitudinalsensor data for pervasive healthcare.</description><author>Paula Lago</author><pubDate>Wed, 31 Jul 2024 15:08:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21665v1</guid></item><item><title>Novel Hybrid Integrated Pix2Pix and WGAN Model with Gradient Penalty for Binary Images Denoising</title><link>http://arxiv.org/abs/2407.11865v2</link><description>This paper introduces a novel approach to image denoising that leverages theadvantages of Generative Adversarial Networks (GANs). Specifically, we proposea model that combines elements of the Pix2Pix model and the Wasserstein GAN(WGAN) with Gradient Penalty (WGAN-GP). This hybrid framework seeks tocapitalize on the denoising capabilities of conditional GANs, as demonstratedin the Pix2Pix model, while mitigating the need for an exhaustive search foroptimal hyperparameters that could potentially ruin the stability of thelearning process. In the proposed method, the GAN's generator is employed toproduce denoised images, harnessing the power of a conditional GAN for noisereduction. Simultaneously, the implementation of the Lipschitz continuityconstraint during updates, as featured in WGAN-GP, aids in reducingsusceptibility to mode collapse. This innovative design allows the proposedmodel to benefit from the strong points of both Pix2Pix and WGAN-GP, generatingsuperior denoising results while ensuring training stability. Drawing onprevious work on image-to-image translation and GAN stabilization techniques,the proposed research highlights the potential of GANs as a general-purposesolution for denoising. The paper details the development and testing of thismodel, showcasing its effectiveness through numerical experiments. The datasetwas created by adding synthetic noise to clean images. Numerical results basedon real-world dataset validation underscore the efficacy of this approach inimage-denoising tasks, exhibiting significant enhancements over traditionaltechniques. Notably, the proposed model demonstrates strong generalizationcapabilities, performing effectively even when trained with synthetic noise.</description><author>Luca Tirel, Ali Mohamed Ali, Hashim A. Hashim</author><pubDate>Wed, 31 Jul 2024 15:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11865v2</guid></item><item><title>Is $F_1$ Score Suboptimal for Cybersecurity Models? Introducing $C_{score}$, a Cost-Aware Alternative for Model Assessment</title><link>http://arxiv.org/abs/2407.14664v2</link><description>The cost of errors related to machine learning classifiers, namely, falsepositives and false negatives, are not equal and are application dependent. Forexample, in cybersecurity applications, the cost of not detecting an attack isvery different from marking a benign activity as an attack. Various designchoices during machine learning model building, such as hyperparameter tuningand model selection, allow a data scientist to trade-off between these twoerrors. However, most of the commonly used metrics to evaluate model quality,such as $F_1$ score, which is defined in terms of model precision and recall,treat both these errors equally, making it difficult for users to optimize forthe actual cost of these errors. In this paper, we propose a new cost-awaremetric, $C_{score}$ based on precision and recall that can replace $F_1$ scorefor model evaluation and selection. It includes a cost ratio that takes intoaccount the differing costs of handling false positives and false negatives. Wederive and characterize the new cost metric, and compare it to $F_1$ score.Further, we use this metric for model thresholding for five cybersecurityrelated datasets for multiple cost ratios. The results show an average costsavings of 49%.</description><author>Manish Marwah, Asad Narayanan, Stephan Jou, Martin Arlitt, Maria Pospelova</author><pubDate>Wed, 31 Jul 2024 15:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14664v2</guid></item><item><title>Defending Jailbreak Attack in VLMs via Cross-modality Information Detector</title><link>http://arxiv.org/abs/2407.21659v1</link><description>Vision Language Models (VLMs) extend the capacity of LLMs to comprehensivelyunderstand vision information, achieving remarkable performance in manyvision-centric tasks. Despite that, recent studies have shown that these modelsare susceptible to jailbreak attacks, which refer to an exploitative techniquewhere malicious users can break the safety alignment of the target model andgenerate misleading and harmful answers. This potential threat is caused byboth the inherent vulnerabilities of LLM and the larger attack scope introducedby vision input. To enhance the security of VLMs against jailbreak attacks,researchers have developed various defense techniques. However, these methodseither require modifications to the model's internal structure or demandsignificant computational resources during the inference phase. Multimodalinformation is a double-edged sword. While it increases the risk of attacks, italso provides additional data that can enhance safeguards. Inspired by this, wepropose $\underline{\textbf{C}}$ross-modality$\underline{\textbf{I}}$nformation$\underline{\textbf{DE}}$tecto$\underline{\textbf{R}}$ ($\textit{CIDER})$, aplug-and-play jailbreaking detector designed to identify maliciously perturbedimage inputs, utilizing the cross-modal similarity between harmful queries andadversarial images. This simple yet effective cross-modality informationdetector, $\textit{CIDER}$, is independent of the target VLMs and requires lesscomputation cost. Extensive experimental results demonstrate the effectivenessand efficiency of $\textit{CIDER}$, as well as its transferability to bothwhite-box and black-box VLMs.</description><author>Yue Xu, Xiuyuan Qi, Zhan Qin, Wenjie Wang</author><pubDate>Wed, 31 Jul 2024 15:02:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21659v1</guid></item><item><title>Beat this! Accurate beat tracking without DBN postprocessing</title><link>http://arxiv.org/abs/2407.21658v1</link><description>We propose a system for tracking beats and downbeats with two objectives:generality across a diverse music range, and high accuracy. We achievegenerality by training on multiple datasets -- including solo instrumentrecordings, pieces with time signature changes, and classical music with hightempo variations -- and by removing the commonly used Dynamic Bayesian Network(DBN) postprocessing, which introduces constraints on the meter and tempo. Forhigh accuracy, among other improvements, we develop a loss function tolerant tosmall time shifts of annotations, and an architecture alternating convolutionswith transformers either over frequency or time. Our system surpasses thecurrent state of the art in F1 score despite using no DBN. However, it canstill fail, especially for difficult and underrepresented genres, and performsworse on continuity metrics, so we publish our model, code, and preprocesseddatasets, and invite others to beat this.</description><author>Francesco Foscarin, Jan Schlüter, Gerhard Widmer</author><pubDate>Wed, 31 Jul 2024 14:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21658v1</guid></item><item><title>Comgra: A Tool for Analyzing and Debugging Neural Networks</title><link>http://arxiv.org/abs/2407.21656v1</link><description>Neural Networks are notoriously difficult to inspect. We introduce comgra, anopen source python library for use with PyTorch. Comgra extracts data about theinternal activations of a model and organizes it in a GUI (graphical userinterface). It can show both summary statistics and individual data points,compare early and late stages of training, focus on individual samples ofinterest, and visualize the flow of the gradient through the network. Thismakes it possible to inspect the model's behavior from many different anglesand save time by rapidly testing different hypotheses without having to rerunit. Comgra has applications for debugging, neural architecture design, andmechanistic interpretability. We publish our library through Python PackageIndex (PyPI) and provide code, documentation, and tutorials athttps://github.com/FlorianDietz/comgra.</description><author>Florian Dietz, Sophie Fellenz, Dietrich Klakow, Marius Kloft</author><pubDate>Wed, 31 Jul 2024 14:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21656v1</guid></item><item><title>MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment</title><link>http://arxiv.org/abs/2407.21654v1</link><description>Recent approaches have shown that large-scale vision-language models such asCLIP can improve semantic segmentation performance. These methods typically aimfor pixel-level vision-language alignment, but often rely on low resolutionimage features from CLIP, resulting in class ambiguities along boundaries.Moreover, the global scene representations in CLIP text embeddings do notdirectly correlate with the local and detailed pixel-level features, makingmeaningful alignment more difficult. To address these limitations, we introduceMTA-CLIP, a novel framework employing mask-level vision-language alignment.Specifically, we first propose Mask-Text Decoder that enhances the maskrepresentations using rich textual data with the CLIP language model.Subsequently, it aligns mask representations with text embeddings usingMask-to-Text Contrastive Learning. Furthermore, we introduce MaskText PromptLearning, utilizing multiple context-specific prompts for text embeddings tocapture diverse class representations across masks. Overall, MTA-CLIP achievesstate-of-the-art, surpassing prior works by an average of 2.8% and 1.3% on onstandard benchmark datasets, ADE20k and Cityscapes, respectively.</description><author>Anurag Das, Xinting Hu, Li Jiang, Bernt Schiele</author><pubDate>Wed, 31 Jul 2024 14:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21654v1</guid></item><item><title>Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records</title><link>http://arxiv.org/abs/2310.19967v3</link><description>Early detection of inflammatory arthritis (IA) is critical to efficient andaccurate hospital referral triage for timely treatment and preventing thedeterioration of the IA disease course, especially under limited healthcareresources. The manual assessment process is the most common approach inpractice for the early detection of IA, but it is extremely labor-intensive andinefficient. A large amount of clinical information needs to be assessed forevery referral from General Practice (GP) to the hospitals. Machine learningshows great potential in automating repetitive assessment tasks and providingdecision support for the early detection of IA. However, most machinelearning-based methods for IA detection rely on blood testing results. But inpractice, blood testing data is not always available at the point of referrals,so we need methods to leverage multimodal data such as semi-structured andunstructured data for early detection of IA. In this research, we presentfusion and ensemble learning-based methods using multimodal data to assistdecision-making in the early detection of IA, and a conformal prediction-basedmethod to quantify the uncertainty of the prediction and detect any unreliablepredictions. To the best of our knowledge, our study is the first attempt toutilize multimodal data to support the early detection of IA from GP referrals.</description><author>Bing Wang, Weizi Li, Anthony Bradlow, Antoni T. Y. Chan, Eghosa Bazuaye</author><pubDate>Wed, 31 Jul 2024 14:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19967v3</guid></item><item><title>Spatial Transformer Network YOLO Model for Agricultural Object Detection</title><link>http://arxiv.org/abs/2407.21652v1</link><description>Object detection plays a crucial role in the field of computer vision byautonomously identifying and locating objects of interest. The You Only LookOnce (YOLO) model is an effective single-shot detector. However, YOLO faceschallenges in cluttered or partially occluded scenes and can struggle withsmall, low-contrast objects. We propose a new method that integrates spatialtransformer networks (STNs) into YOLO to improve performance. The proposedSTN-YOLO aims to enhance the model's effectiveness by focusing on importantareas of the image and improving the spatial invariance of the model before thedetection process. Our proposed method improved object detection performanceboth qualitatively and quantitatively. We explore the impact of differentlocalization networks within the STN module as well as the robustness of themodel across different spatial transformations. We apply the STN-YOLO onbenchmark datasets for Agricultural object detection as well as a new datasetfrom a state-of-the-art plant phenotyping greenhouse facility. Our code anddataset are publicly available.</description><author>Yash Zambre, Ekdev Rajkitkul, Akshatha Mohan, Joshua Peeples</author><pubDate>Wed, 31 Jul 2024 14:53:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21652v1</guid></item><item><title>PP-TIL: Personalized Planning for Autonomous Driving with Instance-based Transfer Imitation Learning</title><link>http://arxiv.org/abs/2407.18569v2</link><description>Personalized motion planning holds significant importance within urbanautomated driving, catering to the unique requirements of individual users.Nevertheless, prior endeavors have frequently encountered difficulties insimultaneously addressing two crucial aspects: personalized planning withinintricate urban settings and enhancing planning performance through datautilization. The challenge arises from the expensive and limited nature of userdata, coupled with the scene state space tending towards infinity. Thesefactors contribute to overfitting and poor generalization problems during modeltraining. Henceforth, we propose an instance-based transfer imitation learningapproach. This method facilitates knowledge transfer from extensive expertdomain data to the user domain, presenting a fundamental resolution to theseissues. We initially train a pre-trained model using large-scale expert data.Subsequently, during the fine-tuning phase, we feed the batch data, whichcomprises expert and user data. Employing the inverse reinforcement learningtechnique, we extract the style feature distribution from user demonstrations,constructing the regularization term for the approximation of user style. Inour experiments, we conducted extensive evaluations of the proposed method.Compared to the baseline methods, our approach mitigates the overfitting issuecaused by sparse user data. Furthermore, we discovered that integrating thedriving model with a differentiable nonlinear optimizer as a safety protectionlayer for end-to-end personalized fine-tuning results in superior planningperformance.</description><author>Fangze Lin, Ying He, Fei Yu</author><pubDate>Wed, 31 Jul 2024 14:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18569v2</guid></item><item><title>Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI</title><link>http://arxiv.org/abs/2403.05245v2</link><description>In general, diffusion model-based MRI reconstruction methods incrementallyremove artificially added noise while imposing data consistency to reconstructthe underlying images. However, real-world MRI acquisitions already containinherent noise due to thermal fluctuations. This phenomenon is particularlynotable when using ultra-fast, high-resolution imaging sequences for advancedresearch, or using low-field systems favored by low- and middle-incomecountries. These common scenarios can lead to sub-optimal performance orcomplete failure of existing diffusion model-based reconstruction techniques.Specifically, as the artificially added noise is gradually removed, theinherent MRI noise becomes increasingly pronounced, making the actual noiselevel inconsistent with the predefined denoising schedule and consequentlyinaccurate image reconstruction. To tackle this problem, we propose a posteriorsampling strategy with a novel NoIse Level Adaptive Data Consistency (Nila-DC)operation. Extensive experiments are conducted on two public datasets and anin-house clinical dataset with field strength ranging from 0.3T to 3T, showingthat our method surpasses the state-of-the-art MRI reconstruction methods, andis highly robust against various noise levels. The code for Nila is availableat https://github.com/Solor-pikachu/Nila.</description><author>Shoujin Huang, Guanxiong Luo, Xi Wang, Ziran Chen, Yuwan Wang, Huaishui Yang, Pheng-Ann Heng, Lingyan Zhang, Mengye Lyu</author><pubDate>Wed, 31 Jul 2024 14:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05245v2</guid></item><item><title>Human interaction classifier for LLM based chatbot</title><link>http://arxiv.org/abs/2407.21647v1</link><description>This study investigates different approaches to classify human interactionsin an artificial intelligence-based environment, specifically for Applus+IDIADA's intelligent agent AIDA. The main objective is to develop a classifierthat accurately identifies the type of interaction received (Conversation,Services, or Document Translation) to direct requests to the appropriatechannel and provide a more specialized and efficient service. Various modelsare compared, including LLM-based classifiers, KNN using Titan and Cohereembeddings, SVM, and artificial neural networks. Results show that SVM and ANNmodels with Cohere embeddings achieve the best overall performance, withsuperior F1 scores and faster execution times compared to LLM-based approaches.The study concludes that the SVM model with Cohere embeddings is the mostsuitable option for classifying human interactions in the AIDA environment,offering an optimal balance between accuracy and computational efficiency.</description><author>Diego Martín, Jordi Sanchez, Xavier Vizcaíno</author><pubDate>Wed, 31 Jul 2024 14:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21647v1</guid></item><item><title>Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent</title><link>http://arxiv.org/abs/2407.21646v1</link><description>In this paper, we present Cross Language Agent -- SimultaneousInterpretation, CLASI, a high-quality and human-like Simultaneous SpeechTranslation (SiST) System. Inspired by professional human interpreters, weutilize a novel data-driven read-write strategy to balance the translationquality and latency. To address the challenge of translating in-domainterminologies, CLASI employs a multi-modal retrieving module to obtain relevantinformation to augment the translation. Supported by LLMs, our approach cangenerate error-tolerated translation by considering the input audio, historicalcontext, and retrieved information. Experimental results show that our systemoutperforms other systems by significant margins. Aligned with professionalhuman interpreters, we evaluate CLASI with a better human evaluation metric,valid information proportion (VIP), which measures the amount of informationthat can be successfully conveyed to the listeners. In the real-worldscenarios, where the speeches are often disfluent, informal, and unclear, CLASIachieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinesetranslation directions, respectively. In contrast, state-of-the-art commercialor open-source systems only achieve 35.4% and 41.6%. On the extremely harddataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%VIP.</description><author>Shanbo Cheng, Zhichao Huang, Tom Ko, Hang Li, Ningxin Peng, Lu Xu, Qini Zhang</author><pubDate>Wed, 31 Jul 2024 14:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21646v1</guid></item><item><title>Lyapunov weights to convey the meaning of time in physics-informed neural networks</title><link>http://arxiv.org/abs/2407.21642v1</link><description>Time is not a dimension as the others. In Physics-Informed Neural Networks(PINN) several proposals attempted to adapt the time sampling or time weightingto take into account the specifics of this special dimension. But theseproposals are not principled and need guidance to be used. We explain heretheoretically why the Lyapunov exponents give actionable insights and propose aweighting scheme to automatically adapt to chaotic, periodic or stabledynamics. We characterize theoretically the best weighting scheme undercomputational constraints as a cumulative exponential integral of the localLyapunov exponent estimators and show that it performs well in practice underthe regimes mentioned above.</description><author>Gabriel Turinici</author><pubDate>Wed, 31 Jul 2024 14:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21642v1</guid></item><item><title>MSA2Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation</title><link>http://arxiv.org/abs/2407.21640v1</link><description>Medical image segmentation involves identifying and separating objectinstances in a medical image to delineate various tissues and structures, atask complicated by the significant variations in size, shape, and density ofthese features. Convolutional neural networks (CNNs) have traditionally beenused for this task but have limitations in capturing long-range dependencies.Transformers, equipped with self-attention mechanisms, aim to address thisproblem. However, in medical image segmentation it is beneficial to merge bothlocal and global features to effectively integrate feature maps across variousscales, capturing both detailed features and broader semantic elements fordealing with variations in structures. In this paper, we introduce MSA2Net, anew deep segmentation framework featuring an expedient design ofskip-connections. These connections facilitate feature fusion by dynamicallyweighting and combining coarse-grained encoder features with fine-graineddecoder feature maps. Specifically, we propose a Multi-Scale Adaptive SpatialAttention Gate (MASAG), which dynamically adjusts the receptive field (Localand Global contextual information) to ensure that spatially relevant featuresare selectively highlighted while minimizing background distractions. Extensiveevaluations involving dermatology, and radiological datasets demonstrate thatour MSA2Net outperforms state-of-the-art (SOTA) works or matches theirperformance. The source code is publicly available athttps://github.com/xmindflow/MSA-2Net.</description><author>Sina Ghorbani Kolahi, Seyed Kamal Chaharsooghi, Toktam Khatibi, Afshin Bozorgpour, Reza Azad, Moein Heidari, Ilker Hacihaliloglu, Dorit Merhof</author><pubDate>Wed, 31 Jul 2024 14:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21640v1</guid></item><item><title>Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models</title><link>http://arxiv.org/abs/2407.16205v2</link><description>The rapid development of Large Language Models (LLMs) has brought remarkablegenerative capabilities across diverse tasks. However, despite the impressiveachievements, these models still have numerous security vulnerabilities,particularly when faced with jailbreak attacks. Therefore, by investigatingjailbreak attacks, we can uncover hidden weaknesses in LLMs and guide us indeveloping more robust defense mechanisms to fortify their security. In thispaper, we further explore the boundary of jailbreak attacks on LLMs and proposeAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takesadvantage of LLMs' growing analyzing and reasoning capability and reveals theirunderlying vulnerabilities when facing analysis-based tasks. We conduct adetailed evaluation of ABJ across various open-source and closed-source LLMs,which achieves 94.8% Attack Success Rate (ASR) and 1.06 Attack Efficiency (AE)on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness andefficiency. Our research highlights the importance of prioritizing andenhancing the safety of LLMs to mitigate the risks of misuse.The code ispublicly available at https://github.com/theshi-1128/ABJ-Attack.</description><author>Shi Lin, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han</author><pubDate>Wed, 31 Jul 2024 14:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16205v2</guid></item><item><title>Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components</title><link>http://arxiv.org/abs/2407.21638v1</link><description>Automation of medical image interpretation could alleviate bottlenecks indiagnostic workflows, and has become of particular interest in recent years dueto advancements in natural language processing. Great strides have been madetowards automated radiology report generation via AI, yet ensuring clinicalaccuracy in generated reports is a significant challenge, hindering deploymentof such methods in clinical practice. In this work we propose a quality controlframework for assessing the reliability of AI-generated radiology reports withrespect to semantics of diagnostic importance using modular auxiliary auditingcomponents (AC). Evaluating our pipeline on the MIMIC-CXR dataset, our findingsshow that incorporating ACs in the form of disease-classifiers can enableauditing that identifies more reliable reports, resulting in higher F1 scorescompared to unfiltered generated reports. Additionally, leveraging theconfidence of the AC labels further improves the audit's effectiveness.</description><author>Hermione Warr, Yasin Ibrahim, Daniel R. McGowan, Konstantinos Kamnitsas</author><pubDate>Wed, 31 Jul 2024 14:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21638v1</guid></item><item><title>Practical aspects for the creation of an audio dataset from field recordings with optimized labeling budget with AI-assisted strategy</title><link>http://arxiv.org/abs/2405.18153v2</link><description>Machine Listening focuses on developing technologies to extract relevantinformation from audio signals. A critical aspect of these projects is theacquisition and labeling of contextualized data, which is inherently complexand requires specific resources and strategies. Despite the availability ofsome audio datasets, many are unsuitable for commercial applications. The paperemphasizes the importance of Active Learning (AL) using expert labelers overcrowdsourcing, which often lacks detailed insights into dataset structures. ALis an iterative process combining human labelers and AI models to optimize thelabeling budget by intelligently selecting samples for human review. Thisapproach addresses the challenge of handling large, constantly growing datasetsthat exceed available computational resources and memory. The paper presents acomprehensive data-centric framework for Machine Listening projects, detailingthe configuration of recording nodes, database structure, and labeling budgetoptimization in resource-constrained scenarios. Applied to an industrial portin Valencia, Spain, the framework successfully labeled 6540 ten-second audiosamples over five months with a small team, demonstrating its effectiveness andadaptability to various resource availability situations. Acknowledgments: The participation of Javier Naranjo-Alcazar, Jordi Grau-Haroand Pedro Zuccarello in this research was funded by the Valencian Institute forBusiness Competitiveness (IVACE) and the FEDER funds by means of projectSoroll-IA2 (IMDEEA/2023/91).</description><author>Javier Naranjo-Alcazar, Jordi Grau-Haro, Ruben Ribes-Serrano, Pedro Zuccarello</author><pubDate>Wed, 31 Jul 2024 14:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18153v2</guid></item><item><title>MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction</title><link>http://arxiv.org/abs/2407.21635v1</link><description>Multi-agent trajectory prediction is crucial to autonomous driving andunderstanding the surrounding environment. Learning-based approaches formulti-agent trajectory prediction, such as primarily relying on graph neuralnetworks, graph transformers, and hypergraph neural networks, have demonstratedoutstanding performance on real-world datasets in recent years. However, thehypergraph transformer-based method for trajectory prediction is yet to beexplored. Therefore, we present a MultiscAle Relational Transformer (MART)network for multi-agent trajectory prediction. MART is a hypergraph transformerarchitecture to consider individual and group behaviors in transformermachinery. The core module of MART is the encoder, which comprises a Pair-wiseRelational Transformer (PRT) and a Hyper Relational Transformer (HRT). Theencoder extends the capabilities of a relational transformer by introducingHRT, which integrates hyperedge features into the transformer mechanism,promoting attention weights to focus on group-wise relations. In addition, wepropose an Adaptive Group Estimator (AGE) designed to infer complex grouprelations in real-world environments. Extensive experiments on three real-worlddatasets (NBA, SDD, and ETH-UCY) demonstrate that our method achievesstate-of-the-art performance, enhancing ADE/FDE by 3.9%/11.8% on the NBAdataset. Code is available at https://github.com/gist-ailab/MART.</description><author>Seongju Lee, Junseok Lee, Yeonguk Yu, Taeri Kim, Kyoobin Lee</author><pubDate>Wed, 31 Jul 2024 14:31:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21635v1</guid></item><item><title>Textual Query-Driven Mask Transformer for Domain Generalized Segmentation</title><link>http://arxiv.org/abs/2407.09033v2</link><description>In this paper, we introduce a method to tackle Domain Generalized SemanticSegmentation (DGSS) by utilizing domain-invariant semantic knowledge from textembeddings of vision-language models. We employ the text embeddings as objectqueries within a transformer-based segmentation framework (textual objectqueries). These queries are regarded as a domain-invariant basis for pixelgrouping in DGSS. To leverage the power of textual object queries, we introducea novel framework named the textual query-driven mask transformer (tqdm). Ourtqdm aims to (1) generate textual object queries that maximally encodedomain-invariant semantics and (2) enhance the semantic clarity of dense visualfeatures. Additionally, we suggest three regularization losses to improve theefficacy of tqdm by aligning between visual and textual features. By utilizingour method, the model can comprehend inherent semantic information for classesof interest, enabling it to generalize to extreme domains (e.g., sketch style).Our tqdm achieves 68.9 mIoU on GTA5$\rightarrow$Cityscapes, outperforming theprior state-of-the-art method by 2.5 mIoU. The project page is available athttps://byeonghyunpak.github.io/tqdm.</description><author>Byeonghyun Pak, Byeongju Woo, Sunghwan Kim, Dae-hwan Kim, Hoseong Kim</author><pubDate>Wed, 31 Jul 2024 14:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09033v2</guid></item><item><title>Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation</title><link>http://arxiv.org/abs/2407.21633v1</link><description>Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems totransition to unfamiliar domains without manual annotation or extensiveretraining. Prior research has approached this objective by embedding promptsinto language models (LMs). Common methodologies include integrating prompts atthe input layer or introducing learnable variables at each transformer layer.Nonetheless, each strategy exhibits inherent limitations. Prompts integrated atthe input layer risk underutilization, with their impact potentiallydiminishing across successive transformer layers. Conversely, the addition oflearnable variables to each layer can complicate the training process andincrease inference latency. To tackle the issues mentioned above, this paperproposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecturedesigned for zero-shot DST. DualLoRA incorporates two distinct Low-RankAdaptation (LoRA) components, targeting both dialogue context processing andprompt optimization, to ensure the comprehensive influence of promptsthroughout the transformer model layers. This is achieved without incurringadditional inference latency, showcasing an efficient integration into existingarchitectures. Through rigorous evaluation on the MultiWOZ and SGD datasets,DualLoRA demonstrates notable improvements across multiple domains,outperforming traditional baseline methods in zero-shot settings. Our code isaccessible at: \url{https://github.com/suntea233/DualLoRA}.</description><author>Xiang Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang</author><pubDate>Wed, 31 Jul 2024 14:26:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21633v1</guid></item><item><title>Lexicase-based Selection Methods with Down-sampling for Symbolic Regression Problems: Overview and Benchmark</title><link>http://arxiv.org/abs/2407.21632v1</link><description>In recent years, several new lexicase-based selection variants have emergeddue to the success of standard lexicase selection in various applicationdomains. For symbolic regression problems, variants that use anepsilon-threshold or batches of training cases, among others, have led toperformance improvements. Lately, especially variants that combine lexicaseselection and down-sampling strategies have received a lot of attention. Thispaper evaluates random as well as informed down-sampling in combination withthe relevant lexicase-based selection methods on a wide range of symbolicregression problems. In contrast to most work, we not only compare the methodsover a given evaluation budget, but also over a given time as time is usuallylimited in practice. We find that for a given evaluation budget,epsilon-lexicase selection in combination with random or informed down-samplingoutperforms all other methods. Only for a rather long running time of 24h, thebest performing method is tournament selection in combination with informeddown-sampling. If the given running time is very short, lexicase variants usingbatches of training cases perform best.</description><author>Alina Geiger, Dominik Sobania, Franz Rothlauf</author><pubDate>Wed, 31 Jul 2024 14:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21632v1</guid></item><item><title>RoadFormer+: Delivering RGB-X Scene Parsing through Scale-Aware Information Decoupling and Advanced Heterogeneous Feature Fusion</title><link>http://arxiv.org/abs/2407.21631v1</link><description>Task-specific data-fusion networks have marked considerable achievements inurban scene parsing. Among these networks, our recently proposed RoadFormersuccessfully extracts heterogeneous features from RGB images and surface normalmaps and fuses these features through attention mechanisms, demonstratingcompelling efficacy in RGB-Normal road scene parsing. However, its performancesignificantly deteriorates when handling other types/sources of data orperforming more universal, all-category scene parsing tasks. To overcome theselimitations, this study introduces RoadFormer+, an efficient, robust, andadaptable model capable of effectively fusing RGB-X data, where ``X'',represents additional types/modalities of data such as depth, thermal, surfacenormal, and polarization. Specifically, we propose a novel hybrid featuredecoupling encoder to extract heterogeneous features and decouple them intoglobal and local components. These decoupled features are then fused through adual-branch multi-scale heterogeneous feature fusion block, which employsparallel Transformer attentions and convolutional neural network modules tomerge multi-scale features across different scales and receptive fields. Thefused features are subsequently fed into a decoder to generate the finalsemantic predictions. Notably, our proposed RoadFormer+ ranks first on theKITTI Road benchmark and achieves state-of-the-art performance in meanintersection over union on the Cityscapes, MFNet, FMB, and ZJU datasets.Moreover, it reduces the number of learnable parameters by 65\% compared toRoadFormer. Our source code will be publicly available atmias.group/RoadFormerPlus.</description><author>Jianxin Huang, Jiahang Li, Ning Jia, Yuxiang Sun, Chengju Liu, Qijun Chen, Rui Fan</author><pubDate>Wed, 31 Jul 2024 14:25:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21631v1</guid></item><item><title>Towards Generalizable Reinforcement Learning via Causality-Guided Self-Adaptive Representations</title><link>http://arxiv.org/abs/2407.20651v2</link><description>General intelligence requires quick adaption across tasks. While existingreinforcement learning (RL) methods have made progress in generalization, theytypically assume only distribution changes between source and target domains.In this paper, we explore a wider range of scenarios where both thedistribution and environment spaces may change. For example, in Atari games, wetrain agents to generalize to tasks with different levels of mode anddifficulty, where there could be new state or action variables that neveroccurred in previous environments. To address this challenging setting, weintroduce a causality-guided self-adaptive representation-based approach,called CSR, that equips the agent to generalize effectively and efficientlyacross a sequence of tasks with evolving dynamics. Specifically, we employcausal representation learning to characterize the latent causal variables andworld models within the RL system. Such compact causal representations uncoverthe structural relationships among variables, enabling the agent toautonomously determine whether changes in the environment stem fromdistribution shifts or variations in space, and to precisely locate thesechanges. We then devise a three-step strategy to fine-tune the model underdifferent scenarios accordingly. Empirical experiments show that CSRefficiently adapts to the target domains with only a few samples andoutperforms state-of-the-art baselines on a wide range of scenarios, includingour simulated environments, Cartpole, and Atari games.</description><author>Yupei Yang, Biwei Huang, Fan Feng, Xinyue Wang, Shikui Tu, Lei Xu</author><pubDate>Wed, 31 Jul 2024 14:24:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20651v2</guid></item><item><title>TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods</title><link>http://arxiv.org/abs/2407.21630v1</link><description>Authorship obfuscation aims to disguise the identity of an author within atext by altering the writing style, vocabulary, syntax, and other linguisticfeatures associated with the text author. This alteration needs to balanceprivacy and utility. While strong obfuscation techniques can effectively hidethe author's identity, they often degrade the quality and usefulness of thetext for its intended purpose. Conversely, maintaining high utility tends toprovide insufficient privacy, making it easier for an adversary to de-anonymizethe author. Thus, achieving an optimal trade-off between these two conflictingobjectives is crucial. In this paper, we propose TAROT: Task-OrientedAuthorship Obfuscation Using Policy Optimization, a new unsupervised authorshipobfuscation method whose goal is to optimize the privacy-utility trade-off byregenerating the entire text considering its downstream utility. Our approachleverages policy optimization as a fine-tuning paradigm over small languagemodels in order to rewrite texts by preserving author identity and downstreamtask utility. We show that our approach largely reduce the accuracy ofattackers while preserving utility. We make our code and models publiclyavailable.</description><author>Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi</author><pubDate>Wed, 31 Jul 2024 14:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21630v1</guid></item><item><title>ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024</title><link>http://arxiv.org/abs/2407.12038v2</link><description>The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024)is part of the ISCSLP 2024 Competitions and Challenges track. While currenttext-to-speech (TTS) technology can generate high-quality audio, its ability toconvey complex emotions and controlled detail content remains limited. Thisconstraint leads to a discrepancy between the generated audio and humansubjective perception in practical applications like companion robots forchildren and marketing bots. The core issue lies in the inconsistency betweenhigh-quality audio generation and the ultimate human subjective experience.Therefore, this challenge aims to enhance the persuasiveness and acceptabilityof synthesized audio, focusing on human alignment convincing and inspirationalaudio generation. A total of 19 teams have registered for the challenge, andthe results of the competition and the competition are described in this paper.</description><author>Ruibo Fu, Rui Liu, Chunyu Qiang, Yingming Gao, Yi Lu, Shuchen Shi, Tao Wang, Ya Li, Zhengqi Wen, Chen Zhang, Hui Bu, Yukun Liu, Xin Qi, Guanjun Li</author><pubDate>Wed, 31 Jul 2024 14:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12038v2</guid></item><item><title>Extended Fiducial Inference: Toward an Automated Process of Statistical Inference</title><link>http://arxiv.org/abs/2407.21622v1</link><description>While fiducial inference was widely considered a big blunder by R.A. Fisher,the goal he initially set --`inferring the uncertainty of model parameters onthe basis of observations' -- has been continually pursued by manystatisticians. To this end, we develop a new statistical inference methodcalled extended Fiducial inference (EFI). The new method achieves the goal offiducial inference by leveraging advanced statistical computing techniqueswhile remaining scalable for big data. EFI involves jointly imputing randomerrors realized in observations using stochastic gradient Markov chain MonteCarlo and estimating the inverse function using a sparse deep neural network(DNN). The consistency of the sparse DNN estimator ensures that the uncertaintyembedded in observations is properly propagated to model parameters through theestimated inverse function, thereby validating downstream statisticalinference. Compared to frequentist and Bayesian methods, EFI offers significantadvantages in parameter estimation and hypothesis testing. Specifically, EFIprovides higher fidelity in parameter estimation, especially when outliers arepresent in the observations; and eliminates the need for theoretical referencedistributions in hypothesis testing, thereby automating the statisticalinference process. EFI also provides an innovative framework forsemi-supervised learning.</description><author>Faming Liang, Sehwan Kim, Yan Sun</author><pubDate>Wed, 31 Jul 2024 14:15:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21622v1</guid></item><item><title>EZSR: Event-based Zero-Shot Recognition</title><link>http://arxiv.org/abs/2407.21616v1</link><description>This paper studies zero-shot object recognition using event camera data.Guided by CLIP, which is pre-trained on RGB images, existing approaches achievezero-shot object recognition by maximizing embedding similarities between eventdata encoded by an event encoder and RGB images encoded by the CLIP imageencoder. Alternatively, several methods learn RGB frame reconstructions fromevent data for the CLIP image encoder. However, these approaches often resultin suboptimal zero-shot performance. This study develops an event encoder without relying on additionalreconstruction networks. We theoretically analyze the performance bottlenecksof previous approaches: global similarity-based objective (i.e., maximizing theembedding similarities) cause semantic misalignments between the learned eventembedding space and the CLIP text embedding space due to the degree of freedom.To mitigate the issue, we explore a scalar-wise regularization strategy.Furthermore, to scale up the number of events and RGB data pairs for training,we also propose a pipeline for synthesizing event data from static RGB images. Experimentally, our data synthesis strategy exhibits an attractive scalingproperty, and our method achieves superior zero-shot object recognitionperformance on extensive standard benchmark datasets, even compared with pastsupervised learning approaches. For example, we achieve 47.84% zero-shotaccuracy on the N-ImageNet dataset.</description><author>Yan Yang, Liyuan Pan, Dongxu Li, Liu Liu</author><pubDate>Wed, 31 Jul 2024 14:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21616v1</guid></item><item><title>Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music</title><link>http://arxiv.org/abs/2407.21615v1</link><description>Generative AI models have recently blossomed, significantly impactingartistic and musical traditions. Research investigating how humans interactwith and deem these models is therefore crucial. Through a listening andreflection study, we explore participants' perspectives on AI- vshuman-generated progressive metal, in symbolic format, using rock music as acontrol group. AI-generated examples were produced by ProgGP, aTransformer-based model. We propose a mixed methods approach to assess theeffects of generation type (human vs. AI), genre (progressive metal vs. rock),and curation process (random vs. cherry-picked). This combines quantitativefeedback on genre congruence, preference, creativity, consistency, playability,humanness, and repeatability, and qualitative feedback to provide insights intolisteners' experiences. A total of 32 progressive metal fans completed thestudy. Our findings validate the use of fine-tuning to achieve genre-specificspecialization in AI music generation, as listeners could distinguish betweenAI-generated rock and progressive metal. Despite some AI-generated excerptsreceiving similar ratings to human music, listeners exhibited a preference forhuman compositions. Thematic analysis identified key features for genre and AIvs. human distinctions. Finally, we consider the ethical implications of ourwork in promoting musical data diversity within MIR research by focusing on anunder-explored genre.</description><author>Pedro Sarmento, Jackson Loth, Mathieu Barthet</author><pubDate>Wed, 31 Jul 2024 14:03:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21615v1</guid></item><item><title>Iterative Ensemble Training with Anti-Gradient Control for Mitigating Memorization in Diffusion Models</title><link>http://arxiv.org/abs/2407.15328v2</link><description>Diffusion models, known for their tremendous ability to generate novel andhigh-quality samples, have recently raised concerns due to their datamemorization behavior, which poses privacy risks. Recent approaches for memorymitigation either only focused on the text modality problem in cross-modalgeneration tasks or utilized data augmentation strategies. In this paper, wepropose a novel training framework for diffusion models from the perspective ofvisual modality, which is more generic and fundamental for mitigatingmemorization. To facilitate forgetting of stored information in diffusion modelparameters, we propose an iterative ensemble training strategy by splitting thedata into multiple shards for training multiple models and intermittentlyaggregating these model parameters. Moreover, practical analysis of lossesillustrates that the training loss for easily memorable images tends to beobviously lower. Thus, we propose an anti-gradient control method to excludethe sample with a lower loss value from the current mini-batch to avoidmemorizing. Extensive experiments and analysis on four datasets are conductedto illustrate the effectiveness of our method, and results show that our methodsuccessfully reduces memory capacity while even improving the performanceslightly. Moreover, to save the computing cost, we successfully apply ourmethod to fine-tune the well-trained diffusion models by limited epochs,demonstrating the applicability of our method. Code is available inhttps://github.com/liuxiao-guan/IET_AGC.</description><author>Xiao Liu, Xiaoliu Guan, Yu Wu, Jiaxu Miao</author><pubDate>Wed, 31 Jul 2024 13:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15328v2</guid></item><item><title>Analysis of Total Variation Minimization for Clustered Federated Learning</title><link>http://arxiv.org/abs/2403.06298v2</link><description>A key challenge in federated learning applications is the statisticalheterogeneity of local datasets. Clustered federated learning addresses thischallenge by identifying clusters of local datasets that are approximatelyhomogeneous. One recent approach to clustered federated learning is generalizedtotal variation minimization (GTVMin). This approach requires a similaritygraph which can be obtained by domain expertise or in a data-driven fashion viagraph learning techniques. Under a widely applicable clustering assumption, wederive an upper bound the deviation between GTVMin solutions and theircluster-wise averages. This bound provides valuable insights into theeffectiveness and robustness of GTVMin in addressing statistical heterogeneitywithin federated learning environments.</description><author>A. Jung</author><pubDate>Wed, 31 Jul 2024 13:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06298v2</guid></item><item><title>Diversifying AI: Towards Creative Chess with AlphaZero</title><link>http://arxiv.org/abs/2308.09175v3</link><description>In recent years, Artificial Intelligence (AI) systems have surpassed humanintelligence in a variety of computational tasks. However, AI systems, likehumans, make mistakes, have blind spots, hallucinate, and struggle togeneralize to new situations. This work explores whether AI can benefit fromcreative decision-making mechanisms when pushed to the limits of itscomputational rationality. In particular, we investigate whether a team ofdiverse AI systems can outperform a single AI in challenging tasks bygenerating more ideas as a group and then selecting the best ones. We studythis question in the game of chess, the so-called drosophila of AI. We build onAlphaZero (AZ) and extend it to represent a league of agents via alatent-conditioned architecture, which we call AZ_db. We train AZ_db togenerate a wider range of ideas using behavioral diversity techniques andselect the most promising ones with sub-additive planning. Our experimentssuggest that AZ_db plays chess in diverse ways, solves more puzzles as a groupand outperforms a more homogeneous team. Notably, AZ_db solves twice as manychallenging puzzles as AZ, including the challenging Penrose positions. Whenplaying chess from different openings, we notice that players in AZ_dbspecialize in different openings, and that selecting a player for each openingusing sub-additive planning results in a 50 Elo improvement over AZ. Ourfindings suggest that diversity bonuses emerge in teams of AI agents, just asthey do in teams of humans and that diversity is a valuable asset in solvingcomputationally hard problems.</description><author>Tom Zahavy, Vivek Veeriah, Shaobo Hou, Kevin Waugh, Matthew Lai, Edouard Leurent, Nenad Tomasev, Lisa Schut, Demis Hassabis, Satinder Singh</author><pubDate>Wed, 31 Jul 2024 13:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09175v3</guid></item><item><title>Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism</title><link>http://arxiv.org/abs/2407.21611v1</link><description>The task of partially spoofed audio localization aims to accurately determineaudio authenticity at a frame level. Although some works have achievedencouraging results, utilizing boundary information within a single modelremains an unexplored research topic. In this work, we propose a novel methodcalled Boundary-aware Attention Mechanism (BAM). Specifically, it consists oftwo core modules: Boundary Enhancement and Boundary Frame-wise Attention. Theformer assembles the intra-frame and inter-frame information to extractdiscriminative boundary features that are subsequently used for boundaryposition detection and authenticity decision, while the latter leveragesboundary prediction results to explicitly control the feature interactionbetween frames, which achieves effective discrimination between real and fakeframes. Experimental results on PartialSpoof database demonstrate our proposedmethod achieves the best performance. The code is available athttps://github.com/media-sec-lab/BAM.</description><author>Jiafeng Zhong, Bin Li, Jiangyan Yi</author><pubDate>Wed, 31 Jul 2024 13:49:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21611v1</guid></item><item><title>Ironing the Graphs: Toward a Correct Geometric Analysis of Large-Scale Graphs</title><link>http://arxiv.org/abs/2407.21609v1</link><description>Graph embedding approaches attempt to project graphs into geometric entities,i.e, manifolds. The idea is that the geometric properties of the projectedmanifolds are helpful in the inference of graph properties. However, if thechoice of the embedding manifold is incorrectly performed, it can lead toincorrect geometric inference. In this paper, we argue that the classicalembedding techniques cannot lead to correct geometric interpretation as theymiss the curvature at each point, of manifold. We advocate that for doingcorrect geometric interpretation the embedding of graph should be done overregular constant curvature manifolds. To this end, we present an embeddingapproach, the discrete Ricci flow graph embedding (dRfge) based on the discreteRicci flow that adapts the distance between nodes in a graph so that the graphcan be embedded onto a constant curvature manifold that is homogeneous andisotropic, i.e., all directions are equivalent and distances comparable,resulting in correct geometric interpretations. A major contribution of thispaper is that for the first time, we prove the convergence of discrete Ricciflow to a constant curvature and stable distance metrics over the edges. Adrawback of using the discrete Ricci flow is the high computational complexitythat prevented its usage in large-scale graph analysis. Another contribution ofthis paper is a new algorithmic solution that makes it feasible to calculatethe Ricci flow for graphs of up to 50k nodes, and beyond. The intuitions behindthe discrete Ricci flow make it possible to obtain new insights into thestructure of large-scale graphs. We demonstrate this through a case study onanalyzing the internet connectivity structure between countries at the BGPlevel.</description><author>Saloua Naama, Kavé Salamatian, Francesco Bronzino</author><pubDate>Wed, 31 Jul 2024 13:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21609v1</guid></item><item><title>U-Net-based Lung Thickness Map for Pixel-level Lung Volume Estimation of Chest X-rays</title><link>http://arxiv.org/abs/2110.12509v5</link><description>Purpose: We aimed to estimate the total lung volume (TLV) from real andsynthetic frontal X-ray radiographs on a pixel level using lung thickness mapsgenerated by a U-Net. Methods: 5,959 thorax X-ray computed tomography (CT) scans were retrievedfrom two publicly available datasets of the lung nodule analysis 2016 (n=656)and the RSNA pulmonary embolism detection challenge 2020 (n=5,303).Additionally, thorax CT scans from 72 subjects (33 healthy: 20 men, mean age[range] = 62.4 [34, 80]; 39 suffering from chronic obstructive pulmonarydisease: 25 men, mean age [range] = 69.0 [47, 91]) were retrospectivelyselected (10.2018-12.2019) from our in-house dataset such that for eachsubject, a frontal chest X-ray radiograph no older than seven days wasavailable. All CT scans and their corresponding lung segmentation were forwardprojected using a simulated X-ray spectrum to generate synthetic radiographsand lung thickness maps, respectively. A U-Net model was trained and tested onsynthetic radiographs from the public datasets to predict lung thickness mapsand consequently estimate TLV. Model performance was further assessed byevaluating the TLV estimations for the in-house synthetic and real radiographpairs using Pearson correlation coefficient (r) and significance testing. Results: Strong correlations were measured between the predicted andCT-derived ground truth TLV values for test data from synthetic($n_{Public}$=1,191, r=0.987, P &lt; 0.001; $n_{In-house}$=72, r=0.973, P &lt; 0.001)and real radiographs (n=72, r=0.908, P &lt; 0.001). Conclusion: TLV from U-Net-generated pixel-level lung thickness maps weresuccessfully estimated for synthetic and real radiographs.</description><author>Tina Dorosti, Manuel Schultheiss, Philipp Schmette, Jule Heuchert, Johannes Thalhammer, Florian Schaff, Thorsten Sellerer, Rafael Schick, Kirsten Taphorn, Korbinian Mechlem, Lorenz Birnbacher, Franz Pfeiffer, Daniela Pfeiffer</author><pubDate>Wed, 31 Jul 2024 13:41:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.12509v5</guid></item><item><title>A comparison between black-, grey- and white-box modeling for the bidirectional Raman amplifier optimization</title><link>http://arxiv.org/abs/2310.05954v2</link><description>Designing and optimizing optical amplifiers to maximize system performance isbecoming increasingly important as optical communication systems strive toincrease throughput. Offline optimization of optical amplifiers relies onmodels ranging from white-box models deeply rooted in physics to black-boxdata-driven and physics-agnostic models. Here, we compare the capabilities ofwhite-, grey- and black-box models on the challenging test case of optimizing abidirectional distributed Raman amplifier to achieve a targetfrequency-distance signal power profile. We show that any of the studiedmethods can achieve similar frequency and distance flatness of between 1 and3.6 dB (depending on the definition of flatness) over the C-band in an 80-kmspan. Then, we discuss the models' applicability, advantages, and drawbacksbased on the target application scenario, in particular in terms offlexibility, optimization speed, and access to training data.</description><author>Metodi P. Yankov, Mehran Soltani, Andrea Carena, Darko Zibar, Francesco Da Ros</author><pubDate>Wed, 31 Jul 2024 13:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05954v2</guid></item><item><title>MicroMIL: Graph-based Contextual Multiple Instance Learning for Patient Diagnosis Using Microscopy Images</title><link>http://arxiv.org/abs/2407.21604v1</link><description>Current histopathology research has primarily focused on using whole-slideimages (WSIs) produced by scanners with weakly-supervised multiple instancelearning (MIL). However, WSIs are costly, memory-intensive, and requireextensive analysis time. As an alternative, microscopy-based analysis offerscost and memory efficiency, though microscopy images face issues with unknownabsolute positions and redundant images due to multiple captures from thesubjective perspectives of pathologists. To this end, we introduce MicroMIL, aweakly-supervised MIL framework specifically built to address these challengesby dynamically clustering images using deep cluster embedding (DCE) and GumbelSoftmax for representative image extraction. Graph edges are then constructedfrom the upper triangular similarity matrix, with nodes connected to their mostsimilar neighbors, and a graph neural network (GNN) is utilized to capturelocal and diverse areas of contextual information. Unlike existing graph-basedMIL methods designed for WSIs that require absolute positions, MicroMILefficiently handles the graph edges without this need. Extensive evaluations onreal-world colon cancer (Seegene) and public BreakHis datasets demonstrate thatMicroMIL outperforms state-of-the-art (SOTA) methods, offering a robust andefficient solution for patient diagnosis using microscopy images. The code isavailable at https://anonymous.4open.science/r/MicroMIL-6C7C</description><author>JongWoo Kim, Bryan Wong, YoungSin Ko, MunYong Yi</author><pubDate>Wed, 31 Jul 2024 13:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21604v1</guid></item><item><title>Higher order quantum reservoir computing for non-intrusive reduced-order models</title><link>http://arxiv.org/abs/2407.21602v1</link><description>Forecasting dynamical systems is of importance to numerous real-worldapplications. When possible, dynamical systems forecasts are constructed basedon first-principles-based models such as through the use of differentialequations. When these equations are unknown, non-intrusive techniques must beutilized to build predictive models from data alone. Machine learning (ML)methods have recently been used for such tasks. Moreover, ML methods providethe added advantage of significant reductions in time-to-solution forpredictions in contrast with first-principle based models. However, manystate-of-the-art ML-based methods for forecasting rely on neural networks,which may be expensive to train and necessitate requirements for large amountsof memory. In this work, we propose a quantum mechanics inspired ML modelingstrategy for learning nonlinear dynamical systems that provides data-drivenforecasts for complex dynamical systems with reduced training time and memorycosts. This approach, denoted the quantum reservoir computing technique (QRC),is a hybrid quantum-classical framework employing an ensemble of interconnectedsmall quantum systems via classical linear feedback connections. By mapping thedynamical state to a suitable quantum representation amenable to unitaryoperations, QRC is able to predict complex nonlinear dynamical systems in astable and accurate manner. We demonstrate the efficacy of this frameworkthrough benchmark forecasts of the NOAA Optimal Interpolation Sea SurfaceTemperature dataset and compare the performance of QRC to other ML methods.</description><author>Vinamr Jain, Romit Maulik</author><pubDate>Wed, 31 Jul 2024 13:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21602v1</guid></item><item><title>Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors</title><link>http://arxiv.org/abs/2407.21600v1</link><description>Simultaneous multislice (SMS) imaging is a powerful technique foraccelerating magnetic resonance imaging (MRI) acquisitions. However, SMSreconstruction remains challenging due to the complex signal interactionsbetween and within the excited slices. This study presents a robust SMS MRIreconstruction method using deep generative priors. Starting from Gaussiannoise, we leverage denoising diffusion probabilistic models (DDPM) to graduallyrecover the individual slices through reverse diffusion iterations whileimposing data consistency from the measured k-space under readout concatenationframework. The posterior sampling procedure is designed such that the DDPMtraining can be performed on single-slice images without special adjustmentsfor SMS tasks. Additionally, our method integrates a low-frequency enhancement(LFE) module to address a practical issue that SMS-accelerated fast spin echo(FSE) and echo-planar imaging (EPI) sequences cannot easily embedautocalibration signals. Extensive experiments demonstrate that our approachconsistently outperforms existing methods and generalizes well to unseendatasets. The code is available at https://github.com/Solor-pikachu/ROGER afterthe review process.</description><author>Shoujin Huang, Guanxiong Luo, Yuwan Wang, Kexin Yang, Lingyan Zhang, Jingzhe Liu, Hua Guo, Min Wang, Mengye Lyu</author><pubDate>Wed, 31 Jul 2024 13:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21600v1</guid></item><item><title>Evaluating SAM2's Role in Camouflaged Object Detection: From SAM to SAM2</title><link>http://arxiv.org/abs/2407.21596v1</link><description>The Segment Anything Model (SAM), introduced by Meta AI Research as a genericobject segmentation model, quickly garnered widespread attention andsignificantly influenced the academic community. To extend its application tovideo, Meta further develops Segment Anything Model 2 (SAM2), a unified modelcapable of both video and image segmentation. SAM2 shows notable improvementsover its predecessor in terms of applicable domains, promptable segmentationaccuracy, and running speed. However, this report reveals a decline in SAM2'sability to perceive different objects in images without prompts in its automode, compared to SAM. Specifically, we employ the challenging task ofcamouflaged object detection to assess this performance decrease, hoping toinspire further exploration of the SAM model family by researchers. The resultsof this paper are provided in \url{https://github.com/luckybird1994/SAMCOD}.</description><author>Lv Tang, Bo Li</author><pubDate>Wed, 31 Jul 2024 13:32:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21596v1</guid></item><item><title>Naeural AI OS -- Decentralized ubiquitous computing MLOps execution engine</title><link>http://arxiv.org/abs/2306.08708v3</link><description>Over the past few years, ubiquitous, or pervasive computing has gainedpopularity as the primary approach for a wide range of applications, includingenterprise-grade systems, consumer applications, and gaming systems. Ubiquitouscomputing refers to the integration of computing technologies into everydayobjects and environments, creating a network of interconnected devices that cancommunicate with each other and with humans. By using ubiquitous computingtechnologies, communities can become more connected and efficient, with membersable to communicate and collaborate more easily. This enabledinterconnectedness and collaboration can lead to a more successful andsustainable community. The spread of ubiquitous computing, however, hasemphasized the importance of automated learning and smart applications ingeneral. Even though there have been significant strides in ArtificialIntelligence and Deep Learning, large scale adoption has been hesitant due tomounting pressure on expensive and highly complex cloud numerical-computeinfrastructures. Adopting, and even developing, practical machine learningsystems can come with prohibitive costs, not only in terms of complexinfrastructures but also of solid expertise in Data Science and MachineLearning. In this paper we present an innovative approach for low-codedevelopment and deployment of end-to-end AI cooperative application pipelines.We address infrastructure allocation, costs, and secure job distribution in afully decentralized global cooperative community based on tokenized economics.</description><author>Beatrice Milik, Stefan Saraev, Cristian Bleotiu, Radu Lupaescu, Bogdan Hobeanu, Andrei Ionut Damian</author><pubDate>Wed, 31 Jul 2024 13:31:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08708v3</guid></item><item><title>Measuring What Matters: Intrinsic Distance Preservation as a Robust Metric for Embedding Quality</title><link>http://arxiv.org/abs/2407.21590v1</link><description>Unsupervised embeddings are fundamental to numerous machine learningapplications, yet their evaluation remains a challenging task. Traditionalassessment methods often rely on extrinsic variables, such as performance indownstream tasks, which can introduce confounding factors and mask the truequality of embeddings. This paper introduces the Intrinsic DistancePreservation Evaluation (IDPE) method, a novel approach for assessing embeddingquality based on the preservation of Mahalanobis distances between data pointsin the original and embedded spaces. We demonstrate the limitations ofextrinsic evaluation methods through a simple example, highlighting how theycan lead to misleading conclusions about embedding quality. IDPE addressesthese issues by providing a task-independent measure of how well embeddingspreserve the intrinsic structure of the original data. Our method leveragesefficient similarity search techniques to make it applicable to large-scaledatasets. We compare IDPE with established intrinsic metrics liketrustworthiness and continuity, as well as extrinsic metrics such as AverageRank and Mean Reciprocal Rank. Our results show that IDPE offers a morecomprehensive and reliable assessment of embedding quality across variousscenarios. We evaluate PCA and t-SNE embeddings using IDPE, revealing insightsinto their performance that are not captured by traditional metrics. This workcontributes to the field by providing a robust, efficient, and interpretablemethod for embedding evaluation. IDPE's focus on intrinsic properties offers avaluable tool for researchers and practitioners seeking to develop and assesshigh-quality embeddings for diverse machine learning applications.</description><author>Steven N. Hart, Thomas E. Tavolara</author><pubDate>Wed, 31 Jul 2024 13:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21590v1</guid></item></channel></rss>