<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 26 Dec 2023 06:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MACS: Mass Conditioned 3D Hand and Object Motion Synthesis</title><link>http://arxiv.org/abs/2312.14929v1</link><description>The physical properties of an object, such as mass, significantly affect howwe manipulate it with our hands. Surprisingly, this aspect has so far beenneglected in prior work on 3D motion synthesis. To improve the naturalness ofthe synthesized 3D hand object motions, this work proposes MACS the first MAssConditioned 3D hand and object motion Synthesis approach. Our approach is basedon cascaded diffusion models and generates interactions that plausibly adjustbased on the object mass and interaction type. MACS also accepts a manuallydrawn 3D object trajectory as input and synthesizes the natural 3D hand motionsconditioned by the object mass. This flexibility enables MACS to be used forvarious downstream applications, such as generating synthetic training data forML tasks, fast animation of hands for graphics workflows, and generatingcharacter interactions for computer games. We show experimentally that asmall-scale dataset is sufficient for MACS to reasonably generalize acrossinterpolated and extrapolated object masses unseen during the training.Furthermore, MACS shows moderate generalization to unseen objects, thanks tothe mass-conditioned contact labels generated by our surface contact synthesismodel ConNet. Our comprehensive user study confirms that the synthesized 3Dhand-object interactions are highly plausible and realistic.</description><author>Soshi Shimada, Franziska Mueller, Jan Bednarik, Bardia Doosti, Bernd Bickel, Danhang Tang, Vladislav Golyanik, Jonathan Taylor, Christian Theobalt, Thabo Beeler</author><pubDate>Fri, 22 Dec 2023 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14929v1</guid></item><item><title>A Survey of Reinforcement Learning from Human Feedback</title><link>http://arxiv.org/abs/2312.14925v1</link><description>Reinforcement learning from human feedback (RLHF) is a variant ofreinforcement learning (RL) that learns from human feedback instead of relyingon an engineered reward function. Building on prior work on the related settingof preference-based reinforcement learning (PbRL), it stands at theintersection of artificial intelligence and human-computer interaction. Thispositioning offers a promising avenue to enhance the performance andadaptability of intelligent systems while also improving the alignment of theirobjectives with human values. The training of Large Language Models (LLMs) hasimpressively demonstrated this potential in recent years, where RLHF played adecisive role in targeting the model's capabilities toward human objectives.This article provides a comprehensive overview of the fundamentals of RLHF,exploring the intricate dynamics between machine agents and human input. Whilerecent focus has been on RLHF for LLMs, our survey adopts a broaderperspective, examining the diverse applications and wide-ranging impact of thetechnique. We delve into the core principles that underpin RLHF, shedding lighton the symbiotic relationship between algorithms and human feedback, anddiscuss the main research trends in the field. By synthesizing the currentlandscape of RLHF research, this article aims to provide researchers as well aspractitioners with a comprehensive understanding of this rapidly growing fieldof research.</description><author>Timo Kaufmann, Paul Weng, Viktor Bengs, Eyke Hüllermeier</author><pubDate>Fri, 22 Dec 2023 18:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14925v1</guid></item><item><title>Training Convolutional Neural Networks with the Forward-Forward algorithm</title><link>http://arxiv.org/abs/2312.14924v1</link><description>The recent successes in analyzing images with deep neural networks are almostexclusively achieved with Convolutional Neural Networks (CNNs). The training ofthese CNNs, and in fact of all deep neural network architectures, uses thebackpropagation algorithm where the output of the network is compared with thedesired result and the difference is then used to tune the weights of thenetwork towards the desired outcome. In a 2022 preprint, Geoffrey Hintonsuggested an alternative way of training which passes the desired resultstogether with the images at the input of the network. This so called ForwardForward (FF) algorithm has up to now only been used in fully connectednetworks. In this paper, we show how the FF paradigm can be extended to CNNs.Our FF-trained CNN, featuring a novel spatially-extended labeling technique,achieves a classification accuracy of 99.0% on the MNIST hand-written digitsdataset. We show how different hyperparameters affect the performance of theproposed algorithm and compare the results with CNN trained with the standardbackpropagation approach. Furthermore, we use Class Activation Maps toinvestigate which type of features are learnt by the FF algorithm.</description><author>Riccardo Scodellaro, Ajinkya Kulkarni, Frauke Alves, Matthias Schröter</author><pubDate>Fri, 22 Dec 2023 18:56:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14924v1</guid></item><item><title>Fast-NTK: Parameter-Efficient Unlearning for Large-Scale Models</title><link>http://arxiv.org/abs/2312.14923v1</link><description>The rapid growth of machine learning has spurred legislative initiatives suchas ``the Right to be Forgotten,'' allowing users to request data removal. Inresponse, ``machine unlearning'' proposes the selective removal of unwanteddata without the need for retraining from scratch. While theNeural-Tangent-Kernel-based (NTK-based) unlearning method excels inperformance, it suffers from significant computational complexity, especiallyfor large-scale models and datasets. Our work introduces ``Fast-NTK,'' a novelNTK-based unlearning algorithm that significantly reduces the computationalcomplexity by incorporating parameter-efficient fine-tuning methods, such asfine-tuning batch normalization layers in a CNN or visual prompts in a visiontransformer. Our experimental results demonstrate scalability to much largerneural networks and datasets (e.g., 88M parameters; 5k images), surpassing thelimitations of previous full-model NTK-based approaches designed for smallercases (e.g., 8M parameters; 500 images). Notably, our approach maintains aperformance comparable to the traditional method of retraining on the retainset alone. Fast-NTK can thus enable for practical and scalable NTK-basedunlearning in deep neural networks.</description><author>Guihong Li, Hsiang Hsu, Chun-Fu Chen, Radu Marculescu</author><pubDate>Fri, 22 Dec 2023 18:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14923v1</guid></item><item><title>Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks</title><link>http://arxiv.org/abs/2312.14922v1</link><description>Neural networks excel at discovering statistical patterns in high-dimensionaldata sets. In practice, higher-order cumulants, which quantify the non-Gaussiancorrelations between three or more variables, are particularly important forthe performance of neural networks. But how efficient are neural networks atextracting features from higher-order cumulants? We study this question in thespiked cumulant model, where the statistician needs to recover a privilegeddirection or "spike" from the order-$p\ge 4$ cumulants of~$d$-dimensionalinputs. We first characterise the fundamental statistical and computationallimits of recovering the spike by analysing the number of samples~$n$ requiredto strongly distinguish between inputs from the spiked cumulant model andisotropic Gaussian inputs. We find that statistical distinguishability requires$n\gtrsim d$ samples, while distinguishing the two distributions in polynomialtime requires $n \gtrsim d^2$ samples for a wide class of algorithms, i.e.those covered by the low-degree conjecture. These results suggest the existenceof a wide statistical-to-computational gap in this problem. Numericalexperiments show that neural networks learn to distinguish the twodistributions with quadratic sample complexity, while "lazy" methods likerandom features are not better than random guessing in this regime. Our resultsshow that neural networks extract information from higher-order correlations inthe spiked cumulant model efficiently, and reveal a large gap in the amount ofdata required by neural networks and random features to learn from higher-ordercumulants.</description><author>Eszter Székely, Lorenzo Bardone, Federica Gerace, Sebastian Goldt</author><pubDate>Fri, 22 Dec 2023 18:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14922v1</guid></item><item><title>A Novel Sampled Clustering Algorithm for Rice Phenotypic Data</title><link>http://arxiv.org/abs/2312.14920v1</link><description>Phenotypic (or Physical) characteristics of plant species are commonly usedto perform clustering. In one of our recent works (Shastri et al. (2021)), weused a probabilistically sampled (using pivotal sampling) and spectrallyclustered algorithm to group soybean species. These techniques were used toobtain highly accurate clusterings at a reduced cost. In this work, we extendthe earlier algorithm to cluster rice species. We improve the base algorithm inthree ways. First, we propose a new function to build the similarity matrix inSpectral Clustering. Commonly, a natural exponential function is used for thispurpose. Based upon the spectral graph theory and the involved Cheeger'sinequality, we propose the use a base "a" exponential function instead. Thisgives a similarity matrix spectrum favorable for clustering, which we supportvia an eigenvalue analysis. Second, the function used to build the similarity matrix in SpectralClustering was earlier scaled with a fixed factor (called global scaling).Based upon the idea of Zelnik-Manor and Perona (2004), we now use a factor thatvaries with matrix elements (called local scaling) and works better. Third, tocompute the inclusion probability of a specie in the pivotal samplingalgorithm, we had earlier used the notion of deviation that captured how farspecie's characteristic values were from their respective base values (computedover all species). A maximum function was used before to find the base values.We now use a median function, which is more intuitive. We support this choiceusing a statistical analysis. With experiments on 1865 rice species, wedemonstrate that in terms of silhouette values, our new Sampled SpectralClustering is 61% better than Hierarchical Clustering (currently prevalent).Also, our new algorithm is significantly faster than Hierarchical Clusteringdue to the involved sampling.</description><author>Mithun Singh, Kapil Ahuja, Milind B. Ratnaparkhe</author><pubDate>Fri, 22 Dec 2023 18:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14920v1</guid></item><item><title>Lift-Attend-Splat: Bird's-eye-view camera-lidar fusion using transformers</title><link>http://arxiv.org/abs/2312.14919v1</link><description>Combining complementary sensor modalities is crucial to providing robustperception for safety-critical robotics applications such as autonomous driving(AD). Recent state-of-the-art camera-lidar fusion methods for AD rely onmonocular depth estimation which is a notoriously difficult task compared tousing depth information from the lidar directly. Here, we find that thisapproach does not leverage depth as expected and show that naively improvingdepth estimation does not lead to improvements in object detection performanceand that, strikingly, removing depth estimation altogether does not degradeobject detection performance. This suggests that relying on monocular depthcould be an unnecessary architectural bottleneck during camera-lidar fusion. Inthis work, we introduce a novel fusion method that bypasses monocular depthestimation altogether and instead selects and fuses camera and lidar featuresin a bird's-eye-view grid using a simple attention mechanism. We show that ourmodel can modulate its use of camera features based on the availability oflidar features and that it yields better 3D object detection on the nuScenesdataset than baselines relying on monocular depth estimation.</description><author>James Gunn, Zygmunt Lenyk, Anuj Sharma, Andrea Donati, Alexandru Buburuzan, John Redford, Romain Mueller</author><pubDate>Fri, 22 Dec 2023 18:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14919v1</guid></item><item><title>PoseGen: Learning to Generate 3D Human Pose Dataset with NeRF</title><link>http://arxiv.org/abs/2312.14915v1</link><description>This paper proposes an end-to-end framework for generating 3D human posedatasets using Neural Radiance Fields (NeRF). Public datasets generally havelimited diversity in terms of human poses and camera viewpoints, largely due tothe resource-intensive nature of collecting 3D human pose data. As a result,pose estimators trained on public datasets significantly underperform whenapplied to unseen out-of-distribution samples. Previous works proposedaugmenting public datasets by generating 2D-3D pose pairs or rendering a largeamount of random data. Such approaches either overlook image rendering orresult in suboptimal datasets for pre-trained models. Here we propose PoseGen,which learns to generate a dataset (human 3D poses and images) with a feedbackloss from a given pre-trained pose estimator. In contrast to prior art, ourgenerated data is optimized to improve the robustness of the pre-trained model.The objective of PoseGen is to learn a distribution of data that maximizes theprediction error of a given pre-trained model. As the learned data distributioncontains OOD samples of the pre-trained model, sampling data from such adistribution for further fine-tuning a pre-trained model improves thegeneralizability of the model. This is the first work that proposes NeRFs for3D human data generation. NeRFs are data-driven and do not require 3D scans ofhumans. Therefore, using NeRF for data generation is a new direction forconvenient user-specific data generation. Our extensive experiments show thatthe proposed PoseGen improves two baseline models (SPIN and HybrIK) on fourdatasets with an average 6% relative improvement.</description><author>Mohsen Gholami, Rabab Ward, Z. Jane Wang</author><pubDate>Fri, 22 Dec 2023 18:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14915v1</guid></item><item><title>Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</title><link>http://arxiv.org/abs/2312.06585v3</link><description>Fine-tuning language models~(LMs) on human-generated data remains a prevalentpractice. However, the performance of such models is often limited by thequantity and diversity of high-quality human data. In this paper, we explorewhether we can go beyond human data on tasks where we have access to scalarfeedback, for example, on math problems where one can verify correctness. To doso, we investigate a simple self-training method based onexpectation-maximization, which we call ReST$^{EM}$, where we (1) generatesamples from the model and filter them using binary feedback, (2) fine-tune themodel on these samples, and (3) repeat this process a few times. Testing onadvanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we findthat ReST$^{EM}$ scales favorably with model size and significantly surpassesfine-tuning only on human data. Overall, our findings suggest self-trainingwith feedback can substantially reduce dependence on human-generated data.</description><author>Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, Noah Fiedel</author><pubDate>Fri, 22 Dec 2023 18:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06585v3</guid></item><item><title>Condition-Invariant Semantic Segmentation</title><link>http://arxiv.org/abs/2305.17349v2</link><description>Adaptation of semantic segmentation networks to different visual conditionsis vital for robust perception in autonomous cars and robots. However, previouswork has shown that most feature-level adaptation methods, which employadversarial training and are validated on synthetic-to-real adaptation, providemarginal gains in condition-level adaptation, being outperformed by simplepixel-level adaptation via stylization. Motivated by these findings, we proposeto leverage stylization in performing feature-level adaptation by aligning theinternal network features extracted by the encoder of the network from theoriginal and the stylized view of each input image with a novel featureinvariance loss. In this way, we encourage the encoder to extract features thatare already invariant to the style of the input, allowing the decoder to focuson parsing these features and not on further abstracting from the specificstyle of the input. We implement our method, named Condition-Invariant SemanticSegmentation (CISS), on the current state-of-the-art domain adaptationarchitecture and achieve outstanding results on condition-level adaptation. Inparticular, CISS sets the new state of the art in the populardaytime-to-nighttime Cityscapes$\to$Dark Zurich benchmark. Furthermore, ourmethod achieves the second-best performance on the normal-to-adverseCityscapes$\to$ACDC benchmark. CISS is shown to generalize well to domainsunseen during training, such as BDD100K-night. Code is publicly available athttps://github.com/SysCV/CISS .</description><author>Christos Sakaridis, David Bruggemann, Fisher Yu, Luc Van Gool</author><pubDate>Fri, 22 Dec 2023 18:26:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17349v2</guid></item><item><title>Strong anti-Hebbian plasticity alters the convexity of network attractor landscapes</title><link>http://arxiv.org/abs/2312.14896v1</link><description>In this paper, we study recurrent neural networks in the presence of pairwiselearning rules. We are specifically interested in how the attractor landscapesof such networks become altered as a function of the strength and nature(Hebbian vs. anti-Hebbian) of learning, which may have a bearing on the abilityof such rules to mediate large-scale optimization problems. Through formalanalysis, we show that a transition from Hebbian to anti-Hebbian learningbrings about a pitchfork bifurcation that destroys convexity in the networkattractor landscape. In larger-scale settings, this implies that anti-Hebbianplasticity will bring about multiple stable equilibria, and such effects may beoutsized at interconnection or `choke' points. Furthermore, attractorlandscapes are more sensitive to slower learning rates than faster ones. Theseresults provide insight into the types of objective functions that can beencoded via different pairwise plasticity rules.</description><author>Lulu Gong, Xudong Chen, ShiNung Ching</author><pubDate>Fri, 22 Dec 2023 18:17:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14896v1</guid></item><item><title>FAST: Feature Aware Similarity Thresholding for Weak Unlearning in Black-Box Generative Models</title><link>http://arxiv.org/abs/2312.14895v1</link><description>The heightened emphasis on the regulation of deep generative models,propelled by escalating concerns pertaining to privacy and compliance withregulatory frameworks, underscores the imperative need for precise controlmechanisms over these models. This urgency is particularly underscored byinstances in which generative models generate outputs that encompassobjectionable, offensive, or potentially injurious content. In response,machine unlearning has emerged to selectively forget specific knowledge orremove the influence of undesirable data subsets from pre-trained models.However, modern machine unlearning approaches typically assume access to modelparameters and architectural details during unlearning, which is not alwaysfeasible. In multitude of downstream tasks, these models function as black-boxsystems, with inaccessible pre-trained parameters, architectures, and trainingdata. In such scenarios, the possibility of filtering undesired outputs becomesa practical alternative. The primary goal of this study is twofold: first, toelucidate the relationship between filtering and unlearning processes, andsecond, to formulate a methodology aimed at mitigating the display ofundesirable outputs generated from models characterized as black-box systems.Theoretical analysis in this study demonstrates that, in the context ofblack-box models, filtering can be seen as a form of weak unlearning. Ourproposed \textbf{\textit{Feature Aware Similarity Thresholding(FAST)}} methodeffectively suppresses undesired outputs by systematically encoding therepresentation of unwanted features in the latent space.</description><author>Subhodip Panda, Prathosh AP</author><pubDate>Fri, 22 Dec 2023 18:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14895v1</guid></item><item><title>DRStageNet: Deep Learning for Diabetic Retinopathy Staging from Fundus Images</title><link>http://arxiv.org/abs/2312.14891v1</link><description>Diabetic retinopathy (DR) is a prevalent complication of diabetes associatedwith a significant risk of vision loss. Timely identification is critical tocurb vision impairment. Algorithms for DR staging from digital fundus images(DFIs) have been recently proposed. However, models often fail to generalizedue to distribution shifts between the source domain on which the model wastrained and the target domain where it is deployed. A common and particularlychallenging shift is often encountered when the source- and target-domainsupports do not fully overlap. In this research, we introduce DRStageNet, adeep learning model designed to mitigate this challenge. We used seven publiclyavailable datasets, comprising a total of 93,534 DFIs that cover a variety ofpatient demographics, ethnicities, geographic origins and comorbidities. Wefine-tune DINOv2, a pretrained model of self-supervised vision transformer, andimplement a multi-source domain fine-tuning strategy to enhance generalizationperformance. We benchmark and demonstrate the superiority of our method to twostate-of-the-art benchmarks, including a recently published foundation model.We adapted the grad-rollout method to our regression task in order to providehigh-resolution explainability heatmaps. The error analysis showed that 59\% ofthe main errors had incorrect reference labels. DRStageNet is accessible at URL[upon acceptance of the manuscript].</description><author>Yevgeniy Men, Jonathan Fhima, Leo Anthony Celi, Lucas Zago Ribeiro, Luis Filipe Nakayama, Joachim A. Behar</author><pubDate>Fri, 22 Dec 2023 18:09:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14891v1</guid></item><item><title>NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes</title><link>http://arxiv.org/abs/2312.14890v1</link><description>Complex reasoning ability is one of the most important features of currentLLMs, which has also been leveraged to play an integral role in complexdecision-making tasks. Therefore, the investigation into the reasoningcapabilities of Large Language Models (LLMs) is critical: numerous benchmarkshave been established to assess the reasoning abilities of LLMs. However,current benchmarks are inadequate in offering a rigorous evaluation of the fullextent of reasoning abilities that LLMs are capable of achieving. They are alsoprone to the risk of overfitting, as these benchmarks, being publiclyaccessible and static, allow models to potentially tailor their responses tospecific benchmark metrics, thereby inflating their performance. Addressingthese limitations, our research introduces a new benchmark, named NPHardEval.This benchmark is designed to evaluate the reasoning abilities of LLMs across abroad spectrum of 900 algorithmic questions, extending up to the NP-Hardcomplexity class. These questions are meticulously chosen to represent a widerange of complexity class below the NP-hard complexity class, offering arigorous measure of the reasoning ability of LLMs. Through this study, we shedlight on the current state of reasoning in LLMs, providing an objective andrigorous perspective through the comparison of LLMs' performance across complexclasses. Moreover, this benchmark is designed with a dynamic update mechanism,where the datapoints are refreshed on a monthly basis. Such regular updatesplay a crucial role in mitigating the risk of LLMs overfitting to thebenchmark, promoting a more accurate and reliable assessment of their reasoningcapabilities. The benchmark dataset and code of NPHardEval are available athttps://github.com/casmlab/NPHardEval.</description><author>Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, Yongfeng Zhang, Libby Hemphill</author><pubDate>Fri, 22 Dec 2023 18:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14890v1</guid></item><item><title>UnIVAL: Unified Model for Image, Video, Audio and Language Tasks</title><link>http://arxiv.org/abs/2307.16184v2</link><description>Large Language Models (LLMs) have made the ambitious quest for generalistagents significantly far from being a fantasy. A key hurdle for building suchgeneral models is the diversity and heterogeneity of tasks and modalities. Apromising solution is unification, allowing the support of a myriad of tasksand modalities within one unified framework. While few large models (e.g.,Flamingo (Alayrac et al., 2022), trained on massive datasets, can support morethan two modalities, current small to mid-scale unified models are stilllimited to 2 modalities, usually image-text or video-text. The question that weask is: is it possible to build efficiently a unified model that can supportall modalities? To answer this, we propose UnIVAL, a step further towards thisambitious goal. Without relying on fancy datasets sizes or models with billionsof parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalitiesand unifies text, images, video, and audio into a single model. Our model isefficiently pretrained on many tasks, based on task balancing and multimodalcurriculum learning. UnIVAL shows competitive performance to existingstate-of-the-art approaches, across image and video-text tasks. The featurerepresentations learned from image and video-text modalities, allows the modelto achieve competitive performance when finetuned on audio-text tasks, despitenot being pretrained on audio. Thanks to the unified model, we propose a novelstudy on multimodal model merging via weight interpolation of models trained ondifferent multimodal tasks, showing their benefits in particular forout-of-distribution generalization. Finally, we motivate unification by showingthe synergy between tasks. The model weights and code are released here:https://github.com/mshukor/UnIVAL.</description><author>Mustafa Shukor, Corentin Dancette, Alexandre Rame, Matthieu Cord</author><pubDate>Fri, 22 Dec 2023 18:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16184v2</guid></item><item><title>On rate-optimal classification from non-private and from private data</title><link>http://arxiv.org/abs/2312.14889v1</link><description>In this paper we revisit the classical problem of classification, but imposeprivacy constraints. Under such constraints, the raw data$(X_1,Y_1),\ldots,(X_n,Y_n)$ cannot be directly observed, and all classifiersare functions of the randomised outcome of a suitable local differentialprivacy mechanism. The statistician is free to choose the form of this privacymechanism, and here we add Laplace distributed noise to a discretisation of thelocation of each feature vector $X_i$ and to its label $Y_i$. Theclassification rule is the privatized version of the well-studied partitioningclassification rule. In addition to the standard Lipschitz and marginconditions, a novel characteristic is introduced, by which the exact rate ofconvergence of the classification error probability is calculated, both fornon-private and private data.</description><author>Balázs Csanád Csáji, László Györfi, Ambrus Tamás</author><pubDate>Fri, 22 Dec 2023 18:07:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14889v1</guid></item><item><title>Sample Path Regularity of Gaussian Processes from the Covariance Kernel</title><link>http://arxiv.org/abs/2312.14886v1</link><description>Gaussian processes (GPs) are the most common formalism for definingprobability distributions over spaces of functions. While applications of GPsare myriad, a comprehensive understanding of GP sample paths, i.e. the functionspaces over which they define a probability measure on, is lacking. Inpractice, GPs are not constructed through a probability measure, but insteadthrough a mean function and a covariance kernel. In this paper we providenecessary and sufficient conditions on the covariance kernel for the samplepaths of the corresponding GP to attain a given regularity. We use theframework of H\"older regularity as it grants us particularly straightforwardconditions, which simplify further in the cases of stationary and isotropicGPs. We then demonstrate that our results allow for novel and unusually tightcharacterisations of the sample path regularities of the GPs commonly used inmachine learning applications, such as the Mat\'ern GPs.</description><author>Nathaël Da Costa, Marvin Pförtner, Lancelot Da Costa, Philipp Hennig</author><pubDate>Fri, 22 Dec 2023 18:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14886v1</guid></item><item><title>Spear Phishing With Large Language Models</title><link>http://arxiv.org/abs/2305.06972v3</link><description>Recent progress in artificial intelligence (AI), particularly in the domainof large language models (LLMs), has resulted in powerful and versatiledual-use systems. This intelligence can be put towards a wide variety ofbeneficial tasks, yet it can also be used to cause harm. This study exploresone such harm by examining how LLMs can be used for spear phishing, a form ofcybercrime that involves manipulating targets into divulging sensitiveinformation. I first explore LLMs' ability to assist with the reconnaissanceand message generation stages of a spear phishing attack, where I find thatLLMs are capable of assisting with the email generation phase of a spearphishing attack. To explore how LLMs could potentially be harnessed to scalespear phishing campaigns, I then create unique spear phishing messages for over600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. Myfindings provide some evidence that these messages are not only realistic butalso cost-effective, with each email costing only a fraction of a cent togenerate. Next, I demonstrate how basic prompt engineering can circumventsafeguards installed in LLMs, highlighting the need for further research intorobust interventions that can help prevent models from being misused. Tofurther address these evolving risks, I explore two potential solutions:structured access schemes, such as application programming interfaces, andLLM-based defensive systems.</description><author>Julian Hazell</author><pubDate>Fri, 22 Dec 2023 18:01:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06972v3</guid></item><item><title>Sampling and estimation on manifolds using the Langevin diffusion</title><link>http://arxiv.org/abs/2312.14882v1</link><description>Error bounds are derived for sampling and estimation using a discretizationof an intrinsically defined Langevin diffusion with invariant measure$d\mu_\phi \propto e^{-\phi} \mathrm{dvol}_g $ on a compact Riemannianmanifold. Two estimators of linear functionals of $\mu_\phi $ based on thediscretized Markov process are considered: a time-averaging estimator based ona single trajectory and an ensemble-averaging estimator based on multipleindependent trajectories. Imposing no restrictions beyond a nominal level ofsmoothness on $\phi$, first-order error bounds, in discretization step size, onthe bias and variances of both estimators are derived. The order of errormatches the optimal rate in Euclidean and flat spaces, and leads to afirst-order bound on distance between the invariant measure $\mu_\phi$ and astationary measure of the discretized Markov process. Generality of the prooftechniques, which exploit links between two partial differential equations andthe semigroup of operators corresponding to the Langevin diffusion, rendersthem amenable for the study of a more general class of sampling algorithmsrelated to the Langevin diffusion. Conditions for extending analysis to thecase of non-compact manifolds are discussed. Numerical illustrations withdistributions, log-concave and otherwise, on the manifolds of positive andnegative curvature elucidate on the derived bounds and demonstrate practicalutility of the sampling algorithm.</description><author>Karthik Bharath, Alexander Lewis, Akash Sharma, Michael V Tretyakov</author><pubDate>Fri, 22 Dec 2023 18:01:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14882v1</guid></item><item><title>SutraNets: Sub-series Autoregressive Networks for Long-Sequence, Probabilistic Forecasting</title><link>http://arxiv.org/abs/2312.14880v1</link><description>We propose SutraNets, a novel method for neural probabilistic forecasting oflong-sequence time series. SutraNets use an autoregressive generative model tofactorize the likelihood of long sequences into products of conditionalprobabilities. When generating long sequences, most autoregressive approachessuffer from harmful error accumulation, as well as challenges in modelinglong-distance dependencies. SutraNets treat long, univariate prediction asmultivariate prediction over lower-frequency sub-series. Autoregressionproceeds across time and across sub-series in order to ensure coherentmultivariate (and, hence, high-frequency univariate) outputs. Since sub-seriescan be generated using fewer steps, SutraNets effectively reduce erroraccumulation and signal path distances. We find SutraNets to significantlyimprove forecasting accuracy over competitive alternatives on six real-worlddatasets, including when we vary the number of sub-series and scale up thedepth and width of the underlying sequence models.</description><author>Shane Bergsma, Timothy Zeyl, Lei Guo</author><pubDate>Fri, 22 Dec 2023 18:00:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14880v1</guid></item><item><title>Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning</title><link>http://arxiv.org/abs/2312.14878v1</link><description>A key method for creating Artificial Intelligence (AI) agents isReinforcement Learning (RL). However, constructing a standalone RL policy thatmaps perception to action directly encounters severe problems, chief among thembeing its lack of generality across multiple tasks and the need for a largeamount of training data. The leading cause is that it cannot effectivelyintegrate prior information into the perception-action cycle when devising thepolicy. Large language models (LLMs) emerged as a fundamental way toincorporate cross-domain knowledge into AI agents but lack crucial learning andadaptation toward specific decision problems. This paper presents a generalframework model for integrating and learning structured reasoning into AIagents' policies. Our methodology is motivated by the modularity found in thehuman brain. The framework utilises the construction of intrinsic and extrinsicfunctions to add previous understandings of reasoning structures. It alsoprovides the adaptive ability to learn models inside every module or function,consistent with the modular structure of cognitive processes. We describe theframework in-depth and compare it with other AI pipelines and existingframeworks. The paper explores practical applications, covering experimentsthat show the effectiveness of our method. Our results indicate that AI agentsperform and adapt far better when organised reasoning and prior knowledge areembedded. This opens the door to more resilient and general AI agent systems.</description><author>Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, Zheng Xiong, Yicheng Luo, Jianye Hao, Kun Shao, Haitham Bou-Ammar, Jun Wang</author><pubDate>Fri, 22 Dec 2023 17:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14878v1</guid></item><item><title>Robust Knowledge Extraction from Large Language Models using Social Choice Theory</title><link>http://arxiv.org/abs/2312.14877v1</link><description>Large-language models (LLMs) have the potential to support a wide range ofapplications like conversational agents, creative writing, text improvement,and general query answering. However, they are ill-suited for query answeringin high-stake domains like medicine because they generate answers at random andtheir answers are typically not robust - even the same query can result indifferent answers when prompted multiple times. In order to improve therobustness of LLM queries, we propose using ranking queries repeatedly and toaggregate the queries using methods from social choice theory. We study rankingqueries in diagnostic settings like medical and fault diagnosis and discuss howthe Partial Borda Choice function from the literature can be applied to mergemultiple query results. We discuss some additional interesting properties inour setting and evaluate the robustness of our approach empirically.</description><author>Nico Potyka, Yuqicheng Zhu, Yunjie He, Evgeny Kharlamov, Steffen Staab</author><pubDate>Fri, 22 Dec 2023 17:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14877v1</guid></item><item><title>Automating the Design of Multigrid Methods with Evolutionary Program Synthesis</title><link>http://arxiv.org/abs/2312.14875v1</link><description>Many of the most fundamental laws of nature can be formulated as partialdifferential equations (PDEs). Understanding these equations is, therefore, ofexceptional importance for many branches of modern science and engineering.However, since the general solution of many PDEs is unknown, the efficientapproximate solution of these equations is one of humanity's greatestchallenges. While multigrid represents one of the most effective methods forsolving PDEs numerically, in many cases, the design of an efficient or at leastworking multigrid solver is an open problem. This thesis demonstrates thatgrammar-guided genetic programming, an evolutionary program synthesistechnique, can discover multigrid methods of unprecedented structure thatachieve a high degree of efficiency and generalization. For this purpose, wedevelop a novel context-free grammar that enables the automated generation ofmultigrid methods in a symbolically-manipulable formal language, based on whichwe can apply the same multigrid-based solver to problems of different sizeswithout having to adapt its internal structure. Treating the automated designof an efficient multigrid method as a program synthesis task allows us to findnovel sequences of multigrid operations, including the combination of differentsmoothing and coarse-grid correction steps on each level of the discretizationhierarchy. To prove the feasibility of this approach, we present itsimplementation in the form of the Python framework EvoStencils, which is freelyavailable as open-source software. This implementation comprises all steps fromrepresenting the algorithmic sequence of a multigrid method in the form of adirected acyclic graph of Python objects to its automatic generation andoptimization using the capabilities of the code generation frameworkExaStencils and the evolutionary computation library DEAP.</description><author>Jonas Schmitt</author><pubDate>Fri, 22 Dec 2023 17:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14875v1</guid></item><item><title>The Framework Tax: Disparities Between Inference Efficiency in NLP Research and Deployment</title><link>http://arxiv.org/abs/2302.06117v2</link><description>Increased focus on the computational efficiency of NLP systems has motivatedthe design of efficient model architectures and improvements to underlyinghardware accelerators. However, the resulting increases in computationalthroughput and reductions in floating point operations have not directlytranslated to improvements in wall-clock inference latency. We demonstrate thatthese discrepancies can be largely attributed to bottlenecks introduced by deeplearning frameworks. We denote this phenomenon as the \textit{framework tax},and observe that the disparity is growing as hardware speed increases overtime. In this work, we examine this phenomenon through a series of case studiesanalyzing the effects of model design decisions, framework paradigms, andhardware platforms on total model latency. Code is available athttps://github.com/JaredFern/Framework-Tax.</description><author>Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell</author><pubDate>Fri, 22 Dec 2023 17:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06117v2</guid></item><item><title>Next Steps for Human-Centered Generative AI: A Technical Perspective</title><link>http://arxiv.org/abs/2306.15774v2</link><description>Through iterative, cross-disciplinary discussions, we define and proposenext-steps for Human-centered Generative AI (HGAI). We contribute acomprehensive research agenda that lays out future directions of Generative AIspanning three levels: aligning with human values; assimilating human intents;and augmenting human abilities. By identifying these next-steps, we intend todraw interdisciplinary research teams to pursue a coherent set of emergentideas in HGAI, focusing on their interested topics while maintaining a coherentbig picture of the future work landscape.</description><author>Xiang 'Anthony' Chen, Jeff Burke, Ruofei Du, Matthew K. Hong, Jennifer Jacobs, Philippe Laban, Dingzeyu Li, Nanyun Peng, Karl D. D. Willis, Chien-Sheng Wu, Bolei Zhou</author><pubDate>Fri, 22 Dec 2023 17:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15774v2</guid></item><item><title>BrainVis: Exploring the Bridge between Brain and Visual Signals via Image Reconstruction</title><link>http://arxiv.org/abs/2312.14871v1</link><description>Analyzing and reconstructing visual stimuli from brain signals effectivelyadvances understanding of the human visual system. However, the EEG signals arecomplex and contain a amount of noise. This leads to substantial limitations inexisting works of visual stimuli reconstruction from EEG, such as difficultiesin aligning EEG embeddings with the fine-grained semantic information and aheavy reliance on additional large self-collected dataset for training. Toaddress these challenges, we propose a novel approach called BrainVis. Firstly,we divide the EEG signals into various units and apply a self-supervisedapproach on them to obtain EEG time-domain features, in an attempt to ease thetraining difficulty. Additionally, we also propose to utilize thefrequency-domain features to enhance the EEG representations. Then, wesimultaneously align EEG time-frequency embeddings with the interpolation ofthe coarse and fine-grained semantics in the CLIP space, to highlight theprimary visual components and reduce the cross-modal alignment difficulty.Finally, we adopt the cascaded diffusion models to reconstruct images. Ourproposed BrainVis outperforms state of the arts in both semantic fidelityreconstruction and generation quality. Notably, we reduce the training datascale to 10% of the previous work.</description><author>Honghao Fu, Zhiqi Shen, Jing Jih Chin, Hao Wang</author><pubDate>Fri, 22 Dec 2023 17:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14871v1</guid></item><item><title>Numerical Reasoning for Financial Reports</title><link>http://arxiv.org/abs/2312.14870v1</link><description>Financial reports offer critical insights into a company's operations, yettheir extensive length typically spanning 30 40 pages poses challenges forswift decision making in dynamic markets. To address this, we leveragedfinetuned Large Language Models (LLMs) to distill key indicators andoperational metrics from these reports basis questions from the user. Wedevised a method to locate critical data, and leverage the FinQA dataset tofine-tune both Llama-2 7B and T5 models for customized question answering. Weachieved results comparable to baseline on the final numerical answer, acompetitive accuracy in numerical reasoning and calculation.</description><author>Abhinav Arun, Ashish Dhiman, Mehul Soni, Yibei Hu</author><pubDate>Fri, 22 Dec 2023 17:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14870v1</guid></item><item><title>Spatiotemporal-Linear: Towards Universal Multivariate Time Series Forecasting</title><link>http://arxiv.org/abs/2312.14869v1</link><description>Within the field of complicated multivariate time series forecasting (TSF),popular techniques frequently rely on intricate deep learning architectures,ranging from transformer-based designs to recurrent neural networks. However,recent findings suggest that simple Linear models can surpass sophisticatedconstructs on diverse datasets. These models directly map observation tomultiple future time steps, thereby minimizing error accumulation in iterativemulti-step prediction. Yet, these models fail to incorporate spatial andtemporal information within the data, which is critical for capturing patternsand dependencies that drive insightful predictions. This oversight often leadsto performance bottlenecks, especially under specific sequence lengths anddataset conditions, preventing their universal application. In response, weintroduce the SpatioTemporal-Linear (STL) framework. STL seamlessly integratestime-embedded and spatially-informed bypasses to augment the Linear-basedarchitecture. These extra routes offer a more robust and refined regression tothe data, particularly when the amount of observation is limited and thecapacity of simple linear layers to capture dependencies declines. Empiricalevidence highlights STL's prowess, outpacing both Linear and Transformerbenchmarks across varied observation and prediction durations and datasets.Such robustness accentuates its suitability across a spectrum of applications,including but not limited to, traffic trajectory and rare disease progressionforecasting. Through this discourse, we not only validate the STL's distinctivecapacities to become a more general paradigm in multivariate time-seriesprediction using deep-learning techniques but also stress the need to tackledata-scarce prediction scenarios for universal application. Code will be madeavailable.</description><author>Aiyinsi Zuo, Haixi Zhang, Zirui Li, Ce Zheng</author><pubDate>Fri, 22 Dec 2023 17:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14869v1</guid></item><item><title>VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation</title><link>http://arxiv.org/abs/2312.14867v1</link><description>In the rapidly advancing field of conditional image generation research,challenges such as limited explainability lie in effectively evaluating theperformance and capabilities of various models. This paper introduces VIESCORE,a Visual Instruction-guided Explainable metric for evaluating any conditionalimage generation tasks. VIESCORE leverages general knowledge from MultimodalLarge Language Models (MLLMs) as the backbone and does not require training orfine-tuning. We evaluate VIESCORE on seven prominent tasks in conditional imagetasks and found: (1) VIESCORE (GPT4-v) achieves a high Spearman correlation of0.3 with human evaluations, while the human-to-human correlation is 0.45. (2)VIESCORE (with open-source MLLM) is significantly weaker than GPT-4v inevaluating synthetic images. (3) VIESCORE achieves a correlation on par withhuman ratings in the generation tasks but struggles in editing tasks. Withthese results, we believe VIESCORE shows its great potential to replace humanjudges in evaluating image synthesis tasks.</description><author>Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, Wenhu Chen</author><pubDate>Fri, 22 Dec 2023 17:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14867v1</guid></item><item><title>YAYI 2: Multilingual Open-Source Large Language Models</title><link>http://arxiv.org/abs/2312.14862v1</link><description>As the latest advancements in natural language processing, large languagemodels (LLMs) have achieved human-level language understanding and generationabilities in many real-world tasks, and even have been regarded as a potentialpath to the artificial general intelligence. To better facilitate research onLLMs, many open-source LLMs, such as Llama 2 and Falcon, have recently beenproposed and gained comparable performances to proprietary models. However,these models are primarily designed for English scenarios and exhibit poorperformances in Chinese contexts. In this technical report, we propose YAYI 2,including both base and chat models, with 30 billion parameters. YAYI 2 ispre-trained from scratch on a multilingual corpus which contains 2.65 trilliontokens filtered by our pre-training data processing pipeline. The base model isaligned with human values through supervised fine-tuning with millions ofinstructions and reinforcement learning from human feedback. Extensiveexperiments on multiple benchmarks, such as MMLU and CMMLU, consistentlydemonstrate that the proposed YAYI 2 outperforms other similar sizedopen-source models.</description><author>Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, Fan Feng, Feifei Zhao, Hailong Sun, Hanxuan Yang, Haojun Pan, Hongyu Liu, Jianbin Guo, Jiangtao Du, Jingyi Wang, Junfeng Li, Lei Sun, Liduo Liu, Lifeng Dong, Lili Liu, Lin Wang, Liwen Zhang, Minzheng Wang, Pin Wang, Ping Yu, Qingxiao Li, Rui Yan, Rui Zou, Ruiqun Li, Taiwen Huang, Xiaodong Wang, Xiaofei Wu, Xin Peng, Xina Zhang, Xing Fang, Xinglin Xiao, Yanni Hao, Yao Dong, Yigang Wang, Ying Liu, Yongyu Jiang, Yungan Wang, Yuqi Wang, Zhangsheng Wang, Zhaoxin Yu, Zhen Luo, Wenji Mao, Lei Wang, Dajun Zeng</author><pubDate>Fri, 22 Dec 2023 17:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14862v1</guid></item><item><title>Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code</title><link>http://arxiv.org/abs/2312.14856v1</link><description>We present a method for systematically evaluating the correctness androbustness of instruction-tuned large language models (LLMs) for codegeneration via a new benchmark, Turbulence. Turbulence consists of a large setof natural language $\textit{question templates}$, each of which is aprogramming problem, parameterised so that it can be asked in many differentforms. Each question template has an associated $\textit{test oracle}$ thatjudges whether a code solution returned by an LLM is correct. Thus, from asingle question template, it is possible to ask an LLM a$\textit{neighbourhood}$ of very similar programming questions, and assess thecorrectness of the result returned for each question. This allows gaps in anLLM's code generation abilities to be identified, including$\textit{anomalies}$ where the LLM correctly solves $\textit{almost all}$questions in a neighbourhood but fails for particular parameter instantiations.We present experiments against five LLMs from OpenAI, Cohere and Meta, each attwo temperature configurations. Our findings show that, across the board,Turbulence is able to reveal gaps in LLM reasoning ability. This goes beyondmerely highlighting that LLMs sometimes produce wrong code (which is nosurprise): by systematically identifying cases where LLMs are able to solvesome problems in a neighbourhood but do not manage to generalise to solve thewhole neighbourhood, our method is effective at highlighting$\textit{robustness}$ issues. We present data and examples that shed light onthe kinds of mistakes that LLMs make when they return incorrect code results.</description><author>Shahin Honarvar, Mark van der Wilk, Alastair Donaldson</author><pubDate>Fri, 22 Dec 2023 17:29:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14856v1</guid></item><item><title>Attesting Distributional Properties of Training Data for Machine Learning</title><link>http://arxiv.org/abs/2308.09552v2</link><description>The success of machine learning (ML) has been accompanied by increasedconcerns about its trustworthiness. Several jurisdictions are preparing MLregulatory frameworks. One such concern is ensuring that model training datahas desirable distributional properties for certain sensitive attributes. Forexample, draft regulations indicate that model trainers are required to showthat training datasets have specific distributional properties, such asreflecting diversity of the population. We propose the notion of property attestation allowing a prover (e.g., modeltrainer) to demonstrate relevant distributional properties of training data toa verifier (e.g., a customer) without revealing the data. We present aneffective hybrid property attestation combining property inference withcryptographic mechanisms.</description><author>Vasisht Duddu, Anudeep Das, Nora Khayata, Hossein Yalame, Thomas Schneider, N. Asokan</author><pubDate>Fri, 22 Dec 2023 17:25:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09552v2</guid></item><item><title>TACO: Topics in Algorithmic COde generation dataset</title><link>http://arxiv.org/abs/2312.14852v1</link><description>We introduce TACO, an open-source, large-scale code generation dataset, witha focus on the optics of algorithms, designed to provide a more challengingtraining dataset and evaluation benchmark in the field of code generationmodels. TACO includes competition-level programming questions that are morechallenging, to enhance or evaluate problem understanding and reasoningabilities in real-world programming scenarios. There are 25433 and 1000 codingproblems in training and test set, as well as up to 1.55 million diversesolution answers. Moreover, each TACO problem includes several fine-grainedlabels such as task topics, algorithms, programming skills, and difficultylevels, providing a more precise reference for the training and evaluation ofcode generation models. The dataset and evaluation scripts are available onHugging Face Hub (https://huggingface.co/datasets/BAAI/TACO) and Github(https://github.com/FlagOpen/TACO).</description><author>Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, Ge Li</author><pubDate>Fri, 22 Dec 2023 17:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14852v1</guid></item><item><title>Large Scale Traning of Graph Neural Networks for Optimal Markov-Chain Partitioning Using the Kemeny Constant</title><link>http://arxiv.org/abs/2312.14847v1</link><description>Traditional clustering algorithms often struggle to capture the complexrelationships within graphs and generalise to arbitrary clustering criteria.The emergence of graph neural networks (GNNs) as a powerful framework forlearning representations of graph data provides new approaches to solving theproblem. Previous work has shown GNNs to be capable of proposing partitioningsusing a variety of criteria, however, these approaches have not yet beenextended to work on Markov chains or kinetic networks. These arise frequentlyin the study of molecular systems and are of particular interest to thebiochemical modelling community. In this work, we propose several GNN-basedarchitectures to tackle the graph partitioning problem for Markov Chainsdescribed as kinetic networks. This approach aims to minimize how much aproposed partitioning changes the Kemeny constant. We propose using anencoder-decoder architecture and show how simple GraphSAGE-based GNNs withlinear layers can outperform much larger and more expressive attention-basedmodels in this context. As a proof of concept, we first demonstrate themethod's ability to cluster randomly connected graphs. We also use a linearchain architecture corresponding to a 1D free energy profile as our kineticnetwork. Subsequently, we demonstrate the effectiveness of our method throughexperiments on a data set derived from molecular dynamics. We compare theperformance of our method to other partitioning techniques such as PCCA+. Weexplore the importance of feature and hyperparameter selection and propose ageneral strategy for large-scale parallel training of GNNs for discoveringoptimal graph partitionings.</description><author>Sam Alexander Martino, João Morado, Chenghao Li, Zhenghao Lu, Edina Rosta</author><pubDate>Fri, 22 Dec 2023 17:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14847v1</guid></item><item><title>On the Use of Metaphor Translation in Psychiatry</title><link>http://arxiv.org/abs/2312.14845v1</link><description>Providing mental healthcare to individuals with limited English proficiency(LEP) remains a pressing problem within psychiatry. Because the majority ofindividuals trained in providing psychiatric care are English speakers, thequality of mental healthcare given to LEP patients is significantly lower thanthat provided for English speakers. The provision of mental healthcare iscontingent on communication and understanding between the patient andhealthcare provider, much more so than in the realm of physical healthcare, andEnglish speakers are often unable to comprehend figurative language such asmetaphors used by LEPs. Hence, Figurative Language Translation is invaluable toproviding equitable psychiatric care. Now, metaphor has been shown to beparamount in both identifying individuals struggling with mental problems andhelping those individuals understand and communicate their experiences.Therefore, this paper aims to survey the potential of Machine Translation forproviding equitable psychiatric healthcare and highlights the need for furtherresearch on the transferability of existing machine and metaphor translationresearch in the domain of psychiatry.</description><author>Lois Wong</author><pubDate>Fri, 22 Dec 2023 17:19:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14845v1</guid></item><item><title>Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Estimating Sample Size and Reducing Overfitting</title><link>http://arxiv.org/abs/2308.11197v3</link><description>This study's first purpose is to provide quantitative evidence that wouldincentivize researchers to instead use the more robust method of nestedcross-validation. The second purpose is to present methods and MATLAB codes fordoing power analysis for ML-based analysis during the design of a study. MonteCarlo simulations were used to quantify the interactions between the employedcross-validation method, the discriminative power of features, thedimensionality of the feature space, and the dimensionality of the model. Fourdifferent cross-validations (single holdout, 10-fold, train-validation-test,and nested 10-fold) were compared based on the statistical power andstatistical confidence of the ML models. Distributions of the null andalternative hypotheses were used to determine the minimum required sample sizefor obtaining a statistically significant outcome ({\alpha}=0.05,1-\b{eta}=0.8). Statistical confidence of the model was defined as theprobability of correct features being selected and hence being included in thefinal model. Our analysis showed that the model generated based on the singleholdout method had very low statistical power and statistical confidence andthat it significantly overestimated the accuracy. Conversely, the nested10-fold cross-validation resulted in the highest statistical confidence and thehighest statistical power, while providing an unbiased estimate of theaccuracy. The required sample size with a single holdout could be 50% higherthan what would be needed if nested cross-validation were used. Confidence inthe model based on nested cross-validation was as much as four times higherthan the confidence in the single holdout-based model. A computational model,MATLAB codes, and lookup tables are provided to assist researchers withestimating the sample size during the design of their future studies.</description><author>Hamzeh Ghasemzadeh, Robert E. Hillman, Daryush D. Mehta</author><pubDate>Fri, 22 Dec 2023 17:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11197v3</guid></item><item><title>Learning Lagrangian Multipliers for the Travelling Salesman Problem</title><link>http://arxiv.org/abs/2312.14836v1</link><description>Lagrangian relaxation is a versatile mathematical technique employed to relaxconstraints in an optimization problem, enabling the generation of dual boundsto prove the optimality of feasible solutions and the design of efficientpropagators in constraint programming (such as the weighted circuitconstraint). However, the conventional process of deriving Lagrangianmultipliers (e.g., using subgradient methods) is often computationallyintensive, limiting its practicality for large-scale or time-sensitiveproblems. To address this challenge, we propose an innovative unsupervisedlearning approach that harnesses the capabilities of graph neural networks toexploit the problem structure, aiming to generate accurate Lagrangianmultipliers efficiently. We apply this technique to the well-known Held-KarpLagrangian relaxation for the travelling salesman problem. The core idea is topredict accurate Lagrangian multipliers and to employ them as a warm start forgenerating Held-Karp relaxation bounds. These bounds are subsequently utilizedto enhance the filtering process carried out by branch-and-bound algorithms. Incontrast to much of the existing literature, which primarily focuses on findingfeasible solutions, our approach operates on the dual side, demonstrating thatlearning can also accelerate the proof of optimality. We conduct experimentsacross various distributions of the metric travelling salesman problem,considering instances with up to 200 cities. The results illustrate that ourapproach can improve the filtering level of the weighted circuit globalconstraint, reduce the optimality gap by a factor two for unsolved instances upto a timeout, and reduce the execution time for solved instances by 10%.</description><author>Augustin Parjadis, Quentin Cappart, Bistra Dilkina, Aaron Ferber, Louis-Martin Rousseau</author><pubDate>Fri, 22 Dec 2023 17:09:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14836v1</guid></item><item><title>Prototype-Guided Text-based Person Search based on Rich Chinese Descriptions</title><link>http://arxiv.org/abs/2312.14834v1</link><description>Text-based person search aims to simultaneously localize and identify thetarget person based on query text from uncropped scene images, which can beregarded as the unified task of person detection and text-based personretrieval task. In this work, we propose a large-scale benchmark dataset namedPRW-TPS-CN based on the widely used person search dataset PRW. Our datasetcontains 47,102 sentences, which means there is quite more information thanexisting dataset. These texts precisely describe the person images from top tobottom, which in line with the natural description order. We also provide bothChinese and English descriptions in our dataset for more comprehensiveevaluation. These characteristics make our dataset more applicable. Toalleviate the inconsistency between person detection and text-based personretrieval, we take advantage of the rich texts in PRW-TPS-CN dataset. Wepropose to aggregate multiple texts as text prototypes to maintain theprominent text features of a person, which can better reflect the wholecharacter of a person. The overall prototypes lead to generating the imageattention map to eliminate the detection misalignment causing the decrease oftext-based person retrieval. Thus, the inconsistency between person detectionand text-based person retrieval is largely alleviated. We conduct extensiveexperiments on the PRW-TPS-CN dataset. The experimental results show thePRW-TPS-CN dataset's effectiveness and the state-of-the-art performance of ourapproach.</description><author>Ziqiang Wu, Bingpeng Ma</author><pubDate>Fri, 22 Dec 2023 17:08:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14834v1</guid></item><item><title>Dreaming of Electrical Waves: Generative Modeling of Cardiac Excitation Waves using Diffusion Models</title><link>http://arxiv.org/abs/2312.14830v1</link><description>Electrical waves in the heart form rotating spiral or scroll waves duringlife-threatening arrhythmias such as atrial or ventricular fibrillation. Thewave dynamics are typically modeled using coupled partial differentialequations, which describe reaction-diffusion dynamics in excitable media. Morerecently, data-driven generative modeling has emerged as an alternative togenerate spatio-temporal patterns in physical and biological systems. Here, weexplore denoising diffusion probabilistic models for the generative modeling ofelectrical wave patterns in cardiac tissue. We trained diffusion models withsimulated electrical wave patterns to be able to generate such wave patterns inunconditional and conditional generation tasks. For instance, we exploredinpainting tasks, such as reconstructing three-dimensional wave dynamics fromsuperficial two-dimensional measurements, and evolving and generatingparameter-specific dynamics. We characterized and compared thediffusion-generated solutions to solutions obtained with biophysical models andfound that diffusion models learn to replicate spiral and scroll waves dynamicsso well that they could serve as an alternative data-driven approach for themodeling of excitation waves in cardiac tissue. For instance, we found that itis possible to initiate ventricular fibrillation (VF) dynamics instantaneouslywithout having to apply pacing protocols in order to induce wavebreak. The VFdynamics can be created in arbitrary ventricular geometries and can be evolvedover time. However, we also found that diffusion models `hallucinate' wavepatterns when given insufficient constraints. Regardless of these limitations,diffusion models are an interesting and powerful tool with many potentialapplications in cardiac arrhythmia research and diagnostics.</description><author>Tanish Baranwal, Jan Lebert, Jan Christoph</author><pubDate>Fri, 22 Dec 2023 17:06:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14830v1</guid></item><item><title>Plan, Posture and Go: Towards Open-World Text-to-Motion Generation</title><link>http://arxiv.org/abs/2312.14828v1</link><description>Conventional text-to-motion generation methods are usually trained on limitedtext-motion pairs, making them hard to generalize to open-world scenarios. Someworks use the CLIP model to align the motion space and the text space, aimingto enable motion generation from natural language motion descriptions. However,they are still constrained to generate limited and unrealistic in-placemotions. To address these issues, we present a divide-and-conquer frameworknamed PRO-Motion, which consists of three modules as motion planner,posture-diffuser and go-diffuser. The motion planner instructs Large LanguageModels (LLMs) to generate a sequence of scripts describing the key postures inthe target motion. Differing from natural languages, the scripts can describeall possible postures following very simple text templates. This significantlyreduces the complexity of posture-diffuser, which transforms a script to aposture, paving the way for open-world generation. Finally, go-diffuser,implemented as another diffusion model, estimates whole-body translations androtations for all postures, resulting in realistic motions. Experimentalresults have shown the superiority of our method with other counterparts, anddemonstrated its capability of generating diverse and realistic motions fromcomplex open-world prompts such as "Experiencing a profound sense of joy". Theproject page is available at https://moonsliu.github.io/Pro-Motion.</description><author>Jinpeng Liu, Wenxun Dai, Chunyu Wang, Yiji Cheng, Yansong Tang, Xin Tong</author><pubDate>Fri, 22 Dec 2023 17:02:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14828v1</guid></item><item><title>An investigation of belief-free DRL and MCTS for inspection and maintenance planning</title><link>http://arxiv.org/abs/2312.14824v1</link><description>We propose a novel Deep Reinforcement Learning (DRL) architecture forsequential decision processes under uncertainty, as encountered in inspectionand maintenance (I&amp;M) planning. Unlike other DRL algorithms for (I&amp;M) planning,the proposed +RQN architecture dispenses with computing the belief state anddirectly handles erroneous observations instead. We apply the algorithm to abasic I&amp;M planning problem for a one-component system subject to deterioration.In addition, we investigate the performance of Monte Carlo tree search for theI&amp;M problem and compare it to the +RQN. The comparison includes a statisticalanalysis of the two methods' resulting policies, as well as their visualizationin the belief space.</description><author>Daniel Koutas, Elizabeth Bismut, Daniel Straub</author><pubDate>Fri, 22 Dec 2023 16:53:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14824v1</guid></item><item><title>Understanding the Regularity of Self-Attention with Optimal Transport</title><link>http://arxiv.org/abs/2312.14820v1</link><description>Transformers and their multi-head attention mechanism have completely changedthe machine learning landscape in just a few years, by outperformingstate-of-art models in a wide range of domains. Still, little is known abouttheir robustness from a theoretical perspective. We tackle this problem bystudying the local Lipschitz constant of self-attention, that provides anattack-agnostic way of measuring the robustness of a neural network. We adopt ameasure-theoretic framework, by viewing inputs as probability measures equippedwith the Wasserstein distance. This allows us to generalize attention to inputsof infinite length, and to derive an upper bound and a lower bound on theLipschitz constant of self-attention on compact sets. The lower boundsignificantly improves prior results, and grows more than exponentially withthe radius of the compact set, which rules out the possibility of obtainingrobustness guarantees without any additional constraint on the input space. Ourresults also point out that measures with a high local Lipschitz constant aretypically made of a few diracs, with a very unbalanced distribution of mass.Finally, we analyze the stability of self-attention under perturbations thatchange the number of tokens, which appears to be a natural question in themeasure-theoretic framework. In particular, we show that for some inputs,attacks that duplicate tokens before perturbing them are more efficient thanattacks that simply move tokens. We call this phenomenon mass splitting.</description><author>Valérie Castin, Pierre Ablin, Gabriel Peyré</author><pubDate>Fri, 22 Dec 2023 16:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14820v1</guid></item><item><title>PARDINUS: Weakly supervised discarding of photo-trapping empty images based on autoencoders</title><link>http://arxiv.org/abs/2312.14812v1</link><description>Photo-trapping cameras are widely employed for wildlife monitoring. Thosecameras take photographs when motion is detected to capture images whereanimals appear. A significant portion of these images are empty - no wildlifeappears in the image. Filtering out those images is not a trivial task since itrequires hours of manual work from biologists. Therefore, there is a notableinterest in automating this task. Automatic discarding of empty photo-trappingimages is still an open field in the area of Machine Learning. Existingsolutions often rely on state-of-the-art supervised convolutional neuralnetworks that require the annotation of the images in the training phase.PARDINUS (Weakly suPervised discARDINg of photo-trapping empty images based onaUtoencoderS) is constructed on the foundation of weakly supervised learningand proves that this approach equals or even surpasses other fully supervisedmethods that require further labeling work.</description><author>David de la Rosa, Antonio J Rivera, María J del Jesus, Francisco Charte</author><pubDate>Fri, 22 Dec 2023 16:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14812v1</guid></item><item><title>A Tricycle Model to Accurately Control an Autonomous Racecar with Locked Differential</title><link>http://arxiv.org/abs/2312.14808v1</link><description>In this paper, we present a novel formulation to model the effects of alocked differential on the lateral dynamics of an autonomous open-wheelracecar. The model is used in a Model Predictive Controller in which weincluded a micro-steps discretization approach to accurately linearize thedynamics and produce a prediction suitable for real-time implementation. Thestability analysis of the model is presented, as well as a brief description ofthe overall planning and control scheme which includes an offline trajectorygeneration pipeline, an online local speed profile planner, and a low-levellongitudinal controller. An improvement of the lateral path tracking isdemonstrated in preliminary experimental results that have been produced on aDallara AV-21 during the first Indy Autonomous Challenge event on the Monza F1racetrack. Final adjustments and tuning have been performed in a high-fidelitysimulator demonstrating the effectiveness of the solution when performing closeto the tire limits.</description><author>Ayoub Raji, Nicola Musiu, Alessandro Toschi, Francesco Prignoli, Eugenio Mascaro, Pietro Musso, Francesco Amerotti, Alexander Liniger, Silvio Sorrentino, Marko Bertogna</author><pubDate>Fri, 22 Dec 2023 16:29:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14808v1</guid></item><item><title>The Effects of Signal-to-Noise Ratio on Generative Adversarial Networks Applied to Marine Bioacoustic Data</title><link>http://arxiv.org/abs/2312.14806v1</link><description>In recent years generative adversarial networks (GANs) have been used tosupplement datasets within the field of marine bioacoustics. This is driven byfactors such as the cost to collect data, data sparsity and aid preprocessing.One notable challenge with marine bioacoustic data is the low signal-to-noiseratio (SNR) posing difficulty when applying deep learning techniques such asGANs. This work investigates the effect SNR has on the audio-based GANperformance and examines three different evaluation methodologies for GANperformance, yielding interesting results on the effects of SNR on GANs,specifically WaveGAN.</description><author>Georgia Atkinson, Nick Wright, A. Stephen McGough, Per Berggren</author><pubDate>Fri, 22 Dec 2023 16:27:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14806v1</guid></item><item><title>Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs. SQL for No-Code Access to Relational Databases</title><link>http://arxiv.org/abs/2312.14798v1</link><description>Large Language Models (LLMs) have spurred progress in text-to-SQL, the taskof generating SQL queries from natural language questions based on a givendatabase schema. Despite the declarative nature of SQL, it continues to be acomplex programming language. In this paper, we investigate the potential of analternative query language with simpler syntax and modular specification ofcomplex queries. The purpose is to create a query language that can be learnedmore easily by modern neural semantic parsing architectures while also enablingnon-programmers to better assess the validity of the query plans produced by aninteractive query plan assistant. The proposed alternative query language is called Query Plan Language (QPL).It is designed to be modular and can be translated into a restricted form ofSQL Common Table Expressions (CTEs). The aim of QPL is to make complex dataretrieval accessible to non-programmers by allowing users to express theirquestions in natural language while also providing an easier-to-verify targetlanguage. The paper demonstrates how neural LLMs can benefit from QPL'smodularity to generate complex query plans in a compositional manner. Thisinvolves a question decomposition strategy and a planning stage. We conduct experiments on a version of the Spider text-to-SQL dataset thathas been converted to QPL. The hierarchical structure of QPL programs enablesus to measure query complexity naturally. Based on this assessment, we identifythe low accuracy of existing text-to-SQL systems on complex compositionalqueries. We present ways to address the challenge of complex queries in aniterative, user-controlled manner, using fine-tuned LLMs and a variety ofprompting strategies in a compositional manner.</description><author>Ben Eyal, Amir Bachar, Ophir Haroche, Michael Elhadad</author><pubDate>Fri, 22 Dec 2023 16:16:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14798v1</guid></item><item><title>On support vector machines under a multiple-cost scenario</title><link>http://arxiv.org/abs/2312.14795v1</link><description>Support Vector Machine (SVM) is a powerful tool in binary classification,known to attain excellent misclassification rates. On the other hand, manyrealworld classification problems, such as those found in medical diagnosis,churn or fraud prediction, involve misclassification costs which may bedifferent in the different classes. However, it may be hard for the user toprovide precise values for such misclassification costs, whereas it may be mucheasier to identify acceptable misclassification rates values. In this paper wepropose a novel SVM model in which misclassification costs are considered byincorporating performance constraints in the problem formulation. Specifically,our aim is to seek the hyperplane with maximal margin yieldingmisclassification rates below given threshold values. Such maximal marginhyperplane is obtained by solving a quadratic convex problem with linearconstraints and integer variables. The reported numerical experience shows thatour model gives the user control on the misclassification rates in one class(possibly at the expense of an increase in misclassification rates for theother class) and is feasible in terms of running times.</description><author>Sandra Benítez-Peña, Rafael Blanquero, Emilio Carrizosa, Pepa Ramírez-Cobo</author><pubDate>Fri, 22 Dec 2023 16:12:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14795v1</guid></item><item><title>An Empirical Study on Compliance with Ranking Transparency in the Software Documentation of EU Online Platforms</title><link>http://arxiv.org/abs/2312.14794v1</link><description>Compliance with the European Union's Platform-to-Business (P2B) Regulation ischallenging for online platforms, and assessing their compliance can bedifficult for public authorities. This is partly due to the lack of automatedtools for assessing the information (e.g., software documentation) platformsprovide concerning ranking transparency. Our study tackles this issue in twoways. First, we empirically evaluate the compliance of six major platforms(Amazon, Bing, Booking, Google, Tripadvisor, and Yahoo), revealing substantialdifferences in their documentation. Second, we introduce and test automatedcompliance assessment tools based on ChatGPT and information retrievaltechnology. These tools are evaluated against human judgments, showingpromising results as reliable proxies for compliance assessments. Our findingscould help enhance regulatory compliance and align with the United NationsSustainable Development Goal 10.3, which seeks to reduce inequality, includingbusiness disparities, on these platforms.</description><author>Francesco Sovrano, Michaël Lognoul, Alberto Bacchelli</author><pubDate>Fri, 22 Dec 2023 16:08:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14794v1</guid></item><item><title>The Rate-Distortion-Perception-Classification Tradeoff: Joint Source Coding and Modulation via Inverse-Domain GANs</title><link>http://arxiv.org/abs/2312.14792v1</link><description>The joint source coding and modulation (JSCM) framework was enabled by recentdevelopments in deep learning, which allows to automatically learn from data,and in an end-to-end fashion, the best compression codes and modulationschemes. In this paper, we show the existence of a strict tradeoff betweenchannel rate, distortion, perception, and classification accuracy in a JSCMscenario. We then propose two image compression methods to navigate thattradeoff: an inverse-domain generative adversarial network (ID-GAN), whichachieves extreme compression, and a simpler, heuristic method that revealsinsights about the performance of ID-GAN. Experiment results not onlycorroborate the theoretical findings, but also demonstrate that the proposedID-GAN algorithm significantly improves system performance compared totraditional separation-based methods and recent deep JSCM architectures.</description><author>Junli Fang, João F. C. Mota, Baoshan Lu, Weicheng Zhang, Xuemin Hong</author><pubDate>Fri, 22 Dec 2023 16:06:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14792v1</guid></item><item><title>Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets</title><link>http://arxiv.org/abs/2310.01438v2</link><description>The advancements in data acquisition, storage, and processing techniques haveresulted in the rapid growth of heterogeneous medical data. Integratingradiological scans, histopathology images, and molecular information withclinical data is essential for developing a holistic understanding of thedisease and optimizing treatment. The need for integrating data from multiplesources is further pronounced in complex diseases such as cancer for enablingprecision medicine and personalized treatments. This work proposes MultimodalIntegration of Oncology Data System (MINDS) - a flexible, scalable, andcost-effective metadata framework for efficiently fusing disparate data frompublic sources such as the Cancer Research Data Commons (CRDC) into aninterconnected, patient-centric framework. MINDS offers an interface forexploring relationships across data types and building cohorts for developinglarge-scale multimodal machine learning models. By harmonizing multimodal data,MINDS aims to potentially empower researchers with greater analytical abilityto uncover diagnostic and prognostic insights and enable evidence-basedpersonalized care. MINDS tracks granular end-to-end data provenance, ensuringreproducibility and transparency. The cloud-native architecture of MINDS canhandle exponential data growth in a secure, cost-optimized manner whileensuring substantial storage optimization, replication avoidance, and dynamicaccess capabilities. Auto-scaling, access controls, and other mechanismsguarantee pipelines' scalability and security. MINDS overcomes the limitationsof existing biomedical data silos via an interoperable metadata-driven approachthat represents a pivotal step toward the future of oncology data integration.</description><author>Aakash Tripathi, Asim Waqas, Kavya Venkatesan, Yasin Yilmaz, Ghulam Rasool</author><pubDate>Fri, 22 Dec 2023 15:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01438v2</guid></item><item><title>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</title><link>http://arxiv.org/abs/2312.13016v3</link><description>We present DiffPortrait3D, a conditional diffusion model that is capable ofsynthesizing 3D-consistent photo-realistic novel views from as few as a singlein-the-wild portrait. Specifically, given a single RGB input, we aim tosynthesize plausible but consistent facial details rendered from novel cameraviews with retained both identity and facial expression. In lieu oftime-consuming optimization and fine-tuning, our zero-shot method generalizeswell to arbitrary face portraits with unposed camera views, extreme facialexpressions, and diverse artistic depictions. At its core, we leverage thegenerative prior of 2D diffusion models pre-trained on large-scale imagedatasets as our rendering backbone, while the denoising is guided withdisentangled attentive control of appearance and camera pose. To achieve this,we first inject the appearance context from the reference image into theself-attention layers of the frozen UNets. The rendering view is thenmanipulated with a novel conditional control module that interprets the camerapose by watching a condition image of a crossed subject from the same view.Furthermore, we insert a trainable cross-view attention module to enhance viewconsistency, which is further strengthened with a novel 3D-aware noisegeneration process during inference. We demonstrate state-of-the-art resultsboth qualitatively and quantitatively on our challenging in-the-wild andmulti-view benchmarks.</description><author>Yuming Gu, You Xie, Hongyi Xu, Guoxian Song, Yichun Shi, Di Chang, Jing Yang, Linjie Luo</author><pubDate>Fri, 22 Dec 2023 15:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13016v3</guid></item><item><title>Factored Online Planning in Many-Agent POMDPs</title><link>http://arxiv.org/abs/2312.11434v2</link><description>In centralized multi-agent systems, often modeled as multi-agent partiallyobservable Markov decision processes (MPOMDPs), the action and observationspaces grow exponentially with the number of agents, making the value andbelief estimation of single-agent online planning ineffective. Prior workpartially tackles value estimation by exploiting the inherent structure ofmulti-agent settings via so-called coordination graphs. Additionally, beliefestimation has been improved by incorporating the likelihood of observationsinto the approximation. However, the challenges of value estimation and beliefestimation have only been tackled individually, which prevents existing methodsfrom scaling to many agents. Therefore, we address these challengessimultaneously. First, we introduce weighted particle filtering to asample-based online planner for MPOMDPs. Second, we present a scalableapproximation of the belief. Third, we bring an approach that exploits thetypical locality of agent interactions to novel online planning algorithms forMPOMDPs operating on a so-called sparse particle filter tree. Our experimentalevaluation against several state-of-the-art baselines shows that our methods(1) are competitive in settings with only a few agents and (2) improve over thebaselines in the presence of many agents.</description><author>Maris F. L. Galesloot, Thiago D. Simão, Sebastian Junges, Nils Jansen</author><pubDate>Fri, 22 Dec 2023 15:56:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11434v2</guid></item><item><title>OsmLocator: locating overlapping scatter marks with a non-training generative perspective</title><link>http://arxiv.org/abs/2312.11146v2</link><description>Automated mark localization in scatter images, greatly helpful fordiscovering knowledge and understanding enormous document images and reasoningin visual question answering AI systems, is a highly challenging problembecause of the ubiquity of overlapping marks. Locating overlapping marks facesmany difficulties such as no texture, less contextual information, hallow shapeand tiny size. Here, we formulate it as a combinatorial optimization problem onclustering-based re-visualization from a non-training generative perspective,to locate scatter marks by finding the status of multi-variables when anobjective function reaches a minimum. The objective function is constructed ondifference between binarized scatter images and corresponding generatedre-visualization based on their clustering. Fundamentally, re-visualizationtries to generate a new scatter graph only taking a rasterized scatter image asan input, and clustering is employed to provide the information for suchre-visualization. This method could stably locate severely-overlapping,variable-size and variable-shape marks in scatter images without dependence ofany training dataset or reference. Meanwhile, we propose an adaptive variant ofsimulated annealing which can works on various connected regions. In addition,we especially built a dataset named SML2023 containing hundreds of scatterimages with different markers and various levels of overlapping severity, andtested the proposed method and compared it to existing methods. The resultsshow that it can accurately locate most marks in scatter images with differentoverlapping severity and marker types, with about 0.3 absolute increase on anassignment-cost-based metric in comparison with state-of-the-art methods. Thiswork is of value to data mining on massive web pages and literatures, andshedding new light on image measurement such as bubble counting.</description><author>Yuming Qiu, Aleksandra Pizurica, Qi Ming, Nicolas Nadisic</author><pubDate>Fri, 22 Dec 2023 15:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11146v2</guid></item><item><title>Compressing Image-to-Image Translation GANs Using Local Density Structures on Their Learned Manifold</title><link>http://arxiv.org/abs/2312.14776v1</link><description>Generative Adversarial Networks (GANs) have shown remarkable success inmodeling complex data distributions for image-to-image translation. Still,their high computational demands prohibit their deployment in practicalscenarios like edge devices. Existing GAN compression methods mainly rely onknowledge distillation or convolutional classifiers' pruning techniques. Thus,they neglect the critical characteristic of GANs: their local density structureover their learned manifold. Accordingly, we approach GAN compression from anew perspective by explicitly encouraging the pruned model to preserve thedensity structure of the original parameter-heavy model on its learnedmanifold. We facilitate this objective for the pruned model by partitioning thelearned manifold of the original generator into local neighborhoods around itsgenerated samples. Then, we propose a novel pruning objective to regularize thepruned model to preserve the local density structure over each neighborhood,resembling the kernel density estimation method. Also, we develop acollaborative pruning scheme in which the discriminator and generator arepruned by two pruning agents. We design the agents to capture interactionsbetween the generator and discriminator by exchanging their peer's feedbackwhen determining corresponding models' architectures. Thanks to such a design,our pruning method can efficiently find performant sub-networks and canmaintain the balance between the generator and discriminator more effectivelycompared to baselines during pruning, thereby showing more stable pruningdynamics. Our experiments on image translation GAN models, Pix2Pix andCycleGAN, with various benchmark datasets and architectures demonstrate ourmethod's effectiveness.</description><author>Alireza Ganjdanesh, Shangqian Gao, Hirad Alipanah, Heng Huang</author><pubDate>Fri, 22 Dec 2023 15:43:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14776v1</guid></item><item><title>Cross-Age and Cross-Site Domain Shift Impacts on Deep Learning-Based White Matter Fiber Estimation in Newborn and Baby Brains</title><link>http://arxiv.org/abs/2312.14773v1</link><description>Deep learning models have shown great promise in estimating tissuemicrostructure from limited diffusion magnetic resonance imaging data. However,these models face domain shift challenges when test and train data are fromdifferent scanners and protocols, or when the models are applied to data withinherent variations such as the developing brains of infants and childrenscanned at various ages. Several techniques have been proposed to address someof these challenges, such as data harmonization or domain adaptation in theadult brain. However, those techniques remain unexplored for the estimation offiber orientation distribution functions in the rapidly developing brains ofinfants. In this work, we extensively investigate the age effect and domainshift within and across two different cohorts of 201 newborns and 165 babiesusing the Method of Moments and fine-tuning strategies. Our results show thatreduced variations in the microstructural development of babies in comparisonto newborns directly impact the deep learning models' cross-age performance. Wealso demonstrate that a small number of target domain samples can significantlymitigate domain shift problems.</description><author>Rizhong Lin, Ali Gholipour, Jean-Philippe Thiran, Davood Karimi, Hamza Kebiri, Meritxell Bach Cuadra</author><pubDate>Fri, 22 Dec 2023 15:39:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14773v1</guid></item><item><title>Integration Of Evolutionary Automated Machine Learning With Structural Sensitivity Analysis For Composite Pipelines</title><link>http://arxiv.org/abs/2312.14770v1</link><description>Automated machine learning (AutoML) systems propose an end-to-end solution toa given machine learning problem, creating either fixed or flexible pipelines.Fixed pipelines are task independent constructs: their general compositionremains the same, regardless of the data. In contrast, the structure offlexible pipelines varies depending on the input, making them finely tailoredto individual tasks. However, flexible pipelines can be structurallyovercomplicated and have poor explainability. We propose the EVOSA approachthat compensates for the negative points of flexible pipelines by incorporatinga sensitivity analysis which increases the robustness and interpretability ofthe flexible solutions. EVOSA quantitatively estimates positive and negativeimpact of an edge or a node on a pipeline graph, and feeds this information tothe evolutionary AutoML optimizer. The correctness and efficiency of EVOSA wasvalidated in tabular, multimodal and computer vision tasks, suggestinggeneralizability of the proposed approach across domains.</description><author>Nikolay O. Nikitin, Maiia Pinchuk, Valerii Pokrovskii, Peter Shevchenko, Andrey Getmanov, Yaroslav Aksenkin, Ilia Revin, Andrey Stebenkov, Ekaterina Poslavskaya, Anna V. Kalyuzhnaya</author><pubDate>Fri, 22 Dec 2023 15:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14770v1</guid></item><item><title>Large Language Model (LLM) Bias Index -- LLMBI</title><link>http://arxiv.org/abs/2312.14769v1</link><description>The Large Language Model Bias Index (LLMBI) is a pioneering approach designedto quantify and address biases inherent in large language models (LLMs), suchas GPT-4. We recognise the increasing prevalence and impact of LLMs acrossdiverse sectors. This research introduces a novel metric, LLMBI, tosystematically measure and mitigate biases potentially skewing model responses.We formulated LLMBI using a composite scoring system incorporating multipledimensions of bias, including but not limited to age, gender, and racialbiases. To operationalise this metric, we engaged in a multi-step process involvingcollecting and annotating LLM responses, applying sophisticated NaturalLanguage Processing (NLP) techniques for bias detection, and computing theLLMBI score through a specially crafted mathematical formula. The formulaintegrates weighted averages of various bias dimensions, a penalty for datasetdiversity deficiencies, and a correction for sentiment biases. Our empiricalanalysis, conducted using responses from OpenAI's API, employs advancedsentiment analysis as a representative method for bias detection. The research reveals LLMs, whilst demonstrating impressive capabilities intext generation, exhibit varying degrees of bias across different dimensions.LLMBI provides a quantifiable measure to compare biases across models and overtime, offering a vital tool for systems engineers, researchers and regulatorsin enhancing the fairness and reliability of LLMs. It highlights the potentialof LLMs in mimicking unbiased human-like responses. Additionally, itunderscores the necessity of continuously monitoring and recalibrating suchmodels to align with evolving societal norms and ethical standards.</description><author>Abiodun Finbarrs Oketunji, Muhammad Anas, Deepthi Saina</author><pubDate>Fri, 22 Dec 2023 15:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14769v1</guid></item><item><title>A quantitative fusion strategy of stock picking and timing based on Particle Swarm Optimized-Back Propagation Neural Network and Multivariate Gaussian-Hidden Markov Model</title><link>http://arxiv.org/abs/2312.05756v3</link><description>In recent years, machine learning (ML) has brought effective approaches andnovel techniques to economic decision, investment forecasting, and riskmanagement, etc., coping the variable and intricate nature of economic andfinancial environments. For the investment in stock market, this researchintroduces a pioneering quantitative fusion model combining stock timing andpicking strategy by leveraging the Multivariate Gaussian-Hidden Markov Model(MGHMM) and Back Propagation Neural Network optimized by Particle Swarm(PSO-BPNN). After the information coefficients (IC) between fifty-two factorsthat have been winsorized, neutralized and standardized and the return of CSI300 index are calculated, a given amount of factors that rank ahead are chooseto be candidate factors heading for the input of PSO-BPNN after dimensionreduction by Principal Component Analysis (PCA), followed by a certain amountof constituent stocks outputted. Subsequently, we conduct the prediction andtrading on the basis of the screening stocks and stock market state outputtedby MGHMM trained using inputting CSI 300 index data after Box-Coxtransformation, bespeaking eximious performance during the period of past fouryears. Ultimately, some conventional forecast and trading methods are comparedwith our strategy in Chinese stock market. Our fusion strategy incorporatingstock picking and timing presented in this article provide a innovativetechnique for financial analysis.</description><author>Huajian Li, Longjian Li, Jiajian Liang, Weinan Dai</author><pubDate>Fri, 22 Dec 2023 15:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05756v3</guid></item><item><title>Enhanced Latent Multi-view Subspace Clustering</title><link>http://arxiv.org/abs/2312.14763v1</link><description>Latent multi-view subspace clustering has been demonstrated to have desirableclustering performance. However, the original latent representation methodvertically concatenates the data matrices from multiple views into a singlematrix along the direction of dimensionality to recover the latentrepresentation matrix, which may result in an incomplete information recovery.To fully recover the latent space representation, we in this paper propose anEnhanced Latent Multi-view Subspace Clustering (ELMSC) method. The ELMSC methodinvolves constructing an augmented data matrix that enhances the representationof multi-view data. Specifically, we stack the data matrices from various viewsinto the block-diagonal locations of the augmented matrix to exploit thecomplementary information. Meanwhile, the non-block-diagonal entries arecomposed based on the similarity between different views to capture theconsistent information. In addition, we enforce a sparse regularization for thenon-diagonal blocks of the augmented self-representation matrix to avoidredundant calculations of consistency information. Finally, a novel iterativealgorithm based on the framework of Alternating Direction Method of Multipliers(ADMM) is developed to solve the optimization problem for ELMSC. Extensiveexperiments on real-world datasets demonstrate that our proposed ELMSC is ableto achieve higher clustering performance than some state-of-art multi-viewclustering methods.</description><author>Long Shi, Lei Cao, Jun Wang, Badong Chen</author><pubDate>Fri, 22 Dec 2023 15:28:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14763v1</guid></item><item><title>On Partial Optimal Transport: Revising the Infeasibility of Sinkhorn and Efficient Gradient Methods</title><link>http://arxiv.org/abs/2312.13970v2</link><description>This paper studies the Partial Optimal Transport (POT) problem between twounbalanced measures with at most $n$ supports and its applications in variousAI tasks such as color transfer or domain adaptation. There is hence the needfor fast approximations of POT with increasingly large problem sizes in arisingapplications. We first theoretically and experimentally investigate theinfeasibility of the state-of-the-art Sinkhorn algorithm for POT due to itsincompatible rounding procedure, which consequently degrades its qualitativeperformance in real world applications like point-cloud registration. To thisend, we propose a novel rounding algorithm for POT, and then provide a feasibleSinkhorn procedure with a revised computation complexity of$\mathcal{\widetilde O}(n^2/\varepsilon^4)$. Our rounding algorithm alsopermits the development of two first-order methods to approximate the POTproblem. The first algorithm, Adaptive Primal-Dual Accelerated Gradient Descent(APDAGD), finds an $\varepsilon$-approximate solution to the POT problem in$\mathcal{\widetilde O}(n^{2.5}/\varepsilon)$, which is better in $\varepsilon$than revised Sinkhorn. The second method, Dual Extrapolation, achieves thecomputation complexity of $\mathcal{\widetilde O}(n^2/\varepsilon)$, therebybeing the best in the literature. We further demonstrate the flexibility of POTcompared to standard OT as well as the practicality of our algorithms on realapplications where two marginal distributions are unbalanced.</description><author>Anh Duc Nguyen, Tuan Dung Nguyen, Quang Minh Nguyen, Hoang H. Nguyen, Lam M. Nguyen, Kim-Chuan Toh</author><pubDate>Fri, 22 Dec 2023 15:28:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13970v2</guid></item><item><title>Diffusion Maps for Signal Filtering in Graph Learning</title><link>http://arxiv.org/abs/2312.14758v1</link><description>This paper explores the application diffusion maps as graph shift operatorsin understanding the underlying geometry of graph signals. The study evaluatesthe improvements in graph learning when using diffusion map generated filtersto the Markov Variation minimization problem. The paper showcases theeffectiveness of this approach through examples involving syntheticallygenerated and real-world temperature sensor data. These examples also comparethe diffusion map graph signal model with other commonly used graph signaloperators. The results provide new approaches for the analysis andunderstanding of complex, non-Euclidean data structures.</description><author>Todd Hildebrant</author><pubDate>Fri, 22 Dec 2023 15:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14758v1</guid></item><item><title>Hazards from Increasingly Accessible Fine-Tuning of Downloadable Foundation Models</title><link>http://arxiv.org/abs/2312.14751v1</link><description>Public release of the weights of pretrained foundation models, otherwiseknown as downloadable access \citep{solaiman_gradient_2023}, enablesfine-tuning without the prohibitive expense of pretraining. Our work arguesthat increasingly accessible fine-tuning of downloadable models may increasehazards. First, we highlight research to improve the accessibility offine-tuning. We split our discussion into research that A) reduces thecomputational cost of fine-tuning and B) improves the ability to share thatcost across more actors. Second, we argue that increasingly accessiblefine-tuning methods may increase hazard through facilitating malicious use andmaking oversight of models with potentially dangerous capabilities moredifficult. Third, we discuss potential mitigatory measures, as well as benefitsof more accessible fine-tuning. Given substantial remaining uncertainty abouthazards, we conclude by emphasizing the urgent need for the development ofmitigations.</description><author>Alan Chan, Ben Bucknall, Herbie Bradley, David Krueger</author><pubDate>Fri, 22 Dec 2023 15:05:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14751v1</guid></item><item><title>Progressing from Anomaly Detection to Automated Log Labeling and Pioneering Root Cause Analysis</title><link>http://arxiv.org/abs/2312.14748v1</link><description>The realm of AIOps is transforming IT landscapes with the power of AI and ML.Despite the challenge of limited labeled data, supervised models show promise,emphasizing the importance of leveraging labels for training, especially indeep learning contexts. This study enhances the field by introducing a taxonomyfor log anomalies and exploring automated data labeling to mitigate labelingchallenges. It goes further by investigating the potential of diverse anomalydetection techniques and their alignment with specific anomaly types. However,the exploration doesn't stop at anomaly detection. The study envisions a futurewhere root cause analysis follows anomaly detection, unraveling the underlyingtriggers of anomalies. This uncharted territory holds immense potential forrevolutionizing IT systems management. In essence, this paper enriches ourunderstanding of anomaly detection, and automated labeling, and sets the stagefor transformative root cause analysis. Together, these advances promise moreresilient IT systems, elevating operational efficiency and user satisfaction inan ever-evolving technological landscape.</description><author>Thorsten Wittkopp, Alexander Acker, Odej Kao</author><pubDate>Fri, 22 Dec 2023 15:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14748v1</guid></item><item><title>Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization</title><link>http://arxiv.org/abs/2310.12794v2</link><description>Large language models (LLMs) have exhibited considerable cross-lingualgeneralization abilities, whereby they implicitly transfer knowledge acrosslanguages. However, the transfer is not equally successful for all languages,especially for low-resource ones, which poses an ongoing challenge. It isunclear whether we have reached the limits of implicit cross-lingualgeneralization and if explicit knowledge transfer is viable. In this paper, weinvestigate the potential for explicitly aligning conceptual correspondencebetween languages to enhance cross-lingual generalization. Using the syntacticaspect of language as a testbed, our analyses of 43 languages reveal a highdegree of alignability among the spaces of structural concepts within eachlanguage for both encoder-only and decoder-only LLMs. We then propose ameta-learning-based method to learn to align conceptual spaces of differentlanguages, which facilitates zero-shot and few-shot generalization in conceptclassification and also offers insights into the cross-lingual in-contextlearning phenomenon. Experiments on syntactic analysis tasks show that ourapproach achieves competitive results with state-of-the-art methods and narrowsthe performance gap between languages, particularly benefiting those withlimited resources.</description><author>Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang</author><pubDate>Fri, 22 Dec 2023 15:00:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12794v2</guid></item><item><title>Unsupervised Melody-to-Lyric Generation</title><link>http://arxiv.org/abs/2305.19228v2</link><description>Automatic melody-to-lyric generation is a task in which song lyrics aregenerated to go with a given melody. It is of significant practical interestand more challenging than unconstrained lyric generation as the music imposesadditional constraints onto the lyrics. The training data is limited as mostsongs are copyrighted, resulting in models that underfit the complicatedcross-modal relationship between melody and lyrics. In this work, we propose amethod for generating high-quality lyrics without training on any alignedmelody-lyric data. Specifically, we design a hierarchical lyric generationframework that first generates a song outline and second the complete lyrics.The framework enables disentanglement of training (based purely on text) frominference (melody-guided text generation) to circumvent the shortage ofparallel data. We leverage the segmentation and rhythm alignment between melody and lyricsto compile the given melody into decoding constraints as guidance duringinference. The two-step hierarchical design also enables content control viathe lyric outline, a much-desired feature for democratizing collaborative songcreation. Experimental results show that our model can generate high-qualitylyrics that are more on-topic, singable, intelligible, and coherent than strongbaselines, for example SongMASS, a SOTA model trained on a parallel dataset,with a 24% relative overall quality improvement based on human ratings.</description><author>Yufei Tian, Anjali Narayan-Chen, Shereen Oraby, Alessandra Cervone, Gunnar Sigurdsson, Chenyang Tao, Wenbo Zhao, Yiwen Chen, Tagyoung Chung, Jing Huang, Nanyun Peng</author><pubDate>Fri, 22 Dec 2023 14:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19228v2</guid></item><item><title>Computational Semantics and Evaluation Benchmark for Interrogative Sentences via Combinatory Categorial Grammar</title><link>http://arxiv.org/abs/2312.14737v1</link><description>We present a compositional semantics for various types of polar questions andwh-questions within the framework of Combinatory Categorial Grammar (CCG). Toassess the explanatory power of our proposed analysis, we introduce aquestion-answering dataset QSEM specifically designed to evaluate the semanticsof interrogative sentences. We implement our analysis using existing CCGparsers and conduct evaluations using the dataset. Through the evaluation, wehave obtained annotated data with CCG trees and semantic representations forabout half of the samples included in QSEM. Furthermore, we discuss thediscrepancy between the theoretical capacity of CCG and the capabilities ofexisting CCG parsers.</description><author>Hayate Funakura, Koji Mineshima</author><pubDate>Fri, 22 Dec 2023 14:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14737v1</guid></item><item><title>Effects of cavity nonlinearities and linear losses on silicon microring-based reservoir computing</title><link>http://arxiv.org/abs/2310.09433v2</link><description>Microring resonators (MRRs) are promising devices for time-delay photonicreservoir computing, but the impact of the different physical effects takingplace in the MRRs on the reservoir computing performance is yet to be fullyunderstood. We numerically analyze the impact of linear losses as well asthermo-optic and free-carrier effects relaxation times on the prediction errorof the time-series task NARMA-10. We demonstrate the existence of threeregions, defined by the input power and the frequency detuning between theoptical source and the microring resonance, that reveal the cavity transitionfrom linear to nonlinear regimes. One of these regions offers very low error intime-series prediction under relatively low input power and number of nodeswhile the other regions either lack nonlinearity or become unstable. This studyprovides insight into the design of the MRR and the optimization of itsphysical properties for improving the prediction performance of time-delayreservoir computing.</description><author>Bernard J. Giron Castro, Christophe Peucheret, Darko Zibar, Francesco Da Ros</author><pubDate>Fri, 22 Dec 2023 14:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09433v2</guid></item><item><title>Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives</title><link>http://arxiv.org/abs/2312.13434v2</link><description>Cognitive diagnosis seeks to estimate the cognitive states of students byexploring their logged practice quiz data. It plays a pivotal role inpersonalized learning guidance within intelligent education systems. In thispaper, we focus on an important, practical, yet often underexplored task:domain-level zero-shot cognitive diagnosis (DZCD), which arises due to theabsence of student practice logs in newly launched domains. Recent cross-domaindiagnostic models have been demonstrated to be a promising strategy for DZCD.These methods primarily focus on how to transfer student states across domains.However, they might inadvertently incorporate non-transferable information intostudent representations, thereby limiting the efficacy of knowledge transfer.To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitivediagnosis framework via one batch of early-bird students towards threediagnostic objectives. Our approach initiates with pre-training a diagnosismodel with dual regularizers, which decouples student states into domain-sharedand domain-specific parts. The shared cognitive signals can be transferred tothe target domain, enriching the cognitive priors for the new domain, whichensures the cognitive state propagation objective. Subsequently, we devise astrategy to generate simulated practice logs for cold-start students throughanalyzing the behavioral patterns from early-bird students, fulfilling thedomain-adaption goal. Consequently, we refine the cognitive states ofcold-start students as diagnostic outcomes via virtual data, aligning with thediagnosis-oriented goal. Finally, extensive experiments on six real-worlddatasets highlight the efficacy of our model for DZCD and its practicalapplication in question recommendation.</description><author>Weibo Gao, Qi Liu, Hao Wang, Linan Yue, Haoyang Bi, Yin Gu, Fangzhou Yao, Zheng Zhang, Xin Li, Yuanjing He</author><pubDate>Fri, 22 Dec 2023 14:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13434v2</guid></item><item><title>Harnessing Diffusion Models for Visual Perception with Meta Prompts</title><link>http://arxiv.org/abs/2312.14733v1</link><description>The issue of generative pretraining for vision models has persisted as along-standing conundrum. At present, the text-to-image (T2I) diffusion modeldemonstrates remarkable proficiency in generating high-definition imagesmatching textual inputs, a feat made possible through its pre-training onlarge-scale image-text pairs. This leads to a natural inquiry: can diffusionmodels be utilized to tackle visual perception tasks? In this paper, we proposea simple yet effective scheme to harness a diffusion model for visualperception tasks. Our key insight is to introduce learnable embeddings (metaprompts) to the pre-trained diffusion models to extract proper features forperception. The effect of meta prompts are two-fold. First, as a directreplacement of the text embeddings in the T2I models, it can activatetask-relevant features during feature extraction. Second, it will be used tore-arrange the extracted features to ensures that the model focuses on the mostpertinent features for the task on hand. Additionally, we design a recurrentrefinement training strategy that fully leverages the property of diffusionmodels, thereby yielding stronger visual features. Extensive experiments acrossvarious benchmarks validate the effectiveness of our approach. Our approachachieves new performance records in depth estimation tasks on NYU depth V2 andKITTI, and in semantic segmentation task on CityScapes. Concurrently, theproposed method attains results comparable to the current state-of-the-art insemantic segmentation on ADE20K and pose estimation on COCO datasets, furtherexemplifying its robustness and versatility.</description><author>Qiang Wan, Zilong Huang, Bingyi Kang, Jiashi Feng, Li Zhang</author><pubDate>Fri, 22 Dec 2023 14:40:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14733v1</guid></item><item><title>Images in Discrete Choice Modeling: Addressing Data Isomorphism in Multi-Modality Inputs</title><link>http://arxiv.org/abs/2312.14724v1</link><description>This paper explores the intersection of Discrete Choice Modeling (DCM) andmachine learning, focusing on the integration of image data into DCM's utilityfunctions and its impact on model interpretability. We investigate theconsequences of embedding high-dimensional image data that shares isomorphicinformation with traditional tabular inputs within a DCM framework. Our studyreveals that neural network (NN) components learn and replicate tabularvariable representations from images when co-occurrences exist, therebycompromising the interpretability of DCM parameters. We propose and benchmarktwo methodologies to address this challenge: architectural design adjustmentsto segregate redundant information, and isomorphic information mitigationthrough source information masking and inpainting. Our experiments, conductedon a semi-synthetic dataset, demonstrate that while architectural modificationsprove inconclusive, direct mitigation at the data source shows to be a moreeffective strategy in maintaining the integrity of DCM's interpretableparameters. The paper concludes with insights into the applicability of ourfindings in real-world settings and discusses the implications for futureresearch in hybrid modeling that combines complex data modalities. Full controlof tabular and image data congruence is attained by using the MIT moral machinedataset, and both inputs are merged into a choice model by deploying theLearning Multinomial Logit (L-MNL) framework.</description><author>Brian Sifringer, Alexandre Alahi</author><pubDate>Fri, 22 Dec 2023 14:33:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14724v1</guid></item><item><title>Gerrymandering Planar Graphs</title><link>http://arxiv.org/abs/2312.14721v1</link><description>We study the computational complexity of the map redistricting problem(gerrymandering). Mathematically, the electoral district designer(gerrymanderer) attempts to partition a weighted graph into $k$ connectedcomponents (districts) such that its candidate (party) wins as many districtsas possible. Prior work has principally concerned the special cases where thegraph is a path or a tree. Our focus concerns the realistic case where thegraph is planar. We prove that the gerrymandering problem is solvable inpolynomial time in $\lambda$-outerplanar graphs, when the number of candidatesand $\lambda$ are constants and the vertex weights (voting weights) arepolynomially bounded. In contrast, the problem is NP-complete in general planargraphs even with just two candidates. This motivates the study of approximationalgorithms for gerrymandering planar graphs. However, when the number ofcandidates is large, we prove it is hard to distinguish between instances wherethe gerrymanderer cannot win a single district and instances where thegerrymanderer can win at least one district. This immediately implies that theredistricting problem is inapproximable in polynomial time in planar graphs,unless P=NP. This conclusion appears terminal for the design of goodapproximation algorithms -- but it is not. The inapproximability bound can becircumvented as it only applies when the maximum number of districts thegerrymanderer can win is extremely small, say one. Indeed, for a fixed numberof candidates, our main result is that there is a constant factor approximationalgorithm for redistricting unweighted planar graphs, provided the optimalvalue is a large enough constant.</description><author>Jack Dippel, Max Dupré la Tour, April Niu, Adrian Vetta</author><pubDate>Fri, 22 Dec 2023 14:31:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14721v1</guid></item><item><title>Differentiable JPEG: The Devil is in the Details</title><link>http://arxiv.org/abs/2309.06978v4</link><description>JPEG remains one of the most widespread lossy image coding methods. However,the non-differentiable nature of JPEG restricts the application in deeplearning pipelines. Several differentiable approximations of JPEG have recentlybeen proposed to address this issue. This paper conducts a comprehensive reviewof existing diff. JPEG approaches and identifies critical details that havebeen missed by previous methods. To this end, we propose a novel diff. JPEGapproach, overcoming previous limitations. Our approach is differentiablew.r.t. the input image, the JPEG quality, the quantization tables, and thecolor conversion parameters. We evaluate the forward and backward performanceof our diff. JPEG approach against existing methods. Additionally, extensiveablations are performed to evaluate crucial design choices. Our proposed diff.JPEG resembles the (non-diff.) reference implementation best, significantlysurpassing the recent-best diff. approach by $3.47$dB (PSNR) on average. Forstrong compression rates, we can even improve PSNR by $9.51$dB. Strongadversarial attack results are yielded by our diff. JPEG, demonstrating theeffective gradient approximation. Our code is available athttps://github.com/necla-ml/Diff-JPEG.</description><author>Christoph Reich, Biplob Debnath, Deep Patel, Srimat Chakradhar</author><pubDate>Fri, 22 Dec 2023 14:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06978v4</guid></item><item><title>Inverse Transfer Multiobjective Optimization</title><link>http://arxiv.org/abs/2312.14713v1</link><description>Transfer optimization enables data-efficient optimization of a target task byleveraging experiential priors from related source tasks. This is especiallyuseful in multiobjective optimization settings where a set of trade-offsolutions is sought under tight evaluation budgets. In this paper, we introducea novel concept of inverse transfer in multiobjective optimization. Inversetransfer stands out by employing probabilistic inverse models to mapperformance vectors in the objective space to population search distributionsin task-specific decision space, facilitating knowledge transfer throughobjective space unification. Building upon this idea, we introduce the firstInverse Transfer Multiobjective Evolutionary Optimizer (invTrEMO). A keyhighlight of invTrEMO is its ability to harness the common objective functionsprevalent in many application areas, even when decision spaces do not preciselyalign between tasks. This allows invTrEMO to uniquely and effectively utilizeinformation from heterogeneous source tasks as well. Furthermore, invTrEMOyields high-precision inverse models as a significant byproduct, enabling thegeneration of tailored solutions on-demand based on user preferences. Empiricalstudies on multi- and many-objective benchmark problems, as well as a practicalcase study, showcase the faster convergence rate and modelling accuracy of theinvTrEMO relative to state-of-the-art evolutionary and Bayesian optimizationalgorithms. The source code of the invTrEMO is made available athttps://github.com/LiuJ-2023/invTrEMO.</description><author>Jiao Liu, Abhishek Gupta, Yew-Soon Ong</author><pubDate>Fri, 22 Dec 2023 14:12:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14713v1</guid></item><item><title>Q-Segment: Segmenting Images In-Sensor for Vessel-Based Medical Diagnosis</title><link>http://arxiv.org/abs/2312.09854v2</link><description>This paper addresses the growing interest in deploying deep learning modelsdirectly in-sensor. We present "Q-Segment", a quantized real-time segmentationalgorithm, and conduct a comprehensive evaluation on a low-power edge visionplatform with an in-sensors processor, the Sony IMX500. One of the main goalsof the model is to achieve end-to-end image segmentation for vessel-basedmedical diagnosis. Deployed on the IMX500 platform, Q-Segment achievesultra-low inference time in-sensor only 0.23 ms and power consumption of only72mW. We compare the proposed network with state-of-the-art models, both floatand quantized, demonstrating that the proposed solution outperforms existingnetworks on various platforms in computing efficiency, e.g., by a factor of 75xcompared to ERFNet. The network employs an encoder-decoder structure with skipconnections, and results in a binary accuracy of 97.25% and an Area Under theReceiver Operating Characteristic Curve (AUC) of 96.97% on the CHASE dataset.We also present a comparison of the IMX500 processing core with the SonySpresense, a low-power multi-core ARM Cortex-M microcontroller, and asingle-core ARM Cortex-M4 showing that it can achieve in-sensor processing withend-to-end low latency (17 ms) and power concumption (254mW). This researchcontributes valuable insights into edge-based image segmentation, laying thefoundation for efficient algorithms tailored to low-power environments.</description><author>Pietro Bonazzi, Julian Moosmann, Yawei Li, Sizhen Bian, Michele Magno</author><pubDate>Fri, 22 Dec 2023 14:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09854v2</guid></item><item><title>Can Machines Learn Robustly, Privately, and Efficiently?</title><link>http://arxiv.org/abs/2312.14712v1</link><description>The success of machine learning (ML) applications relies on vast datasets anddistributed architectures, which, as they grow, present challenges for ML. Inreal-world scenarios, where data often contains sensitive information, issueslike data poisoning and hardware failures are common. Ensuring privacy androbustness is vital for the broad adoption of ML in public life. This paperexamines the costs associated with achieving these objectives in distributedarchitectures. We overview the meanings of privacy and robustness indistributed ML, and clarify how they can be achieved efficiently in isolation.However, we contend that the integration of these objectives entails a notablecompromise in computational efficiency. We delve into this intricate balance,exploring the challenges and solutions for privacy, robustness, andcomputational efficiency in ML applications.</description><author>Youssef Allouah, Rachid Guerraoui, John Stephan</author><pubDate>Fri, 22 Dec 2023 14:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14712v1</guid></item><item><title>How Far Have We Gone in Vulnerability Detection Using Large Language Models</title><link>http://arxiv.org/abs/2311.12420v3</link><description>As software becomes increasingly complex and prone to vulnerabilities,automated vulnerability detection is critically important, yet challenging.Given the significant successes of large language models (LLMs) in varioustasks, there is growing anticipation of their efficacy in vulnerabilitydetection. However, a quantitative understanding of their potential invulnerability detection is still missing. To bridge this gap, we introduce acomprehensive vulnerability benchmark VulBench. This benchmark aggregateshigh-quality data from a wide range of CTF (Capture-the-Flag) challenges andreal-world applications, with annotations for each vulnerable functiondetailing the vulnerability type and its root cause. Through our experimentsencompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based modelsand static analyzers, we find that several LLMs outperform traditional deeplearning approaches in vulnerability detection, revealing an untapped potentialin LLMs. This work contributes to the understanding and utilization of LLMs forenhanced software security.</description><author>Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, Chao Zhang</author><pubDate>Fri, 22 Dec 2023 14:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12420v3</guid></item><item><title>Balancing the Style-Content Trade-Off in Sentiment Transfer Using Polarity-Aware Denoising</title><link>http://arxiv.org/abs/2312.14708v1</link><description>Text sentiment transfer aims to flip the sentiment polarity of a sentence(positive to negative or vice versa) while preserving its sentiment-independentcontent. Although current models show good results at changing the sentiment,content preservation in transferred sentences is insufficient. In this paper,we present a sentiment transfer model based on polarity-aware denoising, whichaccurately controls the sentiment attributes in generated text, preserving thecontent to a great extent and helping to balance the style-content trade-off.Our proposed model is structured around two key stages in the sentimenttransfer process: better representation learning using a shared encoder andsentiment-controlled generation using separate sentiment-specific decoders.Empirical results show that our methods outperforms state-of-the-art baselinesin terms of content preservation while staying competitive in terms of styletransfer accuracy and fluency.</description><author>Sourabrata Mukherjee, Zdeněk Kasner, Ondřej Dušek</author><pubDate>Fri, 22 Dec 2023 14:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14708v1</guid></item><item><title>BonnBeetClouds3D: A Dataset Towards Point Cloud-based Organ-level Phenotyping of Sugar Beet Plants under Field Conditions</title><link>http://arxiv.org/abs/2312.14706v1</link><description>Agricultural production is facing severe challenges in the next decadesinduced by climate change and the need for sustainability, reducing its impacton the environment. Advancements in field management through non-chemicalweeding by robots in combination with monitoring of crops by autonomousunmanned aerial vehicles (UAVs) and breeding of novel and more resilient cropvarieties are helpful to address these challenges. The analysis of planttraits, called phenotyping, is an essential activity in plant breeding, ithowever involves a great amount of manual labor. With this paper, we addressthe problem of automatic fine-grained organ-level geometric analysis needed forprecision phenotyping. As the availability of real-world data in this domain isrelatively scarce, we propose a novel dataset that was acquired using UAVscapturing high-resolution images of a real breeding trial containing 48 plantvarieties and therefore covering great morphological and appearance diversity.This enables the development of approaches for autonomous phenotyping thatgeneralize well to different varieties. Based on overlapping high-resolutionimages from multiple viewing angles, we compute photogrammetric dense pointclouds and provide detailed and accurate point-wise labels for plants, leaves,and salient points as the tip and the base. Additionally, we includemeasurements of phenotypic traits performed by experts from the German FederalPlant Variety Office on the real plants, allowing the evaluation of newapproaches not only on segmentation and keypoint detection but also directly onthe downstream tasks. The provided labeled point clouds enable fine-grainedplant analysis and support further progress in the development of automaticphenotyping approaches, but also enable further research in surfacereconstruction, point cloud completion, and semantic interpretation of pointclouds.</description><author>Elias Marks, Jonas Bömer, Federico Magistri, Anurag Sah, Jens Behley, Cyrill Stachniss</author><pubDate>Fri, 22 Dec 2023 14:06:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14706v1</guid></item><item><title>SCUNet++: Assessment of Pulmonary Embolism CT Image Segmentation Leveraging Swin-UNet and CNN Bottleneck Hybrid Architecture with Multi-Fusion Dense Skip Connection</title><link>http://arxiv.org/abs/2312.14705v1</link><description>Pulmonary embolism (PE) is a prevalent lung disease that can lead to rightventricular hypertrophy and failure in severe cases, ranking second in severityonly to myocardial infarction and sudden death. Pulmonary artery CT angiography(CTPA) is a widely used diagnostic method for PE. However, PE detectionpresents challenges in clinical practice due to limitations in imagingtechnology. CTPA can produce noises similar to PE, making confirmation of itspresence time-consuming and prone to overdiagnosis. Nevertheless, thetraditional segmentation method of PE can not fully consider the hierarchicalstructure of features, local and global spatial features of PE CT images. Inthis paper, we propose an automatic PE segmentation method called SCUNet++(Swin Conv UNet++). This method incorporates multiple fusion dense skipconnections between the encoder and decoder, utilizing the Swin Transformer asthe encoder. And fuses features of different scales in the decoder subnetworkto compensate for spatial information loss caused by the inevitabledownsampling in Swin-UNet or other state-of-the-art methods, effectivelysolving the above problem. We provide a theoretical analysis of this method indetail and validate it on publicly available PE CT image datasets FUMPE andCAD-PE. The experimental results indicate that our proposed method achieved aDice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95thpercentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% andan HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that ourmethod exhibits strong performance in PE segmentation tasks, potentiallyenhancing the accuracy of automatic segmentation of PE and providing a powerfuldiagnostic tool for clinical physicians. Our source code and new FUMPE datasetare available at https://github.com/JustlfC03/SCUNet-plusplus.</description><author>Yifei Chen, Binfeng Zou, Zhaoxin Guo, Yiyu Huang, Yifan Huang, Feiwei Qin, Qinhai Li, Changmiao Wang</author><pubDate>Fri, 22 Dec 2023 14:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14705v1</guid></item><item><title>Time-changed normalizing flows for accurate SDE modeling</title><link>http://arxiv.org/abs/2312.14698v1</link><description>The generative paradigm has become increasingly important in machine learningand deep learning models. Among popular generative models are normalizingflows, which enable exact likelihood estimation by transforming a basedistribution through diffeomorphic transformations. Extending the normalizingflow framework to handle time-indexed flows gave dynamic normalizing flows, apowerful tool to model time series, stochastic processes, and neural stochasticdifferential equations (SDEs). In this work, we propose a novel variant ofdynamic normalizing flows, a Time Changed Normalizing Flow (TCNF), based ontime deformation of a Brownian motion which constitutes a versatile andextensive family of Gaussian processes. This approach enables us to effectivelymodel some SDEs, that cannot be modeled otherwise, including standard ones suchas the well-known Ornstein-Uhlenbeck process, and generalizes priormethodologies, leading to improved results and better inference and predictioncapability.</description><author>Naoufal El Bekri, Lucas Drumetz, Franck Vermet</author><pubDate>Fri, 22 Dec 2023 13:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14698v1</guid></item><item><title>Pola4All: survey of polarimetric applications and an open-source toolkit to analyze polarization</title><link>http://arxiv.org/abs/2312.14697v1</link><description>Polarization information of the light can provide rich cues for computervision and scene understanding tasks, such as the type of material, pose, andshape of the objects. With the advent of new and cheap polarimetric sensors,this imaging modality is becoming accessible to a wider public for solvingproblems such as pose estimation, 3D reconstruction, underwater navigation, anddepth estimation. However, we observe several limitations regarding the usageof this sensorial modality, as well as a lack of standards and publiclyavailable tools to analyze polarization images. Furthermore, althoughpolarization camera manufacturers usually provide acquisition tools tointerface with their cameras, they rarely include processing algorithms thatmake use of the polarization information. In this paper, we review recentadvances in applications that involve polarization imaging, including acomprehensive survey of recent advances on polarization for vision and roboticsperception tasks. We also introduce a complete software toolkit that providescommon standards to communicate with and process information from most of theexisting micro-grid polarization cameras on the market. The toolkit alsoimplements several image processing algorithms for this modality, and it ispublicly available on GitHub: https://github.com/vibot-lab/Pola4all_JEI_2023.</description><author>Joaquin Rodriguez, Lew-Fock-Chong Lew-Yan-Voon, Renato Martins, Olivier Morel</author><pubDate>Fri, 22 Dec 2023 13:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14697v1</guid></item><item><title>AutoNeRF: Training Implicit Scene Representations with Autonomous Agents</title><link>http://arxiv.org/abs/2304.11241v2</link><description>Implicit representations such as Neural Radiance Fields (NeRF) have beenshown to be very effective at novel view synthesis. However, these modelstypically require manual and careful human data collection for training. Inthis paper, we present AutoNeRF, a method to collect data required to trainNeRFs using autonomous embodied agents. Our method allows an agent to explorean unseen environment efficiently and use the experience to build an implicitmap representation autonomously. We compare the impact of different explorationstrategies including handcrafted frontier-based exploration, end-to-end andmodular approaches composed of trained high-level planners and classicallow-level path followers. We train these models with different reward functionstailored to this problem and evaluate the quality of the learnedrepresentations on four different downstream tasks: classical viewpointrendering, map reconstruction, planning, and pose refinement. Empirical resultsshow that NeRFs can be trained on actively collected data using just a singleepisode of experience in an unseen environment, and can be used for severaldownstream robotic tasks, and that modular trained exploration modelsoutperform other classical and end-to-end baselines. Finally, we show thatAutoNeRF can reconstruct large-scale scenes, and is thus a useful tool toperform scene-specific adaptation as the produced 3D environment models can beloaded into a simulator to fine-tune a policy of interest.</description><author>Pierre Marza, Laetitia Matignon, Olivier Simonin, Dhruv Batra, Christian Wolf, Devendra Singh Chaplot</author><pubDate>Fri, 22 Dec 2023 13:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11241v2</guid></item><item><title>RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation</title><link>http://arxiv.org/abs/2306.11706v2</link><description>The ability to leverage heterogeneous robotic experience from differentrobots and tasks to quickly master novel skills and embodiments has thepotential to transform robot learning. Inspired by recent advances infoundation models for vision and language, we propose a multi-embodiment,multi-task generalist agent for robotic manipulation. This agent, namedRoboCat, is a visual goal-conditioned decision transformer capable of consumingaction-labelled visual experience. This data spans a large repertoire of motorcontrol skills from simulated and real robotic arms with varying sets ofobservations and actions. With RoboCat, we demonstrate the ability togeneralise to new tasks and robots, both zero-shot as well as throughadaptation using only 100-1000 examples for the target task. We also show how atrained model itself can be used to generate data for subsequent trainingiterations, thus providing a basic building block for an autonomous improvementloop. We investigate the agent's capabilities, with large-scale evaluationsboth in simulation and on three different real robot embodiments. We find thatas we grow and diversify its training data, RoboCat not only shows signs ofcross-task transfer, but also becomes more efficient at adapting to new tasks.</description><author>Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Żołna, Scott Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, Nicolas Heess</author><pubDate>Fri, 22 Dec 2023 13:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11706v2</guid></item><item><title>A Mathematical Guide to Operator Learning</title><link>http://arxiv.org/abs/2312.14688v1</link><description>Operator learning aims to discover properties of an underlying dynamicalsystem or partial differential equation (PDE) from data. Here, we present astep-by-step guide to operator learning. We explain the types of problems andPDEs amenable to operator learning, discuss various neural networkarchitectures, and explain how to employ numerical PDE solvers effectively. Wealso give advice on how to create and manage training data and conductoptimization. We offer intuition behind the various neural networkarchitectures employed in operator learning by motivating them from thepoint-of-view of numerical linear algebra.</description><author>Nicolas Boullé, Alex Townsend</author><pubDate>Fri, 22 Dec 2023 13:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14688v1</guid></item><item><title>Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval</title><link>http://arxiv.org/abs/2309.17093v2</link><description>Cross-modal Retrieval methods build similarity relations between vision andlanguage modalities by jointly learning a common representation space. However,the predictions are often unreliable due to the Aleatoric uncertainty, which isinduced by low-quality data, e.g., corrupt images, fast-paced videos, andnon-detailed texts. In this paper, we propose a novel Prototype-based AleatoricUncertainty Quantification (PAU) framework to provide trustworthy predictionsby quantifying the uncertainty arisen from the inherent data ambiguity.Concretely, we first construct a set of various learnable prototypes for eachmodality to represent the entire semantics subspace. Then Dempster-ShaferTheory and Subjective Logic Theory are utilized to build an evidentialtheoretical framework by associating evidence with Dirichlet Distributionparameters. The PAU model induces accurate uncertainty and reliable predictionsfor cross-modal retrieval. Extensive experiments are performed on four majorbenchmark datasets of MSR-VTT, MSVD, DiDeMo, and MS-COCO, demonstrating theeffectiveness of our method. The code is accessible athttps://github.com/leolee99/PAU.</description><author>Hao Li, Jingkuan Song, Lianli Gao, Xiaosu Zhu, Heng Tao Shen</author><pubDate>Fri, 22 Dec 2023 13:37:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17093v2</guid></item><item><title>Kernel Heterogeneity Improves Sparseness of Natural Images Representations</title><link>http://arxiv.org/abs/2312.14685v1</link><description>Both biological and artificial neural networks inherently balance theirperformance with their operational cost, which balances their computationalabilities. Typically, an efficient neuromorphic neural network is one thatlearns representations that reduce the redundancies and dimensionality of itsinput. This is for instance achieved in sparse coding, and sparserepresentations derived from natural images yield representations that areheterogeneous, both in their sampling of input features and in the variance ofthose features. Here, we investigated the connection between natural images'structure, particularly oriented features, and their corresponding sparsecodes. We showed that representations of input features scattered acrossmultiple levels of variance substantially improve the sparseness and resilienceof sparse codes, at the cost of reconstruction performance. This echoes thestructure of the model's input, allowing to account for the heterogeneouslyaleatoric structures of natural images. We demonstrate that learning kernelfrom natural images produces heterogeneity by balancing between approximate anddense representations, which improves all reconstruction metrics. Using aparametrized control of the kernels' heterogeneity used by a convolutionalsparse coding algorithm, we show that heterogeneity emphasizes sparseness,while homogeneity improves representation granularity. In a broader context,these encoding strategy can serve as inputs to deep convolutional neuralnetworks. We prove that such variance-encoded sparse image datasets enhancecomputational efficiency, emphasizing the benefits of kernel heterogeneity toleverage naturalistic and variant input structures and possible applications toimprove the throughput of neuromorphic hardware.</description><author>Hugo J. Ladret, Christian Casanova, Laurent Udo Perrinet</author><pubDate>Fri, 22 Dec 2023 13:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14685v1</guid></item><item><title>Engineered Ordinary Differential Equations as Classification Algorithm (EODECA): thorough characterization and testing</title><link>http://arxiv.org/abs/2312.14681v1</link><description>EODECA (Engineered Ordinary Differential Equations as ClassificationAlgorithm) is a novel approach at the intersection of machine learning anddynamical systems theory, presenting a unique framework for classificationtasks [1]. This method stands out with its dynamical system structure,utilizing ordinary differential equations (ODEs) to efficiently handle complexclassification challenges. The paper delves into EODECA's dynamical properties,emphasizing its resilience against random perturbations and robust performanceacross various classification scenarios. Notably, EODECA's design incorporatesthe ability to embed stable attractors in the phase space, enhancingreliability and allowing for reversible dynamics. In this paper, we carry out acomprehensive analysis by expanding on the work [1], and employing a Eulerdiscretization scheme. In particular, we evaluate EODECA's performance acrossfive distinct classification problems, examining its adaptability andefficiency. Significantly, we demonstrate EODECA's effectiveness on the MNISTand Fashion MNIST datasets, achieving impressive accuracies of $98.06\%$ and$88.21\%$, respectively. These results are comparable to those of a multi-layerperceptron (MLP), underscoring EODECA's potential in complex data processingtasks. We further explore the model's learning journey, assessing its evolutionin both pre and post training environments and highlighting its ability tonavigate towards stable attractors. The study also investigates theinvertibility of EODECA, shedding light on its decision-making processes andinternal workings. This paper presents a significant step towards a moretransparent and robust machine learning paradigm, bridging the gap betweenmachine learning algorithms and dynamical systems methodologies.</description><author>Raffaele Marino, Lorenzo Buffoni, Lorenzo Chicchi, Lorenzo Giambagli, Duccio Fanelli</author><pubDate>Fri, 22 Dec 2023 13:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14681v1</guid></item><item><title>MEAOD: Model Extraction Attack against Object Detectors</title><link>http://arxiv.org/abs/2312.14677v1</link><description>The widespread use of deep learning technology across various industries hasmade deep neural network models highly valuable and, as a result, attractivetargets for potential attackers. Model extraction attacks, particularlyquery-based model extraction attacks, allow attackers to replicate a substitutemodel with comparable functionality to the victim model and present asignificant threat to the confidentiality and security of MLaaS platforms.While many studies have explored threats of model extraction attacks againstclassification models in recent years, object detection models, which are morefrequently used in real-world scenarios, have received less attention. In thispaper, we investigate the challenges and feasibility of query-based modelextraction attacks against object detection models and propose an effectiveattack method called MEAOD. It selects samples from the attacker-possesseddataset to construct an efficient query dataset using active learning andenhances the categories with insufficient objects. We additionally improve theextraction effectiveness by updating the annotations of the query dataset.According to our gray-box and black-box scenarios experiments, we achieve anextraction performance of over 70% under the given condition of a 10k querybudget.</description><author>Zeyu Li, Chenghui Shi, Yuwen Pu, Xuhong Zhang, Yu Li, Jinbao Li, Shouling Ji</author><pubDate>Fri, 22 Dec 2023 13:28:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14677v1</guid></item><item><title>In-Context Probing: Toward Building Robust Classifiers via Probing Large Language Models</title><link>http://arxiv.org/abs/2305.14171v3</link><description>Large language models are able to learn new tasks in context, where they areprovided with instructions and a few annotated examples. However, theeffectiveness of in-context learning is dependent on the provided context, andthe performance on a downstream task can vary considerably, depending on theinstruction. Importantly, such dependency on the context can surface inunpredictable ways, e.g., a seemingly more informative instruction might leadto a worse performance. In this paper, we propose an alternative approach,which we term In-Context Probing (ICP). Similar to in-context learning, wecontextualize the representation of the input with an instruction, but insteadof decoding the output prediction, we probe the contextualized representationto predict the label. Through a series of experiments on a diverse set ofclassification tasks, we show that in-context probing is significantly morerobust to changes in instructions. We further show that ICP performscompetitive or superior to finetuning and can be particularly helpful to buildclassifiers on top of smaller models, with less than a hundred trainingexamples.</description><author>Afra Amini, Massimiliano Ciaramita</author><pubDate>Fri, 22 Dec 2023 13:27:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14171v3</guid></item><item><title>Explainability as statistical inference</title><link>http://arxiv.org/abs/2212.03131v2</link><description>A wide variety of model explanation approaches have been proposed in recentyears, all guided by very different rationales and heuristics. In this paper,we take a new route and cast interpretability as a statistical inferenceproblem. We propose a general deep probabilistic model designed to produceinterpretable predictions. The model parameters can be learned via maximumlikelihood, and the method can be adapted to any predictor network architectureand any type of prediction problem. Our method is a case of amortizedinterpretability models, where a neural network is used as a selector to allowfor fast interpretation at inference time. Several popular interpretabilitymethods are shown to be particular cases of regularised maximum likelihood forour general model. We propose new datasets with ground truth selection whichallow for the evaluation of the features importance map. Using these datasets,we show experimentally that using multiple imputation provides more reasonableinterpretations.</description><author>Hugo Henri Joseph Senetaire, Damien Garreau, Jes Frellsen, Pierre-Alexandre Mattei</author><pubDate>Fri, 22 Dec 2023 13:23:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.03131v2</guid></item><item><title>Reconciling Predictive and Statistical Parity: A Causal Approach</title><link>http://arxiv.org/abs/2306.05059v2</link><description>Since the rise of fair machine learning as a critical field of inquiry, manydifferent notions on how to quantify and measure discrimination have beenproposed in the literature. Some of these notions, however, were shown to bemutually incompatible. Such findings make it appear that numerous differentkinds of fairness exist, thereby making a consensus on the appropriate measureof fairness harder to reach, hindering the applications of these tools inpractice. In this paper, we investigate one of these key impossibility resultsthat relates the notions of statistical and predictive parity. Specifically, wederive a new causal decomposition formula for the fairness measures associatedwith predictive parity, and obtain a novel insight into how this criterion isrelated to statistical parity through the legal doctrines of disparatetreatment, disparate impact, and the notion of business necessity. Our resultsshow that through a more careful causal analysis, the notions of statisticaland predictive parity are not really mutually exclusive, but complementary andspanning a spectrum of fairness notions through the concept of businessnecessity. Finally, we demonstrate the importance of our findings on areal-world example.</description><author>Drago Plecko, Elias Bareinboim</author><pubDate>Fri, 22 Dec 2023 13:22:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05059v2</guid></item><item><title>Zero-shot Causal Graph Extrapolation from Text via LLMs</title><link>http://arxiv.org/abs/2312.14670v1</link><description>We evaluate the ability of large language models (LLMs) to infer causalrelations from natural language. Compared to traditional natural languageprocessing and deep learning techniques, LLMs show competitive performance in abenchmark of pairwise relations without needing (explicit) training samples.This motivates us to extend our approach to extrapolating causal graphs throughiterated pairwise queries. We perform a preliminary analysis on a benchmark ofbiomedical abstracts with ground-truth causal graphs validated by experts. Theresults are promising and support the adoption of LLMs for such a crucial stepin causal inference, especially in medical domains, where the amount ofscientific text to analyse might be huge, and the causal statements are oftenimplicit.</description><author>Alessandro Antonucci, Gregorio Piqué, Marco Zaffalon</author><pubDate>Fri, 22 Dec 2023 13:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14670v1</guid></item><item><title>Aligning Language Models with Human Preferences via a Bayesian Approach</title><link>http://arxiv.org/abs/2310.05782v2</link><description>In the quest to advance human-centric natural language generation (NLG)systems, ensuring alignment between NLG models and human preferences iscrucial. For this alignment, current popular methods leverage a reinforcementlearning (RL) approach with a reward model trained on feedback from humans.However, inherent disagreements due to the subjective nature of humanpreferences pose a significant challenge for training the reward model,resulting in a deterioration of the NLG performance. To tackle this issue,previous approaches typically rely on majority voting or averaging toconsolidate multiple inconsistent preferences into a merged one. Althoughstraightforward to understand and execute, such methods suffer from aninability to capture the nuanced degrees of disaggregation among humans and mayonly represent a specialized subset of individuals, thereby lacking the abilityto quantitatively disclose the universality of human preferences. To addressthis challenge, this paper proposes a novel approach, which employs a Bayesianframework to account for the distribution of disagreements among humanpreferences as training a preference model, and names it as d-PM. Besides,considering the RL strategy's inefficient and complex training process over thetraining efficiency, we further propose utilizing the contrastive learningstrategy to train the NLG model with the preference scores derived from thed-PM model. Extensive experiments on two human-centric NLG tasks, i.e.,emotional support conversation and integrity "Rule-of-Thumb" generation, showthat our method consistently exceeds previous SOTA models in both automatic andhuman evaluations.</description><author>Jiashuo Wang, Haozhao Wang, Shichao Sun, Wenjie Li</author><pubDate>Fri, 22 Dec 2023 13:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05782v2</guid></item><item><title>Token-Level Contrastive Learning with Modality-Aware Prompting for Multimodal Intent Recognition</title><link>http://arxiv.org/abs/2312.14667v1</link><description>Multimodal intent recognition aims to leverage diverse modalities such asexpressions, body movements and tone of speech to comprehend user's intent,constituting a critical task for understanding human language and behavior inreal-world multimodal scenarios. Nevertheless, the majority of existing methodsignore potential correlations among different modalities and own limitations ineffectively learning semantic features from nonverbal modalities. In thispaper, we introduce a token-level contrastive learning method withmodality-aware prompting (TCL-MAP) to address the above challenges. Toestablish an optimal multimodal semantic environment for text modality, wedevelop a modality-aware prompting module (MAP), which effectively aligns andfuses features from text, video and audio modalities with similarity-basedmodality alignment and cross-modality attention mechanism. Based on themodality-aware prompt and ground truth labels, the proposed token-levelcontrastive learning framework (TCL) constructs augmented samples and employsNT-Xent loss on the label token. Specifically, TCL capitalizes on the optimaltextual semantic insights derived from intent labels to guide the learningprocesses of other modalities in return. Extensive experiments show that ourmethod achieves remarkable improvements compared to state-of-the-art methods.Additionally, ablation analyses demonstrate the superiority of themodality-aware prompt over the handcrafted prompt, which holds substantialsignificance for multimodal prompt learning. The codes are released athttps://github.com/thuiar/TCL-MAP.</description><author>Qianrui Zhou, Hua Xu, Hao Li, Hanlei Zhang, Xiaohan Zhang, Yifan Wang, Kai Gao</author><pubDate>Fri, 22 Dec 2023 13:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14667v1</guid></item><item><title>Density Uncertainty Quantification with NeRF-Ensembles: Impact of Data and Scene Constraints</title><link>http://arxiv.org/abs/2312.14664v1</link><description>In the fields of computer graphics, computer vision and photogrammetry,Neural Radiance Fields (NeRFs) are a major topic driving current research anddevelopment. However, the quality of NeRF-generated 3D scene reconstructionsand subsequent surface reconstructions, heavily relies on the network output,particularly the density. Regarding this critical aspect, we propose to utilizeNeRF-Ensembles that provide a density uncertainty estimate alongside the meandensity. We demonstrate that data constraints such as low-quality images andposes lead to a degradation of the training process, increased densityuncertainty and decreased predicted density. Even with high-quality input data,the density uncertainty varies based on scene constraints such as acquisitionconstellations, occlusions and material properties. NeRF-Ensembles not onlyprovide a tool for quantifying the uncertainty but exhibit two promisingadvantages: Enhanced robustness and artifact removal. Through the utilizationof NeRF-Ensembles instead of single NeRFs, small outliers are removed, yieldinga smoother output with improved completeness of structures. Furthermore,applying percentile-based thresholds on density uncertainty outliers proves tobe effective for the removal of large (foggy) artifacts in post-processing. Weconduct our methodology on 3 different datasets: (i) synthetic benchmarkdataset, (ii) real benchmark dataset, (iii) real data under realistic recordingconditions and sensors.</description><author>Miriam Jäger, Steven Landgraf, Boris Jutzi</author><pubDate>Fri, 22 Dec 2023 13:01:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14664v1</guid></item><item><title>DG-TTA: Out-of-domain medical image segmentation through Domain Generalization and Test-Time Adaptation</title><link>http://arxiv.org/abs/2312.06275v2</link><description>Applying pre-trained medical segmentation models on out-of-domain imagesoften yields predictions of insufficient quality. Several strategies have beenproposed to maintain model performance, such as finetuning or unsupervised- andsource-free domain adaptation. These strategies set restrictive requirementsfor data availability. In this study, we propose to combine domaingeneralization and test-time adaptation to create a highly effective approachfor reusing pre-trained models in unseen target domains. Domain-generalizedpre-training on source data is used to obtain the best initial performance inthe target domain. We introduce the MIND descriptor previously used in imageregistration tasks as a further technique to achieve generalization and presentsuperior performance for small-scale datasets compared to existing approaches.At test-time, high-quality segmentation for every single unseen scan is ensuredby optimizing the model weights for consistency given different imageaugmentations. That way, our method enables separate use of source and targetdata and thus removes current data availability barriers. Moreover, thepresented method is highly modular as it does not require specific modelarchitectures or prior knowledge of involved domains and labels. We demonstratethis by integrating it into the nnUNet, which is currently the most popular andaccurate framework for medical image segmentation. We employ multiple datasetscovering abdominal, cardiac, and lumbar spine scans and compose severalout-of-domain scenarios in this study. We demonstrate that our method, combinedwith pre-trained whole-body CT models, can effectively segment MR images withhigh accuracy in all of the aforementioned scenarios. Open-source code can befound here: https://github.com/multimodallearning/DG-TTA</description><author>Christian Weihsbach, Christian N. Kruse, Alexander Bigalke, Mattias P. Heinrich</author><pubDate>Fri, 22 Dec 2023 13:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06275v2</guid></item><item><title>A mathematical perspective on Transformers</title><link>http://arxiv.org/abs/2312.10794v2</link><description>Transformers play a central role in the inner workings of large languagemodels. We develop a mathematical framework for analyzing Transformers based ontheir interpretation as interacting particle systems, which reveals thatclusters emerge in long time. Our study explores the underlying theory andoffers new perspectives for mathematicians as well as computer scientists.</description><author>Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, Philippe Rigollet</author><pubDate>Fri, 22 Dec 2023 12:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10794v2</guid></item><item><title>Deep Non-Parametric Time Series Forecaster</title><link>http://arxiv.org/abs/2312.14657v1</link><description>This paper presents non-parametric baseline models for time seriesforecasting. Unlike classical forecasting models, the proposed approach doesnot assume any parametric form for the predictive distribution and insteadgenerates predictions by sampling from the empirical distribution according toa tunable strategy. By virtue of this, the model is always able to producereasonable forecasts (i.e., predictions within the observed data range) withoutfail unlike classical models that suffer from numerical stability on some datadistributions. Moreover, we develop a global version of the proposed methodthat automatically learns the sampling strategy by exploiting the informationacross multiple related time series. The empirical evaluation shows that theproposed methods have reasonable and consistent performance across alldatasets, proving them to be strong baselines to be considered in one'sforecasting toolbox.</description><author>Syama Sundar Rangapuram, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, Tim Januschowski</author><pubDate>Fri, 22 Dec 2023 12:46:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14657v1</guid></item><item><title>SAVAE: Leveraging the variational Bayes autoencoder for survival analysis</title><link>http://arxiv.org/abs/2312.14651v1</link><description>As in many fields of medical research, survival analysis has witnessed agrowing interest in the application of deep learning techniques to modelcomplex, high-dimensional, heterogeneous, incomplete, and censored medicaldata. Current methods often make assumptions about the relations between datathat may not be valid in practice. In response, we introduce SAVAE (SurvivalAnalysis Variational Autoencoder), a novel approach based on VariationalAutoencoders. SAVAE contributes significantly to the field by introducing atailored ELBO formulation for survival analysis, supporting various parametricdistributions for covariates and survival time (as long as the log-likelihoodis differentiable). It offers a general method that consistently performs wellon various metrics, demonstrating robustness and stability through differentexperiments. Our proposal effectively estimates time-to-event, accounting forcensoring, covariate interactions, and time-varying risk associations. Wevalidate our model in diverse datasets, including genomic, clinical, anddemographic data, with varying levels of censoring. This approach demonstratescompetitive performance compared to state-of-the-art techniques, as assessed bythe Concordance Index and the Integrated Brier Score. SAVAE also offers aninterpretable model that parametrically models covariates and time. Moreover,its generative architecture facilitates further applications such asclustering, data imputation, and the generation of synthetic patient datathrough latent space inference from survival data.</description><author>Patricia A. Apellániz, Juan Parras, Santiago Zazo</author><pubDate>Fri, 22 Dec 2023 12:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14651v1</guid></item><item><title>Global Occlusion-Aware Transformer for Robust Stereo Matching</title><link>http://arxiv.org/abs/2312.14650v1</link><description>Despite the remarkable progress facilitated by learning-based stereo-matchingalgorithms, the performance in the ill-conditioned regions, such as theoccluded regions, remains a bottleneck. Due to the limited receptive field,existing CNN-based methods struggle to handle these ill-conditioned regionseffectively. To address this issue, this paper introduces a novelattention-based stereo-matching network called Global Occlusion-AwareTransformer (GOAT) to exploit long-range dependency and occlusion-awarenessglobal context for disparity estimation. In the GOAT architecture, a paralleldisparity and occlusion estimation module PDO is proposed to estimate theinitial disparity map and the occlusion mask using a parallel attentionmechanism. To further enhance the disparity estimates in the occluded regions,an occlusion-aware global aggregation module (OGA) is proposed. This moduleaims to refine the disparity in the occluded regions by leveraging restrictedglobal correlation within the focus scope of the occluded areas. Extensiveexperiments were conducted on several public benchmark datasets includingSceneFlow, KITTI 2015, and Middlebury. The results show that the proposed GOATdemonstrates outstanding performance among all benchmarks, particularly in theoccluded regions.</description><author>Zihua Liu, Yizhou Li, Masatoshi Okutomi</author><pubDate>Fri, 22 Dec 2023 12:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14650v1</guid></item></channel></rss>