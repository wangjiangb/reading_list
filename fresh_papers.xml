<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 21 Aug 2023 14:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training</title><link>http://arxiv.org/abs/2308.09718v1</link><description>The rapid advancement of deep learning models often attributes to theirability to leverage massive training data. In contrast, such privilege has notyet fully benefited 3D deep learning, mainly due to the limited availability oflarge-scale 3D datasets. Merging multiple available data sources and lettingthem collaboratively train a single model is a potential solution. However, dueto the large domain gap between 3D point cloud datasets, such mixed supervisioncould adversely affect the model's performance and lead to degeneratedperformance (i.e., negative transfer) compared to single-dataset training. Inview of this challenge, we introduce Point Prompt Training (PPT), a novelframework for multi-dataset synergistic learning in the context of 3Drepresentation learning that supports multiple pre-training paradigms. Based onthis framework, we propose Prompt-driven Normalization, which adapts the modelto different datasets with domain-specific prompts and Language-guidedCategorical Alignment that decently unifies the multiple-dataset label spacesby leveraging the relationship between label text. Extensive experiments verifythat PPT can overcome the negative transfer associated with synergisticlearning and produce generalizable representations. Notably, it achievesstate-of-the-art performance on each dataset using a single weight-shared modelwith supervised multi-dataset training. Moreover, when served as a pre-trainingframework, it outperforms other pre-training approaches regardingrepresentation quality and attains remarkable state-of-the-art performanceacross over ten diverse downstream tasks spanning both indoor and outdoor 3Dscenarios.</description><author>Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, Hengshuang Zhao</author><pubDate>Fri, 18 Aug 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09718v1</guid></item><item><title>Smoothness Similarity Regularization for Few-Shot GAN Adaptation</title><link>http://arxiv.org/abs/2308.09717v1</link><description>The task of few-shot GAN adaptation aims to adapt a pre-trained GAN model toa small dataset with very few training images. While existing methods performwell when the dataset for pre-training is structurally similar to the targetdataset, the approaches suffer from training instabilities or memorizationissues when the objects in the two domains have a very different structure. Tomitigate this limitation, we propose a new smoothness similarity regularizationthat transfers the inherently learned smoothness of the pre-trained GAN to thefew-shot target domain even if the two domains are very different. We evaluateour approach by adapting an unconditional and a class-conditional GAN todiverse few-shot target domains. Our proposed method significantly outperformsprior few-shot GAN adaptation methods in the challenging case of structurallydissimilar source-target domains, while performing on par with the state of theart for similar source-target domains.</description><author>Vadim Sushko, Ruyu Wang, Juergen Gall</author><pubDate>Fri, 18 Aug 2023 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09717v1</guid></item><item><title>Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization</title><link>http://arxiv.org/abs/2308.09716v1</link><description>The task of lip synchronization (lip-sync) seeks to match the lips of humanfaces with different audio. It has various applications in the film industry aswell as for creating virtual avatars and for video conferencing. This is achallenging problem as one needs to simultaneously introduce detailed,realistic lip movements while preserving the identity, pose, emotions, andimage quality. Many of the previous methods trying to solve this problem sufferfrom image quality degradation due to a lack of complete contextualinformation. In this paper, we present Diff2Lip, an audio-conditioneddiffusion-based model which is able to do lip synchronization in-the-wild whilepreserving these qualities. We train our model on Voxceleb2, a video datasetcontaining in-the-wild talking face videos. Extensive studies show that ourmethod outperforms popular methods like Wav2Lip and PC-AVS in Fr\'echetinception distance (FID) metric and Mean Opinion Scores (MOS) of the users. Weshow results on both reconstruction (same audio-video inputs) as well as cross(different audio-video inputs) settings on Voxceleb2 and LRW datasets. Videoresults and code can be accessed from our project page (https://soumik-kanad.github.io/diff2lip ).</description><author>Soumik Mukhopadhyay, Saksham Suri, Ravi Teja Gadde, Abhinav Shrivastava</author><pubDate>Fri, 18 Aug 2023 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09716v1</guid></item><item><title>Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis</title><link>http://arxiv.org/abs/2308.09713v1</link><description>We present a method that simultaneously addresses the tasks of dynamic scenenovel-view synthesis and six degree-of-freedom (6-DOF) tracking of all densescene elements. We follow an analysis-by-synthesis framework, inspired byrecent work that models scenes as a collection of 3D Gaussians which areoptimized to reconstruct input images via differentiable rendering. To modeldynamic scenes, we allow Gaussians to move and rotate over time while enforcingthat they have persistent color, opacity, and size. By regularizing Gaussians'motion and rotation with local-rigidity constraints, we show that our Dynamic3D Gaussians correctly model the same area of physical space over time,including the rotation of that space. Dense 6-DOF tracking and dynamicreconstruction emerges naturally from persistent dynamic view synthesis,without requiring any correspondence or flow as input. We demonstrate a largenumber of downstream applications enabled by our representation, includingfirst-person view synthesis, dynamic compositional scene synthesis, and 4Dvideo editing.</description><author>Jonathon Luiten, Georgios Kopanas, Bastian Leibe, Deva Ramanan</author><pubDate>Fri, 18 Aug 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09713v1</guid></item><item><title>HumanLiff: Layer-wise 3D Human Generation with Diffusion Model</title><link>http://arxiv.org/abs/2308.09712v1</link><description>3D human generation from 2D images has achieved remarkable progress throughthe synergistic utilization of neural rendering and generative models. Existing3D human generative models mainly generate a clothed 3D human as anundetectable 3D model in a single pass, while rarely considering the layer-wisenature of a clothed human body, which often consists of the human body andvarious clothes such as underwear, outerwear, trousers, shoes, etc. In thiswork, we propose HumanLiff, the first layer-wise 3D human generative model witha unified diffusion process. Specifically, HumanLiff firstly generatesminimal-clothed humans, represented by tri-plane features, in a canonicalspace, and then progressively generates clothes in a layer-wise manner. In thisway, the 3D human generation is thus formulated as a sequence ofdiffusion-based 3D conditional generation. To reconstruct more fine-grained 3Dhumans with tri-plane representation, we propose a tri-plane shift operationthat splits each tri-plane into three sub-planes and shifts these sub-planes toenable feature grid subdivision. To further enhance the controllability of 3Dgeneration with 3D layered conditions, HumanLiff hierarchically fuses tri-planefeatures and 3D layered conditions to facilitate the 3D diffusion modellearning. Extensive experiments on two layer-wise 3D human datasets, SynBody(synthetic) and TightCap (real-world), validate that HumanLiff significantlyoutperforms state-of-the-art methods in layer-wise 3D human generation. Ourcode will be available at https://skhu101.github.io/HumanLiff.</description><author>Shoukang Hu, Fangzhou Hong, Tao Hu, Liang Pan, Haiyi Mei, Weiye Xiao, Lei Yang, Ziwei Liu</author><pubDate>Fri, 18 Aug 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09712v1</guid></item><item><title>Robust Monocular Depth Estimation under Challenging Conditions</title><link>http://arxiv.org/abs/2308.09711v1</link><description>While state-of-the-art monocular depth estimation approaches achieveimpressive results in ideal settings, they are highly unreliable underchallenging illumination and weather conditions, such as at nighttime or in thepresence of rain. In this paper, we uncover these safety-critical issues andtackle them with md4all: a simple and effective solution that works reliablyunder both adverse and ideal conditions, as well as for different types oflearning supervision. We achieve this by exploiting the efficacy of existingmethods under perfect settings. Therefore, we provide valid training signalsindependently of what is in the input. First, we generate a set of complexsamples corresponding to the normal training ones. Then, we train the model byguiding its self- or full-supervision by feeding the generated samples andcomputing the standard losses on the corresponding original images. Doing soenables a single model to recover information across diverse conditions withoutmodifications at inference time. Extensive experiments on two challengingpublic datasets, namely nuScenes and Oxford RobotCar, demonstrate theeffectiveness of our techniques, outperforming prior works by a large margin inboth standard and challenging conditions. Source code and data are availableat: https://md4all.github.io.</description><author>Stefano Gasperini, Nils Morbitzer, HyunJun Jung, Nassir Navab, Federico Tombari</author><pubDate>Fri, 18 Aug 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09711v1</guid></item><item><title>SimDA: Simple Diffusion Adapter for Efficient Video Generation</title><link>http://arxiv.org/abs/2308.09710v1</link><description>The recent wave of AI-generated content has witnessed the great developmentand success of Text-to-Image (T2I) technologies. By contrast, Text-to-Video(T2V) still falls short of expectations though attracting increasing interests.Existing works either train from scratch or adapt large T2I model to videos,both of which are computation and resource expensive. In this work, we proposea Simple Diffusion Adapter (SimDA) that fine-tunes only 24M out of 1.1Bparameters of a strong T2I model, adapting it to video generation in aparameter-efficient way. In particular, we turn the T2I model for T2V bydesigning light-weight spatial and temporal adapters for transfer learning.Besides, we change the original spatial attention to the proposed Latent-ShiftAttention (LSA) for temporal consistency. With similar model architecture, wefurther train a video super-resolution model to generate high-definition(1024x1024) videos. In addition to T2V generation in the wild, SimDA could alsobe utilized in one-shot video editing with only 2 minutes tuning. Doing so, ourmethod could minimize the training effort with extremely few tunable parametersfor model adaptation.</description><author>Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Fri, 18 Aug 2023 18:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09710v1</guid></item><item><title>Neural-network quantum state study of the long-range antiferromagnetic Ising chain</title><link>http://arxiv.org/abs/2308.09709v1</link><description>We investigate quantum phase transitions in the transverse field Ising chainwith algebraically decaying long-range antiferromagnetic interactions by usingthe variational Monte Carlo method with the restricted Boltzmann machine beingemployed as a trial wave function ansatz. In the finite-size scaling analysiswith the order parameter and the second R\'enyi entropy, we find that thecentral charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$in contrast to the critical exponents staying very close to the short-range(SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting thepreviously proposed scenario of conformal invariance breakdown. To identify thethreshold of the Ising universality and the conformal symmetry, we perform twoadditional tests for the universal Binder ratio and the conformal field theory(CFT) description of the correlation function. It turns out that both indicatea noticeable deviation from the SR Ising class at $\alpha_\mathrm{LR} &lt; 2$.However, a closer look at the scaled correlation function for$\alpha_\mathrm{LR} \ge 2$ shows a gradual change from the asymptotic line ofthe CFT verified at $\alpha_\mathrm{LR} = 3$, providing a rough estimate of thethreshold being in the range of $2 \lesssim \alpha_\mathrm{LR} &lt; 3$.</description><author>Jicheol Kim, Dongkyu Kim, Dong-Hee Kim</author><pubDate>Fri, 18 Aug 2023 18:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09709v1</guid></item><item><title>Training with Product Digital Twins for AutoRetail Checkout</title><link>http://arxiv.org/abs/2308.09708v1</link><description>Automating the checkout process is important in smart retail, where userseffortlessly pass products by hand through a camera, triggering automaticproduct detection, tracking, and counting. In this emerging area, due to thelack of annotated training data, we introduce a dataset comprised of product 3Dmodels, which allows for fast, flexible, and large-scale training datageneration through graphic engine rendering. Within this context, we discern anintriguing facet, because of the user "hands-on" approach, bias in userbehavior leads to distinct patterns in the real checkout process. The existenceof such patterns would compromise training effectiveness if training data failto reflect the same. To address this user bias problem, we propose a trainingdata optimization framework, i.e., training with digital twins (DtTrain).Specifically, we leverage the product 3D models and optimize their renderingviewpoint and illumination to generate "digital twins" that visually resemblerepresentative user images. These digital twins, inherit product labels and,when augmented, form the Digital Twin training set (DT set). Because thedigital twins individually mimic user bias, the resulting DT training setbetter reflects the characteristics of the target scenario and allows us totrain more effective product detection and tracking models. In our experiment,we show that DT set outperforms training sets created by existing datasetsynthesis methods in terms of counting accuracy. Moreover, by combining DT setwith pseudo-labeled real checkout data, further improvement is observed. Thecode is available at https://github.com/yorkeyao/Automated-Retail-Checkout.</description><author>Yue Yao, Xinyu Tian, Zheng Tang, Sujit Biswas, Huan Lei, Tom Gedeon, Liang Zheng</author><pubDate>Fri, 18 Aug 2023 18:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09708v1</guid></item><item><title>Segmenting Known Objects and Unseen Unknowns without Prior Knowledge</title><link>http://arxiv.org/abs/2209.05407v4</link><description>Panoptic segmentation methods assign a known class to each pixel given ininput. Even for state-of-the-art approaches, this inevitably enforces decisionsthat systematically lead to wrong predictions for objects outside the trainingcategories. However, robustness against out-of-distribution samples and cornercases is crucial in safety-critical settings to avoid dangerous consequences.Since real-world datasets cannot contain enough data points to adequatelysample the long tail of the underlying distribution, models must be able todeal with unseen and unknown scenarios as well. Previous methods targeted thisby re-identifying already-seen unlabeled objects. In this work, we propose thenecessary step to extend segmentation with a new setting which we term holisticsegmentation. Holistic segmentation aims to identify and separate objects ofunseen, unknown categories into instances without any prior knowledge aboutthem while performing panoptic segmentation of known classes. We tackle thisnew problem with U3HS, which finds unknowns as highly uncertain regions andclusters their corresponding instance-aware embeddings into individual objects.By doing so, for the first time in panoptic segmentation with unknown objects,our U3HS is trained without unknown categories, reducing assumptions andleaving the settings as unconstrained as in real-life scenarios. Extensiveexperiments on public data from MS COCO, Cityscapes, and Lost&amp;Found demonstratethe effectiveness of U3HS for this new, challenging, and assumptions-freesetting called holistic segmentation. Project page:https://holisticseg.github.io.</description><author>Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Nassir Navab, Benjamin Busam, Federico Tombari</author><pubDate>Fri, 18 Aug 2023 18:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05407v4</guid></item><item><title>Guide3D: Create 3D Avatars from Text and Image Guidance</title><link>http://arxiv.org/abs/2308.09705v1</link><description>Recently, text-to-image generation has exhibited remarkable advancements,with the ability to produce visually impressive results. In contrast,text-to-3D generation has not yet reached a comparable level of quality.Existing methods primarily rely on text-guided score distillation sampling(SDS), and they encounter difficulties in transferring 2D attributes of thegenerated images to 3D content. In this work, we aim to develop an effective 3Dgenerative model capable of synthesizing high-resolution textured meshes byleveraging both textual and image information. To this end, we introduceGuide3D, a zero-shot text-and-image-guided generative model for 3D avatargeneration based on diffusion models. Our model involves (1) generatingsparse-view images of a text-consistent character using diffusion models, and(2) jointly optimizing multi-resolution differentiable marching tetrahedralgrids with pixel-aligned image features. We further propose a similarity-awarefeature fusion strategy for efficiently integrating features from differentviews. Moreover, we introduce two novel training objectives as an alternativeto calculating SDS, significantly enhancing the optimization process. Wethoroughly evaluate the performance and components of our framework, whichoutperforms the current state-of-the-art in producing topologically andstructurally correct geometry and high-resolution textures. Guide3D enables thedirect transfer of 2D-generated images to the 3D space. Our code will be madepublicly available.</description><author>Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong</author><pubDate>Fri, 18 Aug 2023 18:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09705v1</guid></item><item><title>Do you know what q-means?</title><link>http://arxiv.org/abs/2308.09701v1</link><description>Clustering is one of the most important tools for analysis of large datasets,and perhaps the most popular clustering algorithm is Lloyd's iteration for$k$-means. This iteration takes $N$ vectors $v_1,\dots,v_N\in\mathbb{R}^d$ andoutputs $k$ centroids $c_1,\dots,c_k\in\mathbb{R}^d$; these partition thevectors into clusters based on which centroid is closest to a particularvector. We present an overall improved version of the "$q$-means" algorithm,the quantum algorithm originally proposed by Kerenidis, Landman, Luongo, andPrakash (2019) which performs $\varepsilon$-$k$-means, an approximate versionof $k$-means clustering. This algorithm does not rely on the quantum linearalgebra primitives of prior work, instead only using its QRAM to prepare andmeasure simple states based on the current iteration's clusters. The timecomplexity is $O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$and maintains the polylogarithmic dependence on $N$ while improving thedependence on most of the other parameters. We also present a "dequantized"algorithm for $\varepsilon$-$k$-means which runs in$O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$ time. Notably, thisclassical algorithm matches the polylogarithmic dependence on $N$ attained bythe quantum algorithms.</description><author>João F. Doriguello, Alessandro Luongo, Ewin Tang</author><pubDate>Fri, 18 Aug 2023 18:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09701v1</guid></item><item><title>End-to-End Feasible Optimization Proxies for Large-Scale Economic Dispatch</title><link>http://arxiv.org/abs/2304.11726v2</link><description>The paper proposes a novel End-to-End Learning and Repair (E2ELR)architecture for training optimization proxies for economic dispatch problems.E2ELR combines deep neural networks with closed-form, differentiable repairlayers, thereby integrating learning and feasibility in an end-to-end fashion.E2ELR is also trained with self-supervised learning, removing the need forlabeled data and the solving of numerous optimization problems offline. E2ELRis evaluated on industry-size power grids with tens of thousands of buses usingan economic dispatch that co-optimizes energy and reserves. The resultsdemonstrate that the self-supervised E2ELR achieves state-of-the-artperformance, with optimality gaps that outperform other baselines by at leastan order of magnitude.</description><author>Wenbo Chen, Mathieu Tanneau, Pascal Van Hentenryck</author><pubDate>Fri, 18 Aug 2023 18:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11726v2</guid></item><item><title>Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition</title><link>http://arxiv.org/abs/2308.09694v1</link><description>We tackle the data scarcity challenge in few-shot point cloud recognition of3D objects by using a joint prediction from a conventional 3D model and awell-trained 2D model. Surprisingly, such an ensemble, though seems trivial,has hardly been shown effective in recent 2D-3D models. We find out the crux isthe less effective training for the ''joint hard samples'', which have highconfidence prediction on different wrong labels, implying that the 2D and 3Dmodels do not collaborate well. To this end, our proposed invariant trainingstrategy, called InvJoint, does not only emphasize the training more on thehard samples, but also seeks the invariance between the conflicting 2D and 3Dambiguous predictions. InvJoint can learn more collaborative 2D and 3Drepresentations for better ensemble. Extensive experiments on 3D shapeclassification with widely adopted ModelNet10/40, ScanObjectNN and Toys4K, andshape retrieval with ShapeNet-Core validate the superiority of our InvJoint.</description><author>Xuanyu Yi, Jiajun Deng, Qianru Sun, Xian-Sheng Hua, Joo-Hwee Lim, Hanwang Zhang</author><pubDate>Fri, 18 Aug 2023 18:43:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09694v1</guid></item><item><title>A Lightweight Transformer for Faster and Robust EBSD Data Collection</title><link>http://arxiv.org/abs/2308.09693v1</link><description>Three dimensional electron back-scattered diffraction (EBSD) microscopy is acritical tool in many applications in materials science, yet its data qualitycan fluctuate greatly during the arduous collection process, particularly viaserial-sectioning. Fortunately, 3D EBSD data is inherently sequential, openingup the opportunity to use transformers, state-of-the-art deep learningarchitectures that have made breakthroughs in a plethora of domains, for dataprocessing and recovery. To be more robust to errors and accelerate this 3DEBSD data collection, we introduce a two step method that recovers missingslices in an 3D EBSD volume, using an efficient transformer model and aprojection algorithm to process the transformer's outputs. Overcoming thecomputational and practical hurdles of deep learning with scarce highdimensional data, we train this model using only synthetic 3D EBSD data withself-supervision and obtain superior recovery accuracy on real 3D EBSD data,compared to existing methods.</description><author>Harry Dong, Sean Donegan, Megna Shah, Yuejie Chi</author><pubDate>Fri, 18 Aug 2023 18:41:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09693v1</guid></item><item><title>Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning</title><link>http://arxiv.org/abs/2308.09691v1</link><description>Advanced Manufacturing (AM) has gained significant interest in the nuclearcommunity for its potential application on nuclear materials. One challenge isto obtain desired material properties via controlling the manufacturing processduring runtime. Intelligent AM based on deep reinforcement learning (DRL)relies on an automated process-level control mechanism to generate optimaldesign variables and adaptive system settings for improved end-productproperties. A high-fidelity thermo-mechanical model for direct energydeposition has recently been developed within the MOOSE framework at the IdahoNational Laboratory (INL). The goal of this work is to develop an accurate andfast-running reduced order model (ROM) for this MOOSE-based AM model that canbe used in a DRL-based process control and optimization method. Operatorlearning (OL)-based methods will be employed due to their capability to learn afamily of differential equations, in this work, produced by changing processvariables in the Gaussian point heat source for the laser. We will developOL-based ROM using Fourier neural operator, and perform a benchmark comparisonof its performance with a conventional deep neural network-based ROM.</description><author>Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</author><pubDate>Fri, 18 Aug 2023 18:38:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09691v1</guid></item><item><title>Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4</title><link>http://arxiv.org/abs/2306.07622v2</link><description>Large language models (LLMs) are currently at the forefront of intertwiningAI systems with human communication and everyday life. Therefore, it is ofgreat importance to evaluate their emerging abilities. In this study, we showthat LLMs, most notably GPT-3, exhibit behavior that strikingly resembleshuman-like intuition -- and the cognitive errors that come with it. However,LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4,learned to avoid succumbing to these errors and perform in a hyperrationalmanner. For our experiments, we probe LLMs with the Cognitive Reflection Test(CRT) as well as semantic illusions that were originally designed toinvestigate intuitive decision-making in humans. Moreover, we probe how sturdythe inclination for intuitive-like decision-making is. Our study demonstratesthat investigating LLMs with methods from psychology has the potential toreveal otherwise unknown emergent traits.</description><author>Thilo Hagendorff, Sarah Fabi</author><pubDate>Fri, 18 Aug 2023 18:33:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07622v2</guid></item><item><title>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</title><link>http://arxiv.org/abs/2308.09687v1</link><description>We introduce Graph of Thoughts (GoT): a framework that advances promptingcapabilities in large language models (LLMs) beyond those offered by paradigmssuch as Chain-ofThought or Tree of Thoughts (ToT). The key idea and primaryadvantage of GoT is the ability to model the information generated by an LLM asan arbitrary graph, where units of information ("LLM thoughts") are vertices,and edges correspond to dependencies between these vertices. This approachenables combining arbitrary LLM thoughts into synergistic outcomes, distillingthe essence of whole networks of thoughts, or enhancing thoughts using feedbackloops. We illustrate that GoT offers advantages over state of the art ondifferent tasks, for example increasing the quality of sorting by 62% over ToT,while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensiblewith new thought transformations and thus can be used to spearhead newprompting schemes. This work brings the LLM reasoning closer to human thinkingor brain mechanisms such as recurrence, both of which form complex networks.</description><author>Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler</author><pubDate>Fri, 18 Aug 2023 18:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09687v1</guid></item><item><title>DarSwin: Distortion Aware Radial Swin Transformer</title><link>http://arxiv.org/abs/2304.09691v2</link><description>Wide-angle lenses are commonly used in perception tasks requiring a largefield of view. Unfortunately, these lenses produce significant distortionsmaking conventional models that ignore the distortion effects unable to adaptto wide-angle images. In this paper, we present a novel transformer-based modelthat automatically adapts to the distortion produced by wide-angle lenses. Weleverage the physical characteristics of such lenses, which are analyticallydefined by the radial distortion profile (assumed to be known), to develop adistortion aware radial swin transformer (DarSwin). In contrast to conventionaltransformer-based architectures, DarSwin comprises a radial patch partitioning,a distortion-based sampling technique for creating token embeddings, and anangular position encoding for radial patch merging. We validate our method onclassification tasks using synthetically distorted ImageNet data and showthrough extensive experiments that DarSwin can perform zero-shot adaptation tounseen distortions of different wide-angle lenses. Compared to other baselines,DarSwin achieves the best results (in terms of Top-1 accuracy) with significantgains when trained on bounded levels of distortions (very-low, low, medium, andhigh) and tested on all including out-of-distribution distortions. The code andmodels are publicly available at https://lvsn.github.io/darswin/</description><author>Akshaya Athwale, Arman Afrasiyabi, Justin Lague, Ichrak Shili, Ola Ahmad, Jean-Francois Lalonde</author><pubDate>Fri, 18 Aug 2023 18:17:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09691v2</guid></item><item><title>Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions</title><link>http://arxiv.org/abs/2308.09685v1</link><description>We present Audiovisual Moments in Time (AVMIT), a large-scale dataset ofaudiovisual action events. In an extensive annotation task 11 participantslabelled a subset of 3-second audiovisual videos from the Moments in Timedataset (MIT). For each trial, participants assessed whether the labelledaudiovisual action event was present and whether it was the most prominentfeature of the video. The dataset includes the annotation of 57,177 audiovisualvideos, each independently evaluated by 3 of 11 trained participants. From thisinitial collection, we created a curated test set of 16 distinct actionclasses, with 60 videos each (960 videos). We also offer 2 sets of pre-computedaudiovisual feature embeddings, using VGGish/YamNet for audio data andVGG16/EfficientNetB0 for visual data, thereby lowering the barrier to entry foraudiovisual DNN research. We explored the advantages of AVMIT annotations andfeature embeddings to improve performance on audiovisual event recognition. Aseries of 6 Recurrent Neural Networks (RNNs) were trained on eitherAVMIT-filtered audiovisual events or modality-agnostic events from MIT, andthen tested on our audiovisual test set. In all RNNs, top 1 accuracy wasincreased by 2.71-5.94\% by training exclusively on audiovisual events, evenoutweighing a three-fold increase in training data. We anticipate that thenewly annotated AVMIT dataset will serve as a valuable resource for researchand comparative experiments involving computational models and humanparticipants, specifically when addressing research questions where audiovisualcorrespondence is of critical importance.</description><author>Michael Joannou, Pia Rotshtein, Uta Noppeney</author><pubDate>Fri, 18 Aug 2023 18:13:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09685v1</guid></item><item><title>ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation</title><link>http://arxiv.org/abs/2302.13848v2</link><description>In addition to the unprecedented ability in imaginary creation, largetext-to-image models are expected to take customized concepts in imagegeneration. Existing works generally learn such concepts in anoptimization-based manner, yet bringing excessive computation or memory burden.In this paper, we instead propose a learning-based encoder, which consists of aglobal and a local mapping networks for fast and accurate customizedtext-to-image generation. In specific, the global mapping network projects thehierarchical features of a given image into multiple new words in the textualword embedding space, i.e., one primary word for well-editable concept andother auxiliary words to exclude irrelevant disturbances (e.g., background). Inthe meantime, a local mapping network injects the encoded patch features intocross attention layers to provide omitted details, without sacrificing theeditability of primary concepts. We compare our method with existingoptimization-based approaches on a variety of user-defined concepts, anddemonstrate that our method enables high-fidelity inversion and more robusteditability with a significantly faster encoding process. Our code is publiclyavailable at https://github.com/csyxwei/ELITE.</description><author>Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, Wangmeng Zuo</author><pubDate>Fri, 18 Aug 2023 18:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13848v2</guid></item><item><title>Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments</title><link>http://arxiv.org/abs/2305.15700v2</link><description>Continual semantic segmentation aims to learn new classes while maintainingthe information from the previous classes. Although prior studies have shownimpressive progress in recent years, the fairness concern in the continualsemantic segmentation needs to be better addressed. Meanwhile, fairness is oneof the most vital factors in deploying the deep learning model, especially inhuman-related or safety applications. In this paper, we present a novelFairness Continual Learning approach to the semantic segmentation problem. Inparticular, under the fairness objective, a new fairness continual learningframework is proposed based on class distributions. Then, a novel PrototypicalContrastive Clustering loss is proposed to address the significant challengesin continual learning, i.e., catastrophic forgetting and background shift. Ourproposed loss has also been proven as a novel, generalized learning paradigm ofknowledge distillation commonly used in continual learning. Moreover, theproposed Conditional Structural Consistency loss further regularized thestructural constraint of the predicted segmentation. Our proposed approach hasachieved State-of-the-Art performance on three standard scene understandingbenchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairnessof the segmentation model.</description><author>Thanh-Dat Truong, Hoang-Quan Nguyen, Bhiksha Raj, Khoa Luu</author><pubDate>Fri, 18 Aug 2023 17:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15700v2</guid></item><item><title>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2308.09678v1</link><description>The current 3D human pose estimators face challenges in adapting to newdatasets due to the scarcity of 2D-3D pose pairs in target domain trainingsets. We present the \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis\textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to overcomethis issue without extensive target domain annotation. Utilizing adiffusion-centric structure, PoSynDA simulates the 3D pose distribution in thetarget domain, filling the data diversity gap. By incorporating amulti-hypothesis network, it creates diverse pose hypotheses and aligns themwith the target domain. Target-specific source augmentation obtains the targetdomain distribution data from the source domain by decoupling the scale andposition parameters. The teacher-student paradigm and low-rank adaptationfurther refine the process. PoSynDA demonstrates competitive performance onbenchmarks, such as Human3.6M, MPI-INF-3DHP, and 3DPW, even comparable with thetarget-trained MixSTE model~\cite{zhang2022mixste}. This work paves the way forthe practical application of 3D human pose estimation. The code is available athttps://github.com/hbing-l/PoSynDA.</description><author>Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Fri, 18 Aug 2023 17:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09678v1</guid></item><item><title>A Part-of-Speech Tagger for Yiddish</title><link>http://arxiv.org/abs/2204.01175v2</link><description>We describe the construction and evaluation of a part-of-speech tagger forYiddish. This is the first step in a larger project of automatically assigningpart-of-speech tags and syntactic structure to Yiddish text for purposes oflinguistic research. We combine two resources for the current work - an80K-word subset of the Penn Parsed Corpus of Historical Yiddish (PPCHY) and 650million words of OCR'd Yiddish text from the Yiddish Book Center (YBC). Yiddishorthography in the YBC corpus has many spelling inconsistencies, and we presentsome evidence that even simple non-contextualized embeddings trained on YBC areable to capture the relationships among spelling variants without the need tofirst "standardize" the corpus. We also use YBC for continued pretraining ofcontexualized embeddings, which are then integrated into a tagger model trainedand evaluated on the PPCHY. We evaluate the tagger performance on a 10-foldcross-validation split, showing that the use of the YBC text for thecontextualized embeddings improves tagger performance. We conclude bydiscussing some next steps, including the need for additional annotatedtraining and test data.</description><author>Seth Kulick, Neville Ryant, Beatrice Santorini, Joel Wallenberg, Assaf Urieli</author><pubDate>Fri, 18 Aug 2023 17:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.01175v2</guid></item><item><title>KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration</title><link>http://arxiv.org/abs/2305.16437v2</link><description>In the realm of facial analysis, accurate landmark detection is crucial forvarious applications, ranging from face recognition and expression analysis toanimation. Conventional heatmap or coordinate regression-based techniques,however, often face challenges in terms of computational burden andquantization errors. To address these issues, we present the KeyPointPositioning System (KeyPosS) - a groundbreaking facial landmark detectionframework that stands out from existing methods. The framework utilizes a fullyconvolutional network to predict a distance map, which computes the distancebetween a Point of Interest (POI) and multiple anchor points. These anchorpoints are ingeniously harnessed to triangulate the POI's position through theTrue-range Multilateration algorithm. Notably, the plug-and-play nature ofKeyPosS enables seamless integration into any decoding stage, ensuring aversatile and adaptable solution. We conducted a thorough evaluation ofKeyPosS's performance by benchmarking it against state-of-the-art models onfour different datasets. The results show that KeyPosS substantiallyoutperforms leading methods in low-resolution settings while requiring aminimal time overhead. The code is available athttps://github.com/zhiqic/KeyPosS.</description><author>Xu Bao, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Wangmeng Xiang, Jingdong Sun, Hanbing Liu, Wei Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Fri, 18 Aug 2023 17:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16437v2</guid></item><item><title>OCR Language Models with Custom Vocabularies</title><link>http://arxiv.org/abs/2308.09671v1</link><description>Language models are useful adjuncts to optical models for producing accurateoptical character recognition (OCR) results. One factor which limits the powerof language models in this context is the existence of many specialized domainswith language statistics very different from those implied by a generallanguage model - think of checks, medical prescriptions, and many otherspecialized document classes. This paper introduces an algorithm forefficiently generating and attaching a domain specific word based languagemodel at run time to a general language model in an OCR system. In order tobest use this model the paper also introduces a modified CTC beam searchdecoder which effectively allows hypotheses to remain in contention based onpossible future completion of vocabulary words. The result is a substantialreduction in word error rate in recognizing material from specialized domains.</description><author>Peter Garst, Reeve Ingle, Yasuhisa Fujii</author><pubDate>Fri, 18 Aug 2023 17:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09671v1</guid></item><item><title>PC-Droid: Faster diffusion and improved quality for particle cloud generation</title><link>http://arxiv.org/abs/2307.06836v3</link><description>Building on the success of PC-JeDi we introduce PC-Droid, a substantiallyimproved diffusion model for the generation of jet particle clouds. Byleveraging a new diffusion formulation, studying more recent integrationsolvers, and training on all jet types simultaneously, we are able to achievestate-of-the-art performance for all types of jets across all evaluationmetrics. We study the trade-off between generation speed and quality bycomparing two attention based architectures, as well as the potential ofconsistency distillation to reduce the number of diffusion steps. Both thefaster architecture and consistency models demonstrate performance surpassingmany competing models, with generation time up to two orders of magnitudefaster than PC-JeDi and three orders of magnitude faster than Delphes.</description><author>Matthew Leigh, Debajyoti Sengupta, John Andrew Raine, Guillaume Quétant, Tobias Golling</author><pubDate>Fri, 18 Aug 2023 17:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06836v3</guid></item><item><title>Variational optimization of the amplitude of neural-network quantum many-body ground states</title><link>http://arxiv.org/abs/2308.09664v1</link><description>Neural-network quantum states (NQSs), variationally optimized by combiningtraditional methods and deep learning techniques, is a new way to find quantummany-body ground states and gradually becomes a competitor of traditionalvariational methods. However, there are still some difficulties in theoptimization of NQSs, such as local minima, slow convergence, and signstructure optimization. Here, we split a quantum many-body variational wavefunction into a multiplication of a real-valued amplitude neural network and asign structure, and focus on the optimization of the amplitude network whilekeeping the sign structure fixed. The amplitude network is a convolutionalneural network (CNN) with residual blocks, namely a ResNet. Our method istested on three typical quantum many-body systems. The obtained ground stateenergies are lower than or comparable to those from traditional variationalMonte Carlo (VMC) methods and density matrix renormalization group (DMRG).Surprisingly, for the frustrated Heisenberg $J_1$-$J_2$ model, our results arebetter than those of the complex-valued CNN in the literature, implying thatthe sign structure of the complex-valued NQS is difficult to be optimized. Wewill study the optimization of the sign structure of NQSs in the future.</description><author>Jia-Qi Wang, Rong-Qiang He, Zhong-Yi Lu</author><pubDate>Fri, 18 Aug 2023 17:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09664v1</guid></item><item><title>GiGaMAE: Generalizable Graph Masked Autoencoder via Collaborative Latent Space Reconstruction</title><link>http://arxiv.org/abs/2308.09663v1</link><description>Self-supervised learning with masked autoencoders has recently gainedpopularity for its ability to produce effective image or textualrepresentations, which can be applied to various downstream tasks withoutretraining. However, we observe that the current masked autoencoder models lackgood generalization ability on graph data. To tackle this issue, we propose anovel graph masked autoencoder framework called GiGaMAE. Different fromexisting masked autoencoders that learn node presentations by explicitlyreconstructing the original graph components (e.g., features or edges), in thispaper, we propose to collaboratively reconstruct informative and integratedlatent embeddings. By considering embeddings encompassing graph topology andattribute information as reconstruction targets, our model could capture moregeneralized and comprehensive knowledge. Furthermore, we introduce a mutualinformation based reconstruction loss that enables the effective reconstructionof multiple targets. This learning objective allows us to differentiate betweenthe exclusive knowledge learned from a single target and common knowledgeshared by multiple targets. We evaluate our method on three downstream taskswith seven datasets as benchmarks. Extensive experiments demonstrate thesuperiority of GiGaMAE against state-of-the-art baselines. We hope our resultswill shed light on the design of foundation models on graph-structured data.Our code is available at: https://github.com/sycny/GiGaMAE.</description><author>Yucheng Shi, Yushun Dong, Qiaoyu Tan, Jundong Li, Ninghao Liu</author><pubDate>Fri, 18 Aug 2023 17:30:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09663v1</guid></item><item><title>Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment</title><link>http://arxiv.org/abs/2308.09662v1</link><description>Larger language models (LLMs) have taken the world by storm with theirmassive multi-tasking capabilities simply by optimizing over a next-wordprediction objective. With the emergence of their properties and encodedknowledge, the risk of LLMs producing harmful outputs increases, making themunfit for scalable deployment for the public. In this work, we propose a newsafety evaluation benchmark RED-EVAL that carries out red-teaming. We show thateven widely deployed models are susceptible to the Chain of Utterances-based(CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 andChatGPT to unethically respond to more than 65% and 73% of harmful queries. Wealso demonstrate the consistency of the RED-EVAL across 8 open-source LLMs ingenerating harmful responses in more than 86% of the red-teaming attempts.Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. Itconstitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting,we collect a dataset that consists of 1.9K harmful questions covering a widerange of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2)SAFE-ALIGN: We demonstrate how the conversational dataset can be used for thesafety alignment of LLMs by minimizing the negative log-likelihood over helpfulresponses and penalizing over harmful responses by gradient accent over sampleloss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safelyaligned when evaluated on RED-EVAL and HHH benchmarks while preserving theutility of the baseline models (TruthfulQA, MMLU, and BBH).</description><author>Rishabh Bhardwaj, Soujanya Poria</author><pubDate>Fri, 18 Aug 2023 17:27:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09662v1</guid></item><item><title>Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning</title><link>http://arxiv.org/abs/2308.09658v1</link><description>There emerges a promising trend of using large language models (LLMs) togenerate code-like plans for complex inference tasks such as visual reasoning.This paradigm, known as LLM-based planning, provides flexibility in problemsolving and endows better interpretability. However, current research is mostlylimited to basic scenarios of simple questions that can be straightforwardanswered in a few inference steps. Planning for the more challenging multi-hopvisual reasoning tasks remains under-explored. Specifically, under multi-hopreasoning situations, the trade-off between accuracy and the complexity ofplan-searching becomes prominent. The prevailing algorithms either address theefficiency issue by employing the fast one-stop generation or adopt a complexiterative generation method to improve accuracy. Both fail to balance the needfor efficiency and performance. Drawing inspiration from the dual system ofcognition in the human brain, the fast and the slow think processes, we proposea hierarchical plan-searching algorithm that integrates the one-stop reasoning(fast) and the Tree-of-thought (slow). Our approach succeeds in performancewhile significantly saving inference steps. Moreover, we repurpose the PTR andthe CLEVER datasets, developing a systematic framework for evaluating theperformance and efficiency of LLMs-based plan-search algorithms under reasoningtasks at different levels of difficulty. Extensive experiments demonstrate thesuperiority of our proposed algorithm in terms of performance and efficiency.The dataset and code will be release soon.</description><author>Pengbo Hu, Ji Qi, Xingyu Li, Hong Li, Xinqi Wang, Bing Quan, Ruiyu Wang, Yi Zhou</author><pubDate>Fri, 18 Aug 2023 17:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09658v1</guid></item><item><title>A Tractable Online Learning Algorithm for the Multinomial Logit Contextual Bandit</title><link>http://arxiv.org/abs/2011.14033v6</link><description>In this paper, we consider the contextual variant of the MNL-Bandit problem.More specifically, we consider a dynamic set optimization problem, where adecision-maker offers a subset (assortment) of products to a consumer andobserves the response in every round. Consumers purchase products to maximizetheir utility. We assume that a set of attributes describe the products, andthe mean utility of a product is linear in the values of these attributes. Wemodel consumer choice behavior using the widely used Multinomial Logit (MNL)model and consider the decision maker problem of dynamically learning the modelparameters while optimizing cumulative revenue over the selling horizon $T$.Though this problem has attracted considerable attention in recent times, manyexisting methods often involve solving an intractable non-convex optimizationproblem. Their theoretical performance guarantees depend on a problem-dependentparameter which could be prohibitively large. In particular, existingalgorithms for this problem have regret bounded by $O(\sqrt{\kappa d T})$,where $\kappa$ is a problem-dependent constant that can have an exponentialdependency on the number of attributes. In this paper, we propose an optimisticalgorithm and show that the regret is bounded by $O(\sqrt{dT} + \kappa)$,significantly improving the performance over existing methods. Further, wepropose a convex relaxation of the optimization step, which allows fortractable decision-making while retaining the favourable regret guarantee.</description><author>Priyank Agrawal, Theja Tulabandhula, Vashist Avadhanula</author><pubDate>Fri, 18 Aug 2023 17:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.14033v6</guid></item><item><title>Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction</title><link>http://arxiv.org/abs/2308.09647v1</link><description>Deploying deep learning models in safety-critical applications remains a verychallenging task, mandating the provision of assurances for the dependableoperation of these models. Uncertainty quantification (UQ) methods estimate themodel's confidence per prediction, informing decision-making by considering theeffect of randomness and model misspecification. Despite the advances ofstate-of-the-art UQ methods, they are computationally expensive or produceconservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQmethod that combines a new adaptive Monte Carlo (MC) dropout method withconformal prediction (CP). MC-CP adaptively modulates the traditional MCdropout at runtime to save memory and computation resources, enablingpredictions to be consumed by CP, yielding robust prediction sets/intervals.Throughout comprehensive experiments, we show that MC-CP delivers significantimprovements over advanced UQ methods, like MC dropout, RAPS and CQR, both inclassification and regression benchmarks. MC-CP can be easily added to existingmodels, making its deployment simple.</description><author>Daniel Bethell, Simos Gerasimou, Radu Calinescu</author><pubDate>Fri, 18 Aug 2023 17:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09647v1</guid></item><item><title>Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification</title><link>http://arxiv.org/abs/2306.10841v2</link><description>This paper presents an innovative reference architecture forblockchain-enabled federated learning (BCFL), a state-of-the-art approach thatamalgamates the strengths of federated learning and blockchain technology. Thisresults in a decentralized, collaborative machine learning system that respectsdata privacy and user-controlled identity. Our architecture strategicallyemploys a decentralized identifier (DID)-based authentication system, allowingparticipants to authenticate and then gain access to the federated learningplatform securely using their self-sovereign DIDs, which are recorded on theblockchain. Ensuring robust security and efficient decentralization through theexecution of smart contracts is a key aspect of our approach. Moreover, ourBCFL reference architecture provides significant extensibility, accommodatingthe integration of various additional elements, as per specific requirementsand use cases, thereby rendering it an adaptable solution for a wide range ofBCFL applications. Participants can authenticate and then gain access to thefederated learning platform securely using their self-sovereign DIDs, which aresecurely recorded on the blockchain. The pivotal contribution of this study isthe successful implementation and validation of a realistic BCFL referencearchitecture, marking a significant milestone in the field. We intend to makethe source code publicly accessible shortly, fostering further advancements andadaptations within the community. This research not only bridges a crucial gapin the current literature but also lays a solid foundation for futureexplorations in the realm of BCFL.</description><author>Eunsu Goh, Daeyeol Kim, Kwangkee Lee, Do-Yup Kim</author><pubDate>Fri, 18 Aug 2023 17:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10841v2</guid></item><item><title>Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG</title><link>http://arxiv.org/abs/2307.12644v2</link><description>rPPG (Remote photoplethysmography) is a technology that measures and analyzesBVP (Blood Volume Pulse) by using the light absorption characteristics ofhemoglobin captured through a camera. Analyzing the measured BVP can derivevarious physiological signals such as heart rate, stress level, and bloodpressure, which can be applied to various applications such as telemedicine,remote patient monitoring, and early prediction of cardiovascular disease. rPPGis rapidly evolving and attracting great attention from both academia andindustry by providing great usability and convenience as it can measurebiosignals using a camera-equipped device without medical or wearable devices.Despite extensive efforts and advances in this field, serious challengesremain, including issues related to skin color, camera characteristics, ambientlighting, and other sources of noise and artifacts, which degrade accuracyperformance. We argue that fair and evaluable benchmarking is urgently requiredto overcome these challenges and make meaningful progress from both academicand commercial perspectives. In most existing work, models are trained, tested,and validated only on limited datasets. Even worse, some studies lack availablecode or reproducibility, making it difficult to fairly evaluate and compareperformance. Therefore, the purpose of this study is to provide a benchmarkingframework to evaluate various rPPG techniques across a wide range of datasetsfor fair evaluation and comparison, including both conventional non-deep neuralnetwork (non-DNN) and deep neural network (DNN) methods. GitHub URL:https://github.com/remotebiosensing/rppg</description><author>Dae-Yeol Kim, Eunsu Goh, KwangKee Lee, JongEui Chae, JongHyeon Mun, Junyeong Na, Chae-bong Sohn, Do-Yup Kim</author><pubDate>Fri, 18 Aug 2023 17:03:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12644v2</guid></item><item><title>biquality-learn: a Python library for Biquality Learning</title><link>http://arxiv.org/abs/2308.09643v1</link><description>The democratization of Data Mining has been widely successful thanks in partto powerful and easy-to-use Machine Learning libraries. These libraries havebeen particularly tailored to tackle Supervised Learning. However, strongsupervision signals are scarce in practice, and practitioners must resort toweak supervision. In addition to weaknesses of supervision, dataset shifts areanother kind of phenomenon that occurs when deploying machine learning modelsin the real world. That is why Biquality Learning has been proposed as amachine learning framework to design algorithms capable of handling multipleweaknesses of supervision and dataset shifts without assumptions on theirnature and level by relying on the availability of a small trusted datasetcomposed of cleanly labeled and representative samples. Thus we proposebiquality-learn: a Python library for Biquality Learning with an intuitive andconsistent API to learn machine learning models from biquality data, withwell-proven algorithms, accessible and easy to use for everyone, and enablingresearchers to experiment in a reproducible way on biquality data.</description><author>Pierre Nodet, Vincent Lemaire, Alexis Bondu, Antoine Cornuéjols</author><pubDate>Fri, 18 Aug 2023 17:01:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09643v1</guid></item><item><title>Revisiting Skin Tone Fairness in Dermatological Lesion Classification</title><link>http://arxiv.org/abs/2308.09640v1</link><description>Addressing fairness in lesion classification from dermatological images iscrucial due to variations in how skin diseases manifest across skin tones.However, the absence of skin tone labels in public datasets hinders building afair classifier. To date, such skin tone labels have been estimated prior tofairness analysis in independent studies using the Individual Typology Angle(ITA). Briefly, ITA calculates an angle based on pixels extracted from skinimages taking into account the lightness and yellow-blue tints. These anglesare then categorised into skin tones that are subsequently used to analysefairness in skin cancer classification. In this work, we review and comparefour ITA-based approaches of skin tone classification on the ISIC18 dataset, acommon benchmark for assessing skin cancer classification fairness in theliterature. Our analyses reveal a high disagreement among previously publishedstudies demonstrating the risks of ITA-based skin tone estimation methods.Moreover, we investigate the causes of such large discrepancy among theseapproaches and find that the lack of diversity in the ISIC18 dataset limits itsuse as a testbed for fairness analysis. Finally, we recommend further researchon robust ITA estimation and diverse dataset acquisition with skin toneannotation to facilitate conclusive fairness assessments of artificialintelligence tools in dermatology. Our code is available athttps://github.com/tkalbl/RevisitingSkinToneFairness.</description><author>Thorsten Kalb, Kaisar Kushibar, Celia Cintas, Karim Lekadir, Oliver Diaz, Richard Osuala</author><pubDate>Fri, 18 Aug 2023 16:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09640v1</guid></item><item><title>Development of a Neural Network-based Method for Improved Imputation of Missing Values in Time Series Data by Repurposing DataWig</title><link>http://arxiv.org/abs/2308.09635v1</link><description>Time series data are observations collected over time intervals. Successfulanalysis of time series data captures patterns such as trends, cyclicity andirregularity, which are crucial for decision making in research, business, andgovernance. However, missing values in time series data occur often and presentobstacles to successful analysis, thus they need to be filled with alternativevalues, a process called imputation. Although various approaches have beenattempted for robust imputation of time series data, even the most advancedmethods still face challenges including limited scalability, poor capacity tohandle heterogeneous data types and inflexibility due to requiring strongassumptions of data missing mechanisms. Moreover, the imputation accuracy ofthese methods still has room for improvement. In this study, I developedtsDataWig (time-series DataWig) by modifying DataWig, a neural network-basedmethod that possesses the capacity to process large datasets and heterogeneousdata types but was designed for non-time series data imputation. Unlike theoriginal DataWig, tsDataWig can directly handle values of time variables andimpute missing values in complex time series datasets. Using one simulated andthree different complex real-world time series datasets, I demonstrated thattsDataWig outperforms the original DataWig and the current state-of-the-artmethods for time series data imputation and potentially has broad applicationdue to not requiring strong assumptions of data missing mechanisms. This studyprovides a valuable solution for robustly imputing missing values inchallenging time series datasets, which often contain millions of samples, highdimensional variables, and heterogeneous data types.</description><author>Daniel Zhang</author><pubDate>Fri, 18 Aug 2023 16:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09635v1</guid></item><item><title>VALERIE22 -- A photorealistic, richly metadata annotated dataset of urban environments</title><link>http://arxiv.org/abs/2308.09632v1</link><description>The VALERIE tool pipeline is a synthetic data generator developed with thegoal to contribute to the understanding of domain-specific factors thatinfluence perception performance of DNNs (deep neural networks). This work wascarried out under the German research project KI Absicherung in order todevelop a methodology for the validation of DNNs in the context of pedestriandetection in urban environments for automated driving. The VALERIE22 datasetwas generated with the VALERIE procedural tools pipeline providing aphotorealistic sensor simulation rendered from automatically synthesizedscenes. The dataset provides a uniquely rich set of metadata, allowingextraction of specific scene and semantic features (like pixel-accurateocclusion rates, positions in the scene and distance + angle to the camera).This enables a multitude of possible tests on the data and we hope to stimulateresearch on understanding performance of DNNs. Based on performance metric acomparison with several other publicly available datasets is provided,demonstrating that VALERIE22 is one of best performing synthetic datasetscurrently available in the open domain.</description><author>Oliver Grau, Korbinian Hagn</author><pubDate>Fri, 18 Aug 2023 16:44:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09632v1</guid></item><item><title>YoloCurvSeg: You Only Label One Noisy Skeleton for Vessel-style Curvilinear Structure Segmentation</title><link>http://arxiv.org/abs/2212.05566v5</link><description>Weakly-supervised learning (WSL) has been proposed to alleviate the conflictbetween data annotation cost and model performance through employingsparsely-grained (i.e., point-, box-, scribble-wise) supervision and has shownpromising performance, particularly in the image segmentation field. However,it is still a very challenging task due to the limited supervision, especiallywhen only a small number of labeled samples are available. Additionally, almostall existing WSL segmentation methods are designed for star-convex structureswhich are very different from curvilinear structures such as vessels andnerves. In this paper, we propose a novel sparsely annotated segmentationframework for curvilinear structures, named YoloCurvSeg. A very essentialcomponent of YoloCurvSeg is image synthesis. Specifically, a backgroundgenerator delivers image backgrounds that closely match the real distributionsthrough inpainting dilated skeletons. The extracted backgrounds are thencombined with randomly emulated curves generated by a Space ColonizationAlgorithm-based foreground generator and through a multilayer patch-wisecontrastive learning synthesizer. In this way, a synthetic dataset with bothimages and curve segmentation labels is obtained, at the cost of only one or afew noisy skeleton annotations. Finally, a segmenter is trained with thegenerated dataset and possibly an unlabeled dataset. The proposed YoloCurvSegis evaluated on four publicly available datasets (OCTA500, CORN, DRIVE andCHASEDB1) and the results show that YoloCurvSeg outperforms state-of-the-artWSL segmentation methods by large margins. With only one noisy skeletonannotation (respectively 0.14\%, 0.03\%, 1.40\%, and 0.65\% of the fullannotation), YoloCurvSeg achieves more than 97\% of the fully-supervisedperformance on each dataset. Code and datasets will be released athttps://github.com/llmir/YoloCurvSeg.</description><author>Li Lin, Linkai Peng, Huaqing He, Pujin Cheng, Jiewei Wu, Kenneth K. Y. Wong, Xiaoying Tang</author><pubDate>Fri, 18 Aug 2023 16:43:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05566v5</guid></item><item><title>Learning Computational Efficient Bots with Costly Features</title><link>http://arxiv.org/abs/2308.09629v1</link><description>Deep reinforcement learning (DRL) techniques have become increasingly used invarious fields for decision-making processes. However, a challenge that oftenarises is the trade-off between both the computational efficiency of thedecision-making process and the ability of the learned agent to solve aparticular task. This is particularly critical in real-time settings such asvideo games where the agent needs to take relevant decisions at a very highfrequency, with a very limited inference time. In this work, we propose a generic offline learning approach where thecomputation cost of the input features is taken into account. We derive theBudgeted Decision Transformer as an extension of the Decision Transformer thatincorporates cost constraints to limit its cost at inference. As a result, themodel can dynamically choose the best input features at each timestep. Wedemonstrate the effectiveness of our method on several tasks, including D4RLbenchmarks and complex 3D environments similar to those found in video games,and show that it can achieve similar performance while using significantlyfewer computational resources compared to classical approaches.</description><author>Anthony Kobanda, Valliappan C. A., Joshua Romoff, Ludovic Denoyer</author><pubDate>Fri, 18 Aug 2023 16:43:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09629v1</guid></item><item><title>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</title><link>http://arxiv.org/abs/2308.05681v2</link><description>Recently, methods for skeleton-based human activity recognition have beenshown to be vulnerable to adversarial attacks. However, these attack methodsrequire either the full knowledge of the victim (i.e. white-box attacks),access to training data (i.e. transfer-based attacks) or frequent model queries(i.e. black-box attacks). All their requirements are highly restrictive,raising the question of how detrimental the vulnerability is. In this paper, weshow that the vulnerability indeed exists. To this end, we consider a newattack task: the attacker has no access to the victim model or the trainingdata or labels, where we coin the term hard no-box attack. Specifically, wefirst learn a motion manifold where we define an adversarial loss to compute anew gradient for the attack, named skeleton-motion-informed (SMI) gradient. Ourgradient contains information of the motion dynamics, which is different fromexisting gradient-based attack methods that compute the loss gradient assumingeach dimension in the data is independent. The SMI gradient can augment manygradient-based attack methods, leading to a new family of no-box attackmethods. Extensive evaluation and comparison show that our method imposes areal threat to existing classifiers. They also show that the SMI gradientimproves the transferability and imperceptibility of adversarial samples inboth no-box and transfer-based black-box settings.</description><author>Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</author><pubDate>Fri, 18 Aug 2023 16:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05681v2</guid></item><item><title>DiLogics: Creating Web Automation Programs With Diverse Logics</title><link>http://arxiv.org/abs/2308.05828v2</link><description>Knowledge workers frequently encounter repetitive web data entry tasks, likeupdating records or placing orders. Web automation increases productivity, buttranslating tasks to web actions accurately and extending to new specificationsis challenging. Existing tools can automate tasks that perform the same logicaltrace of UI actions (e.g., input text in each field in order), but do notsupport tasks requiring different executions based on varied input conditions.We present DiLogics, a programming-by-demonstration system that utilizes NLP toassist users in creating web automation programs that handle diversespecifications. DiLogics first semantically segments input data to structuredtask steps. By recording user demonstrations for each step, DiLogicsgeneralizes the web macros to novel but semantically similar task requirements.Our evaluation showed that non-experts can effectively use DiLogics to createautomation programs that fulfill diverse input instructions. DiLogics providesan efficient, intuitive, and expressive method for developing web automationprograms satisfying diverse specifications.</description><author>Kevin Pu, Jim Yang, Angel Yuan, Minyi Ma, Rui Dong, Xinyu Wang, Yan Chen, Tovi Grossman</author><pubDate>Fri, 18 Aug 2023 16:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05828v2</guid></item><item><title>GeoDTR+: Toward generic cross-view geolocalization via geometric disentanglement</title><link>http://arxiv.org/abs/2308.09624v1</link><description>Cross-View Geo-Localization (CVGL) estimates the location of a ground imageby matching it to a geo-tagged aerial image in a database. Recent works achieveoutstanding progress on CVGL benchmarks. However, existing methods still sufferfrom poor performance in cross-area evaluation, in which the training andtesting data are captured from completely distinct areas. We attribute thisdeficiency to the lack of ability to extract the geometric layout of visualfeatures and models' overfitting to low-level details. Our preliminary workintroduced a Geometric Layout Extractor (GLE) to capture the geometric layoutfrom input features. However, the previous GLE does not fully exploitinformation in the input feature. In this work, we propose GeoDTR+ with anenhanced GLE module that better models the correlations among visual features.To fully explore the LS techniques from our preliminary work, we furtherpropose Contrastive Hard Samples Generation (CHSG) to facilitate modeltraining. Extensive experiments show that GeoDTR+ achieves state-of-the-art(SOTA) results in cross-area evaluation on CVUSA, CVACT, and VIGOR by a largemargin ($16.44\%$, $22.71\%$, and $17.02\%$ without polar transformation) whilekeeping the same-area performance comparable to existing SOTA. Moreover, weprovide detailed analyses of GeoDTR+.</description><author>Xiaohan Zhang, Xingyu Li, Waqas Sultani, Chen Chen, Safwan Wshah</author><pubDate>Fri, 18 Aug 2023 16:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09624v1</guid></item><item><title>Is context all you need? Scaling Neural Sign Language Translation to Large Domains of Discourse</title><link>http://arxiv.org/abs/2308.09622v1</link><description>Sign Language Translation (SLT) is a challenging task that aims to generatespoken language sentences from sign language videos, both of which havedifferent grammar and word/gloss order. From a Neural Machine Translation (NMT)perspective, the straightforward way of training translation models is to usesign language phrase-spoken language sentence pairs. However, humaninterpreters heavily rely on the context to understand the conveyedinformation, especially for sign language interpretation, where the vocabularysize may be significantly smaller than their spoken language equivalent. Taking direct inspiration from how humans translate, we propose a novelmulti-modal transformer architecture that tackles the translation task in acontext-aware manner, as a human would. We use the context from previoussequences and confident predictions to disambiguate weaker visual cues. Toachieve this we use complementary transformer encoders, namely: (1) A VideoEncoder, that captures the low-level video features at the frame-level, (2) ASpotting Encoder, that models the recognized sign glosses in the video, and (3)A Context Encoder, which captures the context of the preceding sign sequences.We combine the information coming from these encoders in a final transformerdecoder to generate spoken language translations. We evaluate our approach on the recently published large-scale BOBSL dataset,which contains ~1.2M sequences, and on the SRF dataset, which was part of theWMT-SLT 2022 challenge. We report significant improvements on state-of-the-arttranslation performance using contextual information, nearly doubling thereported BLEU-4 scores of baseline approaches.</description><author>Ozge Mercanoglu Sincan, Necati Cihan Camgoz, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 16:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09622v1</guid></item><item><title>Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data</title><link>http://arxiv.org/abs/2307.11792v2</link><description>Quantum Machine Learning (QML) has come into the limelight due to theexceptional computational abilities of quantum computers. With the promises ofnear error-free quantum computers in the not-so-distant future, it is importantthat the effect of multi-qubit interactions on quantum neural networks isstudied extensively. This paper introduces a Quantum Convolutional Network withnovel Interaction layers exploiting three-qubit interactions increasing thenetwork's expressibility and entangling capability, for classifying both imageand one-dimensional data. The proposed approach is tested on three publiclyavailable datasets namely MNIST, Fashion MNIST, and Iris datasets, to performbinary and multiclass classifications and is found to supersede the performanceof the existing state-of-the-art methods.</description><author>Jishnu Mahmud, Raisa Mashtura, Shaikh Anowarul Fattah, Mohammad Saquib</author><pubDate>Fri, 18 Aug 2023 16:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11792v2</guid></item><item><title>LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark</title><link>http://arxiv.org/abs/2308.09618v1</link><description>The progress in maritime obstacle detection is hindered by the lack of adiverse dataset that adequately captures the complexity of general maritimeenvironments. We present the first maritime panoptic obstacle detectionbenchmark LaRS, featuring scenes from Lakes, Rivers and Seas. Our majorcontribution is the new dataset, which boasts the largest diversity inrecording locations, scene types, obstacle classes, and acquisition conditionsamong the related datasets. LaRS is composed of over 4000 per-pixel labeled keyframes with nine preceding frames to allow utilization of the temporal texture,amounting to over 40k frames. Each key frame is annotated with 8 thing, 3 stuffclasses and 19 global scene attributes. We report the results of 27 semanticand panoptic segmentation methods, along with several performance insights andfuture research directions. To enable objective evaluation, we have implementedan online evaluation server. The LaRS dataset, evaluation toolkit and benchmarkare publicly available at: https://lojzezust.github.io/lars-dataset</description><author>Lojze Žust, Janez Perš, Matej Kristan</author><pubDate>Fri, 18 Aug 2023 16:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09618v1</guid></item><item><title>Far3D: Expanding the Horizon for Surround-view 3D Object Detection</title><link>http://arxiv.org/abs/2308.09616v1</link><description>Recently 3D object detection from surround-view images has made notableadvancements with its low deployment cost. However, most works have primarilyfocused on close perception range while leaving long-range detection lessexplored. Expanding existing methods directly to cover long distances poseschallenges such as heavy computation costs and unstable convergence. To addressthese limitations, this paper proposes a novel sparse query-based framework,dubbed Far3D. By utilizing high-quality 2D object priors, we generate 3Dadaptive queries that complement the 3D global queries. To efficiently capturediscriminative features across different views and scales for long-rangeobjects, we introduce a perspective-aware aggregation module. Additionally, wepropose a range-modulated 3D denoising approach to address query errorpropagation and mitigate convergence issues in long-range tasks. Significantly,Far3D demonstrates SoTA performance on the challenging Argoverse 2 dataset,covering a wide range of 150 meters, surpassing several LiDAR-based approaches.Meanwhile, Far3D exhibits superior performance compared to previous methods onthe nuScenes dataset. The code will be available soon.</description><author>Xiaohui Jiang, Shuailin Li, Yingfei Liu, Shihao Wang, Fan Jia, Tiancai Wang, Lijin Han, Xiangyu Zhang</author><pubDate>Fri, 18 Aug 2023 16:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09616v1</guid></item><item><title>Constrained Bayesian Optimization Using a Lagrange Multiplier Applied to Power Transistor Design</title><link>http://arxiv.org/abs/2308.09612v1</link><description>We propose a novel constrained Bayesian Optimization (BO) algorithmoptimizing the design process of Laterally-Diffused Metal-Oxide-Semiconductor(LDMOS) transistors while realizing a target Breakdown Voltage (BV). We convertthe constrained BO problem into a conventional BO problem using a Lagrangemultiplier. Instead of directly optimizing the traditional Figure-of-Merit(FOM), we set the Lagrangian as the objective function of BO. This adaptiveobjective function with a changeable Lagrange multiplier can addressconstrained BO problems which have constraints that require costly evaluations,without the need for additional surrogate models to approximate constraints.Our algorithm enables a device designer to set the target BV in the designspace, and obtain a device that satisfies the optimized FOM and the target BVconstraint automatically. Utilizing this algorithm, we have also explored thephysical limits of the FOM for our devices in 30 - 50 V range within thedefined design space.</description><author>Ping-Ju Chuang, Ali Saadat, Sara Ghazvini, Hal Edwards, William G. Vandenberghe</author><pubDate>Fri, 18 Aug 2023 16:14:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09612v1</guid></item><item><title>Language-guided Human Motion Synthesis with Atomic Actions</title><link>http://arxiv.org/abs/2308.09611v1</link><description>Language-guided human motion synthesis has been a challenging task due to theinherent complexity and diversity of human behaviors. Previous methods facelimitations in generalization to novel actions, often resulting in unrealisticor incoherent motion sequences. In this paper, we propose ATOM (ATomic mOtionModeling) to mitigate this problem, by decomposing actions into atomic actions,and employing a curriculum learning strategy to learn atomic actioncomposition. First, we disentangle complex human motions into a set of atomicactions during learning, and then assemble novel actions using the learnedatomic actions, which offers better adaptability to new actions. Moreover, weintroduce a curriculum learning training strategy that leverages masked motionmodeling with a gradual increase in the mask ratio, and thus facilitates atomicaction assembly. This approach mitigates the overfitting problem commonlyencountered in previous methods while enforcing the model to learn bettermotion representations. We demonstrate the effectiveness of ATOM throughextensive experiments, including text-to-motion and action-to-motion synthesistasks. We further illustrate its superiority in synthesizing plausible andcoherent text-guided human motion sequences.</description><author>Yuanhao Zhai, Mingzhen Huang, Tianyu Luan, Lu Dong, Ifeoma Nwogu, Siwei Lyu, David Doermann, Junsong Yuan</author><pubDate>Fri, 18 Aug 2023 16:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09611v1</guid></item><item><title>Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation</title><link>http://arxiv.org/abs/2307.08388v2</link><description>Accurate segmentation of topological tubular structures, such as bloodvessels and roads, is crucial in various fields, ensuring accuracy andefficiency in downstream tasks. However, many factors complicate the task,including thin local structures and variable global morphologies. In this work,we note the specificity of tubular structures and use this knowledge to guideour DSCNet to simultaneously enhance perception in three stages: featureextraction, feature fusion, and loss constraint. First, we propose a dynamicsnake convolution to accurately capture the features of tubular structures byadaptively focusing on slender and tortuous local structures. Subsequently, wepropose a multi-view feature fusion strategy to complement the attention tofeatures from multiple perspectives during feature fusion, ensuring theretention of important information from different global morphologies. Finally,a continuity constraint loss function, based on persistent homology, isproposed to constrain the topological continuity of the segmentation better.Experiments on 2D and 3D datasets show that our DSCNet provides better accuracyand continuity on the tubular structure segmentation task compared with severalmethods. Our codes will be publicly available.</description><author>Yaolei Qi, Yuting He, Xiaoming Qi, Yuan Zhang, Guanyu Yang</author><pubDate>Fri, 18 Aug 2023 16:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08388v2</guid></item><item><title>On the Effectiveness of LayerNorm Tuning for Continual Learning in Vision Transformers</title><link>http://arxiv.org/abs/2308.09610v1</link><description>State-of-the-art rehearsal-free continual learning methods exploit thepeculiarities of Vision Transformers to learn task-specific prompts,drastically reducing catastrophic forgetting. However, there is a tradeoffbetween the number of learned parameters and the performance, making suchmodels computationally expensive. In this work, we aim to reduce this costwhile maintaining competitive performance. We achieve this by revisiting andextending a simple transfer learning idea: learning task-specific normalizationlayers. Specifically, we tune the scale and bias parameters of LayerNorm foreach continual learning task, selecting them at inference time based on thesimilarity between task-specific keys and the output of the pre-trained model.To make the classifier robust to incorrect selection of parameters duringinference, we introduce a two-stage training procedure, where we first optimizethe task-specific parameters and then train the classifier with the sameselection procedure of the inference time. Experiments on ImageNet-R andCIFAR-100 show that our method achieves results that are either superior or onpar with {the state of the art} while being computationally cheaper.</description><author>Thomas De Min, Massimiliano Mancini, Karteek Alahari, Xavier Alameda-Pineda, Elisa Ricci</author><pubDate>Fri, 18 Aug 2023 16:11:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09610v1</guid></item><item><title>CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow</title><link>http://arxiv.org/abs/2211.10408v3</link><description>Despite impressive performance for high-level downstream tasks,self-supervised pre-training methods have not yet fully delivered on densegeometric vision tasks such as stereo matching or optical flow. The applicationof self-supervised concepts, such as instance discrimination or masked imagemodeling, to geometric tasks is an active area of research. In this work, webuild on the recent cross-view completion framework, a variation of maskedimage modeling that leverages a second view from the same scene which makes itwell suited for binocular downstream tasks. The applicability of this concepthas so far been limited in at least two ways: (a) by the difficulty ofcollecting real-world image pairs -- in practice only synthetic data have beenused -- and (b) by the lack of generalization of vanilla transformers to densedownstream tasks for which relative position is more meaningful than absoluteposition. We explore three avenues of improvement. First, we introduce a methodto collect suitable real-world image pairs at large scale. Second, weexperiment with relative positional embeddings and show that they enable visiontransformers to perform substantially better. Third, we scale up visiontransformer based cross-completion architectures, which is made possible by theuse of large amounts of data. With these improvements, we show for the firsttime that state-of-the-art results on stereo matching and optical flow can bereached without using any classical task-specific techniques like correlationvolume, iterative estimation, image warping or multi-scale reasoning, thuspaving the way towards universal vision models.</description><author>Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Brégier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, Jérôme Revaud</author><pubDate>Fri, 18 Aug 2023 16:06:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10408v3</guid></item><item><title>LVLane: Deep Learning for Lane Detection and Classification in Challenging Conditions</title><link>http://arxiv.org/abs/2307.06853v2</link><description>Lane detection plays a pivotal role in the field of autonomous vehicles andadvanced driving assistant systems (ADAS). Despite advances from imageprocessing to deep learning based models, algorithm performance is highlydependent on training data matching the local challenges such as extremelighting conditions, partially visible lane markings, and sparse lane markingslike Botts' dots. To address this, we present an end-to-end lane detection andclassification system based on deep learning methodologies. In our study, weintroduce a unique dataset meticulously curated to encompass scenarios thatpose significant challenges for state-of-the-art (SOTA) lane localizationmodels. Moreover, we propose a CNN-based classification branch, seamlesslyintegrated with the detector, facilitating the identification of distinct lanetypes. This architecture enables informed lane-changing decisions and empowersmore resilient ADAS capabilities. We also investigate the effect of using mixedprecision training and testing on different models and batch sizes.Experimental evaluations conducted on the widely-used TuSimple dataset, CaltechLane dataset, and our LVLane dataset demonstrate the effectiveness of our modelin accurately detecting and classifying lanes amidst challenging scenarios. Ourmethod achieves state-of-the-art classification results on the TuSimpledataset. The code of the work can be found on www.github.com/zillur-av/LVLane.</description><author>Zillur Rahman, Brendan Tran Morris</author><pubDate>Fri, 18 Aug 2023 16:02:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06853v2</guid></item><item><title>Quantum Image Denoising: A Framework via Boltzmann Machines, QUBO, and Quantum Annealing</title><link>http://arxiv.org/abs/2307.06542v3</link><description>We investigate a framework for binary image denoising via restrictedBoltzmann machines (RBMs) that introduces a denoising objective in quadraticunconstrained binary optimization (QUBO) form and is well-suited for quantumannealing. The denoising objective is attained by balancing the distributionlearned by a trained RBM with a penalty term for derivations from the noisyimage. We derive the statistically optimal choice of the penalty parameterassuming the target distribution has been well-approximated, and furthersuggest an empirically supported modification to make the method robust to thatidealistic assumption. We also show under additional assumptions that thedenoised images attained by our method are, in expectation, strictly closer tothe noise-free images than the noisy images are. While we frame the model as animage denoising model, it can be applied to any binary data. As the QUBOformulation is well-suited for implementation on quantum annealers, we test themodel on a D-Wave Advantage machine, and also test on data too large forcurrent quantum annealers by approximating QUBO solutions through classicalheuristics.</description><author>Phillip Kerger, Ryoji Miyazaki</author><pubDate>Fri, 18 Aug 2023 15:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06542v3</guid></item><item><title>Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks</title><link>http://arxiv.org/abs/2308.09605v1</link><description>Physics-informed neural networks (PINNs) have been demonstrated to beefficient in solving partial differential equations (PDEs) from a variety ofexperimental perspectives. Some recent studies have also proposed PINNalgorithms for PDEs on surfaces, including spheres. However, theoreticalunderstanding of the numerical performance of PINNs, especially PINNs onsurfaces or manifolds, is still lacking. In this paper, we establish rigorousanalysis of the physics-informed convolutional neural network (PICNN) forsolving PDEs on the sphere. By using and improving the latest approximationresults of deep convolutional neural networks and spherical harmonic analysis,we prove an upper bound for the approximation error with respect to the Sobolevnorm. Subsequently, we integrate this with innovative localization complexityanalysis to establish fast convergence rates for PICNN. Our theoretical resultsare also confirmed and supplemented by our experiments. In light of thesefindings, we explore potential strategies for circumventing the curse ofdimensionality that arises when solving high-dimensional PDEs.</description><author>Guanhang Lei, Zhen Lei, Lei Shi, Chenyu Zeng, Ding-Xuan Zhou</author><pubDate>Fri, 18 Aug 2023 15:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09605v1</guid></item><item><title>Breaking the Complexity Barrier in Compositional Minimax Optimization</title><link>http://arxiv.org/abs/2308.09604v1</link><description>Compositional minimax optimization is a pivotal yet under-explored challengeacross machine learning, including distributionally robust training and policyevaluation for reinforcement learning. Current techniques exhibit suboptimalcomplexity or rely heavily on large batch sizes. This paper proposes NestedSTOchastic Recursive Momentum (NSTORM), attaining the optimal sample complexityof $O(\kappa^3/\epsilon^3)$ for finding an $\epsilon$-accurate solution.However, NSTORM requires low learning rates, potentially limitingapplicability. Thus we introduce ADAptive NSTORM (ADA-NSTORM) with adaptivelearning rates, proving it achieves the same sample complexity whileexperiments demonstrate greater effectiveness. Our methods match lower boundsfor minimax optimization without large batch requirements, validated throughextensive experiments. This work significantly advances compositional minimaxoptimization, a crucial capability for distributional robustness and policyevaluation</description><author>Jin Liu, Xiaokang Pan, Junwen Duan, Hongdong Li, Youqi Li, Zhe Qu</author><pubDate>Fri, 18 Aug 2023 15:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09604v1</guid></item><item><title>Language-Guided Diffusion Model for Visual Grounding</title><link>http://arxiv.org/abs/2308.09599v1</link><description>Visual grounding (VG) tasks involve explicit cross-modal alignment, assemantically corresponding image regions are to be located for the languagephrases provided. Existing approaches complete such visual-text reasoning in asingle-step manner. Their performance causes high demands on large-scaleanchors and over-designed multi-modal fusion modules based on human priors,leading to complicated frameworks that may be difficult to train and overfit tospecific scenarios. Even worse, such once-for-all reasoning mechanisms areincapable of refining boxes continuously to enhance query-region matching. Incontrast, in this paper, we formulate an iterative reasoning process bydenoising diffusion modeling. Specifically, we propose a language-guideddiffusion framework for visual grounding, LG-DVG, which trains the model toprogressively reason queried object boxes by denoising a set of noisy boxeswith the language guide. To achieve this, LG-DVG gradually perturbsquery-aligned ground truth boxes to noisy ones and reverses this process stepby step, conditional on query semantics. Extensive experiments for our proposedframework on five widely used datasets validate the superior performance ofsolving visual grounding, a cross-modal alignment task, in a generative way.The source codes are available at\url{https://github.com/iQua/vgbase/tree/DiffusionVG}.</description><author>Sijia Chen, Baochun Li</author><pubDate>Fri, 18 Aug 2023 15:54:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09599v1</guid></item><item><title>Neural Superstatistics for Bayesian Estimation of Dynamic Cognitive Model</title><link>http://arxiv.org/abs/2211.13165v3</link><description>Mathematical models of cognition are often memoryless and ignore potentialfluctuations of their parameters. However, human cognition is inherentlydynamic. Thus, we propose to augment mechanistic cognitive models with atemporal dimension and estimate the resulting dynamics from a superstatisticsperspective. Such a model entails a hierarchy between a low-level observationmodel and a high-level transition model. The observation model describes thelocal behavior of a system, and the transition model specifies how theparameters of the observation model evolve over time. To overcome theestimation challenges resulting from the complexity of superstatistical models,we develop and validate a simulation-based deep learning method for Bayesianinference, which can recover both time-varying and time-invariant parameters.We first benchmark our method against two existing frameworks capable ofestimating time-varying parameters. We then apply our method to fit a dynamicversion of the diffusion decision model to long time series of human responsetimes data. Our results show that the deep learning approach is very efficientin capturing the temporal dynamics of the model. Furthermore, we show that theerroneous assumption of static or homogeneous parameters will hide importanttemporal information.</description><author>Lukas Schumacher, Paul-Christian Bürkner, Andreas Voss, Ullrich Köthe, Stefan T. Radev</author><pubDate>Fri, 18 Aug 2023 15:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13165v3</guid></item><item><title>Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models</title><link>http://arxiv.org/abs/2303.08010v2</link><description>Deep Ensembles are a simple, reliable, and effective method of improving boththe predictive performance and uncertainty estimates of deep learningapproaches. However, they are widely criticised as being computationallyexpensive, due to the need to deploy multiple independent models. Recent workhas challenged this view, showing that for predictive accuracy, ensembles canbe more computationally efficient (at inference) than scaling single modelswithin an architecture family. This is achieved by cascading ensemble membersvia an early-exit approach. In this work, we investigate extending theseefficiency gains to tasks related to uncertainty estimation. As many suchtasks, e.g. selective classification, are binary classification, our key novelinsight is to only pass samples within a window close to the binary decisionboundary to later cascade stages. Experiments on ImageNet-scale data across anumber of network architectures and uncertainty tasks show that the proposedwindow-based early-exit approach is able to achieve a superioruncertainty-computation trade-off compared to scaling single models. Forexample, a cascaded EfficientNet-B2 ensemble is able to achieve similarcoverage at 5% risk as a single EfficientNet-B4 with &lt;30% the number of MACs.We also find that cascades/ensembles give more reliable improvements on OODdata vs scaling models up. Code for this work is available at:https://github.com/Guoxoug/window-early-exit.</description><author>Guoxuan Xia, Christos-Savvas Bouganis</author><pubDate>Fri, 18 Aug 2023 15:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08010v2</guid></item><item><title>ChatHaruhi: Reviving Anime Character in Reality via Large Language Model</title><link>http://arxiv.org/abs/2308.09597v1</link><description>Role-playing chatbots built on large language models have drawn interest, butbetter techniques are needed to enable mimicking specific fictional characters.We propose an algorithm that controls language models via an improved promptand memories of the character extracted from scripts. We construct ChatHaruhi,a dataset covering 32 Chinese / English TV / anime characters with over 54ksimulated dialogues. Both automatic and human evaluations show our approachimproves role-playing ability over baselines. Code and data are available athttps://github.com/LC1332/Chat-Haruhi-Suzumiya .</description><author>Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, Linkang Zhan, Yaokai Jia, Pingyu Wu, Haozhen Sun</author><pubDate>Fri, 18 Aug 2023 15:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09597v1</guid></item><item><title>From Sky to the Ground: A Large-scale Benchmark and Simple Baseline Towards Real Rain Removal</title><link>http://arxiv.org/abs/2308.03867v2</link><description>Learning-based image deraining methods have made great progress. However, thelack of large-scale high-quality paired training samples is the main bottleneckto hamper the real image deraining (RID). To address this dilemma and advanceRID, we construct a Large-scale High-quality Paired real rain benchmark(LHP-Rain), including 3000 video sequences with 1 million high-resolution(1920*1080) frame pairs. The advantages of the proposed dataset over theexisting ones are three-fold: rain with higher-diversity and larger-scale,image with higher-resolution and higher-quality ground-truth. Specifically, thereal rains in LHP-Rain not only contain the classical rainstreak/veiling/occlusion in the sky, but also the \textbf{splashing on theground} overlooked by deraining community. Moreover, we propose a novel robustlow-rank tensor recovery model to generate the GT with better separating thestatic background from the dynamic rain. In addition, we design a simpletransformer-based single image deraining baseline, which simultaneously utilizethe self-attention and cross-layer attention within the image and rain layerwith discriminative feature representation. Extensive experiments verify thesuperiority of the proposed dataset and deraining method over state-of-the-art.</description><author>Yun Guo, Xueyao Xiao, Yi Chang, Shumin Deng, Luxin Yan</author><pubDate>Fri, 18 Aug 2023 15:46:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03867v2</guid></item><item><title>Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification</title><link>http://arxiv.org/abs/2308.09596v1</link><description>Graph neural networks (GNNs) are increasingly used in critical humanapplications for predicting node labels in attributed graphs. Their ability toaggregate features from nodes' neighbors for accurate classification also hasthe capacity to exacerbate existing biases in data or to introduce new onestowards members from protected demographic groups. Thus, it is imperative toquantify how GNNs may be biased and to what extent their harmful effects may bemitigated. To this end, we propose two new GNN-agnostic interventions namely,(i) PFR-AX which decreases the separability between nodes in protected andnon-protected groups, and (ii) PostProcess which updates model predictionsbased on a blackbox policy to minimize differences between error rates acrossdemographic groups. Through a large set of experiments on four datasets, weframe the efficacies of our approaches (and three variants) in terms of theiralgorithmic fairness-accuracy tradeoff and benchmark our results against threestrong baseline interventions on three state-of-the-art GNN models. Our resultsshow that no single intervention offers a universally optimal tradeoff, butPFR-AX and PostProcess provide granular control and improve model confidencewhen correctly predicting positive outcomes for nodes in protected groups.</description><author>Arpit Merchant, Carlos Castillo</author><pubDate>Fri, 18 Aug 2023 15:45:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09596v1</guid></item><item><title>Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents</title><link>http://arxiv.org/abs/2308.09595v1</link><description>Robustly cooperating with unseen agents and human partners presentssignificant challenges due to the diverse cooperative conventions thesepartners may adopt. Existing Ad Hoc Teamwork (AHT) methods address thischallenge by training an agent with a population of diverse teammate policiesobtained through maximizing specific diversity metrics. However, theseheuristic diversity metrics do not always maximize the agent's robustness inall cooperative problems. In this work, we first propose that maximizing an AHTagent's robustness requires it to emulate policies in the minimum coverage set(MCS), the set of best-response policies to any partner policies in theenvironment. We then introduce the L-BRDiv algorithm that generates a set ofteammate policies that, when used for AHT training, encourage agents to emulatepolicies from the MCS. L-BRDiv works by solving a constrained optimizationproblem to jointly train teammate policies for AHT training and approximatingAHT agent policies that are members of the MCS. We empirically demonstrate thatL-BRDiv produces more robust AHT agents than state-of-the-art methods in abroader range of two-player cooperative problems without the need for extensivehyperparameter tuning for its objectives. Our study shows that L-BRDivoutperforms the baseline methods by prioritizing discovering distinct membersof the MCS instead of repeatedly finding redundant policies.</description><author>Arrasy Rahman, Jiaxun Cui, Peter Stone</author><pubDate>Fri, 18 Aug 2023 15:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09595v1</guid></item><item><title>Learning to Generate Training Datasets for Robust Semantic Segmentation</title><link>http://arxiv.org/abs/2308.02535v2</link><description>Semantic segmentation techniques have shown significant progress in recentyears, but their robustness to real-world perturbations and data samples notseen during training remains a challenge, particularly in safety-criticalapplications. In this paper, we propose a novel approach to improve therobustness of semantic segmentation techniques by leveraging the synergybetween label-to-image generators and image-to-label segmentation models.Specifically, we design and train Robusta, a novel robust conditionalgenerative adversarial network to generate realistic and plausible perturbed oroutlier images that can be used to train reliable segmentation models. Weconduct in-depth studies of the proposed generative model, assess theperformance and robustness of the downstream segmentation network, anddemonstrate that our approach can significantly enhance the robustness ofsemantic segmentation techniques in the face of real-world perturbations,distribution shifts, and out-of-distribution samples. Our results suggest thatthis approach could be valuable in safety-critical applications, where thereliability of semantic segmentation techniques is of utmost importance andcomes with a limited computational budget in inference. We will release ourcode shortly.</description><author>Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Shihao Zhang, Andrei Bursuc, Angela Yao, Gianni Franchi</author><pubDate>Fri, 18 Aug 2023 15:42:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02535v2</guid></item><item><title>Investigation of Architectures and Receptive Fields for Appearance-based Gaze Estimation</title><link>http://arxiv.org/abs/2308.09593v1</link><description>With the rapid development of deep learning technology in the past decade,appearance-based gaze estimation has attracted great attention from bothcomputer vision and human-computer interaction research communities.Fascinating methods were proposed with variant mechanisms including softattention, hard attention, two-eye asymmetry, feature disentanglement, rotationconsistency, and contrastive learning. Most of these methods take thesingle-face or multi-region as input, yet the basic architecture of gazeestimation has not been fully explored. In this paper, we reveal the fact thattuning a few simple parameters of a ResNet architecture can outperform most ofthe existing state-of-the-art methods for the gaze estimation task on threepopular datasets. With our extensive experiments, we conclude that the stridenumber, input image resolution, and multi-region architecture are critical forthe gaze estimation performance while their effectiveness dependent on thequality of the input face image. We obtain the state-of-the-art performances onthree datasets with 3.64 on ETH-XGaze, 4.50 on MPIIFaceGaze, and 9.13 onGaze360 degrees gaze estimation error by taking ResNet-50 as the backbone.</description><author>Yunhan Wang, Xiangwei Shi, Shalini De Mello, Hyung Jin Chang, Xucong Zhang</author><pubDate>Fri, 18 Aug 2023 15:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09593v1</guid></item><item><title>DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data</title><link>http://arxiv.org/abs/2308.03295v2</link><description>With the rapid evolution of the Internet of Things, many real-worldapplications utilize heterogeneously connected sensors to capture time-seriesinformation. Edge-based machine learning (ML) methodologies are often employedto analyze locally collected data. However, a fundamental issue acrossdata-driven ML approaches is distribution shift. It occurs when a model isdeployed on a data distribution different from what it was trained on, and cansubstantially degrade model performance. Additionally, increasinglysophisticated deep neural networks (DNNs) have been proposed to capture spatialand temporal dependencies in multi-sensor time series data, requiring intensivecomputational resources beyond the capacity of today's edge devices. Whilebrain-inspired hyperdimensional computing (HDC) has been introduced as alightweight solution for edge-based learning, existing HDCs are also vulnerableto the distribution shift challenge. In this paper, we propose DOMINO, a novelHDC learning framework addressing the distribution shift problem in noisymulti-sensor time-series data. DOMINO leverages efficient and parallel matrixoperations on high-dimensional space to dynamically identify and filter outdomain-variant dimensions. Our evaluation on a wide range of multi-sensor timeseries classification tasks shows that DOMINO achieves on average 2.04% higheraccuracy than state-of-the-art (SOTA) DNN-based domain generalizationtechniques, and delivers 16.34x faster training and 2.89x faster inference.More importantly, DOMINO performs notably better when learning from partiallylabeled and highly imbalanced data, providing 10.93x higher robustness againsthardware noises than SOTA DNNs.</description><author>Junyao Wang, Luke Chen, Mohammad Abdullah Al Faruque</author><pubDate>Fri, 18 Aug 2023 15:40:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03295v2</guid></item><item><title>StableVideo: Text-driven Consistency-aware Diffusion Video Editing</title><link>http://arxiv.org/abs/2308.09592v1</link><description>Diffusion-based methods can generate realistic images and videos, but theystruggle to edit existing objects in a video while preserving their appearanceover time. This prevents diffusion models from being applied to natural videoediting in practical scenarios. In this paper, we tackle this problem byintroducing temporal dependency to existing text-driven diffusion models, whichallows them to generate consistent appearance for the edited objects.Specifically, we develop a novel inter-frame propagation mechanism fordiffusion video editing, which leverages the concept of layered representationsto propagate the appearance information from one frame to the next. We thenbuild up a text-driven video editing framework based on this mechanism, namelyStableVideo, which can achieve consistency-aware video editing. Extensiveexperiments demonstrate the strong editing capability of our approach. Comparedwith state-of-the-art video editing methods, our approach shows superiorqualitative and quantitative results. Our code is available at\href{https://github.com/rese1f/StableVideo}{this https URL}.</description><author>Wenhao Chai, Xun Guo, Gaoang Wang, Yan Lu</author><pubDate>Fri, 18 Aug 2023 15:39:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09592v1</guid></item><item><title>O^2-Recon: Completing 3D Reconstruction of Occluded Objects in the Scene with a Pre-trained 2D Diffusion Model</title><link>http://arxiv.org/abs/2308.09591v1</link><description>Occlusion is a common issue in 3D reconstruction from RGB-D videos, oftenblocking the complete reconstruction of objects and presenting an ongoingproblem. In this paper, we propose a novel framework, empowered by a 2Ddiffusion-based in-painting model, to reconstruct complete surfaces for thehidden parts of objects. Specifically, we utilize a pre-trained diffusion modelto fill in the hidden areas of 2D images. Then we use these in-painted imagesto optimize a neural implicit surface representation for each instance for 3Dreconstruction. Since creating the in-painting masks needed for this process istricky, we adopt a human-in-the-loop strategy that involves very little humanengagement to generate high-quality masks. Moreover, some parts of objects canbe totally hidden because the videos are usually shot from limitedperspectives. To ensure recovering these invisible areas, we develop a cascadednetwork architecture for predicting signed distance field, making use ofdifferent frequency bands of positional encoding and maintaining overallsmoothness. Besides the commonly used rendering loss, Eikonal loss, andsilhouette loss, we adopt a CLIP-based semantic consistency loss to guide thesurface from unseen camera angles. Experiments on ScanNet scenes show that ourproposed framework achieves state-of-the-art accuracy and completeness inobject-level reconstruction from scene-level RGB-D videos.</description><author>Yubin Hu, Sheng Ye, Wang Zhao, Matthieu Lin, Yuze He, Yu-Hui Wen, Ying He, Yong-Jin Liu</author><pubDate>Fri, 18 Aug 2023 15:38:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09591v1</guid></item><item><title>Self-supervised Character-to-Character Distillation for Text Recognition</title><link>http://arxiv.org/abs/2211.00288v4</link><description>When handling complicated text images (e.g., irregular structures, lowresolution, heavy occlusion, and uneven illumination), existing supervised textrecognition methods are data-hungry. Although these methods employ large-scalesynthetic text images to reduce the dependence on annotated real images, thedomain gap still limits the recognition performance. Therefore, exploring therobust text feature representations on unlabeled real images by self-supervisedlearning is a good solution. However, existing self-supervised text recognitionmethods conduct sequence-to-sequence representation learning by roughlysplitting the visual features along the horizontal axis, which limits theflexibility of the augmentations, as large geometric-based augmentations maylead to sequence-to-sequence feature inconsistency. Motivated by this, wepropose a novel self-supervised Character-to-Character Distillation method,CCD, which enables versatile augmentations to facilitate general textrepresentation learning. Specifically, we delineate the character structures ofunlabeled real images by designing a self-supervised character segmentationmodule. Following this, CCD easily enriches the diversity of local characterswhile keeping their pairwise alignment under flexible augmentations, using thetransformation matrix between two augmented views from images. Experimentsdemonstrate that CCD achieves state-of-the-art results, with averageperformance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24dB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code is available athttps://github.com/TongkunGuan/CCD.</description><author>Tongkun Guan, Wei Shen, Xue Yang, Qi Feng, Zekun Jiang, Xiaokang Yang</author><pubDate>Fri, 18 Aug 2023 15:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.00288v4</guid></item><item><title>RoCourseNet: Distributionally Robust Training of a Prediction Aware Recourse Model</title><link>http://arxiv.org/abs/2206.00700v2</link><description>Counterfactual (CF) explanations for machine learning (ML) models arepreferred by end-users, as they explain the predictions of ML models byproviding a recourse (or contrastive) case to individuals who are adverselyimpacted by predicted outcomes. Existing CF explanation methods generaterecourses under the assumption that the underlying target ML model remainsstationary over time. However, due to commonly occurring distributional shiftsin training data, ML models constantly get updated in practice, which mightrender previously generated recourses invalid and diminish end-users trust inour algorithmic framework. To address this problem, we propose RoCourseNet, atraining framework that jointly optimizes predictions and recourses that arerobust to future data shifts. This work contains four key contributions: (1) Weformulate the robust recourse generation problem as a tri-level optimizationproblem which consists of two sub-problems: (i) a bi-level problem that findsthe worst-case adversarial shift in the training data, and (ii) an outerminimization problem to generate robust recourses against this worst-caseshift. (2) We leverage adversarial training to solve this tri-leveloptimization problem by: (i) proposing a novel virtual data shift (VDS)algorithm to find worst-case shifted ML models via explicitly considering theworst-case data shift in the training dataset, and (ii) a block-wise coordinatedescent procedure to optimize for prediction and corresponding robustrecourses. (3) We evaluate RoCourseNet's performance on three real-worlddatasets, and show that RoCourseNet consistently achieves more than 96% robustvalidity and outperforms state-of-the-art baselines by at least 10% ingenerating robust CF explanations. (4) Finally, we generalize the RoCourseNetframework to accommodate any parametric post-hoc methods for improving robustvalidity.</description><author>Hangzhi Guo, Feiran Jia, Jinghui Chen, Anna Squicciarini, Amulya Yadav</author><pubDate>Fri, 18 Aug 2023 15:31:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.00700v2</guid></item><item><title>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</title><link>http://arxiv.org/abs/2308.09583v1</link><description>Large language models (LLMs), such as GPT-4, have shown remarkableperformance in natural language processing (NLP) tasks, including challengingmathematical reasoning. However, most existing open-source models are onlypre-trained on large-scale internet data and without math-related optimization.In this paper, we present WizardMath, which enhances the mathematical reasoningabilities of Llama-2, by applying our proposed Reinforcement Learning fromEvol-Instruct Feedback (RLEIF) method to the domain of math. Through extensiveexperiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, wereveal the extraordinary capabilities of our model. WizardMath surpasses allother open-source LLMs by a substantial margin. Furthermore, our model evenoutperforms ChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k,simultaneously surpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. Moredetails and model weights are public at https://github.com/nlpxucan/WizardLMand https://huggingface.co/WizardLM.</description><author>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang</author><pubDate>Fri, 18 Aug 2023 15:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09583v1</guid></item><item><title>Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</title><link>http://arxiv.org/abs/2308.06668v2</link><description>The past decade has witnessed the rapid development of ML and DLmethodologies in agricultural systems, showcased by great successes in varietyof agricultural applications. However, these conventional ML/DL models havecertain limitations: They heavily rely on large, costly-to-acquire labeleddatasets for training, require specialized expertise for development andmaintenance, and are mostly tailored for specific tasks, thus lackinggeneralizability. Recently, foundation models have demonstrated remarkablesuccesses in language and vision tasks across various domains. These models aretrained on a vast amount of data from multiple domains and modalities. Oncetrained, they can accomplish versatile tasks with just minor fine-tuning andminimal task-specific labeled data. Despite their proven effectiveness and hugepotential, there has been little exploration of applying FMs to agriculturefields. Therefore, this study aims to explore the potential of FMs in the fieldof smart agriculture. In particular, we present conceptual tools and technicalbackground to facilitate the understanding of the problem space and uncover newresearch directions in this field. To this end, we first review recent FMs inthe general computer science domain and categorize them into four categories:language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs.Subsequently, we outline the process of developing agriculture FMs and discusstheir potential applications in smart agriculture. We also discuss the uniquechallenges associated with developing AFMs, including model training,validation, and deployment. Through this study, we contribute to theadvancement of AI in agriculture by introducing AFMs as a promising paradigmthat can significantly mitigate the reliance on extensive labeled datasets andenhance the efficiency, effectiveness, and generalization of agricultural AIsystems.</description><author>Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan Yin, Zhaojian Li</author><pubDate>Fri, 18 Aug 2023 15:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06668v2</guid></item><item><title>Generalized Bandit Regret Minimizer Framework in Imperfect Information Extensive-Form Game</title><link>http://arxiv.org/abs/2203.05920v4</link><description>Regret minimization methods are a powerful tool for learning approximate Nashequilibrium (NE) in two-player zero-sum imperfect information extensive-formgames (IIEGs). We consider the problem in the interactive bandit-feedbacksetting where we don't know the dynamics of the IIEG. In general, only theinteractive trajectory and the reached terminal node value $v(z^t)$ arerevealed. To learn NE, the regret minimizer is required to estimate thefull-feedback loss gradient $\ell^t$ by $v(z^t)$ and minimize the regret. Inthis paper, we propose a generalized framework for this learning setting. Itpresents a theoretical framework for the design and the modular analysis of thebandit regret minimization methods. We demonstrate that the most recent banditregret minimization methods can be analyzed as a particular case of ourframework. Following this framework, we describe a novel method SIX-OMD tolearn approximate NE. It is model-free and extremely improves the best existingconvergence rate from the order of $O(\sqrt{X B/T}+\sqrt{Y C/T})$ to $O(\sqrt{M_{\mathcal{X}}/T} +\sqrt{ M_{\mathcal{Y}}/T})$. Moreover, SIX-OMD iscomputationally efficient as it needs to perform the current strategy andaverage strategy updates only along the sampled trajectory.</description><author>Linjian Meng, Yang Gao</author><pubDate>Fri, 18 Aug 2023 15:16:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.05920v4</guid></item><item><title>Physics-Informed Boundary Integral Networks (PIBI-Nets): A Data-Driven Approach for Solving Partial Differential Equations</title><link>http://arxiv.org/abs/2308.09571v1</link><description>Partial differential equations (PDEs) can describe many relevant phenomena indynamical systems. In real-world applications, we commonly need to combineformal PDE models with (potentially noisy) observations. This is especiallyrelevant in settings where we lack information about boundary or initialconditions, or where we need to identify unknown model parameters. In recentyears, Physics-informed neural networks (PINNs) have become a popular tool forproblems of this kind. In high-dimensional settings, however, PINNs oftensuffer from computational problems because they usually require densecollocation points over the entire computational domain. To address thisproblem, we present Physics-Informed Boundary Integral Networks (PIBI-Nets) asa data-driven approach for solving PDEs in one dimension less than the originalproblem space. PIBI-Nets only need collocation points at the computationaldomain boundary, while still achieving highly accurate results, and in severalpractical settings, they clearly outperform PINNs. Exploiting elementaryproperties of fundamental solutions of linear differential operators, wepresent a principled and simple way to handle point sources in inverseproblems. We demonstrate the excellent performance of PIBI-Nets for the Laplaceand Poisson equations, both on artificial data sets and within a real-worldapplication concerning the reconstruction of groundwater flows.</description><author>Monika Nagy-Huber, Volker Roth</author><pubDate>Fri, 18 Aug 2023 15:03:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09571v1</guid></item><item><title>Investigating the Interplay between Features and Structures in Graph Learning</title><link>http://arxiv.org/abs/2308.09570v1</link><description>In the past, the dichotomy between homophily and heterophily has inspiredresearch contributions toward a better understanding of Deep Graph Networks'inductive bias. In particular, it was believed that homophily stronglycorrelates with better node classification predictions of message-passingmethods. More recently, however, researchers pointed out that such dichotomy istoo simplistic as we can construct node classification tasks where graphs arecompletely heterophilic but the performances remain high. Most of these workshave also proposed new quantitative metrics to understand when a graphstructure is useful, which implicitly or explicitly assume the correlationbetween node features and target labels. Our work empirically investigates whathappens when this strong assumption does not hold, by formalising twogenerative processes for node classification tasks that allow us to build andstudy ad-hoc problems. To quantitatively measure the influence of the nodefeatures on the target labels, we also use a metric we call FeatureInformativeness. We construct six synthetic tasks and evaluate the performanceof six models, including structure-agnostic ones. Our findings reveal thatpreviously defined metrics are not adequate when we relax the above assumption.Our contribution to the workshop aims at presenting novel research findingsthat could help advance our understanding of the field.</description><author>Daniele Castellana, Federico Errica</author><pubDate>Fri, 18 Aug 2023 15:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09570v1</guid></item><item><title>PUMGPT: A Large Vision-Language Model for Product Understanding</title><link>http://arxiv.org/abs/2308.09568v1</link><description>Recent developments of multi-modal large language models have demonstratedits strong ability in solving vision-language tasks. In this paper, we focus onthe product understanding task, which plays an essential role in enhancingonline shopping experience. Product understanding task includes a variety ofsub-tasks, which require models to respond diverse queries based on multi-modalproduct information. Traditional methods design distinct model architecturesfor each sub-task. On the contrary, we present PUMGPT, a large vision-languagemodel aims at unifying all product understanding tasks under a singular modelstructure. To bridge the gap between vision and text representations, wepropose Layer-wise Adapters (LA), an approach that provides enhanced alignmentwith fewer visual tokens and enables parameter-efficient fine-tuning. Moreover,the inherent parameter-efficient fine-tuning ability allows PUMGPT to bereadily adapted to new product understanding tasks and emerging products. Wedesign instruction templates to generate diverse product instruction datasets.Simultaneously, we utilize open-domain datasets during training to improve theperformance of PUMGPT and its generalization ability. Through extensiveevaluations, PUMGPT demonstrates its superior performance across multipleproduct understanding tasks, including product captioning, categoryquestion-answering, attribute extraction, attribute question-answering, andeven free-form question-answering about products.</description><author>Shuhui Wu, Zengming Tang, Zongyi Guo, Weiwei Zhang, Baoliang Cui, Haihong Tang, Weiming Lu</author><pubDate>Fri, 18 Aug 2023 15:01:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09568v1</guid></item><item><title>Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift</title><link>http://arxiv.org/abs/2308.09565v1</link><description>Layer normalization (LN) is a widely adopted deep learning techniqueespecially in the era of foundation models. Recently, LN has been shown to besurprisingly effective in federated learning (FL) with non-i.i.d. data.However, exactly why and how it works remains mysterious. In this work, wereveal the profound connection between layer normalization and the label shiftproblem in federated learning. To understand layer normalization better in FL,we identify the key contributing mechanism of normalization methods in FL,called feature normalization (FN), which applies normalization to the latentfeature representation before the classifier head. Although LN and FN do notimprove expressive power, they control feature collapse and local overfittingto heavily skewed datasets, and thus accelerates global training. Empirically,we show that normalization leads to drastic improvements on standard benchmarksunder extreme label shift. Moreover, we conduct extensive ablation studies tounderstand the critical factors of layer normalization in FL. Our resultsverify that FN is an essential ingredient inside LN to significantly improvethe convergence of FL while remaining robust to learning rate choices,especially under extreme label shift where each client has access to fewclasses.</description><author>Guojun Zhang, Mahdi Beitollahi, Alex Bie, Xi Chen</author><pubDate>Fri, 18 Aug 2023 14:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09565v1</guid></item><item><title>Deep Equilibrium Object Detection</title><link>http://arxiv.org/abs/2308.09564v1</link><description>Query-based object detectors directly decode image features into objectinstances with a set of learnable queries. These query vectors areprogressively refined to stable meaningful representations through a sequenceof decoder layers, and then used to directly predict object locations andcategories with simple FFN heads. In this paper, we present a new query-basedobject detector (DEQDet) by designing a deep equilibrium decoder. Our DEQdecoder models the query vector refinement as the fixed point solving of an{implicit} layer and is equivalent to applying {infinite} steps of refinement.To be more specific to object decoding, we use a two-step unrolled equilibriumequation to explicitly capture the query vector refinement. Accordingly, we areable to incorporate refinement awareness into the DEQ training with the inexactgradient back-propagation (RAG). In addition, to stabilize the training of ourDEQDet and improve its generalization ability, we devise the deep supervisionscheme on the optimization path of DEQ with refinement-awareperturbation~(RAP). Our experiments demonstrate DEQDet converges faster,consumes less memory, and achieves better results than the baseline counterpart(AdaMixer). In particular, our DEQDet with ResNet50 backbone and 300 queriesachieves the $49.5$ mAP and $33.0$ AP$_s$ on the MS COCO benchmark under$2\times$ training scheme (24 epochs).</description><author>Shuai Wang, Yao Teng, Limin Wang</author><pubDate>Fri, 18 Aug 2023 14:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09564v1</guid></item><item><title>A Comprehensive Overview of Large Language Models</title><link>http://arxiv.org/abs/2307.06435v2</link><description>Large Language Models (LLMs) have recently demonstrated remarkablecapabilities in natural language processing tasks and beyond. This success ofLLMs has led to a large influx of research contributions in this direction.These works encompass diverse topics such as architectural innovations of theunderlying neural networks, context length improvements, model alignment,training datasets, benchmarking, efficiency and more. With the rapiddevelopment of techniques and regular breakthroughs in LLM research, it hasbecome considerably challenging to perceive the bigger picture of the advancesin this direction. Considering the rapidly emerging plethora of literature onLLMs, it is imperative that the research community is able to benefit from aconcise yet comprehensive overview of the recent developments in this field.This article provides that overview to the research community. It not onlyfocuses on a systematic treatment of the existing literature on a broad rangeof LLM related concept, but also pays special attention to providingcomprehensive summaries with extensive details about the individual existingmodels, datasets and major insights. We also pay heed to aligning our overviewwith the emerging outlook of this research direction by accounting for theother recently materializing reviews of the broader research direction of LLMs.Our self-contained comprehensive overview of LLMs discusses relevant backgroundconcepts along with covering the advanced topics at the frontier of thisresearch direction. This review article is intended to not only provide asystematic survey, but also a quick comprehensive reference for the researchersand practitioners to draw insights from extensive informative summaries of theexisting works to advance the LLM research direction.</description><author>Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian</author><pubDate>Fri, 18 Aug 2023 14:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06435v2</guid></item><item><title>A Principle for Global Optimization with Gradients</title><link>http://arxiv.org/abs/2308.09556v1</link><description>This work demonstrates the utility of gradients for the global optimizationof certain differentiable functions with many suboptimal local minima. To thisend, a principle for generating search directions from non-local quadraticapproximants based on gradients of the objective function is analyzed.Experiments measure the quality of non-local search directions as well as theperformance of a proposed simplistic algorithm, of the covariance matrixadaptation evolution strategy (CMA-ES), and of a randomly reinitializedBroyden-Fletcher-Goldfarb-Shanno (BFGS) method.</description><author>Nils Müller</author><pubDate>Fri, 18 Aug 2023 14:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09556v1</guid></item><item><title>On the Limitations of Model Stealing with Uncertainty Quantification Models</title><link>http://arxiv.org/abs/2305.05293v2</link><description>Model stealing aims at inferring a victim model's functionality at a fractionof the original training cost. While the goal is clear, in practice the model'sarchitecture, weight dimension, and original training data can not bedetermined exactly, leading to mutual uncertainty during stealing. In thiswork, we explicitly tackle this uncertainty by generating multiple possiblenetworks and combining their predictions to improve the quality of the stolenmodel. For this, we compare five popular uncertainty quantification models in amodel stealing task. Surprisingly, our results indicate that the consideredmodels only lead to marginal improvements in terms of label agreement (i.e.,fidelity) to the stolen model. To find the cause of this, we inspect thediversity of the model's prediction by looking at the prediction variance as afunction of training iterations. We realize that during training, the modelstend to have similar predictions, indicating that the network diversity wewanted to leverage using uncertainty quantification models is not (high) enoughfor improvements on the model stealing task.</description><author>David Pape, Sina Däubener, Thorsten Eisenhofer, Antonio Emanuele Cinà, Lea Schönherr</author><pubDate>Fri, 18 Aug 2023 14:35:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05293v2</guid></item><item><title>Attesting Distributional Properties of Training Data for Machine Learning</title><link>http://arxiv.org/abs/2308.09552v1</link><description>The success of machine learning (ML) has been accompanied by increasedconcerns about its trustworthiness. Several jurisdictions are preparing MLregulatory frameworks. One such concern is ensuring that model training datahas desirable distributional properties for certain sensitive attributes. Forexample, draft regulations indicate that model trainers are required to showthat training datasets have specific distributional properties, such asreflecting diversity of the population. We propose the notion of property attestation allowing a prover (e.g., modeltrainer) to demonstrate relevant distributional properties of training data toa verifier (e.g., a customer) without revealing the data. We present aneffective hybrid property attestation combining property inference withcryptographic mechanisms.</description><author>Vasisht Duddu, Anudeep Das, Nora Khayata, Hossein Yalame, Thomas Schneider, N. Asokan</author><pubDate>Fri, 18 Aug 2023 14:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09552v1</guid></item><item><title>Local Function Complexity for Active Learning via Mixture of Gaussian Processes</title><link>http://arxiv.org/abs/1902.10664v5</link><description>Inhomogeneities in real-world data, e.g., due to changes in the observationnoise level or variations in the structural complexity of the source function,pose a unique set of challenges for statistical inference. Accounting for themcan greatly improve predictive power when physical resources or computationtime is limited. In this paper, we draw on recent theoretical results on theestimation of local function complexity (LFC), derived from the domain of localpolynomial smoothing (LPS), to establish a notion of local structuralcomplexity, which is used to develop a model-agnostic active learning (AL)framework. Due to its reliance on pointwise estimates, the LPS model class isnot robust and scalable concerning large input space dimensions that typicallycome along with real-world problems. Here, we derive and estimate the Gaussianprocess regression (GPR)-based analog of the LPS-based LFC and use it as asubstitute in the above framework to make it robust and scalable. We assess theeffectiveness of our LFC estimate in an AL application on a prototypicallow-dimensional synthetic dataset, before taking on the challenging real-worldtask of reconstructing a quantum chemical force field for a small organicmolecule and demonstrating state-of-the-art performance with a significantlyreduced training demand.</description><author>Danny Panknin, Stefan Chmiela, Klaus-Robert Müller, Shinichi Nakajima</author><pubDate>Fri, 18 Aug 2023 14:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1902.10664v5</guid></item><item><title>Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning</title><link>http://arxiv.org/abs/2308.09544v1</link><description>In this work, we investigate exemplar-free class incremental learning (CIL)with knowledge distillation (KD) as a regularization strategy, aiming toprevent forgetting. KD-based methods are successfully used in CIL, but theyoften struggle to regularize the model without access to exemplars of thetraining data from previous tasks. Our analysis reveals that this issueoriginates from substantial representation shifts in the teacher network whendealing with out-of-distribution data. This causes large errors in the KD losscomponent, leading to performance degradation in CIL. Inspired by recenttest-time adaptation methods, we introduce Teacher Adaptation (TA), a methodthat concurrently updates the teacher and the main model during incrementaltraining. Our method seamlessly integrates with KD-based CIL approaches andallows for consistent enhancement of their performance across multipleexemplar-free CIL benchmarks.</description><author>Filip Szatkowski, Mateusz Pyla, Marcin Przewięźlikowski, Sebastian Cygert, Bartłomiej Twardowski, Tomasz Trzciński</author><pubDate>Fri, 18 Aug 2023 14:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09544v1</guid></item><item><title>Latent State Models of Training Dynamics</title><link>http://arxiv.org/abs/2308.09543v1</link><description>The impact of randomness on model training is poorly understood. How dodifferences in data order and initialization actually manifest in the model,such that some training runs outperform others or converge faster? Furthermore,how can we interpret the resulting training dynamics and the phase transitionsthat characterize different trajectories? To understand the effect ofrandomness on the dynamics and outcomes of neural network training, we trainmodels multiple times with different random seeds and compute a variety ofmetrics throughout training, such as the $L_2$ norm, mean, and variance of theneural network's weights. We then fit a hidden Markov model (HMM) over theresulting sequences of metrics. The HMM represents training as a stochasticprocess of transitions between latent states, providing an intuitive overviewof significant changes during training. Using our method, we produce alow-dimensional, discrete representation of training dynamics on grokkingtasks, image classification, and masked language modeling. We use the HMMrepresentation to study phase transitions and identify latent "detour" statesthat slow down convergence.</description><author>Michael Y. Hu, Angelica Chen, Naomi Saphra, Kyunghyun Cho</author><pubDate>Fri, 18 Aug 2023 14:20:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09543v1</guid></item><item><title>Decoupled conditional contrastive learning with variable metadata for prostate lesion detection</title><link>http://arxiv.org/abs/2308.09542v1</link><description>Early diagnosis of prostate cancer is crucial for efficient treatment.Multi-parametric Magnetic Resonance Images (mp-MRI) are widely used for lesiondetection. The Prostate Imaging Reporting and Data System (PI-RADS) hasstandardized interpretation of prostate MRI by defining a score for lesionmalignancy. PI-RADS data is readily available from radiology reports but issubject to high inter-reports variability. We propose a new contrastive lossfunction that leverages weak metadata with multiple annotators per sample andtakes advantage of inter-reports variability by defining metadata confidence.By combining metadata of varying confidence with unannotated data into a singleconditional contrastive loss function, we report a 3% AUC increase on lesiondetection on the public PI-CAI challenge dataset. Code is available at: https://github.com/camilleruppli/decoupled_ccl</description><author>Camille Ruppli, Pietro Gori, Roberto Ardon, Isabelle Bloch</author><pubDate>Fri, 18 Aug 2023 14:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09542v1</guid></item><item><title>Meta-ZSDETR: Zero-shot DETR with Meta-learning</title><link>http://arxiv.org/abs/2308.09540v1</link><description>Zero-shot object detection aims to localize and recognize objects of unseenclasses. Most of existing works face two problems: the low recall of RPN inunseen classes and the confusion of unseen classes with background. In thispaper, we present the first method that combines DETR and meta-learning toperform zero-shot object detection, named Meta-ZSDETR, where model training isformalized as an individual episode based meta-learning task. Different fromFaster R-CNN based methods that firstly generate class-agnostic proposals, andthen classify them with visual-semantic alignment module, Meta-ZSDETR directlypredict class-specific boxes with class-specific queries and further filterthem with the predicted accuracy from classification head. The model isoptimized with meta-contrastive learning, which contains a regression head togenerate the coordinates of class-specific boxes, a classification head topredict the accuracy of generated boxes, and a contrastive head that utilizesthe proposed contrastive-reconstruction loss to further separate differentclasses in visual space. We conduct extensive experiments on two benchmarkdatasets MS COCO and PASCAL VOC. Experimental results show that our methodoutperforms the existing ZSD methods by a large margin.</description><author>Lu Zhang, Chenbo Zhang, Jiajia Zhao, Jihong Guan, Shuigeng Zhou</author><pubDate>Fri, 18 Aug 2023 14:17:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09540v1</guid></item><item><title>Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI</title><link>http://arxiv.org/abs/2308.09538v1</link><description>The application of deep learning models to large-scale data sets requiresmeans for automatic quality assurance. We have previously developed a fullyautomatic algorithm for carotid artery wall segmentation in black-blood MRIthat we aim to apply to large-scale data sets. This method identifies nestedartery walls in 3D patches centered on the carotid artery. In this study, weinvestigate to what extent the uncertainty in the model predictions for thecontour location can serve as a surrogate for error detection and,consequently, automatic quality assurance. We express the quality of automaticsegmentations using the Dice similarity coefficient. The uncertainty in themodel's prediction is estimated using either Monte Carlo dropout or test-timedata augmentation. We found that (1) including uncertainty measurements did notdegrade the quality of the segmentations, (2) uncertainty metrics provide agood proxy of the quality of our contours if the center found during the firststep is enclosed in the lumen of the carotid artery and (3) they could be usedto detect low-quality segmentations at the participant level. This automaticquality assurance tool might enable the application of our model in large-scaledata sets.</description><author>Elina Thibeau-Sutre, Dieuwertje Alblas, Sophie Buurman, Christoph Brune, Jelmer M. Wolterink</author><pubDate>Fri, 18 Aug 2023 14:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09538v1</guid></item><item><title>Small Object Detection via Coarse-to-fine Proposal Generation and Imitation Learning</title><link>http://arxiv.org/abs/2308.09534v1</link><description>The past few years have witnessed the immense success of object detection,while current excellent detectors struggle on tackling size-limited instances.Concretely, the well-known challenge of low overlaps between the priors andobject regions leads to a constrained sample pool for optimization, and thepaucity of discriminative information further aggravates the recognition. Toalleviate the aforementioned issues, we propose CFINet, a two-stage frameworktailored for small object detection based on the Coarse-to-fine pipeline andFeature Imitation learning. Firstly, we introduce Coarse-to-fine RPN (CRPN) toensure sufficient and high-quality proposals for small objects through thedynamic anchor selection strategy and cascade regression. Then, we equip theconventional detection head with a Feature Imitation (FI) branch to facilitatethe region representations of size-limited instances that perplex the model inan imitation manner. Moreover, an auxiliary imitation loss following supervisedcontrastive learning paradigm is devised to optimize this branch. Whenintegrated with Faster RCNN, CFINet achieves state-of-the-art performance onthe large-scale small object detection benchmarks, SODA-D and SODA-A,underscoring its superiority over baseline detector and other mainstreamdetection approaches.</description><author>Xiang Yuan, Gong Cheng, Kebing Yan, Qinghua Zeng, Junwei Han</author><pubDate>Fri, 18 Aug 2023 14:13:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09534v1</guid></item><item><title>Privacy-Preserving 3-Layer Neural Network Training using Mere Homomorphic Encryption Technique</title><link>http://arxiv.org/abs/2308.09531v1</link><description>In this manuscript, we consider the problem of privacy-preserving training ofneural networks in the mere homomorphic encryption setting. We combine severalexsiting techniques available, extend some of them, and finally enable thetraining of 3-layer neural networks for both the regression and classificationproblems using mere homomorphic encryption technique.</description><author>John Chiang</author><pubDate>Fri, 18 Aug 2023 14:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09531v1</guid></item><item><title>A serial dual-channel library occupancy detection system based on Faster RCNN</title><link>http://arxiv.org/abs/2306.16080v2</link><description>The phenomenon of seat occupancy in university libraries is a prevalentissue. However, existing solutions, such as software-based seat reservationsand sensors-based occupancy detection, have proven to be inadequate ineffectively addressing this problem. In this study, we propose a novelapproach: a serial dual-channel object detection model based on Faster RCNN.This model is designed to discern all instances of occupied seats within thelibrary and continuously update real-time information regarding seat occupancystatus. To train the neural network, a distinctive dataset is utilized, whichblends virtual images generated using Unreal Engine 5 (UE5) with real-worldimages. Notably, our test results underscore the remarkable performance upliftattained through the application of self-generated virtual datasets in trainingConvolutional Neural Networks (CNNs), particularly within specializedscenarios. Furthermore, this study introduces a pioneering detection model thatseamlessly amalgamates the Faster R-CNN-based object detection framework with atransfer learning-based object classification algorithm. This amalgamation notonly significantly curtails the computational resources and time investmentsneeded for neural network training but also considerably heightens theefficiency of single-frame detection rates. Additionally, a user-friendly webinterface and a mobile application have been meticulously developed,constituting a computer vision-driven platform for detecting seat occupancywithin library premises. Noteworthy is the substantial enhancement in seatoccupancy recognition accuracy, coupled with a reduction in computationalresources required for neural network training, collectively contributing to aconsiderable amplification in the overall efficiency of library seatmanagement.</description><author>Guoqiang Yang, Xiaowen Chang, Zitong Wang, Min Yang</author><pubDate>Fri, 18 Aug 2023 14:11:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16080v2</guid></item><item><title>Improving 3D Pose Estimation for Sign Language</title><link>http://arxiv.org/abs/2308.09525v1</link><description>This work addresses 3D human pose reconstruction in single images. We presenta method that combines Forward Kinematics (FK) with neural networks to ensure afast and valid prediction of 3D pose. Pose is represented as a hierarchicaltree/graph with nodes corresponding to human joints that model their physicallimits. Given a 2D detection of keypoints in the image, we lift the skeleton to3D using neural networks to predict both the joint rotations and bone lengths.These predictions are then combined with skeletal constraints using an FK layerimplemented as a network layer in PyTorch. The result is a fast and accurateapproach to the estimation of 3D skeletal pose. Through quantitative andqualitative evaluation, we demonstrate the method is significantly moreaccurate than MediaPipe in terms of both per joint positional error and visualappearance. Furthermore, we demonstrate generalization over different datasets.The implementation in PyTorch runs at between 100-200 milliseconds per image(including CNN detection) using CPU only.</description><author>Maksym Ivashechkin, Oscar Mendez, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 14:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09525v1</guid></item><item><title>Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?</title><link>http://arxiv.org/abs/2308.06619v2</link><description>Pruning is a widely used technique for reducing the size of deep neuralnetworks while maintaining their performance. However, such a technique,despite being able to massively compress deep models, is hardly able to removeentire layers from a model (even when structured): is this an addressable task?In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithmaimed at reducing the size of deep neural networks while preserving theirperformance. The key focus of EGP is to prioritize pruning connections inlayers with low entropy, ultimately leading to their complete removal. Throughextensive experiments conducted on popular models like ResNet-18 and Swin-T,our findings demonstrate that EGP effectively compresses deep neural networkswhile maintaining competitive performance levels. Our results not only shedlight on the underlying mechanism behind the advantages of unstructuredpruning, but also pave the way for further investigations into the intricaterelationship between entropy, pruning techniques, and deep learningperformance. The EGP algorithm and its insights hold great promise foradvancing the field of network compression and optimization. The source codefor EGP is released open-source.</description><author>Zhu Liao, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione</author><pubDate>Fri, 18 Aug 2023 14:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06619v2</guid></item><item><title>Denoising Diffusion for 3D Hand Pose Estimation from Images</title><link>http://arxiv.org/abs/2308.09523v1</link><description>Hand pose estimation from a single image has many applications. However,approaches to full 3D body pose estimation are typically trained on day-to-dayactivities or actions. As such, detailed hand-to-hand interactions are poorlyrepresented, especially during motion. We see this in the failure cases oftechniques such as OpenPose or MediaPipe. However, accurate hand poseestimation is crucial for many applications where the global body motion isless important than accurate hand pose estimation. This paper addresses the problem of 3D hand pose estimation from monocularimages or sequences. We present a novel end-to-end framework for 3D handregression that employs diffusion models that have shown excellent ability tocapture the distribution of data for generative purposes. Moreover, we enforcekinematic constraints to ensure realistic poses are generated by incorporatingan explicit forward kinematic layer as part of the network. The proposed modelprovides state-of-the-art performance when lifting a 2D single-hand image to3D. However, when sequence data is available, we add a Transformer module overa temporal window of consecutive frames to refine the results, overcomingjittering and further increasing accuracy. The method is quantitatively and qualitatively evaluated showingstate-of-the-art robustness, generalization, and accuracy on several differentdatasets.</description><author>Maksym Ivashechkin, Oscar Mendez, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 13:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09523v1</guid></item><item><title>Proceedings of the 2nd International Workshop on Adaptive Cyber Defense</title><link>http://arxiv.org/abs/2308.09520v1</link><description>The 2nd International Workshop on Adaptive Cyber Defense was held at theFlorida Institute of Technology, Florida. This workshop was organized to shareresearch that explores unique applications of Artificial Intelligence (AI) andMachine Learning (ML) as foundational capabilities for the pursuit of adaptivecyber defense. The cyber domain cannot currently be reliably and effectivelydefended without extensive reliance on human experts. Skilled cyber defendersare in short supply and often cannot respond fast enough to cyber threats. Building on recent advances in AI and ML the Cyber defense research communityhas been motivated to develop new dynamic and sustainable defenses through theadoption of AI and ML techniques to cyber settings. Bridging critical gapsbetween AI and Cyber researchers and practitioners can accelerate efforts tocreate semi-autonomous cyber defenses that can learn to recognize and respondto cyber attacks or discover and mitigate weaknesses in cooperation with othercyber operation systems and human experts. Furthermore, these defenses areexpected to be adaptive and able to evolve over time to thwart changes inattacker behavior, changes in the system health and readiness, and naturalshifts in user behavior over time. The workshop was comprised of invited keynote talks, technical presentationsand a panel discussion about how AI/ML can enable autonomous mitigation ofcurrent and future cyber attacks. Workshop submissions were peer reviewed by apanel of domain experts with a proceedings consisting of six technical articlesexploring challenging problems of critical importance to national and globalsecurity. Participation in this workshop offered new opportunities to stimulateresearch and innovation in the emerging domain of adaptive and autonomous cyberdefense.</description><author>Marco Carvalho, Damian Marriott, Mark Bilinski, Ahmad Ridley</author><pubDate>Fri, 18 Aug 2023 13:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09520v1</guid></item><item><title>G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory</title><link>http://arxiv.org/abs/2307.14277v2</link><description>The recent video grounding works attempt to introduce vanilla contrastivelearning into video grounding. However, we claim that this naive solution issuboptimal. Contrastive learning requires two key properties: (1)\emph{alignment} of features of similar samples, and (2) \emph{uniformity} ofthe induced distribution of the normalized features on the hypersphere. Due totwo annoying issues in video grounding: (1) the co-existence of some visualentities in both ground truth and other moments, \ie semantic overlapping; (2)only a few moments in the video are annotated, \ie sparse annotation dilemma,vanilla contrastive learning is unable to model the correlations betweentemporally distant moments and learned inconsistent video representations. Bothcharacteristics lead to vanilla contrastive learning being unsuitable for videogrounding. In this paper, we introduce Geodesic and Game Localization (G2L), asemantically aligned and uniform video grounding framework via geodesic andgame theory. We quantify the correlations among moments leveraging the geodesicdistance that guides the model to learn the correct cross-modalrepresentations. Furthermore, from the novel perspective of game theory, wepropose semantic Shapley interaction based on geodesic distance sampling tolearn fine-grained semantic alignment in similar moments. Experiments on threebenchmarks demonstrate the effectiveness of our method.</description><author>Hongxiang Li, Meng Cao, Xuxin Cheng, Yaowei Li, Zhihong Zhu, Yuexian Zou</author><pubDate>Fri, 18 Aug 2023 13:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14277v2</guid></item><item><title>Leveraging Intrinsic Properties for Non-Rigid Garment Alignment</title><link>http://arxiv.org/abs/2308.09519v1</link><description>We address the problem of aligning real-world 3D data of garments, whichbenefits many applications such as texture learning, physical parameterestimation, generative modeling of garments, etc. Existing extrinsic methodstypically perform non-rigid iterative closest point and struggle to aligndetails due to incorrect closest matches and rigidity constraints. Whileintrinsic methods based on functional maps can produce high-qualitycorrespondences, they work under isometric assumptions and become unreliablefor garment deformations which are highly non-isometric. To achievewrinkle-level as well as texture-level alignment, we present a novelcoarse-to-fine two-stage method that leverages intrinsic manifold propertieswith two neural deformation fields, in the 3D space and the intrinsic space,respectively. The coarse stage performs a 3D fitting, where we leverageintrinsic manifold properties to define a manifold deformation field. Thecoarse fitting then induces a functional map that produces an alignment ofintrinsic embeddings. We further refine the intrinsic alignment with a secondneural deformation field for higher accuracy. We evaluate our method with ourcaptured garment dataset, GarmCap. The method achieves accurate wrinkle-leveland texture-level alignment and works for difficult garment types such as longcoats. Our project page ishttps://jsnln.github.io/iccv2023_intrinsic/index.html.</description><author>Siyou Lin, Boyao Zhou, Zerong Zheng, Hongwen Zhang, Yebin Liu</author><pubDate>Fri, 18 Aug 2023 13:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09519v1</guid></item><item><title>3D Segmentation of Humans in Point Clouds with Synthetic Data</title><link>http://arxiv.org/abs/2212.00786v4</link><description>Segmenting humans in 3D indoor scenes has become increasingly important withthe rise of human-centered robotics and AR/VR applications. To this end, wepropose the task of joint 3D human semantic segmentation, instance segmentationand multi-human body-part segmentation. Few works have attempted to directlysegment humans in cluttered 3D scenes, which is largely due to the lack ofannotated training data of humans interacting with 3D scenes. We address thischallenge and propose a framework for generating training data of synthetichumans interacting with real 3D scenes. Furthermore, we propose a noveltransformer-based model, Human3D, which is the first end-to-end model forsegmenting multiple human instances and their body-parts in a unified manner.The key advantage of our synthetic data generation framework is its ability togenerate diverse and realistic human-scene interactions, with highly accurateground truth. Our experiments show that pre-training on synthetic data improvesperformance on a wide variety of 3D human segmentation tasks. Finally, wedemonstrate that Human3D outperforms even task-specific state-of-the-art 3Dsegmentation methods.</description><author>Ayça Takmaz, Jonas Schult, Irem Kaftan, Mertcan Akçay, Bastian Leibe, Robert Sumner, Francis Engelmann, Siyu Tang</author><pubDate>Fri, 18 Aug 2023 13:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00786v4</guid></item><item><title>Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-based Similarity</title><link>http://arxiv.org/abs/2308.09517v1</link><description>Graph representation learning (GRL) methods, such as graph neural networksand graph transformer models, have been successfully used to analyzegraph-structured data, mainly focusing on node classification and linkprediction tasks. However, the existing studies mostly only consider localconnectivity while ignoring long-range connectivity and the roles of nodes. Inthis paper, we propose Unified Graph Transformer Networks (UGT) thateffectively integrate local and global structural information into fixed-lengthvector representations. First, UGT learns local structure by identifying thelocal substructures and aggregating features of the $k$-hop neighborhoods ofeach node. Second, we construct virtual edges, bridging distant nodes withstructural similarity to capture the long-range dependencies. Third, UGT learnsunified representations through self-attention, encoding structural distanceand $p$-step transition probability between node pairs. Furthermore, we proposea self-supervised learning task that effectively learns transition probabilityto fuse local and global structural features, which could then be transferredto other downstream tasks. Experimental results on real-world benchmarkdatasets over various downstream tasks showed that UGT significantlyoutperformed baselines that consist of state-of-the-art models. In addition,UGT reaches the expressive power of the third-order Weisfeiler-Lehmanisomorphism test (3d-WL) in distinguishing non-isomorphic graph pairs. Thesource code is available athttps://github.com/NSLab-CUK/Unified-Graph-Transformer.</description><author>Van Thuy Hoang, O-Joun Lee</author><pubDate>Fri, 18 Aug 2023 13:49:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09517v1</guid></item></channel></rss>