<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 06 Feb 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Test-Time Adaptation for Depth Completion</title><link>http://arxiv.org/abs/2402.03312v1</link><description>It is common to observe performance degradation when transferring modelstrained on some (source) datasets to target testing data due to a domain gapbetween them. Existing methods for bridging this gap, such as domain adaptation(DA), may require the source data on which the model was trained (often notavailable), while others, i.e., source-free DA, require many passes through thetesting data. We propose an online test-time adaptation method for depthcompletion, the task of inferring a dense depth map from a single image andassociated sparse depth map, that closes the performance gap in a single pass.We first present a study on how the domain shift in each data modality affectsmodel performance. Based on our observations that the sparse depth modalityexhibits a much smaller covariate shift than the image, we design an embeddingmodule trained in the source domain that preserves a mapping from featuresencoding only sparse depth to those encoding image and sparse depth. Duringtest time, sparse depth features are projected using this map as a proxy forsource domain features and are used as guidance to train a set of auxiliaryparameters (i.e., adaptation layer) to align image and sparse depth featuresfrom the target test domain to that of the source domain. We evaluate ourmethod on indoor and outdoor scenarios and show that it improves over baselinesby an average of 21.1%.</description><author>Hyoungseob Park, Anjali Gupta, Alex Wong</author><pubDate>Mon, 05 Feb 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03312v1</guid></item><item><title>HASSOD: Hierarchical Adaptive Self-Supervised Object Detection</title><link>http://arxiv.org/abs/2402.03311v1</link><description>The human visual perception system demonstrates exceptional capabilities inlearning without explicit supervision and understanding the part-to-wholecomposition of objects. Drawing inspiration from these two abilities, wepropose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), anovel approach that learns to detect objects and understand their compositionswithout human supervision. HASSOD employs a hierarchical adaptive clusteringstrategy to group regions into object masks based on self-supervised visualrepresentations, adaptively determining the number of objects per image.Furthermore, HASSOD identifies the hierarchical levels of objects in terms ofcomposition, by analyzing coverage relations between masks and constructingtree structures. This additional self-supervised learning task leads toimproved detection performance and enhanced interpretability. Lastly, weabandon the inefficient multi-round self-training process utilized in priormethods and instead adapt the Mean Teacher framework from semi-supervisedlearning, which leads to a smoother and more efficient training process.Through extensive experiments on prevalent image datasets, we demonstrate thesuperiority of HASSOD over existing methods, thereby advancing the state of theart in self-supervised object detection. Notably, we improve Mask AR from 20.2to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page:https://HASSOD-NeurIPS23.github.io.</description><author>Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, Yu-Xiong Wang</author><pubDate>Mon, 05 Feb 2024 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03311v1</guid></item><item><title>V-IRL: Grounding Virtual Intelligence in Real Life</title><link>http://arxiv.org/abs/2402.03310v1</link><description>There is a sensory gulf between the Earth that humans inhabit and the digitalrealms in which modern AI agents are created. To develop AI agents that cansense, think, and act as flexibly as humans in real-world settings, it isimperative to bridge the realism gap between the digital and physical worlds.How can we embody agents in an environment as rich and diverse as the one weinhabit, without the constraints imposed by real hardware and control? Towardsthis end, we introduce V-IRL: a platform that enables agents to scalablyinteract with the real world in a virtual yet realistic environment. Ourplatform serves as a playground for developing agents that can accomplishvarious practical tasks and as a vast testbed for measuring progress incapabilities spanning perception, decision-making, and interaction withreal-world data across the entire globe.</description><author>Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, Saining Xie</author><pubDate>Mon, 05 Feb 2024 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03310v1</guid></item><item><title>AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion</title><link>http://arxiv.org/abs/2402.03309v1</link><description>Underwater perception and 3D surface reconstruction are challenging problemswith broad applications in construction, security, marine archaeology, andenvironmental monitoring. Treacherous operating conditions, fragilesurroundings, and limited navigation control often dictate that submersiblesrestrict their range of motion and, thus, the baseline over which they cancapture measurements. In the context of 3D scene reconstruction, it iswell-known that smaller baselines make reconstruction more challenging. Ourwork develops a physics-based multimodal acoustic-optical neural surfacereconstruction framework (AONeuS) capable of effectively integratinghigh-resolution RGB measurements with low-resolution depth-resolved imagingsonar measurements. By fusing these complementary modalities, our framework canreconstruct accurate high-resolution 3D surfaces from measurements capturedover heavily-restricted baselines. Through extensive simulations and in-labexperiments, we demonstrate that AONeuS dramatically outperforms recentRGB-only and sonar-only inverse-differentiable-rendering--based surfacereconstruction methods. A website visualizing the results of our paper islocated at this address: https://aoneus.github.io/</description><author>Mohamad Qadri, Kevin Zhang, Akshay Hinduja, Michael Kaess, Adithya Pediredla, Christopher A. Metzler</author><pubDate>Mon, 05 Feb 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03309v1</guid></item><item><title>4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes</title><link>http://arxiv.org/abs/2402.03307v1</link><description>We consider the problem of novel view synthesis (NVS) for dynamic scenes.Recent neural approaches have accomplished exceptional NVS results for static3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Priorefforts often encode dynamics by learning a canonical space plus implicit orexplicit deformation fields, which struggle in challenging scenarios likesudden movements or capturing high-fidelity renderings. In this paper, weintroduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamicscenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3DGaussian Splatting in static scenes. We model dynamics at each timestamp bytemporally slicing the 4D Gaussians, which naturally compose dynamic 3DGaussians and can be seamlessly projected into images. As an explicitspatial-temporal representation, 4DGS demonstrates powerful capabilities formodeling complicated dynamics and fine details, especially for scenes withabrupt motions. We further implement our temporal slicing and splattingtechniques in a highly optimized CUDA acceleration framework, achievingreal-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motionsshowcase the superior efficiency and effectiveness of 4DGS, which consistentlyoutperforms existing methods both quantitatively and qualitatively.</description><author>Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen</author><pubDate>Mon, 05 Feb 2024 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03307v1</guid></item><item><title>Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?</title><link>http://arxiv.org/abs/2402.03305v1</link><description>Diffusion models are capable of impressive feats of image generation withuncommon juxtapositions such as astronauts riding horses on the moon withproperly placed shadows. These outputs indicate the ability to performcompositional generalization, but how do the models do so? We performcontrolled experiments on conditional DDPMs learning to generate 2D sphericalGaussian bumps centered at specified $x$- and $y$-positions. Our results showthat the emergence of semantically meaningful latent representations is key toachieving high performance. En route to successful performance over learning,the model traverses three distinct phases of latent representations: (phase A)no latent structure, (phase B) a 2D manifold of disordered states, and (phaseC) a 2D ordered manifold. Corresponding to each of these phases, we identifyqualitatively different generation behaviors: 1) multiple bumps are generated,2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump isgenerated at the correct $x$ and y location. Furthermore, we show that evenunder imbalanced datasets where features ($x$- versus $y$-positions) arerepresented with skewed frequencies, the learning process for $x$ and $y$ iscoupled rather than factorized, demonstrating that simple vanilla-flavoreddiffusion models cannot learn efficient representations in which localizationin $x$ and $y$ are factorized into separate 1D tasks. These findings suggestthe need for future work to find inductive biases that will push generativemodels to discover and exploit factorizable independent structures in theirinputs, which will be required to vault these models into more data-efficientregimes.</description><author>Qiyao Liang, Ziming Liu, Ila Fiete</author><pubDate>Mon, 05 Feb 2024 18:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03305v1</guid></item><item><title>Nevermind: Instruction Override and Moderation in Large Language Models</title><link>http://arxiv.org/abs/2402.03303v1</link><description>Given the impressive capabilities of recent Large Language Models (LLMs), weinvestigate and benchmark the most popular proprietary and different sized opensource models on the task of explicit instruction following in conflictingsituations, e.g. overrides. These include the ability of the model to overridethe knowledge within the weights of the model, the ability to override (ormoderate) extracted knowledge in the prompt, and lastly the ability to performa full jailbreak. Experimentation performed suggest several key findings toimprove instruction following - larger models perform the best in followinginstructions that override internal and contextual instructions, and areobedient, even to a fault. When scaling to longer contexts via rope scaling, asignificant buffer needs to be maintained from the edge of the perplexity cliffin order to maintain instruction following capabilities. Finally, we observeimproving instruction following, and subsequently instructionoverrides/jailbreaks, is fundamentally at odds with the ability of a languagemodel to follow given safety filters or guidelines. Thus, we postulate the mosteffective approach for safe, trustworthy AI should be dealt external to the LLMitself.</description><author>Edward Kim</author><pubDate>Mon, 05 Feb 2024 18:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03303v1</guid></item><item><title>Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining</title><link>http://arxiv.org/abs/2402.03302v1</link><description>Accurate medical image segmentation demands the integration of multi-scaleinformation, spanning from local features to global dependencies. However, itis challenging for existing methods to model long-range global information,where convolutional neural networks (CNNs) are constrained by their localreceptive fields, and vision transformers (ViTs) suffer from high quadraticcomplexity of their attention mechanism. Recently, Mamba-based models havegained great attention for their impressive ability in long sequence modeling.Several studies have demonstrated that these models can outperform popularvision models in various tasks, offering higher accuracy, lower memoryconsumption, and less computational burden. However, existing Mamba-basedmodels are mostly trained from scratch and do not explore the power ofpretraining, which has been proven to be quite effective for data-efficientmedical image analysis. This paper introduces a novel Mamba-based model,Swin-UMamba, designed specifically for medical image segmentation tasks,leveraging the advantages of ImageNet-based pretraining. Our experimentalresults reveal the vital role of ImageNet-based training in enhancing theperformance of Mamba-based models. Swin-UMamba demonstrates superiorperformance with a large margin compared to CNNs, ViTs, and latest Mamba-basedmodels. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMambaoutperforms its closest counterpart U-Mamba by an average score of 3.58%. Thecode and models of Swin-UMamba are publicly available at:https://github.com/JiarunLiu/Swin-UMamba</description><author>Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, Shanshan Wang</author><pubDate>Mon, 05 Feb 2024 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03302v1</guid></item><item><title>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</title><link>http://arxiv.org/abs/2402.03300v1</link><description>Mathematical reasoning poses a significant challenge for language models dueto its complex and structured nature. In this paper, we introduce DeepSeekMath7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120Bmath-related tokens sourced from Common Crawl, together with natural languageand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on thecompetition-level MATH benchmark without relying on external toolkits andvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.The mathematical reasoning capability of DeepSeekMath is attributed to two keyfactors: First, we harness the significant potential of publicly available webdata through a meticulously engineered data selection pipeline. Second, weintroduce Group Relative Policy Optimization (GRPO), a variant of ProximalPolicy Optimization (PPO), that enhances mathematical reasoning abilities whileconcurrently optimizing the memory usage of PPO.</description><author>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, Daya Guo</author><pubDate>Mon, 05 Feb 2024 18:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03300v1</guid></item><item><title>GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models</title><link>http://arxiv.org/abs/2402.03299v1</link><description>The discovery of "jailbreaks" to bypass safety filters of Large LanguageModels (LLMs) and harmful responses have encouraged the community to implementsafety measures. One major safety measure is to proactively test the LLMs withjailbreaks prior to the release. Therefore, such testing will require a methodthat can generate jailbreaks massively and efficiently. In this paper, wefollow a novel yet intuitive strategy to generate jailbreaks in the style ofthe human generation. We propose a role-playing system that assigns fourdifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,we collect existing jailbreaks and split them into different independentcharacteristics using clustering frequency and semantic patterns sentence bysentence. We organize these characteristics into a knowledge graph, making themmore accessible and easier to retrieve. Our system of different roles willleverage this knowledge graph to generate new jailbreaks, which have provedeffective in inducing LLMs to generate unethical or guideline-violatingresponses. In addition, we also pioneer a setting in our system that willautomatically follow the government-issued guidelines to generate jailbreaks totest whether LLMs follow the guidelines accordingly. We refer to our system asGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We haveempirically validated the effectiveness of GUARD on three cutting-edgeopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as awidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to therealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasingGUARD's versatility and contributing valuable insights for the development ofsafer, more reliable LLM-based applications across diverse modalities.</description><author>Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang</author><pubDate>Mon, 05 Feb 2024 18:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03299v1</guid></item><item><title>Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks</title><link>http://arxiv.org/abs/2402.03295v1</link><description>Second-order optimization approaches like the generalized Gauss-Newton methodare considered more powerful as they utilize the curvature information of theobjective function with preconditioning matrices. Albeit offering temptingtheoretical benefits, they are not easily applicable to modern deep learning.The major reason is due to the quadratic memory and cubic time complexity tocompute the inverse of the matrix. These requirements are infeasible even withstate-of-the-art hardware. In this work, we propose Ginger, aneigendecomposition for the inverse of the generalized Gauss-Newton matrix. Ourmethod enjoys efficient linear memory and time complexity for each iteration.Instead of approximating the conditioning matrix, we directly maintain itsinverse to make the approximation more accurate. We provide the convergenceresult of Ginger for non-convex objectives. Our experiments on different taskswith different model architectures verify the effectiveness of our method. Ourcode is publicly available.</description><author>Yongchang Hao, Yanshuai Cao, Lili Mou</author><pubDate>Mon, 05 Feb 2024 18:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03295v1</guid></item><item><title>Flora: Low-Rank Adapters Are Secretly Gradient Compressors</title><link>http://arxiv.org/abs/2402.03293v1</link><description>Despite large neural networks demonstrating remarkable abilities to completedifferent tasks, they require excessive memory usage to store the optimizationstates for training. To alleviate this, the low-rank adaptation (LoRA) isproposed to reduce the optimization states by training fewer parameters.However, LoRA restricts overall weight update matrices to be low-rank, limitingthe model performance. In this work, we investigate the dynamics of LoRA andidentify that it can be approximated by a random projection. Based on thisobservation, we propose Flora, which is able to achieve high-rank updates byresampling the projection matrices while enjoying the sublinear spacecomplexity of optimization states. We conduct experiments across differenttasks and model architectures to verify the effectiveness of our approach.</description><author>Yongchang Hao, Yanshuai Cao, Lili Mou</author><pubDate>Mon, 05 Feb 2024 18:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03293v1</guid></item><item><title>Zero-shot Object-Level OOD Detection with Context-Aware Inpainting</title><link>http://arxiv.org/abs/2402.03292v1</link><description>Machine learning algorithms are increasingly provided as black-box cloudservices or pre-trained models, without access to their training data. Thismotivates the problem of zero-shot out-of-distribution (OOD) detection.Concretely, we aim to detect OOD objects that do not belong to the classifier'slabel set but are erroneously classified as in-distribution (ID) objects. Ourapproach, RONIN, uses an off-the-shelf diffusion model to replace detectedobjects with inpainting. RONIN conditions the inpainting process with thepredicted ID label, drawing the input object closer to the in-distributiondomain. As a result, the reconstructed object is very close to the original inthe ID cases and far in the OOD cases, allowing RONIN to effectivelydistinguish ID and OOD samples. Throughout extensive experiments, wedemonstrate that RONIN achieves competitive results compared to previousapproaches across several datasets, both in zero-shot and non-zero-shotsettings.</description><author>Quang-Huy Nguyen, Jin Peng Zhou, Zhenzhen Liu, Khanh-Huyen Bui, Kilian Q. Weinberger, Dung D. Le</author><pubDate>Mon, 05 Feb 2024 18:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03292v1</guid></item><item><title>InstanceDiffusion: Instance-level Control for Image Generation</title><link>http://arxiv.org/abs/2402.03290v1</link><description>Text-to-image diffusion models produce high quality images but do not offercontrol over individual instances in the image. We introduce InstanceDiffusionthat adds precise instance-level control to text-to-image diffusion models.InstanceDiffusion supports free-form language conditions per instance andallows flexible ways to specify instance locations such as simple singlepoints, scribbles, bounding boxes or intricate instance segmentation masks, andcombinations thereof. We propose three major changes to text-to-image modelsthat enable precise instance-level control. Our UniFusion block enablesinstance-level conditions for text-to-image models, the ScaleU block improvesimage fidelity, and our Multi-instance Sampler improves generations formultiple instances. InstanceDiffusion significantly surpasses specializedstate-of-the-art models for each location condition. Notably, on the COCOdataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$for box inputs, and 25.4% IoU for mask inputs.</description><author>Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra</author><pubDate>Mon, 05 Feb 2024 18:49:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03290v1</guid></item><item><title>Distilled GPT for Source Code Summarization</title><link>http://arxiv.org/abs/2308.14731v2</link><description>A code summary is a brief natural language description of source code.Summaries are usually only a single sentence long, and yet form the backbone ofdeveloper documentation. A short descriptions such as "changes all visiblepolygons to the color blue" can give a programmer a high-level idea of whatcode does without the effort of reading the code itself. Recently, productsbased on Large Language Models such as ChatGPT have demonstrated a strongability to write these descriptions automatically. However, to use these tools,programmers must send their code to untrusted third parties for processing(e.g., via an API call). This loss of custody is not acceptable to manyorganizations. In this paper, we present an alternative: we train an opensource model using sample output generated by GPT-3.5 in a process related toknowledge distillation. Our model is small enough (350m parameters) to be runon a single 16gb GPU, yet we show in our evaluation that it is large enough tomimic GPT-3.5 on this task.</description><author>Chia-Yi Su, Collin McMillan</author><pubDate>Mon, 05 Feb 2024 18:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14731v2</guid></item><item><title>Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS</title><link>http://arxiv.org/abs/2402.03289v1</link><description>Existing large language models (LLMs) for register transfer level codegeneration face challenges like compilation failures and suboptimal power,performance, and area (PPA) efficiency. This is due to the lack of PPAawareness in conventional transformer decoding algorithms. In response, wepresent an automated transformer decoding algorithm that integrates Monte Carlotree-search for lookahead, guiding the transformer to produce compilable,functionally correct, and PPA-optimized code. Empirical evaluation with afine-tuned language model on RTL codesets shows that our proposed techniqueconsistently generates functionally correct code compared to prompting-onlymethods and effectively addresses the PPA-unawareness drawback of naive largelanguage models. For the largest design generated by the state-of-the-art LLM(16-bit adder), our technique can achieve a 31.8% improvement in the area-delayproduct.</description><author>Matthew DeLorenzo, Animesh Basak Chowdhury, Vasudev Gohil, Shailja Thakur, Ramesh Karri, Siddharth Garg, Jeyavijayan Rajendran</author><pubDate>Mon, 05 Feb 2024 18:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03289v1</guid></item><item><title>A Lennard-Jones Layer for Distribution Normalization</title><link>http://arxiv.org/abs/2402.03287v1</link><description>We introduce the Lennard-Jones layer (LJL) for the equalization of thedensity of 2D and 3D point clouds through systematically rearranging pointswithout destroying their overall structure (distribution normalization). LJLsimulates a dissipative process of repulsive and weakly attractive interactionsbetween individual points by considering the nearest neighbor of each point ata given moment in time. This pushes the particles into a potential valley,reaching a well-defined stable configuration that approximates an equidistantsampling after the stabilization process. We apply LJLs to redistributerandomly generated point clouds into a randomized uniform distribution.Moreover, LJLs are embedded in the generation process of point cloud networksby adding them at later stages of the inference process. The improvements in 3Dpoint cloud generation utilizing LJLs are evaluated qualitatively andquantitatively. Finally, we apply LJLs to improve the point distribution of ascore-based 3D point cloud denoising network. In general, we demonstrate thatLJLs are effective for distribution normalization which can be applied atnegligible cost without retraining the given neural network.</description><author>Mulun Na, Jonathan Klein, Biao Zhang, Wojtek Pałubicki, Sören Pirk, Dominik L. Michels</author><pubDate>Mon, 05 Feb 2024 18:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03287v1</guid></item><item><title>Training-Free Consistent Text-to-Image Generation</title><link>http://arxiv.org/abs/2402.03286v1</link><description>Text-to-image models offer a new level of creative flexibility by allowingusers to guide the image generation process through natural language. However,using these models to consistently portray the same subject across diverseprompts remains challenging. Existing approaches fine-tune the model to teachit new words that describe specific user-provided subjects or add imageconditioning to the model. These methods require lengthy per-subjectoptimization or large-scale pre-training. Moreover, they struggle to aligngenerated images with text prompts and face difficulties in portraying multiplesubjects. Here, we present ConsiStory, a training-free approach that enablesconsistent subject generation by sharing the internal activations of thepretrained model. We introduce a subject-driven shared attention block andcorrespondence-based feature injection to promote subject consistency betweenimages. Additionally, we develop strategies to encourage layout diversity whilemaintaining subject consistency. We compare ConsiStory to a range of baselines,and demonstrate state-of-the-art performance on subject consistency and textalignment, without requiring a single optimization step. Finally, ConsiStorycan naturally extend to multi-subject scenarios, and even enable training-freepersonalization for common objects.</description><author>Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon</author><pubDate>Mon, 05 Feb 2024 18:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03286v1</guid></item><item><title>Regularization and Optimization in Model-Based Clustering</title><link>http://arxiv.org/abs/2302.02450v2</link><description>Due to their conceptual simplicity, k-means algorithm variants have beenextensively used for unsupervised cluster analysis. However, one mainshortcoming of these algorithms is that they essentially fit a mixture ofidentical spherical Gaussians to data that vastly deviates from such adistribution. In comparison, general Gaussian Mixture Models (GMMs) can fitricher structures but require estimating a quadratic number of parameters percluster to represent the covariance matrices. This poses two main issues: (i)the underlying optimization problems are challenging due to their larger numberof local minima, and (ii) their solutions can overfit the data. In this work,we design search strategies that circumvent both issues. We develop moreeffective optimization algorithms for general GMMs, and we combine thesealgorithms with regularization strategies that avoid overfitting. Throughextensive computational analyses, we observe that optimization orregularization in isolation does not substantially improve cluster recovery.However, combining these techniques permits a completely new level ofperformance previously unachieved by k-means algorithm variants, unravelingvastly different cluster structures. These results shed new light on thecurrent status quo between GMM and k-means methods and suggest the morefrequent use of general GMMs for data exploration. To facilitate suchapplications, we provide open-source code as well as Julia packages(UnsupervisedClustering.jl and RegularizedCovarianceMatrices.jl) implementingthe proposed techniques.</description><author>Raphael Araujo Sampaio, Joaquim Dias Garcia, Marcus Poggi, Thibaut Vidal</author><pubDate>Mon, 05 Feb 2024 18:40:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02450v2</guid></item><item><title>Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models</title><link>http://arxiv.org/abs/2402.03284v1</link><description>Effective interlocutors account for the uncertain goals, beliefs, andemotions of others. But even the best human conversationalist cannot perfectlyanticipate the trajectory of a dialogue. How well can language models representinherent uncertainty in conversations? We propose FortUne Dial, an expansion ofthe long-standing "conversation forecasting" task: instead of just accuracy,evaluation is conducted with uncertainty-aware metrics, effectively enablingabstention on individual instances. We study two ways in which language modelspotentially represent outcome uncertainty (internally, using scores anddirectly, using tokens) and propose fine-tuning strategies to improvecalibration of both representations. Experiments on eight difficult negotiationcorpora demonstrate that our proposed fine-tuning strategies (a traditionalsupervision strategy and an off-policy reinforcement learning strategy) cancalibrate smaller open-source models to compete with pre-trained models 10xtheir size.</description><author>Anthony Sicilia, Hyunwoo Kim, Khyathi Raghavi Chandu, Malihe Alikhani, Jack Hessel</author><pubDate>Mon, 05 Feb 2024 18:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03284v1</guid></item><item><title>Towards a Flexible Scale-out Framework for Efficient Visual Data Query Processing</title><link>http://arxiv.org/abs/2402.03283v1</link><description>There is growing interest in visual data management systems that supportqueries with specialized operations ranging from resizing an image to runningcomplex machine learning models. With a plethora of such operations, the basicneed to receive query responses in minimal time takes a hit, especially whenthe client desires to run multiple such operations in a single query. Existingsystems provide an ad-hoc approach where different solutions are clubbedtogether to provide an end-to-end visual data management system. Unlike suchsolutions, the Visual Data Management System (VDMS) natively executes querieswith multiple operations, thus providing an end-to-end solution. However, afixed subset of native operations and a synchronous threading architecturelimit its generality and scalability. In this paper, we develop VDMS-Async that adds the capability to runuser-defined operations with VDMS and execute operations within a query on aremote server. VDMS-Async utilizes an event-driven architecture to create anefficient pipeline for executing operations within a query. Our experimentshave shown that VDMS-Async reduces the query execution time by 2-3X compared toexisting state-of-the-art systems. Further, remote operations coupled with anevent-driven architecture enables VDMS-Async to scale query execution timelinearly with the addition of every new remote server. We demonstrate a 64Xreduction in query execution time when adding 64 remote servers.</description><author>Rohit Verma, Arun Raghunath</author><pubDate>Mon, 05 Feb 2024 18:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03283v1</guid></item><item><title>A Framework for Partially Observed Reward-States in RLHF</title><link>http://arxiv.org/abs/2402.03282v1</link><description>The study of reinforcement learning from human feedback (RLHF) has gainedprominence in recent years due to its role in the development of LLMs.Neuroscience research shows that human responses to stimuli are known to dependon partially-observed "internal states." Unfortunately current models of RLHFdo not take take this into consideration. Moreover most RLHF models do notaccount for intermediate feedback, which is gaining importance in empiricalwork and can help improve both sample complexity and alignment. To addressthese limitations, we model RLHF as reinforcement learning with partiallyobserved reward-states (PORRL). We show reductions from the the two dominantforms of human feedback in RLHF - cardinal and dueling feedback to PORRL. Forcardinal feedback, we develop generic statistically efficient algorithms andinstantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, weshow that a naive reduction to cardinal feedback fails to achieve sublineardueling regret. We then present the first explicit reduction that convertsguarantees for cardinal regret to dueling regret. We show that our models andguarantees in both settings generalize and extend existing ones. Finally, weidentify a recursive structure on our model that could improve the statisticaland computational tractability of PORRL, giving examples from past work on RLHFas well as learning perfect reward machines, which PORRL subsumes.</description><author>Chinmaya Kausik, Mirco Mutti, Aldo Pacchiano, Ambuj Tewari</author><pubDate>Mon, 05 Feb 2024 18:38:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03282v1</guid></item><item><title>Mixed Traffic Control and Coordination from Pixels</title><link>http://arxiv.org/abs/2302.09167v4</link><description>Traffic congestion is a persistent problem in our society. Previous methodsfor traffic control have proven futile in alleviating current congestion levelsleading researchers to explore ideas with robot vehicles given the increasedemergence of vehicles with different levels of autonomy on our roads. Thisgives rise to mixed traffic control, where robot vehicles regulate human-drivenvehicles through reinforcement learning (RL). However, most existing studiesuse precise observations that require domain expertise and hand engineering foreach road network's observation space. Additionally, precise observations useglobal information, such as environment outflow, and local information, i.e.,vehicle positions and velocities. Obtaining this information requires updatingexisting road infrastructure with vast sensor environments and communication topotentially unwilling human drivers. We consider image observations, a modalitythat has not been extensively explored for mixed traffic control via RL, as thealternative: 1) images do not require a complete re-imagination of theobservation space from environment to environment; 2) images are ubiquitousthrough satellite imagery, in-car camera systems, and traffic monitoringsystems; and 3) images only require communication to equipment. In this work,we show robot vehicles using image observations can achieve competitiveperformance to using precise information on environments, including ring,figure eight, intersection, merge, and bottleneck. In certain scenarios, ourapproach even outperforms using precision observations, e.g., up to 8% increasein average vehicle velocity in the merge environment, despite only using localtraffic information as opposed to global traffic information.</description><author>Michael Villarreal, Bibek Poudel, Jia Pan, Weizi Li</author><pubDate>Mon, 05 Feb 2024 18:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09167v4</guid></item><item><title>Guiding Language Model Math Reasoning with Planning Tokens</title><link>http://arxiv.org/abs/2310.05707v3</link><description>Large language models (LLMs) have recently attracted considerable interestfor their ability to perform complex reasoning tasks, such as chain-of-thoughtreasoning. However, most of the existing approaches to enhance this abilityrely heavily on data-driven methods, while neglecting the structural aspects ofthe model's reasoning capacity. We find that while LLMs can manage individualreasoning steps well, they struggle with maintaining consistency across anentire reasoning chain. To solve this, we introduce planning tokens at thestart of each reasoning step, serving as a guide for the model, and add theirembeddings to the model parameters. Our approach requires a negligible increasein trainable parameters (just 0.001%) and can be applied through either fullfine-tuning or a more parameter-efficient scheme. We demonstrate our method'seffectiveness by applying it to three different LLMs, showing notable accuracyimprovements across three math word problem datasets w.r.t. standardfine-tuning baselines.</description><author>Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, Alessandro Sordoni</author><pubDate>Mon, 05 Feb 2024 18:33:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05707v3</guid></item><item><title>One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space</title><link>http://arxiv.org/abs/2311.14652v2</link><description>Attention computation takes both the time complexity of $O(n^2)$ and thespace complexity of $O(n^2)$ simultaneously, which makes deploying LargeLanguage Models (LLMs) in streaming applications that involve long contextsrequiring substantial computational resources. In recent OpenAI DevDay (Nov 6,2023), OpenAI released a new model that is able to support a 128K-longdocument, in our paper, we focus on the memory-efficient issue when contextlength $n$ is much greater than 128K ($n \gg 2^d$). Considering a single-layerself-attention with Query, Key, and Value matrices $Q, K, V \in \mathbb{R}^{n\times d}$, the polynomial method approximates the attention output $T \in\mathbb{R}^{n \times d}$. It accomplishes this by constructing $U_1, U_2 \in\mathbb{R}^{n \times t}$ to expedite attention ${\sf Attn}(Q, K, V)$computation within $n^{1+o(1)}$ time executions. Despite this, computing theapproximated attention matrix $U_1U_2^\top \in \mathbb{R}^{n \times n}$ stillnecessitates $O(n^2)$ space, leading to significant memory usage. In responseto these challenges, we introduce a new algorithm that only reads one pass ofthe data in a streaming fashion. This method employs sublinear space $o(n)$ tostore three sketch matrices, alleviating the need for exact $K, V$ storage.Notably, our algorithm exhibits exceptional memory-efficient performance withsuper-long tokens. As the token length $n$ increases, our error guaranteediminishes while the memory usage remains nearly constant. This uniqueattribute underscores the potential of our technique in efficiently handlingLLMs in streaming applications.</description><author>Raghav Addanki, Chenyang Li, Zhao Song, Chiwun Yang</author><pubDate>Mon, 05 Feb 2024 18:30:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14652v2</guid></item><item><title>Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models</title><link>http://arxiv.org/abs/2402.03271v1</link><description>In the face of uncertainty, the ability to seek information is of fundamentalimportance. In many practical applications, such as medical diagnosis andtroubleshooting, the information needed to solve the task is not initiallygiven, and has to be actively sought by asking follow-up questions (forexample, a doctor asking a patient for more details about their symptoms). Inthis work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augmentlarge language models with the ability to actively seek information by askingeffective questions. UoT combines 1) an uncertainty-aware simulation approachwhich enables the model to simulate possible future scenarios and how likelythey are to occur, 2) uncertainty-based rewards motivated by information gainwhich incentivizes the model to seek information, and 3) a reward propagationscheme to select the optimal question to ask in a way that maximizes theexpected reward. In experiments on medical diagnosis, troubleshooting and the'20 Questions' game, UoT achieves an average performance improvement of 57.8%in the rate of successful task completion across multiple LLMs compared withdirect prompting, and also improves efficiency (i.e., the number of questionsneeded to complete the task).</description><author>Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi</author><pubDate>Mon, 05 Feb 2024 18:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03271v1</guid></item><item><title>Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT Protocol</title><link>http://arxiv.org/abs/2402.03270v1</link><description>The large number of sensors and actuators that make up the Internet of Thingsobliges these systems to use diverse technologies and protocols. This meansthat IoT networks are more heterogeneous than traditional networks. This givesrise to new challenges in cybersecurity to protect these systems and deviceswhich are characterized by being connected continuously to the Internet.Intrusion detection systems (IDS) are used to protect IoT systems from thevarious anomalies and attacks at the network level. Intrusion Detection Systems(IDS) can be improved through machine learning techniques. Our work focuses oncreating classification models that can feed an IDS using a dataset containingframes under attacks of an IoT system that uses the MQTT protocol. We haveaddressed two types of method for classifying the attacks, ensemble methods anddeep learning models, more specifically recurrent networks with verysatisfactory results.</description><author>Hector Alaiz-Moreton, Jose Aveleira-Mata, Jorge Ondicol-Garcia, Angel Luis Muñoz-Castañeda, Isaías García, Carmen Benavides</author><pubDate>Mon, 05 Feb 2024 18:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03270v1</guid></item><item><title>ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds</title><link>http://arxiv.org/abs/2402.03269v1</link><description>Traditionally, bioacoustics has relied on spectrograms and continuous,per-frame audio representations for the analysis of animal sounds, also servingas input to machine learning models. Meanwhile, the International PhoneticAlphabet (IPA) system has provided an interpretable, language-independentmethod for transcribing human speech sounds. In this paper, we introduce ISPA(Inter-Species Phonetic Alphabet), a precise, concise, and interpretable systemdesigned for transcribing animal sounds into text. We compare acoustics-basedand feature-based methods for transcribing and classifying animal sounds,demonstrating their comparable performance with baseline methods utilizingcontinuous, dense audio representations. By representing animal sounds withtext, we effectively treat them as a "foreign language," and we show thatestablished human language ML paradigms and models, such as language models,can be successfully applied to improve performance.</description><author>Masato Hagiwara, Marius Miron, Jen-Yu Liu</author><pubDate>Mon, 05 Feb 2024 18:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03269v1</guid></item><item><title>Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation</title><link>http://arxiv.org/abs/2402.03268v1</link><description>Pre-trained language models (LMs) are able to perform complex reasoningwithout explicit fine-tuning. To understand how pre-training with a next-tokenprediction objective contributes to the emergence of such reasoning capability,we propose that we can view an LM as deriving new conclusions by aggregatingindirect reasoning paths seen at pre-training time. We found this perspectiveeffective in two important cases of reasoning: logic reasoning with knowledgegraphs (KGs) and math reasoning with math word problems (MWPs). Morespecifically, we formalize the reasoning paths as random walk paths on theknowledge/reasoning graphs. Analyses of learned LM distributions suggest that aweighted sum of relevant random walk path probabilities is a reasonable way toexplain how LMs reason. Experiments and analysis on multiple KG and MWPdatasets reveal the effect of training on random walk paths and suggest thataugmenting unlabeled random walk reasoning paths can improve real-worldmulti-step reasoning performance.</description><author>Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, William Yang Wang</author><pubDate>Mon, 05 Feb 2024 18:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03268v1</guid></item><item><title>Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding</title><link>http://arxiv.org/abs/2402.00024v2</link><description>Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasinglyrecognized for their potential in the field of cheminformatics, particularly ininterpreting Simplified Molecular Input Line Entry System (SMILES), a standardmethod for representing chemical structures. These LLMs can decode SMILESstrings into vector representations, providing a novel approach tounderstanding chemical graphs. Methods: We investigate the performance of ChatGPT and LLaMA in embeddingSMILES strings. Our evaluation focuses on two key applications: molecularproperty (MP) prediction and drug-drug interaction (DDI) prediction, bothessential in drug development and healthcare. Results: We find that SMILES embeddings generated using LLaMA outperformthose from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-basedSMILES embeddings show results comparable to existing methods in bothprediction tasks. Conclusion: The application of LLMs in cheminformatics, particularly inutilizing SMILES embeddings, shows significant promise for advancing drugdevelopment. This includes improving the prediction of chemical properties andfacilitating the drug discovery process. GitHub:https://github.com/sshaghayeghs/LLaMA-VS-ChatGPT</description><author>Shaghayegh Sadeghi, Alan Bui, Ali Forooghi, Jianguo Lu, Alioune Ngom</author><pubDate>Mon, 05 Feb 2024 18:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00024v2</guid></item><item><title>MobilityGPT: Enhanced Human Mobility Modeling with a GPT model</title><link>http://arxiv.org/abs/2402.03264v1</link><description>Generative models have shown promising results in capturing human mobilitycharacteristics and generating synthetic trajectories. However, it remainschallenging to ensure that the generated geospatial mobility data issemantically realistic, including consistent location sequences, and reflectsreal-world characteristics, such as constraining on geospatial limits. Toaddress these issues, we reformat human mobility modeling as an autoregressivegeneration task, leveraging Generative Pre-trained Transformer (GPT). To ensureits controllable generation to alleviate the above challenges, we propose ageospatially-aware generative model, MobilityGPT. We propose a gravity-basedsampling method to train a transformer for semantic sequence similarity. Then,we constrained the training process via a road connectivity matrix thatprovides the connectivity of sequences in trajectory generation, therebykeeping generated trajectories in geospatial limits. Lastly, we constructed aReinforcement Learning from Trajectory Feedback (RLTF) to minimize the traveldistance between training and the synthetically generated trajectories. Ourexperiments on real-world datasets demonstrate that MobilityGPT outperformsstate-of-the-art methods in generating high-quality mobility trajectories thatare closest to real data in terms of origin-destination similarity, triplength, travel radius, link, and gravity distributions.</description><author>Ammar Haydari, Dongjie Chen, Zhengfeng Lai, Chen-Nee Chuah</author><pubDate>Mon, 05 Feb 2024 18:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03264v1</guid></item><item><title>Weak-to-Strong Jailbreaking on Large Language Models</title><link>http://arxiv.org/abs/2401.17256v2</link><description>Large language models (LLMs) are vulnerable to jailbreak attacks - resultingin harmful, unethical, or biased text generations. However, existingjailbreaking methods are computationally costly. In this paper, we propose theweak-to-strong jailbreaking attack, an efficient method to attack aligned LLMsto produce harmful text. Our key intuition is based on the observation thatjailbroken and aligned models only differ in their initial decodingdistributions. The weak-to-strong attack's key technical insight is using twosmaller models (a safe and an unsafe one) to adversarially modify asignificantly larger safe model's decoding probabilities. We evaluate theweak-to-strong attack on 5 diverse LLMs from 3 organizations. The results showour method can increase the misalignment rate to over 99% on two datasets withjust one forward pass per example. Our study exposes an urgent safety issuethat needs to be addressed when aligning LLMs. As an initial attempt, wepropose a defense strategy to protect against such attacks, but creating moreadvanced defenses remains challenging. The code for replicating the method isavailable at https://github.com/XuandongZhao/weak-to-strong</description><author>Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang</author><pubDate>Mon, 05 Feb 2024 18:19:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17256v2</guid></item><item><title>Learning Best-in-Class Policies for the Predict-then-Optimize Framework</title><link>http://arxiv.org/abs/2402.03256v1</link><description>We propose a novel family of decision-aware surrogate losses, calledPerturbation Gradient (PG) losses, for the predict-then-optimize framework.These losses directly approximate the downstream decision loss and can beoptimized using off-the-shelf gradient-based methods. Importantly, unlikeexisting surrogate losses, the approximation error of our PG losses vanishes asthe number of samples grows. This implies that optimizing our surrogate lossyields a best-in-class policy asymptotically, even in misspecified settings.This is the first such result in misspecified settings and we provide numericalevidence confirming our PG losses substantively outperform existing proposalswhen the underlying model is misspecified and the noise is not centrallysymmetric. Insofar as misspecification is commonplace in practice -- especiallywhen we might prefer a simpler, more interpretable model -- PG losses offer anovel, theoretically justified, method for computationally tractabledecision-aware learning.</description><author>Michael Huang, Vishal Gupta</author><pubDate>Mon, 05 Feb 2024 18:14:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03256v1</guid></item><item><title>Minimum Description Length and Generalization Guarantees for Representation Learning</title><link>http://arxiv.org/abs/2402.03254v1</link><description>A major challenge in designing efficient statistical supervised learningalgorithms is finding representations that perform well not only on availabletraining samples but also on unseen data. While the study of representationlearning has spurred much interest, most existing such approaches areheuristic; and very little is known about theoretical generalizationguarantees. In this paper, we establish a compressibility framework that allows us toderive upper bounds on the generalization error of a representation learningalgorithm in terms of the "Minimum Description Length" (MDL) of the labels orthe latent variables (representations). Rather than the mutual informationbetween the encoder's input and the representation, which is often believed toreflect the algorithm's generalization capability in the related literature butin fact, falls short of doing so, our new bounds involve the "multi-letter"relative entropy between the distribution of the representations (or labels) ofthe training and test sets and a fixed prior. In particular, these new boundsreflect the structure of the encoder and are not vacuous for deterministicalgorithms. Our compressibility approach, which is information-theoretic innature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces twoessential ingredients: block-coding and lossy-compression. The latter allowsour approach to subsume the so-called geometrical compressibility as a specialcase. To the best knowledge of the authors, the established generalizationbounds are the first of their kind for Information Bottleneck (IB) typeencoders and representation learning. Finally, we partly exploit thetheoretical results by introducing a new data-dependent prior. Numericalsimulations illustrate the advantages of well-chosen such priors over classicalpriors used in IB.</description><author>Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski</author><pubDate>Mon, 05 Feb 2024 18:12:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03254v1</guid></item><item><title>Fair Active Ranking from Pairwise Preferences</title><link>http://arxiv.org/abs/2402.03252v1</link><description>We investigate the problem of probably approximately correct and fair (PACF)ranking of items by adaptively evoking pairwise comparisons. Given a set of $n$items that belong to disjoint groups, our goal is to find an $(\epsilon,\delta)$-PACF-Ranking according to a fair objective function that we propose.We assume access to an oracle, wherein, for each query, the learner can choosea pair of items and receive stochastic winner feedback from the oracle. Ourproposed objective function asks to minimize the $\ell_q$ norm of the error ofthe groups, where the error of a group is the $\ell_p$ norm of the error of allthe items within that group, for $p, q \geq 1$. This generalizes the objectivefunction of $\epsilon$-Best-Ranking, proposed by Saha &amp; Gopalan (2019). By adopting our objective function, we gain the flexibility to explorefundamental fairness concepts like equal or proportionate errors within aunified framework. Adjusting parameters $p$ and $q$ allows tailoring tospecific fairness preferences. We present both group-blind and group-awarealgorithms and analyze their sample complexity. We provide matching lowerbounds up to certain logarithmic factors for group-blind algorithms. For arestricted class of group-aware algorithms, we show that we can get reasonablelower bounds. We conduct comprehensive experiments on both real-world andsynthetic datasets to complement our theoretical findings.</description><author>Sruthi Gorantla, Sara Ahmadian</author><pubDate>Mon, 05 Feb 2024 18:09:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03252v1</guid></item><item><title>CLIP Can Understand Depth</title><link>http://arxiv.org/abs/2402.03251v1</link><description>Recent studies on generalizing CLIP for monocular depth estimation revealthat CLIP pre-trained on web-crawled data is inefficient for deriving propersimilarities between image patches and depth-related prompts. In this paper, weadapt CLIP for meaningful quality of monocular depth estimation with denseprediction, without fine-tuning its original vision-language alignment. Byjointly training a compact deconvolutional decoder with a tiny learnableembedding matrix named mirror, as a static prompt for its text encoder, CLIP isenabled to understand depth. With this approach, our model exhibits impressiveperformance matching several previous state-of-the-art vision-only models onthe NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depthestimation model with a large margin. Experiments on temporal depth consistencyand spatial continuity demonstrate that the prior knowledge of CLIP can beeffectively refined by our proposed framework. Furthermore, an ablation studyon mirror proves that the resulting model estimates depth utilizing knowledgenot only from the image encoder but also text encoder despite not being givenany prompt written in a human way. This research demonstrates that throughminimal adjustments, the prior knowledge of vision-language foundation models,such as CLIP, can be generalized even to domains where learning duringpretraining is challenging. We facilitate future works focused on methods toadjust suboptimal prior knowledge of vision-language models using non-humanlanguage prompts, achieving performance on par with task-specificstate-of-the-art methodologies.</description><author>Dunam Kim, Seokju Lee</author><pubDate>Mon, 05 Feb 2024 18:09:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03251v1</guid></item><item><title>HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference</title><link>http://arxiv.org/abs/2402.03247v1</link><description>Several photonic microring resonators (MRRs) based analog accelerators havebeen proposed to accelerate the inference of integer-quantized CNNs withremarkably higher throughput and energy efficiency compared to their electroniccounterparts. However, the existing analog photonic accelerators suffer fromthree shortcomings: (i) severe hampering of wavelength parallelism due tovarious crosstalk effects, (ii) inflexibility of supporting various dataflowsother than the weight-stationary dataflow, and (iii) failure in fullyleveraging the ability of photodetectors to perform in-situ accumulations.These shortcomings collectively hamper the performance and energy efficiency ofprior accelerators. To tackle these shortcomings, we present a novel HybridtimE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybridtime-amplitude analog optical multipliers (TAOMs) that increase the flexibilityof HEANA to support multiple dataflows. A spectrally hitless arrangement ofTAOMs significantly reduces the crosstalk effects, thereby increasing thewavelength parallelism in HEANA. Moreover, HEANA employs our invented balancedphoto-charge accumulators (BPCAs) that enable buffer-less, in-situ, temporalaccumulations to eliminate the need to use reduction networks in HEANA,relieving it from related latency and energy overheads. Our evaluation for theinference of four modern CNNs indicates that HEANA provides improvements ofatleast 66x and 84x in frames-per-second (FPS) and FPS/W (energy-efficiency),respectively, for equal-area comparisons, on gmean over two MRR-based analogCNN accelerators from prior work.</description><author>Sairam Sri Vatsavai, Venkata Sai Praneeth Karempudi, Ishan Thakkar</author><pubDate>Mon, 05 Feb 2024 18:05:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03247v1</guid></item><item><title>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</title><link>http://arxiv.org/abs/2402.03246v1</link><description>Semantic understanding plays a crucial role in Dense SimultaneousLocalization and Mapping (SLAM), facilitating comprehensive sceneinterpretation. Recent advancements that integrate Gaussian Splatting into SLAMsystems have demonstrated its effectiveness in generating high-qualityrenderings through the use of explicit 3D Gaussian representations. Building onthis progress, we propose SGS-SLAM, the first semantic dense visual SLAM systemgrounded in 3D Gaussians, which provides precise 3D semantic segmentationalongside high-fidelity reconstructions. Specifically, we propose to employmulti-channel optimization during the mapping process, integrating appearance,geometric, and semantic constraints with key-frame optimization to enhancereconstruction quality. Extensive experiments demonstrate that SGS-SLAMdelivers state-of-the-art performance in camera pose estimation, mapreconstruction, and semantic segmentation, outperforming existing methodsmeanwhile preserving real-time rendering ability.</description><author>Mingrui Li, Shuhong Liu, Heng Zhou</author><pubDate>Mon, 05 Feb 2024 18:03:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03246v1</guid></item><item><title>Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills</title><link>http://arxiv.org/abs/2402.03244v1</link><description>Large language models (LLMs) have recently been used for sequential decisionmaking in interactive environments. However, leveraging environment rewardsignals for continual LLM actor improvement is not straightforward. We proposeSkill Set Optimization (SSO) for improving LLM actor performance throughconstructing and refining sets of transferable skills. SSO constructs skills byextracting common subtrajectories with high rewards and generating subgoals andinstructions to represent each skill. These skills are provided to the LLMactor in-context to reinforce behaviors with high rewards. Then, SSO furtherrefines the skill set by pruning skills that do not continue to result in highrewards. We evaluate our method in the classic videogame NetHack and the textenvironment ScienceWorld to demonstrate SSO's ability to optimize a set ofskills and perform in-context policy improvement. SSO outperforms baselines by40% in our custom NetHack task and outperforms the previous state-of-the-art inScienceWorld by 35%.</description><author>Kolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Sameer Singh, Peter Clark, Roy Fox</author><pubDate>Mon, 05 Feb 2024 17:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03244v1</guid></item><item><title>PINN-BO: A Black-box Optimization Algorithm using Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2402.03243v1</link><description>Black-box optimization is a powerful approach for discovering global optimain noisy and expensive black-box functions, a problem widely encountered inreal-world scenarios. Recently, there has been a growing interest in leveragingdomain knowledge to enhance the efficacy of machine learning methods. PartialDifferential Equations (PDEs) often provide an effective means for elucidatingthe fundamental principles governing the black-box functions. In this paper, wepropose PINN-BO, a black-box optimization algorithm employing Physics-InformedNeural Networks that integrates the knowledge from Partial DifferentialEquations (PDEs) to improve the sample efficiency of the optimization. Weanalyze the theoretical behavior of our algorithm in terms of regret boundusing advances in NTK theory and prove that the use of the PDE alongside theblack-box function evaluations, PINN-BO leads to a tighter regret bound. Weperform several experiments on a variety of optimization tasks and show thatour algorithm is more sample-efficient compared to existing methods.</description><author>Dat Phan-Trong, Hung The Tran, Alistair Shilton, Sunil Gupta</author><pubDate>Mon, 05 Feb 2024 17:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03243v1</guid></item><item><title>JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching</title><link>http://arxiv.org/abs/2402.03242v1</link><description>Recent approaches in skill matching, employing synthetic training data forclassification or similarity model training, have shown promising results,reducing the need for time-consuming and expensive annotations. However,previous synthetic datasets have limitations, such as featuring only one skillper sentence and generally comprising short sentences. In this paper, weintroduce JobSkape, a framework to generate synthetic data that tackles theselimitations, specifically designed to enhance skill-to-taxonomy matching.Within this framework, we create SkillSkape, a comprehensive open-sourcesynthetic dataset of job postings tailored for skill-matching tasks. Weintroduce several offline metrics that show that our dataset resemblesreal-world data. Additionally, we present a multi-step pipeline for skillextraction and matching tasks using large language models (LLMs), benchmarkingagainst known supervised methodologies. We outline that the downstreamevaluation results on real-world data can beat baselines, underscoring itsefficacy and adaptability.</description><author>Antoine Magron, Anna Dai, Mike Zhang, Syrielle Montariol, Antoine Bosselut</author><pubDate>Mon, 05 Feb 2024 17:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03242v1</guid></item><item><title>Fundamental Limitations of Alignment in Large Language Models</title><link>http://arxiv.org/abs/2304.11082v5</link><description>An important aspect in developing language models that interact with humansis aligning their behavior to be useful and unharmful for their human users.This is usually achieved by tuning the model in a way that enhances desiredbehaviors and inhibits undesired ones, a process referred to as alignment. Inthis paper, we propose a theoretical approach called Behavior ExpectationBounds (BEB) which allows us to formally investigate several inherentcharacteristics and limitations of alignment in large language models.Importantly, we prove that within the limits of this framework, for anybehavior that has a finite probability of being exhibited by the model, thereexist prompts that can trigger the model into outputting this behavior, withprobability that increases with the length of the prompt. This implies that anyalignment process that attenuates an undesired behavior but does not remove italtogether, is not safe against adversarial prompting attacks. Furthermore, ourframework hints at the mechanism by which leading alignment approaches such asreinforcement learning from human feedback make the LLM prone to being promptedinto the undesired behaviors. This theoretical result is being experimentallydemonstrated in large scale by the so called contemporary "chatGPT jailbreaks",where adversarial users trick the LLM into breaking its alignment guardrails bytriggering it into acting as a malicious persona. Our results exposefundamental limitations in alignment of LLMs and bring to the forefront theneed to devise reliable mechanisms for ensuring AI safety.</description><author>Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua</author><pubDate>Mon, 05 Feb 2024 17:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11082v5</guid></item><item><title>FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition</title><link>http://arxiv.org/abs/2402.03241v1</link><description>In this paper, we introduce FROSTER, an effective framework foropen-vocabulary action recognition. The CLIP model has achieved remarkablesuccess in a range of image-based tasks, benefiting from its stronggeneralization capability stemming from pretaining on massive image-text pairs.However, applying CLIP directly to the open-vocabulary action recognition taskis challenging due to the absence of temporal information in CLIP'spretraining. Further, fine-tuning CLIP on action recognition datasets may leadto overfitting and hinder its generalizability, resulting in unsatisfactoryresults when dealing with unseen actions. To address these issues, FROSTER employs a residual feature distillationapproach to ensure that CLIP retains its generalization capability whileeffectively adapting to the action recognition task. Specifically, the residualfeature distillation treats the frozen CLIP model as a teacher to maintain thegeneralizability exhibited by the original CLIP and supervises the featurelearning for the extraction of video-specific features to bridge the gapbetween images and videos. Meanwhile, it uses a residual sub-network forfeature distillation to reach a balance between the two distinct objectives oflearning generalizable and video-specific features. We extensively evaluate FROSTER on open-vocabulary action recognitionbenchmarks under both base-to-novel and cross-dataset settings. FROSTERconsistently achieves state-of-the-art performance on all datasets across theboard. Project page: https://visual-ai.github.io/froster.</description><author>Xiaohu Huang, Hao Zhou, Kun Yao, Kai Han</author><pubDate>Mon, 05 Feb 2024 17:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03241v1</guid></item><item><title>Multimodal Speech Enhancement Using Burst Propagation</title><link>http://arxiv.org/abs/2209.03275v2</link><description>This paper proposes the MBURST, a novel multimodal solution for audio-visualspeech enhancements that consider the most recent neurological discoveriesregarding pyramidal cells of the prefrontal cortex and other brain regions. Theso-called burst propagation implements several criteria to address the creditassignment problem in a more biologically plausible manner: steering the signand magnitude of plasticity through feedback, multiplexing the feedback andfeedforward information across layers through different weight connections,approximating feedback and feedforward connections, and linearizing thefeedback signals. MBURST benefits from such capabilities to learn correlationsbetween the noisy signal and the visual stimuli, thus attributing meaning tothe speech by amplifying relevant information and suppressing noise.Experiments conducted over a Grid Corpus and CHiME3-based dataset show thatMBURST can reproduce similar mask reconstructions to the multimodalbackpropagation-based baseline while demonstrating outstanding energyefficiency management, reducing the neuron firing rates to values up to\textbf{$70\%$} lower. Such a feature implies more sustainable implementations,suitable and desirable for hearing aids or any other similar embedded systems.</description><author>Mohsin Raza, Leandro A. Passos, Ahmed Khubaib, Ahsan Adeel</author><pubDate>Mon, 05 Feb 2024 17:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.03275v2</guid></item><item><title>ActiveAnno3D -- An Active Learning Framework for Multi-Modal 3D Object Detection</title><link>http://arxiv.org/abs/2402.03235v1</link><description>The curation of large-scale datasets is still costly and requires much timeand resources. Data is often manually labeled, and the challenge of creatinghigh-quality datasets remains. In this work, we fill the research gap usingactive learning for multi-modal 3D object detection. We propose ActiveAnno3D,an active learning framework to select data samples for labeling that are ofmaximum informativeness for training. We explore various continuous trainingmethods and integrate the most efficient method regarding computational demandand detection performance. Furthermore, we perform extensive experiments andablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM TrafficIntersection dataset. We show that we can achieve almost the same performancewith PV-RCNN and the entropy-based query strategy when using only half of thetraining data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersectiondataset. BEVFusion achieved an mAP of 64.31 when using half of the trainingdata and 75.0 mAP when using the complete nuScenes dataset. We integrate ouractive learning framework into the proAnno labeling tool to enable AI-assisteddata selection and labeling and minimize the labeling costs. Finally, weprovide code, weights, and visualization results on our website:https://active3d-framework.github.io/active3d-framework.</description><author>Ahmed Ghita, Bjørk Antoniussen, Walter Zimmer, Ross Greer, Christian Creß, Andreas Møgelmose, Mohan M. Trivedi, Alois C. Knoll</author><pubDate>Mon, 05 Feb 2024 17:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03235v1</guid></item><item><title>SLANG: New Concept Comprehension of Large Language Models</title><link>http://arxiv.org/abs/2401.12585v3</link><description>The dynamic nature of language, particularly evident in the realm of slangand memes on the Internet, poses serious challenges to the adaptability oflarge language models (LLMs). Traditionally anchored to static datasets, thesemodels often struggle to keep up with the rapid linguistic evolutioncharacteristic of online communities. This research aims to bridge this gap byenhancing LLMs' comprehension of the evolving new concepts on the Internet,without the high cost of continual retraining. In pursuit of this goal, wepropose a new benchmark $\textbf{SLANG}$, which can autonomously integratesnovel data to stay dataset up-to-date, to assess LLMs' capability incomprehending emerging concepts and an approach $\textbf{FOCUS}$, which usescausal inference to enhance LLMs to understand new phrases and their colloquialcontext. Our benchmark and approach involves digesting real-world instances oflinguistic shifts, serving as contextual beacons, to form more precise andcontextually relevant connections between newly emerging expressions and theirmeanings. The empirical analysis shows that our causal inference-based approachoutperforms the traditional models in terms of precision and relevance in thecomprehension of Internet slang and memes.</description><author>Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Cheng</author><pubDate>Mon, 05 Feb 2024 17:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12585v3</guid></item><item><title>Smart Flow Matching: On The Theory of Flow Matching Algorithms with Applications</title><link>http://arxiv.org/abs/2402.03232v1</link><description>The paper presents the exact formula for the vector field that minimizes theloss for the standard flow. This formula depends analytically on a givendistribution \rho_0 and an unknown one \rho_1. Based on the presented formula,a new loss and algorithm for training a vector field model in the style ofConditional Flow Matching are provided. Our loss, in comparison to the standardConditional Flow Matching approach, exhibits smaller variance when evaluatedthrough Monte Carlo sampling methods. Numerical experiments on synthetic modelsand models on tabular data of large dimensions demonstrate better learningresults with the use of the presented algorithm.</description><author>Gleb Ryzhakov, Svetlana Pavlova, Egor Sevriugov, Ivan Oseledets</author><pubDate>Mon, 05 Feb 2024 17:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03232v1</guid></item><item><title>Improved prediction of future user activity in online A/B testing</title><link>http://arxiv.org/abs/2402.03231v1</link><description>In online randomized experiments or A/B tests, accurate predictions ofparticipant inclusion rates are of paramount importance. These predictions notonly guide experimenters in optimizing the experiment's duration but alsoenhance the precision of treatment effect estimates. In this paper we present anovel, straightforward, and scalable Bayesian nonparametric approach forpredicting the rate at which individuals will be exposed to interventionswithin the realm of online A/B testing. Our approach stands out by offeringdual prediction capabilities: it forecasts both the quantity of new customersexpected in future time windows and, unlike available alternative methods, thenumber of times they will be observed. We derive closed-form expressions forthe posterior distributions of the quantities needed to form predictions aboutfuture user activity, thereby bypassing the need for numerical algorithms suchas Markov chain Monte Carlo. After a comprehensive exposition of our model, wetest its performance on experiments on real and simulated data, where we showits superior performance with respect to existing alternatives in theliterature.</description><author>Lorenzo Masoero, Mario Beraha, Thomas Richardson, Stefano Favaro</author><pubDate>Mon, 05 Feb 2024 17:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03231v1</guid></item><item><title>CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models</title><link>http://arxiv.org/abs/2402.03230v1</link><description>Recent rising interests in patient-specific thoracic surgical planning andsimulation require efficient and robust creation of digital anatomical modelsfrom automatic medical image segmentation algorithms. Deep learning (DL) is nowstate-of-the-art in various radiological tasks, and U-shaped DL models haveparticularly excelled in medical image segmentation since the inception of the2D UNet. To date, many variants of U-shaped models have been proposed by theintegration of different attention mechanisms and network configurations.Leveraging the recent development of large multi-label databases, systematicbenchmark studies for these models can provide valuable insights for clinicaldeployment and future model designs, but such studies are still rare. Weconduct the first benchmark study for variants of 3D U-shaped models (3DUNet,STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet withfour variants) with a focus on CT-based anatomical segmentation for thoracicsurgery. Our study systematically examines the impact of different attentionmechanisms, number of resolution stages, and network configurations onsegmentation accuracy and computational complexity. To allow cross-referencewith other recent benchmarking studies, we also included a performanceassessment of the BTCV abdominal structural segmentation. With the STUNetranking at the top, our study demonstrated the value of CNN-based U-shapedmodels for the investigated tasks and the benefit of residual blocks in networkconfiguration designs to boost segmentation performance.</description><author>Arash Harirpoush, Amirhossein Rasoulian, Marta Kersten-Oertel, Yiming Xiao</author><pubDate>Mon, 05 Feb 2024 17:43:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03230v1</guid></item><item><title>IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images</title><link>http://arxiv.org/abs/2402.03227v1</link><description>In MRI studies, the aggregation of imaging data from multiple acquisitionsites enhances sample size but may introduce site-related variabilities thathinder consistency in subsequent analyses. Deep learning methods for imagetranslation have emerged as a solution for harmonizing MR images across sites.In this study, we introduce IGUANe (Image Generation with Unified AdversarialNetworks), an original 3D model that leverages the strengths of domaintranslation and straightforward application of style transfer methods formulticenter brain MR image harmonization. IGUANe extends CycleGAN architectureby integrating an arbitrary number of domains for training through amany-to-one strategy. During inference, the model can be applied to any image,even from an unknown acquisition site, making it a universal generator forharmonization. Trained on a dataset comprising T1-weighted images from 11different scanners, IGUANe was evaluated on data from unseen sites. Theassessments included the transformation of MR images with traveling subjects,the preservation of pairwise distances between MR images within domains, theevolution of volumetric patterns related to age and Alzheimer$^\prime$s disease(AD), and the performance in age regression and patient classification tasks.Comparisons with other harmonization and normalization methods suggest thatIGUANe better preserves individual information in MR images and is moresuitable for maintaining and reinforcing variabilities related to age and AD.Future studies may further assess IGUANe in other multicenter contexts, eitherusing the same model or retraining it for applications to different imagemodalities.</description><author>Vincent Roca, Grégory Kuchcinski, Jean-Pierre Pruvo, Dorian Manouvriez, Renaud Lopes</author><pubDate>Mon, 05 Feb 2024 17:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03227v1</guid></item><item><title>FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion</title><link>http://arxiv.org/abs/2402.03226v1</link><description>As machine learning models in critical fields increasingly grapple withmultimodal data, they face the dual challenges of handling a wide array ofmodalities, often incomplete due to missing elements, and the temporalirregularity and sparsity of collected samples. Successfully leveraging thiscomplex data, while overcoming the scarcity of high-quality training samples,is key to improving these models' predictive performance. We introduce``FuseMoE'', a mixture-of-experts framework incorporated with an innovativegating function. Designed to integrate a diverse number of modalities, FuseMoEis effective in managing scenarios with missing modalities and irregularlysampled data trajectories. Theoretically, our unique gating functioncontributes to enhanced convergence rates, leading to better performance inmultiple downstream tasks. The practical utility of FuseMoE in real world isvalidated by a challenging set of clinical risk prediction tasks.</description><author>Xing Han, Huy Nguyen, Carl Harris, Nhat Ho, Suchi Saria</author><pubDate>Mon, 05 Feb 2024 17:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03226v1</guid></item><item><title>English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts</title><link>http://arxiv.org/abs/2402.03223v1</link><description>Emotion classification in text is a challenging and subjective task, due tothe involved cognitive inference processes that are required to interpret atextual stimulus. In addition, the set of emotion categories is highlydomain-specific. For instance, literature analysis might require the use ofaesthetic emotions (e.g., finding something beautiful), and social mediaanalysis could benefit from fine-grained sets (e.g., separating anger fromannoyance) in contrast to basic emotion categories. This renders the task aninteresting field for zero-shot classifications, in which the label set is notknown at model development time. Unfortunately, most resources for emotionanalysis are English, and therefore, most studies on emotion analysis have beenperformed in English, including those that involve prompting language modelsfor text labels. This leaves us with a research gap that we address in thispaper: In which language should we prompt for emotion labels on non-Englishtexts? This is particularly of interest when we have access to a multilinguallarge language model, because we could request labels with English prompts evenfor non-English data. Our experiments with natural language inference-basedlanguage models show that it is consistently better to use English prompts evenif the data is in a different language.</description><author>Patrick Barreiß, Roman Klinger, Jeremy Barnes</author><pubDate>Mon, 05 Feb 2024 17:36:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03223v1</guid></item><item><title>AI-as-exploration: Navigating intelligence space</title><link>http://arxiv.org/abs/2401.07964v2</link><description>Artificial Intelligence is a field that lives many lives, and the term hascome to encompass a motley collection of scientific and commercial endeavours.In this paper, I articulate the contours of a rather neglected but centralscientific role that AI has to play, which I dub `AI-as-exploration'.The basicthrust of AI-as-exploration is that of creating and studying systems that canreveal candidate building blocks of intelligence that may differ from the formsof human and animal intelligence we are familiar with. In other words, Isuggest that AI is one of the best tools we have for exploring intelligencespace, namely the space of possible intelligent systems. I illustrate the valueof AI-as-exploration by focusing on a specific case study, i.e., recent work onthe capacity to combine novel and invented concepts in humans and LargeLanguage Models. I show that the latter, despite showing human-level accuracyin such a task, most probably solve it in ways radically different, but no lessrelevant to intelligence research, to those hypothesised for humans.</description><author>Dimitri Coelho Mollo</author><pubDate>Mon, 05 Feb 2024 17:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07964v2</guid></item><item><title>"Define Your Terms" : Enhancing Efficient Offensive Speech Classification with Definition</title><link>http://arxiv.org/abs/2402.03221v1</link><description>The propagation of offensive content through social media channels hasgarnered attention of the research community. Multiple works have proposedvarious semantically related yet subtle distinct categories of offensivespeech. In this work, we explore meta-earning approaches to leverage thediversity of offensive speech corpora to enhance their reliable and efficientdetection. We propose a joint embedding architecture that incorporates theinput's label and definition for classification via Prototypical Network. Ourmodel achieves at least 75% of the maximal F1-score while using less than 10%of the available training data across 4 datasets. Our experimental findingsalso provide a case study of training strategies valuable to combat resourcescarcity.</description><author>Huy Nghiem, Umang Gupta, Fred Morstatter</author><pubDate>Mon, 05 Feb 2024 17:33:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03221v1</guid></item><item><title>The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents</title><link>http://arxiv.org/abs/2402.03220v1</link><description>We investigate the training dynamics of two-layer neural networks whenlearning multi-index target functions. We focus on multi-pass gradient descent(GD) that reuses the batches multiple times and show that it significantlychanges the conclusion about which functions are learnable compared tosingle-pass gradient descent. In particular, multi-pass GD with finite stepsizeis found to overcome the limitations of gradient flow and single-pass GD givenby the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe etal., 2023) of the target function. We show that upon re-using batches, thenetwork achieves in just two time steps an overlap with the target subspaceeven for functions not satisfying the staircase property (Abbe et al., 2021).We characterize the (broad) class of functions efficiently learned in finitetime. The proof of our results is based on the analysis of the DynamicalMean-Field Theory (DMFT). We further provide a closed-form description of thedynamical process of the low-dimensional projections of the weights, andnumerical experiments illustrating the theory.</description><author>Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborová, Florent Krzakala</author><pubDate>Mon, 05 Feb 2024 17:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03220v1</guid></item><item><title>HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations</title><link>http://arxiv.org/abs/2305.14195v3</link><description>While demographic factors like age and gender change the way people talk, andin particular, the way people talk to machines, there is little investigationinto how large pre-trained language models (LMs) can adapt to these changes. Toremedy this gap, we consider how demographic factors in LM language skills canbe measured to determine compatibility with a target demographic. We suggestclinical techniques from Speech Language Pathology, which has norms foracquisition of language skills in humans. We conduct evaluation with a domainexpert (i.e., a clinically licensed speech language pathologist), and alsopropose automated techniques to complement clinical evaluation at scale.Empirically, we focus on age, finding LM capability varies widely depending ontask: GPT-3.5 mimics the ability of humans ranging from age 6-15 at tasksrequiring inference, and simultaneously, outperforms a typical 21 year old atmemorization. GPT-3.5 also has trouble with social language use, exhibitingless than 50% of the tested pragmatic skills. Findings affirm the importance ofconsidering demographic alignment and conversational goals when using LMs aspublic-facing tools. Code, data, and a package will be available.</description><author>Anthony Sicilia, Jennifer C. Gates, Malihe Alikhani</author><pubDate>Mon, 05 Feb 2024 17:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14195v3</guid></item><item><title>BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</title><link>http://arxiv.org/abs/2402.03216v1</link><description>In this paper, we present a new embedding model, called M3-Embedding, whichis distinguished for its versatility in Multi-Linguality, Multi-Functionality,and Multi-Granularity. It can support more than 100 working languages, leadingto new state-of-the-art performances on multi-lingual and cross-lingualretrieval tasks. It can simultaneously perform the three common retrievalfunctionalities of embedding model: dense retrieval, multi-vector retrieval,and sparse retrieval, which provides a unified model foundation for real-worldIR applications. It is able to process inputs of different granularities,spanning from short sentences to long documents of up to 8192 tokens. Theeffective training of M3-Embedding involves the following technicalcontributions. We propose a novel self-knowledge distillation approach, wherethe relevance scores from different retrieval functionalities can be integratedas the teacher signal to enhance the training quality. We also optimize thebatching strategy, enabling a large batch size and high training throughput toensure the discriminativeness of embeddings. To the best of our knowledge,M3-Embedding is the first embedding model which realizes such a strongversatility. The model and code will be publicly available athttps://github.com/FlagOpen/FlagEmbedding.</description><author>Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu</author><pubDate>Mon, 05 Feb 2024 17:26:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03216v1</guid></item><item><title>Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?</title><link>http://arxiv.org/abs/2402.03214v1</link><description>The advent of generative AI images has completely disrupted the art world.Identifying AI generated images from human art is a challenging problem whoseimpact is growing over time. The failure to address this problem allows badactors to defraud individuals paying a premium for human art, and companieswhose stated policies forbid AI imagery. This is also critical for AI modeltrainers, who need to filter training data to avoid potential model collapse.There are several different approaches to distinguishing human art from AIimages, including classifiers trained by supervised learning, research toolstargeting diffusion models, and identification by professional artists usingtheir knowledge of artistic techniques. In this paper, we seek to understandhow well these approaches can perform against today's modern generative modelsin both benign and adversarial settings. We curate real human art across 7styles, generate matching images from 5 generative models, and apply 8detectors (5 automated detectors and 3 different human groups including 180crowdworkers, 4000+ professional artists, and 13 expert artists experienced atdetecting AI). Both Hive and expert artists do very well, but make mistakes indifferent ways (Hive is weaker against adversarial perturbations while Expertartists produce higher false positives). We believe these weaknesses willremain as models continue to evolve, and use our data to demonstrate why acombined team of human and automated detectors provides the best combination ofaccuracy and robustness.</description><author>Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao</author><pubDate>Mon, 05 Feb 2024 17:25:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03214v1</guid></item><item><title>Light and Optimal Schrödinger Bridge Matching</title><link>http://arxiv.org/abs/2402.03207v1</link><description>Schr\"odinger Bridges (SB) have recently gained the attention of the MLcommunity as a promising extension of classic diffusion models which is alsointerconnected to the Entropic Optimal Transport (EOT). Recent solvers for SBexploit the pervasive bridge matching procedures. Such procedures aim torecover a stochastic process transporting the mass between distributions givenonly a transport plan between them. In particular, given the EOT plan, theseprocedures can be adapted to solve SB. This fact is heavily exploited by recentworks giving rives to matching-based SB solvers. The cornerstone here isrecovering the EOT plan: recent works either use heuristical approximations(e.g., the minibatch OT) or establish iterative matching procedures which bythe design accumulate the error during the training. We address theselimitations and propose a novel procedure to learn SB which we call the\textbf{optimal Schr\"odinger bridge matching}. It exploits the optimalparameterization of the diffusion process and provably recovers the SB process\textbf{(a)} with a single bridge matching step and \textbf{(b)} with arbitrarytransport plan as the input. Furthermore, we show that the optimal bridgematching objective coincides with the recently discovered energy-based modeling(EBM) objectives to learn EOT/SB. Inspired by this observation, we develop alight solver (which we call LightSB-M) to implement optimal matching inpractice using the Gaussian mixture parameterization of the Schr\"odingerpotential. We experimentally showcase the performance of our solver in a rangeof practical tasks. The code for the LightSB-M solver can be found at\url{https://github.com/SKholkin/LightSB-Matching}.</description><author>Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, Alexander Korotin</author><pubDate>Mon, 05 Feb 2024 17:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03207v1</guid></item><item><title>Denoising-Diffusion Alignment for Continuous Sign Language Recognition</title><link>http://arxiv.org/abs/2305.03614v3</link><description>As a key to social good, continuous sign language recognition (CSLR) aims topromote active and accessible communication for the hearing impaired. CurrentCSLR research adopts a cross-modality alignment scheme to learn the mappingrelationship between "video clip-textual gloss". However, this local alignmentmethod, especially with weak data annotation, ignores the contextualinformation of modalities and directly reduces the generalization of visualfeatures. To this end, we propose a novel Denoising-Diffusion global Alignmentscheme (DDA), which focuses on modeling the mapping of the "entire video-glosssequence". DDA consists of a partial noising process strategy and adenoising-diffusion autoencoder. The former is used to achieve efficientguidance of the text modality to the visual modality; the latter learns theglobal alignment information of the two modalities in a denoising manner. OurDDA confirms the feasibility of diffusion models for visual representationlearning in CSLR. Experiments on three public benchmarks demonstrate that ourmethod achieves state-of-the-art performances. Furthermore, the proposed methodcan be a plug-and-play optimization to generalize other CSLR methods.</description><author>Leming Guo, Wanli Xue, Ze Kang, Yuxi Zhou, Tiantian Yuan, Zan Gao, Shengyong Chen</author><pubDate>Mon, 05 Feb 2024 17:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03614v3</guid></item><item><title>Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems</title><link>http://arxiv.org/abs/2402.03204v1</link><description>We develop a multi-agent reinforcement learning (MARL) algorithm to minimizethe total energy consumption of multiple massive MIMO (multiple-inputmultiple-output) base stations (BSs) in a multi-cell network while preservingthe overall quality-of-service (QoS) by making decisions on the multi-leveladvanced sleep modes (ASMs) and antenna switching of these BSs. The problem ismodeled as a decentralized partially observable Markov decision process(DEC-POMDP) to enable collaboration between individual BSs, which is necessaryto tackle inter-cell interference. A multi-agent proximal policy optimization(MAPPO) algorithm is designed to learn a collaborative BS control policy. Toenhance its scalability, a modified version called MAPPO-neighbor policy isfurther proposed. Simulation results demonstrate that the trained MAPPO agentachieves better performance compared to baseline policies. Specifically,compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, theMAPPO-neighbor policy reduces power consumption by approximately 8.7% duringlow-traffic hours and improves energy efficiency by approximately 19% duringhigh-traffic hours, respectively.</description><author>Tianzhang Cai, Qichen Wang, Shuai Zhang, Özlem Tuğfe Demir, Cicek Cavdar</author><pubDate>Mon, 05 Feb 2024 17:15:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03204v1</guid></item><item><title>Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection</title><link>http://arxiv.org/abs/2401.15222v2</link><description>Background: The semantics of entities extracted from a clinical text can bedramatically altered by modifiers, including entity negation, uncertainty,conditionality, severity, and subject. Existing models for determiningmodifiers of clinical entities involve regular expression or features weightsthat are trained independently for each modifier. Methods: We develop and evaluate a multi-task transformer architecture designwhere modifiers are learned and predicted jointly using the publicly availableSemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set thatcontains modifiers shared with SemEval as well as novel modifiers specific forOUD. We evaluate the effectiveness of our multi-task learning approach versuspreviously published systems and assess the feasibility of transfer learningfor clinical entity modifiers when only a portion of clinical modifiers areshared. Results: Our approach achieved state-of-the-art results on the ShARe corpusfrom SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,1.7% on unweighted accuracy, and 10% on micro F1 scores. Conclusions: We show that learned weights from our shared model can beeffectively transferred to a new partially matched data set, validating the useof transfer learning for clinical text modifiers</description><author>Abdullateef I. Almudaifer, Whitney Covington, JaMor Hairston, Zachary Deitch, Ankit Anand, Caleb M. Carroll, Estera Crisan, William Bradford, Lauren Walter, Eaton Ellen, Sue S. Feldman, John D. Osborne</author><pubDate>Mon, 05 Feb 2024 17:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15222v2</guid></item><item><title>Guidance with Spherical Gaussian Constraint for Conditional Diffusion</title><link>http://arxiv.org/abs/2402.03201v1</link><description>Recent advances in diffusion models attempt to handle conditional generativetasks by utilizing a differentiable loss function for guidance without the needfor additional training. While these methods achieved certain success, theyoften compromise on sample quality and require small guidance step sizes,leading to longer sampling processes. This paper reveals that the fundamentalissue lies in the manifold deviation during the sampling process when lossguidance is employed. We theoretically show the existence of manifold deviationby establishing a certain lower bound for the estimation error of the lossguidance. To mitigate this problem, we propose Diffusion with SphericalGaussian constraint (DSG), drawing inspiration from the concentrationphenomenon in high-dimensional Gaussian distributions. DSG effectivelyconstrains the guidance step within the intermediate data manifold throughoptimization and enables the use of larger guidance steps. Furthermore, wepresent a closed-form solution for DSG denoising with the Spherical Gaussianconstraint. Notably, DSG can seamlessly integrate as a plugin module withinexisting training-free conditional diffusion methods. Implementing DSG merelyinvolves a few lines of additional code with almost no extra computationaloverhead, yet it leads to significant performance improvements. Comprehensiveexperimental results in various conditional generation tasks validate thesuperiority and adaptability of DSG in terms of both sample quality and timeefficiency.</description><author>Lingxiao Yang, Shutong Ding, Yifan Cai, Jingyi Yu, Jingya Wang, Ye Shi</author><pubDate>Mon, 05 Feb 2024 17:12:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03201v1</guid></item><item><title>Not All Learnable Distribution Classes are Privately Learnable</title><link>http://arxiv.org/abs/2402.00267v2</link><description>We give an example of a class of distributions that is learnable in totalvariation distance with a finite number of samples, but not learnable under$(\varepsilon, \delta)$-differential privacy. This refutes a conjecture ofAshtiani.</description><author>Mark Bun, Gautam Kamath, Argyris Mouzakis, Vikrant Singhal</author><pubDate>Mon, 05 Feb 2024 17:10:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00267v2</guid></item><item><title>Faster Rates for Switchback Experiments</title><link>http://arxiv.org/abs/2312.15574v2</link><description>Switchback experimental design, wherein a single unit (e.g., a whole system)is exposed to a single random treatment for interspersed blocks of time,tackles both cross-unit and temporal interference. Hu and Wager (2022) recentlyproposed a treatment-effect estimator that truncates the beginnings of blocksand established a $T^{-1/3}$ rate for estimating the global average treatmenteffect (GATE) in a Markov setting with rapid mixing. They claim this rate isoptimal and suggest focusing instead on a different (and design-dependent)estimand so as to enjoy a faster rate. For the same design we propose analternative estimator that uses the whole block and surprisingly show that itin fact achieves an estimation rate of $\sqrt{\log T/T}$ for the originaldesign-independent GATE estimand under the same assumptions.</description><author>Su Jia, Nathan Kallus, Christina Lee Yu</author><pubDate>Mon, 05 Feb 2024 17:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15574v2</guid></item><item><title>Isotropy, Clusters, and Classifiers</title><link>http://arxiv.org/abs/2402.03191v1</link><description>Whether embedding spaces use all their dimensions equally, i.e., whether theyare isotropic, has been a recent subject of discussion. Evidence has beenaccrued both for and against enforcing isotropy in embedding spaces. In thepresent paper, we stress that isotropy imposes requirements on the embeddingspace that are not compatible with the presence of clusters -- which alsonegatively impacts linear classification objectives. We demonstrate this factempirically and use it to shed light on previous results from the literature.</description><author>Timothee Mickus, Stig-Arne Grönroos, Joseph Attieh</author><pubDate>Mon, 05 Feb 2024 16:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03191v1</guid></item><item><title>Unified Hallucination Detection for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2402.03190v1</link><description>Despite significant strides in multimodal tasks, Multimodal Large LanguageModels (MLLMs) are plagued by the critical issue of hallucination. The reliabledetection of such hallucinations in MLLMs has, therefore, become a vital aspectof model evaluation and the safeguarding of practical application deployment.Prior research in this domain has been constrained by a narrow focus onsingular tasks, an inadequate range of hallucination categories addressed, anda lack of detailed granularity. In response to these challenges, our workexpands the investigative horizons of hallucination detection. We present anovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitatethe evaluation of advancements in hallucination detection methods.Additionally, we unveil a novel unified multimodal hallucination detectionframework, UNIHD, which leverages a suite of auxiliary tools to validate theoccurrence of hallucinations robustly. We demonstrate the effectiveness ofUNIHD through meticulous evaluation and comprehensive analysis. We also providestrategic insights on the application of specific tools for addressing variouscategories of hallucinations.</description><author>Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Jinjie Gu, Huajun Chen</author><pubDate>Mon, 05 Feb 2024 16:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03190v1</guid></item><item><title>Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss terms</title><link>http://arxiv.org/abs/2402.03188v1</link><description>Advances in face swapping have enabled the automatic generation of highlyrealistic faces. Yet face swaps are perceived differently than when looking atreal faces, with key differences in viewer behavior surrounding the eyes. Faceswapping algorithms generally place no emphasis on the eyes, relying on pixelor feature matching losses that consider the entire face to guide the trainingprocess. We further investigate viewer perception of face swaps, focusing ouranalysis on the presence of an uncanny valley effect. We additionally propose anovel loss equation for the training of face swapping models, leveraging apretrained gaze estimation network to directly improve representation of theeyes. We confirm that viewed face swaps do elicit uncanny responses fromviewers. Our proposed improvements significant reduce viewing angle errorsbetween face swaps and their source material. Our method additionally reducesthe prevalence of the eyes as a deciding factor when viewers perform deepfakedetection tasks. Our findings have implications on face swapping for specialeffects, as digital avatars, as privacy mechanisms, and more; negativeresponses from users could limit effectiveness in said applications. Our gazeimprovements are a first step towards alleviating negative viewer perceptionsvia a targeted approach.</description><author>Ethan Wilson, Frederick Shic, Sophie Jörg, Eakta Jain</author><pubDate>Mon, 05 Feb 2024 16:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03188v1</guid></item><item><title>How Good is a Single Basin?</title><link>http://arxiv.org/abs/2402.03187v1</link><description>The multi-modal nature of neural loss landscapes is often considered to bethe main driver behind the empirical success of deep ensembles. In this work,we probe this belief by constructing various "connected" ensembles which arerestricted to lie in the same basin. Through our experiments, we demonstratethat increased connectivity indeed negatively impacts performance. However,when incorporating the knowledge from other basins implicitly throughdistillation, we show that the gap in performance can be mitigated byre-discovering (multi-basin) deep ensembles within a single basin. Thus, weconjecture that while the extra-basin knowledge is at least partially presentin any given basin, it cannot be easily harnessed without learning it fromother basins.</description><author>Kai Lion, Lorenzo Noci, Thomas Hofmann, Gregor Bachmann</author><pubDate>Mon, 05 Feb 2024 16:51:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03187v1</guid></item><item><title>Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning</title><link>http://arxiv.org/abs/2402.03183v1</link><description>Learning and predicting the performance of given software configurations areof high importance to many software engineering activities. While configurablesoftware systems will almost certainly face diverse running environments (e.g.,version, hardware, and workload), current work often either builds performancemodels under a single environment or fails to properly handle data from diversesettings, hence restricting their accuracy for new environments. In this paper,we target configuration performance learning under multiple environments. We doso by designing SeMPL - a meta-learning framework that learns the commonunderstanding from configurations measured in distinct (meta) environments andgeneralizes them to the unforeseen, target environment. What makes it unique isthat unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that trainthe meta environments in parallel, we train them sequentially, one at a time.The order of training naturally allows discriminating the contributions amongmeta environments in the meta-model built, which fits better with thecharacteristic of configuration data that is known to dramatically differbetween different environments. Through comparing with 15 state-of-the-artmodels under nine systems, our extensive experimental results demonstrate thatSeMPL performs considerably better on 89% of the systems with up to 99%accuracy improvement, while being data-efficient, leading to a maximum of 3.86xspeedup. All code and data can be found at our repository:https://github.com/ideas-labo/SeMPL.</description><author>Jingzhi Gong, Tao Chen</author><pubDate>Mon, 05 Feb 2024 16:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03183v1</guid></item><item><title>Empowering Time Series Analysis with Large Language Models: A Survey</title><link>http://arxiv.org/abs/2402.03182v1</link><description>Recently, remarkable progress has been made over large language models(LLMs), demonstrating their unprecedented capability in varieties of naturallanguage tasks. However, completely training a large general-purpose model fromthe scratch is challenging for time series analysis, due to the large volumesand varieties of time series data, as well as the non-stationarity that leadsto concept drift impeding continuous model adaptation and re-training. Recentadvances have shown that pre-trained LLMs can be exploited to capture complexdependencies in time series data and facilitate various applications. In thissurvey, we provide a systematic overview of existing methods that leverage LLMsfor time series analysis. Specifically, we first state the challenges andmotivations of applying language models in the context of time series as wellas brief preliminaries of LLMs. Next, we summarize the general pipeline forLLM-based time series analysis, categorize existing methods into differentgroups (i.e., direct query, tokenization, prompt design, fine-tune, and modelintegration), and highlight the key ideas within each group. We also discussthe applications of LLMs for both general and spatial-temporal time seriesdata, tailored to specific domains. Finally, we thoroughly discuss futureresearch opportunities to empower time series analysis with LLMs.</description><author>Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, Dongjin Song</author><pubDate>Mon, 05 Feb 2024 16:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03182v1</guid></item><item><title>C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2402.03181v1</link><description>Despite the impressive capabilities of large language models (LLMs) acrossdiverse applications, they still suffer from trustworthiness issues, such ashallucinations and misalignments. Retrieval-augmented language models (RAG)have been proposed to enhance the credibility of generations by groundingexternal knowledge, but the theoretical understandings of their generationrisks remains unexplored. In this paper, we answer: 1) whether RAG can indeedlead to low generation risks, 2) how to provide provable guarantees on thegeneration risks of RAG and vanilla LLMs, and 3) what sufficient conditionsenable RAG models to reduce generation risks. We propose C-RAG, the firstframework to certify generation risks for RAG models. Specifically, we provideconformal risk analysis for RAG models and certify an upper confidence bound ofgeneration risks, which we refer to as conformal generation risk. We alsoprovide theoretical guarantees on conformal generation risks for generalbounded risk functions under test distribution shifts. We prove that RAGachieves a lower conformal generation risk than that of a single LLM when thequality of the retrieval model and transformer is non-trivial. Our intensiveempirical results demonstrate the soundness and tightness of our conformalgeneration risk guarantees across four widely-used NLP datasets on fourstate-of-the-art retrieval models.</description><author>Mintong Kang, Nezihe Merve Gürel, Ning Yu, Dawn Song, Bo Li</author><pubDate>Mon, 05 Feb 2024 16:46:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03181v1</guid></item><item><title>Cool-chic video: Learned video coding with 800 parameters</title><link>http://arxiv.org/abs/2402.03179v1</link><description>We propose a lightweight learned video codec with 900 multiplications perdecoded pixel and 800 parameters overall. To the best of our knowledge, this isone of the neural video codecs with the lowest decoding complexity. It is builtupon the overfitted image codec Cool-chic and supplements it with an intercoding module to leverage the video's temporal redundancies. The proposed modelis able to compress videos using both low-delay and random accessconfigurations and achieves rate-distortion close to AVC while out-performingother overfitted codecs such as FFNeRV. The system is made open-source:orange-opensource.github.io/Cool-Chic.</description><author>Thomas Leguay, Théo Ladune, Pierrick Philippe, Olivier Déforges</author><pubDate>Mon, 05 Feb 2024 16:45:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03179v1</guid></item><item><title>CIDAR: Culturally Relevant Instruction Dataset For Arabic</title><link>http://arxiv.org/abs/2402.03177v1</link><description>Instruction tuning has emerged as a prominent methodology for teaching LargeLanguage Models (LLMs) to follow instructions. However, current instructiondatasets predominantly cater to English or are derived from English-dominatedLLMs, resulting in inherent biases toward Western culture. This biassignificantly impacts the linguistic structures of non-English languages suchas Arabic, which has a distinct grammar reflective of the diverse culturesacross the Arab region. This paper addresses this limitation by introducingCIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabicinstruction-tuning dataset culturally-aligned by human reviewers. CIDARcontains 10,000 instruction and output pairs that represent the Arab region. Wediscuss the cultural relevance of CIDAR via the analysis and comparison toother models fine-tuned on other datasets. Our experiments show that CIDAR canhelp enrich research efforts in aligning LLMs with the Arabic culture. All thecode is available at https://github.com/ARBML/CIDAR.</description><author>Zaid Alyafeai, Khalid Almubarak, Ahmed Ashraf, Deema Alnuhait, Saied Alshahrani, Gubran A. Q. Abdulrahman, Gamil Ahmed, Qais Gawah, Zead Saleh, Mustafa Ghaleb, Yousef Ali, Maged S. Al-Shaibani</author><pubDate>Mon, 05 Feb 2024 16:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03177v1</guid></item><item><title>Comparison of Topic Modelling Approaches in the Banking Context</title><link>http://arxiv.org/abs/2402.03176v1</link><description>Topic modelling is a prominent task for automatic topic extraction in manyapplications such as sentiment analysis and recommendation systems. Theapproach is vital for service industries to monitor their customer discussions.The use of traditional approaches such as Latent Dirichlet Allocation (LDA) fortopic discovery has shown great performances, however, they are not consistentin their results as these approaches suffer from data sparseness and inabilityto model the word order in a document. Thus, this study presents the use ofKernel Principal Component Analysis (KernelPCA) and K-means Clustering in theBERTopic architecture. We have prepared a new dataset using tweets fromcustomers of Nigerian banks and we use this to compare the topic modellingapproaches. Our findings showed KernelPCA and K-means in the BERTopicarchitecture-produced coherent topics with a coherence score of 0.8463.</description><author>Bayode Ogunleye, Tonderai Maswera, Laurence Hirsch, Jotham Gaudoin, Teresa Brunsdon</author><pubDate>Mon, 05 Feb 2024 16:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03176v1</guid></item><item><title>The Matrix: A Bayesian learning model for LLMs</title><link>http://arxiv.org/abs/2402.03175v1</link><description>In this paper, we introduce a Bayesian learning model to understand thebehavior of Large Language Models (LLMs). We explore the optimization metric ofLLMs, which is based on predicting the next token, and develop a novel modelgrounded in this principle. Our approach involves constructing an idealgenerative text model represented by a multinomial transition probabilitymatrix with a prior, and we examine how LLMs approximate this matrix. Wediscuss the continuity of the mapping between embeddings and multinomialdistributions, and present the Dirichlet approximation theorem to approximateany prior. Additionally, we demonstrate how text generation by LLMs aligns withBayesian learning principles and delve into the implications for in-contextlearning, specifically explaining why in-context learning emerges in largermodels where prompts are considered as samples to be updated. Our findingsindicate that the behavior of LLMs is consistent with Bayesian Learning,offering new insights into their functioning and potential applications.</description><author>Siddhartha Dalal, Vishal Misra</author><pubDate>Mon, 05 Feb 2024 16:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03175v1</guid></item><item><title>Decentralized Event-Triggered Online Learning for Safe Consensus of Multi-Agent Systems with Gaussian Process Regression</title><link>http://arxiv.org/abs/2402.03174v1</link><description>Consensus control in multi-agent systems has received significant attentionand practical implementation across various domains. However, managingconsensus control under unknown dynamics remains a significant challenge forcontrol design due to system uncertainties and environmental disturbances. Thispaper presents a novel learning-based distributed control law, augmented by anauxiliary dynamics. Gaussian processes are harnessed to compensate for theunknown components of the multi-agent system. For continuous enhancement inpredictive performance of Gaussian process model, a data-efficient onlinelearning strategy with a decentralized event-triggered mechanism is proposed.Furthermore, the control performance of the proposed approach is ensured viathe Lyapunov theory, based on a probabilistic guarantee for prediction errorbounds. To demonstrate the efficacy of the proposed learning-based controller,a comparative analysis is conducted, contrasting it with both conventionaldistributed control laws and offline learning methodologies.</description><author>Xiaobing Dai, Zewen Yang, Mengtian Xu, Fangzhou Liu, Georges Hattab, Sandra Hirche</author><pubDate>Mon, 05 Feb 2024 16:41:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03174v1</guid></item><item><title>Data Diversity Matters for Robust Instruction Tuning</title><link>http://arxiv.org/abs/2311.14736v2</link><description>Recent works have shown that by curating high quality and diverse instructiontuning datasets, we can significantly improve instruction-followingcapabilities. However, creating such datasets is difficult and most works relyon manual curation or proprietary language models. Automatic data curation isdifficult as it is still not clear how we can define diversity for instructiontuning, how diversity and quality depend on one other, and how we can optimizedataset quality and diversity. To resolve these issue, we propose a newalgorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simplemethod to simultaneously control dataset diversity and quality, allowing us toconduct an in-depth study on the effect of diversity and quality on instructiontuning performance. From this study we draw two key insights (1) there is anatural tradeoff between data diversity and quality and (2) increasing datadiversity significantly improves the worst case instruction followingperformance, therefore improving robustness. We validate the performance ofQDIT on several large scale instruction tuning datasets, where we find it cansubstantially improve worst and average case performance compared toquality-driven data selection.</description><author>Alexander Bukharin, Tuo Zhao</author><pubDate>Mon, 05 Feb 2024 16:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14736v2</guid></item><item><title>Multi: Multimodal Understanding Leaderboard with Text and Images</title><link>http://arxiv.org/abs/2402.03173v1</link><description>Rapid progress in multimodal large language models (MLLMs) highlights theneed to introduce challenging yet realistic benchmarks to the academiccommunity. Existing benchmarks primarily focus on simple natural imageunderstanding, but Multi emerges as a cutting-edge benchmark for MLLMs,offering a comprehensive dataset for evaluating MLLMs against understandingcomplex figures and tables, and scientific questions. This benchmark,reflecting current realistic examination styles, provides multimodal inputs andrequires responses that are either precise or open-ended, similar to real-lifeschool tests. It challenges MLLMs with a variety of tasks, ranging from formuladerivation to image detail analysis, and cross-modality reasoning. Multiincludes over 18,000 questions, with a focus on science-based QA in diverseformats. We also introduce Multi-Elite, a 500-question subset for testing theextremities of MLLMs, and Multi-Extend, which enhances In-Context Learningresearch with more than 4,500 knowledge pieces. Our evaluation indicatessignificant potential for MLLM advancement, with GPT-4V achieving a 63.7%accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and53.7%. Multi serves not only as a robust evaluation platform but also paves theway for the development of expert-level AI.</description><author>Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yiming Sun, Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, Situo Zhang, Zihan Zhao, Liangtai Sun, Kai Yu</author><pubDate>Mon, 05 Feb 2024 16:41:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03173v1</guid></item><item><title>Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings</title><link>http://arxiv.org/abs/2402.03172v1</link><description>Although the International Classification of Diseases (ICD) has been adoptedworldwide, manually assigning ICD codes to clinical text is time-consuming,error-prone, and expensive, motivating the development of automated approaches.This paper describes a novel approach for automated ICD coding, combiningseveral ideas from previous related work. We specifically employ a strongTransformer-based model as a text encoder and, to handle lengthy clinicalnarratives, we explored either (a) adapting the base encoder model into aLongformer, or (b) dividing the text into chunks and processing each chunkindependently. The representations produced by the encoder are combined with alabel embedding mechanism that explores diverse ICD code synonyms. Experimentswith different splits of the MIMIC-III dataset show that the proposed approachoutperforms the current state-of-the-art models in ICD coding, with the labelembeddings significantly contributing to the good performance. Our approachalso leads to properly calibrated classification results, which can effectivelyinform downstream tasks such as quantification.</description><author>Gonçalo Gomes, Isabel Coutinho, Bruno Martins</author><pubDate>Mon, 05 Feb 2024 16:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03172v1</guid></item><item><title>Homograph Attacks on Maghreb Sentiment Analyzers</title><link>http://arxiv.org/abs/2402.03171v1</link><description>We examine the impact of homograph attacks on the Sentiment Analysis (SA)task of different Arabic dialects from the Maghreb North-African countries.Homograph attacks result in a 65.3% decrease in transformer classification froman F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of thisstudy is to highlight LLMs weaknesses' and to prioritize ethical andresponsible Machine Learning.</description><author>Fatima Zahra Qachfar, Rakesh M. Verma</author><pubDate>Mon, 05 Feb 2024 16:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03171v1</guid></item><item><title>Is Mamba Capable of In-Context Learning?</title><link>http://arxiv.org/abs/2402.03170v1</link><description>This work provides empirical evidence that Mamba, a newly proposed selectivestructured state space model, has similar in-context learning (ICL)capabilities as transformers. We evaluated Mamba on tasks involving simplefunction approximation as well as more complex natural language processingproblems. Our results demonstrate that across both categories of tasks, Mambamatches the performance of transformer models for ICL. Further analysis revealsthat like transformers, Mamba appears to solve ICL problems by incrementallyoptimizing its internal representations. Overall, our work suggests that Mambacan be an efficient alternative to transformers for ICL tasks involving longerinput sequences.</description><author>Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, Frank Hutter</author><pubDate>Mon, 05 Feb 2024 16:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03170v1</guid></item><item><title>A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation</title><link>http://arxiv.org/abs/2402.03169v1</link><description>This work presents a comprehensive understanding of the estimation of aplanted low-rank signal from a general spiked tensor model near thecomputational threshold. Relying on standard tools from the theory of largerandom matrices, we characterize the large-dimensional spectral behavior of theunfoldings of the data tensor and exhibit relevant signal-to-noise ratiosgoverning the detectability of the principal directions of the signal. Theseresults allow to accurately predict the reconstruction performance of truncatedmultilinear SVD (MLSVD) in the non-trivial regime. This is particularlyimportant since it serves as an initialization of the higher-order orthogonaliteration (HOOI) scheme, whose convergence to the best low-multilinear-rankapproximation depends entirely on its initialization. We give a sufficientcondition for the convergence of HOOI and show that the number of iterationsbefore convergence tends to $1$ in the large-dimensional limit.</description><author>Hugo Lebeau, Florent Chatelain, Romain Couillet</author><pubDate>Mon, 05 Feb 2024 16:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03169v1</guid></item><item><title>Rethinking Semi-Supervised Imbalanced Node Classification from Bias-Variance Decomposition</title><link>http://arxiv.org/abs/2310.18765v3</link><description>This paper introduces a new approach to address the issue of class imbalancein graph neural networks (GNNs) for learning on graph-structured data. Ourapproach integrates imbalanced node classification and Bias-VarianceDecomposition, establishing a theoretical framework that closely relates dataimbalance to model variance. We also leverage graph augmentation technique toestimate the variance, and design a regularization term to alleviate the impactof imbalance. Exhaustive tests are conducted on multiple benchmarks, includingnaturally imbalanced datasets and public-split class-imbalanced datasets,demonstrating that our approach outperforms state-of-the-art methods in variousimbalanced scenarios. This work provides a novel theoretical perspective foraddressing the problem of imbalanced node classification in GNNs.</description><author>Divin Yan, Gengchen Wei, Chen Yang, Shengzhong Zhang, Zengfeng Huang</author><pubDate>Mon, 05 Feb 2024 16:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18765v3</guid></item><item><title>Equivariant Deep Weight Space Alignment</title><link>http://arxiv.org/abs/2310.13397v2</link><description>Permutation symmetries of deep networks make basic operations like modelmerging and similarity estimation challenging. In many cases, aligning theweights of the networks, i.e., finding optimal permutations between theirweights, is necessary. Unfortunately, weight alignment is an NP-hard problem.Prior research has mainly focused on solving relaxed versions of the alignmentproblem, leading to either time-consuming methods or sub-optimal solutions. Toaccelerate the alignment process and improve its quality, we propose a novelframework aimed at learning to solve the weight alignment problem, which wename Deep-Align. To that end, we first prove that weight alignment adheres totwo fundamental symmetries and then, propose a deep architecture that respectsthese symmetries. Notably, our framework does not require any labeled data. Weprovide a theoretical analysis of our approach and evaluate Deep-Align onseveral types of network architectures and learning setups. Our experimentalresults indicate that a feed-forward pass with Deep-Align produces better orequivalent alignments compared to those produced by current optimizationalgorithms. Additionally, our alignments can be used as an effectiveinitialization for other methods, leading to improved solutions with asignificant speedup in convergence.</description><author>Aviv Navon, Aviv Shamsian, Ethan Fetaya, Gal Chechik, Nadav Dym, Haggai Maron</author><pubDate>Mon, 05 Feb 2024 16:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13397v2</guid></item><item><title>Piecewise Polynomial Regression of Tame Functions via Integer Programming</title><link>http://arxiv.org/abs/2311.13544v2</link><description>We consider approximating so-called tame functions, a class of nonsmooth,nonconvex functions, with piecewise polynomial functions. Tame functions appearin a wide range of applications: functions encountered in the training of deepneural networks with all common activations, value functions of mixed-integerprograms, or wave functions of small molecules. We bound the quality ofapproximation of a tame function by a piecewise polynomial function with agiven number of segments on any full-dimensional cube. We also present thefirst ever mixed-integer programming formulation of piecewise polynomialregression. Together, these can be used to estimate tame functions. Wedemonstrate promising computational results.</description><author>Gilles Bareilles, Johannes Aspman, Jiri Nemecek, Jakub Marecek</author><pubDate>Mon, 05 Feb 2024 16:36:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13544v2</guid></item><item><title>Lumiere: A Space-Time Diffusion Model for Video Generation</title><link>http://arxiv.org/abs/2401.12945v2</link><description>We introduce Lumiere -- a text-to-video diffusion model designed forsynthesizing videos that portray realistic, diverse and coherent motion -- apivotal challenge in video synthesis. To this end, we introduce a Space-TimeU-Net architecture that generates the entire temporal duration of the video atonce, through a single pass in the model. This is in contrast to existing videomodels which synthesize distant keyframes followed by temporal super-resolution-- an approach that inherently makes global temporal consistency difficult toachieve. By deploying both spatial and (importantly) temporal down- andup-sampling and leveraging a pre-trained text-to-image diffusion model, ourmodel learns to directly generate a full-frame-rate, low-resolution video byprocessing it in multiple space-time scales. We demonstrate state-of-the-arttext-to-video generation results, and show that our design easily facilitates awide range of content creation tasks and video editing applications, includingimage-to-video, video inpainting, and stylized generation.</description><author>Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, Inbar Mosseri</author><pubDate>Mon, 05 Feb 2024 16:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12945v2</guid></item><item><title>Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity</title><link>http://arxiv.org/abs/2402.03167v1</link><description>Stochastic bilevel optimization (SBO) is becoming increasingly essential inmachine learning due to its versatility in handling nested structures. Toaddress large-scale SBO, decentralized approaches have emerged as effectiveparadigms in which nodes communicate with immediate neighbors without a centralserver, thereby improving communication efficiency and enhancing algorithmicrobustness. However, current decentralized SBO algorithms face challenges,including expensive inner-loop updates and unclear understanding of theinfluence of network topology, data heterogeneity, and the nested bilevelalgorithmic structures. In this paper, we introduce a single-loop decentralizedSBO (D-SOBA) algorithm and establish its transient iteration complexity, which,for the first time, clarifies the joint influence of network topology and dataheterogeneity on decentralized bilevel algorithms. D-SOBA achieves thestate-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, andtransient iteration complexity under more relaxed assumptions compared toexisting methods. Numerical experiments validate our theoretical findings.</description><author>Boao Kong, Shuchen Zhu, Songtao Lu, Xinmeng Huang, Kun Yuan</author><pubDate>Mon, 05 Feb 2024 16:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03167v1</guid></item><item><title>RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification</title><link>http://arxiv.org/abs/2402.03166v1</link><description>The caliber and configuration of retinal blood vessels serve as importantbiomarkers for various diseases and medical conditions. A thorough analysis ofthe retinal vasculature requires the segmentation of blood vessels and theirclassification into arteries and veins, which is typically performed on colorfundus images obtained by retinography, a widely used imaging technique.Nonetheless, manually performing these tasks is labor-intensive and prone tohuman error. Various automated methods have been proposed to address thisproblem. However, the current state of art in artery/vein segmentation andclassification faces challenges due to manifest classification errors thataffect the topological consistency of segmentation maps. This study presents aninnovative end-to-end framework, RRWNet, designed to recursively refinesemantic segmentation maps and correct manifest classification errors. Theframework consists of a fully convolutional neural network with a Basesubnetwork that generates base segmentation maps from input images, and aRecursive Refinement subnetwork that iteratively and recursively improves thesemaps. Evaluation on public datasets demonstrates the state-of-the-artperformance of the proposed method, yielding more topologically consistentsegmentation maps with fewer manifest classification errors than existingapproaches. In addition, the Recursive Refinement module proves effective inpost-processing segmentation maps from other methods, automatically correctingclassification errors and improving topological consistency. The model code,weights, and predictions are publicly available athttps://github.com/j-morano/rrwnet.</description><author>José Morano, Guilherme Aresta, Hrvoje Bogunović</author><pubDate>Mon, 05 Feb 2024 16:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03166v1</guid></item><item><title>DoGE: Domain Reweighting with Generalization Estimation</title><link>http://arxiv.org/abs/2310.15393v2</link><description>The coverage and composition of the pretraining data significantly impactsthe generalization ability of Large Language Models (LLMs). Despite itsimportance, recent LLMs still rely on heuristics and trial and error toincrease or reduce the influence of data-domains. We propose DOmain reweightingwith Generalization Estimation (DoGE), which optimizes the probability ofsampling from each domain (domain weights) in a principled way. Our approach isa two-stage process consisting of (i) training a proxy model to obtain domainweights using a bi-level optimization algorithm; (ii) training a larger basemodel by sampling training domains according to the learned domain weights. Inour experiments, we extensively show how DoGE improves the generalization ofthe base model to any target data mixture. On the SlimPajama dataset, our basemodel gets better perplexity and few-shot reasoning accuracies across $6$ taskscompared to baseline methods. Moreover, aiming to generalize to out-of-domaintarget tasks, which is unseen in the pretraining corpus (OOD domain), DoGE caneffectively identify inter-domain dependencies, and consistently achievesbetter test perplexity on the target domain.</description><author>Simin Fan, Matteo Pagliardini, Martin Jaggi</author><pubDate>Mon, 05 Feb 2024 16:33:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15393v2</guid></item><item><title>DiffusionWorldViewer: Exposing and Broadening the Worldview Reflected by Generative Text-to-Image Models</title><link>http://arxiv.org/abs/2309.09944v2</link><description>Generative text-to-image (TTI) models produce high-quality images from shorttextual descriptions and are widely used in academic and creative domains. Likehumans, TTI models have a worldview, a conception of the world learned fromtheir training data and task that influences the images they generate for agiven prompt. However, the worldviews of TTI models are often hidden fromusers, making it challenging for users to build intuition about TTI outputs,and they are often misaligned with users' worldviews, resulting in outputimages that do not match user expectations. In response, we introduceDiffusionWorldViewer, an interactive interface that exposes a TTI model'sworldview across output demographics and provides editing tools for aligningoutput images with user perspectives. In a user study with 18 diverse TTIusers, we find that DiffusionWorldViewer helps users represent their variedviewpoints in generated images and challenge the limited worldview reflected incurrent TTI models.</description><author>Zoe De Simone, Angie Boggust, Arvind Satyanarayan, Ashia Wilson</author><pubDate>Mon, 05 Feb 2024 16:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09944v2</guid></item><item><title>Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories</title><link>http://arxiv.org/abs/2402.03164v1</link><description>Representing time is crucial for cyber-physical systems and has been studiedextensively in the Situation Calculus. The most commonly used approachrepresents time by adding a real-valued fluent $\mathit{time}(a)$ that attachesa time point to each action and consequently to each situation. We show that inthis approach, checking whether there is a reachable situation that satisfies agiven formula is undecidable, even if the domain of discourse is restricted toa finite set of objects. We present an alternative approach based onwell-established results from timed automata theory by introducing clocks asreal-valued fluents with restricted successor state axioms and comparisonoperators. %that only allow comparisons against fixed rationals. With thisrestriction, we can show that the reachability problem for finite-domain basicaction theories is decidable. Finally, we apply our results on Golog programrealization by presenting a decidable procedure for determining an actionsequence that is a successful execution of a given program.</description><author>Till Hofmann, Stefan Schupp, Gerhard Lakemeyer</author><pubDate>Mon, 05 Feb 2024 16:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03164v1</guid></item><item><title>Linguistic features for sentence difficulty prediction in ABSA</title><link>http://arxiv.org/abs/2402.03163v1</link><description>One of the challenges of natural language understanding is to deal with thesubjectivity of sentences, which may express opinions and emotions that addlayers of complexity and nuance. Sentiment analysis is a field that aims toextract and analyze these subjective elements from text, and it can be appliedat different levels of granularity, such as document, paragraph, sentence, oraspect. Aspect-based sentiment analysis is a well-studied topic with manyavailable data sets and models. However, there is no clear definition of whatmakes a sentence difficult for aspect-based sentiment analysis. In this paper,we explore this question by conducting an experiment with three data sets:"Laptops", "Restaurants", and "MTSC" (Multi-Target-dependent SentimentClassification), and a merged version of these three datasets. We study theimpact of domain diversity and syntactic diversity on difficulty. We use acombination of classifiers to identify the most difficult sentences and analyzetheir characteristics. We employ two ways of defining sentence difficulty. Thefirst one is binary and labels a sentence as difficult if the classifiers failto correctly predict the sentiment polarity. The second one is a six-levelscale based on how many of the top five best-performing classifiers cancorrectly predict the sentiment polarity. We also define 9 linguistic featuresthat, combined, aim at estimating the difficulty at sentence level.</description><author>Adrian-Gabriel Chifu, Sébastien Fournier</author><pubDate>Mon, 05 Feb 2024 16:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03163v1</guid></item><item><title>Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion</title><link>http://arxiv.org/abs/2402.03162v1</link><description>Recent text-to-video diffusion models have achieved impressive progress. Inpractice, users often desire the ability to control object motion and cameramovement independently for customized video creation. However, current methodslack the focus on separately controlling object motion and camera movement in adecoupled manner, which limits the controllability and flexibility oftext-to-video models. In this paper, we introduce Direct-a-Video, a system thatallows users to independently specify motions for one or multiple objectsand/or camera movements, as if directing a video. We propose a simple yeteffective strategy for the decoupled control of object motion and cameramovement. Object motion is controlled through spatial cross-attentionmodulation using the model's inherent priors, requiring no additionaloptimization. For camera movement, we introduce new temporal cross-attentionlayers to interpret quantitative camera movement parameters. We further employan augmentation-based approach to train these layers in a self-supervisedmanner on a small-scale dataset, eliminating the need for explicit motionannotation. Both components operate independently, allowing individual orcombined control, and can generalize to open-domain scenarios. Extensiveexperiments demonstrate the superiority and effectiveness of our method.Project page: https://direct-a-video.github.io/.</description><author>Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, Jing Liao</author><pubDate>Mon, 05 Feb 2024 16:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03162v1</guid></item><item><title>Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</title><link>http://arxiv.org/abs/2402.03161v1</link><description>In light of recent advances in multimodal Large Language Models (LLMs), thereis increasing attention to scaling them from image-text data to moreinformative real-world videos. Compared to static images, video poses uniquechallenges for effective large-scale pre-training due to the modeling of itsspatiotemporal dynamics. In this paper, we address such limitations invideo-language pre-training with an efficient video decomposition thatrepresents each video as keyframes and temporal motions. These are then adaptedto an LLM using well-designed tokenizers that discretize visual and temporalinformation as a few tokens, thus enabling unified generative pre-training ofvideos, images, and text. At inference, the generated tokens from the LLM arecarefully recovered to the original continuous pixel space to create variousvideo content. Our proposed framework is both capable of comprehending andgenerating image and video content, as demonstrated by its competitiveperformance across 13 multimodal benchmarks in image and video understandingand generation. Our code and models will be available athttps://video-lavit.github.io.</description><author>Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu</author><pubDate>Mon, 05 Feb 2024 16:30:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03161v1</guid></item><item><title>Using Evolutionary Algorithms to Find Cache-Friendly Generalized Morton Layouts for Arrays</title><link>http://arxiv.org/abs/2309.07002v2</link><description>The layout of multi-dimensional data can have a significant impact on theefficacy of hardware caches and, by extension, the performance of applications.Common multi-dimensional layouts include the canonical row-major andcolumn-major layouts as well as the Morton curve layout. In this paper, wedescribe how the Morton layout can be generalized to a very large family ofmulti-dimensional data layouts with widely varying performance characteristics.We posit that this design space can be efficiently explored using acombinatorial evolutionary methodology based on genetic algorithms. To thisend, we propose a chromosomal representation for such layouts as well as amethodology for estimating the fitness of array layouts using cache simulation.We show that our fitness function correlates to kernel running time in realhardware, and that our evolutionary strategy allows us to find candidates withfavorable simulated cache properties in four out of the eight real-worldapplications under consideration in a small number of generations. Finally, wedemonstrate that the array layouts found using our evolutionary method performwell not only in simulated environments but that they can effect significantperformance gains -- up to a factor ten in extreme cases -- in real hardware.</description><author>Stephen Nicholas Swatman, Ana-Lucia Varbanescu, Andy D. Pimentel, Andreas Salzburger, Attila Krasznahorkay</author><pubDate>Mon, 05 Feb 2024 16:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07002v2</guid></item><item><title>Optimal and Near-Optimal Adaptive Vector Quantization</title><link>http://arxiv.org/abs/2402.03158v1</link><description>Quantization is a fundamental optimization for many machine-learning usecases, including compressing gradients, model weights and activations, anddatasets. The most accurate form of quantization is \emph{adaptive}, where theerror is minimized with respect to a given input, rather than optimizing forthe worst case. However, optimal adaptive quantization methods are consideredinfeasible in terms of both their runtime and memory requirements. We revisit the Adaptive Vector Quantization (AVQ) problem and presentalgorithms that find optimal solutions with asymptotically improved time andspace complexity. We also present an even faster near-optimal algorithm forlarge inputs. Our experiments show our algorithms may open the door to usingAVQ more extensively in a variety of machine learning applications.</description><author>Ran Ben-Basat, Yaniv Ben-Itzhak, Michael Mitzenmacher, Shay Vargaftik</author><pubDate>Mon, 05 Feb 2024 16:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03158v1</guid></item><item><title>Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation</title><link>http://arxiv.org/abs/2310.05453v2</link><description>Universal domain adaptation aims to align the classes and reduce the featuregap between the same category of the source and target domains. The targetprivate category is set as the unknown class during the adaptation process, asit is not included in the source domain. However, most existing methodsoverlook the intra-class structure within a category, especially in cases wherethere exists significant concept shift between the samples belonging to thesame category. When samples with large concept shift are forced to be pushedtogether, it may negatively affect the adaptation performance. Moreover, fromthe interpretability aspect, it is unreasonable to align visual features withsignificant differences, such as fighter jets and civil aircraft, into the samecategory. Unfortunately, due to such semantic ambiguity and annotation cost,categories are not always classified in detail, making it difficult for themodel to perform precise adaptation. To address these issues, we propose anovel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn thedifferences between samples belonging to the same category and mine sub-classeswhen there exists significant concept shift between them. By doing so, ourmodel learns a more reasonable feature space that enhances the transferabilityand reflects the inherent differences among samples annotated as the samecategory. We evaluate the effectiveness of our MemSPM method over multiplescenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-artperformance on four benchmarks in most cases.</description><author>Yuxiang Lai, Yi Zhou, Xinghong Liu, Tao Zhou</author><pubDate>Mon, 05 Feb 2024 16:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05453v2</guid></item><item><title>DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for Blind Person Navigation</title><link>http://arxiv.org/abs/2402.03156v1</link><description>This paper introduces DogSurf - a newapproach of using quadruped robots tohelp visually impaired people navigate in real world. The presented methodallows the quadruped robot to detect slippery surfaces, and to use audio andhaptic feedback to inform the user when to stop. A state-of-the-art GRU-basedneural network architecture with mean accuracy of 99.925% was proposed for thetask of multiclass surface classification for quadruped robots. A dataset wascollected on a Unitree Go1 Edu robot. The dataset and code have been posted tothe public domain.</description><author>Artem Bazhenov, Vladimir Berman, Sergei Satsevich, Olga Shalopanova, Miguel Altamirano Cabrera, Artem Lykov, Dzmitry Tsetserukou</author><pubDate>Mon, 05 Feb 2024 16:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03156v1</guid></item><item><title>Improving Neural Additive Models with Bayesian Principles</title><link>http://arxiv.org/abs/2305.16905v3</link><description>Neural additive models (NAMs) enhance the transparency of deep neuralnetworks by handling input features in separate additive sub-networks. However,they lack inherent mechanisms that provide calibrated uncertainties and enableselection of relevant features and interactions. Approaching NAMs from aBayesian perspective, we augment them in three primary ways, namely by a)providing credible intervals for the individual additive sub-networks; b)estimating the marginal likelihood to perform an implicit selection of featuresvia an empirical Bayes procedure; and c) facilitating the ranking of featurepairs as candidates for second-order interaction in fine-tuned models. Inparticular, we develop Laplace-approximated NAMs (LA-NAMs), which show improvedempirical performance on tabular datasets and challenging real-world medicaltasks.</description><author>Kouroche Bouchiat, Alexander Immer, Hugo Yèche, Gunnar Rätsch, Vincent Fortuin</author><pubDate>Mon, 05 Feb 2024 16:22:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16905v3</guid></item></channel></rss>