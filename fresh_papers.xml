<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 Jul 2024 01:19:54 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting</title><link>http://arxiv.org/abs/2407.09475v1</link><description>Deep learning-based trajectory prediction models for autonomous driving oftenstruggle with generalization to out-of-distribution (OOD) scenarios, sometimesperforming worse than simple rule-based models. To address this limitation, wepropose a novel framework, Adaptive Prediction Ensemble (APE), which integratesdeep learning and rule-based prediction experts. A learned routing function,trained concurrently with the deep learning model, dynamically selects the mostreliable prediction based on the input scenario. Our experiments on large-scaledatasets, including Waymo Open Motion Dataset (WOMD) and Argoverse, demonstrateimprovement in zero-shot generalization across datasets. We show that ourmethod outperforms individual prediction models and other variants,particularly in long-horizon prediction and scenarios with a high proportion ofOOD data. This work highlights the potential of hybrid approaches for robustand generalizable motion prediction in autonomous driving.</description><author>Jinning Li, Jiachen Li, Sangjae Bae, David Isele</author><pubDate>Fri, 12 Jul 2024 17:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09475v1</guid></item><item><title>StyleSplat: 3D Object Style Transfer with Gaussian Splatting</title><link>http://arxiv.org/abs/2407.09473v1</link><description>Recent advancements in radiance fields have opened new avenues for creatinghigh-quality 3D assets and scenes. Style transfer can enhance these 3D assetswith diverse artistic styles, transforming creative expression. However,existing techniques are often slow or unable to localize style transfer tospecific objects. We introduce StyleSplat, a lightweight method for stylizing3D objects in scenes represented by 3D Gaussians from reference style images.Our approach first learns a photorealistic representation of the scene using 3DGaussian splatting while jointly segmenting individual 3D objects. We then usea nearest-neighbor feature matching loss to finetune the Gaussians of theselected objects, aligning their spherical harmonic coefficients with the styleimage to ensure consistency and visual appeal. StyleSplat allows for quick,customizable style transfer and localized stylization of multiple objectswithin a scene, each with a different style. We demonstrate its effectivenessacross various 3D scenes and styles, showcasing enhanced control andcustomization in 3D creation.</description><author>Sahil Jain, Avik Kuthiala, Prabhdeep Singh Sethi, Prakanshul Saxena</author><pubDate>Fri, 12 Jul 2024 17:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09473v1</guid></item><item><title>Facial Affective Behavior Analysis with Instruction Tuning</title><link>http://arxiv.org/abs/2404.05052v2</link><description>Facial affective behavior analysis (FABA) is crucial for understanding humanmental states from images. However, traditional approaches primarily deploymodels to discriminate among discrete emotion categories, and lack the finegranularity and reasoning capability for complex facial behaviors. The adventof Multi-modal Large Language Models (MLLMs) has been proven successful ingeneral visual understanding tasks. However, directly harnessing MLLMs for FABAis challenging due to the scarcity of datasets and benchmarks, neglectingfacial prior knowledge, and low training efficiency. To address thesechallenges, we introduce (i) an instruction-following dataset for two FABAtasks, e.g., emotion and action unit recognition, (ii) a benchmark FABA-Benchwith a new metric considering both recognition and generation ability, and(iii) a new MLLM "EmoLA" as a strong baseline to the community. Our initiativeon the dataset and benchmarks reveal the nature and rationale of facialaffective behaviors, i.e., fine-grained facial movement, interpretability, andreasoning. Moreover, to build an effective and efficient FABA MLLM, weintroduce a facial prior expert module with face structure knowledge and alow-rank adaptation module into pre-trained MLLM. We conduct extensiveexperiments on FABA-Bench and four commonly-used FABA datasets. The resultsdemonstrate that the proposed facial prior expert can boost the performance andEmoLA achieves the best results on our FABA-Bench. On commonly-used FABAdatasets, EmoLA is competitive rivaling task-specific state-of-the-art models.</description><author>Yifan Li, Anh Dao, Wentao Bao, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong</author><pubDate>Fri, 12 Jul 2024 17:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05052v2</guid></item><item><title>Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures</title><link>http://arxiv.org/abs/2407.09468v1</link><description>The enduring legacy of Euclidean geometry underpins classical machinelearning, which, for decades, has been primarily developed for data lying inEuclidean space. Yet, modern machine learning increasingly encounters richlystructured data that is inherently nonEuclidean. This data can exhibitintricate geometric, topological and algebraic structure: from the geometry ofthe curvature of space-time, to topologically complex interactions betweenneurons in the brain, to the algebraic transformations describing symmetries ofphysical systems. Extracting knowledge from such non-Euclidean datanecessitates a broader mathematical perspective. Echoing the 19th-centuryrevolutions that gave rise to non-Euclidean geometry, an emerging line ofresearch is redefining modern machine learning with non-Euclidean structures.Its goal: generalizing classical methods to unconventional data types withgeometry, topology, and algebra. In this review, we provide an accessiblegateway to this fast-growing field and propose a graphical taxonomy thatintegrates recent advances into an intuitive unified framework. We subsequentlyextract insights into current challenges and highlight exciting opportunitiesfor future development in this field.</description><author>Sophia Sanborn, Johan Mathe, Mathilde Papillon, Domas Buracas, Hansen J Lillemark, Christian Shewmake, Abby Bertics, Xavier Pennec, Nina Miolane</author><pubDate>Fri, 12 Jul 2024 17:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09468v1</guid></item><item><title>GraspXL: Generating Grasping Motions for Diverse Objects at Scale</title><link>http://arxiv.org/abs/2403.19649v2</link><description>Human hands possess the dexterity to interact with diverse objects such asgrasping specific parts of the objects and/or approaching them from desireddirections. More importantly, humans can grasp objects of any shape withoutobject-specific skills. Recent works synthesize grasping motions followingsingle objectives such as a desired approach heading direction or a graspingarea. Moreover, they usually rely on expensive 3D hand-object data duringtraining and inference, which limits their capability to synthesize graspingmotions for unseen objects at scale. In this paper, we unify the generation ofhand-object grasping motions across multiple motion objectives, diverse objectshapes and dexterous hand morphologies in a policy learning framework GraspXL.The objectives are composed of the graspable area, heading direction duringapproach, wrist rotation, and hand position. Without requiring any 3Dhand-object interaction data, our policy trained with 58 objects can robustlysynthesize diverse grasping motions for more than 500k unseen objects with asuccess rate of 82.2%. At the same time, the policy adheres to objectives,which enables the generation of diverse grasps per object. Moreover, we showthat our framework can be deployed to different dexterous hands and work withreconstructed or generated objects. We quantitatively and qualitativelyevaluate our method to show the efficacy of our approach. Our model, code, andthe large-scale generated motions are available athttps://eth-ait.github.io/graspxl/.</description><author>Hui Zhang, Sammy Christen, Zicong Fan, Otmar Hilliges, Jie Song</author><pubDate>Fri, 12 Jul 2024 17:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19649v2</guid></item><item><title>FairyLandAI: Personalized Fairy Tales utilizing ChatGPT and DALLE-3</title><link>http://arxiv.org/abs/2407.09467v1</link><description>In the diverse world of AI-driven storytelling, there is a unique opportunityto engage young audiences with customized, and personalized narratives. Thispaper introduces FairyLandAI an innovative Large Language Model (LLM) developedthrough OpenAI's API, specifically crafted to create personalized fairytalesfor children. The distinctive feature of FairyLandAI is its dual capability: itnot only generates stories that are engaging, age-appropriate, and reflectiveof various traditions but also autonomously produces imaginative promptssuitable for advanced image generation tools like GenAI and Dalle-3, therebyenriching the storytelling experience. FairyLandAI is expertly tailored toresonate with the imaginative worlds of children, providing narratives that areboth educational and entertaining and in alignment with the moral valuesinherent in different ages. Its unique strength lies in customizing stories tomatch individual children's preferences and cultural backgrounds, heralding anew era in personalized storytelling. Further, its integration with imagegeneration technology offers a comprehensive narrative experience thatstimulates both verbal and visual creativity. Empirical evaluations ofFairyLandAI demonstrate its effectiveness in crafting captivating stories forchildren, which not only entertain but also embody the values and teachings ofdiverse traditions. This model serves as an invaluable tool for parents andeducators, supporting them in imparting meaningful moral lessons throughengaging narratives. FairyLandAI represents a pioneering step in using LLMs,particularly through OpenAI's API, for educational and cultural enrichment,making complex moral narratives accessible and enjoyable for young, imaginativeminds.</description><author>Georgios Makridis, Athanasios Oikonomou, Vasileios Koukos</author><pubDate>Fri, 12 Jul 2024 17:46:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09467v1</guid></item><item><title>EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents</title><link>http://arxiv.org/abs/2403.12014v2</link><description>Recent SOTA approaches for embodied learning via interaction directly employlarge language models (LLMs) as agents to determine the next steps in anenvironment. Due to their world knowledge and reasoning capabilities, LLMagents achieve stronger performance than previous smaller agents based onreinforcement learning (RL); however, frequently calling LLMs is slow andexpensive. Instead of directly employing LLMs as agents, can we use LLMs'reasoning capabilities to adaptively create training environments to helpsmaller RL agents learn useful skills that they are weak at? We propose EnvGen,a novel framework to address this question. We first prompt an LLM to generatetraining environments by giving it the task description and simulatorobjectives that the agents should learn and then asking it to generate a set ofenvironment configurations (e.g., different terrains, items initially given toagents, etc.). Next, we train a small RL agent in a mixture of the original andLLM-generated environments. Then, we enable the LLM to continuously adapt thegenerated environments to progressively improve the skills that the agent isweak at, by providing feedback to the LLM in the form of the agent'sperformance. We demonstrate the usefulness of EnvGen with comprehensiveexperiments in Crafter and Heist environments. We find that a small RL agenttrained with EnvGen can outperform SOTA methods, including a GPT-4 agent, andlearns long-horizon tasks significantly faster. We also show that using an LLMto adapt environments dynamically outperforms curriculum learning approachesand how the environments are adapted to help improve RL agents' weaker skillsover time. Additionally, EnvGen is substantially more efficient as it only usesa small number of LLM calls (e.g., 4 in total), whereas LLM agents requirethousands of calls. Lastly, we present detailed ablation studies for EnvGendesign choices.</description><author>Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal</author><pubDate>Fri, 12 Jul 2024 17:39:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12014v2</guid></item><item><title>Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators</title><link>http://arxiv.org/abs/2407.09453v1</link><description>Nowadays, increasingly larger Deep Neural Networks (DNNs) are beingdeveloped, trained, and utilized. These networks require significantcomputational resources, putting a strain on both advanced and limited devices.Our solution is to implement {\em weight block sparsity}, which is a structuredsparsity that is friendly to hardware. By zeroing certain sections of theconvolution and fully connected layers parameters of pre-trained DNN models, wecan efficiently speed up the DNN's inference process. This results in a smallermemory footprint, faster communication, and fewer operations. Our work presents a vertical system that allows for the training ofconvolution and matrix multiplication weights to exploit 8x8 block sparsity ona single GPU within a reasonable amount of time. Compilers recognize thissparsity and use it for both data compaction and computation splitting intothreads. Blocks like these take full advantage of both spatial and temporallocality, paving the way for fast vector operations and memory reuse. By usingthis system on a Resnet50 model, we were able to reduce the weight by half withminimal accuracy loss, resulting in a two-times faster inference speed. We willpresent performance estimates using accurate and complete code generation forAIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, andVGG16 to demonstrate the necessary synergy between hardware overlay designs andsoftware stacks for compiling and executing machine learning applications.</description><author>Paolo D'Alberto, Taehee Jeong, Akshai Jain, Shreyas Manjunath, Mrinal Sarmah, Samuel Hsu Yaswanth Raparti, Nitesh Pipralia</author><pubDate>Fri, 12 Jul 2024 17:37:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09453v1</guid></item><item><title>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</title><link>http://arxiv.org/abs/2311.09184v2</link><description>While large language models (LLMs) can already achieve strong performance onstandard generic summarization benchmarks, their performance on more complexsummarization task settings is less studied. Therefore, we benchmark LLMs oninstruction controllable text summarization, where the model input consists ofboth a source article and a natural language requirement for desired summarycharacteristics. To this end, we curate an evaluation-only dataset for thistask setting and conduct human evaluations of five LLM-based systems to assesstheir instruction-following capabilities in controllable summarization. We thenbenchmark LLM-based automatic evaluation for this task with 4 differentevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our studyreveals that instruction controllable text summarization remains a challengingtask for LLMs, since (1) all LLMs evaluated still make factual and other typesof errors in their summaries; (2) no LLM-based evaluation methods can achieve astrong alignment with human annotators when judging the quality of candidatesummaries; (3) different LLMs show large performance gaps in summary generationand evaluation capabilities. We make our collected benchmark InstruSum publiclyavailable to facilitate future research in this direction.</description><author>Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</author><pubDate>Fri, 12 Jul 2024 17:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09184v2</guid></item><item><title>Human-like Episodic Memory for Infinite Context LLMs</title><link>http://arxiv.org/abs/2407.09450v1</link><description>Large language models (LLMs) have shown remarkable capabilities, but stillstruggle with processing extensive contexts, limiting their ability to maintaincoherence and accuracy over long sequences. In contrast, the human brain excelsat organising and retrieving episodic experiences across vast temporal scales,spanning a lifetime. In this work, we introduce EM-LLM, a novel approach thatintegrates key aspects of human episodic memory and event cognition into LLMs,enabling them to effectively handle practically infinite context lengths whilemaintaining computational efficiency. EM-LLM organises sequences of tokens intocoherent episodic events using a combination of Bayesian surprise andgraph-theoretic boundary refinement in an on-line fashion. When needed, theseevents are retrieved through a two-stage memory process, combiningsimilarity-based and temporally contiguous retrieval for efficient andhuman-like access to relevant information. Experiments on the LongBench datasetdemonstrate EM-LLM's superior performance, outperforming the state-of-the-artInfLLM model with an overall relative improvement of 4.3% across various tasks,including a 33% improvement on the PassageRetrieval task. Furthermore, ouranalysis reveals strong correlations between EM-LLM's event segmentation andhuman-perceived events, suggesting a bridge between this artificial system andits biological counterpart. This work not only advances LLM capabilities inprocessing extended contexts but also provides a computational framework forexploring human memory mechanisms, opening new avenues for interdisciplinaryresearch in AI and cognitive science.</description><author>Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang</author><pubDate>Fri, 12 Jul 2024 17:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09450v1</guid></item><item><title>ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts</title><link>http://arxiv.org/abs/2407.09447v1</link><description>Typical schemes for automated red-teaming large language models (LLMs) focuson discovering prompts that trigger a frozen language model (the defender) togenerate toxic text. This often results in the prompting model (the adversary)producing text that is unintelligible and unlikely to arise. Here, we propose areinforcement learning formulation of the LLM red-teaming task which allows usto discover prompts that both (1) trigger toxic outputs from a frozen defenderand (2) have low perplexity as scored by the defender. We argue these cases aremost pertinent in a red-teaming setting because of their likelihood to ariseduring normal use of the defender model. We solve this formulation through anovel online and weakly supervised variant of Identity Preference Optimization(IPO) on GPT-2 and GPT-2 XL defenders. We demonstrate that our policy iscapable of generating likely prompts that also trigger toxicity. Finally, wequalitatively analyze learned strategies, trade-offs of likelihood andtoxicity, and discuss implications. Source code is available for this projectat: https://github.com/sisl/ASTPrompter/.</description><author>Amelia F. Hardy, Houjun Liu, Bernard Lange, Mykel J. Kochenderfer</author><pubDate>Fri, 12 Jul 2024 17:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09447v1</guid></item><item><title>The $μ\mathcal{G}$ Language for Programming Graph Neural Networks</title><link>http://arxiv.org/abs/2407.09441v1</link><description>Graph neural networks form a class of deep learning architecturesspecifically designed to work with graph-structured data. As such, they sharethe inherent limitations and problems of deep learning, especially regardingthe issues of explainability and trustworthiness. We propose $\mu\mathcal{G}$,an original domain-specific language for the specification of graph neuralnetworks that aims to overcome these issues. The language's syntax isintroduced, and its meaning is rigorously defined by a denotational semantics.An equivalent characterization in the form of an operational semantics is alsoprovided and, together with a type system, is used to prove the type soundnessof $\mu\mathcal{G}$. We show how $\mu\mathcal{G}$ programs can be representedin a more user-friendly graphical visualization, and provide examples of itsgenerality by showing how it can be used to define some of the most populargraph neural network models, or to develop any custom graph processingapplication.</description><author>Matteo Belenchia, Flavio Corradini, Michela Quadrini, Michele Loreti</author><pubDate>Fri, 12 Jul 2024 17:27:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09441v1</guid></item><item><title>Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models</title><link>http://arxiv.org/abs/2310.01691v2</link><description>Prompt tuning in natural language processing (NLP) has become an increasinglypopular method for adapting large language models to specific tasks. However,the transferability of these prompts, especially continuous prompts, betweendifferent models remains a challenge. In this work, we propose a zero-shotcontinuous prompt transfer method, where source prompts are encoded intorelative space and the corresponding target prompts are searched fortransferring to target models. Experimental results confirm the effectivenessof our method, showing that 'task semantics' in continuous prompts can begeneralized across various language models. Moreover, we find that combining'task semantics' from multiple source models can further enhance thegeneralizability of transfer.</description><author>Zijun Wu, Yongkang Wu, Lili Mou</author><pubDate>Fri, 12 Jul 2024 17:26:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01691v2</guid></item><item><title>Needle in the Haystack for Memory Based Large Language Models</title><link>http://arxiv.org/abs/2407.01437v2</link><description>Current large language models (LLMs) often perform poorly on simple factretrieval tasks. Here we investigate if coupling a dynamically adaptableexternal memory to a LLM can alleviate this problem. For this purpose, we testLarimar, a recently proposed language model architecture which uses an externalassociative memory, on long-context recall tasks including passkey andneedle-in-the-haystack tests. We demonstrate that the external memory ofLarimar, which allows fast write and read of an episode of text samples, can beused at test time to handle contexts much longer than those seen duringtraining. We further show that the latent readouts from the memory (to whichlong contexts are written) control the decoder towards generating correctoutputs, with the memory stored off of the GPU. Compared to existingtransformer-based LLM architectures for long-context recall tasks that uselarger parameter counts or modified attention mechanisms, a relatively smallersize Larimar is able to maintain strong performance without any task-specifictraining or training on longer contexts.</description><author>Elliot Nelson, Georgios Kollias, Payel Das, Subhajit Chaudhury, Soham Dan</author><pubDate>Fri, 12 Jul 2024 17:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01437v2</guid></item><item><title>Let Me DeCode You: Decoder Conditioning with Tabular Data</title><link>http://arxiv.org/abs/2407.09437v1</link><description>Training deep neural networks for 3D segmentation tasks can be challenging,often requiring efficient and effective strategies to improve modelperformance. In this study, we introduce a novel approach, DeCode, thatutilizes label-derived features for model conditioning to support the decoderin the reconstruction process dynamically, aiming to enhance the efficiency ofthe training process. DeCode focuses on improving 3D segmentation performancethrough the incorporation of conditioning embedding with learned numericalrepresentation of 3D-label shape features. Specifically, we develop anapproach, where conditioning is applied during the training phase to guide thenetwork toward robust segmentation. When labels are not available duringinference, our model infers the necessary conditioning embedding directly fromthe input data, thanks to a feed-forward network learned during the trainingphase. This approach is tested using synthetic data and cone-beam computedtomography (CBCT) images of teeth. For CBCT, three datasets are used: onepublicly available and two in-house. Our results show that DeCode significantlyoutperforms traditional, unconditioned models in terms of generalization tounseen data, achieving higher accuracy at a reduced computational cost. Thiswork represents the first of its kind to explore conditioning strategies in 3Ddata segmentation, offering a novel and more efficient method for leveragingannotated data. Our code, pre-trained models are publicly available athttps://github.com/SanoScience/DeCode .</description><author>Tomasz Szczepański, Michal K. Grzeszczyk, Szymon Płotka, Arleta Adamowicz, Piotr Fudalej, Przemysław Korzeniowski, Tomasz Trzciński, Arkadiusz Sitek</author><pubDate>Fri, 12 Jul 2024 17:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09437v1</guid></item><item><title>Toucan: Many-to-Many Translation for 150 African Language Pairs</title><link>http://arxiv.org/abs/2407.04796v2</link><description>We address a notable gap in Natural Language Processing (NLP) by introducinga collection of resources designed to improve Machine Translation (MT) forlow-resource languages, with a specific focus on African languages. First, weintroduce two language models (LMs), Cheetah-1.2B and Cheetah-3.7B, with 1.2billion and 3.7 billion parameters respectively. Next, we finetune theaforementioned models to create toucan, an Afrocentric machine translationmodel designed to support 156 African language pairs. To evaluate Toucan, wecarefully develop an extensive machine translation benchmark, dubbedAfroLingu-MT, tailored for evaluating machine translation. Toucan significantlyoutperforms other models, showcasing its remarkable performance on MT forAfrican languages. Finally, we train a new model, spBLEU-1K, to enhancetranslation evaluation metrics, covering 1K languages, including 614 Africanlanguages. This work aims to advance the field of NLP, fostering cross-culturalunderstanding and knowledge exchange, particularly in regions with limitedlanguage resources such as Africa. The GitHub repository for the Toucan projectis available at https://github.com/UBC-NLP/Toucan.</description><author>AbdelRahim Elmadany, Ife Adebara, Muhammad Abdul-Mageed</author><pubDate>Fri, 12 Jul 2024 17:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04796v2</guid></item><item><title>MUSCLE: A Model Update Strategy for Compatible LLM Evolution</title><link>http://arxiv.org/abs/2407.09435v1</link><description>Large Language Models (LLMs) are frequently updated due to data orarchitecture changes to improve their performance. When updating models,developers often focus on increasing overall performance metrics with lessemphasis on being compatible with previous model versions. However, users oftenbuild a mental model of the functionality and capabilities of a particularmachine learning model they are interacting with. They have to adapt theirmental model with every update -- a draining task that can lead to userdissatisfaction. In practice, fine-tuned downstream task adapters rely onpretrained LLM base models. When these base models are updated, theseuser-facing downstream task models experience instance regression or negativeflips -- previously correct instances are now predicted incorrectly. Thishappens even when the downstream task training procedures remain identical. Ourwork aims to provide seamless model updates to a user in two ways. First, weprovide evaluation metrics for a notion of compatibility to prior modelversions, specifically for generative tasks but also applicable fordiscriminative tasks. We observe regression and inconsistencies betweendifferent model versions on a diverse set of tasks and model updates. Second,we propose a training strategy to minimize the number of inconsistencies inmodel updates, involving training of a compatibility model that can enhancetask fine-tuned language models. We reduce negative flips -- instances where aprior model version was correct, but a new model incorrect -- by up to 40% fromLlama 1 to Llama 2.</description><author>Jessica Echterhoff, Fartash Faghri, Raviteja Vemulapalli, Ting-Yao Hu, Chun-Liang Li, Oncel Tuzel, Hadi Pouransari</author><pubDate>Fri, 12 Jul 2024 17:12:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09435v1</guid></item><item><title>D2S: Representing sparse descriptors and 3D coordinates for camera relocalization</title><link>http://arxiv.org/abs/2307.15250v3</link><description>State-of-the-art visual localization methods mostly rely on complexprocedures to match local descriptors and 3D point clouds. However, theseprocedures can incur significant costs in terms of inference, storage, andupdates over time. In this study, we propose a direct learning-based approachthat utilizes a simple network named D2S to represent complex local descriptorsand their scene coordinates. Our method is characterized by its simplicity andcost-effectiveness. It solely leverages a single RGB image for localizationduring the testing phase and only requires a lightweight model to encode acomplex sparse scene. The proposed D2S employs a combination of a simple lossfunction and graph attention to selectively focus on robust descriptors whiledisregarding areas such as clouds, trees, and several dynamic objects. Thisselective attention enables D2S to effectively perform a binary-semanticclassification for sparse descriptors. Additionally, we propose a simpleoutdoor dataset to evaluate the capabilities of visual localization methods inscene-specific generalization and self-updating from unlabeled observations.Our approach outperforms the state-of-the-art CNN-based methods in scenecoordinate regression in indoor and outdoor environments. It demonstrates theability to generalize beyond training data, including scenarios involvingtransitions from day to night and adapting to domain shifts, even in theabsence of the labeled data sources. The source code, trained models, dataset,and demo videos are available at the following link:https://thpjp.github.io/d2s.</description><author>Bach-Thuan Bui, Huy-Hoang Bui, Dinh-Tuan Tran, Joo-Ho Lee</author><pubDate>Fri, 12 Jul 2024 17:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15250v3</guid></item><item><title>NeuFair: Neural Network Fairness Repair with Dropout</title><link>http://arxiv.org/abs/2407.04268v2</link><description>This paper investigates neuron dropout as a post-processing bias mitigationfor deep neural networks (DNNs). Neural-driven software solutions areincreasingly applied in socially critical domains with significant fairnessimplications. While neural networks are exceptionally good at findingstatistical patterns from data, they may encode and amplify existing biasesfrom the historical data. Existing bias mitigation algorithms often requiremodifying the input dataset or the learning algorithms. We posit that theprevalent dropout methods that prevent over-fitting during training by randomlydropping neurons may be an effective and less intrusive approach to improve thefairness of pre-trained DNNs. However, finding the ideal set of neurons to dropis a combinatorial problem. We propose NeuFair, a family of post-processingrandomized algorithms that mitigate unfairness in pre-trained DNNs via dropoutsduring inference after training. Our randomized search is guided by anobjective to minimize discrimination while maintaining the model's utility. Weshow that our design of randomized algorithms is effective and efficient inimproving fairness (up to 69%) with minimal or no model performancedegradation. We provide intuitive explanations of these phenomena and carefullyexamine the influence of various hyperparameters of search algorithms on theresults. Finally, we empirically and conceptually compare NeuFair to differentstate-of-the-art bias mitigators.</description><author>Vishnu Asutosh Dasu, Ashish Kumar, Saeid Tizpaz-Niari, Gang Tan</author><pubDate>Fri, 12 Jul 2024 17:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04268v2</guid></item><item><title>A Perspective on Foundation Models for the Electric Power Grid</title><link>http://arxiv.org/abs/2407.09434v1</link><description>Foundation models (FMs) currently dominate news headlines. They employadvanced deep learning architectures to extract structural informationautonomously from vast datasets through self-supervision. The resulting richrepresentations of complex systems and dynamics can be applied to manydownstream applications. Therefore, FMs can find uses in electric power grids,challenged by the energy transition and climate change. In this paper, we callfor the development of, and state why we believe in, the potential of FMs forelectric grids. We highlight their strengths and weaknesses amidst thechallenges of a changing grid. We argue that an FM learning from diverse griddata and topologies could unlock transformative capabilities, pioneering a newapproach in leveraging AI to redefine how we manage complexity and uncertaintyin the electric grid. Finally, we discuss a power grid FM concept, namelyGridFM, based on graph neural networks and show how different downstream tasksbenefit.</description><author>Hendrik F. Hamann, Thomas Brunschwiler, Blazhe Gjorgiev, Leonardo S. A. Martins, Alban Puech, Anna Varbella, Jonas Weiss, Juan Bernabe-Moreno, Alexandre Blondin Massé, Seong Choi, Ian Foster, Bri-Mathias Hodge, Rishabh Jain, Kibaek Kim, Vincent Mai, François Mirallès, Martin De Montigny, Octavio Ramos-Leaños, Hussein Suprême, Le Xie, El-Nasser S. Youssef, Arnaud Zinflou, Alexander J. Belvi, Ricardo J. Bessa, Bishnu Prasad Bhattari, Johannes Schmude, Stanislav Sobolevsky</author><pubDate>Fri, 12 Jul 2024 17:09:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09434v1</guid></item><item><title>Rethinking temporal self-similarity for repetitive action counting</title><link>http://arxiv.org/abs/2407.09431v1</link><description>Counting repetitive actions in long untrimmed videos is a challenging taskthat has many applications such as rehabilitation. State-of-the-art methodspredict action counts by first generating a temporal self-similarity matrix(TSM) from the sampled frames and then feeding the matrix to a predictornetwork. The self-similarity matrix, however, is not an optimal input to anetwork since it discards too much information from the frame-wise embeddings.We thus rethink how a TSM can be utilized for counting repetitive actions andpropose a framework that learns embeddings and predicts action startprobabilities at full temporal resolution. The number of repeated actions isthen inferred from the action start probabilities. In contrast to currentapproaches that have the TSM as an intermediate representation, we propose anovel loss based on a generated reference TSM, which enforces that theself-similarity of the learned frame-wise embeddings is consistent with theself-similarity of repeated actions. The proposed framework achievesstate-of-the-art results on three datasets, i.e., RepCount, UCFRep, andCountix.</description><author>Yanan Luo, Jinhui Yi, Yazan Abu Farha, Moritz Wolter, Juergen Gall</author><pubDate>Fri, 12 Jul 2024 17:03:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09431v1</guid></item><item><title>Open (Clinical) LLMs are Sensitive to Instruction Phrasings</title><link>http://arxiv.org/abs/2407.09429v1</link><description>Instruction-tuned Large Language Models (LLMs) can perform a wide range oftasks given natural language instructions to do so, but they are sensitive tohow such instructions are phrased. This issue is especially concerning inhealthcare, as clinicians are unlikely to be experienced prompt engineers andthe potential consequences of inaccurate outputs are heightened in this domain. This raises a practical question: How robust are instruction-tuned LLMs tonatural variations in the instructions provided for clinical NLP tasks? Wecollect prompts from medical doctors across a range of tasks and quantify thesensitivity of seven LLMs -- some general, others specialized -- to natural(i.e., non-adversarial) instruction phrasings. We find that performance variessubstantially across all models, and that -- perhaps surprisingly --domain-specific models explicitly trained on clinical data are especiallybrittle, compared to their general domain counterparts. Further, arbitraryphrasing differences can affect fairness, e.g., valid but distinct instructionsfor mortality prediction yield a range both in overall performance, and interms of differences between demographic groups.</description><author>Alberto Mario Ceballos Arroyo, Monica Munnangi, Jiuding Sun, Karen Y. C. Zhang, Denis Jered McInerney, Byron C. Wallace, Silvio Amir</author><pubDate>Fri, 12 Jul 2024 17:00:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09429v1</guid></item><item><title>Metric Learning from Limited Pairwise Preference Comparisons</title><link>http://arxiv.org/abs/2403.19629v2</link><description>We study metric learning from preference comparisons under the ideal pointmodel, in which a user prefers an item over another if it is closer to theirlatent ideal item. These items are embedded into $\mathbb{R}^d$ equipped withan unknown Mahalanobis distance shared across users. While recent work showsthat it is possible to simultaneously recover the metric and ideal items given$\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have alimited budget of $o(d)$ comparisons. We study whether the metric can still berecovered, even though it is known that learning individual ideal items is nowno longer possible. We show that in general, $o(d)$ comparisons reveal noinformation about the metric, even with infinitely many users. However, whencomparisons are made over items that exhibit low-dimensional structure, eachuser can contribute to learning the metric restricted to a low-dimensionalsubspace so that the metric can be jointly identified. We present adivide-and-conquer approach that achieves this, and provide theoreticalrecovery guarantees and empirical validation.</description><author>Zhi Wang, Geelon So, Ramya Korlakai Vinayak</author><pubDate>Fri, 12 Jul 2024 16:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19629v2</guid></item><item><title>Flow-Based Generative Emulation of Grids of Stellar Evolutionary Models</title><link>http://arxiv.org/abs/2407.09427v1</link><description>We present a flow-based generative approach to emulate grids of stellarevolutionary models. By interpreting the input parameters and output propertiesof these models as multi-dimensional probability distributions, we trainconditional normalizing flows to learn and predict the complex relationshipsbetween grid inputs and outputs in the form of conditional joint distributions.Leveraging the expressive power and versatility of these flows, we showcasetheir ability to emulate a variety of evolutionary tracks and isochrones acrossa continuous range of input parameters. In addition, we describe a simpleBayesian approach for estimating stellar parameters using these flows anddemonstrate its application to asteroseismic datasets of red giants observed bythe Kepler mission. By applying this approach to red giants in open clustersNGC 6791 and NGC 6819, we illustrate how large age uncertainties can arise whenfitting only to global asteroseismic and spectroscopic parameters without priorinformation on initial helium abundances and mixing length parameter values. Wealso conduct inference using the flow at a large scale by determining revisedestimates of masses and radii for 15,388 field red giants. These estimates showimproved agreement with results from existing grid-based modelling, revealdistinct population-level features in the red clump, and suggest that themasses of Kepler red giants previously determined using the correctedasteroseismic scaling relations have been overestimated by 5-10%.</description><author>Marc Hon, Yaguang Li, Joel Ong</author><pubDate>Fri, 12 Jul 2024 16:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09427v1</guid></item><item><title>On the estimation of the number of components in multivariate functional principal component analysis</title><link>http://arxiv.org/abs/2311.04540v2</link><description>Happ and Greven (2018) developed a methodology for principal componentsanalysis of multivariate functional data for data observed on differentdimensional domains. Their approach relies on an estimation of univariatefunctional principal components for each univariate functional feature. In thispaper, we present extensive simulations to investigate choosing the number ofprincipal components to retain. We show empirically that the conventionalapproach of using a percentage of variance explained threshold for eachunivariate functional feature may be unreliable when aiming to explain anoverall percentage of variance in the multivariate functional data, and thus weadvise practitioners to be careful when using it.</description><author>Steven Golovkine, Edward Gunning, Andrew J. Simpkin, Norma Bargary</author><pubDate>Fri, 12 Jul 2024 16:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04540v2</guid></item><item><title>Identifying macro conditional independencies and macro total effects in summary causal graphs with latent confounding</title><link>http://arxiv.org/abs/2407.07934v2</link><description>Understanding causal relationships in dynamic systems is essential fornumerous scientific fields, including epidemiology, economics, and biology.While causal inference methods have been extensively studied, they often relyon fully specified causal graphs, which may not always be available orpractical in complex dynamic systems. Partially specified causal graphs, suchas summary causal graphs (SCGs), provide a simplified representation of causalrelationships, omitting temporal information and focusing on high-level causalstructures. This simplification introduces new challenges concerning the typesof queries of interest: macro queries, which involve relationships betweenclusters represented as vertices in the graph, and micro queries, which pertainto relationships between variables that are not directly visible through thevertices of the graph. In this paper, we first clearly distinguish betweenmacro conditional independencies and micro conditional independencies andbetween macro total effects and micro total effects. Then, we demonstrate thesoundness and completeness of the d-separation to identify macro conditionalindependencies in SCGs. Furthermore, we establish that the do-calculus is soundand complete for identifying macro total effects in SCGs. Conversely, we alsoshow through various examples that these results do not hold when consideringmicro conditional independencies and micro total effects.</description><author>Simon Ferreira, Charles K. Assaad</author><pubDate>Fri, 12 Jul 2024 16:51:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07934v2</guid></item><item><title>Improving Alignment and Robustness with Circuit Breakers</title><link>http://arxiv.org/abs/2406.04313v4</link><description>AI systems can take harmful actions and are highly vulnerable to adversarialattacks. We present an approach, inspired by recent advances in representationengineering, that interrupts the models as they respond with harmful outputswith "circuit breakers." Existing techniques aimed at improving alignment, suchas refusal training, are often bypassed. Techniques such as adversarialtraining try to plug these holes by countering specific attacks. As analternative to refusal training and adversarial training, circuit-breakingdirectly controls the representations that are responsible for harmful outputsin the first place. Our technique can be applied to both text-only andmultimodal language models to prevent the generation of harmful outputs withoutsacrificing utility -- even in the presence of powerful unseen attacks.Notably, while adversarial robustness in standalone image recognition remainsan open challenge, circuit breakers allow the larger multimodal system toreliably withstand image "hijacks" that aim to produce harmful content.Finally, we extend our approach to AI agents, demonstrating considerablereductions in the rate of harmful actions when they are under attack. Ourapproach represents a significant step forward in the development of reliablesafeguards to harmful behavior and adversarial attacks.</description><author>Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, Dan Hendrycks</author><pubDate>Fri, 12 Jul 2024 16:51:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04313v4</guid></item><item><title>TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models</title><link>http://arxiv.org/abs/2407.09424v1</link><description>Large Language Models (LLMs) have the potential to revolutionize the SixthGeneration (6G) communication networks. However, current mainstream LLMsgenerally lack the specialized knowledge in telecom domain. In this paper, forthe first time, we propose a pipeline to adapt any general purpose LLMs to atelecom-specific LLMs. We collect and build telecom-specific pre-train dataset,instruction dataset, preference dataset to perform continual pre-training,instruct tuning and alignment tuning respectively. Besides, due to the lack ofwidely accepted evaluation benchmarks in telecom domain, we extend existingevaluation benchmarks and proposed three new benchmarks, namely, Telecom MathModeling, Telecom Open QnA and Telecom Code Tasks. These new benchmarks providea holistic evaluation of the capabilities of LLMs including math modeling,Open-Ended question answering, code generation, infilling, summarization andanalysis in telecom domain. Our fine-tuned LLM TelecomGPT outperforms state ofthe art (SOTA) LLMs including GPT-4, Llama-3 and Mistral in Telecom MathModeling benchmark significantly and achieve comparable performance in variousevaluation benchmarks such as TeleQnA, 3GPP technical documents classification,telecom code summary and generation and infilling.</description><author>Hang Zou, Qiyang Zhao, Yu Tian, Lina Bariah, Faouzi Bader, Thierry Lestable, Merouane Debbah</author><pubDate>Fri, 12 Jul 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09424v1</guid></item><item><title>A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms</title><link>http://arxiv.org/abs/2306.15552v2</link><description>Recent trends in deep learning (DL) imposed hardware accelerators as the mostviable solution for several classes of high-performance computing (HPC)applications such as image classification, computer vision, and speechrecognition. This survey summarizes and classifies the most recent advances indesigning DL accelerators suitable to reach the performance requirements of HPCapplications. In particular, it highlights the most advanced approaches tosupport deep learning accelerations including not only GPU and TPU-basedaccelerators but also design-specific hardware accelerators such as FPGA-basedand ASIC-based accelerators, Neural Processing Units, open hardwareRISC-V-based accelerators and co-processors. The survey also describesaccelerators based on emerging memory technologies and computing paradigms,such as 3D-stacked Processor-In-Memory, non-volatile memories (mainly,Resistive RAM and Phase Change Memories) to implement in-memory computing,Neuromorphic Processing Units, and accelerators based on Multi-Chip Modules.Among emerging technologies, we also include some insights into quantum-basedaccelerators and photonics. To conclude, the survey classifies the mostinfluential architectures and technologies proposed in the last years, with thepurpose of offering the reader a comprehensive perspective in the rapidlyevolving field of deep learning.</description><author>Cristina Silvano, Daniele Ielmini, Fabrizio Ferrandi, Leandro Fiorin, Serena Curzel, Luca Benini, Francesco Conti, Angelo Garofalo, Cristian Zambelli, Enrico Calore, Sebastiano Fabio Schifano, Maurizio Palesi, Giuseppe Ascia, Davide Patti, Nicola Petra, Davide De Caro, Luciano Lavagno, Teodoro Urso, Valeria Cardellini, Gian Carlo Cardarilli, Robert Birke, Stefania Perri</author><pubDate>Fri, 12 Jul 2024 16:50:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15552v2</guid></item><item><title>Mitigating Entity-Level Hallucination in Large Language Models</title><link>http://arxiv.org/abs/2407.09417v1</link><description>The emergence of Large Language Models (LLMs) has revolutionized how usersaccess information, shifting from traditional search engines to directquestion-and-answer interactions with LLMs. However, the widespread adoption ofLLMs has revealed a significant challenge known as hallucination, wherein LLMsgenerate coherent yet factually inaccurate responses. This hallucinationphenomenon has led to users' distrust in information retrieval systems based onLLMs. To tackle this challenge, this paper proposes Dynamic RetrievalAugmentation based on hallucination Detection (DRAD) as a novel method todetect and mitigate hallucinations in LLMs. DRAD improves upon traditionalretrieval augmentation by dynamically adapting the retrieval process based onreal-time hallucination detection. It features two main components: Real-timeHallucination Detection (RHD) for identifying potential hallucinations withoutexternal models, and Self-correction based on External Knowledge (SEK) forcorrecting these errors using external knowledge. Experiment results show thatDRAD demonstrates superior performance in both detecting and mitigatinghallucinations in LLMs. All of our code and data are open-sourced athttps://github.com/oneal2000/EntityHallucination.</description><author>Weihang Su, Yichen Tang, Qingyao Ai, Changyue Wang, Zhijing Wu, Yiqun Liu</author><pubDate>Fri, 12 Jul 2024 16:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09417v1</guid></item><item><title>OmniSat: Self-Supervised Modality Fusion for Earth Observation</title><link>http://arxiv.org/abs/2404.08351v2</link><description>The field of Earth Observations (EO) offers a wealth of data from diversesensors, presenting a great opportunity for advancing self-supervisedmultimodal learning. However, current multimodal EO datasets and models focuson a single data type, either mono-date images or time series, which limitstheir expressivity. We introduce OmniSat, a novel architecture that exploitsthe spatial alignment between multiple EO modalities to learn expressivemultimodal representations without labels. To demonstrate the advantages ofcombining modalities of different natures, we augment two existing datasetswith new modalities. As demonstrated on three downstream tasks: forestry, landcover classification, and crop mapping. OmniSat can learn rich representationsin an unsupervised manner, leading to improved performance in the semi- andfully-supervised settings, even when only one modality is available forinference. The code and dataset are available athttps://github.com/gastruc/OmniSat.</description><author>Guillaume Astruc, Nicolas Gonthier, Clement Mallet, Loic Landrieu</author><pubDate>Fri, 12 Jul 2024 16:45:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08351v2</guid></item><item><title>A Benchmark Environment for Offline Reinforcement Learning in Racing Games</title><link>http://arxiv.org/abs/2407.09415v1</link><description>Offline Reinforcement Learning (ORL) is a promising approach to reduce thehigh sample complexity of traditional Reinforcement Learning (RL) byeliminating the need for continuous environmental interactions. ORL exploits adataset of pre-collected transitions and thus expands the range of applicationof RL to tasks in which the excessive environment queries increase trainingtime and decrease efficiency, such as in modern AAA games. This paperintroduces OfflineMania a novel environment for ORL research. It is inspired bythe iconic TrackMania series and developed using the Unity 3D game engine. Theenvironment simulates a single-agent racing game in which the objective is tocomplete the track through optimal navigation. We provide a variety of datasetsto assess ORL performance. These datasets, created from policies of varyingability and in different sizes, aim to offer a challenging testbed foralgorithm development and evaluation. We further establish a set of baselinesfor a range of Online RL, ORL, and hybrid Offline to Online RL approaches usingour environment.</description><author>Girolamo Macaluso, Alessandro Sestini, Andrew D. Bagdanov</author><pubDate>Fri, 12 Jul 2024 16:44:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09415v1</guid></item><item><title>Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering</title><link>http://arxiv.org/abs/2403.09622v2</link><description>Visual text rendering poses a fundamental challenge for contemporarytext-to-image generation models, with the core problem lying in text encoderdeficiencies. To achieve accurate text rendering, we identify two crucialrequirements for text encoders: character awareness and alignment with glyphs.Our solution involves crafting a series of customized text encoder, Glyph-ByT5,by fine-tuning the character-aware ByT5 encoder using a meticulously curatedpaired glyph-text dataset. We present an effective method for integratingGlyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model fordesign image generation. This significantly enhances text rendering accuracy,improving it from less than $20\%$ to nearly $90\%$ on our design imagebenchmark. Noteworthy is Glyph-SDXL's newfound ability for text paragraphrendering, achieving high spelling accuracy for tens to hundreds of characterswith automated multi-line layouts. Finally, through fine-tuning Glyph-SDXL witha small set of high-quality, photorealistic images featuring visual text, weshowcase a substantial improvement in scene text rendering capabilities inopen-domain real images. These compelling outcomes aim to encourage furtherexploration in designing customized text encoders for diverse and challengingtasks.</description><author>Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, Yuhui Yuan</author><pubDate>Fri, 12 Jul 2024 16:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09622v2</guid></item><item><title>On scalable oversight with weak LLMs judging strong LLMs</title><link>http://arxiv.org/abs/2407.04622v2</link><description>Scalable oversight protocols aim to enable humans to accurately supervisesuperhuman AI. In this paper we study debate, where two AI's compete toconvince a judge; consultancy, where a single AI tries to convince a judge thatasks questions; and compare to a baseline of direct question-answering, wherethe judge just answers outright without the AI. We use large language models(LLMs) as both AI agents and as stand-ins for human judges, taking the judgemodels to be weaker than agent models. We benchmark on a diverse range ofasymmetries between judges and agents, extending previous work on a singleextractive QA task with information asymmetry, to also include mathematics,coding, logic and multimodal reasoning asymmetries. We find that debateoutperforms consultancy across all tasks when the consultant is randomlyassigned to argue for the correct/incorrect answer. Comparing debate to directquestion answering, the results depend on the type of task: in extractive QAtasks with information asymmetry debate outperforms direct question answering,but in other tasks without information asymmetry the results are mixed.Previous work assigned debaters/consultants an answer to argue for. When weallow them to instead choose which answer to argue for, we find judges are lessfrequently convinced by the wrong answer in debate than in consultancy.Further, we find that stronger debater models increase judge accuracy, thoughmore modestly than in previous studies.</description><author>Zachary Kenton, Noah Y. Siegel, János Kramár, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah D. Goodman, Rohin Shah</author><pubDate>Fri, 12 Jul 2024 16:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04622v2</guid></item><item><title>SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers</title><link>http://arxiv.org/abs/2407.09413v1</link><description>Seeking answers to questions within long scientific research articles is acrucial area of study that aids readers in quickly addressing their inquiries.However, existing question-answering (QA) datasets based on scientific papersare limited in scale and focus solely on textual content. To address thislimitation, we introduce SPIQA (Scientific Paper Image Question Answering), thefirst large-scale QA dataset specifically designed to interpret complex figuresand tables within the context of scientific research articles across variousdomains of computer science. Leveraging the breadth of expertise and ability ofmultimodal large language models (MLLMs) to understand figures, we employautomatic and manual curation to create the dataset. We craft aninformation-seeking task involving multiple images that cover a wide variety ofplots, charts, tables, schematic diagrams, and result visualizations. SPIQAcomprises 270K questions divided into training, validation, and three differentevaluation splits. Through extensive experiments with 12 prominent foundationalmodels, we evaluate the ability of current multimodal systems to comprehend thenuanced aspects of research articles. Additionally, we propose aChain-of-Thought (CoT) evaluation strategy with in-context retrieval thatallows fine-grained, step-by-step assessment and improves model performance. Wefurther explore the upper bounds of performance enhancement with additionaltextual information, highlighting its promising potential for future researchand the dataset's impact on revolutionizing how we interact with scientificliterature.</description><author>Shraman Pramanick, Rama Chellappa, Subhashini Venugopalan</author><pubDate>Fri, 12 Jul 2024 16:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09413v1</guid></item><item><title>Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning</title><link>http://arxiv.org/abs/2311.09821v2</link><description>Knowledge in the real world is being updated constantly. However, it iscostly to frequently update large language models (LLMs). Therefore, it iscrucial for LLMs to understand the concept of temporal knowledge. However,prior works on temporal question answering (TQA) did not emphasize multi-answerand multi-hop types of temporal reasoning. In this paper, we propose a complextemporal question-answering dataset Complex-TR that focuses on multi-answer andmulti-hop temporal reasoning. Besides, we also propose a novel dataaugmentation strategy to improve the complex temporal reasoning capability androbustness of LLMs. We conducted experiments on multiple temporal QA datasets.Experimental results show that our method is able to improve LLMs' performanceon temporal QA benchmarks by significant margins. Our code and data arereleased at: https://github.com/nusnlp/complex-tr.</description><author>Qingyu Tan, Hwee Tou Ng, Lidong Bing</author><pubDate>Fri, 12 Jul 2024 16:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09821v2</guid></item><item><title>LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion</title><link>http://arxiv.org/abs/2404.00292v4</link><description>Camouflaged vision perception is an important vision task with numerouspractical applications. Due to the expensive collection and labeling costs,this community struggles with a major bottleneck that the species category ofits datasets is limited to a small number of object species. However, theexisting camouflaged generation methods require specifying the backgroundmanually, thus failing to extend the camouflaged sample diversity in a low-costmanner. In this paper, we propose a Latent Background KnowledgeRetrieval-Augmented Diffusion (LAKE-RED) for camouflaged image generation. Toour knowledge, our contributions mainly include: (1) For the first time, wepropose a camouflaged generation paradigm that does not need to receive anybackground inputs. (2) Our LAKE-RED is the first knowledge retrieval-augmentedmethod with interpretability for camouflaged generation, in which we propose anidea that knowledge retrieval and reasoning enhancement are separatedexplicitly, to alleviate the task-specific challenges. Moreover, our method isnot restricted to specific foreground targets or backgrounds, offering apotential for extending camouflaged vision perception to more diverse domains.(3) Experimental results demonstrate that our method outperforms the existingapproaches, generating more realistic camouflage images.</description><author>Pancheng Zhao, Peng Xu, Pengda Qin, Deng-Ping Fan, Zhicheng Zhang, Guoli Jia, Bowen Zhou, Jufeng Yang</author><pubDate>Fri, 12 Jul 2024 16:28:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00292v4</guid></item><item><title>StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images</title><link>http://arxiv.org/abs/2403.09302v2</link><description>Stain normalization algorithms aim to transform the color and intensitycharacteristics of a source multi-gigapixel histology image to match those of atarget image, mitigating inconsistencies in the appearance of stains used tohighlight cellular components in the images. We propose a new approach,StainFuser, which treats this problem as a style transfer task using a novelConditional Latent Diffusion architecture, eliminating the need for handcraftedcolor components. With this method, we curate SPI-2M the largest stainnormalization dataset to date of over 2 million histology images with neuralstyle transfer for high-quality transformations. Trained on this data,StainFuser outperforms current state-of-the-art deep learning and handcraftedmethods in terms of the quality of normalized images and in terms of downstreammodel performance on the CoNIC dataset.</description><author>Robert Jewsbury, Ruoyu Wang, Abhir Bhalerao, Nasir Rajpoot, Quoc Dang Vu</author><pubDate>Fri, 12 Jul 2024 16:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09302v2</guid></item><item><title>Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering</title><link>http://arxiv.org/abs/2406.10208v2</link><description>Recently, Glyph-ByT5 has achieved highly accurate visual text renderingperformance in graphic design images. However, it still focuses solely onEnglish and performs relatively poorly in terms of visual appeal. In this work,we address these two fundamental limitations by presenting Glyph-ByT5-v2 andGlyph-SDXL-v2, which not only support accurate visual text rendering for 10different languages but also achieve much better aesthetic quality. To achievethis, we make the following contributions: (i) creating a high-qualitymultilingual glyph-text and graphic design dataset consisting of more than 1million glyph-text pairs and 10 million graphic design image-text pairscovering nine other languages, (ii) building a multilingual visual paragraphbenchmark consisting of 1,000 prompts, with 100 for each language, to assessmultilingual visual spelling accuracy, and (iii) leveraging the lateststep-aware preference learning approach to enhance the visual aestheticquality. With the combination of these techniques, we deliver a powerfulcustomized multilingual text encoder, Glyph-ByT5-v2, and a strong aestheticgraphic generation model, Glyph-SDXL-v2, that can support accurate spelling in10 different languages. We perceive our work as a significant advancement,considering that the latest DALL-E3 and Ideogram 1.0 still struggle with themultilingual visual text rendering task.</description><author>Zeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Lin Liang, Lijuan Wang, Ji Li, Yuhui Yuan</author><pubDate>Fri, 12 Jul 2024 16:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10208v2</guid></item><item><title>Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce</title><link>http://arxiv.org/abs/2407.09395v1</link><description>Text relevance or text matching of query and product is an essentialtechnique for the e-commerce search system to ensure that the displayedproducts can match the intent of the query. Many studies focus on improving theperformance of the relevance model in search system. Recently, pre-trainedlanguage models like BERT have achieved promising performance on the textrelevance task. While these models perform well on the offline test dataset,there are still obstacles to deploy the pre-trained language model to theonline system as their high latency. The two-tower model is extensivelyemployed in industrial scenarios, owing to its ability to harmonize performancewith computational efficiency. Regrettably, such models present an opaque``black box'' nature, which prevents developers from making specialoptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, anefficient and interpretable relevance architecture for Chinese e-commerce. Ourapproach proposes to encode the query and the product into the sparse BoWrepresentation, which is a set of word-weight pairs. The weight means theimportant or the relevant score between the corresponding word and the rawtext. The relevance score is measured by the accumulation of the matched wordbetween the sparse BoW representation of the query and the product. Compared topopular dense distributed representation that usually suffers from the drawbackof black-box, the most advantage of the proposed representation model is highlyexplainable and interventionable, which is a superior advantage to thedeployment and operation of online search engines. Moreover, the onlineefficiency of the proposed model is even better than the most efficient innerproduct form of dense representation ...</description><author>Zhe Lin, Jiwei Tan, Dan Ou, Xi Chen, Shaowei Yao, Bo Zheng</author><pubDate>Fri, 12 Jul 2024 16:18:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09395v1</guid></item><item><title>Open-Canopy: A Country-Scale Benchmark for Canopy Height Estimation at Very High Resolution</title><link>http://arxiv.org/abs/2407.09392v1</link><description>Estimating canopy height and canopy height change at meter resolution fromsatellite imagery has numerous applications, such as monitoring forest health,logging activities, wood resources, and carbon stocks. However, many existingforest datasets are based on commercial or closed data sources, restricting thereproducibility and evaluation of new approaches. To address this gap, weintroduce Open-Canopy, the first open-access and country-scale benchmark forvery high resolution (1.5 m) canopy height estimation. Covering more than87,000 km$^2$ across France, Open-Canopy combines SPOT satellite imagery withhigh resolution aerial LiDAR data. We also propose Open-Canopy-$\Delta$, thefirst benchmark for canopy height change detection between two images taken atdifferent years, a particularly challenging task even for recent models. Toestablish a robust foundation for these benchmarks, we evaluate a comprehensivelist of state-of-the-art computer vision models for canopy height estimation.The dataset and associated codes can be accessed athttps://github.com/fajwel/Open-Canopy.</description><author>Fajwel Fogel, Yohann Perron, Nikola Besic, Laurent Saint-André, Agnès Pellissier-Tanon, Martin Schwartz, Thomas Boudras, Ibrahim Fayad, Alexandre d'Aspremont, Loic Landrieu, Phillipe Ciais</author><pubDate>Fri, 12 Jul 2024 16:16:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09392v1</guid></item><item><title>Bayesian Learning-driven Prototypical Contrastive Loss for Class-Incremental Learning</title><link>http://arxiv.org/abs/2405.11067v2</link><description>The primary objective of methods in continual learning is to learn tasks in asequential manner over time from a stream of data, while mitigating thedetrimental phenomenon of catastrophic forgetting. In this paper, we focus onlearning an optimal representation between previous class prototypes and newlyencountered ones. We propose a prototypical network with a Bayesianlearning-driven contrastive loss (BLCL) tailored specifically forclass-incremental learning scenarios. Therefore, we introduce a contrastiveloss that incorporates new classes into the latent representation by reducingthe intra-class distance and increasing the inter-class distance. Our approachdynamically adapts the balance between the cross-entropy and contrastive lossfunctions with a Bayesian learning technique. Empirical evaluations conductedon both the CIFAR-10 and CIFAR-100 dataset for image classification and imagesof a GNSS-based dataset for interference classification validate the efficacyof our method, showcasing its superiority over existing state-of-the-artapproaches.</description><author>Nisha L. Raichur, Lucas Heublein, Tobias Feigl, Alexander Rügamer, Christopher Mutschler, Felix Ott</author><pubDate>Fri, 12 Jul 2024 16:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11067v2</guid></item><item><title>GAVEL: Generating Games Via Evolution and Language Models</title><link>http://arxiv.org/abs/2407.09388v1</link><description>Automatically generating novel and interesting games is a complex task.Challenges include representing game rules in a computationally workable form,searching through the large space of potential games under most suchrepresentations, and accurately evaluating the originality and quality ofpreviously unseen games. Prior work in automated game generation has largelyfocused on relatively restricted rule representations and relied ondomain-specific heuristics. In this work, we explore the generation of novelgames in the comparatively expansive Ludii game description language, whichencodes the rules of over 1000 board games in a variety of styles and modes ofplay. We draw inspiration from recent advances in large language models andevolutionary computation in order to train a model that intelligently mutatesand recombines games and mechanics expressed as code. We demonstrate bothquantitatively and qualitatively that our approach is capable of generating newand interesting games, including in regions of the potential rules space notcovered by existing games in the Ludii dataset. A sample of the generated gamesare available to play online through the Ludii portal.</description><author>Graham Todd, Alexander Padula, Matthew Stephenson, Éric Piette, Dennis J. N. J. Soemers, Julian Togelius</author><pubDate>Fri, 12 Jul 2024 16:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09388v1</guid></item><item><title>Meta-Analysis with Untrusted Data</title><link>http://arxiv.org/abs/2407.09387v1</link><description>[See paper for full abstract] Meta-analysis is a crucial tool for answeringscientific questions. It is usually conducted on a relatively small amount of``trusted'' data -- ideally from randomized, controlled trials -- which allowcausal effects to be reliably estimated with minimal assumptions. We show howto answer causal questions much more precisely by making two changes. First, weincorporate untrusted data drawn from large observational databases, relatedscientific literature and practical experience -- without sacrificing rigor orintroducing strong assumptions. Second, we train richer models capable ofhandling heterogeneous trials, addressing a long-standing challenge inmeta-analysis. Our approach is based on conformal prediction, whichfundamentally produces rigorous prediction intervals, but doesn't handleindirect observations: in meta-analysis, we observe only noisy effects due tothe limited number of participants in each trial. To handle noise, we develop asimple, efficient version of fully-conformal kernel ridge regression, based ona novel condition called idiocentricity. We introduce noise-correcting terms inthe residuals and analyze their interaction with a ``variance shaving''technique. In multiple experiments on healthcare datasets, our algorithmsdeliver tighter, sounder intervals than traditional ones. This paper charts anew course for meta-analysis and evidence-based medicine, where heterogeneityand untrusted data are embraced for more nuanced and precise predictions.</description><author>Shiva Kaul, Geoffrey J. Gordon</author><pubDate>Fri, 12 Jul 2024 16:07:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09387v1</guid></item><item><title>Radiance Fields from Photons</title><link>http://arxiv.org/abs/2407.09386v1</link><description>Neural radiance fields, or NeRFs, have become the de facto approach forhigh-quality view synthesis from a collection of images captured from multipleviewpoints. However, many issues remain when capturing images in-the-wild underchallenging conditions, such as low light, high dynamic range, or rapid motionleading to smeared reconstructions with noticeable artifacts. In this work, weintroduce quanta radiance fields, a novel class of neural radiance fields thatare trained at the granularity of individual photons using single-photoncameras (SPCs). We develop theory and practical computational techniques forbuilding radiance fields and estimating dense camera poses from unconventional,stochastic, and high-speed binary frame sequences captured by SPCs. Wedemonstrate, both via simulations and a SPC hardware prototype, high-fidelityreconstructions under high-speed motion, in low light, and for extreme dynamicrange settings.</description><author>Sacha Jungerman, Mohit Gupta</author><pubDate>Fri, 12 Jul 2024 16:06:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09386v1</guid></item><item><title>Long-term drought prediction using deep neural networks based on geospatial weather data</title><link>http://arxiv.org/abs/2309.06212v6</link><description>The problem of high-quality drought forecasting up to a year in advance iscritical for agriculture planning and insurance. Yet, it is still unsolved withreasonable accuracy due to data complexity and aridity stochasticity. We tackledrought data by introducing an end-to-end approach that adopts aspatio-temporal neural network model with accessible open monthly climate dataas the input. Our systematic research employs diverse proposed models and five distinctenvironmental regions as a testbed to evaluate the efficacy of the PalmerDrought Severity Index (PDSI) prediction. Key aggregated findings are theexceptional performance of a Transformer model, EarthFormer, in making accurateshort-term (up to six months) forecasts. At the same time, the ConvolutionalLSTM excels in longer-term forecasting.</description><author>Alexander Marusov, Vsevolod Grabar, Yury Maximov, Nazar Sotiriadi, Alexander Bulkin, Alexey Zaytsev</author><pubDate>Fri, 12 Jul 2024 16:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06212v6</guid></item><item><title>The Effectiveness of Curvature-Based Rewiring and the Role of Hyperparameters in GNNs Revisited</title><link>http://arxiv.org/abs/2407.09381v1</link><description>Message passing is the dominant paradigm in Graph Neural Networks (GNNs). Theefficiency of message passing, however, can be limited by the topology of thegraph. This happens when information is lost during propagation due to beingoversquashed when travelling through bottlenecks. To remedy this, recentefforts have focused on graph rewiring techniques, which disconnect the inputgraph originating from the data and the computational graph, on which messagepassing is performed. A prominent approach for this is to use discrete graphcurvature measures, of which several variants have been proposed, to identifyand rewire around bottlenecks, facilitating information propagation. Whileoversquashing has been demonstrated in synthetic datasets, in this work wereevaluate the performance gains that curvature-based rewiring brings toreal-world datasets. We show that in these datasets, edges selected during therewiring process are not in line with theoretical criteria identifyingbottlenecks. This implies they do not necessarily oversquash information duringmessage passing. Subsequently, we demonstrate that SOTA accuracies on thesedatasets are outliers originating from sweeps of hyperparameters -- both theones for training and dedicated ones related to the rewiring algorithm --instead of consistent performance gains. In conclusion, our analysis nuancesthe effectiveness of curvature-based rewiring in real-world datasets and bringsa new perspective on the methods to evaluate GNN accuracy improvements.</description><author>Floriano Tori, Vincent Holst, Vincent Ginis</author><pubDate>Fri, 12 Jul 2024 16:03:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09381v1</guid></item><item><title>When can weak latent factors be statistically inferred?</title><link>http://arxiv.org/abs/2407.03616v2</link><description>This article establishes a new and comprehensive estimation and inferencetheory for principal component analysis (PCA) under the weak factor model thatallow for cross-sectional dependent idiosyncratic components under nearlyminimal the factor strength relative to the noise level or signal-to-noiseratio. Our theory is applicable regardless of the relative growth rate betweenthe cross-sectional dimension $N$ and temporal dimension $T$. This morerealistic assumption and noticeable result requires completely new technicaldevice, as the commonly-used leave-one-out trick is no longer applicable to thecase with cross-sectional dependence. Another notable advancement of our theoryis on PCA inference $ - $ for example, under the regime where $N\asymp T$, weshow that the asymptotic normality for the PCA-based estimator holds as long asthe signal-to-noise ratio (SNR) grows faster than a polynomial rate of $\logN$. This finding significantly surpasses prior work that required a polynomialrate of $N$. Our theory is entirely non-asymptotic, offering finite-samplecharacterizations for both the estimation error and the uncertainty level ofstatistical inference. A notable technical innovation is our closed-formfirst-order approximation of PCA-based estimator, which paves the way forvarious statistical tests. Furthermore, we apply our theories to designeasy-to-implement statistics for validating whether given factors fall in thelinear spans of unknown latent factors, testing structural breaks in the factorloadings for an individual unit, checking whether two units have the same riskexposures, and constructing confidence intervals for systematic risks. Ourempirical studies uncover insightful correlations between our test results andeconomic cycles.</description><author>Jianqing Fan, Yuling Yan, Yuheng Zheng</author><pubDate>Fri, 12 Jul 2024 16:03:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03616v2</guid></item><item><title>Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks</title><link>http://arxiv.org/abs/2405.10632v5</link><description>Model evaluations are central to understanding the safety, risks, andsocietal impacts of AI systems. While most real-world AI applications involvehuman-AI interaction, most current evaluations (e.g., common benchmarks) of AImodels do not. Instead, they incorporate human factors in limited ways,assessing the safety of models in isolation, thereby falling short of capturingthe complexity of human-model interactions. In this paper, we discuss andoperationalize a definition of an emerging category of evaluations -- "humaninteraction evaluations" (HIEs) -- which focus on the assessment of human-modelinteractions or the process and the outcomes of humans using models. First, weargue that HIEs can be used to increase the validity of safety evaluations,assess direct human impact and interaction-specific harms, and guide futureassessments of models' societal impact. Second, we propose a safety-focused HIEdesign framework -- containing a human-LLM interaction taxonomy -- with threestages: (1) identifying the risk or harm area, (2) characterizing the usecontext, and (3) choosing the evaluation parameters. Third, we apply ourframework to two potential evaluations for overreliance and persuasion risks.Finally, we conclude with tangible recommendations for addressing concerns overcosts, replicability, and unrepresentativeness of HIEs.</description><author>Lujain Ibrahim, Saffron Huang, Lama Ahmad, Markus Anderljung</author><pubDate>Fri, 12 Jul 2024 15:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10632v5</guid></item><item><title>FANet: Feature Amplification Network for Semantic Segmentation in Cluttered Background</title><link>http://arxiv.org/abs/2407.09379v1</link><description>Existing deep learning approaches leave out the semantic cues that arecrucial in semantic segmentation present in complex scenarios includingcluttered backgrounds and translucent objects, etc. To handle these challenges,we propose a feature amplification network (FANet) as a backbone network thatincorporates semantic information using a novel feature enhancement module atmulti-stages. To achieve this, we propose an adaptive feature enhancement (AFE)block that benefits from both a spatial context module (SCM) and a featurerefinement module (FRM) in a parallel fashion. SCM aims to exploit largerkernel leverages for the increased receptive field to handle scale variationsin the scene. Whereas our novel FRM is responsible for generating semantic cuesthat can capture both low-frequency and high-frequency regions for bettersegmentation tasks. We perform experiments over challenging real-worldZeroWaste-f dataset which contains background-cluttered and translucentobjects. Our experimental results demonstrate the state-of-the-art performancecompared to existing methods.</description><author>Muhammad Ali, Mamoona Javaid, Mubashir Noman, Mustansar Fiaz, Salman Khan</author><pubDate>Fri, 12 Jul 2024 15:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09379v1</guid></item><item><title>Graph Neural Network Causal Explanation via Neural Causal Models</title><link>http://arxiv.org/abs/2407.09378v1</link><description>Graph neural network (GNN) explainers identify the important subgraph thatensures the prediction for a given graph. Until now, almost all GNN explainersare based on association, which is prone to spurious correlations. We propose{\name}, a GNN causal explainer via causal inference. Our explainer is based onthe observation that a graph often consists of a causal underlying subgraph.{\name} includes three main steps: 1) It builds causal structure and thecorresponding structural causal model (SCM) for a graph, which enables thecause-effect calculation among nodes. 2) Directly calculating the cause-effectin real-world graphs is computationally challenging. It is then enlightened bythe recent neural causal model (NCM), a special type of SCM that is trainable,and design customized NCMs for GNNs. By training these GNN NCMs, thecause-effect can be easily calculated. 3) It uncovers the subgraph thatcausally explains the GNN predictions via the optimized GNN-NCMs. Evaluationresults on multiple synthetic and real-world graphs validate that {\name}significantly outperforms existing GNN explainers in exact groundtruthexplanation identification</description><author>Arman Behnam, Binghui Wang</author><pubDate>Fri, 12 Jul 2024 15:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09378v1</guid></item><item><title>HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context</title><link>http://arxiv.org/abs/2407.09375v1</link><description>This work explores the in-context learning capabilities of State Space Models(SSMs) and presents, to the best of our knowledge, the first theoreticalexplanation of a possible underlying mechanism. We introduce a novel weightconstruction for SSMs, enabling them to predict the next state of any dynamicalsystem after observing previous states without parameter fine-tuning. This isaccomplished by extending the HiPPO framework to demonstrate that continuousSSMs can approximate the derivative of any input signal. Specifically, we findan explicit weight construction for continuous SSMs and provide an asymptoticerror bound on the derivative approximation. The discretization of thiscontinuous SSM subsequently yields a discrete SSM that predicts the next state.Finally, we demonstrate the effectiveness of our parameterization empirically.This work should be an initial step toward understanding how sequence modelsbased on SSMs learn in context.</description><author>Federico Arangath Joseph, Kilian Haefeli, Noah Liniger, Caglar Gulcehre</author><pubDate>Fri, 12 Jul 2024 15:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09375v1</guid></item><item><title>Towards Personalised Patient Risk Prediction Using Temporal Hospital Data Trajectories</title><link>http://arxiv.org/abs/2407.09373v1</link><description>Quantifying a patient's health status provides clinicians with insight intopatient risk, and the ability to better triage and manage resources. EarlyWarning Scores (EWS) are widely deployed to measure overall health status, andrisk of adverse outcomes, in hospital patients. However, current EWS arelimited both by their lack of personalisation and use of static observations.We propose a pipeline that groups intensive care unit patients by thetrajectories of observations data throughout their stay as a basis for thedevelopment of personalised risk predictions. Feature importance is consideredto provide model explainability. Using the MIMIC-IV dataset, six clusters wereidentified, capturing differences in disease codes, observations, lengths ofadmissions and outcomes. Applying the pipeline to data from just the first fourhours of each ICU stay assigns the majority of patients to the same cluster aswhen the entire stay duration is considered. In-hospital mortality predictionmodels trained on individual clusters had higher F1 score performance in fiveof the six clusters when compared against the unclustered patient cohort. Thepipeline could form the basis of a clinical decision support tool, working toimprove the clinical characterisation of risk groups and the early detection ofpatient deterioration.</description><author>Thea Barnes, Enrico Werner, Jeffrey N. Clark, Raul Santos-Rodriguez</author><pubDate>Fri, 12 Jul 2024 15:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09373v1</guid></item><item><title>ConRebSeg: A Segmentation Dataset for Reinforced Concrete Construction</title><link>http://arxiv.org/abs/2407.09372v1</link><description>The construction industry has been traditionally slow in adopting digitaltechnologies. However, these are becoming increasingly necessary due to aplentitude of challenges, such as a shortage of skilled labor and decreasingproductivity levels compared to other industries. Autonomous robotic systemscan alleviate this problem, but the software development process for thesesystems is heavily driven by data, a resource usually challenging to find inthe construction domain due to the lack of public availability. In our work, wetherefore provide a dataset of 14,805 RGB images with segmentation labels forreinforced concrete construction and make it publicly available. We conduct adetailed analysis of our dataset and discuss how to deal with labelinginconsistencies. Furthermore, we establish baselines for the YOLOv8L-seg,DeepLabV3, and U-Net segmentation models and investigate the influence of dataavailability and label inconsistencies on the performance of these models. Ourstudy showed that the models are precise in their predictions but would benefitfrom more data to increase the number of recalled instances. Labelinconsistencies had a negligible effect on model performance, and we,therefore, advocate for a crowd-sourced dataset to boost the development ofautonomous robotic systems in the construction industry.</description><author>Patrick Schmidt, Lazaros Nalpantidis</author><pubDate>Fri, 12 Jul 2024 15:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09372v1</guid></item><item><title>Learning High-Frequency Functions Made Easy with Sinusoidal Positional Encoding</title><link>http://arxiv.org/abs/2407.09370v1</link><description>Fourier features based positional encoding (PE) is commonly used in machinelearning tasks that involve learning high-frequency features fromlow-dimensional inputs, such as 3D view synthesis and time series regressionwith neural tangent kernels. Despite their effectiveness, existing PEs requiremanual, empirical adjustment of crucial hyperparameters, specifically theFourier features, tailored to each unique task. Further, PEs face challenges inefficiently learning high-frequency functions, particularly in tasks withlimited data. In this paper, we introduce sinusoidal PE (SPE), designed toefficiently learn adaptive frequency features closely aligned with the trueunderlying function. Our experiments demonstrate that SPE, withouthyperparameter tuning, consistently achieves enhanced fidelity and fastertraining across various tasks, including 3D view synthesis, Text-to-Speechgeneration, and 1D regression. SPE is implemented as a direct replacement forexisting PEs. Its plug-and-play nature lets numerous tasks easily adopt andbenefit from SPE.</description><author>Chuanhao Sun, Zhihang Yuan, Kai Xu, Luo Mai, Siddharth N, Shuo Chen, Mahesh K. Marina</author><pubDate>Fri, 12 Jul 2024 15:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09370v1</guid></item><item><title>Reshaping the Online Data Buffering and Organizing Mechanism for Continual Test-Time Adaptation</title><link>http://arxiv.org/abs/2407.09367v1</link><description>Continual Test-Time Adaptation (CTTA) involves adapting a pre-trained sourcemodel to continually changing unsupervised target domains. In this paper, wesystematically analyze the challenges of this task: online environment,unsupervised nature, and the risks of error accumulation and catastrophicforgetting under continual domain shifts. To address these challenges, wereshape the online data buffering and organizing mechanism for CTTA. We proposean {uncertainty-aware buffering approach} to identify {and aggregate}significant samples with high certainty from the unsupervised, single-pass datastream. {Based on this}, we propose a graph-based class relation preservationconstraint to overcome catastrophic forgetting. Furthermore, a pseudo-targetreplay objective is used to mitigate error accumulation. Extensive experimentsdemonstrate the superiority of our method in both segmentation andclassification CTTA tasks. Code is available at\href{https://github.com/z1358/OBAO}{this https URL}.</description><author>Zhilin Zhu, Xiaopeng Hong, Zhiheng Ma, Weijun Zhuang, Yaohui Ma, Dai Yong, Yaowei Wang</author><pubDate>Fri, 12 Jul 2024 15:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09367v1</guid></item><item><title>MS-TCRNet: Multi-Stage Temporal Convolutional Recurrent Networks for Action Segmentation Using Sensor-Augmented Kinematics</title><link>http://arxiv.org/abs/2303.07814v2</link><description>Action segmentation is a challenging task in high-level process analysis,typically performed on video or kinematic data obtained from various sensors.This work presents two contributions related to action segmentation onkinematic data. Firstly, we introduce two versions of Multi-Stage TemporalConvolutional Recurrent Networks (MS-TCRNet), specifically designed forkinematic data. The architectures consist of a prediction generator withintra-stage regularization and Bidirectional LSTM or GRU-based refinementstages. Secondly, we propose two new data augmentation techniques, World FrameRotation and Hand Inversion, which utilize the strong geometric structure ofkinematic data to improve algorithm performance and robustness. We evaluate ourmodels on three datasets of surgical suturing tasks: the Variable TissueSimulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)Dataset, both of which are open surgery simulation datasets collected by us, aswell as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), awell-known benchmark in robotic surgery. Our methods achieved state-of-the-artperformance.</description><author>Adam Goldbraikh, Omer Shubi, Or Rubin, Carla M Pugh, Shlomi Laufer</author><pubDate>Fri, 12 Jul 2024 15:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07814v2</guid></item><item><title>Instruction Tuning for Secure Code Generation</title><link>http://arxiv.org/abs/2402.09497v2</link><description>Modern language models (LMs) have gained widespread acceptance in everydayand professional contexts, particularly in programming. An essential procedureenabling this adoption is instruction tuning, which substantially enhances LMs'practical utility by training them to follow user instructions and humanpreferences. However, existing instruction tuning schemes overlook a crucialaspect: the security of generated code. As a result, even the state-of-the-artinstruction-tuned LMs frequently produce unsafe code, posing significantsecurity risks. In this work, we introduce SafeCoder to address this gap.SafeCoder performs security-centric fine-tuning using a diverse andhigh-quality dataset that we collected using an automated pipeline. Weintegrate the security fine-tuning with standard instruction tuning, tofacilitate a joint optimization of both security and utility. Despite itssimplicity, we show that SafeCoder is effective across a variety of popular LMsand datasets. It is able to drastically improve security (by about 30%), whilepreserving utility.</description><author>Jingxuan He, Mark Vero, Gabriela Krasnopolska, Martin Vechev</author><pubDate>Fri, 12 Jul 2024 15:45:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09497v2</guid></item><item><title>Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text</title><link>http://arxiv.org/abs/2407.09364v1</link><description>The significant progress in the development of Large Language Models hascontributed to blurring the distinction between human and AI-generated text.The increasing pervasiveness of AI-generated text and the difficulty indetecting it poses new challenges for our society. In this paper, we tackle theproblem of detecting and attributing AI-generated text by proposing WhosAI, atriplet-network contrastive learning framework designed to predict whether agiven input text has been generated by humans or AI and to unveil theauthorship of the text. Unlike most existing approaches, our proposed frameworkis conceived to learn semantic similarity representations from multiplegenerators at once, thus equally handling both detection and attribution tasks.Furthermore, WhosAI is model-agnostic and scalable to the release of new AItext-generation models by incorporating their generated instances into theembedding space learned by our framework. Experimental results on theTuringBench benchmark of 200K news articles show that our proposed frameworkachieves outstanding results in both the Turing Test and Authorship Attributiontasks, outperforming all the methods listed in the TuringBench benchmarkleaderboards.</description><author>Lucio La Cava, Davide Costa, Andrea Tagarelli</author><pubDate>Fri, 12 Jul 2024 15:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09364v1</guid></item><item><title>Chasing Convex Functions with Long-term Constraints</title><link>http://arxiv.org/abs/2402.14012v2</link><description>We introduce and study a family of online metric problems with long-termconstraints. In these problems, an online player makes decisions $\mathbf{x}_t$in a metric space $(X,d)$ to simultaneously minimize their hitting cost$f_t(\mathbf{x}_t)$ and switching cost as determined by the metric. Over thetime horizon $T$, the player must satisfy a long-term demand constraint$\sum_{t} c(\mathbf{x}_t) \geq 1$, where $c(\mathbf{x}_t)$ denotes the fractionof demand satisfied at time $t$. Such problems can find a wide array ofapplications to online resource allocation in sustainable energy/computingsystems. We devise optimal competitive and learning-augmented algorithms forthe case of bounded hitting cost gradients and weighted $\ell_1$ metrics, andfurther show that our proposed algorithms perform well in numericalexperiments.</description><author>Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</author><pubDate>Fri, 12 Jul 2024 15:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14012v2</guid></item><item><title>A Neural Rewriting System to Solve Algorithmic Problems</title><link>http://arxiv.org/abs/2402.17407v2</link><description>Modern neural network architectures still struggle to learn algorithmicprocedures that require to systematically apply compositional rules to solveout-of-distribution problem instances. In this work, we focus on formulasimplification problems, a class of synthetic benchmarks used to study thesystematic generalization capabilities of neural architectures. We propose amodular architecture designed to learn a general procedure for solving nestedmathematical formulas by only relying on a minimal set of training examples.Inspired by rewriting systems, a classic framework in symbolic artificialintelligence, we include in the architecture three specialized and interactingmodules: the Selector, trained to identify solvable sub-expressions; theSolver, mapping sub-expressions to their values; and the Combiner, replacingsub-expressions in the original formula with the solution provided by theSolver. We benchmark our system against the Neural Data Router, a recent modelspecialized for systematic generalization, and a state-of-the-art largelanguage model (GPT-4) probed with advanced prompting strategies. Wedemonstrate that our approach achieves a higher degree of out-of-distributiongeneralization compared to these alternative approaches on three differenttypes of formula simplification problems, and we discuss its limitations byanalyzing its failures.</description><author>Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti</author><pubDate>Fri, 12 Jul 2024 15:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17407v2</guid></item><item><title>Novel clustered federated learning based on local loss</title><link>http://arxiv.org/abs/2407.09360v1</link><description>This paper proposes LCFL, a novel clustering metric for evaluating clients'data distributions in federated learning. LCFL aligns with federated learningrequirements, accurately assessing client-to-client variations in datadistribution. It offers advantages over existing clustered federated learningmethods, addressing privacy concerns, improving applicability to non-convexmodels, and providing more accurate classification results. LCFL does notrequire prior knowledge of clients' data distributions. We provide a rigorousmathematical analysis, demonstrating the correctness and feasibility of ourframework. Numerical experiments with neural network instances highlight thesuperior performance of LCFL over baselines on several clustered federatedlearning benchmarks.</description><author>Endong Gu, Yongxin Chen, Hao Wen, Xingju Cai, Deren Han</author><pubDate>Fri, 12 Jul 2024 15:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09360v1</guid></item><item><title>ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</title><link>http://arxiv.org/abs/2402.17766v3</link><description>This paper presents ShapeLLM, the first 3D Multimodal Large Language Model(LLM) designed for embodied interaction, exploring a universal 3D objectunderstanding with 3D point clouds and languages. ShapeLLM is built upon animproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-viewimage distillation for enhanced geometry understanding. By utilizing ReCon++ asthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructedinstruction-following data and tested on our newly human-curated benchmark, 3DMM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3Dgeometry understanding and language-unified 3D interaction tasks, such asembodied visual grounding. Project page: https://qizekun.github.io/shapellm/</description><author>Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, Kaisheng Ma</author><pubDate>Fri, 12 Jul 2024 15:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17766v3</guid></item><item><title>What Makes a Good Explanation?: A Harmonized View of Properties of Explanations</title><link>http://arxiv.org/abs/2211.05667v3</link><description>Interpretability provides a means for humans to verify aspects of machinelearning (ML) models and empower human+ML teaming in situations where the taskcannot be fully automated. Different contexts require explanations withdifferent properties. For example, the kind of explanation required todetermine if an early cardiac arrest warning system is ready to be integratedinto a care setting is very different from the type of explanation required fora loan applicant to help determine the actions they might need to take to maketheir application successful. Unfortunately, there is a lack of standardization when it comes to propertiesof explanations: different papers may use the same term to mean differentquantities, and different terms to mean the same quantity. This lack of astandardized terminology and categorization of the properties of MLexplanations prevents us from both rigorously comparing interpretable machinelearning methods and identifying what properties are needed in what contexts. In this work, we survey properties defined in interpretable machine learningpapers, synthesize them based on what they actually measure, and describe thetrade-offs between different formulations of these properties. In doing so, weenable more informed selection of task-appropriate formulations of explanationproperties as well as standardization for future work in interpretable machinelearning.</description><author>Zixi Chen, Varshini Subhash, Marton Havasi, Weiwei Pan, Finale Doshi-Velez</author><pubDate>Fri, 12 Jul 2024 15:34:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05667v3</guid></item><item><title>A Unified Anomaly Synthesis Strategy with Gradient Ascent for Industrial Anomaly Detection and Localization</title><link>http://arxiv.org/abs/2407.09359v1</link><description>Anomaly synthesis strategies can effectively enhance unsupervised anomalydetection. However, existing strategies have limitations in the coverage andcontrollability of anomaly synthesis, particularly for weak defects that arevery similar to normal regions. In this paper, we propose Global and LocalAnomaly co-Synthesis Strategy (GLASS), a novel unified framework designed tosynthesize a broader coverage of anomalies under the manifold and hyperspheredistribution constraints of Global Anomaly Synthesis (GAS) at the feature leveland Local Anomaly Synthesis (LAS) at the image level. Our method synthesizesnear-in-distribution anomalies in a controllable way using Gaussian noiseguided by gradient ascent and truncated projection. GLASS achievesstate-of-the-art results on the MVTec AD (detection AUROC of 99.9\%), VisA, andMPDD datasets and excels in weak defect detection. The effectiveness andefficiency have been further validated in industrial applications for wovenfabric defect detection. The code and dataset are available at:\url{https://github.com/cqylunlun/GLASS}.</description><author>Qiyu Chen, Huiyuan Luo, Chengkan Lv, Zhengtao Zhang</author><pubDate>Fri, 12 Jul 2024 15:33:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09359v1</guid></item><item><title>Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees</title><link>http://arxiv.org/abs/2407.09357v1</link><description>Generating novel molecules is challenging, with most representations leadingto generative models producing many invalid molecules. Spanning Tree-basedGraph Generation (STGG) is a promising approach to ensure the generation ofvalid molecules, outperforming state-of-the-art SMILES and graph diffusionmodels for unconditional generation. In the real world, we want to be able togenerate molecules conditional on one or multiple desired properties ratherthan unconditionally. Thus, in this work, we extend STGG tomulti-property-conditional generation. Our approach, STGG+, incorporates amodern Transformer architecture, random masking of properties during training(enabling conditioning on any subset of properties and classifier-freeguidance), an auxiliary property-prediction loss (allowing the model toself-criticize molecules and select the best ones), and other improvements. Weshow that STGG+ achieves state-of-the-art performance on in-distribution andout-of-distribution conditional generation, and reward maximization.</description><author>Alexia Jolicoeur-Martineau, Aristide Baratin, Kisoo Kwon, Boris Knyazev, Yan Zhang</author><pubDate>Fri, 12 Jul 2024 15:32:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09357v1</guid></item><item><title>AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing</title><link>http://arxiv.org/abs/2303.06252v2</link><description>The intensive care unit (ICU) is a specialized hospital space wherecritically ill patients receive intensive care and monitoring. Comprehensivemonitoring is imperative in assessing patients conditions, in particularacuity, and ultimately the quality of care. However, the extent of patientmonitoring in the ICU is limited due to time constraints and the workload onhealthcare providers. Currently, visual assessments for acuity, including finedetails such as facial expressions, posture, and mobility, are sporadicallycaptured, or not captured at all. These manual observations are subjective tothe individual, prone to documentation errors, and overburden care providerswith the additional workload. Artificial Intelligence (AI) enabled systems hasthe potential to augment the patient visual monitoring and assessment due totheir exceptional learning capabilities. Such systems require robust annotateddata to train. To this end, we have developed pervasive sensing and dataprocessing system which collects data from multiple modalities depth images,color RGB images, accelerometry, electromyography, sound pressure, and lightlevels in ICU for developing intelligent monitoring systems for continuous andgranular acuity, delirium risk, pain, and mobility assessment. This paperpresents the Intelligent Intensive Care Unit (I2CU) system architecture wedeveloped for real-time patient monitoring and visual assessment.</description><author>Subhash Nerella, Ziyuan Guan, Scott Siegel, Jiaqing Zhang, Kia Khezeli, Azra Bihorac, Parisa Rashidi</author><pubDate>Fri, 12 Jul 2024 15:30:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06252v2</guid></item><item><title>FastImpute: A Baseline for Open-source, Reference-Free Genotype Imputation Methods -- A Case Study in PRS313</title><link>http://arxiv.org/abs/2407.09355v1</link><description>Genotype imputation enhances genetic data by predicting missing SNPs usingreference haplotype information. Traditional methods leverage linkagedisequilibrium (LD) to infer untyped SNP genotypes, relying on the similarityof LD structures between genotyped target sets and fully sequenced referencepanels. Recently, reference-free deep learning-based methods have emerged,offering a promising alternative by predicting missing genotypes withoutexternal databases, thereby enhancing privacy and accessibility. However, thesemethods often produce models with tens of millions of parameters, leading tochallenges such as the need for substantial computational resources to trainand inefficiency for client-sided deployment. Our study addresses theselimitations by introducing a baseline for a novel genotype imputation pipelinethat supports client-sided imputation models generalizable across anygenotyping chip and genomic region. This approach enhances patient privacy byperforming imputation directly on edge devices. As a case study, we focus onPRS313, a polygenic risk score comprising 313 SNPs used for breast cancer riskprediction. Utilizing consumer genetic panels such as 23andMe, our modeldemocratizes access to personalized genetic insights by allowing 23andMe usersto obtain their PRS313 score. We demonstrate that simple linear regression cansignificantly improve the accuracy of PRS313 scores when calculated using SNPsimputed from consumer gene panels, such as 23andMe. Our linear regression modelachieved an R^2 of 0.86, compared to 0.33 without imputation and 0.28 withsimple imputation (substituting missing SNPs with the minor allele frequency).These findings suggest that popular SNP analysis libraries could benefit fromintegrating linear regression models for genotype imputation, providing aviable and light-weight alternative to reference based imputation.</description><author>Aaron Ge, Jeya Balasubramanian, Xueyao Wu, Peter Kraft, Jonas S. Almeida</author><pubDate>Fri, 12 Jul 2024 15:28:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09355v1</guid></item><item><title>Imaging Interiors: An Implicit Solution to Electromagnetic Inverse Scattering Problems</title><link>http://arxiv.org/abs/2407.09352v1</link><description>Electromagnetic Inverse Scattering Problems (EISP) have gained wideapplications in computational imaging. By solving EISP, the internal relativepermittivity of the scatterer can be non-invasively determined based on thescattered electromagnetic fields. Despite previous efforts to address EISP,achieving better solutions to this problem has remained elusive, due to thechallenges posed by inversion and discretization. This paper tackles thosechallenges in EISP via an implicit approach. By representing the scatterer'srelative permittivity as a continuous implicit representation, our method isable to address the low-resolution problems arising from discretization.Further, optimizing this implicit representation within a forward frameworkallows us to conveniently circumvent the challenges posed by inverseestimation. Our approach outperforms existing methods on standard benchmarkdatasets. Project page: https://luo-ziyuan.github.io/Imaging-Interiors</description><author>Ziyuan Luo, Boxin Shi, Haoliang Li, Renjie Wan</author><pubDate>Fri, 12 Jul 2024 15:25:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09352v1</guid></item><item><title>Efficient Bayesian Updates for Deep Learning via Laplace Approximations</title><link>http://arxiv.org/abs/2210.06112v2</link><description>Since training deep neural networks takes significant computationalresources, extending the training dataset with new data is difficult, as ittypically requires complete retraining. Moreover, specific applications do notallow costly retraining due to time or computational constraints. We addressthis issue by proposing a novel Bayesian update method for deep neural networksby using a last-layer Laplace approximation. Concretely, we leveragesecond-order optimization techniques on the Gaussian posterior distribution ofa Laplace approximation, computing the inverse Hessian matrix in closed form.This way, our method allows for fast and effective updates upon the arrival ofnew data in a stationary setting. A large-scale evaluation study acrossdifferent data modalities confirms that our updates are a fast and competitivealternative to costly retraining. Furthermore, we demonstrate its applicabilityin a deep active learning scenario by using our update to improve existingselection strategies.</description><author>Denis Huseljic, Marek Herde, Lukas Rauch, Paul Hahn, Zhixin Huang, Daniel Kottke, Stephan Vogt, Bernhard Sick</author><pubDate>Fri, 12 Jul 2024 15:23:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06112v2</guid></item><item><title>Predictable and Performant Reactive Synthesis Modulo Theories via Functional Synthesis</title><link>http://arxiv.org/abs/2407.09348v1</link><description>Reactive synthesis is the process of generating correct controllers fromtemporal logic specifications. Classical LTL reactive synthesis handles(propositional) LTL as a specification language. Boolean abstractions allowreducing LTLt specifications (i.e., LTL with propositions replaced by literalsfrom a theory calT), into equi-realizable LTL specifications. In this paper weextend these results into a full static synthesis procedure. The synthesizedsystem receives from the environment valuations of variables from a rich theorycalT and outputs valuations of system variables from calT. We use theabstraction method to synthesize a reactive Boolean controller from the LTLspecification, and we combine it with functional synthesis to obtain a staticcontroller for the original LTLt specification. We also show that our methodallows responses in the sense that the controller can optimize its outputs inorder to e.g., always provide the smallest safe values. This is the first fullstatic synthesis method for LTLt, which is a deterministic program (hencepredictable and efficient).</description><author>Andoni Rodríguez, Felipe Gorostiaga, César Sánchez</author><pubDate>Fri, 12 Jul 2024 15:23:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09348v1</guid></item><item><title>Pre-training Point Cloud Compact Model with Partial-aware Reconstruction</title><link>http://arxiv.org/abs/2407.09344v1</link><description>The pre-trained point cloud model based on Masked Point Modeling (MPM) hasexhibited substantial improvements across various tasks. However, two drawbackshinder their practical application. Firstly, the positional embedding of maskedpatches in the decoder results in the leakage of their central coordinates,leading to limited 3D representations. Secondly, the excessive model size ofexisting MPM methods results in higher demands for devices. To address these,we propose to pre-train Point cloud Compact Model with Partial-aware\textbf{R}econstruction, named Point-CPR. Specifically, in the decoder, wecouple the vanilla masked tokens with their positional embeddings as randomlymasked queries and introduce a partial-aware prediction module before eachdecoder layer to predict them from the unmasked partial. It prevents thedecoder from creating a shortcut between the central coordinates of maskedpatches and their reconstructed coordinates, enhancing the robustness ofmodels. We also devise a compact encoder composed of local aggregation andMLPs, reducing the parameters and computational requirements compared toexisting Transformer-based encoders. Extensive experiments demonstrate that ourmodel exhibits strong performance across various tasks, especially surpassingthe leading MPM-based model PointGPT-B with only 2% of its parameters.</description><author>Yaohua Zha, Yanzi Wang, Tao Dai, Shu-Tao Xia</author><pubDate>Fri, 12 Jul 2024 15:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09344v1</guid></item><item><title>Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments</title><link>http://arxiv.org/abs/2407.07933v2</link><description>We consider the challenging problem of estimating causal effects from purelyobservational data in the bi-directional Mendelian randomization (MR), wheresome invalid instruments, as well as unmeasured confounding, usually exist. Toaddress this problem, most existing methods attempt to find proper validinstrumental variables (IVs) for the target causal effect by expert knowledgeor by assuming that the causal model is a one-directional MR model. As such, inthis paper, we first theoretically investigate the identification of thebi-directional MR from observational data. In particular, we provide necessaryand sufficient conditions under which valid IV sets are correctly identifiedsuch that the bi-directional MR model is identifiable, including the causaldirections of a pair of phenotypes (i.e., the treatment and outcome). Moreover,based on the identification theory, we develop a cluster fusion-like method todiscover valid IV sets and estimate the causal effects of interest. Wetheoretically demonstrate the correctness of the proposed algorithm.Experimental results show the effectiveness of our method for estimating causaleffects in bi-directional MR.</description><author>Feng Xie, Zhen Yao, Lin Xie, Yan Zeng, Zhi Geng</author><pubDate>Fri, 12 Jul 2024 15:15:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07933v2</guid></item><item><title>Structured Generations: Using Hierarchical Clusters to guide Diffusion Models</title><link>http://arxiv.org/abs/2407.06124v2</link><description>This paper introduces Diffuse-TreeVAE, a deep generative model thatintegrates hierarchical clustering into the framework of Denoising DiffusionProbabilistic Models (DDPMs). The proposed approach generates new images bysampling from a root embedding of a learned latent tree VAE-based structure, itthen propagates through hierarchical paths, and utilizes a second-stage DDPM torefine and generate distinct, high-quality images for each data cluster. Theresult is a model that not only improves image clarity but also ensures thatthe generated samples are representative of their respective clusters,addressing the limitations of previous VAE-based methods and advancing thestate of clustering-based generative modeling.</description><author>Jorge da Silva Goncalves, Laura Manduchi, Moritz Vandenhirtz, Julia E. Vogt</author><pubDate>Fri, 12 Jul 2024 15:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06124v2</guid></item><item><title>CFaults: Model-Based Diagnosis for Fault Localization in C Programs with Multiple Test Cases</title><link>http://arxiv.org/abs/2407.09337v1</link><description>Debugging is one of the most time-consuming and expensive tasks in softwaredevelopment. Several formula-based fault localization (FBFL) methods have beenproposed, but they fail to guarantee a set of diagnoses across all failingtests or may produce redundant diagnoses that are not subset-minimal,particularly for programs with multiple faults. This paper introduces a novel fault localization approach for C programs withmultiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multipleobservations and aggregates all failing test cases into a unified MaxSATformula. Consequently, our method guarantees consistency across observationsand simplifies the fault localization procedure. Experimental results on twobenchmark sets of C programs, TCAS and C-Pack-IPAs, show that CFaults is fasterthan other FBFL approaches like BugAssist and SNIPER. Moreover, CFaults onlygenerates subset-minimal diagnoses of faulty statements, whereas the otherapproaches tend to enumerate redundant diagnoses.</description><author>Pedro Orvalho, Mikoláš Janota, Vasco Manquinho</author><pubDate>Fri, 12 Jul 2024 15:14:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09337v1</guid></item><item><title>Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning</title><link>http://arxiv.org/abs/2406.08002v2</link><description>Despite the recent successes of multi-agent reinforcement learning (MARL)algorithms, efficiently adapting to co-players in mixed-motive environmentsremains a significant challenge. One feasible approach is to hierarchicallymodel co-players' behavior based on inferring their characteristics. However,these methods often encounter difficulties in efficient reasoning andutilization of inferred information. To address these issues, we proposeHierarchical Opponent modeling and Planning (HOP), a novel multi-agentdecision-making algorithm that enables few-shot adaptation to unseen policiesin mixed-motive environments. HOP is hierarchically composed of two modules: anopponent modeling module that infers others' goals and learns correspondinggoal-conditioned policies, and a planning module that employs Monte Carlo TreeSearch (MCTS) to identify the best response. Our approach improves efficiencyby updating beliefs about others' goals both across and within episodes and byusing information from the opponent modeling module to guide planning.Experimental results demonstrate that in mixed-motive environments, HOPexhibits superior few-shot adaptation capabilities when interacting withvarious unseen agents, and excels in self-play scenarios. Furthermore, theemergence of social intelligence during our experiments underscores thepotential of our approach in complex multi-agent environments.</description><author>Yizhe Huang, Anji Liu, Fanqi Kong, Yaodong Yang, Song-Chun Zhu, Xue Feng</author><pubDate>Fri, 12 Jul 2024 15:13:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08002v2</guid></item><item><title>Guidelines for Augmentation Selection in Contrastive Learning for Time Series Classification</title><link>http://arxiv.org/abs/2407.09336v1</link><description>Self-supervised contrastive learning has become a key technique in deeplearning, particularly in time series analysis, due to its ability to learnmeaningful representations without explicit supervision. Augmentation is acritical component in contrastive learning, where different augmentations candramatically impact performance, sometimes influencing accuracy by over 30%.However, the selection of augmentations is predominantly empirical which can besuboptimal, or grid searching that is time-consuming. In this paper, weestablish a principled framework for selecting augmentations based on datasetcharacteristics such as trend and seasonality. Specifically, we construct 12synthetic datasets incorporating trend, seasonality, and integration weights.We then evaluate the effectiveness of 8 different augmentations across thesesynthetic datasets, thereby inducing generalizable associations between timeseries characteristics and augmentation efficiency. Additionally, we evaluatedthe induced associations across 6 real-world datasets encompassing domains suchas activity recognition, disease diagnosis, traffic monitoring, electricityusage, mechanical fault prognosis, and finance. These real-world datasets arediverse, covering a range from 1 to 12 channels, 2 to 10 classes, sequencelengths of 14 to 1280, and data frequencies from 250 Hz to daily intervals. Theexperimental results show that our proposed trend-seasonality-basedaugmentation recommendation algorithm can accurately identify the effectiveaugmentations for a given time series dataset, achieving an average Recall@3 of0.667, outperforming baselines. Our work provides guidance for studiesemploying contrastive learning in time series analysis, with wide-rangingapplications. All the code, datasets, and analysis results will be released athttps://github.com/DL4mHealth/TS-Contrastive-Augmentation-Recommendation.</description><author>Ziyu Liu, Azadeh Alavi, Minyi Li, Xiang Zhang</author><pubDate>Fri, 12 Jul 2024 15:13:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09336v1</guid></item><item><title>Feasibility Study on Active Learning of Smart Surrogates for Scientific Simulations</title><link>http://arxiv.org/abs/2407.07674v2</link><description>High-performance scientific simulations, important for comprehension ofcomplex systems, encounter computational challenges especially when exploringextensive parameter spaces. There has been an increasing interest in developingdeep neural networks (DNNs) as surrogate models capable of accelerating thesimulations. However, existing approaches for training these DNN surrogatesrely on extensive simulation data which are heuristically selected andgenerated with expensive computation -- a challenge under-explored in theliterature. In this paper, we investigate the potential of incorporating activelearning into DNN surrogate training. This allows intelligent and objectiveselection of training simulations, reducing the need to generate extensivesimulation data as well as the dependency of the performance of DNN surrogateson pre-defined training simulations. In the problem context of constructing DNNsurrogates for diffusion equations with sources, we examine the efficacy ofdiversity- and uncertainty-based strategies for selecting training simulations,considering two different DNN architecture. The results set the groundwork fordeveloping the high-performance computing infrastructure for Smart Surrogatesthat supports on-the-fly generation of simulation data steered by activelearning strategies to potentially improve the efficiency of scientificsimulations.</description><author>Pradeep Bajracharya, Javier Quetzalcóatl Toledo-Marín, Geoffrey Fox, Shantenu Jha, Linwei Wang</author><pubDate>Fri, 12 Jul 2024 15:10:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07674v2</guid></item><item><title>State space representations of the Roesser type for convolutional layers</title><link>http://arxiv.org/abs/2403.11938v2</link><description>From the perspective of control theory, convolutional layers (of neuralnetworks) are 2-D (or N-D) linear time-invariant dynamical systems. The usualrepresentation of convolutional layers by the convolution kernel corresponds tothe representation of a dynamical system by its impulse response. However, manyanalysis tools from control theory, e.g., involving linear matrix inequalities,require a state space representation. For this reason, we explicitly provide astate space representation of the Roesser type for 2-D convolutional layerswith $c_\mathrm{in}r_1 + c_\mathrm{out}r_2$ states, where$c_\mathrm{in}$/$c_\mathrm{out}$ is the number of input/output channels of thelayer and $r_1$/$r_2$ characterizes the width/length of the convolution kernel.This representation is shown to be minimal for $c_\mathrm{in} =c_\mathrm{out}$. We further construct state space representations for dilated,strided, and N-D convolutions.</description><author>Patricia Pauli, Dennis Gramlich, Frank Allgöwer</author><pubDate>Fri, 12 Jul 2024 15:08:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11938v2</guid></item><item><title>Detecting Visual Cues in the Intensive Care Unit and Association with Patient Clinical Status</title><link>http://arxiv.org/abs/2311.00565v2</link><description>Intensive Care Units (ICU) provide close supervision and continuous care topatients with life-threatening conditions. However, continuous patientassessment in the ICU is still limited due to time constraints and the workloadon healthcare providers. Existing patient assessments in the ICU such as painor mobility assessment are mostly sporadic and administered manually, thusintroducing the potential for human errors. Developing Artificial intelligence(AI) tools that can augment human assessments in the ICU can be beneficial forproviding more objective and granular monitoring capabilities. For example,capturing the variations in a patient's facial cues related to pain oragitation can help in adjusting pain-related medications or detectingagitation-inducing conditions such as delirium. Additionally, subtle changes invisual cues during or prior to adverse clinical events could potentially aid incontinuous patient monitoring when combined with high-resolution physiologicalsignals and Electronic Health Record (EHR) data. In this paper, we examined theassociation between visual cues and patient condition including acuity status,acute brain dysfunction, and pain. We leveraged our AU-ICU dataset with 107,064frames collected in the ICU annotated with facial action units (AUs) labels bytrained annotators. We developed a new "masked loss computation" technique thataddresses the data imbalance problem by maximizing data resource utilization.We trained the model using our AU-ICU dataset in conjunction with threeexternal datasets to detect 18 AUs. The SWIN Transformer model achieved 0.57mean F1-score and 0.89 mean accuracy on the test set. Additionally, weperformed AU inference on 634,054 frames to evaluate the association betweenfacial AUs and clinically important patient conditions such as acuity status,acute brain dysfunction, and pain.</description><author>Subhash Nerella, Ziyuan Guan, Andrea Davidson, Yuanfang Ren, Tezcan Baslanti, Brooke Armfield, Patrick Tighe, Azra Bihorac, Parisa Rashidi</author><pubDate>Fri, 12 Jul 2024 15:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00565v2</guid></item><item><title>Sina at FigNews 2024: Multilingual Datasets Annotated with Bias and Propaganda</title><link>http://arxiv.org/abs/2407.09327v1</link><description>The proliferation of bias and propaganda on social media is an increasinglysignificant concern, leading to the development of techniques for automaticdetection. This article presents a multilingual corpus of 12, 000 Facebookposts fully annotated for bias and propaganda. The corpus was created as partof the FigNews 2024 Shared Task on News Media Narratives for framing theIsraeli War on Gaza. It covers various events during the War from October 7,2023 to January 31, 2024. The corpus comprises 12, 000 posts in five languages(Arabic, Hebrew, English, French, and Hindi), with 2, 400 posts for eachlanguage. The annotation process involved 10 graduate students specializing inLaw. The Inter-Annotator Agreement (IAA) was used to evaluate the annotationsof the corpus, with an average IAA of 80.8% for bias and 70.15% for propagandaannotations. Our team was ranked among the bestperforming teams in both Biasand Propaganda subtasks. The corpus is open-source and available athttps://sina.birzeit.edu/fada</description><author>Lina Duaibes, Areej Jaber, Mustafa Jarrar, Ahmad Qadi, Mais Qandeel</author><pubDate>Fri, 12 Jul 2024 15:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09327v1</guid></item><item><title>Provable Privacy Advantages of Decentralized Federated Learning via Distributed Optimization</title><link>http://arxiv.org/abs/2407.09324v1</link><description>Federated learning (FL) emerged as a paradigm designed to improve dataprivacy by enabling data to reside at its source, thus embedding privacy as acore consideration in FL architectures, whether centralized or decentralized.Contrasting with recent findings by Pasquini et al., which suggest thatdecentralized FL does not empirically offer any additional privacy or securitybenefits over centralized models, our study provides compelling evidence to thecontrary. We demonstrate that decentralized FL, when deploying distributedoptimization, provides enhanced privacy protection - both theoretically andempirically - compared to centralized approaches. The challenge of quantifyingprivacy loss through iterative processes has traditionally constrained thetheoretical exploration of FL protocols. We overcome this by conducting apioneering in-depth information-theoretical privacy analysis for bothframeworks. Our analysis, considering both eavesdropping and passive adversarymodels, successfully establishes bounds on privacy leakage. We show informationtheoretically that the privacy loss in decentralized FL is upper bounded by theloss in centralized FL. Compared to the centralized case where local gradientsof individual participants are directly revealed, a key distinction ofoptimization-based decentralized FL is that the relevant information includesdifferences of local gradients over successive iterations and the aggregatedsum of different nodes' gradients over the network. This informationcomplicates the adversary's attempt to infer private data. To bridge ourtheoretical insights with practical applications, we present detailed casestudies involving logistic regression and deep neural networks. These examplesdemonstrate that while privacy leakage remains comparable in simpler models,complex models like deep neural networks exhibit lower privacy risks underdecentralized FL.</description><author>Wenrui Yu, Qiongxiu Li, Milan Lopuhaä-Zwakenberg, Mads Græsbøll Christensen, Richard Heusdens</author><pubDate>Fri, 12 Jul 2024 15:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09324v1</guid></item><item><title>Soft Prompt Generation for Domain Generalization</title><link>http://arxiv.org/abs/2404.19286v2</link><description>Large pre-trained vision language models (VLMs) have shown impressivezero-shot ability on downstream tasks with manually designed prompt. To furtheradapt VLMs to downstream tasks, soft prompt is proposed to replace manuallydesigned prompt, which undergoes fine-tuning based on specific domain data.Prior prompt learning methods primarily learn a fixed prompt or residuledprompt from training samples. However, the learned prompts lack diversity andignore information about unseen domains. In this paper, we reframe the promptlearning framework from a generative perspective and propose a simple yetefficient method for the Domain Generalization (DG) task, namely Soft PromptGeneration (SPG). Specifically, SPG consists of a two-stage training phase andan inference phase. During the training phase, we introduce soft prompt labelfor each domain, aiming to incorporate the generative model domain knowledge.During the inference phase, the generator of the generative model is employedto obtain instance-specific soft prompts for the unseen target domain.Extensive experiments on five domain generalization benchmarks of three DGtasks demonstrate that SPG achieves state-of-the-art performance. The code isavailable at https://github.com/renytek13/Soft-Prompt-Generation-with-CGAN.</description><author>Shuanghao Bai, Yuedi Zhang, Wanqi Zhou, Zhirong Luan, Badong Chen</author><pubDate>Fri, 12 Jul 2024 14:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.19286v2</guid></item><item><title>Can large language models explore in-context?</title><link>http://arxiv.org/abs/2403.15371v2</link><description>We investigate the extent to which contemporary Large Language Models (LLMs)can engage in exploration, a core capability in reinforcement learning anddecision making. We focus on native performance of existing LLMs, withouttraining interventions. We deploy LLMs as agents in simple multi-armed banditenvironments, specifying the environment description and interaction historyentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,GPT-4, and Llama2, using a variety of prompt designs, and find that the modelsdo not robustly engage in exploration without substantial interventions: i)Across all of our experiments, only one configuration resulted in satisfactoryexploratory behavior: GPT-4 with chain-of-thought reasoning and an externallysummarized interaction history, presented as sufficient statistics; ii) Allother configurations did not result in robust exploratory behavior, includingthose with chain-of-thought reasoning but unsummarized history. Although thesefindings can be interpreted positively, they suggest that externalsummarization -- which may not be possible in more complex settings -- isimportant for obtaining desirable behavior from LLM agents. We conclude thatnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,may be required to empower LLM-based decision making agents in complexsettings.</description><author>Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins</author><pubDate>Fri, 12 Jul 2024 14:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15371v2</guid></item><item><title>Scalability of Bayesian Network Structure Elicitation with Large Language Models: a Novel Methodology and Comparative Analysis</title><link>http://arxiv.org/abs/2407.09311v1</link><description>In this work, we propose a novel method for Bayesian Networks (BNs) structureelicitation that is based on the initialization of several LLMs with differentexperiences, independently querying them to create a structure of the BN, andfurther obtaining the final structure by majority voting. We compare the methodwith one alternative method on various widely and not widely known BNs ofdifferent sizes and study the scalability of both methods on them. We alsopropose an approach to check the contamination of BNs in LLM, which shows thatsome widely known BNs are inapplicable for testing the LLM usage for BNsstructure elicitation. We also show that some BNs may be inapplicable for suchexperiments because their node names are indistinguishable. The experiments onthe other BNs show that our method performs better than the existing methodwith one of the three studied LLMs; however, the performance of both methodssignificantly decreases with the increase in BN size.</description><author>Nikolay Babakov, Ehud Reiter, Alberto Bugarin</author><pubDate>Fri, 12 Jul 2024 14:52:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09311v1</guid></item><item><title>Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration</title><link>http://arxiv.org/abs/2405.03862v2</link><description>Multi-agent AI systems can be used for simulating collective decision-makingin scientific and practical applications. They can also be used to introduce adiverse group discussion step in chatbot pipelines, enhancing the culturalsensitivity of the chatbot's responses. These applications, however, arepredicated on the ability of AI agents to reliably adopt assigned personas andmimic human interactions. To evaluate the ability of LLM agents to satisfythese requirements, we examine AI agent ensembles engaged in culturalcollaboration and debate by analyzing their private responses and chattranscripts. Our findings suggest that multi-agent discussions can encouragecollective decisions that reflect diverse perspectives, yet this benefit istempered by the agents' susceptibility to conformity due to perceived peerpressure and challenges in maintaining consistent personas and opinions.Instructions that encourage debate in support of one's opinions rather thancollaboration increase the rate of inconstancy. Without addressing the factorswe identify, the full potential of multi-agent frameworks for producing moreculturally diverse AI outputs or more realistic simulations of groupdecision-making will remain untapped.</description><author>Razan Baltaji, Babak Hemmatian, Lav R. Varshney</author><pubDate>Fri, 12 Jul 2024 14:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03862v2</guid></item><item><title>Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing</title><link>http://arxiv.org/abs/2407.01394v2</link><description>Sign language translation from video to spoken text presents uniquechallenges owing to the distinct grammar, expression nuances, and highvariation of visual appearance across different speakers and contexts. Theintermediate gloss annotations of videos aim to guide the translation process.In our work, we focus on {\em Gloss2Text} translation stage and propose severaladvances by leveraging pre-trained large language models (LLMs), dataaugmentation, and novel label-smoothing loss function exploiting glosstranslation ambiguities improving significantly the performance ofstate-of-the-art approaches. Through extensive experiments and ablation studieson the PHOENIX Weather 2014T dataset, our approach surpasses state-of-the-artperformance in {\em Gloss2Text} translation, indicating its efficacy inaddressing sign language translation and suggesting promising avenues forfuture research and development.</description><author>Pooya Fayyazsanavi, Antonios Anastasopoulos, Jana Košecká</author><pubDate>Fri, 12 Jul 2024 14:44:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01394v2</guid></item><item><title>Leveraging Computer Vision in the Intensive Care Unit (ICU) for Examining Visitation and Mobility</title><link>http://arxiv.org/abs/2403.06322v2</link><description>Despite the importance of closely monitoring patients in the Intensive CareUnit (ICU), many aspects are still assessed in a limited manner due to the timeconstraints imposed on healthcare providers. For example, although excessivevisitations during rest hours can potentially exacerbate the risk of circadianrhythm disruption and delirium, it is not captured in the ICU. Likewise, whilemobility can be an important indicator of recovery or deterioration in ICUpatients, it is only captured sporadically or not captured at all. In the pastfew years, the computer vision field has found application in many domains byreducing the human burden. Using computer vision systems in the ICU can alsopotentially enable non-existing assessments or enhance the frequency andaccuracy of existing assessments while reducing the staff workload. In thisstudy, we leverage a state-of-the-art noninvasive computer vision system basedon depth imaging to characterize ICU visitations and patients' mobility. Wethen examine the relationship between visitation and several patient outcomes,such as pain, acuity, and delirium. We found an association betweendeteriorating patient acuity and the incidence of delirium with increasedvisitations. In contrast, self-reported pain, reported using the Defense andVeteran Pain Rating Scale (DVPRS), was correlated with decreased visitations.Our findings highlight the feasibility and potential of using noninvasiveautonomous systems to monitor ICU patients.</description><author>Scott Siegel, Jiaqing Zhang, Sabyasachi Bandyopadhyay, Subhash Nerella, Brandon Silva, Tezcan Baslanti, Azra Bihorac, Parisa Rashidi</author><pubDate>Fri, 12 Jul 2024 14:43:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06322v2</guid></item><item><title>SideSeeing: A multimodal dataset and collection of tools for sidewalk assessment</title><link>http://arxiv.org/abs/2407.06464v2</link><description>This paper introduces SideSeeing, a novel initiative that provides tools anddatasets for assessing the built environment. We present a framework forstreet-level data acquisition, loading, and analysis. Using the framework, wecollected a novel dataset that integrates synchronized video footaged capturedfrom chest-mounted mobile devices with sensor data (accelerometer, gyroscope,magnetometer, and GPS). Each data sample represents a path traversed by a userfilming sidewalks near hospitals in Brazil and the USA. The dataset encompassesthree hours of content covering 12 kilometers around nine hospitals, andincludes 325,000 video frames with corresponding sensor data. Additionally, wepresent a novel 68-element taxonomy specifically created for sidewalk sceneidentification. SideSeeing is a step towards a suite of tools that urbanexperts can use to perform in-depth sidewalk accessibility evaluations.SideSeeing data and tools are publicly available athttps://sites.usp.br/sideseeing/.</description><author>R. J. P. Damaceno, L. Ferreira, F. Miranda, M. Hosseini, R. M. Cesar Jr</author><pubDate>Fri, 12 Jul 2024 14:38:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06464v2</guid></item><item><title>ProDepth: Boosting Self-Supervised Multi-Frame Monocular Depth with Probabilistic Fusion</title><link>http://arxiv.org/abs/2407.09303v1</link><description>Self-supervised multi-frame monocular depth estimation relies on thegeometric consistency between successive frames under the assumption of astatic scene. However, the presence of moving objects in dynamic scenesintroduces inevitable inconsistencies, causing misaligned multi-frame featurematching and misleading self-supervision during training. In this paper, wepropose a novel framework called ProDepth, which effectively addresses themismatch problem caused by dynamic objects using a probabilistic approach. Weinitially deduce the uncertainty associated with static scene assumption byadopting an auxiliary decoder. This decoder analyzes inconsistencies embeddedin the cost volume, inferring the probability of areas being dynamic. We thendirectly rectify the erroneous cost volume for dynamic areas through aProbabilistic Cost Volume Modulation (PCVM) module. Specifically, we deriveprobability distributions of depth candidates from both single-frame andmulti-frame cues, modulating the cost volume by adaptively fusing thosedistributions based on the inferred uncertainty. Additionally, we present aself-supervision loss reweighting strategy that not only masks out incorrectsupervision with high uncertainty but also mitigates the risks in remainingpossible dynamic areas in accordance with the probability. Our proposed methodexcels over state-of-the-art approaches in all metrics on both Cityscapes andKITTI datasets, and demonstrates superior generalization ability on the WaymoOpen dataset.</description><author>Sungmin Woo, Wonjoon Lee, Woo Jin Kim, Dogyoon Lee, Sangyoun Lee</author><pubDate>Fri, 12 Jul 2024 14:37:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09303v1</guid></item><item><title>Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs</title><link>http://arxiv.org/abs/2407.07775v2</link><description>An elusive goal in navigation research is to build an intelligent agent thatcan understand multimodal instructions including natural language and image,and perform useful navigation. To achieve this, we study a widely usefulcategory of navigation tasks we call Multimodal Instruction Navigation withdemonstration Tours (MINT), in which the environment prior is provided througha previously recorded demonstration video. Recent advances in Vision LanguageModels (VLMs) have shown a promising path in achieving this goal as itdemonstrates capabilities in perceiving and reasoning about multimodal inputs.However, VLMs are typically trained to predict textual output and it is an openresearch question about how to best utilize them in navigation. To solve MINT,we present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigationpolicy that combines the environment understanding and common sense reasoningpower of long-context VLMs and a robust low-level navigation policy based ontopological graphs. The high-level policy consists of a long-context VLM thattakes the demonstration tour video and the multimodal user instruction as inputto find the goal frame in the tour video. Next, a low-level policy uses thegoal frame and an offline constructed topological graph to generate robotactions at every timestep. We evaluated Mobility VLA in a 836m^2 real worldenvironment and show that Mobility VLA has a high end-to-end success rates onpreviously unsolved multimodal instructions such as "Where should I returnthis?" while holding a plastic bin. A video demonstrating Mobility VLA can befound here: https://youtu.be/-Tof__Q8_5s</description><author>Hao-Tien Lewis Chiang, Zhuo Xu, Zipeng Fu, Mithun George Jacob, Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, Fei Xia, Jasmine Hsu, Jonathan Hoech, Pete Florence, Sean Kirmani, Sumeet Singh, Vikas Sindhwani, Carolina Parada, Chelsea Finn, Peng Xu, Sergey Levine, Jie Tan</author><pubDate>Fri, 12 Jul 2024 14:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07775v2</guid></item><item><title>PID: Physics-Informed Diffusion Model for Infrared Image Generation</title><link>http://arxiv.org/abs/2407.09299v1</link><description>Infrared imaging technology has gained significant attention for its reliablesensing ability in low visibility conditions, prompting many studies to convertthe abundant RGB images to infrared images. However, most existing imagetranslation methods treat infrared images as a stylistic variation, neglectingthe underlying physical laws, which limits their practical application. Toaddress these issues, we propose a Physics-Informed Diffusion (PID) model fortranslating RGB images to infrared images that adhere to physical laws. Ourmethod leverages the iterative optimization of the diffusion model andincorporates strong physical constraints based on prior knowledge of infraredlaws during training. This approach enhances the similarity between translatedinfrared images and the real infrared domain without increasing extra trainingparameters. Experimental results demonstrate that PID significantly outperformsexisting state-of-the-art methods. Our code is available athttps://github.com/fangyuanmao/PID.</description><author>Fangyuan Mao, Jilin Mei, Shun Lu, Fuyang Liu, Liang Chen, Fangzhou Zhao, Yu Hu</author><pubDate>Fri, 12 Jul 2024 14:32:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09299v1</guid></item><item><title>Structural Design Through Reinforcement Learning</title><link>http://arxiv.org/abs/2407.07288v2</link><description>This paper introduces the Structural Optimization gym (SOgym), a novelopen-source Reinforcement Learning (RL) environment designed to advance machinelearning in Topology Optimization (TO). SOgym enables RL agents to generatephysically viable and structurally robust designs by integrating the physics ofTO into the reward function. To enhance scalability, SOgym leveragesfeature-mapping methods as a mesh-independent interface between the environmentand the agent, allowing efficient interaction with the design variablesregardless of mesh resolution. Baseline results use a model-free ProximalPolicy Optimization agent and a model-based DreamerV3 agent. Three observationspace configurations were tested. The TopOpt game-inspired configuration, aninteractive educational tool that improves students' intuition in designingstructures to minimize compliance under volume constraints, performed best interms of performance and sample efficiency. The 100M parameter version ofDreamerV3 produced structures within 54% of the baseline compliance achieved bytraditional optimization methods and a 0% disconnection rate, an improvementover supervised learning approaches that often struggle with disconnected loadpaths. When comparing the learning rates of the agents to those of engineeringstudents from the TopOpt game experiment, the DreamerV3-100M model shows alearning rate approximately four orders of magnitude lower, an impressive featfor a policy trained from scratch through trial and error. These resultssuggest RL's potential to solve continuous TO problems and its capacity toexplore and learn from diverse design solutions. SOgym provides a platform fordeveloping RL agents for complex structural design challenges and is publiclyavailable to support further research in the field.</description><author>Thomas Rochefort-Beaudoin, Aurelian Vadean, Niels Aage, Sofiane Achiche</author><pubDate>Fri, 12 Jul 2024 14:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07288v2</guid></item><item><title>Transformer Layers as Painters</title><link>http://arxiv.org/abs/2407.09298v1</link><description>Despite their nearly universal adoption for large language models, theinternal workings of transformers are not well understood. We aim to betterunderstand the impact of removing or reorganizing information throughout thelayers of a pretrained transformer. Such an understanding could both yieldbetter usage of existing models as well as to make architectural improvementsto produce new variants. We present a series of empirical studies on frozenmodels that show that the lower and final layers of pretrained transformersdiffer from middle layers, but that middle layers have a surprising amount ofuniformity. We further show that some classes of problems have robustness toskipping layers, running the layers in an order different from how they weretrained, or running the layers in parallel. Our observations suggest that evenfrozen pretrained models may gracefully trade accuracy for latency by skippinglayers or running layers in parallel.</description><author>Qi Sun, Marc Pickett, Aakash Kumar Nain, Llion Jones</author><pubDate>Fri, 12 Jul 2024 14:31:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09298v1</guid></item><item><title>Learning Distances from Data with Normalizing Flows and Score Matching</title><link>http://arxiv.org/abs/2407.09297v1</link><description>Density-based distances (DBDs) offer an elegant solution to the problem ofmetric learning. By defining a Riemannian metric which increases withdecreasing probability density, shortest paths naturally follow the datamanifold and points are clustered according to the modes of the data. We showthat existing methods to estimate Fermat distances, a particular choice of DBD,suffer from poor convergence in both low and high dimensions due to i)inaccurate density estimates and ii) reliance on graph-based paths which areincreasingly rough in high dimensions. To address these issues, we proposelearning the densities using a normalizing flow, a generative model withtractable density estimation, and employing a smooth relaxation method using ascore model initialized from a graph-based proposal. Additionally, we introducea dimension-adapted Fermat distance that exhibits more intuitive behavior whenscaled to high dimensions and offers better numerical properties. Our workpaves the way for practical use of density-based distances, especially inhigh-dimensional spaces.</description><author>Peter Sorrenson, Daniel Behrend-Uriarte, Christoph Schnörr, Ullrich Köthe</author><pubDate>Fri, 12 Jul 2024 14:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09297v1</guid></item><item><title>SS-SfP:Neural Inverse Rendering for Self Supervised Shape from (Mixed) Polarization</title><link>http://arxiv.org/abs/2407.09294v1</link><description>We present a novel inverse rendering-based framework to estimate the 3D shape(per-pixel surface normals and depth) of objects and scenes from single-viewpolarization images, the problem popularly known as Shape from Polarization(SfP). The existing physics-based and learning-based methods for SfP performunder certain restrictions, i.e., (a) purely diffuse or purely specularreflections, which are seldom in the real surfaces, (b) availability of theground truth surface normals for direct supervision that are hard to acquireand are limited by the scanner's resolution, and (c) known refractive index. Toovercome these restrictions, we start by learning to separate thepartially-polarized diffuse and specular reflection components, which we callreflectance cues, based on a modified polarization reflection model and thenestimate shape under mixed polarization through an inverse-rendering basedself-supervised deep learning framework called SS-SfP, guided by thepolarization data and estimated reflectance cues. Furthermore, we also obtainthe refractive index as a non-linear least squares solution. Through extensivequantitative and qualitative evaluation, we establish the efficacy of theproposed framework over simple single-object scenes from DeepSfP dataset andcomplex in-the-wild scenes from SPW dataset in an entirely self-supervisedsetting. To the best of our knowledge, this is the first learning-basedapproach to address SfP under mixed polarization in a completelyself-supervised framework.</description><author>Ashish Tiwari, Shanmuganathan Raman</author><pubDate>Fri, 12 Jul 2024 14:29:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09294v1</guid></item><item><title>Do Multi-Document Summarization Models Synthesize?</title><link>http://arxiv.org/abs/2301.13844v2</link><description>Multi-document summarization entails producing concise synopses ofcollections of inputs. For some applications, the synopsis should accuratelysynthesize inputs with respect to a key aspect, e.g., a synopsis of filmreviews written about a particular movie should reflect the average criticconsensus. As a more consequential example, narrative summaries that accompanybiomedical systematic reviews of clinical trial results should accuratelysummarize the potentially conflicting results from individual trials. In thispaper we ask: To what extent do modern multi-document summarization modelsimplicitly perform this sort of synthesis? We run experiments over opinion andevidence synthesis datasets using a suite of summarization models, fromfine-tuned transformers to GPT-4. We find that existing models partiallyperform synthesis, but imperfectly: even the best performing models areover-sensitive to changes in input ordering and under-sensitive to changes ininput compositions (e.g., ratio of positive to negative reviews). We propose asimple, general, effective method for improving model synthesis capabilities bygenerating an explicitly diverse set of candidate outputs, and then selectingfrom these the string best aligned with the expected aggregate measure for theinputs, or abstaining when the model produces no good candidate.</description><author>Jay DeYoung, Stephanie C. Martinez, Iain J. Marshall, Byron C. Wallace</author><pubDate>Fri, 12 Jul 2024 14:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13844v2</guid></item><item><title>WSESeg: Introducing a Dataset for the Segmentation of Winter Sports Equipment with a Baseline for Interactive Segmentation</title><link>http://arxiv.org/abs/2407.09288v1</link><description>In this paper we introduce a new dataset containing instance segmentationmasks for ten different categories of winter sports equipment, called WSESeg(Winter Sports Equipment Segmentation). Furthermore, we carry out interactivesegmentation experiments on said dataset to explore possibilities for efficientfurther labeling. The SAM and HQ-SAM models are conceptualized as foundationmodels for performing user guided segmentation. In order to measure theirclaimed generalization capability we evaluate them on WSESeg. Since interactivesegmentation offers the benefit of creating easily exploitable ground truthdata during test-time, we are going to test various online adaptation methodsfor the purpose of exploring potentials for improvements without having tofine-tune the models explicitly. Our experiments show that our adaptationmethods drastically reduce the Failure Rate (FR) and Number of Clicks (NoC)metrics, which generally leads faster to better interactive segmentationresults.</description><author>Robin Schön, Daniel Kienzle, Rainer Lienhart</author><pubDate>Fri, 12 Jul 2024 14:20:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09288v1</guid></item><item><title>Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments</title><link>http://arxiv.org/abs/2407.09287v1</link><description>In this study, we address the issue of enabling an artificial intelligenceagent to execute complex language instructions within virtual environments. Inour framework, we assume that these instructions involve intricate linguisticstructures and multiple interdependent tasks that must be navigatedsuccessfully to achieve the desired outcomes. To effectively manage thesecomplexities, we propose a hierarchical framework that combines the deeplanguage comprehension of large language models with the adaptiveaction-execution capabilities of reinforcement learning agents. The languagemodule (based on LLM) translates the language instruction into a high-levelaction plan, which is then executed by a pre-trained reinforcement learningagent. We have demonstrated the effectiveness of our approach in two differentenvironments: in IGLU, where agents are instructed to build structures, and inCrafter, where agents perform tasks and interact with objects in thesurrounding environment according to language commands.</description><author>Zoya Volovikova, Alexey Skrynnik, Petr Kuderov, Aleksandr I. Panov</author><pubDate>Fri, 12 Jul 2024 14:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09287v1</guid></item><item><title>MetaFood CVPR 2024 Challenge on Physically Informed 3D Food Reconstruction: Methods and Results</title><link>http://arxiv.org/abs/2407.09285v1</link><description>The increasing interest in computer vision applications for nutrition anddietary monitoring has led to the development of advanced 3D reconstructiontechniques for food items. However, the scarcity of high-quality data andlimited collaboration between industry and academia have constrained progressin this field. Building on recent advancements in 3D reconstruction, we hostthe MetaFood Workshop and its challenge for Physically Informed 3D FoodReconstruction. This challenge focuses on reconstructing volume-accurate 3Dmodels of food items from 2D images, using a visible checkerboard as a sizereference. Participants were tasked with reconstructing 3D models for 20selected food items of varying difficulty levels: easy, medium, and hard. Theeasy level provides 200 images, the medium level provides 30 images, and thehard level provides only 1 image for reconstruction. In total, 16 teamssubmitted results in the final testing phase. The solutions developed in thischallenge achieved promising results in 3D food reconstruction, withsignificant potential for improving portion estimation for dietary assessmentand nutritional monitoring. More details about this workshop challenge andaccess to the dataset can be found athttps://sites.google.com/view/cvpr-metafood-2024.</description><author>Jiangpeng He, Yuhao Chen, Gautham Vinod, Talha Ibn Mahmud, Fengqing Zhu, Edward Delp, Alexander Wong, Pengcheng Xi, Ahmad AlMughrabi, Umair Haroon, Ricardo Marques, Petia Radeva, Jiadong Tang, Dianyi Yang, Yu Gao, Zhaoxiang Liang, Yawei Jueluo, Chengyu Shi, Pengyu Wang</author><pubDate>Fri, 12 Jul 2024 14:15:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09285v1</guid></item></channel></rss>