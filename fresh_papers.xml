<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 06 Sep 2023 06:00:33 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction</title><link>http://arxiv.org/abs/2309.02436v1</link><description>Neural implicit representations have recently demonstrated compelling resultson dense Simultaneous Localization And Mapping (SLAM) but suffer from theaccumulation of errors in camera tracking and distortion in the reconstruction.Purposely, we present GO-SLAM, a deep-learning-based dense visual SLAMframework globally optimizing poses and 3D reconstruction in real-time. Robustpose estimation is at its core, supported by efficient loop closing and onlinefull bundle adjustment, which optimize per frame by utilizing the learnedglobal geometry of the complete history of input frames. Simultaneously, weupdate the implicit and continuous surface representation on-the-fly to ensureglobal consistency of 3D reconstruction. Results on various synthetic andreal-world datasets demonstrate that GO-SLAM outperforms state-of-the-artapproaches at tracking robustness and reconstruction accuracy. Furthermore,GO-SLAM is versatile and can run with monocular, stereo, and RGB-D input.</description><author>Youmin Zhang, Fabio Tosi, Stefano Mattoccia, Matteo Poggi</author><pubDate>Tue, 05 Sep 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02436v1</guid></item><item><title>Efficient RL via Disentangled Environment and Agent Representations</title><link>http://arxiv.org/abs/2309.02435v1</link><description>Agents that are aware of the separation between themselves and theirenvironments can leverage this understanding to form effective representationsof visual input. We propose an approach for learning such structuredrepresentations for RL algorithms, using visual knowledge of the agent, such asits shape or mask, which is often inexpensive to obtain. This is incorporatedinto the RL objective using a simple auxiliary loss. We show that our method,Structured Environment-Agent Representations, outperforms state-of-the-artmodel-free approaches over 18 different challenging visual simulationenvironments spanning 5 different robots. Website at https://sear-rl.github.io/</description><author>Kevin Gmelin, Shikhar Bahl, Russell Mendonca, Deepak Pathak</author><pubDate>Tue, 05 Sep 2023 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02435v1</guid></item><item><title>ReliTalk: Relightable Talking Portrait Generation from a Single Video</title><link>http://arxiv.org/abs/2309.02434v1</link><description>Recent years have witnessed great progress in creating vivid audio-drivenportraits from monocular videos. However, how to seamlessly adapt the createdvideo avatars to other scenarios with different backgrounds and lightingconditions remains unsolved. On the other hand, existing relighting studiesmostly rely on dynamically lighted or multi-view data, which are too expensivefor creating video portraits. To bridge this gap, we propose ReliTalk, a novelframework for relightable audio-driven talking portrait generation frommonocular videos. Our key insight is to decompose the portrait's reflectancefrom implicitly learned audio-driven facial normals and images. Specifically,we involve 3D facial priors derived from audio features to predict delicatenormal maps through implicit functions. These initially predicted normals thentake a crucial part in reflectance decomposition by dynamically estimating thelighting condition of the given video. Moreover, the stereoscopic facerepresentation is refined using the identity-consistent loss under simulatedmultiple lighting conditions, addressing the ill-posed problem caused bylimited views available from a single monocular video. Extensive experimentsvalidate the superiority of our proposed framework on both real and syntheticdatasets. Our code is released in https://github.com/arthur-qiu/ReliTalk.</description><author>Haonan Qiu, Zhaoxi Chen, Yuming Jiang, Hang Zhou, Xiangyu Fan, Lei Yang, Wayne Wu, Ziwei Liu</author><pubDate>Tue, 05 Sep 2023 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02434v1</guid></item><item><title>Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach</title><link>http://arxiv.org/abs/2309.02429v1</link><description>Estimating the transferability of publicly available pretrained models to atarget task has assumed an important place for transfer learning tasks inrecent years. Existing efforts propose metrics that allow a user to choose onemodel from a pool of pre-trained models without having to fine-tune each modelindividually and identify one explicitly. With the growth in the number ofavailable pre-trained models and the popularity of model ensembles, it alsobecomes essential to study the transferability of multiple-source models for agiven target task. The few existing efforts study transferability in suchmulti-source ensemble settings using just the outputs of the classificationlayer and neglect possible domain or task mismatch. Moreover, they overlook themost important factor while selecting the source models, viz., the cohesivenessfactor between them, which can impact the performance and confidence in theprediction of the ensemble. To address these gaps, we propose a novel OptimaltranSport-based suBmOdular tRaNsferability metric (OSBORN) to estimate thetransferability of an ensemble of models to a downstream task. OSBORNcollectively accounts for image domain difference, task difference, andcohesiveness of models in the ensemble to provide reliable estimates oftransferability. We gauge the performance of OSBORN on both imageclassification and semantic segmentation tasks. Our setup includes 28 sourcedatasets, 11 target datasets, 5 model architectures, and 2 pre-trainingmethods. We benchmark our method against current state-of-the-art metricsMS-LEEP and E-LEEP, and outperform them consistently using the proposedapproach.</description><author>Vimal K B, Saketh Bachu, Tanmay Garg, Niveditha Lakshmi Narasimhan, Raghavan Konuru, Vineeth N Balasubramanian</author><pubDate>Tue, 05 Sep 2023 18:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02429v1</guid></item><item><title>V1T: large-scale mouse V1 response prediction using a Vision Transformer</title><link>http://arxiv.org/abs/2302.03023v4</link><description>Accurate predictive models of the visual cortex neural response to naturalvisual stimuli remain a challenge in computational neuroscience. In this work,we introduce V1T, a novel Vision Transformer based architecture that learns ashared visual and behavioral representation across animals. We evaluate ourmodel on two large datasets recorded from mouse primary visual cortex andoutperform previous convolution-based models by more than 12.7% in predictionperformance. Moreover, we show that the self-attention weights learned by theTransformer correlate with the population receptive fields. Our model thus setsa new benchmark for neural response prediction and can be used jointly withbehavioral and neural recordings to reveal meaningful characteristic featuresof the visual cortex.</description><author>Bryan M. Li, Isabel M. Cornacchia, Nathalie L. Rochefort, Arno Onken</author><pubDate>Tue, 05 Sep 2023 18:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03023v4</guid></item><item><title>Tensorization: Creating and Utilising Multidimensional Datasets for Multiway Analysis and Tensorised Deep Neural Networks -- Python Tutorial and Survey</title><link>http://arxiv.org/abs/2309.02428v1</link><description>As the size and complexity of data continue to increase, the need forefficient and effective analysis methods becomes ever more crucial.Tensorization, the process of converting 2-dimensional datasets intomultidimensional structures, has emerged as a promising approach for multiwayanalysis methods. This paper explores the steps involved in tensorization,multidimensional data sources, various multiway analysis methods employed, andthe benefits of these approaches. A small example of Blind Source Separation(BSS) is presented comparing 2-dimensional algorithms and a multiway algorithmin Python. Results indicate that multiway analysis is more expressive.Additionally, tensorization techniques aid in compressing deep learning modelsby reducing the number of required parameters while enhancing the expression ofrelationships across dimensions. A survey of the multi-away analysis methodsand integration with various Deep Neural Networks models is presented usingcase studies in different domains.</description><author>Manal Helal</author><pubDate>Tue, 05 Sep 2023 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02428v1</guid></item><item><title>Cognitive Architectures for Language Agents</title><link>http://arxiv.org/abs/2309.02427v1</link><description>Recent efforts have incorporated large language models (LLMs) with externalresources (e.g., the Internet) or internal control flows (e.g., promptchaining) for tasks requiring grounding or reasoning. However, these effortshave largely been piecemeal, lacking a systematic framework for constructing afully-fledged language agent. To address this challenge, we draw on the richhistory of agent design in symbolic artificial intelligence to develop ablueprint for a new wave of cognitive language agents. We first show that LLMshave many of the same properties as production systems, and recent efforts toimprove their grounding or reasoning mirror the development of cognitivearchitectures built around production systems. We then propose CognitiveArchitectures for Language Agents (CoALA), a conceptual framework tosystematize diverse methods for LLM-based reasoning, grounding, learning, anddecision making as instantiations of language agents in the framework. Finally,we use the CoALA framework to highlight gaps and propose actionable directionstoward more capable language agents in the future.</description><author>Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths</author><pubDate>Tue, 05 Sep 2023 18:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02427v1</guid></item><item><title>Monotone Tree-Based GAMI Models by Adapting XGBoost</title><link>http://arxiv.org/abs/2309.02426v1</link><description>Recent papers have used machine learning architecture to fit low-orderfunctional ANOVA models with main effects and second-order interactions. TheseGAMI (GAM + Interaction) models are directly interpretable as the functionalmain effects and interactions can be easily plotted and visualized.Unfortunately, it is not easy to incorporate the monotonicity requirement intothe existing GAMI models based on boosted trees, such as EBM (Lou et al. 2013)and GAMI-Lin-T (Hu et al. 2022). This paper considers models of the form$f(x)=\sum_{j,k}f_{j,k}(x_j, x_k)$ and develops monotone tree-based GAMImodels, called monotone GAMI-Tree, by adapting the XGBoost algorithm. It isstraightforward to fit a monotone model to $f(x)$ using the options in XGBoost.However, the fitted model is still a black box. We take a different approach:i) use a filtering technique to determine the important interactions, ii) fit amonotone XGBoost algorithm with the selected interactions, and finally iii)parse and purify the results to get a monotone GAMI model. Simulated datasetsare used to demonstrate the behaviors of mono-GAMI-Tree and EBM, both of whichuse piecewise constant fits. Note that the monotonicity requirement is for thefull model. Under certain situations, the main effects will also be monotone.But, as seen in the examples, the interactions will not be monotone.</description><author>Linwei Hu, Soroush Aramideh, Jie Chen, Vijayan N. Nair</author><pubDate>Tue, 05 Sep 2023 18:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02426v1</guid></item><item><title>On the Minimax Regret in Online Ranking with Top-k Feedback</title><link>http://arxiv.org/abs/2309.02425v1</link><description>In online ranking, a learning algorithm sequentially ranks a set of items andreceives feedback on its ranking in the form of relevance scores. Sinceobtaining relevance scores typically involves human annotation, it is of greatinterest to consider a partial feedback setting where feedback is restricted tothe top-$k$ items in the rankings. Chaudhuri and Tewari [2017] developed aframework to analyze online ranking algorithms with top $k$ feedback. A keyelement in their work was the use of techniques from partial monitoring. Inthis paper, we further investigate online ranking with top $k$ feedback andsolve some open problems posed by Chaudhuri and Tewari [2017]. We provide afull characterization of minimax regret rates with the top $k$ feedback modelfor all $k$ and for the following ranking performance measures: Pairwise Loss,Discounted Cumulative Gain, and Precision@n. In addition, we give an efficientalgorithm that achieves the minimax regret rate for Precision@n.</description><author>Mingyuan Zhang, Ambuj Tewari</author><pubDate>Tue, 05 Sep 2023 18:53:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02425v1</guid></item><item><title>Colonoscopy 3D Video Dataset with Paired Depth from 2D-3D Registration</title><link>http://arxiv.org/abs/2206.08903v3</link><description>Screening colonoscopy is an important clinical application for several 3Dcomputer vision techniques, including depth estimation, surface reconstruction,and missing region detection. However, the development, evaluation, andcomparison of these techniques in real colonoscopy videos remain largelyqualitative due to the difficulty of acquiring ground truth data. In this work,we present a Colonoscopy 3D Video Dataset (C3VD) acquired with a highdefinition clinical colonoscope and high-fidelity colon models for benchmarkingcomputer vision methods in colonoscopy. We introduce a novel multimodal 2D-3Dregistration technique to register optical video sequences with ground truthrendered views of a known 3D model. The different modalities are registered bytransforming optical images to depth maps with a Generative Adversarial Networkand aligning edge features with an evolutionary optimizer. This registrationmethod achieves an average translation error of 0.321 millimeters and anaverage rotation error of 0.159 degrees in simulation experiments whereerror-free ground truth is available. The method also leverages videoinformation, improving registration accuracy by 55.6% for translation and 60.4%for rotation compared to single frame registration. 22 short video sequenceswere registered to generate 10,015 total frames with paired ground truth depth,surface normals, optical flow, occlusion, six degree-of-freedom pose, coveragemaps, and 3D models. The dataset also includes screening videos acquired by agastroenterologist with paired ground truth pose and 3D surface models. Thedataset and registration source code are available at durr.jhu.edu/C3VD.</description><author>Taylor L. Bobrow, Mayank Golhar, Rohan Vijayan, Venkata S. Akshintala, Juan R. Garcia, Nicholas J. Durr</author><pubDate>Tue, 05 Sep 2023 18:51:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08903v3</guid></item><item><title>EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding</title><link>http://arxiv.org/abs/2309.02423v1</link><description>With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI),large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed.However, most current research is built on resources derived from third-personvideo action recognition. This inherent domain gap between first- andthird-person action videos, which have not been adequately addressed before,makes current Ego-HOI suboptimal. This paper rethinks and proposes a newframework as an infrastructure to advance Ego-HOI recognition by Probing,Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets,balanced test sets and a new baseline, which are complete with atraining-finetuning strategy. With our new framework, we not only achievestate-of-the-art performance on Ego-HOI benchmarks but also build several newand effective mechanisms and settings to advance further research. We believeour data and the findings will pave a new way for Ego-HOI understanding. Codeand data are available at https://mvig-rhos.com/ego_pca</description><author>Yue Xu, Yong-Lu Li, Zhemin Huang, Michael Xu Liu, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang</author><pubDate>Tue, 05 Sep 2023 18:51:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02423v1</guid></item><item><title>BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge</title><link>http://arxiv.org/abs/2308.16458v2</link><description>Pre-trained language models like ChatGPT have significantly improved codegeneration. As these models scale up, there is an increasing need for theoutput to handle more intricate tasks. Moreover, in bioinformatics, generatingfunctional programs poses additional notable challenges due to the amount ofdomain knowledge, the need for complicated data operations, and intricatefunctional dependencies between the operations. Here, we present BioCoder, abenchmark developed to evaluate existing pre-trained models in generatingbioinformatics code. In relation to function-code generation, BioCoder coverspotential package dependencies, class declarations, and global variables. Itincorporates 1026 functions and 1243 methods in Python and Java from GitHub and253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testingframework for evaluation, and we have applied it to evaluate many modelsincluding InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+,InstructCodeT5+, and ChatGPT. Our detailed analysis of these models emphasizesthe importance of domain knowledge, pragmatic code generation, and contextualunderstanding. Our dataset, benchmark, Docker images, and scripts required fortesting are all available at https://github.com/gersteinlab/biocoder.</description><author>Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark Gerstein</author><pubDate>Tue, 05 Sep 2023 18:51:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16458v2</guid></item><item><title>Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test</title><link>http://arxiv.org/abs/2309.02422v1</link><description>Maximum mean discrepancy (MMD) refers to a general class of nonparametrictwo-sample tests that are based on maximizing the mean difference over samplesfrom one distribution $P$ versus another $Q$, over all choices of datatransformations $f$ living in some function space $\mathcal{F}$. Inspired byrecent work that connects what are known as functions of $\textit{Radon boundedvariation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we studythe MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space ofa given smoothness order $k \geq 0$. This test, which we refer to as the$\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as ageneralization of the well-known and classical Kolmogorov-Smirnov (KS) test tomultiple dimensions and higher orders of smoothness. It is also intimatelyconnected to neural networks: we prove that the witness in the RKS test -- thefunction $f$ achieving the maximum mean difference -- is always a ridge splineof degree $k$, i.e., a single neuron in a neural network. This allows us toleverage the power of modern deep learning toolkits to (approximately) optimizethe criterion that underlies the RKS test. We prove that the RKS test hasasymptotically full power at distinguishing any distinct pair $P \not= Q$ ofdistributions, derive its asymptotic null distribution, and carry out extensiveexperiments to elucidate the strengths and weakenesses of the RKS test versusthe more traditional kernel MMD test.</description><author>Seunghoon Paik, Michael Celentano, Alden Green, Ryan J. Tibshirani</author><pubDate>Tue, 05 Sep 2023 18:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02422v1</guid></item><item><title>Doppelgangers: Learning to Disambiguate Images of Similar Structures</title><link>http://arxiv.org/abs/2309.02420v1</link><description>We consider the visual disambiguation task of determining whether a pair ofvisually similar images depict the same or distinct 3D surfaces (e.g., the sameor opposite sides of a symmetric building). Illusory image matches, where twoimages observe distinct but visually similar 3D surfaces, can be challengingfor humans to differentiate, and can also lead 3D reconstruction algorithms toproduce erroneous results. We propose a learning-based approach to visualdisambiguation, formulating it as a binary classification task on image pairs.To that end, we introduce a new dataset for this problem, Doppelgangers, whichincludes image pairs of similar structures with ground truth labels. We alsodesign a network architecture that takes the spatial distribution of localkeypoints and matches as input, allowing for better reasoning about both localand global cues. Our evaluation shows that our method can distinguish illusorymatches in difficult cases, and can be integrated into SfM pipelines to producecorrect, disambiguated 3D reconstructions. See our project page for our code,datasets, and more results: http://doppelgangers-3d.github.io/.</description><author>Ruojin Cai, Joseph Tung, Qianqian Wang, Hadar Averbuch-Elor, Bharath Hariharan, Noah Snavely</author><pubDate>Tue, 05 Sep 2023 18:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02420v1</guid></item><item><title>Computing SHAP Efficiently Using Model Structure Information</title><link>http://arxiv.org/abs/2309.02417v1</link><description>SHAP (SHapley Additive exPlanations) has become a popular method to attributethe prediction of a machine learning model on an input to its features. Onemain challenge of SHAP is the computation time. An exact computation of Shapleyvalues requires exponential time complexity. Therefore, many approximationmethods are proposed in the literature. In this paper, we propose methods thatcan compute SHAP exactly in polynomial time or even faster for SHAP definitionsthat satisfy our additivity and dummy assumptions (eg, kernal SHAP and baselineSHAP). We develop different strategies for models with different levels ofmodel structure information: known functional decomposition, known order ofmodel (defined as highest order of interaction in the model), or unknown order.For the first case, we demonstrate an additive property and a way to computeSHAP from the lower-order functional components. For the second case, we deriveformulas that can compute SHAP in polynomial time. Both methods yield exactSHAP results. Finally, if even the order of model is unknown, we propose aniterative way to approximate Shapley values. The three methods we propose arecomputationally efficient when the order of model is not high which istypically the case in practice. We compare with sampling approach proposed inCastor &amp; Gomez (2008) using simulation studies to demonstrate the efficacy ofour proposed methods.</description><author>Linwei Hu, Ke Wang</author><pubDate>Tue, 05 Sep 2023 18:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02417v1</guid></item><item><title>First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians</title><link>http://arxiv.org/abs/2309.02412v1</link><description>In this work, we develop first-order (Hessian-free) and zero-order(derivative-free) implementations of the Cubically regularized Newton methodfor solving general non-convex optimization problems. For that, we employfinite difference approximations of the derivatives. We use a special adaptivesearch procedure in our algorithms, which simultaneously fits both theregularization constant and the parameters of the finite differenceapproximations. It makes our schemes free from the need to know the actualLipschitz constants. Additionally, we equip our algorithms with the lazyHessian update that reuse a previously computed Hessian approximation matrixfor several iterations. Specifically, we prove the global complexity bound of$\mathcal{O}( n^{1/2} \epsilon^{-3/2})$ function and gradient evaluations forour new Hessian-free method, and a bound of $\mathcal{O}( n^{3/2}\epsilon^{-3/2} )$ function evaluations for the derivative-free method, where$n$ is the dimension of the problem and $\epsilon$ is the desired accuracy forthe gradient norm. These complexity bounds significantly improve the previouslyknown ones in terms of the joint dependence on $n$ and $\epsilon$, for thefirst-order and zeroth-order non-convex optimization.</description><author>Nikita Doikov, Geovani Nunes Grapiglia</author><pubDate>Tue, 05 Sep 2023 18:40:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02412v1</guid></item><item><title>Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</title><link>http://arxiv.org/abs/2309.02411v1</link><description>In this paper, we present Delta-LoRA, which is a novel parameter-efficientapproach to fine-tune large language models (LLMs). In contrast to LoRA andother low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updatesthe low-rank matrices $\bA$ and $\bB$, but also propagate the learning to thepre-trained weights $\bW$ via updates utilizing the delta of the product of twolow-rank matrices ($\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$). Such astrategy effectively addresses the limitation that the incremental update oflow-rank matrices is inadequate for learning representations capable fordownstream tasks. Moreover, as the update of $\bW$ does not need to compute thegradients of $\bW$ and store their momentums, Delta-LoRA shares comparablememory requirements and computational costs with LoRA. Extensive experimentsshow that Delta-LoRA significantly outperforms existing low-rank adaptationmethods. We further support these results with comprehensive analyses thatunderscore the effectiveness of Delta-LoRA.</description><author>Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, Lei Zhang</author><pubDate>Tue, 05 Sep 2023 18:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02411v1</guid></item><item><title>Generating Realistic Images from In-the-wild Sounds</title><link>http://arxiv.org/abs/2309.02405v1</link><description>Representing wild sounds as images is an important but challenging task dueto the lack of paired datasets between sound and images and the significantdifferences in the characteristics of these two modalities. Previous studieshave focused on generating images from sound in limited categories or music. Inthis paper, we propose a novel approach to generate images from in-the-wildsounds. First, we convert sound into text using audio captioning. Second, wepropose audio attention and sentence attention to represent the richcharacteristics of sound and visualize the sound. Lastly, we propose a directsound optimization with CLIPscore and AudioCLIP and generate images with adiffusion-based model. In experiments, it shows that our model is able togenerate high quality images from wild sounds and outperforms baselines in bothquantitative and qualitative evaluations on wild audio datasets.</description><author>Taegyeong Lee, Jeonghun Kang, Hyeonyu Kim, Taehwan Kim</author><pubDate>Tue, 05 Sep 2023 18:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02405v1</guid></item><item><title>Voice Morphing: Two Identities in One Voice</title><link>http://arxiv.org/abs/2309.02404v1</link><description>In a biometric system, each biometric sample or template is typicallyassociated with a single identity. However, recent research has demonstratedthe possibility of generating "morph" biometric samples that can successfullymatch more than a single identity. Morph attacks are now recognized as apotential security threat to biometric systems. However, most morph attackshave been studied on biometric modalities operating in the image domain, suchas face, fingerprint, and iris. In this preliminary work, we introduce VoiceIdentity Morphing (VIM) - a voice-based morph attack that can synthesize speechsamples that impersonate the voice characteristics of a pair of individuals.Our experiments evaluate the vulnerabilities of two popular speaker recognitionsystems, ECAPA-TDNN and x-vector, to VIM, with a success rate (MMPMR) of over80% at a false match rate of 1% on the Librispeech dataset.</description><author>Sushanta K. Pani, Anurag Chowdhury, Morgan Sandler, Arun Ross</author><pubDate>Tue, 05 Sep 2023 18:36:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02404v1</guid></item><item><title>Substitution-based Semantic Change Detection using Contextual Embeddings</title><link>http://arxiv.org/abs/2309.02403v1</link><description>Measuring semantic change has thus far remained a task where methods usingcontextual embeddings have struggled to improve upon simpler techniques relyingonly on static word vectors. Moreover, many of the previously proposedapproaches suffer from downsides related to scalability and ease ofinterpretation. We present a simplified approach to measuring semantic changeusing contextual embeddings, relying only on the most probable substitutes formasked terms. Not only is this approach directly interpretable, it is also farmore efficient in terms of storage, achieves superior average performanceacross the most frequently cited datasets for this task, and allows for morenuanced investigation of change than is possible with static word vectors.</description><author>Dallas Card</author><pubDate>Tue, 05 Sep 2023 18:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02403v1</guid></item><item><title>Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis</title><link>http://arxiv.org/abs/2308.09830v2</link><description>This paper explores the integration of two AI subdisciplines employed in thedevelopment of artificial agents that exhibit intelligent behavior: LargeLanguage Models (LLMs) and Cognitive Architectures (CAs). We present threeintegration approaches, each grounded in theoretical models and supported bypreliminary empirical evidence. The modular approach, which introduces fourmodels with varying degrees of integration, makes use of chain-of-thoughtprompting, and draws inspiration from augmented LLMs, the Common Model ofCognition, and the simulation theory of cognition. The agency approach,motivated by the Society of Mind theory and the LIDA cognitive architecture,proposes the formation of agent collections that interact at micro and macrocognitive levels, driven by either LLMs or symbolic components. Theneuro-symbolic approach, which takes inspiration from the CLARION cognitivearchitecture, proposes a model where bottom-up learning extracts symbolicrepresentations from an LLM layer and top-down guidance utilizes symbolicrepresentations to direct prompt engineering in the LLM layer. These approachesaim to harness the strengths of both LLMs and CAs, while mitigating theirweaknesses, thereby advancing the development of more robust AI systems. Wediscuss the tradeoffs and challenges associated with each approach.</description><author>Oscar J. Romero, John Zimmerman, Aaron Steinfeld, Anthony Tomasic</author><pubDate>Tue, 05 Sep 2023 18:32:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09830v2</guid></item><item><title>Prototype-based Dataset Comparison</title><link>http://arxiv.org/abs/2309.02401v1</link><description>Dataset summarisation is a fruitful approach to dataset inspection. However,when applied to a single dataset the discovery of visual concepts is restrictedto those most prominent. We argue that a comparative approach can expand uponthis paradigm to enable richer forms of dataset inspection that go beyond themost prominent concepts. To enable dataset comparison we present a module thatlearns concept-level prototypes across datasets. We leverage self-supervisedlearning to discover these prototypes without supervision, and we demonstratethe benefits of our approach in two case-studies. Our findings show thatdataset comparison extends dataset inspection and we hope to encourage moreworks in this direction. Code and usage instructions available athttps://github.com/Nanne/ProtoSim</description><author>Nanne van Noord</author><pubDate>Tue, 05 Sep 2023 18:27:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02401v1</guid></item><item><title>Towards Human-centered Explainable AI: A Survey of User Studies for Model Explanations</title><link>http://arxiv.org/abs/2210.11584v3</link><description>Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AIresearch. A better understanding of the needs of XAI users, as well ashuman-centered evaluations of explainable models are both a necessity and achallenge. In this paper, we explore how HCI and AI researchers conduct userstudies in XAI applications based on a systematic literature review. Afteridentifying and thoroughly analyzing 97core papers with human-based XAIevaluations over the past five years, we categorize them along the measuredcharacteristics of explanatory methods, namely trust, understanding, usability,and human-AI collaboration performance. Our research shows that XAI isspreading more rapidly in certain application domains, such as recommendersystems than in others, but that user evaluations are still rather sparse andincorporate hardly any insights from cognitive or social sciences. Based on acomprehensive discussion of best practices, i.e., common models, designchoices, and measures in user studies, we propose practical guidelines ondesigning and conducting user studies for XAI researchers and practitioners.Lastly, this survey also highlights several open research directions,particularly linking psychological science and human-centered XAI.</description><author>Yao Rong, Tobias Leemann, Thai-trang Nguyen, Lisa Fiedler, Peizhu Qian, Vaibhav Unhelkar, Tina Seidel, Gjergji Kasneci, Enkelejda Kasneci</author><pubDate>Tue, 05 Sep 2023 18:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.11584v3</guid></item><item><title>zPROBE: Zero Peek Robustness Checks for Federated Learning</title><link>http://arxiv.org/abs/2206.12100v3</link><description>Privacy-preserving federated learning allows multiple users to jointly traina model with coordination of a central server. The server only learns the finalaggregation result, thus the users' (private) training data is not leaked fromthe individual model updates. However, keeping the individual updates privateallows malicious users to perform Byzantine attacks and degrade the accuracywithout being detected. Best existing defenses against Byzantine workers relyon robust rank-based statistics, e.g., median, to find malicious updates.However, implementing privacy-preserving rank-based statistics is nontrivialand not scalable in the secure domain, as it requires sorting all individualupdates. We establish the first private robustness check that uses high breakpoint rank-based statistics on aggregated model updates. By exploitingrandomized clustering, we significantly improve the scalability of our defensewithout compromising privacy. We leverage our statistical bounds inzero-knowledge proofs to detect and remove malicious updates without revealingthe private user updates. Our novel framework, zPROBE, enables Byzantineresilient and secure federated learning. Empirical evaluations demonstrate thatzPROBE provides a low overhead solution to defend against state-of-the-artByzantine attacks while preserving privacy.</description><author>Zahra Ghodsi, Mojan Javaheripi, Nojan Sheybani, Xinqiao Zhang, Ke Huang, Farinaz Koushanfar</author><pubDate>Tue, 05 Sep 2023 18:14:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12100v3</guid></item><item><title>In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms</title><link>http://arxiv.org/abs/2309.02393v1</link><description>The recent ubiquitous adoption of remote conferencing has been accompanied byomnipresent frustration with distorted or otherwise unclear voicecommunication. Audio enhancement can compensate for low-quality input signalsfrom, for example, small true wireless earbuds, by applying noise suppressiontechniques. Such processing relies on voice activity detection (VAD) with lowlatency and the added capability of discriminating the wearer's voice fromothers - a task of significant computational complexity. The tight energybudget of devices as small as modern earphones, however, requires any systemattempting to tackle this problem to do so with minimal power and processingoverhead, while not relying on speaker-specific voice samples and training dueto usability concerns. This paper presents the design and implementation of a custom researchplatform for low-power wireless earbuds based on novel, commercial, MEMSbone-conduction microphones. Such microphones can record the wearer's speechwith much greater isolation, enabling personalized voice activity detection andfurther audio enhancement applications. Furthermore, the paper accuratelyevaluates a proposed low-power personalized speech detection algorithm based onbone conduction data and a recurrent neural network running on the implementedresearch platform. This algorithm is compared to an approach based ontraditional microphone input. The performance of the bone conduction system,achieving detection of speech within 12.8ms at an accuracy of 95\% isevaluated. Different SoC choices are contrasted, with the final implementationbased on the cutting-edge Ambiq Apollo 4 Blue SoC achieving 2.64mW averagepower consumption at 14uJ per inference, reaching 43h of battery life on aminiature 32mAh li-ion cell and without duty cycling.</description><author>Philipp Schilk, Niccolò Polvani, Andrea Ronco, Milos Cernak, Michele Magno</author><pubDate>Tue, 05 Sep 2023 18:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02393v1</guid></item><item><title>Explaining grokking through circuit efficiency</title><link>http://arxiv.org/abs/2309.02390v1</link><description>One of the most surprising puzzles in neural network generalisation isgrokking: a network with perfect training accuracy but poor generalisationwill, upon further training, transition to perfect generalisation. We proposethat grokking occurs when the task admits a generalising solution and amemorising solution, where the generalising solution is slower to learn butmore efficient, producing larger logits with the same parameter norm. Wehypothesise that memorising circuits become more inefficient with largertraining datasets while generalising circuits do not, suggesting there is acritical dataset size at which memorisation and generalisation are equallyefficient. We make and confirm four novel predictions about grokking, providingsignificant evidence in favour of our explanation. Most strikingly, wedemonstrate two novel and surprising behaviours: ungrokking, in which a networkregresses from perfect to low test accuracy, and semi-grokking, in which anetwork shows delayed generalisation to partial rather than perfect testaccuracy.</description><author>Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, Ramana Kumar</author><pubDate>Tue, 05 Sep 2023 18:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02390v1</guid></item><item><title>Benchmarks for Detecting Measurement Tampering</title><link>http://arxiv.org/abs/2308.15605v2</link><description>When training powerful AI systems to perform complex tasks, it may bechallenging to provide training signals which are robust to optimization. Oneconcern is \textit{measurement tampering}, where the AI system manipulatesmultiple measurements to create the illusion of good results instead ofachieving the desired outcome. In this work, we build four new text-baseddatasets to evaluate measurement tampering detection techniques on largelanguage models. Concretely, given sets of text inputs and measurements aimedat determining if some outcome occurred, as well as a base model able toaccurately predict measurements, the goal is to determine if examples where allmeasurements indicate the outcome occurred actually had the outcome occur, orif this was caused by measurement tampering. We demonstrate techniques thatoutperform simple baselines on most datasets, but don't achieve maximumperformance. We believe there is significant room for improvement for bothtechniques and datasets, and we are excited for future work tacklingmeasurement tampering.</description><author>Fabien Roger, Ryan Greenblatt, Max Nadeau, Buck Shlegeris, Nate Thomas</author><pubDate>Tue, 05 Sep 2023 17:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15605v2</guid></item><item><title>nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources</title><link>http://arxiv.org/abs/2309.02373v1</link><description>State-of-the-art language models like T5 have revolutionized the NLPlandscape, but their computational demands hinder a large portion of theresearch community. To address this challenge, we present nanoT5, aspecially-optimized PyTorch framework for efficient pre-training andfine-tuning of T5 models. Drawing on insights from optimizer differences andprioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on asingle GPU in just 16 hours, without any loss in performance. With theintroduction of this open-source framework, we hope to widen the accessibilityto language modelling research and cater to the community's demand for moreuser-friendly T5 (Encoder-Decoder) implementations. Our contributions,including configurations, codebase, software/hardware insights, and pre-trainedmodels, are available to the public, aiming to strike a balance betweenresearch accessibility and resource constraints in NLP.</description><author>Piotr Nawrot</author><pubDate>Tue, 05 Sep 2023 17:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02373v1</guid></item><item><title>The star-shaped space of solutions of the spherical negative perceptron</title><link>http://arxiv.org/abs/2305.10623v2</link><description>Empirical studies on the landscape of neural networks have shown thatlow-energy configurations are often found in complex connected structures,where zero-energy paths between pairs of distant solutions can be constructed.Here we consider the spherical negative perceptron, a prototypical non-convexneural network model framed as a continuous constraint satisfaction problem. Weintroduce a general analytical method for computing energy barriers in thesimplex with vertex configurations sampled from the equilibrium. We find thatin the over-parameterized regime the solution manifold displays simpleconnectivity properties. There exists a large geodesically convex componentthat is attractive for a wide range of optimization dynamics. Inside thisregion we identify a subset of atypical high-margin solutions that aregeodesically connected with most other solutions, giving rise to a star-shapedgeometry. We analytically characterize the organization of the connected spaceof solutions and show numerical evidence of a transition, at larger constraintdensities, where the aforementioned simple geodesic connectivity breaks down.</description><author>Brandon Livio Annesi, Clarissa Lauditi, Carlo Lucibello, Enrico M. Malatesta, Gabriele Perugini, Fabrizio Pittorino, Luca Saglietti</author><pubDate>Tue, 05 Sep 2023 17:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10623v2</guid></item><item><title>Learning Efficient Abstract Planning Models that Choose What to Predict</title><link>http://arxiv.org/abs/2208.07737v3</link><description>An effective approach to solving long-horizon tasks in robotics domains withcontinuous state and action spaces is bilevel planning, wherein a high-levelsearch over an abstraction of an environment is used to guide low-leveldecision-making. Recent work has shown how to enable such bilevel planning bylearning abstract models in the form of symbolic operators and neural samplers.In this work, we show that existing symbolic operator learning approaches fallshort in many robotics domains where a robot's actions tend to cause a largenumber of irrelevant changes in the abstract state. This is primarily becausethey attempt to learn operators that exactly predict all observed changes inthe abstract state. To overcome this issue, we propose to learn operators that'choose what to predict' by only modelling changes necessary for abstractplanning to achieve specified goals. Experimentally, we show that our approachlearns operators that lead to efficient planning across 10 different hybridrobotics domains, including 4 from the challenging BEHAVIOR-100 benchmark,while generalizing to novel initial states, goals, and objects.</description><author>Nishanth Kumar, Willie McClinton, Rohan Chitnis, Tom Silver, Tomás Lozano-Pérez, Leslie Pack Kaelbling</author><pubDate>Tue, 05 Sep 2023 17:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07737v3</guid></item><item><title>The Descriptive Complexity of Graph Neural Networks</title><link>http://arxiv.org/abs/2303.04613v3</link><description>We analyse the power of graph neural networks (GNNs) in terms of Booleancircuit complexity and descriptive complexity. We prove that the graph queries that can be computed by a polynomial-sizebounded-depth family of GNNs are exactly those definable in the guardedfragment GFO+C of first-order logic with counting and with built-in relations.This puts GNNs in the circuit complexity class TC^0. Remarkably, the GNNfamilies may use arbitrary real weights and a wide class of activationfunctions that includes the standard ReLU, logistic "sigmod", and hyperbolictangent functions. If the GNNs are allowed to use random initialisation andglobal readout (both standard features of GNNs widely used in practice), theycan compute exactly the same queries as bounded depth Boolean circuits withthreshold gates, that is, exactly the queries in TC^0. Moreover, we show that queries computable by a single GNN with piecewiselinear activations and rational weights are definable in GFO+C without built-inrelations. Therefore, they are contained in uniform TC^0.</description><author>Martin Grohe</author><pubDate>Tue, 05 Sep 2023 17:22:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04613v3</guid></item><item><title>Two to Five Truths in Non-Negative Matrix Factorization</title><link>http://arxiv.org/abs/2305.05389v2</link><description>In this paper, we explore the role of matrix scaling on a matrix of countswhen building a topic model using non-negative matrix factorization. We presenta scaling inspired by the normalized Laplacian (NL) for graphs that can greatlyimprove the quality of a non-negative matrix factorization. The resultsparallel those in the spectral graph clustering work of \cite{Priebe:2019},where the authors proved adjacency spectral embedding (ASE) spectral clusteringwas more likely to discover core-periphery partitions and Laplacian SpectralEmbedding (LSE) was more likely to discover affinity partitions. In textanalysis non-negative matrix factorization (NMF) is typically used on a matrixof co-occurrence ``contexts'' and ``terms" counts. The matrix scaling inspiredby LSE gives significant improvement for text topic models in a variety ofdatasets. We illustrate the dramatic difference a matrix scalings in NMF cangreatly improve the quality of a topic model on three datasets where humanannotation is available. Using the adjusted Rand index (ARI), a measure clustersimilarity we see an increase of 50\% for Twitter data and over 200\% for anewsgroup dataset versus using counts, which is the analogue of ASE. For cleandata, such as those from the Document Understanding Conference, NL gives over40\% improvement over ASE. We conclude with some analysis of this phenomenonand some connections of this scaling with other matrix scaling methods.</description><author>John M. Conroy, Neil P Molino, Brian Baughman, Rod Gomez, Ryan Kaliszewski, Nicholas A. Lines</author><pubDate>Tue, 05 Sep 2023 17:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05389v2</guid></item><item><title>STEP -- Towards Structured Scene-Text Spotting</title><link>http://arxiv.org/abs/2309.02356v1</link><description>We introduce the structured scene-text spotting task, which requires ascene-text OCR system to spot text in the wild according to a query regularexpression. Contrary to generic scene text OCR, structured scene-text spottingseeks to dynamically condition both scene text detection and recognition onuser-provided regular expressions. To tackle this task, we propose theStructured TExt sPotter (STEP), a model that exploits the provided textstructure to guide the OCR process. STEP is able to deal with regularexpressions that contain spaces and it is not bound to detection at theword-level granularity. Our approach enables accurate zero-shot structured textspotting in a wide variety of real-world reading scenarios and is solelytrained on publicly available data. To demonstrate the effectiveness of ourapproach, we introduce a new challenging test dataset that contains severaltypes of out-of-vocabulary structured text, reflecting important readingapplications of fields such as prices, dates, serial numbers, license platesetc. We demonstrate that STEP can provide specialised OCR performance on demandin all tested scenarios.</description><author>Sergi Garcia-Bordils, Dimosthenis Karatzas, Marçal Rusiñol</author><pubDate>Tue, 05 Sep 2023 17:11:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02356v1</guid></item><item><title>A Lightweight and Transferable Design for Robust LEGO Manipulation</title><link>http://arxiv.org/abs/2309.02354v1</link><description>LEGO is a well-known platform for prototyping pixelized objects. However,robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due tothe tight connections and accuracy requirement. This paper investigates safeand efficient robotic LEGO manipulation. In particular, this paper reduces thecomplexity of the manipulation by hardware-software co-design. An end-of-armtool (EOAT) is designed, which reduces the problem dimension and allows largeindustrial robots to easily manipulate LEGO bricks. In addition, this paperuses evolution strategy to safely optimize the robot motion for LEGOmanipulation. Experiments demonstrate that the EOAT performs reliably inmanipulating LEGO bricks and the learning framework can effectively and safelyimprove the manipulation performance to a 100\% success rate. The co-design isdeployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) todemonstrate its generalizability and transferability. In the end, we show thatthe proposed solution enables sustainable robotic LEGO prototyping, in whichthe robot can repeatedly assemble and disassemble different prototypes.</description><author>Ruixuan Liu, Yifan Sun, Changliu Liu</author><pubDate>Tue, 05 Sep 2023 17:11:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02354v1</guid></item><item><title>Exact Inference for Continuous-Time Gaussian Process Dynamics</title><link>http://arxiv.org/abs/2309.02351v1</link><description>Physical systems can often be described via a continuous-time dynamicalsystem. In practice, the true system is often unknown and has to be learnedfrom measurement data. Since data is typically collected in discrete time, e.g.by sensors, most methods in Gaussian process (GP) dynamics model learning aretrained on one-step ahead predictions. This can become problematic in severalscenarios, e.g. if measurements are provided at irregularly-sampled time stepsor physical system properties have to be conserved. Thus, we aim for a GP modelof the true continuous-time dynamics. Higher-order numerical integratorsprovide the necessary tools to address this problem by discretizing thedynamics function with arbitrary accuracy. Many higher-order integratorsrequire dynamics evaluations at intermediate time steps making exact GPinference intractable. In previous work, this problem is often tackled byapproximating the GP posterior with variational inference. However, exact GPinference is preferable in many scenarios, e.g. due to its mathematicalguarantees. In order to make direct inference tractable, we propose to leveragemultistep and Taylor integrators. We demonstrate how to derive flexibleinference schemes for these types of integrators. Further, we derive tailoredsampling schemes that allow to draw consistent dynamics functions from thelearned posterior. This is crucial to sample consistent predictions from thedynamics model. We demonstrate empirically and theoretically that our approachyields an accurate representation of the continuous-time system.</description><author>Katharina Ensinger, Nicholas Tagliapietra, Sebastian Ziesche, Sebastian Trimpe</author><pubDate>Tue, 05 Sep 2023 17:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02351v1</guid></item><item><title>Coincident Learning for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2301.11368v2</link><description>Anomaly detection is an important task for complex systems (e.g., industrialfacilities, manufacturing, large-scale science experiments), where failures ina sub-system can lead to low yield, faulty products, or even damage tocomponents. While complex systems often have a wealth of data, labeledanomalies are typically rare (or even nonexistent) and expensive to acquire.Unsupervised approaches are therefore common and typically search for anomalieseither by distance or density of examples in the input feature space (or someassociated low-dimensional representation). This paper presents a novelapproach called CoAD, which is specifically designed for multi-modal tasks andidentifies anomalies based on \textit{coincident} behavior across two differentslices of the feature space. We define an \textit{unsupervised} metric,$\hat{F}_\beta$, out of analogy to the supervised classification $F_\beta$statistic. CoAD uses $\hat{F}_\beta$ to train an anomaly detection algorithm on\textit{unlabeled data}, based on the expectation that anomalous behavior inone feature slice is coincident with anomalous behavior in the other. Themethod is illustrated using a synthetic outlier data set and a MNIST-basedimage data set, and is compared to prior state-of-the-art on two real-worldtasks: a metal milling data set and a data set from a particle accelerator.</description><author>Ryan Humble, Zhe Zhang, Finn O'Shea, Eric Darve, Daniel Ratner</author><pubDate>Tue, 05 Sep 2023 17:04:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11368v2</guid></item><item><title>AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data</title><link>http://arxiv.org/abs/2305.18798v2</link><description>Semi-supervised anomaly detection (SSAD) methods have demonstrated theireffectiveness in enhancing unsupervised anomaly detection (UAD) by leveragingfew-shot but instructive abnormal instances. However, the dominance ofhomogeneous normal data over anomalies biases the SSAD models againsteffectively perceiving anomalies. To address this issue and achieve balancedsupervision between heavily imbalanced normal and abnormal data, we develop anovel framework called AnoOnly (Anomaly Only). Unlike existing SSAD methodsthat resort to strict loss supervision, AnoOnly suspends it and introduces aform of weak supervision for normal data. This weak supervision is instantiatedthrough the utilization of batch normalization, which implicitly performscluster learning on normal data. When integrated into existing SSAD methods,the proposed AnoOnly demonstrates remarkable performance enhancements acrossvarious models and datasets, achieving new state-of-the-art performance.Additionally, our AnoOnly is natively robust to label noise when suffering fromdata contamination. Our code is publicly available athttps://github.com/cool-xuan/AnoOnly.</description><author>Yixuan Zhou, Peiyu Yang, Yi Qu, Xing Xu, Zhe Sun, Andrzej Cichocki</author><pubDate>Tue, 05 Sep 2023 17:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18798v2</guid></item><item><title>Generating Infinite-Resolution Texture using GANs with Patch-by-Patch Paradigm</title><link>http://arxiv.org/abs/2309.02340v1</link><description>In this paper, we introduce a novel approach for generating texture images ofinfinite resolutions using Generative Adversarial Networks (GANs) based on apatch-by-patch paradigm. Existing texture synthesis techniques often rely ongenerating a large-scale texture using a one-forward pass to the generatingmodel, this limits the scalability and flexibility of the generated images. Incontrast, the proposed approach trains GANs models on a single texture image togenerate relatively small patches that are locally correlated and can beseamlessly concatenated to form a larger image while using a constant GPUmemory footprint. Our method learns the local texture structure and is able togenerate arbitrary-size textures, while also maintaining coherence anddiversity. The proposed method relies on local padding in the generator toensure consistency between patches and utilizes spatial stochastic modulationto allow for local variations and diversity within the large-scale image.Experimental results demonstrate superior scalability compared to existingapproaches while maintaining visual coherence of generated textures.</description><author>Alhasan Abdellatif, Ahmed H. Elsheikh</author><pubDate>Tue, 05 Sep 2023 16:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02340v1</guid></item><item><title>DEEPBEAS3D: Deep Learning and B-Spline Explicit Active Surfaces</title><link>http://arxiv.org/abs/2309.02335v1</link><description>Deep learning-based automatic segmentation methods have becomestate-of-the-art. However, they are often not robust enough for direct clinicalapplication, as domain shifts between training and testing data affect theirperformance. Failure in automatic segmentation can cause sub-optimal resultsthat require correction. To address these problems, we propose a novel 3Dextension of an interactive segmentation framework that represents asegmentation from a convolutional neural network (CNN) as a B-spline explicitactive surface (BEAS). BEAS ensures segmentations are smooth in 3D space,increasing anatomical plausibility, while allowing the user to precisely editthe 3D surface. We apply this framework to the task of 3D segmentation of theanal sphincter complex (AS) from transperineal ultrasound (TPUS) images, andcompare it to the clinical tool used in the pelvic floor disorder clinic (4DView VOCAL, GE Healthcare; Zipf, Austria). Experimental results show that: 1)the proposed framework gives the user explicit control of the surface contour;2) the perceived workload calculated via the NASA-TLX index was reduced by 30%compared to VOCAL; and 3) it required 7 0% (170 seconds) less user time thanVOCAL (p&lt; 0.00001)</description><author>Helena Williams, João Pedrosa, Muhammad Asad, Laura Cattani, Tom Vercauteren, Jan Deprest, Jan D'hooge</author><pubDate>Tue, 05 Sep 2023 16:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02335v1</guid></item><item><title>PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference</title><link>http://arxiv.org/abs/2309.02334v1</link><description>Field-programmable gate arrays (FPGAs) are widely used to implement deeplearning inference. Standard deep neural network inference involves thecomputation of interleaved linear maps and nonlinear activation functions.Prior work for ultra-low latency implementations has hardcoded the combinationof linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Ourwork is motivated by the idea that the LUTs in an FPGA can be used to implementa much greater variety of functions than this. In this paper, we propose anovel approach to training neural networks for FPGA deployment usingmultivariate polynomials as the basic building block. Our method takesadvantage of the flexibility offered by the soft logic, hiding the polynomialevaluation inside the LUTs with zero overhead. We show that by using polynomialbuilding blocks, we can achieve the same accuracy using considerably fewerlayers of soft logic than by using linear functions, leading to significantlatency and area improvements. We demonstrate the effectiveness of thisapproach in three tasks: network intrusion detection, jet identification at theCERN Large Hadron Collider, and handwritten digit recognition using the MNISTdataset.</description><author>Marta Andronic, George A. Constantinides</author><pubDate>Tue, 05 Sep 2023 16:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02334v1</guid></item><item><title>Resilient VAE: Unsupervised Anomaly Detection at the SLAC Linac Coherent Light Source</title><link>http://arxiv.org/abs/2309.02333v1</link><description>Significant advances in utilizing deep learning for anomaly detection havebeen made in recent years. However, these methods largely assume the existenceof a normal training set (i.e., uncontaminated by anomalies) or even acompletely labeled training set. In many complex engineering systems, such asparticle accelerators, labels are sparse and expensive; in order to performanomaly detection in these cases, we must drop these assumptions and utilize acompletely unsupervised method. This paper introduces the Resilient VariationalAutoencoder (ResVAE), a deep generative model specifically designed for anomalydetection. ResVAE exhibits resilience to anomalies present in the training dataand provides feature-level anomaly attribution. During the training process,ResVAE learns the anomaly probability for each sample as well as eachindividual feature, utilizing these probabilities to effectively disregardanomalous examples in the training data. We apply our proposed method to detectanomalies in the accelerator status at the SLAC Linac Coherent Light Source(LCLS). By utilizing shot-to-shot data from the beam position monitoringsystem, we demonstrate the exceptional capability of ResVAE in identifyingvarious types of anomalies that are visible in the accelerator.</description><author>Ryan Humble, William Colocho, Finn O'Shea, Daniel Ratner, Eric Darve</author><pubDate>Tue, 05 Sep 2023 16:53:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02333v1</guid></item><item><title>Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations</title><link>http://arxiv.org/abs/2309.02332v1</link><description>In the intricate architecture of the mammalian central nervous system,neurons form populations. Axonal bundles communicate between these clustersusing spike trains as their medium. However, these neuron populations' preciseencoding and operations have yet to be discovered. In our analysis, thestarting point is a state-of-the-art mechanistic model of a generic neuronendowed with plasticity. From this simple framework emerges a profoundmathematical construct: The representation and manipulation of information canbe precisely characterized by an algebra of finite convex cones. Furthermore,these neuron populations are not merely passive transmitters. They act asoperators within this algebraic structure, mirroring the functionality of alow-level programming language. When these populations interconnect, theyembody succinct yet potent algebraic expressions. These networks allow them toimplement many operations, such as specialization, generalization, noveltydetection, dimensionality reduction, inverse modeling, prediction, andassociative memory. In broader terms, this work illuminates the potential ofmatrix embeddings in advancing our understanding in fields like cognitivescience and AI. These embeddings enhance the capacity for concept processingand hierarchical description over their vector counterparts.</description><author>Martin N. P. Nilsson</author><pubDate>Tue, 05 Sep 2023 16:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02332v1</guid></item><item><title>Stochastic PDE representation of random fields for large-scale Gaussian process regression and statistical finite element analysis</title><link>http://arxiv.org/abs/2305.13879v2</link><description>The efficient representation of random fields on geometrically complexdomains is crucial for Bayesian modelling in engineering and machine learning.Today's prevalent random field representations are either intended forunbounded domains or are too restrictive in terms of possible field properties.Because of these limitations, techniques leveraging the historicallyestablished link between stochastic PDEs (SPDEs) and random fields have beengaining interest. The SPDE representation is especially appealing forengineering applications which already have a finite element discretisation forsolving the physical conservation equations. In contrast to the densecovariance matrix of a random field, its inverse, the precision matrix, isusually sparse and equal to the stiffness matrix of an elliptic SPDE. We usethe SPDE representation to develop a scalable framework for large-scalestatistical finite element analysis and Gaussian process (GP) regression oncomplex geometries. The statistical finite element method (statFEM) introducedby Girolami et al. (2022) is a novel approach for synthesising measurement dataand finite element models. In both statFEM and GP regression, we use the SPDEformulation to obtain the relevant prior probability densities with a sparseprecision matrix. The properties of the priors are governed by the parametersand possibly fractional order of the SPDE so that we can model on boundeddomains and manifolds anisotropic, non-stationary random fields with arbitrarysmoothness. The observation models for statFEM and GP regression are such thatthe posterior probability densities are Gaussians with a closed-form mean andprecision. The respective mean vector and precision matrix and can be evaluatedusing only sparse matrix operations. We demonstrate the versatility of theproposed framework and its convergence properties with Poisson and thin-shellexamples.</description><author>Kim Jie Koh, Fehmi Cirak</author><pubDate>Tue, 05 Sep 2023 16:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13879v2</guid></item><item><title>Neurosymbolic Meta-Reinforcement Lookahead Learning Achieves Safe Self-Driving in Non-Stationary Environments</title><link>http://arxiv.org/abs/2309.02328v1</link><description>In the area of learning-driven artificial intelligence advancement, theintegration of machine learning (ML) into self-driving (SD) technology standsas an impressive engineering feat. Yet, in real-world applications outside theconfines of controlled laboratory scenarios, the deployment of self-drivingtechnology assumes a life-critical role, necessitating heightened attentionfrom researchers towards both safety and efficiency. To illustrate, when aself-driving model encounters an unfamiliar environment in real-time execution,the focus must not solely revolve around enhancing its anticipated performance;equal consideration must be given to ensuring its execution or real-timeadaptation maintains a requisite level of safety. This study introduces analgorithm for online meta-reinforcement learning, employing lookahead symbolicconstraints based on \emph{Neurosymbolic Meta-Reinforcement Lookahead Learning}(NUMERLA). NUMERLA proposes a lookahead updating mechanism that harmonizes theefficiency of online adaptations with the overarching goal of ensuringlong-term safety. Experimental results demonstrate NUMERLA confers theself-driving agent with the capacity for real-time adaptability, leading tosafe and self-adaptive driving under non-stationary urban human-vehicleinteraction scenarios.</description><author>Haozhe Lei, Quanyan Zhu</author><pubDate>Tue, 05 Sep 2023 16:47:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02328v1</guid></item><item><title>Revisiting File Context for Source Code Summarization</title><link>http://arxiv.org/abs/2309.02326v1</link><description>Source code summarization is the task of writing natural languagedescriptions of source code. A typical use case is generating short summariesof subroutines for use in API documentation. The heart of almost all currentresearch into code summarization is the encoder-decoder neural architecture,and the encoder input is almost always a single subroutine or other short codesnippet. The problem with this setup is that the information needed to describethe code is often not present in the code itself -- that information oftenresides in other nearby code. In this paper, we revisit the idea of ``filecontext'' for code summarization. File context is the idea of encoding selectinformation from other subroutines in the same file. We propose a novelmodification of the Transformer architecture that is purpose-built to encodefile context and demonstrate its improvement over several baselines. We findthat file context helps on a subset of challenging examples where traditionalapproaches struggle.</description><author>Aakash Bansal, Chia-Yi Su, Collin McMillan</author><pubDate>Tue, 05 Sep 2023 16:44:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02326v1</guid></item><item><title>SeisCLIP: A seismology foundation model pre-trained by multi-modal data for multi-purpose seismic feature extraction</title><link>http://arxiv.org/abs/2309.02320v1</link><description>Training specific deep learning models for particular tasks is common acrossvarious domains within seismology. However, this approach encounters twolimitations: inadequate labeled data for certain tasks and limitedgeneralization across regions. To address these challenges, we developSeisCLIP, a seismology foundation model trained through contrastive learningfrom multi-modal data. It consists of a transformer encoder for extractingcrucial features from time-frequency seismic spectrum and an MLP encoder forintegrating the phase and source information of the same event. These encodersare jointly pre-trained on a vast dataset and the spectrum encoder issubsequently fine-tuned on smaller datasets for various downstream tasks.Notably, SeisCLIP's performance surpasses that of baseline methods in eventclassification, localization, and focal mechanism analysis tasks, employingdistinct datasets from different regions. In conclusion, SeisCLIP holdssignificant potential as a foundational model in the field of seismology,paving the way for innovative directions in foundation-model-based seismologyresearch.</description><author>Xu Si, Xinming Wu, Hanlin Sheng, Jun Zhu, Zefeng Li</author><pubDate>Tue, 05 Sep 2023 16:40:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02320v1</guid></item><item><title>Point-SLAM: Dense Neural Point Cloud-based SLAM</title><link>http://arxiv.org/abs/2304.04278v2</link><description>We propose a dense neural simultaneous localization and mapping (SLAM)approach for monocular RGBD input which anchors the features of a neural scenerepresentation in a point cloud that is iteratively generated in aninput-dependent data-driven manner. We demonstrate that both tracking andmapping can be performed with the same point-based neural scene representationby minimizing an RGBD-based re-rendering loss. In contrast to recent denseneural SLAM methods which anchor the scene features in a sparse grid, ourpoint-based approach allows dynamically adapting the anchor point density tothe information density of the input. This strategy reduces runtime and memoryusage in regions with fewer details and dedicates higher point density toresolve fine details. Our approach performs either better or competitive toexisting dense neural RGBD SLAM methods in tracking, mapping and renderingaccuracy on the Replica, TUM-RGBD and ScanNet datasets. The source code isavailable at https://github.com/eriksandstroem/Point-SLAM.</description><author>Erik Sandström, Yue Li, Luc Van Gool, Martin R. Oswald</author><pubDate>Tue, 05 Sep 2023 16:37:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04278v2</guid></item><item><title>TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA Reconstruction</title><link>http://arxiv.org/abs/2309.02318v1</link><description>Four-dimensional Digital Subtraction Angiography (4D DSA) plays a criticalrole in the diagnosis of many medical diseases, such as ArteriovenousMalformations (AVM) and Arteriovenous Fistulas (AVF). Despite its significantapplication value, the reconstruction of 4D DSA demands numerous views toeffectively model the intricate vessels and radiocontrast flow, therebyimplying a significant radiation dose. To address this high radiation issue, wepropose a Time-aware Attenuation Voxel (TiAVox) approach for sparse-view 4D DSAreconstruction, which paves the way for high-quality 4D imaging. Additionally,2D and 3D DSA imaging results can be generated from the reconstructed 4D DSAimages. TiAVox introduces 4D attenuation voxel grids, which reflect attenuationproperties from both spatial and temporal dimensions. It is optimized byminimizing discrepancies between the rendered images and sparse 2D DSA images.Without any neural network involved, TiAVox enjoys specific physicalinterpretability. The parameters of each learnable voxel represent theattenuation coefficients. We validated the TiAVox approach on both clinical andsimulated datasets, achieving a 31.23 Peak Signal-to-Noise Ratio (PSNR) fornovel view synthesis using only 30 views on the clinically sourced dataset,whereas traditional Feldkamp-Davis-Kress methods required 133 views. Similarly,with merely 10 views from the synthetic dataset, TiAVox yielded a PSNR of 34.32for novel view synthesis and 41.40 for 3D reconstruction. We also executedablation studies to corroborate the essential components of TiAVox. The codewill be publically available.</description><author>Zhenghong Zhou, Huangxuan Zhao, Jiemin Fang, Dongqiao Xiang, Lei Chen, Lingxia Wu, Feihong Wu, Wenyu Liu, Chuansheng Zheng, Xinggang Wang</author><pubDate>Tue, 05 Sep 2023 16:34:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02318v1</guid></item><item><title>A study on the impact of pre-trained model on Just-In-Time defect prediction</title><link>http://arxiv.org/abs/2309.02317v1</link><description>Previous researchers conducting Just-In-Time (JIT) defect prediction taskshave primarily focused on the performance of individual pre-trained models,without exploring the relationship between different pre-trained models asbackbones. In this study, we build six models: RoBERTaJIT, CodeBERTJIT,BARTJIT, PLBARTJIT, GPT2JIT, and CodeGPTJIT, each with a distinct pre-trainedmodel as its backbone. We systematically explore the differences andconnections between these models. Specifically, we investigate the performanceof the models when using Commit code and Commit message as inputs, as well asthe relationship between training efficiency and model distribution among thesesix models. Additionally, we conduct an ablation experiment to explore thesensitivity of each model to inputs. Furthermore, we investigate how the modelsperform in zero-shot and few-shot scenarios. Our findings indicate that eachmodel based on different backbones shows improvements, and when the backbone'spre-training model is similar, the training resources that need to be consumedare much more closer. We also observe that Commit code plays a significant rolein defect detection, and different pre-trained models demonstrate better defectdetection ability with a balanced dataset under few-shot scenarios. Theseresults provide new insights for optimizing JIT defect prediction tasks usingpre-trained models and highlight the factors that require more attention whenconstructing such models. Additionally, CodeGPTJIT and GPT2JIT achieved betterperformance than DeepJIT and CC2Vec on the two datasets respectively under 2000training samples. These findings emphasize the effectiveness oftransformer-based pre-trained models in JIT defect prediction tasks, especiallyin scenarios with limited training data.</description><author>Yuxiang Guo, Xiaopeng Gao, Zhenyu Zhang, W. K. Chan, Bo Jiang</author><pubDate>Tue, 05 Sep 2023 16:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02317v1</guid></item><item><title>InterviewBot: Real-Time End-to-End Dialogue System to Interview Students for College Admission</title><link>http://arxiv.org/abs/2303.15049v3</link><description>We present the InterviewBot that dynamically integrates conversation historyand customized topics into a coherent embedding space to conduct 10 minshybrid-domain (open and closed) conversations with foreign students applying toU.S. colleges for assessing their academic and cultural readiness. To build aneural-based end-to-end dialogue model, 7,361 audio recordings ofhuman-to-human interviews are automatically transcribed, where 440 are manuallycorrected for finetuning and evaluation. To overcome the input/output sizelimit of a transformer-based encoder-decoder model, two new methods areproposed, context attention and topic storing, allowing the model to makerelevant and consistent interactions. Our final model is tested bothstatistically by comparing its responses to the interview data and dynamicallyby inviting professional interviewers and various students to interact with itin real-time, finding it highly satisfactory in fluency and context awareness.</description><author>Zihao Wang, Nathan Keyes, Terry Crawford, Jinho D. Choi</author><pubDate>Tue, 05 Sep 2023 16:33:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15049v3</guid></item><item><title>"An Adapt-or-Die Type of Situation": Perception, Adoption, and Use of Text-To-Image-Generation AI by Game Industry Professionals</title><link>http://arxiv.org/abs/2302.12601v5</link><description>Text-to-image generation (TTIG) models, a recent addition to creative AI, cangenerate images based on a text description. These models have begun to rivalthe work of professional creatives, and sparked discussions on the future ofcreative work, loss of jobs, and copyright issues, amongst other importantimplications. To support the sustainable adoption of TTIG, we must providerich, reliable and transparent insights into how professionals perceive, adoptand use TTIG. Crucially though, the public debate is shallow, narrow andlacking transparency, while academic work has focused on studying the use ofTTIG in a general artist population, but not on the perceptions and attitudesof professionals in a specific industry. In this paper, we contribute aqualitative, exploratory interview study on TTIG in the Finnish videogameindustry. Through a Template Analysis on semi-structured interviews with 14game professionals, we reveal 12 overarching themes, structured into 49sub-themes on professionals' perception, adoption and use of TTIG systems ingames industry practice. Experiencing (yet another) change of roles andcreative processes, our participants' reflections can inform discussions withinthe industry, be used by policymakers to inform urgently needed legislation,and support researchers in games, HCI and AI to support the sustainable,professional use of TTIG to benefit people and games as cultural artefacts.</description><author>Veera Vimpari, Annakaisa Kultima, Perttu Hämäläinen, Christian Guckelsberger</author><pubDate>Tue, 05 Sep 2023 16:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12601v5</guid></item><item><title>Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization</title><link>http://arxiv.org/abs/2309.02311v1</link><description>Recent computational approaches for combating online hate speech involve theautomatic generation of counter narratives by adapting PretrainedTransformer-based Language Models (PLMs) with human-curated data. This process,however, can produce in-domain overfitting, resulting in models generatingacceptable narratives only for hatred similar to training data, with littleportability to other targets or to real-world toxic language. This paperintroduces novel attention regularization methodologies to improve thegeneralization capabilities of PLMs for counter narratives generation.Overfitting to training-specific terms is then discouraged, resulting in morediverse and richer narratives. We experiment with two attention-basedregularization techniques on a benchmark English dataset. Regularized modelsproduce better counter narratives than state-of-the-art approaches in mostcases, both in terms of automatic metrics and human evaluation, especially whenhateful targets are not present in the training data. This work paves the wayfor better and more flexible counter-speech generation models, a task for whichdatasets are highly challenging to produce.</description><author>Helena Bonaldi, Giuseppe Attanasio, Debora Nozza, Marco Guerini</author><pubDate>Tue, 05 Sep 2023 16:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02311v1</guid></item><item><title>Graph Self-Contrast Representation Learning</title><link>http://arxiv.org/abs/2309.02304v1</link><description>Graph contrastive learning (GCL) has recently emerged as a promising approachfor graph representation learning. Some existing methods adopt the 1-vs-Kscheme to construct one positive and K negative samples for each graph, but itis difficult to set K. For those methods that do not use negative samples, itis often necessary to add additional strategies to avoid model collapse, whichcould only alleviate the problem to some extent. All these drawbacks willundoubtedly have an adverse impact on the generalizability and efficiency ofthe model. In this paper, to address these issues, we propose a novel graphself-contrast framework GraphSC, which only uses one positive and one negativesample, and chooses triplet loss as the objective. Specifically, self-contrasthas two implications. First, GraphSC generates both positive and negative viewsof a graph sample from the graph itself via graph augmentation functions ofvarious intensities, and use them for self-contrast. Second, GraphSC usesHilbert-Schmidt Independence Criterion (HSIC) to factorize the representationsinto multiple factors and proposes a masked self-contrast mechanism to betterseparate positive and negative samples. Further, Since the triplet loss onlyoptimizes the relative distance between the anchor and its positive/negativesamples, it is difficult to ensure the absolute distance between the anchor andpositive sample. Therefore, we explicitly reduced the absolute distance betweenthe anchor and positive sample to accelerate convergence. Finally, we conductextensive experiments to evaluate the performance of GraphSC against 19 otherstate-of-the-art methods in both unsupervised and transfer learning settings.</description><author>Minjie Chen, Yao Cheng, Ye Wang, Xiang Li, Ming Gao</author><pubDate>Tue, 05 Sep 2023 16:13:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02304v1</guid></item><item><title>LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models</title><link>http://arxiv.org/abs/2308.16137v2</link><description>In recent years, there have been remarkable advancements in the performanceof Transformer-based Large Language Models (LLMs) across various domains. Asthese LLMs are deployed for increasingly complex tasks, they often face theneed to conduct longer reasoning processes or understand larger contexts. Inthese situations, the length generalization failure of LLMs on long sequencesbecomes more prominent. Most pre-training schemes truncate training sequencesto a fixed length. LLMs often struggle to generate fluent and coherent texts,let alone carry out downstream tasks, after longer contexts, even with relativepositional encoding designed to cope with this problem. Common solutions suchas finetuning on longer corpora often involve daunting hardware and time costsand require careful training process design. To more efficiently leverage thegeneration capacity of existing LLMs, we theoretically and empiricallyinvestigate the main out-of-distribution (OOD) factors contributing to thisproblem. Inspired by this diagnosis, we propose a simple yet effective solutionfor on-the-fly length generalization, LM-Infinite. It involves only a$\Lambda$-shaped attention mask (to avoid excessive attended tokens) and adistance limit (to avoid unseen distances) while requiring no parameter updatesor learning. We find it applicable to a variety of LLMs using relative-positionencoding methods. LM-Infinite is computationally efficient with $O(n)$ time andspace, and demonstrates consistent text generation fluency and quality to aslong as 32k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decodingspeedup. On downstream tasks such as passkey retrieval, it continues to work oninputs much longer than training lengths where vanilla models fail immediately.</description><author>Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang</author><pubDate>Tue, 05 Sep 2023 16:09:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16137v2</guid></item><item><title>CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning</title><link>http://arxiv.org/abs/2309.02301v1</link><description>Nowadays, the research on Large Vision-Language Models (LVLMs) has beensignificantly promoted thanks to the success of Large Language Models (LLM).Nevertheless, these Vision-Language Models (VLMs) are suffering from thedrawback of hallucination -- due to insufficient understanding of vision andlanguage modalities, VLMs may generate incorrect perception information whendoing downstream applications, for example, captioning a non-existent entity.To address the hallucination phenomenon, on the one hand, we introduce aContrastive Instruction Evaluation Method (CIEM), which is an automaticpipeline that leverages an annotated image-text dataset coupled with an LLM togenerate factual/contrastive question-answer pairs for the evaluation of thehallucination of VLMs. On the other hand, based on CIEM, we further propose anew instruction tuning method called CIT (the abbreviation of ContrastiveInstruction Tuning) to alleviate the hallucination of VLMs by automaticallyproducing high-quality factual/contrastive question-answer pairs andcorresponding justifications for model tuning. Through extensive experiments onCIEM and CIT, we pinpoint the hallucination issues commonly present in existingVLMs, the disability of the current instruction-tuning dataset to handle thehallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEMand public datasets.</description><author>Hongyu Hu, Jiyuan Zhang, Minyi Zhao, Zhenbang Sun</author><pubDate>Tue, 05 Sep 2023 16:06:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02301v1</guid></item><item><title>A novel automatic wind power prediction framework based on multi-time scale and temporal attention mechanisms</title><link>http://arxiv.org/abs/2302.01222v5</link><description>Wind energy is a widely distributed, renewable, and environmentally friendlyenergy source that plays a crucial role in mitigating global warming andaddressing energy shortages. Nevertheless, wind power generation ischaracterized by volatility, intermittence, and randomness, which hinder itsability to serve as a reliable power source for the grid. Accurate wind powerforecasting is crucial for developing a new power system that heavily relies onrenewable energy sources. However, traditional wind power forecasting systemsprimarily focus on ultra-short-term or short-term forecasts, limiting theirability to address the diverse adjustment requirements of the power systemsimultaneously. To overcome these challenges, We propose an automatic frameworkcapable of forecasting wind power across multi-time scale. The framework basedon the tree-structured Parzen estimator (TPE) and temporal fusion transformer(TFT) that can provide ultra-short-term, short-term and medium-term wind powerforecasting power.Our approach employs the TFT for wind power forecasting andcategorizes features based on their properties. Additionally, we introduce ageneric algorithm to simultaneously fine-tune the hyperparameters of thedecomposition method and model. We evaluate the performance of our framework byconducting ablation experiments using three commonly used decompositionalgorithms and six state-of-the-art models for forecasting multi-time scale.The experimental results demonstrate that our proposed method considerablyimproves prediction accuracy on the public dataset Engiehttps://opendata-renewables.engie.com. Compared to the second-beststate-of-the-art model, our approach exhibits a reduction of 31.75% and 28.74%in normalized mean absolute error (nMAE) for 24-hour forecasting, and 20.79%and 16.93% in nMAE for 48-hour forecasting, respectively.</description><author>Meiyu Jiang, Jun Shen, Xuetao Jiang, Lihui Luo, Rui Zhou, Qingguo Zhou</author><pubDate>Tue, 05 Sep 2023 16:02:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01222v5</guid></item><item><title>RESTORE: Graph Embedding Assessment Through Reconstruction</title><link>http://arxiv.org/abs/2308.14659v2</link><description>Following the success of Word2Vec embeddings, graph embeddings (GEs) havegained substantial traction. GEs are commonly generated and evaluatedextrinsically on downstream applications, but intrinsic evaluations of theoriginal graph properties in terms of topological structure and semanticinformation have been lacking. Understanding these will help identify thedeficiency of the various families of GE methods when vectorizing graphs interms of preserving the relevant knowledge or learning incorrect knowledge. Toaddress this, we propose RESTORE, a framework for intrinsic GEs assessmentthrough graph reconstruction. We show that reconstructing the original graphfrom the underlying GEs yields insights into the relative amount of informationpreserved in a given vector form. We first introduce the graph reconstructiontask. We generate GEs from three GE families based on factorization methods,random walks, and deep learning (with representative algorithms from eachfamily) on the CommonSense Knowledge Graph (CSKG). We analyze theireffectiveness in preserving the (a) topological structure of node-level graphreconstruction with an increasing number of hops and (b) semantic informationon various word semantic and analogy tests. Our evaluations show deeplearning-based GE algorithm (SDNE) is overall better at preserving (a) with amean average precision (mAP) of 0.54 and 0.35 for 2 and 3-hop reconstructionrespectively, while the factorization-based algorithm (HOPE) is better atencapsulating (b) with an average Euclidean distance of 0.14, 0.17, and 0.11for 1, 2, and 3-hop reconstruction respectively. The modest performance ofthese GEs leaves room for further research avenues on better graphrepresentation learning.</description><author>Hong Yung Yip, Chidaksh Ravuru, Neelabha Banerjee, Shashwat Jha, Amit Sheth, Aman Chadha, Amitava Das</author><pubDate>Tue, 05 Sep 2023 16:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14659v2</guid></item><item><title>Inferring effective couplings with Restricted Boltzmann Machines</title><link>http://arxiv.org/abs/2309.02292v1</link><description>Generative models offer a direct way to model complex data. Among them,energy-based models provide us with a neural network model that aims toaccurately reproduce all statistical correlations observed in the data at thelevel of the Boltzmann weight of the model. However, one challenge is tounderstand the physical interpretation of such models. In this study, wepropose a simple solution by implementing a direct mapping between the energyfunction of the Restricted Boltzmann Machine and an effective Ising spinHamiltonian that includes high-order interactions between spins. This mappingincludes interactions of all possible orders, going beyond the conventionalpairwise interactions typically considered in the inverse Ising approach, andallowing the description of complex datasets. Earlier work attempted to achievethis goal, but the proposed mappings did not do properly treat the complexityof the problem or did not contain direct prescriptions for practicalapplication. To validate our method, we perform several controlled numericalexperiments where the training samples are equilibrium samples of predefinedmodels containing local external fields, two-body and three-body interactionsin various low-dimensional topologies. The results demonstrate theeffectiveness of our proposed approach in learning the correct interactionnetwork and pave the way for its application in modeling interesting datasets.We also evaluate the quality of the inferred model based on different trainingmethods.</description><author>Aurélien Decelle, Cyril Furtlehner, Alfonso De Jesus Navas Gómez, Beatriz Seoane</author><pubDate>Tue, 05 Sep 2023 15:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02292v1</guid></item><item><title>ATM: Action Temporality Modeling for Video Question Answering</title><link>http://arxiv.org/abs/2309.02290v1</link><description>Despite significant progress in video question answering (VideoQA), existingmethods fall short of questions that require causal/temporal reasoning acrossframes. This can be attributed to imprecise motion representations. Weintroduce Action Temporality Modeling (ATM) for temporality reasoning viathree-fold uniqueness: (1) rethinking the optical flow and realizing thatoptical flow is effective in capturing the long horizon temporality reasoning;(2) training the visual-text embedding by contrastive learning in anaction-centric manner, leading to better action representations in both visionand text modalities; and (3) preventing the model from answering the questiongiven the shuffled video in the fine-tuning stage, to avoid spuriouscorrelation between appearance and motion and hence ensure faithful temporalityreasoning. In the experiments, we show that ATM outperforms previous approachesin terms of the accuracy on multiple VideoQAs and exhibits better truetemporality reasoning ability.</description><author>Junwen Chen, Jie Zhu, Yu Kong</author><pubDate>Tue, 05 Sep 2023 15:52:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02290v1</guid></item><item><title>Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts</title><link>http://arxiv.org/abs/2308.16609v2</link><description>Graph classification, aiming at learning the graph-level representations foreffective class assignments, has received outstanding achievements, whichheavily relies on high-quality datasets that have balanced class distribution.In fact, most real-world graph data naturally presents a long-tailed form,where the head classes occupy much more samples than the tail classes, it thusis essential to study the graph-level classification over long-tailed datawhile still remaining largely unexplored. However, most existing long-tailedlearning methods in visions fail to jointly optimize the representationlearning and classifier training, as well as neglect the mining of thehard-to-classify classes. Directly applying existing methods to graphs may leadto sub-optimal performance, since the model trained on graphs would be moresensitive to the long-tailed distribution due to the complex topologicalcharacteristics. Hence, in this paper, we propose a novel long-tailedgraph-level classification framework via Collaborative Multi-expert Learning(CoMe) to tackle the problem. To equilibrate the contributions of head and tailclasses, we first develop balanced contrastive learning from the view ofrepresentation learning, and then design an individual-expert classifiertraining based on hard class mining. In addition, we execute gated fusion anddisentangled knowledge distillation among the multiple experts to promote thecollaboration in a multi-expert framework. Comprehensive experiments areperformed on seven widely-used benchmark datasets to demonstrate thesuperiority of our method CoMe over state-of-the-art baselines.</description><author>Siyu Yi, Zhengyang Mao, Wei Ju, Yongdao Zhou, Luchen Liu, Xiao Luo, Ming Zhang</author><pubDate>Tue, 05 Sep 2023 15:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16609v2</guid></item><item><title>Optimal Observation-Intervention Trade-Off in Optimisation Problems with Causal Structure</title><link>http://arxiv.org/abs/2309.02287v1</link><description>We consider the problem of optimising an expensive-to-evaluate grey-boxobjective function, within a finite budget, where known side-information existsin the form of the causal structure between the design variables. Standardblack-box optimisation ignores the causal structure, often making itinefficient and expensive. The few existing methods that consider the causalstructure are myopic and do not fully accommodate the observation-interventiontrade-off that emerges when estimating causal effects. In this paper, we showthat the observation-intervention trade-off can be formulated as a non-myopicoptimal stopping problem which permits an efficient solution. We givetheoretical results detailing the structure of the optimal stopping times anddemonstrate the generality of our approach by showing that it can be integratedwith existing causal Bayesian optimisation algorithms. Experimental resultsshow that our formulation can enhance existing algorithms on real and syntheticbenchmarks.</description><author>Kim Hammar, Neil Dhir</author><pubDate>Tue, 05 Sep 2023 15:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02287v1</guid></item><item><title>Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes</title><link>http://arxiv.org/abs/2309.02286v1</link><description>Current scene graph datasets suffer from strong long-tail distributions oftheir predicate classes. Due to a very low number of some predicate classes inthe test sets, no reliable metrics can be retrieved for the rarest classes. Weconstruct a new panoptic scene graph dataset and a set of metrics that aredesigned as a benchmark for the predictive performance especially on rarepredicate classes. To construct the new dataset, we propose a model-assistedannotation pipeline that efficiently finds rare predicate classes that arehidden in a large set of images like needles in a haystack. Contrary to prior scene graph datasets, Haystack contains explicit negativeannotations, i.e. annotations that a given relation does not have a certainpredicate class. Negative annotations are helpful especially in the field ofscene graph generation and open up a whole new set of possibilities to improvecurrent scene graph generation models. Haystack is 100% compatible with existing panoptic scene graph datasets andcan easily be integrated with existing evaluation pipelines. Our dataset andcode can be found here: https://lorjul.github.io/haystack/. It includesannotation files and simple to use scripts and utilities, to help withintegrating our dataset in existing work.</description><author>Julian Lorenz, Florian Barthel, Daniel Kienzle, Rainer Lienhart</author><pubDate>Tue, 05 Sep 2023 15:45:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02286v1</guid></item><item><title>PromptTTS 2: Describing and Generating Voices with Text Prompt</title><link>http://arxiv.org/abs/2309.02285v1</link><description>Speech conveys more information than just text, as the same word can beuttered in various voices to convey diverse information. Compared totraditional text-to-speech (TTS) methods relying on speech prompts (referencespeech) for voice variability, using text prompts (descriptions) is moreuser-friendly since speech prompts can be hard to find or may not exist at all.TTS approaches based on the text prompt face two challenges: 1) the one-to-manyproblem, where not all details about voice variability can be described in thetext prompt, and 2) the limited availability of text prompt datasets, wherevendors and large cost of data labeling are required to write text prompt forspeech. In this work, we introduce PromptTTS 2 to address these challenges witha variation network to provide variability information of voice not captured bytext prompts, and a prompt generation pipeline to utilize the large languagemodels (LLM) to compose high quality text prompts. Specifically, the variationnetwork predicts the representation extracted from the reference speech (whichcontains full information about voice) based on the text prompt representation.For the prompt generation pipeline, it generates text prompts for speech with aspeech understanding model to recognize voice attributes (e.g., gender, speed)from speech and a large language model to formulate text prompt based on therecognition results. Experiments on a large-scale (44K hours) speech datasetdemonstrate that compared to the previous works, PromptTTS 2 generates voicesmore consistent with text prompts and supports the sampling of diverse voicevariability, thereby offering users more choices on voice generation.Additionally, the prompt generation pipeline produces high-quality prompts,eliminating the large labeling cost. The demo page of PromptTTS 2 is availableonline\footnote{https://speechresearch.github.io/prompttts2}.</description><author>Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian</author><pubDate>Tue, 05 Sep 2023 15:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02285v1</guid></item><item><title>2nd Place Winning Solution for the CVPR2023 Visual Anomaly and Novelty Detection Challenge: Multimodal Prompting for Data-centric Anomaly Detection</title><link>http://arxiv.org/abs/2306.09067v2</link><description>This technical report introduces the winning solution of the team Segment AnyAnomaly for the CVPR2023 Visual Anomaly and Novelty Detection (VAND) challenge.Going beyond uni-modal prompt, e.g., language prompt, we present a novelframework, i.e., Segment Any Anomaly + (SAA$+$), for zero-shot anomalysegmentation with multi-modal prompts for the regularization of cascaded modernfoundation models. Inspired by the great zero-shot generalization ability offoundation models like Segment Anything, we first explore their assembly (SAA)to leverage diverse multi-modal prior knowledge for anomaly localization.Subsequently, we further introduce multimodal prompts (SAA$+$) derived fromdomain expert knowledge and target image context to enable the non-parameteradaptation of foundation models to anomaly segmentation. The proposed SAA$+$model achieves state-of-the-art performance on several anomaly segmentationbenchmarks, including VisA and MVTec-AD, in the zero-shot setting. We willrelease the code of our winning solution for the CVPR2023 VAN.</description><author>Yunkang Cao, Xiaohao Xu, Chen Sun, Yuqi Cheng, Liang Gao, Weiming Shen</author><pubDate>Tue, 05 Sep 2023 15:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09067v2</guid></item><item><title>s-ID: Causal Effect Identification in a Sub-Population</title><link>http://arxiv.org/abs/2309.02281v1</link><description>Causal inference in a sub-population involves identifying the causal effectof an intervention on a specific subgroup within a larger population. However,ignoring the subtleties introduced by sub-populations can either lead toerroneous inference or limit the applicability of existing methods. Weintroduce and advocate for a causal inference problem in sub-populations(henceforth called s-ID), in which we merely have access to observational dataof the targeted sub-population (as opposed to the entire population). Existinginference problems in sub-populations operate on the premise that the givendata distributions originate from the entire population, thus, cannot tacklethe s-ID problem. To address this gap, we provide necessary and sufficientconditions that must hold in the causal graph for a causal effect in asub-population to be identifiable from the observational distribution of thatsub-population. Given these conditions, we present a sound and completealgorithm for the s-ID problem.</description><author>Amir Mohammad Abouei, Ehsan Mokhtarian, Negar Kiyavash</author><pubDate>Tue, 05 Sep 2023 15:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02281v1</guid></item><item><title>A Comparison of Residual-based Methods on Fault Detection</title><link>http://arxiv.org/abs/2309.02274v1</link><description>An important initial step in fault detection for complex industrial systemsis gaining an understanding of their health condition. Subsequently, continuousmonitoring of this health condition becomes crucial to observe its evolution,track changes over time, and isolate faults. As faults are typically rareoccurrences, it is essential to perform this monitoring in an unsupervisedmanner. Various approaches have been proposed not only to detect faults in anunsupervised manner but also to distinguish between different potential faulttypes. In this study, we perform a comprehensive comparison between tworesidual-based approaches: autoencoders, and the input-output models thatestablish a mapping between operating conditions and sensor readings. Weexplore the sensor-wise residuals and aggregated residuals for the entiresystem in both methods. The performance evaluation focuses on three tasks:health indicator construction, fault detection, and health indicatorinterpretation. To perform the comparison, we utilize the Commercial ModularAero-Propulsion System Simulation (C-MAPSS) dynamical model, specifically asubset of the turbofan engine dataset containing three different fault types.All models are trained exclusively on healthy data. Fault detection is achievedby applying a threshold that is determined based on the healthy condition. Thedetection results reveal that both models are capable of detecting faults withan average delay of around 20 cycles and maintain a low false positive rate.While the fault detection performance is similar for both models, theinput-output model provides better interpretability regarding potential faulttypes and the possible faulty components.</description><author>Chi-Ching Hsu, Gaetan Frusque, Olga Fink</author><pubDate>Tue, 05 Sep 2023 15:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02274v1</guid></item><item><title>Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette</title><link>http://arxiv.org/abs/2309.02272v1</link><description>This paper introduces a novel graph-based filter method for automatic featureselection (abbreviated as GB-AFS) for multi-class classification tasks. Themethod determines the minimum combination of features required to sustainprediction performance while maintaining complementary discriminating abilitiesbetween different classes. It does not require any user-defined parameters suchas the number of features to select. The methodology employs theJeffries-Matusita (JM) distance in conjunction with t-distributed StochasticNeighbor Embedding (t-SNE) to generate a low-dimensional space reflecting howeffectively each feature can differentiate between each pair of classes. Theminimum number of features is selected using our newly developed MeanSimplified Silhouette (abbreviated as MSS) index, designed to evaluate theclustering results for the feature selection task. Experimental results onpublic data sets demonstrate the superior performance of the proposed GB-AFSover other filter-based techniques and automatic feature selection approaches.Moreover, the proposed algorithm maintained the accuracy achieved whenutilizing all features, while using only $7\%$ to $30\%$ of the features.Consequently, this resulted in a reduction of the time needed forclassifications, from $15\%$ to $70\%$.</description><author>David Levin, Gonen Singer</author><pubDate>Tue, 05 Sep 2023 15:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02272v1</guid></item><item><title>Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark</title><link>http://arxiv.org/abs/2309.00367v2</link><description>The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduceda set of graph learning tasks strongly dependent on long-range interactionbetween vertices. Empirical evidence suggests that on these tasks GraphTransformers significantly outperform Message Passing GNNs (MPGNNs). In thispaper, we carefully reevaluate multiple MPGNN baselines as well as the GraphTransformer GPS (Ramp\'a\v{s}ek et al. 2022) on LRGB. Through a rigorousempirical analysis, we demonstrate that the reported performance gap isoverestimated due to suboptimal hyperparameter choices. It is noteworthy thatacross multiple datasets the performance gap completely vanishes after basichyperparameter optimization. In addition, we discuss the impact of lackingfeature normalization for LRGB's vision datasets and highlight a spuriousimplementation of LRGB's link prediction metric. The principal aim of our paperis to establish a higher standard of empirical rigor within the graph machinelearning community.</description><author>Jan Tönshoff, Martin Ritzert, Eran Rosenbluth, Martin Grohe</author><pubDate>Tue, 05 Sep 2023 15:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00367v2</guid></item><item><title>SAM-Deblur: Let Segment Anything Boost Image Deblurring</title><link>http://arxiv.org/abs/2309.02270v1</link><description>Image deblurring is a critical task in the field of image restoration, aimingto eliminate blurring artifacts. However, the challenge of addressingnon-uniform blurring leads to an ill-posed problem, which limits thegeneralization performance of existing deblurring models. To solve the problem,we propose a framework SAM-Deblur, integrating prior knowledge from the SegmentAnything Model (SAM) into the deblurring task for the first time. Inparticular, SAM-Deblur is divided into three stages. First, We preprocess theblurred images, obtain image masks via SAM, and propose a mask dropout methodfor training to enhance model robustness. Then, to fully leverage thestructural priors generated by SAM, we propose a Mask Average Pooling (MAP)unit specifically designed to average SAM-generated segmented areas, serving asa plug-and-play component which can be seamlessly integrated into existingdeblurring networks. Finally, we feed the fused features generated by the MAPUnit into the deblurring model to obtain a sharp image. Experimental results onthe RealBlurJ, ReloBlur, and REDS datasets reveal that incorporating ourmethods improves NAFNet's PSNR by 0.05, 0.96, and 7.03, respectively. Code willbe available at \href{https://github.com/HPLQAQ/SAM-Deblur}{SAM-Deblur}.</description><author>Siwei Li, Mingxuan Liu, Yating Zhang, Shu Chen, Haoxiang Li, Hong Chen, Zifei Dou</author><pubDate>Tue, 05 Sep 2023 15:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02270v1</guid></item><item><title>Event-based Stereo Visual Odometry with Native Temporal Resolution via Continuous-time Gaussian Process Regression</title><link>http://arxiv.org/abs/2306.01188v3</link><description>Event-based cameras asynchronously capture individual visual changes in ascene. This makes them more robust than traditional frame-based cameras tohighly dynamic motions and poor illumination. It also means that everymeasurement in a scene can occur at a unique time. Handling these different measurement times is a major challenge of usingevent-based cameras. It is often addressed in visual odometry (VO) pipelines byapproximating temporally close measurements as occurring at one common time.This grouping simplifies the estimation problem but, absent additional sensors,sacrifices the inherent temporal resolution of event-based cameras. This paper instead presents a complete stereo VO pipeline that estimatesdirectly with individual event-measurement times without requiring any groupingor approximation in the estimation state. It uses continuous-time trajectoryestimation to maintain the temporal fidelity and asynchronous nature ofevent-based cameras through Gaussian process regression with a physicallymotivated prior. Its performance is evaluated on the MVSEC dataset, where itachieves 7.9e-3 and 5.9e-3 RMS relative error on two independent sequences,outperforming the existing publicly available event-based stereo VO pipeline bytwo and four times, respectively.</description><author>Jianeng Wang, Jonathan D. Gammell</author><pubDate>Tue, 05 Sep 2023 15:30:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01188v3</guid></item><item><title>Dynamic Loss For Robust Learning</title><link>http://arxiv.org/abs/2211.12506v2</link><description>Label noise and class imbalance commonly coexist in real-world data. Previousworks for robust learning, however, usually address either one type of the databiases and underperform when facing them both. To mitigate this gap, this workpresents a novel meta-learning based dynamic loss that automatically adjuststhe objective functions with the training process to robustly learn aclassifier from long-tailed noisy data. Concretely, our dynamic loss comprisesa label corrector and a margin generator, which respectively correct noisylabels and generate additive per-class classification margins by perceiving theunderlying data distribution as well as the learning state of the classifier.Equipped with a new hierarchical sampling strategy that enriches a small amountof unbiased metadata with diverse and hard samples, the two components in thedynamic loss are optimized jointly through meta-learning and cultivate theclassifier to well adapt to clean and balanced test data. Extensive experimentsshow our method achieves state-of-the-art accuracy on multiple real-world andsynthetic datasets with various types of data biases, including CIFAR-10/100,Animal-10N, ImageNet-LT, and Webvision. Code will soon be publicly available.</description><author>Shenwang Jiang, Jianan Li, Jizhou Zhang, Ying Wang, Tingfa Xu</author><pubDate>Tue, 05 Sep 2023 15:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12506v2</guid></item><item><title>MA-VAE: Multi-head Attention-based Variational Autoencoder Approach for Anomaly Detection in Multivariate Time-series Applied to Automotive Endurance Powertrain Testing</title><link>http://arxiv.org/abs/2309.02253v1</link><description>A clear need for automatic anomaly detection applied to automotive testinghas emerged as more and more attention is paid to the data recorded and manualevaluation by humans reaches its capacity. Such real-world data is massive,diverse, multivariate and temporal in nature, therefore requiring modelling ofthe testee behaviour. We propose a variational autoencoder with multi-headattention (MA-VAE), which, when trained on unlabelled data, not only providesvery few false positives but also manages to detect the majority of theanomalies presented. In addition to that, the approach offers a novel way toavoid the bypass phenomenon, an undesirable behaviour investigated inliterature. Lastly, the approach also introduces a new method to remapindividual windows to a continuous time series. The results are presented inthe context of a real-world industrial data set and several experiments areundertaken to further investigate certain aspects of the proposed model. Whenconfigured properly, it is 9% of the time wrong when an anomaly is flagged anddiscovers 67% of the anomalies present. Also, MA-VAE has the potential toperform well with only a fraction of the training and validation subset,however, to extract it, a more sophisticated threshold estimation method isrequired.</description><author>Lucas Correia, Jan-Christoph Goos, Philipp Klein, Thomas Bäck, Anna V. Kononova</author><pubDate>Tue, 05 Sep 2023 15:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02253v1</guid></item><item><title>When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning</title><link>http://arxiv.org/abs/2309.00007v2</link><description>With the great success of deep neural networks, adversarial learning hasreceived widespread attention in various studies, ranging from multi-classlearning to multi-label learning. However, existing adversarial attacks towardmulti-label learning only pursue the traditional visual imperceptibility butignore the new perceptible problem coming from measures such as Precision@$k$and mAP@$k$. Specifically, when a well-trained multi-label classifier performsfar below the expectation on some samples, the victim can easily realize thatthis performance degeneration stems from attack, rather than the model itself.Therefore, an ideal multi-labeling adversarial attack should manage to not onlydeceive visual perception but also evade monitoring of measures. To this end,this paper first proposes the concept of measure imperceptibility. Then, anovel loss function is devised to generate such adversarial perturbations thatcould achieve both visual and measure imperceptibility. Furthermore, anefficient algorithm, which enjoys a convex objective, is established tooptimize this objective. Finally, extensive experiments on large-scalebenchmark datasets, such as PASCAL VOC 2012, MS COCO, and NUS WIDE, demonstratethe superiority of our proposed method in attacking the top-$k$ multi-labelsystems.</description><author>Yuchen Sun, Qianqian Xu, Zitai Wang, Qingming Huang</author><pubDate>Tue, 05 Sep 2023 15:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00007v2</guid></item><item><title>RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning</title><link>http://arxiv.org/abs/2309.02250v1</link><description>In the domain of machine learning algorithms, the significance of the lossfunction is paramount, especially in supervised learning tasks. It serves as afundamental pillar that profoundly influences the behavior and efficacy ofsupervised learning algorithms. Traditional loss functions, while widely used,often struggle to handle noisy and high-dimensional data, impede modelinterpretability, and lead to slow convergence during training. In this paper,we address the aforementioned constraints by proposing a novel robust, bounded,sparse, and smooth (RoBoSS) loss function for supervised learning. Further, weincorporate the RoBoSS loss function within the framework of support vectormachine (SVM) and introduce a new robust algorithm named$\mathcal{L}_{rbss}$-SVM. For the theoretical analysis, theclassification-calibrated property and generalization ability are alsopresented. These investigations are crucial for gaining deeper insights intothe performance of the RoBoSS loss function in the classification tasks and itspotential to generalize well to unseen data. To empirically demonstrate theeffectiveness of the proposed $\mathcal{L}_{rbss}$-SVM, we evaluate it on $88$real-world UCI and KEEL datasets from diverse domains. Additionally, toexemplify the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM within thebiomedical realm, we evaluated it on two medical datasets: theelectroencephalogram (EEG) signal dataset and the breast cancer (BreaKHis)dataset. The numerical results substantiate the superiority of the proposed$\mathcal{L}_{rbss}$-SVM model, both in terms of its remarkable generalizationperformance and its efficiency in training time.</description><author>Mushir Akhtar, M. Tanveer, Mohd. Arshad</author><pubDate>Tue, 05 Sep 2023 14:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02250v1</guid></item><item><title>Encoding Seasonal Climate Predictions for Demand Forecasting with Modular Neural Network</title><link>http://arxiv.org/abs/2309.02248v1</link><description>Current time-series forecasting problems use short-term weather attributes asexogenous inputs. However, in specific time-series forecasting solutions (e.g.,demand prediction in the supply chain), seasonal climate predictions arecrucial to improve its resilience. Representing mid to long-term seasonalclimate forecasts is challenging as seasonal climate predictions are uncertain,and encoding spatio-temporal relationship of climate forecasts with demand iscomplex. We propose a novel modeling framework that efficiently encodes seasonalclimate predictions to provide robust and reliable time-series forecasting forsupply chain functions. The encoding framework enables effective learning oflatent representations -- be it uncertain seasonal climate prediction or othertime-series data (e.g., buyer patterns) -- via a modular neural networkarchitecture. Our extensive experiments indicate that learning suchrepresentations to model seasonal climate forecast results in an errorreduction of approximately 13\% to 17\% across multiple real-world data setscompared to existing demand forecasting methods.</description><author>Smit Marvaniya, Jitendra Singh, Nicolas Galichet, Fred Ochieng Otieno, Geeth De Mel, Kommy Weldemariam</author><pubDate>Tue, 05 Sep 2023 14:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02248v1</guid></item><item><title>Augmenting Chest X-ray Datasets with Non-Expert Annotations</title><link>http://arxiv.org/abs/2309.02244v1</link><description>The advancement of machine learning algorithms in medical image analysisrequires the expansion of training datasets. A popular and cost-effectiveapproach is automated annotation extraction from free-text medical reports,primarily due to the high costs associated with expert clinicians annotatingchest X-ray images. However, it has been shown that the resulting datasets aresusceptible to biases and shortcuts. Another strategy to increase the size of adataset is crowdsourcing, a widely adopted practice in general computer visionwith some success in medical image analysis. In a similar vein tocrowdsourcing, we enhance two publicly available chest X-ray datasets byincorporating non-expert annotations. However, instead of using diagnosticlabels, we annotate shortcuts in the form of tubes. We collect 3.5k chest drainannotations for CXR14, and 1k annotations for 4 different tube types inPadChest. We train a chest drain detector with the non-expert annotations thatgeneralizes well to expert labels. Moreover, we compare our annotations tothose provided by experts and show "moderate" to "almost perfect" agreement.Finally, we present a pathology agreement study to raise awareness about groundtruth annotations. We make our annotations and code available.</description><author>Cathrine Damgaard, Trine Naja Eriksen, Dovile Juodelyte, Veronika Cheplygina, Amelia Jiménez-Sánchez</author><pubDate>Tue, 05 Sep 2023 14:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02244v1</guid></item><item><title>Self-Similarity-Based and Novelty-based loss for music structure analysis</title><link>http://arxiv.org/abs/2309.02243v1</link><description>Music Structure Analysis (MSA) is the task aiming at identifying musicalsegments that compose a music track and possibly label them based on theirsimilarity. In this paper we propose a supervised approach for the task ofmusic boundary detection. In our approach we simultaneously learn features andconvolution kernels. For this we jointly optimize -- a loss based on theSelf-Similarity-Matrix (SSM) obtained with the learned features, denoted bySSM-loss, and -- a loss based on the novelty score obtained applying thelearned kernels to the estimated SSM, denoted by novelty-loss. We alsodemonstrate that relative feature learning, through self-attention, isbeneficial for the task of MSA. Finally, we compare the performances of ourapproach to previously proposed approaches on the standard RWC-Pop, and varioussubsets of SALAMI.</description><author>Geoffroy Peeters</author><pubDate>Tue, 05 Sep 2023 14:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02243v1</guid></item><item><title>CausalOps -- Towards an Industrial Lifecycle for Causal Probabilistic Graphical Models</title><link>http://arxiv.org/abs/2308.01375v2</link><description>Causal probabilistic graph-based models have gained widespread utility,enabling the modeling of cause-and-effect relationships across diverse domains.With their rising adoption in new areas, such as automotive system safety andmachine learning, the need for an integrated lifecycle framework akin to DevOpsand MLOps has emerged. Currently, a process reference for organizationsinterested in employing causal engineering is missing. To address this gap andfoster widespread industrial adoption, we propose CausalOps, a novel lifecycleframework for causal model development and application. By defining keyentities, dependencies, and intermediate artifacts generated during causalengineering, we establish a consistent vocabulary and workflow model. This workcontextualizes causal model usage across different stages and stakeholders,outlining a holistic view of creating and maintaining them. CausalOps' aim isto drive the adoption of causal methods in practical applications withininterested organizations and the causality community.</description><author>Robert Maier, Andreas Schlattl, Thomas Guess, Jürgen Mottok</author><pubDate>Tue, 05 Sep 2023 14:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01375v2</guid></item><item><title>Dialog Action-Aware Transformer for Dialog Policy Learning</title><link>http://arxiv.org/abs/2309.02240v1</link><description>Recent works usually address Dialog policy learning DPL by training areinforcement learning (RL) agent to determine the best dialog action. However,existing works on deep RL require a large volume of agent-user interactions toachieve acceptable performance. In this paper, we propose to make full use ofthe plain text knowledge from the pre-trained language model to accelerate theRL agent's learning speed. Specifically, we design a dialog action-awaretransformer encoder (DaTrans), which integrates a new fine-tuning procedurenamed masked last action task to encourage DaTrans to be dialog-aware anddistils action-specific features. Then, DaTrans is further optimized in an RLsetting with ongoing interactions and evolves through exploration in the dialogaction space toward maximizing long-term accumulated rewards. The effectivenessand efficiency of the proposed model are demonstrated with both simulatorevaluation and human evaluation.</description><author>Huimin Wang, Wai-Chung Kwan, Kam-Fai Wong</author><pubDate>Tue, 05 Sep 2023 14:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02240v1</guid></item><item><title>Sample Size in Natural Language Processing within Healthcare Research</title><link>http://arxiv.org/abs/2309.02237v1</link><description>Sample size calculation is an essential step in most data-based disciplines.Large enough samples ensure representativeness of the population and determinethe precision of estimates. This is true for most quantitative studies,including those that employ machine learning methods, such as natural languageprocessing, where free-text is used to generate predictions and classifyinstances of text. Within the healthcare domain, the lack of sufficient corporaof previously collected data can be a limiting factor when determining samplesizes for new studies. This paper tries to address the issue by makingrecommendations on sample sizes for text classification tasks in the healthcaredomain. Models trained on the MIMIC-III database of critical care records from BethIsrael Deaconess Medical Center were used to classify documents as having ornot having Unspecified Essential Hypertension, the most common diagnosis codein the database. Simulations were performed using various classifiers ondifferent sample sizes and class proportions. This was repeated for acomparatively less common diagnosis code within the database of diabetesmellitus without mention of complication. Smaller sample sizes resulted in better results when using a K-nearestneighbours classifier, whereas larger sample sizes provided better results withsupport vector machines and BERT models. Overall, a sample size larger than1000 was sufficient to provide decent performance metrics. The simulations conducted within this study provide guidelines that can beused as recommendations for selecting appropriate sample sizes and classproportions, and for predicting expected performance, when building classifiersfor textual healthcare data. The methodology used here can be modified forsample size estimates calculations with other datasets.</description><author>Jaya Chaturvedi, Diana Shamsutdinova, Felix Zimmer, Sumithra Velupillai, Daniel Stahl, Robert Stewart, Angus Roberts</author><pubDate>Tue, 05 Sep 2023 14:42:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02237v1</guid></item><item><title>Distributionally Robust Model-based Reinforcement Learning with Large State Spaces</title><link>http://arxiv.org/abs/2309.02236v1</link><description>Three major challenges in reinforcement learning are the complex dynamicalsystems with large state spaces, the costly data acquisition processes, and thedeviation of real-world dynamics from the training environment deployment. Toovercome these issues, we study distributionally robust Markov decisionprocesses with continuous state spaces under the widely used Kullback-Leibler,chi-square, and total variation uncertainty sets. We propose a model-basedapproach that utilizes Gaussian Processes and the maximum variance reductionalgorithm to efficiently learn multi-output nominal transition dynamics,leveraging access to a generative model (i.e., simulator). We furtherdemonstrate the statistical sample complexity of the proposed method fordifferent uncertainty sets. These complexity bounds are independent of thenumber of states and extend beyond linear dynamics, ensuring the effectivenessof our approach in identifying near-optimal distributionally-robust policies.The proposed method can be further combined with other model-freedistributionally robust reinforcement learning methods to obtain a near-optimalrobust policy. Experimental results demonstrate the robustness of our algorithmto distributional shifts and its superior performance in terms of the number ofsamples needed.</description><author>Shyam Sundhar Ramesh, Pier Giuseppe Sessa, Yifan Hu, Andreas Krause, Ilija Bogunovic</author><pubDate>Tue, 05 Sep 2023 14:42:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02236v1</guid></item><item><title>Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering</title><link>http://arxiv.org/abs/2309.02233v1</link><description>Large-scale language models (LLMs), such as ChatGPT, are capable ofgenerating human-like responses for various downstream tasks, such astask-oriented dialogues and question answering. However, applying LLMs tomedical domains remains challenging due to their inability to leveragedomain-specific knowledge. In this study, we present the Large-scale LanguageModels Augmented with Medical Textbooks (LLM-AMT), which integratesauthoritative medical textbooks as the cornerstone of its design, enhancing itsproficiency in the specialized domain through plug-and-play modules, comprisedof a Hybrid Textbook Retriever, supplemented by the Query Augmenter and the LLMReader. Experimental evaluation on three open-domain medical question-answeringtasks reveals a substantial enhancement in both the professionalism andaccuracy of the LLM responses when utilizing LLM-AMT, exhibiting an improvementranging from 11.4% to 13.2%. Despite being 100 times smaller, we found thatmedical textbooks as the retrieval corpus serves as a more valuable externalknowledge source than Wikipedia in the medical domain. Our experiments showthat textbook augmentation results in a performance improvement ranging from9.7% to 12.2% over Wikipedia augmentation.</description><author>Yubo Wang, Xueguang Ma, Wenhu Chen</author><pubDate>Tue, 05 Sep 2023 14:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02233v1</guid></item><item><title>FSD: An Initial Chinese Dataset for Fake Song Detection</title><link>http://arxiv.org/abs/2309.02232v1</link><description>Singing voice synthesis and singing voice conversion have significantlyadvanced, revolutionizing musical experiences. However, the rise of "DeepfakeSongs" generated by these technologies raises concerns about authenticity.Unlike Audio DeepFake Detection (ADD), the field of song deepfake detectionlacks specialized datasets or methods for song authenticity verification. Inthis paper, we initially construct a Chinese Fake Song Detection (FSD) datasetto investigate the field of song deepfake detection. The fake songs in the FSDdataset are generated by five state-of-the-art singing voice synthesis andsinging voice conversion methods. Our initial experiments on FSD revealed theineffectiveness of existing speech-trained ADD models for the task of SongDeepFake Detection. Thus, we employ the FSD dataset for the training of ADDmodels. We subsequently evaluate these models under two scenarios: one with theoriginal songs and another with separated vocal tracks. Experiment results showthat song-trained ADD models exhibit an approximate 38.58% reduction in averageequal error rate compared to speech-trained ADD models on the FSD test set.</description><author>Yuankun Xie, Jingjing Zhou, Xiaolin Lu, Zhenghao Jiang, Yuxin Yang, Haonan Cheng, Long Ye</author><pubDate>Tue, 05 Sep 2023 14:37:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02232v1</guid></item><item><title>DCP-Net: A Distributed Collaborative Perception Network for Remote Sensing Semantic Segmentation</title><link>http://arxiv.org/abs/2309.02230v1</link><description>Onboard intelligent processing is widely applied in emergency tasks in thefield of remote sensing. However, it is predominantly confined to an individualplatform with a limited observation range as well as susceptibility tointerference, resulting in limited accuracy. Considering the current state ofmulti-platform collaborative observation, this article innovatively presents adistributed collaborative perception network called DCP-Net. Firstly, theproposed DCP-Net helps members to enhance perception performance by integratingfeatures from other platforms. Secondly, a self-mutual information match moduleis proposed to identify collaboration opportunities and select suitablepartners, prioritizing critical collaborative features and reducing redundanttransmission cost. Thirdly, a related feature fusion module is designed toaddress the misalignment between local and collaborative features, improvingthe quality of fused features for the downstream task. We conduct extensiveexperiments and visualization analyses using three semantic segmentationdatasets, including Potsdam, iSAID and DFC23. The results demonstrate thatDCP-Net outperforms the existing methods comprehensively, improving mIoU by2.61%~16.89% at the highest collaboration efficiency, which promotes theperformance to a state-of-the-art level.</description><author>Zhechao Wang, Peirui Cheng, Shujing Duan, Kaiqiang Chen, Zhirui Wang, Xinming Li, Xian Sun</author><pubDate>Tue, 05 Sep 2023 14:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02230v1</guid></item><item><title>AMR4NLI: Interpretable and robust NLI measures from semantic graphs</title><link>http://arxiv.org/abs/2306.00936v2</link><description>The task of natural language inference (NLI) asks whether a given premise(expressed in NL) entails a given NL hypothesis. NLI benchmarks contain humanratings of entailment, but the meaning relationships driving these ratings arenot formalized. Can the underlying sentence pair relationships be made moreexplicit in an interpretable yet robust fashion? We compare semantic structuresto represent premise and hypothesis, including sets of contextualizedembeddings and semantic graphs (Abstract Meaning Representations), and measurewhether the hypothesis is a semantic substructure of the premise, utilizinginterpretable metrics. Our evaluation on three English benchmarks finds valuein both contextualized embeddings and semantic graphs; moreover, they providecomplementary signals, and can be leveraged together in a hybrid model.</description><author>Juri Opitz, Shira Wein, Julius Steen, Anette Frank, Nathan Schneider</author><pubDate>Tue, 05 Sep 2023 14:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00936v2</guid></item><item><title>How Expressive are Graph Neural Networks in Recommendation?</title><link>http://arxiv.org/abs/2308.11127v2</link><description>Graph Neural Networks (GNNs) have demonstrated superior performance onvarious graph learning tasks, including recommendation, where they leverageuser-item collaborative filtering signals in graphs. However, theoreticalformulations of their capability are scarce, despite their empiricaleffectiveness in state-of-the-art recommender models. Recently, research hasexplored the expressiveness of GNNs in general, demonstrating that messagepassing GNNs are at most as powerful as the Weisfeiler-Lehman test, and thatGNNs combined with random node initialization are universal. Nevertheless, theconcept of "expressiveness" for GNNs remains vaguely defined. Most existingworks adopt the graph isomorphism test as the metric of expressiveness, butthis graph-level task may not effectively assess a model's ability inrecommendation, where the objective is to distinguish nodes of differentcloseness. In this paper, we provide a comprehensive theoretical analysis ofthe expressiveness of GNNs in recommendation, considering three levels ofexpressiveness metrics: graph isomorphism (graph-level), node automorphism(node-level), and topological closeness (link-level). We propose thetopological closeness metric to evaluate GNNs' ability to capture thestructural distance between nodes, which aligns closely with the objective ofrecommendation. To validate the effectiveness of this new metric in evaluatingrecommendation performance, we introduce a learning-less GNN algorithm that isoptimal on the new metric and can be optimal on the node-level metric withsuitable modification. We conduct extensive experiments comparing the proposedalgorithm against various types of state-of-the-art GNN models to explore theexplainability of the new metric in the recommendation task. Forreproducibility, implementation codes are available athttps://github.com/HKUDS/GTE.</description><author>Xuheng Cai, Lianghao Xia, Xubin Ren, Chao Huang</author><pubDate>Tue, 05 Sep 2023 14:35:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11127v2</guid></item><item><title>Dense Object Grounding in 3D Scenes</title><link>http://arxiv.org/abs/2309.02224v1</link><description>Localizing objects in 3D scenes according to the semantics of a given naturallanguage is a fundamental yet important task in the field of multimediaunderstanding, which benefits various real-world applications such as roboticsand autonomous driving. However, the majority of existing 3D object groundingmethods are restricted to a single-sentence input describing an individualobject, which cannot comprehend and reason more contextualized descriptions ofmultiple objects in more practical 3D cases. To this end, we introduce a newchallenging task, called 3D Dense Object Grounding (3D DOG), to jointlylocalize multiple objects described in a more complicated paragraph rather thana single sentence. Instead of naively localizing each sentence-guided objectindependently, we found that dense objects described in the same paragraph areoften semantically related and spatially located in a focused region of the 3Dscene. To explore such semantic and spatial relationships of densely referredobjects for more accurate localization, we propose a novel Stacked Transformerbased framework for 3D DOG, named 3DOGSFormer. Specifically, we first devise acontextual query-driven local transformer decoder to generate initial groundingproposals for each target object. Then, we employ a proposal-guided globaltransformer decoder that exploits the local object features to learn theircorrelation for further refining initial grounding proposals. Extensiveexperiments on three challenging benchmarks (Nr3D, Sr3D, and ScanRefer) showthat our proposed 3DOGSFormer outperforms state-of-the-art 3D single-objectgrounding methods and their dense-object variants by significant margins.</description><author>Wencan Huang, Daizong Liu, Wei Hu</author><pubDate>Tue, 05 Sep 2023 14:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02224v1</guid></item><item><title>Robustness and Generalizability of Deepfake Detection: A Study with Diffusion Models</title><link>http://arxiv.org/abs/2309.02218v1</link><description>The rise of deepfake images, especially of well-known personalities, poses aserious threat to the dissemination of authentic information. To tackle this,we present a thorough investigation into how deepfakes are produced and howthey can be identified. The cornerstone of our research is a rich collection ofartificial celebrity faces, titled DeepFakeFace (DFF). We crafted the DFFdataset using advanced diffusion models and have shared it with the communitythrough online platforms. This data serves as a robust foundation to train andtest algorithms designed to spot deepfakes. We carried out a thorough review ofthe DFF dataset and suggest two evaluation methods to gauge the strength andadaptability of deepfake recognition tools. The first method tests whether analgorithm trained on one type of fake images can recognize those produced byother methods. The second evaluates the algorithm's performance with imperfectimages, like those that are blurry, of low quality, or compressed. Given variedresults across deepfake methods and image changes, our findings stress the needfor better deepfake detectors. Our DFF dataset and tests aim to boost thedevelopment of more effective tools against deepfakes.</description><author>Haixu Song, Shiyu Huang, Yinpeng Dong, Wei-Wei Tu</author><pubDate>Tue, 05 Sep 2023 14:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02218v1</guid></item><item><title>Advanced Underwater Image Restoration in Complex Illumination Conditions</title><link>http://arxiv.org/abs/2309.02217v1</link><description>Underwater image restoration has been a challenging problem for decades sincethe advent of underwater photography. Most solutions focus on shallow waterscenarios, where the scene is uniformly illuminated by the sunlight. However,the vast majority of uncharted underwater terrain is located beyond 200 metersdepth where natural light is scarce and artificial illumination is needed. Insuch cases, light sources co-moving with the camera, dynamically change thescene appearance, which make shallow water restoration methods inadequate. Inparticular for multi-light source systems (composed of dozens of LEDsnowadays), calibrating each light is time-consuming, error-prone and tedious,and we observe that only the integrated illumination within the viewing volumeof the camera is critical, rather than the individual light sources. The keyidea of this paper is therefore to exploit the appearance changes of objects orthe seafloor, when traversing the viewing frustum of the camera. Through newconstraints assuming Lambertian surfaces, corresponding image pixels constrainthe light field in front of the camera, and for each voxel a signal factor anda backscatter value are stored in a volumetric grid that can be used for veryefficient image restoration of camera-light platforms, which facilitatesconsistently texturing large 3D models and maps that would otherwise bedominated by lighting and medium artifacts. To validate the effectiveness ofour approach, we conducted extensive experiments on simulated and real-worlddatasets. The results of these experiments demonstrate the robustness of ourapproach in restoring the true albedo of objects, while mitigating theinfluence of lighting and medium effects. Furthermore, we demonstrate ourapproach can be readily extended to other scenarios, including in-air imagingwith artificial illumination or other similar cases.</description><author>Yifan Song, Mengkun She, Kevin Köser</author><pubDate>Tue, 05 Sep 2023 14:22:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02217v1</guid></item><item><title>Improving equilibrium propagation without weight symmetry through Jacobian homeostasis</title><link>http://arxiv.org/abs/2309.02214v1</link><description>Equilibrium propagation (EP) is a compelling alternative to thebackpropagation of error algorithm (BP) for computing gradients of neuralnetworks on biological or analog neuromorphic substrates. Still, the algorithmrequires weight symmetry and infinitesimal equilibrium perturbations, i.e.,nudges, to estimate unbiased gradients efficiently. Both requirements arechallenging to implement in physical systems. Yet, whether and how weightasymmetry affects its applicability is unknown because, in practice, it may bemasked by biases introduced through the finite nudge. To address this question,we study generalized EP, which can be formulated without weight symmetry, andanalytically isolate the two sources of bias. For complex-differentiablenon-symmetric networks, we show that the finite nudge does not pose a problem,as exact derivatives can still be estimated via a Cauchy integral. In contrast,weight asymmetry introduces bias resulting in low task performance due to pooralignment of EP's neuronal error vectors compared to BP. To mitigate thisissue, we present a new homeostatic objective that directly penalizesfunctional asymmetries of the Jacobian at the network's fixed point. Thishomeostatic objective dramatically improves the network's ability to solvecomplex tasks such as ImageNet 32x32. Our results lay the theoreticalgroundwork for studying and mitigating the adverse effects of imperfections ofphysical networks on learning algorithms that rely on the substrate'srelaxation dynamics.</description><author>Axel Laborieux, Friedemann Zenke</author><pubDate>Tue, 05 Sep 2023 14:20:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02214v1</guid></item><item><title>Distributionally Robust Machine Learning with Multi-source Data</title><link>http://arxiv.org/abs/2309.02211v1</link><description>Classical machine learning methods may lead to poor prediction performancewhen the target distribution differs from the source populations. This paperutilizes data from multiple sources and introduces a group distributionallyrobust prediction model defined to optimize an adversarial reward aboutexplained variance with respect to a class of target distributions. Compared toclassical empirical risk minimization, the proposed robust prediction modelimproves the prediction accuracy for target populations with distributionshifts. We show that our group distributionally robust prediction model is aweighted average of the source populations' conditional outcome models. Weleverage this key identification result to robustify arbitrary machine learningalgorithms, including, for example, random forests and neural networks. Wedevise a novel bias-corrected estimator to estimate the optimal aggregationweight for general machine-learning algorithms and demonstrate its improvementin the convergence rate. Our proposal can be seen as a distributionally robustfederated learning approach that is computationally efficient and easy toimplement using arbitrary machine learning base algorithms, satisfies someprivacy constraints, and has a nice interpretation of different sources'importance for predicting a given target covariate distribution. We demonstratethe performance of our proposed group distributionally robust method onsimulated and real data with random forests and neural networks asbase-learning algorithms.</description><author>Zhenyu Wang, Peter Bühlmann, Zijian Guo</author><pubDate>Tue, 05 Sep 2023 14:19:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02211v1</guid></item><item><title>Continual Cross-Dataset Adaptation in Road Surface Classification</title><link>http://arxiv.org/abs/2309.02210v1</link><description>Accurate road surface classification is crucial for autonomous vehicles (AVs)to optimize driving conditions, enhance safety, and enable advanced roadmapping. However, deep learning models for road surface classification sufferfrom poor generalization when tested on unseen datasets. To update these modelswith new information, also the original training dataset must be taken intoaccount, in order to avoid catastrophic forgetting. This is, however,inefficient if not impossible, e.g., when the data is collected in streams orlarge amounts. To overcome this limitation and enable fast and efficientcross-dataset adaptation, we propose to employ continual learning finetuningmethods designed to retain past knowledge while adapting to new data, thuseffectively avoiding forgetting. Experimental results demonstrate thesuperiority of this approach over naive finetuning, achieving performance closeto fresh retraining. While solving this known problem, we also provide ageneral description of how the same technique can be adopted in other AVscenarios. We highlight the potential computational and economic benefits thata continual-based adaptation can bring to the AV industry, while also reducinggreenhouse emissions due to unnecessary joint retraining.</description><author>Paolo Cudrano, Matteo Bellusci, Giuseppe Macino, Matteo Matteucci</author><pubDate>Tue, 05 Sep 2023 14:18:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02210v1</guid></item><item><title>Language Models for Novelty Detection in System Call Traces</title><link>http://arxiv.org/abs/2309.02206v1</link><description>Due to the complexity of modern computer systems, novel and unexpectedbehaviors frequently occur. Such deviations are either normal occurrences, suchas software updates and new user activities, or abnormalities, such asmisconfigurations, latency issues, intrusions, and software bugs. Regardless,novel behaviors are of great interest to developers, and there is a genuineneed for efficient and effective methods to detect them. Nowadays, researchersconsider system calls to be the most fine-grained and accurate source ofinformation to investigate the behavior of computer systems. Accordingly, thispaper introduces a novelty detection methodology that relies on a probabilitydistribution over sequences of system calls, which can be seen as a languagemodel. Language models estimate the likelihood of sequences, and sincenovelties deviate from previously observed behaviors by definition, they wouldbe unlikely under the model. Following the success of neural networks forlanguage models, three architectures are evaluated in this work: the widespreadLSTM, the state-of-the-art Transformer, and the lower-complexity Longformer.However, large neural networks typically require an enormous amount of data tobe trained effectively, and to the best of our knowledge, no massive moderndatasets of kernel traces are publicly available. This paper addresses thislimitation by introducing a new open-source dataset of kernel traces comprisingover 2 million web requests with seven distinct behaviors. The proposedmethodology requires minimal expert hand-crafting and achieves an F-score andAuROC greater than 95% on most novelties while being data- and task-agnostic.The source code and trained models are publicly available on GitHub while thedatasets are available on Zenodo.</description><author>Quentin Fournier, Daniel Aloise, Leandro R. Costa</author><pubDate>Tue, 05 Sep 2023 14:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02206v1</guid></item><item><title>On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence</title><link>http://arxiv.org/abs/2309.02202v1</link><description>Best Arm Identification (BAI) problems are progressively used fordata-sensitive applications, such as designing adaptive clinical trials, tuninghyper-parameters, and conducting user studies to name a few. Motivated by thedata privacy concerns invoked by these applications, we study the problem ofBAI with fixed confidence under $\epsilon$-global Differential Privacy (DP).First, to quantify the cost of privacy, we derive a lower bound on the samplecomplexity of any $\delta$-correct BAI algorithm satisfying $\epsilon$-globalDP. Our lower bound suggests the existence of two privacy regimes depending onthe privacy budget $\epsilon$. In the high-privacy regime (small $\epsilon$),the hardness depends on a coupled effect of privacy and a novelinformation-theoretic quantity, called the Total Variation Characteristic Time.In the low-privacy regime (large $\epsilon$), the sample complexity lower boundreduces to the classical non-private lower bound. Second, we propose AdaP-TT,an $\epsilon$-global DP variant of the Top Two algorithm. AdaP-TT runs inarm-dependent adaptive episodes and adds Laplace noise to ensure a goodprivacy-utility trade-off. We derive an asymptotic upper bound on the samplecomplexity of AdaP-TT that matches with the lower bound up to multiplicativeconstants in the high-privacy regime. Finally, we provide an experimentalanalysis of AdaP-TT that validates our theoretical results.</description><author>Achraf Azize, Marc Jourdan, Aymen Al Marjani, Debabrota Basu</author><pubDate>Tue, 05 Sep 2023 14:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02202v1</guid></item><item><title>Delving into Ipsilateral Mammogram Assessment under Multi-View Network</title><link>http://arxiv.org/abs/2309.02197v1</link><description>In many recent years, multi-view mammogram analysis has been focused widelyon AI-based cancer assessment. In this work, we aim to explore diverse fusionstrategies (average and concatenate) and examine the model's learning behaviorwith varying individuals and fusion pathways, involving Coarse Layer and FineLayer. The Ipsilateral Multi-View Network, comprising five fusion types (Pre,Early, Middle, Last, and Post Fusion) in ResNet-18, is employed. Notably, theMiddle Fusion emerges as the most balanced and effective approach, enhancingdeep-learning models' generalization performance by +5.29\% (concatenate) and+5.9\% (average) in VinDr-Mammo dataset and +2.03\% (concatenate) and +3\%(average) in CMMD dataset on macro F1-Score. The paper emphasizes the crucialrole of layer assignment in multi-view network extraction with variousstrategies.</description><author>Thai Ngoc Toan Truong, Thanh-Huy Nguyen, Ba Thinh Lam, Vu Minh Duy Nguyen, Hong Phuc Nguyen</author><pubDate>Tue, 05 Sep 2023 13:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02197v1</guid></item><item><title>Sparse Function-space Representation of Neural Networks</title><link>http://arxiv.org/abs/2309.02195v1</link><description>Deep neural networks (NNs) are known to lack uncertainty estimates andstruggle to incorporate new data. We present a method that mitigates theseissues by converting NNs from weight space to function space, via a dualparameterization. Importantly, the dual parameterization enables us toformulate a sparse representation that captures information from the entiredata set. This offers a compact and principled way of capturing uncertainty andenables us to incorporate new data without retraining whilst retainingpredictive performance. We provide proof-of-concept demonstrations with theproposed approach for quantifying uncertainty in supervised learning on UCIbenchmark tasks.</description><author>Aidan Scannell, Riccardo Mereu, Paul Chang, Ella Tamir, Joni Pajarinen, Arno Solin</author><pubDate>Tue, 05 Sep 2023 13:56:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02195v1</guid></item><item><title>Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing</title><link>http://arxiv.org/abs/2309.02193v1</link><description>In the era of 5G mobile communication, there has been a significant surge inresearch focused on unmanned aerial vehicles (UAVs) and mobile edge computingtechnology. UAVs can serve as intelligent servers in edge computingenvironments, optimizing their flight trajectories to maximize communicationsystem throughput. Deep reinforcement learning (DRL)-based trajectoryoptimization algorithms may suffer from poor training performance due tointricate terrain features and inadequate training data. To overcome thislimitation, some studies have proposed leveraging federated learning (FL) tomitigate the data isolation problem and expedite convergence. Nevertheless, theefficacy of global FL models can be negatively impacted by the highheterogeneity of local data, which could potentially impede the trainingprocess and even compromise the performance of local agents. This work proposesa novel solution to address these challenges, namely personalized federateddeep reinforcement learning (PF-DRL), for multi-UAV trajectory optimization.PF-DRL aims to develop individualized models for each agent to address the datascarcity issue and mitigate the negative impact of data heterogeneity.Simulation results demonstrate that the proposed algorithm achieves superiortraining performance with faster convergence rates, and improves servicequality compared to other DRL-based approaches.</description><author>Zhengrong Song, Chuan Ma, Ming Ding, Howard H. Yang, Yuwen Qian, Xiangwei Zhou</author><pubDate>Tue, 05 Sep 2023 13:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02193v1</guid></item><item><title>Exchanging-based Multimodal Fusion with Transformer</title><link>http://arxiv.org/abs/2309.02190v1</link><description>We study the problem of multimodal fusion in this paper. Recentexchanging-based methods have been proposed for vision-vision fusion, which aimto exchange embeddings learned from one modality to the other. However, most ofthem project inputs of multimodalities into different low-dimensional spacesand cannot be applied to the sequential input data. To solve these issues, inthis paper, we propose a novel exchanging-based multimodal fusion model MuSEfor text-vision fusion based on Transformer. We first use two encoders toseparately map multimodal inputs into different low-dimensional spaces. Then weemploy two decoders to regularize the embeddings and pull them into the samespace. The two decoders capture the correlations between texts and images withthe image captioning task and the text-to-image generation task, respectively.Further, based on the regularized embeddings, we present CrossTransformer,which uses two Transformer encoders with shared parameters as the backbonemodel to exchange knowledge between multimodalities. Specifically,CrossTransformer first learns the global contextual information of the inputsin the shallow layers. After that, it performs inter-modal exchange byselecting a proportion of tokens in one modality and replacing their embeddingswith the average of embeddings in the other modality. We conduct extensiveexperiments to evaluate the performance of MuSE on the Multimodal Named EntityRecognition task and the Multimodal Sentiment Analysis task. Our results showthe superiority of MuSE against other competitors. Our code and data areprovided at https://github.com/RecklessRonan/MuSE.</description><author>Renyu Zhu, Chengcheng Han, Yong Qian, Qiushi Sun, Xiang Li, Ming Gao, Xuezhi Cao, Yunsen Xian</author><pubDate>Tue, 05 Sep 2023 13:48:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02190v1</guid></item><item><title>Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification</title><link>http://arxiv.org/abs/2309.02189v1</link><description>Environmental, Social, and Governance (ESG) has been used as a metric tomeasure the negative impacts and enhance positive outcomes of companies inareas such as the environment, society, and governance. Recently, investorshave increasingly recognized the significance of ESG criteria in theirinvestment choices, leading businesses to integrate ESG principles into theiroperations and strategies. The Multi-Lingual ESG Issue Identification (ML-ESG)shared task encompasses the classification of news documents into 35 distinctESG issue labels. In this study, we explored multiple strategies harnessingBERT language models to achieve accurate classification of news documentsacross these labels. Our analysis revealed that the RoBERTa classifier emergedas one of the most successful approaches, securing the second-place positionfor the English test dataset, and sharing the fifth-place position for theFrench test dataset. Furthermore, our SVM-based binary model tailored for theChinese language exhibited exceptional performance, earning the second-placerank on the test dataset.</description><author>Elvys Linhares Pontes, Mohamed Benjannet, Lam Kim Ming</author><pubDate>Tue, 05 Sep 2023 13:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02189v1</guid></item><item><title>Incorporating Dictionaries into a Neural Network Architecture to Extract COVID-19 Medical Concepts From Social Media</title><link>http://arxiv.org/abs/2309.02188v1</link><description>We investigate the potential benefit of incorporating dictionary informationinto a neural network architecture for natural language processing. Inparticular, we make use of this architecture to extract several conceptsrelated to COVID-19 from an on-line medical forum. We use a sample from theforum to manually curate one dictionary for each concept. In addition, we useMetaMap, which is a tool for extracting biomedical concepts, to identify asmall number of semantic concepts. For a supervised concept extraction task onthe forum data, our best model achieved a macro $F_1$ score of 90\%. A majordifficulty in medical concept extraction is obtaining labelled data from whichto build supervised models. We investigate the utility of our models totransfer to data derived from a different source in two ways. First forproducing labels via weak learning and second to perform concept extraction.The dataset we use in this case comprises COVID-19 related tweets and weachieve an $F_1$ score 81\% for symptom concept extraction trained on weaklylabelled data. The utility of our dictionaries is compared with a COVID-19symptom dictionary that was constructed directly from Twitter. Furtherexperiments that incorporate BERT and a COVID-19 version of BERTweetdemonstrate that the dictionaries provide a commensurate result. Our resultsshow that incorporating small domain dictionaries to deep learning models canimprove concept extraction tasks. Moreover, models built using dictionariesgeneralize well and are transferable to different datasets on a similar task.</description><author>Abul Hasan, Mark Levene, David Weston</author><pubDate>Tue, 05 Sep 2023 13:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02188v1</guid></item></channel></rss>