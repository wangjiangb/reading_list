<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 09 Feb 2025 13:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SMART: Advancing Scalable Map Priors for Driving Topology Reasoning</title><link>http://arxiv.org/abs/2502.04329v1</link><description>Topology reasoning is crucial for autonomous driving as it enablescomprehensive understanding of connectivity and relationships between lanes andtraffic elements. While recent approaches have shown success in perceivingdriving topology using vehicle-mounted sensors, their scalability is hinderedby the reliance on training data captured by consistent sensor configurations.We identify that the key factor in scalable lane perception and topologyreasoning is the elimination of this sensor-dependent feature. To address this,we propose SMART, a scalable solution that leverages easily availablestandard-definition (SD) and satellite maps to learn a map prior model,supervised by large-scale geo-referenced high-definition (HD) maps independentof sensor settings. Attributed to scaled training, SMART alone achievessuperior offline lane topology understanding using only SD and satelliteinputs. Extensive experiments further demonstrate that SMART can be seamlesslyintegrated into any online topology reasoning methods, yielding significantimprovements of up to 28% on the OpenLane-V2 benchmark.</description><author>Junjie Ye, David Paz, Hengyuan Zhang, Yuliang Guo, Xinyu Huang, Henrik I. Christensen, Yue Wang, Liu Ren</author><pubDate>Thu, 06 Feb 2025 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04329v1</guid></item><item><title>Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment</title><link>http://arxiv.org/abs/2502.04328v1</link><description>Recent advances in large language models, particularly following GPT-4o, havesparked increasing interest in developing omni-modal models capable ofunderstanding more modalities. While some open-source alternatives haveemerged, there is still a notable lag behind specialized single-modality modelsin performance. In this paper, we present Ola, an Omni-modal language modelthat achieves competitive performance across image, video, and audiounderstanding compared to specialized counterparts. The core design of Ola liesin its progressive modality alignment strategy that extends the supportingmodality of the language model progressively. Our training pipeline begins withthe most distinct modalities: image and text, then gradually expands the skillsets of the model using speech data that connects language and audio knowledge,and video data that connects all modalities. The progressive learning pipelinealso enables us to maintain a relatively small size of the cross-modalalignment data, making developing omni-modal from existing vision-languagemodels easy and less costly. Moreover, to unlock an advanced interactiveexperience like GPT-4o, we further design a sentence-wise decoding solution forstreaming speech generation. Extensive experiments demonstrate that Olasurpasses existing open omni-modal LLMs across all modalities while achievinghighly competitive performance compared to state-of-the-art specialized modelsof similar sizes. We aim to make Ola a fully open omni-modal understandingsolution to advance future research in this emerging field. Model weights,code, and data are open-sourced at https://github.com/Ola-Omni/Ola.</description><author>Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao</author><pubDate>Thu, 06 Feb 2025 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04328v1</guid></item><item><title>Value-Based Deep RL Scales Predictably</title><link>http://arxiv.org/abs/2502.04327v1</link><description>Scaling data and compute is critical to the success of machine learning.However, scaling demands predictability: we want methods to not only performwell with more compute or data, but also have their performance be predictablefrom small-scale runs, without running the large-scale experiment. In thispaper, we show that value-based off-policy RL methods are predictable despitecommunity lore regarding their pathological behavior. First, we show that dataand compute requirements to attain a given performance level lie on a Paretofrontier, controlled by the updates-to-data (UTD) ratio. By estimating thisfrontier, we can predict this data requirement when given more compute, andthis compute requirement when given more data. Second, we determine the optimalallocation of a total resource budget across data and compute for a givenperformance and use it to determine hyperparameters that maximize performancefor a given budget. Third, this scaling behavior is enabled by first estimatingpredictable relationships between hyperparameters, which is used to manageeffects of overfitting and plasticity loss unique to RL. We validate ourapproach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAIgym, and IsaacGym, when extrapolating to higher levels of data, compute,budget, or performance.</description><author>Oleh Rybkin, Michal Nauman, Preston Fu, Charlie Snell, Pieter Abbeel, Sergey Levine, Aviral Kumar</author><pubDate>Thu, 06 Feb 2025 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04327v1</guid></item><item><title>WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs</title><link>http://arxiv.org/abs/2502.04326v1</link><description>In this paper, we introduce WorldSense, the first benchmark to assess themulti-modal video understanding, that simultaneously encompasses visual, audio,and text inputs. In contrast to existing benchmarks, our WorldSense has severalfeatures: (i) collaboration of omni-modality, we design the evaluation tasks tofeature a strong coupling of audio and video, requiring models to effectivelyutilize the synergistic perception of omni-modality; (ii) diversity of videosand tasks, WorldSense encompasses a diverse collection of 1,662 audio-visualsynchronised videos, systematically categorized into 8 primary domains and 67fine-grained subcategories to cover the broad scenarios, and 3,172 multi-choiceQA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii)high-quality annotations, all the QA pairs are manually labeled by 80 expertannotators with multiple rounds of correction to ensure quality. Based on ourWorldSense, we extensively evaluate various state-of-the-art models. Theexperimental results indicate that existing models face significant challengesin understanding real-world scenarios (48.0% best accuracy). We hope ourWorldSense can provide a platform for evaluating the ability in constructingand understanding coherent contexts from omni-modality.</description><author>Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, Weidi Xie</author><pubDate>Thu, 06 Feb 2025 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04326v1</guid></item><item><title>Can Grammarly and ChatGPT accelerate language change? AI-powered technologies and their impact on the English language: wordiness vs. conciseness</title><link>http://arxiv.org/abs/2502.04324v1</link><description>The proliferation of NLP-powered language technologies, AI-based naturallanguage generation models, and English as a mainstream means of communicationamong both native and non-native speakers make the output of AI-powered toolsespecially intriguing to linguists. This paper investigates how Grammarly andChatGPT affect the English language regarding wordiness vs. conciseness. A casestudy focusing on the purpose subordinator in order to is presented toillustrate the way in which Grammarly and ChatGPT recommend shorter grammaticalstructures instead of longer and more elaborate ones. Although the analysedsentences were produced by native speakers, are perfectly correct, and wereextracted from a language corpus of contemporary English, both Grammarly andChatGPT suggest more conciseness and less verbosity, even for relatively shortsentences. The present article argues that technologies such as Grammarly notonly mirror language change but also have the potential to facilitate oraccelerate it.</description><author>Karolina Rudnicka</author><pubDate>Thu, 06 Feb 2025 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04324v1</guid></item><item><title>The Uniformly Rotated Mondrian Kernel</title><link>http://arxiv.org/abs/2502.04323v1</link><description>First proposed by Rahimi and Recht, random features are used to decrease thecomputational cost of kernel machines in large-scale problems. The Mondriankernel is one such example of a fast random feature approximation of theLaplace kernel, generated by a computationally efficient hierarchical randompartition of the input space known as the Mondrian process. In this work, westudy a variation of this random feature map by using uniformly randomlyrotated Mondrian processes to approximate a kernel that is invariant underrotations. We obtain a closed-form expression for this isotropic kernel, aswell as a uniform convergence rate of the uniformly rotated Mondrian kernel tothis limit. To this end, we utilize techniques from the theory of stationaryrandom tessellations in stochastic geometry and prove a new result on thegeometry of the typical cell of the superposition of uniformly random rotationsof Mondrian tessellations. Finally, we test the empirical performance of thisrandom feature map on both synthetic and real-world datasets, demonstrating itsimproved performance over the Mondrian kernel on a debiased dataset.</description><author>Calvin Osborne, Eliza O'Reilly</author><pubDate>Thu, 06 Feb 2025 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04323v1</guid></item><item><title>Variation of sentence length across time and genre</title><link>http://arxiv.org/abs/2502.04321v1</link><description>The goal of this paper is threefold: i) to present some practical aspects ofusing full-text version of Corpus of Historical American English (COHA), thelargest diachronic multi-genre corpus of the English language, in theinvestigation of a linguistic trend of change; ii) to test a widely heldassumption that sentence length in written English has been steadily decreasingover the past few centuries; iii) to point to a possible link between thechanges in sentence length and changes in the English syntactic usage. Theempirical proof of concept for iii) is provided by the decline in the frequencyof the non-finite purpose subordinator in order to. Sentence length, genre andthe likelihood of occurrence of in order to are shown to be interrelated.</description><author>Karolina Rudnicka</author><pubDate>Thu, 06 Feb 2025 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04321v1</guid></item><item><title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title><link>http://arxiv.org/abs/2502.04322v1</link><description>Despite extensive safety alignment efforts, large language models (LLMs)remain vulnerable to jailbreak attacks that elicit harmful behavior. Whileexisting studies predominantly focus on attack methods that require technicalexpertise, two critical questions remain underexplored: (1) Are jailbrokenresponses truly useful in enabling average users to carry out harmful actions?(2) Do safety vulnerabilities exist in more common, simple human-LLMinteractions? In this paper, we demonstrate that LLM responses most effectivelyfacilitate harmful actions when they are both actionable and informative--twoattributes easily elicited in multi-step, multilingual interactions. Using thisinsight, we propose HarmScore, a jailbreak metric that measures how effectivelyan LLM response enables harmful actions, and Speak Easy, a simple multi-step,multilingual attack framework. Notably, by incorporating Speak Easy into directrequest and jailbreak baselines, we see an average absolute increase of 0.319in Attack Success Rate and 0.426 in HarmScore in both open-source andproprietary LLMs across four safety benchmarks. Our work reveals a critical yetoften overlooked vulnerability: Malicious users can easily exploit commoninteraction patterns for harmful intentions.</description><author>Yik Siu Chan, Narutatsu Ri, Yuxin Xiao, Marzyeh Ghassemi</author><pubDate>Thu, 06 Feb 2025 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04322v1</guid></item><item><title>ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features</title><link>http://arxiv.org/abs/2502.04320v1</link><description>Do the rich representations of multi-modal diffusion transformers (DiTs)exhibit unique properties that enhance their interpretability? We introduceConceptAttention, a novel method that leverages the expressive power of DiTattention layers to generate high-quality saliency maps that precisely locatetextual concepts within images. Without requiring additional training,ConceptAttention repurposes the parameters of DiT attention layers to producehighly contextualized concept embeddings, contributing the major discovery thatperforming linear projections in the output space of DiT attention layersyields significantly sharper saliency maps compared to commonly usedcross-attention mechanisms. Remarkably, ConceptAttention even achievesstate-of-the-art performance on zero-shot image segmentation benchmarks,outperforming 11 other zero-shot interpretability methods on theImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Ourwork contributes the first evidence that the representations of multi-modal DiTmodels like Flux are highly transferable to vision tasks like segmentation,even outperforming multi-modal foundation models like CLIP.</description><author>Alec Helbling, Tuna Han Salih Meral, Ben Hoover, Pinar Yanardag, Duen Horng Chau</author><pubDate>Thu, 06 Feb 2025 18:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04320v1</guid></item><item><title>sshELF: Single-Shot Hierarchical Extrapolation of Latent Features for 3D Reconstruction from Sparse-Views</title><link>http://arxiv.org/abs/2502.04318v1</link><description>Reconstructing unbounded outdoor scenes from sparse outward-facing viewsposes significant challenges due to minimal view overlap. Previous methodsoften lack cross-scene understanding and their primitive-centric formulationsoverload local features to compensate for missing global context, resulting inblurriness in unseen parts of the scene. We propose sshELF, a fast, single-shotpipeline for sparse-view 3D scene reconstruction via hierarchal extrapolationof latent features. Our key insights is that disentangling informationextrapolation from primitive decoding allows efficient transfer of structuralpatterns across training scenes. Our method: (1) learns cross-scene priors togenerate intermediate virtual views to extrapolate to unobserved regions, (2)offers a two-stage network design separating virtual view generation from 3Dprimitive decoding for efficient training and modular model design, and (3)integrates a pre-trained foundation model for joint inference of latentfeatures and texture, improving scene understanding and generalization. sshELFcan reconstruct 360 degree scenes from six sparse input views and achievescompetitive results on synthetic and real-world datasets. We find that sshELFfaithfully reconstructs occluded regions, supports real-time rendering, andprovides rich latent features for downstream applications. The code will bereleased.</description><author>Eyvaz Najafli, Marius Kästingschäfer, Sebastian Bernhard, Thomas Brox, Andreas Geiger</author><pubDate>Thu, 06 Feb 2025 18:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04318v1</guid></item><item><title>Factorized Implicit Global Convolution for Automotive Computational Fluid Dynamics Prediction</title><link>http://arxiv.org/abs/2502.04317v1</link><description>Computational Fluid Dynamics (CFD) is crucial for automotive design,requiring the analysis of large 3D point clouds to study how vehicle geometryaffects pressure fields and drag forces. However, existing deep learningapproaches for CFD struggle with the computational complexity of processinghigh-resolution 3D data. We propose Factorized Implicit Global Convolution(FIGConv), a novel architecture that efficiently solves CFD problems for verylarge 3D meshes with arbitrary input and output geometries. FIGConv achievesquadratic complexity $O(N^2)$, a significant improvement over existing 3Dneural CFD models that require cubic complexity $O(N^3)$. Our approach combinesFactorized Implicit Grids to approximate high-resolution domains, efficientglobal convolutions through 2D reparameterization, and a U-shaped architecturefor effective information gathering and integration. We validate our approachon the industry-standard Ahmed body dataset and the large-scale DrivAerNetdataset. In DrivAerNet, our model achieves an $R^2$ value of 0.95 for dragprediction, outperforming the previous state-of-the-art by a significantmargin. This represents a 40% improvement in relative mean squared error and a70% improvement in absolute mean squared error over previous methods.</description><author>Chris Choy, Alexey Kamenev, Jean Kossaifi, Max Rietmann, Jan Kautz, Kamyar Azizzadenesheli</author><pubDate>Thu, 06 Feb 2025 18:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04317v1</guid></item><item><title>ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters</title><link>http://arxiv.org/abs/2502.04315v1</link><description>Recent advances in large language models (LLMs) have shown remarkableperformance across diverse tasks. However, these models are typically deployedwith fixed weights, which limits their ability to adapt dynamically to thevariability inherent in real-world data during inference. This paper introducesChamaleonLLM, a novel framework that enables inference-time adaptation of LLMsby leveraging batch-aware clustering and on-the-fly generation of low-rankupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeablemasks), our method dynamically generates adaptive modifications to the decoderweights based on the aggregated statistics of clustered batches. Byintelligently grouping similar inputs and computing context-aware low-rankupdates via a hyper-network, ChamaleonLLM achieves significant performancegains, outperforming conventional LoRA methods while eliminating the overheadof maintaining multiple expert models. Our experiments highlight the potentialof our approach to serve as a versatile and highly adaptive solution forlanguage model inference. ChamaleonLLM is open-sourced to ensure thereproducibility of our experiments:https://anonymous.4open.science/r/ChamaleonLLM/</description><author>Kamer Ali Yuksel, Hassan Sawaf</author><pubDate>Thu, 06 Feb 2025 18:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04315v1</guid></item><item><title>BOUQuET: dataset, Benchmark and Open initiative for Universal Quality Evaluation in Translation</title><link>http://arxiv.org/abs/2502.04314v1</link><description>This paper presents BOUQuET, a multicentric and multi-register/domain datasetand benchmark, and its broader collaborative extension initiative. This datasetis handcrafted in non-English languages first, each of these source languagesbeing represented among the 23 languages commonly used by half of the world'spopulation and therefore having the potential to serve as pivot languages thatwill enable more accurate translations. The dataset is specially designed toavoid contamination and be multicentric, so as to enforce representation ofmultilingual language features. In addition, the dataset goes beyond thesentence level, as it is organized in paragraphs of various lengths. Comparedwith related machine translation (MT) datasets, we show that BOUQuET has abroader representation of domains while simplifying the translation task fornon-experts. Therefore, BOUQuET is specially suitable for the open initiativeand call for translation participation that we are launching to extend it to amulti-way parallel corpus to any written language.</description><author>The Omnilingual MT Team, Pierre Andrews, Mikel Artetxe, Mariano Coria Meglioli, Marta R. Costa-jussà, Joe Chuang, David Dale, Cynthia Gao, Jean Maillard, Alex Mourachko, Christophe Ropers, Safiyyah Saleem, Eduardo Sánchez, Ioannis Tsiamas, Arina Turkatenko, Albert Ventayol-Boada, Shireen Yates</author><pubDate>Thu, 06 Feb 2025 18:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04314v1</guid></item><item><title>Great Models Think Alike and this Undermines AI Oversight</title><link>http://arxiv.org/abs/2502.04313v1</link><description>As Language Model (LM) capabilities advance, evaluating and supervising themat scale is getting harder for humans. There is hope that other language modelscan automate both these tasks, which we refer to as "AI Oversight". We studyhow model similarity affects both aspects of AI oversight by proposing aprobabilistic metric for LM similarity based on overlap in model mistakes.Using this metric, we first show that LLM-as-a-judge scores favor modelssimilar to the judge, generalizing recent self-preference results. Then, westudy training on LM annotations, and find complementary knowledge between theweak supervisor and strong student model plays a crucial role in gains from"weak-to-strong generalization". As model capabilities increase, it becomesharder to find their mistakes, and we might defer more to AI oversight.However, we observe a concerning trend -- model mistakes are becoming moresimilar with increasing capabilities, pointing to risks from correlatedfailures. Our work underscores the importance of reporting and correcting formodel similarity, especially in the emerging paradigm of AI oversight.</description><author>Shashwat Goel, Joschka Struber, Ilze Amanda Auzina, Karuna K Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya Prabhu, Matthias Bethge, Jonas Geiping</author><pubDate>Thu, 06 Feb 2025 18:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04313v1</guid></item><item><title>Consistency of augmentation graph and network approximability in contrastive learning</title><link>http://arxiv.org/abs/2502.04312v1</link><description>Contrastive learning leverages data augmentation to develop featurerepresentation without relying on large labeled datasets. However, despite itsempirical success, the theoretical foundations of contrastive learning remainincomplete, with many essential guarantees left unaddressed, particularly therealizability assumption concerning neural approximability of an optimalspectral contrastive loss solution. In this work, we overcome these limitationsby analyzing the pointwise and spectral consistency of the augmentation graphLaplacian. We establish that, under specific conditions for data generation andgraph connectivity, as the augmented dataset size increases, the augmentationgraph Laplacian converges to a weighted Laplace-Beltrami operator on thenatural data manifold. These consistency results ensure that the graphLaplacian spectrum effectively captures the manifold geometry. Consequently,they give way to a robust framework for establishing neural approximability,directly resolving the realizability assumption in a current paradigm.</description><author>Chenghui Li, A. Martina Neuman</author><pubDate>Thu, 06 Feb 2025 18:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04312v1</guid></item><item><title>Conformal Risk Minimization with Variance Reduction</title><link>http://arxiv.org/abs/2411.01696v2</link><description>Conformal prediction (CP) is a distribution-free framework for achievingprobabilistic guarantees on black-box models. CP is generally applied to amodel post-training. Recent research efforts, on the other hand, have focusedon optimizing CP efficiency during training. We formalize this concept as theproblem of conformal risk minimization (CRM). In this direction, conformaltraining (ConfTr) by Stutz et al.(2022) is a technique that seeks to minimizethe expected prediction set size of a model by simulating CP in-betweentraining updates. Despite its potential, we identify a strong source of sampleinefficiency in ConfTr that leads to overly noisy estimated gradients,introducing training instability and limiting practical use. To address thischallenge, we propose variance-reduced conformal training (VR-ConfTr), a CRMmethod that incorporates a variance reduction technique in the gradientestimation of the ConfTr objective function. Through extensive experiments onvarious benchmark datasets, we demonstrate that VR-ConfTr consistently achievesfaster convergence and smaller prediction sets compared to baselines.</description><author>Sima Noorani, Orlando Romero, Nicolo Dal Fabbro, Hamed Hassani, George J. Pappas</author><pubDate>Thu, 06 Feb 2025 18:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01696v2</guid></item><item><title>SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning</title><link>http://arxiv.org/abs/2407.17460v2</link><description>Reinforcement learning (RL) enables social robots to generate trajectorieswithout relying on human-designed rules or interventions, making it generallymore effective than rule-based systems in adapting to complex, dynamicreal-world scenarios. However, social navigation is a safety-critical task thatrequires robots to avoid collisions with pedestrians, whereas existing RL-basedsolutions often fall short of ensuring safety in complex environments. In thispaper, we propose SoNIC, which to the best of our knowledge is the firstalgorithm that integrates adaptive conformal inference (ACI) with constrainedreinforcement learning (CRL) to enable safe policy learning for socialnavigation. Specifically, our method not only augments RL observations withACI-generated nonconformity scores, which inform the agent of the quantifieduncertainty but also employs these uncertainty estimates to effectively guidethe behaviors of RL agents by using constrained reinforcement learning. Thisintegration regulates the behaviors of RL agents and enables them to handlesafety-critical situations. On the standard CrowdNav benchmark, our methodachieves a success rate of 96.93%, which is 11.67% higher than the previousstate-of-the-art RL method and results in 4.5 times fewer collisions and 2.8times fewer intrusions to ground-truth human future trajectories as well asenhanced robustness in out-of-distribution scenarios. To further validate ourapproach, we deploy our algorithm on a real robot by developing a ROS2-basednavigation system. Our experiments demonstrate that the system can generaterobust and socially polite decision-making when interacting with both sparseand dense crowds. The video demos can be found on our project website:https://sonic-social-nav.github.io/.</description><author>Jianpeng Yao, Xiaopan Zhang, Yu Xia, Zejin Wang, Amit K. Roy-Chowdhury, Jiachen Li</author><pubDate>Thu, 06 Feb 2025 18:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17460v2</guid></item><item><title>SWAG: Long-term Surgical Workflow Prediction with Generative-based Anticipation</title><link>http://arxiv.org/abs/2412.18849v2</link><description>While existing approaches excel at recognising current surgical phases, theyprovide limited foresight and intraoperative guidance into future proceduralsteps. Similarly, current anticipation methods are constrained to predictingshort-term and singular events, neglecting the dense and sequential nature ofsurgical workflows. To address these needs and limitations, we propose SWAG(Surgical Workflow Anticipative Generation), a framework to combine phaserecognition and anticipation, using a generative approach for surgical workflowguidance. This paper investigates two distinct decoding methods-single-pass(SP) and auto-regressive (AR)-to generate sequences of future surgical phasesat minute intervals over long horizons of up to 60 minutes. We propose a novelembedding approach using prior knowledge to enhance the accuracy of phaseanticipation. Additionally, our anticipative framework offers remaining timeregression and proposes a regression-to-classification (R2C) method. SWAG'sperformance was evaluated on the Cholec80 and AutoLaparo21 datasets. Oursingle-pass model with prior knowledge embeddings (SP*) achieves 49.8% meanaccuracy over 18-minute anticipation on AutoLaparo21, while the simple SP withR2C extension reaches 56.6% mean accuracy over the same horizon on Cholec80.Moreover, our approach outperforms existing methods on the phase remaining timeregression task, achieving weighted mean absolute errors of 0.32 and 0.48minutes for 2- and 3-minute horizons, respectively. SWAG demonstratesversatility across classification and regression tasks and creates a temporalcontinuity between surgical workflow recognition and anticipation. Whilefurther studies are required to understand the impact of generative-basedanticipation intraoperatively, our method provides steps towards thisdirection.</description><author>Maxence Boels, Yang Liu, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin</author><pubDate>Thu, 06 Feb 2025 18:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18849v2</guid></item><item><title>Finding Pegasus: Enhancing Unsupervised Anomaly Detection in High-Dimensional Data using a Manifold-Based Approach</title><link>http://arxiv.org/abs/2502.04310v1</link><description>Unsupervised machine learning methods are well suited to searching foranomalies at scale but can struggle with the high-dimensional representation ofmany modern datasets, hence dimensionality reduction (DR) is often performedfirst. In this paper we analyse unsupervised anomaly detection (AD) from theperspective of the manifold created in DR. We present an idealisedillustration, "Finding Pegasus", and a novel formal framework with which wecategorise AD methods and their results into "on manifold" and "off manifold".We define these terms and show how they differ. We then use this insight todevelop an approach of combining AD methods which significantly boosts ADrecall without sacrificing precision in situations employing high DR. Whentested on MNIST data, our approach of combining AD methods improves recall byas much as 16 percent compared with simply combining with the best standaloneAD method (Isolation Forest), a result which shows great promise for itsapplication to real-world data.</description><author>R. P. Nathan, Nikolaos Nikolaou, Ofer Lahav</author><pubDate>Thu, 06 Feb 2025 18:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04310v1</guid></item><item><title>From Probability to Counterfactuals: the Increasing Complexity of Satisfiability in Pearl's Causal Hierarchy</title><link>http://arxiv.org/abs/2405.07373v3</link><description>The framework of Pearl's Causal Hierarchy (PCH) formalizes three types ofreasoning: probabilistic (i.e. purely observational), interventional, andcounterfactual, that reflect the progressive sophistication of human thoughtregarding causation. We investigate the computational complexity aspects ofreasoning in this framework focusing mainly on satisfiability problemsexpressed in probabilistic and causal languages across the PCH. That is, givena system of formulas in the standard probabilistic and causal languages, doesthere exist a model satisfying the formulas? Our main contribution is to prove the exact computational complexitiesshowing that languages allowing addition and marginalization (via the summationoperator) yield NP^PP, PSPACE-, and NEXP-complete satisfiability problems,depending on the level of the PCH. These are the first results to demonstrate astrictly increasing complexity across the PCH: from probabilistic to causal andcounterfactual reasoning. On the other hand, in the case of full languages,i.e. allowing addition, marginalization, and multiplication, we show that thesatisfiability for the counterfactual level remains the same as for theprobabilistic and causal levels, solving an open problem in the field.</description><author>Julian Dörfler, Benito van der Zander, Markus Bläser, Maciej Liskiewicz</author><pubDate>Thu, 06 Feb 2025 18:53:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07373v3</guid></item><item><title>Targeted Learning for Data Fairness</title><link>http://arxiv.org/abs/2502.04309v1</link><description>Data and algorithms have the potential to produce and perpetuatediscrimination and disparate treatment. As such, significant effort has beeninvested in developing approaches to defining, detecting, and eliminatingunfair outcomes in algorithms. In this paper, we focus on performingstatistical inference for fairness. Prior work in fairness inference haslargely focused on inferring the fairness properties of a given predictivealgorithm. Here, we expand fairness inference by evaluating fairness in thedata generating process itself, referred to here as data fairness. We performinference on data fairness using targeted learning, a flexible framework fornonparametric inference. We derive estimators demographic parity, equalopportunity, and conditional mutual information. Additionally, we find that ourestimators for probabilistic metrics exploit double robustness. To validate ourapproach, we perform several simulations and apply our estimators to real data.</description><author>Alexander Asemota, Giles Hooker</author><pubDate>Thu, 06 Feb 2025 18:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04309v1</guid></item><item><title>HOG-Diff: Higher-Order Guided Diffusion for Graph Generation</title><link>http://arxiv.org/abs/2502.04308v1</link><description>Graph generation is a critical yet challenging task as empirical analysesrequire a deep understanding of complex, non-Euclidean structures. Althoughdiffusion models have recently made significant achievements in graphgeneration, these models typically adapt from the frameworks designed for imagegeneration, making them ill-suited for capturing the topological properties ofgraphs. In this work, we propose a novel Higher-order Guided Diffusion(HOG-Diff) model that follows a coarse-to-fine generation curriculum and isguided by higher-order information, enabling the progressive generation ofplausible graphs with inherent topological structures. We further prove thatour model exhibits a stronger theoretical guarantee than classical diffusionframeworks. Extensive experiments on both molecular and generic graphgeneration tasks demonstrate that our method consistently outperforms orremains competitive with state-of-the-art baselines. Our code is available athttps://github.com/Yiminghh/HOG-Diff.</description><author>Yiming Huang, Tolga Birdal</author><pubDate>Thu, 06 Feb 2025 18:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04308v1</guid></item><item><title>DexterityGen: Foundation Controller for Unprecedented Dexterity</title><link>http://arxiv.org/abs/2502.04307v1</link><description>Teaching robots dexterous manipulation skills, such as tool use, presents asignificant challenge. Current approaches can be broadly categorized into twostrategies: human teleoperation (for imitation learning) and sim-to-realreinforcement learning. The first approach is difficult as it is hard forhumans to produce safe and dexterous motions on a different embodiment withouttouch feedback. The second RL-based approach struggles with the domain gap andinvolves highly task-specific reward engineering on complex tasks. Our keyinsight is that RL is effective at learning low-level motion primitives, whilehumans excel at providing coarse motion commands for complex, long-horizontasks. Therefore, the optimal solution might be a combination of bothapproaches. In this paper, we introduce DexterityGen (DexGen), which uses RL topretrain large-scale dexterous motion primitives, such as in-hand rotation ortranslation. We then leverage this learned dataset to train a dexterousfoundational controller. In the real world, we use human teleoperation as aprompt to the controller to produce highly dexterous behavior. We evaluate theeffectiveness of DexGen in both simulation and real world, demonstrating thatit is a general-purpose controller that can realize input dexterousmanipulation commands and significantly improves stability by 10-100x measuredas duration of holding objects across diverse tasks. Notably, with DexGen wedemonstrate unprecedented dexterous skills including diverse objectreorientation and dexterous tool use such as pen, syringe, and screwdriver forthe first time.</description><author>Zhao-Heng Yin, Changhao Wang, Luis Pineda, Francois Hogan, Krishna Bodduluri, Akash Sharma, Patrick Lancaster, Ishita Prasad, Mrinal Kalakrishnan, Jitendra Malik, Mike Lambeta, Tingfan Wu, Pieter Abbeel, Mustafa Mukadam</author><pubDate>Thu, 06 Feb 2025 18:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04307v1</guid></item><item><title>Understanding Self-Supervised Learning via Gaussian Mixture Models</title><link>http://arxiv.org/abs/2411.03517v2</link><description>Self-supervised learning attempts to learn representations from un-labeleddata; it does so via a loss function that encourages the embedding of a pointto be close to that of its augmentations. This simple idea performs remarkablywell, yet it is not precisely theoretically understood why this is the case. Inthis paper we analyze self-supervised learning in a natural context:dimensionality reduction in Gaussian Mixture Models. Crucially, we define anaugmentation of a data point as being another independent draw from the sameunderlying mixture component. We show that vanilla contrastive learning(specifically, the InfoNCE loss) is able to find the optimal lower-dimensionalsubspace even when the Gaussians are not isotropic -- something that vanillaspectral techniques cannot do. We also prove a similar result for"non-contrastive" self-supervised learning (i.e., SimSiam loss). We furtherextend our analyses to multi-modal contrastive learning algorithms (e.g.,CLIP). In this setting we show that contrastive learning learns the subset offisher-optimal subspace, effectively filtering out all the noise from thelearnt representations. Finally, we corroborate our theoretical finding throughsynthetic data experiments.</description><author>Parikshit Bansal, Ali Kavis, Sujay Sanghavi</author><pubDate>Thu, 06 Feb 2025 18:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03517v2</guid></item><item><title>ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization</title><link>http://arxiv.org/abs/2502.04306v1</link><description>Recent research has leveraged large language model multi-agent systems forcomplex problem-solving while trying to reduce the manual effort required tobuild them, driving the development of automated agent workflow optimizationmethods. However, existing methods remain inflexible due to representationallimitations, a lack of adaptability, and poor scalability when relying ondiscrete optimization techniques. We address these challenges with ScoreFlow, asimple yet high-performance framework that leverages efficient gradient-basedoptimization in a continuous space. ScoreFlow incorporates Score-DPO, a novelvariant of the direct preference optimization method that accounts forquantitative feedback. Across six benchmarks spanning question answering,coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement overexisting baselines. Moreover, it empowers smaller models to outperform largerones with lower inference costs. Project:https://github.com/Gen-Verse/ScoreFlow</description><author>Yinjie Wang, Ling Yang, Guohao Li, Mengdi Wang, Bryon Aragam</author><pubDate>Thu, 06 Feb 2025 18:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04306v1</guid></item><item><title>Strong Equivalence in Answer Set Programming with Constraints</title><link>http://arxiv.org/abs/2502.04302v1</link><description>We investigate the concept of strong equivalence within the extendedframework of Answer Set Programming with constraints. Two groups of rules areconsidered strongly equivalent if, informally speaking, they have the samemeaning in any context. We demonstrate that, under certain assumptions, strongequivalence between rule sets in this extended setting can be preciselycharacterized by their equivalence in the logic of Here-and-There withconstraints. Furthermore, we present a translation from the language of severalclingo-based answer set solvers that handle constraints into the language ofHere-and-There with constraints. This translation enables us to leverage thelogic of Here-and-There to reason about strong equivalence within the contextof these solvers. We also explore the computational complexity of determiningstrong equivalence in this context.</description><author>Pedro Cabalar, Jorge Fandinno, Torsten Schaub, Philipp Wanko</author><pubDate>Thu, 06 Feb 2025 18:43:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04302v1</guid></item><item><title>Estimating the Probabilities of Rare Outputs in Language Models</title><link>http://arxiv.org/abs/2410.13211v2</link><description>We consider the problem of low probability estimation: given a machinelearning model and a formally-specified input distribution, how can we estimatethe probability of a binary property of the model's output, even when thatprobability is too small to estimate by random sampling? This problem ismotivated by the need to improve worst-case performance, which distributionshift can make much more likely. We study low probability estimation in thecontext of argmax sampling from small transformer language models. We comparetwo types of methods: importance sampling, which involves searching for inputsgiving rise to the rare output, and activation extrapolation, which involvesextrapolating a probability distribution fit to the model's logits. We findthat importance sampling outperforms activation extrapolation, but bothoutperform naive sampling. Finally, we explain how minimizing the probabilityestimate of an undesirable behavior generalizes adversarial training, and arguethat new methods for low probability estimation are needed to provide strongerguarantees about worst-case performance.</description><author>Gabriel Wu, Jacob Hilton</author><pubDate>Thu, 06 Feb 2025 18:43:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13211v2</guid></item><item><title>Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of Model Prediction and Scientific Understanding of Soil Organic Carbon</title><link>http://arxiv.org/abs/2502.00672v2</link><description>Big data and the rapid development of artificial intelligence (AI) provideunprecedented opportunities to enhance our understanding of the global carboncycle and other biogeochemical processes. However, retrieving mechanisticknowledge from big data remains a challenge. Here, we develop aBiogeochemistry-Informed Neural Network (BINN) that seamlessly integrates avectorized process-based soil carbon cycle model (i.e., Community Land Modelversion 5, CLM5) into a neural network (NN) structure to examine mechanismsgoverning soil organic carbon (SOC) storage from big data. BINN demonstrateshigh accuracy in retrieving biogeochemical parameter values from synthetic datain a parameter recovery experiment. We use BINN to predict six major processesregulating the soil carbon cycle (or components in process-based models) from25,925 observed SOC profiles across the conterminous US and compared them withthe same processes previously retrieved by a Bayesian inference-basedPROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao etal. 2020; 2023). The high agreement between the spatial patterns of theretrieved processes using the two approaches with an average correlationcoefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledgefrom big data. Additionally, the integration of neural networks andprocess-based models in BINN improves computational efficiency by more than 50times over PRODA. We conclude that BINN is a transformative tool that harnessesthe power of both AI and process-based modeling, facilitating new scientificdiscoveries while improving interpretability and accuracy of Earth systemmodels.</description><author>Haodi Xu, Joshua Fan, Feng Tao, Lifen Jiang, Fengqi You, Benjamin Z. Houlton, Ying Sun, Carla P. Gomes, Yiqi Luo</author><pubDate>Thu, 06 Feb 2025 18:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.00672v2</guid></item><item><title>MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation</title><link>http://arxiv.org/abs/2502.04299v1</link><description>This paper presents a method that allows users to design cinematic videoshots in the context of image-to-video generation. Shot design, a criticalaspect of filmmaking, involves meticulously planning both camera movements andobject motions in a scene. However, enabling intuitive shot design in modernimage-to-video generation systems presents two main challenges: first,effectively capturing user intentions on the motion design, where both cameramovements and scene-space object motions must be specified jointly; and second,representing motion information that can be effectively utilized by a videodiffusion model to synthesize the image animations. To address thesechallenges, we introduce MotionCanvas, a method that integrates user-drivencontrols into image-to-video (I2V) generation models, allowing users to controlboth object and camera motions in a scene-aware manner. By connecting insightsfrom classical computer graphics and contemporary video generation techniques,we demonstrate the ability to achieve 3D-aware motion control in I2V synthesiswithout requiring costly 3D-related training data. MotionCanvas enables usersto intuitively depict scene-space motion intentions, and translates them intospatiotemporal motion-conditioning signals for video diffusion models. Wedemonstrate the effectiveness of our method on a wide range of real-world imagecontent and shot-design scenarios, highlighting its potential to enhance thecreative workflows in digital content creation and adapt to various image andvideo editing applications.</description><author>Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, Feng Liu</author><pubDate>Thu, 06 Feb 2025 18:41:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04299v1</guid></item><item><title>Statistical guarantees for continuous-time policy evaluation: blessing of ellipticity and new tradeoffs</title><link>http://arxiv.org/abs/2502.04297v1</link><description>We study the estimation of the value function for continuous-time Markovdiffusion processes using a single, discretely observed ergodic trajectory. Ourwork provides non-asymptotic statistical guarantees for the least-squarestemporal-difference (LSTD) method, with performance measured in the first-orderSobolev norm. Specifically, the estimator attains an $O(1 / \sqrt{T})$convergence rate when using a trajectory of length $T$; notably, this rate isachieved as long as $T$ scales nearly linearly with both the mixing time of thediffusion and the number of basis functions employed. A key insight of our approach is that the ellipticity inherent in thediffusion process ensures robust performance even as the effective horizondiverges to infinity. Moreover, we demonstrate that the Markovian component ofthe statistical error can be controlled by the approximation error, while themartingale component grows at a slower rate relative to the number of basisfunctions. By carefully balancing these two sources of error, our analysisreveals novel trade-offs between approximation and statistical errors.</description><author>Wenlong Mou</author><pubDate>Thu, 06 Feb 2025 18:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04297v1</guid></item><item><title>Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression</title><link>http://arxiv.org/abs/2502.04296v1</link><description>We propose Heterogeneous Masked Autoregression (HMA) for modelingaction-video dynamics to generate high-quality data and evaluation in scalingrobot learning. Building interactive video world models and policies forrobotics is difficult due to the challenge of handling diverse settings whilemaintaining computational efficiency to run in real time. HMA usesheterogeneous pre-training from observations and action sequences acrossdifferent robotic embodiments, domains, and tasks. HMA uses maskedautoregression to generate quantized or soft tokens for video predictions.\ourshort achieves better visual fidelity and controllability than the previousrobotic video generation models with 15 times faster speed in the real world.After post-training, this model can be used as a video simulator from low-levelaction inputs for evaluating policies and generating synthetic data. See thislink https://liruiw.github.io/hma for more information.</description><author>Lirui Wang, Kevin Zhao, Chaoqi Liu, Xinlei Chen</author><pubDate>Thu, 06 Feb 2025 18:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04296v1</guid></item><item><title>MPAX: Mathematical Programming in JAX</title><link>http://arxiv.org/abs/2412.09734v2</link><description>This paper presents MPAX (Mathematical Programming in JAX), a versatile andefficient toolbox for integrating linear programming (LP) into machine learningworkflows. MPAX implemented the state-of-the-art first-order methods, restartedaverage primal-dual hybrid gradient and reflected restarted Halpern primal-dualhybrid gradient, to solve LPs in JAX. This provides native support for hardwareaccelerations along with features like batch solving, auto-differentiation, anddevice parallelism. Extensive numerical experiments demonstrate the advantagesof MPAX over existing solvers. The solver is available athttps://github.com/MIT-Lu-Lab/MPAX.</description><author>Haihao Lu, Zedong Peng, Jinwen Yang</author><pubDate>Thu, 06 Feb 2025 18:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09734v2</guid></item><item><title>Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization</title><link>http://arxiv.org/abs/2502.04295v1</link><description>Large Language Models (LLMs) have shown significant capability across varioustasks, with their real-world effectiveness often driven by prompt design. Whilerecent research has focused on optimizing prompt content, the role of promptformatting, a critical but often overlooked dimension, has received limitedsystematic investigation. In this paper, we introduce Content-Format IntegratedPrompt Optimization (CFPO), an innovative methodology that jointly optimizesboth prompt content and formatting through an iterative refinement process.CFPO leverages natural language mutations to explore content variations andemploys a dynamic format exploration strategy that systematically evaluatesdiverse format options. Our extensive evaluations across multiple tasks andopen-source LLMs demonstrate that CFPO demonstrates measurable performanceimprovements compared to content-only optimization methods. This highlights theimportance of integrated content-format optimization and offers a practical,model-agnostic approach to enhancing LLM performance. Code will be available athttps://github.com/HenryLau7/CFPO.</description><author>Yuanye Liu, Jiahang Xu, Li Lyna Zhang, Qi Chen, Xuan Feng, Yang Chen, Zhongxin Guo, Yuqing Yang, Cheng Peng</author><pubDate>Thu, 06 Feb 2025 18:36:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04295v1</guid></item><item><title>Prediction-Powered E-Values</title><link>http://arxiv.org/abs/2502.04294v1</link><description>Quality statistical inference requires a sufficient amount of data, which canbe missing or hard to obtain. To this end, prediction-powered inference hasrisen as a promising methodology, but existing approaches are largely limitedto Z-estimation problems such as inference of means and quantiles. In thispaper, we apply ideas of prediction-powered inference to e-values. By doing so,we inherit all the usual benefits of e-values -- such as anytime-validity,post-hoc validity and versatile sequential inference -- as well as greatlyexpand the set of inferences achievable in a prediction-powered manner. Inparticular, we show that every inference procedure that can be framed in termsof e-values has a prediction-powered counterpart, given by our method. Weshowcase the effectiveness of our framework across a wide range of inferencetasks, from simple hypothesis testing and confidence intervals to more involvedprocedures for change-point detection and causal discovery, which were out ofreach of previous techniques. Our approach is modular and easily integrableinto existing algorithms, making it a compelling choice for practicalapplications.</description><author>Daniel Csillag, Claudio José Struchiner, Guilherme Tegoni Goedert</author><pubDate>Thu, 06 Feb 2025 18:36:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04294v1</guid></item><item><title>GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation</title><link>http://arxiv.org/abs/2502.04293v1</link><description>A key challenge in model-free category-level pose estimation is theextraction of contextual object features that generalize across varyinginstances within a specific category. Recent approaches leverage foundationalfeatures to capture semantic and geometry cues from data. However, theseapproaches fail under partial visibility. We overcome this with afirst-complete-then-aggregate strategy for feature extraction utilizing classpriors. In this paper, we present GCE-Pose, a method that enhances poseestimation for novel instances by integrating category-level global contextprior. GCE-Pose performs semantic shape reconstruction with a proposed SemanticShape Reconstruction (SSR) module. Given an unseen partial RGB-D objectinstance, our SSR module reconstructs the instance's global geometry andsemantics by deforming category-specific 3D semantic prototypes through alearned deep Linear Shape Model. We further introduce a Global Context Enhanced(GCE) feature fusion module that effectively fuses features from partial RGB-Dobservations and the reconstructed global context. Extensive experimentsvalidate the impact of our global context prior and the effectiveness of theGCE fusion module, demonstrating that GCE-Pose significantly outperformsexisting methods on challenging real-world datasets HouseCat6D andNOCS-REAL275. Our project page is available athttps://colin-de.github.io/GCE-Pose/.</description><author>Weihang Li, Hongli Xu, Junwen Huang, Hyunjun Jung, Peter KT Yu, Nassir Navab, Benjamin Busam</author><pubDate>Thu, 06 Feb 2025 18:35:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04293v1</guid></item><item><title>Every Call is Precious: Global Optimization of Black-Box Functions with Unknown Lipschitz Constants</title><link>http://arxiv.org/abs/2502.04290v1</link><description>Optimizing expensive, non-convex, black-box Lipschitz continuous functionspresents significant challenges, particularly when the Lipschitz constant ofthe underlying function is unknown. Such problems often demand numerousfunction evaluations to approximate the global optimum, which can beprohibitive in terms of time, energy, or resources. In this work, we introduceEvery Call is Precious (ECP), a novel global optimization algorithm thatminimizes unpromising evaluations by strategically focusing on potentiallyoptimal regions. Unlike previous approaches, ECP eliminates the need toestimate the Lipschitz constant, thereby avoiding additional functionevaluations. ECP guarantees no-regret performance for infinite evaluationbudgets and achieves minimax-optimal regret bounds within finite budgets.Extensive ablation studies validate the algorithm's robustness, while empiricalevaluations show that ECP outperforms 10 benchmark algorithms includingLipschitz, Bayesian, bandits, and evolutionary methods across 30multi-dimensional non-convex synthetic and real-world optimization problems,which positions ECP as a competitive approach for global optimization.</description><author>Fares Fourati, Salma Kharrat, Vaneet Aggarwal, Mohamed-Slim Alouini</author><pubDate>Thu, 06 Feb 2025 18:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04290v1</guid></item><item><title>Retro-Rank-In: A Ranking-Based Approach for Inorganic Materials Synthesis Planning</title><link>http://arxiv.org/abs/2502.04289v1</link><description>Retrosynthesis strategically plans the synthesis of a chemical targetcompound from simpler, readily available precursor compounds. This process iscritical for synthesizing novel inorganic materials, yet traditional methods ininorganic chemistry continue to rely on trial-and-error experimentation.Emerging machine-learning approaches struggle to generalize to entirely newreactions due to their reliance on known precursors, as they frameretrosynthesis as a multi-label classification task. To address theselimitations, we propose Retro-Rank-In, a novel framework that reformulates theretrosynthesis problem by embedding target and precursor materials into ashared latent space and learning a pairwise ranker on a bipartite graph ofinorganic compounds. We evaluate Retro-Rank-In's generalizability onchallenging retrosynthesis dataset splits designed to mitigate data duplicatesand overlaps. For instance, for Cr2AlB2, it correctly predicts the verifiedprecursor pair CrB + Al despite never seeing them in training, a capabilityabsent in prior work. Extensive experiments show that Retro-Rank-In sets a newstate-of-the-art, particularly in out-of-distribution generalization andcandidate set ranking, offering a powerful tool for accelerating inorganicmaterial synthesis.</description><author>Thorben Prein, Elton Pan, Sami Haddouti, Marco Lorenz, Janik Jehkul, Tymoteusz Wilk, Cansu Moran, Menelaos Panagiotis Fotiadis, Artur P. Toshev, Elsa Olivetti, Jennifer L. M. Rupp</author><pubDate>Thu, 06 Feb 2025 18:34:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04289v1</guid></item><item><title>Leveraging Geolocation in Clinical Records to Improve Alzheimer's Disease Diagnosis Using DMV Framework</title><link>http://arxiv.org/abs/2502.04288v1</link><description>Alzheimer's Disease (AD) early detection is critical for enabling timelyintervention and improving patient outcomes. This paper presents a DMVframework using Llama3-70B and GPT-4o as embedding models to analyze clinicalnotes and predict a continuous risk score associated with early AD onset.Framing the task as a regression problem, we model the relationship betweenlinguistic features in clinical notes (inputs) and a target variable (datavalue) that answers specific questions related to AD risk within certain topiccategories. By leveraging a multi-faceted feature set that includes geolocationdata, we capture additional environmental context potentially linked to AD. Ourresults demonstrate that the integration of the geolocation informationsignificantly decreases the error of predicting early AD risk scores over priormodels by 28.57% (Llama3-70B) and 33.47% (GPT4-o). Our findings suggest thatthis combined approach can enhance the predictive accuracy of AD riskassessment, supporting early diagnosis and intervention in clinical settings.Additionally, the framework's ability to incorporate geolocation data providesa more comprehensive risk assessment model that could help healthcare providersbetter understand and address environmental factors contributing to ADdevelopment.</description><author>Peng Zhang, Divya Chaudhary</author><pubDate>Thu, 06 Feb 2025 18:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04288v1</guid></item><item><title>A Methodology for Studying Linguistic and Cultural Change in China, 1900-1950</title><link>http://arxiv.org/abs/2502.04286v1</link><description>This paper presents a quantitative approach to studying linguistic andcultural change in China during the first half of the twentieth century, aperiod that remains understudied in computational humanities research. Thedramatic changes in Chinese language and culture during this time call forgreater reflection on the tools and methods used for text analysis. Thispreliminary study offers a framework for analyzing Chinese texts from the latenineteenth and twentieth centuries, demonstrating how established methods suchas word counts and word embeddings can provide new historical insights into thecomplex negotiations between Western modernity and Chinese cultural discourse.</description><author>Spencer Dean Stewart</author><pubDate>Thu, 06 Feb 2025 18:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04286v1</guid></item><item><title>DECAF: Learning to be Fair in Multi-agent Resource Allocation</title><link>http://arxiv.org/abs/2502.04281v1</link><description>A wide variety of resource allocation problems operate under resourceconstraints that are managed by a central arbitrator, with agents who evaluateand communicate preferences over these resources. We formulate this broad classof problems as Distributed Evaluation, Centralized Allocation (DECA) problemsand propose methods to learn fair and efficient policies in centralizedresource allocation. Our methods are applied to learning long-term fairness ina novel and general framework for fairness in multi-agent systems. We showthree different methods based on Double Deep Q-Learning: (1) A joint weightedoptimization of fairness and utility, (2) a split optimization, learning twoseparate Q-estimators for utility and fairness, and (3) an online policyperturbation to guide existing black-box utility functions toward fairsolutions. Our methods outperform existing fair MARL approaches on multipleresource allocation domains, even when evaluated using diverse fairnessfunctions, and allow for flexible online trade-offs between utility andfairness.</description><author>Ashwin Kumar, William Yeoh</author><pubDate>Thu, 06 Feb 2025 18:29:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04281v1</guid></item><item><title>ACECODER: Acing Coder RL via Automated Test-Case Synthesis</title><link>http://arxiv.org/abs/2502.01718v2</link><description>Most progress in recent coder models has been driven by supervisedfine-tuning (SFT), while the potential of reinforcement learning (RL) remainslargely unexplored, primarily due to the lack of reliable reward data/model inthe code domain. In this paper, we address this challenge by leveragingautomated large-scale test-case synthesis to enhance code model training.Specifically, we design a pipeline that generates extensive (question,test-cases) pairs from existing code data. Using these test cases, we constructpreference pairs based on pass rates over sampled programs to train rewardmodels with Bradley-Terry loss. It shows an average of 10-point improvement forLlama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins throughbest-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5.Furthermore, we conduct reinforcement learning with both reward models andtest-case pass rewards, leading to consistent improvements across HumanEval,MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-styletraining to start from Qwen2.5-Coder-base directly and show that our RLtraining can improve model on HumanEval-plus by over 25\% and MBPP-plus by 6\%for merely 80 optimization steps. We believe our results highlight the hugepotential of reinforcement learning in coder models.</description><author>Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, Wenhu Chen</author><pubDate>Thu, 06 Feb 2025 18:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.01718v2</guid></item><item><title>Gaussian Process Regression for Inverse Problems in Linear PDEs</title><link>http://arxiv.org/abs/2502.04276v1</link><description>This paper introduces a computationally efficient algorithm in system theoryfor solving inverse problems governed by linear partial differential equations(PDEs). We model solutions of linear PDEs using Gaussian processes with priorsdefined based on advanced commutative algebra and algebraic analysis. Theimplementation of these priors is algorithmic and achieved using the Macaulay2computer algebra software. An example application includes identifying the wavespeed from noisy data for classical wave equations, which are widely used inphysics. The method achieves high accuracy while enhancing computationalefficiency.</description><author>Xin Li, Markus Lange-Hegermann, Bogdan Raiţă</author><pubDate>Thu, 06 Feb 2025 18:20:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04276v1</guid></item><item><title>Orthogonal Representation Learning for Estimating Causal Quantities</title><link>http://arxiv.org/abs/2502.04274v1</link><description>Representation learning is widely used for estimating causal quantities(e.g., the conditional average treatment effect) from observational data. Whileexisting representation learning methods have the benefit of allowing forend-to-end learning, they do not have favorable theoretical properties ofNeyman-orthogonal learners, such as double robustness and quasi-oracleefficiency. Also, such representation learning methods often employ additionalconstraints, like balancing, which may even lead to inconsistent estimation. Inthis paper, we propose a novel class of Neyman-orthogonal learners for causalquantities defined at the representation level, which we call OR-learners. OurOR-learners have several practical advantages: they allow for consistentestimation of causal quantities based on any learned representation, whileoffering favorable theoretical properties including double robustness andquasi-oracle efficiency. In multiple experiments, we show that, under certainregularity conditions, our OR-learners improve existing representation learningmethods and achieve state-of-the-art performance. To the best of our knowledge,our OR-learners are the first work to offer a unified framework ofrepresentation learning methods and Neyman-orthogonal learners for causalquantities estimation.</description><author>Valentyn Melnychuk, Dennis Frauen, Jonas Schweisthal, Stefan Feuerriegel</author><pubDate>Thu, 06 Feb 2025 18:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04274v1</guid></item><item><title>Electrical Impedance Tomography for Anisotropic Media: a Machine Learning Approach to Classify Inclusions</title><link>http://arxiv.org/abs/2502.04273v1</link><description>We consider the problem in Electrical Impedance Tomography (EIT) ofidentifying one or multiple inclusions in a background-conducting body$\Omega\subset\mathbb{R}^2$, from the knowledge of a finite number ofelectrostatic measurements taken on its boundary $\partial\Omega$ and modelledby the Dirichlet-to-Neumann (D-N) matrix. Once the presence of one inclusion in$\Omega$ is established, our model, combined with the machine learningtechniques of Artificial Neural Networks (ANN) and Support Vector Machines(SVM), may be used to determine the size of the inclusion, the presence ofmultiple inclusions, and also that of anisotropy within the inclusion(s).Utilising both real and simulated datasets within a 16-electrode setup, weachieve a high rate of inclusion detection and show that two measurements aresufficient to achieve a good level of accuracy when predicting the size of aninclusion. This underscores the substantial potential of integrating machinelearning approaches with the more classical analysis of EIT and the inverseinclusion problem to extract critical insights, such as the presence ofanisotropy.</description><author>Romina Gaburro, Patrick Healy, Shraddha Naidu, Clifford Nolan</author><pubDate>Thu, 06 Feb 2025 18:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04273v1</guid></item><item><title>InfAlign: Inference-aware language model alignment</title><link>http://arxiv.org/abs/2412.19792v3</link><description>Language model alignment is a critical step in training modern generativelanguage models. Alignment targets to improve win rate of a sample from thealigned model against the base model. Today, we are increasingly usinginference-time algorithms (e.g., Best-of-N, controlled decoding, tree search)to decode from language models rather than standard sampling. We show that thistrain/test mismatch makes standard RLHF framework sub-optimal in view of suchinference-time methods. To this end, we propose a framework for inference-awarealignment (InfAlign), which aims to optimize inference-time win rate of thealigned policy against the base model. We prove that for any inference-timedecoding procedure, the optimal aligned policy is the solution to the standardRLHF problem with a transformation of the reward. This motivates us to providethe calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem,which involves a reward calibration step and a KL-regularized rewardmaximization step with a transformation of the calibrated reward. For best-of-Nsampling and best-of-N jailbreaking, we propose specific transformationsoffering up to 3-8% improvement on inference-time win rates. Finally, we alsoshow that our proposed reward calibration method is a strong baseline foroptimizing standard win rate.</description><author>Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, Ananda Theertha Suresh, Ahmad Beirami</author><pubDate>Thu, 06 Feb 2025 18:15:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19792v3</guid></item><item><title>Articulate-Anything: Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model</title><link>http://arxiv.org/abs/2410.13882v3</link><description>Interactive 3D simulated objects are crucial in AR/VR, animations, androbotics, driving immersive experiences and advanced automation. However,creating these articulated objects requires extensive human effort andexpertise, limiting their broader applications. To overcome this challenge, wepresent Articulate-Anything, a system that automates the articulation ofdiverse, complex objects from many input modalities, including text, images,and videos. Articulate-Anything leverages vision-language models (VLMs) togenerate code that can be compiled into an interactable digital twin for use instandard 3D simulators. Our system exploits existing 3D asset datasets via amesh retrieval mechanism, along with an actor-critic system that iterativelyproposes, evaluates, and refines solutions for articulating the objects,self-correcting errors to achieve a robust outcome. Qualitative evaluationsdemonstrate Articulate-Anything's capability to articulate complex and evenambiguous object affordances by leveraging rich grounded inputs. In extensivequantitative experiments on the standard PartNet-Mobility dataset,Articulate-Anything substantially outperforms prior work, increasing thesuccess rate from 8.7-11.6% to 75% and setting a new bar for state-of-the-artperformance. We further showcase the utility of our system by generating 3Dassets from in-the-wild video inputs, which are then used to train roboticpolicies for fine-grained manipulation tasks in simulation that go beyond basicpick and place. These policies are then transferred to a real robotic system.</description><author>Long Le, Jason Xie, William Liang, Hung-Ju Wang, Yue Yang, Yecheng Jason Ma, Kyle Vedder, Arjun Krishna, Dinesh Jayaraman, Eric Eaton</author><pubDate>Thu, 06 Feb 2025 18:15:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13882v3</guid></item><item><title>Advantage Alignment Algorithms</title><link>http://arxiv.org/abs/2406.14662v3</link><description>Artificially intelligent agents are increasingly being integrated into humandecision-making: from large language model (LLM) assistants to autonomousvehicles. These systems often optimize their individual objective, leading toconflicts, particularly in general-sum games where naive reinforcement learningagents empirically converge to Pareto-suboptimal Nash equilibria. To addressthis issue, opponent shaping has emerged as a paradigm for finding sociallybeneficial equilibria in general-sum games. In this work, we introduceAdvantage Alignment, a family of algorithms derived from first principles thatperform opponent shaping efficiently and intuitively. We achieve this byaligning the advantages of interacting agents, increasing the probability ofmutually beneficial actions when their interaction has been positive. We provethat existing opponent shaping methods implicitly perform Advantage Alignment.Compared to these methods, Advantage Alignment simplifies the mathematicalformulation of opponent shaping, reduces the computational burden and extendsto continuous action domains. We demonstrate the effectiveness of ouralgorithms across a range of social dilemmas, achieving state-of-the-artcooperation and robustness against exploitation.</description><author>Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans, Razvan Ciuca, Tianyu Zhang, Gauthier Gidel, Aaron Courville</author><pubDate>Thu, 06 Feb 2025 18:12:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14662v3</guid></item><item><title>Low-skilled Occupations Face the Highest Upskilling Pressure</title><link>http://arxiv.org/abs/2101.11505v5</link><description>Substantial scholarship has estimated the susceptibility of jobs toautomation, but little has examined how job contents evolve in the informationage as new technologies substitute for tasks, shifting required skills ratherthan eliminating entire jobs. Here we explore patterns of occupational skillchange and characterize occupations and workers subject to the greatestreskilling requirements. Recent work found that changing skill requirements aregreatest for STEM occupations in the 2010s. Nevertheless, analyzing 167 milliononline job posts covering 727 occupations, we find that skill change isgreatest for low-skilled occupations when accounting for distance betweenskills. We further investigate the differences in skill change across employerand market size, as well as social demographic groups. We find that jobs fromsmall employers and markets experienced larger skill upgrades to catch up withthe skill demands of their large employers and markets. Female and minorityworkers are disproportionately employed in low-skilled jobs and face the mostsignificant skill adjustments. While these varied skill changes could createuneven reskilling pressures across workers, they may also lead to a narrowingof gaps in job quality and prospects. We conclude by showcasing our model'spotential to chart job evolution directions using skill embedding spaces.</description><author>Di Tong, Lingfei Wu, James Allen Evans</author><pubDate>Thu, 06 Feb 2025 18:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2101.11505v5</guid></item><item><title>Variational decision diagrams for quantum-inspired machine learning applications</title><link>http://arxiv.org/abs/2502.04271v1</link><description>Decision diagrams (DDs) have emerged as an efficient tool for simulatingquantum circuits due to their capacity to exploit data redundancies in quantumstates and quantum operations, enabling the efficient computation ofprobability amplitudes. However, their application in quantum machine learning(QML) has remained unexplored. This paper introduces variational decisiondiagrams (VDDs), a novel graph structure that combines the structural benefitsof DDs with the adaptability of variational methods for efficientlyrepresenting quantum states. We investigate the trainability of VDDs byapplying them to the ground state estimation problem for transverse-field Isingand Heisenberg Hamiltonians. Analysis of gradient variance suggests thattraining VDDs is possible, as no signs of vanishing gradients--also known asbarren plateaus--are observed. This work provides new insights into the use ofdecision diagrams in QML as an alternative to design and train variationalans\"atze.</description><author>Santiago Acevedo-Mancera, Vladimir Vargas-Calderón, Herbert Vinck-Posada</author><pubDate>Thu, 06 Feb 2025 18:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04271v1</guid></item><item><title>PILAF: Optimal Human Preference Sampling for Reward Modeling</title><link>http://arxiv.org/abs/2502.04270v1</link><description>As large language models increasingly drive real-world applications, aligningthem with human values becomes paramount. Reinforcement Learning from HumanFeedback (RLHF) has emerged as a key technique, translating preference datainto reward models when oracle human values remain inaccessible. In practice,RLHF mostly relies on approximate reward models, which may not consistentlyguide the policy toward maximizing the underlying human values. We proposePolicy-Interpolated Learning for Aligned Feedback (PILAF), a novel responsesampling strategy for preference labeling that explicitly aligns preferencelearning with maximizing the underlying oracle reward. PILAF is theoreticallygrounded, demonstrating optimality from both an optimization and a statisticalperspective. The method is straightforward to implement and demonstrates strongperformance in iterative and online RLHF settings where feedback curation iscritical.</description><author>Yunzhen Feng, Ariel Kwiatkowski, Kunhao Zheng, Julia Kempe, Yaqi Duan</author><pubDate>Thu, 06 Feb 2025 18:09:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04270v1</guid></item><item><title>Robust Reward Alignment via Hypothesis Space Batch Cutting</title><link>http://arxiv.org/abs/2502.02921v2</link><description>Reward design for reinforcement learning and optimal control agents ischallenging. Preference-based alignment addresses this by enabling agents tolearn rewards from ranked trajectory pairs provided by humans. However,existing methods often struggle from poor robustness to unknown false humanpreferences. In this work, we propose a robust and efficient reward alignmentmethod based on a novel and geometrically interpretable perspective: hypothesisspace batched cutting. Our method iteratively refines the reward hypothesisspace through "cuts" based on batches of human preferences. Within each batch,human preferences, queried based on disagreement, are grouped using a votingfunction to determine the appropriate cut, ensuring a bounded human querycomplexity. To handle unknown erroneous preferences, we introduce aconservative cutting method within each batch, preventing erroneous humanpreferences from making overly aggressive cuts to the hypothesis space. Thisguarantees provable robustness against false preferences. We evaluate ourmethod in a model predictive control setting across diverse tasks, includingDM-Control, dexterous in-hand manipulation, and locomotion. The resultsdemonstrate that our framework achieves comparable or superior performance tostate-of-the-art methods in error-free settings while significantlyoutperforming existing method when handling high percentage of erroneous humanpreferences.</description><author>Zhixian Xie, Haode Zhang, Yizhe Feng, Wanxin Jin</author><pubDate>Thu, 06 Feb 2025 18:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02921v2</guid></item><item><title>How does a Multilingual LM Handle Multiple Languages?</title><link>http://arxiv.org/abs/2502.04269v1</link><description>Multilingual language models have significantly advanced due to rapidprogress in natural language processing. Models like BLOOM 1.7B, trained ondiverse multilingual datasets, aim to bridge linguistic gaps. However, theireffectiveness in capturing linguistic knowledge, particularly for low-resourcelanguages, remains an open question. This study critically examines MLMscapabilities in multilingual understanding, semantic representation, andcross-lingual knowledge transfer. While these models perform well forhigh-resource languages, they struggle with less-represented ones.Additionally, traditional evaluation methods often overlook their internalsyntactic and semantic encoding. This research addresses key limitations through three objectives. First, itassesses semantic similarity by analyzing multilingual word embeddings forconsistency using cosine similarity. Second, it examines BLOOM-1.7B and Qwen2through Named Entity Recognition and sentence similarity tasks to understandtheir linguistic structures. Third, it explores cross-lingual knowledgetransfer by evaluating generalization from high-resource to low-resourcelanguages in sentiment analysis and text classification. By leveraging linguistic probing, performance metrics, and visualizations,this study provides insights into the strengths and limitations of MLMs. Thefindings aim to enhance multilingual NLP models, ensuring better support forboth high- and low-resource languages, thereby promoting inclusivity inlanguage technologies.</description><author>Santhosh Kakarla, Gautama Shastry Bulusu Venkata, Aishwarya Gaddam</author><pubDate>Thu, 06 Feb 2025 18:08:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04269v1</guid></item><item><title>Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances</title><link>http://arxiv.org/abs/2502.04268v1</link><description>With the rapidly increasing demand for oriented object detection (OOD),recent research involving weakly-supervised detectors for learning OOD frompoint annotations has gained great attention. In this paper, we rethink thischallenging task setting with the layout among instances and presentPoint2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. Itlearns an upper bound for each instance by treating objects as 2D Gaussiandistributions and minimizing their overlap. 2) Voronoi watershed loss. Itlearns a lower bound for each instance through watershed on Voronoitessellation. 3) Consistency loss. It learns the size/rotation variationbetween two output sets with respect to an input image and its augmented view.Supplemented by a few devised techniques, e.g. edge loss and copy-paste, thedetector is further enhanced.To our best knowledge, Point2RBox-v2 is the firstapproach to explore the spatial layout among instances for learningpoint-supervised OOD. Our solution is elegant and lightweight, yet it isexpected to give a competitive performance especially in densely packed scenes:62.61%/86.15%/34.71% on DOTA/HRSC/FAIR1M. Code is available athttps://github.com/VisionXLab/point2rbox-v2.</description><author>Yi Yu, Botao Ren, Peiyuan Zhang, Mingxin Liu, Junwei Luo, Shaofeng Zhang, Feipeng Da, Junchi Yan, Xue Yang</author><pubDate>Thu, 06 Feb 2025 18:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04268v1</guid></item><item><title>PRISM-TopoMap: Online Topological Mapping with Place Recognition and Scan Matching</title><link>http://arxiv.org/abs/2404.01674v3</link><description>Mapping is one of the crucial tasks enabling autonomous navigation of amobile robot. Conventional mapping methods output a dense geometric maprepresentation, e.g. an occupancy grid, which is not trivial to keep consistentfor prolonged runs covering large environments. Meanwhile, capturing thetopological structure of the workspace enables fast path planning, is typicallyless prone to odometry error accumulation, and does not consume much memory.Following this idea, this paper introduces PRISM-TopoMap -- a topologicalmapping method that maintains a graph of locally aligned locations not relyingon global metric coordinates. The proposed method involves original learnablemultimodal place recognition paired with the scan matching pipeline forlocalization and loop closure in the graph of locations. The latter is updatedonline, and the robot is localized in a proper node at each time step. Weconduct a broad experimental evaluation of the suggested approach in a range ofphoto-realistic environments and on a real robot, and compare it to state ofthe art. The results of the empirical evaluation confirm that PRISM-Topomapconsistently outperforms competitors computationally-wise, achieves highmapping quality and performs well on a real robot. The code of PRISM-Topomap isopen-sourced and is available at:https://github.com/kirillMouraviev/prism-topomap.</description><author>Kirill Muravyev, Alexander Melekhin, Dmitry Yudin, Konstantin Yakovlev</author><pubDate>Thu, 06 Feb 2025 18:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01674v3</guid></item><item><title>Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion</title><link>http://arxiv.org/abs/2502.04263v1</link><description>Pre-trained multi-modal Vision-Language Models like CLIP are widely usedoff-the-shelf for a variety of applications. In this paper, we show that thecommon practice of individually exploiting the text or image encoders of thesepowerful multi-modal models is highly suboptimal for intra-modal tasks likeimage-to-image retrieval. We argue that this is inherently due to theCLIP-style inter-modal contrastive loss that does not enforce any intra-modalconstraints, leading to what we call intra-modal misalignment. To demonstratethis, we leverage two optimization-based modality inversion techniques that maprepresentations from their input modality to the complementary one without anyneed for auxiliary data or additional trained adapters. We empirically showthat, in the intra-modal tasks of image-to-image and text-to-text retrieval,approaching these tasks inter-modally significantly improves performance withrespect to intra-modal baselines on more than fifteen datasets. Additionally,we demonstrate that approaching a native inter-modal task (e.g. zero-shot imageclassification) intra-modally decreases performance, further validating ourfindings. Finally, we show that incorporating an intra-modal term in thepre-training objective or narrowing the modality gap between the text and imagefeature embedding spaces helps reduce the intra-modal misalignment. The code ispublicly available at: https://github.com/miccunifi/Cross-the-Gap.</description><author>Marco Mistretta, Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, Andrew D. Bagdanov</author><pubDate>Thu, 06 Feb 2025 17:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04263v1</guid></item><item><title>Efficient Randomized Experiments Using Foundation Models</title><link>http://arxiv.org/abs/2502.04262v1</link><description>Randomized experiments are the preferred approach for evaluating the effectsof interventions, but they are costly and often yield estimates withsubstantial uncertainty. On the other hand, in silico experiments leveragingfoundation models offer a cost-effective alternative that can potentiallyattain higher statistical precision. However, the benefits of in silicoexperiments come with a significant risk: statistical inferences are not validif the models fail to accurately predict experimental responses tointerventions. In this paper, we propose a novel approach that integrates thepredictions from multiple foundation models with experimental data whilepreserving valid statistical inference. Our estimator is consistent andasymptotically normal, with asymptotic variance no larger than the standardestimator based on experimental data alone. Importantly, these statisticalproperties hold even when model predictions are arbitrarily biased. Empiricalresults across several randomized experiments show that our estimator offerssubstantial precision gains, equivalent to a reduction of up to 20% in thesample size needed to match the same precision as the standard estimator basedon experimental data alone.</description><author>Piersilvio De Bartolomeis, Javier Abad, Guanbo Wang, Konstantin Donhauser, Raymond M. Duch, Fanny Yang, Issa J. Dahabreh</author><pubDate>Thu, 06 Feb 2025 17:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04262v1</guid></item><item><title>Realistic Image-to-Image Machine Unlearning via Decoupling and Knowledge Retention</title><link>http://arxiv.org/abs/2502.04260v1</link><description>Machine Unlearning allows participants to remove their data from a trainedmachine learning model in order to preserve their privacy, and security.However, the machine unlearning literature for generative models is ratherlimited. The literature for image-to-image generative model (I2I model)considers minimizing the distance between Gaussian noise and the output of I2Imodel for forget samples as machine unlearning. However, we argue that themachine learning model performs fairly well on unseen data i.e., a retrainedmodel will be able to catch generic patterns in the data and hence will notgenerate an output which is equivalent to Gaussian noise. In this paper, weconsider that the model after unlearning should treat forget samples asout-of-distribution (OOD) data, i.e., the unlearned model should no longerrecognize or encode the specific patterns found in the forget samples. Toachieve this, we propose a framework which decouples the model parameters withgradient ascent, ensuring that forget samples are OOD for unlearned model withtheoretical guarantee. We also provide $(\epsilon, \delta)$-unlearningguarantee for model updates with gradient ascent. The unlearned model isfurther fine-tuned on the remaining samples to maintain its performance. Wealso propose an attack model to ensure that the unlearned model has effectivelyremoved the influence of forget samples. Extensive empirical evaluation on twolarge-scale datasets, ImageNet-1K and Places365 highlights the superiority ofour approach. To show comparable performance with retrained model, we also showthe comparison of a simple AutoEncoder on various baselines on CIFAR-10dataset.</description><author>Ayush K. Varshney, Vicenç Torra</author><pubDate>Thu, 06 Feb 2025 17:46:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04260v1</guid></item><item><title>Combining Language and App UI Analysis for the Automated Assessment of Bug Reproduction Steps</title><link>http://arxiv.org/abs/2502.04251v1</link><description>Bug reports are essential for developers to confirm software problems,investigate their causes, and validate fixes. Unfortunately, reports often missimportant information or are written unclearly, which can cause delays,increased issue resolution effort, or even the inability to solve issues. Oneof the most common components of reports that are problematic is the steps toreproduce the bug(s) (S2Rs), which are essential to replicate the describedprogram failures and reason about fixes. Given the proclivity for deficienciesin reported S2Rs, prior work has proposed techniques that assist reporters inwriting or assessing the quality of S2Rs. However, automated understanding ofS2Rs is challenging, and requires linking nuanced natural language phrases withspecific, semantically related program information. Prior techniques oftenstruggle to form such language to program connections - due to issues inlanguage variability and limitations of information gleaned from programanalyses. To more effectively tackle the problem of S2R quality annotation, we proposea new technique called AstroBR, which leverages the language understandingcapabilities of LLMs to identify and extract the S2Rs from bug reports and mapthem to GUI interactions in a program state model derived via dynamic analysis.We compared AstroBR to a related state-of-the-art approach and we found thatAstroBR annotates S2Rs 25.2% better (in terms of F1 score) than the baseline.Additionally, AstroBR suggests more accurate missing S2Rs than the baseline (by71.4% in terms of F1 score).</description><author>Junayed Mahmud, Antu Saha, Oscar Chaparro, Kevin Moran, Andrian Marcus</author><pubDate>Thu, 06 Feb 2025 17:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04251v1</guid></item><item><title>Model-agnostic meta-learners for estimating heterogeneous treatment effects over time</title><link>http://arxiv.org/abs/2407.05287v2</link><description>Estimating heterogeneous treatment effects (HTEs) over time is crucial inmany disciplines such as personalized medicine. For example, electronic healthrecords are commonly collected over several time periods and then used topersonalize treatment decisions. Existing works for this task have mostlyfocused on model-based learners (i.e., learners that adapt specificmachine-learning models). In contrast, model-agnostic learners -- so-calledmeta-learners -- are largely unexplored. In our paper, we propose severalmeta-learners that are model-agnostic and thus can be used in combination witharbitrary machine learning models (e.g., transformers) to estimate HTEs overtime. Here, our focus is on learners that can be obtained via weightedpseudo-outcome regressions, which allows for efficient estimation by targetingthe treatment effect directly. We then provide a comprehensive theoreticalanalysis that characterizes the different learners and that allows us to offerinsights into when specific learners are preferable. Finally, we confirm ourtheoretical insights through numerical experiments. In sum, while meta-learnersare already state-of-the-art for the static setting, we are the first topropose a comprehensive set of meta-learners for estimating HTEs in thetime-varying setting.</description><author>Dennis Frauen, Konstantin Hess, Stefan Feuerriegel</author><pubDate>Thu, 06 Feb 2025 17:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05287v2</guid></item><item><title>Free Energy Risk Metrics for Systemically Safe AI: Gatekeeping Multi-Agent Study</title><link>http://arxiv.org/abs/2502.04249v1</link><description>We investigate the Free Energy Principle as a foundation for measuring riskin agentic and multi-agent systems. From these principles we introduce aCumulative Risk Exposure metric that is flexible to differing contexts andneeds. We contrast this to other popular theories for safe AI that hinge onmassive amounts of data or describing arbitrarily complex world models. In ourframework, stakeholders need only specify their preferences over systemoutcomes, providing straightforward and transparent decision rules for riskgovernance and mitigation. This framework naturally accounts for uncertainty inboth world model and preference model, allowing for decision-making that isepistemically and axiologically humble, parsimonious, and future-proof. Wedemonstrate this novel approach in a simplified autonomous vehicle environmentwith multi-agent vehicles whose driving policies are mediated by gatekeepersthat evaluate, in an online fashion, the risk to the collective safety in theirneighborhood, and intervene through each vehicle's policy when appropriate. Weshow that the introduction of gatekeepers in an AV fleet, even at lowpenetration, can generate significant positive externalities in terms ofincreased system safety.</description><author>Michael Walters, Rafael Kaufmann, Justice Sefas, Thomas Kopinski</author><pubDate>Thu, 06 Feb 2025 17:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04249v1</guid></item><item><title>Adapting to Evolving Adversaries with Regularized Continual Robust Training</title><link>http://arxiv.org/abs/2502.04248v1</link><description>Robust training methods typically defend against specific attack types, suchas Lp attacks with fixed budgets, and rarely account for the fact thatdefenders may encounter new attacks over time. A natural solution is to adaptthe defended model to new adversaries as they arise via fine-tuning, a methodwhich we call continual robust training (CRT). However, when implementednaively, fine-tuning on new attacks degrades robustness on previous attacks.This raises the question: how can we improve the initial training andfine-tuning of the model to simultaneously achieve robustness against previousand new attacks? We present theoretical results which show that the gap in amodel's robustness against different attacks is bounded by how far each attackperturbs a sample in the model's logit space, suggesting that regularizing withrespect to this logit space distance can help maintain robustness againstprevious attacks. Extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, andImageNette) and over 100 attack combinations demonstrate that the proposedregularization improves robust accuracy with little overhead in training time.Our findings and open-source code lay the groundwork for the deployment ofmodels robust to evolving attacks.</description><author>Sihui Dai, Christian Cianfarani, Arjun Bhagoji, Vikash Sehwag, Prateek Mittal</author><pubDate>Thu, 06 Feb 2025 17:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04248v1</guid></item><item><title>Student-t processes as infinite-width limits of posterior Bayesian neural networks</title><link>http://arxiv.org/abs/2502.04247v1</link><description>The asymptotic properties of Bayesian Neural Networks (BNNs) have beenextensively studied, particularly regarding their approximations by Gaussianprocesses in the infinite-width limit. We extend these results by showing thatposterior BNNs can be approximated by Student-t processes, which offer greaterflexibility in modeling uncertainty. Specifically, we show that, if theparameters of a BNN follow a Gaussian prior distribution, and the variance ofboth the last hidden layer and the Gaussian likelihood function follows anInverse-Gamma prior distribution, then the resulting posterior BNN converges toa Student-t process in the infinite-width limit. Our proof leverages theWasserstein metric to establish control over the convergence rate of theStudent-t process approximation.</description><author>Francesco Caporali, Stefano Favaro, Dario Trevisan</author><pubDate>Thu, 06 Feb 2025 17:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04247v1</guid></item><item><title>TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali &amp; Marathi</title><link>http://arxiv.org/abs/2502.04245v1</link><description>India's rich cultural and linguistic diversity poses various challenges inthe domain of Natural Language Processing (NLP), particularly in Named EntityRecognition (NER). NER is a NLP task that aims to identify and classify tokensinto different entity groups like Person, Location, Organization, Number, etc.This makes NER very useful for downstream tasks like context-awareanonymization. This paper details our work to build a multilingual NER modelfor the three most spoken languages in India - Hindi, Bengali &amp; Marathi. Wetrain a custom transformer model and fine tune a few pretrained models,achieving an F1 Score of 92.11 for a total of 6 entity groups. Through thispaper, we aim to introduce a single model to perform NER and significantlyreduce the inconsistencies in entity groups and tag names, across the threelanguages.</description><author>Mohammed Amaan Dhamaskar, Rasika Ransing</author><pubDate>Thu, 06 Feb 2025 17:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04245v1</guid></item><item><title>An object detection approach for lane change and overtake detection from motion profiles</title><link>http://arxiv.org/abs/2502.04244v1</link><description>In the application domain of fleet management and driver monitoring, it isvery challenging to obtain relevant driving events and activities from dashcamfootage while minimizing the amount of information stored and analyzed. In thispaper, we address the identification of overtake and lane change maneuvers witha novel object detection approach applied to motion profiles, a compactrepresentation of driving video footage into a single image. To train and testour model we created an internal dataset of motion profile images obtained froma heterogeneous set of dashcam videos, manually labeled with overtake and lanechange maneuvers by the ego-vehicle. In addition to a standard object-detectionapproach, we show how the inclusion of CoordConvolution layers further improvesthe model performance, in terms of mAP and F1 score, yielding state-of-the artperformance when compared to other baselines from the literature. The extremelylow computational requirements of the proposed solution make it especiallysuitable to run in device.</description><author>Andrea Benericetti, Niccolò Bellaccini, Henrique Piñeiro Monteagudo, Matteo Simoncini, Francesco Sambo</author><pubDate>Thu, 06 Feb 2025 17:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04244v1</guid></item><item><title>HMT: Hierarchical Memory Transformer for Efficient Long Context Language Processing</title><link>http://arxiv.org/abs/2405.06067v3</link><description>Transformer-based large language models (LLM) have been widely used inlanguage processing applications. However, due to the memory constraints of thedevices, most of them restrict the context window. Even though recurrent modelsin previous works can memorize past tokens to enable unlimited context andmaintain effectiveness, they have ``flat'' memory architectures. Sucharchitectures have limitations in selecting and filtering information. Sincehumans are good at learning and self-adjustment, we believe that imitatingbrain memory hierarchy is beneficial for model memorization. Thus, we proposethe Hierarchical Memory Transformer (HMT), a novel framework that facilitates amodel's long-context processing ability by imitating human memorizationbehavior. Leveraging memory-augmented segment-level recurrence, we organize thememory hierarchy by preserving tokens from early input segments, passing memoryembeddings along the sequence, and recalling relevant information from history.Evaluating general language modeling, question-answering tasks, and thesummarization task, we show that HMT consistently improves the long-contextprocessing ability of existing models. Furthermore, HMT achieves a comparableor superior generation quality to long-context LLMs with $2 \sim 57\times$fewer parameters and $2.5 \sim 116\times$ less inference memory, significantlyoutperforming previous memory-augmented models. Code on Github:https://github.com/OswaldHe/HMT-pytorch.</description><author>Zifan He, Yingqi Cao, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong</author><pubDate>Thu, 06 Feb 2025 17:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06067v3</guid></item><item><title>VideoSAM: A Large Vision Foundation Model for High-Speed Video Segmentation</title><link>http://arxiv.org/abs/2410.21304v3</link><description>High-speed video (HSV) segmentation is essential for analyzing dynamicphysical processes in scientific and industrial applications, such as boilingheat transfer. Existing models like U-Net struggle with generalization andaccurately segmenting complex bubble formations. We present VideoSAM, aspecialized adaptation of the Segment Anything Model (SAM), fine-tuned on adiverse HSV dataset for phase detection. Through diverse experiments, VideoSAMdemonstrates superior performance across four fluid environments -- Water,FC-72, Nitrogen, and Argon -- significantly outperforming U-Net in complexsegmentation tasks. In addition to introducing VideoSAM, we contribute anopen-source HSV segmentation dataset designed for phase detection, enablingfuture research in this domain. Our findings underscore VideoSAM's potential toset new standards in robust and accurate HSV segmentation. The code and datasetused in this study are available online athttps://github.com/chikap421/videosam.</description><author>Chika Maduabuchi, Ericmoore Jossou, Matteo Bucci</author><pubDate>Thu, 06 Feb 2025 17:34:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21304v3</guid></item><item><title>A Theoretical Framework for Data Efficient Multi-Source Transfer Learning Based on Cramér-Rao Bound</title><link>http://arxiv.org/abs/2502.04242v1</link><description>Multi-source transfer learning provides an effective solution to datascarcity in real-world supervised learning scenarios by leveraging multiplesource tasks. In this field, existing works typically use all available samplesfrom sources in training, which constrains their training efficiency and maylead to suboptimal results. To address this, we propose a theoretical frameworkthat answers the question: what is the optimal quantity of source samplesneeded from each source task to jointly train the target model? Specifically,we introduce a generalization error measure that aligns with cross-entropyloss, and minimize it based on the Cram\'er-Rao Bound to determine the optimaltransfer quantity for each source task. Additionally, we develop anarchitecture-agnostic and data-efficient algorithm OTQMS to implement ourtheoretical results for training deep multi-source transfer learning models.Experimental studies on diverse architectures and two real-world benchmarkdatasets show that our proposed algorithm significantly outperformsstate-of-the-art approaches in both accuracy and data efficiency. The code andsupplementary materials are available inhttps://anonymous.4open.science/r/Materials.</description><author>Qingyue Zhang, Haohao Fu, Guanbo Huang, Yaoyuan Liang, Chang Chu, Tianren Peng, Yanru Wu, Qi Li, Yang Li, Shao-Lun Huang</author><pubDate>Thu, 06 Feb 2025 17:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04242v1</guid></item><item><title>Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks</title><link>http://arxiv.org/abs/2407.02138v2</link><description>Trustworthiness in model predictions is crucial for safety-criticalapplications in the real world. However, deep neural networks often suffer fromthe issues of uncertainty estimation, such as miscalibration. In this study, wepropose $k$-Nearest Neighbor Uncertainty Estimation ($k$NN-UE), which is a newuncertainty estimation method that uses not only the distances from theneighbors, but also the ratio of labels in the neighbors. Experiments onsentiment analysis, natural language inference, and named entity recognitionshow that our proposed method outperforms the baselines and recentdensity-based methods in several calibration and uncertainty metrics. Moreover,our analyses indicate that approximate nearest neighbor search techniquesreduce the inference overhead without significantly degrading the uncertaintyestimation performance when they are appropriately combined.</description><author>Wataru Hashimoto, Hidetaka Kamigaito, Taro Watanabe</author><pubDate>Thu, 06 Feb 2025 17:32:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02138v2</guid></item><item><title>Causal Learning for Heterogeneous Subgroups Based on Nonlinear Causal Kernel Clustering</title><link>http://arxiv.org/abs/2501.11622v2</link><description>Due to the challenge posed by multi-source and heterogeneous data collectedfrom diverse environments, causal relationships among features can exhibitvariations influenced by different time spans, regions, or strategies. Thisdiversity makes a single causal model inadequate for accurately representingcomplex causal relationships in all observational data, a crucial considerationin causal learning. To address this challenge, the nonlinear Causal KernelClustering method is introduced for heterogeneous subgroup causal learning,highlighting variations in causal relationships across diverse subgroups.\textcolor{new}{The main component for clustering heterogeneous subgroups liesin the construction of the $u$-centered sample mapping function with theproperty of unbiased estimation, which assesses the differences in potentialnonlinear causal relationships in various samples and supported by causalidentifiability theory.} Experimental results indicate that the method performswell in identifying heterogeneous subgroups and enhancing causal learning,leading to a reduction in prediction error.</description><author>Lu Liu, Yang Tang, Kexuan Zhang, Qiyu Sun</author><pubDate>Thu, 06 Feb 2025 17:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.11622v2</guid></item><item><title>MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion</title><link>http://arxiv.org/abs/2502.04235v1</link><description>Despite the remarkable capabilities of large language models across varioustasks, their continued scaling faces a critical challenge: the scarcity ofhigh-quality pretraining data. While model architectures continue to evolve,the natural language data struggles to scale up. To tackle this bottleneck, wepropose \textbf{MA}ssive \textbf{G}enre-\textbf{A}udience~(MAGA) reformulationmethod, which systematic synthesizes diverse, contextually-rich pretrainingdata from existing corpus. This work makes three main contributions: (1) Wepropose MAGA reformulation method, a lightweight and scalable approach forpretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) Weevaluate MAGACorpus with different data budget scaling strategies,demonstrating consistent improvements across various model sizes (134M-13B),establishing the necessity for next-generation large-scale syntheticpretraining language models. (3) Through comprehensive analysis, we investigateprompt engineering's impact on synthetic training collapse and reveallimitations in conventional collapse detection metrics using validation losses.Our work shows that MAGA can substantially expand training datasets whilemaintaining quality, offering a reliably pathway for scaling models beyond datalimitations.</description><author>Xintong Hao, Ke Shen, Chenggang Li</author><pubDate>Thu, 06 Feb 2025 17:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04235v1</guid></item><item><title>A Classification System Approach in Predicting Chinese Censorship</title><link>http://arxiv.org/abs/2502.04234v1</link><description>This paper is dedicated to using a classifier to predict whether a Weibo postwould be censored under the Chinese internet. Through randomized sampling from\citeauthor{Fu2021} and Chinese tokenizing strategies, we constructed a cleanedChinese phrase dataset with binary censorship markings. Utilizing variousprobability-based information retrieval methods on the data, we were able toderive 4 logistic regression models for classification. Furthermore, weexperimented with pre-trained transformers to perform similar classificationtasks. After evaluating both the macro-F1 and ROC-AUC metrics, we concludedthat the Fined-Tuned BERT model exceeds other strategies in performance.</description><author>Matt Prodani, Tianchu Ze, Yushen Hu</author><pubDate>Thu, 06 Feb 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04234v1</guid></item><item><title>Graph machine learning for flight delay prediction due to holding manouver</title><link>http://arxiv.org/abs/2502.04233v1</link><description>Flight delays due to holding maneuvers are a critical and costly phenomenonin aviation, driven by the need to manage air traffic congestion and ensuresafety. Holding maneuvers occur when aircraft are instructed to circle indesignated airspace, often due to factors such as airport congestion, adverseweather, or air traffic control restrictions. This study models the predictionof flight delays due to holding maneuvers as a graph problem, leveragingadvanced Graph Machine Learning (Graph ML) techniques to capture complexinterdependencies in air traffic networks. Holding maneuvers, while crucial forsafety, cause increased fuel usage, emissions, and passenger dissatisfaction,making accurate prediction essential for operational efficiency. Traditionalmachine learning models, typically using tabular data, often overlookspatial-temporal relations within air traffic data. To address this, we modelthe problem of predicting holding as edge feature prediction in a directed(multi)graph where we apply both CatBoost, enriched with graph featurescapturing network centrality and connectivity, and Graph Attention Networks(GATs), which excel in relational data contexts. Our results indicate thatCatBoost outperforms GAT in this imbalanced dataset, effectively predictingholding events and offering interpretability through graph-based featureimportance. Additionally, we discuss the model's potential operational impactthrough a web-based tool that allows users to simulate real-time delaypredictions. This research underscores the viability of graph-based approachesfor predictive analysis in aviation, with implications for enhancing fuelefficiency, reducing delays, and improving passenger experience.</description><author>Jorge L. Franco, Manoel V. Machado Neto, Filipe A. N. Verri, Diego R. Amancio</author><pubDate>Thu, 06 Feb 2025 17:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04233v1</guid></item><item><title>How Reliable are Causal Probing Interventions?</title><link>http://arxiv.org/abs/2408.15510v3</link><description>Causal probing aims to analyze foundation models by examining how interveningon their representation of various latent properties impacts their outputs.Recent works have cast doubt on the theoretical basis of several leading causalprobing methods, but it has been unclear how to systematically evaluate theeffectiveness of these methods in practice. To address this, we define two keycausal probing desiderata: completeness (how thoroughly the representation ofthe target property has been transformed) and selectivity (how littlenon-targeted properties have been impacted). We find that there is an inherenttradeoff between the two, which we define as reliability, their harmonic mean.We introduce an empirical analysis framework to measure and evaluate thesequantities, allowing us to make the first direct comparisons between differentfamilies of leading causal probing methods (e.g., linear vs. nonlinear, orconcept removal vs. counterfactual interventions). We find that: (1) no methodis reliable across all layers; (2) more reliable methods have a greater impacton LLM behavior; (3) nonlinear interventions are more reliable in early andintermediate layers, and linear interventions are more reliable in laterlayers; and (4) concept removal methods are far less reliable thancounterfactual interventions, suggesting that they may not be an effectiveapproach to causal probing.</description><author>Marc Canby, Adam Davies, Chirag Rastogi, Julia Hockenmaier</author><pubDate>Thu, 06 Feb 2025 17:16:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15510v3</guid></item><item><title>On robust recovery of signals from indirect observations</title><link>http://arxiv.org/abs/2501.01935v2</link><description>We consider an uncertain linear inverse problem as follows. Given observation$\omega=Ax_*+\zeta$ where $A\in {\bf R}^{m\times p}$ and $\zeta\in {\bf R}^{m}$is observation noise, we want to recover unknown signal $x_*$, known to belongto a convex set ${\cal X}\subset{\bf R}^{n}$. As opposed to the "standard"setting of such problem, we suppose that the model noise $\zeta$ is "corrupted"-- contains an uncertain (deterministic dense or singular) component.Specifically, we assume that $\zeta$ decomposes into $\zeta=N\nu_*+\xi$ where$\xi$ is the random noise and $N\nu_*$ is the "adversarial contamination" withknown $\cal N\subset {\bf R}^n$ such that $\nu_*\in \cal N$ and $N\in {\bfR}^{m\times n}$. We consider two "uncertainty setups" in which $\cal N$ iseither a convex bounded set or is the set of sparse vectors (with at most $s$nonvanishing entries). We analyse the performance of "uncertainty-immunized"polyhedral estimates -- a particular class of nonlinear estimates as introducedin [15, 16] -- and show how "presumably good" estimates of the sort may beconstructed in the situation where the signal set is an ellitope (essentially,a symmetric convex set delimited by quadratic surfaces) by means of efficientconvex optimization routines.</description><author>Yannis Bekri, Anatoli Juditsky, Arkadi Nemirovski</author><pubDate>Thu, 06 Feb 2025 17:15:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01935v2</guid></item><item><title>XAttnMark: Learning Robust Audio Watermarking with Cross-Attention</title><link>http://arxiv.org/abs/2502.04230v1</link><description>The rapid proliferation of generative audio synthesis and editingtechnologies has raised significant concerns about copyright infringement, dataprovenance, and the spread of misinformation through deepfake audio.Watermarking offers a proactive solution by embedding imperceptible,identifiable, and traceable marks into audio content. While recent neuralnetwork-based watermarking methods like WavMark and AudioSeal have improvedrobustness and quality, they struggle to achieve both robust detection andaccurate attribution simultaneously. This paper introduces Cross-AttentionRobust Audio Watermark (XAttnMark), which bridges this gap by leveragingpartial parameter sharing between the generator and the detector, across-attention mechanism for efficient message retrieval, and a temporalconditioning module for improved message distribution. Additionally, we proposea psychoacoustic-aligned temporal-frequency masking loss that capturesfine-grained auditory masking effects, enhancing watermark imperceptibility.Our approach achieves state-of-the-art performance in both detection andattribution, demonstrating superior robustness against a wide range of audiotransformations, including challenging generative editing with strong editingstrength. The project webpage is available athttps://liuyixin-louis.github.io/xattnmark/.</description><author>Yixin Liu, Lie Lu, Jihui Jin, Lichao Sun, Andrea Fanelli</author><pubDate>Thu, 06 Feb 2025 17:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04230v1</guid></item><item><title>Dark Distillation: Backdooring Distilled Datasets without Accessing Raw Data</title><link>http://arxiv.org/abs/2502.04229v1</link><description>Dataset distillation (DD) enhances training efficiency and reduces bandwidthby condensing large datasets into smaller synthetic ones. It enables models toachieve performance comparable to those trained on the raw full dataset and hasbecome a widely adopted method for data sharing. However, security concerns inDD remain underexplored. Existing studies typically assume that maliciousbehavior originates from dataset owners during the initial distillationprocess, where backdoors are injected into raw datasets. In contrast, this workis the first to address a more realistic and concerning threat: attackers mayintercept the dataset distribution process, inject backdoors into the distilleddatasets, and redistribute them to users. While distilled datasets werepreviously considered resistant to backdoor attacks, we demonstrate that theyremain vulnerable to such attacks. Furthermore, we show that attackers do noteven require access to any raw data to inject the backdoors successfully.Specifically, our approach reconstructs conceptual archetypes for each classfrom the model trained on the distilled dataset. Backdoors are then injectedinto these archetypes to update the distilled dataset. Moreover, we ensure theupdated dataset not only retains the backdoor but also preserves the originaloptimization trajectory, thus maintaining the knowledge of the raw dataset. Toachieve this, a hybrid loss is designed to integrate backdoor information alongthe benign optimization trajectory, ensuring that previously learnedinformation is not forgotten. Extensive experiments demonstrate that distilleddatasets are highly vulnerable to backdoor attacks, with risks pervasive acrossvarious raw datasets, distillation methods, and downstream training strategies.Moreover, our attack method is efficient, capable of synthesizing a maliciousdistilled dataset in under one minute in certain cases.</description><author>Ziyuan Yang, Ming Yan, Yi Zhang, Joey Tianyi Zhou</author><pubDate>Thu, 06 Feb 2025 17:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04229v1</guid></item><item><title>Keep It Light! Simplifying Image Clustering Via Text-Free Adapters</title><link>http://arxiv.org/abs/2502.04226v1</link><description>Many competitive clustering pipelines have a multi-modal design, leveraginglarge language models (LLMs) or other text encoders, and text-image pairs,which are often unavailable in real-world downstream applications.Additionally, such frameworks are generally complicated to train and requiresubstantial computational resources, making widespread adoption challenging. Inthis work, we show that in deep clustering, competitive performance with morecomplex state-of-the-art methods can be achieved using a text-free and highlysimplified training pipeline. In particular, our approach, Simple Clusteringvia Pre-trained models (SCP), trains only a small cluster head while leveragingpre-trained vision model feature representations and positive data pairs.Experiments on benchmark datasets including CIFAR-10, CIFAR-20, CIFAR-100,STL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highlycompetitive performance. Furthermore, we provide a theoretical resultexplaining why, at least under ideal conditions, additional text-basedembeddings may not be necessary to achieve strong clustering performance invision.</description><author>Yicen Li, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios, Paul D. McNicholas</author><pubDate>Thu, 06 Feb 2025 17:12:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04226v1</guid></item><item><title>Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents</title><link>http://arxiv.org/abs/2502.04223v1</link><description>Optical Character Recognition (OCR) technology is widely used to extract textfrom images of documents, facilitating efficient digitization and dataretrieval. However, merely extracting text is insufficient when dealing withcomplex documents. Fully comprehending such documents requires an understandingof their structure -- including formatting, formulas, tables, and the readingorder of multiple blocks and columns across multiple pages -- as well assemantic information for detecting elements like footnotes and image captions.This comprehensive understanding is crucial for downstream tasks such asretrieval, document question answering, and data curation for training LargeLanguage Models (LLMs) and Vision Language Models (VLMs). To address this, weintroduce \'Eclair, a general-purpose text-extraction tool specificallydesigned to process a wide range of document types. Given an image, \'Eclair isable to extract formatted text in reading order, along with bounding boxes andtheir corresponding semantic classes. To thoroughly evaluate these novelcapabilities, we introduce our diverse human-annotated benchmark fordocument-level OCR and semantic classification. \'Eclair achievesstate-of-the-art accuracy on this benchmark, outperforming other methods acrosskey metrics. Additionally, we evaluate \'Eclair on established benchmarks,demonstrating its versatility and strength across several evaluation standards.</description><author>Ilia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle, Philipp Fischer, Kateryna Chumachenko, Timo Roman, Jarno Seppänen, Jupinder Parmar, Joseph Jennings, Andrew Tao, Karan Sapra</author><pubDate>Thu, 06 Feb 2025 17:07:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04223v1</guid></item><item><title>E(n) Equivariant Topological Neural Networks</title><link>http://arxiv.org/abs/2405.15429v5</link><description>Graph neural networks excel at modeling pairwise interactions, but theycannot flexibly accommodate higher-order interactions and features. Topologicaldeep learning (TDL) has emerged recently as a promising tool for addressingthis issue. TDL enables the principled modeling of arbitrary multi-way,hierarchical higher-order interactions by operating on combinatorialtopological spaces, such as simplicial or cell complexes, instead of graphs.However, little is known about how to leverage geometric features such aspositions and velocities for TDL. This paper introduces E(n)-EquivariantTopological Neural Networks (ETNNs), which are E(n)-equivariant message-passingnetworks operating on combinatorial complexes, formal objects unifying graphs,hypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometricnode features while respecting rotation, reflection, and translationequivariance. Moreover, being TDL models, ETNNs are natively ready for settingswith heterogeneous interactions. We provide a theoretical analysis to show theimproved expressiveness of ETNNs over architectures for geometric graphs. Wealso show how E(n)-equivariant variants of TDL models can be directly derivedfrom our framework. The broad applicability of ETNNs is demonstrated throughtwo tasks of vastly different scales: i) molecular property prediction on theQM9 benchmark and ii) land-use regression for hyper-local estimation of airpollution with multi-resolution irregular geospatial data. The results indicatethat ETNNs are an effective tool for learning from diverse types of richlystructured data, as they match or surpass SotA equivariant TDL models with asignificantly smaller computational burden, thus highlighting the benefits of aprincipled geometric inductive bias. Our implementation of ETNNs can be foundat https://github.com/NSAPH-Projects/topological-equivariant-networks.</description><author>Claudio Battiloro, Ege Karaismailoğlu, Mauricio Tec, George Dasoulas, Michelle Audirac, Francesca Dominici</author><pubDate>Thu, 06 Feb 2025 17:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15429v5</guid></item><item><title>Fully Autonomous AI Agents Should Not be Developed</title><link>http://arxiv.org/abs/2502.02649v2</link><description>This paper argues that fully autonomous AI agents should not be developed. Insupport of this position, we build from prior scientific literature and currentproduct marketing to delineate different AI agent levels and detail the ethicalvalues at play in each, documenting trade-offs in potential benefits and risks.Our analysis reveals that risks to people increase with the autonomy of asystem: The more control a user cedes to an AI agent, the more risks to peoplearise. Particularly concerning are safety risks, which affect human life andimpact further values.</description><author>Margaret Mitchell, Avijit Ghosh, Alexandra Sasha Luccioni, Giada Pistilli</author><pubDate>Thu, 06 Feb 2025 17:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02649v2</guid></item><item><title>NLP-Based .NET CLR Event Logs Analyzer</title><link>http://arxiv.org/abs/2502.04219v1</link><description>In this paper, we present a tool for analyzing .NET CLR event logs based on anovel method inspired by Natural Language Processing (NLP) approach. Ourresearch addresses the growing need for effective monitoring and optimizationof software systems through detailed event log analysis. We utilize aBERT-based architecture with an enhanced tokenization process customized toevent logs. The tool, developed using Python, its libraries, and an SQLitedatabase, allows both conducting experiments for academic purposes andefficiently solving industry-emerging tasks. Our experiments demonstrate theefficacy of our approach in compressing event sequences, detecting recurringpatterns, and identifying anomalies. The trained model shows promising results,with a high accuracy rate in anomaly detection, which demonstrates thepotential of NLP methods to improve the reliability and stability of softwaresystems.</description><author>Maxim Stavtsev, Sergey Shershakov</author><pubDate>Thu, 06 Feb 2025 17:01:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04219v1</guid></item><item><title>Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data</title><link>http://arxiv.org/abs/2502.04218v1</link><description>Large Language Models (LLMs) have been shown to be biased in prior work, asthey generate text that is in line with stereotypical views of the world orthat is not representative of the viewpoints and values of historicallymarginalized demographic groups. In this work, we propose using data fromparallel men's and women's events at the Olympic Games to investigate differentforms of gender bias in language models. We define three metrics to measurebias, and find that models are consistently biased against women when thegender is ambiguous in the prompt. In this case, the model frequently retrievesonly the results of the men's event with or without acknowledging them as such,revealing pervasive gender bias in LLMs in the context of athletics.</description><author>Laura Biester</author><pubDate>Thu, 06 Feb 2025 17:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04218v1</guid></item><item><title>Audio-visual cross-modality knowledge transfer for machine learning-based in-situ monitoring in laser additive manufacturing</title><link>http://arxiv.org/abs/2408.05307v3</link><description>Various machine learning (ML)-based in-situ monitoring systems have beendeveloped to detect anomalies and defects in laser additive manufacturing (LAM)processes. While multimodal fusion, which integrates data from visual, audio,and other modalities, can improve monitoring performance, it also increaseshardware, computational, and operational costs. This paper introduces across-modality knowledge transfer (CMKT) methodology for LAM in-situmonitoring, which transfers knowledge from a source modality to a targetmodality. CMKT enhances the representativeness of the features extracted fromthe target modality, allowing the removal of source modality sensors duringprediction. This paper proposes three CMKT methods: semantic alignment, fullysupervised mapping, and semi-supervised mapping. The semantic alignment methodestablishes a shared encoded space between modalities to facilitate knowledgetransfer. It employs a semantic alignment loss to align the distributions ofidentical groups (e.g., visual and audio defective groups) and a separationloss to distinguish different groups (e.g., visual defective and audiodefect-free groups). The two mapping methods transfer knowledge by derivingfeatures from one modality to another using fully supervised andsemi-supervised learning approaches. In a case study for LAM in-situ defectdetection, the proposed CMKT methods were compared with multimodal audio-visualfusion. The semantic alignment method achieved an accuracy of 98.6% whileremoving the audio modality during the prediction phase, which is comparable tothe 98.2% accuracy obtained through multimodal fusion. Using explainableartificial intelligence, we discovered that semantic alignment CMKT can extractmore representative features while reducing noise by leveraging the inherentcorrelations between modalities.</description><author>Jiarui Xie, Mutahar Safdar, Lequn Chen, Seung Ki Moon, Yaoyao Fiona Zhao</author><pubDate>Thu, 06 Feb 2025 16:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05307v3</guid></item><item><title>A Unified Theory of Quantum Neural Network Loss Landscapes</title><link>http://arxiv.org/abs/2408.11901v4</link><description>Classical neural networks with random initialization famously behave asGaussian processes in the limit of many neurons, which allows one to completelycharacterize their training and generalization behavior. No such generalunderstanding exists for quantum neural networks (QNNs), which -- outside ofcertain special cases -- are known to not behave as Gaussian processes whenrandomly initialized. We here prove that QNNs and their first two derivativesinstead generally form what we call "Wishart processes," where certainalgebraic properties of the network determine the hyperparameters of theprocess. This Wishart process description allows us to, for the first time:give necessary and sufficient conditions for a QNN architecture to have aGaussian process limit; calculate the full gradient distribution, generalizingpreviously known barren plateau results; and calculate the local minimadistribution of algebraically constrained QNNs. Our unified framework suggestsa certain simple operational definition for the "trainability" of a given QNNmodel using a newly introduced, experimentally accessible quantity we call the"degrees of freedom" of the network architecture.</description><author>Eric R. Anschuetz</author><pubDate>Thu, 06 Feb 2025 16:55:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11901v4</guid></item><item><title>Algorithmic causal structure emerging through compression</title><link>http://arxiv.org/abs/2502.04210v1</link><description>We explore the relationship between causality, symmetry, and compression. Webuild on and generalize the known connection between learning and compressionto a setting where causal models are not identifiable. We propose a frameworkwhere causality emerges as a consequence of compressing data across multipleenvironments. We define algorithmic causality as an alternative definition ofcausality when traditional assumptions for causal identifiability do not hold.We demonstrate how algorithmic causal and symmetric structures can emerge fromminimizing upper bounds on Kolmogorov complexity, without knowledge ofintervention targets. We hypothesize that these insights may also provide anovel perspective on the emergence of causality in machine learning models,such as large language models, where causal relationships may not be explicitlyidentifiable.</description><author>Liang Wendong, Simon Buchholz, Bernhard Schölkopf</author><pubDate>Thu, 06 Feb 2025 16:50:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04210v1</guid></item><item><title>Enhanced Feature-based Image Stitching for Endoscopic Videos in Pediatric Eosinophilic Esophagitis</title><link>http://arxiv.org/abs/2502.04207v1</link><description>Video endoscopy represents a major advance in the investigation ofgastrointestinal diseases. Reviewing endoscopy videos often involves frequentadjustments and reorientations to piece together a complete view, which can beboth time-consuming and prone to errors. Image stitching techniques addressthis issue by providing a continuous and complete visualization of the examinedarea. However, endoscopic images, particularly those of the esophagus, presentunique challenges. The smooth surface, lack of distinct feature points, andnon-horizontal orientation complicate the stitching process, renderingtraditional feature-based methods often ineffective for these types of images.In this paper, we propose a novel preprocessing pipeline designed to enhanceendoscopic image stitching through advanced computational techniques. Ourapproach converts endoscopic video data into continuous 2D images by followingfour key steps: (1) keyframe selection, (2) image rotation adjustment tocorrect distortions, (3) surface unwrapping using polar coordinatetransformation to generate a flat image, and (4) feature point matchingenhanced by Adaptive Histogram Equalization for improved feature detection. Weevaluate stitching quality through the assessment of valid feature point matchpairs. Experiments conducted on 20 pediatric endoscopy videos demonstrate thatour method significantly improves image alignment and stitching qualitycompared to traditional techniques, laying a robust foundation for moreeffective panoramic image creation.</description><author>Juming Xiong, Muyang Li, Ruining Deng, Tianyuan Yao, Regina N Tyree, Girish Hiremath, Yuankai Huo</author><pubDate>Thu, 06 Feb 2025 16:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04207v1</guid></item><item><title>Ensuring Reliability via Hyperparameter Selection: Review and Advances</title><link>http://arxiv.org/abs/2502.04206v1</link><description>Hyperparameter selection is a critical step in the deployment of artificialintelligence (AI) models, particularly in the current era of foundational,pre-trained, models. By framing hyperparameter selection as a multiplehypothesis testing problem, recent research has shown that it is possible toprovide statistical guarantees on population risk measures attained by theselected hyperparameter. This paper reviews the Learn-Then-Test (LTT)framework, which formalizes this approach, and explores several extensionstailored to engineering-relevant scenarios. These extensions encompassdifferent risk measures and statistical guarantees, multi-objectiveoptimization, the incorporation of prior knowledge and dependency structuresinto the hyperparameter selection process, as well as adaptivity. The paperalso includes illustrative applications for communication systems.</description><author>Amirmohammad Farzaneh, Osvaldo Simeone</author><pubDate>Thu, 06 Feb 2025 16:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04206v1</guid></item><item><title>"Short-length" Adversarial Training Helps LLMs Defend "Long-length" Jailbreak Attacks: Theoretical and Empirical Evidence</title><link>http://arxiv.org/abs/2502.04204v1</link><description>Jailbreak attacks against large language models (LLMs) aim to induce harmfulbehaviors in LLMs through carefully crafted adversarial prompts. To mitigateattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,training LLMs on some of the most adversarial prompts to help them learn how tobehave safely under attacks. During AT, the length of adversarial prompts playsa critical role in the robustness of aligned LLMs. This paper focuses onadversarial suffix jailbreak attacks and unveils that to defend against ajailbreak attack with an adversarial suffix of length $\Theta(M)$, it is enoughto align LLMs on prompts with adversarial suffixes of length$\Theta(\sqrt{M})$. Theoretically, we analyze the adversarial in-contextlearning of linear transformers on linear regression tasks and prove a robustgeneralization bound for trained transformers. The bound depends on the term$\Theta(\sqrt{M_{\text{test}}}/M_{\text{train}})$, where $M_{\text{train}}$ and$M_{\text{test}}$ are the number of adversarially perturbed in-context samplesduring training and testing. Empirically, we conduct AT on popular open-sourceLLMs and evaluate their robustness against jailbreak attacks of differentadversarial suffix lengths. Results confirm a positive correlation between theattack success rate and the ratio of the square root of the adversarial suffixduring jailbreaking to the length during AT. Our findings show that it ispractical to defend "long-length" jailbreak attacks via efficient"short-length" AT. The code is available at https://github.com/fshp971/adv-icl.</description><author>Shaopeng Fu, Liang Ding, Di Wang</author><pubDate>Thu, 06 Feb 2025 16:44:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04204v1</guid></item><item><title>Safeguarding connected autonomous vehicle communication: Protocols, intra- and inter-vehicular attacks and defenses</title><link>http://arxiv.org/abs/2502.04201v1</link><description>The advancements in autonomous driving technology, coupled with the growinginterest from automotive manufacturers and tech companies, suggest a risingadoption of Connected Autonomous Vehicles (CAVs) in the near future. Despitesome evidence of higher accident rates in AVs, these incidents tend to resultin less severe injuries compared to traditional vehicles due to cooperativesafety measures. However, the increased complexity of CAV systems exposes themto significant security vulnerabilities, potentially compromising theirperformance and communication integrity. This paper contributes by presenting adetailed analysis of existing security frameworks and protocols, focusing onintra- and inter-vehicle communications. We systematically evaluate theeffectiveness of these frameworks in addressing known vulnerabilities andpropose a set of best practices for enhancing CAV communication security. Thepaper also provides a comprehensive taxonomy of attack vectors in CAVecosystems and suggests future research directions for designing more robustsecurity mechanisms. Our key contributions include the development of a newclassification system for CAV security threats, the proposal of practicalsecurity protocols, and the introduction of use cases that demonstrate howthese protocols can be integrated into real-world CAV applications. Theseinsights are crucial for advancing secure CAV adoption and ensuring the safeintegration of autonomous vehicles into intelligent transportation systems.</description><author>Mohammed Aledhari, Rehma Razzak, Mohamed Rahouti, Abbas Yazdinejad, Reza M. Parizi, Basheer Qolomany, Mohsen Guizani, Junaid Qadir, Ala Al-Fuqaha</author><pubDate>Thu, 06 Feb 2025 16:43:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04201v1</guid></item><item><title>Expanding Training Data for Endoscopic Phenotyping of Eosinophilic Esophagitis</title><link>http://arxiv.org/abs/2502.04199v1</link><description>Eosinophilic esophagitis (EoE) is a chronic esophageal disorder marked byeosinophil-dominated inflammation. Diagnosing EoE usually involves endoscopicinspection of the esophageal mucosa and obtaining esophageal biopsies forhistologic confirmation. Recent advances have seen AI-assisted endoscopicimaging, guided by the EREFS system, emerge as a potential alternative toreduce reliance on invasive histological assessments. Despite theseadvancements, significant challenges persist due to the limited availability ofdata for training AI models - a common issue even in the development of AI formore prevalent diseases. This study seeks to improve the performance of deeplearning-based EoE phenotype classification by augmenting our training datawith a diverse set of images from online platforms, public datasets, andelectronic textbooks increasing our dataset from 435 to 7050 images. Weutilized the Data-efficient Image Transformer for image classification andincorporated attention map visualizations to boost interpretability. Thefindings show that our expanded dataset and model enhancements improveddiagnostic accuracy, robustness, and comprehensive analysis, enhancing patientoutcomes.</description><author>Juming Xiong, Hou Xiong, Quan Liu, Ruining Deng, Regina N Tyree, Girish Hiremath, Yuankai Huo</author><pubDate>Thu, 06 Feb 2025 16:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04199v1</guid></item><item><title>Relaxed Quantile Regression: Prediction Intervals for Asymmetric Noise</title><link>http://arxiv.org/abs/2406.03258v2</link><description>Constructing valid prediction intervals rather than point estimates is awell-established approach for uncertainty quantification in the regressionsetting. Models equipped with this capacity output an interval of values inwhich the ground truth target will fall with some prespecified probability.This is an essential requirement in many real-world applications where simplepoint predictions' inability to convey the magnitude and frequency of errorsrenders them insufficient for high-stakes decisions. Quantile regression is aleading approach for obtaining such intervals via the empirical estimation ofquantiles in the (non-parametric) distribution of outputs. This method issimple, computationally inexpensive, interpretable, assumption-free, andeffective. However, it does require that the specific quantiles being learnedare chosen a priori. This results in (a) intervals that are arbitrarilysymmetric around the median which is sub-optimal for realistic skeweddistributions, or (b) learning an excessive number of intervals. In this work,we propose Relaxed Quantile Regression (RQR), a direct alternative to quantileregression based interval construction that removes this arbitrary constraintwhilst maintaining its strengths. We demonstrate that this added flexibilityresults in intervals with an improvement in desirable qualities (e.g. meanwidth) whilst retaining the essential coverage guarantees of quantileregression.</description><author>Thomas Pouplin, Alan Jeffares, Nabeel Seedat, Mihaela van der Schaar</author><pubDate>Thu, 06 Feb 2025 16:32:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03258v2</guid></item><item><title>The Best Instruction-Tuning Data are Those That Fit</title><link>http://arxiv.org/abs/2502.04194v1</link><description>High-quality supervised fine-tuning (SFT) data are crucial for elicitingstrong capabilities from pretrained large language models (LLMs). Typically,instructions are paired with multiple responses sampled from other LLMs, whichare often out of the distribution of the target model to be fine-tuned. This,at scale, can lead to diminishing returns and even hurt the models' performanceand robustness. We propose **GRAPE**, a novel SFT framework that accounts forthe unique characteristics of the target model. For each instruction, itgathers responses from various LLMs and selects the one with the highestprobability measured by the target model, indicating that it aligns mostclosely with the target model's pretrained distribution; it then proceeds withstandard SFT training. We first evaluate GRAPE with a controlled experiment, where we sample varioussolutions for each question in UltraInteract from multiple models and fine-tunecommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B onGRAPE-selected data. GRAPE significantly outperforms strong baselines,including distilling from the strongest model with an absolute gain of up to13.8%, averaged across benchmarks, and training on 3x more data with a maximumperformance improvement of 17.3%. GRAPE's strong performance generalizes torealistic settings. We experiment with the post-training data used for Tulu3and Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more databy 6.1% and a state-of-the-art data selection approach by 3% on averageperformance. Remarkably, using 1/3 of the data and half the number of epochs,GRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.</description><author>Dylan Zhang, Qirun Dai, Hao Peng</author><pubDate>Thu, 06 Feb 2025 16:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04194v1</guid></item><item><title>PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models?</title><link>http://arxiv.org/abs/2502.04192v1</link><description>Multiple works have emerged to push the boundaries on multi-modal largelanguage models (MLLMs) towards pixel-level understanding. Such approaches haveshown strong performance on benchmarks for referring expression segmentationand grounded conversation generation. The current trend in pixel-level MLLMs isto train with pixel-level grounding supervision on large-scale labelled data.However, we show that such MLLMs when evaluated on recent challenging visioncentric benchmarks, exhibit a weak ability in visual question answering.Surprisingly, some of these methods even downgrade the grounding ability ofMLLMs that were never trained with such supervision. In this work, we proposetwo novel challenging benchmarks and show that MLLMs without pixel-levelgrounding supervision can outperform the state of the art in such tasks whenevaluating both the pixel-level grounding and visual question answering. Wepropose simple baselines to extract the grounding information that can beplugged into any MLLM, which we call as PixFoundation. More importantly, westudy the research question of ``When does grounding emerge in MLLMs that arenot trained with pixel-level grounding supervision?'' We show that groundingcan coincide with object parts or location/appearance information. Coderepository is at https://github.com/MSiam/PixFoundation/.</description><author>Mennatullah Siam</author><pubDate>Thu, 06 Feb 2025 16:29:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04192v1</guid></item><item><title>CAST: Corpus-Aware Self-similarity Enhanced Topic modelling</title><link>http://arxiv.org/abs/2410.15136v2</link><description>Topic modelling is a pivotal unsupervised machine learning technique forextracting valuable insights from large document collections. Existing neuraltopic modelling methods often encode contextual information of documents, whileignoring contextual details of candidate centroid words, leading to theinaccurate selection of topic words due to the contextualization gap. Inparallel, it is found that functional words are frequently selected overtopical words. To address these limitations, we introduce CAST: Corpus-AwareSelf-similarity Enhanced Topic modelling, a novel topic modelling method thatbuilds upon candidate centroid word embeddings contextualized on the dataset,and a novel self-similarity-based method to filter out less meaningful tokens.Inspired by findings in contrastive learning that self-similarities offunctional token embeddings in different contexts are much lower than topicaltokens, we find self-similarity to be an effective metric to prevent functionalwords from acting as candidate topic words. Our approach significantly enhancesthe coherence and diversity of generated topics, as well as the topic model'sability to handle noisy data. Experiments on news benchmark datasets and oneTwitter dataset demonstrate the method's superiority in generating coherent,diverse topics, and handling noisy data, outperforming strong baselines.</description><author>Yanan Ma, Chenghao Xiao, Chenhan Yuan, Sabine N van der Veer, Lamiece Hassan, Chenghua Lin, Goran Nenadic</author><pubDate>Thu, 06 Feb 2025 16:21:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15136v2</guid></item><item><title>TabularARGN: A Flexible and Efficient Auto-Regressive Framework for Generating High-Fidelity Synthetic Data</title><link>http://arxiv.org/abs/2501.12012v2</link><description>Synthetic data generation for tabular datasets must balance fidelity,efficiency, and versatility to meet the demands of real-world applications. Weintroduce the Tabular Auto-Regressive Generative Network (TabularARGN), aflexible framework designed to handle mixed-type, multivariate, and sequentialdatasets. By training on all possible conditional probabilities, TabularARGNsupports advanced features such as fairness-aware generation, imputation, andconditional generation on any subset of columns. The framework achievesstate-of-the-art synthetic data quality while significantly reducing trainingand inference times, making it ideal for large-scale datasets with diversestructures. Evaluated across established benchmarks, including realisticdatasets with complex relationships, TabularARGN demonstrates its capability tosynthesize high-quality data efficiently. By unifying flexibility andperformance, this framework paves the way for practical synthetic datageneration across industries.</description><author>Paul Tiwald, Ivona Krchova, Andrey Sidorenko, Mariana Vargas Vieyra, Mario Scriminaci, Michael Platzer</author><pubDate>Thu, 06 Feb 2025 16:18:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12012v2</guid></item><item><title>Multi-agent Architecture Search via Agentic Supernet</title><link>http://arxiv.org/abs/2502.04180v1</link><description>Large Language Model (LLM)-empowered multi-agent systems extend the cognitiveboundaries of individual agents through disciplined collaboration andinteraction, while constructing these systems often requires labor-intensivemanual designs. Despite the availability of methods to automate the design ofagentic workflows, they typically seek to identify a static, complex,one-size-fits-all system, which, however, fails to dynamically allocateinference resources based on the difficulty and domain of each query. Toaddress this challenge, we shift away from the pursuit of a monolithic agenticsystem, instead optimizing the \textbf{agentic supernet}, a probabilistic andcontinuous distribution of agentic architectures. We introduce MaAS, anautomated framework that samples query-dependent agentic systems from thesupernet, delivering high-quality solutions and tailored resource allocation(\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluationacross six benchmarks demonstrates that MaAS \textbf{(I)} requires only$6\sim45\%$ of the inference costs of existing handcrafted or automatedmulti-agent systems, \textbf{(II)} surpasses them by $0.54\%\sim11.82\%$, and\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbonetransferability.</description><author>Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, Xiang Wang</author><pubDate>Thu, 06 Feb 2025 16:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04180v1</guid></item><item><title>Model Human Learners: Computational Models to Guide Instructional Design</title><link>http://arxiv.org/abs/2502.02456v2</link><description>Instructional designers face an overwhelming array of design choices, makingit challenging to identify the most effective interventions. To address thisissue, I propose the concept of a Model Human Learner, a unified computationalmodel of learning that can aid designers in evaluating candidate interventions.This paper presents the first successful demonstration of this concept, showingthat a computational model can accurately predict the outcomes of two human A/Bexperiments -- one testing a problem sequencing intervention and the othertesting an item design intervention. It also demonstrates that such a model cangenerate learning curves without requiring human data and provide theoreticalinsights into why an instructional intervention is effective. These findingslay the groundwork for future Model Human Learners that integrate cognitive andlearning theories to support instructional design across diverse tasks andinterventions.</description><author>Christopher J. MacLellan</author><pubDate>Thu, 06 Feb 2025 16:11:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.02456v2</guid></item><item><title>MRAMG-Bench: A BeyondText Benchmark for Multimodal Retrieval-Augmented Multimodal Generation</title><link>http://arxiv.org/abs/2502.04176v1</link><description>Recent advancements in Retrieval-Augmented Generation (RAG) have shownremarkable performance in enhancing response accuracy and relevance byintegrating external knowledge into generative models. However, existing RAGmethods primarily focus on providing text-only answers, even in multimodalretrieval-augmented generation scenarios. In this work, we introduce theMultimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aimsto generate answers that combine both text and images, fully leveraging themultimodal data within a corpus. Despite the importance of this task, there isa notable absence of a comprehensive benchmark to effectively evaluate MRAMGperformance. To bridge this gap, we introduce the MRAMG-Bench, a carefullycurated, human-annotated dataset comprising 4,346 documents, 14,190 images, and4,800 QA pairs, sourced from three categories: Web Data, Academic Papers, andLifestyle. The dataset incorporates diverse difficulty levels and complexmulti-image scenarios, providing a robust foundation for evaluating multimodalgeneration tasks. To facilitate rigorous evaluation, our MRAMG-Benchincorporates a comprehensive suite of both statistical and LLM-based metrics,enabling a thorough analysis of the performance of popular generative models inthe MRAMG task. Besides, we propose an efficient multimodal answer generationframework that leverages both LLMs and MLLMs to generate multimodal responses.Our datasets are available at: https://huggingface.co/MRAMG.</description><author>Qinhan Yu, Zhiyou Xiao, Binghui Li, Zhengren Wang, Chong Chen, Wentao Zhang</author><pubDate>Thu, 06 Feb 2025 16:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04176v1</guid></item><item><title>Lexical Substitution is not Synonym Substitution: On the Importance of Producing Contextually Relevant Word Substitutes</title><link>http://arxiv.org/abs/2502.04173v1</link><description>Lexical Substitution is the task of replacing a single word in a sentencewith a similar one. This should ideally be one that is not necessarily onlysynonymous, but also fits well into the surrounding context of the target word,while preserving the sentence's grammatical structure. Recent advances inLexical Substitution have leveraged the masked token prediction task ofPre-trained Language Models to generate replacements for a given word in asentence. With this technique, we introduce ConCat, a simple augmented approachwhich utilizes the original sentence to bolster contextual information sent tothe model. Compared to existing approaches, it proves to be very effective inguiding the model to make contextually relevant predictions for the targetword. Our study includes a quantitative evaluation, measured via sentencesimilarity and task performance. In addition, we conduct a qualitative humananalysis to validate that users prefer the substitutions proposed by ourmethod, as opposed to previous methods. Finally, we test our approach on theprevailing benchmark for Lexical Substitution, CoInCo, revealing potentialpitfalls of the benchmark. These insights serve as the foundation for acritical discussion on the way in which Lexical Substitution is evaluated.</description><author>Juraj Vladika, Stephen Meisenbacher, Florian Matthes</author><pubDate>Thu, 06 Feb 2025 16:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04173v1</guid></item><item><title>Bench4Merge: A Comprehensive Benchmark for Merging in Realistic Dense Traffic with Micro-Interactive Vehicles</title><link>http://arxiv.org/abs/2410.15912v2</link><description>While the capabilities of autonomous driving have advanced rapidly, merginginto dense traffic remains a significant challenge, many motion planningmethods for this scenario have been proposed but it is hard to evaluate them.Most existing closed-loop simulators rely on rule-based controls for othervehicles, which results in a lack of diversity and randomness, thus failing toaccurately assess the motion planning capabilities in highly interactivescenarios. Moreover, traditional evaluation metrics are insufficient forcomprehensively evaluating the performance of merging in dense traffic. Inresponse, we proposed a closed-loop evaluation benchmark for assessing motionplanning capabilities in merging scenarios. Our approach involves othervehicles trained in large scale datasets with micro-behavioral characteristicsthat significantly enhance the complexity and diversity. Additionally, we haverestructured the evaluation mechanism by leveraging large language models toassess each autonomous vehicle merging onto the main road. Extensiveexperiments have demonstrated the advanced nature of this evaluation benchmark.Through this benchmark, we have obtained an evaluation of existing methods andidentified common issues. The environment and vehicle motion planning models wehave designed can be accessed athttps://anonymous.4open.science/r/Bench4Merge-EB5D</description><author>Zhengming Wang, Junli Wang, Pengfei Li, Zhaohan Li, Peng Li, Yilun Chen</author><pubDate>Thu, 06 Feb 2025 16:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15912v2</guid></item></channel></rss>