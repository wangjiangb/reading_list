<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 08 May 2023 10:20:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Otter: A Multi-Modal Model with In-Context Instruction Tuning</title><link>http://arxiv.org/abs/2305.03726v1</link><description>Large language models (LLMs) have demonstrated significant universalcapabilities as few/zero-shot learners in various tasks due to theirpre-training on vast amounts of text data, as exemplified by GPT-3, whichboosted to InstrctGPT and ChatGPT, effectively following natural languageinstructions to accomplish real-world tasks. In this paper, we propose tointroduce instruction tuning into multi-modal models, motivated by the Flamingomodel's upstream interleaved format pretraining dataset. We adopt a similarapproach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo(open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT andshowcasing improved instruction-following ability and in-context learning. Wealso optimize OpenFlamingo's implementation for researchers, democratizing therequired training resources from 1$\times$ A100 GPU to 4$\times$ RTX-3090 GPUs,and integrate both OpenFlamingo and Otter into Huggingface Transformers formore researchers to incorporate the models into their customized training andinference pipelines.</description><author>Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu</author><pubDate>Fri, 05 May 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03726v1</guid></item><item><title>DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception</title><link>http://arxiv.org/abs/2305.03724v1</link><description>Closing the domain gap between training and deployment and incorporatingmultiple sensor modalities are two challenging yet critical topics forself-driving. Existing work only focuses on single one of the above topics,overlooking the simultaneous domain and modality shift which pervasively existsin real-world scenarios. A model trained with multi-sensor data collected inEurope may need to run in Asia with a subset of input sensors available. Inthis work, we propose DualCross, a cross-modality cross-domain adaptationframework to facilitate the learning of a more robust monocular bird's-eye-view(BEV) perception model, which transfers the point cloud knowledge from a LiDARsensor in one domain during the training phase to the camera-only testingscenario in a different domain. This work results in the first open analysis ofcross-domain cross-sensor perception and adaptation for monocular 3D tasks inthe wild. We benchmark our approach on large-scale datasets under a wide rangeof domain shifts and show state-of-the-art results against various baselines.</description><author>Yunze Man, Liang-Yan Gui, Yu-Xiong Wang</author><pubDate>Fri, 05 May 2023 18:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03724v1</guid></item><item><title>DSPDet3D: Dynamic Spatial Pruning for 3D Small Object Detection</title><link>http://arxiv.org/abs/2305.03716v1</link><description>In this paper, we propose a new detection framework for 3D small objectdetection. Although deep learning-based 3D object detection methods haveachieved great success in recent years, current methods still struggle on smallobjects due to weak geometric information. With in-depth study, we findincreasing the spatial resolution of the feature maps significantly boosts theperformance of 3D small object detection. And more interestingly, though thecomputational overhead increases dramatically with resolution, the growthmainly comes from the upsampling operation of the decoder. Inspired by this, wepresent a high-resolution multi-level detector with dynamic spatial pruningnamed DSPDet3D, which detects objects from large to small by iterativeupsampling and meanwhile prunes the spatial representation of the scene atregions where there is no smaller object to be detected in higher resolution.As the 3D detector only needs to predict sparse bounding boxes, pruning a largeamount of uninformative features does not degrade the detection performance butsignificantly reduces the computational cost of upsampling. In this way, ourDSPDet3D achieves high accuracy on small object detection while requiring evenless memory footprint and inference time. On ScanNet and TO-SCENE dataset, ourmethod improves the detection performance of small objects to a new level whileachieving leading inference speed among all mainstream indoor 3D objectdetection methods.</description><author>Xiuwei Xu, Zhihao Sun, Ziwei Wang, Hongmin Liu, Jie Zhou, Jiwen Lu</author><pubDate>Fri, 05 May 2023 18:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03716v1</guid></item><item><title>Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management</title><link>http://arxiv.org/abs/2305.03715v1</link><description>This study investigates the potential of an ambulatory device thatincorporates Large Language Models (LLMs) in cadence with other specialized MLmodels to assess anemia severity in sickle cell patients in real time. Thedevice would rely on sensor data that measures angiogenic material levels toassess anemia severity, providing real-time information to patients andclinicians to reduce the frequency of vaso-occlusive crises because of theearly detection of anemia severity, allowing for timely interventions andpotentially reducing the likelihood of serious complications. The mainchallenges in developing such a device are the creation of a reliablenon-invasive tool for angiogenic level assessment, a biophysics model and thepractical consideration of an LLM communicating with emergency personnel onbehalf of an incapacitated patient. A possible system is proposed, and thelimitations of this approach are discussed.</description><author>Oluwatosin Ogundare, Subuola Sofolahan</author><pubDate>Fri, 05 May 2023 18:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03715v1</guid></item><item><title>Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos</title><link>http://arxiv.org/abs/2305.03713v1</link><description>Modern generators render talking-head videos with impressive levels ofphotorealism, ushering in new user experiences such as videoconferencing underconstrained bandwidth budgets. Their safe adoption, however, requires amechanism to verify if the rendered video is trustworthy. For instance, forvideoconferencing we must identify cases in which a synthetic video portraituses the appearance of an individual without their consent. We term this taskavatar fingerprinting. We propose to tackle it by leveraging facial motionsignatures unique to each person. Specifically, we learn an embedding in whichthe motion signatures of one identity are grouped together, and pushed awayfrom those of other identities, regardless of the appearance in the syntheticvideo. Avatar fingerprinting algorithms will be critical as talking headgenerators become more ubiquitous, and yet no large scale datasets exist forthis new task. Therefore, we contribute a large dataset of people deliveringscripted and improvised short monologues, accompanied by synthetic videos inwhich we render videos of one person using the facial appearance of another.Project page: https://research.nvidia.com/labs/nxp/avatar-fingerprinting/.</description><author>Ekta Prashnani, Koki Nagano, Shalini De Mello, David Luebke, Orazio Gallo</author><pubDate>Fri, 05 May 2023 18:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03713v1</guid></item><item><title>Statistical Inference for Fairness Auditing</title><link>http://arxiv.org/abs/2305.03712v1</link><description>Before deploying a black-box model in high-stakes problems, it is importantto evaluate the model's performance on sensitive subpopulations. For example,in a recidivism prediction task, we may wish to identify demographic groups forwhich our prediction model has unacceptably high false positive rates orcertify that no such groups exist. In this paper, we frame this task, oftenreferred to as "fairness auditing," in terms of multiple hypothesis testing. Weshow how the bootstrap can be used to simultaneously bound performancedisparities over a collection of groups with statistical guarantees. Ourmethods can be used to flag subpopulations affected by model underperformance,and certify subpopulations for which the model performs adequately. Crucially,our audit is model-agnostic and applicable to nearly any performance metric orgroup fairness criterion. Our methods also accommodate extremely rich -- eveninfinite -- collections of subpopulations. Further, we generalize beyondsubpopulations by showing how to assess performance over certain distributionshifts. We test the proposed methods on benchmark datasets in predictiveinference and algorithmic fairness and find that our audits can provideinterpretable and trustworthy guarantees.</description><author>John J. Cherian, Emmanuel J. Cand√®s</author><pubDate>Fri, 05 May 2023 18:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03712v1</guid></item><item><title>Is dataset condensation a silver bullet for healthcare data sharing?</title><link>http://arxiv.org/abs/2305.03711v1</link><description>Safeguarding personal information is paramount for healthcare data sharing, achallenging issue without any silver bullet thus far. We study the prospect ofa recent deep-learning advent, dataset condensation (DC), in sharing healthcaredata for AI research, and the results are promising. The condensed dataabstracts original records and irreversibly conceals individual-level knowledgeto achieve a bona fide de-identification, which permits free sharing. Moreover,the original deep-learning utilities are well preserved in the condensed datawith compressed volume and accelerated model convergences. In PhysioNet-2012, acondensed dataset of 20 samples can orient deep models attaining 80.3% test AUCof mortality prediction (versus 85.8% of 5120 original records), an inspiringdiscovery generalised to MIMIC-III and Coswara datasets. We also interpret theinhere privacy protections of DC through theoretical analysis and empiricalevidence. Dataset condensation opens a new gate to sharing healthcare data forAI research with multiple desirable traits.</description><author>Yujiang Wang, Anshul Thakur, Mingzhi Dong, Pingchuan Ma, Stavros Petridis, Li Shang, Tingting Zhu, David Clifton</author><pubDate>Fri, 05 May 2023 18:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03711v1</guid></item><item><title>Data Encoding For Healthcare Data Democratisation and Information Leakage Prevention</title><link>http://arxiv.org/abs/2305.03710v1</link><description>The lack of data democratization and information leakage from trained modelshinder the development and acceptance of robust deep learning-based healthcaresolutions. This paper argues that irreversible data encoding can provide aneffective solution to achieve data democratization without violating theprivacy constraints imposed on healthcare data and clinical models. An idealencoding framework transforms the data into a new space where it isimperceptible to a manual or computational inspection. However, encoded datashould preserve the semantics of the original data such that deep learningmodels can be trained effectively. This paper hypothesizes the characteristicsof the desired encoding framework and then exploits random projections andrandom quantum encoding to realize this framework for dense and longitudinal ortime-series data. Experimental evaluation highlights that models trained onencoded time-series data effectively uphold the information bottleneckprinciple and hence, exhibit lesser information leakage from trained models.</description><author>Anshul Thakur, Tingting Zhu, Vinayak Abrol, Jacob Armstrong, Yujiang Wang, David A. Clifton</author><pubDate>Fri, 05 May 2023 18:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03710v1</guid></item><item><title>LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application</title><link>http://arxiv.org/abs/2305.01095v2</link><description>The development of Adaptive Cruise Control (ACC) systems aims to enhance thesafety and comfort of vehicles by automatically regulating the speed of thevehicle to ensure a safe gap from the preceding vehicle. However, conventionalACC systems are unable to adapt themselves to changing driving conditions anddrivers' behavior. To address this limitation, we propose a Long Short-TermMemory (LSTM) based ACC system that can learn from past driving experiences andadapt and predict new situations in real time. The model is constructed basedon the real-world highD dataset, acquired from German highways with theassistance of camera-equipped drones. We evaluated the ACC system underaggressive lane changes when the side lane preceding vehicle cut off, forcingthe targeted driver to reduce speed. To this end, the proposed system wasassessed on a simulated driving environment and compared with a feedforwardArtificial Neural Network (ANN) model and Model Predictive Control (MPC) model.The results show that the LSTM-based system is 19.25% more accurate than theANN model and 5.9% more accurate than the MPC model in terms of predictingfuture values of subject vehicle acceleration. The simulation is done inMatlab/Simulink environment.</description><author>Rajmeet Singh, Saeed Mozaffari, Mahdi Rezaei, Shahpour Alirezaee</author><pubDate>Fri, 05 May 2023 18:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01095v2</guid></item><item><title>Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl</title><link>http://arxiv.org/abs/2305.01582v3</link><description>PySR is an open-source library for practical symbolic regression, a type ofmachine learning which aims to discover human-interpretable symbolic models.PySR was developed to democratize and popularize symbolic regression for thesciences, and is built on a high-performance distributed back-end, a flexiblesearch algorithm, and interfaces with several deep learning packages. PySR'sinternal search algorithm is a multi-population evolutionary algorithm, whichconsists of a unique evolve-simplify-optimize loop, designed for optimizationof unknown scalar constants in newly-discovered empirical expressions. PySR'sbackend is the extremely optimized Julia library SymbolicRegression.jl, whichcan be used directly from Julia. It is capable of fusing user-defined operatorsinto SIMD kernels at runtime, performing automatic differentiation, anddistributing populations of expressions to thousands of cores across a cluster.In describing this software, we also introduce a new benchmark,"EmpiricalBench," to quantify the applicability of symbolic regressionalgorithms in science. This benchmark measures recovery of historical empiricalequations from original and synthetic datasets.</description><author>Miles Cranmer</author><pubDate>Fri, 05 May 2023 18:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01582v3</guid></item><item><title>A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text</title><link>http://arxiv.org/abs/2305.02265v2</link><description>Pretrained Vision-Language Models (VLMs) have achieved remarkable performancein image retrieval from text. However, their performance drops drastically whenconfronted with linguistically complex texts that they struggle to comprehend.Inspired by the Divide-and-Conquer algorithm and dual-process theory, in thispaper, we regard linguistically complex texts as compound proposition textscomposed of multiple simple proposition sentences and propose an end-to-endNeural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains threemain components: 1) Divide: a proposition generator divides the compoundproposition text into simple proposition sentences and produces theircorresponding representations, 2) Conquer: a pretrained VLMs-basedvisual-linguistic interactor achieves the interaction between decomposedproposition sentences and images, 3) Combine: a neural-symbolic reasonercombines the above reasoning states to obtain the final solution via a neurallogic reasoning approach. According to the dual-process theory, thevisual-linguistic interactor and neural-symbolic reasoner could be regarded asanalogical reasoning System 1 and logical reasoning System 2. We conductextensive experiments on a challenging image retrieval from contextualdescriptions data set. Experimental results and analyses indicate NDCRsignificantly improves performance in the complex image-text reasoning problem.Code link: https://github.com/YunxinLi/NDCR.</description><author>Yunxin Li, Baotian Hu, Yuxin Ding, Lin Ma, Min Zhang</author><pubDate>Fri, 05 May 2023 18:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02265v2</guid></item><item><title>Fine-Grained Product Classification on Leaflet Advertisements</title><link>http://arxiv.org/abs/2305.03706v1</link><description>In this paper, we describe a first publicly available fine-grained productrecognition dataset based on leaflet images. Using advertisement leaflets,collected over several years from different European retailers, we provide atotal of 41.6k manually annotated product images in 832 classes. Further, weinvestigate three different approaches for this fine-grained productclassification task, Classification by Image, by Text, as well as by Image andText. The approach "Classification by Text" uses the text extracted directlyfrom the leaflet product images. We show, that the combination of image andtext as input improves the classification of visual difficult to distinguishproducts. The final model leads to an accuracy of 96.4% with a Top-3 score of99.2%. We release our code athttps://github.com/ladwigd/Leaflet-Product-Classification.</description><author>Daniel Ladwig, Bianca Lamm, Janis Keuper</author><pubDate>Fri, 05 May 2023 18:38:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03706v1</guid></item><item><title>LMEye: An Interactive Perception Network for Large Language Models</title><link>http://arxiv.org/abs/2305.03701v1</link><description>Training a Large Visual Language Model (LVLM) from scratch, like GPT-4, isresource-intensive. Our paper proposes an alternative method called LMEye, aplay-plug-in Interactive Perception Network for Large Language Models (LLMs),aiming to improve the accuracy of image understanding for the LVLM. Previousmethods that infuse visual information into LLMs utilize a static visualmapping network, but lack dynamic interaction between the LLMs and visualinformation. LMEye addresses this issue by allowing the LLM to incorporate thevisual information that aligned with human instruction. Specifically, the LMEyenetwork consists of a static visual mapping network to provide the basicperception of an image to LLMs. Then, it also contains additional linear layersresponsible for acquiring requests from LLMs, decomposing image features, andtransmitting the interleaved information to LLMs, respectively. In this way,LLMs act to be in charge of understanding human instructions, sending it to theinteractive perception network, and generating the response based on theinterleaved multimodal information. We evaluate LMEye through extensiveexperiments on multimodal question answering and reasoning tasks, demonstratingthat it significantly improves the zero-shot performance of LLMs on multimodaltasks compared to previous methods.</description><author>Yunxin Li, Baotian Hu, Xinyu Chen, Lin Ma, Min Zhang</author><pubDate>Fri, 05 May 2023 18:27:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03701v1</guid></item><item><title>GAN-Based Multi-View Video Coding with Spatio-Temporal EPI Reconstruction</title><link>http://arxiv.org/abs/2205.03599v2</link><description>The introduction of multiple viewpoints in video scenes inevitably increasesthe bitrates required for storage and transmission. To reduce bitrates,researchers have developed methods to skip intermediate viewpoints duringcompression and delivery, and ultimately reconstruct them using SideInformation (SI). Typically, depth maps are used to construct SI. However,their methods suffer from inaccuracies in reconstruction and inherently highbitrates. In this paper, we propose a novel multi-view video coding method thatleverages the image generation capabilities of Generative Adversarial Network(GAN) to improve the reconstruction accuracy of SI. Additionally, we considerincorporating information from adjacent temporal and spatial viewpoints tofurther reduce SI redundancy. At the encoder, we construct a spatio-temporalEpipolar Plane Image (EPI) and further utilize a convolutional network toextract the latent code of a GAN as SI. At the decoder side, we combine the SIand adjacent viewpoints to reconstruct intermediate views using the GANgenerator. Specifically, we establish a joint encoder constraint forreconstruction cost and SI entropy to achieve an optimal trade-off betweenreconstruction quality and bitrates overhead. Experiments demonstratesignificantly improved Rate-Distortion (RD) performance compared withstate-of-the-art methods.</description><author>Chengdong Lan, Hao Yan, Cheng Luo, Tiesong Zhao</author><pubDate>Fri, 05 May 2023 18:19:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.03599v2</guid></item><item><title>Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements</title><link>http://arxiv.org/abs/2305.03695v1</link><description>Despite the much discussed capabilities of today's language models, they arestill prone to silly and unexpected commonsense failures. We consider aretrospective verification approach that reflects on the correctness of LMoutputs, and introduce Vera, a general-purpose model that estimates theplausibility of declarative statements based on commonsense knowledge. Trainedon ~7M commonsense statements created from 19 QA datasets and two large-scaleknowledge bases, and with a combination of three training objectives, Vera is aversatile model that effectively separates correct from incorrect statementsacross diverse commonsense domains. When applied to solving commonsenseproblems in the verification format, Vera substantially outperforms existingmodels that can be repurposed for commonsense verification, and it furtherexhibits generalization capabilities to unseen tasks and provideswell-calibrated outputs. We find that Vera excels at filtering LM-generatedcommonsense knowledge and is useful in detecting erroneous commonsensestatements generated by models like ChatGPT in real-world settings.</description><author>Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi</author><pubDate>Fri, 05 May 2023 18:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03695v1</guid></item><item><title>Mining bias-target Alignment from Voronoi Cells</title><link>http://arxiv.org/abs/2305.03691v1</link><description>Despite significant research efforts, deep neural networks are stillvulnerable to biases: this raises concerns about their fairness and limitstheir generalization. In this paper, we propose a bias-agnostic approach tomitigate the impact of bias in deep neural networks. Unlike traditionaldebiasing approaches, we rely on a metric to quantify ``biasalignment/misalignment'' on target classes, and use this information todiscourage the propagation of bias-target alignment information through thenetwork. We conduct experiments on several commonly used datasets for debiasingand compare our method to supervised and bias-specific approaches. Our resultsindicate that the proposed method achieves comparable performance tostate-of-the-art supervised approaches, although it is bias-agnostic, even inpresence of multiple biases in the same sample.</description><author>R√©mi Nahon, Van-Tam Nguyen, Enzo Tartaglione</author><pubDate>Fri, 05 May 2023 18:09:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03691v1</guid></item><item><title>COLA: How to adapt vision-language models to Compose Objects Localized with Attributes?</title><link>http://arxiv.org/abs/2305.03689v1</link><description>Compositional reasoning is a hallmark of human visual intelligence; yetdespite the size of large vision-language models, they struggle to representsimple compositions by combining objects with their attributes. To measure thislack of compositional capability, we design Cola, a text-to-image retrievalbenchmark to Compose Objects Localized with Attributes. Using Cola as atestbed, we explore modeling designs to adapt pre-trained vision-languagemodels to reason compositionally about multiple attributes attached to multipleobjects. We explore 6 finetuning strategies on 2 seminal vision-languagemodels, using 3 finetuning datasets and 2 test benchmarks (Cola and CREPE).Surprisingly, our optimal finetuning strategy improves a 151M parameter CLIP,which disjointly encodes image and language during pretraining, to perform aswell as a 241M parameter FLAVA, which uses a multi-modal transformer encoderduring pretraining to attend over both vision and language modalities. Thisoptimal finetuning strategy is a lightweight multi-modal adapter that jointlyattends over both image and language features generated by the pretrainedmodel. We show this works better than common strategies such asprompt/fine-tuning, or tuning a comparable number of unimodal layers.</description><author>Arijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan A. Plummer, Ranjay Krishna, Kate Saenko</author><pubDate>Fri, 05 May 2023 18:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03689v1</guid></item><item><title>DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition</title><link>http://arxiv.org/abs/2305.03688v1</link><description>The MultiCoNER \RNum{2} shared task aims to tackle multilingual named entityrecognition (NER) in fine-grained and noisy scenarios, and it inherits thesemantic ambiguity and low-context setting of the MultiCoNER \RNum{1} task. Tocope with these problems, the previous top systems in the MultiCoNER \RNum{1}either incorporate the knowledge bases or gazetteers. However, they stillsuffer from insufficient knowledge, limited context length, single retrievalstrategy. In this paper, our team \textbf{DAMO-NLP} proposes a unifiedretrieval-augmented system (U-RaNER) for fine-grained multilingual NER. Weperform error analysis on the previous top systems and reveal that theirperformance bottleneck lies in insufficient knowledge. Also, we discover thatthe limited context length causes the retrieval knowledge to be invisible tothe model. To enhance the retrieval context, we incorporate the entity-centricWikidata knowledge base, while utilizing the infusion approach to broaden thecontextual scope of the model. Also, we explore various search strategies andrefine the quality of retrieval knowledge. Our system\footnote{We will releasethe dataset, code, and scripts of our system at {\small\url{https://github.com/modelscope/AdaSeq/tree/master/examples/U-RaNER}}.} wins9 out of 13 tracks in the MultiCoNER \RNum{2} shared task. Additionally, wecompared our system with ChatGPT, one of the large language models which haveunlocked strong capabilities on many tasks. The results show that there isstill much room for improvement for ChatGPT on the extraction task.</description><author>Zeqi Tan, Shen Huang, Zixia Jia, Jiong Cai, Yinghui Li, Weiming Lu, Yueting Zhuang, Kewei Tu, Pengjun Xie, Fei Huang, Yong Jiang</author><pubDate>Fri, 05 May 2023 17:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03688v1</guid></item><item><title>On Preimage Approximation for Neural Networks</title><link>http://arxiv.org/abs/2305.03686v1</link><description>Neural network verification mainly focuses on local robustness properties.However, often it is important to know whether a given property holds globallyfor the whole input domain, and if not then for what proportion of the inputthe property is true. While exact preimage generation can construct anequivalent representation of neural networks that can aid such (quantitative)global robustness verification, it is intractable at scale. In this work, wepropose an efficient and practical anytime algorithm for generating symbolicunder-approximations of the preimage of neural networks based on linearrelaxation. Our algorithm iteratively minimizes the volume approximation errorby partitioning the input region into subregions, where the neural networkrelaxation bounds become tighter. We further employ sampling and differentiableapproximations to the volume in order to prioritize regions to split andoptimize the parameters of the relaxation, leading to faster improvement andmore compact under-approximations. Evaluation results demonstrate that ourapproach is able to generate preimage approximations significantly faster thanexact methods and scales to neural network controllers for which exact preimagegeneration is intractable. We also demonstrate an application of our approachto quantitative global verification.</description><author>Xiyue Zhang, Benjie Wang, Marta Kwiatkowska</author><pubDate>Fri, 05 May 2023 17:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03686v1</guid></item><item><title>How Segment Anything Model (SAM) Boost Medical Image Segmentation?</title><link>http://arxiv.org/abs/2305.03678v1</link><description>Due to the flexibility of prompting, foundation models have become thedominant force in the domains of natural language processing and imagegeneration. With the recent introduction of the Segment Anything Model (SAM),the prompt-driven paradigm has entered the realm of image segmentation,bringing with a range of previously unexplored capabilities. However, itremains unclear whether it can be applicable to medical image segmentation dueto the significant differences between natural images and medical images. Inthis report, we summarize recent efforts to extend the success of SAM tomedical image segmentation tasks, including both empirical benchmarking andmethodological adaptations, and discuss potential future directions for SAM inmedical image segmentation. We also set up a collection of literature reviewsto boost the research on this topic at https://github.com/YichiZhang98/SAM4MIS.</description><author>Yichi Zhang, Rushi Jiao</author><pubDate>Fri, 05 May 2023 17:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03678v1</guid></item><item><title>Sparse high-dimensional linear regression with a partitioned empirical Bayes ECM algorithm</title><link>http://arxiv.org/abs/2209.08139v4</link><description>Bayesian variable selection methods are powerful techniques for fitting andinferring on sparse high-dimensional linear regression models. However, manyare computationally intensive or require restrictive prior distributions onmodel parameters. In this paper, we proposed a computationally efficient andpowerful Bayesian approach for sparse high-dimensional linear regression.Minimal prior assumptions on the parameters are used through the use of plug-inempirical Bayes estimates of hyperparameters. Efficient maximum a posteriori(MAP) estimation is completed through a Parameter-ExpandedExpectation-Conditional-Maximization (PX-ECM) algorithm. The PX-ECM results ina robust computationally efficient coordinate-wise optimization, which adjustsfor the impact of other predictor variables. The completion of the E-step usesan approach motivated by the popular two-groups approach to multiple testing.The result is a PaRtitiOned empirical Bayes Ecm (PROBE) algorithm applied tosparse high-dimensional linear regression, which can be completed usingone-at-a-time or all-at-once type optimization. We compare the empiricalproperties of PROBE to comparable approaches with numerous simulation studiesand an analysis of cancer cell lines drug response study. The proposed approachis implemented in the R package probe.</description><author>Alexander C. McLain, Anja Zgodic, Howard Bondell</author><pubDate>Fri, 05 May 2023 17:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08139v4</guid></item><item><title>A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding</title><link>http://arxiv.org/abs/2305.03668v1</link><description>Webpages have been a rich, scalable resource for vision-language and languageonly tasks. Yet only pieces of webpages are kept: image-caption pairs, longtext articles, or raw HTML, never all in one place. Webpage tasks haveresultingly received little attention and structured image-text data leftunderused. To study multimodal webpage understanding, we introduce theWikipedia Webpage suite (WikiWeb2M) of 2M pages. We verify its utility on threegenerative tasks: page description generation, section summarization, andcontextual image captioning. We design a novel attention mechanism PrefixGlobal, which selects the most relevant image and text content as global tokensto attend to the rest of the webpage for context. By using page structure toseparate such tokens, it performs better than full attention with lowercomputational complexity. Experiments show that the new annotations fromWikiWeb2M improve task performance compared to data from prior work. We alsoinclude ablations on sequence length, input features, and model size.</description><author>Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, Mandy Guo</author><pubDate>Fri, 05 May 2023 17:38:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03668v1</guid></item><item><title>Causal Discovery with Stage Variables for Health Time Series</title><link>http://arxiv.org/abs/2305.03662v1</link><description>Using observational data to learn causal relationships is essential whenrandomized experiments are not possible, such as in healthcare. Discoveringcausal relationships in time-series health data is even more challenging whenrelationships change over the course of a disease, such as medications that aremost effective early on or for individuals with severe disease. Stage variablessuch as weeks of pregnancy, disease stages, or biomarkers like HbA1c, caninfluence what causal relationships are true for a patient. However, causalinference within each stage is often not possible due to limited amounts ofdata, and combining all data risks incorrect or missed inferences. To addressthis, we propose Causal Discovery with Stage Variables (CDSV), which uses stagevariables to reweight data from multiple time-series while accounting fordifferent causal relationships in each stage. In simulated data, CDSV discoversmore causes with fewer false discoveries compared to baselines, in eICU it hasa lower FDR than baselines, and in MIMIC-III it discovers more clinicallyrelevant causes of high blood pressure.</description><author>Bharat Srikishan, Samantha Kleinberg</author><pubDate>Fri, 05 May 2023 17:30:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03662v1</guid></item><item><title>Predicting COVID-19 and pneumonia complications from admission texts</title><link>http://arxiv.org/abs/2305.03661v1</link><description>In this paper we present a novel approach to risk assessment for patientshospitalized with pneumonia or COVID-19 based on their admission reports. Weapplied a Longformer neural network to admission reports and other textual dataavailable shortly after admission to compute risk scores for the patients. Weused patient data of multiple European hospitals to demonstrate that ourapproach outperforms the Transformer baselines. Our experiments show that theproposed model generalises across institutions and diagnoses. Also, our methodhas several other advantages described in the paper.</description><author>Dmitriy Umerenkov, Oleg Cherkashin, Alexander Nesterov, Victor Gombolevskiy, Irina Demko, Alexander Yalunin, Vladimir Kokh</author><pubDate>Fri, 05 May 2023 17:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03661v1</guid></item><item><title>Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models</title><link>http://arxiv.org/abs/2305.03660v1</link><description>We propose Retrieval Augmented Generation (RAG) as an approach for automatedradiology report writing that leverages multimodally aligned embeddings from acontrastively pretrained vision language model for retrieval of relevantcandidate radiology text for an input radiology image and a general domaingenerative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 forreport generation using the relevant radiology text retrieved. This approachkeeps hallucinated generations under check and provides capabilities togenerate report content in the format we desire leveraging the instructionfollowing capabilities of these generative models. Our approach achieves betterclinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb scoreof 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for differentclinical settings as it allows to augment the automated radiology reportgeneration process with content relevant for that setting while also having theability to inject user intents and requirements in the prompts as part of thereport generation process to modulate the content and format of the generatedreports as applicable for that clinical setting.</description><author>Mercy Ranjit, Gopinath Ganapathy, Ranjit Manuel, Tanuja Ganu</author><pubDate>Fri, 05 May 2023 17:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03660v1</guid></item><item><title>Combating Online Misinformation Videos: Characterization, Detection, and Future Directions</title><link>http://arxiv.org/abs/2302.03242v2</link><description>With information consumption via online video streaming becoming increasinglypopular, misinformation video poses a new threat to the health of the onlineinformation ecosystem. Though previous studies have made much progress indetecting misinformation in text and image formats, video-based misinformationbrings new and unique challenges to automatic detection systems: 1) highinformation heterogeneity brought by various modalities, 2) blurred distinctionbetween misleading video manipulation and ubiquitous artistic video editing,and 3) new patterns of misinformation propagation due to the dominant role ofrecommendation systems on online video platforms. To facilitate research onthis challenging task, we conduct this survey to present advances inmisinformation video detection research. We first analyze and characterize themisinformation video from three levels including signals, semantics, andintents. Based on the characterization, we systematically review existing worksfor detection from features of various modalities to techniques for clueintegration. We also introduce existing resources including representativedatasets and widely used tools. Besides summarizing existing studies, wediscuss related areas and outline open issues and future directions toencourage and guide more research on misinformation video detection. Ourcorresponding public repository is available athttps://github.com/ICTMCG/Awesome-Misinfo-Video-Detection.</description><author>Yuyan Bu, Qiang Sheng, Juan Cao, Peng Qi, Danding Wang, Jintao Li</author><pubDate>Fri, 05 May 2023 17:26:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03242v2</guid></item><item><title>White-Box Multi-Objective Adversarial Attack on Dialogue Generation</title><link>http://arxiv.org/abs/2305.03655v1</link><description>Pre-trained transformers are popular in state-of-the-art dialogue generation(DG) systems. Such language models are, however, vulnerable to variousadversarial samples as studied in traditional tasks such as textclassification, which inspires our curiosity about their robustness in DGsystems. One main challenge of attacking DG models is that perturbations on thecurrent sentence can hardly degrade the response accuracy because the unchangedchat histories are also considered for decision-making. Instead of merelypursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe thatcrafting adversarial samples to force longer generation outputs benefits attackeffectiveness -- the generated responses are typically irrelevant, lengthy, andrepetitive. To this end, we propose a white-box multi-objective attack methodcalled DGSlow. Specifically, DGSlow balances two objectives -- generationaccuracy and length, via a gradient-based multi-objective optimizer and appliesan adaptive searching mechanism to iteratively craft adversarial samples withonly a few modifications. Comprehensive experiments on four benchmark datasetsdemonstrate that DGSlow could significantly degrade state-of-the-art DG modelswith a higher success rate than traditional accuracy-based methods. Besides,our crafted sentences also exhibit strong transferability in attacking othermodels.</description><author>Yufei Li, Zexin Li, Yingfan Gao, Cong Liu</author><pubDate>Fri, 05 May 2023 17:21:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03655v1</guid></item><item><title>Cuttlefish: Low-Rank Model Training without All the Tuning</title><link>http://arxiv.org/abs/2305.02538v2</link><description>Recent research has shown that training low-rank neural networks caneffectively reduce the total number of trainable parameters without sacrificingpredictive accuracy, resulting in end-to-end speedups. However, low-rank modeltraining necessitates adjusting several additional factorizationhyperparameters, such as the rank of the factorization at each layer. In thispaper, we tackle this challenge by introducing Cuttlefish, an automatedlow-rank training approach that eliminates the need for tuning factorizationhyperparameters. Cuttlefish leverages the observation that after a few epochsof full-rank training, the stable rank (i.e., an approximation of the truerank) of each layer stabilizes at a constant value. Cuttlefish switches fromfull-rank to low-rank training once the stable ranks of all layers haveconverged, setting the dimension of each factorization to its correspondingstable rank. Our results show that Cuttlefish generates models up to 5.6 timessmaller than full-rank models, and attains up to a 1.2 times faster end-to-endtraining process while preserving comparable accuracy. Moreover, Cuttlefishoutperforms state-of-the-art low-rank model training methods and otherprominent baselines. The source code for our implementation can be found at:https://github.com/hwang595/Cuttlefish.</description><author>Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos</author><pubDate>Fri, 05 May 2023 17:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02538v2</guid></item><item><title>On the Effectiveness of Equivariant Regularization for Robust Online Continual Learning</title><link>http://arxiv.org/abs/2305.03648v1</link><description>Humans can learn incrementally, whereas neural networks forget previouslyacquired information catastrophically. Continual Learning (CL) approaches seekto bridge this gap by facilitating the transfer of knowledge to both previoustasks (backward transfer) and future ones (forward transfer) during training. Recent research has shown that self-supervision can produce versatile modelsthat can generalize well to diverse downstream tasks. However, contrastiveself-supervised learning (CSSL), a popular self-supervision technique, haslimited effectiveness in online CL (OCL). OCL only permits one iteration of theinput dataset, and CSSL's low sample efficiency hinders its use on the inputdata-stream. In this work, we propose Continual Learning via Equivariant Regularization(CLER), an OCL approach that leverages equivariant tasks for self-supervision,avoiding CSSL's limitations. Our method represents the first attempt atcombining equivariant knowledge with CL and can be easily integrated withexisting OCL methods. Extensive ablations shed light on how equivariant pretexttasks affect the network's information flow and its impact on CL dynamics.</description><author>Lorenzo Bonicelli, Matteo Boschini, Emanuele Frascaroli, Angelo Porrello, Matteo Pennisi, Giovanni Bellitto, Simone Palazzo, Concetto Spampinato, Simone Calderara</author><pubDate>Fri, 05 May 2023 17:10:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03648v1</guid></item><item><title>Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs</title><link>http://arxiv.org/abs/2305.03642v1</link><description>Results from Randomized Controlled Trials (RCTs) establish the comparativeeffectiveness of interventions, and are in turn critical inputs forevidence-based care. However, results from RCTs are presented in (oftenunstructured) natural language articles describing the design, execution, andoutcomes of trials; clinicians must manually extract findings pertaining tointerventions and outcomes of interest from such articles. This onerous manualprocess has motivated work on (semi-)automating extraction of structuredevidence from trial reports. In this work we propose and evaluate atext-to-text model built on instruction-tuned Large Language Models (LLMs) tojointly extract Interventions, Outcomes, and Comparators (ICO elements) fromclinical abstracts, and infer the associated results reported. Manual (expert)and automated evaluations indicate that framing evidence extraction as aconditional generation task and fine-tuning LLMs for this purpose realizesconsiderable ($\sim$20 point absolute F1 score) gains over the previous SOTA.We perform ablations and error analyses to assess aspects that contribute tomodel performance, and to highlight potential directions for furtherimprovements. We apply our model to a collection of published RCTs throughmid-2022, and release a searchable database of structured findings (anonymouslyfor now): bit.ly/joint-relations-extraction-mlhc</description><author>Somin Wadhwa, Jay DeYoung, Benjamin Nye, Silvio Amir, Byron C. Wallace</author><pubDate>Fri, 05 May 2023 17:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03642v1</guid></item><item><title>Predicting air quality via multimodal AI and satellite imagery</title><link>http://arxiv.org/abs/2211.00780v2</link><description>Climate change may be classified as the most important environmental problemthat the Earth is currently facing, and affects all living species on Earth.Given that air-quality monitoring stations are typically ground-based theirabilities to detect pollutant distributions are often restricted to wide areas.Satellites however have the potential for studying the atmosphere at large; theEuropean Space Agency (ESA) Copernicus project satellite, "Sentinel-5P" is anewly launched satellite capable of measuring a variety of pollutantinformation with publicly available data outputs. This paper seeks to create amulti-modal machine learning model for predicting air-quality metrics wheremonitoring stations do not exist. The inputs of this model will include afusion of ground measurements and satellite data with the goal of highlightingpollutant distribution and motivating change in societal and industrialbehaviors. A new dataset of European pollution monitoring station measurementsis created with features including $\textit{altitude, population, etc.}$ fromthe ESA Copernicus project. This dataset is used to train a multi-modal MLmodel, Air Quality Network (AQNet) capable of fusing these various types ofdata sources to output predictions of various pollutants. These predictions arethen aggregated to create an "air-quality index" that could be used to compareair quality over different regions. Three pollutants, NO$_2$, O$_3$, andPM$_{10}$, are predicted successfully by AQNet and the network was found to beuseful compared to a model only using satellite imagery. It was also found thatthe addition of supporting data improves predictions. When testing thedeveloped AQNet on out-of-sample data of the UK and Ireland, we obtainsatisfactory estimates though on average pollution metrics were roughlyoverestimated by around 20\%.</description><author>Andrew Rowley, Oktay Karaku≈ü</author><pubDate>Fri, 05 May 2023 17:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.00780v2</guid></item><item><title>Asynchronous Events-based Panoptic Segmentation using Graph Mixer Neural Network</title><link>http://arxiv.org/abs/2305.03640v1</link><description>In the context of robotic grasping, object segmentation encounters severaldifficulties when faced with dynamic conditions such as real-time operation,occlusion, low lighting, motion blur, and object size variability. In responseto these challenges, we propose the Graph Mixer Neural Network that includes anovel collaborative contextual mixing layer, applied to 3D event graphs formedon asynchronous events. The proposed layer is designed to spread spatiotemporalcorrelation within an event graph at four nearest neighbor levels parallelly.We evaluate the effectiveness of our proposed method on the Event-basedSegmentation (ESD) Dataset, which includes five unique image degradationchallenges, including occlusion, blur, brightness, trajectory, scale variance,and segmentation of known and unknown objects. The results show that ourproposed approach outperforms state-of-the-art methods in terms of meanintersection over the union and pixel accuracy. Code available at:https://github.com/sanket0707/GNN-Mixer.git</description><author>Sanket Kachole, Yusra Alkendi, Fariborz Baghaei Naeini, Dimitrios Makris, Yahya Zweiri</author><pubDate>Fri, 05 May 2023 16:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03640v1</guid></item><item><title>Posterior Regularization on Bayesian Hierarchical Mixture Clustering</title><link>http://arxiv.org/abs/2105.06903v7</link><description>Bayesian hierarchical mixture clustering (BHMC) improves traditionalBayesianhierarchical clustering by replacing conventional Gaussian-to-Gaussian kernelswith a Hierarchical Dirichlet Process Mixture Model(HDPMM) for parent-to-childdiffusion in the generative process. However,BHMC may produce trees with highnodal variance, indicating weak separation between nodes at higher levels. Toaddress this issue, we employ Posterior Regularization, which imposesmax-margin constraints on nodes at every level to enhance cluster separation.We illustrate how to apply PR toBHMC and demonstrate its effectiveness inimproving the BHMC model.</description><author>Weipeng Huang, Tin Lok James Ng, Nishma Laitonjam, Neil J. Hurley</author><pubDate>Fri, 05 May 2023 16:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2105.06903v7</guid></item><item><title>Improving LaCAM for Scalable Eventually Optimal Multi-Agent Pathfinding</title><link>http://arxiv.org/abs/2305.03632v1</link><description>This study extends the recently-developed LaCAM algorithm for multi-agentpathfinding (MAPF). LaCAM is a sub-optimal search-based algorithm that useslazy successor generation to dramatically reduce the planning effort. Wepresent two enhancements. First, we propose its anytime version, called LaCAM*,which eventually converges to optima, provided that solution costs areaccumulated transition costs. Second, we improve the successor generation toquickly obtain initial solutions. Exhaustive experiments demonstrate theirutility. For instance, LaCAM* sub-optimally solved 99% of the instancesretrieved from the MAPF benchmark, where the number of agents varied up to athousand, within ten seconds on a standard desktop PC, while ensuring eventualconvergence to optima; developing a new horizon of MAPF algorithms.</description><author>Keisuke Okumura</author><pubDate>Fri, 05 May 2023 16:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03632v1</guid></item><item><title>Multi-scale Sinusoidal Embeddings Enable Learning on High Resolution Mass Spectrometry Data</title><link>http://arxiv.org/abs/2207.02980v2</link><description>Small molecules in biological samples are studied to provide informationabout disease states, environmental toxins, natural product drug discovery, andmany other applications. The primary window into the composition of smallmolecule mixtures is tandem mass spectrometry (MS2), which produces data thatare of high sensitivity and part per million resolution. We adopt multi-scalesinusoidal embeddings of the mass data in MS2 designed to meet the challenge oflearning from the full resolution of MS2 data. Using these embeddings, weprovide a new state of the art model for spectral library search, the standardtask for initial evaluation of MS2 data. We also introduce a new task, chemicalproperty prediction from MS2 data, that has natural applications inhigh-throughput MS2 experiments and show that an average $R^2$ of 80\% fornovel compounds can be achieved across 10 chemical properties prioritized bymedicinal chemists. We use dimensionality reduction techniques and experimentswith different floating point resolutions to show the essential rolemulti-scale sinusoidal embeddings play in learning from MS2 data.</description><author>Gennady Voronov, Rose Lightheart, Joe Davison, Christoph A. Krettler, David Healey, Thomas Butler</author><pubDate>Fri, 05 May 2023 16:43:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.02980v2</guid></item><item><title>Verifiable Learning for Robust Tree Ensembles</title><link>http://arxiv.org/abs/2305.03626v1</link><description>Verifying the robustness of machine learning models against evasion attacksat test time is an important research problem. Unfortunately, prior workestablished that this problem is NP-hard for decision tree ensembles, hencebound to be intractable for specific inputs. In this paper, we identify arestricted class of decision tree ensembles, called large-spread ensembles,which admit a security verification algorithm running in polynomial time. Wethen propose a new approach called verifiable learning, which advocates thetraining of such restricted model classes which are amenable for efficientverification. We show the benefits of this idea by designing a new trainingalgorithm that automatically learns a large-spread decision tree ensemble fromlabelled data, thus enabling its security verification in polynomial time.Experimental results on publicly available datasets confirm that large-spreadensembles trained using our algorithm can be verified in a matter of seconds,using standard commercial hardware. Moreover, large-spread ensembles are morerobust than traditional ensembles against evasion attacks, while incurring injust a relatively small loss of accuracy in the non-adversarial setting.</description><author>Stefano Calzavara, Lorenzo Cazzaro, Giulio Ermanno Pibiri, Nicola Prezza</author><pubDate>Fri, 05 May 2023 16:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03626v1</guid></item><item><title>Unsupervised Domain Transfer for Science: Exploring Deep Learning Methods for Translation between LArTPC Detector Simulations with Differing Response Models</title><link>http://arxiv.org/abs/2304.12858v2</link><description>Deep learning (DL) techniques have broad applications in science, especiallyin seeking to streamline the pathway to potential solutions and discoveries.Frequently, however, DL models are trained on the results of simulation yetapplied to real experimental data. As such, any systematic differences betweenthe simulated and real data may degrade the model's performance -- an effectknown as "domain shift." This work studies a toy model of the systematicdifferences between simulated and real data. It presents a fully unsupervised,task-agnostic method to reduce differences between two systematically differentsamples. The method is based on the recent advances in unpaired image-to-imagetranslation techniques and is validated on two sets of samples of simulatedLiquid Argon Time Projection Chamber (LArTPC) detector events, created toillustrate common systematic differences between the simulated and real data ina controlled way. LArTPC-based detectors represent the next-generation particledetectors, producing unique high-resolution particle track data. This workopen-sources the generated LArTPC data set, called Simple Liquid-Argon TrackSamples (or SLATS), allowing researchers from diverse domains to study theLArTPC-like data for the first time. The code and trained models are availableat https://github.com/LS4GAN/uvcgan4slats.</description><author>Yi Huang, Dmitrii Torbunov, Brett Viren, Haiwang Yu, Jin Huang, Meifeng Lin, Yihui Ren</author><pubDate>Fri, 05 May 2023 16:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12858v2</guid></item><item><title>Optimizing Hyperparameters with Conformal Quantile Regression</title><link>http://arxiv.org/abs/2305.03623v1</link><description>Many state-of-the-art hyperparameter optimization (HPO) algorithms rely onmodel-based optimizers that learn surrogate models of the target function toguide the search. Gaussian processes are the de facto surrogate model due totheir ability to capture uncertainty but they make strong assumptions about theobservation noise, which might not be warranted in practice. In this work, wepropose to leverage conformalized quantile regression which makes minimalassumptions about the observation noise and, as a result, models the targetfunction in a more realistic and robust fashion which translates to quicker HPOconvergence on empirical benchmarks. To apply our method in a multi-fidelitysetting, we propose a simple, yet effective, technique that aggregates observedresults across different resource levels and outperforms conventional methodsacross many empirical tasks.</description><author>David Salinas, Jacek Golebiowski, Aaron Klein, Matthias Seeger, Cedric Archambeau</author><pubDate>Fri, 05 May 2023 16:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03623v1</guid></item><item><title>Segmentation of fundus vascular images based on a dual-attention mechanism</title><link>http://arxiv.org/abs/2305.03617v1</link><description>Accurately segmenting blood vessels in retinal fundus images is crucial inthe early screening, diagnosing, and evaluating some ocular diseases. However,significant light variations and non-uniform contrast in these images makesegmentation quite challenging. Thus, this paper employ an attention fusionmechanism that combines the channel attention and spatial attention mechanismsconstructed by Transformer to extract information from retinal fundus images inboth spatial and channel dimensions. To eliminate noise from the encoder image,a spatial attention mechanism is introduced in the skip connection. Moreover, aDropout layer is employed to randomly discard some neurons, which can preventoverfitting of the neural network and improve its generalization performance.Experiments were conducted on publicly available datasets DERIVE, STARE, andCHASEDB1. The results demonstrate that our method produces satisfactory resultscompared to some recent retinal fundus image segmentation algorithms.</description><author>Yuanyuan Peng, Pengpeng Luan, Zixu Zhang</author><pubDate>Fri, 05 May 2023 16:22:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03617v1</guid></item><item><title>Conditional Diffusion Feature Refinement for Continuous Sign Language Recognition</title><link>http://arxiv.org/abs/2305.03614v1</link><description>In this work, we are dedicated to leveraging the denoising diffusion models'success and formulating feature refinement as the autoencoder-formed diffusionprocess. The state-of-the-art CSLR framework consists of a spatial module, avisual module, a sequence module, and a sequence learning function. However,this framework has faced sequence module overfitting caused by the objectivefunction and small-scale available benchmarks, resulting in insufficient modeltraining. To overcome the overfitting problem, some CSLR studies enforce thesequence module to learn more visual temporal information or be guided by moreinformative supervision to refine its representations. In this work, we proposea novel autoencoder-formed conditional diffusion feature refinement~(ACDR) torefine the sequence representations to equip desired properties by learning theencoding-decoding optimization process in an end-to-end way. Specifically, forthe ACDR, a noising Encoder is proposed to progressively add noise equippedwith semantic conditions to the sequence representations. And a denoisingDecoder is proposed to progressively denoise the noisy sequence representationswith semantic conditions. Therefore, the sequence representations can be imbuedwith the semantics of provided semantic conditions. Further, a semanticconstraint is employed to prevent the denoised sequence representations fromsemantic corruption. Extensive experiments are conducted to validate theeffectiveness of our ACDR, benefiting state-of-the-art methods and achieving anotable gain on three benchmarks.</description><author>Leming Guo, Wanli Xue, Qing Guo, Yuxi Zhou, Tiantian Yuan, Shengyong Chen</author><pubDate>Fri, 05 May 2023 16:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03614v1</guid></item><item><title>Initial Steps Towards Tackling High-dimensional Surrogate Modeling for Neuroevolution Using Kriging Partial Least Squares</title><link>http://arxiv.org/abs/2305.03612v1</link><description>Surrogate-assisted evolutionary algorithms (SAEAs) aim to use efficientcomputational models with the goal of approximating the fitness function inevolutionary computation systems. This area of research has been active forover two decades and has received significant attention from the specialisedresearch community in different areas, for example, single and many objectiveoptimisation or dynamic and stationary optimisation problems. An emergent andexciting area that has received little attention from the SAEAs community is inneuroevolution. This refers to the use of evolutionary algorithms in theautomatic configuration of artificial neural network (ANN) architectures,hyper-parameters and/or the training of ANNs. However, ANNs suffer from twomajor issues: (a) the use of highly-intense computational power for theircorrect training, and (b) the highly specialised human expertise required tocorrectly configure ANNs necessary to get a well-performing network. This workaims to fill this important research gap in SAEAs in neuroevolution byaddressing these two issues. We demonstrate how one can use a Kriging PartialLeast Squares method that allows efficient computation of good approximatesurrogate models compared to the well-known Kriging method, which normallycannot be used in neuroevolution due to the high dimensionality of the data.</description><author>Fergal Stapleton, Edgar Galv√°n</author><pubDate>Fri, 05 May 2023 16:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03612v1</guid></item><item><title>Data Curation for Image Captioning with Text-to-Image Generative Models</title><link>http://arxiv.org/abs/2305.03610v1</link><description>Recent advances in image captioning are mainly driven by large-scalevision-language pretraining, relying heavily on computational resources andincreasingly large multimodal datasets. Instead of scaling up pretraining data,we ask whether it is possible to improve performance by improving the qualityof the samples in existing datasets. We pursue this question through twoapproaches to data curation: one that assumes that some examples should beavoided due to mismatches between the image and caption, and one that assumesthat the mismatch can be addressed by replacing the image, for which we use thestate-of-the-art Stable Diffusion model. These approaches are evaluated usingthe BLIP model on MS COCO and Flickr30K in both finetuning and few-shotlearning settings. Our simple yet effective approaches consistently outperformbaselines, indicating that better image captioning models can be trained bycurating existing resources. Finally, we conduct a human study to understandthe errors made by the Stable Diffusion model and highlight directions forfuture work in text-to-image generation.</description><author>Wenyan Li, Jonas F. Lotz, Chen Qiu, Desmond Elliott</author><pubDate>Fri, 05 May 2023 16:16:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03610v1</guid></item><item><title>Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications</title><link>http://arxiv.org/abs/2302.05763v2</link><description>Human-robot interaction (HRI) research is progressively addressingmulti-party scenarios, where a robot interacts with more than one human user atthe same time. Conversely, research is still at an early stage for human-robotcollaboration. The use of machine learning techniques to handle such type ofcollaboration requires data that are less feasible to produce than in a typicalHRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRCapplications. Based upon these concepts, this study also proposes analternative way of gathering data regarding multi-user activity, by collectingdata related to single users and merging them in post-processing, to reduce theeffort involved in producing recordings of pair settings. To validate thisstatement, 3D skeleton poses of activity of single users were collected andmerged in pairs. After this, such datapoints were used to separately train along short-term memory (LSTM) network and a variational autoencoder (VAE)composed of spatio-temporal graph convolutional networks (STGCN) to recognisethe joint activities of the pairs of people. The results showed that it ispossible to make use of data collected in this way for pair HRC settings andget similar performances compared to using training data regarding groups ofusers recorded under the same settings, relieving from the technicaldifficulties involved in producing these data. The related code and collected data are publicly available.</description><author>Francesco Semeraro, Jon Carberry, Angelo Cangelosi</author><pubDate>Fri, 05 May 2023 16:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05763v2</guid></item><item><title>Differentially Private Topological Data Analysis</title><link>http://arxiv.org/abs/2305.03609v1</link><description>This paper is the first to attempt differentially private (DP) topologicaldata analysis (TDA), producing near-optimal private persistence diagrams. Weanalyze the sensitivity of persistence diagrams in terms of the bottleneckdistance, and we show that the commonly used \v{C}ech complex has sensitivitythat does not decrease as the sample size $n$ increases. This makes itchallenging for the persistence diagrams of \v{C}ech complexes to beprivatized. As an alternative, we show that the persistence diagram obtained bythe $L^1$-distance to measure (DTM) has sensitivity $O(1/n)$. Based on thesensitivity analysis, we propose using the exponential mechanism whose utilityfunction is defined in terms of the bottleneck distance of the $L^1$-DTMpersistence diagrams. We also derive upper and lower bounds of the accuracy ofour privacy mechanism; the obtained bounds indicate that the privacy error ofour mechanism is near-optimal. We demonstrate the performance of our privatizedpersistence diagrams through simulations as well as on a real dataset trackinghuman movement.</description><author>Taegyu Kang, Sehwan Kim, Jinwon Sohn, Jordan Awan</author><pubDate>Fri, 05 May 2023 16:15:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03609v1</guid></item><item><title>On the Optimality, Stability, and Feasibility of Control Barrier Functions: An Adaptive Learning-Based Approach</title><link>http://arxiv.org/abs/2305.03608v1</link><description>Safety has been a critical issue for the deployment of learning-basedapproaches in real-world applications. To address this issue, control barrierfunction (CBF) and its variants have attracted extensive attention forsafety-critical control. However, due to the myopic one-step nature of CBF andthe lack of principled methods to design the class-$\mathcal{K}$ functions,there are still fundamental limitations of current CBFs: optimality, stability,and feasibility. In this paper, we proposed a novel and unified approach toaddress these limitations with Adaptive Multi-step Control Barrier Function(AM-CBF), where we parameterize the class-$\mathcal{K}$ function by a neuralnetwork and train it together with the reinforcement learning policy. Moreover,to mitigate the myopic nature, we propose a novel \textit{multi-step trainingand single-step execution} paradigm to make CBF farsighted while the executionremains solving a single-step convex quadratic program. Our method is evaluatedon the first and second-order systems in various scenarios, where our approachoutperforms the conventional CBF both qualitatively and quantitatively.</description><author>Alaa Eddine Chriat, Chuangchuang Sun</author><pubDate>Fri, 05 May 2023 16:11:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03608v1</guid></item><item><title>A Dual Semantic-Aware Recurrent Global-Adaptive Network For Vision-and-Language Navigation</title><link>http://arxiv.org/abs/2305.03602v1</link><description>Vision-and-Language Navigation (VLN) is a realistic but challenging task thatrequires an agent to locate the target region using verbal and visual cues.While significant advancements have been achieved recently, there are still twobroad limitations: (1) The explicit information mining for significant guidingsemantics concealed in both vision and language is still under-explored; (2)The previously structured map method provides the average historical appearanceof visited nodes, while it ignores distinctive contributions of various imagesand potent information retention in the reasoning process. This work proposes adual semantic-aware recurrent global-adaptive network (DSRG) to address theabove problems. First, DSRG proposes an instruction-guidance linguistic module(IGL) and an appearance-semantics visual module (ASV) for boosting vision andlanguage semantic learning respectively. For the memory mechanism, a globaladaptive aggregation module (GAA) is devised for explicit panoramic observationfusion, and a recurrent memory fusion module (RMF) is introduced to supplyimplicit temporal hidden states. Extensive experimental results on the R2R andREVERIE datasets demonstrate that our method achieves better performance thanexisting methods.</description><author>Liuyi Wang, Zongtao He, Jiagui Tang, Ronghao Dang, Naijia Wang, Chengju Liu, Qijun Chen</author><pubDate>Fri, 05 May 2023 16:06:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03602v1</guid></item><item><title>PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding</title><link>http://arxiv.org/abs/2111.08792v2</link><description>We present PredProp, a method for optimization of weights and states inpredictive coding networks (PCNs) based on the precision of propagated errorsand neural activity. PredProp jointly addresses inference and learning viastochastic gradient descent and adaptively weights parameter updates byapproximate curvature. Due to the relation between propagated error covarianceand the Fisher information matrix, PredProp implements approximate NaturalGradient Descent. We demonstrate PredProp's effectiveness in the context ofdense decoder networks and simple image benchmark datasets. We found thatPredProp performs favorably over Adam, a widely used adaptive learning rateoptimizer in the tested configurations. Furthermore, available optimizationmethods for weight parameters benefit from using PredProp's error precisionduring inference. Since hierarchical predictive coding layers are optimisedindividually using local errors, the required precisions factorize overhierarchical layers. Extending beyond classical PCNs with a single set ofdecoder layers per hierarchical layer, we also generalize PredProp to deepneural networks in each PCN layer by additionally factorizing over the weightsin each PCN layer.</description><author>Andr√© Ofner, Sebastian Stober</author><pubDate>Fri, 05 May 2023 16:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.08792v2</guid></item><item><title>Human Attention-Guided Explainable Artificial Intelligence for Computer Vision Models</title><link>http://arxiv.org/abs/2305.03601v1</link><description>We examined whether embedding human attention knowledge into saliency-basedexplainable AI (XAI) methods for computer vision models could enhance theirplausibility and faithfulness. We first developed new gradient-based XAImethods for object detection models to generate object-specific explanations byextending the current methods for image classification models. Interestingly,while these gradient-based methods worked well for explaining imageclassification models, when being used for explaining object detection models,the resulting saliency maps generally had lower faithfulness than humanattention maps when performing the same task. We then developed HumanAttention-Guided XAI (HAG-XAI) to learn from human attention how to bestcombine explanatory information from the models to enhance explanationplausibility by using trainable activation functions and smoothing kernels tomaximize XAI saliency map's similarity to human attention maps. While for imageclassification models, HAG-XAI enhanced explanation plausibility at the expenseof faithfulness, for object detection models it enhanced plausibility andfaithfulness simultaneously and outperformed existing methods. The learnedfunctions were model-specific, well generalizable to other databases.</description><author>Guoyang Liu, Jindi Zhang, Antoni B. Chan, Janet H. Hsiao</author><pubDate>Fri, 05 May 2023 16:05:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03601v1</guid></item><item><title>NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports</title><link>http://arxiv.org/abs/2305.03598v1</link><description>How can we interpret and retrieve medical evidence to support clinicaldecisions? Clinical trial reports (CTR) amassed over the years containindispensable information for the development of personalized medicine.However, it is practically infeasible to manually inspect over 400,000+clinical trial reports in order to find the best evidence for experimentaltreatments. Natural Language Inference (NLI) offers a potential solution tothis problem, by allowing the scalable computation of textual entailment.However, existing NLI models perform poorly on biomedical corpora, andpreviously published datasets fail to capture the full complexity of inferenceover CTRs. In this work, we present a novel resource to advance research on NLIfor reasoning on CTRs. The resource includes two main tasks. Firstly, todetermine the inference relation between a natural language statement, and aCTR. Secondly, to retrieve supporting facts to justify the predicted relation.We provide NLI4CT, a corpus of 2400 statements and CTRs, annotated for thesetasks. Baselines on this corpus expose the limitations of existing NLI models,with 6 state-of-the-art NLI models achieving a maximum F1 score of 0.627. Tothe best of our knowledge, we are the first to design a task that covers theinterpretation of full CTRs. To encourage further work on this challengingdataset, we make the corpus, competition leaderboard, website and code toreplicate the baseline experiments available at:https://github.com/ai-systems/nli4ct</description><author>Ma√´l Jullien, Marco Valentino, Hannah Frost, Paul O'Regan, Donal Landers, Andr√© Freitas</author><pubDate>Fri, 05 May 2023 16:03:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03598v1</guid></item><item><title>Learning Discriminative Representations and Decision Boundaries for Open Intent Detection</title><link>http://arxiv.org/abs/2203.05823v3</link><description>Open intent detection is a significant problem in natural languageunderstanding, which aims to identify the unseen open intent while ensuringknown intent identification performance. However, current methods face twomajor challenges. Firstly, they struggle to learn friendly representations todetect the open intent with prior knowledge of only known intents. Secondly,there is a lack of an effective approach to obtaining specific and compactdecision boundaries for known intents. To address these issues, this paperpresents an original framework called DA-ADB, which successively learnsdistance-aware intent representations and adaptive decision boundaries for openintent detection. Specifically, we first leverage distance information toenhance the distinguishing capability of the intent representations. Then, wedesign a novel loss function to obtain appropriate decision boundaries bybalancing both empirical and open space risks. Extensive experimentsdemonstrate the effectiveness of the proposed distance-aware and boundarylearning strategies. Compared to state-of-the-art methods, our frameworkachieves substantial improvements on three benchmark datasets. Furthermore, ityields robust performance with varying proportions of labeled data and knowncategories.</description><author>Hanlei Zhang, Hua Xu, Shaojie Zhao, Qianrui Zhou</author><pubDate>Fri, 05 May 2023 16:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.05823v3</guid></item><item><title>HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer</title><link>http://arxiv.org/abs/2305.03595v1</link><description>Visual localization is critical to many applications in computer vision androbotics. To address single-image RGB localization, state-of-the-artfeature-based methods match local descriptors between a query image and apre-built 3D model. Recently, deep neural networks have been exploited toregress the mapping between raw pixels and 3D coordinates in the scene, andthus the matching is implicitly performed by the forward pass through thenetwork. However, in a large and ambiguous environment, learning such aregression task directly can be difficult for a single network. In this work,we present a new hierarchical scene coordinate network to predict pixel scenecoordinates in a coarse-to-fine manner from a single RGB image. The proposedmethod, which is an extension of HSCNet, allows us to train compact modelswhich scale robustly to large environments. It sets a new state-of-the-art forsingle-image localization on the 7-Scenes, 12 Scenes, Cambridge Landmarksdatasets, and the combined indoor scenes.</description><author>Shuzhe Wang, Zakaria Laskar, Iaroslav Melekhov, Xiaotian Li, Yi Zhao, Giorgos Tolias, Juho Kannala</author><pubDate>Fri, 05 May 2023 16:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03595v1</guid></item><item><title>Finding Outliers in Gaussian Model-Based Clustering</title><link>http://arxiv.org/abs/1907.01136v4</link><description>Unsupervised classification, or clustering, is a problem often plagued byoutliers, yet there is a paucity of work on handling outliers in unsupervisedclassification. Outlier algorithms tend to fall into two broad categories:outlier inclusion methods and trimming methods, which often requirepre-specification of the number of points to remove. The fact that sampleMahalanobis distance is beta-distributed is used to derive an approximatedistribution for the log-likelihoods of subset finite Gaussian mixture models.An algorithm is proposed that removes the least likely points, which are deemedoutliers, until the log-likelihoods adhere to the reference distribution. Thisresults in a trimming method which inherently estimates the number of outlierspresent.</description><author>Katharine M. Clark, Paul D. McNicholas</author><pubDate>Fri, 05 May 2023 15:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1907.01136v4</guid></item><item><title>Now It Sounds Like You: Learning Personalized Vocabulary On Device</title><link>http://arxiv.org/abs/2305.03584v1</link><description>In recent years, Federated Learning (FL) has shown significant advancementsin its ability to perform various natural language processing (NLP) tasks. Thiswork focuses on applying personalized FL for on-device language modeling. Dueto limitations of memory and latency, these models cannot support thecomplexity of sub-word tokenization or beam search decoding, resulting in thedecision to deploy a closed-vocabulary language model. However,closed-vocabulary models are unable to handle out-of-vocabulary (OOV) wordsbelonging to specific users. To address this issue, We propose a noveltechnique called "OOV expansion" that improves OOV coverage and increases modelaccuracy while minimizing the impact on memory and latency. This methodintroduces a personalized "OOV adapter" that effectively transfers knowledgefrom a central model and learns word embedding for personalized vocabulary. OOVexpansion significantly outperforms standard FL personalization methods on aset of common FL benchmarks.</description><author>Sid Wang, Ashish Shenoy, Pierce Chuang, John Nguyen</author><pubDate>Fri, 05 May 2023 15:44:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03584v1</guid></item><item><title>A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning</title><link>http://arxiv.org/abs/2305.03582v1</link><description>In this paper, we present a multimodal \textit{and} dynamical VAE (MDVAE)applied to unsupervised audio-visual speech representation learning. The latentspace is structured to dissociate the latent dynamical factors that are sharedbetween the modalities from those that are specific to each modality. A staticlatent variable is also introduced to encode the information that is constantover time within an audiovisual speech sequence. The model is trained in anunsupervised manner on an audiovisual emotional speech dataset, in two stages.In the first stage, a vector quantized VAE (VQ-VAE) is learned independentlyfor each modality, without temporal modeling. The second stage consists inlearning the MDVAE model on the intermediate representation of the VQ-VAEsbefore quantization. The disentanglement between static versus dynamical andmodality-specific versus modality-common information occurs during this secondtraining stage. Extensive experiments are conducted to investigate howaudiovisual speech latent factors are encoded in the latent space of MDVAE.These experiments include manipulating audiovisual speech, audiovisual facialimage denoising, and audiovisual speech emotion recognition. The results showthat MDVAE effectively combines the audio and visual information in its latentspace. They also show that the learned static representation of audiovisualspeech can be used for emotion recognition with few labeled data, and withbetter accuracy compared with unimodal baselines and a state-of-the-artsupervised model based on an audiovisual transformer architecture.</description><author>Samir Sadok, Simon Leglaive, Laurent Girin, Xavier Alameda-Pineda, Renaud S√©guier</author><pubDate>Fri, 05 May 2023 15:37:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03582v1</guid></item><item><title>Progressive-Hint Prompting Improves Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2304.09797v2</link><description>The performance of Large Language Models (LLMs) in reasoning tasks dependsheavily on prompt design, with Chain-of-Thought (CoT) and self-consistencybeing critical methods that enhance this ability. However, these methods do notfully exploit the answers generated by the LLM to guide subsequent responses.This paper proposes a new prompting method, named Progressive-Hint Prompting(PHP), that enables automatic multiple interactions between users and LLMs byusing previously generated answers as hints to progressively guide toward thecorrect answers. PHP is orthogonal to CoT and self-consistency, making it easyto combine with state-of-the-art techniques to further improve performance. Weconducted an extensive and comprehensive evaluation to demonstrate theeffectiveness of the proposed method. Our experimental results on sixbenchmarks show that combining CoT and self-consistency with PHP significantlyimproves accuracy while remaining highly efficient. For instance, withtext-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decodingcompared to Complex CoT, and a 46.17% reduction in sample paths withself-consistency. With GPT-4 and PHP, we achieve state-of-the-art performanceson SVAMP (89.1% -&gt; 91.9%), GSM8K (92% -&gt; 95.5%), AQuA (76.4% -&gt; 79.9%) and MATH(50.2% -&gt; 53.9%).</description><author>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li</author><pubDate>Fri, 05 May 2023 15:33:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09797v2</guid></item><item><title>Scope Restriction for Scalable Real-Time Railway Rescheduling: An Exploratory Study</title><link>http://arxiv.org/abs/2305.03574v1</link><description>With the aim to stimulate future research, we describe an exploratory studyof a railway rescheduling problem. A widely used approach in practice and stateof the art is to decompose these complex problems by geographical scope.Instead, we propose defining a core problem that restricts a reschedulingproblem in response to a disturbance to only trains that need to berescheduled, hence restricting the scope in both time and space. In thiscontext, the difficulty resides in defining a scoper that can predict a subsetof train services that will be affected by a given disturbance. We reportpreliminary results using the Flatland simulation environment that highlightsthe potential and challenges of this idea. We provide an extensible playgroundopen-source implementation based on the Flatland railway environment andAnswer-Set Programming.</description><author>Erik Nygren, Christian Eichenberger, Emma Frejinger</author><pubDate>Fri, 05 May 2023 15:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03574v1</guid></item><item><title>In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models</title><link>http://arxiv.org/abs/2305.03573v1</link><description>The phenomena of in-context learning has typically been thought of as"learning from examples". In this work which focuses on Machine Translation, wepresent a perspective of in-context learning as the desired generation taskmaintaining coherency with its context, i.e., the prompt examples. We firstinvestigate randomly sampled prompts across 4 domains, and find thattranslation performance improves when shown in-domain prompts. Next, weinvestigate coherency for the in-domain setting, which uses prompt examplesfrom a moving window. We study this with respect to other factors that havepreviously been identified in the literature such as length, surface similarityand sentence embedding similarity. Our results across 3 models (GPTNeo2.7B,Bloom3B, XGLM2.9B), and three translation directions(\texttt{en}$\rightarrow$\{\texttt{pt, de, fr}\}) suggest that the long-termcoherency of the prompts and the test sentence is a good indicator ofdownstream translation performance. In doing so, we demonstrate the efficacy ofIn-context Machine Translation for on-the-fly adaptation.</description><author>Suzanna Sia, Kevin Duh</author><pubDate>Fri, 05 May 2023 15:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03573v1</guid></item><item><title>Learn how to Prune Pixels for Multi-view Neural Image-based Synthesis</title><link>http://arxiv.org/abs/2305.03572v1</link><description>Image-based rendering techniques stand at the core of an immersive experiencefor the user, as they generate novel views given a set of multiple inputimages. Since they have shown good performance in terms of objective andsubjective quality, the research community devotes great effort to theirimprovement. However, the large volume of data necessary to render at thereceiver's side hinders applications in limited bandwidth environments orprevents their employment in real-time applications. We present LeHoPP, amethod for input pixel pruning, where we examine the importance of each inputpixel concerning the rendered view, and we avoid the use of irrelevant pixels.Even without retraining the image-based rendering network, our approach shows agood trade-off between synthesis quality and pixel rate. When tested in thegeneral neural rendering framework, compared to other pruning baselines, LeHoPPgains between $0.9$ dB and $3.6$ dB on average.</description><author>Marta Milovanoviƒá, Enzo Tartaglione, Marco Cagnazzo, F√©lix Henry</author><pubDate>Fri, 05 May 2023 15:29:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03572v1</guid></item><item><title>Model-free Reinforcement Learning of Semantic Communication by Stochastic Policy Gradient</title><link>http://arxiv.org/abs/2305.03571v1</link><description>Motivated by the recent success of Machine Learning tools in wirelesscommunications, the idea of semantic communication by Weaver from 1949 hasgained attention. It breaks with Shannon's classic design paradigm by aiming totransmit the meaning, i.e., semantics, of a message instead of its exactversion, allowing for information rate savings. In this work, we apply theStochastic Policy Gradient (SPG) to design a semantic communication system byreinforcement learning, not requiring a known or differentiable channel model -a crucial step towards deployment in practice. Further, we motivate the use ofSPG for both classic and semantic communication from the maximization of themutual information between received and target variables. Numerical resultsshow that our approach achieves comparable performance to a model-awareapproach based on the reparametrization trick, albeit with a decreasedconvergence rate.</description><author>Edgar Beck, Carsten Bockelmann, Armin Dekorsy</author><pubDate>Fri, 05 May 2023 15:27:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03571v1</guid></item><item><title>Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</title><link>http://arxiv.org/abs/2302.12173v2</link><description>Large Language Models (LLMs) are increasingly being integrated into variousapplications. The functionalities of recent LLMs can be flexibly modulated vianatural language prompts. This renders them susceptible to targeted adversarialprompting, e.g., Prompt Injection (PI) attacks enable attackers to overrideoriginal instructions and employed controls. So far, it was assumed that theuser is directly prompting the LLM. But, what if it is not the user prompting?We argue that LLM-Integrated Applications blur the line between data andinstructions. We reveal new attack vectors, using Indirect Prompt Injection,that enable adversaries to remotely (without a direct interface) exploitLLM-integrated applications by strategically injecting prompts into data likelyto be retrieved. We derive a comprehensive taxonomy from a computer securityperspective to systematically investigate impacts and vulnerabilities,including data theft, worming, information ecosystem contamination, and othernovel security risks. We demonstrate our attacks' practical viability againstboth real-world systems, such as Bing's GPT-4 powered Chat and code-completionengines, and synthetic applications built on GPT-4. We show how processingretrieved prompts can act as arbitrary code execution, manipulate theapplication's functionality, and control how and if other APIs are called.Despite the increasing integration and reliance on LLMs, effective mitigationsof these emerging threats are currently lacking. By raising awareness of thesevulnerabilities and providing key insights into their implications, we aim topromote the safe and responsible deployment of these powerful models and thedevelopment of robust defenses that protect users and systems from potentialattacks.</description><author>Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz</author><pubDate>Fri, 05 May 2023 15:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.12173v2</guid></item><item><title>A vector quantized masked autoencoder for audiovisual speech emotion recognition</title><link>http://arxiv.org/abs/2305.03568v1</link><description>While fully-supervised models have been shown to be effective for audiovisualspeech emotion recognition (SER), the limited availability of labeled dataremains a major challenge in the field. To address this issue, self-supervisedlearning approaches, such as masked autoencoders (MAEs), have gained popularityas potential solutions. In this paper, we propose the VQ-MAE-AV model, a vectorquantized MAE specifically designed for audiovisual speech self-supervisedrepresentation learning. Unlike existing multimodal MAEs that rely on theprocessing of the raw audiovisual speech data, the proposed method employs aself-supervised paradigm based on discrete audio and visual speechrepresentations learned by two pre-trained vector quantized variationalautoencoders. Experimental results show that the proposed approach, which ispre-trained on the VoxCeleb2 database and fine-tuned on standard emotionalaudiovisual speech datasets, outperforms the state-of-the-art audiovisual SERmethods.</description><author>Samir Sadok, Simon Leglaive, Renaud S√©guier</author><pubDate>Fri, 05 May 2023 15:19:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03568v1</guid></item><item><title>The geometry of financial institutions -- Wasserstein clustering of financial data</title><link>http://arxiv.org/abs/2305.03565v1</link><description>The increasing availability of granular and big data on various objects ofinterest has made it necessary to develop methods for condensing thisinformation into a representative and intelligible map. Financial regulation isa field that exemplifies this need, as regulators require diverse and oftenhighly granular data from financial institutions to monitor and assess theiractivities. However, processing and analyzing such data can be a daunting task,especially given the challenges of dealing with missing values and identifyingclusters based on specific features. To address these challenges, we propose a variant of Lloyd's algorithm thatapplies to probability distributions and uses generalized Wassersteinbarycenters to construct a metric space which represents given data on variousobjects in condensed form. By applying our method to the financial regulationcontext, we demonstrate its usefulness in dealing with the specific challengesfaced by regulators in this domain. We believe that our approach can also beapplied more generally to other fields where large and complex data sets needto be represented in concise form.</description><author>Lorenz Riess, Mathias Beiglb√∂ck, Johannes Temme, Andreas Wolf, Julio Backhoff</author><pubDate>Fri, 05 May 2023 15:16:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03565v1</guid></item><item><title>Learning Node Representations against Perturbations</title><link>http://arxiv.org/abs/2008.11416v3</link><description>Recent graph neural networks (GNN) has achieved remarkable performance innode representation learning. One key factor of GNN's success is the\emph{smoothness} property on node representations. Despite this, most GNNmodels are fragile to the perturbations on graph inputs and could learnunreliable node representations. In this paper, we study how to learn noderepresentations against perturbations in GNN. Specifically, we consider that anode representation should remain stable under slight perturbations on theinput, and node representations from different structures should beidentifiable, which two are termed as the \emph{stability} and\emph{identifiability} on node representations, respectively. To this end, wepropose a novel model called Stability-Identifiability GNN AgainstPerturbations (SIGNNAP) that learns reliable node representations in anunsupervised manner. SIGNNAP formalizes the \emph{stability} and\emph{identifiability} by a contrastive objective and preserves the\emph{smoothness} with existing GNN backbones. The proposed method is a genericframework that can be equipped with many other backbone models (e.g. GCN,GraphSage and GAT). Extensive experiments on six benchmarks under bothtransductive and inductive learning setups of node classification demonstratethe effectiveness of our method. Codes and data are availableonline:~\url{https://github.com/xuChenSJTU/SIGNNAP-master-online}</description><author>Xu Chen, Yuangang Pan, Ivor Tsang, Ya Zhang</author><pubDate>Fri, 05 May 2023 15:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2008.11416v3</guid></item><item><title>ADATIME: A Benchmarking Suite for Domain Adaptation on Time Series Data</title><link>http://arxiv.org/abs/2203.08321v2</link><description>Unsupervised domain adaptation methods aim to generalize well on unlabeledtest data that may have a different (shifted) distribution from the trainingdata. Such methods are typically developed on image data, and their applicationto time series data is less explored. Existing works on time series domainadaptation suffer from inconsistencies in evaluation schemes, datasets, andbackbone neural network architectures. Moreover, labeled target data are oftenused for model selection, which violates the fundamental assumption ofunsupervised domain adaptation. To address these issues, we develop abenchmarking evaluation suite (AdaTime) to systematically and fairly evaluatedifferent domain adaptation methods on time series data. Specifically, westandardize the backbone neural network architectures and benchmarkingdatasets, while also exploring more realistic model selection approaches thatcan work with no labeled data or just a few labeled samples. Our evaluationincludes adapting state-of-the-art visual domain adaptation methods to timeseries data as well as the recent methods specifically developed for timeseries data. We conduct extensive experiments to evaluate 11 state-of-the-artmethods on five representative datasets spanning 50 cross-domain scenarios. Ourresults suggest that with careful selection of hyper-parameters, visual domainadaptation methods are competitive with methods proposed for time series domainadaptation. In addition, we find that hyper-parameters could be selected basedon realistic model selection approaches. Our work unveils practical insightsfor applying domain adaptation methods on time series data and builds a solidfoundation for future works in the field. The code is available at\href{https://github.com/emadeldeen24/AdaTime}{github.com/emadeldeen24/AdaTime}.</description><author>Mohamed Ragab, Emadeldeen Eldele, Wee Ling Tan, Chuan-Sheng Foo, Zhenghua Chen, Min Wu, Chee-Keong Kwoh, Xiaoli Li</author><pubDate>Fri, 05 May 2023 15:06:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.08321v2</guid></item><item><title>Contrastive Graph Clustering in Curvature Spaces</title><link>http://arxiv.org/abs/2305.03555v1</link><description>Graph clustering is a longstanding research topic, and has achievedremarkable success with the deep learning methods in recent years.Nevertheless, we observe that several important issues largely remain open. Onthe one hand, graph clustering from the geometric perspective is appealing buthas rarely been touched before, as it lacks a promising space for geometricclustering. On the other hand, contrastive learning boosts the deep graphclustering but usually struggles in either graph augmentation or hard samplemining. To bridge this gap, we rethink the problem of graph clustering fromgeometric perspective and, to the best of our knowledge, make the first attemptto introduce a heterogeneous curvature space to graph clustering problem.Correspondingly, we present a novel end-to-end contrastive graph clusteringmodel named CONGREGATE, addressing geometric graph clustering with Riccicurvatures. To support geometric clustering, we construct a theoreticallygrounded Heterogeneous Curvature Space where deep representations are generatedvia the product of the proposed fully Riemannian graph convolutional nets.Thereafter, we train the graph clusters by an augmentation-free reweightedcontrastive approach where we pay more attention to both hard negatives andhard positives in our curvature space. Empirical results on real-world graphsshow that our model outperforms the state-of-the-art competitors.</description><author>Li Sun, Feiyang Wang, Junda Ye, Hao Peng, Philip S. Yu</author><pubDate>Fri, 05 May 2023 15:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03555v1</guid></item><item><title>Over-the-Air Federated Averaging with Limited Power and Privacy Budgets</title><link>http://arxiv.org/abs/2305.03547v1</link><description>To jointly overcome the communication bottleneck and privacy leakage ofwireless federated learning (FL), this paper studies a differentially privateover-the-air federated averaging (DP-OTA-FedAvg) system with a limited sumpower budget. With DP-OTA-FedAvg, the gradients are aligned by an alignmentcoefficient and aggregated over the air, and channel noise is employed toprotect privacy. We aim to improve the learning performance by jointlydesigning the device scheduling, alignment coefficient, and the number ofaggregation rounds of federated averaging (FedAvg) subject to sum power andprivacy constraints. We first present the privacy analysis based ondifferential privacy (DP) to quantify the impact of the alignment coefficienton privacy preservation in each communication round. Furthermore, to study howthe device scheduling, alignment coefficient, and the number of the globalaggregation affect the learning process, we conduct the convergence analysis ofDP-OTA-FedAvg in the cases of convex and non-convex loss functions. Based onthese analytical results, we formulate an optimization problem to minimize theoptimality gap of the DP-OTA-FedAvg subject to limited sum power and privacybudgets. The problem is solved by decoupling it into two sub-problems. Giventhe number of communication rounds, we conclude the relationship between thenumber of scheduled devices and the alignment coefficient, which offers a setof potential optimal solution pairs of device scheduling and the alignmentcoefficient. Thanks to the reduced search space, the optimal solution can beefficiently obtained. The effectiveness of the proposed policy is validatedthrough simulations.</description><author>Na Yan, Kezhi Wang, Cunhua Pan, Kok Keong Chai, Feng Shu, Jiangzhou Wang</author><pubDate>Fri, 05 May 2023 14:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03547v1</guid></item><item><title>Breast Cancer Immunohistochemical Image Generation: a Benchmark Dataset and Challenge Review</title><link>http://arxiv.org/abs/2305.03546v1</link><description>For invasive breast cancer, immunohistochemical (IHC) techniques are oftenused to detect the expression level of human epidermal growth factor receptor-2(HER2) in breast tissue to formulate a precise treatment plan. From theperspective of saving manpower, material and time costs, directly generatingIHC-stained images from hematoxylin and eosin (H&amp;E) stained images is avaluable research direction. Therefore, we held the breast cancerimmunohistochemical image generation challenge, aiming to explore novel ideasof deep learning technology in pathological image generation and promoteresearch in this field. The challenge provided registered H&amp;E and IHC-stainedimage pairs, and participants were required to use these images to train amodel that can directly generate IHC-stained images from correspondingH&amp;E-stained images. We selected and reviewed the five highest-ranking methodsbased on their PSNR and SSIM metrics, while also providing overviews of thecorresponding pipelines and implementations. In this paper, we further analyzethe current limitations in the field of breast cancer immunohistochemical imagegeneration and forecast the future development of this field. We hope that thereleased dataset and the challenge will inspire more scholars to jointly studyhigher-quality IHC-stained image generation.</description><author>Chuang Zhu, Shengjie Liu, Feng Xu, Zekuan Yu, Arpit Aggarwal, Germ√°n Corredor, Anant Madabhushi, Qixun Qu, Hongwei Fan, Fangda Li, Yueheng Li, Xianchao Guan, Yongbing Zhang, Vivek Kumar Singh, Farhan Akram, Md. Mostafa Kamal Sarker, Zhongyue Shi, Mulan Jin</author><pubDate>Fri, 05 May 2023 14:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03546v1</guid></item><item><title>UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language Models</title><link>http://arxiv.org/abs/2305.01624v2</link><description>Recent research demonstrates that external knowledge injection can advancepre-trained language models (PLMs) in a variety of downstream NLP tasks.However, existing knowledge injection methods are either applicable tostructured knowledge or unstructured knowledge, lacking a unified usage. Inthis paper, we propose a UNified knowledge inTERface, UNTER, to provide aunified perspective to exploit both structured knowledge and unstructuredknowledge. In UNTER, we adopt the decoder as a unified knowledge interface,aligning span representations obtained from the encoder with theircorresponding knowledge. This approach enables the encoder to uniformly invokespan-related knowledge from its parameters for downstream applications.Experimental results show that, with both forms of knowledge injected, UNTERgains continuous improvements on a series of knowledge-driven NLP tasks,including entity typing, named entity recognition and relation extraction,especially in low-resource scenarios.</description><author>Deming Ye, Yankai Lin, Zhengyan Zhang, Maosong Sun</author><pubDate>Fri, 05 May 2023 14:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01624v2</guid></item><item><title>Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation</title><link>http://arxiv.org/abs/2305.03520v1</link><description>The issue of word sense ambiguity poses a significant challenge in naturallanguage processing due to the scarcity of annotated data to feed machinelearning models to face the challenge. Therefore, unsupervised word sensedisambiguation methods have been developed to overcome that challenge withoutrelying on annotated data. This research proposes a new context-aware approachto unsupervised word sense disambiguation, which provides a flexible mechanismfor incorporating contextual information into the similarity measurementprocess. We experiment with a popular benchmark dataset to evaluate theproposed strategy and compare its performance with state-of-the-artunsupervised word sense disambiguation techniques. The experimental resultsindicate that our approach substantially enhances disambiguation accuracy andsurpasses the performance of several existing techniques. Our findingsunderscore the significance of integrating contextual information in semanticsimilarity measurements to manage word sense ambiguity in unsupervisedscenarios effectively.</description><author>Jorge Martinez-Gil</author><pubDate>Fri, 05 May 2023 14:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03520v1</guid></item><item><title>Next-generation Surgical Navigation: Multi-view Marker-less 6DoF Pose Estimation of Surgical Instruments</title><link>http://arxiv.org/abs/2305.03535v1</link><description>State-of-the-art research of traditional computer vision is increasinglyleveraged in the surgical domain. A particular focus in computer-assistedsurgery is to replace marker-based tracking systems for instrument localizationwith pure image-based 6DoF pose estimation. However, the state of the art hasnot yet met the accuracy required for surgical navigation. In this context, wepropose a high-fidelity marker-less optical tracking system for surgicalinstrument localization. We developed a multi-view camera setup consisting ofstatic and mobile cameras and collected a large-scale RGB-D video dataset withdedicated synchronization and data fusions methods. Different state-of-the-artpose estimation methods were integrated into a deep learning pipeline andevaluated on multiple camera configurations. Furthermore, the performanceimpacts of different input modalities and camera positions, as well as trainingon purely synthetic data, were compared. The best model achieved an averageposition and orientation error of 1.3 mm and 1.0{\deg} for a surgical drill aswell as 3.8 mm and 5.2{\deg} for a screwdriver. These results significantlyoutperform related methods in the literature and are close to clinical-gradeaccuracy, demonstrating that marker-less tracking of surgical instruments isbecoming a feasible alternative to existing marker-based systems.</description><author>Jonas Hein, Nicola Cavalcanti, Daniel Suter, Lukas Zingg, Fabio Carrillo, Mazda Farshad, Marc Pollefeys, Nassir Navab, Philipp F√ºrnstahl</author><pubDate>Fri, 05 May 2023 14:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03535v1</guid></item><item><title>ViT-Calibrator: Decision Stream Calibration for Vision Transformer</title><link>http://arxiv.org/abs/2304.04354v2</link><description>A surge of interest has emerged in utilizing Transformers in diverse visiontasks owing to its formidable performance. However, existing approachesprimarily focus on optimizing internal model architecture designs that oftenentail significant trial and error with high burdens. In this work, we proposea new paradigm dubbed Decision Stream Calibration that boosts the performanceof general Vision Transformers. To achieve this, we shed light on theinformation propagation mechanism in the learning procedure by exploring thecorrelation between different tokens and the relevance coefficient of multipledimensions. Upon further analysis, it was discovered that 1) the final decisionis associated with tokens of foreground targets, while token features offoreground target will be transmitted into the next layer as much as possible,and the useless token features of background area will be eliminated graduallyin the forward propagation. 2) Each category is solely associated with specificsparse dimensions in the tokens. Based on the discoveries mentioned above, wedesigned a two-stage calibration scheme, namely ViT-Calibrator, including tokenpropagation calibration stage and dimension propagation calibration stage.Extensive experiments on commonly used datasets show that the proposed approachcan achieve promising results. The source codes are given in the supplements.</description><author>Lin Chen, Zhijie Jia, Tian Qiu, Lechao Cheng, Jie Lei, Zunlei Feng, Mingli Song</author><pubDate>Fri, 05 May 2023 14:41:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04354v2</guid></item><item><title>Random Smoothing Regularization in Kernel Gradient Descent Learning</title><link>http://arxiv.org/abs/2305.03531v1</link><description>Random smoothing data augmentation is a unique form of regularization thatcan prevent overfitting by introducing noise to the input data, encouraging themodel to learn more generalized features. Despite its success in variousapplications, there has been a lack of systematic study on the regularizationability of random smoothing. In this paper, we aim to bridge this gap bypresenting a framework for random smoothing regularization that can adaptivelyand effectively learn a wide range of ground truth functions belonging to theclassical Sobolev spaces. Specifically, we investigate two underlying functionspaces: the Sobolev space of low intrinsic dimension, which includes theSobolev space in $D$-dimensional Euclidean space or low-dimensionalsub-manifolds as special cases, and the mixed smooth Sobolev space with atensor structure. By using random smoothing regularization as novelconvolution-based smoothing kernels, we can attain optimal convergence rates inthese cases using a kernel gradient descent algorithm, either with earlystopping or weight decay. It is noteworthy that our estimator can adapt to thestructural assumptions of the underlying data and avoid the curse ofdimensionality. This is achieved through various choices of injected noisedistributions such as Gaussian, Laplace, or general polynomial noises, allowingfor broad adaptation to the aforementioned structural assumptions of theunderlying data. The convergence rate depends only on the effective dimension,which may be significantly smaller than the actual data dimension. We conductnumerical experiments on simulated data to validate our theoretical results.</description><author>Liang Ding, Tianyang Hu, Jiahang Jiang, Donghao Li, Wenjia Wang, Yuan Yao</author><pubDate>Fri, 05 May 2023 14:37:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03531v1</guid></item><item><title>Exploring Softly Masked Language Modelling for Controllable Symbolic Music Generation</title><link>http://arxiv.org/abs/2305.03530v1</link><description>This document presents some early explorations of applying Softly MaskedLanguage Modelling (SMLM) to symbolic music generation. SMLM can be seen as ageneralisation of masked language modelling (MLM), where instead of eachelement of the input set being either known or unknown, elements can be partlyknown. We demonstrate some results of applying SMLM to constrained symbolicmusic generation using a transformer encoder architecture. Several audioexamples are available at https://erl-j.github.io/smlm-web-supplement/</description><author>Nicolas Jonason, Bob L. T. Sturm</author><pubDate>Fri, 05 May 2023 14:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03530v1</guid></item><item><title>Learning Decision Trees with Gradient Descent</title><link>http://arxiv.org/abs/2305.03515v1</link><description>Decision Trees (DTs) are commonly used for many machine learning tasks due totheir high degree of interpretability. However, learning a DT from data is adifficult optimization problem, as it is non-convex and non-differentiable.Therefore, common approaches learn DTs using a greedy growth algorithm thatminimizes the impurity locally at each internal node. Unfortunately, thisgreedy procedure can lead to suboptimal trees. In this paper, we present anovel approach for learning hard, axis-aligned DTs with gradient descent. Theproposed method uses backpropagation with a straight-through operator on adense DT representation to jointly optimize all tree parameters. Our approachoutperforms existing methods on binary classification benchmarks and achievescompetitive results for multi-class tasks.</description><author>Sascha Marton, Stefan L√ºdtke, Christian Bartelt, Heiner Stuckenschmidt</author><pubDate>Fri, 05 May 2023 14:24:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03515v1</guid></item><item><title>High-Level Context Representation for Emotion Recognition in Images</title><link>http://arxiv.org/abs/2305.03500v1</link><description>Emotion recognition is the task of classifying perceived emotions in people.Previous works have utilized various nonverbal cues to extract features fromimages and correlate them to emotions. Of these cues, situational context isparticularly crucial in emotion perception since it can directly influence theemotion of a person. In this paper, we propose an approach for high-levelcontext representation extraction from images. The model relies on a single cueand a single encoding stream to correlate this representation with emotions.Our model competes with the state-of-the-art, achieving an mAP of 0.3002 on theEMOTIC dataset while also being capable of execution on consumer-grade hardwareat approximately 90 frames per second. Overall, our approach is more efficientthan previous models and can be easily deployed to address real-world problemsrelated to emotion recognition.</description><author>Willams de Lima Costa, Estefania Talavera Martinez, Lucas Silva Figueiredo, Veronica Teichrieb</author><pubDate>Fri, 05 May 2023 14:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03500v1</guid></item><item><title>HD2Reg: Hierarchical Descriptors and Detectors for Point Cloud Registration</title><link>http://arxiv.org/abs/2305.03487v1</link><description>Feature Descriptors and Detectors are two main components of feature-basedpoint cloud registration. However, little attention has been drawn to theexplicit representation of local and global semantics in the learning ofdescriptors and detectors. In this paper, we present a framework thatexplicitly extracts dual-level descriptors and detectors and performscoarse-to-fine matching with them. First, to explicitly learn local and globalsemantics, we propose a hierarchical contrastive learning strategy, trainingthe robust matching ability of high-level descriptors, and refining the localfeature space using low-level descriptors. Furthermore, we propose to learndual-level saliency maps that extract two groups of keypoints in two differentsenses. To overcome the weak supervision of binary matchability labels, wepropose a ranking strategy to label the significance ranking of keypoints, andthus provide more fine-grained supervision signals. Finally, we propose aglobal-to-local matching scheme to obtain robust and accurate correspondencesby leveraging the complementary dual-level features.Quantitative experiments on3DMatch and KITTI odometry datasets show that our method achieves robust andaccurate point cloud registration and outperforms recent keypoint-basedmethods.</description><author>Canhui Tang, Yiheng Li, Shaoyi Du, Guofa Wang, Zhiqiang Tian</author><pubDate>Fri, 05 May 2023 13:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03487v1</guid></item><item><title>Uncertainty Quantification for Bayesian Optimization</title><link>http://arxiv.org/abs/2002.01569v2</link><description>Bayesian optimization is a class of global optimization techniques. InBayesian optimization, the underlying objective function is modeled as arealization of a Gaussian process. Although the Gaussian process assumptionimplies a random distribution of the Bayesian optimization outputs,quantification of this uncertainty is rarely studied in the literature. In thiswork, we propose a novel approach to assess the output uncertainty of Bayesianoptimization algorithms, which proceeds by constructing confidence regions ofthe maximum point (or value) of the objective function. These regions can becomputed efficiently, and their confidence levels are guaranteed by the uniformerror bounds for sequential Gaussian process regression newly developed in thepresent work. Our theory provides a unified uncertainty quantificationframework for all existing sequential sampling policies and stopping criteria.</description><author>Rui Tuo, Wenjia Wang</author><pubDate>Fri, 05 May 2023 13:41:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2002.01569v2</guid></item><item><title>Zoo Guide to Network Embedding</title><link>http://arxiv.org/abs/2305.03474v1</link><description>Networks have provided extremely successful models of data and complexsystems. Yet, as combinatorial objects, networks do not have in generalintrinsic coordinates and do not typically lie in an ambient space. The processof assigning an embedding space to a network has attracted lots of interest inthe past few decades, and has been efficiently applied to fundamental problemsin network inference, such as link prediction, node classification, andcommunity detection. In this review, we provide a user-friendly guide to thenetwork embedding literature and current trends in this field which will allowthe reader to navigate through the complex landscape of methods and approachesemerging from the vibrant research activity on these subjects.</description><author>Anthony Baptista, Rub√©n J. S√°nchez-Garc√≠a, Ana√Øs Baudot, Ginestra Bianconi</author><pubDate>Fri, 05 May 2023 13:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03474v1</guid></item><item><title>Generative Steganography Diffusion</title><link>http://arxiv.org/abs/2305.03472v1</link><description>Generative steganography (GS) is an emerging technique that generates stegoimages directly from secret data. Various GS methods based on GANs or Flow havebeen developed recently. However, existing GAN-based GS methods cannotcompletely recover the hidden secret data due to the lack of networkinvertibility, while Flow-based methods produce poor image quality due to thestringent reversibility restriction in each module. To address this issue, wepropose a novel GS scheme called "Generative Steganography Diffusion" (GSD) bydevising an invertible diffusion model named "StegoDiffusion". It not onlygenerates realistic stego images but also allows for 100\% recovery of thehidden secret data. The proposed StegoDiffusion model leverages a non-Markovchain with a fast sampling technique to achieve efficient stego imagegeneration. By constructing an ordinary differential equation (ODE) based onthe transition probability of the generation process in StegoDiffusion, secretdata and stego images can be converted to each other through the approximatesolver of ODE -- Euler iteration formula, enabling the use of irreversible butmore expressive network structures to achieve model invertibility. Our proposedGSD has the advantages of both reversibility and high performance,significantly outperforming existing GS methods in all metrics.</description><author>Ping Wei, Qing Zhou, Zichi Wang, Zhenxing Qian, Xinpeng Zhang, Sheng Li</author><pubDate>Fri, 05 May 2023 13:29:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03472v1</guid></item><item><title>Tree species classification from hyperspectral data using graph-regularized neural networks</title><link>http://arxiv.org/abs/2208.08675v2</link><description>We propose a novel graph-regularized neural network (GRNN) algorithm for treespecies classification. The proposed algorithm encompasses superpixel-basedsegmentation for graph construction, a pixel-wise neural network classifier,and the label propagation technique to generate an accurate and realistic(emulating tree crowns) classification map on a sparsely annotated data set.GRNN outperforms several state-of-the-art techniques not only for the standardIndian Pines HSI but also achieves a high classification accuracy (approx. 92%)on a new HSI data set collected over the heterogeneous forests of French Guiana(FG) when less than 1% of the pixels are labeled. We further show that GRNN iscompetitive with the state-of-the-art semi-supervised methods and exhibits asmall deviation in accuracy for different numbers of training samples and overrepeated trials with randomly sampled labeled pixels for training.</description><author>Debmita Bandyopadhyay, Subhadip Mukherjee, James Ball, Gr√©goire Vincent, David A. Coomes, Carola-Bibiane Sch√∂nlieb</author><pubDate>Fri, 05 May 2023 13:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.08675v2</guid></item><item><title>Contrastive Learning for Sleep Staging based on Inter Subject Correlation</title><link>http://arxiv.org/abs/2305.03178v1</link><description>In recent years, multitudes of researches have applied deep learning toautomatic sleep stage classification. Whereas actually, these works have paidless attention to the issue of cross-subject in sleep staging. At the sametime, emerging neuroscience theories on inter-subject correlations can providenew insights for cross-subject analysis. This paper presents the MViTime modelthat have been used in sleep staging study. And we implement the inter-subjectcorrelation theory through contrastive learning, providing a feasible solutionto address the cross-subject problem in sleep stage classification. Finally,experimental results and conclusions are presented, demonstrating that thedeveloped method has achieved state-of-the-art performance on sleep staging.The results of the ablation experiment also demonstrate the effectiveness ofthe cross-subject approach based on contrastive learning.</description><author>Tongxu Zhang, Bei Wang</author><pubDate>Fri, 05 May 2023 13:16:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03178v1</guid></item><item><title>Reducing Idleness in Financial Cloud via Multi-objective Evolutionary Reinforcement Learning based Load Balancer</title><link>http://arxiv.org/abs/2305.03463v1</link><description>In recent years, various companies started to shift their data services fromtraditional data centers onto cloud. One of the major motivations is to saveoperation costs with the aid of cloud elasticity. This paper discusses anemerging need from financial services to reduce idle servers retaining very fewuser connections, without disconnecting them from the server side. This paperconsiders this need as a bi-objective online load balancing problem. A neuralnetwork based scalable policy is designed to route user requests to variednumbers of servers for elasticity. An evolutionary multi-objective trainingframework is proposed to optimize the weights of the policy. Not only the newobjective of idleness is reduced by over 130% more than traditional industrialsolutions, but the original load balancing objective is slightly improved.Extensive simulations help reveal the detailed applicability of the proposedmethod to the emerging problem of reducing idleness in financial services.</description><author>Peng Yang, Laoming Zhang, Haifeng Liu, Guiying Li</author><pubDate>Fri, 05 May 2023 13:09:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03463v1</guid></item><item><title>General Neural Gauge Fields</title><link>http://arxiv.org/abs/2305.03462v1</link><description>The recent advance of neural fields, such as neural radiance fields, hassignificantly pushed the boundary of scene representation learning. Aiming toboost the computation efficiency and rendering quality of 3D scenes, a popularline of research maps the 3D coordinate system to another measuring system,e.g., 2D manifolds and hash tables, for modeling neural fields. The conversionof coordinate systems can be typically dubbed as gauge transformation, which isusually a pre-defined mapping function, e.g., orthogonal projection or spatialhash function. This begs a question: can we directly learn a desired gaugetransformation along with the neural field in an end-to-end manner? In thiswork, we extend this problem to a general paradigm with a taxonomy of discrete&amp; continuous cases, and develop an end-to-end learning framework to jointlyoptimize the gauge transformation and neural fields. To counter the problemthat the learning of gauge transformations can collapse easily, we derive ageneral regularization mechanism from the principle of information conservationduring the gauge transformation. To circumvent the high computation cost ingauge learning with regularization, we directly derive an information-invariantgauge transformation which allows to preserve scene information inherently andyield superior performance.</description><author>Fangneng Zhan, Lingjie Liu, Adam Kortylewski, Christian Theobalt</author><pubDate>Fri, 05 May 2023 13:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03462v1</guid></item><item><title>Interactive Acquisition of Fine-grained Visual Concepts by Exploiting Semantics of Generic Characterizations in Discourse</title><link>http://arxiv.org/abs/2305.03461v1</link><description>Interactive Task Learning (ITL) concerns learning about unforeseen domainconcepts via natural interactions with human users. The learner faces a numberof significant constraints: learning should be online, incremental andfew-shot, as it is expected to perform tangible belief updates right afternovel words denoting unforeseen concepts are introduced. In this work, weexplore a challenging symbol grounding task--discriminating among objectclasses that look very similar--within the constraints imposed by ITL. Wedemonstrate empirically that more data-efficient grounding results fromexploiting the truth-conditions of the teacher's generic statements (e.g., "Xshave attribute Z.") and their implicatures in context (e.g., as an answer to"How are Xs and Ys different?", one infers Y lacks attribute Z).</description><author>Jonghyuk Park, Alex Lascarides, Subramanian Ramamoorthy</author><pubDate>Fri, 05 May 2023 13:06:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03461v1</guid></item><item><title>Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question</title><link>http://arxiv.org/abs/2305.03458v1</link><description>Hybrid question answering (HybridQA) over the financial report contains bothtextual and tabular data, and requires the model to select the appropriateevidence for the numerical reasoning task. Existing methods based onencoder-decoder framework employ a expression tree-based decoder to solvenumerical reasoning problems. However, encoders rely more on Machine ReadingComprehension (MRC) methods, which take table serialization and text splicingas input, damaging the granularity relationship between table and text as wellas the spatial structure information of table itself. In order to solve theseproblems, the paper proposes a Multi-View Graph (MVG) Encoder to take therelations among the granularity into account and capture the relations frommultiple view. By utilizing MVGE as a module, we constuct Tabular View,Relation View and Numerical View which aim to retain the originalcharacteristics of the hybrid data. We validate our model on the publiclyavailable table-text hybrid QA benchmark (TAT-QA) and outperform thestate-of-the-art model.</description><author>Yifan Wei, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, Kang Liu</author><pubDate>Fri, 05 May 2023 13:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03458v1</guid></item><item><title>T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering</title><link>http://arxiv.org/abs/2305.03453v1</link><description>Large Language Models (LLMs) have recently demonstrated exceptionalperformance in various Natural Language Processing (NLP) tasks. They have alsoshown the ability to perform chain-of-thought (CoT) reasoning to solve complexproblems. Recent studies have explored CoT reasoning in complex multimodalscenarios, such as the science question answering task, by fine-tuningmultimodal models with high-quality human-annotated CoT rationales. However,collecting high-quality COT rationales is usually time-consuming and costly.Besides, the annotated rationales are hardly accurate due to the redundantinformation involved or the essential information missed. To address theseissues, we propose a novel method termed \emph{T-SciQ} that aims at teachingscience question answering with LLM signals. The T-SciQ approach generateshigh-quality CoT rationales as teaching signals and is advanced to train muchsmaller models to perform CoT reasoning in complex modalities. Additionally, weintroduce a novel data mixing strategy to produce more effective teaching datasamples for simple and complex science question answer problems. Extensiveexperimental results show that our T-SciQ method achieves a newstate-of-the-art performance on the ScienceQA benchmark, with an accuracy of96.18%. Moreover, our approach outperforms the most powerful fine-tunedbaseline by 4.5%.</description><author>Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen</author><pubDate>Fri, 05 May 2023 12:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03453v1</guid></item><item><title>A technical note on bilinear layers for interpretability</title><link>http://arxiv.org/abs/2305.03452v1</link><description>The ability of neural networks to represent more features than neurons makesinterpreting them challenging. This phenomenon, known as superposition, hasspurred efforts to find architectures that are more interpretable than standardmultilayer perceptrons (MLPs) with elementwise activation functions. In thisnote, I examine bilinear layers, which are a type of MLP layer that aremathematically much easier to analyze while simultaneously performing betterthan standard MLPs. Although they are nonlinear functions of their input, Idemonstrate that bilinear layers can be expressed using only linear operationsand third order tensors. We can integrate this expression for bilinear layersinto a mathematical framework for transformer circuits, which was previouslylimited to attention-only transformers. These results suggest that bilinearlayers are easier to analyze mathematically than current architectures and thusmay lend themselves to deeper safety insights by allowing us to talk moreformally about circuits in neural networks. Additionally, bilinear layers mayoffer an alternative path for mechanistic interpretability throughunderstanding the mechanisms of feature construction instead of enumerating a(potentially exponentially) large number of features in large models.</description><author>Lee Sharkey</author><pubDate>Fri, 05 May 2023 12:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03452v1</guid></item><item><title>Rethinking the Event Coding Pipeline with Prompt Entailment</title><link>http://arxiv.org/abs/2210.05257v2</link><description>For monitoring crises, political events are extracted from the news. Thelarge amount of unstructured full-text event descriptions makes a case-by-caseanalysis unmanageable, particularly for low-resource humanitarian aidorganizations. This creates a demand to classify events into event types, atask referred to as event coding. Typically, domain experts craft an event typeontology, annotators label a large dataset and technical experts develop asupervised coding system. In this work, we propose PR-ENT, a new event codingapproach that is more flexible and resource-efficient, while maintainingcompetitive accuracy: first, we extend an event description such as "Militaryinjured two civilians'' by a template, e.g. "People were [Z]" and prompt apre-trained (cloze) language model to fill the slot Z. Second, we select answercandidates Z* = {"injured'', "hurt"...} by treating the event description aspremise and the filled templates as hypothesis in a textual entailment task.This allows domain experts to draft the codebook directly as labeled promptsand interpretable answer candidates. This human-in-the-loop process is guidedby our interactive codebook design tool. We evaluate PR-ENT in severalrobustness checks: perturbing the event description and prompt template,restricting the vocabulary and removing contextual information.</description><author>Cl√©ment Lefebvre, Niklas Stoehr</author><pubDate>Fri, 05 May 2023 12:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05257v2</guid></item><item><title>LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models</title><link>http://arxiv.org/abs/2305.03445v1</link><description>Figurative language is a challenge for language models since itsinterpretation is based on the use of words in a way that deviates from theirconventional order and meaning. Yet, humans can easily understand and interpretmetaphors, similes or idioms as they can be derived from embodied metaphors.Language is a proxy for embodiment and if a metaphor is conventional andlexicalised, it becomes easier for a system without a body to make sense ofembodied concepts. Yet, the intricate relation between embodiment and featuressuch as concreteness or age of acquisition has not been studied in the contextof figurative language interpretation concerning language models. Hence, thepresented study shows how larger language models perform better at interpretingmetaphoric sentences when the action of the metaphorical sentence is moreembodied. The analysis rules out multicollinearity with other features (e.g.word length or concreteness) and provides initial evidence that larger languagemodels conceptualise embodied concepts to a degree that facilitates figurativelanguage understanding.</description><author>Philipp Wicke</author><pubDate>Fri, 05 May 2023 12:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03445v1</guid></item><item><title>Multi-Step Short-Term Wind Speed Prediction with Rank Pooling and Fast Fourier Transformation</title><link>http://arxiv.org/abs/2211.14434v2</link><description>Short-term wind speed prediction is essential for economical wind powerutilization. The real-world wind speed data is typically intermittent andfluctuating, presenting great challenges to existing shallow models. In thispaper, we present a novel deep hybrid model for multi-step wind speedprediction, namely LR-FFT-RP-MLP/LSTM (Linear Fast Fourier Transformation RankPooling Multiple-Layer Perception/Long Short-Term Memory). Our hybrid modelprocesses the local and global input features simultaneously. We leverage RankPooling (RP) for the local feature extraction to capture the temporal structurewhile maintaining the temporal order. Besides, to understand the wind periodicpatterns, we exploit Fast Fourier Transformation (FFT) to extract globalfeatures and relevant frequency components in the wind speed data. Theresulting local and global features are respectively integrated with theoriginal data and are fed into an MLP/LSTM layer for the initial wind speedpredictions. Finally, we leverage a linear regression layer to collaboratethese initial predictions to produce the final wind speed prediction. Theproposed hybrid model is evaluated using real wind speed data collected from2010 to 2020, demonstrating superior forecasting capabilities when compared tostate-of-the-art single and hybrid models. Overall, this study presents apromising approach for improving the accuracy of wind speed forecasting.</description><author>Hailong Shu</author><pubDate>Fri, 05 May 2023 12:39:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14434v2</guid></item><item><title>Advances on the classification of radio image cubes</title><link>http://arxiv.org/abs/2305.03435v1</link><description>Modern radio telescopes will daily generate data sets on the scale ofexabytes for systems like the Square Kilometre Array (SKA). Massive data setsare a source of unknown and rare astrophysical phenomena that lead todiscoveries. Nonetheless, this is only plausible with the exploitation ofintensive machine intelligence to complement human-aided and traditionalstatistical techniques. Recently, there has been a surge in scientificpublications focusing on the use of artificial intelligence in radio astronomy,addressing challenges such as source extraction, morphological classification,and anomaly detection. This study presents a succinct, but comprehensive reviewof the application of machine intelligence techniques on radio images withemphasis on the morphological classification of radio galaxies. It aims topresent a detailed synthesis of the relevant papers summarizing the literaturebased on data complexity, data pre-processing, and methodological novelty inradio astronomy. The rapid advancement and application of computer intelligencein radio astronomy has resulted in a revolution and a new paradigm shift in theautomation of daunting data processes. However, the optimal exploitation ofartificial intelligence in radio astronomy, calls for continued collaborativeefforts in the creation of annotated data sets. Additionally, in order toquickly locate radio galaxies with similar or dissimilar physicalcharacteristics, it is necessary to index the identified radio sources.Nonetheless, this issue has not been adequately addressed in the literature,making it an open area for further study.</description><author>Steven Ndung'u, Trienko Grobler, Stefan J. Wijnholds, Dimka Karastoyanova, George Azzopardi</author><pubDate>Fri, 05 May 2023 12:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03435v1</guid></item><item><title>Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects</title><link>http://arxiv.org/abs/2305.03433v1</link><description>This perspective paper proposes a series of interactive scenarios thatutilize Artificial Intelligence (AI) to enhance classroom teaching, such asdialogue auto-completion, knowledge and style transfer, and assessment ofAI-generated content. By leveraging recent developments in Large LanguageModels (LLMs), we explore the potential of AI to augment and enrichteacher-student dialogues and improve the quality of teaching. Our goal is toproduce innovative and meaningful conversations between teachers and students,create standards for evaluation, and improve the efficacy of AI-for-Educationinitiatives. In Section 3, we discuss the challenges of utilizing existing LLMsto effectively complete the educated tasks and present a unified framework foraddressing diverse education dataset, processing lengthy conversations, andcondensing information to better accomplish more downstream tasks. In Section4, we summarize the pivoting tasks including Teacher-Student DialogueAuto-Completion, Expert Teaching Knowledge and Style Transfer, and Assessmentof AI-Generated Content (AIGC), providing a clear path for future research. InSection 5, we also explore the use of external and adjustable LLMs to improvethe generated content through human-in-the-loop supervision and reinforcementlearning. Ultimately, this paper seeks to highlight the potential for AI to aidthe field of education and promote its further exploration.</description><author>Kehui Tan, Tianqi Pang, Chenyou Fan</author><pubDate>Fri, 05 May 2023 12:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03433v1</guid></item><item><title>Simulating H.P. Lovecraft horror literature with the ChatGPT large language model</title><link>http://arxiv.org/abs/2305.03429v1</link><description>In this paper, we present a novel approach to simulating H.P. Lovecraft'shorror literature using the ChatGPT large language model, specifically theGPT-4 architecture. Our study aims to generate text that emulates Lovecraft'sunique writing style and themes, while also examining the effectiveness ofprompt engineering techniques in guiding the model's output. To achieve this,we curated a prompt containing several specialized literature references andemployed advanced prompt engineering methods. We conducted an empiricalevaluation of the generated text by administering a survey to a sample ofundergraduate students. Utilizing statistical hypothesis testing, we assessedthe students ability to distinguish between genuine Lovecraft works and thosegenerated by our model. Our findings demonstrate that the participants wereunable to reliably differentiate between the two, indicating the effectivenessof the GPT-4 model and our prompt engineering techniques in emulatingLovecraft's literary style. In addition to presenting the GPT model'scapabilities, this paper provides a comprehensive description of its underlyingarchitecture and offers a comparative analysis with related work that simulatesother notable authors and philosophers, such as Dennett. By exploring thepotential of large language models in the context of literary emulation, ourstudy contributes to the body of research on the applications and limitationsof these models in various creative domains.</description><author>Eduardo C. Garrido-Merch√°n, Jos√© Luis Arroyo-Barrig√ºete, Roberto Gozalo-Brihuela</author><pubDate>Fri, 05 May 2023 12:03:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03429v1</guid></item><item><title>GAANet: Ghost Auto Anchor Network for Detecting Varying Size Drones in Dark</title><link>http://arxiv.org/abs/2305.03425v1</link><description>The usage of drones has tremendously increased in different sectors spanningfrom military to industrial applications. Despite all the benefits they offer,their misuse can lead to mishaps, and tackling them becomes more challengingparticularly at night due to their small size and low visibility conditions. Toovercome those limitations and improve the detection accuracy at night, wepropose an object detector called Ghost Auto Anchor Network (GAANet) forinfrared (IR) images. The detector uses a YOLOv5 core to address challenges inobject detection for IR images, such as poor accuracy and a high false alarmrate caused by extended altitudes, poor lighting, and low image resolution. Toimprove performance, we implemented auto anchor calculation, modified theconventional convolution block to ghost-convolution, adjusted the input channelsize, and used the AdamW optimizer. To enhance the precision of multiscale tinyobject recognition, we also introduced an additional extra-small object featureextractor and detector. Experimental results in a custom IR dataset withmultiple classes (birds, drones, planes, and helicopters) demonstrate thatGAANet shows improvement compared to state-of-the-art detectors. In comparisonto GhostNet-YOLOv5, GAANet has higher overall mean average precision (mAP@50),recall, and precision around 2.5\%, 2.3\%, and 1.4\%, respectively. The datasetand code for this paper are available as open source athttps://github.com/ZeeshanKaleem/GhostAutoAnchorNet.</description><author>Misha Urooj Khan, Maham Misbah, Zeeshan Kaleem, Yansha Deng, Abbas Jamalipour</author><pubDate>Fri, 05 May 2023 11:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03425v1</guid></item><item><title>Smart-Tree: Neural Medial Axis Approximation of Point Clouds for 3D Tree Skeletonization</title><link>http://arxiv.org/abs/2303.11560v2</link><description>This paper introduces Smart-Tree, a supervised method for approximating themedial axes of branch skeletons from a tree point cloud. Smart-Tree uses asparse voxel convolutional neural network to extract the radius and directiontowards the medial axis of each input point. A greedy algorithm performs robustskeletonization using the estimated medial axis. Our proposed method providesrobustness to complex tree structures and improves fidelity when dealing withself-occlusions, complex geometry, touching branches, and varying pointdensities. We evaluate Smart-Tree using a multi-species synthetic tree datasetand perform qualitative analysis on a real-world tree point cloud. Ourexperimentation with synthetic and real-world datasets demonstrates therobustness of our approach over the current state-of-the-art method. Thedataset and source code are publicly available.</description><author>Harry Dobbs, Oliver Batchelor, Richard Green, James Atlas</author><pubDate>Fri, 05 May 2023 11:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11560v2</guid></item><item><title>Using ChatGPT for Entity Matching</title><link>http://arxiv.org/abs/2305.03423v1</link><description>Entity Matching is the task of deciding if two entity descriptions refer tothe same real-world entity. State-of-the-art entity matching methods often relyon fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacksof using these models for entity matching are that (i) the models requiresignificant amounts of fine-tuning data for reaching a good performance and(ii) the fine-tuned models are not robust concerning out-of-distributionentities. In this paper, we investigate using ChatGPT for entity matching as amore robust, training data-efficient alternative to traditional Transformermodels. We perform experiments along three dimensions: (i) general promptdesign, (ii) in-context learning, and (iii) provision of higher-level matchingknowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,reaching an average zero-shot performance of 83% F1 on a challenging matchingtask on which RoBERTa requires 2000 training examples for reaching a similarperformance. Adding in-context demonstrations to the prompts further improvesthe F1 by up to 5% even using only a small set of 20 handpicked examples.Finally, we show that guiding the zero-shot model by stating higher-levelmatching rules leads to similar gains as providing in-context examples.</description><author>Ralph Peeters, Christian Bizer</author><pubDate>Fri, 05 May 2023 11:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03423v1</guid></item><item><title>Evolution under Length Constraints for CNN Architecture design</title><link>http://arxiv.org/abs/2305.03416v1</link><description>In recent years, the CNN architectures designed by evolution algorithms haveproven to be competitive with handcrafted architectures designed by experts.However, these algorithms need a lot of computational power, which is beyondthe capabilities of most researchers and engineers. To overcome this problem,we propose an evolution architecture under length constraints. It consists oftwo algorithms: a search length strategy to find an optimal space and a searcharchitecture strategy based on genetic algorithm to find the best individual inthe optimal space. Our algorithms reduce drastically resource cost and alsokeep good performance. On the Cifar-10 dataset, our framework presentsoutstanding performance with an error rate of 5.12% and only 4.6 GPU a day toconverge to the optimal individual -22 GPU a day less than the lowest costautomatic evolutionary algorithm in the peer competition.</description><author>Ousmane Youme, Jean Marie Dembele, Eugene C. Ezin, Christophe Cambier</author><pubDate>Fri, 05 May 2023 11:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03416v1</guid></item><item><title>Adaptive Graph Convolutional Subspace Clustering</title><link>http://arxiv.org/abs/2305.03414v1</link><description>Spectral-type subspace clustering algorithms have shown excellent performancein many subspace clustering applications. The existing spectral-type subspaceclustering algorithms either focus on designing constraints for thereconstruction coefficient matrix or feature extraction methods for findinglatent features of original data samples. In this paper, inspired by graphconvolutional networks, we use the graph convolution technique to develop afeature extraction method and a coefficient matrix constraint simultaneously.And the graph-convolutional operator is updated iteratively and adaptively inour proposed algorithm. Hence, we call the proposed method adaptive graphconvolutional subspace clustering (AGCSC). We claim that by using AGCSC, theaggregated feature representation of original data samples is suitable forsubspace clustering, and the coefficient matrix could reveal the subspacestructure of the original data set more faithfully. Finally, plenty of subspaceclustering experiments prove our conclusions and show that AGCSC outperformssome related methods as well as some deep models.</description><author>Lai Wei, Zhengwei Chen, Jun Yin, Changming Zhu, Rigui Zhou, Jin Liu</author><pubDate>Fri, 05 May 2023 11:27:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03414v1</guid></item><item><title>Domain-agnostic segmentation of thalamic nuclei from joint structural and diffusion MRI</title><link>http://arxiv.org/abs/2305.03413v1</link><description>The human thalamus is a highly connected subcortical grey-matter structurewithin the brain. It comprises dozens of nuclei with different function andconnectivity, which are affected differently by disease. For this reason, thereis growing interest in studying the thalamic nuclei in vivo with MRI. Tools areavailable to segment the thalamus from 1 mm T1 scans, but the contrast of thelateral and internal boundaries is too faint to produce reliable segmentations.Some tools have attempted to incorporate information from diffusion MRI in thesegmentation to refine these boundaries, but do not generalise well acrossdiffusion MRI acquisitions. Here we present the first CNN that can segmentthalamic nuclei from T1 and diffusion data of any resolution without retrainingor fine tuning. Our method builds on a public histological atlas of thethalamic nuclei and silver standard segmentations on high-quality diffusiondata obtained with a recent Bayesian adaptive segmentation tool. We combinethese with an approximate degradation model for fast domain randomisationduring training. Our CNN produces a segmentation at 0.7 mm isotropicresolution, irrespective of the resolution of the input. Moreover, it uses aparsimonious model of the diffusion signal at each voxel (fractional anisotropyand principal eigenvector) that is compatible with virtually any set ofdirections and b-values, including huge amounts of legacy data. We show resultsof our proposed method on three heterogeneous datasets acquired on dozens ofdifferent scanners. An implementation of the method is publicly available athttps://freesurfer.net/fswiki/ThalamicNucleiDTI.</description><author>Henry F. J. Tregidgo, Sonja Soskic, Mark D. Olchanyi, Juri Althonayan, Benjamin Billot, Chiara Maffei, Polina Golland, Anastasia Yendiki, Daniel C. Alexander, Martina Bocchetta, Jonathan D. Rohrer, Juan Eugenio Iglesias</author><pubDate>Fri, 05 May 2023 11:26:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03413v1</guid></item><item><title>Assessing Trustworthiness of Autonomous Systems</title><link>http://arxiv.org/abs/2305.03411v1</link><description>As Autonomous Systems (AS) become more ubiquitous in society, moreresponsible for our safety and our interaction with them more frequent, it isessential that they are trustworthy. Assessing the trustworthiness of AS is amandatory challenge for the verification and development community. This willrequire appropriate standards and suitable metrics that may serve toobjectively and comparatively judge trustworthiness of AS across the broadrange of current and future applications. The meta-expression `trustworthiness'is examined in the context of AS capturing the relevant qualities that comprisethis term in the literature. Recent developments in standards and frameworksthat support assurance of autonomous systems are reviewed. A list of keychallenges are identified for the community and we present an outline of aprocess that can be used as a trustworthiness assessment framework for AS.</description><author>Gregory Chance, Dhaminda B. Abeywickrama, Beckett LeClair, Owen Kerr, Kerstin Eder</author><pubDate>Fri, 05 May 2023 11:26:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03411v1</guid></item></channel></rss>