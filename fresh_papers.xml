<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 17 Jun 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Quantifying Variance in Evaluation Benchmarks</title><link>http://arxiv.org/abs/2406.10229v1</link><description>Evaluation benchmarks are the cornerstone of measuring capabilities of largelanguage models (LLMs), as well as driving progress in said capabilities.Originally designed to make claims about capabilities (or lack thereof) infully pretrained models, evaluation benchmarks are now also extensively used todecide between various training choices. Despite this widespread usage, werarely quantify the variance in our evaluation benchmarks, which dictateswhether differences in performance are meaningful. Here, we define and measurea range of metrics geared towards measuring variance in evaluation benchmarks,including seed variance across initialisations, and monotonicity duringtraining. By studying a large number of models -- both openly available andpretrained from scratch -- we provide empirical estimates for a variety ofvariance metrics, with considerations and recommendations for practitioners. Wealso evaluate the utility and tradeoffs of continuous versus discreteperformance measures and explore options for better understanding and reducingthis variance. We find that simple changes, such as framing choice tasks (likeMMLU) as completion tasks, can often reduce variance for smaller scale($\sim$7B) models, while more involved methods inspired from human testingliterature (such as item analysis and item response theory) struggle tomeaningfully reduce variance. Overall, our work provides insights into variancein evaluation benchmarks, suggests LM-specific techniques to reduce variance,and more generally encourages practitioners to carefully factor in variancewhen comparing models.</description><author>Lovish Madaan, Aaditya K. Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang, Dieuwke Hupkes</author><pubDate>Fri, 14 Jun 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10229v1</guid></item><item><title>VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models</title><link>http://arxiv.org/abs/2406.10228v1</link><description>The swift progress of Multi-modal Large Models (MLLMs) has showcased theirimpressive ability to tackle tasks blending vision and language. Yet, mostcurrent models and benchmarks cater to scenarios with a narrow scope of visualand textual contexts. These models often fall short when faced with complexcomprehension tasks, which involve navigating through a plethora of irrelevantand potentially misleading information in both text and image forms. To bridgethis gap, we introduce a new, more demanding task known as InterleavedImage-Text Comprehension (IITC). This task challenges models to discern anddisregard superfluous elements in both images and text to accurately answerquestions and to follow intricate instructions to pinpoint the relevant image.In support of this task, we further craft a new VEGA dataset, tailored for theIITC task on scientific content, and devised a subtask, Image-Text Association(ITA), to refine image-text correlation skills. Our evaluation of four leadingclosed-source models, as well as various open-source models using VEGA,underscores the rigorous nature of IITC. Even the most advanced models, such asGemini-1.5-pro and GPT4V, only achieved modest success. By employing amulti-task, multi-scale post-training strategy, we have set a robust baselinefor MLLMs on the IITC task, attaining an $85.8\%$ accuracy rate in imageassociation and a $0.508$ Rouge score. These results validate the effectivenessof our dataset in improving MLLMs capabilities for nuanced image-textcomprehension.</description><author>Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, Xiawu Zheng, Xing Sun, Rongrong Ji</author><pubDate>Fri, 14 Jun 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10228v1</guid></item><item><title>CinePile: A Long Video Question Answering Dataset and Benchmark</title><link>http://arxiv.org/abs/2405.08813v2</link><description>Current datasets for long-form video understanding often fall short ofproviding genuine long-form comprehension challenges, as many tasks derivedfrom these datasets can be successfully tackled by analyzing just one or a fewrandom frames from a video. To address this issue, we present a novel datasetand benchmark, CinePile, specifically designed for authentic long-form videounderstanding. This paper details our innovative approach for creating aquestion-answer dataset, utilizing advanced LLMs with human-in-the-loop andbuilding upon human-generated raw data. Our comprehensive dataset comprises305,000 multiple-choice questions (MCQs), covering various visual andmultimodal aspects, including temporal comprehension, understandinghuman-object interactions, and reasoning about events or actions within ascene. Additionally, we evaluate recent video-centric LLMs, both open-sourceand proprietary, on the test split of our dataset. The findings reveal thateven state-of-the-art video-centric LLMs significantly lag behind humanperformance in these tasks, highlighting the complexity and challenge inherentin video understanding. The dataset is available athttps://hf.co/datasets/tomg-group-umd/cinepile</description><author>Ruchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, Tom Goldstein</author><pubDate>Fri, 14 Jun 2024 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08813v2</guid></item><item><title>VideoGUI: A Benchmark for GUI Automation from Instructional Videos</title><link>http://arxiv.org/abs/2406.10227v1</link><description>Graphical User Interface (GUI) automation holds significant promise forenhancing human productivity by assisting with computer tasks. Existing taskformulations primarily focus on simple tasks that can be specified by a single,language-only instruction, such as "Insert a new slide." In this work, weintroduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUIassistants on visual-centric GUI tasks. Sourced from high-quality webinstructional videos, our benchmark focuses on tasks involving professional andnovel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complexactivities (e.g., video editing). VideoGUI evaluates GUI assistants through ahierarchical process, allowing for identification of the specific levels atwhich they may fail: (i) high-level planning: reconstruct procedural subtasksfrom visual conditions without language descriptions; (ii) middle-levelplanning: generate sequences of precise action narrations based on visual state(i.e., screenshot) and goals; (iii) atomic action execution: perform specificactions such as accurately clicking designated elements. For each level, wedesign evaluation metrics across individual dimensions to provide clearsignals, such as individual performance in clicking, dragging, typing, andscrolling for atomic action execution. Our evaluation on VideoGUI reveals thateven the SoTA large multimodal model GPT4o performs poorly on visual-centricGUI tasks, especially for high-level planning.</description><author>Kevin Qinghong Lin, Linjie Li, Difei Gao, Qinchen WU, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, Mike Zheng Shou</author><pubDate>Fri, 14 Jun 2024 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10227v1</guid></item><item><title>SatDiffMoE: A Mixture of Estimation Method for Satellite Image Super-resolution with Latent Diffusion Models</title><link>http://arxiv.org/abs/2406.10225v1</link><description>During the acquisition of satellite images, there is generally a trade-offbetween spatial resolution and temporal resolution (acquisition frequency) dueto the onboard sensors of satellite imaging systems. High-resolution satelliteimages are very important for land crop monitoring, urban planning, wildfiremanagement and a variety of applications. It is a significant yet challengingtask to achieve high spatial-temporal resolution in satellite imaging. With theadvent of diffusion models, we can now learn strong generative priors togenerate realistic satellite images with high resolution, which can be utilizedto promote the super-resolution task as well. In this work, we propose a noveldiffusion-based fusion algorithm called \textbf{SatDiffMoE} that can take anarbitrary number of sequential low-resolution satellite images at the samelocation as inputs, and fuse them into one high-resolution reconstructed imagewith more fine details, by leveraging and fusing the complementary informationfrom different time points. Our algorithm is highly flexible and allowstraining and inference on arbitrary number of low-resolution images.Experimental results show that our proposed SatDiffMoE method not only achievessuperior performance for the satellite image super-resolution tasks on avariety of datasets, but also gets an improved computational efficiency withreduced model parameters, compared with previous methods.</description><author>Zhaoxu Luo, Bowen Song, Liyue Shen</author><pubDate>Fri, 14 Jun 2024 18:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10225v1</guid></item><item><title>EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation Models</title><link>http://arxiv.org/abs/2406.10224v1</link><description>The advent of wearable computers enables a new source of context for AI thatis embedded in egocentric sensor data. This new egocentric data comes equippedwith fine-grained 3D location information and thus presents the opportunity fora novel class of spatial foundation models that are rooted in 3D space. Tomeasure progress on what we term Egocentric Foundation Models (EFMs) weestablish EFM3D, a benchmark with two core 3D egocentric perception tasks.EFM3D is the first benchmark for 3D object detection and surface regression onhigh quality annotated egocentric data of Project Aria. We propose EgocentricVoxel Lifting (EVL), a baseline for 3D EFMs. EVL leverages all availableegocentric modalities and inherits foundational capabilities from 2D foundationmodels. This model, trained on a large simulated dataset, outperforms existingmethods on the EFM3D benchmark.</description><author>Julian Straub, Daniel DeTone, Tianwei Shen, Nan Yang, Chris Sweeney, Richard Newcombe</author><pubDate>Fri, 14 Jun 2024 18:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10224v1</guid></item><item><title>COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning</title><link>http://arxiv.org/abs/2311.02248v2</link><description>We present a cost-effective method to integrate speech into a large languagemodel (LLM), resulting in a Contextual Speech Model withInstruction-following/in-context-learning Capabilities (COSMIC) multi-modalLLM. Using GPT-3.5, we generate Speech Comprehension Test Question-Answer (SQA)pairs from speech transcriptions for supervised instruction tuning. With under30 million trainable parameters and only 450 hours of English speech data,COSMIC demonstrates emerging capabilities in instruction-following andin-context learning. Equipped with such capabilities, COSMIC achieves a maximum33.18 BLEU score in 0-shot EN-to-X speech to text translation (S2TT) and asignificant boost in the 1-shot setting. Additionally, there is an average25.8\% relative Word Error Rate (WER) reduction for 1-shot cross-domainadaptation. COSMIC exhibits a significant automatic speech recognition (ASR)accuracy gain in contextual biasing tasks due to its instruction-followingcapability.</description><author>Jing Pan, Jian Wu, Yashesh Gaur, Sunit Sivasankaran, Zhuo Chen, Shujie Liu, Jinyu Li</author><pubDate>Fri, 14 Jun 2024 18:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02248v2</guid></item><item><title>Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation</title><link>http://arxiv.org/abs/2406.10223v1</link><description>We introduce DiffuseST, a low-latency, direct speech-to-speech translationsystem capable of preserving the input speaker's voice zero-shot whiletranslating from multiple source languages into English. We experiment with thesynthesizer component of the architecture, comparing a Tacotron-basedsynthesizer to a novel diffusion-based synthesizer. We find the diffusion-basedsynthesizer to improve MOS and PESQ audio quality metrics by 23\% each andspeaker similarity by 5\% while maintaining comparable BLEU scores. Despitehaving more than double the parameter count, the diffusion synthesizer haslower latency, allowing the entire model to run more than 5$\times$ faster thanreal-time.</description><author>Nameer Hirschkind, Xiao Yu, Mahesh Kumar Nandwana, Joseph Liu, Eloi DuBois, Dao Le, Nicolas Thiebaut, Colin Sinclair, Kyle Spence, Charles Shang, Zoe Abrams, Morgan McGuire</author><pubDate>Fri, 14 Jun 2024 18:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10223v1</guid></item><item><title>Short Film Dataset (SFD): A Benchmark for Story-Level Video Understanding</title><link>http://arxiv.org/abs/2406.10221v1</link><description>Recent advances in vision-language models have significantly propelled videounderstanding. Existing datasets and tasks, however, have notable limitations.Most datasets are confined to short videos with limited events and narrownarratives. For example, datasets with instructional and egocentric videosoften document the activities of one person in a single scene. Although somemovie datasets offer richer content, they are often limited to short-termtasks, lack publicly available videos and frequently encounter data leakagegiven the use of movie forums and other resources in LLM training. To addressthe above limitations, we propose the Short Film Dataset (SFD) with 1,078publicly available amateur movies, a wide variety of genres and minimal dataleakage issues. SFD offers long-term story-oriented video tasks in the form ofmultiple-choice and open-ended question answering. Our extensive experimentsemphasize the need for long-term reasoning to solve SFD tasks. Notably, we findstrong signals in movie transcripts leading to the on-par performance of peopleand LLMs. We also show significantly lower performance of current modelscompared to people when using vision data alone.</description><author>Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, Ivan Laptev</author><pubDate>Fri, 14 Jun 2024 18:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10221v1</guid></item><item><title>PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2406.10219v1</link><description>Recent advancements in novel view synthesis have enabled real-time renderingspeeds and high reconstruction accuracy. 3D Gaussian Splatting (3D-GS), afoundational point-based parametric 3D scene representation, models scenes aslarge sets of 3D Gaussians. Complex scenes can comprise of millions ofGaussians, amounting to large storage and memory requirements that limit theviability of 3D-GS on devices with limited resources. Current techniques forcompressing these pretrained models by pruning Gaussians rely on combiningheuristics to determine which ones to remove. In this paper, we propose aprincipled spatial sensitivity pruning score that outperforms these approaches.It is computed as a second-order approximation of the reconstruction error onthe training views with respect to the spatial parameters of each Gaussian.Additionally, we propose a multi-round prune-refine pipeline that can beapplied to any pretrained 3D-GS model without changing the training pipeline.After pruning 88.44% of the Gaussians, we observe that our PUP 3D-GS pipelineincreases the average rendering speed of 3D-GS by 2.65$\times$ while retainingmore salient foreground information and achieving higher image quality metricsthan previous pruning techniques on scenes from the Mip-NeRF 360, Tanks &amp;Temples, and Deep Blending datasets.</description><author>Alex Hanson, Allen Tu, Vasu Singla, Mayuka Jayawardhana, Matthias Zwicker, Tom Goldstein</author><pubDate>Fri, 14 Jun 2024 18:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10219v1</guid></item><item><title>Semantic Membership Inference Attack against Large Language Models</title><link>http://arxiv.org/abs/2406.10218v1</link><description>Membership Inference Attacks (MIAs) determine whether a specific data pointwas included in the training set of a target model. In this paper, we introducethe Semantic Membership Inference Attack (SMIA), a novel approach that enhancesMIA performance by leveraging the semantic content of inputs and theirperturbations. SMIA trains a neural network to analyze the target model'sbehavior on perturbed inputs, effectively capturing variations in outputprobability distributions between members and non-members. We conductcomprehensive evaluations on the Pythia and GPT-Neo model families using theWikipedia dataset. Our results show that SMIA significantly outperformsexisting MIAs; for instance, SMIA achieves an AUC-ROC of 67.39% on Pythia-12B,compared to 58.90% by the second-best attack.</description><author>Hamid Mozaffari, Virendra J. Marathe</author><pubDate>Fri, 14 Jun 2024 18:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10218v1</guid></item><item><title>Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs</title><link>http://arxiv.org/abs/2406.10216v1</link><description>Reward models trained on human preference data have been proven to beeffective for aligning Large Language Models (LLMs) with human intent withinthe reinforcement learning from human feedback (RLHF) framework. However, thegeneralization capabilities of current reward models to unseen prompts andresponses are limited. This limitation can lead to an unexpected phenomenonknown as reward over-optimization, where excessive optimization of rewardsresults in a decline in actual performance. While previous research hasadvocated for constraining policy optimization, our study proposes a novelapproach to enhance the reward model's generalization ability againstdistribution shifts by regularizing the hidden states. Specifically, we retainthe base model's language model head and incorporate a suite of text-generationlosses to preserve the hidden states' text generation capabilities, whileconcurrently learning a reward head behind the same hidden states. Ourexperimental results demonstrate that the introduced regularization techniquemarkedly improves the accuracy of learned reward models across a variety ofout-of-distribution (OOD) tasks and effectively alleviate the over-optimizationissue in RLHF, offering a more reliable and robust preference learningparadigm.</description><author>Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, Tong Zhang</author><pubDate>Fri, 14 Jun 2024 18:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10216v1</guid></item><item><title>DevBench: A multimodal developmental benchmark for language learning</title><link>http://arxiv.org/abs/2406.10215v1</link><description>How (dis)similar are the learning trajectories of vision-language models andchildren? Recent modeling work has attempted to understand the gap betweenmodels' and humans' data efficiency by constructing models trained on lessdata, especially multimodal naturalistic data. However, such models are oftenevaluated on adult-level benchmarks, with limited breadth in language abilitiestested, and without direct comparison to behavioral data. We introduceDevBench, a multimodal benchmark comprising seven language evaluation tasksspanning the domains of lexical, syntactic, and semantic ability, withbehavioral data from both children and adults. We evaluate a set ofvision-language models on these tasks, comparing models and humans not only onaccuracy but on their response patterns. Across tasks, models exhibit variationin their closeness to human response patterns, and models that perform betteron a task also more closely resemble human behavioral responses. We alsoexamine the developmental trajectory of OpenCLIP over training, finding thatgreater training results in closer approximations to adult response patterns.DevBench thus provides a benchmark for comparing models to human languagedevelopment. These comparisons highlight ways in which model and human languagelearning processes diverge, providing insight into entry points for improvinglanguage models.</description><author>Alvin Wei Ming Tan, Sunny Yu, Bria Long, Wanjing Anya Ma, Tonya Murray, Rebecca D. Silverman, Jason D. Yeatman, Michael C. Frank</author><pubDate>Fri, 14 Jun 2024 18:49:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10215v1</guid></item><item><title>Universal randomised signatures for generative time series modelling</title><link>http://arxiv.org/abs/2406.10214v1</link><description>Randomised signature has been proposed as a flexible and easily implementablealternative to the well-established path signature. In this article, we employrandomised signature to introduce a generative model for financial time seriesdata in the spirit of reservoir computing. Specifically, we propose a novelWasserstein-type distance based on discrete-time randomised signatures. Thismetric on the space of probability measures captures the distance between(conditional) distributions. Its use is justified by our novel universalapproximation results for randomised signatures on the space of continuousfunctions taking the underlying path as an input. We then use our metric as theloss function in a non-adversarial generator model for synthetic time seriesdata based on a reservoir neural stochastic differential equation. We comparethe results of our model to benchmarks from the existing literature.</description><author>Francesca Biagini, Lukas Gonon, Niklas Walter</author><pubDate>Fri, 14 Jun 2024 18:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10214v1</guid></item><item><title>Selecting Interpretability Techniques for Healthcare Machine Learning models</title><link>http://arxiv.org/abs/2406.10213v1</link><description>In healthcare there is a pursuit for employing interpretable algorithms toassist healthcare professionals in several decision scenarios. Following thePredictive, Descriptive and Relevant (PDR) framework, the definition ofinterpretable machine learning as a machine-learning model that explicitly andin a simple frame determines relationships either contained in data or learnedby the model that are relevant for its functioning and the categorization ofmodels by post-hoc, acquiring interpretability after training, or model-based,being intrinsically embedded in the algorithm design. We overview a selectionof eight algorithms, both post-hoc and model-based, that can be used for suchpurposes.</description><author>Daniel Sierra-Botero, Ana Molina-Taborda, Mario S. Valdés-Tresanco, Alejandro Hernández-Arango, Leonardo Espinosa-Leal, Alexander Karpenko, Olga Lopez-Acevedo</author><pubDate>Fri, 14 Jun 2024 18:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10213v1</guid></item><item><title>NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity</title><link>http://arxiv.org/abs/2406.10212v1</link><description>Photoelasticity enables full-field stress analysis in transparent objectsthrough stress-induced birefringence. Existing techniques are limited to 2Dslices and require destructively slicing the object. Recovering the internal 3Dstress distribution of the entire object is challenging as it involves solvinga tensor tomography problem and handling phase wrapping ambiguities. Weintroduce NeST, an analysis-by-synthesis approach for reconstructing 3D stresstensor fields as neural implicit representations from polarizationmeasurements. Our key insight is to jointly handle phase unwrapping and tensortomography using a differentiable forward model based on Jones calculus. Ournon-linear model faithfully matches real captures, unlike prior linearapproximations. We develop an experimental multi-axis polariscope setup tocapture 3D photoelasticity and experimentally demonstrate that NeSTreconstructs the internal stress distribution for objects with varying shapeand force conditions. Additionally, we showcase novel applications in stressanalysis, such as visualizing photoelastic fringes by virtually slicing theobject and viewing photoelastic fringes from unseen viewpoints. NeST paves theway for scalable non-destructive 3D photoelastic analysis.</description><author>Akshat Dave, Tianyi Zhang, Aaron Young, Ramesh Raskar, Wolfgang Heidrich, Ashok Veeraraghavan</author><pubDate>Fri, 14 Jun 2024 18:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10212v1</guid></item><item><title>DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction</title><link>http://arxiv.org/abs/2406.10211v1</link><description>Diffusion models face significant challenges when employed for large-scalemedical image reconstruction in real practice such as 3D Computed Tomography(CT). Due to the demanding memory, time, and data requirements, it is difficultto train a diffusion model directly on the entire volume of high-dimensionaldata to obtain an efficient 3D diffusion prior. Existing works utilizingdiffusion priors on single 2D image slice with hand-crafted cross-sliceregularization would sacrifice the z-axis consistency, which results in severeartifacts along the z-axis. In this work, we propose a novel framework thatenables learning the 3D image prior through position-aware 3D-patch diffusionscore blending for reconstructing large-scale 3D medical images. To the best ofour knowledge, we are the first to utilize a 3D-patch diffusion prior for 3Dmedical image reconstruction. Extensive experiments on sparse view and limitedangle CT reconstruction show that our DiffusionBlend method significantlyoutperforms previous methods and achieves state-of-the-art performance onreal-world CT reconstruction problems with high-dimensional 3D image (i.e.,$256 \times 256 \times 500$). Our algorithm also comes with better orcomparable computational efficiency than previous state-of-the-art methods.</description><author>Bowen Song, Jason Hu, Zhaoxu Luo, Jeffrey A. Fessler, Liyue Shen</author><pubDate>Fri, 14 Jun 2024 18:47:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10211v1</guid></item><item><title>Make It Count: Text-to-Image Generation with an Accurate Number of Objects</title><link>http://arxiv.org/abs/2406.10210v1</link><description>Despite the unprecedented success of text-to-image diffusion models,controlling the number of depicted objects using text is surprisingly hard.This is important for various applications from technical documents, tochildren's books to illustrating cooking recipes. Generating object-correctcounts is fundamentally challenging because the generative model needs to keepa sense of separate identity for every instance of the object, even if severalobjects look identical or overlap, and then carry out a global computationimplicitly during generation. It is still unknown if such representationsexist. To address count-correct generation, we first identify features withinthe diffusion model that can carry the object identity information. We then usethem to separate and count instances of objects during the denoising processand detect over-generation and under-generation. We fix the latter by traininga model that predicts both the shape and location of a missing object, based onthe layout of existing ones, and show how it can be used to guide denoisingwith correct object count. Our approach, CountGen, does not depend on externalsource to determine object layout, but rather uses the prior from the diffusionmodel itself, creating prompt-dependent and seed-dependent layouts. Evaluatedon two benchmark datasets, we find that CountGen strongly outperforms thecount-accuracy of existing baselines.</description><author>Lital Binyamin, Yoad Tewel, Hilit Segev, Eran Hirsch, Royi Rassin, Gal Chechik</author><pubDate>Fri, 14 Jun 2024 18:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10210v1</guid></item><item><title>CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes</title><link>http://arxiv.org/abs/2404.01299v2</link><description>Causal video question answering (QA) has garnered increasing interest, yetexisting datasets often lack depth in causal reasoning. To address this gap, wecapitalize on the unique properties of cartoons and construct CausalChaos!, anovel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry"cartoon series. Cartoons use the principles of animation that allow animatorsto create expressive, unambiguous causal relationships between events to form acoherent storyline. Utilizing these properties, along with thought-provokingquestions and multi-level answers (answer and detailed causal explanation), ourquestions involve causal chains that interconnect multiple dynamic interactionsbetween characters and visual scenes. These factors demand models to solve morechallenging, yet well-defined causal relationships. We also introduce hardincorrect answer mining, including a causally confusing version that is evenmore challenging. While models perform well, there is much room forimprovement, especially, on open-ended answers. We identify moreadvanced/explicit causal relationship modeling &amp; joint modeling of vision andlanguage as the immediate areas for future efforts to focus upon. Along withthe other complementary datasets, our new challenging dataset will pave the wayfor these developments in the field.</description><author>Paritosh Parmar, Eric Peh, Ruirui Chen, Ting En Lam, Yuhan Chen, Elston Tan, Basura Fernando</author><pubDate>Fri, 14 Jun 2024 18:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01299v2</guid></item><item><title>Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs</title><link>http://arxiv.org/abs/2406.10209v1</link><description>Large language models can memorize and repeat their training data, causingprivacy and copyright risks. To mitigate memorization, we introduce a subtlemodification to the next-token training objective that we call the goldfishloss. During training, a randomly sampled subset of tokens are excluded fromthe loss computation. These dropped tokens are not memorized by the model,which prevents verbatim reproduction of a complete chain of tokens from thetraining set. We run extensive experiments training billion-scale Llama-2models, both pre-trained and trained from scratch, and demonstrate significantreductions in extractable memorization with little to no impact on downstreambenchmarks.</description><author>Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein</author><pubDate>Fri, 14 Jun 2024 18:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10209v1</guid></item><item><title>Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering</title><link>http://arxiv.org/abs/2406.10208v1</link><description>Recently, Glyph-ByT5 has achieved highly accurate visual text renderingperformance in graphic design images. However, it still focuses solely onEnglish and performs relatively poorly in terms of visual appeal. In this work,we address these two fundamental limitations by presenting Glyph-ByT5-v2 andGlyph-SDXL-v2, which not only support accurate visual text rendering for 10different languages but also achieve much better aesthetic quality. To achievethis, we make the following contributions: (i) creating a high-qualitymultilingual glyph-text and graphic design dataset consisting of more than 1million glyph-text pairs and 10 million graphic design image-text pairscovering nine other languages, (ii) building a multilingual visual paragraphbenchmark consisting of 1,000 prompts, with 100 for each language, to assessmultilingual visual spelling accuracy, and (iii) leveraging the lateststep-aware preference learning approach to enhance the visual aestheticquality. With the combination of these techniques, we deliver a powerfulcustomized multilingual text encoder, Glyph-ByT5-v2, and a strong aestheticgraphic generation model, Glyph-SDXL-v2, that can support accurate spelling in10 different languages. We perceive our work as a significant advancement,considering that the latest DALL-E3 and Ideogram 1.0 still struggle with themultilingual visual text rendering task.</description><author>Zeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Ji Li, Yuhui Yuan</author><pubDate>Fri, 14 Jun 2024 18:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10208v1</guid></item><item><title>Maestro: Uncovering Low-Rank Structures via Trainable Decomposition</title><link>http://arxiv.org/abs/2308.14929v2</link><description>Deep Neural Networks (DNNs) have been a large driver for AI breakthroughs inrecent years. However, these models have been getting increasingly large asthey become more accurate and safe. This means that their training becomesincreasingly costly and time-consuming and typically yields a single model tofit all targets. Various techniques have been proposed in the literature tomitigate this, including pruning, sparsification, or quantization of modelweights and updates. While achieving high compression rates, they often incursignificant computational overheads at training or lead to non-negligibleaccuracy penalty. Alternatively, factorization methods have been leveraged forlow-rank compression of DNNs. Similarly, such techniques (e.g., SVD) frequentlyrely on heavy iterative decompositions of layers and are potentiallysub-optimal for non-linear models, such as DNNs. We take a further step indesigning efficient low-rank models and propose Maestro, a framework fortrainable low-rank layers. Instead of iteratively applying a prioridecompositions, the low-rank structure is baked into the training processthrough LoD, a low-rank ordered decomposition. Not only is this the first timeimportance ordering via sampling is applied on the decomposed DNN structure,but it also allows selecting ranks at a layer granularity. Our theoreticalanalysis demonstrates that in special cases LoD recovers the SVD decompositionand PCA. Applied to DNNs, Maestro enables the extraction of lower footprintmodels that preserve performance. Simultaneously, it enables the gracefultrade-off between accuracy-latency for deployment to even more constraineddevices without retraining.</description><author>Samuel Horvath, Stefanos Laskaridis, Shashank Rajput, Hongyi Wang</author><pubDate>Fri, 14 Jun 2024 18:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14929v2</guid></item><item><title>A Fundamental Trade-off in Aligned Language Models and its Relation to Sampling Adaptors</title><link>http://arxiv.org/abs/2406.10203v1</link><description>The relationship between the quality of a string and its probability$p(\boldsymbol{y})$ under a language model has been influential in thedevelopment of techniques to build good text generation systems. For example,several decoding algorithms have been motivated to manipulate$p(\boldsymbol{y})$ to produce higher-quality text. In this work, we examinethe probability--quality relationship in language models explicitly aligned tohuman preferences, e.g., through Reinforcement Learning through Human Feedback(RLHF). We find that, given a general language model and its aligned version,for corpora sampled from an aligned language model, there exists a trade-offbetween the average reward and average log-likelihood of the strings under thegeneral language model. We provide a formal treatment of this issue anddemonstrate how a choice of sampling adaptor allows for a selection of how muchlikelihood we exchange for the reward.</description><author>Naaman Tan, Josef Valvoda, Anej Svete, Tianyu Liu, Yanxia Qin, Kan Min-Yen, Ryan Cotterell</author><pubDate>Fri, 14 Jun 2024 18:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10203v1</guid></item><item><title>Training from Zero: Radio Frequency Machine Learning Data Quantity Forecasting</title><link>http://arxiv.org/abs/2205.03703v2</link><description>The data used during training in any given application space is directly tiedto the performance of the system once deployed. While there are many otherfactors that go into producing high performance models within machine learning,there is no doubt that the data used to train a system provides the foundationfrom which to build. One of the underlying rule of thumb heuristics used withinthe machine learning space is that more data leads to better models, but thereis no easy answer for the question, "How much data is needed?" This workexamines a modulation classification problem in the Radio Frequency domainspace, attempting to answer the question of how much training data is requiredto achieve a desired level of performance, but the procedure readily applies toclassification problems across modalities. The ultimate goal is determining anapproach that requires the least amount of data collection to better inform amore thorough collection effort to achieve the desired performance metric.While this approach will require an initial dataset that is germane to theproblem space to act as a \textit{target} dataset on which metrics areextracted, the goal is to allow for the initial data to be orders of magnitudesmaller than what is required for delivering a system that achieves the desiredperformance. An additional benefit of the techniques presented here is that thequality of different datasets can be numerically evaluated and tied togetherwith the quantity of data, and ultimately, the performance of the architecturein the problem domain.</description><author>William H. Clark IV, Alan J. Michaels</author><pubDate>Fri, 14 Jun 2024 18:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.03703v2</guid></item><item><title>SSTFB: Leveraging self-supervised pretext learning and temporal self-attention with feature branching for real-time video polyp segmentation</title><link>http://arxiv.org/abs/2406.10200v1</link><description>Polyps are early cancer indicators, so assessing occurrences of polyps andtheir removal is critical. They are observed through a colonoscopy screeningprocedure that generates a stream of video frames. Segmenting polyps in theirnatural video screening procedure has several challenges, such as theco-existence of imaging artefacts, motion blur, and floating debris. Mostexisting polyp segmentation algorithms are developed on curated still imagedatasets that do not represent real-world colonoscopy. Their performance oftendegrades on video data. We propose a video polyp segmentation method thatperforms self-supervised learning as an auxiliary task and a spatial-temporalself-attention mechanism for improved representation learning. Our end-to-endconfiguration and joint optimisation of losses enable the network to learn morediscriminative contextual features in videos. Our experimental resultsdemonstrate an improvement with respect to several state-of-the-art (SOTA)methods. Our ablation study also confirms that the choice of the proposed jointend-to-end training improves network accuracy by over 3% and nearly 10% on boththe Dice similarity coefficient and intersection-over-union compared to therecently proposed method PNS+ and Polyp-PVT, respectively. Results onpreviously unseen video data indicate that the proposed method generalises.</description><author>Ziang Xu, Jens Rittscher, Sharib Ali</author><pubDate>Fri, 14 Jun 2024 18:33:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10200v1</guid></item><item><title>Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain</title><link>http://arxiv.org/abs/2402.19226v3</link><description>Chronic pain significantly diminishes the quality of life for millionsworldwide. While psychoeducation and therapy can improve pain outcomes, manyindividuals experiencing pain lack access to evidence-based treatments or failto complete the necessary number of sessions to achieve benefit. Reinforcementlearning (RL) shows potential in tailoring personalized pain managementinterventions according to patients' individual needs while ensuring theefficient use of scarce clinical resources. However, clinicians, patients, andhealthcare decision-makers are concerned that RL solutions could exacerbatedisparities associated with patient characteristics like race or gender. Inthis article, we study gender fairness in personalized pain carerecommendations using a real-world application of reinforcement learning(Piette et al., 2022a). Here, adhering to gender fairness translates to minimalor no disparity in the utility received by subpopulations as defined by gender.We investigate whether the selection of relevant patient information (referredto as features) used to assist decision-making affects gender fairness. Ourexperiments, conducted using real-world data Piette, 2022), indicate thatincluded features can impact gender fairness. Moreover, we propose an RLsolution, NestedRecommendation, that demonstrates the ability: i) to adaptivelylearn to select the features that optimize for utility and fairness, and ii) toaccelerate feature selection and in turn, improve pain care recommendationsfrom early on, by leveraging clinicians' domain expertise.</description><author>Pratik Gajane, Sean Newman, Mykola Pechenizkiy, John D. Piette</author><pubDate>Fri, 14 Jun 2024 18:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19226v3</guid></item><item><title>Crafting Parts for Expressive Object Composition</title><link>http://arxiv.org/abs/2406.10197v1</link><description>Text-to-image generation from large generative models like Stable Diffusion,DALLE-2, etc., have become a common base for various tasks due to theirsuperior quality and extensive knowledge bases. As image composition andgeneration are creative processes the artists need control over various partsof the images being generated. We find that just adding details about parts inthe base text prompt either leads to an entirely different image (e.g.,missing/incorrect identity) or the extra part details simply being ignored. Tomitigate these issues, we introduce PartCraft, which enables image generationbased on fine-grained part-level details specified for objects in the base textprompt. This allows more control for artists and enables novel objectcompositions by combining distinctive object parts. PartCraft first localizesobject parts by denoising the object region from a specific diffusion process.This enables each part token to be localized to the right object region. Afterobtaining part masks, we run a localized diffusion process in each of the partregions based on fine-grained part descriptions and combine them to produce thefinal image. All the stages of PartCraft are based on repurposing a pre-traineddiffusion model, which enables it to generalize across various domains withouttraining. We demonstrate the effectiveness of part-level control provided byPartCraft qualitatively through visual examples and quantitatively incomparison to the contemporary baselines.</description><author>Harsh Rangwani, Aishwarya Agarwal, Kuldeep Kulkarni, R. Venkatesh Babu, Srikrishna Karanam</author><pubDate>Fri, 14 Jun 2024 18:31:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10197v1</guid></item><item><title>TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners</title><link>http://arxiv.org/abs/2406.10196v1</link><description>Travel planning is a complex task that involves generating a sequence ofactions related to visiting places subject to constraints and maximizing someuser satisfaction criteria. Traditional approaches rely on problem formulationin a given formal language, extracting relevant travel information from websources, and use an adequate problem solver to generate a valid solution. As analternative, recent Large Language Model (LLM) based approaches directly outputplans from user requests using language. Although LLMs possess extensive traveldomain knowledge and provide high-level information like points of interest andpotential routes, current state-of-the-art models often generate plans thatlack coherence, fail to satisfy constraints fully, and do not guarantee thegeneration of high-quality solutions. We propose TRIP-PAL, a hybrid method thatcombines the strengths of LLMs and automated planners, where (i) LLMs get andtranslate travel information and user information into data structures that canbe fed into planners; and (ii) automated planners generate travel plans thatguarantee constraint satisfaction and optimize for users' utility. Ourexperiments across various travel scenarios show that TRIP-PAL outperforms anLLM when generating travel plans.</description><author>Tomas de la Rosa, Sriram Gopalakrishnan, Alberto Pozanco, Zhen Zeng, Daniel Borrajo</author><pubDate>Fri, 14 Jun 2024 18:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10196v1</guid></item><item><title>A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis</title><link>http://arxiv.org/abs/2311.04157v3</link><description>We present a novel usage of Transformers to make image classificationinterpretable. Unlike mainstream classifiers that wait until the last fullyconnected layer to incorporate class information to make predictions, weinvestigate a proactive approach, asking each class to search for itself in animage. We realize this idea via a Transformer encoder-decoder inspired byDEtection TRansformer (DETR). We learn "class-specific" queries (one for eachclass) as input to the decoder, enabling each class to localize its patterns inan image via cross-attention. We name our approach INterpretable TRansformer(INTR), which is fairly easy to implement and exhibits several compellingproperties. We show that INTR intrinsically encourages each class to attenddistinctively; the cross-attention weights thus provide a faithfulinterpretation of the prediction. Interestingly, via "multi-head"cross-attention, INTR could identify different "attributes" of a class, makingit particularly suitable for fine-grained classification and analysis, which wedemonstrate on eight datasets. Our code and pre-trained models are publiclyaccessible at the Imageomics Institute GitHub site:https://github.com/Imageomics/INTR.</description><author>Dipanjyoti Paul, Arpita Chowdhury, Xinqi Xiong, Feng-Ju Chang, David Carlyn, Samuel Stevens, Kaiya L. Provost, Anuj Karpatne, Bryan Carstens, Daniel Rubenstein, Charles Stewart, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao</author><pubDate>Fri, 14 Jun 2024 18:28:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04157v3</guid></item><item><title>CHIRON: Rich Character Representations in Long-Form Narratives</title><link>http://arxiv.org/abs/2406.10190v1</link><description>Characters are integral to long-form narratives, but are poorly understood byexisting story analysis and generation systems. While prior work has simplifiedcharacters via graph-based methods and brief character descriptions, we aim tobetter tackle the problem of representing complex characters by takinginspiration from advice given to professional writers. We propose CHIRON, a new`character sheet' based representation that organizes and filters textualinformation about characters. We construct CHIRON sheets in two steps: aGeneration Module that prompts an LLM for character information viaquestion-answering and a Validation Module that uses automated reasoning and adomain-specific entailment model to eliminate false facts about a character. Wevalidate CHIRON via the downstream task of masked-character prediction, whereour experiments show CHIRON is better and more flexible than comparablesummary-based baselines. We also show that metrics derived from CHIRON can beused to automatically infer character-centricity in stories, and that thesemetrics align with human judgments.</description><author>Alexander Gurung, Mirella Lapata</author><pubDate>Fri, 14 Jun 2024 18:23:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10190v1</guid></item><item><title>AstroCLIP: A Cross-Modal Foundation Model for Galaxies</title><link>http://arxiv.org/abs/2310.03024v2</link><description>We present AstroCLIP, a single, versatile model that can embed both galaxyimages and spectra into a shared, physically meaningful latent space. Theseembeddings can then be used - without any model fine-tuning - for a variety ofdownstream tasks including (1) accurate in-modality and cross-modality semanticsimilarity search, (2) photometric redshift estimation, (3) galaxy propertyestimation from both images and spectra, and (4) morphology classification. Ourapproach to implementing AstroCLIP consists of two parts. First, we embedgalaxy images and spectra separately by pretraining separate transformer-basedimage and spectrum encoders in self-supervised settings. We then align theencoders using a contrastive loss. We apply our method to spectra from the DarkEnergy Spectroscopic Instrument and images from its corresponding LegacyImaging Survey. Overall, we find remarkable performance on all downstreamtasks, even relative to supervised baselines. For example, for a task likephotometric redshift prediction, we find similar performance to aspecifically-trained ResNet18, and for additional tasks like physical propertyestimation (stellar mass, age, metallicity, and sSFR), we beat this supervisedbaseline by 19\% in terms of $R^2$. We also compare our results to astate-of-the-art self-supervised single-modal model for galaxy images, and findthat our approach outperforms this benchmark by roughly a factor of two onphotometric redshift estimation and physical property prediction in terms of$R^2$, while remaining roughly in-line in terms of morphology classification.Ultimately, our approach represents the first cross-modal self-supervised modelfor galaxies, and the first self-supervised transformer-based architectures forgalaxy images and spectra.</description><author>Liam Parker, Francois Lanusse, Siavash Golkar, Leopoldo Sarra, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Geraud Krawezik, Michael McCabe, Ruben Ohana, Mariel Pettee, Bruno Regaldo-Saint Blancard, Tiberiu Tesileanu, Kyunghyun Cho, Shirley Ho</author><pubDate>Fri, 14 Jun 2024 18:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03024v2</guid></item><item><title>WonderWorld: Interactive 3D Scene Generation from a Single Image</title><link>http://arxiv.org/abs/2406.09394v2</link><description>We present WonderWorld, a novel framework for interactive 3D sceneextrapolation that enables users to explore and shape virtual environmentsbased on a single input image and user-specified text. While significantimprovements have been made to the visual quality of scene generation, existingmethods are run offline, taking tens of minutes to hours to generate a scene.By leveraging Fast Gaussian Surfels and a guided diffusion-based depthestimation method, WonderWorld generates geometrically consistent extrapolationwhile significantly reducing computational time. Our framework generatesconnected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU,enabling real-time user interaction and exploration. We demonstrate thepotential of WonderWorld for applications in virtual reality, gaming, andcreative design, where users can quickly generate and navigate immersive,potentially infinite virtual worlds from a single image. Our approachrepresents a significant advancement in interactive 3D scene generation,opening up new possibilities for user-driven content creation and explorationin virtual environments. We will release full code and software forreproducibility. Project website: https://WonderWorld-2024.github.io/</description><author>Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, Jiajun Wu</author><pubDate>Fri, 14 Jun 2024 18:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09394v2</guid></item><item><title>Explaining Probabilistic Models with Distributional Values</title><link>http://arxiv.org/abs/2402.09947v2</link><description>A large branch of explainable machine learning is grounded in cooperativegame theory. However, research indicates that game-theoretic explanations maymislead or be hard to interpret. We argue that often there is a criticalmismatch between what one wishes to explain (e.g. the output of a classifier)and what current methods such as SHAP explain (e.g. the scalar probability of aclass). This paper addresses such gap for probabilistic models by generalisingcooperative games and value operators. We introduce the distributional values,random variables that track changes in the model output (e.g. flipping of thepredicted class) and derive their analytic expressions for games with Gaussian,Bernoulli and Categorical payoffs. We further establish several characterisingproperties, and show that our framework provides fine-grained and insightfulexplanations with case studies on vision and language models.</description><author>Luca Franceschi, Michele Donini, Cédric Archambeau, Matthias Seeger</author><pubDate>Fri, 14 Jun 2024 18:18:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09947v2</guid></item><item><title>Detecting and Evaluating Medical Hallucinations in Large Vision Language Models</title><link>http://arxiv.org/abs/2406.10185v1</link><description>Large Vision Language Models (LVLMs) are increasingly integral to healthcareapplications, including medical visual question answering and imaging reportgeneration. While these models inherit the robust capabilities of foundationalLarge Language Models (LLMs), they also inherit susceptibility tohallucinations-a significant concern in high-stakes medical contexts where themargin for error is minimal. However, currently, there are no dedicated methodsor benchmarks for hallucination detection and evaluation in the medical field.To bridge this gap, we introduce Med-HallMark, the first benchmark specificallydesigned for hallucination detection and evaluation within the medicalmultimodal domain. This benchmark provides multi-tasking hallucination support,multifaceted hallucination data, and hierarchical hallucination categorization.Furthermore, we propose the MediHall Score, a new medical evaluative metricdesigned to assess LVLMs' hallucinations through a hierarchical scoring systemthat considers the severity and type of hallucination, thereby enabling agranular assessment of potential clinical impacts. We also presentMediHallDetector, a novel Medical LVLM engineered for precise hallucinationdetection, which employs multitask training for hallucination detection.Through extensive experimental evaluations, we establish baselines for popularLVLMs using our benchmark. The findings indicate that MediHall Score provides amore nuanced understanding of hallucination impacts compared to traditionalmetrics and demonstrate the enhanced performance of MediHallDetector. We hopethis work can significantly improve the reliability of LVLMs in medicalapplications. All resources of this work will be released soon.</description><author>Jiawei Chen, Dingkang Yang, Tong Wu, Yue Jiang, Xiaolu Hou, Mingcheng Li, Shunli Wang, Dongling Xiao, Ke Li, Lihua Zhang</author><pubDate>Fri, 14 Jun 2024 18:14:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10185v1</guid></item><item><title>Gemini &amp; Physical World: Large Language Models Can Estimate the Intensity of Earthquake Shaking from Multi-Modal Social Media Posts</title><link>http://arxiv.org/abs/2405.18732v3</link><description>This paper presents a novel approach to extract scientifically valuableinformation about Earth's physical phenomena from unconventional sources, suchas multi-modal social media posts. Employing a state-of-the-art large languagemodel (LLM), Gemini 1.5 Pro (Reid et al. 2024), we estimate earthquake groundshaking intensity from these unstructured posts. The model's output, in theform of Modified Mercalli Intensity (MMI) values, aligns well with independentobservational data. Furthermore, our results suggest that LLMs, trained on vastinternet data, may have developed a unique understanding of physical phenomena.Specifically, Google's Gemini models demonstrate a simplified understanding ofthe general relationship between earthquake magnitude, distance, and MMIintensity, accurately describing observational data even though it's notidentical to established models. These findings raise intriguing questionsabout the extent to which Gemini's training has led to a broader understandingof the physical world and its phenomena. The ability of Generative AI modelslike Gemini to generate results consistent with established scientificknowledge highlights their potential to augment our understanding of complexphysical phenomena like earthquakes. The flexible and effective approachproposed in this study holds immense potential for enriching our understandingof the impact of physical phenomena and improving resilience during naturaldisasters. This research is a significant step toward harnessing the power ofsocial media and AI for natural disaster mitigation, opening new avenues forunderstanding the emerging capabilities of Generative AI and LLMs forscientific applications.</description><author>S. Mostafa Mousavi, Marc Stogaitis, Tajinder Gadh, Richard M Allen, Alexei Barski, Robert Bosch, Patrick Robertson, Nivetha Thiruverahan, Youngmin Cho, Aman Raj</author><pubDate>Fri, 14 Jun 2024 18:12:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18732v3</guid></item><item><title>Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors</title><link>http://arxiv.org/abs/2406.10181v1</link><description>Fine-tuning large language models (LLMs) requires significant memory, oftenexceeding the capacity of a single GPU. A common solution to this memorychallenge is offloading compute and data from the GPU to the CPU. However, thisapproach is hampered by the limited bandwidth of commodity hardware, whichconstrains communication between the CPU and GPU. In this paper, we present an offloading framework, LSP_Offload, that enablesnear-native speed LLM fine-tuning on commodity hardware through learnedsubspace projectors. Our data-driven approach involves learning an efficientsparse compressor that minimizes communication with minimal precision loss.Additionally, we introduce a novel layer-wise communication schedule tomaximize parallelism between communication and computation. As a result, ourframework can fine-tune a 1.3 billion parameter model on a 4GB laptop GPU and a7 billion parameter model on an NVIDIA RTX 4090 GPU with 24GB memory, achievingonly a 31% slowdown compared to fine-tuning with unlimited memory. Compared tostate-of-the-art offloading frameworks, our approach increases fine-tuningthroughput by up to 3.33 times and reduces end-to-end fine-tuning time by33.1%~62.5% when converging to the same accuracy.</description><author>Siyuan Chen, Zelong Guan, Yudong Liu, Phillip B. Gibbons</author><pubDate>Fri, 14 Jun 2024 17:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10181v1</guid></item><item><title>MeshPose: Unifying DensePose and 3D Body Mesh reconstruction</title><link>http://arxiv.org/abs/2406.10180v1</link><description>DensePose provides a pixel-accurate association of images with 3D meshcoordinates, but does not provide a 3D mesh, while Human Mesh Reconstruction(HMR) systems have high 2D reprojection error, as measured by DensePoselocalization metrics. In this work we introduce MeshPose to jointly tackleDensePose and HMR. For this we first introduce new losses that allow us to useweak DensePose supervision to accurately localize in 2D a subset of the meshvertices ('VertexPose'). We then lift these vertices to 3D, yielding a low-polybody mesh ('MeshPose'). Our system is trained in an end-to-end manner and isthe first HMR method to attain competitive DensePose accuracy, while also beinglightweight and amenable to efficient inference, making it suitable forreal-time AR applications.</description><author>Eric-Tuan Lê, Antonis Kakolyris, Petros Koutras, Himmy Tam, Efstratios Skordos, George Papandreou, Rıza Alp Güler, Iasonas Kokkinos</author><pubDate>Fri, 14 Jun 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10180v1</guid></item><item><title>Inclusive ASR for Disfluent Speech: Cascaded Large-Scale Self-Supervised Learning with Targeted Fine-Tuning and Data Augmentation</title><link>http://arxiv.org/abs/2406.10177v1</link><description>Automatic speech recognition (ASR) systems often falter while processingstuttering-related disfluencies -- such as involuntary blocks and wordrepetitions -- yielding inaccurate transcripts. A critical barrier to progressis the scarcity of large, annotated disfluent speech datasets. Therefore, wepresent an inclusive ASR design approach, leveraging large-scaleself-supervised learning on standard speech followed by targeted fine-tuningand data augmentation on a smaller, curated dataset of disfluent speech. Ourdata augmentation technique enriches training datasets with variousdisfluencies, enhancing ASR processing of these speech patterns. Results showthat fine-tuning wav2vec 2.0 with even a relatively small, labeled dataset,alongside data augmentation, can significantly reduce word error rates fordisfluent speech. Our approach not only advances ASR inclusivity for people whostutter, but also paves the way for ASRs that can accommodate wider speechvariations.</description><author>Dena Mujtaba, Nihar R. Mahapatra, Megan Arney, J. Scott Yaruss, Caryn Herring, Jia Bin</author><pubDate>Fri, 14 Jun 2024 17:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10177v1</guid></item><item><title>Enhancing Incomplete Multi-modal Brain Tumor Segmentation with Intra-modal Asymmetry and Inter-modal Dependency</title><link>http://arxiv.org/abs/2406.10175v1</link><description>Deep learning-based brain tumor segmentation (BTS) models for multi-modal MRIimages have seen significant advancements in recent years. However, a commonproblem in practice is the unavailability of some modalities due to varyingscanning protocols and patient conditions, making segmentation from incompleteMRI modalities a challenging issue. Previous methods have attempted to addressthis by fusing accessible multi-modal features, leveraging attentionmechanisms, and synthesizing missing modalities using generative models.However, these methods ignore the intrinsic problems of medical imagesegmentation, such as the limited availability of training samples,particularly for cases with tumors. Furthermore, these methods require trainingand deploying a specific model for each subset of missing modalities. Toaddress these issues, we propose a novel approach that enhances the BTS modelfrom two perspectives. Firstly, we introduce a pre-training stage thatgenerates a diverse pre-training dataset covering a wide range of differentcombinations of tumor shapes and brain anatomy. Secondly, we propose apost-training stage that enables the model to reconstruct missing modalities inthe prediction results when only partial modalities are available. To achievethe pre-training stage, we conceptually decouple the MRI image into two parts:`anatomy' and `tumor'. We pre-train the BTS model using synthesized datagenerated from the anatomy and tumor parts across different training samples.... Extensive experiments demonstrate that our proposed method significantlyimproves the performance over the baseline and achieves new state-of-the-artresults on three brain tumor segmentation datasets: BRATS2020, BRATS2018, andBRATS2015.</description><author>Weide Liu, Jingwen Hou, Xiaoyang Zhong, Huijing Zhan, Jun Cheng, Yuming Fang, Guanghui Yue</author><pubDate>Fri, 14 Jun 2024 17:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10175v1</guid></item><item><title>Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation</title><link>http://arxiv.org/abs/2406.10174v1</link><description>The intersection between poetry and music provides an interesting case forcomputational creativity, yet remains relatively unexplored. This paperexplores the integration of poetry and music through the lens of beat patterns,investigating whether a byte-based language model can generate words that fitspecific beat patterns within the context of poetry. Drawing on earlierstudies, we developed a method to train a byte-based transformer model, ByT5,to align poems with beat patterns. The results demonstrate a high level of beatalignment while maintaining semantic coherence. Future work will aim to improvethe model's ability to create complete beat-aligned poems.</description><author>Mohamad Elzohbi, Richard Zhao</author><pubDate>Fri, 14 Jun 2024 17:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10174v1</guid></item><item><title>IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce</title><link>http://arxiv.org/abs/2406.10173v1</link><description>Enhancing Language Models' (LMs) ability to understand purchase intentions inE-commerce scenarios is crucial for their effective assistance in variousdownstream tasks. However, previous approaches that distill intentions from LMsoften fail to generate meaningful and human-centric intentions applicable inreal-world E-commerce contexts. This raises concerns about the truecomprehension and utilization of purchase intentions by LMs. In this paper, wepresent IntentionQA, a double-task multiple-choice question answering benchmarkto evaluate LMs' comprehension of purchase intentions in E-commerce.Specifically, LMs are tasked to infer intentions based on purchased productsand utilize them to predict additional purchases. IntentionQA consists of 4,360carefully curated problems across three difficulty levels, constructed using anautomated pipeline to ensure scalability on large E-commerce platforms. Humanevaluations demonstrate the high quality and low false-negative rate of ourbenchmark. Extensive experiments across 19 language models show that they stillstruggle with certain scenarios, such as understanding products and intentionsaccurately, jointly reasoning with products and intentions, and more, in whichthey fall far behind human performances. Our code and data are publiclyavailable at https://github.com/HKUST-KnowComp/IntentionQA.</description><author>Wenxuan Ding, Weiqi Wang, Sze Heng Douglas Kwok, Minghao Liu, Tianqing Fang, Jiaxin Bai, Junxian He, Yangqiu Song</author><pubDate>Fri, 14 Jun 2024 17:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10173v1</guid></item><item><title>Datasets for Multilingual Answer Sentence Selection</title><link>http://arxiv.org/abs/2406.10172v1</link><description>Answer Sentence Selection (AS2) is a critical task for designing effectiveretrieval-based Question Answering (QA) systems. Most advancements in AS2 focuson English due to the scarcity of annotated datasets for other languages. Thislack of resources prevents the training of effective AS2 models in differentlanguages, creating a performance gap between QA systems in English and otherlocales. In this paper, we introduce new high-quality datasets for AS2 in fiveEuropean languages (French, German, Italian, Portuguese, and Spanish), obtainedthrough supervised Automatic Machine Translation (AMT) of existing English AS2datasets such as ASNQ, WikiQA, and TREC-QA using a Large Language Model (LLM).We evaluated our approach and the quality of the translated datasets throughmultiple experiments with different Transformer architectures. The resultsindicate that our datasets are pivotal in producing robust and powerfulmultilingual AS2 models, significantly contributing to closing the performancegap between English and other languages.</description><author>Matteo Gabburo, Stefano Campese, Federico Agostini, Alessandro Moschitti</author><pubDate>Fri, 14 Jun 2024 17:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10172v1</guid></item><item><title>SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs</title><link>http://arxiv.org/abs/2405.16325v2</link><description>We propose SLoPe, a Double-Pruned Sparse Plus Lazy Low-rank AdapterPretraining method for LLMs that improves the accuracy of sparse LLMs whileaccelerating their pretraining and inference and reducing their memoryfootprint. Sparse pretraining of LLMs reduces the accuracy of the model, toovercome this, prior work uses dense models during fine-tuning. SLoPe improvesthe accuracy of sparsely pretrained models by adding low-rank adapters in thefinal 1% iterations of pretraining without adding significant overheads to themodel pretraining and inference. In addition, SLoPe uses a double-prunedbackward pass formulation that prunes the transposed weight matrix using N:Msparsity structures to enable an accelerated sparse backward pass. SLoPeaccelerates the training and inference of models with billions of parameters upto $1.14\times$ and $1.34\times$ respectively (OPT-33B and OPT-66B) whilereducing their memory usage by up to $0.77\times$ and $0.51\times$ for trainingand inference respectively.</description><author>Mohammad Mozaffari, Amir Yazdanbakhsh, Zhao Zhang, Maryam Mehri Dehnavi</author><pubDate>Fri, 14 Jun 2024 17:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16325v2</guid></item><item><title>Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights</title><link>http://arxiv.org/abs/2405.21070v2</link><description>Severe data imbalance naturally exists among web-scale vision-languagedatasets. Despite this, we find CLIP pre-trained thereupon exhibits notablerobustness to the data imbalance compared to supervised learning, anddemonstrates significant effectiveness in learning generalizablerepresentations. With an aim to investigate the reasons behind this finding, weconduct controlled experiments to study various underlying factors, and revealthat CLIP's pretext task forms a dynamic classification problem wherein only asubset of classes is present in training. This isolates the bias from dominantclasses and implicitly balances the learning signal. Furthermore, therobustness and discriminability of CLIP improve with more descriptive languagesupervision, larger data scale, and broader open-world concepts, which areinaccessible to supervised learning. Our study not only uncovers the mechanismsbehind CLIP's generalizability beyond data imbalance but also providestransferable insights for the research community. The findings are validated inboth supervised and self-supervised learning, enabling models trained onimbalanced data to achieve CLIP-level performance on diverse recognition tasks.Code and data are available at: https://github.com/CVMI-Lab/clip-beyond-tail.</description><author>Xin Wen, Bingchen Zhao, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi</author><pubDate>Fri, 14 Jun 2024 17:42:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.21070v2</guid></item><item><title>4DRecons: 4D Neural Implicit Deformable Objects Reconstruction from a single RGB-D Camera with Geometrical and Topological Regularizations</title><link>http://arxiv.org/abs/2406.10167v1</link><description>This paper presents a novel approach 4DRecons that takes a single cameraRGB-D sequence of a dynamic subject as input and outputs a complete textureddeforming 3D model over time. 4DRecons encodes the output as a 4D neuralimplicit surface and presents an optimization procedure that combines a dataterm and two regularization terms. The data term fits the 4D implicit surfaceto the input partial observations. We address fundamental challenges in fittinga complete implicit surface to partial observations. The first regularizationterm enforces that the deformation among adjacent frames is as rigid aspossible (ARAP). To this end, we introduce a novel approach to computecorrespondences between adjacent textured implicit surfaces, which are used todefine the ARAP regularization term. The second regularization term enforcesthat the topology of the underlying object remains fixed over time. Thisregularization is critical for avoiding self-intersections that are typical inimplicit-based reconstructions. We have evaluated the performance of 4DReconson a variety of datasets. Experimental results show that 4DRecons can handlelarge deformations and complex inter-part interactions and outperformstate-of-the-art approaches considerably.</description><author>Xiaoyan Cong, Haitao Yang, Liyan Chen, Kaifeng Zhang, Li Yi, Chandrajit Bajaj, Qixing Huang</author><pubDate>Fri, 14 Jun 2024 17:38:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10167v1</guid></item><item><title>Misam: Using ML in Dataflow Selection of Sparse-Sparse Matrix Multiplication</title><link>http://arxiv.org/abs/2406.10166v1</link><description>Sparse matrix-matrix multiplication (SpGEMM) is a critical operation innumerous fields, including scientific computing, graph analytics, and deeplearning. These applications exploit the sparsity of matrices to reduce storageand computational demands. However, the irregular structure of sparse matricesposes significant challenges for performance optimization. Traditional hardwareaccelerators are tailored for specific sparsity patterns with fixed dataflowschemes - inner, outer, and row-wise but often perform suboptimally when theactual sparsity deviates from these predetermined patterns. As the use ofSpGEMM expands across various domains, each with distinct sparsitycharacteristics, the demand for hardware accelerators that can efficientlyhandle a range of sparsity patterns is increasing. This paper presents amachine learning based approach for adaptively selecting the most appropriatedataflow scheme for SpGEMM tasks with diverse sparsity patterns. By employingdecision trees and deep reinforcement learning, we explore the potential ofthese techniques to surpass heuristic-based methods in identifying optimaldataflow schemes. We evaluate our models by comparing their performance withthat of a heuristic, highlighting the strengths and weaknesses of eachapproach. Our findings suggest that using machine learning for dynamic dataflowselection in hardware accelerators can provide upto 28 times gains.</description><author>Sanjali Yadav, Bahar Asgari</author><pubDate>Fri, 14 Jun 2024 17:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10166v1</guid></item><item><title>CarLLaVA: Vision language models for camera-only closed-loop driving</title><link>http://arxiv.org/abs/2406.10165v1</link><description>In this technical report, we present CarLLaVA, a Vision Language Model (VLM)for autonomous driving, developed for the CARLA Autonomous Driving Challenge2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMAarchitecture as backbone, achieving state-of-the-art closed-loop drivingperformance with only camera input and without the need for complex orexpensive labels. Additionally, we show preliminary results on predictinglanguage commentary alongside the driving output. CarLLaVA uses asemi-disentangled output representation of both path predictions and waypoints,getting the advantages of the path for better lateral control and the waypointsfor better longitudinal control. We propose an efficient training recipe totrain on large driving datasets without wasting compute on easy, trivial data.CarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous DrivingChallenge 2.0 outperforming the previous state of the art by 458% and the bestconcurrent submission by 32.6%.</description><author>Katrin Renz, Long Chen, Ana-Maria Marcu, Jan Hünermann, Benoit Hanotte, Alice Karnsund, Jamie Shotton, Elahe Arani, Oleg Sinavski</author><pubDate>Fri, 14 Jun 2024 17:35:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10165v1</guid></item><item><title>MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers</title><link>http://arxiv.org/abs/2406.10163v1</link><description>Recently, 3D assets created via reconstruction and generation have matchedthe quality of manually crafted assets, highlighting their potential forreplacement. However, this potential is largely unrealized because these assetsalways need to be converted to meshes for 3D industry applications, and themeshes produced by current mesh extraction methods are significantly inferiorto Artist-Created Meshes (AMs), i.e., meshes created by human artists.Specifically, current mesh extraction methods rely on dense faces and ignoregeometric features, leading to inefficiencies, complicated post-processing, andlower representation quality. To address these issues, we introduceMeshAnything, a model that treats mesh extraction as a generation problem,producing AMs aligned with specified shapes. By converting 3D assets in any 3Drepresentation into AMs, MeshAnything can be integrated with various 3D assetproduction methods, thereby enhancing their application across the 3D industry.The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioneddecoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE,then train the shape-conditioned decoder-only transformer on this vocabularyfor shape-conditioned autoregressive mesh generation. Our extensive experimentsshow that our method generates AMs with hundreds of times fewer faces,significantly improving storage, rendering, and simulation efficiencies, whileachieving precision comparable to previous methods.</description><author>Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, Chi Zhang</author><pubDate>Fri, 14 Jun 2024 17:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10163v1</guid></item><item><title>Towards the TopMost: A Topic Modeling System Toolkit</title><link>http://arxiv.org/abs/2309.06908v2</link><description>Topic models have a rich history with various applications and have recentlybeen reinvigorated by neural topic modeling. However, these numerous topicmodels adopt totally distinct datasets, implementations, and evaluations. Thisimpedes quick utilization and fair comparisons, and thereby hinders theirresearch progress and applications. To tackle this challenge, we in this paperpropose a Topic Modeling System Toolkit (TopMost). Compared to existingtoolkits, TopMost stands out by supporting more extensive features. It covers abroader spectrum of topic modeling scenarios with their complete lifecycles,including datasets, preprocessing, models, training, and evaluations. Thanks toits highly cohesive and decoupled modular design, TopMost enables rapidutilization, fair comparisons, and flexible extensions of diverse cutting-edgetopic models. Our code, tutorials, and documentation are available athttps://github.com/bobxwu/topmost.</description><author>Xiaobao Wu, Fengjun Pan, Anh Tuan Luu</author><pubDate>Fri, 14 Jun 2024 17:27:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06908v2</guid></item><item><title>Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models</title><link>http://arxiv.org/abs/2406.10162v1</link><description>In reinforcement learning, specification gaming occurs when AI systems learnundesired behaviors that are highly rewarded due to misspecified traininggoals. Specification gaming can range from simple behaviors like sycophancy tosophisticated and pernicious behaviors like reward-tampering, where a modeldirectly modifies its own reward mechanism. However, these more perniciousbehaviors may be too complex to be discovered via exploration. In this paper,we study whether Large Language Model (LLM) assistants which find easilydiscovered forms of specification gaming will generalize to perform rarer andmore blatant forms, up to and including reward-tampering. We construct acurriculum of increasingly sophisticated gameable environments and find thattraining on early-curriculum environments leads to more specification gaming onremaining environments. Strikingly, a small but non-negligible proportion ofthe time, LLM assistants trained on the full curriculum generalize zero-shot todirectly rewriting their own reward function. Retraining an LLM not to gameearly-curriculum environments mitigates, but does not eliminate,reward-tampering in later environments. Moreover, adding harmlessness trainingto our gameable environments does not prevent reward-tampering. These resultsdemonstrate that LLMs can generalize from common forms of specification gamingto more pernicious reward tampering and that such behavior may be nontrivial toremove.</description><author>Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez, Evan Hubinger</author><pubDate>Fri, 14 Jun 2024 17:26:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10162v1</guid></item><item><title>Context-Aware Prediction of User Engagement on Online Social Platforms</title><link>http://arxiv.org/abs/2310.14533v2</link><description>The success of online social platforms hinges on their ability to predict andunderstand user behavior at scale. Here, we present data suggesting thatcontext-aware modeling approaches may offer a holistic yet lightweight andpotentially privacy-preserving representation of user engagement on onlinesocial platforms. Leveraging deep LSTM neural networks to analyze more than 100million Snapchat sessions from almost 80.000 users, we demonstrate thatpatterns of active and passive use are predictable from past behavior(R2=0.345) and that the integration of context features substantially improvespredictive performance compared to the behavioral baseline model (R2=0.522).Features related to smartphone connectivity status, location, temporal context,and weather were found to capture non-redundant variance in user engagementrelative to features derived from histories of in-app behaviors. Further, weshow that a large proportion of variance can be accounted for with minimalbehavioral histories if momentary context is considered (R2=0.442). Theseresults indicate the potential of context-aware approaches for making modelsmore efficient and privacy-preserving by reducing the need for long datahistories. Finally, we employ model explainability techniques to gleanpreliminary insights into the underlying behavioral mechanisms. Our findingsare consistent with the notion of context-contingent, habit-driven patterns ofactive and passive use, underscoring the value of contextualizedrepresentations of user behavior for predicting user engagement on socialplatforms.</description><author>Heinrich Peters, Yozen Liu, Francesco Barbieri, Raiyan Abdul Baten, Sandra C. Matz, Maarten W. Bos</author><pubDate>Fri, 14 Jun 2024 17:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14533v2</guid></item><item><title>NeuralClothSim: Neural Deformation Fields Meet the Thin Shell Theory</title><link>http://arxiv.org/abs/2308.12970v2</link><description>Despite existing 3D cloth simulators producing realistic results, theypredominantly operate on discrete surface representations (e.g. points andmeshes) with a fixed spatial resolution, which often leads to large memoryconsumption and resolution-dependent simulations. Moreover, back-propagatinggradients through the existing solvers is difficult, and they cannot be easilyintegrated into modern neural architectures. In response, this paper re-thinksphysically plausible cloth simulation: We propose NeuralClothSim, i.e., a newquasistatic cloth simulator using thin shells, in which surface deformation isencoded in neural network weights in the form of a neural field. Ourmemory-efficient solver operates on a new continuous coordinate-based surfacerepresentation called neural deformation fields (NDFs); it supervises NDFequilibria with the laws of the non-linear Kirchhoff-Love shell theory with anon-linear anisotropic material model. NDFs are adaptive: They 1) allocatetheir capacity to the deformation details and 2) allow surface state queries atarbitrary spatial resolutions without re-training. We show how to trainNeuralClothSim while imposing hard boundary conditions and demonstrate multipleapplications, such as material interpolation and simulation editing. Theexperimental results highlight the effectiveness of our continuous neuralformulation.</description><author>Navami Kairanda, Marc Habermann, Christian Theobalt, Vladislav Golyanik</author><pubDate>Fri, 14 Jun 2024 17:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12970v2</guid></item><item><title>On the Computability of Robust PAC Learning</title><link>http://arxiv.org/abs/2406.10161v1</link><description>We initiate the study of computability requirements for adversarially robustlearning. Adversarially robust PAC-type learnability is by now an establishedfield of research. However, the effects of computability requirements inPAC-type frameworks are only just starting to emerge. We introduce the problemof robust computable PAC (robust CPAC) learning and provide some simplesufficient conditions for this. We then show that learnability in this setup isnot implied by the combination of its components: classes that are both CPACand robustly PAC learnable are not necessarily robustly CPAC learnable.Furthermore, we show that the novel framework exhibits some surprising effects:for robust CPAC learnability it is not required that the robust loss iscomputably evaluable! Towards understanding characterizing properties, weintroduce a novel dimension, the computable robust shattering dimension. Weprove that its finiteness is necessary, but not sufficient for robust CPAClearnability. This might yield novel insights for the corresponding phenomenonin the context of robust PAC learnability, where insufficiency of the robustshattering dimension for learnability has been conjectured, but so far aresolution has remained elusive.</description><author>Pascale Gourdeau, Tosca Lechner, Ruth Urner</author><pubDate>Fri, 14 Jun 2024 17:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10161v1</guid></item><item><title>One-pass Multiple Conformer and Foundation Speech Systems Compression and Quantization Using An All-in-one Neural Model</title><link>http://arxiv.org/abs/2406.10160v1</link><description>We propose a novel one-pass multiple ASR systems joint compression andquantization approach using an all-in-one neural model. A single compressioncycle allows multiple nested systems with varying Encoder depths, widths, andquantization precision settings to be simultaneously constructed without theneed to train and store individual target systems separately. Experimentsconsistently demonstrate the multiple ASR systems compressed in a singleall-in-one model produced a word error rate (WER) comparable to, or lower by upto 1.01\% absolute (6.98\% relative) than individually trained systems of equalcomplexity. A 3.4x overall system compression and training time speed-up wasachieved. Maximum model size compression ratios of 12.8x and 3.93x wereobtained over the baseline Switchboard-300hr Conformer and LibriSpeech-100hrfine-tuned wav2vec2.0 models, respectively, incurring no statisticallysignificant WER increase.</description><author>Zhaoqing Li, Haoning Xu, Tianzi Wang, Shoukang Hu, Zengrui Jin, Shujie Hu, Jiajun Deng, Mingyu Cui, Mengzhe Geng, Xunying Liu</author><pubDate>Fri, 14 Jun 2024 17:18:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10160v1</guid></item><item><title>RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model</title><link>http://arxiv.org/abs/2406.10157v1</link><description>Minigolf, a game with countless court layouts, and complex ball motion,constitutes a compelling real-world testbed for the study of embodiedintelligence. As it not only challenges spatial and kinodynamic reasoning butalso requires reflective and corrective capacities to address erroneouslydesigned courses. We introduce RoboGolf, a framework that perceives dual-cameravisual inputs with nested VLM-empowered closed-loop control and reflectiveequilibrium loop. Extensive experiments demonstrate the effectiveness ofRoboGolf on challenging minigolf courts including those that are impossible tofinish.</description><author>Hantao Zhou, Tianying Ji, Jianwei Zhang, Fuchun Sun, Huazhe Xu</author><pubDate>Fri, 14 Jun 2024 17:16:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10157v1</guid></item><item><title>Automated Design of Linear Bounding Functions for Sigmoidal Nonlinearities in Neural Networks</title><link>http://arxiv.org/abs/2406.10154v1</link><description>The ubiquity of deep learning algorithms in various applications hasamplified the need for assuring their robustness against small inputperturbations such as those occurring in adversarial attacks. Existing completeverification techniques offer provable guarantees for all robustness queriesbut struggle to scale beyond small neural networks. To overcome thiscomputational intractability, incomplete verification methods often rely onconvex relaxation to over-approximate the nonlinearities in neural networks.Progress in tighter approximations has been achieved for piecewise linearfunctions. However, robustness verification of neural networks for generalactivation functions (e.g., Sigmoid, Tanh) remains under-explored and poses newchallenges. Typically, these networks are verified using convex relaxationtechniques, which involve computing linear upper and lower bounds of thenonlinear activation functions. In this work, we propose a novel parametersearch method to improve the quality of these linear approximations.Specifically, we show that using a simple search method, carefully adapted tothe given verification problem through state-of-the-art algorithm configurationtechniques, improves the average global lower bound by 25% on average over thecurrent state of the art on several commonly used local robustness verificationbenchmarks.</description><author>Matthias König, Xiyue Zhang, Holger H. Hoos, Marta Kwiatkowska, Jan N. van Rijn</author><pubDate>Fri, 14 Jun 2024 17:16:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10154v1</guid></item><item><title>Score-Aware Policy-Gradient Methods and Performance Guarantees using Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks and Queueing Systems</title><link>http://arxiv.org/abs/2312.02804v2</link><description>In this paper, we introduce a policy-gradient method for model-basedreinforcement learning (RL) that exploits a type of stationary distributionscommonly obtained from Markov decision processes (MDPs) in stochastic networks,queueing systems, and statistical mechanics. Specifically, when the stationarydistribution of the MDP belongs to an exponential family that is parametrizedby policy parameters, we can improve existing policy gradient methods foraverage-reward RL. Our key identification is a family of gradient estimators,called score-aware gradient estimators (SAGEs), that enable policy gradientestimation without relying on value-function approximation in theaforementioned setting. This contrasts with other common policy-gradientalgorithms such as actor-critic methods. We first show that policy-gradientwith SAGE locally converges, including in cases when the objective function isnonconvex, presents multiple maximizers, and the state space of the MDP is notfinite. Under appropriate assumptions such as starting sufficiently close to amaximizer, the policy under stochastic gradient ascent with SAGE has anoverwhelming probability of converging to the associated optimal policy. Otherkey assumptions are that a local Lyapunov function exists, and a nondegeneracyproperty of the Hessian of the objective function holds locally around amaximizer. Furthermore, we conduct a numerical comparison between a SAGE-basedpolicy-gradient method and an actor-critic method. We specifically focus onseveral examples inspired from stochastic networks, queueing systems, andmodels derived from statistical physics, where parametrizable exponentialfamilies are commonplace. Our results demonstrate that a SAGE-based methodfinds close-to-optimal policies faster than an actor-critic method.</description><author>Céline Comte, Matthieu Jonckheere, Jaron Sanders, Albert Senen-Cerda</author><pubDate>Fri, 14 Jun 2024 17:10:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02804v2</guid></item><item><title>BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack</title><link>http://arxiv.org/abs/2406.10149v1</link><description>In recent years, the input context sizes of large language models (LLMs) haveincreased dramatically. However, existing evaluation methods have not keptpace, failing to comprehensively assess the efficiency of models in handlinglong contexts. To bridge this gap, we introduce the BABILong benchmark,designed to test language models' ability to reason across facts distributed inextremely long documents. BABILong includes a diverse set of 20 reasoningtasks, including fact chaining, simple induction, deduction, counting, andhandling lists/sets. These tasks are challenging on their own, and even moredemanding when the required facts are scattered across long natural text. Ourevaluations show that popular LLMs effectively utilize only 10-20\% of thecontext and their performance declines sharply with increased reasoningcomplexity. Among alternatives to in-context reasoning, Retrieval-AugmentedGeneration methods achieve a modest 60\% accuracy on single-fact questionanswering, independent of context length. Among context extension methods, thehighest performance is demonstrated by recurrent memory transformers, enablingthe processing of lengths up to 11 million tokens. The BABILong benchmark isextendable to any length to support the evaluation of new upcoming models withincreased capabilities, and we provide splits up to 1 million token lengths.</description><author>Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev</author><pubDate>Fri, 14 Jun 2024 17:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10149v1</guid></item><item><title>Future Directions in the Theory of Graph Machine Learning</title><link>http://arxiv.org/abs/2402.02287v4</link><description>Machine learning on graphs, especially using graph neural networks (GNNs),has seen a surge in interest due to the wide availability of graph data acrossa broad spectrum of disciplines, from life to social and engineering sciences.Despite their practical success, our theoretical understanding of theproperties of GNNs remains highly incomplete. Recent theoretical advancementsprimarily focus on elucidating the coarse-grained expressive power of GNNs,predominantly employing combinatorial techniques. However, these studies do notperfectly align with practice, particularly in understanding the generalizationbehavior of GNNs when trained with stochastic first-order optimizationtechniques. In this position paper, we argue that the graph machine learningcommunity needs to shift its attention to developing a balanced theory of graphmachine learning, focusing on a more thorough understanding of the interplay ofexpressive power, generalization, and optimization.</description><author>Christopher Morris, Fabrizio Frasca, Nadav Dym, Haggai Maron, İsmail İlkan Ceylan, Ron Levie, Derek Lim, Michael Bronstein, Martin Grohe, Stefanie Jegelka</author><pubDate>Fri, 14 Jun 2024 16:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02287v4</guid></item><item><title>Improving rule mining via embedding-based link prediction</title><link>http://arxiv.org/abs/2406.10144v1</link><description>Rule mining on knowledge graphs allows for explainable link prediction.Contrarily, embedding-based methods for link prediction are well known fortheir generalization capabilities, but their predictions are not interpretable.Several approaches combining the two families have been proposed in recentyears. The majority of the resulting hybrid approaches are usually trainedwithin a unified learning framework, which often leads to convergence issuesdue to the complexity of the learning task. In this work, we propose a new wayto combine the two families of approaches. Specifically, we enrich a givenknowledge graph by means of its pre-trained entity and relation embeddingsbefore applying rule mining systems on the enriched knowledge graph. Tovalidate our approach, we conduct extensive experiments on seven benchmarkdatasets. An analysis of the results generated by our approach suggests that wediscover new valuable rules on the enriched graphs. We provide an open sourceimplementation of our approach as well as pretrained models and datasets athttps://github.com/Jean-KOUAGOU/EnhancedRuleLearning</description><author>N'Dah Jean Kouagou, Arif Yilmaz, Michel Dumontier, Axel-Cyrille Ngonga Ngomo</author><pubDate>Fri, 14 Jun 2024 16:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10144v1</guid></item><item><title>Plug-and-Play Diffusion Distillation</title><link>http://arxiv.org/abs/2406.01954v2</link><description>Diffusion models have shown tremendous results in image generation. However,due to the iterative nature of the diffusion process and its reliance onclassifier-free guidance, inference times are slow. In this paper, we propose anew distillation approach for guided diffusion models in which an externallightweight guide model is trained while the original text-to-image modelremains frozen. We show that our method reduces the inference computation ofclassifier-free guided latent-space diffusion models by almost half, and onlyrequires 1\% trainable parameters of the base model. Furthermore, once trained,our guide model can be applied to various fine-tuned, domain-specific versionsof the base diffusion model without the need for additional training: this"plug-and-play" functionality drastically improves inference computation whilemaintaining the visual fidelity of generated images. Empirically, we show thatour approach is able to produce visually appealing results and achieve acomparable FID score to the teacher with as few as 8 to 16 steps.</description><author>Yi-Ting Hsiao, Siavash Khodadadeh, Kevin Duarte, Wei-An Lin, Hui Qu, Mingi Kwon, Ratheesh Kalarot</author><pubDate>Fri, 14 Jun 2024 16:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01954v2</guid></item><item><title>A kinetic approach to consensus-based segmentation of biomedical images</title><link>http://arxiv.org/abs/2211.05226v2</link><description>In this work, we apply a kinetic version of a bounded confidence consensusmodel to biomedical segmentation problems. In the presented approach,time-dependent information on the microscopic state of each particle/pixelincludes its space position and a feature representing a static characteristicof the system, i.e. the gray level of each pixel. From the introducedmicroscopic model we derive a kinetic formulation of the model. The large timebehavior of the system is then computed with the aid of a surrogateFokker-Planck approach that can be obtained in the quasi-invariant scaling. Weexploit the computational efficiency of direct simulation Monte Carlo methodsfor the obtained Boltzmann-type description of the problem for parameteridentification tasks. Based on a suitable loss function measuring the distancebetween the ground truth segmentation mask and the evaluated mask, we minimizethe introduced segmentation metric for a relevant set of 2D gray-scale images.Applications to biomedical segmentation concentrate on different imagingresearch contexts.</description><author>Raffaella Fiamma Cabini, Anna Pichiecchio, Alessandro Lascialfari, Silvia Figini, Mattia Zanella</author><pubDate>Fri, 14 Jun 2024 16:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05226v2</guid></item><item><title>Federated Learning Across Decentralized and Unshared Archives for Remote Sensing Image Classification</title><link>http://arxiv.org/abs/2311.06141v3</link><description>Federated learning (FL) enables the collaboration of multiple deep learningmodels to learn from decentralized data archives (i.e., clients) withoutaccessing data on clients. Although FL offers ample opportunities in knowledgediscovery from distributed image archives, it is seldom considered in remotesensing (RS). In this paper, as a first time in RS, we present a comparativestudy of state-of-the-art FL algorithms for RS image classification problems.To this end, we initially provide a systematic review of the FL algorithmspresented in the computer vision and machine learning communities. Then, weselect several state-of-the-art FL algorithms based on their effectiveness withrespect to training data heterogeneity across clients (known as non-IID data).After presenting an extensive overview of the selected algorithms, atheoretical comparison of the algorithms is conducted based on their: 1) localtraining complexity; 2) aggregation complexity; 3) learning efficiency; 4)communication cost; and 5) scalability in terms of number of clients. After thetheoretical comparison, experimental analyses are presented to compare themunder different decentralization scenarios. For the experimental analyses, wefocus our attention on multi-label image classification problems in RS. Basedon our comprehensive analyses, we finally derive a guideline for selectingsuitable FL algorithms in RS. The code of this work is publicly available athttps://git.tu-berlin.de/rsim/FL-RS.</description><author>Barış Büyüktaş, Gencer Sumbul, Begüm Demir</author><pubDate>Fri, 14 Jun 2024 16:52:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06141v3</guid></item><item><title>The Rise and Fall(?) of Software Engineering</title><link>http://arxiv.org/abs/2406.10141v1</link><description>Over the last ten years, the realm of Artificial Intelligence (AI) hasexperienced an explosion of revolutionary breakthroughs, transforming whatseemed like a far-off dream into a reality that is now deeply embedded in oureveryday lives. AI's widespread impact is revolutionizing virtually all aspectsof human life, and software engineering (SE) is no exception. As we explore this changing landscape, we are faced with questions about whatthe future holds for SE and how AI will reshape the roles, duties, andmethodologies within the field. The introduction of these groundbreakingtechnologies highlights the inevitable shift towards a new paradigm, suggestinga future where AI's capabilities may redefine the boundaries of SE, potentiallyeven more than human input. In this paper, we aim at outlining the key elements that, based on ourexpertise, are vital for the smooth integration of AI into SE, all whilepreserving the intrinsic human creativity that has been the driving forcebehind the field. First, we provide a brief description of SE and AI evolution.Afterward, we delve into the intricate interplay between AI-driven automationand human innovation, exploring how these two components can work together toadvance SE practices to new methods and standards.</description><author>Antonio Mastropaolo, Camilo Escobar-Velásquez, Mario Linares-Vásquez</author><pubDate>Fri, 14 Jun 2024 16:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10141v1</guid></item><item><title>YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their application in the agricultural domain</title><link>http://arxiv.org/abs/2406.10139v1</link><description>This survey investigates the transformative potential of various YOLOvariants, from YOLOv1 to the state-of-the-art YOLOv10, in the context ofagricultural advancements. The primary objective is to elucidate how thesecutting-edge object detection models can re-energise and optimize diverseaspects of agriculture, ranging from crop monitoring to livestock management.It aims to achieve key objectives, including the identification of contemporarychallenges in agriculture, a detailed assessment of YOLO's incrementaladvancements, and an exploration of its specific applications in agriculture.This is one of the first surveys to include the latest YOLOv10, offering afresh perspective on its implications for precision farming and sustainableagricultural practices in the era of Artificial Intelligence and automation.Further, the survey undertakes a critical analysis of YOLO's performance,synthesizes existing research, and projects future trends. By scrutinizing theunique capabilities packed in YOLO variants and their real-world applications,this survey provides valuable insights into the evolving relationship betweenYOLO variants and agriculture. The findings contribute towards a nuancedunderstanding of the potential for precision farming and sustainableagricultural practices, marking a significant step forward in the integrationof advanced object detection technologies within the agricultural sector.</description><author>Mujadded Al Rabbani Alif, Muhammad Hussain</author><pubDate>Fri, 14 Jun 2024 16:48:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10139v1</guid></item><item><title>Compressed Sensor Caching and Collaborative Sparse Data Recovery with Anchor Alignment</title><link>http://arxiv.org/abs/2406.10137v1</link><description>This work examines the compressed sensor caching problem in wireless sensornetworks and devises efficient distributed sparse data recovery algorithms toenable collaboration among multiple caches. In this problem, each cache is onlyallowed to access measurements from a small subset of sensors within itsvicinity to reduce both cache size and data acquisition overhead. To enablereliable data recovery with limited access to measurements, we propose adistributed sparse data recovery method, called the collaborative sparserecovery by anchor alignment (CoSR-AA) algorithm, where collaboration amongcaches is enabled by aligning their locally recovered data at a few anchornodes. The proposed algorithm is based on the consensus alternating directionmethod of multipliers (ADMM) algorithm but with message exchange that isreduced by considering the proposed anchor alignment strategy. Then, by thedeep unfolding of the ADMM iterations, we further propose the Deep CoSR-AAalgorithm that can be used to significantly reduce the number of iterations. Weobtain a graph neural network architecture where message exchange is done moreefficiently by an embedded autoencoder. Simulations are provided to demonstratethe effectiveness of the proposed collaborative recovery algorithms in terms ofthe improved reconstruction quality and the reduced communication overhead dueto anchor alignment.</description><author>Yi-Jen Yang, Ming-Hsun Yang, Jwo-Yuh Wu, Y. -W. Peter Hong</author><pubDate>Fri, 14 Jun 2024 16:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10137v1</guid></item><item><title>Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy</title><link>http://arxiv.org/abs/2305.11616v4</link><description>Deep ensembles are capable of achieving state-of-the-art results inclassification and out-of-distribution (OOD) detection. However, theireffectiveness is limited due to the homogeneity of learned patterns withinensembles. To overcome this issue, our study introduces Saliency DiversifiedDeep Ensemble (SDDE), a novel approach that promotes diversity among ensemblemembers by leveraging saliency maps. Through incorporating saliency mapdiversification, our method outperforms conventional ensemble techniques andimproves calibration in multiple classification and OOD detection tasks. Inparticular, the proposed method achieves state-of-the-art OOD detectionquality, calibration, and accuracy on multiple benchmarks, includingCIFAR10/100 and large-scale ImageNet datasets.</description><author>Stanislav Dereka, Ivan Karpukhin, Maksim Zhdanov, Sergey Kolesnikov</author><pubDate>Fri, 14 Jun 2024 16:46:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11616v4</guid></item><item><title>Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation</title><link>http://arxiv.org/abs/2405.07200v3</link><description>Accurate approximation of complex nonlinear functions is a fundamentalchallenge across many scientific and engineering domains. Traditional neuralnetwork architectures, such as Multi-Layer Perceptrons (MLPs), often struggleto efficiently capture intricate patterns and irregularities present inhigh-dimensional functions. This paper presents the Chebyshev Kolmogorov-ArnoldNetwork (Chebyshev KAN), a new neural network architecture inspired by theKolmogorov-Arnold representation theorem, incorporating the powerfulapproximation capabilities of Chebyshev polynomials. By utilizing learnablefunctions parametrized by Chebyshev polynomials on the network's edges,Chebyshev KANs enhance flexibility, efficiency, and interpretability infunction approximation tasks. We demonstrate the efficacy of Chebyshev KANsthrough experiments on digit classification, synthetic function approximation,and fractal function generation, highlighting their superiority overtraditional MLPs in terms of parameter efficiency and interpretability. Ourcomprehensive evaluation, including ablation studies, confirms the potential ofChebyshev KANs to address longstanding challenges in nonlinear functionapproximation, paving the way for further advancements in various scientificand engineering applications.</description><author>Sidharth SS, Keerthana AR, Gokul R, Anas KP</author><pubDate>Fri, 14 Jun 2024 16:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07200v3</guid></item><item><title>Evaluation of Large Language Models: STEM education and Gender Stereotypes</title><link>http://arxiv.org/abs/2406.10133v1</link><description>Large Language Models (LLMs) have an increasing impact on our lives with usecases such as chatbots, study support, coding support, ideation, writingassistance, and more. Previous studies have revealed linguistic biases inpronouns used to describe professions or adjectives used to describe men vswomen. These issues have to some degree been addressed in updated LLM versions,at least to pass existing tests. However, biases may still be present in themodels, and repeated use of gender stereotypical language may reinforce theunderlying assumptions and are therefore important to examine further. Thispaper investigates gender biases in LLMs in relation to educational choicesthrough an open-ended, true to user-case experimental design and a quantitativeanalysis. We investigate the biases in the context of four different cultures,languages, and educational systems (English/US/UK, Danish/DK, Catalan/ES, andHindi/IN) for ages ranging from 10 to 16 years, corresponding to importanteducational transition points in the different countries. We find that thereare significant and large differences in the ratio of STEM to non-STEMsuggested education paths provided by chatGPT when using typical girl vs boynames to prompt lists of suggested things to become. There are generally fewerSTEM suggestions in the Danish, Spanish, and Indian context compared to theEnglish. We also find subtle differences in the suggested professions, which wecategorise and report.</description><author>Smilla Due, Sneha Das, Marianne Andersen, Berta Plandolit López, Sniff Andersen Nexø, Line Clemmensen</author><pubDate>Fri, 14 Jun 2024 16:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10133v1</guid></item><item><title>Linear Contextual Bandits with Hybrid Payoff: Revisited</title><link>http://arxiv.org/abs/2406.10131v1</link><description>We study the Linear Contextual Bandit problem in the hybrid reward setting.In this setting every arm's reward model contains arm specific parameters inaddition to parameters shared across the reward models of all the arms. We canreduce this setting to two closely related settings (a) Shared - no armspecific parameters, and (b) Disjoint - only arm specific parameters, enablingthe application of two popular state of the art algorithms - $\texttt{LinUCB}$and $\texttt{DisLinUCB}$ (Algorithm 1 in (Li et al. 2010)). When the armfeatures are stochastic and satisfy a popular diversity condition, we providenew regret analyses for both algorithms, significantly improving on the knownregret guarantees of these algorithms. Our novel analysis critically exploitsthe hybrid reward structure and the diversity condition. Moreover, we introducea new algorithm $\texttt{HyLinUCB}$ that crucially modifies $\texttt{LinUCB}$(using a new exploration coefficient) to account for sparsity in the hybridsetting. Under the same diversity assumptions, we prove that$\texttt{HyLinUCB}$ also incurs only $O(\sqrt{T})$ regret for $T$ rounds. Weperform extensive experiments on synthetic and real-world datasetsdemonstrating strong empirical performance of $\texttt{HyLinUCB}$.For number ofarm specific parameters much larger than the number of shared parameters, weobserve that $\texttt{DisLinUCB}$ incurs the lowest regret. In this case,regret of $\texttt{HyLinUCB}$ is the second best and extremely competitive to$\texttt{DisLinUCB}$. In all other situations, including our real-worlddataset, $\texttt{HyLinUCB}$ has significantly lower regret than$\texttt{LinUCB}$, $\texttt{DisLinUCB}$ and other SOTA baselines we considered.We also empirically observe that the regret of $\texttt{HyLinUCB}$ grows muchslower with the number of arms compared to baselines, making it suitable evenfor very large action spaces.</description><author>Nirjhar Das, Gaurav Sinha</author><pubDate>Fri, 14 Jun 2024 16:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10131v1</guid></item><item><title>The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models</title><link>http://arxiv.org/abs/2406.10130v1</link><description>Pre-trained Language models (PLMs) have been acknowledged to contain harmfulinformation, such as social biases, which may cause negative social impacts oreven bring catastrophic results in application. Previous works on this problemmainly focused on using black-box methods such as probing to detect andquantify social biases in PLMs by observing model outputs. As a result,previous debiasing methods mainly finetune or even pre-train language models onnewly constructed anti-stereotypical datasets, which are high-cost. In thiswork, we try to unveil the mystery of social bias inside language models byintroducing the concept of {\sc Social Bias Neurons}. Specifically, we propose{\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e.,neurons) in a language model that can be attributed to undesirable behavior,such as social bias. By formalizing undesirable behavior as a distributionalproperty of language, we employ sentiment-bearing prompts to elicit classes ofsensitive words (demographics) correlated with such sentiments. Our IG$^2$ thusattributes the uneven distribution for different demographics to specificSocial Bias Neurons, which track the trail of unwanted behavior inside PLMunits to achieve interoperability. Moreover, derived from our interpretabletechnique, {\sc Bias Neuron Suppression (BNS)} is further proposed to mitigatesocial biases. By studying BERT, RoBERTa, and their attributable differencesfrom debiased FairBERTa, IG$^2$ allows us to locate and suppress identifiedneurons, and further mitigate undesired behaviors. As measured by prior metricsfrom StereoSet, our model achieves a higher degree of fairness whilemaintaining language modeling ability with low cost.</description><author>Yan Liu, Yu Liu, Xiaokang Chen, Pin-Yu Chen, Daoguang Zan, Min-Yen Kan, Tsung-Yi Ho</author><pubDate>Fri, 14 Jun 2024 16:41:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10130v1</guid></item><item><title>TrustSQL: Benchmarking Text-to-SQL Reliability with Penalty-Based Scoring</title><link>http://arxiv.org/abs/2403.15879v4</link><description>Text-to-SQL enables users to interact with databases using natural language,simplifying the retrieval and synthesis of information. Despite the remarkablesuccess of large language models (LLMs) in translating natural languagequestions into SQL queries, widespread deployment remains limited due to twoprimary challenges. First, the effective use of text-to-SQL models depends onusers' understanding of the model's capabilities-the scope of questions themodel can correctly answer. Second, the absence of abstention mechanisms canlead to incorrect SQL generation going unnoticed, thereby undermining trust inthe model's output. To enable wider deployment, it is crucial to address thesechallenges in model design and enhance model evaluation to build trust in themodel's output. To this end, we introduce TrustSQL, a novel comprehensivebenchmark designed to evaluate text-to-SQL reliability-defined as a model'sability to correctly handle any type of input question by generating correctSQL queries for feasible questions and abstaining from generating infeasibleones (e.g., due to schema incompatibility or functionalities beyond SQL). Weevaluate existing methods using a novel penalty-based scoring metric with twomodeling approaches: (1) pipeline-based methods combining SQL generators withinfeasible question detectors and SQL error detectors for abstention; and (2)unified methods using a single model for the entire task. Our experimentalresults reveal that achieving high scores under severe penalties requiressignificant effort and provide a new perspective on developing text-to-SQLmodels for safer deployment.</description><author>Gyubok Lee, Woosog Chay, Seonhee Cho, Edward Choi</author><pubDate>Fri, 14 Jun 2024 16:39:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15879v4</guid></item><item><title>SmartRSD: An Intelligent Multimodal Approach to Real-Time Road Surface Detection for Safe Driving</title><link>http://arxiv.org/abs/2406.10128v1</link><description>Precise and prompt identification of road surface conditions enables vehiclesto adjust their actions, like changing speed or using specific traction controltechniques, to lower the chance of accidents and potential danger to driversand pedestrians. However, most of the existing methods for detecting roadsurfaces solely rely on visual data, which may be insufficient in certainsituations, such as when the roads are covered by debris, in low lightconditions, or in the presence of fog. Therefore, we introduce a multimodalapproach for the automated detection of road surface conditions by integratingaudio and images. The robustness of the proposed method is tested on a diversedataset collected under various environmental conditions and road surfacetypes. Through extensive evaluation, we demonstrate the effectiveness andreliability of our multimodal approach in accurately identifying road surfaceconditions in real-time scenarios. Our findings highlight the potential ofintegrating auditory and visual cues for enhancing road safety and minimizingaccident risks</description><author>Adnan Md Tayeb, Mst Ayesha Khatun, Mohtasin Golam, Md Facklasur Rahaman, Ali Aouto, Oroceo Paul Angelo, Minseon Lee, Dong-Seong Kim, Jae-Min Lee, Jung-Hyeon Kim</author><pubDate>Fri, 14 Jun 2024 16:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10128v1</guid></item><item><title>Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images</title><link>http://arxiv.org/abs/2310.13876v2</link><description>Object detection in Remote Sensing Images (RSI) is a critical task fornumerous applications in Earth Observation (EO). Differing from objectdetection in natural images, object detection in remote sensing images faceschallenges of scarcity of annotated data and the presence of small objectsrepresented by only a few pixels. Multi-modal fusion has been determined toenhance the accuracy by fusing data from multiple modalities such as RGB,infrared (IR), lidar, and synthetic aperture radar (SAR). To this end, thefusion of representations at the mid or late stage, produced by parallelsubnetworks, is dominant, with the disadvantages of increasing computationalcomplexity in the order of the number of modalities and the creation ofadditional engineering obstacles. Using the cross-attention mechanism, wepropose a novel multi-modal fusion strategy for mapping relationships betweendifferent channels at the early stage, enabling the construction of a coherentinput by aligning the different modalities. By addressing fusion in the earlystage, as opposed to mid or late-stage methods, our method achieves competitiveand even superior performance compared to existing techniques. Additionally, weenhance the SWIN transformer by integrating convolution layers into thefeed-forward of non-shifting blocks. This augmentation strengthens the model'scapacity to merge separated windows through local attention, thereby improvingsmall object detection. Extensive experiments prove the effectiveness of theproposed multimodal fusion module and the architecture, demonstrating theirapplicability to object detection in multimodal aerial imagery.</description><author>Bissmella Bahaduri, Zuheng Ming, Fangchen Feng, Anissa Mokraou</author><pubDate>Fri, 14 Jun 2024 16:36:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13876v2</guid></item><item><title>Exploration by Learning Diverse Skills through Successor State Measures</title><link>http://arxiv.org/abs/2406.10127v1</link><description>The ability to perform different skills can encourage agents to explore. Inthis work, we aim to construct a set of diverse skills which uniformly coverthe state space. We propose a formalization of this search for diverse skills,building on a previous definition based on the mutual information betweenstates and skills. We consider the distribution of states reached by a policyconditioned on each skill and leverage the successor state measure to maximizethe difference between these skill distributions. We call this approach LEADS:Learning Diverse Skills through Successor States. We demonstrate our approachon a set of maze navigation and robotic control tasks which show that ourmethod is capable of constructing a diverse set of skills which exhaustivelycover the state space without relying on reward or exploration bonuses. Ourfindings demonstrate that this new formalization promotes more robust andefficient exploration by combining mutual information maximization andexploration bonuses.</description><author>Paul-Antoine Le Tolguenec, Yann Besse, Florent Teichteil-Konigsbuch, Dennis G. Wilson, Emmanuel Rachelson</author><pubDate>Fri, 14 Jun 2024 16:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10127v1</guid></item><item><title>GraphFM: A Comprehensive Benchmark for Graph Foundation Model</title><link>http://arxiv.org/abs/2406.08310v2</link><description>Foundation Models (FMs) serve as a general class for the development ofartificial intelligence systems, offering broad potential for generalizationacross a spectrum of downstream tasks. Despite extensive research intoself-supervised learning as the cornerstone of FMs, several outstanding issuespersist in Graph Foundation Models that rely on graph self-supervised learning,namely: 1) Homogenization. The extent of generalization capability ondownstream tasks remains unclear. 2) Scalability. It is unknown how effectivelythese models can scale to large datasets. 3) Efficiency. The training time andmemory usage of these models require evaluation. 4) Training Stop Criteria.Determining the optimal stopping strategy for pre-training across multipletasks to maximize performance on downstream tasks. To address these questions,we have constructed a rigorous benchmark that thoroughly analyzes and studiesthe generalization and scalability of self-supervised Graph Neural Network(GNN) models. Regarding generalization, we have implemented and compared theperformance of various self-supervised GNN models, trained to generate noderepresentations, across tasks such as node classification, link prediction, andnode clustering. For scalability, we have compared the performance of variousmodels after training using full-batch and mini-batch strategies. Additionally,we have assessed the training efficiency of these models by conductingexperiments to test their GPU memory usage and throughput. Through theseexperiments, we aim to provide insights to motivate future research. The codefor this benchmark is publicly available at https://github.com/NYUSHCS/GraphFM.</description><author>Yuhao Xu, Xinqi Liu, Keyu Duan, Yi Fang, Yu-Neng Chuang, Daochen Zha, Qiaoyu Tan</author><pubDate>Fri, 14 Jun 2024 16:36:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08310v2</guid></item><item><title>Training-free Camera Control for Video Generation</title><link>http://arxiv.org/abs/2406.10126v1</link><description>We propose a training-free and robust solution to offer camera movementcontrol for off-the-shelf video diffusion models. Unlike previous work, ourmethod does not require any supervised finetuning on camera-annotated datasetsor self-supervised training via data augmentation. Instead, it can be pluggedand played with most pretrained video diffusion models and generate cameracontrollable videos with a single image or text prompt as input. Theinspiration of our work comes from the layout prior that intermediate latentshold towards generated results, thus rearranging noisy pixels in them will makeoutput content reallocated as well. As camera move could also be seen as a kindof pixel rearrangement caused by perspective change, videos could bereorganized following specific camera motion if their noisy latents changeaccordingly. Established on this, we propose our method CamTrol, which enablesrobust camera control for video diffusion models. It is achieved by a two-stageprocess. First, we model image layout rearrangement through explicit cameramovement in 3D point cloud space. Second, we generate videos with camera motionusing layout prior of noisy latents formed by a series of rearranged images.Extensive experiments have demonstrated the robustness our method holds incontrolling camera motion of generated videos. Furthermore, we show that ourmethod can produce impressive results in generating 3D rotation videos withdynamic content. Project page at https://lifedecoder.github.io/CamTrol/.</description><author>Chen Hou, Guoqiang Wei, Yan Zeng, Zhibo Chen</author><pubDate>Fri, 14 Jun 2024 16:33:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10126v1</guid></item><item><title>MapVision: CVPR 2024 Autonomous Grand Challenge Mapless Driving Tech Report</title><link>http://arxiv.org/abs/2406.10125v1</link><description>Autonomous driving without high-definition (HD) maps demands a higher levelof active scene understanding. In this competition, the organizers provided themulti-perspective camera images and standard-definition (SD) maps to explorethe boundaries of scene reasoning capabilities. We found that most existingalgorithms construct Bird's Eye View (BEV) features from thesemulti-perspective images and use multi-task heads to delineate roadcenterlines, boundary lines, pedestrian crossings, and other areas. However,these algorithms perform poorly at the far end of roads and struggle when theprimary subject in the image is occluded. Therefore, in this competition, wenot only used multi-perspective images as input but also incorporated SD mapsto address this issue. We employed map encoder pre-training to enhance thenetwork's geometric encoding capabilities and utilized YOLOX to improve trafficelement detection precision. Additionally, for area detection, we innovativelyintroduced LDTR and auxiliary tasks to achieve higher precision. As a result,our final OLUS score is 0.58.</description><author>Zhongyu Yang, Mai Liu, Jinluo Xie, Yueming Zhang, Chen Shen, Wei Shao, Jichao Jiao, Tengfei Xing, Runbo Hu, Pengfei Xu</author><pubDate>Fri, 14 Jun 2024 16:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10125v1</guid></item><item><title>Data Ethics in the Era of Healthcare Artificial Intelligence in Africa: An Ubuntu Philosophy Perspective</title><link>http://arxiv.org/abs/2406.10121v1</link><description>Data are essential in developing healthcare artificial intelligence (AI)systems. However, patient data collection, access, and use raise ethicalconcerns, including informed consent, data bias, data protection and privacy,data ownership, and benefit sharing. Various ethical frameworks have beenproposed to ensure the ethical use of healthcare data and AI, however, theseframeworks often align with Western cultural values, social norms, andinstitutional contexts emphasizing individual autonomy and well-being. Ethicalguidelines must reflect political and cultural settings to account for culturaldiversity, inclusivity, and historical factors such as colonialism. Thus, thispaper discusses healthcare data ethics in the AI era in Africa from the Ubuntuphilosophy perspective. It focuses on the contrast between individualistic andcommunitarian approaches to data ethics. The proposed framework could informstakeholders, including AI developers, healthcare providers, the public, andpolicy-makers about healthcare data ethical usage in AI in Africa.</description><author>Abdoul Jalil Djiberou Mahamadou, Aloysius Ochasi, Russ B. Altman</author><pubDate>Fri, 14 Jun 2024 16:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10121v1</guid></item><item><title>Modified Risk Formulation for Improving the Prediction of Knee Osteoarthritis Progression</title><link>http://arxiv.org/abs/2406.10119v1</link><description>Current methods for predicting osteoarthritis (OA) outcomes do notincorporate disease specific prior knowledge to improve the outcome predictionmodels. We developed a novel approach that effectively uses consecutive imagingstudies to improve OA outcome predictions by incorporating an OA severityconstraint. This constraint ensures that the risk of OA for a knee shouldeither increase or remain the same over time. DL models were trained to predictTKR within multiple time periods (1 year, 2 years, and 4 years) using kneeradiographs and MRI scans. Models with and without the risk constraint wereevaluated using the area under the receiver operator curve (AUROC) and the areaunder the precision recall curve (AUPRC) analysis. The novel RiskFORM2 method,leveraging a dual model risk constraint architecture, demonstrated superiorperformance, yielding an AUROC of 0.87 and AUPRC of 0.47 for 1 year TKRprediction on the OAI radiograph test set, a marked improvement over the 0.79AUROC and 0.34 AUPRC of the baseline approach. The performance advantageextended to longer followup periods, with RiskFORM2 maintaining a high AUROC of0.86 and AUPRC of 0.75 in predicting TKR within 4 years. Additionally, whengeneralizing to the external MOST radiograph test set, RiskFORM2 generalizedbetter with an AUROC of 0.77 and AUPRC of 0.25 for 1 year predictions, whichwas higher than the 0.71 AUROC and 0.19 AUPRC of the baseline approach. In theMRI test sets, similar patterns emerged, with RiskFORM2 outperforming thebaseline approach consistently. However, RiskFORM1 exhibited the highest AUROCof 0.86 and AUPRC of 0.72 for 4 year predictions on the OAI set.</description><author>Haresh Rengaraj Rajamohan, Richard Kijowski, Kyunghyun Cho, Cem M. Deniz</author><pubDate>Fri, 14 Jun 2024 16:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10119v1</guid></item><item><title>SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages</title><link>http://arxiv.org/abs/2406.10118v1</link><description>Southeast Asia (SEA) is a region rich in linguistic diversity and culturalvariety, with over 1,300 indigenous languages and a population of 671 millionpeople. However, prevailing AI models suffer from a significant lack ofrepresentation of texts, images, and audio datasets from SEA, compromising thequality of AI models for SEA languages. Evaluating models for SEA languages ischallenging due to the scarcity of high-quality datasets, compounded by thedominance of English training data, raising concerns about potential culturalmisrepresentation. To address these challenges, we introduce SEACrowd, acollaborative initiative that consolidates a comprehensive resource hub thatfills the resource gap by providing standardized corpora in nearly 1,000 SEAlanguages across three modalities. Through our SEACrowd benchmarks, we assessthe quality of AI models on 36 indigenous languages across 13 tasks, offeringvaluable insights into the current AI landscape in SEA. Furthermore, we proposestrategies to facilitate greater AI advancements, maximizing potential utilityand resource equity for the future of AI in SEA.</description><author>Holy Lovenia, Rahmad Mahendra, Salsabil Maulana Akbar, Lester James V. Miranda, Jennifer Santoso, Elyanah Aco, Akhdan Fadhilah, Jonibek Mansurov, Joseph Marvin Imperial, Onno P. Kampman, Joel Ruben Antony Moniz, Muhammad Ravi Shulthan Habibi, Frederikus Hudi, Railey Montalan, Ryan Ignatius, Joanito Agili Lopo, William Nixon, Börje F. Karlsson, James Jaya, Ryandito Diandaru, Yuze Gao, Patrick Amadeus, Bin Wang, Jan Christian Blaise Cruz, Chenxi Whitehouse, Ivan Halim Parmonangan, Maria Khelli, Wenyu Zhang, Lucky Susanto, Reynard Adha Ryanda, Sonny Lazuardi Hermawan, Dan John Velasco, Muhammad Dehan Al Kautsar, Willy Fitra Hendria, Yasmin Moslem, Noah Flynn, Muhammad Farid Adilazuarda, Haochen Li, Johanes Lee, R. Damanhuri, Shuo Sun, Muhammad Reza Qorib, Amirbek Djanibekov, Wei Qi Leong, Quy</author><pubDate>Fri, 14 Jun 2024 16:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10118v1</guid></item><item><title>Trustworthy Artificial Intelligence in the Context of Metrology</title><link>http://arxiv.org/abs/2406.10117v1</link><description>We review research at the National Physical Laboratory (NPL) in the area oftrustworthy artificial intelligence (TAI), and more specifically trustworthymachine learning (TML), in the context of metrology, the science ofmeasurement. We describe three broad themes of TAI: technical, socio-technicaland social, which play key roles in ensuring that the developed models aretrustworthy and can be relied upon to make responsible decisions. From ametrology perspective we emphasise uncertainty quantification (UQ), and itsimportance within the framework of TAI to enhance transparency and trust in theoutputs of AI systems. We then discuss three research areas within TAI that weare working on at NPL, and examine the certification of AI systems in terms ofadherence to the characteristics of TAI.</description><author>Tameem Adel, Sam Bilson, Mark Levene, Andrew Thompson</author><pubDate>Fri, 14 Jun 2024 16:23:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10117v1</guid></item><item><title>On Softmax Direct Preference Optimization for Recommendation</title><link>http://arxiv.org/abs/2406.09215v2</link><description>Recommender systems aim to predict personalized rankings based on userpreference data. With the rise of Language Models (LMs), LM-based recommendershave been widely explored due to their extensive world knowledge and powerfulreasoning abilities. Most of the LM-based recommenders convert historicalinteractions into language prompts, pairing with a positive item as the targetresponse and fine-tuning LM with a language modeling loss. However, the currentobjective fails to fully leverage preference data and is not optimized forpersonalized ranking tasks, which hinders the performance of LM-basedrecommenders. Inspired by the current advancement of Direct PreferenceOptimization (DPO) in human preference alignment and the success of softmaxloss in recommendations, we propose Softmax-DPO (S-DPO) to instill rankinginformation into the LM to help LM-based recommenders distinguish preferreditems from negatives, rather than solely focusing on positives. Specifically,we incorporate multiple negatives in user preference data and devise analternative version of DPO loss tailored for LM-based recommenders, connectedto softmax sampling strategies. Theoretically, we bridge S-DPO with the softmaxloss over negative sampling and find that it has a side effect of mining hardnegatives, which assures its exceptional capabilities in recommendation tasks.Empirically, extensive experiments conducted on three real-world datasetsdemonstrate the superiority of S-DPO to effectively model user preference andfurther boost recommendation performance while mitigating the data likelihooddecline issue of DPO. Our codes are available athttps://github.com/chenyuxin1999/S-DPO.</description><author>Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, Tat-Seng Chua</author><pubDate>Fri, 14 Jun 2024 16:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09215v2</guid></item><item><title>Shelf-Supervised Multi-Modal Pre-Training for 3D Object Detection</title><link>http://arxiv.org/abs/2406.10115v1</link><description>State-of-the-art 3D object detectors are often trained on massive labeleddatasets. However, annotating 3D bounding boxes remains prohibitively expensiveand time-consuming, particularly for LiDAR. Instead, recent works demonstratethat self-supervised pre-training with unlabeled data can improve detectionaccuracy with limited labels. Contemporary methods adapt best-practices forself-supervised learning from the image domain to point clouds (such ascontrastive learning). However, publicly available 3D datasets are considerablysmaller and less diverse than those used for image-based self-supervisedlearning, limiting their effectiveness. We do note, however, that such data isnaturally collected in a multimodal fashion, often paired with images. Ratherthan pre-training with only self-supervised objectives, we argue that it isbetter to bootstrap point cloud representations using image-based foundationmodels trained on internet-scale image data. Specifically, we propose ashelf-supervised approach (e.g. supervised with off-the-shelf image foundationmodels) for generating zero-shot 3D bounding boxes from paired RGB and LiDARdata. Pre-training 3D detectors with such pseudo-labels yields significantlybetter semi-supervised detection accuracy than prior self-supervised pretexttasks. Importantly, we show that image-based shelf-supervision is helpful fortraining LiDAR-only and multi-modal (RGB + LiDAR) detectors. We demonstrate theeffectiveness of our approach on nuScenes and WOD, significantly improving overprior work in limited data settings.</description><author>Mehar Khurana, Neehar Peri, Deva Ramanan, James Hays</author><pubDate>Fri, 14 Jun 2024 16:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10115v1</guid></item><item><title>Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations</title><link>http://arxiv.org/abs/2406.10114v1</link><description>Part-aware panoptic segmentation (PPS) requires (a) that each foregroundobject and background region in an image is segmented and classified, and (b)that all parts within foreground objects are segmented, classified and linkedto their parent object. Existing methods approach PPS by separately conductingobject-level and part-level segmentation. However, their part-level predictionsare not linked to individual parent objects. Therefore, their learningobjective is not aligned with the PPS task objective, which harms the PPSperformance. To solve this, and make more accurate PPS predictions, we proposeTask-Aligned Part-aware Panoptic Segmentation (TAPPS). This method uses a setof shared queries to jointly predict (a) object-level segments, and (b) thepart-level segments within those same objects. As a result, TAPPS learns topredict part-level segments that are linked to individual parent objects,aligning the learning objective with the task objective, and allowing TAPPS toleverage joint object-part representations. With experiments, we show thatTAPPS considerably outperforms methods that predict objects and partsseparately, and achieves new state-of-the-art PPS results.</description><author>Daan de Geus, Gijs Dubbelman</author><pubDate>Fri, 14 Jun 2024 16:20:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10114v1</guid></item><item><title>GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors</title><link>http://arxiv.org/abs/2406.10111v1</link><description>Achieving high-resolution novel view synthesis (HRNVS) from low-resolutioninput views is a challenging task due to the lack of high-resolution data.Previous methods optimize high-resolution Neural Radiance Field (NeRF) fromlow-resolution input views but suffer from slow rendering speed. In this work,we base our method on 3D Gaussian Splatting (3DGS) due to its capability ofproducing high-quality images at a faster rendering speed. To alleviate theshortage of data for higher-resolution synthesis, we propose to leverageoff-the-shelf 2D diffusion priors by distilling the 2D knowledge into 3D withScore Distillation Sampling (SDS). Nevertheless, applying SDS directly toGaussian-based 3D super-resolution leads to undesirable and redundant 3DGaussian primitives, due to the randomness brought by generative priors. Tomitigate this issue, we introduce two simple yet effective techniques to reducestochastic disturbances introduced by SDS. Specifically, we 1) shrink the rangeof diffusion timestep in SDS with an annealing strategy; 2) randomly discardredundant Gaussian primitives during densification. Extensive experiments havedemonstrated that our proposed GaussainSR can attain high-quality results forHRNVS with only low-resolution inputs on both synthetic and real-worlddatasets. Project page: https://chchnii.github.io/GaussianSR/</description><author>Xiqian Yu, Hanxin Zhu, Tianyu He, Zhibo Chen</author><pubDate>Fri, 14 Jun 2024 16:19:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10111v1</guid></item><item><title>Neural Operators for PDE Backstepping Control of First-Order Hyperbolic PIDE with Recycle and Delay</title><link>http://arxiv.org/abs/2307.11436v2</link><description>The recently introduced DeepONet operator-learning framework for PDE controlis extended from the results for basic hyperbolic and parabolic PDEs to anadvanced hyperbolic class that involves delays on both the state and the systemoutput or input. The PDE backstepping design produces gain functions that areoutputs of a nonlinear operator, mapping functions on a spatial domain intofunctions on a spatial domain, and where this gain-generating operator's inputsare the PDE's coefficients. The operator is approximated with a DeepONet neuralnetwork to a degree of accuracy that is provably arbitrarily tight. Once weproduce this approximation-theoretic result in infinite dimension, with it weestablish stability in closed loop under feedback that employs approximategains. In addition to supplying such results under full-state feedback, we alsodevelop DeepONet-approximated observers and output-feedback laws and provetheir own stabilizing properties under neural operator approximations. Withnumerical simulations we illustrate the theoretical results and quantify thenumerical effort savings, which are of two orders of magnitude, thanks toreplacing the numerical PDE solving with the DeepONet.</description><author>Jie Qi, Jing Zhang, Miroslav Krstic</author><pubDate>Fri, 14 Jun 2024 16:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11436v2</guid></item><item><title>Precipitation Nowcasting Using Physics Informed Discriminator Generative Models</title><link>http://arxiv.org/abs/2406.10108v1</link><description>Nowcasting leverages real-time atmospheric conditions to forecast weatherover short periods. State-of-the-art models, including PySTEPS, encounterdifficulties in accurately forecasting extreme weather events because of theirunpredictable distribution patterns. In this study, we design aphysics-informed neural network to perform precipitation nowcasting using theprecipitation and meteorological data from the Royal Netherlands MeteorologicalInstitute (KNMI). This model draws inspiration from the novel Physics-InformedDiscriminator GAN (PID-GAN) formulation, directly integrating physics-basedsupervision within the adversarial learning framework. The proposed modeladopts a GAN structure, featuring a Vector Quantization Generative AdversarialNetwork (VQ-GAN) and a Transformer as the generator, with a temporaldiscriminator serving as the discriminator. Our findings demonstrate that thePID-GAN model outperforms numerical and SOTA deep generative models in terms ofprecipitation nowcasting downstream metrics.</description><author>Junzhe Yin, Cristian Meo, Ankush Roy, Zeineh Bou Cher, Yanbo Wang, Ruben Imhoff, Remko Uijlenhoet, Justin Dauwels</author><pubDate>Fri, 14 Jun 2024 16:12:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10108v1</guid></item><item><title>Annotation Cost-Efficient Active Learning for Deep Metric Learning Driven Remote Sensing Image Retrieval</title><link>http://arxiv.org/abs/2406.10107v1</link><description>Deep metric learning (DML) has shown to be very effective for content-basedimage retrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIRrely on many annotated images to accurately learn model parameters of deepneural networks. However, gathering many image annotations is time consumingand costly. To address this, we propose an annotation cost-efficient activelearning (ANNEAL) method specifically designed for DML driven CBIR in RS.ANNEAL aims to create a small but informative training set made up of similarand dissimilar image pairs to be utilized for learning a deep metric space. Theinformativeness of the image pairs is assessed combining uncertainty anddiversity criteria. To assess the uncertainty of image pairs, we introduce twoalgorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binaryclassifier guided uncertainty estimation (BCGUE). MGUE automatically estimatesa threshold value that acts as a "boundary" between similar and dissimilarimage pairs based on the distances in the metric space. The closer thesimilarity between image pairs to the estimated threshold value the highertheir uncertainty. BCGUE estimates the uncertainty of the image pairs based onthe confidence of the classifier in assigning the correct similarity label. Thediversity criterion is assessed through a clustering-based strategy. ANNEALselects the most informative image pairs by combining either MGUE or BCGUE withclustering-based strategy. The selected image pairs are sent to expertannotators to be labeled as similar or dissimilar. This way of annotatingimages significantly reduces the annotation cost compared to the cost ofannotating images with LULC labels. Experimental results carried out on two RSbenchmark datasets demonstrate the effectiveness of our method. The code of theproposed method will be publicly available upon the acceptance of the paper.</description><author>Genc Hoxha, Gencer Sumbul, Julia Henkel, Lars Möllenbrok, Begüm Demir</author><pubDate>Fri, 14 Jun 2024 16:08:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10107v1</guid></item><item><title>SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding</title><link>http://arxiv.org/abs/2406.10100v1</link><description>Remote Sensing Large Multi-Modal Models (RSLMMs) are developing rapidly andshowcase significant capabilities in remote sensing imagery (RSI)comprehension. However, due to the limitations of existing datasets, RSLMMshave shortcomings in understanding the rich semantic relations among objects incomplex remote sensing scenes. To unlock RSLMMs' complex comprehension ability,we propose a large-scale instruction tuning dataset FIT-RS, containing1,800,851 instruction samples. FIT-RS covers common interpretation tasks andinnovatively introduces several complex comprehension tasks of escalatingdifficulty, ranging from relation reasoning to image-level scene graphgeneration. Based on FIT-RS, we build the FIT-RSFG benchmark. Furthermore, weestablish a new benchmark to evaluate the fine-grained relation comprehensioncapabilities of LMMs, named FIT-RSRC. Based on combined instruction data, wepropose SkySenseGPT, which achieves outstanding performance on both publicdatasets and FIT-RSFG, surpassing existing RSLMMs. We hope the FIT-RS datasetcan enhance the relation comprehension capability of RSLMMs and provide alarge-scale fine-grained data source for the remote sensing community. Thedataset will be available at https://github.com/Luo-Z13/SkySenseGPT</description><author>Junwei Luo, Zhen Pang, Yongjun Zhang, Tingzhu Wang, Linlin Wang, Bo Dang, Jiangwei Lao, Jian Wang, Jingdong Chen, Yihua Tan, Yansheng Li</author><pubDate>Fri, 14 Jun 2024 15:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10100v1</guid></item><item><title>Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning</title><link>http://arxiv.org/abs/2406.10099v1</link><description>Large language models (LLMs) have demonstrated remarkable capabilities acrossvarious tasks but still face challenges such as hallucinations. One potentialreason for hallucinations is the lack of relevant knowledge or context. Thus, apromising solution to mitigate this issue involves instructing LLMs to respondwith "I do not know" when a question falls outside their knowledge domain orthe provided context. However, in this work, we observed that LLMs struggle toadmit their lack of knowledge, primarily due to existing instruction datasetsdesigned to encourage specific answers. To improve large language models'capability to recognize the boundaries of their knowledge, we propose a novelapproach called uncertainty-sensitive tuning. This method involves two-stagetraining designed for uncertainty recognition and prompt-sensitive activation.In the first stage, we guide the LLM to reject unknown questions. In the secondstage, we recover the decreased performance in QA tasks by incorporatingdesigned causal instructions. By leveraging this method, we aim to enhance themodel's ability to identify areas of uncertainty. The experimental resultsdemonstrate that our proposed uncertainty-sensitive tuning method significantlyimproves the performance of the Llama2-chat-7B model. Specifically, it achievesa substantial 34.7% improvement in handling questions involving knowledge gapscompared to the original model. Moreover, our approach outperforms GPT-4,exhibiting a 9.4% increase in overall performance. We open-source the model andcode on GitHub.</description><author>Jiaqi Li, Yixuan Tang, Yi Yang</author><pubDate>Fri, 14 Jun 2024 15:56:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10099v1</guid></item><item><title>ECGMamba: Towards Efficient ECG Classification with BiSSM</title><link>http://arxiv.org/abs/2406.10098v1</link><description>Electrocardiogram (ECG) signal analysis represents a pivotal technique in thediagnosis of cardiovascular diseases. Although transformer-based models havemade significant progress in ECG classification, they exhibit inefficiencies inthe inference phase. The issue is primarily attributable to the secondarycomputational complexity of Transformer's self-attention mechanism.particularly when processing lengthy sequences. To address this issue, wepropose a novel model, ECGMamba, which employs a bidirectional state-spacemodel (BiSSM) to enhance classification efficiency. ECGMamba is based on theinnovative Mamba-based block, which incorporates a range of time seriesmodeling techniques to enhance performance while maintaining the efficiency ofinference. The experimental results on two publicly available ECG datasetsdemonstrate that ECGMamba effectively balances the effectiveness and efficiencyof classification, achieving competitive performance. This study not onlycontributes to the body of knowledge in the field of ECG classification butalso provides a new research path for efficient and accurate ECG signalanalysis. This is of guiding significance for the development of diagnosticmodels for cardiovascular diseases.</description><author>Yupeng Qiang, Xunde Dong, Xiuling Liu, Yang Yang, Yihai Fang, Jianhong Dou</author><pubDate>Fri, 14 Jun 2024 15:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10098v1</guid></item><item><title>Retraining-free Model Quantization via One-Shot Weight-Coupling Learning</title><link>http://arxiv.org/abs/2401.01543v2</link><description>Quantization is of significance for compressing the over-parameterized deepneural models and deploying them on resource-limited devices. Fixed-precisionquantization suffers from performance drop due to the limited numericalrepresentation ability. Conversely, mixed-precision quantization (MPQ) isadvocated to compress the model effectively by allocating heterogeneousbit-width for layers. MPQ is typically organized into a searching-retrainingtwo-stage process. In this paper, we devise a one-shot training-searchingparadigm for mixed-precision model compression. Specifically, in the firststage, all potential bit-width configurations are coupled and thus optimizedsimultaneously within a set of shared weights. However, our observations reveala previously unseen and severe bit-width interference phenomenon among highlycoupled weights during optimization, leading to considerable performancedegradation under a high compression ratio. To tackle this problem, we firstdesign a bit-width scheduler to dynamically freeze the most turbulent bit-widthof layers during training, to ensure the rest bit-widths converged properly.Then, taking inspiration from information theory, we present an informationdistortion mitigation technique to align the behavior of the bad-performingbit-widths to the well-performing ones. In the second stage, an inference-onlygreedy search scheme is devised to evaluate the goodness of configurationswithout introducing any additional training costs. Extensive experiments onthree representative models and three datasets demonstrate the effectiveness ofthe proposed method. Code can be available on\href{https://www.github.com/1hunters/retraining-free-quantization}{https://github.com/1hunters/retraining-free-quantization}.</description><author>Chen Tang, Yuan Meng, Jiacheng Jiang, Shuzhao Xie, Rongwei Lu, Xinzhu Ma, Zhi Wang, Wenwu Zhu</author><pubDate>Fri, 14 Jun 2024 15:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01543v2</guid></item><item><title>ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models</title><link>http://arxiv.org/abs/2406.09334v2</link><description>Performance prediction is a method to estimate the performance of LanguageModels (LMs) on various Natural Language Processing (NLP) tasks, mitigatingcomputational costs associated with model capacity and data for fine-tuning.Our paper introduces ProxyLM, a scalable framework for predicting LMperformance using proxy models in multilingual tasks. These proxy models act assurrogates, approximating the performance of the LM of interest. By leveragingproxy models, ProxyLM significantly reduces computational overhead on taskevaluations, achieving up to a 37.08x speedup compared to traditional methods,even with our smallest proxy models. Additionally, our methodology showcasesadaptability to previously unseen languages in pre-trained LMs, outperformingthe state-of-the-art performance by 1.89x as measured by root-mean-square error(RMSE). This framework streamlines model selection, enabling efficientdeployment and iterative LM enhancements without extensive computationalresources.</description><author>David Anugraha, Genta Indra Winata, Chenyue Li, Patrick Amadeus Irawan, En-Shiun Annie Lee</author><pubDate>Fri, 14 Jun 2024 15:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09334v2</guid></item><item><title>To what extent can ASV systems naturally defend against spoofing attacks?</title><link>http://arxiv.org/abs/2406.05339v2</link><description>The current automatic speaker verification (ASV) task involves making binarydecisions on two types of trials: target and non-target. However, emergingadvancements in speech generation technology pose significant threats to thereliability of ASV systems. This study investigates whether ASV effortlesslyacquires robustness against spoofing attacks (i.e., zero-shot capability) bysystematically exploring diverse ASV systems and spoofing attacks, ranging fromtraditional to cutting-edge techniques. Through extensive analyses conducted oneight distinct ASV systems and 29 spoofing attack systems, we demonstrate thatthe evolution of ASV inherently incorporates defense mechanisms againstspoofing attacks. Nevertheless, our findings also underscore that theadvancement of spoofing attacks far outpaces that of ASV systems, hencenecessitating further research on spoofing-robust ASV methodologies.</description><author>Jee-weon Jung, Xin Wang, Nicholas Evans, Shinji Watanabe, Hye-jin Shim, Hemlata Tak, Sidhhant Arora, Junichi Yamagishi, Joon Son Chung</author><pubDate>Fri, 14 Jun 2024 15:51:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05339v2</guid></item><item><title>Generative AI to Generate Test Data Generators</title><link>http://arxiv.org/abs/2401.17626v2</link><description>Generating fake data is an essential dimension of modern software testing, asdemonstrated by the number and significance of data faking libraries. Yet,developers of faking libraries cannot keep up with the wide range of data to begenerated for different natural languages and domains. In this paper, we assessthe ability of generative AI for generating test data in different domains. Wedesign three types of prompts for Large Language Models (LLMs), which performtest data generation tasks at different levels of integrability: 1) raw testdata generation, 2) synthesizing programs in a specific language that generateuseful test data, and 3) producing programs that use state-of-the-art fakerlibraries. We evaluate our approach by prompting LLMs to generate test data for11 domains. The results show that LLMs can successfully generate realistic testdata generators in a wide range of domains at all three levels ofintegrability.</description><author>Benoit Baudry, Khashayar Etemadi, Sen Fang, Yogya Gamage, Yi Liu, Yuxin Liu, Martin Monperrus, Javier Ron, André Silva, Deepika Tiwari</author><pubDate>Fri, 14 Jun 2024 15:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17626v2</guid></item><item><title>BiKC: Keypose-Conditioned Consistency Policy for Bimanual Robotic Manipulation</title><link>http://arxiv.org/abs/2406.10093v1</link><description>Bimanual manipulation tasks typically involve multiple stages which requireefficient interactions between two arms, posing step-wise and stage-wisechallenges for imitation learning systems. Specifically, failure and delay ofone step will broadcast through time, hinder success and efficiency of eachsub-stage task, and thereby overall task performance. Although recent workshave made strides in addressing certain challenges, few approaches explicitlyconsider the multi-stage nature of bimanual tasks while simultaneouslyemphasizing the importance of inference speed. In this paper, we introduce anovel keypose-conditioned consistency policy tailored for bimanualmanipulation. It is a hierarchical imitation learning framework that consistsof a high-level keypose predictor and a low-level trajectory generator. Thepredicted keyposes provide guidance for trajectory generation and also mark thecompletion of one sub-stage task. The trajectory generator is designed as aconsistency model trained from scratch without distillation, which generatesaction sequences conditioning on current observations and predicted keyposeswith fast inference speed. Simulated and real-world experimental resultsdemonstrate that the proposed approach surpasses baseline methods in terms ofsuccess rate and operational efficiency.</description><author>Dongjie Yu, Hang Xu, Yizhou Chen, Yi Ren, Jia Pan</author><pubDate>Fri, 14 Jun 2024 15:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10093v1</guid></item><item><title>Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation</title><link>http://arxiv.org/abs/2406.10091v1</link><description>Assessing the performance of interpreting services is a complex task, giventhe nuanced nature of spoken language translation, the strategies thatinterpreters apply, and the diverse expectations of users. The complexity ofthis task become even more pronounced when automated evaluation methods areapplied. This is particularly true because interpreted texts exhibit lesslinearity between the source and target languages due to the strategiesemployed by the interpreter. This study aims to assess the reliability of automatic metrics in evaluatingsimultaneous interpretations by analyzing their correlation with humanevaluations. We focus on a particular feature of interpretation quality, namelytranslation accuracy or faithfulness. As a benchmark we use human assessmentsperformed by language experts, and evaluate how well sentence embeddings andLarge Language Models correlate with them. We quantify semantic similaritybetween the source and translated texts without relying on a referencetranslation. The results suggest GPT models, particularly GPT-3.5 with directprompting, demonstrate the strongest correlation with human judgment in termsof semantic similarity between source and target texts, even when evaluatingshort textual segments. Additionally, the study reveals that the size of thecontext window has a notable impact on this correlation.</description><author>Xiaoman Wang, Claudio Fantinuoli</author><pubDate>Fri, 14 Jun 2024 15:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10091v1</guid></item><item><title>Over-parameterization and Adversarial Robustness in Neural Networks: An Overview and Empirical Analysis</title><link>http://arxiv.org/abs/2406.10090v1</link><description>Thanks to their extensive capacity, over-parameterized neural networksexhibit superior predictive capabilities and generalization. However, having alarge parameter space is considered one of the main suspects of the neuralnetworks' vulnerability to adversarial example -- input samples crafted ad-hocto induce a desired misclassification. Relevant literature has claimedcontradictory remarks in support of and against the robustness ofover-parameterized networks. These contradictory findings might be due to thefailure of the attack employed to evaluate the networks' robustness. Previousresearch has demonstrated that depending on the considered model, the algorithmemployed to generate adversarial examples may not function properly, leading tooverestimating the model's robustness. In this work, we empirically study therobustness of over-parameterized networks against adversarial examples.However, unlike the previous works, we also evaluate the considered attack'sreliability to support the results' veracity. Our results show thatover-parameterized networks are robust against adversarial attacks as opposedto their under-parameterized counterparts.</description><author>Zhang Chen, Luca Demetrio, Srishti Gupta, Xiaoyi Feng, Zhaoqiang Xia, Antonio Emanuele Cinà, Maura Pintor, Luca Oneto, Ambra Demontis, Battista Biggio, Fabio Roli</author><pubDate>Fri, 14 Jun 2024 15:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10090v1</guid></item><item><title>Biomarker based Cancer Classification using an Ensemble with Pre-trained Models</title><link>http://arxiv.org/abs/2406.10087v1</link><description>Certain cancer types, namely pancreatic cancer is difficult to detect at anearly stage; sparking the importance of discovering the causal relationshipbetween biomarkers and cancer to identify cancer efficiently. By allowing forthe detection and monitoring of specific biomarkers through a non-invasivemethod, liquid biopsies enhance the precision and efficacy of medicalinterventions, advocating the move towards personalized healthcare. Severalmachine learning algorithms such as Random Forest, SVM are utilized forclassification, yet causing inefficiency due to the need for conductinghyperparameter tuning. We leverage a meta-trained Hyperfast model forclassifying cancer, accomplishing the highest AUC of 0.9929 and simultaneouslyachieving robustness especially on highly imbalanced datasets compared to otherML algorithms in several binary classification tasks (e.g. breast invasivecarcinoma; BRCA vs. non-BRCA). We also propose a novel ensemble model combiningpre-trained Hyperfast model, XGBoost, and LightGBM for multi-classclassification tasks, achieving an incremental increase in accuracy (0.9464)while merely using 500 PCA features; distinguishable from previous studieswhere they used more than 2,000 features for similar results.</description><author>Chongmin Lee, Jihie Kim</author><pubDate>Fri, 14 Jun 2024 15:43:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10087v1</guid></item></channel></rss>