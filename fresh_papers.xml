<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 12 Aug 2024 01:00:54 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents</title><link>http://arxiv.org/abs/2311.09805v3</link><description>Recent LLMs have demonstrated remarkable performance in solving exam-likemath word problems. However, the degree to which these numerical reasoningskills are effective in real-world scenarios, particularly in expert domains,is still largely unexplored. This paper introduces DocMath-Eval, acomprehensive benchmark specifically designed to evaluate the numericalreasoning capabilities of LLMs in the context of understanding and analyzingspecialized documents containing both text and tables. We conduct an extensiveevaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought promptingmethods, aiming to comprehensively assess the capabilities and limitations ofexisting LLMs in DocMath-Eval. We found that even the current best-performingsystem (i.e., GPT-4o) still significantly lags behind human experts in solvingcomplex numerical reasoning problems grounded in long contexts. We believe thatDocMath-Eval can serve as a valuable benchmark for evaluating LLMs'capabilities in solving challenging numerical reasoning problems within expertdomains.</description><author>Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, Arman Cohan</author><pubDate>Fri, 09 Aug 2024 17:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09805v3</guid></item><item><title>ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability</title><link>http://arxiv.org/abs/2404.14712v4</link><description>Earth system predictability is challenged by the complexity of environmentaldynamics and the multitude of variables involved. Current AI foundation models,although advanced by leveraging large and heterogeneous data, are oftenconstrained by their size and data integration, limiting their effectiveness inaddressing the full range of Earth system prediction challenges. To overcomethese limitations, we introduce the Oak Ridge Base Foundation Model for EarthSystem Predictability (ORBIT), an advanced vision transformer model that scalesup to 113 billion parameters using a novel hybrid tensor-data orthogonalparallelism technique. As the largest model of its kind, ORBIT surpasses thecurrent climate AI foundation model size by a thousandfold. Performance scalingtests conducted on the Frontier supercomputer have demonstrated that ORBITachieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scalingefficiency maintained at 41% to 85% across 49,152 AMD GPUs. These breakthroughsestablish new advances in AI-driven climate modeling and demonstrate promise tosignificantly improve the Earth system predictability.</description><author>Xiao Wang, Siyan Liu, Aristeidis Tsaris, Jong-Youl Choi, Ashwin Aji, Ming Fan, Wei Zhang, Junqi Yin, Moetasim Ashfaq, Dan Lu, Prasanna Balaprakash</author><pubDate>Fri, 09 Aug 2024 14:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14712v4</guid></item><item><title>Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2408.04594v2</link><description>High-performance Multimodal Large Language Models (MLLMs) rely heavily ondata quality. This study introduces a novel dataset named Img-Diff, designed toenhance fine-grained image recognition in MLLMs by leveraging insights fromcontrastive learning and image difference captioning. By analyzing objectdifferences between similar images, we challenge models to identify bothmatching and distinct components. We utilize the Stable-Diffusion-XL model andadvanced image editing techniques to create pairs of similar images thathighlight object replacements. Our methodology includes a Difference AreaGenerator for object differences identifying, followed by a Difference CaptionsGenerator for detailed difference descriptions. The result is a relativelysmall but high-quality dataset of "object replacement" samples. We use the theproposed dataset to finetune state-of-the-art (SOTA) MLLMs such as MGM-7B,yielding comprehensive improvements of performance scores over SOTA models thattrained with larger-scale datasets, in numerous image difference and VisualQuestion Answering tasks. For instance, our trained models notably surpass theSOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigatealternative methods for generating image difference data through "objectremoval" and conduct a thorough evaluation to confirm the dataset's diversity,quality, and robustness, presenting several insights on the synthesis of such acontrastive dataset. To encourage further research and advance the field ofmultimodal data synthesis and enhancement of MLLMs' fundamental capabilitiesfor image understanding, we release our codes and dataset athttps://github.com/modelscope/data-juicer/tree/ImgDiff.</description><author>Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen</author><pubDate>Fri, 09 Aug 2024 14:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04594v2</guid></item><item><title>DreamLCM: Towards High-Quality Text-to-3D Generation via Latent Consistency Model</title><link>http://arxiv.org/abs/2408.02993v2</link><description>Recently, the text-to-3D task has developed rapidly due to the appearance ofthe SDS method. However, the SDS method always generates 3D objects with poorquality due to the over-smooth issue. This issue is attributed to two factors:1) the DDPM single-step inference produces poor guidance gradients; 2) therandomness from the input noises and timesteps averages the details of the 3Dcontents. In this paper, to address the issue, we propose DreamLCM whichincorporates the Latent Consistency Model (LCM). DreamLCM leverages thepowerful image generation capabilities inherent in LCM, enabling generatingconsistent and high-quality guidance, i.e., predicted noises or images. Poweredby the improved guidance, the proposed method can provide accurate and detailedgradients to optimize the target 3D models. In addition, we propose twostrategies to enhance the generation quality further. Firstly, we propose aguidance calibration strategy, utilizing Euler Solver to calibrate the guidancedistribution to accelerate 3D models to converge. Secondly, we propose a dualtimestep strategy, increasing the consistency of guidance and optimizing 3Dmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCMachieves state-of-the-art results in both generation quality and trainingefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.</description><author>Yiming Zhong, Xiaolin Zhang, Yao Zhao, Yunchao Wei</author><pubDate>Fri, 09 Aug 2024 14:12:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02993v2</guid></item><item><title>Back-Projection Diffusion: Solving the Wideband Inverse Scattering Problem with Diffusion Models</title><link>http://arxiv.org/abs/2408.02866v2</link><description>We present Wideband back-projection diffusion, an end-to-end probabilisticframework for approximating the posterior distribution induced by the inversescattering map from wideband scattering data. This framework leveragesconditional diffusion models coupled with the underlying physics ofwave-propagation and symmetries in the problem, to produce highly accuratereconstructions. The framework introduces a factorization of the score functioninto a physics-based latent representation inspired by the filteredback-propagation formula and a conditional score function conditioned on thislatent representation. These two steps are also constrained to obey symmetriesin the formulation while being amenable to compression by imposing the rankstructure found in the filtered back-projection formula. As a result,empirically, our framework is able to provide sharp reconstructionseffortlessly, even recovering sub-Nyquist features in the multiple-scatteringregime. It has low-sample and computational complexity, its number ofparameters scales sub-linearly with the target resolution, and it has stabletraining dynamics.</description><author>Borong Zhang, Martín Guerra, Qin Li, Leonardo Zepeda-Núñez</author><pubDate>Fri, 09 Aug 2024 13:44:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02866v2</guid></item><item><title>Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon</title><link>http://arxiv.org/abs/2408.04377v2</link><description>Anomaly detection in time series data is a critical challenge across variousdomains. Traditional methods typically focus on identifying anomalies inimmediate subsequent steps, often underestimating the significance of temporaldynamics such as delay time and horizons of anomalies, which generally requireextensive post-analysis. This paper introduces a novel approach for detectingtime series anomalies called Anomaly Prediction, incorporating temporalinformation directly into the prediction results. We propose a new datasetspecifically designed to evaluate this approach and conduct comprehensiveexperiments using several state-of-the-art time series forecasting methods. Theresults demonstrate the efficacy of our approach in providing timely andaccurate anomaly predictions, setting a new benchmark for future research inthis field.</description><author>Jiang You, Arben Cela, René Natowicz, Jacob Ouanounou, Patrick Siarry</author><pubDate>Fri, 09 Aug 2024 13:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04377v2</guid></item><item><title>Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters</title><link>http://arxiv.org/abs/2408.04093v2</link><description>Self-attention is the core mathematical operation of modern transformerarchitectures and is also a significant computational bottleneck due to itsquadratic complexity in the sequence length. In this work, we derive the scalarenergy function whose gradient computes the self-attention block, thuselucidating the theoretical underpinnings of self-attention, providing aBayesian interpretation of the operation and linking it closely withenergy-based models such as Hopfield Networks. Our formulation reveals that thereduction across the sequence axis can be efficiently computed in parallelthrough a tree reduction. Our algorithm, for parallelizing attentioncomputation across multiple GPUs enables cross-device decoding to be performedasymptotically faster (up to 8x faster in our experiments) than alternativeapproaches such as Ring Attention, while also requiring significantly lesscommunication volume and incurring 2x less peak memory. Our code is publiclyavailable here: \url{https://github.com/Zyphra/tree_attention}.</description><author>Vasudev Shyam, Jonathan Pilault, Emily Shepperd, Quentin Anthony, Beren Millidge</author><pubDate>Fri, 09 Aug 2024 13:07:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04093v2</guid></item><item><title>The Distributional Uncertainty of the SHAP score in Explainable Machine Learning</title><link>http://arxiv.org/abs/2401.12731v3</link><description>Attribution scores reflect how important the feature values in an inputentity are for the output of a machine learning model. One of the most popularattribution scores is the SHAP score, which is an instantiation of the generalShapley value used in coalition game theory. The definition of this scorerelies on a probability distribution on the entity population. Since the exactdistribution is generally unknown, it needs to be assigned subjectively or beestimated from data, which may lead to misleading feature scores. In thispaper, we propose a principled framework for reasoning on SHAP scores underunknown entity population distributions. In our framework, we consider anuncertainty region that contains the potential distributions, and the SHAPscore of a feature becomes a function defined over this region. We study thebasic problems of finding maxima and minima of this function, which allows usto determine tight ranges for the SHAP scores of all features. In particular,we pinpoint the complexity of these problems, and other related ones, showingthem to be NP-complete. Finally, we present experiments on a real-worlddataset, showing that our framework may contribute to a more robust featurescoring.</description><author>Santiago Cifuentes, Leopoldo Bertossi, Nina Pardal, Sergio Abriola, Maria Vanina Martinez, Miguel Romero</author><pubDate>Fri, 09 Aug 2024 09:08:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12731v3</guid></item><item><title>GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI</title><link>http://arxiv.org/abs/2408.03361v3</link><description>Large Vision-Language Models (LVLMs) are capable of handling diverse datatypes such as imaging, text, and physiological signals, and can be applied invarious fields. In the medical field, LVLMs have a high potential to offersubstantial assistance for diagnosis and treatment. Before that, it is crucialto develop benchmarks to evaluate LVLMs' effectiveness in various medicalapplications. Current benchmarks are often built upon specific academicliterature, mainly focusing on a single domain, and lacking varying perceptualgranularities. Thus, they face specific challenges, including limited clinicalrelevance, incomplete evaluations, and insufficient guidance for interactiveLVLMs. To address these limitations, we developed the GMAI-MMBench, the mostcomprehensive general medical AI benchmark with well-categorized data structureand multi-perceptual granularity to date. It is constructed from 285 datasetsacross 39 medical image modalities, 18 clinical-related tasks, 18 departments,and 4 perceptual granularities in a Visual Question Answering (VQA) format.Additionally, we implemented a lexical tree structure that allows users tocustomize evaluation tasks, accommodating various assessment needs andsubstantially supporting medical AI research and applications. We evaluated 50LVLMs, and the results show that even the advanced GPT-4o only achieves anaccuracy of 52%, indicating significant room for improvement. Moreover, weidentified five key insufficiencies in current cutting-edge LVLMs that need tobe addressed to advance the development of better medical applications. Webelieve that GMAI-MMBench will stimulate the community to build the nextgeneration of LVLMs toward GMAI.</description><author>Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, Benyou Wang, Shaoting Zhang, Bin Fu, Jianfei Cai, Bohan Zhuang, Eric J Seibel, Junjun He, Yu Qiao</author><pubDate>Fri, 09 Aug 2024 08:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03361v3</guid></item><item><title>Online Model-based Anomaly Detection in Multivariate Time Series: Taxonomy, Survey, Research Challenges and Future Directions</title><link>http://arxiv.org/abs/2408.03747v2</link><description>Time-series anomaly detection plays an important role in engineeringprocesses, like development, manufacturing and other operations involvingdynamic systems. These processes can greatly benefit from advances in thefield, as state-of-the-art approaches may aid in cases involving, for example,highly dimensional data. To provide the reader with understanding of theterminology, this survey introduces a novel taxonomy where a distinctionbetween online and offline, and training and inference is made. Additionally,it presents the most popular data sets and evaluation metrics used in theliterature, as well as a detailed analysis. Furthermore, this survey providesan extensive overview of the state-of-the-art model-based online semi- andunsupervised anomaly detection approaches for multivariate time-series data,categorising them into different model families and other properties. Thebiggest research challenge revolves around benchmarking, as currently there isno reliable way to compare different approaches against one another. Thisproblem is two-fold: on the one hand, public data sets suffers from at leastone fundamental flaw, while on the other hand, there is a lack of intuitive andrepresentative evaluation metrics in the field. Moreover, the way mostpublications choose a detection threshold disregards real-world conditions,which hinders the application in the real world. To allow for tangible advancesin the field, these issues must be addressed in future work.</description><author>Lucas Correia, Jan-Christoph Goos, Philipp Klein, Thomas Bäck, Anna V. Kononova</author><pubDate>Fri, 09 Aug 2024 08:10:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03747v2</guid></item><item><title>Graph Matching via convex relaxation to the simplex</title><link>http://arxiv.org/abs/2310.20609v3</link><description>This paper addresses the Graph Matching problem, which consists of findingthe best possible alignment between two input graphs, and has many applicationsin computer vision, network deanonymization and protein alignment. A commonapproach to tackle this problem is through convex relaxations of the NP-hard\emph{Quadratic Assignment Problem} (QAP). Here, we introduce a new convex relaxation onto the unit simplex and developan efficient mirror descent scheme with closed-form iterations for solving thisproblem. Under the correlated Gaussian Wigner model, we show that the simplexrelaxation admits a unique solution with high probability. In the noiselesscase, this is shown to imply exact recovery of the ground truth permutation.Additionally, we establish a novel sufficiency condition for the input matrixin standard greedy rounding methods, which is less restrictive than thecommonly used `diagonal dominance' condition. We use this condition to showexact one-step recovery of the ground truth (holding almost surely) via themirror descent scheme, in the noiseless setting. We also use this condition toobtain significantly improved conditions for the GRAMPA algorithm [Fan et al.2019] in the noiseless setting.</description><author>Ernesto Araya Valdivia, Hemant Tyagi</author><pubDate>Fri, 09 Aug 2024 07:53:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20609v3</guid></item><item><title>Optimization Dynamics of Equivariant and Augmented Neural Networks</title><link>http://arxiv.org/abs/2303.13458v4</link><description>We investigate the optimization of neural networks on symmetric data, andcompare the strategy of constraining the architecture to be equivariant to thatof using data augmentation. Our analysis reveals that that the relativegeometry of the admissible and the equivariant layers, respectively, plays akey role. Under natural assumptions on the data, network, loss, and group ofsymmetries, we show that compatibility of the spaces of admissible layers andequivariant layers, in the sense that the corresponding orthogonal projectionscommute, implies that the sets of equivariant stationary points are identicalfor the two strategies. If the linear layers of the network also are given aunitary parametrization, the set of equivariant layers is even invariant underthe gradient flow for augmented models. Our analysis however also reveals thateven in the latter situation, stationary points may be unstable for augmentedtraining although they are stable for the manifestly equivariant models.</description><author>Oskar Nordenfors, Fredrik Ohlsson, Axel Flinth</author><pubDate>Fri, 09 Aug 2024 06:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13458v4</guid></item><item><title>Advancing Prompt Learning through an External Layer</title><link>http://arxiv.org/abs/2407.19674v5</link><description>Prompt learning represents a promising method for adapting pre-trainedvision-language models (VLMs) to various downstream tasks by learning a set oftext embeddings. One challenge inherent to these methods is the poorgeneralization performance due to the invalidity of the learned text embeddingsfor unseen tasks. A straightforward approach to bridge this gap is to freezethe text embeddings in prompts, which results in a lack of capacity to adaptVLMs for downstream tasks. To address this dilemma, we propose a paradigmcalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose atextual external layer and learnable visual embeddings for adapting VLMs todownstream tasks. The learnable external layer is built upon valid embeddingsof pre-trained CLIP. This design considers the balance of learning capabilitiesbetween the two branches. To align the textual and visual features, we proposea novel two-pronged approach: i) we introduce the optimal transport as thediscrepancy metric to align the vision and text modalities, and ii) weintroduce a novel strengthening feature to enhance the interaction betweenthese two modalities. Four representative experiments (i.e., base-to-novelgeneralization, few-shot learning, cross-dataset generalization, domain shiftsgeneralization) across 15 datasets demonstrate that our method outperforms theexisting prompt learning method.</description><author>Fangming Cui, Xun Yang, Chao Wu, Liang Xiao, Xinmei Tian</author><pubDate>Fri, 09 Aug 2024 06:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19674v5</guid></item><item><title>CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging</title><link>http://arxiv.org/abs/2407.11652v7</link><description>Federated Learning (FL) offers a privacy-preserving approach to train modelson decentralized data. Its potential in healthcare is significant, butchallenges arise due to cross-client variations in medical image data,exacerbated by limited annotations. This paper introduces Cross-ClientVariations Adaptive Federated Learning (CCVA-FL) to address these issues.CCVA-FL aims to minimize cross-client variations by transforming images into acommon feature space. It involves expert annotation of a subset of images fromeach client, followed by the selection of a client with the least datacomplexity as the target. Synthetic medical images are then generated usingScalable Diffusion Models with Transformers (DiT) based on the target client'sannotated images. These synthetic images, capturing diversity and representingthe original data, are shared with other clients. Each client then translatesits local images into the target image space using image-to-image translation.The translated images are subsequently used in a federated learning setting todevelop a server model. Our results demonstrate that CCVA-FL outperformsVanilla Federated Averaging by effectively addressing data distributiondifferences across clients without compromising privacy.</description><author>Sunny Gupta, Amit Sethi</author><pubDate>Fri, 09 Aug 2024 05:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11652v7</guid></item><item><title>GMISeg: General Medical Image Segmentation without Re-Training</title><link>http://arxiv.org/abs/2311.12539v4</link><description>The online shopping behavior has the characteristics of rich granularitydimension and data sparsity and previous researches on user behavior predictiondid not seriously discuss feature selection and ensemble design. In this paper,we proposed a SE-Stacking model based on information fusion and ensemblelearning for user purchase behavior prediction. After successfully utilizingthe ensemble feature selection method to screen purchase-related factors, weused the Stacking algorithm for user purchase behavior prediction. In ourefforts to avoid the deviation of prediction results, we optimized the model byselecting ten different kinds of models as base learners and modifying relevantparameters specifically for them. The experiments conducted on apublicly-available dataset shows that the SE-Stacking model can achieve a98.40% F1-score, about 0.09% higher than the optimal base models. TheSE-Stacking model not only has a good application in the prediction of userpurchase behavior but also has practical value combining with the actuale-commerce scene. At the same time, it has important significance for academicresearch and the development of this field.</description><author>Jing Xu</author><pubDate>Fri, 09 Aug 2024 01:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12539v4</guid></item><item><title>MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image Segmentation</title><link>http://arxiv.org/abs/2312.07128v3</link><description>Chest X-ray is one of the most common radiological examination types for thediagnosis of chest diseases. Nowadays, the automatic classification technologyof radiological images has been widely used in clinical diagnosis and treatmentplans. However, each disease has its own different response characteristicreceptive field region, which is the main challenge for chest diseaseclassification tasks. Besides, the imbalance of sample data categories furtherincreases the difficulty of tasks. To solve these problems, we propose a newmulti-label chest disease image classification scheme based on a multi-scaleattention network. In this scheme, multi-scale information is iteratively fusedto focus on regions with a high probability of disease, to effectively minemore meaningful information from data, and the classification performance canbe improved only by image level annotation. We also designed a new lossfunction to improve the rationality of visual perception and the performance ofmulti-label image classification by forcing the consistency of attentionregions before and after image transformation. A comprehensive experiment wascarried out on the public Chest X-Ray14 and CheXpert datasets to achieve stateof the art results, which verified the effectiveness of this method in chestX-ray image classification.</description><author>Jing Xu</author><pubDate>Fri, 09 Aug 2024 01:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07128v3</guid></item><item><title>YOWOv3: An Efficient and Generalized Framework for Human Action Detection and Recognition</title><link>http://arxiv.org/abs/2408.02623v2</link><description>In this paper, we propose a new framework called YOWOv3, which is an improvedversion of YOWOv2, designed specifically for the task of Human Action Detectionand Recognition. This framework is designed to facilitate extensiveexperimentation with different configurations and supports easy customizationof various components within the model, reducing efforts required forunderstanding and modifying the code. YOWOv3 demonstrates its superiorperformance compared to YOWOv2 on two widely used datasets for Human ActionDetection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessormodel YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2,respectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model -YOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33%and 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate thatYOWOv3 significantly reduces the number of parameters and GFLOPs while stillachieving comparable performance.</description><author>Duc Manh Nguyen Dang, Viet Hang Duong, Jia Ching Wang, Nhan Bui Duc</author><pubDate>Fri, 09 Aug 2024 00:17:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02623v2</guid></item><item><title>WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models</title><link>http://arxiv.org/abs/2408.03837v2</link><description>WalledEval is a comprehensive AI safety testing toolkit designed to evaluatelarge language models (LLMs). It accommodates a diverse range of models,including both open-weight and API-based ones, and features over 35 safetybenchmarks covering areas such as multilingual safety, exaggerated safety, andprompt injections. The framework supports both LLM and judge benchmarking, andincorporates custom mutators to test safety against various text-stylemutations such as future tense and paraphrasing. Additionally, WalledEvalintroduces WalledGuard, a new, small and performant content moderation tool,and SGXSTest, a benchmark for assessing exaggerated safety in culturalcontexts. We make WalledEval publicly available athttps://github.com/walledai/walledeval</description><author>Prannaya Gupta, Le Qi Yau, Hao Han Low, I-Shiang Lee, Hugo Maximus Lim, Yu Xin Teoh, Jia Hng Koh, Dar Win Liew, Rishabh Bhardwaj, Rajat Bhardwaj, Soujanya Poria</author><pubDate>Thu, 08 Aug 2024 18:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03837v2</guid></item><item><title>SteP: Stacked LLM Policies for Web Actions</title><link>http://arxiv.org/abs/2310.03720v4</link><description>Performing tasks on the web presents fundamental challenges to large languagemodels (LLMs), including combinatorially large open-world tasks and variationsacross web interfaces. Simply specifying a large prompt to handle all possiblebehaviors and states is extremely complex, and results in behavior leaksbetween unrelated behaviors. Decomposition to distinct policies can addressthis challenge, but requires carefully handing off control between policies. Wepropose Stacked LLM Policies for Web Actions (SteP), an approach to dynamicallycompose policies to solve a diverse set of web tasks. SteP defines a MarkovDecision Process where the state is a stack of policies representing thecontrol state, i.e., the chain of policy calls. Unlike traditional methods thatare restricted to static hierarchies, SteP enables dynamic control that adaptsto the complexity of the task. We evaluate SteP against multiple baselines andweb environments including WebArena, MiniWoB++, and a CRM. On WebArena, StePimproves (14.9\% to 33.5\%) over SOTA that use GPT-4 policies, while onMiniWob++, SteP is competitive with prior works while using significantly lessdata. Our code and data are available athttps://asappresearch.github.io/webagents-step.</description><author>Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, Ryan McDonald</author><pubDate>Thu, 08 Aug 2024 18:00:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03720v4</guid></item><item><title>LiDAR-Event Stereo Fusion with Hallucinations</title><link>http://arxiv.org/abs/2408.04633v1</link><description>Event stereo matching is an emerging technique to estimate depth fromneuromorphic cameras; however, events are unlikely to trigger in the absence ofmotion or the presence of large, untextured regions, making the correspondenceproblem extremely challenging. Purposely, we propose integrating a stereo eventcamera with a fixed-frequency active sensor -- e.g., a LiDAR -- collectingsparse depth measurements, overcoming the aforementioned limitations. Suchdepth hints are used by hallucinating -- i.e., inserting fictitious events --the stacks or raw input streams, compensating for the lack of information inthe absence of brightness changes. Our techniques are general, can be adaptedto any structured representation to stack events and outperformstate-of-the-art fusion methods applied to event-based stereo.</description><author>Luca Bartolomei, Matteo Poggi, Andrea Conti, Stefano Mattoccia</author><pubDate>Thu, 08 Aug 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04633v1</guid></item><item><title>Arctic-TILT. Business Document Understanding at Sub-Billion Scale</title><link>http://arxiv.org/abs/2408.04632v1</link><description>The vast portion of workloads employing LLMs involves answering questionsgrounded on PDF or scan content. We introduce the Arctic-TILT achievingaccuracy on par with models 1000$\times$ its size on these use cases. It can befine-tuned and deployed on a single 24GB GPU, lowering operational costs whileprocessing Visually Rich Documents with up to 400k tokens. The modelestablishes state-of-the-art results on seven diverse Document Understandingbenchmarks, as well as provides reliable confidence scores and quick inference,which are essential for processing files in large-scale or time-sensitiveenterprise environments.</description><author>Łukasz Borchmann, Michał Pietruszka, Wojciech Jaśkowski, Dawid Jurkiewicz, Piotr Halama, Paweł Józiak, Łukasz Garncarek, Paweł Liskowski, Karolina Szyndler, Andrzej Gretkowski, Julita Ołtusek, Gabriela Nowakowska, Artur Zawłocki, Łukasz Duhr, Paweł Dyda, Michał Turski</author><pubDate>Thu, 08 Aug 2024 17:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04632v1</guid></item><item><title>Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</title><link>http://arxiv.org/abs/2408.04631v1</link><description>We present Puppet-Master, an interactive video generative model that canserve as a motion prior for part-level dynamics. At test time, given a singleimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master cansynthesize a video depicting realistic part-level motion faithful to the givendrag interactions. This is achieved by fine-tuning a large-scale pre-trainedvideo diffusion model, for which we propose a new conditioning architecture toinject the dragging control effectively. More importantly, we introduce theall-to-first attention mechanism, a drop-in replacement for the widely adoptedspatial attention modules, which significantly improves generation quality byaddressing the appearance and background issues in existing models. Unlikeother motion-conditioned video generators that are trained on in-the-wildvideos and mostly move an entire object, Puppet-Master is learned fromObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. Wepropose a strategy to automatically filter out sub-optimal animations andaugment the synthetic renderings with meaningful motion trajectories.Puppet-Master generalizes well to real images across various categories andoutperforms existing methods in a zero-shot manner on a real-world benchmark.See our project page for more results: vgg-puppetmaster.github.io.</description><author>Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi</author><pubDate>Thu, 08 Aug 2024 17:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04631v1</guid></item><item><title>LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP</title><link>http://arxiv.org/abs/2408.04628v1</link><description>Standard natural language processing (NLP) pipelines operate on symbolicrepresentations of language, which typically consist of sequences of discretetokens. However, creating an analogous representation for ancient logographicwriting systems is an extremely labor intensive process that requires expertknowledge. At present, a large portion of logographic data persists in a purelyvisual form due to the absence of transcription -- this issue poses abottleneck for researchers seeking to apply NLP toolkits to study ancientlogographic languages: most of the relevant data are images of writing. This paper investigates whether direct processing of visual representationsof language offers a potential solution. We introduce LogogramNLP, the firstbenchmark enabling NLP analysis of ancient logographic languages, featuringboth transcribed and visual datasets for four writing systems along withannotations for tasks like classification, translation, and parsing. Ourexperiments compare systems that employ recent visual and text encodingstrategies as backbones. The results demonstrate that visual representationsoutperform textual representations for some investigated tasks, suggesting thatvisual processing pipelines may unlock a large amount of cultural heritage dataof logographic languages for NLP-based analyses.</description><author>Danlu Chen, Freda Shi, Aditi Agarwal, Jacobo Myerston, Taylor Berg-Kirkpatrick</author><pubDate>Thu, 08 Aug 2024 17:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04628v1</guid></item><item><title>Dual-View Data Hallucination with Semantic Relation Guidance for Few-Shot Image Recognition</title><link>http://arxiv.org/abs/2401.07061v2</link><description>Learning to recognize novel concepts from just a few image samples is verychallenging as the learned model is easily overfitted on the few data andresults in poor generalizability. One promising but underexplored solution isto compensate the novel classes by generating plausible samples. However, mostexisting works of this line exploit visual information only, rendering thegenerated data easy to be distracted by some challenging factors contained inthe few available samples. Being aware of the semantic information in thetextual modality that reflects human concepts, this work proposes a novelframework that exploits semantic relations to guide dual-view datahallucination for few-shot image recognition. The proposed framework enablesgenerating more diverse and reasonable data samples for novel classes througheffective information transfer from base classes. Specifically, aninstance-view data hallucination module hallucinates each sample of a novelclass to generate new data by employing local semantic correlated attention andglobal semantic feature fusion derived from base classes. Meanwhile, aprototype-view data hallucination module exploits semantic-aware measure toestimate the prototype of a novel class and the associated distribution fromthe few samples, which thereby harvests the prototype as a more stable sampleand enables resampling a large number of samples. We conduct extensiveexperiments and comparisons with state-of-the-art methods on several popularfew-shot benchmarks to verify the effectiveness of the proposed framework.</description><author>Hefeng Wu, Guangzhi Ye, Ziyang Zhou, Ling Tian, Qing Wang, Liang Lin</author><pubDate>Thu, 08 Aug 2024 17:52:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07061v2</guid></item><item><title>Transformer Explainer: Interactive Learning of Text-Generative Models</title><link>http://arxiv.org/abs/2408.04619v1</link><description>Transformers have revolutionized machine learning, yet their inner workingsremain opaque to many. We present Transformer Explainer, an interactivevisualization tool designed for non-experts to learn about Transformers throughthe GPT-2 model. Our tool helps users understand complex Transformer conceptsby integrating a model overview and enabling smooth transitions acrossabstraction levels of mathematical operations and model structures. It runs alive GPT-2 instance locally in the user's browser, empowering users toexperiment with their own input and observe in real-time how the internalcomponents and parameters of the Transformer work together to predict the nexttokens. Our tool requires no installation or special hardware, broadening thepublic's education access to modern generative AI techniques. Our open-sourcedtool is available at https://poloclub.github.io/transformer-explainer/. A videodemo is available at https://youtu.be/ECR4oAwocjs.</description><author>Aeree Cho, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J. Wang, Seongmin Lee, Benjamin Hoover, Duen Horng Chau</author><pubDate>Thu, 08 Aug 2024 17:49:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04619v1</guid></item><item><title>Better Alignment with Instruction Back-and-Forth Translation</title><link>http://arxiv.org/abs/2408.04614v1</link><description>We propose a new method, instruction back-and-forth translation, to constructhigh-quality synthetic data grounded in world knowledge for aligning largelanguage models (LLMs). Given documents from a web corpus, we generate andcurate synthetic instructions using the backtranslation approach proposed by Liet al.(2023a), and rewrite the responses to improve their quality further basedon the initial documents. Fine-tuning with the resulting (backtranslatedinstruction, rewritten response) pairs yields higher win rates on AlpacaEvalthan using other common instruction datasets such as Humpback, ShareGPT, OpenOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting theresponses with an LLM outperforms direct distillation, and the two generatedtext distributions exhibit significant distinction in embedding space. Furtheranalysis shows that our backtranslated instructions are of higher quality thanother sources of synthetic instructions, while our responses are more diverseand complex than those obtained from distillation. Overall we find thatinstruction back-and-forth translation combines the best of both worlds --making use of the information diversity and quantity found on the web, whileensuring the quality of the responses which is necessary for effectivealignment.</description><author>Thao Nguyen, Jeffrey Li, Sewoong Oh, Ludwig Schmidt, Jason Weston, Luke Zettlemoyer, Xian Li</author><pubDate>Thu, 08 Aug 2024 17:42:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04614v1</guid></item><item><title>Know Your Limits: A Survey of Abstention in Large Language Models</title><link>http://arxiv.org/abs/2407.18418v2</link><description>Abstention, the refusal of large language models (LLMs) to provide an answer,is increasingly recognized for its potential to mitigate hallucinations andenhance safety in LLM systems. In this survey, we introduce a framework toexamine abstention from three perspectives: the query, the model, and humanvalues. We organize the literature on abstention methods, benchmarks, andevaluation metrics using this framework, and discuss merits and limitations ofprior work. We further identify and motivate areas for future work, centeredaround whether abstention can be achieved as a meta-capability that transcendsspecific tasks or domains, while still providing opportunities to optimizeabstention abilities based on context.</description><author>Bingbing Wen, Jihan Yao, Shangbin Feng, Chenjun Xu, Yulia Tsvetkov, Bill Howe, Lucy Lu Wang</author><pubDate>Thu, 08 Aug 2024 17:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18418v2</guid></item><item><title>Quantifying the Impact of Population Shift Across Age and Sex for Abdominal Organ Segmentation</title><link>http://arxiv.org/abs/2408.04610v1</link><description>Deep learning-based medical image segmentation has seen tremendous progressover the last decade, but there is still relatively little transfer intoclinical practice. One of the main barriers is the challenge of domaingeneralisation, which requires segmentation models to maintain high performanceacross a wide distribution of image data. This challenge is amplified by themany factors that contribute to the diverse appearance of medical images, suchas acquisition conditions and patient characteristics. The impact of shiftingpatient characteristics such as age and sex on segmentation performance remainsrelatively under-studied, especially for abdominal organs, despite that this iscrucial for ensuring the fairness of the segmentation model. We perform thefirst study to determine the impact of population shift with respect to age andsex on abdominal CT image segmentation, by leveraging two large publicdatasets, and introduce a novel metric to quantify the impact. We find thatpopulation shift is a challenge similar in magnitude to cross-dataset shift forabdominal organ segmentation, and that the effect is asymmetric anddataset-dependent. We conclude that dataset diversity in terms of known patientcharacteristics is not necessarily equivalent to dataset diversity in terms ofimage features. This implies that simple population matching to ensure goodgeneralisation and fairness may be insufficient, and we recommend that fairnessresearch should be directed towards better understanding and quantifyingmedical image dataset diversity in terms of performance-relevantcharacteristics such as organ morphology.</description><author>Kate Čevora, Ben Glocker, Wenjia Bai</author><pubDate>Thu, 08 Aug 2024 17:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04610v1</guid></item><item><title>Risk and cross validation in ridge regression with correlated samples</title><link>http://arxiv.org/abs/2408.04607v1</link><description>Recent years have seen substantial advances in our understanding ofhigh-dimensional ridge regression, but existing theories assume that trainingexamples are independent. By leveraging recent techniques from random matrixtheory and free probability, we provide sharp asymptotics for the in- andout-of-sample risks of ridge regression when the data points have arbitrarycorrelations. We demonstrate that in this setting, the generalized crossvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.However, in the case where the noise residuals have the same correlations asthe data points, one can modify the GCV to yield an efficiently-computableunbiased estimator that concentrates in the high-dimensional limit, which wedub CorrGCV. We further extend our asymptotic analysis to the case where thetest point has nontrivial correlations with the training set, a setting oftenencountered in time series forecasting. Assuming knowledge of the correlationstructure of the time series, this again yields an extension of the GCVestimator, and sharply characterizes the degree to which such test points yieldan overly optimistic prediction of long-time risk. We validate the predictionsof our theory across a variety of high dimensional data.</description><author>Alexander Atanasov, Jacob A. Zavatone-Veth, Cengiz Pehlevan</author><pubDate>Thu, 08 Aug 2024 17:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04607v1</guid></item><item><title>Enhanced Prototypical Part Network (EPPNet) For Explainable Image Classification Via Prototypes</title><link>http://arxiv.org/abs/2408.04606v1</link><description>Explainable Artificial Intelligence (xAI) has the potential to enhance thetransparency and trust of AI-based systems. Although accurate predictions canbe made using Deep Neural Networks (DNNs), the process used to arrive at suchpredictions is usually hard to explain. In terms of perceptibly human-friendlyrepresentations, such as word phrases in text or super-pixels in images,prototype-based explanations can justify a model's decision. In this work, weintroduce a DNN architecture for image classification, the EnhancedPrototypical Part Network (EPPNet), which achieves strong performance whilediscovering relevant prototypes that can be used to explain the classificationresults. This is achieved by introducing a novel cluster loss that helps todiscover more relevant human-understandable prototypes. We also introduce afaithfulness score to evaluate the explainability of the results based on thediscovered prototypes. Our score not only accounts for the relevance of thelearned prototypes but also the performance of a model. Our evaluations on theCUB-200-2011 dataset show that the EPPNet outperforms state-of-the-artxAI-based methods, in terms of both classification accuracy and explainability</description><author>Bhushan Atote, Victor Sanchez</author><pubDate>Thu, 08 Aug 2024 17:26:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04606v1</guid></item><item><title>Fall Detection for Industrial Setups Using YOLOv8 Variants</title><link>http://arxiv.org/abs/2408.04605v1</link><description>This paper presents the development of an industrial fall detection systemutilizing YOLOv8 variants, enhanced by our proposed augmentation pipeline toincrease dataset variance and improve detection accuracy. Among the modelsevaluated, the YOLOv8m model, consisting of 25.9 million parameters and 79.1GFLOPs, demonstrated a respectable balance between computational efficiency anddetection performance, achieving a mean Average Precision (mAP) of 0.971 at 50%Intersection over Union (IoU) across both "Fall Detected" and "Human in Motion"categories. Although the YOLOv8l and YOLOv8x models presented higher precisionand recall, particularly in fall detection, their higher computational demandsand model size make them less suitable for resource-constrained environments.</description><author>Gracile Astlin Pereira</author><pubDate>Thu, 08 Aug 2024 17:24:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04605v1</guid></item><item><title>Towards High-resolution 3D Anomaly Detection via Group-Level Feature Contrastive Learning</title><link>http://arxiv.org/abs/2408.04604v1</link><description>High-resolution point clouds~(HRPCD) anomaly detection~(AD) plays a criticalrole in precision machining and high-end equipment manufacturing. Despiteconsiderable 3D-AD methods that have been proposed recently, they still cannotmeet the requirements of the HRPCD-AD task. There are several challenges: i) Itis difficult to directly capture HRPCD information due to large amounts ofpoints at the sample level; ii) The advanced transformer-based methods usuallyobtain anisotropic features, leading to degradation of the representation; iii)The proportion of abnormal areas is very small, which makes it difficult tocharacterize. To address these challenges, we propose a novel group-levelfeature-based network, called Group3AD, which has a significantly efficientrepresentation ability. First, we design an Intercluster UniformityNetwork~(IUN) to present the mapping of different groups in the feature spaceas several clusters, and obtain a more uniform distribution between clustersrepresenting different parts of the point clouds in the feature space. Then, anIntracluster Alignment Network~(IAN) is designed to encourage groups within thecluster to be distributed tightly in the feature space. In addition, we proposean Adaptive Group-Center Selection~(AGCS) based on geometric information toimprove the pixel density of potential anomalous regions during inference. Theexperimental results verify the effectiveness of our proposed Group3AD, whichsurpasses Reg3D-AD by the margin of 5\% in terms of object-level AUROC onReal3D-AD. We provide the code and supplementary information on our website:https://github.com/M-3LAB/Group3AD.</description><author>Hongze Zhu, Guoyang Xie, Chengbin Hou, Tao Dai, Can Gao, Jinbao Wang, Linlin Shen</author><pubDate>Thu, 08 Aug 2024 17:24:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04604v1</guid></item><item><title>Improving Network Interpretability via Explanation Consistency Evaluation</title><link>http://arxiv.org/abs/2408.04600v1</link><description>While deep neural networks have achieved remarkable performance, they tend tolack transparency in prediction. The pursuit of greater interpretability inneural networks often results in a degradation of their original performance.Some works strive to improve both interpretability and performance, but theyprimarily depend on meticulously imposed conditions. In this paper, we proposea simple yet effective framework that acquires more explainable activationheatmaps and simultaneously increase the model performance, without the needfor any extra supervision. Specifically, our concise framework introduces a newmetric, i.e., explanation consistency, to reweight the training samplesadaptively in model learning. The explanation consistency metric is utilized tomeasure the similarity between the model's visual explanations of the originalsamples and those of semantic-preserved adversarial samples, whose backgroundregions are perturbed by using image adversarial attack techniques. Ourframework then promotes the model learning by paying closer attention to thosetraining samples with a high difference in explanations (i.e., low explanationconsistency), for which the current model cannot provide robustinterpretations. Comprehensive experimental results on various benchmarksdemonstrate the superiority of our framework in multiple aspects, includinghigher recognition accuracy, greater data debiasing capability, strongernetwork robustness, and more precise localization ability on both regularnetworks and interpretable networks. We also provide extensive ablation studiesand qualitative analyses to unveil the detailed contribution of each component.</description><author>Hefeng Wu, Hao Jiang, Keze Wang, Ziyi Tang, Xianghuan He, Liang Lin</author><pubDate>Thu, 08 Aug 2024 17:20:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04600v1</guid></item><item><title>Code-switching in text and speech reveals information-theoretic audience design</title><link>http://arxiv.org/abs/2408.04596v1</link><description>In this work, we use language modeling to investigate the factors thatinfluence code-switching. Code-switching occurs when a speaker alternatesbetween one language variety (the primary language) and another (the secondarylanguage), and is widely observed in multilingual contexts. Recent work hasshown that code-switching is often correlated with areas of high informationload in the primary language, but it is unclear whether high primary languageload only makes the secondary language relatively easier to produce atcode-switching points (speaker-driven code-switching), or whethercode-switching is additionally used by speakers to signal the need for greaterattention on the part of listeners (audience-driven code-switching). In thispaper, we use bilingual Chinese-English online forum posts and transcripts ofspontaneous Chinese-English speech to replicate prior findings that highprimary language (Chinese) information load is correlated with switches to thesecondary language (English). We then demonstrate that the information load ofthe English productions is even higher than that of meaning equivalent Chinesealternatives, and these are therefore not easier to produce, providing evidenceof audience-driven influences in code-switching at the level of thecommunication channel, not just at the sociolinguistic level, in both writingand speech.</description><author>Debasmita Bhattacharya, Marten van Schijndel</author><pubDate>Thu, 08 Aug 2024 17:14:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04596v1</guid></item><item><title>Inference with the Upper Confidence Bound Algorithm</title><link>http://arxiv.org/abs/2408.04595v1</link><description>In this paper, we discuss the asymptotic behavior of the Upper ConfidenceBound (UCB) algorithm in the context of multiarmed bandit problems and discussits implication in downstream inferential tasks. While inferential tasks becomechallenging when data is collected in a sequential manner, we argue that thisproblem can be alleviated when the sequential algorithm at hand satisfiescertain stability property. This notion of stability is motivated from theseminal work of Lai and Wei (1982). Our first main result shows that such astability property is always satisfied for the UCB algorithm, and as a resultthe sample means for each arm are asymptotically normal. Next, we examine thestability properties of the UCB algorithm when the number of arms $K$ isallowed to grow with the number of arm pulls $T$. We show that in such a casethe arms are stable when $\frac{\log K}{\log T} \rightarrow 0$, and the numberof near-optimal arms are large.</description><author>Koulik Khamaru, Cun-Hui Zhang</author><pubDate>Thu, 08 Aug 2024 17:11:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04595v1</guid></item><item><title>Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2408.04594v1</link><description>High-performance Multimodal Large Language Models (MLLMs) rely heavily ondata quality. This study introduces a novel dataset named Img-Diff, designed toenhance fine-grained image recognition in MLLMs by leveraging insights fromcontrastive learning and image difference captioning. By analyzing objectdifferences between similar images, we challenge models to identify bothmatching and distinct components. We utilize the Stable-Diffusion-XL model andadvanced image editing techniques to create pairs of similar images thathighlight object replacements. Our methodology includes a Difference AreaGenerator for object differences identifying, followed by a Difference CaptionsGenerator for detailed difference descriptions. The result is a relativelysmall but high-quality dataset of "object replacement" samples. We use the theproposed dataset to fine-tune state-of-the-art (SOTA) MLLMs such as MGM-7B,yielding comprehensive improvements of performance scores over SOTA models thattrained with larger-scale datasets, in numerous image difference and VisualQuestion Answering tasks. For instance, our trained models notably surpass theSOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigatealternative methods for generating image difference data through "objectremoval" and conduct thorough evaluation to confirm the dataset's diversity,quality, and robustness, presenting several insights on synthesis of suchcontrastive dataset. To encourage further research and advance the field ofmultimodal data synthesis and enhancement of MLLMs' fundamental capabilitiesfor image understanding, we release our codes and dataset athttps://github.com/modelscope/data-juicer/tree/ImgDiff.</description><author>Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen</author><pubDate>Thu, 08 Aug 2024 17:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04594v1</guid></item><item><title>Self-Taught Evaluators</title><link>http://arxiv.org/abs/2408.02666v2</link><description>Model-based evaluation is at the heart of successful model development -- asa reward model for training, and as a replacement for human evaluation. Totrain such evaluators, the standard approach is to collect a large amount ofhuman preference judgments over model responses, which is costly and the databecomes stale as models improve. In this work, we present an approach that aimsto im-prove evaluators without human annotations, using synthetic training dataonly. Starting from unlabeled instructions, our iterative self-improvementscheme generates contrasting model outputs and trains an LLM-as-a-Judge toproduce reasoning traces and final judgments, repeating this training at eachnew iteration using the improved predictions. Without any labeled preferencedata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperformscommonly used LLM judges such as GPT-4 and matches the performance of thetop-performing reward models trained with labeled examples.</description><author>Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li</author><pubDate>Thu, 08 Aug 2024 17:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02666v2</guid></item><item><title>SAM 2 in Robotic Surgery: An Empirical Evaluation for Robustness and Generalization in Surgical Video Segmentation</title><link>http://arxiv.org/abs/2408.04593v1</link><description>The recent Segment Anything Model (SAM) 2 has demonstrated remarkablefoundational competence in semantic segmentation, with its memory mechanism andmask decoder further addressing challenges in video tracking and objectocclusion, thereby achieving superior results in interactive segmentation forboth images and videos. Building upon our previous empirical studies, wefurther explore the zero-shot segmentation performance of SAM 2 inrobot-assisted surgery based on prompts, alongside its robustness againstreal-world corruption. For static images, we employ two forms of prompts:1-point and bounding box, while for video sequences, the 1-point prompt isapplied to the initial frame. Through extensive experimentation on the MICCAIEndoVis 2017 and EndoVis 2018 benchmarks, SAM 2, when utilizing bounding boxprompts, outperforms state-of-the-art (SOTA) methods in comparativeevaluations. The results with point prompts also exhibit a substantialenhancement over SAM's capabilities, nearing or even surpassing existingunprompted SOTA methodologies. Besides, SAM 2 demonstrates improved inferencespeed and less performance degradation against various image corruption.Although slightly unsatisfactory results remain in specific edges or regions,SAM 2's robust adaptability to 1-point prompts underscores its potential fordownstream surgical tasks with limited prompt requirements.</description><author>Jieming Yu, An Wang, Wenzhen Dong, Mengya Xu, Mobarakol Islam, Jie Wang, Long Bai, Hongliang Ren</author><pubDate>Thu, 08 Aug 2024 17:08:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04593v1</guid></item><item><title>HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain Shifts</title><link>http://arxiv.org/abs/2408.04591v1</link><description>Generalized Category Discovery (GCD) is a challenging task in which, given apartially labelled dataset, models must categorize all unlabelled instances,regardless of whether they come from labelled categories or from new ones. Inthis paper, we challenge a remaining assumption in this task: that all imagesshare the same domain. Specifically, we introduce a new task and method tohandle GCD when the unlabelled data also contains images from different domainsto the labelled set. Our proposed `HiLo' networks extract High-level semanticand Low-level domain features, before minimizing the mutual information betweenthe representations. Our intuition is that the clusterings based on domaininformation and semantic information should be independent. We further extendour method with a specialized domain augmentation tailored for the GCD task, aswell as a curriculum learning approach. Finally, we construct a benchmark fromcorrupted fine-grained datasets as well as a large-scale evaluation onDomainNet with real-world domain shifts, reimplementing a number of GCDbaselines in this setting. We demonstrate that HiLo outperforms SoTA categorydiscovery models by a large margin on all evaluations.</description><author>Hongjun Wang, Sagar Vaze, Kai Han</author><pubDate>Thu, 08 Aug 2024 17:04:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04591v1</guid></item><item><title>Learn To Learn More Precisely</title><link>http://arxiv.org/abs/2408.04590v1</link><description>Meta-learning has been extensively applied in the domains of few-shotlearning and fast adaptation, achieving remarkable performance. WhileMeta-learning methods like Model-Agnostic Meta-Learning (MAML) and its variantsprovide a good set of initial parameters for the model, the model still tendsto learn shortcut features, which leads to poor generalization. In this paper,we propose the formal conception of "learn to learn more precisely", which aimsto make the model learn precise target knowledge from data and reduce theeffect of noisy knowledge, such as background and noise. To achieve thistarget, we proposed a simple and effective meta-learning framework named MetaSelf-Distillation(MSD) to maximize the consistency of learned knowledge,enhancing the models' ability to learn precise target knowledge. In the innerloop, MSD uses different augmented views of the same support data to update themodel respectively. Then in the outer loop, MSD utilizes the same query data tooptimize the consistency of learned knowledge, enhancing the model's ability tolearn more precisely. Our experiment demonstrates that MSD exhibits remarkableperformance in few-shot classification tasks in both standard and augmentedscenarios, effectively boosting the accuracy and consistency of knowledgelearned by the model.</description><author>Runxi Cheng, Yongxian Wei, Xianglong He, Wanyun Zhu, Songsong Huang, Fei Richard Yu, Fei Ma, Chun Yuan</author><pubDate>Thu, 08 Aug 2024 17:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04590v1</guid></item><item><title>The Distributional Uncertainty of the SHAP score in Explainable Machine Learning</title><link>http://arxiv.org/abs/2401.12731v2</link><description>Attribution scores reflect how important the feature values in an inputentity are for the output of a machine learning model. One of the most popularattribution scores is the SHAP score, which is an instantiation of the generalShapley value used in coalition game theory. The definition of this scorerelies on a probability distribution on the entity population. Since the exactdistribution is generally unknown, it needs to be assigned subjectively or beestimated from data, which may lead to misleading feature scores. In thispaper, we propose a principled framework for reasoning on SHAP scores underunknown entity population distributions. In our framework, we consider anuncertainty region that contains the potential distributions, and the SHAPscore of a feature becomes a function defined over this region. We study thebasic problems of finding maxima and minima of this function, which allows usto determine tight ranges for the SHAP scores of all features. In particular,we pinpoint the complexity of these problems, and other related ones, showingthem to be NP-complete. Finally, we present experiments on a real-worlddataset, showing that our framework may contribute to a more robust featurescoring.</description><author>Santiago Cifuentes, Leopoldo Bertossi, Nina Pardal, Sergio Abriola, Maria Vanina Martinez, Miguel Romero</author><pubDate>Thu, 08 Aug 2024 16:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12731v2</guid></item><item><title>Sampling for View Synthesis: From Local Light Field Fusion to Neural Radiance Fields and Beyond</title><link>http://arxiv.org/abs/2408.04586v1</link><description>Capturing and rendering novel views of complex real-world scenes is along-standing problem in computer graphics and vision, with applications inaugmented and virtual reality, immersive experiences and 3D photography. Theadvent of deep learning has enabled revolutionary advances in this area,classically known as image-based rendering. However, previous approachesrequire intractably dense view sampling or provide little or no guidance forhow users should sample views of a scene to reliably render high-quality novelviews. Local light field fusion proposes an algorithm for practical viewsynthesis from an irregular grid of sampled views that first expands eachsampled view into a local light field via a multiplane image scenerepresentation, then renders novel views by blending adjacent local lightfields. Crucially, we extend traditional plenoptic sampling theory to derive abound that specifies precisely how densely users should sample views of a givenscene when using our algorithm. We achieve the perceptual quality of Nyquistrate view sampling while using up to 4000x fewer views. Subsequent developmentshave led to new scene representations for deep learning with view synthesis,notably neural radiance fields, but the problem of sparse view synthesis from asmall number of images has only grown in importance. We reprise some of therecent results on sparse and even single image view synthesis, while posing thequestion of whether prescriptive sampling guidelines are feasible for the newgeneration of image-based rendering algorithms.</description><author>Ravi Ramamoorthi</author><pubDate>Thu, 08 Aug 2024 16:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04586v1</guid></item><item><title>Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness</title><link>http://arxiv.org/abs/2408.04585v1</link><description>With the increasing demand for practical applications of Large LanguageModels (LLMs), many attention-efficient models have been developed to balanceperformance and computational cost. However, the adversarial robustness ofthese models remains under-explored. In this work, we design a framework toinvestigate the trade-off between efficiency, performance, and adversarialrobustness of LLMs by comparing three prominent models with varying levels ofcomplexity and efficiency -- Transformer++, Gated Linear Attention (GLA)Transformer, and MatMul-Free LM -- utilizing the GLUE and AdvGLUE datasets. TheAdvGLUE dataset extends the GLUE dataset with adversarial samples designed tochallenge model robustness. Our results show that while the GLA Transformer andMatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstratehigher efficiency and either superior or comparative robustness on AdvGLUEtasks compared to Transformer++ across different attack levels. These findingshighlight the potential of simplified architectures to achieve a compellingbalance between efficiency, performance, and adversarial robustness, offeringvaluable insights for applications where resource constraints and resilience toadversarial attacks are critical.</description><author>Xiaojing Fan, Chunliang Tao</author><pubDate>Thu, 08 Aug 2024 16:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04585v1</guid></item><item><title>Unveiling the Power of Sparse Neural Networks for Feature Selection</title><link>http://arxiv.org/abs/2408.04583v1</link><description>Sparse Neural Networks (SNNs) have emerged as powerful tools for efficientfeature selection. Leveraging the dynamic sparse training (DST) algorithmswithin SNNs has demonstrated promising feature selection capabilities whiledrastically reducing computational overheads. Despite these advancements,several critical aspects remain insufficiently explored for feature selection.Questions persist regarding the choice of the DST algorithm for networktraining, the choice of metric for ranking features/neurons, and thecomparative performance of these methods across diverse datasets when comparedto dense networks. This paper addresses these gaps by presenting acomprehensive systematic analysis of feature selection with sparse neuralnetworks. Moreover, we introduce a novel metric considering sparse neuralnetwork characteristics, which is designed to quantify feature importancewithin the context of SNNs. Our findings show that feature selection with SNNstrained with DST algorithms can achieve, on average, more than $50\%$ memoryand $55\%$ FLOPs reduction compared to the dense networks, while outperformingthem in terms of the quality of the selected features. Our code and thesupplementary material are available on GitHub(\url{https://github.com/zahraatashgahi/Neuron-Attribution}).</description><author>Zahra Atashgahi, Tennison Liu, Mykola Pechenizkiy, Raymond Veldhuis, Decebal Constantin Mocanu, Mihaela van der Schaar</author><pubDate>Thu, 08 Aug 2024 16:48:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04583v1</guid></item><item><title>TPA3D: Triplane Attention for Fast Text-to-3D Generation</title><link>http://arxiv.org/abs/2312.02647v2</link><description>Due to the lack of large-scale text-3D correspondence data, recent text-to-3Dgeneration works mainly rely on utilizing 2D diffusion models for synthesizing3D data. Since diffusion-based methods typically require significantoptimization time for both training and inference, the use of GAN-based modelswould still be desirable for fast 3D generation. In this work, we proposeTriplane Attention for text-guided 3D generation (TPA3D), an end-to-endtrainable GAN-based deep learning model for fast text-to-3D generation. Withonly 3D shape data and their rendered 2D images observed during training, ourTPA3D is designed to retrieve detailed visual descriptions for synthesizing thecorresponding 3D mesh data. This is achieved by the proposed attentionmechanisms on the extracted sentence and word-level text features. In ourexperiments, we show that TPA3D generates high-quality 3D textured shapesaligned with fine-grained descriptions, while impressive computation efficiencycan be observed.</description><author>Bin-Shih Wu, Hong-En Chen, Sheng-Yu Huang, Yu-Chiang Frank Wang</author><pubDate>Thu, 08 Aug 2024 16:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02647v2</guid></item><item><title>SAM2-Adapter: Evaluating &amp; Adapting Segment Anything 2 in Downstream Tasks: Camouflage, Shadow, Medical Image Segmentation, and More</title><link>http://arxiv.org/abs/2408.04579v1</link><description>The advent of large models, also known as foundation models, hassignificantly transformed the AI research landscape, with models like SegmentAnything (SAM) achieving notable success in diverse image segmentationscenarios. Despite its advancements, SAM encountered limitations in handlingsome complex low-level segmentation tasks like camouflaged object and medicalimaging. In response, in 2023, we introduced SAM-Adapter, which demonstratedimproved performance on these challenging tasks. Now, with the release ofSegment Anything 2 (SAM2), a successor with enhanced architecture and a largertraining corpus, we reassess these challenges. This paper introducesSAM2-Adapter, the first adapter designed to overcome the persistent limitationsobserved in SAM2 and achieve new state-of-the-art (SOTA) results in specificdownstream tasks including medical image segmentation, camouflaged (concealed)object detection, and shadow detection. SAM2-Adapter builds on theSAM-Adapter's strengths, offering enhanced generalizability and composabilityfor diverse applications. We present extensive experimental resultsdemonstrating SAM2-Adapter's effectiveness. We show the potential and encouragethe research community to leverage the SAM2 model with our SAM2-Adapter forachieving superior segmentation outcomes. Code, pre-trained models, and dataprocessing protocols are available athttp://tianrun-chen.github.io/SAM-Adaptor/</description><author>Tianrun Chen, Ankang Lu, Lanyun Zhu, Chaotao Ding, Chunan Yu, Deyi Ji, Zejian Li, Lingyun Sun, Papa Mao, Ying Zang</author><pubDate>Thu, 08 Aug 2024 16:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04579v1</guid></item><item><title>SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals</title><link>http://arxiv.org/abs/2408.04575v1</link><description>Explainable Artificial Intelligence (XAI) is essential for enhancing thetransparency and accountability of AI models, especially in natural languageprocessing (NLP) tasks. This paper introduces SCENE (Soft CounterfactualEvaluation for Natural language Explainability), a novel evaluation method thatleverages large language models (LLMs) to generate Soft Counterfactualexplanations in a zero-shot manner. By focusing on token-based substitutions,SCENE creates contextually appropriate and seman-tically meaningful SoftCounterfactuals without extensive fine-tuning. SCENE adopts Validitysoft andCsoft metrics to evaluate the effectiveness of model-agnostic XAI methods intext classification tasks. Applied to CNN, RNN, and BERT architectures, SCENEprovides valuable insights into the strengths and limitations of various XAItechniques.</description><author>Haoran Zheng, Utku Pamuksuz</author><pubDate>Thu, 08 Aug 2024 16:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04575v1</guid></item><item><title>Automatic Generation of Behavioral Test Cases For Natural Language Processing Using Clustering and Prompting</title><link>http://arxiv.org/abs/2408.00161v2</link><description>Recent work in behavioral testing for natural language processing (NLP)models, such as Checklist, is inspired by related paradigms in softwareengineering testing. They allow evaluation of general linguistic capabilitiesand domain understanding, hence can help evaluate conceptual soundness andidentify model weaknesses. However, a major challenge is the creation of testcases. The current packages rely on semi-automated approach using manualdevelopment which requires domain expertise and can be time consuming. Thispaper introduces an automated approach to develop test cases by exploiting thepower of large language models and statistical techniques. It clusters the textrepresentations to carefully construct meaningful groups and then applyprompting techniques to automatically generate Minimal Functionality Tests(MFT). The well-known Amazon Reviews corpus is used to demonstrate ourapproach. We analyze the behavioral test profiles across four differentclassification algorithms and discuss the limitations and strengths of thosemodels.</description><author>Ying Li, Rahul Singh, Tarun Joshi, Agus Sudjianto</author><pubDate>Thu, 08 Aug 2024 16:31:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00161v2</guid></item><item><title>Mathematical Programming For Adaptive Experiments</title><link>http://arxiv.org/abs/2408.04570v1</link><description>Adaptive experimentation can significantly improve statistical power, butstandard algorithms overlook important practical issues including batched anddelayed feedback, personalization, non-stationarity, multiple objectives, andconstraints. To address these issues, the current algorithm design paradigmcrafts tailored methods for each problem instance. Since it is infeasible todevise novel algorithms for every real-world instance, practitioners often haveto resort to suboptimal approximations that do not address all of theirchallenges. Moving away from developing bespoke algorithms for each setting, wepresent a mathematical programming view of adaptive experimentation that canflexibly incorporate a wide range of objectives, constraints, and statisticalprocedures. By formulating a dynamic program in the batched limit, our modelingframework enables the use of scalable optimization methods (e.g., SGD andauto-differentiation) to solve for treatment allocations. We evaluate ourframework on benchmarks modeled after practical challenges such asnon-stationarity, personalization, multi-objectives, and constraints. Unlikebespoke algorithms such as modified variants of Thomson sampling, ourmathematical programming approach provides remarkably robust performance acrossinstances.</description><author>Ethan Che, Daniel R. Jiang, Hongseok Namkoong, Jimmy Wang</author><pubDate>Thu, 08 Aug 2024 16:29:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04570v1</guid></item><item><title>Activation thresholds and expressiveness of polynomial neural networks</title><link>http://arxiv.org/abs/2408.04569v1</link><description>Polynomial neural networks have been implemented in a range of applicationsand present an advantageous framework for theoretical machine learning. Apolynomial neural network of fixed architecture and activation degree gives analgebraic map from the network's weights to a set of polynomials. The image ofthis map is the space of functions representable by the network. Its Zariskiclosure is an affine variety known as a neurovariety. The dimension of apolynomial neural network's neurovariety provides a measure of itsexpressivity. In this work, we introduce the notion of the activation thresholdof a network architecture which expresses when the dimension of a neurovarietyachieves its theoretical maximum. In addition, we prove expressiveness resultsfor polynomial neural networks with equi-width~architectures.</description><author>Bella Finkel, Jose Israel Rodriguez, Chenxi Wu, Thomas Yahl</author><pubDate>Thu, 08 Aug 2024 16:28:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04569v1</guid></item><item><title>Learning Fine-Grained Grounded Citations for Attributed Large Language Models</title><link>http://arxiv.org/abs/2408.04568v1</link><description>Despite the impressive performance on information-seeking tasks, largelanguage models (LLMs) still struggle with hallucinations. Attributed LLMs,which augment generated text with in-line citations, have shown potential inmitigating hallucinations and improving verifiability. However, currentapproaches suffer from suboptimal citation quality due to their reliance onin-context learning. Furthermore, the practice of citing only coarse documentidentifiers makes it challenging for users to perform fine-grainedverification. In this work, we introduce FRONT, a training framework designedto teach LLMs to generate Fine-Grained Grounded Citations. By grounding modeloutputs in fine-grained supporting quotes, these quotes guide the generation ofgrounded and consistent responses, not only improving citation quality but alsofacilitating fine-grained verification. Experiments on the ALCE benchmarkdemonstrate the efficacy of FRONT in generating superior grounded responses andhighly supportive citations. With LLaMA-2-7B, the framework significantlyoutperforms all the baselines, achieving an average of 14.21% improvement incitation quality across all datasets, even surpassing ChatGPT.</description><author>Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Weihong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng, Duyu Tang, Dandan Tu, Bing Qin</author><pubDate>Thu, 08 Aug 2024 16:28:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04568v1</guid></item><item><title>Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User's Casual Sketches</title><link>http://arxiv.org/abs/2408.04567v1</link><description>3D Content Generation is at the heart of many computer graphics applications,including video gaming, film-making, virtual and augmented reality, etc. Thispaper proposes a novel deep-learning based approach for automaticallygenerating interactive and playable 3D game scenes, all from the user's casualprompts such as a hand-drawn sketch. Sketch-based input offers a natural, andconvenient way to convey the user's design intention in the content creationprocess. To circumvent the data-deficient challenge in learning (i.e. the lackof large training data of 3D scenes), our method leverages a pre-trained 2Ddenoising diffusion model to generate a 2D image of the scene as the conceptualguidance. In this process, we adopt the isometric projection mode to factor outunknown camera poses while obtaining the scene layout. From the generatedisometric image, we use a pre-trained image understanding method to segment theimage into meaningful parts, such as off-ground objects, trees, and buildings,and extract the 2D scene layout. These segments and layouts are subsequentlyfed into a procedural content generation (PCG) engine, such as a 3D video gameengine like Unity or Unreal, to create the 3D scene. The resulting 3D scene canbe seamlessly integrated into a game development environment and is readilyplayable. Extensive tests demonstrate that our method can efficiently generatehigh-quality and interactive 3D game scenes with layouts that closely followthe user's intention.</description><author>Yongzhi Xu, Yonhon Ng, Yifu Wang, Inkyu Sa, Yunfei Duan, Yang Li, Pan Ji, Hongdong Li</author><pubDate>Thu, 08 Aug 2024 16:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04567v1</guid></item><item><title>Loss Functions and Metrics in Deep Learning</title><link>http://arxiv.org/abs/2307.02694v3</link><description>When training or evaluating deep learning models, two essential parts arepicking the proper loss function and deciding on performance metrics. In thispaper, we provide a comprehensive overview of the most common loss functionsand metrics used across many different types of deep learning tasks, fromgeneral tasks such as regression and classification to more specific tasks inComputer Vision and Natural Language Processing. We introduce the formula foreach loss and metric, discuss their strengths and limitations, and describe howthese methods can be applied to various problems within deep learning. We hopethis work serves as a reference for researchers and practitioners in the field,helping them make informed decisions when selecting the most appropriate lossfunction and performance metrics for their deep learning projects.</description><author>Juan Terven, Diana M. Cordova-Esparza, Alfonso Ramirez-Pedraza, Edgar A. Chavez-Urbiola, Julio A. Romero-Gonzalez</author><pubDate>Thu, 08 Aug 2024 16:24:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02694v3</guid></item><item><title>Dialectical Reconciliation via Structured Argumentative Dialogues</title><link>http://arxiv.org/abs/2306.14694v3</link><description>We present a novel framework designed to extend model reconciliationapproaches, commonly used in human-aware planning, for enhanced human-AIinteraction. By adopting a structured argumentation-based dialogue paradigm,our framework enables dialectical reconciliation to address knowledgediscrepancies between an explainer (AI agent) and an explainee (human user),where the goal is for the explainee to understand the explainer's decision. Weformally describe the operational semantics of our proposed framework,providing theoretical guarantees. We then evaluate the framework's efficacy``in the wild'' via computational and human-subject experiments. Our findingssuggest that our framework offers a promising direction for fostering effectivehuman-AI interactions in domains where explainability is important.</description><author>Stylianos Loukas Vasileiou, Ashwin Kumar, William Yeoh, Tran Cao Son, Francesca Toni</author><pubDate>Thu, 08 Aug 2024 16:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14694v3</guid></item><item><title>ESP-MedSAM: Efficient Self-Prompting SAM for Universal Domain-Generalized Image Segmentation</title><link>http://arxiv.org/abs/2407.14153v3</link><description>The universality of deep neural networks across different modalities andtheir generalization capabilities to unseen domains play an essential role inmedical image segmentation. The recent Segment Anything Model (SAM) hasdemonstrated its potential in both settings. However, the huge computationalcosts, demand for manual annotations as prompts and conflict-prone decodingprocess of SAM degrade its generalizability and applicability in clinicalscenarios. To address these issues, we propose an efficient self-prompting SAMfor universal domain-generalized medical image segmentation, named ESP-MedSAM.Specifically, we first devise the Multi-Modal Decoupled Knowledge Distillation(MMDKD) strategy to construct a lightweight semi-parameter sharing imageencoder that produces discriminative visual features for diverse modalities.Further, we introduce the Self-Patch Prompt Generator (SPPG) to automaticallygenerate high-quality dense prompt embeddings for guiding segmentationdecoding. Finally, we design the Query-Decoupled Modality Decoder (QDMD) thatleverages a one-to-one strategy to provide an independent decoding channel forevery modality. Extensive experiments indicate that ESP-MedSAM outperformsstate-of-the-arts in diverse medical imaging segmentation tasks, displayingsuperior modality universality and generalization capabilities. Especially,ESP-MedSAM uses only 4.5\% parameters compared to SAM-H. The source code isavailable at https://github.com/xq141839/ESP-MedSAM.</description><author>Qing Xu, Jiaxuan Li, Xiangjian He, Ziyu Liu, Zhen Chen, Wenting Duan, Chenxin Li, Maggie M. He, Fiseha B. Tesema, Wooi P. Cheah, Yi Wang, Rong Qu, Jonathan M. Garibaldi</author><pubDate>Thu, 08 Aug 2024 16:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14153v3</guid></item><item><title>Conversational Prompt Engineering</title><link>http://arxiv.org/abs/2408.04560v1</link><description>Prompts are how humans communicate with LLMs. Informative prompts areessential for guiding LLMs to produce the desired output. However, promptengineering is often tedious and time-consuming, requiring significantexpertise, limiting its widespread use. We propose Conversational PromptEngineering (CPE), a user-friendly tool that helps users create personalizedprompts for their specific tasks. CPE uses a chat model to briefly interactwith users, helping them articulate their output preferences and integratingthese into the prompt. The process includes two main stages: first, the modeluses user-provided unlabeled data to generate data-driven questions and utilizeuser responses to shape the initial instruction. Then, the model shares theoutputs generated by the instruction and uses user feedback to further refinethe instruction and the outputs. The final result is a few-shot prompt, wherethe outputs approved by the user serve as few-shot examples. A user study onsummarization tasks demonstrates the value of CPE in creating personalized,high-performing prompts. The results suggest that the zero-shot prompt obtainedis comparable to its - much longer - few-shot counterpart, indicatingsignificant savings in scenarios involving repetitive tasks with large textvolumes.</description><author>Liat Ein-Dor, Orith Toledo-Ronen, Artem Spector, Shai Gretz, Lena Dankin, Alon Halfon, Yoav Katz, Noam Slonim</author><pubDate>Thu, 08 Aug 2024 16:18:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04560v1</guid></item><item><title>Environment Complexity and Nash Equilibria in a Sequential Social Dilemma</title><link>http://arxiv.org/abs/2408.02148v2</link><description>Multi-agent reinforcement learning (MARL) methods, while effective inzero-sum or positive-sum games, often yield suboptimal outcomes in general-sumgames where cooperation is essential for achieving globally optimal outcomes.Matrix game social dilemmas, which abstract key aspects of general-suminteractions, such as cooperation, risk, and trust, fail to model the temporaland spatial dynamics characteristic of real-world scenarios. In response, ourstudy extends matrix game social dilemmas into more complex, higher-dimensionalMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemmato more closely match the decision-space of a one-shot matrix game while alsointroducing variable environment complexity. Our findings indicate that ascomplexity increases, MARL agents trained in these environments converge tosuboptimal strategies, consistent with the risk-dominant Nash equilibriastrategies found in matrix games. Our work highlights the impact of environmentcomplexity on achieving optimal outcomes in higher-dimensional game-theoreticMARL environments.</description><author>Mustafa Yasir, Andrew Howes, Vasilios Mavroudis, Chris Hicks</author><pubDate>Thu, 08 Aug 2024 16:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02148v2</guid></item><item><title>Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models</title><link>http://arxiv.org/abs/2408.04556v1</link><description>Large language models (LLMs) have exhibited remarkable proficiency across adiverse array of natural language processing (NLP) tasks. However, adaptingLLMs to downstream applications typically necessitates computationallyintensive and memory-demanding fine-tuning procedures. To mitigate theseburdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as apromising approach to tailor LLMs with minimal computational overhead. WhilePEFT methods offer substantial advantages, they do not fully address thepervasive issue of bias propagation from pre-training data. In this work, weintroduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT methoddesigned to counteract bias inheritance. BA-LoRA incorporates three distinctregularization terms: (1) consistency regularizer, (2) diversity regularizer,and (3) singular vector decomposition regularizer. These regularizerscollectively aim to improve the generative models' consistency, diversity, andgeneralization capabilities during the fine-tuning process. Through extensiveexperiments on a variety of natural language understanding (NLU) and naturallanguage generation (NLG) tasks, employing prominent LLMs such as LLaMA,Mistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance ofLoRA and its state-of-the-art variants. Moreover, our method effectivelymitigates the deleterious effects of pre-training bias, leading to morereliable and robust model outputs. The code is available athttps://github.com/cyp-jlu-ai/BA-LoRA.</description><author>Yupeng Chang, Yi Chang, Yuan Wu</author><pubDate>Thu, 08 Aug 2024 16:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04556v1</guid></item><item><title>Study of detecting behavioral signatures within DeepFake videos</title><link>http://arxiv.org/abs/2208.03561v2</link><description>There is strong interest in the generation of synthetic video imagery ofpeople talking for various purposes, including entertainment, communication,training, and advertisement. With the development of deep fake generationmodels, synthetic video imagery will soon be visually indistinguishable to thenaked eye from a naturally capture video. In addition, many methods arecontinuing to improve to avoid more careful, forensic visual analysis. Somedeep fake videos are produced through the use of facial puppetry, whichdirectly controls the head and face of the synthetic image through themovements of the actor, allow the actor to 'puppet' the image of another. Inthis paper, we address the question of whether one person's movements can bedistinguished from the original speaker by controlling the visual appearance ofthe speaker but transferring the behavior signals from another source. Weconduct a study by comparing synthetic imagery that: 1) originates from adifferent person speaking a different utterance, 2) originates from the sameperson speaking a different utterance, and 3) originates from a differentperson speaking the same utterance. Our study shows that synthetic videos inall three cases are seen as less real and less engaging than the originalsource video. Our results indicate that there could be a behavioral signaturethat is detectable from a person's movements that is separate from their visualappearance, and that this behavioral signature could be used to distinguish adeep fake from a properly captured video.</description><author>Qiaomu Miao, Sinhwa Kang, Stacy Marsella, Steve DiPaola, Chao Wang, Ari Shapiro</author><pubDate>Thu, 08 Aug 2024 16:12:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.03561v2</guid></item><item><title>Uncertainty for Active Learning on Graphs</title><link>http://arxiv.org/abs/2405.01462v2</link><description>Uncertainty Sampling is an Active Learning strategy that aims to improve thedata efficiency of machine learning models by iteratively acquiring labels ofdata points with the highest uncertainty. While it has proven effective forindependent data its applicability to graphs remains under-explored. We proposethe first extensive study of Uncertainty Sampling for node classification: (1)We benchmark Uncertainty Sampling beyond predictive uncertainty and highlight asignificant performance gap to other Active Learning strategies. (2) We developground-truth Bayesian uncertainty estimates in terms of the data generatingprocess and prove their effectiveness in guiding Uncertainty Sampling towardoptimal queries. We confirm our results on synthetic data and design anapproximate approach that consistently outperforms other uncertainty estimatorson real datasets. (3) Based on this analysis, we relate pitfalls in modelinguncertainty to existing methods. Our analysis enables and informs thedevelopment of principled uncertainty estimation on graphs.</description><author>Dominik Fuchsgruber, Tom Wollschläger, Bertrand Charpentier, Antonio Oroz, Stephan Günnemann</author><pubDate>Thu, 08 Aug 2024 16:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01462v2</guid></item><item><title>Molyé: A Corpus-based Approach to Language Contact in Colonial France</title><link>http://arxiv.org/abs/2408.04554v1</link><description>Whether or not several Creole languages which developed during the earlymodern period can be considered genetic descendants of European languages hasbeen the subject of intense debate. This is in large part due to the absence ofevidence of intermediate forms. This work introduces a new open corpus, theMoly\'e corpus, which combines stereotypical representations of three kinds oflanguage variation in Europe with early attestations of French-based Creolelanguages across a period of 400 years. It is intended to facilitate futureresearch on the continuity between contact situations in Europe and Creolophone(former) colonies.</description><author>Rasul Dent, Juliette Janès, Thibault Clérice, Pedro Ortiz Suarez, Benoît Sagot</author><pubDate>Thu, 08 Aug 2024 16:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04554v1</guid></item><item><title>Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation</title><link>http://arxiv.org/abs/2406.01561v3</link><description>Diffusion-based text-to-image generation models trained on extensivetext-image pairs have shown the capacity to generate photorealistic imagesconsistent with textual descriptions. However, a significant limitation ofthese models is their slow sample generation, which requires iterativerefinement through the same network. In this paper, we enhance Score identityDistillation (SiD) by developing long and short classifier-free guidance (LSG)to efficiently distill pretrained Stable Diffusion models without using realtraining data. SiD aims to optimize a model-based explicit score matching loss,utilizing a score-identity-based approximation alongside the proposed LSG forpractical computation. By training exclusively with fake images synthesizedwith its one-step generator, SiD equipped with LSG rapidly improves FID andCLIP scores, achieving state-of-the-art FID performance while maintaining acompetitive CLIP score. Specifically, its data-free distillation of StableDiffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validationset, with a CLIP score of 0.304 at an LSG scale of 1.5, and an FID of 9.56 witha CLIP score of 0.313 at an LSG scale of 2. Our code and distilled one-steptext-to-image generators are available athttps://github.com/mingyuanzhou/SiD-LSG.</description><author>Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, Hai Huang</author><pubDate>Thu, 08 Aug 2024 16:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01561v3</guid></item><item><title>DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents</title><link>http://arxiv.org/abs/2311.09805v2</link><description>Recent LLMs have demonstrated remarkable performance in solving exam-likemath word problems. However, the degree to which these numerical reasoningskills are effective in real-world scenarios, particularly in expert domains,is still largely unexplored. This paper introduces DocMath-Eval, acomprehensive benchmark specifically designed to evaluate the numericalreasoning capabilities of LLMs in the context of understanding and analyzingspecialized documents containing both text and tables. We evaluate a widespectrum of 48 LLMs with Chain-of-Thought and Program-of-Thought promptingmethods, aiming to comprehensively assess the capabilities and limitations ofexisting LLMs in DocMath-Eval. We found that even the current best-performingsystem (i.e., GPT-4o) still significantly lags behind human experts in solvingcomplex numerical reasoning problems grounded in long contexts. We believe thatDocMath-Eval can serve as a valuable benchmark for evaluating LLMs'capabilities in solving challenging numerical reasoning problems within expertdomains.</description><author>Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, Arman Cohan</author><pubDate>Thu, 08 Aug 2024 15:56:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09805v2</guid></item><item><title>Quantum Machine Learning: Performance and Security Implications in Real-World Applications</title><link>http://arxiv.org/abs/2408.04543v1</link><description>Quantum computing has garnered significant attention in recent years fromboth academia and industry due to its potential to achieve a "quantumadvantage" over classical computers. The advent of quantum computing introducesnew challenges for security and privacy. This poster explores the performanceand security implications of quantum computing through a case study of machinelearning in a real-world application. We compare the performance of quantummachine learning (QML) algorithms to their classical counterparts using theAlzheimer's disease dataset. Our results indicate that QML algorithms showpromising potential while they still have not surpassed classical algorithms interms of learning capability and convergence difficulty, and running quantumalgorithms through simulations on classical computers requires significantlylarge memory space and CPU time. Our study also indicates that QMLs haveinherited vulnerabilities from classical machine learning algorithms while alsointroduce new attack vectors.</description><author>Zhengping Jay Luo, Tyler Stewart, Mourya Narasareddygari, Rui Duan, Shangqing Zhao</author><pubDate>Thu, 08 Aug 2024 15:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04543v1</guid></item><item><title>MemeMind at ArAIEval Shared Task: Spotting Persuasive Spans in Arabic Text with Persuasion Techniques Identification</title><link>http://arxiv.org/abs/2408.04540v1</link><description>This paper focuses on detecting propagandistic spans and persuasiontechniques in Arabic text from tweets and news paragraphs. Each entry in thedataset contains a text sample and corresponding labels that indicate the startand end positions of propaganda techniques within the text. Tokens fallingwithin a labeled span were assigned "B" (Begin) or "I" (Inside), "O",corresponding to the specific propaganda technique. Using attention masks, wecreated uniform lengths for each span and assigned BIO tags to each token basedon the provided labels. Then, we used AraBERT-base pre-trained model for Arabictext tokenization and embeddings with a token classification layer to identifypropaganda techniques. Our training process involves a two-phase fine-tuningapproach. First, we train only the classification layer for a few epochs,followed by full model fine-tuning, updating all parameters. This methodologyallows the model to adapt to the specific characteristics of the propagandadetection task while leveraging the knowledge captured by the pre-trainedAraBERT model. Our approach achieved an F1 score of 0.2774, securing the 3rdposition in the leaderboard of Task 1.</description><author>Md Rafiul Biswas, Zubair Shah, Wajdi Zaghouani</author><pubDate>Thu, 08 Aug 2024 15:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04540v1</guid></item><item><title>State Representations as Incentives for Reinforcement Learning Agents: A Sim2Real Analysis on Robotic Grasping</title><link>http://arxiv.org/abs/2309.11984v3</link><description>Choosing an appropriate representation of the environment for the underlyingdecision-making process of the reinforcement learning agent is not alwaysstraightforward. The state representation should be inclusive enough to allowthe agent to informatively decide on its actions and disentangled enough tosimplify policy training and the corresponding sim2real transfer. Given thisoutlook, this work examines the effect of various representations inincentivizing the agent to solve a specific robotic task: antipodal and planarobject grasping. A continuum of state representations is defined, starting fromhand-crafted numerical states to encoded image-based representations, withdecreasing levels of induced task-specific knowledge. The effects of eachrepresentation on the ability of the agent to solve the task in simulation andthe transferability of the learned policy to the real robot are examined andcompared against a model-based approach with complete system knowledge. Theresults show that reinforcement learning agents using numerical states canperform on par with non-learning baselines. Furthermore, we find that agentsusing image-based representations from pre-trained environment embeddingvectors perform better than end-to-end trained agents, and hypothesize thatseparation of representation learning from reinforcement learning can benefitsim2real transfer. Finally, we conclude that incentivizing the staterepresentation with task-specific knowledge facilitates faster convergence foragent training and increases success rates in sim2real robot control.</description><author>Panagiotis Petropoulakis, Ludwig Gräf, Mohammadhossein Malmir, Josip Josifovski, Alois Knoll</author><pubDate>Thu, 08 Aug 2024 15:46:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11984v3</guid></item><item><title>ParetoTracker: Understanding Population Dynamics in Multi-objective Evolutionary Algorithms through Visual Analytics</title><link>http://arxiv.org/abs/2408.04539v1</link><description>Multi-objective evolutionary algorithms (MOEAs) have emerged as powerfultools for solving complex optimization problems characterized by multiple,often conflicting, objectives. While advancements have been made incomputational efficiency as well as diversity and convergence of solutions, acritical challenge persists: the internal evolutionary mechanisms are opaque tohuman users. Drawing upon the successes of explainable AI in explaining complexalgorithms and models, we argue that the need to understand the underlyingevolutionary operators and population dynamics within MOEAs aligns well with avisual analytics paradigm. This paper introduces ParetoTracker, a visualanalytics framework designed to support the comprehension and inspection ofpopulation dynamics in the evolutionary processes of MOEAs. Informed bypreliminary literature review and expert interviews, the framework establishesa multi-level analysis scheme, which caters to user engagement and explorationranging from examining overall trends in performance metrics to conductingfine-grained inspections of evolutionary operations. In contrast toconventional practices that require manual plotting of solutions for eachgeneration, ParetoTracker facilitates the examination of temporal trends anddynamics across consecutive generations in an integrated visual interface. Theeffectiveness of the framework is demonstrated through case studies and expertinterviews focused on widely adopted benchmark optimization problems.</description><author>Zherui Zhang, Fan Yang, Ran Cheng, Yuxin Ma</author><pubDate>Thu, 08 Aug 2024 15:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04539v1</guid></item><item><title>FinanceMath: Knowledge-Intensive Math Reasoning in Finance Domains</title><link>http://arxiv.org/abs/2311.09797v2</link><description>We introduce FinanceMath, a novel benchmark designed to evaluate LLMs'capabilities in solving knowledge-intensive math reasoning problems. Comparedto prior works, this study features three core advancements. First, FinanceMathincludes 1,200 problems with a hybrid of textual and tabular content. Theseproblems require college-level knowledge in the finance domain for effectiveresolution. Second, we provide expert-annotated, detailed solution referencesin Python program format, ensuring a high-quality benchmark for LLM assessment.We also construct a finance-domain knowledge bank and investigate variousknowledge integration strategies. Finally, we evaluate a wide spectrum of 44LLMs with both Chain-of-Thought and Program-of-Thought prompting methods. Ourexperimental results reveal that the current best-performing system (i.e.,GPT-4o) achieves only 60.9% accuracy using CoT prompting, leaving substantialroom for improvement. Moreover, while augmenting LLMs with external knowledgecan improve model performance (e.g., from 47.5% to 54.5% for Gemini-1.5-Pro),their accuracy remains significantly lower than the estimated human expertperformance of 92%. We believe that FinanceMath can advance future research inthe area of domain-specific knowledge retrieval and integration, particularlywithin the context of solving reasoning-intensive tasks.</description><author>Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, Arman Cohan</author><pubDate>Thu, 08 Aug 2024 15:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09797v2</guid></item><item><title>Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided</title><link>http://arxiv.org/abs/2404.01288v2</link><description>Large language models (LLMs) have offered new opportunities for emotionalsupport, and recent work has shown that they can produce empathic responses topeople in distress. However, long-term mental well-being requires emotionalself-regulation, where a one-time empathic response falls short. This worktakes a first step by engaging with cognitive reappraisals, a strategy frompsychology practitioners that uses language to targetedly change negativeappraisals that an individual makes of the situation; such appraisals is knownto sit at the root of human emotional experience. We hypothesize thatpsychologically grounded principles could enable such advanced psychologycapabilities in LLMs, and design RESORT which consists of a series ofreappraisal constitutions across multiple dimensions that can be used as LLMinstructions. We conduct a first-of-its-kind expert evaluation (by clinicalpsychologists with M.S. or Ph.D. degrees) of an LLM's zero-shot ability togenerate cognitive reappraisal responses to medium-length social media messagesasking for support. This fine-grained evaluation showed that even LLMs at the7B scale guided by RESORT are capable of generating empathic responses that canhelp users reappraise their situations.</description><author>Hongli Zhan, Allen Zheng, Yoon Kyung Lee, Jina Suh, Junyi Jessy Li, Desmond C. Ong</author><pubDate>Thu, 08 Aug 2024 15:44:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01288v2</guid></item><item><title>Synchronous Multi-modal Semantic CommunicationSystem with Packet-level Coding</title><link>http://arxiv.org/abs/2408.04535v1</link><description>Although the semantic communication with joint semantic-channel coding designhas shown promising performance in transmitting data of different modalitiesover physical layer channels, the synchronization and packet-level forwarderror correction of multimodal semantics have not been well studied. Due to theindependent design of semantic encoders, synchronizing multimodal features inboth the semantic and time domains is a challenging problem. In this paper, wetake the facial video and speech transmission as an example and propose aSynchronous Multimodal Semantic Communication System (SyncSC) with Packet-LevelCoding. To achieve semantic and time synchronization, 3D Morphable Mode (3DMM)coefficients and text are transmitted as semantics, and we propose a semanticcodec that achieves similar quality of reconstruction and synchronization withlower bandwidth, compared to traditional methods. To protect semantic packetsunder the erasure channel, we propose a packet-Level Forward Error Correction(FEC) method, called PacSC, that maintains a certain visual quality performanceeven at high packet loss rates. Particularly, for text packets, a text packetloss concealment module, called TextPC, based on Bidirectional EncoderRepresentations from Transformers (BERT) is proposed, which significantlyimproves the performance of traditional FEC methods. The simulation resultsshow that our proposed SyncSC reduce transmission overhead and achievehigh-quality synchronous transmission of video and speech over the packet lossnetwork.</description><author>Yun Tian, Jingkai Ying, Zhijin Qin, Ye Jin, Xiaoming Tao</author><pubDate>Thu, 08 Aug 2024 15:42:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04535v1</guid></item><item><title>Autonomous, Self-driving Multi-Step Growth of Semiconductor Heterostructures Guided by Machine Learning</title><link>http://arxiv.org/abs/2408.03508v2</link><description>The semiconductor industry has prioritized automating repetitive tasks byclosed-loop, autonomous experimentation which enables accelerated optimizationof complex multi-step processes. The emergence of machine learning (ML) hasushered in automated process with minimal human intervention. In this work, wedevelop SemiEpi, a self-driving automation platform capable of executingmolecular beam epitaxy (MBE) growth with multi-steps, continuous in-situmonitoring, and on-the-fly feedback control. By integrating standard hardware,homemade software, curve fitting, and multiple ML models, SemiEpi operatesautonomously, eliminating the need for extensive expertise in MBE processes toachieve optimal outcomes. The platform actively learns from previousexperimental results, identifying favorable conditions and proposing newexperiments to achieve the desired results. We standardize and optimize growthfor InAs/GaAs quantum dots (QDs) heterostructures to showcase the power ofML-guided multi-step growth. A temperature calibration was implemented to getthe initial growth condition, and fine control of the process was executedusing ML. Leveraging RHEED movies acquired during the growth, SemiEpisuccessfully identified and optimized a novel route for multi-stepheterostructure growth. This work demonstrates the capabilities of closed-loop,ML-guided systems in addressing challenges in multi-step growth for any device.Our method is critical to achieve repeatable materials growth usingcommercially scalable tools. Our strategy facilitates the development of ahardware-independent process and enhancing process repeatability and stability,even without exhaustive knowledge of growth parameters.</description><author>Chao Shen, Wenkang Zhan, Hongyu Sun, Kaiyao Xin, Bo Xu, Zhanguo Wang, Chao Zhao</author><pubDate>Thu, 08 Aug 2024 15:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03508v2</guid></item><item><title>How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression</title><link>http://arxiv.org/abs/2408.04532v1</link><description>Despite the remarkable success of transformer-based models in variousreal-world tasks, their underlying mechanisms remain poorly understood. Recentstudies have suggested that transformers can implement gradient descent as anin-context learner for linear regression problems and have developed varioustheoretical analyses accordingly. However, these works mostly focus on theexpressive power of transformers by designing specific parameter constructions,lacking a comprehensive understanding of their inherent working mechanismspost-training. In this study, we consider a sparse linear regression problemand investigate how a trained multi-head transformer performs in-contextlearning. We experimentally discover that the utilization of multi-headsexhibits different patterns across layers: multiple heads are utilized andessential in the first layer, while usually only a single head is sufficientfor subsequent layers. We provide a theoretical explanation for thisobservation: the first layer preprocesses the context data, and the followinglayers execute simple optimization steps based on the preprocessed context.Moreover, we demonstrate that such a preprocess-then-optimize algorithm cansignificantly outperform naive gradient descent and ridge regressionalgorithms. Further experimental results support our explanations. Our findingsoffer insights into the benefits of multi-head attention and contribute tounderstanding the more intricate mechanisms hidden within trained transformers.</description><author>Xingwu Chen, Lei Zhao, Difan Zou</author><pubDate>Thu, 08 Aug 2024 15:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04532v1</guid></item><item><title>An Autonomous GIS Agent Framework for Geospatial Data Retrieval</title><link>http://arxiv.org/abs/2407.21024v2</link><description>Powered by the emerging large language models (LLMs), autonomous geographicinformation systems (GIS) agents have the potential to accomplish spatialanalyses and cartographic tasks. However, a research gap exists to supportfully autonomous GIS agents: how to enable agents to discover and download thenecessary data for geospatial analyses. This study proposes an autonomous GISagent framework capable of retrieving required geospatial data by generating,executing, and debugging programs. The framework utilizes the LLM as thedecision-maker, selects the appropriate data source (s) from a pre-definedsource list, and fetches the data from the chosen source. Each data source hasa handbook that records the metadata and technical details for data retrieval.The proposed framework is designed in a plug-and-play style to ensureflexibility and extensibility. Human users or autonomous data scrawlers can addnew data sources by adding new handbooks. We developed a prototype agent basedon the framework, released as a QGIS plugin (GeoData Retrieve Agent) and aPython program. Experiment results demonstrate its capability of retrievingdata from various sources including OpenStreetMap, administrative boundariesand demographic data from the US Census Bureau, satellite basemaps from ESRIWorld Imagery, global digital elevation model (DEM) from OpenTopography.org,weather data from a commercial provider, the COVID-19 cases from the NYTimesGitHub. Our study is among the first attempts to develop an autonomousgeospatial data retrieval agent.</description><author>Huan Ning, Zhenlong Li, Temitope Akinboyewa, M. Naser Lessani</author><pubDate>Thu, 08 Aug 2024 15:32:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21024v2</guid></item><item><title>AExGym: Benchmarks and Environments for Adaptive Experimentation</title><link>http://arxiv.org/abs/2408.04531v1</link><description>Innovations across science and industry are evaluated using randomized trials(a.k.a. A/B tests). While simple and robust, such static designs areinefficient or infeasible for testing many hypotheses. Adaptive designs cangreatly improve statistical power in theory, but they have seen limitedadoption due to their fragility in practice. We present a benchmark foradaptive experimentation based on real-world datasets, highlighting prominentpractical challenges to operationalizing adaptivity: non-stationarity,batched/delayed feedback, multiple outcomes and objectives, and externalvalidity. Our benchmark aims to spur methodological development that putspractical performance (e.g., robustness) as a central concern, rather thanmathematical guarantees on contrived instances. We release an open sourcelibrary, AExGym, which is designed with modularity and extensibility in mind toallow experimentation practitioners to develop custom environments andalgorithms.</description><author>Jimmy Wang, Ethan Che, Daniel R. Jiang, Hongseok Namkoong</author><pubDate>Thu, 08 Aug 2024 15:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04531v1</guid></item><item><title>Reasoning about Study Regulations in Answer Set Programming</title><link>http://arxiv.org/abs/2408.04528v1</link><description>We are interested in automating reasoning with and about study regulations,catering to various stakeholders, ranging from administrators, over faculty, tostudents at different stages. Our work builds on an extensive analysis ofvarious study programs at the University of Potsdam. The conceptualization ofthe underlying principles provides us with a formal account of studyregulations. In particular, the formalization reveals the properties ofadmissible study plans. With these at end, we propose an encoding of studyregulations in Answer Set Programming that produces corresponding study plans.Finally, we show how this approach can be extended to a generic user interfacefor exploring study plans.</description><author>Susana Hahn, Cedric Martens, Amade Nemes, Henry Otunuya, Javier Romero, Torsten Schaub, Sebastian Schellhorn</author><pubDate>Thu, 08 Aug 2024 15:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04528v1</guid></item><item><title>Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs</title><link>http://arxiv.org/abs/2408.04526v1</link><description>Hybrid Reinforcement Learning (RL), where an agent learns from both anoffline dataset and online explorations in an unknown environment, has garneredsignificant recent interest. A crucial question posed by Xie et al. (2022) iswhether hybrid RL can improve upon the existing lower bounds established inpurely offline and purely online RL without relying on the single-policyconcentrability assumption. While Li et al. (2023) provided an affirmativeanswer to this question in the tabular PAC RL case, the question remainsunsettled for both the regret-minimizing RL case and the non-tabular case. In this work, building upon recent advancements in offline RL andreward-agnostic exploration, we develop computationally efficient algorithmsfor both PAC and regret-minimizing RL with linear function approximation,without single-policy concentrability. We demonstrate that these algorithmsachieve sharper error or regret bounds that are no worse than, and can improveon, the optimal sample complexity in offline RL (the first algorithm, for PACRL) and online RL (the second algorithm, for regret-minimizing RL) in linearMarkov decision processes (MDPs), regardless of the quality of the behaviorpolicy. To our knowledge, this work establishes the tightest theoreticalguarantees currently available for hybrid RL in linear MDPs.</description><author>Kevin Tan, Wei Fan, Yuting Wei</author><pubDate>Thu, 08 Aug 2024 15:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04526v1</guid></item><item><title>Smooth Deep Saliency</title><link>http://arxiv.org/abs/2404.02282v3</link><description>In this work, we investigate methods to reduce the noise in deep saliencymaps coming from convolutional downsampling. Those methods make theinvestigated models more interpretable for gradient-based saliency maps,computed in hidden layers. We evaluate the faithfulness of those methods usinginsertion and deletion metrics, finding that saliency maps computed in hiddenlayers perform better compared to both the input layer and GradCAM. We test ourapproach on different models trained for image classification on ImageNet1K,and models trained for tumor detection on Camelyon16 and in-house real-worlddigital pathology scans of stained tissue samples. Our results show that thecheckerboard noise in the gradient gets reduced, resulting in smoother andtherefore easier to interpret saliency maps.</description><author>Rudolf Herdt, Maximilian Schmidt, Daniel Otero Baguer, Peter Maaß</author><pubDate>Thu, 08 Aug 2024 15:25:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02282v3</guid></item><item><title>Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height Estimation</title><link>http://arxiv.org/abs/2408.04523v1</link><description>Estimating global tree canopy height is crucial for forest conservation andclimate change applications. However, capturing high-resolution ground truthcanopy height using LiDAR is expensive and not available globally. An efficientalternative is to train a canopy height estimator to operate on single-viewremotely sensed imagery. The primary obstacle to this approach is that thesemethods require significant training data to generalize well globally andacross uncommon edge cases. Recent monocular depth estimation foundation modelshave show strong zero-shot performance even for complex scenes. In this paperwe leverage the representations learned by these models to transfer to theremote sensing domain for measuring canopy height. Our findings suggest thatour proposed Depth Any Canopy, the result of fine-tuning the Depth Anything v2model for canopy height estimation, provides a performant and efficientsolution, surpassing the current state-of-the-art with superior or comparableperformance using only a fraction of the computational resources andparameters. Furthermore, our approach requires less than \$1.30 in compute andresults in an estimated carbon footprint of 0.14 kgCO2. Code, experimentalresults, and model checkpoints are openly available athttps://github.com/DarthReca/depth-any-canopy.</description><author>Daniele Rege Cambrin, Isaac Corley, Paolo Garza</author><pubDate>Thu, 08 Aug 2024 15:24:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04523v1</guid></item><item><title>Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models</title><link>http://arxiv.org/abs/2408.04522v1</link><description>As diverse linguistic communities and users adopt large language models(LLMs), assessing their safety across languages becomes critical. Despiteongoing efforts to make LLMs safe, they can still be made to behave unsafelywith jailbreaking, a technique in which models are prompted to act outsidetheir operational guidelines. Research on LLM safety and jailbreaking, however,has so far mostly focused on English, limiting our understanding of LLM safetyin other languages. We contribute towards closing this gap by investigating theeffectiveness of many-shot jailbreaking, where models are prompted with unsafedemonstrations to induce unsafe behaviour, in Italian. To enable our analysis,we create a new dataset of unsafe Italian question-answer pairs. With thisdataset, we identify clear safety vulnerabilities in four families ofopen-weight LLMs. We find that the models exhibit unsafe behaviors even whenprompted with few unsafe demonstrations, and -- more alarmingly -- that thistendency rapidly escalates with more demonstrations.</description><author>Fabio Pernisi, Dirk Hovy, Paul Röttger</author><pubDate>Thu, 08 Aug 2024 15:24:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04522v1</guid></item><item><title>Advancing Molecular Machine (Learned) Representations with Stereoelectronics-Infused Molecular Graphs</title><link>http://arxiv.org/abs/2408.04520v1</link><description>Molecular representation is a foundational element in our understanding ofthe physical world. Its importance ranges from the fundamentals of chemicalreactions to the design of new therapies and materials. Previous molecularmachine learning models have employed strings, fingerprints, global features,and simple molecular graphs that are inherently information-sparserepresentations. However, as the complexity of prediction tasks increases, themolecular representation needs to encode higher fidelity information. This workintroduces a novel approach to infusing quantum-chemical-rich information intomolecular graphs via stereoelectronic effects. We show that the explicitaddition of stereoelectronic interactions significantly improves theperformance of molecular machine learning models. Furthermore,stereoelectronics-infused representations can be learned and deployed with atailored double graph neural network workflow, enabling its application to anydownstream molecular machine learning task. Finally, we show that the learnedrepresentations allow for facile stereoelectronic evaluation of previouslyintractable systems, such as entire proteins, opening new avenues of moleculardesign.</description><author>Daniil A. Boiko, Thiago Reschützegger, Benjamin Sanchez-Lengeling, Samuel M. Blau, Gabe Gomes</author><pubDate>Thu, 08 Aug 2024 15:21:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04520v1</guid></item><item><title>Articulatory Configurations across Genders and Periods in French Radio and TV archives</title><link>http://arxiv.org/abs/2408.04519v1</link><description>This paper studies changes in articulatory configurations across genders andperiods using an inversion from acoustic to articulatory parameters. From adiachronic corpus based on French media archives spanning 60 years from 1955 to2015, automatic transcription and forced alignment allowed extracting thecentral frame of each vowel. More than one million frames were obtained fromover a thousand speakers across gender and age categories. Their formants wereused from these vocalic frames to fit the parameters of Maeda's articulatorymodel. Evaluations of the quality of these processes are provided. We focushere on two parameters of Maeda's model linked to total vocal tract length: therelative position of the larynx (higher for females) and the lips protrusion(more protruded for males). Implications for voice quality across genders arediscussed. The effect across periods seems gender independent; thus, theassertion that females lowered their pitch with time is not supported.</description><author>Benjamin Elie, David Doukhan, Rémi Uro, Lucas Ondel-Yang, Albert Rilliard, Simon Devauchelle</author><pubDate>Thu, 08 Aug 2024 15:20:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04519v1</guid></item><item><title>Saliency Detection in Educational Videos: Analyzing the Performance of Current Models, Identifying Limitations and Advancement Directions</title><link>http://arxiv.org/abs/2408.04515v1</link><description>Identifying the regions of a learning resource that a learner pays attentionto is crucial for assessing the material's impact and improving its design andrelated support systems. Saliency detection in videos addresses the automaticrecognition of attention-drawing regions in single frames. In educationalsettings, the recognition of pertinent regions in a video's visual stream canenhance content accessibility and information retrieval tasks such as videosegmentation, navigation, and summarization. Such advancements can pave the wayfor the development of advanced AI-assisted technologies that support learningwith greater efficacy. However, this task becomes particularly challenging foreducational videos due to the combination of unique characteristics such astext, voice, illustrations, animations, and more. To the best of our knowledge,there is currently no study that evaluates saliency detection approaches ineducational videos. In this paper, we address this gap by evaluating fourstate-of-the-art saliency detection approaches for educational videos. Wereproduce the original studies and explore the replication capabilities forgeneral-purpose (non-educational) datasets. Then, we investigate thegeneralization capabilities of the models and evaluate their performance oneducational videos. We conduct a comprehensive analysis to identify commonfailure scenarios and possible areas of improvement. Our experimental resultsshow that educational videos remain a challenging context for generic videosaliency detection models.</description><author>Evelyn Navarrete, Ralph Ewerth, Anett Hoppe</author><pubDate>Thu, 08 Aug 2024 15:15:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04515v1</guid></item><item><title>FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression</title><link>http://arxiv.org/abs/2403.16677v2</link><description>Nanosatellite constellations equipped with sensors capturing large geographicregions provide unprecedented opportunities for Earth observation. Asconstellation sizes increase, network contention poses a downlink bottleneck.Orbital Edge Computing (OEC) leverages limited onboard compute resources toreduce transfer costs by processing the raw captures at the source. However,current solutions have limited practicability due to reliance on crudefiltering methods or over-prioritizing particular downstream tasks. This work presents FOOL, an OEC-native and task-agnostic feature compressionmethod that preserves prediction performance. FOOL partitions high-resolutionsatellite imagery to maximize throughput. Further, it embeds context andleverages inter-tile dependencies to lower transfer costs with negligibleoverhead. While FOOL is a feature compressor, it can recover images withcompetitive scores on quality measures at lower bitrates. We extensivelyevaluate transfer cost reduction by including the peculiarity of intermittentlyavailable network connections in low earth orbit. Lastly, we test thefeasibility of our system for standardized nanosatellite form factors. Wedemonstrate that FOOL permits downlinking over 100x the data volume withoutrelying on prior information on the downstream tasks.</description><author>Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar</author><pubDate>Thu, 08 Aug 2024 15:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16677v2</guid></item><item><title>Conformal Temporal Logic Planning using Large Language Models</title><link>http://arxiv.org/abs/2309.10092v4</link><description>This paper addresses planning problems for mobile robots. We considermissions that require accomplishing multiple high-level sub-tasks, expressed innatural language (NL), in a temporal and logical order. To formally define themission, we treat these sub-tasks as atomic predicates in a Linear TemporalLogic (LTL) formula. We refer to this task specification framework as LTL-NL.Our goal is to design plans, defined as sequences of robot actions,accomplishing LTL-NL tasks. This action planning problem cannot be solveddirectly by existing LTL planners because of the NL nature of atomicpredicates. To address it, we propose HERACLEs, a hierarchical neuro-symbolicplanner that relies on a novel integration of (i) existing symbolic plannersgenerating high-level task plans determining the order at which the NLsub-tasks should be accomplished; (ii) pre-trained Large Language Models (LLMs)to design sequences of robot actions based on these task plans; and (iii)conformal prediction acting as a formal interface between (i) and (ii) andmanaging uncertainties due to LLM imperfections. We show, both theoreticallyand empirically, that HERACLEs can achieve user-defined mission success rates.Finally, we provide comparative experiments demonstrating that HERACLEsoutperforms LLM-based planners that require the mission to be defined solelyusing NL. Additionally, we present examples demonstrating that our approachenhances user-friendliness compared to conventional symbolic approaches.</description><author>Jun Wang, Jiaming Tong, Kaiyuan Tan, Yevgeniy Vorobeychik, Yiannis Kantaros</author><pubDate>Thu, 08 Aug 2024 14:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10092v4</guid></item><item><title>Knowledge-Aided Semantic Communication Leveraging Probabilistic Graphical Modeling</title><link>http://arxiv.org/abs/2408.04499v1</link><description>In this paper, we propose a semantic communication approach based onprobabilistic graphical model (PGM). The proposed approach involvesconstructing a PGM from a training dataset, which is then shared as commonknowledge between the transmitter and receiver. We evaluate the importance ofvarious semantic features and present a PGM-based compression algorithmdesigned to eliminate predictable portions of semantic information.Furthermore, we introduce a technique to reconstruct the discarded semanticinformation at the receiver end, generating approximate results based on thePGM. Simulation results indicate a significant improvement in transmissionefficiency over existing methods, while maintaining the quality of thetransmitted images.</description><author>Haowen Wan, Qianqian Yang, Jiancheng Tang, Zhiguo shi</author><pubDate>Thu, 08 Aug 2024 14:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04499v1</guid></item><item><title>Model-Based Transfer Learning for Contextual Reinforcement Learning</title><link>http://arxiv.org/abs/2408.04498v1</link><description>Deep reinforcement learning is a powerful approach to complex decisionmaking. However, one issue that limits its practical application is itsbrittleness, sometimes failing to train in the presence of small changes in theenvironment. This work is motivated by the empirical observation that directlyapplying an already trained model to a related task often works remarkablywell, also called zero-shot transfer. We take this practical trick one stepfurther to consider how to systematically select good tasks to train,maximizing overall performance across a range of tasks. Given the high cost oftraining, it is critical to choose a small set of training tasks. The key ideabehind our approach is to explicitly model the performance loss (generalizationgap) incurred by transferring a trained model. We hence introduce Model-BasedTransfer Learning (MBTL) for solving contextual RL problems. In this work, wemodel the performance loss as a simple linear function of task contextsimilarity. Furthermore, we leverage Bayesian optimization techniques toefficiently model and estimate the unknown training performance of the taskspace. We theoretically show that the method exhibits regret that is sublinearin the number of training tasks and discuss conditions to further tightenregret bounds. We experimentally validate our methods using urban traffic andstandard control benchmarks. Despite the conceptual simplicity, theexperimental results suggest that MBTL can achieve greater performance thanstrong baselines, including exhaustive training on all tasks, multi-tasktraining, and random selection of training tasks. This work lays thefoundations for investigating explicit modeling of generalization, therebyenabling principled yet effective methods for contextual RL.</description><author>Jung-Hoon Cho, Vindula Jayawardana, Sirui Li, Cathy Wu</author><pubDate>Thu, 08 Aug 2024 14:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04498v1</guid></item><item><title>Towards Synergistic Deep Learning Models for Volumetric Cirrhotic Liver Segmentation in MRIs</title><link>http://arxiv.org/abs/2408.04491v1</link><description>Liver cirrhosis, a leading cause of global mortality, requires precisesegmentation of ROIs for effective disease monitoring and treatment planning.Existing segmentation models often fail to capture complex feature interactionsand generalize across diverse datasets. To address these limitations, wepropose a novel synergistic theory that leverages complementary latent spacesfor enhanced feature interaction modeling. Our proposed architecture,nnSynergyNet3D integrates continuous and discrete latent spaces for 3D volumesand features auto-configured training. This approach captures both fine-grainedand coarse features, enabling effective modeling of intricate featureinteractions. We empirically validated nnSynergyNet3D on a private dataset of628 high-resolution T1 abdominal MRI scans from 339 patients. Our modeloutperformed the baseline nnUNet3D by approximately 2%. Additionally, zero-shottesting on healthy liver CT scans from the public LiTS dataset demonstratedsuperior cross-modal generalization capabilities. These results highlight thepotential of synergistic latent space models to improve segmentation accuracyand robustness, thereby enhancing clinical workflows by ensuring consistencyacross CT and MRI modalities.</description><author>Vandan Gorade, Onkar Susladkar, Gorkem Durak, Elif Keles, Ertugrul Aktas, Timurhan Cebeci, Alpay Medetalibeyoglu, Daniela Ladner, Debesh Jha, Ulas Bagci</author><pubDate>Thu, 08 Aug 2024 14:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04491v1</guid></item><item><title>Color Mismatches in Stereoscopic Video: Real-World Dataset and Deep Correction Method</title><link>http://arxiv.org/abs/2303.06657v3</link><description>Stereoscopic videos can contain color mismatches between the left and rightviews due to minor variations in camera settings, lenses, and even objectreflections captured from different positions. The presence of color mismatchescan lead to viewer discomfort and headaches. This problem can be solved bytransferring color between stereoscopic views, but traditional methods oftenlack quality, while neural-network-based methods can easily overfit onartificial data. The scarcity of stereoscopic videos with real-world colormismatches hinders the evaluation of different methods' performance. Therefore,we filmed a video dataset, which includes both distorted frames with colormismatches and ground-truth data, using a beam-splitter. Our secondcontribution is a deep multiscale neural network that solves thecolor-mismatch-correction task by leveraging stereo correspondences. Theexperimental results demonstrate the effectiveness of the proposed method on aconventional dataset, but there remains room for improvement on challengingreal-world data.</description><author>Egor Chistov, Nikita Alutis, Dmitriy Vatolin</author><pubDate>Thu, 08 Aug 2024 14:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06657v3</guid></item><item><title>Unsupervised Mastoidectomy for Cochlear CT Mesh Reconstruction Using Highly Noisy Data</title><link>http://arxiv.org/abs/2407.15787v2</link><description>Cochlear Implant (CI) procedures involve inserting an array of electrodesinto the cochlea located inside the inner ear. Mastoidectomy is a surgicalprocedure that uses a high-speed drill to remove part of the mastoid region ofthe temporal bone, providing safe access to the cochlea through the middle andinner ear. We aim to develop an intraoperative navigation system that registersplans created using 3D preoperative Computerized Tomography (CT) volumes withthe 2D surgical microscope view. Herein, we propose a method to synthesize themastoidectomy volume using only the preoperative CT scan, where the mastoid isintact. We introduce an unsupervised learning framework designed to synthesizemastoidectomy. For model training purposes, this method uses postoperative CTscans to avoid manual data cleaning or labeling, even when the region removedduring mastoidectomy is visible but affected by metal artifacts, lowsignal-to-noise ratio, or electrode wiring. Our approach estimatesmastoidectomy regions with a mean dice score of 70.0%. This approach representsa major step forward for CI intraoperative navigation by predicting realisticmastoidectomy-removed regions in preoperative planning that can be used toregister the pre-surgery plan to intraoperative microscopy.</description><author>Yike Zhang, Dingjie Su, Eduardo Davalos, Jack H. Noble</author><pubDate>Thu, 08 Aug 2024 14:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15787v2</guid></item><item><title>Optimization Dynamics of Equivariant and Augmented Neural Networks</title><link>http://arxiv.org/abs/2303.13458v3</link><description>We investigate the optimization of neural networks on symmetric data, andcompare the strategy of constraining the architecture to be equivariant to thatof using data augmentation. Our analysis reveals that that the relativegeometry of the admissible and the equivariant layers, respectively, plays akey role. Under natural assumptions on the data, network, loss, and group ofsymmetries, we show that compatibility of the spaces of admissible layers andequivariant layers, in the sense that the corresponding orthogonal projectionscommute, implies that the sets of equivariant stationary points are identicalfor the two strategies. If the linear layers of the network also are given aunitary parametrization, the set of equivariant layers is even invariant underthe gradient flow for augmented models. Our analysis however also reveals thateven in the latter situation, stationary points may be unstable for augmentedtraining although they are stable for the manifestly equivariant models.</description><author>Oskar Nordenfors, Fredrik Ohlsson Axel Flinth</author><pubDate>Thu, 08 Aug 2024 14:26:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13458v3</guid></item><item><title>Statistical Framework for Clustering MU-MIMO Wireless via Second Order Statistics</title><link>http://arxiv.org/abs/2408.04484v1</link><description>This work explores the clustering of wireless users by examining thedistances between their channel covariance matrices, which reside on theRiemannian manifold of positive definite matrices. Specifically, we consider anestimator of the Log-Euclidean distance between multiple sample covariancematrices (SCMs) consistent when the number of samples and the observation sizegrow unbounded at the same rate. Within the context of multi-user MIMO(MU-MIMO) wireless communication systems, we develop a statistical frameworkthat allows to accurate predictions of the clustering algorithm's performanceunder realistic conditions. Specifically, we present a central limit theoremthat establishes the asymptotic Gaussianity of the consistent estimator of thelog-Euclidean distance computed over two sample covariance matrices.</description><author>Roberto Pereira, Xavier Mestre</author><pubDate>Thu, 08 Aug 2024 14:23:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04484v1</guid></item><item><title>Unsupervised Object Localization in the Era of Self-Supervised ViTs: A Survey</title><link>http://arxiv.org/abs/2310.12904v3</link><description>The recent enthusiasm for open-world vision systems show the high interest ofthe community to perform perception tasks outside of the closed-vocabularybenchmark setups which have been so popular until now. Being able to discoverobjects in images/videos without knowing in advance what objects populate thedataset is an exciting prospect. But how to find objects without knowinganything about them? Recent works show that it is possible to performclass-agnostic unsupervised object localization by exploiting self-supervisedpre-trained features. We propose here a survey of unsupervised objectlocalization methods that discover objects in images without requiring anymanual annotation in the era of self-supervised ViTs. We gather links ofdiscussed methods in the repositoryhttps://github.com/valeoai/Awesome-Unsupervised-Object-Localization.</description><author>Oriane Siméoni, Éloi Zablocki, Spyros Gidaris, Gilles Puy, Patrick Pérez</author><pubDate>Thu, 08 Aug 2024 14:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12904v3</guid></item><item><title>SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene Scenarios</title><link>http://arxiv.org/abs/2408.04482v1</link><description>Most of the sophisticated AI models utilize huge amounts of annotated dataand heavy training to achieve high-end performance. However, there are certainchallenges that hinder the deployment of AI models "in-the-wild" scenarios,i.e., inefficient use of unlabeled data, lack of incorporation of humanexpertise, and lack of interpretation of the results. To mitigate thesechallenges, we propose a novel Explainable Active Learning (XAL) model,XAL-based semantic segmentation model "SegXAL", that can (i) effectivelyutilize the unlabeled data, (ii) facilitate the "Human-in-the-loop" paradigm,and (iii) augment the model decisions in an interpretable way. In particular,we investigate the application of the SegXAL model for semantic segmentation indriving scene scenarios. The SegXAL model proposes the image regions thatrequire labeling assistance from Oracle by dint of explainable AI (XAI) anduncertainty measures in a weakly-supervised manner. Specifically, we propose anovel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty(EBU) module to get an Explainable Error Mask, which enables the machineteachers/human experts to provide intuitive reasoning behind the results and tosolicit feedback to the AI system via an active learning strategy. Such amechanism bridges the semantic gap between man and machine throughcollaborative intelligence, where humans and AI actively enhance each other'scomplementary strengths. A novel high-confidence sample selection techniquebased on the DICE similarity coefficient is also presented within the SegXALframework. Extensive quantitative and qualitative analyses are carried out inthe benchmarking Cityscape dataset. Results show the outperformance of ourproposed SegXAL against other state-of-the-art models.</description><author>Sriram Mandalika, Athira Nambiar</author><pubDate>Thu, 08 Aug 2024 14:19:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04482v1</guid></item><item><title>NFDI4Health workflow and service for synthetic data generation, assessment and risk management</title><link>http://arxiv.org/abs/2408.04478v1</link><description>Individual health data is crucial for scientific advancements, particularlyin developing Artificial Intelligence (AI); however, sharing real patientinformation is often restricted due to privacy concerns. A promising solutionto this challenge is synthetic data generation. This technique creates entirelynew datasets that mimic the statistical properties of real data, whilepreserving confidential patient information. In this paper, we present theworkflow and different services developed in the context of Germany's NationalData Infrastructure project NFDI4Health. First, two state-of-the-art AI tools(namely, VAMBN and MultiNODEs) for generating synthetic health data areoutlined. Further, we introduce SYNDAT (a public web-based tool) which allowsusers to visualize and assess the quality and risk of synthetic data providedby desired generative models. Additionally, the utility of the proposed methodsand the web-based tool is showcased using data from Alzheimer's DiseaseNeuroimaging Initiative (ADNI) and the Center for Cancer Registry Data of theRobert Koch Institute (RKI).</description><author>Sobhan Moazemi, Tim Adams, Hwei Geok NG, Lisa Kühnel, Julian Schneider, Anatol-Fiete Näher, Juliane Fluck, Holger Fröhlich</author><pubDate>Thu, 08 Aug 2024 14:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04478v1</guid></item><item><title>Halfway Escape Optimization: A Quantum-Inspired Solution for Complex Optimization Problems</title><link>http://arxiv.org/abs/2405.02850v3</link><description>This paper first proposes the Halfway Escape Optimization (HEO) algorithm, anovel quantum-inspired metaheuristic designed to address complex optimizationproblems characterized by rugged landscapes and high-dimensionality with anefficient convergence rate. The study presents a comprehensive comparativeevaluation of HEO's performance against established optimization algorithms,including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), ArtificialFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behavedParticle Swarm Optimization (QPSO). The primary analysis encompasses 14benchmark functions with dimension 30, demonstrating HEO's effectiveness andadaptability in navigating complex optimization landscapes and providingvaluable insights into its performance. The simple test of HEO in TravelingSalesman Problem (TSP), Pressure Vessel Design and Tubular Column Design infersits feasibility and potential weakness in real-time applications.</description><author>Jiawen Li, Anwar PP Abdul Majeed, Pascal Lefevre</author><pubDate>Thu, 08 Aug 2024 14:06:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02850v3</guid></item><item><title>Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate</title><link>http://arxiv.org/abs/2408.04472v1</link><description>Competitive debate is a comprehensive and complex computational argumentationtask. Large Language Models (LLMs) encounter hallucinations and lackcompetitiveness in this task. To address these challenges, we introduce Agentfor Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMsdesigned to enhance their capabilities in competitive debate. Drawinginspiration from human behavior in debate preparation and execution,Agent4Debate employs a collaborative architecture where four specialized agents(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.These agents work throughout the debate process, covering multiple stages frominitial research and argument formulation to rebuttal and summary. Tocomprehensively evaluate framework performance, we construct the Chinese DebateArena, comprising 66 carefully selected Chinese debate motions. We recruite tenexperienced human debaters and collect records of 200 debates involvingAgent4Debate, baseline models, and humans. The evaluation employs the Debatrixautomatic scoring system and professional human reviewers based on theestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicatethat the state-of-the-art Agent4Debate exhibits capabilities comparable tothose of humans. Furthermore, ablation studies demonstrate the effectiveness ofeach component in the agent structure.</description><author>Yiqun Zhang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song</author><pubDate>Thu, 08 Aug 2024 14:02:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04472v1</guid></item><item><title>What could go wrong? Discovering and describing failure modes in computer vision</title><link>http://arxiv.org/abs/2408.04471v1</link><description>Deep learning models are effective, yet brittle. Even carefully trained,their behavior tends to be hard to predict when confronted without-of-distribution samples. In this work, our goal is to propose a simple yeteffective solution to predict and describe via natural language potentialfailure modes of computer vision models. Given a pretrained model and a set ofsamples, our aim is to find sentences that accurately describe the visualconditions in which the model underperforms. In order to study this importanttopic and foster future research on it, we formalize the problem ofLanguage-Based Error Explainability (LBEE) and propose a set of metrics toevaluate and compare different methods for this task. We propose solutions thatoperate in a joint vision-and-language embedding space, and can characterizethrough language descriptions model failures caused, e.g., by objects unseenduring training or adverse visual conditions. We experiment with differenttasks, such as classification under the presence of dataset bias and semanticsegmentation in unseen environments, and show that the proposed methodologyisolates nontrivial sentences associated with specific error causes. We hopeour work will help practitioners better understand the behavior of models,increasing their overall safety and interpretability.</description><author>Gabriela Csurka, Tyler L. Hayes, Diane Larlus, Riccardo Volpi</author><pubDate>Thu, 08 Aug 2024 14:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04471v1</guid></item><item><title>MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image Segmentation</title><link>http://arxiv.org/abs/2312.07128v2</link><description>Chest X-ray is one of the most common radiological examination types for thediagnosis of chest diseases. Nowadays, the automatic classification technologyof radiological images has been widely used in clinical diagnosis and treatmentplans. However, each disease has its own different response characteristicreceptive field region, which is the main challenge for chest diseaseclassification tasks. Besides, the imbalance of sample data categories furtherincreases the difficulty of tasks. To solve these problems, we propose a newmulti-label chest disease image classification scheme based on a multi-scaleattention network. In this scheme, multi-scale information is iteratively fusedto focus on regions with a high probability of disease, to effectively minemore meaningful information from data, and the classification performance canbe improved only by image level annotation. We also designed a new lossfunction to improve the rationality of visual perception and the performance ofmulti-label image classification by forcing the consistency of attentionregions before and after image transformation. A comprehensive experiment wascarried out on the public Chest X-Ray14 and CheXpert datasets to achieve stateof the art results, which verified the effectiveness of this method in chestX-ray image classification.</description><author>Jing Xu</author><pubDate>Thu, 08 Aug 2024 13:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07128v2</guid></item><item><title>GMISeg: General Medical Image Segmentation without Re-Training</title><link>http://arxiv.org/abs/2311.12539v3</link><description>The online shopping behavior has the characteristics of rich granularitydimension and data sparsity and previous researches on user behavior predictiondid not seriously discuss feature selection and ensemble design. In this paper,we proposed a SE-Stacking model based on information fusion and ensemblelearning for user purchase behavior prediction. After successfully utilizingthe ensemble feature selection method to screen purchase-related factors, weused the Stacking algorithm for user purchase behavior prediction. In ourefforts to avoid the deviation of prediction results, we optimized the model byselecting ten different kinds of models as base learners and modifying relevantparameters specifically for them. The experiments conducted on apublicly-available dataset shows that the SE-Stacking model can achieve a98.40% F1-score, about 0.09% higher than the optimal base models. TheSE-Stacking model not only has a good application in the prediction of userpurchase behavior but also has practical value combining with the actuale-commerce scene. At the same time, it has important significance for academicresearch and the development of this field.</description><author>Jing Xu</author><pubDate>Thu, 08 Aug 2024 13:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12539v3</guid></item><item><title>RL-ADN: A High-Performance Deep Reinforcement Learning Environment for Optimal Energy Storage Systems Dispatch in Active Distribution Networks</title><link>http://arxiv.org/abs/2408.03685v2</link><description>Deep Reinforcement Learning (DRL) presents a promising avenue for optimizingEnergy Storage Systems (ESSs) dispatch in distribution networks. This paperintroduces RL-ADN, an innovative open-source library specifically designed forsolving the optimal ESSs dispatch in active distribution networks. RL-ADNoffers unparalleled flexibility in modeling distribution networks, and ESSs,accommodating a wide range of research goals. A standout feature of RL-ADN isits data augmentation module, based on Gaussian Mixture Model and Copula (GMC)functions, which elevates the performance ceiling of DRL agents. Additionally,RL-ADN incorporates the Laurent power flow solver, significantly reducing thecomputational burden of power flow calculations during training withoutsacrificing accuracy. The effectiveness of RL-ADN is demonstrated using indifferent sizes of distribution networks, showing marked performanceimprovements in the adaptability of DRL algorithms for ESS dispatch tasks. Thisenhancement is particularly beneficial from the increased diversity of trainingscenarios. Furthermore, RL-ADN achieves a tenfold increase in computationalefficiency during training, making it highly suitable for large-scale networkapplications. The library sets a new benchmark in DRL-based ESSs dispatch indistribution networks and it is poised to advance DRL applications indistribution network operations significantly. RL-ADN is available at:https://github.com/ShengrenHou/RL-ADN andhttps://github.com/distributionnetworksTUDelft/RL-ADN.</description><author>Shengren Hou, Shuyi Gao, Weijie Xia, Edgar Mauricio Salazar Duque, Peter Palensky, Pedro P. Vergara</author><pubDate>Thu, 08 Aug 2024 13:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03685v2</guid></item></channel></rss>