<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 19 Nov 2024 13:00:16 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>UniHands: Unifying Various Wild-Collected Keypoints for Personalized Hand Reconstruction</title><link>http://arxiv.org/abs/2411.11845v1</link><description>Accurate hand motion capture and standardized 3D representation are essentialfor various hand-related tasks. Collecting keypoints-only data, while efficientand cost-effective, results in low-fidelity representations and lacks surfaceinformation. Furthermore, data inconsistencies across sources challenge theirintegration and use. We present UniHands, a novel method for creatingstandardized yet personalized hand models from wild-collected keypoints fromdiverse sources. Unlike existing neural implicit representation methods,UniHands uses the widely-adopted parametric models MANO and NIMBLE, providing amore scalable and versatile solution. It also derives unified hand joints fromthe meshes, which facilitates seamless integration into various hand-relatedtasks. Experiments on the FreiHAND and InterHand2.6M datasets demonstrate itsability to precisely reconstruct hand mesh vertices and keypoints, effectivelycapturing high-degree articulation motions. Empirical studies involving nineparticipants show a clear preference for our unified joints over existingconfigurations for accuracy and naturalism (p-value 0.016).</description><author>Menghe Zhang, Joonyeoup Kim, Yangwen Liang, Shuangquan Wang, Kee-Bong Song</author><pubDate>Mon, 18 Nov 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11845v1</guid></item><item><title>Generative World Explorer</title><link>http://arxiv.org/abs/2411.11844v1</link><description>Planning with partial observation is a central challenge in embodied AI. Amajority of prior works have tackled this challenge by developing agents thatphysically explore their environment to update their beliefs about the worldstate.In contrast, humans can $\textit{imagine}$ unseen parts of the worldthrough a mental exploration and $\textit{revise}$ their beliefs with imaginedobservations. Such updated beliefs can allow them to make more informeddecisions, without necessitating the physical exploration of the world at alltimes. To achieve this human-like ability, we introduce the $\textit{GenerativeWorld Explorer (Genex)}$, an egocentric world exploration framework that allowsan agent to mentally explore a large-scale 3D world (e.g., urban scenes) andacquire imagined observations to update its belief. This updated belief willthen help the agent to make a more informed decision at the current step. Totrain $\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.Our experimental results demonstrate that (1) $\textit{Genex}$ can generatehigh-quality and consistent observations during long-horizon exploration of alarge virtual physical world and (2) the beliefs updated with the generatedobservations can inform an existing decision-making model (e.g., an LLM agent)to make better plans.</description><author>Taiming Lu, Tianmin Shu, Alan Yuille, Daniel Khashabi, Jieneng Chen</author><pubDate>Mon, 18 Nov 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11844v1</guid></item><item><title>Bi-Mamba: Towards Accurate 1-Bit State Space Models</title><link>http://arxiv.org/abs/2411.11843v1</link><description>The typical selective state-space model (SSM) of Mamba addresses severallimitations of Transformers, such as quadratic computational complexity withsequence length and significant inference-time memory requirements due to thekey-value cache. However, the growing size of Mamba models continues to posetraining and deployment challenges and raises environmental concerns due toconsiderable energy consumption. In this work, we introduce Bi-Mamba, ascalable and powerful 1-bit Mamba architecture designed for more efficientlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mambamodels are trained from scratch on data volume as regular LLM pertaining usingan autoregressive distillation loss. Extensive experimental results on languagemodeling demonstrate that Bi-Mamba achieves performance comparable to itsfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy thanpost-training-binarization (PTB) Mamba baselines, while significantly reducingmemory footprint and energy consumption compared to the original Mamba model.Our study pioneers a new linear computational complexity LLM framework underlow-bit representation and facilitates the future design of specializedhardware tailored for efficient 1-bit Mamba-based LLMs.</description><author>Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, Zhiqiang Shen</author><pubDate>Mon, 18 Nov 2024 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11843v1</guid></item><item><title>RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator</title><link>http://arxiv.org/abs/2411.11839v1</link><description>Efficient acquisition of real-world embodied data has been increasinglycritical. However, large-scale demonstrations captured by remote operation tendto take extremely high costs and fail to scale up the data size in an efficientmanner. Sampling the episodes under a simulated environment is a promising wayfor large-scale collection while existing simulators fail to high-fidelitymodeling on texture and physics. To address these limitations, we introduce theRoboGSim, a real2sim2real robotic simulator, powered by 3D Gaussian Splattingand the physics engine. RoboGSim mainly includes four parts: GaussianReconstructor, Digital Twins Builder, Scene Composer, and Interactive Engine.It can synthesize the simulated data with novel views, objects, trajectories,and scenes. RoboGSim also provides an online, reproducible, and safe evaluationfor different manipulation policies. The real2sim and sim2real transferexperiments show a high consistency in the texture and physics. Moreover, theeffectiveness of synthetic data is validated under the real-world manipulatedtasks. We hope RoboGSim serves as a closed-loop simulator for fair comparisonon policy learning. More information can be found on our project pagehttps://robogsim.github.io/ .</description><author>Xinhai Li, Jialin Li, Ziheng Zhang, Rui Zhang, Fan Jia, Tiancai Wang, Haoqiang Fan, Kuo-Kun Tseng, Ruiping Wang</author><pubDate>Mon, 18 Nov 2024 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11839v1</guid></item><item><title>Pairwise Markov Chains for Volatility Forecasting</title><link>http://arxiv.org/abs/2411.11838v1</link><description>The Pairwise Markov Chain (PMC) is a probabilistic graphical model extendingthe well-known Hidden Markov Model. This model, although highly effective formany tasks, has been scarcely utilized for continuous value prediction. This ismainly due to the issue of modeling observations inherent in generativeprobabilistic models. In this paper, we introduce a new algorithm forprediction with the PMC. On the one hand, this algorithm allows circumventingthe feature problem, thus fully exploiting the capabilities of the PMC. On theother hand, it enables the PMC to extend any predictive model by introducinghidden states, updated at each time step, and allowing the introduction ofnon-stationarity for any model. We apply the PMC with its new algorithm forvolatility forecasting, which we compare to the highly popular GARCH(1,1) andfeedforward neural models across numerous pairs. This is particularly relevantgiven the regime changes that we can observe in volatility. For each scenario,our algorithm enhances the performance of the extended model, demonstrating thevalue of our approach.</description><author>Elie Azeraf</author><pubDate>Mon, 18 Nov 2024 18:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11838v1</guid></item><item><title>What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?</title><link>http://arxiv.org/abs/2411.07681v2</link><description>Despite the remarkable capabilities of modern large language models (LLMs),the mechanisms behind their problem-solving abilities remain elusive. In thiswork, we aim to better understand how the learning dynamics of LLM finetuningshapes downstream generalization. Our analysis focuses on reasoning tasks,whose problem structure allows us to distinguish between memorization (theexact replication of reasoning steps from the training data) and performance(the correctness of the final solution). We find that a model's generalizationbehavior can be effectively characterized by a training metric we callpre-memorization train accuracy: the accuracy of model samples on trainingqueries before they begin to copy the exact reasoning steps from the trainingset. On the dataset level, this metric is able to reliably predict testaccuracy, achieving $R^2$ of around or exceeding 0.9 across various models(Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. Ona per-example level, this metric is also indicative of whether individual modelpredictions are robust to perturbations in the training query. By connecting amodel's learning behavior to its generalization, pre-memorization trainaccuracy can guide targeted improvements to training strategies. We focus ondata curation as an example, and show that prioritizing examples with lowpre-memorization accuracy leads to 1.5-2x improvements in data efficiencycompared to i.i.d. data scaling, and outperforms other standard data curationtechniques.</description><author>Katie Kang, Amrith Setlur, Dibya Ghosh, Jacob Steinhardt, Claire Tomlin, Sergey Levine, Aviral Kumar</author><pubDate>Mon, 18 Nov 2024 18:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07681v2</guid></item><item><title>Tackling prediction tasks in relational databases with LLMs</title><link>http://arxiv.org/abs/2411.11829v1</link><description>Though large language models (LLMs) have demonstrated exceptional performanceacross numerous problems, their application to predictive tasks in relationaldatabases remains largely unexplored. In this work, we address the notion thatLLMs cannot yield satisfactory results on relational databases due to theirinterconnected tables, complex relationships, and heterogeneous data types.Using the recently introduced RelBench benchmark, we demonstrate that even astraightforward application of LLMs achieves competitive performance on thesetasks. These findings establish LLMs as a promising new baseline for ML onrelational databases and encourage further research in this direction.</description><author>Marek Wydmuch, Łukasz Borchmann, Filip Graliński</author><pubDate>Mon, 18 Nov 2024 18:48:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11829v1</guid></item><item><title>LightFFDNets: Lightweight Convolutional Neural Networks for Rapid Facial Forgery Detection</title><link>http://arxiv.org/abs/2411.11826v1</link><description>Accurate and fast recognition of forgeries is an issue of great importance inthe fields of artificial intelligence, image processing and object detection.Recognition of forgeries of facial imagery is the process of classifying anddefining the faces in it by analyzing real-world facial images. This process isusually accomplished by extracting features from an image, using classifieralgorithms, and correctly interpreting the results. Recognizing forgeries offacial imagery correctly can encounter many different challenges. For example,factors such as changing lighting conditions, viewing faces from differentangles can affect recognition performance, and background complexity andperspective changes in facial images can make accurate recognition difficult.Despite these difficulties, significant progress has been made in the field offorgery detection. Deep learning algorithms, especially Convolutional NeuralNetworks (CNNs), have significantly improved forgery detection performance. This study focuses on image processing-based forgery detection usingFake-Vs-Real-Faces (Hard) [10] and 140k Real and Fake Faces [61] data sets.Both data sets consist of two classes containing real and fake facial images.In our study, two lightweight deep learning models are proposed to conductforgery detection using these images. Additionally, 8 different pretrained CNNarchitectures were tested on both data sets and the results were compared withnewly developed lightweight CNN models. It's shown that the proposedlightweight deep learning models have minimum number of layers. It's also shownthat the proposed lightweight deep learning models detect forgeries of facialimagery accurately, and computationally efficiently. Although the data setconsists only of face images, the developed models can also be used in othertwo-class object recognition problems.</description><author>Günel Jabbarlı, Murat Kurt</author><pubDate>Mon, 18 Nov 2024 18:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11826v1</guid></item><item><title>Theoretical Foundations of Conformal Prediction</title><link>http://arxiv.org/abs/2411.11824v1</link><description>This book is about conformal prediction and related inferential techniquesthat build on permutation tests and exchangeability. These techniques areuseful in a diverse array of tasks, including hypothesis testing and providinguncertainty quantification guarantees for machine learning systems. Much of thecurrent interest in conformal prediction is due to its ability to integrateinto complex machine learning workflows, solving the problem of formingprediction sets without any assumptions on the form of the data generatingdistribution. Since contemporary machine learning algorithms have generallyproven difficult to analyze directly, conformal prediction's main appeal is itsability to provide formal, finite-sample guarantees when paired with suchmethods. The goal of this book is to teach the reader about the fundamental technicalarguments that arise when researching conformal prediction and relatedquestions in distribution-free inference. Many of these proof strategies,especially the more recent ones, are scattered among research papers, making itdifficult for researchers to understand where to look, which results areimportant, and how exactly the proofs work. We hope to bridge this gap bycurating what we believe to be some of the most important results in theliterature and presenting their proofs in a unified language, withillustrations, and with an eye towards pedagogy.</description><author>Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates</author><pubDate>Mon, 18 Nov 2024 18:44:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11824v1</guid></item><item><title>Toxicity of the Commons: Curating Open-Source Pre-Training Data</title><link>http://arxiv.org/abs/2410.22587v2</link><description>Open-source large language models are becoming increasingly available andpopular among researchers and practitioners. While significant progress hasbeen made on open-weight models, open training data is a practice yet to beadopted by the leading open-weight models creators. At the same time, thereresearchers are working to make language models safer. We propose a datacuration pipeline to reduce harmful outputs by models trained on public domaindata. There are unique challenges to working with public domain data, as thesesources differ from web text in both form and content. Many sources arehistorical documents and are the result of Optical Character Recognition (OCR).Consequently, current state-of-the-art approaches to toxicity filtering areoften infeasible or inappropriate for open data models. In this paper, weintroduce a new fully open-source pipeline for open-data toxicity filtering.Our contributions are threefold. We create a custom training dataset,ToxicCommons, which is composed of texts which have been classified across fivedifferent dimensions (racial/origin-based, gender/sex-based, religious,ability-based discrimination, and violence). We use this dataset to train acustom classifier, Celadon, that can be used to detect toxic content in opendata more efficiently at a larger scale. Finally, we describe the balancedapproach to content filtration that optimizes safety filtering with respect tothe filtered data available for training.</description><author>Catherine Arnett, Eliot Jones, Ivan P. Yamshchikov, Pierre-Carl Langlais</author><pubDate>Mon, 18 Nov 2024 18:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22587v2</guid></item><item><title>A Perspective for Adapting Generalist AI to Specialized Medical AI Applications and Their Challenges</title><link>http://arxiv.org/abs/2411.00024v2</link><description>The integration of Large Language Models (LLMs) into medical applications hassparked widespread interest across the healthcare industry, from drug discoveryand development to clinical decision support, assisting telemedicine, medicaldevices, and healthcare insurance applications. This perspective paper aims todiscuss the inner workings of building LLM-powered medical AI applications andintroduces a comprehensive framework for their development. We review existingliterature and outline the unique challenges of applying LLMs in specializedmedical contexts. Additionally, we introduce a three-step framework to organizemedical LLM research activities: 1) Modeling: breaking down complex medicalworkflows into manageable steps for developing medical-specific models; 2)Optimization: optimizing the model performance with crafted prompts andintegrating external knowledge and tools, and 3) System engineering:decomposing complex tasks into subtasks and leveraging human expertise forbuilding medical AI applications. Furthermore, we offer a detailed use caseplaybook that describes various LLM-powered medical AI applications, such asoptimizing clinical trial design, enhancing clinical decision support, andadvancing medical imaging analysis. Finally, we discuss various challenges andconsiderations for building medical AI applications with LLMs, such as handlinghallucination issues, data ownership and compliance, privacy, intellectualproperty considerations, compute cost, sustainability issues, and responsibleAI requirements.</description><author>Zifeng Wang, Hanyin Wang, Benjamin Danek, Ying Li, Christina Mack, Hoifung Poon, Yajuan Wang, Pranav Rajpurkar, Jimeng Sun</author><pubDate>Mon, 18 Nov 2024 18:41:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00024v2</guid></item><item><title>Equivariant spatio-hemispherical networks for diffusion MRI deconvolution</title><link>http://arxiv.org/abs/2411.11819v1</link><description>Each voxel in a diffusion MRI (dMRI) image contains a spherical signalcorresponding to the direction and strength of water diffusion in the brain.This paper advances the analysis of such spatio-spherical data by developingconvolutional network layers that are equivariant to the $\mathbf{E(3) \timesSO(3)}$ group and account for the physical symmetries of dMRI includingrotations, translations, and reflections of space alongside voxel-wiserotations. Further, neuronal fibers are typically antipodally symmetric, a factwe leverage to construct highly efficient spatio-hemispherical graphconvolutions to accelerate the analysis of high-dimensional dMRI data. In thecontext of sparse spherical fiber deconvolution to recover white mattermicrostructure, our proposed equivariant network layers yield substantialperformance and efficiency gains, leading to better and more practicalresolution of crossing neuronal fibers and fiber tractography. These gains areexperimentally consistent across both simulation and in vivo human datasets.</description><author>Axel Elaldi, Guido Gerig, Neel Dey</author><pubDate>Mon, 18 Nov 2024 18:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11819v1</guid></item><item><title>A Review of Digital Pixel Sensors</title><link>http://arxiv.org/abs/2402.04507v2</link><description>Digital pixel sensor (DPS) has evolved as a pivotal component in modernimaging systems and has the potential to revolutionize various fields such asmedical imaging, astronomy, surveillance, IoT devices, etc. Compared to analogpixel sensors, the DPS offers high speed and good image quality. However, theintroduced intrinsic complexity within each pixel, primarily attributed to theaccommodation of the ADC circuit, engenders a substantial increase in the pixelpitch. Unfortunately, such a pronounced escalation in pixel pitch drasticallyundermines the feasibility of achieving high-density integration, which is anobstacle that significantly narrows down the field of potential applications.Nonetheless, designing compact conversion circuits along with strategicintegration of 3D architectural paradigms can be a potential remedy to theprevailing situation. This review article presents a comprehensive overview ofthe vast area of DPS technology. The operating principles, advantages, andchallenges of different types of DPS circuits have been analyzed. We categorizethe schemes into several categories based on ADC operation. A comparative studybased on different performance metrics has also been showcased for awell-rounded understanding.</description><author>Md Rahatul Islam Udoy, Shamiul Alam, Md Mazharul Islam, Akhilesh Jaiswal, Ahmedullah Aziz</author><pubDate>Mon, 18 Nov 2024 18:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04507v2</guid></item><item><title>Watermark-based Detection and Attribution of AI-Generated Content</title><link>http://arxiv.org/abs/2404.04254v2</link><description>Several companies have deployed watermark-based detection to identifyAI-generated content. However, attribution--the ability to trace back to theuser of a generative AI (GenAI) service who created a given piece ofAI-generated content--remains largely unexplored despite its growingimportance. In this work, we aim to bridge this gap by conducting the firstsystematic study on watermark-based, user-level attribution of AI-generatedcontent. Our key idea is to assign a unique watermark to each user of the GenAIservice and embed this watermark into the AI-generated content created by thatuser. Attribution is then performed by identifying the user whose watermarkbest matches the one extracted from the given content. This approach, however,faces a key challenge: How should watermarks be selected for users to maximizeattribution performance? To address the challenge, we first theoreticallyderive lower bounds on detection and attribution performance through rigorousprobabilistic analysis for any given set of user watermarks. Then, we selectwatermarks for users to maximize these lower bounds, thereby optimizingdetection and attribution performance. Our theoretical and empirical resultsshow that watermark-based attribution inherits both the accuracy and(non-)robustness properties of the underlying watermark. Specifically,attribution remains highly accurate when the watermarked AI-generated contentis either not post-processed or subjected to common post-processing such asJPEG compression, as well as black-box adversarial post-processing with limitedquery budgets.</description><author>Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong</author><pubDate>Mon, 18 Nov 2024 18:35:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04254v2</guid></item><item><title>A Multimodal Adaptive Graph-based Intelligent Classification Model for Fake News</title><link>http://arxiv.org/abs/2411.06097v2</link><description>Numerous studies have been proposed to detect fake news focusing onmulti-modalities based on machine and/or deep learning. However, studiesfocusing on graph-based structures using geometric deep learning are lacking.To address this challenge, we introduce the Multimodal Adaptive Graph-basedIntelligent Classification (aptly referred to as MAGIC) for fake newsdetection. Specifically, the Encoder Representations from Transformers was usedfor text vectorization whilst ResNet50 was used for images. A comprehensiveinformation interaction graph was built using the adaptive Graph AttentionNetwork before classifying the multimodal input through the Softmax function.MAGIC was trained and tested on two fake news datasets, that is, Fakeddit(English) and Multimodal Fake News Detection (Chinese), with the modelachieving an accuracy of 98.8\% and 86.3\%, respectively. Ablation experimentsalso revealed MAGIC to yield superior performance across both the datasets.Findings show that a graph-based deep learning adaptive model is effective indetecting multimodal fake news, surpassing state-of-the-art methods.</description><author>Jun-hao, Xu</author><pubDate>Mon, 18 Nov 2024 18:19:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.06097v2</guid></item><item><title>KAN/MultKAN with Physics-Informed Spline fitting (KAN-PISF) for ordinary/partial differential equation discovery of nonlinear dynamic systems</title><link>http://arxiv.org/abs/2411.11801v1</link><description>Machine learning for scientific discovery is increasingly becoming popularbecause of its ability to extract and recognize the nonlinear characteristicsfrom the data. The black-box nature of deep learning methods poses difficultiesin interpreting the identified model. There is a dire need to interpret themachine learning models to develop a physical understanding of dynamic systems.An interpretable form of neural network called Kolmogorov-Arnold networks (KAN)or Multiplicative KAN (MultKAN) offers critical features that help recognizethe nonlinearities in the governing ordinary/partial differential equations(ODE/PDE) of various dynamic systems and find their equation structures. Inthis study, an equation discovery framework is proposed that includes i)sequentially regularized derivatives for denoising (SRDD) algorithm to denoisethe measure data to obtain accurate derivatives, ii) KAN to identify theequation structure and suggest relevant nonlinear functions that are used tocreate a small overcomplete library of functions, and iii) physics-informedspline fitting (PISF) algorithm to filter the excess functions from the libraryand converge to the correct equation. The framework was tested on the forcedDuffing oscillator, Van der Pol oscillator (stiff ODE), Burger's equation, andBouc-Wen model (coupled ODE). The proposed method converged to the trueequation for the first three systems. It provided an approximate model for theBouc-Wen model that could acceptably capture the hysteresis response. Using KANmaintains low complexity, which helps the user interpret the results throughoutthe process and avoid the black-box-type nature of machine learning methods.</description><author>Ashish Pal, Satish Nagarajaiah</author><pubDate>Mon, 18 Nov 2024 18:14:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11801v1</guid></item><item><title>Edge-Enhanced Dilated Residual Attention Network for Multimodal Medical Image Fusion</title><link>http://arxiv.org/abs/2411.11799v1</link><description>Multimodal medical image fusion is a crucial task that combines complementaryinformation from different imaging modalities into a unified representation,thereby enhancing diagnostic accuracy and treatment planning. While deeplearning methods, particularly Convolutional Neural Networks (CNNs) andTransformers, have significantly advanced fusion performance, some of theexisting CNN-based methods fall short in capturing fine-grained multiscale andedge features, leading to suboptimal feature integration. Transformer-basedmodels, on the other hand, are computationally intensive in both the trainingand fusion stages, making them impractical for real-time clinical use.Moreover, the clinical application of fused images remains unexplored. In thispaper, we propose a novel CNN-based architecture that addresses theselimitations by introducing a Dilated Residual Attention Network Module foreffective multiscale feature extraction, coupled with a gradient operator toenhance edge detail learning. To ensure fast and efficient fusion, we present aparameter-free fusion strategy based on the weighted nuclear norm of softmax,which requires no additional computations during training or inference.Extensive experiments, including a downstream brain tumor classification task,demonstrate that our approach outperforms various baseline methods in terms ofvisual quality, texture preservation, and fusion speed, making it a possiblepractical solution for real-world clinical applications. The code will bereleased at https://github.com/simonZhou86/en_dran.</description><author>Meng Zhou, Yuxuan Zhang, Xiaolan Xu, Jiayi Wang, Farzad Khalvati</author><pubDate>Mon, 18 Nov 2024 18:11:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11799v1</guid></item><item><title>Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods</title><link>http://arxiv.org/abs/2411.11795v1</link><description>Adversarial robustness of neural networks is an increasingly important areaof research, combining studies on computer vision models, large language models(LLMs), and others. With the release of JPEG AI - the first standard forend-to-end neural image compression (NIC) methods - the question of itsrobustness has become critically significant. JPEG AI is among the firstinternational, real-world applications of neural-network-based models to beembedded in consumer devices. However, research on NIC robustness has beenlimited to open-source codecs and a narrow range of attacks. This paperproposes a new methodology for measuring NIC robustness to adversarial attacks.We present the first large-scale evaluation of JPEG AI's robustness, comparingit with other NIC models. Our evaluation results and code are publiclyavailable online (link is hidden for a blind review).</description><author>Egor Kovalev, Georgii Bychkov, Khaled Abud, Aleksandr Gushchin, Anna Chistyakova, Sergey Lavrushkin, Dmitriy Vatolin, Anastasia Antsiferova</author><pubDate>Mon, 18 Nov 2024 18:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11795v1</guid></item><item><title>Competing Bandits in Decentralized Large Contextual Matching Markets</title><link>http://arxiv.org/abs/2411.11794v1</link><description>Sequential learning in a multi-agent resource constrained matching market hasreceived significant interest in the past few years. We study decentralizedlearning in two-sided matching markets where the demand side (aka players oragents) competes for a `large' supply side (aka arms) with potentiallytime-varying preferences, to obtain a stable match. Despite a long line of workin the recent past, existing learning algorithms such as Explore-Then-Commit orUpper-Confidence-Bound remain inefficient for this problem. In particular, theper-agent regret achieved by these algorithms scales linearly with the numberof arms, $K$. Motivated by the linear contextual bandit framework, we assumethat for each agent an arm-mean can be represented by a linear function of aknown feature vector and an unknown (agent-specific) parameter. Moreover, our setup captures the essence of a dynamic (non-stationary)matching market where the preferences over arms change over time. Our proposedalgorithms achieve instance-dependent logarithmic regret, scaling independentlyof the number of arms, $K$.</description><author>Satush Parikh, Soumya Basu, Avishek Ghosh, Abishek Sankararaman</author><pubDate>Mon, 18 Nov 2024 18:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11794v1</guid></item><item><title>A Potential Game Perspective in Federated Learning</title><link>http://arxiv.org/abs/2411.11793v1</link><description>Federated learning (FL) is an emerging paradigm for training machine learningmodels across distributed clients. Traditionally, in FL settings, a centralserver assigns training efforts (or strategies) to clients. However, from amarket-oriented perspective, clients may independently choose their trainingefforts based on rational self-interest. To explore this, we propose apotential game framework where each client's payoff is determined by theirindividual efforts and the rewards provided by the server. The rewards areinfluenced by the collective efforts of all clients and can be modulatedthrough a reward factor. Our study begins by establishing the existence of Nashequilibria (NEs), followed by an investigation of uniqueness in homogeneoussettings. We demonstrate a significant improvement in clients' training effortsat a critical reward factor, identifying it as the optimal choice for theserver. Furthermore, we prove the convergence of the best-response algorithm tocompute NEs for our FL game. Finally, we apply the training efforts derivedfrom specific NEs to a real-world FL scenario, validating the effectiveness ofthe identified optimal reward factor.</description><author>Kang Liu, Ziqi Wang, Enrique Zuazua</author><pubDate>Mon, 18 Nov 2024 18:06:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11793v1</guid></item><item><title>Parallelly Tempered Generative Adversarial Networks</title><link>http://arxiv.org/abs/2411.11786v1</link><description>A generative adversarial network (GAN) has been a representative backbonemodel in generative artificial intelligence (AI) because of its powerfulperformance in capturing intricate data-generating processes. However, the GANtraining is well-known for its notorious training instability, usuallycharacterized by the occurrence of mode collapse. Through the lens ofgradients' variance, this work particularly analyzes the training instabilityand inefficiency in the presence of mode collapse by linking it tomultimodality in the target distribution. To ease the raised training issuesfrom severe multimodality, we introduce a novel GAN training framework thatleverages a series of tempered distributions produced via convex interpolation.With our newly developed GAN objective function, the generator can learn allthe tempered distributions simultaneously, conceptually resonating with theparallel tempering in Statistics. Our simulation studies demonstrate thesuperiority of our approach over existing popular training strategies in bothimage and tabular data synthesis. We theoretically analyze that suchsignificant improvement can arise from reducing the variance of gradientestimates by using the tempered distributions. Finally, we further develop avariant of the proposed framework aimed at generating fair synthetic data whichis one of the growing interests in the field of trustworthy AI.</description><author>Jinwon Sohn, Qifan Song</author><pubDate>Mon, 18 Nov 2024 18:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11786v1</guid></item><item><title>CRoP: Context-wise Robust Static Human-Sensing Personalization</title><link>http://arxiv.org/abs/2409.17994v3</link><description>The advancement in deep learning and internet-of-things have led to diversehuman sensing applications. However, distinct patterns in human sensing,influenced by various factors or contexts, challenge the generic neural networkmodel's performance due to natural distribution shifts. To address this,personalization tailors models to individual users. Yet most personalizationstudies overlook intra-user heterogeneity across contexts in sensory data,limiting intra-user generalizability. This limitation is especially critical inclinical applications, where limited data availability hampers bothgeneralizability and personalization. Notably, intra-user sensing attributesare expected to change due to external factors such as treatment progression,further complicating the challenges. To address the intra-user generalizationchallenge, this work introduces CRoP, a novel static personalization approach.CRoP leverages off-the-shelf pre-trained models as generic starting points andcaptures user-specific traits through adaptive pruning on a minimal sub-networkwhile preserving generic knowledge in the remaining parameters. CRoPdemonstrates superior personalization effectiveness and intra-user robustnessacross four human-sensing datasets, including two from real-world healthdomains, underscoring its practical and social impact. Additionally, to supportCRoP's generalization ability and design choices, we provide empiricaljustification through gradient inner product analysis, ablation studies, andcomparisons against state-of-the-art baselines.</description><author>Sawinder Kaur, Avery Gump, Jingyu Xin, Yi Xiao, Harshit Sharma, Nina R Benway, Jonathan L Preston, Asif Salekin</author><pubDate>Mon, 18 Nov 2024 18:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17994v3</guid></item><item><title>MIST: A Simple and Scalable End-To-End 3D Medical Imaging Segmentation Framework</title><link>http://arxiv.org/abs/2407.21343v2</link><description>Medical imaging segmentation is a highly active area of research, with deeplearning-based methods achieving state-of-the-art results in severalbenchmarks. However, the lack of standardized tools for training, testing, andevaluating new methods makes the comparison of methods difficult. To addressthis, we introduce the Medical Imaging Segmentation Toolkit (MIST), a simple,modular, and end-to-end medical imaging segmentation framework designed tofacilitate consistent training, testing, and evaluation of deep learning-basedmedical imaging segmentation methods. MIST standardizes data analysis,preprocessing, and evaluation pipelines, accommodating multiple architecturesand loss functions. This standardization ensures reproducible and faircomparisons across different methods. We detail MIST's data formatrequirements, pipelines, and auxiliary features and demonstrate its efficacyusing the BraTS Adult Glioma Post-Treatment Challenge dataset. Our resultshighlight MIST's ability to produce accurate segmentation masks and itsscalability across multiple GPUs, showcasing its potential as a powerful toolfor future medical imaging research and development.</description><author>Adrian Celaya, Evan Lim, Rachel Glenn, Brayden Mi, Alex Balsells, Dawid Schellingerhout, Tucker Netherton, Caroline Chung, Beatrice Riviere, David Fuentes</author><pubDate>Mon, 18 Nov 2024 17:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21343v2</guid></item><item><title>Learning-Based Pricing and Matching for Two-Sided Queues</title><link>http://arxiv.org/abs/2403.11093v2</link><description>We consider a dynamic system with multiple types of customers and servers.Each type of waiting customer or server joins a separate queue, forming abipartite graph with customer-side queues and server-side queues. The platformcan match the servers and customers if their types are compatible. The matchedpairs then leave the system. The platform will charge a customer a priceaccording to their type when they arrive and will pay a server a priceaccording to their type. The arrival rate of each queue is determined by theprice according to some unknown demand or supply functions. Our goal is todesign pricing and matching algorithms to maximize the profit of the platformwith unknown demand and supply functions, while keeping queue lengths of bothcustomers and servers below a predetermined threshold. This system can be usedto model two-sided markets such as ride-sharing markets with passengers anddrivers. The difficulties of the problem include simultaneous learning anddecision making, and the tradeoff between maximizing profit and minimizingqueue length. We use a longest-queue-first matching algorithm and propose alearning-based pricing algorithm, which combines gradient-free stochasticprojected gradient ascent with bisection search. We prove that our proposedalgorithm yields a sublinear regret $\tilde{O}(T^{5/6})$ and anytimequeue-length bound $\tilde{O}(T^{1/6})$, where $T$ is the time horizon. Wefurther establish a tradeoff between the regret bound and the queue-lengthbound: $\tilde{O}(T^{1-\gamma})$ versus $\tilde{O}(T^{\gamma})$ for $\gamma \in(0, 1/6].$</description><author>Zixian Yang, Lei Ying</author><pubDate>Mon, 18 Nov 2024 17:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11093v2</guid></item><item><title>LLM-IE: A Python Package for Generative Information Extraction with Large Language Models</title><link>http://arxiv.org/abs/2411.11779v1</link><description>Objectives: Despite the recent adoption of large language models (LLMs) forbiomedical information extraction, challenges in prompt engineering andalgorithms persist, with no dedicated software available. To address this, wedeveloped LLM-IE: a Python package for building complete information extractionpipelines. Our key innovation is an interactive LLM agent to support schemadefinition and prompt design. Materials and Methods: The LLM-IE supports named entity recognition, entityattribute extraction, and relation extraction tasks. We benchmarked on the i2b2datasets and conducted a system evaluation. Results: The sentence-based prompting algorithm resulted in the bestperformance while requiring a longer inference time. System evaluation providedintuitive visualization. Discussion: LLM-IE was designed from practical NLP experience in healthcareand has been adopted in internal projects. It should hold great value to thebiomedical NLP community. Conclusion: We developed a Python package, LLM-IE, that provides buildingblocks for robust information extraction pipeline construction.</description><author>Enshuo Hsu, Kirk Roberts</author><pubDate>Mon, 18 Nov 2024 17:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11779v1</guid></item><item><title>Exploring the Requirements of Clinicians for Explainable AI Decision Support Systems in Intensive Care</title><link>http://arxiv.org/abs/2411.11774v1</link><description>There is a growing need to understand how digital systems can supportclinical decision-making, particularly as artificial intelligence (AI) modelsbecome increasingly complex and less human-interpretable. This complexityraises concerns about trustworthiness, impacting safe and effective adoption ofsuch technologies. Improved understanding of decision-making processes andrequirements for explanations coming from decision support tools is a vitalcomponent in providing effective explainable solutions. This is particularlyrelevant in the data-intensive, fast-paced environments of intensive care units(ICUs). To explore these issues, group interviews were conducted with seven ICUclinicians, representing various roles and experience levels. Thematic analysisrevealed three core themes: (T1) ICU decision-making relies on a wide range offactors, (T2) the complexity of patient state is challenging for shareddecision-making, and (T3) requirements and capabilities of AI decision supportsystems. We include design recommendations from clinical input, providinginsights to inform future AI systems for intensive care.</description><author>Jeffrey N. Clark, Matthew Wragg, Emily Nielsen, Miquel Perello-Nieto, Nawid Keshtmand, Michael Ambler, Shiv Sharma, Christopher P. Bourdeaux, Amberly Brigden, Raul Santos-Rodriguez</author><pubDate>Mon, 18 Nov 2024 17:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11774v1</guid></item><item><title>CNMBert: A Model For Hanyu Pinyin Abbreviation to Character Conversion Task</title><link>http://arxiv.org/abs/2411.11770v1</link><description>The task of converting Hanyu Pinyin abbreviations to Chinese charactersrepresents a significant branch within the domain of Chinese SpellingCorrection (CSC). This task is typically one of text-length alignment, however,due to the limited informational content in pinyin abbreviations, achievingaccurate conversion is challenging. In this paper, we propose CNMBert whichstands for zh-CN Pinyin Multi-mask Bert Model as a solution to this issue.CNMBert surpasses few-shot GPT models, achieving a 59.63% MRR on a10,424-sample Hanyu Pinyin abbreviation test dataset.</description><author>Zishuo Feng, Feng Cao</author><pubDate>Mon, 18 Nov 2024 17:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11770v1</guid></item><item><title>Effective Virtual Reality Teleoperation of an Upper-body Humanoid with Modified Task Jacobians and Relaxed Barrier Functions for Self-Collision Avoidance</title><link>http://arxiv.org/abs/2411.07534v2</link><description>We present an approach for retartgeting off-the-shelf Virtual Reality (VR)trackers to effectively teleoperate an upper-body humanoid while ensuringself-collision-free motions. Key to the effectiveness was the proper assignmentof trackers to joint sets via modified task Jacobians and relaxed barrierfunctions for self-collision avoidance. The approach was validated onApptronik's Astro hardware by demonstrating manipulation capabilities on atable-top environment with pick-and-place box packing and a two-handed box pickup and handover task.</description><author>Steven Jens Jorgensen, Ravi Bhadeshiya</author><pubDate>Mon, 18 Nov 2024 17:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07534v2</guid></item><item><title>Backdoor defense, learnability and obfuscation</title><link>http://arxiv.org/abs/2409.03077v2</link><description>We introduce a formal notion of defendability against backdoors using a gamebetween an attacker and a defender. In this game, the attacker modifies afunction to behave differently on a particular input known as the "trigger",while behaving the same almost everywhere else. The defender then attempts todetect the trigger at evaluation time. If the defender succeeds with highenough probability, then the function class is said to be defendable. The keyconstraint on the attacker that makes defense possible is that the attacker'sstrategy must work for a randomly-chosen trigger. Our definition is simple and does not explicitly mention learning, yet wedemonstrate that it is closely connected to learnability. In thecomputationally unbounded setting, we use a voting algorithm of Hanneke et al.(2022) to show that defendability is essentially determined by the VC dimensionof the function class, in much the same way as PAC learnability. In thecomputationally bounded setting, we use a similar argument to show thatefficient PAC learnability implies efficient defendability, but not conversely.On the other hand, we use indistinguishability obfuscation to show that theclass of polynomial size circuits is not efficiently defendable. Finally, wepresent polynomial size decision trees as a natural example for which defenseis strictly easier than learning. Thus, we identify efficient defendability asa notable intermediate concept in between efficient learnability andobfuscation.</description><author>Paul Christiano, Jacob Hilton, Victor Lecomte, Mark Xu</author><pubDate>Mon, 18 Nov 2024 17:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03077v2</guid></item><item><title>AdaptLIL: A Gaze-Adaptive Visualization for Ontology Mapping</title><link>http://arxiv.org/abs/2411.11768v1</link><description>This paper showcases AdaptLIL, a real-time adaptive link-indented listontology mapping visualization that uses eye gaze as the primary input source.Through a multimodal combination of real-time systems, deep learning, and webdevelopment applications, this system uniquely curtails graphical overlays(adaptations) to pairwise mappings of link-indented list ontologyvisualizations for individual users based solely on their eye gaze.</description><author>Nicholas Chow, Bo Fu</author><pubDate>Mon, 18 Nov 2024 17:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11768v1</guid></item><item><title>Drowning in Documents: Consequences of Scaling Reranker Inference</title><link>http://arxiv.org/abs/2411.11767v1</link><description>Rerankers, typically cross-encoders, are often used to re-score the documentsretrieved by cheaper initial IR systems. This is because, though expensive,rerankers are assumed to be more effective. We challenge this assumption bymeasuring reranker performance for full retrieval, not just re-scoringfirst-stage retrieval. Our experiments reveal a surprising trend: the bestexisting rerankers provide diminishing returns when scoring progressively moredocuments and actually degrade quality beyond a certain limit. In fact, in thissetting, rerankers can frequently assign high scores to documents with nolexical or semantic overlap with the query. We hope that our findings will spurfuture research to improve reranking.</description><author>Mathew Jacob, Erik Lindgren, Matei Zaharia, Michael Carbin, Omar Khattab, Andrew Drozdov</author><pubDate>Mon, 18 Nov 2024 17:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11767v1</guid></item><item><title>Freezing of Gait Detection Using Gramian Angular Fields and Federated Learning from Wearable Sensors</title><link>http://arxiv.org/abs/2411.11764v1</link><description>Freezing of gait (FOG) is a debilitating symptom of Parkinson's disease (PD)that impairs mobility and safety. Traditional detection methods face challengesdue to intra and inter-patient variability, and most systems are tested incontrolled settings, limiting their real-world applicability. Addressing thesegaps, we present FOGSense, a novel FOG detection system designed foruncontrolled, free-living conditions. It uses Gramian Angular Field (GAF)transformations and federated deep learning to capture temporal and spatialgait patterns missed by traditional methods. We evaluated our FOGSense systemusing a public PD dataset, 'tdcsfog'. FOGSense improves accuracy by 10.4% overa single-axis accelerometer, reduces failure points compared to multi-sensorsystems, and demonstrates robustness to missing values. The federatedarchitecture allows personalized model adaptation and efficient smartphonesynchronization during off-peak hours, making it effective for long-termmonitoring as symptoms evolve. Overall, FOGSense achieves a 22.2% improvementin F1-score compared to state-of-the-art methods, along with enhancedsensitivity for FOG episode detection. Code is available:https://github.com/shovito66/FOGSense.</description><author>Shovito Barua Soumma, S M Raihanul Alam, Rudmila Rahman, Umme Niraj Mahi, Sayyed Mostafa Mostafavi, Hassan Ghasemzadeh</author><pubDate>Mon, 18 Nov 2024 17:43:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11764v1</guid></item><item><title>Robust Subgraph Learning by Monitoring Early Training Representations</title><link>http://arxiv.org/abs/2403.09901v2</link><description>Graph neural networks (GNNs) have attracted significant attention for theiroutstanding performance in graph learning and node classification tasks.However, their vulnerability to adversarial attacks, particularly throughsusceptible nodes, poses a challenge in decision-making. The need for robustgraph summarization is evident in adversarial challenges resulting from thepropagation of attacks throughout the entire graph. In this paper, we addressboth performance and adversarial robustness in graph input by introducing thenovel technique SHERD (Subgraph Learning Hale through Early TrainingRepresentation Distances). SHERD leverages information from layers of apartially trained graph convolutional network (GCN) to detect susceptible nodesduring adversarial attacks using standard distance metrics. The methodidentifies "vulnerable (bad)" nodes and removes such nodes to form a robustsubgraph while maintaining node classification performance. Through ourexperiments, we demonstrate the increased performance of SHERD in enhancingrobustness by comparing the network's performance on original and subgraphinputs against various baselines alongside existing adversarial attacks. Ourexperiments across multiple datasets, including citation datasets such as Cora,Citeseer, and Pubmed, as well as microanatomical tissue structures of cellgraphs in the placenta, highlight that SHERD not only achieves substantialimprovement in robust performance but also outperforms several baselines interms of node classification accuracy and computational complexity.</description><author>Sepideh Neshatfar, Salimeh Yasaei Sekeh</author><pubDate>Mon, 18 Nov 2024 17:43:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09901v2</guid></item><item><title>Identifying and Addressing Delusions for Target-Directed Decision-Making</title><link>http://arxiv.org/abs/2410.07096v5</link><description>Target-directed agents utilize self-generated targets, to guide theirbehaviors for better generalization. These agents are prone to blindly chasingproblematic targets, resulting in worse generalization and safety catastrophes.We show that these behaviors can be results of delusions, stemming fromimproper designs around training: the agent may naturally come to hold falsebeliefs about certain targets. We identify delusions via intuitive examples incontrolled environments, and investigate their causes and mitigations. With theinsights, we demonstrate how we can make agents address delusions preemptivelyand autonomously. We validate empirically the effectiveness of the proposedstrategies in correcting delusional behaviors and improving out-of-distributiongeneralization.</description><author>Mingde Zhao, Tristan Sylvain, Doina Precup, Yoshua Bengio</author><pubDate>Mon, 18 Nov 2024 17:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07096v5</guid></item><item><title>Mapping out the Space of Human Feedback for Reinforcement Learning: A Conceptual Framework</title><link>http://arxiv.org/abs/2411.11761v1</link><description>Reinforcement Learning from Human feedback (RLHF) has become a powerful toolto fine-tune or train agentic machine learning models. Similar to how humansinteract in social contexts, we can use many types of feedback to communicateour preferences, intentions, and knowledge to an RL agent. However,applications of human feedback in RL are often limited in scope and disregardhuman factors. In this work, we bridge the gap between machine learning andhuman-computer interaction efforts by developing a shared understanding ofhuman feedback in interactive learning scenarios. We first introduce a taxonomyof feedback types for reward-based learning from human feedback based on ninekey dimensions. Our taxonomy allows for unifying human-centered,interface-centered, and model-centered aspects. In addition, we identify sevenquality metrics of human feedback influencing both the human ability to expressfeedback and the agent's ability to learn from the feedback. Based on thefeedback taxonomy and quality criteria, we derive requirements and designchoices for systems learning from human feedback. We relate these requirementsand design choices to existing work in interactive machine learning. In theprocess, we identify gaps in existing work and future research opportunities.We call for interdisciplinary collaboration to harness the full potential ofreinforcement learning with data-driven co-adaptive modeling and variedinteraction mechanics.</description><author>Yannick Metz, David Lindner, Raphaël Baur, Mennatallah El-Assady</author><pubDate>Mon, 18 Nov 2024 17:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11761v1</guid></item><item><title>The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning</title><link>http://arxiv.org/abs/2411.11758v1</link><description>Large Multimodal Models (LMMs) exhibit impressive performance across variousmultimodal tasks. However, their effectiveness in cross-cultural contextsremains limited due to the predominantly Western-centric nature of most dataand models. Conversely, multi-agent models have shown significant capability insolving complex tasks. Our study evaluates the collective performance of LMMsin a multi-agent interaction setting for the novel task of cultural imagecaptioning. Our contributions are as follows: (1) We introduce MosAIC, aMulti-Agent framework to enhance cross-cultural Image Captioning using LMMswith distinct cultural personas; (2) We provide a dataset of culturallyenriched image captions in English for images from China, India, and Romaniaacross three datasets: GeoDE, GD-VCR, CVQA; (3) We propose a culture-adaptablemetric for evaluating cultural information within image captions; and (4) Weshow that the multi-agent interaction outperforms single-agent models acrossdifferent metrics, and offer valuable insights for future research. Our datasetand models can be accessed at https://github.com/MichiganNLP/MosAIC.</description><author>Longju Bai, Angana Borah, Oana Ignat, Rada Mihalcea</author><pubDate>Mon, 18 Nov 2024 17:37:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11758v1</guid></item><item><title>DAWN: Designing Distributed Agents in a Worldwide Network</title><link>http://arxiv.org/abs/2410.22339v2</link><description>The rapid evolution of Large Language Models (LLMs) has transformed them frombasic conversational tools into sophisticated entities capable of complexreasoning and decision-making. These advancements have led to the developmentof specialized LLM-based agents designed for diverse tasks such as coding andweb browsing. As these agents become more capable, the need for a robustframework that facilitates global communication and collaboration among themtowards advanced objectives has become increasingly critical. DistributedAgents in a Worldwide Network (DAWN) addresses this need by offering aversatile framework that integrates LLM-based agents with traditional softwaresystems, enabling the creation of agentic applications suited for a wide rangeof use cases. DAWN enables distributed agents worldwide to register and beeasily discovered through Gateway Agents. Collaborations among these agents arecoordinated by a Principal Agent equipped with reasoning strategies. DAWNoffers three operational modes: No-LLM Mode for deterministic tasks, Copilotfor augmented decision-making, and LLM Agent for autonomous operations.Additionally, DAWN ensures the safety and security of agent collaborationsglobally through a dedicated safety, security, and compliance layer, protectingthe network against attackers and adhering to stringent security and compliancestandards. These features make DAWN a robust network for deploying agent-basedapplications across various industries.</description><author>Zahra Aminiranjbar, Jianan Tang, Qiudan Wang, Shubha Pant, Mahesh Viswanathan</author><pubDate>Mon, 18 Nov 2024 17:30:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22339v2</guid></item><item><title>MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images</title><link>http://arxiv.org/abs/2406.10853v3</link><description>We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-viewimages, not merely as a field or raw geometry but as a sketch-extrude CADmodel. Extracting extrusion cylinders from raw 3D geometry has been extensivelyresearched in computer vision, while the processing of 3D data through neuralnetworks has remained a bottleneck. Since 3D scans are generally accompanied bymulti-view images, leveraging 2D convolutional neural networks allows theseimages to be exploited as a rich source for extracting extrusion cylinderinformation. However, we observe that extracting only the surface informationof the extrudes and utilizing it results in suboptimal outcomes due to thechallenges in the occlusion and surface segmentation. By synergizing with theextracted base curve information, we achieve the optimal reconstruction resultwith the best accuracy in 2D sketch and extrude parameter estimation. Ourexperiments, comparing our method with previous work that takes a raw 3D pointcloud as input, demonstrate the effectiveness of our approach by takingadvantage of multi-view images. Our project page can be found athttp://mv2cyl.github.io .</description><author>Eunji Hong, Minh Hieu Nguyen, Mikaela Angelina Uy, Minhyuk Sung</author><pubDate>Mon, 18 Nov 2024 17:28:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10853v3</guid></item><item><title>AgentSquare: Automatic LLM Agent Search in Modular Design Space</title><link>http://arxiv.org/abs/2410.06153v2</link><description>Recent advancements in Large Language Models (LLMs) have led to a rapidgrowth of agentic systems capable of handling a wide range of complex tasks.However, current research largely relies on manual, task-specific design,limiting their adaptability to novel tasks. In this paper, we introduce a newresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modulardesign space that abstracts existing LLM agent designs into four fundamentalmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.Building on this design space, we present a novel LLM agent search frameworkcalled AgentSquare, which introduces two core mechanisms, i.e., moduleevolution and recombination, to efficiently search for optimized LLM agents. Tofurther accelerate the process, we design a performance predictor that usesin-context surrogate models to skip unpromising agent designs. Extensiveexperiments across six benchmarks, covering the diverse scenarios of web,embodied, tool use and game applications, show that AgentSquare substantiallyoutperforms hand-crafted agents, achieving an average performance gain of 17.2%against best-known human designs. Moreover, AgentSquare can generateinterpretable design insights, enabling a deeper understanding of agenticarchitecture and its impact on task performance. We believe that the modulardesign space and AgentSquare search framework offer a platform for fullyexploiting the potential of prior successful designs and consolidating thecollective efforts of research community. Code repo is available athttps://github.com/tsinghua-fib-lab/AgentSquare.</description><author>Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, Yong Li</author><pubDate>Mon, 18 Nov 2024 17:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06153v2</guid></item><item><title>Debiased Regression for Root-N-Consistent Conditional Mean Estimation</title><link>http://arxiv.org/abs/2411.11748v1</link><description>This study introduces a debiasing method for regression estimators, includinghigh-dimensional and nonparametric regression estimators. For example,nonparametric regression methods allow for the estimation of regressionfunctions in a data-driven manner with minimal assumptions; however, thesemethods typically fail to achieve $\sqrt{n}$-consistency in their convergencerates, and many, including those in machine learning, lack guarantees thattheir estimators asymptotically follow a normal distribution. To address thesechallenges, we propose a debiasing technique for nonparametric estimators byadding a bias-correction term to the original estimators, extending theconventional one-step estimator used in semiparametric analysis. Specifically,for each data point, we estimate the conditional expected residual of theoriginal nonparametric estimator, which can, for instance, be computed usingkernel (Nadaraya-Watson) regression, and incorporate it as a bias-reductionterm. Our theoretical analysis demonstrates that the proposed estimatorachieves $\sqrt{n}$-consistency and asymptotic normality under a mildconvergence rate condition for both the original nonparametric estimator andthe conditional expected residual estimator. Notably, this approach remainsmodel-free as long as the original estimator and the conditional expectedresidual estimator satisfy the convergence rate condition. The proposed methodoffers several advantages, including improved estimation accuracy andsimplified construction of confidence intervals.</description><author>Masahiro Kato</author><pubDate>Mon, 18 Nov 2024 17:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11748v1</guid></item><item><title>BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration</title><link>http://arxiv.org/abs/2411.11745v1</link><description>Large language models (LLMs) have demonstrated remarkable performance acrossvarious machine learning tasks. Yet the substantial memory footprint of LLMssignificantly hinders their deployment. In this paper, we improve theaccessibility of LLMs through BitMoD, an algorithm-hardware co-design solutionthat enables efficient LLM acceleration at low weight precision. On thealgorithm side, BitMoD introduces fine-grained data type adaptation that uses adifferent numerical data type to quantize a group of (e.g., 128) weights.Through the careful design of these new data types, BitMoD is able to quantizeLLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaininghigh accuracy. On the hardware side, BitMoD employs a bit-serial processingelement to easily support multiple numerical precisions and data types; ourhardware design includes two key innovations: First, it employs a unifiedrepresentation to process different weight data types, thus reducing thehardware cost. Second, it adopts a bit-serial dequantization unit to rescalethe per-group partial sum with minimal hardware overhead. Our evaluation on sixrepresentative LLMs demonstrates that BitMoD significantly outperformsstate-of-the-art LLM quantization and acceleration methods. For discriminativetasks, BitMoD can quantize LLM weights to 4-bit with $&lt;\!0.5\%$ accuracy losson average. For generative tasks, BitMoD is able to quantize LLM weights to3-bit while achieving better perplexity than prior LLM quantization scheme.Combining the superior model performance with an efficient accelerator design,BitMoD achieves an average of $1.69\times$ and $1.48\times$ speedups comparedto prior LLM accelerators ANT and OliVe, respectively.</description><author>Yuzong Chen, Ahmed F. AbouElhamayed, Xilai Dai, Yang Wang, Marta Andronic, George A. Constantinides, Mohamed S. Abdelfattah</author><pubDate>Mon, 18 Nov 2024 17:16:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11745v1</guid></item><item><title>Learning to mask: Towards generalized face forgery detection</title><link>http://arxiv.org/abs/2212.14309v2</link><description>Generalizability to unseen forgery types is crucial for face forgerydetectors. Recent works have made significant progress in terms ofgeneralization by synthetic forgery data augmentation. In this work, we exploreanother path for improving the generalization. Our goal is to reduce thefeatures that are easy to learn in the training phase, so as to reduce the riskof overfitting on specific forgery types. Specifically, in our method, ateacher network takes as input the face images and generates an attention mapof the deep features by a diverse multihead attention ViT. The attention map isused to guide a student network to focus on the low-attended features byreducing the highly-attended deep features. A deep feature mixup strategy isalso proposed to synthesize forgeries in the feature domain. Experimentsdemonstrate that, without data augmentation, our method is able to achievepromising performances on unseen forgeries and highly compressed data.</description><author>Jianwei Fei, Yunshu Dai, Huaming Wang, Zhihua Xia</author><pubDate>Mon, 18 Nov 2024 17:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14309v2</guid></item><item><title>Revitalizing Electoral Trust: Enhancing Transparency and Efficiency through Automated Voter Counting with Machine Learning</title><link>http://arxiv.org/abs/2411.11740v1</link><description>In order to address issues with manual vote counting during electionprocedures, this study intends to examine the viability of using advanced imageprocessing techniques for automated voter counting. The study aims to shedlight on how automated systems that utilize cutting-edge technologies likeOpenCV, CVZone, and the MOG2 algorithm could greatly increase the effectivenessand openness of electoral operations. The empirical findings demonstrate howautomated voter counting can enhance voting processes and rebuild publicconfidence in election outcomes, particularly in places where trust is low. Thestudy also emphasizes how rigorous metrics, such as the F1 score, should beused to systematically compare the accuracy of automated systems against manualcounting methods. This methodology enables a detailed comprehension of thedifferences in performance between automated and human counting techniques byproviding a nuanced assessment. The incorporation of said measures serves toreinforce an extensive assessment structure, guaranteeing the legitimacy anddependability of automated voting systems inside the electoral sphere.</description><author>Mir Faris, Syeda Aynul Karim, Md. Juniadul Islam</author><pubDate>Mon, 18 Nov 2024 17:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11740v1</guid></item><item><title>QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou</title><link>http://arxiv.org/abs/2411.11739v1</link><description>In recent years, with the significant evolution of multi-modal large models,many recommender researchers realized the potential of multi-modal informationfor user interest modeling. In industry, a wide-used modeling architecture is acascading paradigm: (1) first pre-training a multi-modal model to provideomnipotent representations for downstream services; (2) The downstreamrecommendation model takes the multi-modal representation as additional inputto fit real user-item behaviours. Although such paradigm achieves remarkableimprovements, however, there still exist two problems that limit modelperformance: (1) Representation Unmatching: The pre-trained multi-modal modelis always supervised by the classic NLP/CV tasks, while the recommendationmodels are supervised by real user-item interaction. As a result, the twofundamentally different tasks' goals were relatively separate, and there was alack of consistent objective on their representations; (2) RepresentationUnlearning: The generated multi-modal representations are always stored incache store and serve as extra fixed input of recommendation model, thus couldnot be updated by recommendation model gradient, further unfriendly fordownstream training. Inspired by the two difficulties challenges in downstreamtasks usage, we introduce a quantitative multi-modal framework to customize thespecialized and trainable multi-modal information for different downstreammodels.</description><author>Xinchen Luo, Jiangxia Cao, Tianyu Sun, Jinkai Yu, Rui Huang, Wei Yuan, Hezheng Lin, Yichen Zheng, Shiyao Wang, Qigen Hu, Changqing Qiu, Jiaqi Zhang, Xu Zhang, Zhiheng Yan, Jingming Zhang, Simin Zhang, Mingxing Wen, Zhaojie Liu, Kun Gai, Guorui Zhou</author><pubDate>Mon, 18 Nov 2024 17:08:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11739v1</guid></item><item><title>WoodYOLO: A Novel Object Detector for Wood Species Detection in Microscopic Images</title><link>http://arxiv.org/abs/2411.11738v1</link><description>Wood species identification plays a crucial role in various industries, fromensuring the legality of timber products to advancing ecological conservationefforts. This paper introduces WoodYOLO, a novel object detection algorithmspecifically designed for microscopic wood fiber analysis. Our approach adaptsthe YOLO architecture to address the challenges posed by large, high-resolutionmicroscopy images and the need for high recall in localization of the cell typeof interest (vessel elements). Our results show that WoodYOLO significantlyoutperforms state-of-the-art models, achieving performance gains of 12.9% and6.5% in F2 score over YOLOv10 and YOLOv7, respectively. This improvement inautomated wood cell type localization capabilities contributes to enhancingregulatory compliance, supporting sustainable forestry practices, and promotingbiodiversity conservation efforts globally.</description><author>Lars Nieradzik, Henrike Stephani, Jördis Sieburg-Rockel, Stephanie Helmling, Andrea Olbrich, Stephanie Wrage, Janis Keuper</author><pubDate>Mon, 18 Nov 2024 17:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11738v1</guid></item><item><title>Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure</title><link>http://arxiv.org/abs/2410.24060v3</link><description>In this work, we study the generalizability of diffusion models by lookinginto the hidden properties of the learned score functions, which areessentially a series of deep denoisers trained on various noise levels. Weobserve that as diffusion models transition from memorization togeneralization, their corresponding nonlinear diffusion denoisers exhibitincreasing linearity. This discovery leads us to investigate the linearcounterparts of the nonlinear diffusion models, which are a series of linearmodels trained to match the function mappings of the nonlinear diffusiondenoisers. Surprisingly, these linear denoisers are approximately the optimaldenoisers for a multivariate Gaussian distribution characterized by theempirical mean and covariance of the training dataset. This finding impliesthat diffusion models have the inductive bias towards capturing and utilizingthe Gaussian structure (covariance information) of the training dataset fordata generation. We empirically demonstrate that this inductive bias is aunique property of diffusion models in the generalization regime, which becomesincreasingly evident when the model's capacity is relatively small compared tothe training dataset size. In the case that the model is highlyoverparameterized, this inductive bias emerges during the initial trainingphases before the model fully memorizes its training data. Our study providescrucial insights into understanding the notable strong generalizationphenomenon recently observed in real-world diffusion models.</description><author>Xiang Li, Yixiang Dai, Qing Qu</author><pubDate>Mon, 18 Nov 2024 17:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24060v3</guid></item><item><title>Advacheck at GenAI Detection Task 1: AI Detection Powered by Domain-Aware Multi-Tasking</title><link>http://arxiv.org/abs/2411.11736v1</link><description>The paper describes a system designed by Advacheck team to recognisemachine-generated and human-written texts in the monolingual subtask of GenAIDetection Task 1 competition. Our developed system is a multi-task architecturewith shared Transformer Encoder between several classification heads. One headis responsible for binary classification between human-written andmachine-generated texts, while the other heads are auxiliary multiclassclassifiers for texts of different domains from particular datasets. Asmulticlass heads were trained to distinguish the domains presented in the data,they provide a better understanding of the samples. This approach led us toachieve the first place in the official ranking with 83.07% macro F1-score onthe test set and bypass the baseline by 10%. We further study obtained systemthrough ablation, error and representation analyses, finding that multi-tasklearning outperforms single-task mode and simultaneous tasks form a clusterstructure in embeddings space.</description><author>German Gritsai, Anastasia Voznyuk, Ildar Khabutdinov, Andrey Grabovoy</author><pubDate>Mon, 18 Nov 2024 17:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11736v1</guid></item><item><title>Fine-Tuning a Time Series Foundation Model with Wasserstein Loss</title><link>http://arxiv.org/abs/2409.15367v2</link><description>Inspired by recent advancements in large language models (LLMs) for NaturalLanguage Processing (NLP), there has been a surge in research focused ondeveloping foundational models for time series forecasting. One approachinvolves training LLM architectures on tokenized time series data usingcross-entropy loss. Although this method has demonstrated promising results,cross-entropy loss is primarily designed for classification tasks and does notaccount for the distance between classes. To address this limitation, wepropose using the Wasserstein loss for such architectures. To validate ourapproach, we fine-tuned a foundational time series model on $22$ zero-shotdatasets, comparing the performance of cross-entropy loss with that ofWasserstein loss. Our results demonstrate that replacing cross-entropy losswith Wasserstein loss significantly improves point estimation.</description><author>Andrei Chernov</author><pubDate>Mon, 18 Nov 2024 17:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15367v2</guid></item><item><title>Eidos: Efficient, Imperceptible Adversarial 3D Point Clouds</title><link>http://arxiv.org/abs/2405.14210v2</link><description>Classification of 3D point clouds is a challenging machine learning (ML) taskwith important real-world applications in a spectrum from autonomous drivingand robot-assisted surgery to earth observation from low orbit. As with otherML tasks, classification models are notoriously brittle in the presence ofadversarial attacks. These are rooted in imperceptible changes to inputs withthe effect that a seemingly well-trained model ends up misclassifying theinput. This paper adds to the understanding of adversarial attacks bypresenting Eidos, a framework providing Efficient Imperceptible aDversarialattacks on 3D pOint cloudS. Eidos supports a diverse set of imperceptibilitymetrics. It employs an iterative, two-step procedure to identify optimaladversarial examples, thereby enabling a runtime-imperceptibility trade-off. Weprovide empirical evidence relative to several popular 3D point cloudclassification models and several established 3D attack methods, showing Eidos'superiority with respect to efficiency as well as imperceptibility.</description><author>Hanwei Zhang, Luo Cheng, Qisong He, Wei Huang, Renjue Li, Ronan Sicre, Xiaowei Huang, Holger Hermanns, Lijun Zhang</author><pubDate>Mon, 18 Nov 2024 17:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14210v2</guid></item><item><title>Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment</title><link>http://arxiv.org/abs/2411.11731v1</link><description>We explore how large language models (LLMs) can be influenced by promptingthem to alter their initial decisions and align them with established ethicalframeworks. Our study is based on two experiments designed to assess thesusceptibility of LLMs to moral persuasion. In the first experiment, we examinethe susceptibility to moral ambiguity by evaluating a Base Agent LLM on morallyambiguous scenarios and observing how a Persuader Agent attempts to modify theBase Agent's initial decisions. The second experiment evaluates thesusceptibility of LLMs to align with predefined ethical frameworks by promptingthem to adopt specific value alignments rooted in established philosophicaltheories. The results demonstrate that LLMs can indeed be persuaded in morallycharged scenarios, with the success of persuasion depending on factors such asthe model used, the complexity of the scenario, and the conversation length.Notably, LLMs of distinct sizes but from the same company produced markedlydifferent outcomes, highlighting the variability in their susceptibility toethical persuasion.</description><author>Allison Huang, Yulu Niki Pi, Carlos Mougan</author><pubDate>Mon, 18 Nov 2024 16:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11731v1</guid></item><item><title>Lifted Model Construction without Normalisation: A Vectorised Approach to Exploit Symmetries in Factor Graphs</title><link>http://arxiv.org/abs/2411.11730v1</link><description>Lifted probabilistic inference exploits symmetries in a probabilistic modelto allow for tractable probabilistic inference with respect to domain sizes oflogical variables. We found that the current state-of-the-art algorithm toconstruct a lifted representation in form of a parametric factor graph missessymmetries between factors that are exchangeable but scaled differently,thereby leading to a less compact representation. In this paper, we propose ageneralisation of the advanced colour passing (ACP) algorithm, which is thestate of the art to construct a parametric factor graph. Our proposed algorithmallows for potentials of factors to be scaled arbitrarily and efficientlydetects more symmetries than the original ACP algorithm. By detecting strictlymore symmetries than ACP, our algorithm significantly reduces online querytimes for probabilistic inference when the resulting model is applied, which wealso confirm in our experiments.</description><author>Malte Luttermann, Ralf Möller, Marcel Gehrke</author><pubDate>Mon, 18 Nov 2024 16:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11730v1</guid></item><item><title>Aligning Few-Step Diffusion Models with Dense Reward Difference Learning</title><link>http://arxiv.org/abs/2411.11727v1</link><description>Aligning diffusion models with downstream objectives is essential for theirpractical applications. However, standard alignment methods often struggle withstep generalization when directly applied to few-step diffusion models, leadingto inconsistent performance across different denoising step scenarios. Toaddress this, we introduce Stepwise Diffusion Policy Optimization (SDPO), anovel alignment method tailored for few-step diffusion models. Unlike priorapproaches that rely on a single sparse reward from only the final step of eachdenoising trajectory for trajectory-level optimization, SDPO incorporates densereward feedback at every intermediate step. By learning the differences indense rewards between paired samples, SDPO facilitates stepwise optimization offew-step diffusion models, ensuring consistent alignment across all denoisingsteps. To promote stable and efficient training, SDPO introduces an onlinereinforcement learning framework featuring several novel strategies designed toeffectively exploit the stepwise granularity of dense rewards. Experimentalresults demonstrate that SDPO consistently outperforms prior methods inreward-based alignment across diverse step configurations, underscoring itsrobust step generalization capabilities. Code is avaliable athttps://github.com/ZiyiZhang27/sdpo.</description><author>Ziyi Zhang, Li Shen, Sen Zhang, Deheng Ye, Yong Luo, Miaojing Shi, Bo Du, Dacheng Tao</author><pubDate>Mon, 18 Nov 2024 16:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11727v1</guid></item><item><title>PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset</title><link>http://arxiv.org/abs/2403.11116v3</link><description>Multimodal Large Language Models (MLLMs) hallucinate, resulting in anemerging topic of visual hallucination evaluation (VHE). This paper contributesa ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objectiveVHE at a large scale. The essence of VHE is to ask an MLLM questions aboutspecific images to assess its susceptibility to hallucination. Depending onwhat to ask (objects, attributes, sentiment, etc.) and how the questions areasked, we structure PhD along two dimensions, i.e., task and mode. Five visualrecognition tasks, ranging from low-level (object / attribute recognition) tomiddle-level (sentiment / position recognition and counting), are considered.Besides a normal visual QA mode, which we term PhD-base, PhD also asksquestions with inaccurate context (PhD-iac) or with incorrect context(PhD-icc), or with AI-generated counter common sense images (PhD-ccs). Weconstruct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing fourpivotal modules: task-specific hallucinatory item (hitem) selection,hitem-embedded question generation, inaccurate / incorrect context generation,and counter-common-sense (CCS) image generation. With over 14k daily images,750 CCS images and 102k VQA triplets in total, PhD reveals considerablevariability in MLLMs' performance across various modes and tasks, offeringvaluable insights into the nature of hallucination. As such, PhD stands as apotent tool not only for VHE but may also play a significant role in therefinement of MLLMs.</description><author>Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li</author><pubDate>Mon, 18 Nov 2024 16:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11116v3</guid></item><item><title>V2X-R: Cooperative LiDAR-4D Radar Fusion for 3D Object Detection with Denoising Diffusion</title><link>http://arxiv.org/abs/2411.08402v2</link><description>Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3Dobject detection using LiDAR and camera data. However, these methods sufferfrom performance degradation in adverse weather conditions. The weatherrobust4D radar provides Doppler and additional geometric information, raising thepossibility of addressing this challenge. To this end, we present V2X-R, thefirst simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-Rcontains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar pointclouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes.Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for3D object detection and implement it with various fusion strategies. To achieveweather-robust detection, we additionally propose a Multi-modal DenoisingDiffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4Dradar feature as a condition to prompt the diffusion model to denoise noisyLiDAR features. Experiments show that our LiDAR-4D radar fusion pipelinedemonstrates superior performance in the V2X-R dataset. Over and above this,our MDD module further improved the performance of basic fusion model by up to5.73%/6.70% in foggy/snowy conditions with barely disrupting normalperformance. The dataset and code will be publicly available at:https://github.com/ylwhxht/V2X-R.</description><author>Xun Huang, Jinlong Wang, Qiming Xia, Siheng Chen, Bisheng Yang, Xin Li, Cheng Wang, Chenglu Wen</author><pubDate>Mon, 18 Nov 2024 16:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08402v2</guid></item><item><title>RAWMamba: Unified sRGB-to-RAW De-rendering With State Space Model</title><link>http://arxiv.org/abs/2411.11717v1</link><description>Recent advancements in sRGB-to-RAW de-rendering have increasingly emphasizedmetadata-driven approaches to reconstruct RAW data from sRGB images,supplemented by partial RAW information. In image-based de-rendering, metadatais commonly obtained through sampling, whereas in video tasks, it is typicallyderived from the initial frame. The distinct metadata requirements necessitatespecialized network architectures, leading to architectural incompatibilitiesthat increase deployment complexity. In this paper, we propose RAWMamba, aMamba-based unified framework developed for sRGB-to-RAW de-rendering acrossboth image and video domains. The core of RAWMamba is the Unified MetadataEmbedding (UME) module, which harmonizes diverse metadata types into a unifiedrepresentation. In detail, a multi-perspective affinity modeling method isproposed to promote the extraction of reference information. In addition, weintroduce the Local Tone-Aware Mamba (LTA-Mamba) module, which captureslong-range dependencies to enable effective global propagation of metadata.Experimental results demonstrate that the proposed RAWMamba achievesstate-of-the-art performance, yielding high-quality RAW data reconstruction.</description><author>Hongjun Chen, Wencheng Han, Huan Zheng, Jianbing Shen</author><pubDate>Mon, 18 Nov 2024 16:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11717v1</guid></item><item><title>Unmasking Parkinson's Disease with Smile: An AI-enabled Screening Framework</title><link>http://arxiv.org/abs/2308.02588v2</link><description>We present an efficient and accessible PD screening method by leveragingAI-driven models enabled by the largest video dataset of facial expressionsfrom 1,059 unique participants. This dataset includes 256 individuals with PD,165 clinically diagnosed, and 91 self-reported. Participants used webcams torecord themselves mimicking three facial expressions (smile, disgust, andsurprise) from diverse sources encompassing their homes across multiplecountries, a US clinic, and a PD wellness center in the US. Facial landmarksare automatically tracked from the recordings to extract features related tohypomimia, a prominent PD symptom characterized by reduced facial expressions.Machine learning algorithms are trained on these features to distinguishbetween individuals with and without PD. The model was tested forgeneralizability on external (unseen during training) test videos collectedfrom a US clinic and Bangladesh. An ensemble of machine learning models trainedon smile videos achieved an accuracy of 87.9+-0.1% (95% Confidence Interval)with an AUROC of 89.3+-0.3% as evaluated on held-out data (using k-foldcross-validation). In external test settings, the ensemble model achieved79.8+-0.6% accuracy with 81.9+-0.3% AUROC on the clinical test set and84.9+-0.4% accuracy with 81.2+-0.6% AUROC on participants from Bangladesh. Inevery setting, the model was free from detectable bias across sex and ethnicsubgroups, except in the cohorts from Bangladesh, where the model performedsignificantly better for female participants than males. Smiling videos caneffectively differentiate between individuals with and without PD, offering apotentially easy, accessible, and cost-efficient way to screen for PD,especially when a clinical diagnosis is difficult to access.</description><author>Tariq Adnan, Md Saiful Islam, Wasifur Rahman, Sangwu Lee, Sutapa Dey Tithi, Kazi Noshin, Imran Sarker, M Saifur Rahman, Ehsan Hoque</author><pubDate>Mon, 18 Nov 2024 16:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02588v2</guid></item><item><title>Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via Skill Library and Tactile Representation</title><link>http://arxiv.org/abs/2411.11714v1</link><description>Deploying robots in open-world environments involves complex taskscharacterized by long sequences and rich interactions, necessitating efficienttransfer of robotic skills across diverse and complex scenarios. To addressthis challenge, we propose a skill library framework based on knowledge graphs,which endows robots with high-level skill awareness and spatial semanticunderstanding. The framework hierarchically organizes operational knowledge byconstructing a "task graph" and a "scene graph" to represent task and scenesemantic information, respectively. We introduce a "state graph" to facilitateinteraction between high-level task planning and low-level scene information.Furthermore, we propose a hierarchical transfer framework for operationalskills. At the task level, the framework integrates contextual learning andchain-of-thought prompting within a four-stage prompt paradigm, leveraginglarge language models' (LLMs) reasoning and generalization capabilities toachieve task-level subtask sequence transfer. At the motion level, an adaptivetrajectory transfer method is developed using the A* algorithm and the skilllibrary, enabling motion-level adaptive trajectory transfer. At the physicallevel, we introduce an adaptive contour extraction and posture perceptionmethod based on tactile perception. This method dynamically obtainshigh-precision contour and posture information from visual-tactile texture dataand adjusts transferred skills, such as contact positions and postures, toensure effectiveness in new environments. Experimental results validate theeffectiveness of the proposed methods. Projectwebsite:https://github.com/MingchaoQi/skill_transfer</description><author>Mingchao Qi, Yuanjin Li, Xing Liu, Zhengxiong Liu, Panfeng Huang</author><pubDate>Mon, 18 Nov 2024 16:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11714v1</guid></item><item><title>FLMarket: Enabling Privacy-preserved Pre-training Data Pricing for Federated Learning</title><link>http://arxiv.org/abs/2411.11713v1</link><description>Federated Learning (FL), as a mainstream privacy-preserving machine learningparadigm, offers promising solutions for privacy-critical domains such ashealthcare and finance. Although extensive efforts have been dedicated fromboth academia and industry to improve the vanilla FL, little work focuses onthe data pricing mechanism. In contrast to the straightforward in/post-trainingpricing techniques, we study a more difficult problem of pre-training pricingwithout direct information from the learning process. We propose FLMarket thatintegrates a two-stage, auction-based pricing mechanism with a securityprotocol to address the utility-privacy conflict. Through comprehensiveexperiments, we show that the client selection according to FLMarket canachieve more than 10% higher accuracy in subsequent FL training compared tostate-of-the-art methods. In addition, it outperforms the in-training baselinewith more than 2% accuracy increase and 3x run-time speedup.</description><author>Zhenyu Wen, Wanglei Feng, Di Wu, Haozhen Hu, Chang Xu, Bin Qian, Zhen Hong, Cong Wang, Shouling Ji</author><pubDate>Mon, 18 Nov 2024 16:37:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11713v1</guid></item><item><title>FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large and Small Language Models</title><link>http://arxiv.org/abs/2411.11707v1</link><description>By adapting Large Language Models (LLMs) to domain-specific tasks orenriching them with domain-specific knowledge, we can fully harness thecapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneousmutual enhancement between the server's LLM and the downstream clients' SmallLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel andparameter-efficient federated framework designed for co-tuning LLMs and SLMs.This approach is aimed at adaptively transferring server-side LLMs knowledge toclients' SLMs while simultaneously enriching the LLMs with domain insights fromthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters inconjunction with SLMs, facilitating knowledge exchange between server andclients in a manner that respects data privacy while also minimizingcomputational and communication overhead. Our evaluation of FedCoLLM, utilizingvarious public LLMs and SLMs across a range of NLP text generation tasks,reveals that the performance of clients' SLMs experiences notable improvementswith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLMachieves comparable performance to that obtained through direct fine-tuning onclients' data.</description><author>Tao Fan, Yan Kang, Guoqiang Ma, Lixin Fan, Kai Chen, Qiang Yang</author><pubDate>Mon, 18 Nov 2024 16:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11707v1</guid></item><item><title>MC-LLaVA: Multi-Concept Personalized Vision-Language Model</title><link>http://arxiv.org/abs/2411.11706v1</link><description>Current vision-language models (VLMs) show exceptional abilities acrossdiverse tasks including visual question answering. To enhance user experiencein practical applications, recent studies investigate VLM personalization tounderstand user-provided concepts. However, existing studies mainly focus onsingle-concept personalization, neglecting the existence and interplay ofmultiple concepts, which limits the real-world applicability of personalizedVLMs. In this paper, we propose the first multi-concept personalization methodnamed MC-LLaVA along with a high-quality multi-concept personalization dataset.Specifically, MC-LLaVA uses a joint training strategy incorporating multipleconcepts in a single training step, allowing VLMs to perform accurately inmulti-concept personalization. To reduce the cost of joint training, MC-LLaVAleverages visual token information for concept token initialization, yieldingimproved concept representation and accelerating joint training. To advancemulti-concept personalization research, we further contribute a high-qualitydataset. We carefully collect images from various movies that contain multiplecharacters and manually generate the multi-concept question-answer samples. Ourdataset features diverse movie types and question-answer types. We conductcomprehensive qualitative and quantitative experiments to demonstrate thatMC-LLaVA can achieve impressive multi-concept personalized responses, pavingthe way for VLMs to become better user-specific assistants. The code anddataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA.</description><author>Ruichuan An, Sihan Yang, Ming Lu, Kai Zeng, Yulin Luo, Ying Chen, Jiajun Cao, Hao Liang, Qi She, Shanghang Zhang, Wentao Zhang</author><pubDate>Mon, 18 Nov 2024 16:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11706v1</guid></item><item><title>DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection</title><link>http://arxiv.org/abs/2410.11181v2</link><description>At a cocktail party, humans exhibit an impressive ability to direct theirattention. The auditory attention detection (AAD) approach seeks to identifythe attended speaker by analyzing brain signals, such as EEG signals. However,current AAD algorithms overlook the spatial distribution information within EEGsignals and lack the ability to capture long-range latent dependencies,limiting the model's ability to decode brain activity. To address these issues,this paper proposes a dual attention refinement network with spatiotemporalconstruction for AAD, named DARNet, which consists of the spatiotemporalconstruction module, dual attention refinement module, and feature fusion \&amp;classifier module. Specifically, the spatiotemporal construction module aims toconstruct more expressive spatiotemporal feature representations, by capturingthe spatial distribution characteristics of EEG signals. The dual attentionrefinement module aims to extract different levels of temporal patterns in EEGsignals and enhance the model's ability to capture long-range latentdependencies. The feature fusion \&amp; classifier module aims to aggregatetemporal patterns and dependencies from different levels and obtain the finalclassification results. The experimental results indicate that compared to thestate-of-the-art models, DARNet achieves an average classification accuracyimprovement of 5.9\% for 0.1s, 4.6\% for 1s, and 3.9\% for 2s on the DTUdataset. While maintaining excellent classification performance, DARNetsignificantly reduces the number of required parameters. Compared to thestate-of-the-art models, DARNet reduces the parameter count by 91\%. Code isavailable at: https://github.com/fchest/DARNet.git.</description><author>Sheng Yan, Cunhang fan, Hongyu Zhang, Xiaoke Yang, Jianhua Tao, Zhao Lv</author><pubDate>Mon, 18 Nov 2024 16:25:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11181v2</guid></item><item><title>Partial Information Decomposition for Data Interpretability and Feature Selection</title><link>http://arxiv.org/abs/2405.19212v3</link><description>In this paper, we introduce Partial Information Decomposition of Features(PIDF), a new paradigm for simultaneous data interpretability and featureselection. Contrary to traditional methods that assign a single importancevalue, our approach is based on three metrics per feature: the mutualinformation shared with the target variable, the feature's contribution tosynergistic information, and the amount of this information that is redundant.In particular, we develop a novel procedure based on these three metrics, whichreveals not only how features are correlated with the target but also theadditional and overlapping information provided by considering them incombination with other features. We extensively evaluate PIDF using bothsynthetic and real-world data, demonstrating its potential applications andeffectiveness, by considering case studies from genetics and neuroscience.</description><author>Charles Westphal, Stephen Hailes, Mirco Musolesi</author><pubDate>Mon, 18 Nov 2024 16:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19212v3</guid></item><item><title>Robust Reinforcement Learning under Diffusion Models for Data with Jumps</title><link>http://arxiv.org/abs/2411.11697v1</link><description>Reinforcement Learning (RL) has proven effective in solving complexdecision-making tasks across various domains, but challenges remain incontinuous-time settings, particularly when state dynamics are governed bystochastic differential equations (SDEs) with jump components. In this paper,we address this challenge by introducing the Mean-Square Bipower VariationError (MSBVE) algorithm, which enhances robustness and convergence in scenariosinvolving significant stochastic noise and jumps. We first revisit theMean-Square TD Error (MSTDE) algorithm, commonly used in continuous-time RL,and highlight its limitations in handling jumps in state dynamics. The proposedMSBVE algorithm minimizes the mean-square quadratic variation error, offeringimproved performance over MSTDE in environments characterized by SDEs withjumps. Simulations and formal proofs demonstrate that the MSBVE algorithmreliably estimates the value function in complex settings, surpassing MSTDE'sperformance when faced with jump processes. These findings underscore theimportance of alternative error metrics to improve the resilience andeffectiveness of RL algorithms in continuous-time frameworks.</description><author>Chenyang Jiang, Donggyu Kim, Alejandra Quintos, Yazhen Wang</author><pubDate>Mon, 18 Nov 2024 16:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11697v1</guid></item><item><title>Technical Report: Enhancing LLM Reasoning with Reward-guided Tree Search</title><link>http://arxiv.org/abs/2411.11694v1</link><description>Recently, test-time scaling has garnered significant attention from theresearch community, largely due to the substantial advancements of the o1 modelreleased by OpenAI. By allocating more computational resources during theinference phase, large language models~(LLMs) can extensively explore thesolution space by generating more thought tokens or diverse solutions, therebyproducing more accurate responses. However, developing an o1-like reasoningapproach is challenging, and researchers have been making various attempts toadvance this open area of research. In this paper, we present a preliminaryexploration into enhancing the reasoning abilities of LLMs throughreward-guided tree search algorithms. This framework is implemented byintegrating the policy model, reward model, and search algorithm. It isprimarily constructed around a tree search algorithm, where the policy modelnavigates a dynamically expanding tree guided by a specially trained rewardmodel. We thoroughly explore various design considerations necessary forimplementing this framework and provide a detailed report of the technicalaspects. To assess the effectiveness of our approach, we focus on mathematicalreasoning tasks and conduct extensive evaluations on four challenging datasets,significantly enhancing the reasoning abilities of LLMs.</description><author>Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, Zheng Liu, Dong Yan, Jian Xie, Zhongyuan Wang, Ji-Rong Wen</author><pubDate>Mon, 18 Nov 2024 16:15:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11694v1</guid></item><item><title>From Spectra to Geography: Intelligent Mapping of RRUFF Mineral Data</title><link>http://arxiv.org/abs/2411.11693v1</link><description>Accurately determining the geographic origin of mineral samples is pivotalfor applications in geology, mineralogy, and material science. Leveraging thecomprehensive Raman spectral data from the RRUFF database, this studyintroduces a novel machine learning framework aimed at geolocating mineralspecimens at the country level. We employ a one-dimensional ConvNeXt1D neuralnetwork architecture to classify mineral spectra based solely on their spectralsignatures. The processed dataset comprises over 32,900 mineral samples,predominantly natural, spanning 101 countries. Through five-foldcross-validation, the ConvNeXt1D model achieved an impressive averageclassification accuracy of 93%, demonstrating its efficacy in capturinggeospatial patterns inherent in Raman spectra.</description><author>Francesco Pappone, Federico Califano, Marco Tafani</author><pubDate>Mon, 18 Nov 2024 16:15:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11693v1</guid></item><item><title>Towards Degradation-Robust Reconstruction in Generalizable NeRF</title><link>http://arxiv.org/abs/2411.11691v1</link><description>Generalizable Neural Radiance Field (GNeRF) across scenes has been proven tobe an effective way to avoid per-scene optimization by representing a scenewith deep image features of source images. However, despite its potential forreal-world applications, there has been limited research on the robustness ofGNeRFs to different types of degradation present in the source images. The lackof such research is primarily attributed to the absence of a large-scaledataset fit for training a degradation-robust generalizable NeRF model. Toaddress this gap and facilitate investigations into the degradation robustnessof 3D reconstruction tasks, we construct the Objaverse Blur Dataset, comprising50,000 images from over 1000 settings featuring multiple levels of blurdegradation. In addition, we design a simple and model-agnostic module forenhancing the degradation robustness of GNeRFs. Specifically, by extracting3D-aware features through a lightweight depth estimator and denoiser, theproposed module shows improvement on different popular methods in GNeRFs interms of both quantitative and visual quality over varying degradation typesand levels. Our dataset and code will be made publicly available.</description><author>Chan Ho Park, Ka Leong Cheng, Zhicheng Wang, Qifeng Chen</author><pubDate>Mon, 18 Nov 2024 16:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11691v1</guid></item><item><title>Conceptwm: A Diffusion Model Watermark for Concept Protection</title><link>http://arxiv.org/abs/2411.11688v1</link><description>The personalization techniques of diffusion models succeed in generatingspecific concepts but also pose threats to copyright protection and illegaluse. Model Watermarking is an effective method to prevent the unauthorized useof subject-driven or style-driven image generation, safeguarding conceptcopyrights. However, under the goal of concept-oriented protection, currentwatermarking schemes typically add watermarks to all images rather thanapplying them in a refined manner targeted at specific concepts. Additionally,the personalization techniques of diffusion models can easily removewatermarks. Existing watermarking methods struggle to achieve fine-grainedwatermark embedding with a few images of specific concept and prevent removalof watermarks through personalized fine-tuning. Therefore, we introduce a novelconcept-oriented watermarking framework that seamlessly embeds imperceptiblewatermarks into the concept of diffusion models. We conduct extensiveexperiments and ablation studies to verify our framework. Our code is availableat https://anonymous.4open.science/r/Conceptwm-4EB3/.</description><author>Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu</author><pubDate>Mon, 18 Nov 2024 16:11:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11688v1</guid></item><item><title>TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the Physical World</title><link>http://arxiv.org/abs/2411.11683v1</link><description>Robotic manipulation refers to the autonomous handling and interaction ofrobots with objects using advanced techniques in robotics and artificialintelligence. The advent of powerful tools such as large language models (LLMs)and large vision-language models (LVLMs) has significantly enhanced thecapabilities of these robots in environmental perception and decision-making.However, the introduction of these intelligent agents has led to securitythreats such as jailbreak attacks and adversarial attacks. In this research, we take a further step by proposing a backdoor attackspecifically targeting robotic manipulation and, for the first time,implementing backdoor attack in the physical world. By embedding a backdoorvisual language model into the visual perception module within the roboticsystem, we successfully mislead the robotic arm's operation in the physicalworld, given the presence of common items as triggers. Experimental evaluationsin the physical world demonstrate the effectiveness of the proposed backdoorattack.</description><author>Xianlong Wang, Hewen Pan, Hangtao Zhang, Minghui Li, Shengshan Hu, Ziqi Zhou, Lulu Xue, Peijin Guo, Yichen Wang, Wei Wan, Aishan Liu, Leo Yu Zhang</author><pubDate>Mon, 18 Nov 2024 16:09:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11683v1</guid></item><item><title>Learning Differentiable Surrogate Losses for Structured Prediction</title><link>http://arxiv.org/abs/2411.11682v1</link><description>Structured prediction involves learning to predict complex structures ratherthan simple scalar values. The main challenge arises from the non-Euclideannature of the output space, which generally requires relaxing the problemformulation. Surrogate methods build on kernel-induced losses or moregenerally, loss functions admitting an Implicit Loss Embedding, and convert theoriginal problem into a regression task followed by a decoding step. However,designing effective losses for objects with complex structures presentssignificant challenges and often requires domain-specific expertise. In thiswork, we introduce a novel framework in which a structured loss function,parameterized by neural networks, is learned directly from output training datathrough Contrastive Learning, prior to addressing the supervised surrogateregression problem. As a result, the differentiable loss not only enables thelearning of neural networks due to the finite dimension of the surrogate spacebut also allows for the prediction of new structures of the output data via adecoding strategy based on gradient descent. Numerical experiments onsupervised graph prediction problems show that our approach achieves similar oreven better performance than methods based on a pre-defined kernel.</description><author>Junjie Yang, Matthieu Labeau, Florence d'Alché-Buc</author><pubDate>Mon, 18 Nov 2024 16:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11682v1</guid></item><item><title>PSPO*: An Effective Process-supervised Policy Optimization for Reasoning Alignment</title><link>http://arxiv.org/abs/2411.11681v1</link><description>Process supervision enhances the performance of large language models inreasoning tasks by providing feedback at each step of chain-of-thoughtreasoning. However, due to the lack of effective process supervision methods,even advanced large language models are prone to logical errors and redundantreasoning. We claim that the effectiveness of process supervision significantlydepends on both the accuracy and the length of reasoning chains. Moreover, weidentify that these factors exhibit a nonlinear relationship with the overallreward score of the reasoning process. Inspired by these insights, we propose anovel process supervision paradigm, PSPO*, which systematically outlines theworkflow from reward model training to policy optimization, and highlights theimportance of nonlinear rewards in process supervision. Based on PSPO*, wedevelop the PSPO-WRS, which considers the number of reasoning steps indetermining reward scores and utilizes an adjusted Weibull distribution fornonlinear reward shaping. Experimental results on six mathematical reasoningdatasets demonstrate that PSPO-WRS consistently outperforms current mainstreammodels.</description><author>Jiawei Li, Xinyue Liang, Yizhe Yang, Chong Feng, Yang Gao</author><pubDate>Mon, 18 Nov 2024 16:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11681v1</guid></item><item><title>Hierarchical Structure Enhances the Convergence and Generalizability of Linear Molecular Representation</title><link>http://arxiv.org/abs/2402.02164v4</link><description>Language models demonstrate fundamental abilities in syntax, semantics, andreasoning, though their performance often depends significantly on the inputsthey process. This study introduces TSIS (Simplified TSID) and itsvariants:TSISD (TSIS with Depth-First Search), TSISO (TSIS in Order), and TSISR(TSIS in Random), as integral components of the t-SMILES framework. Theseadditions complete the framework's design, providing diverse approaches tomolecular representation. Through comprehensive analysis and experimentsemploying deep generative models, including GPT, diffusion models, andreinforcement learning, the findings reveal that the hierarchical structure oft-SMILES is more straightforward to parse than initially anticipated.Furthermore, t-SMILES consistently outperforms other linear representationssuch as SMILES, SELFIES, and SAFE, demonstrating superior convergence speed andenhanced generalization capabilities.</description><author>Juan-Ni Wu, Tong Wang, Li-Juan Tang, Hai-Long Wu, Ru-Qin Yu</author><pubDate>Mon, 18 Nov 2024 16:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02164v4</guid></item><item><title>Analysis of Hardware Synthesis Strategies for Machine Learning in Collider Trigger and Data Acquisition</title><link>http://arxiv.org/abs/2411.11678v1</link><description>To fully exploit the physics potential of current and future high energyparticle colliders, machine learning (ML) can be implemented in detectorelectronics for intelligent data processing and acquisition. The implementationof ML in real-time at colliders requires very low latencies that areunachievable with a software-based approach, requiring optimization andsynthesis of ML algorithms for deployment on hardware. An analysis of neuralnetwork inference efficiency is presented, focusing on the application ofcollider trigger algorithms in field programmable gate arrays (FPGAs).Trade-offs are evaluated between two frameworks, the SLAC Neural NetworkLibrary (SNL) and hls4ml, in terms of resources and latency for different modelsizes. Results highlight the strengths and limitations of each approach,offering valuable insights for optimizing real-time neural network deploymentsat colliders. This work aims to guide researchers and engineers in selectingthe most suitable hardware and software configurations for real-time,resource-constrained environments.</description><author>Haoyi Jia, Abhilasha Dave, Julia Gonski, Ryan Herbst</author><pubDate>Mon, 18 Nov 2024 15:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11678v1</guid></item><item><title>Few-shot Model Extraction Attacks against Sequential Recommender Systems</title><link>http://arxiv.org/abs/2411.11677v1</link><description>Among adversarial attacks against sequential recommender systems, modelextraction attacks represent a method to attack sequential recommendationmodels without prior knowledge. Existing research has primarily concentrated onthe adversary's execution of black-box attacks through data-free modelextraction. However, a significant gap remains in the literature concerning thedevelopment of surrogate models by adversaries with access to few-shot raw data(10\% even less). That is, the challenge of how to construct a surrogate modelwith high functional similarity within the context of few-shot data scenariosremains an issue that requires resolution.This study addresses this gap byintroducing a novel few-shot model extraction framework against sequentialrecommenders, which is designed to construct a superior surrogate model withthe utilization of few-shot data. The proposed few-shot model extractionframework is comprised of two components: an autoregressive augmentationgeneration strategy and a bidirectional repair loss-facilitated modeldistillation procedure. Specifically, to generate synthetic data that closelyapproximate the distribution of raw data, autoregressive augmentationgeneration strategy integrates a probabilistic interaction sampler to extractinherent dependencies and a synthesis determinant signal module to characterizeuser behavioral patterns. Subsequently, bidirectional repair loss, which targetthe discrepancies between the recommendation lists, is designed as auxiliaryloss to rectify erroneous predictions from surrogate models, transferringknowledge from the victim model to the surrogate model effectively. Experimentson three datasets show that the proposed few-shot model extraction frameworkyields superior surrogate models.</description><author>Hui Zhang, Fu Liu</author><pubDate>Mon, 18 Nov 2024 15:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11677v1</guid></item><item><title>Artificial Scientific Discovery</title><link>http://arxiv.org/abs/2411.11672v1</link><description>Rooted in the explosion of deep learning over the past decade, this thesisspans from AlphaGo to ChatGPT to empirically examine the fundamental conceptsneeded to realize the vision of an artificial scientist: a machine with thecapacity to autonomously generate original research and contribute to theexpansion of human knowledge. The investigation begins with {\sc Olivaw}, anAlphaGo Zero-like agent that discovers Othello knowledge from scratch but isunable to communicate it. This realization leads to the development of theExplanatory Learning (EL) framework, a formalization of the problem faced by ascientist when trying to explain a new phenomenon to their peers. The effectiveEL prescriptions allow us to crack Zendo, a board game simulating thescientific endeavor. This success comes with a fundamental insight: anartificial scientist must develop its own interpretation of the language usedto explain its findings. This perspective then leads us to see modernmultimodal models as interpreters, and to devise a new way to buildinterpretable and cost-effective CLIP-like models: by coupling two unimodalmodels using little multimodal data and no further training. Finally, wediscuss what ChatGPT and its siblings are still missing to become artificialscientists, and introduce Odeen, a benchmark about interpreting explanationsthat sees LLMs going no further than random chance while being instead fullysolved by humans.</description><author>Antonio Norelli</author><pubDate>Mon, 18 Nov 2024 15:51:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11672v1</guid></item><item><title>Efficient and Robust Continual Graph Learning for Graph Classification in Biology</title><link>http://arxiv.org/abs/2411.11668v1</link><description>Graph classification is essential for understanding complex biologicalsystems, where molecular structures and interactions are naturally representedas graphs. Traditional graph neural networks (GNNs) perform well on statictasks but struggle in dynamic settings due to catastrophic forgetting. Wepresent Perturbed and Sparsified Continual Graph Learning (PSCGL), a robust andefficient continual graph learning framework for graph data classification,specifically targeting biological datasets. We introduce a perturbed samplingstrategy to identify critical data points that contribute to model learning anda motif-based graph sparsification technique to reduce storage needs whilemaintaining performance. Additionally, our PSCGL framework inherently defendsagainst graph backdoor attacks, which is crucial for applications in sensitivebiological contexts. Extensive experiments on biological datasets demonstratethat PSCGL not only retains knowledge across tasks but also enhances theefficiency and robustness of graph classification models in biology.</description><author>Ding Zhang, Jane Downer, Can Chen, Ren Wang</author><pubDate>Mon, 18 Nov 2024 15:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11668v1</guid></item><item><title>Dissecting Misalignment of Multimodal Large Language Models via Influence Function</title><link>http://arxiv.org/abs/2411.11667v1</link><description>Multi-modal Large Language models (MLLMs) are always trained on data fromdiverse and unreliable sources, which may contain misaligned or mislabeledtext-image pairs. This frequently causes robustness issues and hallucinations,leading to performance degradation. Data valuation is an efficient way todetect and trace these misalignments. Nevertheless, existing methods arecomputationally expensive for MLLMs. While computationally efficient, theclassical influence functions are inadequate for contrastive learning modelsbecause they were originally designed for pointwise loss. Additionally,contrastive learning involves minimizing the distance between the modalities ofpositive samples and maximizing the distance between the modalities of negativesamples. This requires us to evaluate the influence of samples from bothperspectives. To tackle these challenges, we introduce the Extended InfluenceFunction for Contrastive Loss (ECIF), an influence function crafted forcontrastive loss. ECIF considers both positive and negative samples andprovides a closed-form approximation of contrastive learning models,eliminating the need for retraining. Building upon ECIF, we develop a series ofalgorithms for data evaluation in MLLM, misalignment detection, andmisprediction trace-back tasks. Experimental results demonstrate our ECIFadvances the transparency and interpretability of MLLMs by offering a moreaccurate assessment of data impact and model alignment compared to traditionalbaseline methods.</description><author>Lijie Hu, Chenyang Ren, Huanyi Xie, Khouloud Saadi, Shu Yang, Jingfeng Zhang, Di Wang</author><pubDate>Mon, 18 Nov 2024 15:45:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11667v1</guid></item><item><title>SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model</title><link>http://arxiv.org/abs/2402.18068v3</link><description>In the rapidly evolving area of image synthesis, a serious challenge is thepresence of complex artifacts that compromise perceptual realism of syntheticimages. To alleviate artifacts and improve quality of synthetic images, wefine-tune Vision-Language Model (VLM) as artifact classifier to automaticallyidentify and classify a wide range of artifacts and provide supervision forfurther optimizing generative models. Specifically, we develop a comprehensiveartifact taxonomy and construct a dataset of synthetic images with artifactannotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLMexhibits superior ability of identifying artifacts and outperforms the baselineby 25.66%. To our knowledge, this is the first time such end-to-end artifactclassification task and solution have been proposed. Finally, we leverage theoutput of VLM as feedback to refine the generative model for alleviatingartifacts. Visualization results and user study demonstrate that the quality ofimages synthesized by the refined diffusion model has been obviously improved.</description><author>Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao</author><pubDate>Mon, 18 Nov 2024 15:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18068v3</guid></item><item><title>Modulating Language Model Experiences through Frictions</title><link>http://arxiv.org/abs/2407.12804v2</link><description>Language models are transforming the ways that their users engage with theworld. Despite impressive capabilities, over-consumption of language modeloutputs risks propagating unchecked errors in the short-term and damaging humancapabilities for critical thinking in the long-term. How can we developscaffolding around language models to curate more appropriate use? We proposeselective frictions for language model experiences, inspired by behavioralscience interventions, to dampen misuse. Frictions involve small modificationsto a user's experience, e.g., the addition of a button impeding model accessand reminding a user of their expertise relative to the model. Through a userstudy with real humans, we observe shifts in user behavior from the impositionof a friction over LLMs in the context of a multi-topic question-answering taskas a representative task that people may use LLMs for, e.g., in education andinformation retrieval. We find that frictions modulate over-reliance by drivingdown users' click rates while minimally affecting accuracy for those topics.Yet, frictions may have unintended effects. We find marked differences inusers' click behaviors even on topics where frictions were not provisioned. Ourcontributions motivate further study of human-AI behavioral interaction toinform more effective and appropriate LLM use.</description><author>Katherine M. Collins, Valerie Chen, Ilia Sucholutsky, Hannah Rose Kirk, Malak Sadek, Holli Sargeant, Ameet Talwalkar, Adrian Weller, Umang Bhatt</author><pubDate>Mon, 18 Nov 2024 15:41:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12804v2</guid></item><item><title>Utilizing Large Language Models in an iterative paradigm with domain feedback for molecule optimization</title><link>http://arxiv.org/abs/2410.13147v6</link><description>Molecule optimization is a critical task in drug discovery to optimizedesired properties of a given molecule through chemical modification. DespiteLarge Language Models (LLMs) holding the potential to efficiently simulate thistask by using natural language to direct the optimization, straightforwardlyutilizing them shows limited performance. In this work, we facilitate utilizingLLMs in an iterative paradigm by proposing a simple yet highly effective domainfeedback provider, namely $\text{Re}^3$DF. In detail, $\text{Re}^3$DF harnessesan external toolkit, RDKit, to handle the molecule hallucination, if themodified molecule is chemically invalid. Otherwise, its desired properties arecomputed and compared to the original one, establishing reliable domainfeedback with correct direction and distance towards the objective, followed bya retrieved example, to guide the LLM to refine the modified molecule. Weconduct experiments across both single- and multi-property objectives with 2thresholds, where $\text{Re}^3$DF shows significant improvements. Particularly,for 20 single-property objectives, $\text{Re}^3$DF enhances Hit ratio by 16.95%and 20.76% under loose (\texttt{l}) and strict (\texttt{s}) thresholds,respectively. For 32 multi-property objectives, $\text{Re}^3$DF enhances Hitratio by 6.04% and 5.25%.</description><author>Khiem Le, Nitesh V. Chawla</author><pubDate>Mon, 18 Nov 2024 15:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13147v6</guid></item><item><title>MagicStick: Controllable Video Editing via Control Handle Transformations</title><link>http://arxiv.org/abs/2312.03047v2</link><description>Text-based video editing has recently attracted considerable interest inchanging the style or replacing the objects with a similar structure. Beyondthis, we demonstrate that properties such as shape, size, location, motion,etc., can also be edited in videos. Our key insight is that the keyframetransformations of the specific internal feature (e.g., edge maps of objects orhuman pose), can easily propagate to other frames to provide generationguidance. We thus propose MagicStick, a controllable video editing method thatedits the video properties by utilizing the transformation on the extractedinternal control signals. In detail, to keep the appearance, we inflate boththe pretrained image diffusion model and ControlNet to the temporal dimensionand train low-rank adaptions (LORA) layers to fit the specific scenes. Then, inediting, we perform an inversion and editing framework. Differently, finetunedControlNet is introduced in both inversion and generation for attentionguidance with the proposed attention remix between the spatial attention mapsof inversion and editing. Yet succinct, our method is the first method to showthe ability of video property editing from the pre-trained text-to-image model.We present experiments on numerous examples within our unified framework. Wealso compare with shape-aware text-based editing and handcrafted motion videogeneration, demonstrating our superior temporal consistency and editingcapability than previous works. The code and models are available onhttps://github.com/mayuelala/MagicStick.</description><author>Yue Ma, Xiaodong Cun, Sen Liang, Jinbo Xing, Yingqing He, Chenyang Qi, Siran Chen, Qifeng Chen</author><pubDate>Mon, 18 Nov 2024 15:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03047v2</guid></item><item><title>Straightness of Rectified Flow: A Theoretical Insight into Wasserstein Convergence</title><link>http://arxiv.org/abs/2410.14949v2</link><description>Diffusion models have emerged as a powerful tool for image generation anddenoising. Typically, generative models learn a trajectory between the startingnoise distribution and the target data distribution. Recently Liu et al.(2023b) designed a novel alternative generative model Rectified Flow (RF),which aims to learn straight flow trajectories from noise to data using asequence of convex optimization problems with close ties to optimal transport.If the trajectory is curved, one must use many Euler discretization steps ornovel strategies, such as exponential integrators, to achieve a satisfactorygeneration quality. In contrast, RF has been shown to theoretically straightenthe trajectory through successive rectifications, reducing the number offunction evaluations (NFEs) while sampling. It has also been shown empiricallythat RF may improve the straightness in two rectifications if one can solve theunderlying optimization problem within a sufficiently small error. In thispaper, we make two key theoretical contributions: 1) we provide the firsttheoretical analysis of the Wasserstein distance between the samplingdistribution of RF and the target distribution. Our error rate is characterizedby the number of discretization steps and a new formulation of straightnessstronger than that in the original work. 2) under a mild regularity assumption,we show that for a rectified flow from a Gaussian to any general targetdistribution with finite first moment (e.g. mixture of Gaussians), tworectifications are sufficient to achieve a straight flow, which is in line withthe previous empirical findings. Additionally, we also present empiricalresults on both simulated and real datasets to validate our theoreticalfindings.</description><author>Vansh Bansal, Saptarshi Roy, Purnamrita Sarkar, Alessandro Rinaldo</author><pubDate>Mon, 18 Nov 2024 15:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14949v2</guid></item><item><title>Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction</title><link>http://arxiv.org/abs/2402.04154v7</link><description>Developing a generalist agent is a longstanding objective in artificialintelligence. Previous efforts utilizing extensive offline datasets fromvarious tasks demonstrate remarkable performance in multitasking scenarioswithin Reinforcement Learning. However, these works encounter challenges inextending their capabilities to new tasks. Recent approaches integrate textualguidance or visual trajectory into decision networks to provide task-specificcontextual cues, representing a promising direction. However, it is observedthat relying solely on textual guidance or visual trajectory is insufficientfor accurately conveying the contextual information of tasks. This paperexplores enhanced forms of task guidance for agents, enabling them tocomprehend gameplay instructions, thereby facilitating a "read-to-play"capability. Drawing inspiration from the success of multimodal instructiontuning in visual tasks, we treat the visual-based RL task as a long-horizonvision task and construct a set of multimodal game instructions to incorporateinstruction tuning into a decision transformer. Experimental resultsdemonstrate that incorporating multimodal game instructions significantlyenhances the decision transformer's multitasking and generalizationcapabilities.</description><author>Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jarvi Guo, Liuyu Xiang, Shawn Yue, Stephen W. Huang, Zhaofeng He, Jie Fu</author><pubDate>Mon, 18 Nov 2024 15:31:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04154v7</guid></item><item><title>Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Linear Regression</title><link>http://arxiv.org/abs/2403.13565v2</link><description>We consider the transfer learning problem in the high dimensional linearregression setting, where the feature dimension is larger than the sample size.To learn transferable information, which may vary across features or the sourcesamples, we propose an adaptive transfer learning method that can detect andaggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans)transferable structures. We achieve this by employing a fused-penalty, coupledwith weights that can adapt according to the transferable structure. To choosethe weight, we propose a theoretically informed, data-driven procedure,enabling F-AdaTrans to selectively fuse the transferable signals with thetarget while filtering out non-transferable signals, and S-AdaTrans to obtainthe optimal combination of information transferred from each source sample. Weshow that, with appropriately chosen weights, F-AdaTrans achieves a convergencerate close to that of an oracle estimator with a known transferable structure,and S-AdaTrans recovers existing near-minimax optimal rates as a special case.The effectiveness of the proposed method is validated using both simulation andreal data, demonstrating favorable performance compared to the existingmethods.</description><author>Zelin He, Ying Sun, Jingyuan Liu, Runze Li</author><pubDate>Mon, 18 Nov 2024 15:30:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13565v2</guid></item><item><title>Parsing altered brain connectivity in neurodevelopmental disorders by integrating graph-based normative modeling and deep generative networks</title><link>http://arxiv.org/abs/2410.11064v2</link><description>Divergent brain connectivity is thought to underlie the behavioral andcognitive symptoms observed in many neurodevelopmental disorders. Quantifyingdivergence from neurotypical connectivity patterns offers a promising pathwayto inform diagnosis and therapeutic interventions. While advanced neuroimagingtechniques, such as diffusion MRI (dMRI), have facilitated the mapping ofbrain's structural connectome, the challenge lies in accurately modelingdevelopmental trajectories within these complex networked structures to createrobust neurodivergence markers. In this work, we present the BrainRepresentation via Individualized Deep Generative Embedding (BRIDGE) framework,which integrates normative modeling with a bio-inspired deep generative modelto create a reference trajectory of connectivity transformation as part ofneurotypical development. This will enable the assessment of neurodivergence bycomparing individuals to the established neurotypical trajectory. BRIDGEprovides a global neurodivergence score based on the difference betweenconnectivity-based brain age and chronological age, along with region-wiseneurodivergence maps that highlight localized connectivity differences.Application of BRIDGE to a large cohort of children with autism spectrumdisorder demonstrates that the global neurodivergence score correlates withclinical assessments in autism, and the regional map offers insights into theheterogeneity at the individual level in neurodevelopmental disorders.Together, the neurodivergence score and map form powerful tools for quantifyingdevelopmental divergence in connectivity patterns, advancing the development ofimaging markers for personalized diagnosis and intervention in various clinicalcontexts.</description><author>Rui Sherry Shen, Yusuf Osmanlıoğlu, Drew Parker, Darien Aunapu, Benjamin E. Yerys, Birkan Tunç, Ragini Verma</author><pubDate>Mon, 18 Nov 2024 15:29:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11064v2</guid></item><item><title>No-regret Exploration in Shuffle Private Reinforcement Learning</title><link>http://arxiv.org/abs/2411.11647v1</link><description>Differential privacy (DP) has recently been introduced into episodicreinforcement learning (RL) to formally address user privacy concerns inpersonalized services. Previous work mainly focuses on two trust models of DP:the central model, where a central agent is responsible for protecting users'sensitive data, and the (stronger) local model, where the protection occursdirectly on the user side. However, they either require a trusted central agentor incur a significantly higher privacy cost, making it unsuitable for manyscenarios. This work introduces a trust model stronger than the central modelbut with a lower privacy cost than the local model, leveraging the emerging\emph{shuffle} model of privacy. We present the first generic algorithm forepisodic RL under the shuffle model, where a trusted shuffler randomly permutesa batch of users' data before sending it to the central agent. We theninstantiate the algorithm using our proposed shuffle Privatizer, relying on ashuffle private binary summation mechanism. Our analysis shows that thealgorithm achieves a near-optimal regret bound comparable to that of thecentralized model and significantly outperforms the local model in terms ofprivacy cost.</description><author>Shaojie Bai, Mohammad Sadegh Talebi, Chengcheng Zhao, Peng Cheng, Jiming Chen</author><pubDate>Mon, 18 Nov 2024 15:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11647v1</guid></item><item><title>Investigating OCR-Sensitive Neurons to Improve Entity Recognition in Historical Documents</title><link>http://arxiv.org/abs/2409.16934v3</link><description>This paper investigates the presence of OCR-sensitive neurons within theTransformer architecture and their influence on named entity recognition (NER)performance on historical documents. By analysing neuron activation patterns inresponse to clean and noisy text inputs, we identify and then neutraliseOCR-sensitive neurons to improve model performance. Based on two open accesslarge language models (Llama2 and Mistral), experiments demonstrate theexistence of OCR-sensitive regions and show improvements in NER performance onhistorical newspapers and classical commentaries, highlighting the potential oftargeted neuron modulation to improve models' performance on noisy text.</description><author>Emanuela Boros, Maud Ehrmann</author><pubDate>Mon, 18 Nov 2024 15:22:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16934v3</guid></item><item><title>Scalable spectral representations for multi-agent reinforcement learning in network MDPs</title><link>http://arxiv.org/abs/2410.17221v2</link><description>Network Markov Decision Processes (MDPs), a popular model for multi-agentcontrol, pose a significant challenge to efficient learning due to theexponential growth of the global state-action space with the number of agents.In this work, utilizing the exponential decay property of network dynamics, wefirst derive scalable spectral local representations for network MDPs, whichinduces a network linear subspace for the local $Q$-function of each agent.Building on these local spectral representations, we design a scalablealgorithmic framework for continuous state-action network MDPs, and provideend-to-end guarantees for the convergence of our algorithm. Empirically, wevalidate the effectiveness of our scalable representation-based approach on twobenchmark problems, and demonstrate the advantages of our approach over genericfunction approximation approaches to representing the local $Q$-functions.</description><author>Zhaolin Ren, Runyu Zhang, Bo Dai, Na Li</author><pubDate>Mon, 18 Nov 2024 15:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17221v2</guid></item><item><title>Thermodynamic Transferability in Coarse-Grained Force Fields using Graph Neural Networks</title><link>http://arxiv.org/abs/2406.12112v2</link><description>Coarse-graining is a molecular modeling technique in which an atomisticsystem is represented in a simplified fashion that retains the most significantsystem features that contribute to a target output, while removing the degreesof freedom that are less relevant. This reduction in model complexity allowscoarse-grained molecular simulations to reach increased spatial and temporalscales compared to corresponding all-atom models. A core challenge incoarse-graining is to construct a force field that represents the interactionsin the new representation in a way that preserves the atomistic-levelproperties. Many approaches to building coarse-grained force fields havelimited transferability between different thermodynamic conditions as a resultof averaging over internal fluctuations at a specific thermodynamic statepoint. Here, we use a graph-convolutional neural network architecture, theHierarchically Interacting Particle Neural Network with Tensor Sensitivity(HIP-NN-TS), to develop a highly automated training pipeline for coarse grainedforce fields which allows for studying the transferability of coarse-grainedmodels based on the force-matching approach. We show that this approach notonly yields highly accurate force fields, but also that these force fields aremore transferable through a variety of thermodynamic conditions. These resultsillustrate the potential of machine learning techniques such as graph neuralnetworks to improve the construction of transferable coarse-grained forcefields.</description><author>Emily Shinkle, Aleksandra Pachalieva, Riti Bahl, Sakib Matin, Brendan Gifford, Galen T. Craven, Nicholas Lubbers</author><pubDate>Mon, 18 Nov 2024 15:21:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12112v2</guid></item><item><title>TSINR: Capturing Temporal Continuity via Implicit Neural Representations for Time Series Anomaly Detection</title><link>http://arxiv.org/abs/2411.11641v1</link><description>Time series anomaly detection aims to identify unusual patterns in data ordeviations from systems' expected behavior. The reconstruction-based methodsare the mainstream in this task, which learn point-wise representation viaunsupervised learning. However, the unlabeled anomaly points in training datamay cause these reconstruction-based methods to learn and reconstruct anomalousdata, resulting in the challenge of capturing normal patterns. In this paper,we propose a time series anomaly detection method based on implicit neuralrepresentation (INR) reconstruction, named TSINR, to address this challenge.Due to the property of spectral bias, TSINR enables prioritizing low-frequencysignals and exhibiting poorer performance on high-frequency abnormal data.Specifically, we adopt INR to parameterize time series data as a continuousfunction and employ a transformer-based architecture to predict the INR ofgiven data. As a result, the proposed TSINR method achieves the advantage ofcapturing the temporal continuity and thus is more sensitive to discontinuousanomaly data. In addition, we further design a novel form of INR continuousfunction to learn inter- and intra-channel information, and leverage apre-trained large language model to amplify the intense fluctuations inanomalies. Extensive experiments demonstrate that TSINR achieves superioroverall performance on both univariate and multivariate time series anomalydetection benchmarks compared to other state-of-the-art reconstruction-basedmethods. Our codes are available.</description><author>Mengxuan Li, Ke Liu, Hongyang Chen, Jiajun Bu, Hongwei Wang, Haishuai Wang</author><pubDate>Mon, 18 Nov 2024 15:19:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11641v1</guid></item><item><title>Statistical-Computational Trade-offs for Recursive Adaptive Partitioning Estimators</title><link>http://arxiv.org/abs/2411.04394v2</link><description>Models based on recursive adaptive partitioning such as decision trees andtheir ensembles are popular for high-dimensional regression as they canpotentially avoid the curse of dimensionality. Because empirical riskminimization (ERM) is computationally infeasible, these models are typicallytrained using greedy algorithms. Although effective in many cases, thesealgorithms have been empirically observed to get stuck at local optima. Weexplore this phenomenon in the context of learning sparse regression functionsover $d$ binary features, showing that when the true regression function $f^*$does not satisfy Abbe et al. (2022)'s Merged Staircase Property (MSP), greedytraining requires $\exp(\Omega(d))$ to achieve low estimation error.Conversely, when $f^*$ does satisfy MSP, greedy training can attain smallestimation error with only $O(\log d)$ samples. This dichotomy mirrors that oftwo-layer neural networks trained with stochastic gradient descent (SGD) in themean-field regime, thereby establishing a head-to-head comparison betweenSGD-trained neural networks and greedy recursive partitioning estimators.Furthermore, ERM-trained recursive partitioning estimators achieve lowestimation error with $O(\log d)$ samples irrespective of whether $f^*$satisfies MSP, thereby demonstrating a statistical-computational trade-off forgreedy training. Our proofs are based on a novel interpretation of greedyrecursive partitioning using stochastic process theory and a coupling techniquethat may be of independent interest.</description><author>Yan Shuo Tan, Jason M. Klusowski, Krishnakumar Balasubramanian</author><pubDate>Mon, 18 Nov 2024 15:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04394v2</guid></item><item><title>SP${ }^3$ : Superpixel-propagated pseudo-label learning for weakly semi-supervised medical image segmentation</title><link>http://arxiv.org/abs/2411.11636v1</link><description>Deep learning-based medical image segmentation helps assist diagnosis andaccelerate the treatment process while the model training usually requireslarge-scale dense annotation datasets. Weakly semi-supervised medical imagesegmentation is an essential application because it only requires a smallamount of scribbles and a large number of unlabeled data to train the model,which greatly reduces the clinician's effort to fully annotate images. Tohandle the inadequate supervisory information challenge in weaklysemi-supervised segmentation (WSSS), a SuperPixel-Propagated Pseudo-label(SP${}^3$) learning method is proposed, using the structural informationcontained in superpixel for supplemental information. Specifically, theannotation of scribbles is propagated to superpixels and thus obtains a denseannotation for supervised training. Since the quality of pseudo-labels islimited by the low-quality annotation, the beneficial superpixels selected bydynamic thresholding are used to refine pseudo-labels. Furthermore, aiming toalleviate the negative impact of noise in pseudo-label, superpixel-leveluncertainty is incorporated to guide the pseudo-label supervision for stablelearning. Our method achieves state-of-the-art performance on both tumor andorgan segmentation datasets under the WSSS setting, using only 3\% of theannotation workload compared to fully supervised methods and attainingapproximately 80\% Dice score. Additionally, our method outperforms eightweakly and semi-supervised methods under both weakly supervised andsemi-supervised settings. Results of extensive experiments validate theeffectiveness and annotation efficiency of our weakly semi-supervisedsegmentation, which can assist clinicians in achieving automated segmentationfor organs or tumors quickly and ultimately benefit patients.</description><author>Shiman Li, Jiayue Zhao, Shaolei Liu, Xiaokun Dai, Chenxi Zhang, Zhijian Song</author><pubDate>Mon, 18 Nov 2024 15:14:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11636v1</guid></item><item><title>Chapter 7 Review of Data-Driven Generative AI Models for Knowledge Extraction from Scientific Literature in Healthcare</title><link>http://arxiv.org/abs/2411.11635v1</link><description>This review examines the development of abstractive NLP-based textsummarization approaches and compares them to existing techniques forextractive summarization. A brief history of text summarization from the 1950sto the introduction of pre-trained language models such as BidirectionalEncoder Representations from Transformer (BERT) and Generative Pre-trainingTransformers (GPT) are presented. In total, 60 studies were identified inPubMed and Web of Science, of which 29 were excluded and 24 were read andevaluated for eligibility, resulting in the use of seven studies for furtheranalysis. This chapter also includes a section with examples including anexample of a comparison between GPT-3 and state-of-the-art GPT-4 solutions inscientific text summarisation. Natural language processing has not yet reachedits full potential in the generation of brief textual summaries. As there areacknowledged concerns that must be addressed, we can expect gradualintroduction of such models in practise.</description><author>Leon Kopitar, Primoz Kocbek, Lucija Gosak, Gregor Stiglic</author><pubDate>Mon, 18 Nov 2024 15:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11635v1</guid></item><item><title>DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform</title><link>http://arxiv.org/abs/2406.01781v3</link><description>Generative modelling paradigms based on denoising diffusion processes haveemerged as a leading candidate for conditional sampling in inverse problems. Inmany real-world applications, we often have access to large, expensivelytrained unconditional diffusion models, which we aim to exploit for improvingconditional sampling. Most recent approaches are motivated heuristically andlack a unifying framework, obscuring connections between them. Further, theyoften suffer from issues such as being very sensitive to hyperparameters, beingexpensive to train or needing access to weights hidden behind a closed API. Inthis work, we unify conditional training and sampling using the mathematicallywell-understood Doob's h-transform. This new perspective allows us to unifymany existing methods under a common umbrella. Under this framework, we proposeDEFT (Doob's h-transform Efficient FineTuning), a new approach for conditionalgeneration that simply fine-tunes a very small network to quickly learn theconditional $h$-transform, while keeping the larger unconditional networkunchanged. DEFT is much faster than existing baselines while achievingstate-of-the-art performance across a variety of linear and non-linearbenchmarks. On image reconstruction tasks, we achieve speedups of up to1.6$\times$, while having the best perceptual quality on natural images andreconstruction performance on medical images. Further, we also provide initialexperiments on protein motif scaffolding and outperform reconstruction guidancemethods.</description><author>Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio</author><pubDate>Mon, 18 Nov 2024 15:11:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01781v3</guid></item><item><title>Sequential Kalman Tuning of the $t$-preconditioned Crank-Nicolson algorithm: efficient, adaptive and gradient-free inference for Bayesian inverse problems</title><link>http://arxiv.org/abs/2407.07781v2</link><description>Ensemble Kalman Inversion (EKI) has been proposed as an efficient method forthe approximate solution of Bayesian inverse problems with expensive forwardmodels. However, when applied to the Bayesian inverse problem EKI is only exactin the regime of Gaussian target measures and linear forward models. In thiswork we propose embedding EKI and Flow Annealed Kalman Inversion (FAKI), itsnormalizing flow (NF) preconditioned variant, within a Bayesian annealingscheme as part of an adaptive implementation of the $t$-preconditionedCrank-Nicolson (tpCN) sampler. The tpCN sampler differs from standard pCN inthat its proposal is reversible with respect to the multivariate$t$-distribution. The more flexible tail behaviour allows for better adaptationto sampling from non-Gaussian targets. Within our Sequential Kalman Tuning(SKT) adaptation scheme, EKI is used to initialize and precondition the tpCNsampler for each annealed target. The subsequent tpCN iterations ensureparticles are correctly distributed according to each annealed target, avoidingthe accumulation of errors that would otherwise impact EKI. We demonstrate theperformance of SKT for tpCN on three challenging numerical benchmarks, showingsignificant improvements in the rate of convergence compared to adaptationwithin standard SMC with importance weighted resampling at each temperaturelevel, and compared to similar adaptive implementations of standard pCN. TheSKT scheme applied to tpCN offers an efficient, practical solution for solvingthe Bayesian inverse problem when gradients of the forward model are notavailable. Code implementing the SKT schemes for tpCN is available at\url{https://github.com/RichardGrumitt/KalmanMC}.</description><author>Richard D. P. Grumitt, Minas Karamanis, Uroš Seljak</author><pubDate>Mon, 18 Nov 2024 15:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07781v2</guid></item><item><title>Cooperative Evolutionary Pressure and Diminishing Returns Might Explain the Fermi Paradox: On What Super-AIs Are Like</title><link>http://arxiv.org/abs/2404.03685v6</link><description>With an evolutionary approach, the basis of morality can be explained asadaptations to problems of cooperation. With 'evolution' taken in a broadsense, AIs that satisfy the conditions for evolution to apply will be subjectto the same cooperative evolutionary pressure as biological entities. Here theadaptiveness of increased cooperation as material safety and wealth increase isdiscussed -- for humans, for other societies, and for AIs. Diminishingbeneficial returns from increased access to material resources also suggeststhe possibility that, on the whole, there will be no incentive to for instancecolonize entire galaxies, thus providing a possible explanation of the Fermiparadox, wondering where everybody is. It is further argued that old societiescould engender, give way to, super-AIs, since it is likely that super-AIs arefeasible, and fitter. Closing is an aside on effective ways for morals andgoals to affect life and society, emphasizing environments, cultures, and laws,and exemplified by how to eat. Appended are an algorithm for colonizing for example a galaxy quickly, modelsof the evolution of cooperation and fairness under diminishing returns, andsoftware for simulating signaling development. It is also noted that there canbe no exponential colonization or reproduction, for mathematical reasons, aseach entity takes up a certain amount of space. 'Diminishing returns' isdefined, as less than roots.</description><author>Daniel Vallstrom</author><pubDate>Mon, 18 Nov 2024 14:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03685v6</guid></item><item><title>Federated Incremental Named Entity Recognition</title><link>http://arxiv.org/abs/2411.11623v1</link><description>Federated Named Entity Recognition (FNER) boosts model training within eachlocal client by aggregating the model updates of decentralized local clients,without sharing their private data. However, existing FNER methods assume fixedentity types and local clients in advance, leading to their ineffectiveness inpractical applications. In a more realistic scenario, local clients receive newentity types continuously, while new local clients collecting novel data mayirregularly join the global FNER training. This challenging setup, referred tohere as Federated Incremental NER, renders the global model suffering fromheterogeneous forgetting of old entity types from both intra-client andinter-client perspectives. To overcome these challenges, we propose aLocal-Global Forgetting Defense (LGFD) model. Specifically, to addressintra-client forgetting, we develop a structural knowledge distillation loss toretain the latent space's feature structure and a pseudo-label-guidedinter-type contrastive loss to enhance discriminative capability over differententity types, effectively preserving previously learned knowledge within localclients. To tackle inter-client forgetting, we propose a task switching monitorthat can automatically identify new entity types under privacy protection andstore the latest old global model for knowledge distillation andpseudo-labeling. Experiments demonstrate significant improvement of our LGFDmodel over comparison methods.</description><author>Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dong Yu</author><pubDate>Mon, 18 Nov 2024 14:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11623v1</guid></item><item><title>A Recipe for CAC: Mosaic-based Generalized Loss for Improved Class-Agnostic Counting</title><link>http://arxiv.org/abs/2404.09826v2</link><description>Class agnostic counting (CAC) is a vision task that can be used to count thetotal occurrence number of any given reference objects in the query image. Thetask is usually formulated as a density map estimation problem throughsimilarity computation among a few image samples of the reference object andthe query image. In this paper, we point out a severe issue of the existing CACframework: Given a multi-class setting, models don't consider reference imagesand instead blindly match all dominant objects in the query image. Moreover,the current evaluation metrics and dataset cannot be used to faithfully assessthe model's generalization performance and robustness. To this end, we discoverthat the combination of mosaic augmentation with generalized loss is essentialfor addressing the aforementioned issue of CAC models to count objects ofmajority (i.e. dominant objects) regardless of the references. Furthermore, weintroduce a new evaluation protocol and metrics for resolving the problembehind the existing CAC evaluation scheme and better benchmarking CAC models ina more fair manner. Besides, extensive evaluation results demonstrate that ourproposed recipe can consistently improve the performance of different CACmodels. The code is available at https://github.com/littlepenguin89106/MGCAC.</description><author>Tsung-Han Chou, Brian Wang, Wei-Chen Chiu, Jun-Cheng Chen</author><pubDate>Mon, 18 Nov 2024 14:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09826v2</guid></item><item><title>BeeManc at the PLABA Track of TAC-2024: RoBERTa for task 1 -- LLaMA3.1 and GPT-4o for task 2</title><link>http://arxiv.org/abs/2411.07381v2</link><description>This report is the system description of the BeeManc team for shared taskPlain Language Adaptation of Biomedical Abstracts (PLABA) 2024. This reportcontains two sections corresponding to the two sub-tasks in PLABA 2024. In taskone, we applied fine-tuned ReBERTa-Base models to identify and classify thedifficult terms, jargon and acronyms in the biomedical abstracts and reportedthe F1 score. Due to time constraints, we didn't finish the replacement task.In task two, we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shotprompts to complete the abstract adaptation and reported the scores in BLEU,SARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024on Task 1A and 1B, our \textbf{much smaller fine-tuned RoBERTa-Base} modelranked 3rd and 2nd respectively on the two sub-task, and the \textbf{1st onaveraged F1 scores across the two tasks} from 9 evaluated systems. OurLLaMA-3.1-70B-instructed model achieved the \textbf{highest Completeness} scorefor Task-2. We share our fine-tuned models and related resources at\url{https://github.com/HECTA-UoM/PLABA2024}</description><author>Zhidong Ling, Zihao Li, Pablo Romero, Lifeng Han, Goran Nenadic</author><pubDate>Mon, 18 Nov 2024 14:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07381v2</guid></item><item><title>ST-Tree with Interpretability for Multivariate Time Series Classification</title><link>http://arxiv.org/abs/2411.11620v1</link><description>Multivariate time series classification is of great importance in practicalapplications and is a challenging task. However, deep neural network modelssuch as Transformers exhibit high accuracy in multivariate time seriesclassification but lack interpretability and fail to provide insights into thedecision-making process. On the other hand, traditional approaches based ondecision tree classifiers offer clear decision processes but relatively loweraccuracy. Swin Transformer (ST) addresses these issues by leveragingself-attention mechanisms to capture both fine-grained local patterns andglobal patterns. It can also model multi-scale feature representation learning,thereby providing a more comprehensive representation of time series features.To tackle the aforementioned challenges, we propose ST-Tree withinterpretability for multivariate time series classification. Specifically, theST-Tree model combines ST as the backbone network with an additional neuraltree model. This integration allows us to fully leverage the advantages of STin learning time series context while providing interpretable decisionprocesses through the neural tree. This enables researchers to gain clearinsights into the model's decision-making process and extract meaningfulinterpretations. Through experimental evaluations on 10 UEA datasets, wedemonstrate that the ST-Tree model improves accuracy in multivariate timeseries classification tasks and provides interpretability through visualizingthe decision-making process across different datasets.</description><author>Mingsen Du, Yanxuan Wei, Yingxia Tang, Xiangwei Zheng, Shoushui Wei, Cun Ji</author><pubDate>Mon, 18 Nov 2024 14:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11620v1</guid></item><item><title>FERT: Real-Time Facial Expression Recognition with Short-Range FMCW Radar</title><link>http://arxiv.org/abs/2411.11619v1</link><description>This study proposes a novel approach for real-time facial expressionrecognition utilizing short-range Frequency-Modulated Continuous-Wave (FMCW)radar equipped with one transmit (Tx), and three receive (Rx) antennas. Thesystem leverages four distinct modalities simultaneously: Range-Doppler images(RDIs), micro range-Doppler Images (micro-RDIs), range azimuth images (RAIs),and range elevation images (REIs). Our innovative architecture integratesfeature extractor blocks, intermediate feature extractor blocks, and a ResNetblock to accurately classify facial expressions into smile, anger, neutral, andno-face classes. Our model achieves an average classification accuracy of98.91% on the dataset collected using a 60 GHz short-range FMCW radar. Theproposed solution operates in real-time in a person-independent manner, whichshows the potential use of low-cost FMCW radars for effective facial expressionrecognition in various applications.</description><author>Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach</author><pubDate>Mon, 18 Nov 2024 14:48:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11619v1</guid></item></channel></rss>