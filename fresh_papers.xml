<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 09 Aug 2023 06:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations</title><link>http://arxiv.org/abs/2308.04431v1</link><description>In machine learning, incorporating more data is often seen as a reliablestrategy for improving model performance; this work challenges that notion bydemonstrating that the addition of external datasets in many cases can hurt theresulting model's performance. In a large-scale empirical study acrosscombinations of four different open-source chest x-ray datasets and 9 differentlabels, we demonstrate that in 43% of settings, a model trained on data fromtwo hospitals has poorer worst group accuracy over both hospitals than a modeltrained on just a single hospital's data. This surprising result occurs eventhough the added hospital makes the training distribution more similar to thetest distribution. We explain that this phenomenon arises from the spuriouscorrelation that emerges between the disease and hospital, due tohospital-specific image artifacts. We highlight the trade-off one encounterswhen training on multiple datasets, between the obvious benefit of additionaldata and insidious cost of the introduced spurious correlation. In some cases,balancing the dataset can remove the spurious correlation and improveperformance, but it is not always an effective strategy. We contextualize ourresults within the literature on spurious correlations to help explain theseoutcomes. Our experiments underscore the importance of exercising caution whenselecting training data for machine learning models, especially in settingswhere there is a risk of spurious correlations such as with medical imaging.The risks outlined highlight the need for careful data selection and modelevaluation in future research and practice.</description><author>Rhys Compton, Lily Zhang, Aahlad Puli, Rajesh Ranganath</author><pubDate>Tue, 08 Aug 2023 18:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04431v1</guid></item><item><title>SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore</title><link>http://arxiv.org/abs/2308.04430v1</link><description>The legality of training language models (LMs) on copyrighted or otherwiserestricted data is under intense debate. However, as we show, model performancesignificantly degrades if trained only on low-risk text (e.g., out-of-copyrightbooks or government documents), due to its limited size and domain coverage. Wepresent SILO, a new language model that manages this risk-performance tradeoffduring inference. SILO is built by (1) training a parametric LM on Open LicenseCorpus (OLC), a new corpus we curate with 228B tokens of public domain andpermissively licensed text and (2) augmenting it with a more general and easilymodifiable nonparametric datastore (e.g., containing copyrighted books or news)that is only queried during inference. The datastore allows use of high-riskdata without training on it, supports sentence-level data attribution, andenables data producers to opt out from the model by removing content from thestore. These capabilities can foster compliance with data-use regulations suchas the fair use doctrine in the United States and the GDPR in the EuropeanUnion. Our experiments show that the parametric LM struggles on domains notcovered by OLC. However, access to the datastore greatly improves out of domainperformance, closing 90% of the performance gap with an LM trained on the Pile,a more diverse corpus with mostly high-risk text. We also analyze whichnonparametric approach works best, where the remaining errors lie, and howperformance scales with datastore size. Our results suggest that it is possibleto build high quality language models while mitigating their legal risk.</description><author>Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer</author><pubDate>Tue, 08 Aug 2023 18:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04430v1</guid></item><item><title>Meta-Learning Operators to Optimality from Multi-Task Non-IID Data</title><link>http://arxiv.org/abs/2308.04428v1</link><description>A powerful concept behind much of the recent progress in machine learning isthe extraction of common features across data from heterogeneous sources ortasks. Intuitively, using all of one's data to learn a common representationfunction benefits both computational effort and statistical generalization byleaving a smaller number of parameters to fine-tune on a given task. Towardtheoretically grounding these merits, we propose a general setting ofrecovering linear operators $M$ from noisy vector measurements $y = Mx + w$,where the covariates $x$ may be both non-i.i.d. and non-isotropic. Wedemonstrate that existing isotropy-agnostic meta-learning approaches incurbiases on the representation update, which causes the scaling of the noiseterms to lose favorable dependence on the number of source tasks. This in turncan cause the sample complexity of representation learning to be bottleneckedby the single-task data size. We introduce an adaptation, $\texttt{De-bias &amp;Feature-Whiten}$ ($\texttt{DFW}$), of the popular alternatingminimization-descent (AMD) scheme proposed in Collins et al., (2021), andestablish linear convergence to the optimal representation with noise levelscaling down with the $\textit{total}$ source data size. This leads togeneralization bounds on the same order as an oracle empirical risk minimizer.We verify the vital importance of $\texttt{DFW}$ on various numericalsimulations. In particular, we show that vanilla alternating-minimizationdescent fails catastrophically even for iid, but mildly non-isotropic data. Ouranalysis unifies and generalizes prior work, and provides a flexible frameworkfor a wider range of applications, such as in controls and dynamical systems.</description><author>Thomas T. C. K. Zhang, Leonardo F. Toso, James Anderson, Nikolai Matni</author><pubDate>Tue, 08 Aug 2023 18:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04428v1</guid></item><item><title>A Deep-Learning Method Using Auto-encoder and Generative Adversarial Network for Anomaly Detection on Ancient Stone Stele Surfaces</title><link>http://arxiv.org/abs/2308.04426v1</link><description>Accurate detection of natural deterioration and man-made damage on thesurfaces of ancient stele in the first instance is essential for theirpreventive conservation. Existing methods for cultural heritage preservationare not able to achieve this goal perfectly due to the difficulty of balancingaccuracy, efficiency, timeliness, and cost. This paper presents a deep-learningmethod to automatically detect above mentioned emergencies on ancient stonestele in real time, employing autoencoder (AE) and generative adversarialnetwork (GAN). The proposed method overcomes the limitations of existingmethods by requiring no extensive anomaly samples while enabling comprehensivedetection of unpredictable anomalies. the method includes stages of monitoring,data acquisition, pre-processing, model structuring, and post-processing.Taking the Longmen Grottoes' stone steles as a case study, an unsupervisedlearning model based on AE and GAN architectures is proposed and validated witha reconstruction accuracy of 99.74\%. The method's evaluation revealed theproficient detection of seven artificially designed anomalies and demonstratedprecision and reliability without false alarms. This research provides novelideas and possibilities for the application of deep learning in the field ofcultural heritage.</description><author>Yikun Liu, Yuning Wang, Cheng Liu</author><pubDate>Tue, 08 Aug 2023 18:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04426v1</guid></item><item><title>A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition</title><link>http://arxiv.org/abs/2308.04424v1</link><description>The joint task of Dialog Sentiment Classification (DSC) and Act Recognition(DAR) aims to predict the sentiment label and act label for each utterance in adialog simultaneously. However, current methods encode the dialog context inonly one direction, which limits their ability to thoroughly comprehend thecontext. Moreover, these methods overlook the explicit correlations betweensentiment and act labels, which leads to an insufficient ability to capturerich sentiment and act clues and hinders effective and accurate reasoning. Toaddress these issues, we propose a Bi-directional Multi-hop Inference Model(BMIM) that leverages a feature selection network and a bi-directionalmulti-hop inference network to iteratively extract and integrate rich sentimentand act clues in a bi-directional manner. We also employ contrastive learningand dual learning to explicitly model the correlations of sentiment and actlabels. Our experiments on two widely-used datasets show that BMIM outperformsstate-of-the-art baselines by at least 2.6% on F1 score in DAR and 1.4% on F1score in DSC. Additionally, Our proposed model not only improves theperformance but also enhances the interpretability of the joint sentiment andact prediction task.</description><author>Li Zheng, Fei Li, Yuyang Chai, Chong Teng, Donghong Ji</author><pubDate>Tue, 08 Aug 2023 18:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04424v1</guid></item><item><title>YOLOCS: Object Detection based on Dense Channel Compression for Feature Spatial Solidification</title><link>http://arxiv.org/abs/2305.04170v3</link><description>In this study, we examine the associations between channel features andconvolutional kernels during the processes of feature purification and gradientbackpropagation, with a focus on the forward and backward propagation withinthe network. Consequently, we propose a method called Dense Channel Compressionfor Feature Spatial Solidification. Drawing upon the central concept of thismethod, we introduce two innovative modules for backbone and head networks: theDense Channel Compression for Feature Spatial Solidification Structure (DCFS)and the Asymmetric Multi-Level Compression Decoupled Head (ADH). Whenintegrated into the YOLOv5 model, these two modules demonstrate exceptionalperformance, resulting in a modified model referred to as YOLOCS. Evaluated onthe MSCOCO dataset, the large, medium, and small YOLOCS models yield AP of50.1%, 47.6%, and 42.5%, respectively. Maintaining inference speeds remarkablysimilar to those of the YOLOv5 model, the large, medium, and small YOLOCSmodels surpass the YOLOv5 model's AP by 1.1%, 2.3%, and 5.2%, respectively.</description><author>Lin Huang, Weisheng Li, Linlin Shen, Haojie Fu, Xue Xiao, Suihan Xiao</author><pubDate>Tue, 08 Aug 2023 18:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04170v3</guid></item><item><title>Learning To Rank Diversely At Airbnb</title><link>http://arxiv.org/abs/2210.07774v3</link><description>Airbnb is a two-sided marketplace, bringing together hosts who own listingsfor rent, with prospective guests from around the globe. Applying neuralnetwork-based learning to rank techniques has led to significant improvementsin matching guests with hosts. These improvements in ranking were driven by acore strategy: order the listings by their estimated booking probabilities,then iterate on techniques to make these booking probability estimates more andmore accurate. Embedded implicitly in this strategy was an assumption that thebooking probability of a listing could be determined independently of otherlistings in search results. In this paper we discuss how this assumption,pervasive throughout the commonly-used learning to rank frameworks, is false.We provide a theoretical foundation correcting this assumption, followed byefficient neural network architectures based on the theory. Explicitlyaccounting for possible similarities between listings, and reducing them todiversify the search results generated strong positive impact. We discuss thesemetric wins as part of the online A/B tests of the theory. Our method providesa practical way to diversify search results for large-scale production rankingsystems.</description><author>Malay Haldar, Mustafa Abdool, Liwei He, Dillon Davis, Huiji Gao, Sanjeev Katariya</author><pubDate>Tue, 08 Aug 2023 18:48:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07774v3</guid></item><item><title>Gzip versus bag-of-words for text classification</title><link>http://arxiv.org/abs/2307.15002v5</link><description>The effectiveness of compression in text classification ('gzip') has recentlygarnered lots of attention. In this note we show that `bag-of-words' approachescan achieve similar or better results, and are more efficient.</description><author>Juri Opitz</author><pubDate>Tue, 08 Aug 2023 18:39:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15002v5</guid></item><item><title>DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from Optical Satellite Images</title><link>http://arxiv.org/abs/2308.04417v1</link><description>Optical satellite images are a critical data source; however, cloud coveroften compromises their quality, hindering image applications and analysis.Consequently, effectively removing clouds from optical satellite images hasemerged as a prominent research direction. While recent advancements in cloudremoval primarily rely on generative adversarial networks, which may yieldsuboptimal image quality, diffusion models have demonstrated remarkable successin diverse image-generation tasks, showcasing their potential in addressingthis challenge. This paper presents a novel framework called DiffCR, whichleverages conditional guided diffusion with deep convolutional networks forhigh-performance cloud removal for optical satellite imagery. Specifically, weintroduce a decoupled encoder for conditional image feature extraction,providing a robust color representation to ensure the close similarity ofappearance information between the conditional input and the synthesizedoutput. Moreover, we propose a novel and efficient time and condition fusionblock within the cloud removal model to accurately simulate the correspondencebetween the appearance in the conditional image and the target image at a lowcomputational cost. Extensive experimental evaluations on two commonly usedbenchmark datasets demonstrate that DiffCR consistently achievesstate-of-the-art performance on all metrics, with parameter and computationalcomplexities amounting to only 5.1% and 5.4%, respectively, of those previousbest methods. The source code, pre-trained models, and all the experimentalresults will be publicly available at https://github.com/XavierJiezou/DiffCRupon the paper's acceptance of this work.</description><author>Xuechao Zou, Kai Li, Junliang Xing, Yu Zhang, Shiying Wang, Lei Jin, Pin Tao</author><pubDate>Tue, 08 Aug 2023 18:34:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04417v1</guid></item><item><title>InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models</title><link>http://arxiv.org/abs/2306.03082v2</link><description>Large language models~(LLMs) are instruction followers, but it can bechallenging to find the best instruction for different situations, especiallyfor black-box LLMs on which backpropagation is forbidden. Instead of directlyoptimizing the discrete instruction, we optimize a low-dimensional soft promptapplied to an open-source LLM to generate the instruction for the black-boxLLM. On each iteration of the proposed method, which we call InstructZero, asoft prompt is converted into an instruction using the open-source LLM, whichis then submitted to the black-box LLM for zero-shot evaluation, and theperformance is sent to Bayesian optimization to produce new soft promptsimproving the zero-shot performance. We evaluate InstructZero on differentcombinations of open-source LLMs and APIs including Vicuna and ChatGPT. Ourresults show that InstructZero outperforms SOTA auto-instruction methods acrossa variety of downstream tasks. Our code and data are publicly available athttps://github.com/Lichang-Chen/InstructZero.</description><author>Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, Tianyi Zhou</author><pubDate>Tue, 08 Aug 2023 18:33:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03082v2</guid></item><item><title>Evaluating Data Attribution for Text-to-Image Models</title><link>http://arxiv.org/abs/2306.09345v2</link><description>While large text-to-image models are able to synthesize "novel" images, theseimages are necessarily a reflection of the training data. The problem of dataattribution in such models -- which of the images in the training set are mostresponsible for the appearance of a given generated image -- is a difficult yetimportant one. As an initial step toward this problem, we evaluate attributionthrough "customization" methods, which tune an existing large-scale modeltoward a given exemplar object or style. Our key insight is that this allows usto efficiently create synthetic images that are computationally influenced bythe exemplar by construction. With our new dataset of such exemplar-influencedimages, we are able to evaluate various data attribution algorithms anddifferent possible feature spaces. Furthermore, by training on our dataset, wecan tune standard models, such as DINO, CLIP, and ViT, toward the attributionproblem. Even though the procedure is tuned towards small exemplar sets, weshow generalization to larger sets. Finally, by taking into account theinherent uncertainty of the problem, we can assign soft attribution scores overa set of training images.</description><author>Sheng-Yu Wang, Alexei A. Efros, Jun-Yan Zhu, Richard Zhang</author><pubDate>Tue, 08 Aug 2023 18:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09345v2</guid></item><item><title>Digging into Depth Priors for Outdoor Neural Radiance Fields</title><link>http://arxiv.org/abs/2308.04413v1</link><description>Neural Radiance Fields (NeRF) have demonstrated impressive performance invision and graphics tasks, such as novel view synthesis and immersive reality.However, the shape-radiance ambiguity of radiance fields remains a challenge,especially in the sparse viewpoints setting. Recent work resorts to integratingdepth priors into outdoor NeRF training to alleviate the issue. However, thecriteria for selecting depth priors and the relative merits of different priorshave not been thoroughly investigated. Moreover, the relative merits ofselecting different approaches to use the depth priors is also an unexploredproblem. In this paper, we provide a comprehensive study and evaluation ofemploying depth priors to outdoor neural radiance fields, covering common depthsensing technologies and most application ways. Specifically, we conductextensive experiments with two representative NeRF methods equipped with fourcommonly-used depth priors and different depth usages on two widely usedoutdoor datasets. Our experimental results reveal several interesting findingsthat can potentially benefit practitioners and researchers in training theirNeRF models with depth priors. Project Page:https://cwchenwang.github.io/outdoor-nerf-depth</description><author>Chen Wang, Jiadai Sun, Lina Liu, Chenming Wu, Zhelun Shen, Dayan Wu, Yuchao Dai, Liangjun Zhang</author><pubDate>Tue, 08 Aug 2023 18:18:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04413v1</guid></item><item><title>Probabilistic Invariant Learning with Randomized Linear Classifiers</title><link>http://arxiv.org/abs/2308.04412v1</link><description>Designing models that are both expressive and preserve known invariances oftasks is an increasingly hard problem. Existing solutions tradeoff invariancefor computational or memory resources. In this work, we show how to leveragerandomness and design models that are both expressive and invariant but useless resources. Inspired by randomized algorithms, our key insight is thataccepting probabilistic notions of universal approximation and invariance canreduce our resource requirements. More specifically, we propose a class ofbinary classification models called Randomized Linear Classifiers (RLCs). Wegive parameter and sample size conditions in which RLCs can, with highprobability, approximate any (smooth) function while preserving invariance tocompact group transformations. Leveraging this result, we design three RLCsthat are provably probabilistic invariant for classification tasks over sets,graphs, and spherical data. We show how these models can achieve probabilisticinvariance and universality using less resources than (deterministic) neuralnetworks and their invariant counterparts. Finally, we empirically demonstratethe benefits of this new class of models on invariant tasks where deterministicinvariant neural networks are known to struggle.</description><author>Leonardo Cotta, Gal Yehuda, Assaf Schuster, Chris J. Maddison</author><pubDate>Tue, 08 Aug 2023 18:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04412v1</guid></item><item><title>V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection</title><link>http://arxiv.org/abs/2308.04409v1</link><description>We introduce a highly performant 3D object detector for point clouds usingthe DETR framework. The prior attempts all end up with suboptimal resultsbecause they fail to learn accurate inductive biases from the limited scale oftraining data. In particular, the queries often attend to points that are faraway from the target objects, violating the locality principle in objectdetection. To address the limitation, we introduce a novel 3D Vertex RelativePosition Encoding (3DV-RPE) method which computes position encoding for eachpoint based on its relative position to the 3D boxes predicted by the queriesin each decoder layer, thus providing clear information to guide the model tofocus on points near the objects, in accordance with the principle of locality.In addition, we systematically improve the pipeline from various aspects suchas data normalization based on our understanding of the task. We showexceptional results on the challenging ScanNetV2 benchmark, achievingsignificant improvements over the previous 3DETR in$\rm{AP}_{25}$/$\rm{AP}_{50}$ from 65.0\%/47.0\% to 77.8\%/66.0\%,respectively. In addition, our method sets a new record on ScanNetV2 and SUNRGB-D datasets.Code will be released at http://github.com/yichaoshen-MS/V-DETR.</description><author>Yichao Shen, Zigang Geng, Yuhui Yuan, Yutong Lin, Ze Liu, Chunyu Wang, Han Hu, Nanning Zheng, Baining Guo</author><pubDate>Tue, 08 Aug 2023 18:14:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04409v1</guid></item><item><title>XGBD: Explanation-Guided Graph Backdoor Detection</title><link>http://arxiv.org/abs/2308.04406v1</link><description>Backdoor attacks pose a significant security risk to graph learning models.Backdoors can be embedded into the target model by inserting backdoor triggersinto the training dataset, causing the model to make incorrect predictions whenthe trigger is present. To counter backdoor attacks, backdoor detection hasbeen proposed. An emerging detection strategy in the vision and NLP domains isbased on an intriguing phenomenon: when training models on a mixture ofbackdoor and clean samples, the loss on backdoor samples drops significantlyfaster than on clean samples, allowing backdoor samples to be easily detectedby selecting samples with the lowest loss values. However, the ignorance oftopological feature information on graph data limits its detectioneffectiveness when applied directly to the graph domain. To this end, wepropose an explanation-guided backdoor detection method to take advantage ofthe topological information. Specifically, we train a helper model on the graphdataset, feed graph samples into the model, and then adopt explanation methodsto attribute model prediction to an important subgraph. We observe thatbackdoor samples have distinct attribution distribution than clean samples, sothe explanatory subgraph could serve as more discriminative features fordetecting backdoor samples. Comprehensive experiments on multiple populardatasets and attack methods demonstrate the effectiveness and explainability ofour method. Our code is available:https://github.com/GuanZihan/GNN_backdoor_detection.</description><author>Zihan Guan, Mengnan Du, Ninghao Liu</author><pubDate>Tue, 08 Aug 2023 18:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04406v1</guid></item><item><title>Person Re-Identification without Identification via Event Anonymization</title><link>http://arxiv.org/abs/2308.04402v1</link><description>Wide-scale use of visual surveillance in public spaces puts individualprivacy at stake while increasing resource consumption (energy, bandwidth, andcomputation). Neuromorphic vision sensors (event-cameras) have been recentlyconsidered a valid solution to the privacy issue because they do not capturedetailed RGB visual information of the subjects in the scene. However, recentdeep learning architectures have been able to reconstruct images from eventcameras with high fidelity, reintroducing a potential threat to privacy forevent-based vision applications. In this paper, we aim to anonymizeevent-streams to protect the identity of human subjects against such imagereconstruction attacks. To achieve this, we propose an end-to-end networkarchitecture jointly optimized for the twofold objective of preserving privacyand performing a downstream task such as person ReId. Our network learns toscramble events, enforcing the degradation of images recovered from the privacyattacker. In this work, we also bring to the community the first everevent-based person ReId dataset gathered to evaluate the performance of ourapproach. We validate our approach with extensive experiments and reportresults on the synthetic event data simulated from the publicly availableSoftBio dataset and our proposed Event-ReId dataset.</description><author>Shafiq Ahmad, Pietro Morerio, Alessio Del Bue</author><pubDate>Tue, 08 Aug 2023 18:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04402v1</guid></item><item><title>Character-level NMT and language similarity</title><link>http://arxiv.org/abs/2308.04398v1</link><description>We explore the effectiveness of character-level neural machine translationusing Transformer architecture for various levels of language similarity andsize of the training dataset on translation between Czech and Croatian, German,Hungarian, Slovak, and Spanish. We evaluate the models using automatic MTmetrics and show that translation between similar languages benefits fromcharacter-level input segmentation, while for less related languages,character-level vanilla Transformer-base often lags behind subword-levelsegmentation. We confirm previous findings that it is possible to close the gapby finetuning the already trained subword-level models to character-level.</description><author>Josef Jon, Ondřej Bojar</author><pubDate>Tue, 08 Aug 2023 18:01:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04398v1</guid></item><item><title>Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models</title><link>http://arxiv.org/abs/2308.04399v1</link><description>Major advances in Machine Learning (ML) and Artificial Intelligence (AI)increasingly take the form of developing and releasing general-purpose models.These models are designed to be adapted by other businesses and agencies toperform a particular, domain-specific function. This process has become knownas adaptation or fine-tuning. This paper offers a model of the fine-tuningprocess where a Generalist brings the technological product (here an ML model)to a certain level of performance, and one or more Domain-specialist(s) adaptsit for use in a particular domain. Both entities are profit-seeking and incurcosts when they invest in the technology, and they must reach a bargainingagreement on how to share the revenue for the technology to reach the market.For a relatively general class of cost and revenue functions, we characterizethe conditions under which the fine-tuning game yields a profit-sharingsolution. We observe that any potential domain-specialization will eithercontribute, free-ride, or abstain in their uptake of the technology, and weprovide conditions yielding these different strategies. We show how methodsbased on bargaining solutions and sub-game perfect equilibria provide insightsinto the strategic behavior of firms in these types of interactions, and wefind that profit-sharing can still arise even when one firm has significantlyhigher costs than another. We also provide methods for identifyingPareto-optimal bargaining arrangements for a general set of utility functions.</description><author>Benjamin Laufer, Jon Kleinberg, Hoda Heidari</author><pubDate>Tue, 08 Aug 2023 18:01:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04399v1</guid></item><item><title>LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake Extraction from Remote Sensing Imagery</title><link>http://arxiv.org/abs/2308.04397v1</link><description>Lake extraction from remote sensing imagery is challenging due to the complexshapes of lakes and the presence of noise. Existing methods suffer from blurredsegmentation boundaries and poor foreground modeling. In this paper, we proposea hybrid CNN-Transformer architecture, called LEFormer, for accurate lakeextraction. LEFormer contains four main modules: CNN encoder, Transformerencoder, cross-encoder fusion, and lightweight decoder. The CNN encoderrecovers local spatial information and improves fine-scale details.Simultaneously, the Transformer encoder captures long-range dependenciesbetween sequences of any length, allowing them to obtain global features andcontext information better. Finally, a lightweight decoder is employed for maskprediction. We evaluate the performance and efficiency of LEFormer on twodatasets, the Surface Water (SW) and the Qinghai-Tibet Plateau Lake (QTPL).Experimental results show that LEFormer consistently achieves state-of-the-art(SOTA) performance and efficiency on these two datasets, outperforming existingmethods. Specifically, LEFormer achieves 90.86% and 97.42% mIoU on the SW andQTPL datasets with a parameter count of 3.61M, respectively, while being 20xminor than the previous SOTA method.</description><author>Ben Chen, Xuechao Zou, Yu Zhang, Jiayu Li, Kai Li, Pin Tao</author><pubDate>Tue, 08 Aug 2023 18:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04397v1</guid></item><item><title>Event Abstraction for Enterprise Collaboration Systems to Support Social Process Mining</title><link>http://arxiv.org/abs/2308.04396v1</link><description>One aim of Process Mining (PM) is the discovery of process models from eventlogs of information systems. PM has been successfully applied toprocess-oriented enterprise systems but is less suited for communication- anddocument-oriented Enterprise Collaboration Systems (ECS). ECS event logs arevery fine-granular and PM applied to their logs results in spaghetti models. Acommon solution for this is event abstraction, i.e., converting low-level logsinto more abstract high-level logs before running discovery algorithms. ECSlogs come with special characteristics that have so far not been fullyaddressed by existing event abstraction approaches. We aim to close this gapwith a tailored ECS event abstraction (ECSEA) approach that trains a model bycomparing recorded actual user activities (high-level traces) with thesystem-generated low-level traces (extracted from the ECS). The model allows usto automatically convert future low-level traces into an abstracted high-levellog that can be used for PM. Our evaluation shows that the algorithm producesaccurate results. ECSEA is a preprocessing method that is essential for theinterpretation of collaborative work activity in ECS, which we call SocialProcess Mining.</description><author>Jonas Blatt, Patrick Delfmann, Petra Schubert</author><pubDate>Tue, 08 Aug 2023 18:00:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04396v1</guid></item><item><title>Data Augmentation-Based Unsupervised Domain Adaptation In Medical Imaging</title><link>http://arxiv.org/abs/2308.04395v1</link><description>Deep learning-based models in medical imaging often struggle to generalizeeffectively to new scans due to data heterogeneity arising from differences inhardware, acquisition parameters, population, and artifacts. This limitationpresents a significant challenge in adopting machine learning models forclinical practice. We propose an unsupervised method for robust domainadaptation in brain MRI segmentation by leveraging MRI-specific augmentationtechniques. To evaluate the effectiveness of our method, we conduct extensiveexperiments across diverse datasets, modalities, and segmentation tasks,comparing against the state-of-the-art methods. The results show that ourproposed approach achieves high accuracy, exhibits broad applicability, andshowcases remarkable robustness against domain shift in various tasks,surpassing the state-of-the-art performance in the majority of cases.</description><author>Sebastian Nørgaard Llambias, Mads Nielsen, Mostafa Mehdipour Ghazi</author><pubDate>Tue, 08 Aug 2023 18:00:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04395v1</guid></item><item><title>Learning Evaluation Models from Large Language Models for Sequence Generation</title><link>http://arxiv.org/abs/2308.04386v1</link><description>Large language models achieve state-of-the-art performance on sequencegeneration evaluation, but typically have a large number of parameters. This isa computational challenge as presented by applying their evaluation capabilityat scale. To overcome the challenge, in this paper, we propose \textbf{ECT}, an\textbf{e}valuation \textbf{c}apability \textbf{t}ransfer method, to transferthe evaluation capability from LLMs to relatively lightweight language models.Based on the proposed ECT, we learn various evaluation models from ChatGPT, andemploy them as reward models to improve sequence generation models viareinforcement learning and reranking approaches. Experimental results onmachine translation, text style transfer, and summarization tasks demonstratethe effectiveness of our ECT. Notably, applying the learned evaluation modelsto sequence generation models results in better generated sequences asevaluated by commonly used metrics and ChatGPT.</description><author>Chenglong Wang, Hang Zhou, Kaiyan Chang, Tongran Liu, Chunliang Zhang, Quan Du, Tong Xiao, Jingbo Zhu</author><pubDate>Tue, 08 Aug 2023 17:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04386v1</guid></item><item><title>DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds</title><link>http://arxiv.org/abs/2308.04383v1</link><description>Point clouds are naturally sparse, while image pixels are dense. Theinconsistency limits feature fusion from both modalities for point-wise sceneflow estimation. Previous methods rarely predict scene flow from the entirepoint clouds of the scene with one-time inference due to the memoryinefficiency and heavy overhead from distance calculation and sorting involvedin commonly used farthest point sampling, KNN, and ball query algorithms forlocal feature aggregation. To mitigate these issues in scene flow learning, weregularize raw points to a dense format by storing 3D coordinates in 2D grids.Unlike the sampling operation commonly used in existing works, the dense 2Drepresentation 1) preserves most points in the given scene, 2) brings in asignificant boost of efficiency, and 3) eliminates the density gap betweenpoints and pixels, allowing us to perform effective feature fusion. We alsopresent a novel warping projection technique to alleviate the information lossproblem resulting from the fact that multiple points could be mapped into onegrid during projection when computing cost volume. Sufficient experimentsdemonstrate the efficiency and effectiveness of our method, outperforming theprior-arts on the FlyingThings3D and KITTI dataset.</description><author>Chensheng Peng, Guangming Wang, Xian Wan Lo, Xinrui Wu, Chenfeng Xu, Masayoshi Tomizuka, Wei Zhan, Hesheng Wang</author><pubDate>Tue, 08 Aug 2023 17:37:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04383v1</guid></item><item><title>Your Negative May not Be True Negative: Boosting Image-Text Matching with False Negative Elimination</title><link>http://arxiv.org/abs/2308.04380v1</link><description>Most existing image-text matching methods adopt triplet loss as theoptimization objective, and choosing a proper negative sample for the tripletof &lt;anchor, positive, negative&gt; is important for effectively training themodel, e.g., hard negatives make the model learn efficiently and effectively.However, we observe that existing methods mainly employ the most similarsamples as hard negatives, which may not be true negatives. In other words, thesamples with high similarity but not paired with the anchor may reservepositive semantic associations, and we call them false negatives. Repellingthese false negatives in triplet loss would mislead the semantic representationlearning and result in inferior retrieval performance. In this paper, wepropose a novel False Negative Elimination (FNE) strategy to select negativesvia sampling, which could alleviate the problem introduced by false negatives.Specifically, we first construct the distributions of positive and negativesamples separately via their similarities with the anchor, based on thefeatures extracted from image and text encoders. Then we calculate the falsenegative probability of a given sample based on its similarity with the anchorand the above distributions via the Bayes' rule, which is employed as thesampling weight during negative sampling process. Since there may not exist anyfalse negative in a small batch size, we design a memory module with momentumto retain a large negative buffer and implement our negative sampling strategyspanning over the buffer. In addition, to make the model focus on hardnegatives, we reassign the sampling weights for the simple negatives with acut-down strategy. The extensive experiments are conducted on Flickr30K andMS-COCO, and the results demonstrate the superiority of our proposed falsenegative elimination strategy. The code is available athttps://github.com/LuminosityX/FNE.</description><author>Haoxuan Li, Yi Bin, Junrong Liao, Yang Yang, Heng Tao Shen</author><pubDate>Tue, 08 Aug 2023 17:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04380v1</guid></item><item><title>Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making</title><link>http://arxiv.org/abs/2308.04375v1</link><description>Artificial intelligence (AI) is increasingly being considered to assist humandecision-making in high-stake domains (e.g. health). However, researchers havediscussed an issue that humans can over-rely on wrong suggestions of the AImodel instead of achieving human AI complementary performance. In this work, weutilized salient feature explanations along with what-if, counterfactualexplanations to make humans review AI suggestions more analytically to reduceoverreliance on AI and explored the effect of these explanations on trust andreliance on AI during clinical decision-making. We conducted an experiment withseven therapists and ten laypersons on the task of assessing post-strokesurvivors' quality of motion, and analyzed their performance, agreement levelon the task, and reliance on AI without and with two types of AI explanations.Our results showed that the AI model with both salient features andcounterfactual explanations assisted therapists and laypersons to improve theirperformance and agreement level on the task when `right' AI outputs arepresented. While both therapists and laypersons over-relied on `wrong' AIoutputs, counterfactual explanations assisted both therapists and laypersons toreduce their over-reliance on `wrong' AI outputs by 21\% compared to salientfeature explanations. Specifically, laypersons had higher performance degradesby 18.0 f1-score with salient feature explanations and 14.0 f1-score withcounterfactual explanations than therapists with performance degrades of 8.6and 2.8 f1-scores respectively. Our work discusses the potential ofcounterfactual explanations to better estimate the accuracy of an AI model andreduce over-reliance on `wrong' AI outputs and implications for improvinghuman-AI collaborative decision-making.</description><author>Min Hun Lee, Chong Jun Chew</author><pubDate>Tue, 08 Aug 2023 17:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04375v1</guid></item><item><title>Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning</title><link>http://arxiv.org/abs/2308.04373v1</link><description>The main premise of federated learning is that machine learning model updatesare computed locally, in particular to preserve user data privacy, as thosenever leave the perimeter of their device. This mechanism supposes the generalmodel, once aggregated, to be broadcast to collaborating and non maliciousnodes. However, without proper defenses, compromised clients can easily probethe model inside their local memory in search of adversarial examples. Forinstance, considering image-based applications, adversarial examples consist ofimperceptibly perturbed images (to the human eye) misclassified by the localmodel, which can be later presented to a victim node's counterpart model toreplicate the attack. To mitigate such malicious probing, we introduce Pelta, anovel shielding mechanism leveraging trusted hardware. By harnessing thecapabilities of Trusted Execution Environments (TEEs), Pelta masks part of theback-propagation chain rule, otherwise typically exploited by attackers for thedesign of malicious samples. We evaluate Pelta on a state of the art ensemblemodel and demonstrate its effectiveness against the Self Attention Gradientadversarial Attack.</description><author>Simon Queyrut, Yérom-David Bromberg, Valerio Schiavoni</author><pubDate>Tue, 08 Aug 2023 17:22:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04373v1</guid></item><item><title>Some Options for Instantiation of Bipolar Argument Graphs with Deductive Arguments</title><link>http://arxiv.org/abs/2308.04372v1</link><description>Argument graphs provide an abstract representation of an argumentativesituation. A bipolar argument graph is a directed graph where each node denotesan argument, and each arc denotes the influence of one argument on another.Here we assume that the influence is supporting, attacking, or ambiguous. In abipolar argument graph, each argument is atomic and so it has no internalstructure. Yet to better understand the nature of the individual arguments, andhow they interact, it is important to consider their internal structure. Toaddress this need, this paper presents a framework based on the use of logicalarguments to instantiate bipolar argument graphs, and a set of possibleconstraints on instantiating arguments that take into account the internalstructure of the arguments, and the types of relationship between arguments.</description><author>Anthony Hunter</author><pubDate>Tue, 08 Aug 2023 17:22:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04372v1</guid></item><item><title>Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models</title><link>http://arxiv.org/abs/2307.06713v2</link><description>A wide variety of natural language tasks are currently being addressed withlarge-scale language models (LLMs). These models are usually trained with avery large amount of unsupervised text data and adapted to perform a downstreamnatural language task using methods like fine-tuning, calibration or in-contextlearning. In this work, we propose an approach to adapt the prior classdistribution to perform text classification tasks without the need for labelledsamples and only few in-domain sample queries. The proposed approach treats theLLM as a black box, adding a stage where the model posteriors are calibrated tothe task. Results show that these methods outperform the un-adapted model fordifferent number of training shots in the prompt and a previous approach werecalibration is performed without using any adaptation data.</description><author>Lautaro Estienne</author><pubDate>Tue, 08 Aug 2023 17:21:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06713v2</guid></item><item><title>Randomized algorithms for precise measurement of differentially-private, personalized recommendations</title><link>http://arxiv.org/abs/2308.03735v2</link><description>Personalized recommendations form an important part of today's internetecosystem, helping artists and creators to reach interested users, and helpingusers to discover new and engaging content. However, many users today areskeptical of platforms that personalize recommendations, in part due tohistorically careless treatment of personal data and data privacy. Now,businesses that rely on personalized recommendations are entering a newparadigm, where many of their systems must be overhauled to be privacy-first.In this article, we propose an algorithm for personalized recommendations thatfacilitates both precise and differentially-private measurement. We consideradvertising as an example application, and conduct offline experiments toquantify how the proposed privacy-preserving algorithm affects key metricsrelated to user experience, advertiser value, and platform revenue compared tothe extremes of both (private) non-personalized and non-private, personalizedimplementations.</description><author>Allegra Laro, Yanqing Chen, Hao He, Babak Aghazadeh</author><pubDate>Tue, 08 Aug 2023 17:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03735v2</guid></item><item><title>Cumulative Reasoning With Large Language Models</title><link>http://arxiv.org/abs/2308.04371v1</link><description>While language models are powerful and versatile, they often fail to addresshighly complex problems. This is because solving complex problems requiresdeliberate thinking, which has been only minimally guided during training. Inthis paper, we propose a new method called Cumulative Reasoning (CR), whichemploys language models in a cumulative and iterative manner to emulate humanthought processes. By decomposing tasks into smaller components, \ournamebstreamlines the problem-solving process, rendering it both more manageable andeffective. For logical inference tasks, CR consistently outperforms existingmethods with an improvement up to 9.3\%, and achieves the astonishing accuracyof 98.04\% on the curated FOLIO wiki dataset. In the context of the Game of 24,CR achieves an accuracy of 94\%, which signifies a substantial enhancement of20\% over the previous state-of-the-art method.</description><author>Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao</author><pubDate>Tue, 08 Aug 2023 17:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04371v1</guid></item><item><title>When Super-Resolution Meets Camouflaged Object Detection: A Comparison Study</title><link>http://arxiv.org/abs/2308.04370v1</link><description>Super Resolution (SR) and Camouflaged Object Detection (COD) are two hottopics in computer vision with various joint applications. For instance,low-resolution surveillance images can be successively processed bysuper-resolution techniques and camouflaged object detection. However, inprevious work, these two areas are always studied in isolation. In this paper,we, for the first time, conduct an integrated comparative evaluation for both.Specifically, we benchmark different super-resolution methods on commonly usedCOD datasets, and meanwhile, we evaluate the robustness of different COD modelsby using COD data processed by SR methods. Our goal is to bridge these twodomains, discover novel experimental phenomena, summarize new experim.</description><author>Juan Wen, Shupeng Cheng, Peng Xu, Bowen Zhou, Radu Timofte, Weiyan Hou, Luc Van Gool</author><pubDate>Tue, 08 Aug 2023 17:17:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04370v1</guid></item><item><title>SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition</title><link>http://arxiv.org/abs/2308.04369v1</link><description>Event camera-based pattern recognition is a newly arising research topic inrecent years. Current researchers usually transform the event streams intoimages, graphs, or voxels, and adopt deep neural networks for event-basedclassification. Although good performance can be achieved on simple eventrecognition datasets, however, their results may be still limited due to thefollowing two issues. Firstly, they adopt spatial sparse event streams forrecognition only, which may fail to capture the color and detailed textureinformation well. Secondly, they adopt either Spiking Neural Networks (SNN) forenergy-efficient recognition with suboptimal results, or Artificial NeuralNetworks (ANN) for energy-intensive, high-performance recognition. However,seldom of them consider achieving a balance between these two aspects. In thispaper, we formally propose to recognize patterns by fusing RGB frames and eventstreams simultaneously and propose a new RGB frame-event recognition frameworkto address the aforementioned issues. The proposed method contains four mainmodules, i.e., memory support Transformer network for RGB frame encoding,spiking neural network for raw event stream encoding, multi-modal bottleneckfusion module for RGB-Event feature aggregation, and prediction head. Due tothe scarce of RGB-Event based classification dataset, we also propose alarge-scale PokerEvent dataset which contains 114 classes, and 27102frame-event pairs recorded using a DVS346 event camera. Extensive experimentson two RGB-Event based classification datasets fully validated theeffectiveness of our proposed framework. We hope this work will boost thedevelopment of pattern recognition by fusing RGB frames and event streams. Bothour dataset and source code of this work will be released athttps://github.com/Event-AHU/SSTFormer.</description><author>Xiao Wang, Zongzhen Wu, Yao Rong, Lin Zhu, Bo Jiang, Jin Tang, Yonghong Tian</author><pubDate>Tue, 08 Aug 2023 17:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04369v1</guid></item><item><title>Hybrid Spectral Denoising Transformer with Guided Attention</title><link>http://arxiv.org/abs/2303.09040v2</link><description>In this paper, we present a Hybrid Spectral Denoising Transformer (HSDT) forhyperspectral image denoising. Challenges in adapting transformer for HSI arisefrom the capabilities to tackle existing limitations of CNN-based methods incapturing the global and local spatial-spectral correlations while maintainingefficiency and flexibility. To address these issues, we introduce a hybridapproach that combines the advantages of both models with a Spatial-SpectralSeparable Convolution (S3Conv), Guided Spectral Self-Attention (GSSA), andSelf-Modulated Feed-Forward Network (SM-FFN). Our S3Conv works as a lightweightalternative to 3D convolution, which extracts more spatial-spectral correlatedfeatures while keeping the flexibility to tackle HSIs with an arbitrary numberof bands. These features are then adaptively processed by GSSA which per-forms3D self-attention across the spectral bands, guided by a set of learnablequeries that encode the spectral signatures. This not only enriches our modelwith powerful capabilities for identifying global spectral correlations butalso maintains linear complexity. Moreover, our SM-FFN proposes theself-modulation that intensifies the activations of more informative regions,which further strengthens the aggregated features. Extensive experiments areconducted on various datasets under both simulated and real-world noise, and itshows that our HSDT significantly outperforms the existing state-of-the-artmethods while maintaining low computational overhead. Code is at https://github.com/Zeqiang-Lai/HSDT.</description><author>Zeqiang Lai, Chenggang Yan, Ying Fu</author><pubDate>Tue, 08 Aug 2023 17:14:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09040v2</guid></item><item><title>GaitRef: Gait Recognition with Refined Sequential Skeletons</title><link>http://arxiv.org/abs/2304.07916v3</link><description>Identifying humans with their walking sequences, known as gait recognition,is a useful biometric understanding task as it can be observed from a longdistance and does not require cooperation from the subject. Two commonmodalities used for representing the walking sequence of a person aresilhouettes and joint skeletons. Silhouette sequences, which record theboundary of the walking person in each frame, may suffer from the variantappearances from carried-on objects and clothes of the person. Framewise jointdetections are noisy and introduce some jitters that are not consistent withsequential detections. In this paper, we combine the silhouettes and skeletonsand refine the framewise joint predictions for gait recognition. With temporalinformation from the silhouette sequences, we show that the refined skeletonscan improve gait recognition performance without extra annotations. We compareour methods on four public datasets, CASIA-B, OUMVLP, Gait3D and GREW, and showstate-of-the-art performance.</description><author>Haidong Zhu, Wanrong Zheng, Zhaoheng Zheng, Ram Nevatia</author><pubDate>Tue, 08 Aug 2023 17:06:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07916v3</guid></item><item><title>Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders</title><link>http://arxiv.org/abs/2305.19259v3</link><description>Stochastic Gradient Descent (SGD) algorithms are widely used in optimizingneural networks, with Random Reshuffling (RR) and Single Shuffle (SS) beingpopular choices for cycling through random or single permutations of thetraining data. However, the convergence properties of these algorithms in thenon-convex case are not fully understood. Existing results suggest that, inrealistic training scenarios where the number of epochs is smaller than thetraining set size, RR may perform worse than SGD. In this paper, we analyze a general SGD algorithm that allows for arbitrarydata orderings and show improved convergence rates for non-convex functions.Specifically, our analysis reveals that SGD with random and single shuffling isalways faster or at least as good as classical SGD with replacement, regardlessof the number of iterations. Overall, our study highlights the benefits ofusing SGD with random/single shuffling and provides new insights into itsconvergence properties for non-convex optimization.</description><author>Anastasia Koloskova, Nikita Doikov, Sebastian U. Stich, Martin Jaggi</author><pubDate>Tue, 08 Aug 2023 17:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19259v3</guid></item><item><title>Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization</title><link>http://arxiv.org/abs/2307.03571v2</link><description>This paper presents a framework for smooth optimization of objectives with$\ell_q$ and $\ell_{p,q}$ regularization for (structured) sparsity. Findingsolutions to these non-smooth and possibly non-convex problems typically relieson specialized optimization routines. In contrast, the method studied here iscompatible with off-the-shelf (stochastic) gradient descent that is ubiquitousin deep learning, thereby enabling differentiable sparse regularization withoutapproximations. The proposed optimization transfer comprises anoverparametrization of selected model parameters followed by a change ofpenalties. In the overparametrized problem, smooth and convex $\ell_2$regularization induces non-smooth and non-convex regularization in the originalparametrization. We show that the resulting surrogate problem not only has anidentical global optimum but also exactly preserves the local minima. This isparticularly useful in non-convex regularization, where finding globalsolutions is NP-hard and local minima often generalize well. We provide anintegrative overview that consolidates various literature strands onsparsity-inducing parametrizations in a general setting and meaningfully extendexisting approaches. The feasibility of our approach is evaluated throughnumerical experiments, demonstrating its effectiveness by matching oroutperforming common implementations of convex and non-convex regularizers.</description><author>Chris Kolb, Christian L. Müller, Bernd Bischl, David Rügamer</author><pubDate>Tue, 08 Aug 2023 17:05:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03571v2</guid></item><item><title>SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling</title><link>http://arxiv.org/abs/2308.04365v1</link><description>Causal inference is a crucial goal of science, enabling researchers to arriveat meaningful conclusions regarding the predictions of hypotheticalinterventions using observational data. Path models, Structural Equation Models(SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means tounambiguously specify assumptions regarding the causal structure underlying aphenomenon. Unlike DAGs, which make very few assumptions about the functionaland parametric form, SEM assumes linearity. This can result in functionalmisspecification which prevents researchers from undertaking reliable effectsize estimation. In contrast, we propose Super Learner Equation Modeling, apath modeling technique integrating machine learning Super Learner ensembles.We empirically demonstrate its ability to provide consistent and unbiasedestimates of causal effects, its competitive performance for linear models whencompared with SEM, and highlight its superiority over SEM when dealing withnon-linear relationships. We provide open-source code, and a tutorial notebookwith example usage, accentuating the easy-to-use nature of the method.</description><author>Matthew J. Vowels</author><pubDate>Tue, 08 Aug 2023 17:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04365v1</guid></item><item><title>PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-performance Cloud Removal from Multi-temporal Satellite Imagery</title><link>http://arxiv.org/abs/2303.16565v2</link><description>Satellite imagery analysis plays a pivotal role in remote sensing; however,information loss due to cloud cover significantly impedes its application.Although existing deep cloud removal models have achieved notable outcomes,they scarcely consider contextual information. This study introduces ahigh-performance cloud removal architecture, termed Progressive Multi-scaleAttention Autoencoder (PMAA), which concurrently harnesses global and localinformation to construct robust contextual dependencies using a novelMulti-scale Attention Module (MAM) and a novel Local Interaction Module (LIM).PMAA establishes long-range dependencies of multi-scale features using MAM andmodulates the reconstruction of fine-grained details utilizing LIM, enablingsimultaneous representation of fine- and coarse-grained features at the samelevel. With the help of diverse and multi-scale features, PMAA consistentlyoutperforms the previous state-of-the-art model CTGAN on two benchmarkdatasets. Moreover, PMAA boasts considerable efficiency advantages, with only0.5% and 14.6% of the parameters and computational complexity of CTGAN,respectively. These comprehensive results underscore PMAA's potential as alightweight cloud removal network suitable for deployment on edge devices toaccomplish large-scale cloud removal tasks. Our source code and pre-trainedmodels are available at https://github.com/XavierJiezou/PMAA.</description><author>Xuechao Zou, Kai Li, Junliang Xing, Pin Tao, Yachao Cui</author><pubDate>Tue, 08 Aug 2023 17:01:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16565v2</guid></item><item><title>Learning Unbiased Image Segmentation: A Case Study with Plain Knee Radiographs</title><link>http://arxiv.org/abs/2308.04356v1</link><description>Automatic segmentation of knee bony anatomy is essential in orthopedics, andit has been around for several years in both pre-operative and post-operativesettings. While deep learning algorithms have demonstrated exceptionalperformance in medical image analysis, the assessment of fairness and potentialbiases within these models remains limited. This study aims to revisit deeplearning-powered knee-bony anatomy segmentation using plain radiographs touncover visible gender and racial biases. The current contribution offers thepotential to advance our understanding of biases, and it provides practicalinsights for researchers and practitioners in medical imaging. The proposedmitigation strategies mitigate gender and racial biases, ensuring fair andunbiased segmentation results. Furthermore, this work promotes equal access toaccurate diagnoses and treatment outcomes for diverse patient populations,fostering equitable and inclusive healthcare provision.</description><author>Nickolas Littlefield, Johannes F. Plate, Kurt R. Weiss, Ines Lohse, Avani Chhabra, Ismaeel A. Siddiqui, Zoe Menezes, George Mastorakos, Sakshi Mehul Thakar, Mehrnaz Abedian, Matthew F. Gong, Luke A. Carlson, Hamidreza Moradi, Soheyla Amirian, Ahmad P. Tafti</author><pubDate>Tue, 08 Aug 2023 17:01:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04356v1</guid></item><item><title>3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</title><link>http://arxiv.org/abs/2308.04352v1</link><description>3D vision-language grounding (3D-VL) is an emerging field that aims toconnect the 3D physical world with natural language, which is crucial forachieving embodied intelligence. Current 3D-VL models rely heavily onsophisticated modules, auxiliary losses, and optimization tricks, which callsfor a simple and unified model. In this paper, we propose 3D-VisTA, apre-trained Transformer for 3D Vision and Text Alignment that can be easilyadapted to various downstream tasks. 3D-VisTA simply utilizes self-attentionlayers for both single-modal modeling and multi-modal fusion without anysophisticated task-specific design. To further enhance its performance on 3D-VLtasks, we construct ScanScribe, the first large-scale 3D scene-text pairsdataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB-D scans for 1,185unique indoor scenes originating from ScanNet and 3R-Scan datasets, along withpaired 278K scene descriptions generated from existing 3D-VL tasks, templates,and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/objectmodeling and scene-text matching. It achieves state-of-the-art results onvarious 3D-VL tasks, ranging from visual grounding and dense captioning toquestion answering and situated reasoning. Moreover, 3D-VisTA demonstratessuperior data efficiency, obtaining strong performance even with limitedannotations during downstream task fine-tuning.</description><author>Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li</author><pubDate>Tue, 08 Aug 2023 16:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04352v1</guid></item><item><title>Learning to Pan-sharpening with Memories of Spatial Details</title><link>http://arxiv.org/abs/2306.16181v3</link><description>Pan-sharpening, as one of the most commonly used techniques in remote sensingsystems, aims to inject spatial details from panchromatic images intomultispectral images (MS) to obtain high-resolution multispectral images. Sincedeep learning has received widespread attention because of its powerful fittingability and efficient feature extraction, a variety of pan-sharpening methodshave been proposed to achieve remarkable performance. However, currentpan-sharpening methods usually require the paired panchromatic (PAN) and MSimages as input, which limits their usage in some scenarios. To address thisissue, in this paper we observe that the spatial details from PAN images aremainly high-frequency cues, i.e., the edges reflect the contour of input PANimages. This motivates us to develop a PAN-agnostic representation to storesome base edges, so as to compose the contour for the corresponding PAN imagevia them. As a result, we can perform the pan-sharpening task with only the MSimage when inference. To this end, a memory-based network is adapted to extractand memorize the spatial details during the training phase and is used toreplace the process of obtaining spatial information from PAN images wheninference, which is called Memory-based Spatial Details Network (MSDN).Finally, we integrate the proposed MSDN module into the existing deeplearning-based pan-sharpening methods to achieve an end-to-end pan-sharpeningnetwork. With extensive experiments on the Gaofen1 and WorldView-4 satellites,we verify that our method constructs good spatial details without PAN imagesand achieves the best performance. The code is available athttps://github.com/Zhao-Tian-yi/Learning-to-Pan-sharpening-with-Memories-of-Spatial-Details.git.</description><author>Maoxun Yuan, Tianyi Zhao, Bo Li, Xingxing Wei</author><pubDate>Tue, 08 Aug 2023 16:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16181v3</guid></item><item><title>DiffBFR: Bootstrapping Diffusion Model Towards Blind Face Restoration</title><link>http://arxiv.org/abs/2305.04517v2</link><description>Blind face restoration (BFR) is important while challenging. Prior worksprefer to exploit GAN-based frameworks to tackle this task due to the balanceof quality and efficiency. However, these methods suffer from poor stabilityand adaptability to long-tail distribution, failing to simultaneously retainsource identity and restore detail. We propose DiffBFR to introduce DiffusionProbabilistic Model (DPM) for BFR to tackle the above problem, given itssuperiority over GAN in aspects of avoiding training collapse and generatinglong-tail distribution. DiffBFR utilizes a two-step design, that first restoresidentity information from low-quality images and then enhances texture detailsaccording to the distribution of real faces. This design is implemented withtwo key components: 1) Identity Restoration Module (IRM) for preserving theface details in results. Instead of denoising from pure Gaussian randomdistribution with LQ images as the condition during the reverse process, wepropose a novel truncated sampling method which starts from LQ images with partnoise added. We theoretically prove that this change shrinks the evidence lowerbound of DPM and then restores more original details. With theoretical proof,two cascade conditional DPMs with different input sizes are introduced tostrengthen this sampling effect and reduce training difficulty in thehigh-resolution image generated directly. 2) Texture Enhancement Module (TEM)for polishing the texture of the image. Here an unconditional DPM, a LQ-freemodel, is introduced to further force the restorations to appear realistic. Wetheoretically proved that this unconditional DPM trained on pure HQ imagescontributes to justifying the correct distribution of inference images outputfrom IRM in pixel-level space. Truncated sampling with fractional time step isutilized to polish pixel-level textures while preserving identity information.</description><author>Xinmin Qiu, Congying Han, Zicheng Zhang, Bonan Li, Tiande Guo, Xuecheng Nie</author><pubDate>Tue, 08 Aug 2023 16:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04517v2</guid></item><item><title>Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles</title><link>http://arxiv.org/abs/2308.04346v1</link><description>We investigate the potential for nationality biases in natural languageprocessing (NLP) models using human evaluation methods. Biased NLP models canperpetuate stereotypes and lead to algorithmic discrimination, posing asignificant challenge to the fairness and justice of AI systems. Our studyemploys a two-step mixed-methods approach that includes both quantitative andqualitative analysis to identify and understand the impact of nationality biasin a text generation model. Through our human-centered quantitative analysis,we measure the extent of nationality bias in articles generated by AI sources.We then conduct open-ended interviews with participants, performing qualitativecoding and thematic analysis to understand the implications of these biases onhuman readers. Our findings reveal that biased NLP models tend to replicate andamplify existing societal biases, which can translate to harm if used in asociotechnical setting. The qualitative analysis from our interviews offersinsights into the experience readers have when encountering such articles,highlighting the potential to shift a reader's perception of a country. Thesefindings emphasize the critical role of public perception in shaping AI'simpact on society and the need to correct biases in AI systems.</description><author>Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao `Kenneth' Huang, Shomir Wilson</author><pubDate>Tue, 08 Aug 2023 16:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04346v1</guid></item><item><title>Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval</title><link>http://arxiv.org/abs/2308.04343v1</link><description>Most existing cross-modal retrieval methods employ two-stream encoders withdifferent architectures for images and texts, \textit{e.g.}, CNN for images andRNN/Transformer for texts. Such discrepancy in architectures may inducedifferent semantic distribution spaces and limit the interactions betweenimages and texts, and further result in inferior alignment between images andtexts. To fill this research gap, inspired by recent advances of Transformersin vision tasks, we propose to unify the encoder architectures withTransformers for both modalities. Specifically, we design a cross-modalretrieval framework purely based on two-stream Transformers, dubbed\textbf{Hierarchical Alignment Transformers (HAT)}, which consists of an imageTransformer, a text Transformer, and a hierarchical alignment module. With suchidentical architectures, the encoders could produce representations with moresimilar characteristics for images and texts, and make the interactions andalignments between them much easier. Besides, to leverage the rich semantics,we devise a hierarchical alignment scheme to explore multi-levelcorrespondences of different layers between images and texts. To evaluate theeffectiveness of the proposed HAT, we conduct extensive experiments on twobenchmark datasets, MSCOCO and Flickr30K. Experimental results demonstrate thatHAT outperforms SOTA baselines by a large margin. Specifically, on two keytasks, \textit{i.e.}, image-to-text and text-to-image retrieval, HAT achieves7.6\% and 16.7\% relative score improvement of Recall@1 on MSCOCO, and 4.4\%and 11.6\% on Flickr30k respectively. The code is available at\url{https://github.com/LuminosityX/HAT}.</description><author>Yi Bin, Haoxuan Li, Yahui Xu, Xing Xu, Yang Yang, Heng Tao Shen</author><pubDate>Tue, 08 Aug 2023 16:43:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04343v1</guid></item><item><title>Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage</title><link>http://arxiv.org/abs/2308.04341v1</link><description>Machine learning models are increasingly utilized across impactful domains topredict individual outcomes. As such, many models provide algorithmic recourseto individuals who receive negative outcomes. However, recourse can beleveraged by adversaries to disclose private information. This work presentsthe first attempt at mitigating such attacks. We present two novel methods togenerate differentially private recourse: Differentially Private Model (DPM)and Laplace Recourse (LR). Using logistic regression classifiers and real worldand synthetic datasets, we find that DPM and LR perform well in reducing whatan adversary can infer, especially at low FPR. When training dataset size islarge enough, we find particular success in preventing privacy leakage whilemaintaining model and recourse accuracy with our novel LR method.</description><author>Catherine Huang, Chelse Swoopes, Christina Xiao, Jiaqi Ma, Himabindu Lakkaraju</author><pubDate>Tue, 08 Aug 2023 16:38:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04341v1</guid></item><item><title>MedMine: Examining Pre-trained Language Models on Medication Mining</title><link>http://arxiv.org/abs/2308.03629v2</link><description>Automatic medication mining from clinical and biomedical text has become apopular topic due to its real impact on healthcare applications and the recentdevelopment of powerful language models (LMs). However, fully-automaticextraction models still face obstacles to be overcome such that they can bedeployed directly into clinical practice for better impacts. Such obstaclesinclude their imbalanced performances on different entity types and clinicalevents. In this work, we examine current state-of-the-art pre-trained languagemodels (PLMs) on such tasks, via fine-tuning including the monolingual modelMed7 and multilingual large language model (LLM) XLM-RoBERTa. We compare theiradvantages and drawbacks using historical medication mining shared task datasets from n2c2-2018 challenges. We report the findings we get from thesefine-tuning experiments such that they can facilitate future research onaddressing them, for instance, how to combine their outputs, merge such models,or improve their overall accuracy by ensemble learning and data augmentation.MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}</description><author>Haifa Alrdahi, Lifeng Han, Hendrik Šuvalov, Goran Nenadic</author><pubDate>Tue, 08 Aug 2023 16:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03629v2</guid></item><item><title>A Lightweight and Accurate Face Detection Algorithm Based on Retinaface</title><link>http://arxiv.org/abs/2308.04340v1</link><description>In this paper, we propose a lightweight and accurate face detection algorithmLAFD (Light and accurate face detection) based on Retinaface. Backbone networkin the algorithm is a modified MobileNetV3 network which adjusts the size ofthe convolution kernel, the channel expansion multiplier of the invertedresiduals block and the use of the SE attention mechanism. Deformableconvolution network(DCN) is introduced in the context module and the algorithmuses focal loss function instead of cross-entropy loss function as theclassification loss function of the model. The test results on the WIDERFACEdataset indicate that the average accuracy of LAFD is 94.1%, 92.2% and 82.1%for the "easy", "medium" and "hard" validation subsets respectively with animprovement of 3.4%, 4.0% and 8.3% compared to Retinaface and 3.1%, 4.1% and4.1% higher than the well-performing lightweight model, LFFD. If the inputimage is pre-processed and scaled to 1560px in length or 1200px in width, themodel achieves an average accuracy of 86.2% on the 'hard' validation subset.The model is lightweight, with a size of only 10.2MB.</description><author>Baozhu Liu, Hewei Yu</author><pubDate>Tue, 08 Aug 2023 16:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04340v1</guid></item><item><title>Pengembangan Model untuk Mendeteksi Kerusakan pada Terumbu Karang dengan Klasifikasi Citra</title><link>http://arxiv.org/abs/2308.04337v1</link><description>The abundant biodiversity of coral reefs in Indonesian waters is a valuableasset that needs to be preserved. Rapid climate change and uncontrolled humanactivities have led to the degradation of coral reef ecosystems, includingcoral bleaching, which is a critical indicator of coral health conditions.Therefore, this research aims to develop an accurate classification model todistinguish between healthy corals and corals experiencing bleaching. Thisstudy utilizes a specialized dataset consisting of 923 images collected fromFlickr using the Flickr API. The dataset comprises two distinct classes:healthy corals (438 images) and bleached corals (485 images). These images havebeen resized to a maximum of 300 pixels in width or height, whichever islarger, to maintain consistent sizes across the dataset. The method employed in this research involves the use of machine learningmodels, particularly convolutional neural networks (CNN), to recognize anddifferentiate visual patterns associated with healthy and bleached corals. Inthis context, the dataset can be used to train and test various classificationmodels to achieve optimal results. By leveraging the ResNet model, it was foundthat a from-scratch ResNet model can outperform pretrained models in terms ofprecision and accuracy. The success in developing accurate classificationmodels will greatly benefit researchers and marine biologists in gaining abetter understanding of coral reef health. These models can also be employed tomonitor changes in the coral reef environment, thereby making a significantcontribution to conservation and ecosystem restoration efforts that havefar-reaching impacts on life.</description><author>Fadhil Muhammad, Alif Bintang Elfandra, Iqbal Pahlevi Amin, Alfan Farizki Wicaksono</author><pubDate>Tue, 08 Aug 2023 16:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04337v1</guid></item><item><title>Towards an AI to Win Ghana's National Science and Maths Quiz</title><link>http://arxiv.org/abs/2308.04333v1</link><description>Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is thequestion we seek to answer in the NSMQ AI project, an open-source project thatis building AI to compete live in the NSMQ and win. The NSMQ is an annual livescience and mathematics competition for senior secondary school students inGhana in which 3 teams of 2 students compete by answering questions acrossbiology, chemistry, physics, and math in 5 rounds over 5 progressive stagesuntil a winning team is crowned for that year. The NSMQ is an exciting livequiz competition with interesting technical challenges across speech-to-text,text-to-speech, question-answering, and human-computer interaction. In thisongoing work that began in January 2023, we give an overview of the project,describe each of the teams, progress made thus far, and the next steps towardour planned launch and debut of the AI in October for NSMQ 2023. An AI thatconquers this grand challenge can have real-world impact on education such asenabling millions of students across Africa to have one-on-one learning supportfrom this AI.</description><author>George Boateng, Jonathan Abrefah Mensah, Kevin Takyi Yeboah, William Edor, Andrew Kojo Mensah-Onumah, Naafi Dasana Ibrahim, Nana Sam Yeboah</author><pubDate>Tue, 08 Aug 2023 16:26:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04333v1</guid></item><item><title>P-NOC: Adversarial CAM Generation for Weakly Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2305.12522v2</link><description>To mitigate the necessity for large amounts of supervised segmentationannotation sets, multiple Weakly Supervised Semantic Segmentation (WSSS)strategies have been devised. These will often rely on advanced data and modelregularization strategies to instigate the development of useful properties(e.g., prediction completeness and fidelity to semantic boundaries) insegmentation priors, notwithstanding the lack of annotated information. In thiswork, we first create a strong baseline by analyzing complementary WSSStechniques and regularizing strategies, considering their strengths andlimitations. We then propose a new Class-specific Adversarial Erasing strategy,comprising two adversarial CAM generating networks being gradually refined toproduce robust semantic segmentation proposals. Empirical results suggest thatour approach induces substantial improvement in the effectiveness of thebaseline, resulting in a noticeable improvement over both Pascal VOC 2012 andMS COCO 2014 datasets.</description><author>Lucas David, Helio Pedrini, Zanoni Dias</author><pubDate>Tue, 08 Aug 2023 16:22:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12522v2</guid></item><item><title>RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback</title><link>http://arxiv.org/abs/2308.04332v1</link><description>To use reinforcement learning from human feedback (RLHF) in practicalapplications, it is crucial to learn reward models from diverse sources ofhuman feedback and to consider human factors involved in providing feedback ofdifferent types. However, the systematic study of learning from diverse typesof feedback is held back by limited standardized tooling available toresearchers. To bridge this gap, we propose RLHF-Blender, a configurable,interactive interface for learning from human feedback. RLHF-Blender provides amodular experimentation framework and implementation that enables researchersto systematically investigate the properties and qualities of human feedbackfor reward learning. The system facilitates the exploration of various feedbacktypes, including demonstrations, rankings, comparisons, and natural languageinstructions, as well as studies considering the impact of human factors ontheir effectiveness. We discuss a set of concrete research opportunitiesenabled by RLHF-Blender. More information is available athttps://rlhfblender.info/.</description><author>Yannick Metz, David Lindner, Raphaël Baur, Daniel Keim, Mennatallah El-Assady</author><pubDate>Tue, 08 Aug 2023 16:21:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04332v1</guid></item><item><title>Domain Adaptive Person Search via GAN-based Scene Synthesis for Cross-scene Videos</title><link>http://arxiv.org/abs/2308.04322v1</link><description>Person search has recently been a challenging task in the computer visiondomain, which aims to search specific pedestrians from realcameras.Nevertheless, most surveillance videos comprise only a handful ofimages of each pedestrian, which often feature identical backgrounds andclothing. Hence, it is difficult to learn more discriminative features forperson search in real scenes. To tackle this challenge, we draw on GenerativeAdversarial Networks (GAN) to synthesize data from surveillance videos. GAN hasthrived in computer vision problems because it produces high-quality imagesefficiently. We merely alter the popular Fast R-CNN model, which is capable ofprocessing videos and yielding accurate detection outcomes. In order toappropriately relieve the pressure brought by the two-stage model, we design anAssisted-Identity Query Module (AIDQ) to provide positive images for the behindpart. Besides, the proposed novel GAN-based Scene Synthesis model that cansynthesize high-quality cross-id person images for person search tasks. Inorder to facilitate the feature learning of the GAN-based Scene Synthesismodel, we adopt an online learning strategy that collaboratively learns thesynthesized images and original images. Extensive experiments on two widelyused person search benchmarks, CUHK-SYSU and PRW, have shown that our methodhas achieved great performance, and the extensive ablation study furtherjustifies our GAN-synthetic data can effectively increase the variability ofthe datasets and be more realistic.</description><author>Huibing Wang, Tianxiang Cui, Mingze Yao, Huijuan Pang, Yushan Du</author><pubDate>Tue, 08 Aug 2023 16:15:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04322v1</guid></item><item><title>All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2308.04321v1</link><description>In this work, we propose a new transformer-based regularization to betterlocalize objects for Weakly supervised semantic segmentation (WSSS). Inimage-level WSSS, Class Activation Map (CAM) is adopted to generate objectlocalization as pseudo segmentation labels. To address the partial activationissue of the CAMs, consistency regularization is employed to maintainactivation intensity invariance across various image augmentations. However,such methods ignore pair-wise relations among regions within each CAM, whichcapture context and should also be invariant across image views. To this end,we propose a new all-pairs consistency regularization (ACR). Given a pair ofaugmented views, our approach regularizes the activation intensities between apair of augmented views, while also ensuring that the affinity across regionswithin each view remains consistent. We adopt vision transformers as theself-attention mechanism naturally embeds pair-wise affinity. This enables usto simply regularize the distance between the attention matrices of augmentedimage pairs. Additionally, we introduce a novel class-wise localization methodthat leverages the gradients of the class token. Our method can be seamlesslyintegrated into existing WSSS methods using transformers without modifying thearchitectures. We evaluate our method on PASCAL VOC and MS COCO datasets. Ourmethod produces noticeably better class localization maps (67.3% mIoU on PASCALVOC train), resulting in superior WSSS performances.</description><author>Weixuan Sun, Yanhao Zhang, Zhen Qin, Zheyuan Liu, Lin Cheng, Fanyi Wang, Yiran Zhong, Nick Barnes</author><pubDate>Tue, 08 Aug 2023 16:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04321v1</guid></item><item><title>Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training</title><link>http://arxiv.org/abs/2307.07873v3</link><description>Adversarial examples (AEs) for DNNs have been shown to be transferable: AEsthat successfully fool white-box surrogate models can also deceive otherblack-box models with different architectures. Although a bunch of empiricalstudies have provided guidance on generating highly transferable AEs, many ofthese findings lack explanations and even lead to inconsistent advice. In thispaper, we take a further step towards understanding adversarialtransferability, with a particular focus on surrogate aspects. Starting fromthe intriguing little robustness phenomenon, where models adversarially trainedwith mildly perturbed adversarial samples can serve as better surrogates, weattribute it to a trade-off between two predominant factors: model smoothnessand gradient similarity. Our investigations focus on their joint effects,rather than their separate correlations with transferability. Through a seriesof theoretical and empirical analyses, we conjecture that the data distributionshift in adversarial training explains the degradation of gradient similarity.Building on these insights, we explore the impacts of data augmentation andgradient regularization on transferability and identify that the trade-offgenerally exists in the various training mechanisms, thus building acomprehensive blueprint for the regulation mechanism behind transferability.Finally, we provide a general route for constructing better surrogates to boosttransferability which optimizes both model smoothness and gradient similaritysimultaneously, e.g., the combination of input gradient regularization andsharpness-aware minimization (SAM), validated by extensive experiments. Insummary, we call for attention to the united impacts of these two factors forlaunching effective transfer attacks, rather than optimizing one while ignoringthe other, and emphasize the crucial role of manipulating surrogate models.</description><author>Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi, Minghui Li, Xiaogeng Liu, Wei Wan, Hai Jin</author><pubDate>Tue, 08 Aug 2023 16:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07873v3</guid></item><item><title>MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows</title><link>http://arxiv.org/abs/2302.01075v5</link><description>The conventional understanding of adversarial training in generativeadversarial networks (GANs) is that the discriminator is trained to estimate adivergence, and the generator learns to minimize this divergence. We argue thatdespite the fact that many variants of GANs were developed following thisparadigm, the current theoretical understanding of GANs and their practicalalgorithms are inconsistent. In this paper, we leverage Wasserstein gradientflows which characterize the evolution of particles in the sample space, togain theoretical insights and algorithmic inspiration of GANs. We introduce aunified generative modeling framework - MonoFlow: the particle evolution isrescaled via a monotonically increasing mapping of the log density ratio. Underour framework, adversarial training can be viewed as a procedure firstobtaining MonoFlow's vector field via training the discriminator and thegenerator learns to draw the particle flow defined by the corresponding vectorfield. We also reveal the fundamental difference between variational divergenceminimization and adversarial training. This analysis helps us to identify whattypes of generator loss functions can lead to the successful training of GANsand suggest that GANs may have more loss designs beyond the literature (e.g.,non-saturated loss), as long as they realize MonoFlow. Consistent empiricalstudies are included to validate the effectiveness of our framework.</description><author>Mingxuan Yi, Zhanxing Zhu, Song Liu</author><pubDate>Tue, 08 Aug 2023 16:12:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01075v5</guid></item><item><title>Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs</title><link>http://arxiv.org/abs/2308.04314v1</link><description>Recently, there has been extensive study of cooperative multi-agentmulti-armed bandits where a set of distributed agents cooperatively play thesame multi-armed bandit game. The goal is to develop bandit algorithms with theoptimal group and individual regrets and low communication between agents. Theprior work tackled this problem using two paradigms: leader-follower and fullydistributed algorithms. Prior algorithms in both paradigms achieve the optimalgroup regret. The leader-follower algorithms achieve constant communicationcosts but fail to achieve optimal individual regrets. The state-of-the-artfully distributed algorithms achieve optimal individual regrets but fail toachieve constant communication costs. This paper presents a simple yeteffective communication policy and integrates it into a learning algorithm forcooperative bandits. Our algorithm achieves the best of both paradigms: optimalindividual regret and constant communication costs.</description><author>Lin Yang, Xuchuang Wang, Mohammad Hajiesmaili, Lijun Zhang, John C. S. Lui, Don Towsley</author><pubDate>Tue, 08 Aug 2023 16:02:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04314v1</guid></item><item><title>Apple Vision Pro for Healthcare: "The Ultimate Display"?</title><link>http://arxiv.org/abs/2308.04313v1</link><description>At the Worldwide Developers Conference (WWDC) in June 2023, Apple introducedthe Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, morespecifically it is a Virtual Reality (VR) device with an additional VideoSee-Through (VST) capability. The VST capability turns the Vision Pro also intoan Augmented Reality (AR) device. The AR feature is enabled by streaming thereal world via cameras to the (VR) screens in front of the user's eyes. This isof course not unique and similar to other devices, like the Varjo XR-3.Nevertheless, the Vision Pro has some interesting features, like an inside-outscreen that can show the headset wearers' eyes to "outsiders" or a button onthe top, called "Digital Crown", that allows you to seamlessly blend digitalcontent with your physical space by turning it. In addition, it is untethered,except for the cable to the battery, which makes the headset more agile,compared to the Varjo XR-3. This could actually come closer to the "UltimateDisplay", which Ivan Sutherland had already sketched in 1965. Not available tothe public yet, like the Ultimate Display, we want to take a look into thecrystal ball in this perspective to see if it can overcome some clinicalchallenges that - especially - AR still faces in the medical domain, but alsogo beyond and discuss if the Vision Pro could support clinicians in essentialtasks to spend more time with their patients.</description><author>Jan Egger, Christina Gsaxner, Xiaojun Chen, Jiang Bian, Jens Kleesiek, Behrus Puladi</author><pubDate>Tue, 08 Aug 2023 16:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04313v1</guid></item><item><title>Interpretable Goal-Based model for Vehicle Trajectory Prediction in Interactive Scenarios</title><link>http://arxiv.org/abs/2308.04312v1</link><description>The abilities to understand the social interaction behaviors between avehicle and its surroundings while predicting its trajectory in an urbanenvironment are critical for road safety in autonomous driving. Socialinteractions are hard to explain because of their uncertainty. In recent years,neural network-based methods have been widely used for trajectory predictionand have been shown to outperform hand-crafted methods. However, these methodssuffer from their lack of interpretability. In order to overcome thislimitation, we combine the interpretability of a discrete choice model with thehigh accuracy of a neural network-based model for the task of vehicletrajectory prediction in an interactive environment. We implement and evaluateour model using the INTERACTION dataset and demonstrate the effectiveness ofour proposed architecture to explain its predictions without compromising theaccuracy.</description><author>Amina Ghoul, Itheri Yahiaoui, Anne Verroust-Blondet, Fawzi Nashashibi</author><pubDate>Tue, 08 Aug 2023 16:00:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04312v1</guid></item><item><title>A Voting-Stacking Ensemble of Inception Networks for Cervical Cytology Classification</title><link>http://arxiv.org/abs/2308.02781v2</link><description>Cervical cancer is one of the most severe diseases threatening women'shealth. Early detection and diagnosis can significantly reduce cancer risk, inwhich cervical cytology classification is indispensable. Researchers haverecently designed many networks for automated cervical cancer diagnosis, butthe limited accuracy and bulky size of these individual models cannot meetpractical application needs. To address this issue, we propose aVoting-Stacking ensemble strategy, which employs three Inception networks asbase learners and integrates their outputs through a voting ensemble. Thesamples misclassified by the ensemble model generate a new training set onwhich a linear classification model is trained as the meta-learner and performsthe final predictions. In addition, a multi-level Stacking ensemble frameworkis designed to improve performance further. The method is evaluated on theSIPakMed, Herlev, and Mendeley datasets, achieving accuracies of 100%, 100%,and 100%, respectively. The experimental results outperform the currentstate-of-the-art (SOTA) methods, demonstrating its potential for reducingscreening workload and helping pathologists detect cervical cancer.</description><author>Linyi Qian, Qian Huang, Yulin Chen, Junzhou Chen</author><pubDate>Tue, 08 Aug 2023 15:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02781v2</guid></item><item><title>Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?</title><link>http://arxiv.org/abs/2307.12344v2</link><description>While deep neural network models offer unmatched classification performance,they are prone to learning spurious correlations in the data. Such dependencieson confounding information can be difficult to detect using performance metricsif the test data comes from the same distribution as the training data.Interpretable ML methods such as post-hoc explanations or inherentlyinterpretable classifiers promise to identify faulty model reasoning. However,there is mixed evidence whether many of these techniques are actually able todo so. In this paper, we propose a rigorous evaluation strategy to assess anexplanation technique's ability to correctly identify spurious correlations.Using this strategy, we evaluate five post-hoc explanation techniques and oneinherently interpretable method for their ability to detect three types ofartificially added confounders in a chest x-ray diagnosis task. We find thatthe post-hoc technique SHAP, as well as the inherently interpretable Attri-Netprovide the best performance and can be used to reliably identify faulty modelbehavior.</description><author>Susu Sun, Lisa M. Koch, Christian F. Baumgartner</author><pubDate>Tue, 08 Aug 2023 15:52:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12344v2</guid></item><item><title>Set-based value operators for non-stationary Markovian environments</title><link>http://arxiv.org/abs/2207.07271v3</link><description>This paper analyzes finite state Markov Decision Processes (MDPs) withuncertain parameters in compact sets and re-examines results from robust MDPvia set-based fixed point theory. To this end, we generalize the Bellman andpolicy evaluation operators to contracting operators on the value functionspace and denote them as \emph{value operators}. We lift these value operatorsto act on \emph{sets} of value functions and denote them as \emph{set-basedvalue operators}. We prove that the set-based value operators are\emph{contractions} in the space of compact value function sets. Leveraginginsights from set theory, we generalize the rectangularity condition in classicrobust MDP literature to a containment condition for all value operators, whichis weaker and can be applied to a larger set of parameter-uncertain MDPs andcontracting operators in dynamic programming. We prove that both therectangularity condition and the containment condition sufficiently ensure thatthe set-based value operator's fixed point set contains its own extremaelements. For convex and compact sets of uncertain MDP parameters, we showequivalence between the classic robust value function and the supremum of thefixed point set of the set-based Bellman operator. Under dynamically changingMDP parameters in compact sets, we prove a set convergence result for valueiteration, which otherwise may not converge to a single value function.Finally, we derive novel guarantees for probabilistic path-planning problems inplanet exploration and stratospheric station-keeping.</description><author>Sarah H. Q. Li, Assalé Adjé, Pierre-Loïc Garoche, Behçet Açıkmeşe</author><pubDate>Tue, 08 Aug 2023 15:51:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.07271v3</guid></item><item><title>Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review</title><link>http://arxiv.org/abs/2308.04306v1</link><description>The history of metaphor research also marks the evolution of knowledgeinfusion research. With the continued advancement of deep learning techniquesin recent years, the natural language processing community has shown greatinterest in applying knowledge to successful results in metaphor recognitiontasks. Although there has been a gradual increase in the number of approachesinvolving knowledge injection in the field of metaphor recognition, there is alack of a complete review article on knowledge injection based approaches.Therefore, the goal of this paper is to provide a comprehensive review ofresearch advances in the application of deep learning for knowledge injectionin metaphor recognition tasks. In this paper, we systematically summarize andgeneralize the mainstream knowledge and knowledge injection principles, as wellas review the datasets, evaluation metrics, and benchmark models used inmetaphor recognition tasks. Finally, we explore the current issues facingknowledge injection methods and provide an outlook on future researchdirections.</description><author>Cheng Yang, Wenye Zhao, Qingbao Huang</author><pubDate>Tue, 08 Aug 2023 15:51:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04306v1</guid></item><item><title>Inherently Interpretable Multi-Label Classification Using Class-Specific Counterfactuals</title><link>http://arxiv.org/abs/2303.00500v2</link><description>Interpretability is essential for machine learning algorithms in high-stakesapplication fields such as medical image analysis. However, high-performingblack-box neural networks do not provide explanations for their predictions,which can lead to mistrust and suboptimal human-ML collaboration. Post-hocexplanation techniques, which are widely used in practice, have been shown tosuffer from severe conceptual problems. Furthermore, as we show in this paper,current explanation techniques do not perform adequately in the multi-labelscenario, in which multiple medical findings may co-occur in a single image. Wepropose Attri-Net, an inherently interpretable model for multi-labelclassification. Attri-Net is a powerful classifier that provides transparent,trustworthy, and human-understandable explanations. The model first generatesclass-specific attribution maps based on counterfactuals to identify whichimage regions correspond to certain medical findings. Then a simple logisticregression classifier is used to make predictions based solely on theseattribution maps. We compare Attri-Net to five post-hoc explanation techniquesand one inherently interpretable classifier on three chest X-ray datasets. Wefind that Attri-Net produces high-quality multi-label explanations consistentwith clinical knowledge and has comparable classification performance tostate-of-the-art classification models.</description><author>Susu Sun, Stefano Woerner, Andreas Maier, Lisa M. Koch, Christian F. Baumgartner</author><pubDate>Tue, 08 Aug 2023 15:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00500v2</guid></item><item><title>The Model Inversion Eavesdropping Attack in Semantic Communication Systems</title><link>http://arxiv.org/abs/2308.04304v1</link><description>In recent years, semantic communication has been a popular research topic forits superiority in communication efficiency. As semantic communication relieson deep learning to extract meaning from raw messages, it is vulnerable toattacks targeting deep learning models. In this paper, we introduce the modelinversion eavesdropping attack (MIEA) to reveal the risk of privacy leaks inthe semantic communication system. In MIEA, the attacker first eavesdrops thesignal being transmitted by the semantic communication system and then performsmodel inversion attack to reconstruct the raw message, where both the white-boxand black-box settings are considered. Evaluation results show that MIEA cansuccessfully reconstruct the raw message with good quality under differentchannel conditions. We then propose a defense method based on randompermutation and substitution to defend against MIEA in order to achieve securesemantic communication. Our experimental results demonstrate the effectivenessof the proposed defense method in preventing MIEA.</description><author>Yuhao Chen, Qianqian Yang, Zhiguo Shi, Jiming Chen</author><pubDate>Tue, 08 Aug 2023 15:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04304v1</guid></item><item><title>Vehicle Motion Forecasting using Prior Information and Semantic-assisted Occupancy Grid Maps</title><link>http://arxiv.org/abs/2308.04303v1</link><description>Motion prediction is a challenging task for autonomous vehicles due touncertainty in the sensor data, the non-deterministic nature of future, andcomplex behavior of agents. In this paper, we tackle this problem byrepresenting the scene as dynamic occupancy grid maps (DOGMs), associatingsemantic labels to the occupied cells and incorporating map information. Wepropose a novel framework that combines deep-learning-based spatio-temporal andprobabilistic approaches to predict vehicle behaviors.Contrary to theconventional OGM prediction methods, evaluation of our work is conductedagainst the ground truth annotations. We experiment and validate our results onreal-world NuScenes dataset and show that our model shows superior ability topredict both static and dynamic vehicles compared to OGM predictions.Furthermore, we perform an ablation study and assess the role of semanticlabels and map in the architecture.</description><author>Rabbia Asghar, Manuel Diaz-Zapata, Lukas Rummelhard, Anne Spalanzani, Christian Laugier</author><pubDate>Tue, 08 Aug 2023 15:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04303v1</guid></item><item><title>Actor-Critic with variable time discretization via sustained actions</title><link>http://arxiv.org/abs/2308.04299v1</link><description>Reinforcement learning (RL) methods work in discrete time. In order to applyRL to inherently continuous problems like robotic control, a specific timediscretization needs to be defined. This is a choice between sparse timecontrol, which may be easier to train, and finer time control, which may allowfor better ultimate performance. In this work, we propose SusACER, anoff-policy RL algorithm that combines the advantages of different timediscretization settings. Initially, it operates with sparse time discretizationand gradually switches to a fine one. We analyze the effects of the changingtime discretization in robotic control environments: Ant, HalfCheetah, Hopper,and Walker2D. In all cases our proposed algorithm outperforms state of the art.</description><author>Jakub Łyskawa, Paweł Wawrzyński</author><pubDate>Tue, 08 Aug 2023 15:45:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04299v1</guid></item><item><title>Engineering LaCAM$^\ast$: Towards Real-Time, Large-Scale, and Near-Optimal Multi-Agent Pathfinding</title><link>http://arxiv.org/abs/2308.04292v1</link><description>This paper addresses the challenges of real-time, large-scale, andnear-optimal multi-agent pathfinding (MAPF) through enhancements to therecently proposed LaCAM* algorithm. LaCAM* is a scalable search-based algorithmthat guarantees the eventual finding of optimal solutions for cumulativetransition costs. While it has demonstrated remarkable planning success rates,surpassing various state-of-the-art MAPF methods, its initial solution qualityis far from optimal, and its convergence speed to the optimum is slow. Toovercome these limitations, this paper introduces several improvementtechniques, partly drawing inspiration from other MAPF methods. We provideempirical evidence that the fusion of these techniques significantly improvesthe solution quality of LaCAM*, thus further pushing the boundaries of MAPFalgorithms.</description><author>Keisuke Okumura</author><pubDate>Tue, 08 Aug 2023 15:36:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04292v1</guid></item><item><title>Practical and Rigorous Uncertainty Bounds for Gaussian Process Regression</title><link>http://arxiv.org/abs/2105.02796v2</link><description>Gaussian Process Regression is a popular nonparametric regression methodbased on Bayesian principles that provides uncertainty estimates for itspredictions. However, these estimates are of a Bayesian nature, whereas forsome important applications, like learning-based control with safetyguarantees, frequentist uncertainty bounds are required. Although such rigorousbounds are available for Gaussian Processes, they are too conservative to beuseful in applications. This often leads practitioners to replacing thesebounds by heuristics, thus breaking all theoretical guarantees. To address thisproblem, we introduce new uncertainty bounds that are rigorous, yet practicallyuseful at the same time. In particular, the bounds can be explicitly evaluatedand are much less conservative than state of the art results. Furthermore, weshow that certain model misspecifications lead to only graceful degradation. Wedemonstrate these advantages and the usefulness of our results forlearning-based control with numerical examples.</description><author>Christian Fiedler, Carsten W. Scherer, Sebastian Trimpe</author><pubDate>Tue, 08 Aug 2023 15:34:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2105.02796v2</guid></item><item><title>Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On</title><link>http://arxiv.org/abs/2308.04288v1</link><description>Fabricating and designing 3D garments has become extremely demanding with theincreasing need for synthesizing realistic dressed persons for a variety ofapplications, e.g. 3D virtual try-on, digitalization of 2D clothes into 3Dapparel, and cloth animation. It thus necessitates a simple and straightforwardpipeline to obtain high-quality texture from simple input, such as 2D referenceimages. Since traditional warping-based texture generation methods require asignificant number of control points to be manually selected for each type ofgarment, which can be a time-consuming and tedious process. We propose a novelmethod, called Cloth2Tex, which eliminates the human burden in this process.Cloth2Tex is a self-supervised method that generates texture maps withreasonable layout and structural consistency. Another key feature of Cloth2Texis that it can be used to support high-fidelity texture inpainting. This isdone by combining Cloth2Tex with a prevailing latent diffusion model. Weevaluate our approach both qualitatively and quantitatively and demonstratethat Cloth2Tex can generate high-quality texture maps and achieve the bestvisual effects in comparison to other methods. Project page:tomguluson92.github.io/projects/cloth2tex/</description><author>Daiheng Gao, Xu Chen, Xindi Zhang, Qi Wang, Ke Sun, Bang Zhang, Liefeng Bo, Qixing Huang</author><pubDate>Tue, 08 Aug 2023 15:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04288v1</guid></item><item><title>Genie: Show Me the Data for Quantization</title><link>http://arxiv.org/abs/2212.04780v3</link><description>Zero-shot quantization is a promising approach for developing lightweightdeep neural networks when data is inaccessible owing to various reasons,including cost and issues related to privacy. By exploiting the learnedparameters ($\mu$ and $\sigma$) of batch normalization layers in anFP32-pre-trained model, zero-shot quantization schemes focus on generatingsynthetic data. Subsequently, they distill knowledge from the pre-trained model(teacher) to the quantized model (student) such that the quantized model can beoptimized with the synthetic dataset. However, thus far, zero-shot quantizationhas primarily been discussed in the context of quantization-aware trainingmethods, which require task-specific losses and long-term optimization as muchas retraining. We thus introduce a post-training quantization scheme forzero-shot quantization that produces high-quality quantized networks within afew hours. Furthermore, we propose a framework called Genie~that generates datasuited for quantization. With the data synthesized by Genie, we can producerobust quantized models without real datasets, which is comparable to few-shotquantization. We also propose a post-training quantization algorithm to enhancethe performance of quantized models. By combining them, we can bridge the gapbetween zero-shot and few-shot quantization while significantly improving thequantization performance compared to that of existing approaches. In otherwords, we can obtain a unique state-of-the-art zero-shot quantization approach.The code is available at \url{https://github.com/SamsungLabs/Genie}.</description><author>Yongkweon Jeon, Chungman Lee, Ho-young Kim</author><pubDate>Tue, 08 Aug 2023 15:30:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04780v3</guid></item><item><title>Comparative Analysis of the wav2vec 2.0 Feature Extractor</title><link>http://arxiv.org/abs/2308.04286v1</link><description>Automatic speech recognition (ASR) systems typically use handcrafted featureextraction pipelines. To avoid their inherent information loss and to achievemore consistent modeling from speech to transcribed text, neural raw waveformfeature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model,which has recently gained large popularity, uses a convolutional FE whichoperates directly on the speech waveform. However, it is not yet studiedextensively in the literature. In this work, we study its capability to replacethe standard feature extraction methods in a connectionist temporalclassification (CTC) ASR model and compare it to an alternative neural FE. Weshow that both are competitive with traditional FEs on the LibriSpeechbenchmark and analyze the effect of the individual components. Furthermore, weanalyze the learned filters and show that the most important information forthe ASR system is obtained by a set of bandpass filters.</description><author>Peter Vieting, Ralf Schlüter, Hermann Ney</author><pubDate>Tue, 08 Aug 2023 15:29:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04286v1</guid></item><item><title>Vision-Based Autonomous Navigation for Unmanned Surface Vessel in Extreme Marine Conditions</title><link>http://arxiv.org/abs/2308.04283v1</link><description>Visual perception is an important component for autonomous navigation ofunmanned surface vessels (USV), particularly for the tasks related toautonomous inspection and tracking. These tasks involve vision-based navigationtechniques to identify the target for navigation. Reduced visibility underextreme weather conditions in marine environments makes it difficult forvision-based approaches to work properly. To overcome these issues, this paperpresents an autonomous vision-based navigation framework for tracking targetobjects in extreme marine conditions. The proposed framework consists of anintegrated perception pipeline that uses a generative adversarial network (GAN)to remove noise and highlight the object features before passing them to theobject detector (i.e., YOLOv5). The detected visual features are then used bythe USV to track the target. The proposed framework has been thoroughly testedin simulation under extremely reduced visibility due to sandstorms and fog. Theresults are compared with state-of-the-art de-hazing methods across thebenchmarked MBZIRC simulation dataset, on which the proposed scheme hasoutperformed the existing methods across various metrics.</description><author>Muhayyuddin Ahmed, Ahsan Baidar Bakht, Taimur Hassan, Waseem Akram, Ahmed Humais, Lakmal Seneviratne, Shaoming He, Defu Lin, Irfan Hussain</author><pubDate>Tue, 08 Aug 2023 15:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04283v1</guid></item><item><title>In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning</title><link>http://arxiv.org/abs/2308.04275v1</link><description>In this note, we explore inference-time alignment through in-contextlearning. We consider a vanilla pretrained language model Llama-2 before anyfine-tuning and retrieve an average of 9 demonstration alignment examples whenthe model is prompted to follow chat-style instructions. Compared to directprompting, the in-context alignment without changing model weights leads to a7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, makingthe vanilla language model comparable to strong baselines with alignmentfine-tuning.</description><author>Xiaochuang Han</author><pubDate>Tue, 08 Aug 2023 15:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04275v1</guid></item><item><title>Machine learning for rapid discovery of laminar flow channel wall modifications that enhance heat transfer</title><link>http://arxiv.org/abs/2101.08130v2</link><description>Numerical simulation of fluids plays an essential role in modeling manyphysical phenomena, which enables technological advancements, contributes tosustainable practices, and expands our understanding of various natural andengineered systems. The calculation of heat transfer in fluid flow in simpleflat channels is a relatively easy task for various simulation methods.However, once the channel geometry becomes more complex, numerical simulationsbecome a bottleneck in optimizing wall geometries. We present a combination ofaccurate numerical simulations of arbitrary, flat, and non-flat channels andmachine learning models predicting drag coefficient and Stanton number. We showthat convolutional neural networks (CNN) can accurately predict the targetproperties at a fraction of the time of numerical simulations. We use the CNNmodels in a virtual high-throughput screening approach to explore a largenumber of possible, randomly generated wall architectures. Data Augmentationwas applied to existing geometries data to add generated new training datawhich have the same number of parameters of heat transfer to improve themodel's generalization. The general approach is not only applicable to simpleflow setups as presented here but can be extended to more complex tasks, suchas multiphase or even reactive unit operations in chemical engineering.</description><author>Yuri Koide, Arjun J. Kaithakkal, Matthias Schniewind, Bradley P. Ladewig, Alexander Stroh, Pascal Friederich</author><pubDate>Tue, 08 Aug 2023 15:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2101.08130v2</guid></item><item><title>Lossy and Lossless (L$^2$) Post-training Model Size Compression</title><link>http://arxiv.org/abs/2308.04269v1</link><description>Deep neural networks have delivered remarkable performance and have beenwidely used in various visual tasks. However, their huge size causessignificant inconvenience for transmission and storage. Many previous studieshave explored model size compression. However, these studies often approachvarious lossy and lossless compression methods in isolation, leading tochallenges in achieving high compression ratios efficiently. This work proposesa post-training model size compression method that combines lossy and losslesscompression in a unified way. We first propose a unified parametric weighttransformation, which ensures different lossy compression methods can beperformed jointly in a post-training manner. Then, a dedicated differentiablecounter is introduced to guide the optimization of lossy compression to arriveat a more suitable point for later lossless compression. Additionally, ourmethod can easily control a desired global compression ratio and allocateadaptive ratios for different layers. Finally, our method can achieve a stable$10\times$ compression ratio without sacrificing accuracy and a $20\times$compression ratio with minor accuracy loss in a short time. Our code isavailable at https://github.com/ModelTC/L2_Compression .</description><author>Yumeng Shi, Shihao Bai, Xiuying Wei, Ruihao Gong, Jianlei Yang</author><pubDate>Tue, 08 Aug 2023 15:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04269v1</guid></item><item><title>Teacher-Student Architecture for Knowledge Distillation: A Survey</title><link>http://arxiv.org/abs/2308.04268v1</link><description>Although Deep neural networks (DNNs) have shown a strong capacity to solvelarge-scale problems in many areas, such DNNs are hard to be deployed inreal-world systems due to their voluminous parameters. To tackle this issue,Teacher-Student architectures were proposed, where simple student networks witha few parameters can achieve comparable performance to deep teacher networkswith many parameters. Recently, Teacher-Student architectures have beeneffectively and widely embraced on various knowledge distillation (KD)objectives, including knowledge compression, knowledge expansion, knowledgeadaptation, and knowledge enhancement. With the help of Teacher-Studentarchitectures, current studies are able to achieve multiple distillationobjectives through lightweight and generalized student networks. Different fromexisting KD surveys that primarily focus on knowledge compression, this surveyfirst explores Teacher-Student architectures across multiple distillationobjectives. This survey presents an introduction to various knowledgerepresentations and their corresponding optimization objectives. Additionally,we provide a systematic overview of Teacher-Student architectures withrepresentative learning algorithms and effective distillation schemes. Thissurvey also summarizes recent applications of Teacher-Student architecturesacross multiple purposes, including classification, recognition, generation,ranking, and regression. Lastly, potential research directions in KD areinvestigated, focusing on architecture design, knowledge quality, andtheoretical studies of regression-based learning, respectively. Through thiscomprehensive survey, industry practitioners and the academic community cangain valuable insights and guidelines for effectively designing, learning, andapplying Teacher-Student architectures on various distillation objectives.</description><author>Chengming Hu, Xuan Li, Dan Liu, Haolun Wu, Xi Chen, Ju Wang, Xue Liu</author><pubDate>Tue, 08 Aug 2023 15:09:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04268v1</guid></item><item><title>FLIRT: Feedback Loop In-context Red Teaming</title><link>http://arxiv.org/abs/2308.04265v1</link><description>Warning: this paper contains content that may be inappropriate or offensive. As generative models become available for public use in various applications,testing and analyzing vulnerabilities of these models has become a priority.Here we propose an automatic red teaming framework that evaluates a given modeland exposes its vulnerabilities against unsafe and inappropriate contentgeneration. Our framework uses in-context learning in a feedback loop to redteam models and trigger them into unsafe content generation. We proposedifferent in-context attack strategies to automatically learn effective anddiverse adversarial prompts for text-to-image models. Our experimentsdemonstrate that compared to baseline approaches, our proposed strategy issignificantly more effective in exposing vulnerabilities in Stable Diffusion(SD) model, even when the latter is enhanced with safety features. Furthermore,we demonstrate that the proposed framework is effective for red teamingtext-to-text models, resulting in significantly higher toxic responsegeneration rate compared to previously reported numbers.</description><author>Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta</author><pubDate>Tue, 08 Aug 2023 15:03:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04265v1</guid></item><item><title>BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning</title><link>http://arxiv.org/abs/2308.04263v1</link><description>This paper introduces BarlowRL, a data-efficient reinforcement learning agentthat combines the Barlow Twins self-supervised learning framework with DER(Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and itscontrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoidsdimensional collapse by enforcing information spread to the whole space. Thishelps RL algorithms to utilize uniformly spread state representation thateventually results in a remarkable performance. The integration of Barlow Twinswith DER enhances data efficiency and achieves superior performance in the RLtasks. BarlowRL demonstrates the potential of incorporating self-supervisedlearning techniques to improve RL algorithms.</description><author>Omer Veysel Cagatan</author><pubDate>Tue, 08 Aug 2023 14:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04263v1</guid></item><item><title>SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction</title><link>http://arxiv.org/abs/2308.04262v1</link><description>Transformers have emerged as viable alternatives to convolutional neuralnetworks owing to their ability to learn non-local region relationships in thespatial domain. The self-attention mechanism of the transformer enablestransformers to capture long-range dependencies in the images, which might bedesirable for accelerated MRI image reconstruction as the effect ofundersampling is non-local in the image domain. Despite its computationalefficiency, the window-based transformers suffer from restricted receptivefields as the dependencies are limited to within the scope of the imagewindows. We propose a window-based transformer network that integrates dilatedattention mechanism and convolution for accelerated MRI image reconstruction.The proposed network consists of dilated and dense neighborhood attentiontransformers to enhance the distant neighborhood pixel relationship andintroduce depth-wise convolutions within the transformer module to learnlow-level translation invariant features for accelerated MRI imagereconstruction. The proposed model is trained in a self-supervised manner. Weperform extensive experiments for multi-coil MRI acceleration for coronal PD,coronal PDFS and axial T2 contrasts with 4x and 5x under-sampling inself-supervised learning based on k-space splitting. We compare our methodagainst other reconstruction architectures and the parallel domainself-supervised learning baseline. Results show that the proposed modelexhibits improvement margins of (i) around 1.40 dB in PSNR and around 0.028 inSSIM on average over other architectures (ii) around 1.44 dB in PSNR and around0.029 in SSIM over parallel domain self-supervised learning. The code isavailable at https://github.com/rahul-gs-16/sdlformer.git</description><author>Rahul G. S., Sriprabha Ramnarayanan, Mohammad Al Fahim, Keerthi Ram, Preejith S. P, Mohanasankar Sivaprakasam</author><pubDate>Tue, 08 Aug 2023 14:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04262v1</guid></item><item><title>Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets</title><link>http://arxiv.org/abs/2308.04258v1</link><description>This work presents a text-to-audio-retrieval system based on pre-trained textand spectrogram transformers. Our method projects recordings and textualdescriptions into a shared audio-caption space in which related examples fromdifferent modalities are close. Through a systematic analysis, we examine howeach component of the system influences retrieval performance. As a result, weidentify two key components that play a crucial role in driving performance:the self-attention-based audio encoder for audio embedding and the utilizationof additional human-generated and synthetic data sets during pre-training. Wefurther experimented with augmenting ClothoV2 captions with available keywordsto increase their variety; however, this only led to marginal improvements. Oursystem ranked first in the 2023's DCASE Challenge, and it outperforms thecurrent state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.</description><author>Paul Primus, Khaled Koutini, Gerhard Widmer</author><pubDate>Tue, 08 Aug 2023 14:46:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04258v1</guid></item><item><title>Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts</title><link>http://arxiv.org/abs/2307.11661v2</link><description>Contrastive pretrained large Vision-Language Models (VLMs) like CLIP haverevolutionized visual representation learning by providing good performance ondownstream datasets. VLMs are 0-shot adapted to a downstream dataset bydesigning prompts that are relevant to the dataset. Such prompt engineeringmakes use of domain expertise and a validation dataset. Meanwhile, recentdevelopments in generative pretrained models like GPT-4 mean they can be usedas advanced internet search tools. They can also be manipulated to providevisual information in any structure. In this work, we show that GPT-4 can beused to generate text that is visually descriptive and how this can be used toadapt CLIP to downstream tasks. We show considerable improvements in 0-shottransfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.We also design a simple few-shot adapter that learns to choose the bestpossible sentences to construct generalizable classifiers that outperform therecently proposed CoCoOP by ~2% on average and by over 4% on 4 specializedfine-grained datasets. The code, prompts, and auxiliary text dataset isavailable at https://github.com/mayug/VDT-Adapter.</description><author>Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuinness, Noel E. O'Connor</author><pubDate>Tue, 08 Aug 2023 14:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11661v2</guid></item><item><title>CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages</title><link>http://arxiv.org/abs/2308.04255v1</link><description>We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation ofthe South Slavic languages, which is based on the Stanza natural languageprocessing pipeline. We describe the main improvements in CLASSLA-Stanza withrespect to Stanza, and give a detailed description of the model trainingprocess for the latest 2.1 release of the pipeline. We also report performancescores produced by the pipeline for different languages and varieties.CLASSLA-Stanza exhibits consistently high performance across all the supportedlanguages and outperforms or expands its parent pipeline Stanza at all thesupported tasks. We also present the pipeline's new functionality enablingefficient processing of web data and the reasons that led to itsimplementation.</description><author>Luka Terčon, Nikola Ljubešić</author><pubDate>Tue, 08 Aug 2023 14:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04255v1</guid></item><item><title>Blur aware metric depth estimation with multi-focus plenoptic cameras</title><link>http://arxiv.org/abs/2308.04252v1</link><description>While a traditional camera only captures one point of view of a scene, aplenoptic or light-field camera, is able to capture spatial and angularinformation in a single snapshot, enabling depth estimation from a singleacquisition. In this paper, we present a new metric depth estimation algorithmusing only raw images from a multi-focus plenoptic camera. The proposedapproach is especially suited for the multi-focus configuration where severalmicro-lenses with different focal lengths are used. The main goal of our bluraware depth estimation (BLADE) approach is to improve disparity estimation fordefocus stereo images by integrating both correspondence and defocus cues. Wethus leverage blur information where it was previously considered a drawback.We explicitly derive an inverse projection model including the defocus blurproviding depth estimates up to a scale factor. A method to calibrate theinverse model is then proposed. We thus take into account depth scaling toachieve precise and accurate metric depth estimates. Our results show thatintroducing defocus cues improves the depth estimation. We demonstrate theeffectiveness of our framework and depth scaling calibration on relative depthestimation setups and on real-world 3D complex scenes with ground truthacquired with a 3D lidar scanner.</description><author>Mathieu Labussière, Céline Teulière, Omar Ait-Aider</author><pubDate>Tue, 08 Aug 2023 14:38:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04252v1</guid></item><item><title>MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion</title><link>http://arxiv.org/abs/2308.04249v1</link><description>Reconstructing visual stimuli from brain recordings has been a meaningful andchallenging task. Especially, the achievement of precise and controllable imagereconstruction bears great significance in propelling the progress andutilization of brain-computer interfaces. Despite the advancements in compleximage reconstruction techniques, the challenge persists in achieving a cohesivealignment of both semantic (concepts and objects) and structure (position,orientation, and size) with the image stimuli. To address the aforementionedissue, we propose a two-stage image reconstruction model called MindDiffuser.In Stage 1, the VQ-VAE latent representations and the CLIP text embeddingsdecoded from fMRI are put into Stable Diffusion, which yields a preliminaryimage that contains semantic information. In Stage 2, we utilize the CLIPvisual feature decoded from fMRI as supervisory information, and continuallyadjust the two feature vectors decoded in Stage 1 through backpropagation toalign the structural information. The results of both qualitative andquantitative analyses demonstrate that our model has surpassed the currentstate-of-the-art models on Natural Scenes Dataset (NSD). The subsequentexperimental findings corroborate the neurobiological plausibility of themodel, as evidenced by the interpretability of the multimodal feature employed,which align with the corresponding brain responses.</description><author>Yizhuo Lu, Changde Du, Qiongyi zhou, Dianpeng Wang, Huiguang He</author><pubDate>Tue, 08 Aug 2023 14:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04249v1</guid></item><item><title>A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends</title><link>http://arxiv.org/abs/2302.03512v3</link><description>As more and more Arabic texts emerged on the Internet, extracting importantinformation from these Arabic texts is especially useful. As a fundamentaltechnology, Named entity recognition (NER) serves as the core component ininformation extraction technology, while also playing a critical role in manyother Natural Language Processing (NLP) systems, such as question answering andknowledge graph building. In this paper, we provide a comprehensive review ofthe development of Arabic NER, especially the recent advances in deep learningand pre-trained language model. Specifically, we first introduce the backgroundof Arabic NER, including the characteristics of Arabic and existing resourcesfor Arabic NER. Then, we systematically review the development of Arabic NERmethods. Traditional Arabic NER systems focus on feature engineering anddesigning domain-specific rules. In recent years, deep learning methods achievesignificant progress by representing texts via continuous vectorrepresentations. With the growth of pre-trained language model, Arabic NERyields better performance. Finally, we conclude the method gap between ArabicNER and NER methods from other languages, which helps outline future directionsfor Arabic NER.</description><author>Xiaoye Qu, Yingjie Gu, Qingrong Xia, Zechang Li, Zhefeng Wang, Baoxing Huai</author><pubDate>Tue, 08 Aug 2023 14:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03512v3</guid></item><item><title>Gloss Alignment Using Word Embeddings</title><link>http://arxiv.org/abs/2308.04248v1</link><description>Capturing and annotating Sign language datasets is a time consuming andcostly process. Current datasets are orders of magnitude too small tosuccessfully train unconstrained \acf{slt} models. As a result, research hasturned to TV broadcast content as a source of large-scale training data,consisting of both the sign language interpreter and the associated audiosubtitle. However, lack of sign language annotation limits the usability ofthis data and has led to the development of automatic annotation techniquessuch as sign spotting. These spottings are aligned to the video rather than thesubtitle, which often results in a misalignment between the subtitle andspotted signs. In this paper we propose a method for aligning spottings withtheir corresponding subtitles using large spoken language models. Using asingle modality means our method is computationally inexpensive and can beutilized in conjunction with existing alignment techniques. We quantitativelydemonstrate the effectiveness of our method on the \acf{mdgs} and \acf{bobsl}datasets, recovering up to a 33.22 BLEU-1 score in word alignment.</description><author>Harry Walsh, Ozge Mercanoglu Sincan, Ben Saunders, Richard Bowden</author><pubDate>Tue, 08 Aug 2023 14:26:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04248v1</guid></item><item><title>AICSD: Adaptive Inter-Class Similarity Distillation for Semantic Segmentation</title><link>http://arxiv.org/abs/2308.04243v1</link><description>In recent years, deep neural networks have achieved remarkable accuracy incomputer vision tasks. With inference time being a crucial factor, particularlyin dense prediction tasks such as semantic segmentation, knowledge distillationhas emerged as a successful technique for improving the accuracy of lightweightstudent networks. The existing methods often neglect the information inchannels and among different classes. To overcome these limitations, this paperproposes a novel method called Inter-Class Similarity Distillation (ICSD) forthe purpose of knowledge distillation. The proposed method transfers high-orderrelations from the teacher network to the student network by independentlycomputing intra-class distributions for each class from network outputs. Thisis followed by calculating inter-class similarity matrices for distillationusing KL divergence between distributions of each pair of classes. To furtherimprove the effectiveness of the proposed method, an Adaptive Loss Weighting(ALW) training strategy is proposed. Unlike existing methods, the ALW strategygradually reduces the influence of the teacher network towards the end oftraining process to account for errors in teacher's predictions. Extensiveexperiments conducted on two well-known datasets for semantic segmentation,Cityscapes and Pascal VOC 2012, validate the effectiveness of the proposedmethod in terms of mIoU and pixel accuracy. The proposed method outperformsmost of existing knowledge distillation methods as demonstrated by bothquantitative and qualitative evaluations. Code is available at:https://github.com/AmirMansurian/AICSD</description><author>Amir M. Mansourian, Rozhan Ahmadi, Shohreh Kasaei</author><pubDate>Tue, 08 Aug 2023 14:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04243v1</guid></item><item><title>AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks</title><link>http://arxiv.org/abs/2307.09724v3</link><description>To deliver the artistic expression of the target style, recent studiesexploit the attention mechanism owing to its ability to map the local patchesof the style image to the corresponding patches of the content image. However,because of the low semantic correspondence between arbitrary content andartworks, the attention module repeatedly abuses specific local patches fromthe style image, resulting in disharmonious and evident repetitive artifacts.To overcome this limitation and accomplish impeccable artistic style transfer,we focus on enhancing the attention mechanism and capturing the rhythm ofpatterns that organize the style. In this paper, we introduce a novel metric,namely pattern repeatability, that quantifies the repetition of patterns in thestyle image. Based on the pattern repeatability, we propose AestheticPattern-Aware style transfer Networks (AesPA-Net) that discover the sweet spotof local and global style expressions. In addition, we propose a novelself-supervisory task to encourage the attention mechanism to learn precise andmeaningful semantic correspondence. Lastly, we introduce the patch-wise styleloss to transfer the elaborate rhythm of local patterns. Through qualitativeand quantitative evaluations, we verify the reliability of the proposed patternrepeatability that aligns with human perception, and demonstrate thesuperiority of the proposed framework.</description><author>Kibeom Hong, Seogkyu Jeon, Junsoo Lee, Namhyuk Ahn, Kunhee Kim, Pilhyeon Lee, Daesik Kim, Youngjung Uh, Hyeran Byun</author><pubDate>Tue, 08 Aug 2023 14:14:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09724v3</guid></item><item><title>AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models</title><link>http://arxiv.org/abs/2308.04241v1</link><description>The product carbon footprint (PCF) is crucial for decarbonizing the supplychain, as it measures the direct and indirect greenhouse gas emissions causedby all activities during the product's life cycle. However, PCF accountingoften requires expert knowledge and significant time to construct life cyclemodels. In this study, we test and compare the emergent ability of five largelanguage models (LLMs) in modeling the 'cradle-to-gate' life cycles of productsand generating the inventory data of inputs and outputs, revealing theirlimitations as a generalized PCF knowledge database. By utilizing LLMs, wepropose an automatic AI-driven PCF accounting framework, called AutoPCF, whichalso applies deep learning algorithms to automatically match calculationparameters, and ultimately calculate the PCF. The results of estimating thecarbon footprint for three case products using the AutoPCF frameworkdemonstrate its potential in achieving automatic modeling and estimation of PCFwith a large reduction in modeling time from days to minutes.</description><author>Zhu Deng, Jinjie Liu, Biao Luo, Can Yuan, Qingrun Yang, Lei Xiao, Wenwen Zhou, Zhu Liu</author><pubDate>Tue, 08 Aug 2023 14:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04241v1</guid></item><item><title>Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction</title><link>http://arxiv.org/abs/2308.04237v1</link><description>Consider a setting in which devices and a server share a pre-trained model.The server wishes to make an inference on a new input given the model. Deviceshave access to data, previously not used for training, and can communicate tothe server over a common wireless channel. If the devices have no access to thenew input, can communication from devices to the server enhance the quality ofthe inference decision at the server? Recent work has introduced federatedconformal prediction (CP), which leverages devices-to-server communication toimprove the reliability of the server's decision. With federated CP, devicescommunicate to the server information about the loss accrued by the sharedpre-trained model on the local data, and the server leverages this informationto calibrate a decision interval, or set, so that it is guaranteed to containthe correct answer with a pre-defined target reliability level. Previous workassumed noise-free communication, whereby devices can communicate a single realnumber to the server. In this paper, we study for the first time federated CPin a wireless setting. We introduce a novel protocol, termed wireless federatedconformal prediction (WFCP), which builds on type-based multiple access (TBMA)and on a novel quantile correction strategy. WFCP is proved to provide formalreliability guarantees in terms of coverage of the predicted set produced bythe server. Using numerical results, we demonstrate the significant advantagesof WFCP against digital implementations of existing federated CP schemes,especially in regimes with limited communication resources and/or large numberof devices.</description><author>Meiyi Zhu, Matteo Zecchin, Sangwoo Park, Caili Guo, Chunyan Feng, Osvaldo Simeone</author><pubDate>Tue, 08 Aug 2023 14:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04237v1</guid></item><item><title>Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach</title><link>http://arxiv.org/abs/2304.08134v3</link><description>Nowadays, face recognition systems surpass human performance on severaldatasets. However, there are still edge cases that the machine can't correctlyclassify. This paper investigates the effect of a combination of machine andhuman operators in the face verification task. First, we look closer at theedge cases for several state-of-the-art models to discover common datasets'challenging settings. Then, we conduct a study with 60 participants on theseselected tasks with humans and provide an extensive analysis. Finally, wedemonstrate that combining machine and human decisions can further improve theperformance of state-of-the-art face verification systems on various benchmarkdatasets. Code and data are publicly available on GitHub.</description><author>Martin Knoche, Gerhard Rigoll</author><pubDate>Tue, 08 Aug 2023 13:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08134v3</guid></item><item><title>Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond</title><link>http://arxiv.org/abs/2306.09841v3</link><description>Logical reasoning consistently plays a fundamental and significant role inthe domains of knowledge engineering and artificial intelligence. Recently,Large Language Models (LLMs) have emerged as a noteworthy innovation in naturallanguage processing (NLP), exhibiting impressive achievements across variousclassic NLP tasks. However, the question of whether LLMs can effectivelyaddress the task of logical reasoning, which requires gradual cognitiveinference similar to human intelligence, remains unanswered. To this end, weaim to bridge this gap and provide comprehensive evaluations in this paper.Firstly, to offer systematic evaluations, we select fifteen typical logicalreasoning datasets and organize them into deductive, inductive, abductive andmixed-form reasoning settings. Considering the comprehensiveness ofevaluations, we include three representative LLMs (i.e., text-davinci-003,ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,one-shot and three-shot settings. Secondly, different from previous evaluationsrelying only on simple metrics (e.g., accuracy), we propose fine-levelevaluations from objective and subjective manners, covering both answers andexplanations. Additionally, to uncover the logical flaws of LLMs, problematiccases will be attributed to five error types from two dimensions, i.e.,evidence selection process and reasoning process. Thirdly, to avoid theinfluences of knowledge bias and purely focus on benchmarking the logicalreasoning capability of LLMs, we propose a new dataset with neutral content. Itcontains 3,000 samples and covers deductive, inductive and abductive settings.Based on the in-depth evaluations, this paper finally forms a generalevaluation scheme of logical reasoning capability from six dimensions. Itreflects the pros and cons of LLMs and gives guiding directions for futureworks.</description><author>Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, Erik Cambria</author><pubDate>Tue, 08 Aug 2023 13:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09841v3</guid></item><item><title>A Comparative Study of Image-to-Image Translation Using GANs for Synthetic Child Race Data</title><link>http://arxiv.org/abs/2308.04232v1</link><description>The lack of ethnic diversity in data has been a limiting factor of facerecognition techniques in the literature. This is particularly the case forchildren where data samples are scarce and presents a challenge when seeking toadapt machine vision algorithms that are trained on adult data to work onchildren. This work proposes the utilization of image-to-image transformationto synthesize data of different races and thus adjust the ethnicity ofchildren's face data. We consider ethnicity as a style and compare threedifferent Image-to-Image neural network based methods, specifically pix2pix,CycleGAN, and CUT networks to implement Caucasian child data and Asian childdata conversion. Experimental validation results on synthetic data demonstratethe feasibility of using image-to-image transformation methods to generatevarious synthetic child data samples with broader ethnic diversity.</description><author>Wang Yao, Muhammad Ali Farooq, Joseph Lemley, Peter Corcoran</author><pubDate>Tue, 08 Aug 2023 13:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04232v1</guid></item><item><title>A semantic backdoor attack against Graph Convolutional Networks</title><link>http://arxiv.org/abs/2302.14353v3</link><description>Graph convolutional networks (GCNs) have been very effective in addressingthe issue of various graph-structured related tasks, such as nodeclassification and graph classification. However, recent research has shownthat GCNs are vulnerable to a new type of threat called a backdoor attack,where the adversary can inject a hidden backdoor into GCNs so that the attackedmodel performs well on benign samples, but its prediction will be maliciouslychanged to the attacker-specified target label if the hidden backdoor isactivated by the attacker-defined trigger. In this paper, we investigatewhether such semantic backdoor attacks are possible for GCNs and propose asemantic backdoor attack against GCNs (SBAG) under the context of graphclassification to reveal the existence of this security vulnerability in GCNs.SBAG uses a certain type of node in the samples as a backdoor trigger andinjects a hidden backdoor into GCN models by poisoning training data. Thebackdoor will be activated, and the GCN models will give maliciousclassification results specified by the attacker even on unmodified samples aslong as the samples contain enough trigger nodes. We evaluate SBAG on fourgraph datasets. The experimental results indicate that SBAG can achieve attacksuccess rates of approximately 99.9% and over 82% for two kinds of attacksamples, respectively, with poisoning rates of less than 5%.</description><author>Jiazhu Dai, Zhipeng Xiong</author><pubDate>Tue, 08 Aug 2023 13:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14353v3</guid></item><item><title>OpinionConv: Conversational Product Search with Grounded Opinions</title><link>http://arxiv.org/abs/2308.04226v1</link><description>When searching for products, the opinions of others play an important role inmaking informed decisions. Subjective experiences about a product can be avaluable source of information. This is also true in sales conversations, wherea customer and a sales assistant exchange facts and opinions about products.However, training an AI for such conversations is complicated by the fact thatlanguage models do not possess authentic opinions for their lack of real-worldexperience. We address this problem by leveraging product reviews as a richsource of product opinions to ground conversational AI in true subjectivenarratives. With OpinionConv, we develop the first conversational AI forsimulating sales conversations. To validate the generated conversations, weconduct several user studies showing that the generated opinions are perceivedas realistic. Our assessors also confirm the importance of opinions as aninformative basis for decision-making.</description><author>Vahid Sadiri Javadi, Martin Potthast, Lucie Flek</author><pubDate>Tue, 08 Aug 2023 13:45:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04226v1</guid></item><item><title>Will your Doorbell Camera still recognize you as you grow old</title><link>http://arxiv.org/abs/2308.04224v1</link><description>Robust authentication for low-power consumer devices such as doorbell camerasposes a valuable and unique challenge. This work explores the effect of age andaging on the performance of facial authentication methods. Two public agedatasets, AgeDB and Morph-II have been used as baselines in this work. Aphoto-realistic age transformation method has been employed to augment a set ofhigh-quality facial images with various age effects. Then the effect of thesesynthetic aging data on the high-performance deep-learning-based facerecognition model is quantified by using various metrics including ReceiverOperating Characteristic (ROC) curves and match score distributions.Experimental results demonstrate that long-term age effects are still asignificant challenge for the state-of-the-art facial authentication method.</description><author>Wang Yao, Muhammad Ali Farooq, Joseph Lemley, Peter Corcoran</author><pubDate>Tue, 08 Aug 2023 13:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04224v1</guid></item><item><title>Real-Time Progressive Learning: Mutually Reinforcing Learning and Control with Neural-Network-Based Selective Memory</title><link>http://arxiv.org/abs/2308.04223v1</link><description>Memory, as the basis of learning, determines the storage, update andforgetting of the knowledge and further determines the efficiency of learning.Featured with a mechanism of memory, a radial basis function neural network(RBFNN) based learning control scheme named real-time progressive learning(RTPL) is proposed to learn the unknown dynamics of the system with guaranteedstability and closed-loop performance. Instead of the stochastic gradientdescent (SGD) update law in adaptive neural control (ANC), RTPL adopts theselective memory recursive least squares (SMRLS) algorithm to update theweights of the RBFNN. Through SMRLS, the approximation capabilities of theRBFNN are uniformly distributed over the feature space and thus the passiveknowledge forgetting phenomenon of SGD method is suppressed. Subsequently, RTPLachieves the following merits over the classical ANC: 1) guaranteed learningcapability under low-level persistent excitation (PE), 2) improved learningperformance (learning speed, accuracy and generalization capability), and 3)low gain requirement ensuring robustness of RTPL in practical applications.Moreover, the RTPL based learning and control will gradually reinforce eachother during the task execution, making it appropriate for long-term learningcontrol tasks. As an example, RTPL is used to address the tracking controlproblem of a class of nonlinear systems with RBFNN being an adaptivefeedforward controller. Corresponding theoretical analysis and simulationstudies demonstrate the effectiveness of RTPL.</description><author>Yiming Fei, Jiangang Li, Yanan Li</author><pubDate>Tue, 08 Aug 2023 13:39:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04223v1</guid></item><item><title>Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models</title><link>http://arxiv.org/abs/2308.04220v1</link><description>In this work, we propose a methodology for investigating the application ofsemantic attention to enhance the explainability of Graph Neural Network(GNN)-based models, introducing semantically-informed perturbations andestablishing a correlation between predicted feature-importance weights andmodel accuracy. Graph Deep Learning (GDL) has emerged as a promising field fortasks like scene interpretation, leveraging flexible graph structures toconcisely describe complex features and relationships. As traditionalexplainability methods used in eXplainable AI (XAI) cannot be directly appliedto such structures, graph-specific approaches are introduced. Attentionmechanisms have demonstrated their efficacy in estimating the importance ofinput features in deep learning models and thus have been previously employedto provide feature-based explanations for GNN predictions. Building upon theseinsights, we extend existing attention-based graph-explainability methodsinvestigating the use of attention weights as importance indicators ofsemantically sorted feature sets. Through analysing the behaviour of predictedattention-weights distribution in correlation with model accuracy, we gainvaluable insights into feature importance with respect to the behaviour of theGNN model. We apply our methodology to a lidar pointcloud estimation modelsuccessfully identifying key semantic classes that contribute to enhancedperformance effectively generating reliable post-hoc semantic explanations.</description><author>Efimia Panagiotaki, Daniele De Martini, Lars Kunze</author><pubDate>Tue, 08 Aug 2023 13:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04220v1</guid></item><item><title>AquaSAM: Underwater Image Foreground Segmentation</title><link>http://arxiv.org/abs/2308.04218v1</link><description>The Segment Anything Model (SAM) has revolutionized natural imagesegmentation, nevertheless, its performance on underwater images is stillrestricted. This work presents AquaSAM, the first attempt to extend the successof SAM on underwater images with the purpose of creating a versatile method forthe segmentation of various underwater targets. To achieve this, we begin byclassifying and extracting various labels automatically in SUIM dataset.Subsequently, we develop a straightforward fine-tuning method to adapt SAM togeneral foreground underwater image segmentation. Through extensive experimentsinvolving eight segmentation tasks like human divers, we demonstrate thatAquaSAM outperforms the default SAM model especially at hard tasks like coralreefs. AquaSAM achieves an average Dice Similarity Coefficient (DSC) of 7.13(%) improvement and an average of 8.27 (%) on mIoU improvement in underwatersegmentation tasks.</description><author>Muduo Xu, Jianhao Su, Yutao Liu</author><pubDate>Tue, 08 Aug 2023 13:30:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04218v1</guid></item><item><title>Selective Memory Recursive Least Squares: Recast Forgetting into Memory in RBF Neural Network Based Real-Time Learning</title><link>http://arxiv.org/abs/2211.07909v2</link><description>In radial basis function neural network (RBFNN) based real-time learningtasks, forgetting mechanisms are widely used such that the neural network cankeep its sensitivity to new data. However, with forgetting mechanisms, someuseful knowledge will get lost simply because they are learned a long time ago,which we refer to as the passive knowledge forgetting phenomenon. To addressthis problem, this paper proposes a real-time training method named selectivememory recursive least squares (SMRLS) in which the classical forgettingmechanisms are recast into a memory mechanism. Different from the forgettingmechanism, which mainly evaluates the importance of samples according to thetime when samples are collected, the memory mechanism evaluates the importanceof samples through both temporal and spatial distribution of samples. WithSMRLS, the input space of the RBFNN is evenly divided into a finite number ofpartitions and a synthesized objective function is developed using synthesizedsamples from each partition. In addition to the current approximation error,the neural network also updates its weights according to the recorded data fromthe partition being visited. Compared with classical training methods includingthe forgetting factor recursive least squares (FFRLS) and stochastic gradientdescent (SGD) methods, SMRLS achieves improved learning speed andgeneralization capability, which are demonstrated by corresponding simulationresults.</description><author>Yiming Fei, Jiangang Li, Yanan Li</author><pubDate>Tue, 08 Aug 2023 13:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07909v2</guid></item></channel></rss>