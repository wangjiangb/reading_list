<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 03 Oct 2023 06:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Construction numbers: How to build a graph?</title><link>http://arxiv.org/abs/2302.13186v3</link><description>Counting the number of linear extensions of a partial order was considered byStanley about 50 years ago. For the partial order on the vertices and edges ofa graph determined by inclusion, we call such linear extensions {\itconstruction sequences} for the graph as each edge follows both of itsendpoints. The number of such sequences for paths, cycles, stars, double-stars,and complete graphs is found. For paths, we agree with Stanley (the Tangentnumbers) and get formulas for the other classes. Structure and applications arealso studied.</description><author>Paul C. Kainen</author><pubDate>Mon, 02 Oct 2023 18:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13186v3</guid></item><item><title>Vision-Language Dataset Distillation</title><link>http://arxiv.org/abs/2308.07545v2</link><description>Dataset distillation methods promise to reduce large-scale datasets down tosignificantly smaller sets of (potentially synthetic) training examples, whichpreserve sufficient information for training a new model from scratch. So far,dataset distillation methods have been developed for image classification.However, with the rise in capabilities of vision-language models (VLMs), andespecially given the scale of datasets necessary to train these models, thetime is ripe to expand dataset distillation methods beyond imageclassification. In this work, we take the first steps towards this goal byexpanding the idea of trajectory matching to create a distillation method forvision-language datasets. A key challenge is that vision-language datasets donot have a set of discrete classes. To overcome this, our proposedvision-language dataset distillation method jointly distills the image-textpairs in a contrastive formulation. Since there are no existing baselines, wecompare our approach to three coreset selection methods (strategic subsamplingof the training dataset), which we adapt to the vision-language setting. Wedemonstrate significant improvements on the challenging Flickr30K and COCOretrieval benchmarks: for example, on Flickr30K, the best coreset selectionmethod selecting 1000 image-text pairs for training achieves only 5.6%image-to-text retrieval accuracy (i.e., recall@1); in contrast, our datasetdistillation approach almost doubles that to 9.9% with just 100 (an order ofmagnitude fewer) training pairs.</description><author>Xindi Wu, Byron Zhang, Zhiwei Deng, Olga Russakovsky</author><pubDate>Mon, 02 Oct 2023 18:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07545v2</guid></item><item><title>ICML 2023 Topological Deep Learning Challenge : Design and Results</title><link>http://arxiv.org/abs/2309.15188v2</link><description>This paper presents the computational challenge on topological deep learningthat was hosted within the ICML 2023 Workshop on Topology and Geometry inMachine Learning. The competition asked participants to provide open-sourceimplementations of topological neural networks from the literature bycontributing to the python packages TopoNetX (data processing) and TopoModelX(deep learning). The challenge attracted twenty-eight qualifying submissions inits two-month duration. This paper describes the design of the challenge andsummarizes its main findings.</description><author>Mathilde Papillon, Mustafa Hajij, Florian Frantzen, Josef Hoppe, Helen Jenne, Johan Mathe, Audun Myers, Theodore Papamarkou, Michael T. Schaub, Ghada Zamzmi, Tolga Birdal, Tamal Dey, Tim Doster, Tegan Emerson, Gurusankar Gopalakrishnan, Devendra Govil, Vincent Grande, Aldo Guzmán-Sáenz, Henry Kvinge, Neal Livesay, Jan Meisner, Soham Mukherjee, Shreyas N. Samaga, Karthikeyan Natesan Ramamurthy, Maneel Reddy Karri, Paul Rosen, Sophia Sanborn, Michael Scholkemper, Robin Walters, Jens Agerberg, Georg Bökman, Sadrodin Barikbin, Claudio Battiloro, Gleb Bazhenov, Guillermo Bernardez, Aiden Brent, Sergio Escalera, Simone Fiorellino, Dmitrii Gavrilev, Mohammed Hassanin, Paul Häusner, Odin Hoff Gardaa, Abdelwahed Khamis, Manuel Lecha, German Magai, Tatiana Malygina, Pavlo Melnyk, Rubén Ballester, Ka</author><pubDate>Mon, 02 Oct 2023 18:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15188v2</guid></item><item><title>A Counterfactual Fair Model for Longitudinal Electronic Health Records via Deconfounder</title><link>http://arxiv.org/abs/2308.11819v3</link><description>The fairness issue of clinical data modeling, especially on Electronic HealthRecords (EHRs), is of utmost importance due to EHR's complex latent structureand potential selection bias. It is frequently necessary to mitigate healthdisparity while keeping the model's overall accuracy in practice. However,traditional methods often encounter the trade-off between accuracy andfairness, as they fail to capture the underlying factors beyond observed data.To tackle this challenge, we propose a novel model called Fair LongitudinalMedical Deconfounder (FLMD) that aims to achieve both fairness and accuracy inlongitudinal Electronic Health Records (EHR) modeling. Drawing inspiration fromthe deconfounder theory, FLMD employs a two-stage training process. In thefirst stage, FLMD captures unobserved confounders for each encounter, whicheffectively represents underlying medical factors beyond observed EHR, such aspatient genotypes and lifestyle habits. This unobserved confounder is crucialfor addressing the accuracy/fairness dilemma. In the second stage, FLMDcombines the learned latent representation with other relevant features to makepredictions. By incorporating appropriate fairness criteria, such ascounterfactual fairness, FLMD ensures that it maintains high predictionaccuracy while simultaneously minimizing health disparities. We conductedcomprehensive experiments on two real-world EHR datasets to demonstrate theeffectiveness of FLMD. Apart from the comparison of baseline methods and FLMDvariants in terms of fairness and accuracy, we assessed the performance of allmodels on disturbed/imbalanced and synthetic datasets to showcase thesuperiority of FLMD across different settings and provide valuable insightsinto its capabilities.</description><author>Zheng Liu, Xiaohan Li, Philip Yu</author><pubDate>Mon, 02 Oct 2023 18:46:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11819v3</guid></item><item><title>Efficient Deep Learning of Robust, Adaptive Policies using Tube MPC-Guided Data Augmentation</title><link>http://arxiv.org/abs/2303.15688v2</link><description>The deployment of agile autonomous systems in challenging, unstructuredenvironments requires adaptation capabilities and robustness to uncertainties.Existing robust and adaptive controllers, such as those based on modelpredictive control (MPC), can achieve impressive performance at the cost ofheavy online onboard computations. Strategies that efficiently learn robust andonboard-deployable policies from MPC have emerged, but they still lackfundamental adaptation capabilities. In this work, we extend an existingefficient Imitation Learning (IL) algorithm for robust policy learning from MPCwith the ability to learn policies that adapt to challenging model/environmentuncertainties. The key idea of our approach consists in modifying the ILprocedure by conditioning the policy on a learned lower-dimensionalmodel/environment representation that can be efficiently estimated online. Wetailor our approach to the task of learning an adaptive position and attitudecontrol policy to track trajectories under challenging disturbances on amultirotor. Evaluations in simulation show that a high-quality adaptive policycan be obtained in about $1.3$ hours. We additionally empirically demonstraterapid adaptation to in- and out-of-training-distribution uncertainties,achieving a $6.1$ cm average position error under wind disturbances thatcorrespond to about $50\%$ of the weight of the robot, and that are $36\%$larger than the maximum wind seen during training.</description><author>Tong Zhao, Andrea Tagliabue, Jonathan P. How</author><pubDate>Mon, 02 Oct 2023 18:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15688v2</guid></item><item><title>Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics</title><link>http://arxiv.org/abs/2309.06687v2</link><description>Although Deep Reinforcement Learning (DRL) has achieved notable success innumerous robotic applications, designing a high-performing reward functionremains a challenging task that often requires substantial manual input.Recently, Large Language Models (LLMs) have been extensively adopted to addresstasks demanding in-depth common-sense knowledge, such as reasoning andplanning. Recognizing that reward function design is also inherently linked tosuch knowledge, LLM offers a promising potential in this context. Motivated bythis, we propose in this work a novel LLM framework with a self-refinementmechanism for automated reward function design. The framework commences withthe LLM formulating an initial reward function based on natural languageinputs. Then, the performance of the reward function is assessed, and theresults are presented back to the LLM for guiding its self-refinement process.We examine the performance of our proposed framework through a variety ofcontinuous robotic control tasks across three diverse robotic systems. Theresults indicate that our LLM-designed reward functions are able to rival oreven surpass manually designed reward functions, highlighting the efficacy andapplicability of our approach.</description><author>Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan Shu, Lei Ma</author><pubDate>Mon, 02 Oct 2023 18:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06687v2</guid></item><item><title>ChemCrow: Augmenting large-language models with chemistry tools</title><link>http://arxiv.org/abs/2304.05376v5</link><description>Over the last decades, excellent computational chemistry tools have beendeveloped. Integrating them into a single platform with enhanced accessibilitycould help reaching their full potential by overcoming steep learning curves.Recently, large-language models (LLMs) have shown strong performance in tasksacross domains, but struggle with chemistry-related problems. Moreover, thesemodels lack access to external knowledge sources, limiting their usefulness inscientific applications. In this study, we introduce ChemCrow, an LLM chemistryagent designed to accomplish tasks across organic synthesis, drug discovery,and materials design. By integrating 18 expert-designed tools, ChemCrowaugments the LLM performance in chemistry, and new capabilities emerge. Ouragent autonomously planned and executed the syntheses of an insect repellent,three organocatalysts, and guided the discovery of a novel chromophore. Ourevaluation, including both LLM and expert assessments, demonstrates ChemCrow'seffectiveness in automating a diverse set of chemical tasks. Surprisingly, wefind that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4completions and Chemcrow's performance. Our work not only aids expert chemistsand lowers barriers for non-experts, but also fosters scientific advancement bybridging the gap between experimental and computational chemistry.</description><author>Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller</author><pubDate>Mon, 02 Oct 2023 18:03:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05376v5</guid></item><item><title>Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver</title><link>http://arxiv.org/abs/2301.01913v3</link><description>Constraint programming is known for being an efficient approach for solvingcombinatorial problems. Important design choices in a solver are the branchingheuristics, which are designed to lead the search to the best solutions in aminimum amount of time. However, developing these heuristics is atime-consuming process that requires problem-specific expertise. Thisobservation has motivated many efforts to use machine learning to automaticallylearn efficient heuristics without expert intervention. To the best of ourknowledge, it is still an open research question. Although several genericvariable-selection heuristics are available in the literature, the options fora generic value-selection heuristic are more scarce. In this paper, we proposeto tackle this issue by introducing a generic learning procedure that can beused to obtain a value-selection heuristic inside a constraint programmingsolver. This has been achieved thanks to the combination of a deep Q-learningalgorithm, a tailored reward signal, and a heterogeneous graph neural networkarchitecture. Experiments on graph coloring, maximum independent set, andmaximum cut problems show that our framework is able to find better solutionsclose to optimality without requiring a large amounts of backtracks while beinggeneric.</description><author>Tom Marty, Tristan François, Pierre Tessier, Louis Gauthier, Louis-Martin Rousseau, Quentin Cappart</author><pubDate>Mon, 02 Oct 2023 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.01913v3</guid></item><item><title>An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning</title><link>http://arxiv.org/abs/2305.15016v2</link><description>This paper proposes an unsupervised method that leverages topologicalcharacteristics of data manifolds to estimate class separability of the datawithout requiring labels. Experiments conducted in this paper on severaldatasets demonstrate a clear correlation and consistency between the classseparability estimated by the proposed method with supervised metrics likeFisher Discriminant Ratio~(FDR) and cross-validation of a classifier, whichboth require labels. This can enable implementing learning paradigms aimed atlearning from both labeled and unlabeled data, like semi-supervised andtransductive learning. This would be particularly useful when we have limitedlabeled data and a relatively large unlabeled dataset that can be used toenhance the learning process. The proposed method is implemented for languagemodel fine-tuning with automated stopping criterion by monitoring classseparability of the embedding-space manifold in an unsupervised setting. Theproposed methodology has been first validated on synthetic data, where theresults show a clear consistency between class separability estimated by theproposed method and class separability computed by FDR. The method has beenalso implemented on both public and internal data. The results show that theproposed method can effectively aid -- without the need for labels -- adecision on when to stop or continue the fine-tuning of a language model andwhich fine-tuning iteration is expected to achieve a maximum classificationperformance through quantification of the class separability of the embeddingmanifold.</description><author>Najah Ghalyan, Kostis Gourgoulias, Yash Satsangi, Sean Moran, Maxime Labonne, Joseph Sabelja</author><pubDate>Mon, 02 Oct 2023 17:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15016v2</guid></item><item><title>Sample-efficient Model-based Reinforcement Learning for Quantum Control</title><link>http://arxiv.org/abs/2304.09718v2</link><description>We propose a model-based reinforcement learning (RL) approach for noisytime-dependent gate optimization with improved sample complexity overmodel-free RL. Sample complexity is the number of controller interactions withthe physical system. Leveraging an inductive bias, inspired by recent advancesin neural ordinary differential equations (ODEs), we use an auto-differentiableODE parametrised by a learnable Hamiltonian ansatz to represent the modelapproximating the environment whose time-dependent part, including the control,is fully known. Control alongside Hamiltonian learning of continuoustime-independent parameters is addressed through interactions with the system.We demonstrate an order of magnitude advantage in the sample complexity of ourmethod over standard model-free RL in preparing some standard unitary gateswith closed and open system dynamics, in realistic numerical experimentsincorporating single shot measurements, arbitrary Hilbert space truncations anduncertainty in Hamiltonian parameters. Also, the learned Hamiltonian can beleveraged by existing control methods like GRAPE for further gradient-basedoptimization with the controllers found by RL as initializations. Our algorithmthat we apply on nitrogen vacancy (NV) centers and transmons in this paper iswell suited for controlling partially characterised one and two qubit systems.</description><author>Irtaza Khalid, Carrie A. Weidner, Edmond A. Jonckheere, Sophie G. Shermer, Frank C. Langbein</author><pubDate>Mon, 02 Oct 2023 17:50:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09718v2</guid></item><item><title>Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data</title><link>http://arxiv.org/abs/2309.13876v2</link><description>Pre-training speech models on large volumes of data has achieved remarkablesuccess. OpenAI Whisper is a multilingual multitask model trained on 680k hoursof supervised speech data. It generalizes well to various speech recognitionand translation benchmarks even in a zero-shot setup. However, the fullpipeline for developing such models (from data collection to training) is notpublicly accessible, which makes it difficult for researchers to furtherimprove its performance and address training-related issues such as efficiency,robustness, fairness, and bias. This work presents an Open Whisper-style SpeechModel (OWSM), which reproduces Whisper-style training using an open-sourcetoolkit and publicly available data. OWSM even supports more translationdirections and can be more efficient to train. We will publicly release allscripts used for data preparation, training, inference, and scoring as well aspre-trained models and training logs to promote open science.</description><author>Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji Watanabe</author><pubDate>Mon, 02 Oct 2023 02:10:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13876v2</guid></item><item><title>The minimax risk in testing the histogram of discrete distributions for uniformity under missing ball alternatives</title><link>http://arxiv.org/abs/2305.18111v4</link><description>We consider the problem of testing the fit of a sample of items from manycategories to the uniform distribution over the categories. As a class ofalternative hypotheses, we consider the removal of an $\ell_p$ ball of radius$\epsilon$ around the uniform rate sequence for $p \leq 2$. When the number ofsamples $n$ and number of categories $N$ go to infinity while $\epsilon$ goesto zero, the minimax risk $R_\epsilon^*$ in testing based on the sample'shistogram (number of absent categories, singletons, collisions, ...) asymptotesto $2\Phi(-n N^{2-2/p} \epsilon^2/\sqrt{8N})$, with $\Phi(x)$ the normal CDF.This characterization allows comparing the many estimators previously proposedfor this problem at the constant level rather than the rate of convergence oftheir risks. The minimax test mostly relies on collisions when $n/N$ is small,but otherwise behaves like the chisquared test. Empirical studies over a rangeof problem parameters show that this estimate is accurate in finite samples andthat our test is significantly better than the chisquared test or a test thatonly uses collisions. Our analysis relies on the asymptotic normality ofhistogram ordinates, the equivalence between the minimax setting and a Bayesiansetting, and the characterization of the least favorable prior by reducing amulti-dimensional optimization problem to a one-dimensional problem.</description><author>Alon Kipnis</author><pubDate>Mon, 02 Oct 2023 02:07:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18111v4</guid></item><item><title>Quasi-optimal Reinforcement Learning with Continuous Actions</title><link>http://arxiv.org/abs/2301.08940v2</link><description>Many real-world applications of reinforcement learning (RL) require makingdecisions in continuous action environments. In particular, determining theoptimal dose level plays a vital role in developing medical treatment regimes.One challenge in adapting existing RL algorithms to medical applications,however, is that the popular infinite support stochastic policies, e.g.,Gaussian policy, may assign riskily high dosages and harm patients seriously.Hence, it is important to induce a policy class whose support only containsnear-optimal actions, and shrink the action-searching area for effectivenessand reliability. To achieve this, we develop a novel \emph{quasi-optimallearning algorithm}, which can be easily optimized in off-policy settings withguaranteed convergence under general function approximations. Theoretically, weanalyze the consistency, sample complexity, adaptability, and convergence ofthe proposed algorithm. We evaluate our algorithm with comprehensive simulatedexperiments and a dose suggestion real application to Ohio Type 1 diabetesdataset.</description><author>Yuhan Li, Wenzhuo Zhou, Ruoqing Zhu</author><pubDate>Mon, 02 Oct 2023 01:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08940v2</guid></item><item><title>Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework</title><link>http://arxiv.org/abs/2309.13278v2</link><description>We study high-confidence off-policy evaluation in the context ofinfinite-horizon Markov decision processes, where the objective is to establisha confidence interval (CI) for the target policy value using only offline datapre-collected from unknown behavior policies. This task faces two primarychallenges: providing a comprehensive and rigorous error quantification in CIestimation, and addressing the distributional shift that results fromdiscrepancies between the distribution induced by the target policy and theoffline data-generating process. Motivated by an innovative unified erroranalysis, we jointly quantify the two sources of estimation errors: themisspecification error on modeling marginalized importance weights and thestatistical uncertainty due to sampling, within a single interval. This unifiedframework reveals a previously hidden tradeoff between the errors, whichundermines the tightness of the CI. Relying on a carefully designeddiscriminator function, the proposed estimator achieves a dual purpose:breaking the curse of the tradeoff to attain the tightest possible CI, andadapting the CI to ensure robustness against distributional shifts. Our methodis applicable to time-dependent data without assuming any weak dependenceconditions via leveraging a local supermartingale/martingale structure.Theoretically, we show that our algorithm is sample-efficient, error-robust,and provably convergent even in non-linear function approximation settings. Thenumerical performance of the proposed method is examined in synthetic datasetsand an OhioT1DM mobile health study.</description><author>Wenzhuo Zhou, Yuhan Li, Ruoqing Zhu, Annie Qu</author><pubDate>Mon, 02 Oct 2023 01:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13278v2</guid></item><item><title>A Model-Agnostic Graph Neural Network for Integrating Local and Global Information</title><link>http://arxiv.org/abs/2309.13459v2</link><description>Graph Neural Networks (GNNs) have achieved promising performance in a varietyof graph-focused tasks. Despite their success, existing GNNs suffer from twosignificant limitations: a lack of interpretability in results due to theirblack-box nature, and an inability to learn representations of varying orders.To tackle these issues, we propose a novel Model-agnostic Graph Neural Network(MaGNet) framework, which is able to sequentially integrate information ofvarious orders, extract knowledge from high-order neighbors, and providemeaningful and interpretable results by identifying influential compact graphstructures. In particular, MaGNet consists of two components: an estimationmodel for the latent representation of complex relationships under graphtopology, and an interpretation model that identifies influential nodes, edges,and important node features. Theoretically, we establish the generalizationerror bound for MaGNet via empirical Rademacher complexity, and showcase itspower to represent layer-wise neighborhood mixing. We conduct comprehensivenumerical studies using simulated data to demonstrate the superior performanceof MaGNet in comparison to several state-of-the-art alternatives. Furthermore,we apply MaGNet to a real-world case study aimed at extracting task-criticalinformation from brain activity data, thereby highlighting its effectiveness inadvancing scientific research.</description><author>Wenzhuo Zhou, Annie Qu, Keiland W. Cooper, Norbert Fortin, Babak Shahbaba</author><pubDate>Mon, 02 Oct 2023 01:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13459v2</guid></item><item><title>Stackelberg Batch Policy Learning</title><link>http://arxiv.org/abs/2309.16188v2</link><description>Batch reinforcement learning (RL) defines the task of learning from a fixedbatch of data lacking exhaustive exploration. Worst-case optimality algorithms,which calibrate a value-function model class from logged experience and performsome type of pessimistic evaluation under the learned model, have emerged as apromising paradigm for batch RL. However, contemporary works on this streamhave commonly overlooked the hierarchical decision-making structure hidden inthe optimization landscape. In this paper, we adopt a game-theoreticalviewpoint and model the policy learning diagram as a two-player general-sumgame with a leader-follower structure. We propose a novel stochasticgradient-based learning algorithm: StackelbergLearner, in which the leaderplayer updates according to the total derivative of its objective instead ofthe usual individual gradient, and the follower player makes individual updatesand ensures transition-consistent pessimistic reasoning. The derived learningdynamic naturally lends StackelbergLearner to a game-theoretic interpretationand provides a convergence guarantee to differentiable Stackelberg equilibria.From a theoretical standpoint, we provide instance-dependent regret bounds withgeneral function approximation, which shows that our algorithm can learn abest-effort policy that is able to compete against any comparator policy thatis covered by batch data. Notably, our theoretical regret guarantees onlyrequire realizability without any data coverage and strong functionapproximation conditions, e.g., Bellman closedness, which is in contrast toprior works lacking such guarantees. Through comprehensive experiments, we findthat our algorithm consistently performs as well or better as compared tostate-of-the-art methods in batch RL benchmark and real-world datasets.</description><author>Wenzhuo Zhou, Annie Qu</author><pubDate>Mon, 02 Oct 2023 01:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16188v2</guid></item><item><title>Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs</title><link>http://arxiv.org/abs/2306.06479v2</link><description>We prove that, for the fundamental regression task of learning a singleneuron, training a one-hidden layer ReLU network of any width by gradient flowfrom a small initialisation converges to zero loss and is implicitly biased tominimise the rank of network parameters. By assuming that the training pointsare correlated with the teacher neuron, we complement previous work thatconsidered orthogonal datasets. Our results are based on a detailednon-asymptotic analysis of the dynamics of each hidden neuron throughout thetraining. We also show and characterise a surprising distinction in thissetting between interpolator networks of minimal rank and those of minimalEuclidean norm. Finally we perform a range of numerical experiments, whichcorroborate our theoretical findings.</description><author>Dmitry Chistikov, Matthias Englert, Ranko Lazic</author><pubDate>Mon, 02 Oct 2023 01:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06479v2</guid></item><item><title>Multimodal Neurons in Pretrained Text-Only Transformers</title><link>http://arxiv.org/abs/2308.01544v2</link><description>Language models demonstrate remarkable capacity to generalize representationslearned in one modality to downstream tasks in other modalities. Can we tracethis ability to individual neurons? We study the case where a frozen texttransformer is augmented with vision using a self-supervised visual encoder anda single linear projection learned on an image-to-text task. Outputs of theprojection layer are not immediately decodable into language describing imagecontent; instead, we find that translation between modalities occurs deeperwithin the transformer. We introduce a procedure for identifying "multimodalneurons" that convert visual representations into corresponding text, anddecoding the concepts they inject into the model's residual stream. In a seriesof experiments, we show that multimodal neurons operate on specific visualconcepts across inputs, and have a systematic causal effect on imagecaptioning.</description><author>Sarah Schwettmann, Neil Chowdhury, Samuel Klein, David Bau, Antonio Torralba</author><pubDate>Mon, 02 Oct 2023 00:24:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01544v2</guid></item><item><title>SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for Exposing Deepfakes</title><link>http://arxiv.org/abs/2211.11296v2</link><description>Modern deepfake detectors have achieved encouraging results, when trainingand test images are drawn from the same data collection. However, when thesedetectors are applied to images produced with unknown deepfake-generationtechniques, considerable performance degradations are commonly observed. Inthis paper, we propose a novel deepfake detector, called SeeABLE, thatformalizes the detection problem as a (one-class) out-of-distribution detectiontask and generalizes better to unseen deepfakes. Specifically, SeeABLE firstgenerates local image perturbations (referred to as soft-discrepancies) andthen pushes the perturbed faces towards predefined prototypes using a novelregression-based bounded contrastive loss. To strengthen the generalizationperformance of SeeABLE to unknown deepfake types, we generate a rich set ofsoft discrepancies and train the detector: (i) to localize, which part of theface was modified, and (ii) to identify the alteration type. To demonstrate thecapabilities of SeeABLE, we perform rigorous experiments on several widely-useddeepfake datasets and show that our model convincingly outperforms competingstate-of-the-art detectors, while exhibiting highly encouraging generalizationcapabilities.</description><author>Nicolas Larue, Ngoc-Son Vu, Vitomir Struc, Peter Peer, Vassilis Christophides</author><pubDate>Mon, 02 Oct 2023 00:22:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11296v2</guid></item><item><title>EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset</title><link>http://arxiv.org/abs/2301.03213v5</link><description>Visual object tracking is a key component to many egocentric vision problems.However, the full spectrum of challenges of egocentric tracking faced by anembodied AI is underrepresented in many existing datasets; these tend to focuson relatively short, third-person videos. Egocentric video has severaldistinguishing characteristics from those commonly found in past datasets:frequent large camera motions and hand interactions with objects commonly leadto occlusions or objects exiting the frame, and object appearance can changerapidly due to widely different points of view, scale, or object states.Embodied tracking is also naturally long-term, and being able to consistently(re-)associate objects to their appearances and disappearances over as long asa lifetime is critical. Previous datasets under-emphasize this re-detectionproblem, and their "framed" nature has led to adoption of variousspatiotemporal priors that we find do not necessarily generalize to egocentricvideo. We thus introduce EgoTracks, a new dataset for long-term egocentricvisual object tracking. Sourced from the Ego4D dataset, this new datasetpresents a significant challenge to recent state-of-the-art single-objecttracking models, which we find score poorly on traditional tracking metrics forour new dataset, compared to popular benchmarks. We further show improvementsthat can be made to a STARK tracker to significantly increase its performanceon egocentric data, resulting in a baseline model we call EgoSTARK. We publiclyrelease our annotations and benchmark, hoping our dataset leads to furtheradvancements in tracking.</description><author>Hao Tang, Kevin Liang, Matt Feiszli, Weiyao Wang</author><pubDate>Sun, 01 Oct 2023 23:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03213v5</guid></item><item><title>Unlocking Tuning-free Generalization: Minimizing the PAC-Bayes Bound with Trainable Priors</title><link>http://arxiv.org/abs/2305.19243v2</link><description>It is widely recognized that the generalization ability of neural networkscan be greatly enhanced through carefully designing the training procedure. Thecurrent state-of-the-art training approach involves utilizing stochasticgradient descent (SGD) or Adam optimization algorithms along with a combinationof additional regularization techniques such as weight decay, dropout, or noiseinjection. Optimal generalization can only be achieved by tuning a multitude ofhyperparameters through grid search, which can be time-consuming andnecessitates additional validation datasets. To address this issue, weintroduce a practical PAC-Bayes training framework that is nearly tuning-freeand requires no additional regularization while achieving comparable testingperformance to that of SGD/Adam after a complete grid search and with extraregularizations. Our proposed algorithm demonstrates the remarkable potentialof PAC training to achieve state-of-the-art performance on deep neural networkswith enhanced robustness and interpretability.</description><author>Xitong Zhang, Avrajit Ghosh, Guangliang Liu, Rongrong Wang</author><pubDate>Sun, 01 Oct 2023 23:36:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19243v2</guid></item><item><title>On the Implicit Bias of Adam</title><link>http://arxiv.org/abs/2309.00079v2</link><description>In previous literature, backward error analysis was used to find ordinarydifferential equations (ODEs) approximating the gradient descent trajectory. Itwas found that finite step sizes implicitly regularize solutions because termsappearing in the ODEs penalize the two-norm of the loss gradients. We provethat the existence of similar implicit regularization in RMSProp and Adamdepends on their hyperparameters and the training stage, but with a different"norm" involved: the corresponding ODE terms either penalize the (perturbed)one-norm of the loss gradients or, on the contrary, hinder its decrease (thelatter case being typical). We also conduct numerical experiments and discusshow the proven facts can influence generalization.</description><author>Matias D. Cattaneo, Jason M. Klusowski, Boris Shigida</author><pubDate>Sun, 01 Oct 2023 23:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00079v2</guid></item><item><title>Training Diffusion Models with Reinforcement Learning</title><link>http://arxiv.org/abs/2305.13301v3</link><description>Diffusion models are a class of flexible generative models trained with anapproximation to the log-likelihood objective. However, most use cases ofdiffusion models are not concerned with likelihoods, but instead withdownstream objectives such as human-perceived image quality or drugeffectiveness. In this paper, we investigate reinforcement learning methods fordirectly optimizing diffusion models for such objectives. We describe howposing denoising as a multi-step decision-making problem enables a class ofpolicy gradient algorithms, which we refer to as denoising diffusion policyoptimization (DDPO), that are more effective than alternative reward-weightedlikelihood approaches. Empirically, DDPO is able to adapt text-to-imagediffusion models to objectives that are difficult to express via prompting,such as image compressibility, and those derived from human feedback, such asaesthetic quality. Finally, we show that DDPO can improve prompt-imagealignment using feedback from a vision-language model without the need foradditional data collection or human annotation. The project's website can befound at http://rl-diffusion.github.io .</description><author>Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine</author><pubDate>Sun, 01 Oct 2023 23:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13301v3</guid></item><item><title>Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners</title><link>http://arxiv.org/abs/2306.00561v2</link><description>In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fittedwith a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitatesthe modelling of local-global interactions in every decoder transformer blockthrough attention heads of several distinct local and global windows. Empiricalresults on ten downstream audio tasks show that MW-MAEs consistently outperformstandard MAEs in overall performance and learn better general-purpose audiorepresentations, along with demonstrating considerably better scalingcharacteristics. Investigating attention distances and entropies reveals thatMW-MAE encoders learn heads with broader local and global attention. Analyzingattention head feature representations through Projection Weighted CanonicalCorrelation Analysis (PWCCA) shows that attention heads with the same windowsizes across the decoder layers of the MW-MAE learn correlated featurerepresentations which enables each block to independently capture local andglobal information, leading to a decoupled decoder feature hierarchy. Code forfeature extraction and downstream experiments along with pre-trained modelswill be released publically.</description><author>Sarthak Yadav, Sergios Theodoridis, Lars Kai Hansen, Zheng-Hua Tan</author><pubDate>Sun, 01 Oct 2023 22:53:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00561v2</guid></item><item><title>Holistic Evaluation of Language Models</title><link>http://arxiv.org/abs/2211.09110v2</link><description>Language models (LMs) are becoming the foundation for almost all majorlanguage technologies, but their capabilities, limitations, and risks are notwell understood. We present Holistic Evaluation of Language Models (HELM) toimprove the transparency of language models. First, we taxonomize the vastspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)that are of interest for LMs. Then we select a broad subset based on coverageand feasibility, noting what's missing or underrepresented (e.g. questionanswering for neglected English dialects, metrics for trustworthiness). Second,we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,robustness, fairness, bias, toxicity, and efficiency) for each of 16 corescenarios when possible (87.5% of the time). This ensures metrics beyondaccuracy don't fall to the wayside, and that trade-offs are clearly exposed. Wealso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyzespecific aspects (e.g. reasoning, disinformation). Third, we conduct alarge-scale evaluation of 30 prominent language models (spanning open,limited-access, and closed models) on all 42 scenarios, 21 of which were notpreviously used in mainstream LM evaluation. Prior to HELM, models on averagewere evaluated on just 17.9% of the core HELM scenarios, with some prominentmodels not sharing a single scenario in common. We improve this to 96.0%: nowall 30 models have been densely benchmarked on the same core scenarios andmetrics under standardized conditions. Our evaluation surfaces 25 top-levelfindings. For full transparency, we release all raw model prompts andcompletions publicly for further analysis, as well as a general modulartoolkit. We intend for HELM to be a living benchmark for the community,continuously updated with new scenarios, metrics, and models.</description><author>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda</author><pubDate>Sun, 01 Oct 2023 22:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09110v2</guid></item><item><title>SEENN: Towards Temporal Spiking Early-Exit Neural Networks</title><link>http://arxiv.org/abs/2304.01230v2</link><description>Spiking Neural Networks (SNNs) have recently become more popular as abiologically plausible substitute for traditional Artificial Neural Networks(ANNs). SNNs are cost-efficient and deployment-friendly because they processinput in both spatial and temporal manner using binary spikes. However, weobserve that the information capacity in SNNs is affected by the number oftimesteps, leading to an accuracy-efficiency tradeoff. In this work, we study afine-grained adjustment of the number of timesteps in SNNs. Specifically, wetreat the number of timesteps as a variable conditioned on different inputsamples to reduce redundant timesteps for certain data. We call our methodSpiking Early-Exit Neural Networks (SEENNs). To determine the appropriatenumber of timesteps, we propose SEENN-I which uses a confidence scorethresholding to filter out the uncertain predictions, and SEENN-II whichdetermines the number of timesteps by reinforcement learning. Moreover, wedemonstrate that SEENN is compatible with both the directly trained SNN and theANN-SNN conversion. By dynamically adjusting the number of timesteps, our SEENNachieves a remarkable reduction in the average number of timesteps duringinference. For example, our SEENN-II ResNet-19 can achieve 96.1% accuracy withan average of 1.08 timesteps on the CIFAR-10 test dataset. Code is shared athttps://github.com/Intelligent-Computing-Lab-Yale/SEENN.</description><author>Yuhang Li, Tamar Geller, Youngeun Kim, Priyadarshini Panda</author><pubDate>Sun, 01 Oct 2023 22:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01230v2</guid></item><item><title>OpenMixup: A Comprehensive Mixup Benchmark for Visual Classification</title><link>http://arxiv.org/abs/2209.04851v2</link><description>Data mixing, or mixup, is a data-dependent augmentation technique that hasgreatly enhanced the generalizability of modern deep neural networks. However,a full grasp of mixup methodology necessitates a top-down hierarchicalunderstanding from systematic impartial evaluations and empirical analysis,both of which are currently lacking within the community. In this paper, wepresent OpenMixup, the first comprehensive mixup benchmarking study forsupervised visual classification. OpenMixup offers a unified mixup-based modeldesign and training framework, encompassing a wide collection of data mixingalgorithms, a diverse range of widely-used backbones and modules, and a set ofmodel analysis toolkits. To ensure fair and complete comparisons, large-scalestandard evaluations of various mixup baselines are conducted across 12diversified image datasets with meticulous confounders and tweaking powered byour modular and extensible codebase framework. Interesting observations andinsights are derived through detailed empirical analysis of how mixup policies,network architectures, and dataset properties affect the mixup visualclassification performance. We hope that OpenMixup can bolster thereproducibility of previously gained insights and facilitate a betterunderstanding of mixup properties, thereby giving the community a kick-startfor the development and evaluation of new mixup methods. The source code anduser documents are available at \url{https://github.com/Westlake-AI/openmixup}.</description><author>Siyuan Li, Zedong Wang, Zicheng Liu, Di Wu, Cheng Tan, Weiyang Jin, Stan Z. Li</author><pubDate>Sun, 01 Oct 2023 22:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.04851v2</guid></item><item><title>Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection</title><link>http://arxiv.org/abs/2212.08108v3</link><description>Deep learning-based vulnerability detection has shown great performance and,in some studies, outperformed static analysis tools. However, thehighest-performing approaches use token-based transformer models, which are notthe most efficient to capture code semantics required for vulnerabilitydetection. Classical program analysis techniques such as dataflow analysis candetect many types of bugs based on their root causes. In this paper, we proposeto combine such causal-based vulnerability detection algorithms with deeplearning, aiming to achieve more efficient and effective vulnerabilitydetection. Specifically, we designed DeepDFA, a dataflow analysis-inspiredgraph learning framework and an embedding technique that enables graph learningto simulate dataflow computation. We show that DeepDFA is both performant andefficient. DeepDFA outperformed all non-transformer baselines. It was trainedin 9 minutes, 75x faster than the highest-performing baseline model. When usingonly 50+ vulnerable and several hundreds of total examples as training data,the model retained the same performance as 100% of the dataset. DeepDFA alsogeneralized to real-world vulnerabilities in DbgBench; it detected 8.7 out of17 vulnerabilities on average across folds and was able to distinguish betweenpatched and buggy versions, while the highest-performing baseline models didnot detect any vulnerabilities. By combining DeepDFA with a large languagemodel, we surpassed the state-of-the-art vulnerability detection performance onthe Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Ourreplication package is located at https://doi.org/10.6084/m9.figshare.21225413 .</description><author>Benjamin Steenhoek, Hongyang Gao, Wei Le</author><pubDate>Sun, 01 Oct 2023 21:48:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08108v3</guid></item><item><title>General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Datasets</title><link>http://arxiv.org/abs/2308.00180v2</link><description>Underwater gliders have been widely used in oceanography for a range ofapplications. However, unpredictable events like shark strike or remoraattachment can lead to abnormal glider behavior or even loss of the glider.This paper employs an anomaly detection algorithm to assess operationalconditions of underwater gliders in the ocean environment. Prompt alerts areprovided to glider pilots upon detecting any anomaly, so that they can takecontrol of the glider to prevent further harm. The detection algorithm isapplied to abundant datasets collected in real glider deployments led by theSkidaway Institute of Oceanography (SkIO) in the University of Georgia and theUniversity of South Florida (USF). In order to demonstrate generality, theexperimental evaluation is applied to four glider deployment datasets.Specifically, we utilize post-recovery DBD datasets carrying high-resolutioninformation to perform detailed analysis of the anomaly and compare it withpilot logs. Additionally, we implement the online detection based on thereal-time subsets of data transmitted from the glider at the surfacing events.While the real-time glider data may not contain as much rich information as thepost-recovery one, the online detection is of great importance as it allowsglider pilots to monitor potential abnormal conditions in real time.</description><author>Ruochu Yang, Chad Lembke, Fumin Zhang, Catherine Edwards</author><pubDate>Sun, 01 Oct 2023 21:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00180v2</guid></item><item><title>Project Aria: A New Tool for Egocentric Multi-Modal AI Research</title><link>http://arxiv.org/abs/2308.13561v3</link><description>Egocentric, multi-modal data as available on future augmented reality (AR)devices provides unique challenges and opportunities for machine perception.These future devices will need to be all-day wearable in a socially acceptableform-factor to support always available, context-aware and personalized AIapplications. Our team at Meta Reality Labs Research built the Aria device, anegocentric, multi-modal data recording and streaming device with the goal tofoster and accelerate research in this area. In this paper, we describe theAria device hardware including its sensor configuration and the correspondingsoftware tools that enable recording and processing of such data.</description><author>Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander Gamino, Andrew Turner, Arjang Talattof, Arnie Yuan, Bilal Souti, Brighid Meredith, Cheng Peng, Chris Sweeney, Cole Wilson, Dan Barnes, Daniel DeTone, David Caruso, Derek Valleroy, Dinesh Ginjupalli, Duncan Frost, Edward Miller, Elias Mueggler, Evgeniy Oleinik, Fan Zhang, Guruprasad Somasundaram, Gustavo Solaira, Harry Lanaras, Henry Howard-Jenkins, Huixuan Tang, Hyo Jin Kim, Jaime Rivera, Ji Luo, Jing Dong, Julian Straub, Kevin Bailey, Kevin Eckenhoff, Lingni Ma, Luis Pesqueira, Mark Schwesinger, Maurizio Monge, Nan Yang, Nick Charron, Nikhil Raina, Omkar Parkhi, Peter Borschowa, Pierre Moulon, Prince Gupta, Raul Mur-Artal, Robbie Pennington, Sachin Kulkarni, Sagar Miglani, Santosh Gondi, Saransh Solanki, Sean Diener, </author><pubDate>Sun, 01 Oct 2023 21:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13561v3</guid></item><item><title>Learning invariant representations of time-homogeneous stochastic dynamical systems</title><link>http://arxiv.org/abs/2307.09912v2</link><description>We consider the general class of time-homogeneous stochastic dynamicalsystems, both discrete and continuous, and study the problem of learning arepresentation of the state that faithfully captures its dynamics. This isinstrumental to learn the transfer operator of the system, that in turn can beused for numerous tasks, such as forecasting and interpreting the systemdynamics. We show that the search for a good representation can be cast as anoptimization problem over neural networks. Our approach is supported by recentresults in statistical learning theory, highlighting the role of approximationerror and metric distortion in the context of transfer operator regression. Theobjective function we propose is associated with projection operators from therepresentation space to the data space, overcomes metric distortion, and can beempirically estimated from data. In the discrete time setting, we furtherderive a relaxed objective function that is differentiable and numericallywell-conditioned. We compare our method against state-of-the-art approaches ondifferent datasets, showing better performance across the board.</description><author>Vladimir R. Kostic, Pietro Novelli, Riccardo Grazzi, Karim Lounici, Massimiliano Pontil</author><pubDate>Sun, 01 Oct 2023 21:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09912v2</guid></item><item><title>Noise Stability Optimization for Flat Minima with Tight Rates</title><link>http://arxiv.org/abs/2306.08553v2</link><description>Generalization properties are a central aspect of the design and analysis oflearning algorithms. One notion that has been considered in many previous worksas leading to good generalization is flat minima, which informally describes aloss surface that is insensitive to noise perturbations. However, the design ofefficient algorithms (that are easy to analyze) to find them is relativelyunder-explored. In this paper, we propose a new algorithm to address thisissue, which minimizes a stochastic optimization objective that averages noiseperturbations injected into the weights of a function. This algorithm is shownto enjoy both theoretical and empirical advantages compared to existingalgorithms involving worst-case perturbations. Theoretically, we show tightconvergence rates of our algorithm to find first-order stationary points of thestochastic objective. Empirically, the algorithm induces a penalty on the traceof the Hessian, leading to iterates that are flatter than SGD and otheralternatives, with tighter generalization gaps. Altogether, this workcontributes a provable and practical algorithm to find flat minima byoptimizing the noise stability properties of a function.</description><author>Haotian Ju, Dongyue Li, Hongyang R. Zhang</author><pubDate>Sun, 01 Oct 2023 21:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08553v2</guid></item><item><title>ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing</title><link>http://arxiv.org/abs/2305.09770v5</link><description>Despite a surge collection of XAI methods, users still struggle to obtainrequired AI explanations. Previous research suggests chatbots as dynamicsolutions, but the effective design of conversational XAI agents for practicalhuman needs remains under-explored. This paper focuses on Conversational XAIfor AI-assisted scientific writing tasks. Drawing from human linguistictheories and formative studies, we identify four design rationales:"multifaceted", "controllability", "mix-initiative", "context-awaredrill-down". We incorporate them into an interactive prototype, ConvXAI, whichfacilitates heterogeneous AI explanations for scientific writing throughdialogue. In two studies with 21 users, ConvXAI outperforms a GUI-basedbaseline on improving human-perceived understanding and writing improvement.The paper further discusses the practical human usage patterns in interactingwith ConvXAI for scientific co-writing.</description><author>Hua Shen, Chieh-Yang Huang, Tongshuang Wu, Ting-Hao 'Kenneth' Huang</author><pubDate>Sun, 01 Oct 2023 21:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09770v5</guid></item><item><title>Optimal Rates for Bandit Nonstochastic Control</title><link>http://arxiv.org/abs/2305.15352v2</link><description>Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) controlare foundational and extensively researched problems in optimal control. Weinvestigate LQR and LQG problems with semi-adversarial perturbations andtime-varying adversarial bandit loss functions. The best-known sublinear regretalgorithm of~\cite{gradu2020non} has a $T^{\frac{3}{4}}$ time horizondependence, and its authors posed an open question about whether a tight rateof $\sqrt{T}$ could be achieved. We answer in the affirmative, giving analgorithm for bandit LQR and LQG which attains optimal regret (up tologarithmic factors) for both known and unknown systems. A central component ofour method is a new scheme for bandit convex optimization with memory, which isof independent interest.</description><author>Y. Jennifer Sun, Stephen Newman, Elad Hazan</author><pubDate>Sun, 01 Oct 2023 20:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15352v2</guid></item><item><title>A Decision Making Framework for Recommended Maintenance of Road Segments</title><link>http://arxiv.org/abs/2307.10085v3</link><description>Due to limited budgets allocated for road maintenance projects in variouscountries, road management departments face difficulties in making scientificmaintenance decisions. This paper aims to provide road management departmentswith more scientific decision tools and evidence. The framework proposed inthis paper mainly has the following four innovative points: 1) Predictingpavement performance deterioration levels of road sections as decision basisrather than accurately predicting specific indicator values; 2) Determiningmaintenance route priorities based on multiple factors; 3) Making maintenanceplan decisions by establishing deep reinforcement learning models to formulatepredictive strategies based on past maintenance performance evaluations, whileconsidering both technical and management indicators; 4) Determining repairsection priorities according to actual and suggested repair effects. Byresolving these four issues, the framework can make intelligent decisionsregarding optimal maintenance plans and sections, taking into account limitedfunds and historical maintenance management experiences.</description><author>Haoyu Sun, Yan Yan</author><pubDate>Sun, 01 Oct 2023 20:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10085v3</guid></item><item><title>FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging</title><link>http://arxiv.org/abs/2109.09658v4</link><description>The recent advancements in artificial intelligence (AI) combined with theextensive amount of data generated by today's clinical systems, has led to thedevelopment of imaging AI solutions across the whole value chain of medicalimaging, including image reconstruction, medical image segmentation,image-based diagnosis and treatment planning. Notwithstanding the successes andfuture potential of AI in medical imaging, many stakeholders are concerned ofthe potential risks and ethical implications of imaging AI solutions, which areperceived as complex, opaque, and difficult to comprehend, utilise, and trustin critical clinical applications. Despite these concerns and risks, there arecurrently no concrete guidelines and best practices for guiding future AIdevelopments in medical imaging towards increased trust, safety and adoption.To bridge this gap, this paper introduces a careful selection of guidingprinciples drawn from the accumulated experiences, consensus, and bestpractices from five large European projects on AI in Health Imaging. Theseguiding principles are named FUTURE-AI and its building blocks consist of (i)Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustnessand (vi) Explainability. In a step-by-step approach, these guidelines arefurther translated into a framework of concrete recommendations for specifying,developing, evaluating, and deploying technically, clinically and ethicallytrustworthy AI solutions into clinical practice.</description><author>Karim Lekadir, Richard Osuala, Catherine Gallin, Noussair Lazrak, Kaisar Kushibar, Gianna Tsakou, Susanna Aussó, Leonor Cerdá Alberich, Kostas Marias, Manolis Tsiknakis, Sara Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, Henry C Woodruff, Philippe Lambin, Luis Martí-Bonmatí</author><pubDate>Sun, 01 Oct 2023 20:19:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.09658v4</guid></item><item><title>Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments</title><link>http://arxiv.org/abs/2305.15700v4</link><description>Continual semantic segmentation aims to learn new classes while maintainingthe information from the previous classes. Although prior studies have shownimpressive progress in recent years, the fairness concern in the continualsemantic segmentation needs to be better addressed. Meanwhile, fairness is oneof the most vital factors in deploying the deep learning model, especially inhuman-related or safety applications. In this paper, we present a novelFairness Continual Learning approach to the semantic segmentation problem. Inparticular, under the fairness objective, a new fairness continual learningframework is proposed based on class distributions. Then, a novel PrototypicalContrastive Clustering loss is proposed to address the significant challengesin continual learning, i.e., catastrophic forgetting and background shift. Ourproposed loss has also been proven as a novel, generalized learning paradigm ofknowledge distillation commonly used in continual learning. Moreover, theproposed Conditional Structural Consistency loss further regularized thestructural constraint of the predicted segmentation. Our proposed approach hasachieved State-of-the-Art performance on three standard scene understandingbenchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairnessof the segmentation model.</description><author>Thanh-Dat Truong, Hoang-Quan Nguyen, Bhiksha Raj, Khoa Luu</author><pubDate>Sun, 01 Oct 2023 20:03:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15700v4</guid></item><item><title>SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling</title><link>http://arxiv.org/abs/2306.11886v2</link><description>Pre-training robot policies with a rich set of skills can substantiallyaccelerate the learning of downstream tasks. Prior works have definedpre-training tasks via natural language instructions, but doing so requirestedious human annotation of hundreds of thousands of instructions. Thus, wepropose SPRINT, a scalable offline policy pre-training approach whichsubstantially reduces the human effort needed for pre-training a diverse set ofskills. Our method uses two core ideas to automatically expand a base set ofpre-training tasks: instruction relabeling via large language models andcross-trajectory skill chaining through offline reinforcement learning. As aresult, SPRINT pre-training equips robots with a much richer repertoire ofskills. Experimental results in a household simulator and on a real robotkitchen manipulation task show that SPRINT leads to substantially fasterlearning of new long-horizon tasks than previous pre-training approaches.Website at https://clvrai.com/sprint.</description><author>Jesse Zhang, Karl Pertsch, Jiahui Zhang, Joseph J. Lim</author><pubDate>Sun, 01 Oct 2023 19:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11886v2</guid></item><item><title>Understanding the Difficulty of Training Transformers</title><link>http://arxiv.org/abs/2004.08249v3</link><description>Transformers have proved effective in many NLP tasks. However, their trainingrequires non-trivial efforts regarding designing cutting-edge optimizers andlearning rate schedulers carefully (e.g., conventional SGD fails to trainTransformers effectively). Our objective here is to understand $\textit{whatcomplicates Transformer training}$ from both empirical and theoreticalperspectives. Our analysis reveals that unbalanced gradients are not the rootcause of the instability of training. Instead, we identify an amplificationeffect that influences training substantially -- for each layer in amulti-layer Transformer model, heavy dependency on its residual branch makestraining unstable, since it amplifies small parameter perturbations (e.g.,parameter updates) and results in significant disturbances in the model output.Yet we observe that a light dependency limits the model potential and leads toinferior trained models. Inspired by our analysis, we propose Admin($\textbf{Ad}$aptive $\textbf{m}$odel $\textbf{in}$itialization) to stabilizestabilize the early stage's training and unleash its full potential in the latestage. Extensive experiments show that Admin is more stable, converges faster,and leads to better performance. Implementations are released at:https://github.com/LiyuanLucasLiu/Transforemr-Clinic.</description><author>Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Jiawei Han</author><pubDate>Sun, 01 Oct 2023 19:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2004.08249v3</guid></item><item><title>OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch</title><link>http://arxiv.org/abs/2309.10706v2</link><description>Large language models (LLMs) with billions of parameters have demonstratedoutstanding performance on various natural language processing tasks. Thisreport presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model,to contribute an LLM variant to the Chinese-oriented open-source modelcommunity. We enhance OpenBA with effective and efficient techniques as well asadopt a three-stage training strategy to train the model from scratch. Oursolution can also achieve very competitive performance with only 380B tokens,which is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on theMMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report providesthe main details to pre-train an analogous model, including pre-training dataprocessing, Bilingual Flan data collection, the empirical observations thatinspire our model architecture design, training objectives of different stages,and other enhancement techniques. Additionally, we also provide the fine-tuningdetails of OpenBA on four downstream tasks. We have refactored our code tofollow the design principles of the Huggingface Transformers Library, making itmore convenient for developers to use, and released checkpoints of differenttraining stages at https://huggingface.co/openBA. More details of our projectare available at https://github.com/OpenNLG/openBA.git.</description><author>Juntao Li, Zecheng Tang, Yuyang Ding, Pinzheng Wang, Pei Guo, Wangjie You, Dan Qiao, Wenliang Chen, Guohong Fu, Qiaoming Zhu, Guodong Zhou, Min Zhang</author><pubDate>Sun, 01 Oct 2023 19:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10706v2</guid></item><item><title>Relation-Oriented: Toward Causal Knowledge-Aligned AI</title><link>http://arxiv.org/abs/2307.16387v7</link><description>This study examines the inherent limitations of the prevailingObservation-Oriented learning paradigm by understanding relationship modelingfrom a unique dimensionality perspective. This paradigm necessitates theidentification of modeling objects prior to defining relations, confiningmodels to observational space, and limiting their access to dynamical temporalfeatures. By relying on a singular, absolute timeline, it often neglects themulti-dimensional nature of the temporal feature space, compromising therobustness and generalizability of structural causal models and contributingsignificantly to the AI misalignment issue. Drawing from the relation-centric essence of human cognition, this studypresents a new Relation-Oriented paradigm. Supported by extensive efficacyexperiments, this paradigm, and its methodological counterpart,relation-defined representation learning, aim to construct interpretable AIgrounded in established knowledge.</description><author>Jia Li, Xiang Li</author><pubDate>Sun, 01 Oct 2023 19:01:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16387v7</guid></item><item><title>Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models</title><link>http://arxiv.org/abs/2306.09251v2</link><description>Diffusion models, which convert noise into new data instances by learning toreverse a Markov diffusion process, have become a cornerstone in contemporarygenerative modeling. While their practical power has now been widelyrecognized, the theoretical underpinnings remain far from mature. In this work,we develop a suite of non-asymptotic theory towards understanding the datageneration process of diffusion models in discrete time, assuming access to$\ell_2$-accurate estimates of the (Stein) score functions. For a populardeterministic sampler (based on the probability flow ODE), we establish aconvergence rate proportional to $1/T$ (with $T$ the total number of steps),improving upon past results; for another mainstream stochastic sampler (i.e., atype of the denoising diffusion probabilistic model), we derive a convergencerate proportional to $1/\sqrt{T}$, matching the state-of-the-art theory.Imposing only minimal assumptions on the target data distribution (e.g., nosmoothness assumption is imposed), our results characterize how $\ell_2$ scoreestimation errors affect the quality of the data generation processes. Incontrast to prior works, our theory is developed based on an elementary yetversatile non-asymptotic approach without resorting to toolboxes for SDEs andODEs. Further, we design two accelerated variants, improving the convergence to$1/T^2$ for the ODE-based sampler and $1/T$ for the DDPM-type sampler, whichmight be of independent theoretical and empirical interest.</description><author>Gen Li, Yuting Wei, Yuxin Chen, Yuejie Chi</author><pubDate>Sun, 01 Oct 2023 18:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09251v2</guid></item><item><title>Train Hard, Fight Easy: Robust Meta Reinforcement Learning</title><link>http://arxiv.org/abs/2301.11147v2</link><description>A major challenge of reinforcement learning (RL) in real-world applicationsis the variation between environments, tasks or clients. Meta-RL (MRL)addresses this issue by learning a meta-policy that adapts to new tasks.Standard MRL methods optimize the average return over tasks, but often sufferfrom poor results in tasks of high risk or difficulty. This limits systemreliability since test tasks are not known in advance. In this work, we definea robust MRL objective with a controlled robustness level. Optimization ofanalogous robust objectives in RL is known to lead to both *biased gradients*and *data inefficiency*. We prove that the gradient bias disappears in ourproposed MRL framework. The data inefficiency is addressed via the novel RobustMeta RL algorithm (RoML). RoML is a meta-algorithm that generates a robustversion of any given MRL algorithm, by identifying and over-sampling hardertasks throughout training. We demonstrate that RoML achieves robust returns onmultiple navigation and continuous control benchmarks.</description><author>Ido Greenberg, Shie Mannor, Gal Chechik, Eli Meirom</author><pubDate>Sun, 01 Oct 2023 18:13:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11147v2</guid></item><item><title>Leveraging Inpainting for Single-Image Shadow Removal</title><link>http://arxiv.org/abs/2302.05361v3</link><description>Fully-supervised shadow removal methods achieve the best restorationqualities on public datasets but still generate some shadow remnants. One ofthe reasons is the lack of large-scale shadow &amp; shadow-free image pairs.Unsupervised methods can alleviate the issue but their restoration qualitiesare much lower than those of fully-supervised methods. In this work, we findthat pretraining shadow removal networks on the image inpainting dataset canreduce the shadow remnants significantly: a naive encoder-decoder network getscompetitive restoration quality w.r.t. the state-of-the-art methods via only10% shadow &amp; shadow-free image pairs. After analyzing networks with/withoutinpainting pre-training via the information stored in the weight (IIW), we findthat inpainting pretraining improves restoration quality in non-shadow regionsand enhances the generalization ability of networks significantly.Additionally, shadow removal fine-tuning enables networks to fill in thedetails of shadow regions. Inspired by these observations we formulate shadowremoval as an adaptive fusion task that takes advantage of both shadow removaland image inpainting. Specifically, we develop an adaptive fusion networkconsisting of two encoders, an adaptive fusion block, and a decoder. The twoencoders are responsible for extracting the feature from the shadow image andthe shadow-masked image respectively. The adaptive fusion block is responsiblefor combining these features in an adaptive manner. Finally, the decoderconverts the adaptive fused features to the desired shadow-free result. Theextensive experiments show that our method empowered with inpaintingoutperforms all state-of-the-art methods.</description><author>Xiaoguang Li, Qing Guo, Rabab Abdelfattah, Di Lin, Wei Feng, Ivor Tsang, Song Wang</author><pubDate>Sun, 01 Oct 2023 18:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05361v3</guid></item><item><title>AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation</title><link>http://arxiv.org/abs/2306.06842v2</link><description>Aerial Image Segmentation is a top-down perspective semantic segmentation andhas several challenging characteristics such as strong imbalance in theforeground-background distribution, complex background, intra-classheterogeneity, inter-class homogeneity, and tiny objects. To handle theseproblems, we inherit the advantages of Transformers and propose AerialFormer,which unifies Transformers at the contracting path with lightweightMulti-Dilated Convolutional Neural Networks (MD-CNNs) at the expanding path.Our AerialFormer is designed as a hierarchical structure, in which Transformerencoder outputs multi-scale features and MD-CNNs decoder aggregates informationfrom the multi-scales. Thus, it takes both local and global contexts intoconsideration to render powerful representations and high-resolutionsegmentation. We have benchmarked AerialFormer on three common datasetsincluding iSAID, LoveDA, and Potsdam. Comprehensive experiments and extensiveablation studies show that our proposed AerialFormer outperforms previousstate-of-the-art methods with remarkable performance. Our source code will bepublicly available upon acceptance.</description><author>Kashu Yamazaki, Taisei Hanyu, Minh Tran, Adrian de Luis, Roy McCann, Haitao Liao, Chase Rainwater, Meredith Adkins, Jackson Cothren, Ngan Le</author><pubDate>Sun, 01 Oct 2023 18:04:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06842v2</guid></item><item><title>Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing</title><link>http://arxiv.org/abs/2306.06599v2</link><description>Existing regression models tend to fall short in both accuracy anduncertainty estimation when the label distribution is imbalanced. In thispaper, we propose a probabilistic deep learning model, dubbed variationalimbalanced regression (VIR), which not only performs well in imbalancedregression but naturally produces reasonable uncertainty estimation as abyproduct. Different from typical variational autoencoders assuming I.I.D.representations (a data point's representation is not directly affected byother data points), our VIR borrows data with similar regression labels tocompute the latent representation's variational distribution; furthermore,different from deterministic regression models producing point estimates, VIRpredicts the entire normal-inverse-gamma distributions and modulates theassociated conjugate distributions to impose probabilistic reweighting on theimbalanced data, thereby providing better uncertainty estimation. Experimentsin several real-world datasets show that our VIR can outperformstate-of-the-art imbalanced regression models in terms of both accuracy anduncertainty estimation. Code will soon be available at\url{https://github.com/Wang-ML-Lab/variational-imbalanced-regression}.</description><author>Ziyan Wang, Hao Wang</author><pubDate>Sun, 01 Oct 2023 17:47:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06599v2</guid></item><item><title>OKRidge: Scalable Optimal k-Sparse Ridge Regression</title><link>http://arxiv.org/abs/2304.06686v2</link><description>We consider an important problem in scientific discovery, namely identifyingsparse governing equations for nonlinear dynamical systems. This involvessolving sparse ridge regression problems to provable optimality in order todetermine which terms drive the underlying dynamics. We propose a fastalgorithm, OKRidge, for sparse ridge regression, using a novel lower boundcalculation involving, first, a saddle point formulation, and from there,either solving (i) a linear system or (ii) using an ADMM-based approach, wherethe proximal operators can be efficiently evaluated by solving another linearsystem and an isotonic regression problem. We also propose a method towarm-start our solver, which leverages a beam search. Experimentally, ourmethods attain provable optimality with run times that are orders of magnitudefaster than those of the existing MIP formulations solved by the commercialsolver Gurobi.</description><author>Jiachang Liu, Sam Rosen, Chudi Zhong, Cynthia Rudin</author><pubDate>Sun, 01 Oct 2023 17:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06686v2</guid></item><item><title>Pointwise-in-Time Explanation for Linear Temporal Logic Rules</title><link>http://arxiv.org/abs/2306.13956v2</link><description>The new field of Explainable Planning (XAIP) has produced a variety ofapproaches to explain and describe the behavior of autonomous agents to humanobservers. Many summarize agent behavior in terms of the constraints, or''rules,'' which the agent adheres to during its trajectories. In this work, wenarrow the focus from summary to specific moments in individual trajectories,offering a ''pointwise-in-time'' view. Our novel framework, which we define onLinear Temporal Logic (LTL) rules, assigns an intuitive status to any rule inorder to describe the trajectory progress at individual time steps; here, arule is classified as active, satisfied, inactive, or violated. Given atrajectory, a user may query for status of specific LTL rules at individualtrajectory time steps. In this paper, we present this novel framework, namedRule Status Assessment (RSA), and provide an example of its implementation. Wefind that pointwise-in-time status assessment is useful as a post-hocdiagnostic, enabling a user to systematically track the agent's behavior withrespect to a set of rules.</description><author>Noel Brindise, Cedric Langbort</author><pubDate>Sun, 01 Oct 2023 17:35:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13956v2</guid></item><item><title>Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack</title><link>http://arxiv.org/abs/2301.02615v2</link><description>Backdoor poisoning attacks pose a well-known risk to neural networks.However, most studies have focused on lenient threat models. We introduceSilent Killer, a novel attack that operates in clean-label, black-box settings,uses a stealthy poison and trigger and outperforms existing methods. Weinvestigate the use of universal adversarial perturbations as triggers inclean-label attacks, following the success of such approaches underpoison-label settings. We analyze the success of a naive adaptation and findthat gradient alignment for crafting the poison is required to ensure highsuccess rates. We conduct thorough experiments on MNIST, CIFAR10, and a reducedversion of ImageNet and achieve state-of-the-art results.</description><author>Tzvi Lederer, Gallil Maimon, Lior Rokach</author><pubDate>Sun, 01 Oct 2023 17:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.02615v2</guid></item><item><title>Federated Learning with Uncertainty via Distilled Predictive Distributions</title><link>http://arxiv.org/abs/2206.07562v2</link><description>Most existing federated learning methods are unable to estimatemodel/predictive uncertainty since the client models are trained using thestandard loss function minimization approach which ignores such uncertainties.In many situations, however, especially in limited data settings, it isbeneficial to take into account the uncertainty in the model parameters at eachclient as it leads to more accurate predictions and also because reliableestimates of uncertainty can be used for tasks, such as out-of-distribution(OOD) detection, and sequential decision-making tasks, such as active learning.We present a framework for federated learning with uncertainty where, in eachround, each client infers the posterior distribution over its parameters aswell as the posterior predictive distribution (PPD), distills the PPD into asingle deep neural network, and sends this network to the server. Unlike someof the recent Bayesian approaches to federated learning, our approach does notrequire sending the whole posterior distribution of the parameters from eachclient to the server but only the PPD in the distilled form as a deep neuralnetwork. In addition, when making predictions at test time, it does not requirecomputationally expensive Monte-Carlo averaging over the posterior distributionbecause our approach always maintains the PPD in the form of a single deepneural network. Moreover, our approach does not make any restrictiveassumptions, such as the form of the clients' posterior distributions, or oftheir PPDs. We evaluate our approach on classification in federated setting, aswell as active learning and OOD detection in federated settings, on which ourapproach outperforms various existing federated learning baselines.</description><author>Shrey Bhatt, Aishwarya Gupta, Piyush Rai</author><pubDate>Sun, 01 Oct 2023 17:18:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.07562v2</guid></item><item><title>An Optical Control Environment for Benchmarking Reinforcement Learning Algorithms</title><link>http://arxiv.org/abs/2203.12114v2</link><description>Deep reinforcement learning has the potential to address various scientificproblems. In this paper, we implement an optics simulation environment forreinforcement learning based controllers. The environment captures the essenceof nonconvexity, nonlinearity, and time-dependent noise inherent in opticalsystems, offering a more realistic setting. Subsequently, we provide thebenchmark results of several reinforcement learning algorithms on the proposedsimulation environment. The experimental findings demonstrate the superiorityof off-policy reinforcement learning approaches over traditional controlalgorithms in navigating the intricacies of complex optical controlenvironments. The code of the paper is available athttps://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking.</description><author>Abulikemu Abuduweili, Changliu Liu</author><pubDate>Sun, 01 Oct 2023 16:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.12114v2</guid></item><item><title>Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting</title><link>http://arxiv.org/abs/2308.07939v2</link><description>Continual Learning (CL) is a process in which there is still huge gap betweenhuman and deep learning model efficiency. Recently, many CL algorithms weredesigned. Most of them have many problems with learning in dynamic and complexenvironments. In this work new architecture based approach Ada-QPacknet isdescribed. It incorporates the pruning for extracting the sub-network for eachtask. The crucial aspect in architecture based CL methods is theirs capacity.In presented method the size of the model is reduced by efficient linear andnonlinear quantisation approach. The method reduces the bit-width of theweights format. The presented results shows that low bit quantisation achievessimilar accuracy as floating-point sub-network on a well-know CL scenarios. Toour knowledge it is the first CL strategy which incorporates both compressiontechniques pruning and quantisation for generating task sub-networks. Thepresented algorithm was tested on well-known episode combinations and comparedwith most popular algorithms. Results show that proposed approach outperformsmost of the CL strategies in task and class incremental scenarios.</description><author>Marcin Pietroń, Dominik Żurek, Kamil Faber, Roberto Corizzo</author><pubDate>Sun, 01 Oct 2023 16:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07939v2</guid></item><item><title>MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning</title><link>http://arxiv.org/abs/2309.05653v2</link><description>We introduce MAmmoTH, a series of open-source large language models (LLMs)specifically tailored for general math problem-solving. The MAmmoTH models aretrained on MathInstruct, our meticulously curated instruction tuning dataset.MathInstruct is compiled from 13 math datasets with intermediate rationales,six of which have rationales newly curated by us. It presents a unique hybridof chain-of-thought (CoT) and program-of-thought (PoT) rationales, and alsoensures extensive coverage of diverse fields in math. The hybrid of CoT and PoTnot only unleashes the potential of tool use but also allows different thoughtprocesses for different math problems. As a result, the MAmmoTH seriessubstantially outperform existing open-source models on nine mathematicalreasoning datasets across all scales with an average accuracy gain between 16%and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (acompetition-level dataset), which exceeds the best open-source 7B model(WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH,even surpassing GPT-4's CoT result. Our work underscores the importance ofdiverse problem coverage and the use of hybrid rationales in developingsuperior math generalist models.</description><author>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen</author><pubDate>Sun, 01 Oct 2023 16:25:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05653v2</guid></item><item><title>TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration</title><link>http://arxiv.org/abs/2304.02419v2</link><description>We propose a novel task for generating 3D dance movements that simultaneouslyincorporate both text and music modalities. Unlike existing works that generatedance movements using a single modality such as music, our goal is to producericher dance movements guided by the instructive information provided by thetext. However, the lack of paired motion data with both music and textmodalities limits the ability to generate dance movements that integrate both.To alleviate this challenge, we propose to utilize a 3D human motion VQ-VAE toproject the motions of the two datasets into a latent space consisting ofquantized vectors, which effectively mix the motion tokens from the twodatasets with different distributions for training. Additionally, we propose across-modal transformer to integrate text instructions into motion generationarchitecture for generating 3D dance movements without degrading theperformance of music-conditioned dance generation. To better evaluate thequality of the generated motion, we introduce two novel metrics, namely MotionPrediction Distance (MPD) and Freezing Score (FS), to measure the coherence andfreezing percentage of the generated motion. Extensive experiments show thatour approach can generate realistic and coherent dance movements conditioned onboth text and music while maintaining comparable performance with the twosingle modalities. Code is available at https://garfield-kh.github.io/TM2D/.</description><author>Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo, Zihang Jiang, Xinxin Zuo, Michael Bi Mi, Xinchao Wang</author><pubDate>Sun, 01 Oct 2023 16:23:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02419v2</guid></item><item><title>MMASD: A Multimodal Dataset for Autism Intervention Analysis</title><link>http://arxiv.org/abs/2306.08243v3</link><description>Autism spectrum disorder (ASD) is a developmental disorder characterized bysignificant social communication impairments and difficulties perceiving andpresenting communication cues. Machine learning techniques have been broadlyadopted to facilitate autism studies and assessments. However, computationalmodels are primarily concentrated on specific analysis and validated on privatedatasets in the autism community, which limits comparisons across models due toprivacy-preserving data sharing complications. This work presents a novelprivacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmarkdataset, collected from play therapy interventions of children with Autism.MMASD includes data from 32 children with ASD, and 1,315 data samples segmentedfrom over 100 hours of intervention recordings. To promote public access, eachdata sample consists of four privacy-preserving modalities of data; some ofwhich are derived from original videos: (1) optical flow, (2) 2D skeleton, (3)3D skeleton, and (4) clinician ASD evaluation scores of children, e.g., ADOSscores. MMASD aims to assist researchers and therapists in understandingchildren's cognitive status, monitoring their progress during therapy, andcustomizing the treatment plan accordingly. It also has inspiration fordownstream tasks such as action quality assessment and interpersonal synchronyestimation. MMASD dataset can be easily accessed athttps://github.com/Li-Jicheng/MMASD-A-Multimodal-Dataset-for-Autism-Intervention-Analysis.</description><author>Jicheng Li, Vuthea Chheang, Pinar Kullu, Eli Brignac, Zhang Guo, Kenneth E. Barner, Anjana Bhat, Roghayeh Leila Barmaki</author><pubDate>Sun, 01 Oct 2023 16:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08243v3</guid></item><item><title>Reconstructing Human Expressiveness in Piano Performances with a Transformer Network</title><link>http://arxiv.org/abs/2306.06040v2</link><description>Capturing intricate and subtle variations in human expressiveness in musicperformance using computational approaches is challenging. In this paper, wepropose a novel approach for reconstructing human expressiveness in pianoperformance with a multi-layer bi-directional Transformer encoder. To addressthe needs for large amounts of accurately captured and score-alignedperformance data in training neural networks, we use transcribed scoresobtained from an existing transcription model to train our model. We integratepianist identities to control the sampling process and explore the ability ofour system to model variations in expressiveness for different pianists. Thesystem is evaluated through statistical analysis of generated expressiveperformances and a listening test. Overall, the results suggest that our methodachieves state-of-the-art in generating human-like piano performances fromtranscribed scores, while fully and consistently reconstructing humanexpressiveness poses further challenges.</description><author>Jingjing Tang, Geraint Wiggins, Gyorgy Fazekas</author><pubDate>Sun, 01 Oct 2023 16:14:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06040v2</guid></item><item><title>DreamDecompiler: Bayesian Program Learning by Decompiling Amortised Knowledge</title><link>http://arxiv.org/abs/2306.07856v2</link><description>Solving program induction problems requires searching through an enormousspace of possibilities. DreamCoder is an inductive program synthesis systemthat, whilst solving problems, learns to simplify search in an iterativewake-sleep procedure. The cost of search is amortised by training a neuralsearch policy, reducing search breadth and effectively "compiling" usefulinformation to compose program solutions across tasks. Additionally, a libraryof program components is learnt to express discovered solutions in fewercomponents, reducing search depth. In DreamCoder, the neural search policy hasonly an indirect effect on the library learnt through the program solutions ithelps discover. We present an approach for library learning that directlyleverages the neural search policy, effectively "decompiling" its amortisedknowledge to extract relevant program components. This provides strongeramortised inference: the amortised knowledge learnt to reduce search breadth isnow also used to reduce search depth. We integrate our approach with DreamCoderand demonstrate faster domain proficiency with improved generalisation on arange of domains, particularly when fewer example solutions are available.</description><author>Alessandro B. Palmarini, Christopher G. Lucas, N. Siddharth</author><pubDate>Sun, 01 Oct 2023 16:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07856v2</guid></item><item><title>Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic Scenarios with Mean-Field Equilibria</title><link>http://arxiv.org/abs/2309.16263v2</link><description>Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-AgentReinforcement Learning (MARL), often requiring agents to balance individualgains with collective rewards. In this regard, this paper aims to investigatestrategies to invoke cooperation in game-theoretic scenarios, namely theIterated Prisoner's Dilemma, where agents must optimize both individual andgroup outcomes. Existing cooperative strategies are analyzed for theireffectiveness in promoting group-oriented behavior in repeated games.Modifications are proposed where encouraging group rewards will also result ina higher individual gain, addressing real-world dilemmas seen in distributedsystems. The study extends to scenarios with exponentially growing agentpopulations ($N \longrightarrow +\infty$), where traditional computation andequilibrium determination are challenging. Leveraging mean-field game theory,equilibrium solutions and reward structures are established for infinitelylarge agent sets in repeated games. Finally, practical insights are offeredthrough simulations using the Multi Agent-Posthumous Credit Assignment trainer,and the paper explores adapting simulation algorithms to create scenariosfavoring cooperation for group rewards. These practical implementations bridgetheoretical concepts with real-world applications.</description><author>Vaigarai Sathi, Sabahat Shaik, Jaswanth Nidamanuri</author><pubDate>Sun, 01 Oct 2023 15:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16263v2</guid></item><item><title>KITE: Keypoint-Conditioned Policies for Semantic Manipulation</title><link>http://arxiv.org/abs/2306.16605v3</link><description>While natural language offers a convenient shared interface for humans androbots, enabling robots to interpret and follow language commands remains alongstanding challenge in manipulation. A crucial step to realizing aperformant instruction-following robot is achieving semantic manipulation,where a robot interprets language at different specificities, from high-levelinstructions like "Pick up the stuffed animal" to more detailed inputs like"Grab the left ear of the elephant." To tackle this, we propose Keypoints +Instructions to Execution (KITE), a two-step framework for semanticmanipulation which attends to both scene semantics (distinguishing betweendifferent objects in a visual scene) and object semantics (precisely localizingdifferent parts within an object instance). KITE first grounds an inputinstruction in a visual scene through 2D image keypoints, providing a highlyaccurate object-centric bias for downstream action inference. Provided an RGB-Dscene observation, KITE then executes a learned keypoint-conditioned skill tocarry out the instruction. The combined precision of keypoints andparameterized skills enables fine-grained manipulation with generalization toscene and object variations. Empirically, we demonstrate KITE in 3 real-worldenvironments: long-horizon 6-DoF tabletop manipulation, semantic grasping, anda high-precision coffee-making task. In these settings, KITE achieves a 75%,70%, and 71% overall success rate for instruction-following, respectively. KITEoutperforms frameworks that opt for pre-trained visual language models overkeypoint-based grounding, or omit skills in favor of end-to-end visuomotorcontrol, all while being trained from fewer or comparable amounts ofdemonstrations. Supplementary material, datasets, code, and videos can be foundon our website: http://tinyurl.com/kite-site.</description><author>Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, Jeannette Bohg</author><pubDate>Sun, 01 Oct 2023 15:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16605v3</guid></item><item><title>Class Incremental Learning via Likelihood Ratio Based Task Prediction</title><link>http://arxiv.org/abs/2309.15048v2</link><description>Class incremental learning (CIL) is a challenging setting of continuallearning, which learns a series of tasks sequentially. Each task consists of aset of unique classes. The key feature of CIL is that no task identifier (ortask-id) is provided at test time for each test sample. Predicting the task-idfor each test sample is a challenging problem. An emerging theoreticallyjustified and effective approach is to train a task-specific model for eachtask in a shared network for all tasks based on a task-incremental learning(TIL) method to deal with forgetting. The model for each task in this approachis an out-of-distribution (OOD) detector rather than a conventional classifier.The OOD detector can perform both within-task (in-distribution (IND)) classprediction and OOD detection. The OOD detection capability is the key fortask-id prediction during inference for each test sample. However, this paperargues that using a traditional OOD detector for task-id prediction issub-optimal because additional information (e.g., the replay data and thelearned tasks) available in CIL can be exploited to design a better andprincipled method for task-id prediction. We call the new method TPLR (Task-idPrediction based on Likelihood Ratio}). TPLR markedly outperforms strong CILbaselines.</description><author>Haowei Lin, Yijia Shao, Weinan Qian, Ningxin Pan, Yiduo Guo, Bing Liu</author><pubDate>Sun, 01 Oct 2023 15:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15048v2</guid></item><item><title>Towards Probabilistic Causal Discovery, Inference &amp; Explanations for Autonomous Drones in Mine Surveying Tasks</title><link>http://arxiv.org/abs/2308.10047v2</link><description>Causal modelling offers great potential to provide autonomous agents theability to understand the data-generation process that governs theirinteractions with the world. Such models capture formal knowledge as well asprobabilistic representations of noise and uncertainty typically encountered byautonomous robots in real-world environments. Thus, causality can aidautonomous agents in making decisions and explaining outcomes, but deployingcausality in such a manner introduces new challenges. Here we identifychallenges relating to causality in the context of a drone system operating ina salt mine. Such environments are challenging for autonomous agents because ofthe presence of confounders, non-stationarity, and a difficulty in buildingcomplete causal models ahead of time. To address these issues, we propose aprobabilistic causal framework consisting of: causally-informed POMDP planning,online SCM adaptation, and post-hoc counterfactual explanations. Further, weoutline planned experimentation to evaluate the framework integrated with adrone system in simulated mine environments and on a real-world mine dataset.</description><author>Ricardo Cannizzaro, Rhys Howard, Paulina Lewinska, Lars Kunze</author><pubDate>Sun, 01 Oct 2023 15:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10047v2</guid></item><item><title>ClusterFormer: Clustering As A Universal Visual Learner</title><link>http://arxiv.org/abs/2309.13196v2</link><description>This paper presents CLUSTERFORMER, a universal vision model that is based onthe CLUSTERing paradigm with TransFORMER. It comprises two novel designs: 1.recurrent cross-attention clustering, which reformulates the cross-attentionmechanism in Transformer and enables recursive updates of cluster centers tofacilitate strong representation learning; and 2. feature dispatching, whichuses the updated cluster centers to redistribute image features throughsimilarity-based metrics, resulting in a transparent pipeline. This elegantdesign streamlines an explainable and transferable workflow, capable oftackling heterogeneous vision tasks (i.e., image classification, objectdetection, and image segmentation) with varying levels of clusteringgranularity (i.e., image-, box-, and pixel-level). Empirical resultsdemonstrate that CLUSTERFORMER outperforms various well-known specializedarchitectures, achieving 83.41% top-1 acc. over ImageNet-1K for imageclassification, 54.2% and 47.0% mAP over MS COCO for object detection andinstance segmentation, 52.4% mIoU over ADE20K for semantic segmentation, and55.8% PQ over COCO Panoptic for panoptic segmentation. For its efficacy, wehope our work can catalyze a paradigm shift in universal models in computervision.</description><author>James C. Liang, Yiming Cui, Qifan Wang, Tong Geng, Wenguan Wang, Dongfang Liu</author><pubDate>Sun, 01 Oct 2023 15:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13196v2</guid></item><item><title>Large-Scale Bidirectional Training for Zero-Shot Image Captioning</title><link>http://arxiv.org/abs/2211.06774v3</link><description>When trained on large-scale datasets, image captioning models can understandthe content of images from a general domain but often fail to generateaccurate, detailed captions. To improve performance, pretraining-and-finetuninghas been a key strategy for image captioning. However, we find that large-scalebidirectional training between image and text enables zero-shot imagecaptioning. In this paper, we introduce Bidirectional Image Text Training inlargER Scale, BITTERS, an efficient training and inference framework forzero-shot image captioning. We also propose a new evaluation benchmark whichcomprises of high quality datasets and an extensive set of metrics to properlyevaluate zero-shot captioning accuracy and societal bias. We additionallyprovide an efficient finetuning approach for keyword extraction. We show thatcareful selection of large-scale training set and model architecture is the keyto achieving zero-shot image captioning.</description><author>Taehoon Kim, Mark Marsden, Pyunghwan Ahn, Sangyun Kim, Sihaeng Lee, Alessandra Sala, Seung Hwan Kim</author><pubDate>Sun, 01 Oct 2023 14:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06774v3</guid></item><item><title>Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data</title><link>http://arxiv.org/abs/2309.09454v2</link><description>The asymptotically efficient online learning problem is investigated forstochastic censored regression models, which arise from various fields oflearning and statistics but up to now still lacks comprehensive theoreticalstudies on the efficiency of the learning algorithms. For this, we propose atwo-step online algorithm, where the first step focuses on achieving algorithmconvergence, and the second step is dedicated to improving the estimationperformance. Under a general excitation condition on the data, we show that ouralgorithm is strongly consistent and asymptotically normal by employing thestochastic Lyapunov function method and limit theories for martingales.Moreover, we show that the covariances of the estimates can achieve theCramer-Rao (C-R) bound asymptotically, indicating that the performance of theproposed algorithm is the best possible that one can expect in general. Unlikemost of the existing works, our results are obtained without resorting to thetraditionally used but stringent conditions such as independent and identicallydistributed (i.i.d) assumption on the data, and thus our results do not excludeapplications to stochastic dynamical systems with feedback. A numerical exampleis also provided to illustrate the superiority of the proposed online algorithmover the existing related ones in the literature.</description><author>Lantian Zhang, Lei Guo</author><pubDate>Sun, 01 Oct 2023 14:45:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09454v2</guid></item><item><title>Approximation Guarantees for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II)</title><link>http://arxiv.org/abs/2203.02693v3</link><description>Recent theoretical works have shown that the NSGA-II efficiently computes thefull Pareto front when the population size is large enough. In this work, westudy how well it approximates the Pareto front when the population size issmaller. For the OneMinMax benchmark, we point out situations in which the parents andoffspring cover well the Pareto front, but the next population has large gapson the Pareto front. Our mathematical proofs suggest as reason for thisundesirable behavior that the NSGA-II in the selection stage computes thecrowding distance once and then removes individuals with smallest crowdingdistance without considering that a removal increases the crowding distance ofsome individuals. We then analyze two variants not prone to this problem. For the NSGA-II thatupdates the crowding distance after each removal (Kukkonen and Deb (2006)) andthe steady-state NSGA-II (Nebro and Durillo (2009)), we prove that the gaps inthe Pareto front are never more than a small constant factor larger than thetheoretical minimum. This is the first mathematical work on the approximationability of the NSGA-II and the first runtime analysis for the steady-stateNSGA-II. Experiments also show the superior approximation ability of the twoNSGA-II variants.</description><author>Weijie Zheng, Benjamin Doerr</author><pubDate>Sun, 01 Oct 2023 14:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.02693v3</guid></item><item><title>ResFields: Residual Neural Fields for Spatiotemporal Signals</title><link>http://arxiv.org/abs/2309.03160v2</link><description>Neural fields, a category of neural networks trained to representhigh-frequency signals, have gained significant attention in recent years dueto their impressive performance in modeling complex 3D data, especially largeneural signed distance (SDFs) or radiance fields (NeRFs) via a singlemulti-layer perceptron (MLP). However, despite the power and simplicity ofrepresenting signals with an MLP, these methods still face challenges whenmodeling large and complex temporal signals due to the limited capacity ofMLPs. In this paper, we propose an effective approach to address thislimitation by incorporating temporal residual layers into neural fields, dubbedResFields, a novel class of networks specifically designed to effectivelyrepresent complex temporal signals. We conduct a comprehensive analysis of theproperties of ResFields and propose a matrix factorization technique to reducethe number of trainable parameters and enhance generalization capabilities.Importantly, our formulation seamlessly integrates with existing techniques andconsistently improves results across various challenging tasks: 2D videoapproximation, dynamic shape modeling via temporal SDFs, and dynamic NeRFreconstruction. Lastly, we demonstrate the practical utility of ResFields byshowcasing its effectiveness in capturing dynamic 3D scenes from sparse sensoryinputs of a lightweight capture system.</description><author>Marko Mihajlovic, Sergey Prokudin, Marc Pollefeys, Siyu Tang</author><pubDate>Sun, 01 Oct 2023 14:16:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03160v2</guid></item><item><title>Enhancing Robustness of AI Offensive Code Generators via Data Augmentation</title><link>http://arxiv.org/abs/2306.05079v2</link><description>In this work, we present a method to add perturbations to the codedescriptions to create new inputs in natural language (NL) fromwell-intentioned developers that diverge from the original ones due to the useof new words or because they miss part of them. The goal is to analyze how andto what extent perturbations affect the performance of AI code generators inthe context of security-oriented code. First, we show that perturbeddescriptions preserve the semantics of the original, non-perturbed ones. Then,we use the method to assess the robustness of three state-of-the-art codegenerators against the newly perturbed inputs, showing that the performance ofthese AI-based solutions is highly affected by perturbations in the NLdescriptions. To enhance their robustness, we use the method to perform dataaugmentation, i.e., to increase the variability and diversity of the NLdescriptions in the training data, proving its effectiveness against bothperturbed and non-perturbed code descriptions.</description><author>Cristina Improta, Pietro Liguori, Roberto Natella, Bojan Cukic, Domenico Cotroneo</author><pubDate>Sun, 01 Oct 2023 14:01:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05079v2</guid></item><item><title>Building and Road Segmentation Using EffUNet and Transfer Learning Approach</title><link>http://arxiv.org/abs/2307.03980v2</link><description>In city, information about urban objects such as water supply, railway lines,power lines, buildings, roads, etc., is necessary for city planning. Inparticular, information about the spread of these objects, locations andcapacity is needed for the policymakers to make impactful decisions. Thisthesis aims to segment the building and roads from the aerial image captured bythe satellites and UAVs. Many different architectures have been proposed forthe semantic segmentation task and UNet being one of them. In this thesis, wepropose a novel architecture based on Google's newly proposed EfficientNetV2 asan encoder for feature extraction with UNet decoder for constructing thesegmentation map. Using this approach we achieved a benchmark score for theMassachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153respectively.</description><author>Sahil Gangurde</author><pubDate>Sun, 01 Oct 2023 13:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03980v2</guid></item><item><title>Unsupervised Polychromatic Neural Representation for CT Metal Artifact Reduction</title><link>http://arxiv.org/abs/2306.15203v2</link><description>Emerging neural reconstruction techniques based on tomography (e.g., NeRF,NeAT, and NeRP) have started showing unique capabilities in medical imaging. Inthis work, we present a novel Polychromatic neural representation (Polyner) totackle the challenging problem of CT imaging when metallic implants existwithin the human body. CT metal artifacts arise from the drastic variation ofmetal's attenuation coefficients at various energy levels of the X-rayspectrum, leading to a nonlinear metal effect in CT measurements. Recovering CTimages from metal-affected measurements hence poses a complicated nonlinearinverse problem where empirical models adopted in previous metal artifactreduction (MAR) approaches lead to signal loss and strongly aliasedreconstructions. Polyner instead models the MAR problem from a nonlinearinverse problem perspective. Specifically, we first derive a polychromaticforward model to accurately simulate the nonlinear CT acquisition process.Then, we incorporate our forward model into the implicit neural representationto accomplish reconstruction. Lastly, we adopt a regularizer to preserve thephysical properties of the CT images across different energy levels whileeffectively constraining the solution space. Our Polyner is an unsupervisedmethod and does not require any external training data. Experimenting withmultiple datasets shows that our Polyner achieves comparable or betterperformance than supervised methods on in-domain datasets while demonstratingsignificant performance improvements on out-of-domain datasets. To the best ofour knowledge, our Polyner is the first unsupervised MAR method thatoutperforms its supervised counterparts. The code for this work is availableat: https://github.com/iwuqing/Polyner.</description><author>Qing Wu, Lixuan Chen, Ce Wang, Hongjiang Wei, S. Kevin Zhou, Jingyi Yu, Yuyao Zhang</author><pubDate>Sun, 01 Oct 2023 12:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15203v2</guid></item><item><title>Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models</title><link>http://arxiv.org/abs/2307.03723v2</link><description>Control of surface texture in strip steel is essential to meet customerrequirements during galvanizing and temper rolling processes. Traditionalmethods rely on post-production stylus measurements, while on-line techniquesoffer non-contact and real-time measurements of the entire strip. However,ensuring accurate measurement is imperative for their effective utilization inthe manufacturing pipeline. Moreover, accurate on-line measurements enablereal-time adjustments of manufacturing processing parameters during production,ensuring consistent quality and the possibility of closed-loop control of thetemper mill. In this study, we leverage state-of-the-art machine learningmodels to enhance the transformation of on-line measurements into significantlya more accurate Ra surface roughness metric. By comparing a selection ofdata-driven approaches, including both deep learning and non-deep learningmethods, to the close-form transformation, we evaluate their potential forimproving surface texture control in temper strip steel manufacturing.</description><author>Alex Milne, Xianghua Xie</author><pubDate>Sun, 01 Oct 2023 12:21:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03723v2</guid></item><item><title>DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder</title><link>http://arxiv.org/abs/2303.17550v4</link><description>While recent research has made significant progress in speech-driven talkingface generation, the quality of the generated video still lags behind that ofreal recordings. One reason for this is the use of handcrafted intermediaterepresentations like facial landmarks and 3DMM coefficients, which are designedbased on human knowledge and are insufficient to precisely describe facialmovements. Additionally, these methods require an external pretrained model forextracting these representations, whose performance sets an upper bound ontalking face generation. To address these limitations, we propose a novelmethod called DAE-Talker that leverages data-driven latent representationsobtained from a diffusion autoencoder (DAE). DAE contains an image encoder thatencodes an image into a latent vector and a DDIM image decoder thatreconstructs the image from it. We train our DAE on talking face video framesand then extract their latent representations as the training target for aConformer-based speech2latent model. This allows DAE-Talker to synthesize fullvideo frames and produce natural head movements that align with the content ofspeech, rather than relying on a predetermined head pose from a template video.We also introduce pose modelling in speech2latent for pose controllability.Additionally, we propose a novel method for generating continuous video frameswith the DDIM image decoder trained on individual frames, eliminating the needfor modelling the joint distribution of consecutive frames directly. Ourexperiments show that DAE-Talker outperforms existing popular methods inlip-sync, video fidelity, and pose naturalness. We also conduct ablationstudies to analyze the effectiveness of the proposed techniques and demonstratethe pose controllability of DAE-Talker.</description><author>Chenpeng Du, Qi Chen, Tianyu He, Xu Tan, Xie Chen, Kai Yu, Sheng Zhao, Jiang Bian</author><pubDate>Sun, 01 Oct 2023 12:20:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17550v4</guid></item><item><title>Measuring the Instability of Fine-Tuning</title><link>http://arxiv.org/abs/2302.07778v2</link><description>Fine-tuning pre-trained language models on downstream tasks with varyingrandom seeds has been shown to be unstable, especially on small datasets. Manyprevious studies have investigated this instability and proposed methods tomitigate it. However, most studies only used the standard deviation ofperformance scores (SD) as their measure, which is a narrow characterization ofinstability. In this paper, we analyze SD and six other measures quantifyinginstability at different levels of granularity. Moreover, we propose asystematic framework to evaluate the validity of these measures. Finally, weanalyze the consistency and difference between different measures byreassessing existing instability mitigation methods. We hope our results willinform the development of better measurements of fine-tuning instability.</description><author>Yupei Du, Dong Nguyen</author><pubDate>Sun, 01 Oct 2023 11:38:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07778v2</guid></item><item><title>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</title><link>http://arxiv.org/abs/2307.12856v2</link><description>Pre-trained large language models (LLMs) have recently achieved bettergeneralization and sample efficiency in autonomous web automation. However, theperformance on real-world websites has still suffered from (1) open domainness,(2) limited context length, and (3) lack of inductive bias on HTML. Weintroduce WebAgent, an LLM-driven agent that learns from self-experience tocomplete tasks on real websites following natural language instructions.WebAgent plans ahead by decomposing instructions into canonicalsub-instructions, summarizes long HTML documents into task-relevant snippets,and acts on websites via Python programs generated from those. We designWebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, newpre-trained LLMs for long HTML documents using local and global attentionmechanisms and a mixture of long-span denoising objectives, for planning andsummarization. We empirically demonstrate that our modular recipe improves thesuccess on real websites by over 50%, and that HTML-T5 is the best model tosolve various HTML understanding tasks; achieving 18.7% higher success ratethan the prior method on MiniWoB web automation benchmark, and SoTA performanceon Mind2Web, an offline task planning evaluation.</description><author>Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust</author><pubDate>Sun, 01 Oct 2023 11:30:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12856v2</guid></item><item><title>Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Map</title><link>http://arxiv.org/abs/2302.00456v2</link><description>Given that Transformers are ubiquitous in wide tasks, interpreting theirinternals is a pivotal issue. Still, their particular components, feed-forward(FF) blocks, have typically been less analyzed despite their substantialparameter amounts. We analyze the input contextualization effects of FF blocksby rendering them in the attention maps as a human-friendly visualizationscheme. Our experiments with both masked- and causal-language models revealthat FF networks modify the input contextualization to emphasize specific typesof linguistic compositions. In addition, FF and its surrounding components tendto cancel out each other's effects, suggesting potential redundancy in theprocessing of the Transformer layer.</description><author>Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui</author><pubDate>Sun, 01 Oct 2023 11:27:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00456v2</guid></item><item><title>Multimodal Web Navigation with Instruction-Finetuned Foundation Models</title><link>http://arxiv.org/abs/2305.11854v2</link><description>The progress of autonomous web navigation has been hindered by the dependenceon billions of exploratory interactions via online reinforcement learning, anddomain-specific model designs that make it difficult to leverage generalizationfrom rich out-of-domain data. In this work, we study data-driven offlinetraining for web agents with vision-language foundation models. We propose aninstruction-following multimodal agent, WebGUM, that observes both webpagescreenshots and HTML pages and outputs web navigation actions, such as clickand type. WebGUM is trained by jointly finetuning an instruction-finetunedlanguage model and a vision encoder with temporal and local perception on alarge corpus of demonstrations. We empirically demonstrate this recipe improvesthe agent's ability of grounded multimodal perception, HTML comprehension, andmulti-step reasoning, outperforming prior works by a significant margin. On theMiniWoB, we improve over the previous best offline methods by more than 45.8%,even outperforming online-finetuned SoTA, humans, and GPT-4-based agent. On theWebShop benchmark, our 3-billion-parameter model achieves superior performanceto the existing SoTA, PaLM-540B. Furthermore, WebGUM exhibits strong positivetransfer to the real-world planning tasks on the Mind2Web. We also collect 347Khigh-quality demonstrations using our trained models, 38 times larger thanprior work, and make them available to promote future research in thisdirection.</description><author>Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, Izzeddin Gur</author><pubDate>Sun, 01 Oct 2023 11:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11854v2</guid></item><item><title>Generating Transferable and Stealthy Adversarial Patch via Attention-guided Adversarial Inpainting</title><link>http://arxiv.org/abs/2308.05320v2</link><description>Adversarial patch attacks can fool the face recognition (FR) models via smallpatches. However, previous adversarial patch attacks often result in unnaturalpatterns that are easily noticeable. Generating transferable and stealthyadversarial patches that can efficiently deceive the black-box FR models whilehaving good camouflage is challenging because of the huge stylistic differencebetween the source and target images. To generate transferable,natural-looking, and stealthy adversarial patches, we propose an innovativetwo-stage attack called Adv-Inpainting, which extracts style features andidentity features from the attacker and target faces, respectively and thenfills the patches with misleading and inconspicuous content guided by attentionmaps. In the first stage, we extract multi-scale style embeddings by apyramid-like network and identity embeddings by a pretrained FR model andpropose a novel Attention-guided Adaptive Instance Normalization layer (AAIN)to merge them via background-patch cross-attention maps. The proposed layer canadaptively fuse identity and style embeddings by fully exploiting prioritycontextual information. In the second stage, we design an Adversarial PatchRefinement Network (APR-Net) with a novel boundary variance loss, a spatialdiscounted reconstruction loss, and a perceptual loss to boost the stealthinessfurther. Experiments demonstrate that our attack can generate adversarialpatches with improved visual quality, better stealthiness, and strongertransferability than state-of-the-art adversarial patch attacks and semanticattacks.</description><author>Yanjie Li, Mingxing Duan, Xuelong Dai, Bin Xiao</author><pubDate>Sun, 01 Oct 2023 10:14:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05320v2</guid></item><item><title>Time Travel in LLMs: Tracing Data Contamination in Large Language Models</title><link>http://arxiv.org/abs/2308.08493v2</link><description>Data contamination, i.e., the presence of test data from downstream tasks inthe training data of large language models (LLMs), is a potential major issuein measuring LLMs' real effectiveness on other tasks. We propose astraightforward yet effective method for identifying data contamination withinLLMs. At its core, our approach starts by identifying potential contaminationat the instance level; using this information, our approach then assesses widercontamination at the partition level. To estimate contamination of individualinstances, we employ "guided instruction:" a prompt consisting of the datasetname, partition type, and the random-length initial segment of a referenceinstance, asking the LLM to complete it. An instance is flagged as contaminatedif the LLM's output either exactly or nearly matches the latter segment of thereference. To understand if an entire partition is contaminated, we propose twoideas. The first idea marks a dataset partition as contaminated if the averageoverlap score with the reference instances (as measured by ROUGE-L or BLEURT)is statistically significantly better with the completions from guidedinstruction compared to a "general instruction" that does not include thedataset and partition name. The second idea marks a dataset partition ascontaminated if a classifier based on GPT-4 with few-shot in-context learningprompt marks multiple generated completions as exact/near-exact matches of thecorresponding reference instances. Our best method achieves an accuracy between92% and 100% in detecting if an LLM is contaminated with seven datasets,containing train and test/validation partitions, when contrasted with manualevaluation by human experts. Further, our findings indicate that GPT-4 iscontaminated with AG News, WNLI, and XSum datasets.</description><author>Shahriar Golchin, Mihai Surdeanu</author><pubDate>Sun, 01 Oct 2023 10:11:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08493v2</guid></item><item><title>Efficient Explorative Key-term Selection Strategies for Conversational Contextual Bandits</title><link>http://arxiv.org/abs/2303.00315v2</link><description>Conversational contextual bandits elicit user preferences by occasionallyquerying for explicit feedback on key-terms to accelerate learning. However,there are aspects of existing approaches which limit their performance. First,information gained from key-term-level conversations and arm-levelrecommendations is not appropriately incorporated to speed up learning. Second,it is important to ask explorative key-terms to quickly elicit the user'spotential interests in various domains to accelerate the convergence of userpreference estimation, which has never been considered in existing works. Totackle these issues, we first propose ``ConLinUCB", a general framework forconversational bandits with better information incorporation, combiningarm-level and key-term-level feedback to estimate user preference in one stepat each time. Based on this framework, we further design two bandit algorithmswith explorative key-term selection strategies, ConLinUCB-BS and ConLinUCB-MCR.We prove tighter regret upper bounds of our proposed algorithms. Particularly,ConLinUCB-BS achieves a regret bound of $O(d\sqrt{T\log T})$, better than theprevious result $O(d\sqrt{T}\log T)$. Extensive experiments on synthetic andreal-world data show significant advantages of our algorithms in learningaccuracy (up to 54\% improvement) and computational efficiency (up to 72\%improvement), compared to the classic ConUCB algorithm, showing the potentialbenefit to recommender systems.</description><author>Zhiyong Wang, Xutong Liu, Shuai Li, John C. S. Lui</author><pubDate>Sun, 01 Oct 2023 09:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00315v2</guid></item><item><title>Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation</title><link>http://arxiv.org/abs/2302.06857v2</link><description>Creating the photo-realistic version of people sketched portraits is usefulto various entertainment purposes. Existing studies only generate portraits inthe 2D plane with fixed views, making the results less vivid. In this paper, wepresent Stereoscopic Simplified Sketch-to-Portrait (SSSP), which explores thepossibility of creating Stereoscopic 3D-aware portraits from simple contoursketches by involving 3D generative models. Our key insight is to designsketch-aware constraints that can fully exploit the prior knowledge of atri-plane-based 3D-aware generative model. Specifically, our designedregion-aware volume rendering strategy and global consistency constraintfurther enhance detail correspondences during sketch encoding. Moreover, inorder to facilitate the usage of layman users, we propose a Contour-to-Sketchmodule with vector quantized representations, so that easily drawn contours candirectly guide the generation of 3D portraits. Extensive comparisons show thatour method generates high-quality results that match the sketch. Our usabilitystudy verifies that our system is greatly preferred by user.</description><author>Yasheng Sun, Qianyi Wu, Hang Zhou, Kaisiyuan Wang, Tianshu Hu, Chen-Chieh Liao, Shio Miyafuji, Ziwei Liu, Hideki Koike</author><pubDate>Sun, 01 Oct 2023 09:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06857v2</guid></item><item><title>L-MAE: Masked Autoencoders are Semantic Segmentation Datasets Augmenter</title><link>http://arxiv.org/abs/2211.11242v2</link><description>Generating semantic segmentation datasets has consistently been laborious andtime-consuming, particularly in the context of large models or specializeddomains(i.e. Medical Imaging or Remote Sensing). Specifically, large modelsnecessitate a substantial volume of data, while datasets in professionaldomains frequently require the involvement of domain experts. Both scenariosare susceptible to inaccurate data labeling, which can significantly affect theultimate performance of the trained model. This paper proposes a simple andeffective label pixel-level completion method, \textbf{Label Mask AutoEncoder}(L-MAE), which fully uses the existing information in the label to generate thecomplete label. The proposed model are the first to apply the Mask Auto-Encoderto downstream tasks. In detail, L-MAE adopts the fusion strategy that stacksthe label and the corresponding image, namely fuse map. Moreover, since some ofthe image information is lost when masking the fuse map, direct reconstructionmay lead to poor performance. We proposed Image Patch Supplement algorithm tosupplement the missing information during the mask-reconstruct process, andempirically found that an average of 4.1\% mIoU can be improved. We conducted a experiment to evaluate the efficacy of L-MAE to complete thedataset. We employed a degraded Pascal VOC dataset and the degraded datasetenhanced by L-MAE to train an identical conventional semantic segmentationmodel for the initial set of experiments. The results of these experimentsdemonstrate a performance enhancement of 13.5\% in the model trained with theL-MAE-enhanced dataset compared to the unenhanced dataset.</description><author>Jiaru Jia, Mingzhe Liu, Jiake Xie, Xin Chen, Hong Zhang, Feixiang Zhao, Aiqing Yang</author><pubDate>Sun, 01 Oct 2023 08:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11242v2</guid></item><item><title>Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation</title><link>http://arxiv.org/abs/2305.15852v2</link><description>Large language models (large LMs) are susceptible to producing text thatcontains hallucinated content. An important instance of this problem isself-contradiction, where the LM generates two contradictory sentences withinthe same context. In this work, we present a comprehensive investigation intoself-contradiction for various instruction-tuned LMs, covering evaluation,detection, and mitigation. Our analysis reveals the prevalence ofself-contradictions when LMs generate text for open-domain topics, e.g., in17.7% of all sentences produced by ChatGPT. Self-contradiction also complementsretrieval-based methods, as a large portion of them (e.g., 35.8% for ChatGPT)cannot be verified using Wikipedia. We then propose a novel prompting-basedframework designed to effectively detect and mitigate self-contradictions. Ourdetector achieves high accuracy, e.g., around 80% F1 score when promptingChatGPT. The mitigation algorithm iteratively refines the generated text toremove contradictory information while preserving text fluency andinformativeness. Importantly, our entire framework is applicable to black-boxLMs and does not require external grounded knowledge. Our approach ispractically effective and has been released as a push-button tool to benefitthe public, available at https://chatprotect.ai/.</description><author>Niels Mündler, Jingxuan He, Slobodan Jenko, Martin Vechev</author><pubDate>Sun, 01 Oct 2023 08:22:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15852v2</guid></item><item><title>HighLight: Efficient and Flexible DNN Acceleration with Hierarchical Structured Sparsity</title><link>http://arxiv.org/abs/2305.12718v2</link><description>Due to complex interactions among various deep neural network (DNN)optimization techniques, modern DNNs can have weights and activations that aredense or sparse with diverse sparsity degrees. To offer a good trade-offbetween accuracy and hardware performance, an ideal DNN accelerator should havehigh flexibility to efficiently translate DNN sparsity into reductions inenergy and/or latency without incurring significant complexity overhead. This paper introduces hierarchical structured sparsity (HSS), with the keyinsight that we can systematically represent diverse sparsity degrees by havingthem hierarchically composed from multiple simple sparsity patterns. As aresult, HSS simplifies the underlying hardware since it only needs to supportsimple sparsity patterns; this significantly reduces the sparsity accelerationoverhead, which improves efficiency. Motivated by such opportunities, wepropose a simultaneously efficient and flexible accelerator, named HighLight,to accelerate DNNs that have diverse sparsity degrees (including dense). Due tothe flexibility of HSS, different HSS patterns can be introduced to DNNs tomeet different applications' accuracy requirements. Compared to existing works,HighLight achieves a geomean of up to 6.4x better energy-delay product (EDP)across workloads with diverse sparsity degrees, and always sits on theEDP-accuracy Pareto frontier for representative DNNs</description><author>Yannan Nellie Wu, Po-An Tsai, Saurav Muralidharan, Angshuman Parashar, Vivienne Sze, Joel S. Emer</author><pubDate>Sun, 01 Oct 2023 07:33:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12718v2</guid></item><item><title>CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts</title><link>http://arxiv.org/abs/2309.05494v2</link><description>Social media platforms play an essential role in crisis communication, butanalyzing crisis-related social media texts is challenging due to theirinformal nature. Transformer-based pre-trained models like BERT and RoBERTahave shown success in various NLP tasks, but they are not tailored forcrisis-related texts. Furthermore, general-purpose sentence encoders are usedto generate sentence embeddings, regardless of the textual complexities incrisis-related texts. Advances in applications like text classification,semantic search, and clustering contribute to effective processing ofcrisis-related texts, which is essential for emergency responders to gain acomprehensive view of a crisis event, whether historical or real-time. Toaddress these gaps in crisis informatics literature, this study introducesCrisisTransformers, an ensemble of pre-trained language models and sentenceencoders trained on an extensive corpus of over 15 billion word tokens fromtweets associated with more than 30 crisis events, including disease outbreaks,natural disasters, conflicts, and other critical incidents. We evaluateexisting models and CrisisTransformers on 18 crisis-specific public datasets.Our pre-trained models outperform strong baselines across all datasets inclassification tasks, and our best-performing sentence encoder improves thestate-of-the-art by 17.43% in sentence encoding tasks. Additionally, weinvestigate the impact of model initialization on convergence and evaluate thesignificance of domain-specific models in generating semantically meaningfulsentence embeddings. All models are publicly released(https://huggingface.co/crisistransformers), with the anticipation that theywill serve as a robust baseline for tasks involving the analysis ofcrisis-related social media texts.</description><author>Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera</author><pubDate>Sun, 01 Oct 2023 06:29:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05494v2</guid></item><item><title>Physical Adversarial Attack meets Computer Vision: A Decade Survey</title><link>http://arxiv.org/abs/2209.15179v3</link><description>Despite the impressive achievements of Deep Neural Networks (DNNs) incomputer vision, their vulnerability to adversarial attacks remains a criticalconcern. Extensive research has demonstrated that incorporating sophisticatedperturbations into input images can lead to a catastrophic degradation in DNNs'performance. This perplexing phenomenon not only exists in the digital spacebut also in the physical world. Consequently, it becomes imperative to evaluatethe security of DNNs-based systems to ensure their safe deployment inreal-world scenarios, particularly in security-sensitive applications. Tofacilitate a profound understanding of this topic, this paper presents acomprehensive overview of physical adversarial attacks. Firstly, we distillfour general steps for launching physical adversarial attacks. Building uponthis foundation, we uncover the pervasive role of artifacts carryingadversarial perturbations in the physical world. These artifacts influence eachstep. To denote them, we introduce a new term: adversarial medium. Then, wetake the first step to systematically evaluate the performance of physicaladversarial attacks, taking the adversarial medium as a first attempt. Ourproposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness,Stealthiness, Robustness, Practicability, Aesthetics, and Economics. We alsoprovide comparative results across task categories, together with insightfulobservations and suggestions for future research directions.</description><author>Hui Wei, Hao Tang, Xuemei Jia, Zhixiang Wang, Hanxun Yu, Zhubo Li, Shin'ichi Satoh, Luc Van Gool, Zheng Wang</author><pubDate>Sun, 01 Oct 2023 06:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.15179v3</guid></item><item><title>Latent Multimodal Functional Graphical Model Estimation</title><link>http://arxiv.org/abs/2210.17237v3</link><description>Joint multimodal functional data acquisition, where functional data frommultiple modes are measured simultaneously from the same subject, has emergedas an exciting modern approach enabled by recent engineering breakthroughs inthe neurological and biological sciences. One prominent motivation to acquiresuch data is to enable new discoveries of the underlying connectivity bycombining multimodal signals. Despite the scientific interest, there remains agap in principled statistical methods for estimating the graph underlyingmultimodal functional data. To this end, we propose a new integrative frameworkthat models the data generation process and identifies operators mapping fromthe observation space to the latent space. We then develop an estimator thatsimultaneously estimates the transformation operators and the latent graph.This estimator is based on the partial correlation operator, which werigorously extend from the multivariate to the functional setting. Ourprocedure is provably efficient, with the estimator converging to a stationarypoint with quantifiable statistical error. Furthermore, we show recovery of thelatent graph under mild conditions. Our work is applied to analyzesimultaneously acquired multimodal brain imaging data where the graph indicatesfunctional connectivity of the brain. We present simulation and empiricalresults that support the benefits of joint estimation.</description><author>Katherine Tsai, Boxin Zhao, Sanmi Koyejo, Mladen Kolar</author><pubDate>Sun, 01 Oct 2023 05:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.17237v3</guid></item><item><title>Towards Robust Robot 3D Perception in Urban Environments: The UT Campus Object Dataset</title><link>http://arxiv.org/abs/2309.13549v2</link><description>We introduce the UT Campus Object Dataset (CODa), a mobile robot egocentricperception dataset collected on the University of Texas Austin Campus. Ourdataset contains 8.5 hours of multimodal sensor data: synchronized 3D pointclouds and stereo RGB video from a 128-channel 3D LiDAR and two 1.25MP RGBcameras at 10 fps; RGB-D videos from an additional 0.5MP sensor at 7 fps, and a9-DOF IMU sensor at 40 Hz. We provide 58 minutes of ground-truth annotationscontaining 1.3 million 3D bounding boxes with instance IDs for 53 semanticclasses, 5000 frames of 3D semantic annotations for urban terrain, andpseudo-ground truth localization. We repeatedly traverse identical geographiclocations for a wide range of indoor and outdoor areas, weather conditions, andtimes of the day. Using CODa, we empirically demonstrate that: 1) 3D objectdetection performance in urban settings is significantly higher when trainedusing CODa compared to existing datasets even when employing state-of-the-artdomain adaptation approaches, 2) sensor-specific fine-tuning improves 3D objectdetection accuracy and 3) pretraining on CODa improves cross-dataset 3D objectdetection performance in urban settings compared to pretraining on AV datasets.Using our dataset and annotations, we release benchmarks for 3D objectdetection and 3D semantic segmentation using established metrics. In thefuture, the CODa benchmark will include additional tasks like unsupervisedobject discovery and re-identification. We publicly release CODa on the TexasData Repository, pre-trained models, dataset development package, andinteractive dataset viewer on our website at https://amrl.cs.utexas.edu/coda.We expect CODa to be a valuable dataset for research in egocentric 3Dperception and planning for autonomous navigation in urban environments.</description><author>Arthur Zhang, Chaitanya Eranki, Christina Zhang, Ji-Hwan Park, Raymond Hong, Pranav Kalyani, Lochana Kalyanaraman, Arsh Gamare, Arnav Bagad, Maria Esteva, Joydeep Biswas</author><pubDate>Sun, 01 Oct 2023 05:01:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13549v2</guid></item><item><title>AdaptSim: Task-Driven Simulation Adaptation for Sim-to-Real Transfer</title><link>http://arxiv.org/abs/2302.04903v2</link><description>Simulation parameter settings such as contact models and object geometryapproximations are critical to training robust robotic policies capable oftransferring from simulation to real-world deployment. Previous approachestypically handcraft distributions over such parameters (domain randomization),or identify parameters that best match the dynamics of the real environment(system identification). However, there is often an irreducible gap betweensimulation and reality: attempting to match the dynamics between simulation andreality across all states and tasks may be infeasible and may not lead topolicies that perform well in reality for a specific task. Addressing thisissue, we propose AdaptSim, a new task-driven adaptation framework forsim-to-real transfer that aims to optimize task performance in target (real)environments -- instead of matching dynamics between simulation and reality.First, we meta-learn an adaptation policy in simulation using reinforcementlearning for adjusting the simulation parameter distribution based on thecurrent policy's performance in a target environment. We then perform iterativereal-world adaptation by inferring new simulation parameter distributions forpolicy training, using a small amount of real data. We perform experiments inthree robotic tasks: (1) swing-up of linearized double pendulum, (2) dynamictable-top pushing of a bottle, and (3) dynamic scooping of food pieces with aspatula. Our extensive simulation and hardware experiments demonstrate AdaptSimachieving 1-3x asymptotic performance and $\sim$2x real data efficiency whenadapting to different environments, compared to methods based on Sys-ID anddirectly training the task policy in target environments. Website:https://irom-lab.github.io/AdaptSim/</description><author>Allen Z. Ren, Hongkai Dai, Benjamin Burchfiel, Anirudha Majumdar</author><pubDate>Sun, 01 Oct 2023 04:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04903v2</guid></item><item><title>Ground Manipulator Primitive Tasks to Executable Actions using Large Language Models</title><link>http://arxiv.org/abs/2308.06810v2</link><description>Layered architectures have been widely used in robot systems. The majority ofthem implement planning and execution functions in separate layers. However,there still lacks a straightforward way to transit high-level tasks in theplanning layer to the low-level motor commands in the execution layer. In orderto tackle this challenge, we propose a novel approach to ground the manipulatorprimitive tasks to robot low-level actions using large language models (LLMs).We designed a program-function-like prompt based on the task frame formalism.In this way, we enable LLMs to generate position/force set-points for hybridcontrol. Evaluations over several state-of-the-art LLMs are provided.</description><author>Yue Cao, C. S. George Lee</author><pubDate>Sun, 01 Oct 2023 04:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06810v2</guid></item><item><title>Persistent Homology Meets Object Unity: Object Recognition in Clutter</title><link>http://arxiv.org/abs/2305.03815v2</link><description>Recognition of occluded objects in unseen and unstructured indoorenvironments is a challenging problem for mobile robots. To address thischallenge, we propose a new descriptor, TOPS, for point clouds generated fromdepth images and an accompanying recognition framework, THOR, inspired by humanreasoning. The descriptor employs a novel slicing-based approach to computetopological features from filtrations of simplicial complexes using persistenthomology, and facilitates reasoning-based recognition using object unity. Apartfrom a benchmark dataset, we report performance on a new dataset, the UW IndoorScenes (UW-IS) Occluded dataset, curated using commodity hardware to reflectreal-world scenarios with different environmental conditions and degrees ofobject occlusion. THOR outperforms state-of-the-art methods on both thedatasets and achieves substantially higher recognition accuracy for all thescenarios of the UW-IS Occluded dataset. Therefore, THOR, is a promising steptoward robust recognition in low-cost robots, meant for everyday use in indoorsettings.</description><author>Ekta U. Samani, Ashis G. Banerjee</author><pubDate>Sun, 01 Oct 2023 04:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03815v2</guid></item><item><title>Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging</title><link>http://arxiv.org/abs/2309.01026v2</link><description>We present a method for zero-shot recommendation of multimodal non-stationarycontent that leverages recent advancements in the field of generative AI. Wepropose rendering inputs of different modalities as textual descriptions and toutilize pre-trained LLMs to obtain their numerical representations by computingsemantic embeddings. Once unified representations of all content items areobtained, the recommendation can be performed by computing an appropriatesimilarity metric between them without any additional learning. We demonstrateour approach on a synthetic multimodal nudging environment, where the inputsconsist of tabular, textual, and visual data.</description><author>Rachel M. Harrison, Anton Dereventsov, Anton Bibin</author><pubDate>Sun, 01 Oct 2023 03:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01026v2</guid></item><item><title>Normalised clustering accuracy: An asymmetric external cluster validity measure</title><link>http://arxiv.org/abs/2209.02935v2</link><description>There is no, nor will there ever be, single best clustering algorithm, but wewould still like to be able to distinguish between methods which work well oncertain task types and those that systematically underperform. Clusteringalgorithms are traditionally evaluated using either internal or externalvalidity measures. Internal measures quantify different aspects of the obtainedpartitions, e.g., the average degree of cluster compactness or pointseparability. Yet, their validity is questionable, because the clusterings theypromote can sometimes be meaningless. External measures, on the other hand,compare the algorithms' outputs to the reference, ground truth groupings thatare provided by experts. In this paper, we argue that the commonly-usedclassical partition similarity scores, such as the normalised mutualinformation, Fowlkes-Mallows, or adjusted Rand index, miss some desirableproperties, e.g., they do not identify worst-case scenarios correctly or arenot easily interpretable. This makes comparing clustering algorithms acrossmany benchmark datasets difficult. To remedy these issues, we propose andanalyse a new measure: a version of the optimal set-matching accuracy, which isnormalised, monotonic, scale invariant, and corrected for the imbalancedness ofcluster sizes (but neither symmetric nor adjusted for chance).</description><author>Marek Gagolewski</author><pubDate>Sun, 01 Oct 2023 03:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.02935v2</guid></item><item><title>Debiased Mapping for Full-Reference Image Quality Assessment</title><link>http://arxiv.org/abs/2302.11464v3</link><description>Mapping images to deep feature space for comparisons has been wildly adoptedin recent learning-based full-reference image quality assessment (FR-IQA)models. Analogous to the classical classification task, the ideal mapping spacefor quality regression should possess both inter-class separability andintra-class compactness. The inter-class separability that focuses on thediscrimination of images with different quality levels has been highlyemphasized in existing models. However, the intra-class compactness thatmaintains small objective quality variance of images with the same orindistinguishable quality escapes the research attention, potentially leadingto the perception-biased measures. In this paper, we reveal that such bias ismainly caused by the unsuitable subspace that the features are projected andcompared in. To account for this, we develop the Debiased Mapping based qualityMeasure (DMM), which relies on the orthonormal bases of deep learning featuresformed by singular value decomposition (SVD). The SVD in deep learning featuredomain, which overwhelmingly separates the quality variations with singularvalues and projection bases, facilitates the quality inference with dedicatedlydesigned distance measure. Experiments on different IQA databases demonstratethe mapping method is able to mitigate the perception bias efficiently, and thesuperior performance on quality prediction verifies the effectiveness of ourmethod. The implementation will be publicly available.</description><author>Baoliang Chen, Hanwei Zhu, Lingyu Zhu, Shiqi Wang</author><pubDate>Sun, 01 Oct 2023 03:54:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11464v3</guid></item><item><title>Are Human-generated Demonstrations Necessary for In-context Learning?</title><link>http://arxiv.org/abs/2309.14681v2</link><description>Despite the promising few-shot ability of large language models (LLMs), thestandard paradigm of In-context Learning (ICL) suffers the disadvantages ofsusceptibility to selected demonstrations and the intricacy to generate thesedemonstrations. In this paper, we raise the fundamental question that whetherhuman-generated demonstrations are necessary for ICL. To answer this question,we propose self-contemplation prompting strategy (SEC), a paradigm free fromhuman-crafted demonstrations. The key point of SEC is that, instead of usinghand-crafted examples as demonstrations in ICL, SEC asks LLMs to first createdemonstrations on their own, based on which the final output is generated. SECis a flexible framework and can be adapted to both the vanilla ICL and thechain-of-thought (CoT), but with greater ease: as the manual-generation processof both examples and rationale can be saved. Extensive experiments inarithmetic reasoning, commonsense reasoning, multi-task language understanding,and code generation benchmarks, show that SEC, which does not requirehand-crafted demonstrations, significantly outperforms the zero-shot learningstrategy, and achieves comparable results to ICL with hand-crafteddemonstrations. This demonstrates that, for many tasks, contemporary LLMspossess a sufficient level of competence to exclusively depend on their owncapacity for decision making, removing the need for external training data.Code is available at https://github.com/ruili33/SEC.</description><author>Rui Li, Guoyin Wang, Jiwei Li</author><pubDate>Sun, 01 Oct 2023 03:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14681v2</guid></item><item><title>MOTOR: A Time-To-Event Foundation Model For Structured Medical Records</title><link>http://arxiv.org/abs/2301.03150v3</link><description>We present a self-supervised, time-to-event (TTE) foundation model calledMOTOR (Many Outcome Time Oriented Representations) which is pretrained ontimestamped sequences of events in electronic health records (EHR) and healthinsurance claims. TTE models are used for estimating the probabilitydistribution of the time until a specific event occurs, which is an importanttask in medical settings. TTE models provide many advantages overclassification using fixed time horizons, including naturally handling censoredobservations, but are challenging to train with limited labeled data. MOTORaddresses this challenge by pretraining on up to 55M patient records (9Bclinical events). We evaluate MOTOR's transfer learning performance on 19tasks, across 3 patient databases (a private EHR system, MIMIC-IV, and Merativeclaims data). Task-specific models adapted from MOTOR improve time-dependent Cstatistics by 4.6% over state-of-the-art, improve label efficiency by up to 95%,and are more robust to temporal distributional shifts. We further evaluatecross-site portability by adapting our MOTOR foundation model for sixprediction tasks on the MIMIC-IV dataset, where it outperforms all baselines.MOTOR is the first foundation model for medical TTE predictions and we releasea 143M parameter pretrained model for research use at [redacted URL].</description><author>Ethan Steinberg, Yizhe Xu, Jason Fries, Nigam Shah</author><pubDate>Sun, 01 Oct 2023 03:43:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03150v3</guid></item><item><title>Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks</title><link>http://arxiv.org/abs/2303.16454v2</link><description>In this work we develop a novel approach using deep neural networks toreconstruct the conductivity distribution in elliptic problems from onemeasurement of the solution over the whole domain. The approach is based on amixed reformulation of the governing equation and utilizes the standardleast-squares objective, with deep neural networks as ansatz functions toapproximate the conductivity and flux simultaneously. We provide a thoroughanalysis of the deep neural network approximations of the conductivity for bothcontinuous and empirical losses, including rigorous error estimates that areexplicit in terms of the noise level, various penalty parameters and neuralnetwork architectural parameters (depth, width and parameter bound). We alsoprovide multiple numerical experiments in two- and multi-dimensions toillustrate distinct features of the approach, e.g., excellent stability withrespect to data noise and capability of solving high-dimensional problems.</description><author>Bangti Jin, Xiyao Li, Qimeng Quan, Zhi Zhou</author><pubDate>Sun, 01 Oct 2023 03:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16454v2</guid></item><item><title>mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization</title><link>http://arxiv.org/abs/2302.09693v2</link><description>Modern deep learning models are over-parameterized, where different optimacan result in widely varying generalization performance. The Sharpness-AwareMinimization (SAM) technique modifies the fundamental loss function that steersgradient descent methods toward flatter minima, which are believed to exhibitenhanced generalization prowess. Our study delves into a specific variant ofSAM known as micro-batch SAM (mSAM). This variation involves aggregatingupdates derived from adversarial perturbations across multiple shards(micro-batches) of a mini-batch during training. We extend a recently developedand well-studied general framework for flatness analysis to theoretically showthat SAM achieves flatter minima than SGD, and mSAM achieves even flatterminima than SAM. We provide a thorough empirical evaluation of various imageclassification and natural language processing tasks to substantiate thistheoretical advancement. We also show that contrary to previous work, mSAM canbe implemented in a flexible and parallelizable manner without significantlyincreasing computational costs. Our implementation of mSAM yields superiorgeneralization performance across a wide range of tasks compared to SAM,further supporting our theoretical framework.</description><author>Kayhan Behdin, Qingquan Song, Aman Gupta, Sathiya Keerthi, Ayan Acharya, Borja Ocejo, Gregory Dexter, Rajiv Khanna, David Durfee, Rahul Mazumder</author><pubDate>Sun, 01 Oct 2023 03:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09693v2</guid></item><item><title>Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning</title><link>http://arxiv.org/abs/2309.06034v2</link><description>Graph anomaly detection (GAD) has attracted increasing attention in machinelearning and data mining. Recent works have mainly focused on how to capturericher information to improve the quality of node embeddings for GAD. Despitetheir significant advances in detection performance, there is still a relativedearth of research on the properties of the task. GAD aims to discern theanomalies that deviate from most nodes. However, the model is prone to learnthe pattern of normal samples which make up the majority of samples. Meanwhile,anomalies can be easily detected when their behaviors differ from normality.Therefore, the performance can be further improved by enhancing the ability tolearn the normal pattern. To this end, we propose a normality learning-basedGAD framework via multi-scale contrastive learning networks (NLGAD forabbreviation). Specifically, we first initialize the model with the contrastivenetworks on different scales. To provide sufficient and reliable normal nodesfor normality learning, we design an effective hybrid strategy for normalityselection. Finally, the model is refined with the only input of reliable normalnodes and learns a more accurate estimate of normality so that anomalous nodescan be more easily distinguished. Eventually, extensive experiments on sixbenchmark graph datasets demonstrate the effectiveness of our normalitylearning-based scheme on GAD. Notably, the proposed algorithm improves thedetection performance (up to 5.89% AUC gain) compared with the state-of-the-artmethods. The source code is released at https://github.com/FelixDJC/NLGAD.</description><author>Jingcan Duan, Pei Zhang, Siwei Wang, Jingtao Hu, Hu Jin, Jiaxin Zhang, Haifang Zhou, Xinwang Liu</author><pubDate>Sun, 01 Oct 2023 03:16:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06034v2</guid></item><item><title>ARISE: Graph Anomaly Detection on Attributed Networks via Substructure Awareness</title><link>http://arxiv.org/abs/2211.15255v3</link><description>Recently, graph anomaly detection on attributed networks has attractedgrowing attention in data mining and machine learning communities. Apart fromattribute anomalies, graph anomaly detection also aims at suspicioustopological-abnormal nodes that exhibit collective anomalous behavior. Closelyconnected uncorrelated node groups form uncommonly dense substructures in thenetwork. However, existing methods overlook that the topology anomaly detectionperformance can be improved by recognizing such a collective pattern. To thisend, we propose a new graph anomaly detection framework on attributed networksvia substructure awareness (ARISE for abbreviation). Unlike previousalgorithms, we focus on the substructures in the graph to discernabnormalities. Specifically, we establish a region proposal module to discoverhigh-density substructures in the network as suspicious regions. The averagenode-pair similarity can be regarded as the topology anomaly degree of nodeswithin substructures. Generally, the lower the similarity, the higher theprobability that internal nodes are topology anomalies. To distill betterembeddings of node attributes, we further introduce a graph contrastivelearning scheme, which observes attribute anomalies in the meantime. In thisway, ARISE can detect both topology and attribute anomalies. Ultimately,extensive experiments on benchmark datasets show that ARISE greatly improvesdetection performance (up to 7.30% AUC and 17.46% AUPRC gains) compared tostate-of-the-art attributed networks anomaly detection (ANAD) algorithms.</description><author>Jingcan Duan, Bin Xiao, Siwei Wang, Haifang Zhou, Xinwang Liu</author><pubDate>Sun, 01 Oct 2023 03:09:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15255v3</guid></item><item><title>State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory</title><link>http://arxiv.org/abs/2309.13414v2</link><description>State-space models have gained popularity in sequence modelling due to theirsimple and efficient network structures. However, the absence of nonlinearactivation along the temporal direction limits the model's capacity. In thispaper, we prove that stacking state-space models with layer-wise nonlinearactivation is sufficient to approximate any continuous sequence-to-sequencerelationship. Our findings demonstrate that the addition of layer-wisenonlinear activation enhances the model's capacity to learn complex sequencepatterns. Meanwhile, it can be seen both theoretically and empirically that thestate-space models do not fundamentally resolve the exponential decaying memoryissue. Theoretical results are justified by numerical verifications.</description><author>Shida Wang, Beichen Xue</author><pubDate>Sun, 01 Oct 2023 02:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13414v2</guid></item><item><title>A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation</title><link>http://arxiv.org/abs/2304.02858v2</link><description>Class imbalance (CI) in classification problems arises when the number ofobservations belonging to one class is lower than the other. Ensemble learningcombines multiple models to obtain a robust model and has been prominently usedwith data augmentation methods to address class imbalance problems. In the lastdecade, a number of strategies have been added to enhance ensemble learning anddata augmentation methods, along with new methods such as generativeadversarial networks (GANs). A combination of these has been applied in manystudies, but the true rank of different combinations would require acomputational review. In this paper, we present a computational review to evaluate dataaugmentation and ensemble learning methods used to address prominent benchmarkCI problems. We present a general framework that evaluates 10 data augmentationand 10 ensemble learning methods for CI problems. Our objective is to identifythe most effective combination for improving classification performance onimbalanced datasets. The results indicate that combinations of dataaugmentation methods with ensemble learning can significantly improveclassification performance on imbalanced datasets. Our study is vital for thedevelopment of novel models for handling imbalanced datasets.</description><author>Azal Ahmad Khan, Omkar Chaudhari, Rohitash Chandra</author><pubDate>Sun, 01 Oct 2023 02:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02858v2</guid></item></channel></rss>