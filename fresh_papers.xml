<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 22 Jul 2024 13:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks</title><link>http://arxiv.org/abs/2407.14509v1</link><description>We propose a permutation-based explanation method for image classifiers.Current image-model explanations like activation maps are limited toinstance-based explanations in the pixel space, making it difficult tounderstand global model behavior. In contrast, permutation based explanationsfor tabular data classifiers measure feature importance by comparing modelperformance on data before and after permuting a feature. We propose anexplanation method for image-based models that permutes interpretable conceptsacross dataset images. Given a dataset of images labeled with specific conceptslike captions, we permute a concept across examples in the text space and thengenerate images via a text-conditioned diffusion model. Feature importance isthen reflected by the change in model performance relative to unpermuted data.When applied to a set of concepts, the method generates a ranking of featureimportance. We show this approach recovers underlying model feature importanceon synthetic and real-world image classification tasks.</description><author>Sarah Jabbour, Gregory Kondas, Ella Kazerooni, Michael Sjoding, David Fouhey, Jenna Wiens</author><pubDate>Fri, 19 Jul 2024 17:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14509v1</guid></item><item><title>Internal Consistency and Self-Feedback in Large Language Models: A Survey</title><link>http://arxiv.org/abs/2407.14507v1</link><description>Large language models (LLMs) are expected to respond accurately but oftenexhibit deficient reasoning or generate hallucinatory content. To addressthese, studies prefixed with ``Self-'' such as Self-Consistency, Self-Improve,and Self-Refine have been initiated. They share a commonality: involving LLMsevaluating and updating itself to mitigate the issues. Nonetheless, theseefforts lack a unified perspective on summarization, as existing surveyspredominantly focus on categorization without examining the motivations behindthese works. In this paper, we summarize a theoretical framework, termed InternalConsistency, which offers unified explanations for phenomena such as the lackof reasoning and the presence of hallucinations. Internal Consistency assessesthe coherence among LLMs' latent layer, decoding layer, and response layerbased on sampling methodologies. Expanding upon the Internal Consistencyframework, we introduce a streamlined yet effective theoretical frameworkcapable of mining Internal Consistency, named Self-Feedback. The Self-Feedbackframework consists of two modules: Self-Evaluation and Self-Update. Thisframework has been employed in numerous studies. We systematically classify these studies by tasks and lines of work;summarize relevant evaluation methods and benchmarks; and delve into theconcern, ``Does Self-Feedback Really Work?'' We propose several criticalviewpoints, including the ``Hourglass Evolution of Internal Consistency'',``Consistency Is (Almost) Correctness'' hypothesis, and ``The Paradox of Latentand Explicit Reasoning''. Furthermore, we outline promising directions forfuture research. We have open-sourced the experimental code, reference list,and statistical data, available at\url{https://github.com/IAAR-Shanghai/ICSFSurvey}.</description><author>Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Feiyu Xiong, Zhiyu Li</author><pubDate>Fri, 19 Jul 2024 17:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14507v1</guid></item><item><title>On Pre-training of Multimodal Language Models Customized for Chart Understanding</title><link>http://arxiv.org/abs/2407.14506v1</link><description>Recent studies customizing Multimodal Large Language Models (MLLMs) fordomain-specific tasks have yielded promising results, especially in the fieldof scientific chart comprehension. These studies generally utilize visualinstruction tuning with specialized datasets to enhance question and answer(QA) accuracy within the chart domain. However, they often neglect thefundamental discrepancy between natural image-caption pre-training data anddigital chart image-QA data, particularly in the models' capacity to extractunderlying numeric values from charts. This paper tackles this oversight byexploring the training processes necessary to improve MLLMs' comprehension ofcharts. We present three key findings: (1) Incorporating raw data values inalignment pre-training markedly improves comprehension of chart data. (2)Replacing images with their textual representation randomly during end-to-endfine-tuning transfer the language reasoning capability to chart interpretationskills. (3) Requiring the model to first extract the underlying chart data andthen answer the question in the fine-tuning can further improve the accuracy.Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chartcomprehension. CHOPINLLM effectively interprets various types of charts,including unannotated ones, while maintaining robust reasoning abilities.Furthermore, we establish a new benchmark to evaluate MLLMs' understanding ofdifferent chart types across various comprehension levels. Experimental resultsshow that CHOPINLLM exhibits strong performance in understanding both annotatedand unannotated charts across a wide range of types.</description><author>Wan-Cyuan Fan, Yen-Chun Chen, Mengchen Liu, Lu Yuan, Leonid Sigal</author><pubDate>Fri, 19 Jul 2024 17:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14506v1</guid></item><item><title>T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation</title><link>http://arxiv.org/abs/2407.14505v1</link><description>Text-to-video (T2V) generation models have advanced significantly, yet theirability to compose different objects, attributes, actions, and motions into avideo remains unexplored. Previous text-to-video benchmarks also neglect thisimportant ability for evaluation. In this work, we conduct the first systematicstudy on compositional text-to-video generation. We propose T2V-CompBench, thefirst benchmark tailored for compositional text-to-video generation.T2V-CompBench encompasses diverse aspects of compositionality, includingconsistent attribute binding, dynamic attribute binding, spatial relationships,motion binding, action binding, object interactions, and generative numeracy.We further carefully design evaluation metrics of MLLM-based metrics,detection-based metrics, and tracking-based metrics, which can better reflectthe compositional text-to-video generation quality of seven proposed categorieswith 700 text prompts. The effectiveness of the proposed metrics is verified bycorrelation with human evaluations. We also benchmark various text-to-videogenerative models and conduct in-depth analysis across different models anddifferent compositional categories. We find that compositional text-to-videogeneration is highly challenging for current models, and we hope that ourattempt will shed light on future research in this direction.</description><author>Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, Xihui Liu</author><pubDate>Fri, 19 Jul 2024 17:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14505v1</guid></item><item><title>Nonlinear Schr√∂dinger Network</title><link>http://arxiv.org/abs/2407.14504v1</link><description>Deep neural networks (DNNs) have achieved exceptional performance acrossvarious fields by learning complex nonlinear mappings from large-scaledatasets. However, they encounter challenges such as high computational costsand limited interpretability. To address these issues, hybrid approaches thatintegrate physics with AI are gaining interest. This paper introduces a novelphysics-based AI model called the "Nonlinear Schr\"odinger Network", whichtreats the Nonlinear Schr\"odinger Equation (NLSE) as a general-purposetrainable model for learning complex patterns including nonlinear mappings andmemory effects from data. Existing physics-informed machine learning methodsuse neural networks to approximate the solutions of partial differentialequations (PDEs). In contrast, our approach directly treats the PDE as atrainable model to obtain general nonlinear mappings that would otherwiserequire neural networks. As a physics-inspired approach, it offers a moreinterpretable and parameter-efficient alternative to traditional black-boxneural networks, achieving comparable or better accuracy in time seriesclassification tasks while significantly reducing the number of requiredparameters. Notably, the trained Nonlinear Schr\"odinger Network isinterpretable, with all parameters having physical meanings as properties of avirtual physical system that transforms the data to a more separable space.This interpretability allows for insight into the underlying dynamics of thedata transformation process. Applications to time series forecasting have alsobeen explored. While our current implementation utilizes the NLSE, the proposedmethod of using physics equations as trainable models to learn nonlinearmappings from data is not limited to the NLSE and may be extended to othermaster equations of physics.</description><author>Yiming Zhou, Callen MacPhee, Tingyi Zhou, Bahram Jalali</author><pubDate>Fri, 19 Jul 2024 17:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14504v1</guid></item><item><title>Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification</title><link>http://arxiv.org/abs/2407.14503v1</link><description>When applying reinforcement learning from human feedback (RLHF), the rewardis learned from data and, therefore, always has some error. It is common tomitigate this by regularizing the policy with KL divergence from a base model,with the hope that balancing reward with regularization will achieve desirableoutcomes despite this reward misspecification. We show that when the rewardfunction has light-tailed error, optimal policies under less restrictive KLpenalties achieve arbitrarily high utility. However, if error is heavy-tailed,some policies obtain arbitrarily high reward despite achieving no more utilitythan the base model--a phenomenon we call catastrophic Goodhart. We adapt adiscrete optimization method to measure the tails of reward models, findingthat they are consistent with light-tailed error. However, the pervasiveness ofheavy-tailed distributions in many real-world applications indicates thatfuture sources of RL reward could have heavy-tailed error, increasing thelikelihood of reward hacking even with KL regularization.</description><author>Thomas Kwa, Drake Thomas, Adri√† Garriga-Alonso</author><pubDate>Fri, 19 Jul 2024 17:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14503v1</guid></item><item><title>M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models</title><link>http://arxiv.org/abs/2407.14502v1</link><description>We introduce the Multi-Motion Discrete Diffusion Models (M2D2M), a novelapproach for human motion generation from textual descriptions of multipleactions, utilizing the strengths of discrete diffusion models. This approachadeptly addresses the challenge of generating multi-motion sequences, ensuringseamless transitions of motions and coherence across a series of actions. Thestrength of M2D2M lies in its dynamic transition probability within thediscrete diffusion model, which adapts transition probabilities based on theproximity between motion tokens, encouraging mixing between different modes.Complemented by a two-phase sampling strategy that includes independent andjoint denoising steps, M2D2M effectively generates long-term, smooth, andcontextually coherent human motion sequences, utilizing a model trained forsingle-motion generation. Extensive experiments demonstrate that M2D2Msurpasses current state-of-the-art benchmarks for motion generation from textdescriptions, showcasing its efficacy in interpreting language semantics andgenerating dynamic, realistic motions.</description><author>Seunggeun Chi, Hyung-gun Chi, Hengbo Ma, Nakul Agarwal, Faizan Siddiqui, Karthik Ramani, Kwonjoon Lee</author><pubDate>Fri, 19 Jul 2024 17:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14502v1</guid></item><item><title>Indoor Air Quality Dataset with Activities of Daily Living in Low to Middle-income Communities</title><link>http://arxiv.org/abs/2407.14501v1</link><description>In recent years, indoor air pollution has posed a significant threat to oursociety, claiming over 3.2 million lives annually. Developing nations, such asIndia, are most affected since lack of knowledge, inadequate regulation, andoutdoor air pollution lead to severe daily exposure to pollutants. However,only a limited number of studies have attempted to understand how indoor airpollution affects developing countries like India. To address this gap, wepresent spatiotemporal measurements of air quality from 30 indoor sites oversix months during summer and winter seasons. The sites are geographicallylocated across four regions of type: rural, suburban, and urban, covering thetypical low to middle-income population in India. The dataset contains varioustypes of indoor environments (e.g., studio apartments, classrooms, researchlaboratories, food canteens, and residential households), and can provide thebasis for data-driven learning model research aimed at coping with uniquepollution patterns in developing countries. This unique dataset demandsadvanced data cleaning and imputation techniques for handling missing data dueto power failure or network outages during data collection. Furthermore,through a simple speech-to-text application, we provide real-time indooractivity labels annotated by occupants. Therefore, environmentalists and MLenthusiasts can utilize this dataset to understand the complex patterns of thepollutants under different indoor activities, identify recurring sources ofpollution, forecast exposure, improve floor plans and room structures of modernindoor designs, develop pollution-aware recommender systems, etc.</description><author>Prasenjit Karmakar, Swadhin Pradhan, Sandip Chakraborty</author><pubDate>Fri, 19 Jul 2024 17:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14501v1</guid></item><item><title>Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery</title><link>http://arxiv.org/abs/2407.14499v1</link><description>Concept Bottleneck Models (CBMs) have recently been proposed to address the'black-box' problem of deep neural networks, by first mapping images to ahuman-understandable concept space and then linearly combining concepts forclassification. Such models typically require first coming up with a set ofconcepts relevant to the task and then aligning the representations of afeature extractor to map to these concepts. However, even with powerfulfoundational feature extractors like CLIP, there are no guarantees that thespecified concepts are detectable. In this work, we leverage recent advances inmechanistic interpretability and propose a novel CBM approach -- calledDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: insteadof pre-selecting concepts based on the downstream classification task, we usesparse autoencoders to first discover concepts learnt by the model, and thenname them and train linear probes for classification. Our concept extractionstrategy is efficient, since it is agnostic to the downstream task, and usesconcepts already known to the model. We perform a comprehensive evaluationacross multiple datasets and CLIP architectures and show that our method yieldssemantically meaningful concepts, assigns appropriate names to them that makethem easy to interpret, and yields performant and interpretable CBMs. Codeavailable at https://github.com/neuroexplicit-saar/discover-then-name.</description><author>Sukrut Rao, Sweta Mahajan, Moritz B√∂hle, Bernt Schiele</author><pubDate>Fri, 19 Jul 2024 17:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14499v1</guid></item><item><title>Enhancing Layout Hotspot Detection Efficiency with YOLOv8 and PCA-Guided Augmentation</title><link>http://arxiv.org/abs/2407.14498v1</link><description>In this paper, we present a YOLO-based framework for layout hotspotdetection, aiming to enhance the efficiency and performance of the design rulechecking (DRC) process. Our approach leverages the YOLOv8 vision model todetect multiple hotspots within each layout image, even when dealing with largelayout image sizes. Additionally, to enhance pattern-matching effectiveness, weintroduce a novel approach to augment the layout image using informationextracted through Principal Component Analysis (PCA). The core of our proposedmethod is an algorithm that utilizes PCA to extract valuable auxiliaryinformation from the layout image. This extracted information is thenincorporated into the layout image as an additional color channel. Thisaugmentation significantly improves the accuracy of multi-hotspot detectionwhile reducing the false alarm rate of the object detection algorithm. Weevaluate the effectiveness of our framework using four datasets generated fromlayouts found in the ICCAD-2019 benchmark dataset. The results demonstrate thatour framework achieves a precision (recall) of approximately 83% (86%) whilemaintaining a false alarm rate of less than 7.4\%. Also, the studies show thatthe proposed augmentation approach could improve the detection ability ofnever-seen-before (NSB) hotspots by about 10%.</description><author>Dongyang Wu, Siyang Wang, Mehdi Kamal, Massoud Pedram</author><pubDate>Fri, 19 Jul 2024 17:49:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14498v1</guid></item><item><title>Learning Collective Variables with Synthetic Data Augmentation through Physics-Inspired Geodesic Interpolation</title><link>http://arxiv.org/abs/2402.01542v4</link><description>In molecular dynamics simulations, rare events, such as protein folding, aretypically studied using enhanced sampling techniques, most of which are basedon the definition of a collective variable (CV) along which accelerationoccurs. Obtaining an expressive CV is crucial, but often hindered by the lackof information about the particular event, e.g., the transition from unfoldedto folded conformation. We propose a simulation-free data augmentation strategyusing physics-inspired metrics to generate geodesic interpolations resemblingprotein folding transitions, thereby improving sampling efficiency without truetransition state samples. This new data can be used to improve the accuracy ofclassifier-based methods. Alternatively, a regression-based learning scheme forCV models can be adopted by leveraging the interpolation progress parameter.</description><author>Soojung Yang, Juno Nam, Johannes C. B. Dietschreit, Rafael G√≥mez-Bombarelli</author><pubDate>Fri, 19 Jul 2024 17:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01542v4</guid></item><item><title>Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2404.06910v2</link><description>Despite the successes of large language models (LLMs), they exhibitsignificant drawbacks, particularly when processing long contexts. Theirinference cost scales quadratically with respect to sequence length, making itexpensive for deployment in some real-world text processing applications, suchas retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the"distraction phenomenon", where irrelevant context in the prompt degradesoutput quality. To address these drawbacks, we propose a novel RAG promptingmethodology, *superposition prompting*, which can be directly applied topre-trained transformer-based LLMs *without the need for fine-tuning*. At ahigh level, superposition prompting allows the LLM to process input documentsin parallel *prompt paths*, discarding paths once they are deemed irrelevant.We demonstrate the capability of our method to simultaneously enhance timeefficiency across a variety of question-answering benchmarks using multiplepre-trained LLMs. Furthermore, our technique significantly improves accuracywhen the retrieved context is large relative the context the model was trainedon. For example, our approach facilitates a 93x reduction in compute time while*improving* accuracy by 43% on the NaturalQuestions-Open dataset with theMPT-7B instruction-tuned model over naive RAG.</description><author>Thomas Merth, Qichen Fu, Mohammad Rastegari, Mahyar Najibi</author><pubDate>Fri, 19 Jul 2024 17:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06910v2</guid></item><item><title>Conformal Thresholded Intervals for Efficient Regression</title><link>http://arxiv.org/abs/2407.14495v1</link><description>This paper introduces Conformal Thresholded Intervals (CTI), a novelconformal regression method that aims to produce the smallest possibleprediction set with guaranteed coverage. Unlike existing methods that rely onnested conformal framework and full conditional distribution estimation, CTIestimates the conditional probability density for a new response to fall intoeach interquantile interval using off-the-shelf multi-output quantileregression. CTI constructs prediction sets by thresholding the estimatedconditional interquantile intervals based on their length, which is inverselyproportional to the estimated probability density. The threshold is determinedusing a calibration set to ensure marginal coverage. Experimental resultsdemonstrate that CTI achieves optimal performance across various datasets.</description><author>Rui Luo, Zhixin Zhou</author><pubDate>Fri, 19 Jul 2024 17:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14495v1</guid></item><item><title>InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques</title><link>http://arxiv.org/abs/2407.14494v1</link><description>Mechanistic interpretability methods aim to identify the algorithm a neuralnetwork implements, but it is difficult to validate such methods when the truealgorithm is unknown. This work presents InterpBench, a collection ofsemi-synthetic yet realistic transformers with known circuits for evaluatingthese techniques. We train these neural networks using a stricter version ofInterchange Intervention Training (IIT) which we call Strict IIT (SIIT). Likethe original, SIIT trains neural networks by aligning their internalcomputation with a desired high-level causal model, but it also preventsnon-circuit nodes from affecting the model's output. We evaluate SIIT on sparsetransformers produced by the Tracr tool and find that SIIT models maintainTracr's original circuit while being more realistic. SIIT can also traintransformers with larger circuits, like Indirect Object Identification (IOI).Finally, we use our benchmark to evaluate existing circuit discoverytechniques.</description><author>Rohan Gupta, Iv√°n Arcuschin, Thomas Kwa, Adri√† Garriga-Alonso</author><pubDate>Fri, 19 Jul 2024 17:46:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14494v1</guid></item><item><title>PD-TPE: Parallel Decoder with Text-guided Position Encoding for 3D Visual Grounding</title><link>http://arxiv.org/abs/2407.14491v1</link><description>3D visual grounding aims to locate the target object mentioned by free-formednatural language descriptions in 3D point cloud scenes. Most previous workrequires the encoder-decoder to simultaneously align the attribute informationof the target object and its relational information with the surroundingenvironment across modalities. This causes the queries' attention to bedispersed, potentially leading to an excessive focus on points irrelevant tothe input language descriptions. To alleviate these issues, we propose PD-TPE,a visual-language model with a double-branch decoder. The two branches performproposal feature decoding and surrounding layout awareness in parallel. Sincetheir attention maps are not influenced by each other, the queries focus ontokens relevant to each branch's specific objective. In particular, we design anovel Text-guided Position Encoding method, which differs between the twobranches. In the main branch, the priori relies on the relative positionsbetween tokens and predicted 3D boxes, which direct the model to pay moreattention to tokens near the object; in the surrounding branch, it is guided bythe similarity between visual and text features, so that the queries attend totokens that can provide effective layout information. Extensive experimentsdemonstrate that we surpass the state-of-the-art on two widely adopted 3Dvisual grounding datasets, ScanRefer and NR3D, by 1.8% and 2.2%, respectively.Codes will be made publicly available.</description><author>Chenshu Hou, Liang Peng, Xiaopei Wu, Wenxiao Wang, Xiaofei He</author><pubDate>Fri, 19 Jul 2024 17:44:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14491v1</guid></item><item><title>Evaluating the Reliability of Self-Explanations in Large Language Models</title><link>http://arxiv.org/abs/2407.14487v1</link><description>This paper investigates the reliability of explanations generated by largelanguage models (LLMs) when prompted to explain their previous output. Weevaluate two kinds of such self-explanations - extractive and counterfactual -using three state-of-the-art LLMs (2B to 8B parameters) on two differentclassification tasks (objective and subjective). Our findings reveal, that,while these self-explanations can correlate with human judgement, they do notfully and accurately follow the model's decision process, indicating a gapbetween perceived and actual model reasoning. We show that this gap can bebridged because prompting LLMs for counterfactual explanations can producefaithful, informative, and easy-to-verify results. These counterfactuals offera promising alternative to traditional explainability methods (e.g. SHAP,LIME), provided that prompts are tailored to specific tasks and checked forvalidity.</description><author>Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren</author><pubDate>Fri, 19 Jul 2024 17:41:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14487v1</guid></item><item><title>Explainable Post hoc Portfolio Management Financial Policy of a Deep Reinforcement Learning agent</title><link>http://arxiv.org/abs/2407.14486v1</link><description>Financial portfolio management investment policies computed quantitatively bymodern portfolio theory techniques like the Markowitz model rely on a set onassumptions that are not supported by data in high volatility markets. Hence,quantitative researchers are looking for alternative models to tackle thisproblem. Concretely, portfolio management is a problem that has beensuccessfully addressed recently by Deep Reinforcement Learning (DRL)approaches. In particular, DRL algorithms train an agent by estimating thedistribution of the expected reward of every action performed by an agent givenany financial state in a simulator. However, these methods rely on Deep NeuralNetworks model to represent such a distribution, that although they areuniversal approximator models, they cannot explain its behaviour, given by aset of parameters that are not interpretable. Critically, financial investorspolicies require predictions to be interpretable, so DRL agents are not suitedto follow a particular policy or explain their actions. In this work, wedeveloped a novel Explainable Deep Reinforcement Learning (XDRL) approach forportfolio management, integrating the Proximal Policy Optimization (PPO) withthe model agnostic explainable techniques of feature importance, SHAP and LIMEto enhance transparency in prediction time. By executing our methodology, wecan interpret in prediction time the actions of the agent to assess whetherthey follow the requisites of an investment policy or to assess the risk offollowing the agent suggestions. To the best of our knowledge, our proposedapproach is the first explainable post hoc portfolio management financialpolicy of a DRL agent. We empirically illustrate our methodology bysuccessfully identifying key features influencing investment decisions, whichdemonstrate the ability to explain the agent actions in prediction time.</description><author>Alejandra de la Rica Escudero, Eduardo C. Garrido-Merchan, Maria Coronado-Vaca</author><pubDate>Fri, 19 Jul 2024 17:40:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14486v1</guid></item><item><title>Modeling Long Sequences in Bladder Cancer Recurrence: A Comparative Evaluation of LSTM,Transformer,and Mamba</title><link>http://arxiv.org/abs/2405.18518v2</link><description>Traditional survival analysis methods often struggle with complextime-dependent data,failing to capture and interpret dynamic characteristicsadequately.This study aims to evaluate the performance of three long-sequencemodels,LSTM,Transformer,and Mamba,in analyzing recurrence event data andintegrating them with the Cox proportional hazards model.This study integratesthe advantages of deep learning models for handling long-sequence data with theCox proportional hazards model to enhance the performance in analyzingrecurrent events with dynamic time information.Additionally,this study comparesthe ability of different models to extract and utilize features fromtime-dependent clinical recurrence data.The LSTM-Cox model outperformed boththe Transformer-Cox and Mamba-Cox models in prediction accuracy and modelfit,achieving a Concordance index of up to 0.90 on the test set.Significantpredictors of bladder cancer recurrence,such as treatment stop time,maximumtumor size at recurrence and recurrence frequency,were identified.The LSTM-Coxmodel aligned well with clinical outcomes,effectively distinguishing betweenhigh-risk and low-risk patient groups.This study demonstrates that the LSTM-Coxmodel is a robust and efficient method for recurrent data analysis and featureextraction,surpassing newer models like Transformer and Mamba.It offers apractical approach for integrating deep learning technologies into clinicalrisk prediction systems,thereby improving patient management and treatmentoutcomes.</description><author>Runquan Zhang, Jiawen Jiang, Xiaoping Shi</author><pubDate>Fri, 19 Jul 2024 17:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18518v2</guid></item><item><title>ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities</title><link>http://arxiv.org/abs/2407.14482v1</link><description>In this work, we introduce ChatQA 2, a Llama3-based model designed to bridgethe gap between open-access LLMs and leading proprietary models (e.g.,GPT-4-Turbo) in long-context understanding and retrieval-augmented generation(RAG) capabilities. These two capabilities are essential for LLMs to processlarge volumes of information that cannot fit into a single prompt and arecomplementary to each other, depending on the downstream tasks andcomputational budgets. We present a detailed continued training recipe toextend the context window of Llama3-70B-base from 8K to 128K tokens, along witha three-stage instruction tuning process to enhance the model'sinstruction-following, RAG performance, and long-context understandingcapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B modelachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-contextunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, wefind that the state-of-the-art long-context retriever can alleviate the top-kcontext fragmentation issue in RAG, further improving RAG-based results forlong-context understanding tasks. We also provide extensive comparisons betweenRAG and long-context solutions using state-of-the-art long-context LLMs.</description><author>Peng Xu, Wei Ping, Xianchao Wu, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro</author><pubDate>Fri, 19 Jul 2024 17:35:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14482v1</guid></item><item><title>EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM</title><link>http://arxiv.org/abs/2312.06660v2</link><description>This paper presents EdgeSAM, an accelerated variant of the Segment AnythingModel (SAM), optimized for efficient execution on edge devices with minimalcompromise in performance. Our approach involves distilling the originalViT-based SAM image encoder into a purely CNN-based architecture, better suitedfor edge devices. We carefully benchmark various distillation strategies anddemonstrate that taskagnostic encoder distillation fails to capture the fullknowledge embodied in SAM. To overcome this bottleneck, we include both theprompt encoder and mask decoder in the distillation process, with box and pointprompts in the loop, so that the distilled model can accurately capture theintricate dynamics between user input and mask generation. To mitigate datasetbias issues stemming from point prompt distillation, we incorporate alightweight module within the encoder. As a result, EdgeSAM achieves a 37-foldspeed increase compared to the original SAM, and it also outperformsMobileSAM/EfficientSAM, being over 7 times as fast when deployed on edgedevices while enhancing the mIoUs on COCO and LVIS by 2.3/1.5 and 3.1/1.6,respectively. It is also the first SAM variant that can run at over 30 FPS onan iPhone 14. Code and demo are available athttps://www.mmlab-ntu.com/project/edgesam.</description><author>Chong Zhou, Xiangtai Li, Chen Change Loy, Bo Dai</author><pubDate>Fri, 19 Jul 2024 17:33:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06660v2</guid></item><item><title>Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts</title><link>http://arxiv.org/abs/2306.11372v2</link><description>Large language models (LLMs) are known to effectively perform tasks by simplyobserving few exemplars. However, in low-resource languages, obtaining suchhand-picked exemplars can still be challenging, where unsupervised techniquesmay be necessary. Moreover, competent generative capabilities of LLMs areobserved only in high-resource languages, while their performances amongunder-represented languages fall behind due to pre-training data imbalance. Toelicit LLMs' ability onto low-resource languages without any supervised data,we propose to assemble synthetic exemplars from a diverse set of high-resourcelanguages to prompt the LLMs to translate from any language into English. Theseprompts are then used to create intra-lingual exemplars to perform tasks in thetarget languages. Our unsupervised prompting method performs on par withsupervised few-shot learning in LLMs of different sizes for translationsbetween English and 13 Indic and 21 African low-resource languages. We alsoshow that fine-tuning a 7B model on data generated from our method helps itperform competitively with a 175B model. In non-English translation tasks, ourmethod even outperforms supervised prompting by up to 3 chrF++ in manylow-resource languages. When evaluated on zero-shot multilingual summarization,our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and isalso favored by GPT-4.</description><author>Xuan-Phi Nguyen, Sharifah Mahani Aljunied, Shafiq Joty, Lidong Bing</author><pubDate>Fri, 19 Jul 2024 17:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11372v2</guid></item><item><title>A review on vision-based motion estimation</title><link>http://arxiv.org/abs/2407.14478v1</link><description>Compared to contact sensors-based motion measurement, vision-based motionmeasurement has advantages of low cost and high efficiency and have been underactive development in the past decades. This paper provides a review onexisting motion measurement methods. In addition to the development of eachbranch of vision-based motion measurement methods, this paper also discussedthe advantages and disadvantages of existing methods. Based on this discussion,it was identified that existing methods have a common limitation in optimallybalancing accuracy and robustness. To address issue, we developed the Gaussiankernel-based motion measurement method. Preliminary study shows that thedeveloped method can achieve high accuracy on simple synthesized images.</description><author>Hongyi Liu, Haifeng Wang</author><pubDate>Fri, 19 Jul 2024 17:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14478v1</guid></item><item><title>Data-Centric Human Preference Optimization with Rationales</title><link>http://arxiv.org/abs/2407.14477v1</link><description>Reinforcement learning from human feedback plays a crucial role in aligninglanguage models towards human preferences, traditionally represented throughcomparisons between pairs or sets of responses within a given context. Whilemany studies have enhanced algorithmic techniques to optimize learning fromsuch data, this work shifts focus to improving preference learning through adata-centric approach. Specifically, we propose enriching existing preferencedatasets with machine-generated rationales that explain the reasons behindchoices. We develop a simple and principled framework to augment currentpreference learning methods with rationale information. Our comprehensiveanalysis highlights how rationales enhance learning efficiency. Extensiveexperiments reveal that rationale-enriched preference learning offers multipleadvantages: it improves data efficiency, accelerates convergence tohigher-performing models, and reduces verbosity bias and hallucination.Furthermore, this framework is versatile enough to integrate with variouspreference optimization algorithms. Overall, our findings highlight thepotential of re-imagining data design for preference learning, demonstratingthat even freely available machine-generated rationales can significantly boostperformance across multiple dimensions. The code repository is available athttps: //github.com/reds-lab/preference-learning-with-rationales</description><author>Hoang Anh Just, Ming Jin, Anit Sahu, Huy Phan, Ruoxi Jia</author><pubDate>Fri, 19 Jul 2024 17:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14477v1</guid></item><item><title>Contrastive Learning with Counterfactual Explanations for Radiology Report Generation</title><link>http://arxiv.org/abs/2407.14474v1</link><description>Due to the common content of anatomy, radiology images with theircorresponding reports exhibit high similarity. Such inherent data bias canpredispose automatic report generation models to learn entangled and spuriousrepresentations resulting in misdiagnostic reports. To tackle these, we proposea novel \textbf{Co}unter\textbf{F}actual \textbf{E}xplanations-based framework(CoFE) for radiology report generation. Counterfactual explanations serve as apotent tool for understanding how decisions made by algorithms can be changedby asking ``what if'' scenarios. By leveraging this concept, CoFE can learnnon-spurious visual representations by contrasting the representations betweenfactual and counterfactual images. Specifically, we derive counterfactualimages by swapping a patch between positive and negative samples until apredicted diagnosis shift occurs. Here, positive and negative samples are themost semantically similar but have different diagnosis labels. Additionally,CoFE employs a learnable prompt to efficiently fine-tune the pre-trained largelanguage model, encapsulating both factual and counterfactual content toprovide a more generalizable prompt representation. Extensive experiments ontwo benchmarks demonstrate that leveraging the counterfactual explanationsenables CoFE to generate semantically coherent and factually complete reportsand outperform in terms of language generation and clinical efficacy metrics.</description><author>Mingjie Li, Haokun Lin, Liang Qiu, Xiaodan Liang, Ling Chen, Abdulmotaleb Elsaddik, Xiaojun Chang</author><pubDate>Fri, 19 Jul 2024 17:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14474v1</guid></item><item><title>MLMT-CNN for Object Detection and Segmentation in Multi-layer and Multi-spectral Images</title><link>http://arxiv.org/abs/2407.14473v1</link><description>Precisely localising solar Active Regions (AR) from multi-spectral images isa challenging but important task in understanding solar activity and itsinfluence on space weather. A main challenge comes from each modality capturinga different location of the 3D objects, as opposed to typical multi-spectralimaging scenarios where all image bands observe the same scene. Thus, we referto this special multi-spectral scenario as multi-layer. We present a multi-taskdeep learning framework that exploits the dependencies between image bands toproduce 3D AR localisation (segmentation and detection) where different imagebands (and physical locations) have their own set of results. Furthermore, toaddress the difficulty of producing dense AR annotations for trainingsupervised machine learning (ML) algorithms, we adapt a training strategy basedon weak labels (i.e. bounding boxes) in a recursive manner. We compare ourdetection and segmentation stages against baseline approaches for solar imageanalysis (multi-channel coronal hole detection, SPOCA for ARs) andstate-of-the-art deep learning methods (Faster RCNN, U-Net). Additionally, bothdetection a nd segmentation stages are quantitatively validated on artificiallycreated data of similar spatial configurations made from annotated multi-modalmagnetic resonance images. Our framework achieves an average of 0.72 IoU(segmentation) and 0.90 F1 score (detection) across all modalities, comparingto the best performing baseline methods with scores of 0.53 and 0.58,respectively, on the artificial dataset, and 0.84 F1 score in the AR detectiontask comparing to baseline of 0.82 F1 score. Our segmentation results arequalitatively validated by an expert on real ARs.</description><author>Majedaldein Almahasneh, Adeline Paiement, Xianghua Xie, Jean Aboudarham</author><pubDate>Fri, 19 Jul 2024 17:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14473v1</guid></item><item><title>Check-Eval: A Checklist-based Approach for Evaluating Text Quality</title><link>http://arxiv.org/abs/2407.14467v1</link><description>Evaluating the quality of text generated by large language models (LLMs)remains a significant challenge. Traditional metrics often fail to align wellwith human judgments, particularly in tasks requiring creativity and nuance. Inthis paper, we propose Check-Eval, a novel evaluation framework leveraging LLMsto assess the quality of generated text through a checklist-based approach.Check-Eval can be employed as both a reference-free and reference-dependentevaluation method, providing a structured and interpretable assessment of textquality. The framework consists of two main stages: checklist generation andchecklist evaluation. We validate Check-Eval on two benchmark datasets:Portuguese Legal Semantic Textual Similarity and SummEval. Our resultsdemonstrate that Check-Eval achieves higher correlations with human judgmentscompared to existing metrics, such as G-Eval and GPTScore, underscoring itspotential as a more reliable and effective evaluation framework for naturallanguage generation tasks. The code for our experiments is available athttps://anonymous.4open.science/r/check-eval-0DB4.</description><author>Jayr Pereira, Roberto Lotufo</author><pubDate>Fri, 19 Jul 2024 17:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14467v1</guid></item><item><title>Discovering environments with XRM</title><link>http://arxiv.org/abs/2309.16748v2</link><description>Environment annotations are essential for the success of manyout-of-distribution (OOD) generalization methods. Unfortunately, these arecostly to obtain and often limited by human annotators' biases. To achieverobust generalization, it is essential to develop algorithms for automaticenvironment discovery within datasets. Current proposals, which divide examplesbased on their training error, suffer from one fundamental problem. Thesemethods introduce hyper-parameters and early-stopping criteria, which require avalidation set with human-annotated environments, the very information subjectto discovery. In this paper, we propose Cross-Risk-Minimization (XRM) toaddress this issue. XRM trains twin networks, each learning from one randomhalf of the training data, while imitating confident held-out mistakes made byits sibling. XRM provides a recipe for hyper-parameter tuning, does not requireearly-stopping, and can discover environments for all training and validationdata. Algorithms built on top of XRM environments achieve oracleworst-group-accuracy, addressing a long-standing challenge in OODgeneralization. Code available at\url{https://github.com/facebookresearch/XRM}.</description><author>Mohammad Pezeshki, Diane Bouchacourt, Mark Ibrahim, Nicolas Ballas, Pascal Vincent, David Lopez-Paz</author><pubDate>Fri, 19 Jul 2024 17:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16748v2</guid></item><item><title>AttentNet: Fully Convolutional 3D Attention for Lung Nodule Detection</title><link>http://arxiv.org/abs/2407.14464v1</link><description>Motivated by the increasing popularity of attention mechanisms, we observethat popular convolutional (conv.) attention models like Squeeze-and-Excite(SE) and Convolutional Block Attention Module (CBAM) rely on expensivemulti-layer perception (MLP) layers. These MLP layers significantly increasecomputational complexity, making such models less applicable to 3D imagecontexts, where data dimensionality and computational costs are higher. In 3Dmedical imaging, such as 3D pulmonary CT scans, efficient processing is crucialdue to the large data volume. Traditional 2D attention generalized to 3Dincreases the computational load, creating demand for more efficient attentionmechanisms for 3D tasks. We investigate the possibility of incorporating fullyconvolutional (conv.) attention in 3D context. We present two 3D fully conv.attention blocks, demonstrating their effectiveness in 3D context. Usingpulmonary CT scans for 3D lung nodule detection, we present AttentNet, anautomated lung nodule detection framework from CT images, performing detectionas an ensemble of two stages, candidate proposal and false positive (FP)reduction. We compare the proposed 3D attention blocks to popular 2D conv.attention methods generalized to 3D modules and to self-attention units. Forthe FP reduction stage, we also use a joint analysis approach to aggregatespatial information from different contextual levels. We use LUNA-16 lungnodule detection dataset to demonstrate the benefits of the proposed fullyconv. attention blocks compared to baseline popular lung nodule detectionmethods when no attention is used. Our work does not aim at achievingstate-of-the-art results in the lung nodule detection task, rather todemonstrate the benefits of incorporating fully conv. attention within a 3Dcontext.</description><author>Majedaldein Almahasneh, Xianghua Xie, Adeline Paiement</author><pubDate>Fri, 19 Jul 2024 17:06:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14464v1</guid></item><item><title>SurvReLU: Inherently Interpretable Survival Analysis via Deep ReLU Networks</title><link>http://arxiv.org/abs/2407.14463v1</link><description>Survival analysis models time-to-event distributions with censorship.Recently, deep survival models using neural networks have dominated due totheir representational power and state-of-the-art performance. However, their"black-box" nature hinders interpretability, which is crucial in real-worldapplications. In contrast, "white-box" tree-based survival models offer betterinterpretability but struggle to converge to global optima due to greedyexpansion. In this paper, we bridge the gap between previous deep survivalmodels and traditional tree-based survival models through deep rectified linearunit (ReLU) networks. We show that a deliberately constructed deep ReLU network(SurvReLU) can harness the interpretability of tree-based structures with therepresentational power of deep survival models. Empirical studies on bothsimulated and real survival benchmark datasets show the effectiveness of theproposed SurvReLU in terms of performance and interoperability. The code isavailable at \href{https://github.com/xs018/SurvReLU}{\color{magenta}{https://github.com/xs018/SurvReLU}}.</description><author>Xiaotong Sun, Peijie Qiu, Shengfan Zhang</author><pubDate>Fri, 19 Jul 2024 17:06:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14463v1</guid></item><item><title>PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer</title><link>http://arxiv.org/abs/2407.14459v1</link><description>Spectral Graph Neural Networks have demonstrated superior performance ingraph representation learning. However, many current methods focus on employingshared polynomial coefficients for all nodes, i.e., learning node-unifiedfilters, which limits the filters' flexibility for node-level tasks. The recentDSF attempts to overcome this limitation by learning node-wise coefficientsbased on positional encoding. However, the initialization and updating processof the positional encoding are burdensome, hindering scalability on large-scalegraphs. In this work, we propose a scalable node-wise filter, PolyAttn.Leveraging the attention mechanism, PolyAttn can directly learn node-wisefilters in an efficient manner, offering powerful representation capabilities.Building on PolyAttn, we introduce the whole model, named PolyFormer. In thelens of Graph Transformer models, PolyFormer, which calculates attention scoreswithin nodes, shows great scalability. Moreover, the model captures spectralinformation, enhancing expressiveness while maintaining efficiency. With theseadvantages, PolyFormer offers a desirable balance between scalability andexpressiveness for node-level tasks. Extensive experiments demonstrate that ourproposed methods excel at learning arbitrary node-wise filters, showingsuperior performance on both homophilic and heterophilic graphs, and handlinggraphs containing up to 100 million nodes. The code is available athttps://github.com/air029/PolyFormer.</description><author>Jiahong Ma, Mingguo He, Zhewei Wei</author><pubDate>Fri, 19 Jul 2024 17:01:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14459v1</guid></item><item><title>AudioInsight: Detecting Social Contexts Relevant to Social Anxiety from Speech</title><link>http://arxiv.org/abs/2407.14458v1</link><description>During social interactions, understanding the intricacies of the context canbe vital, particularly for socially anxious individuals. While previousresearch has found that the presence of a social interaction can be detectedfrom ambient audio, the nuances within social contexts, which influence howanxiety provoking interactions are, remain largely unexplored. As analternative to traditional, burdensome methods like self-report, this studypresents a novel approach that harnesses ambient audio segments to detectsocial threat contexts. We focus on two key dimensions: number of interactionpartners (dyadic vs. group) and degree of evaluative threat (explicitlyevaluative vs. not explicitly evaluative). Building on data from a Zoom-basedsocial interaction study (N=52 college students, of whom the majority N=45 aresocially anxious), we employ deep learning methods to achieve strong detectionperformance. Under sample-wide 5-fold Cross Validation (CV), our modeldistinguished dyadic from group interactions with 90\% accuracy and detectedevaluative threat at 83\%. Using a leave-one-group-out CV, accuracies were 82\%and 77\%, respectively. While our data are based on virtual interactions due topandemic constraints, our method has the potential to extend to diversereal-world settings. This research underscores the potential of passive sensingand AI to differentiate intricate social contexts, and may ultimately advancethe ability of context-aware digital interventions to offer personalized mentalhealth support.</description><author>Varun Reddy, Zhiyuan Wang, Emma Toner, Max Larrazabal, Mehdi Boukhechba, Bethany A. Teachman, Laura E. Barnes</author><pubDate>Fri, 19 Jul 2024 17:01:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14458v1</guid></item><item><title>Multi-Attribute Vision Transformers are Efficient and Robust Learners</title><link>http://arxiv.org/abs/2402.08070v2</link><description>Since their inception, Vision Transformers (ViTs) have emerged as acompelling alternative to Convolutional Neural Networks (CNNs) across a widespectrum of tasks. ViTs exhibit notable characteristics, including globalattention, resilience against occlusions, and adaptability to distributionshifts. One underexplored aspect of ViTs is their potential for multi-attributelearning, referring to their ability to simultaneously grasp multipleattribute-related tasks. In this paper, we delve into the multi-attributelearning capability of ViTs, presenting a straightforward yet effectivestrategy for training various attributes through a single ViT network asdistinct tasks. We assess the resilience of multi-attribute ViTs againstadversarial attacks and compare their performance against ViTs designed forsingle attributes. Moreover, we further evaluate the robustness ofmulti-attribute ViTs against a recent transformer based attack calledPatch-Fool. Our empirical findings on the CelebA dataset provide validation forour assertion. Our code is available at https://github.com/hananshafi/MTL-ViT</description><author>Hanan Gani, Nada Saadi, Noor Hussein, Karthik Nandakumar</author><pubDate>Fri, 19 Jul 2024 16:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08070v2</guid></item><item><title>SynthBA: Reliable Brain Age Estimation Across Multiple MRI Sequences and Resolutions</title><link>http://arxiv.org/abs/2406.00365v2</link><description>Brain age is a critical measure that reflects the biological ageing processof the brain. The gap between brain age and chronological age, referred to asbrain PAD (Predicted Age Difference), has been utilized to investigateneurodegenerative conditions. Brain age can be predicted using MRIs and machinelearning techniques. However, existing methods are often sensitive toacquisition-related variabilities, such as differences in acquisitionprotocols, scanners, MRI sequences, and resolutions, significantly limitingtheir application in highly heterogeneous clinical settings. In this study, weintroduce Synthetic Brain Age (SynthBA), a robust deep-learning model designedfor predicting brain age. SynthBA utilizes an advanced domain randomizationtechnique, ensuring effective operation across a wide array ofacquisition-related variabilities. To assess the effectiveness and robustnessof SynthBA, we evaluate its predictive capabilities on internal and externaldatasets, encompassing various MRI sequences and resolutions, and compare itwith state-of-the-art techniques. Additionally, we calculate the brain PAD in alarge cohort of subjects with Alzheimer's Disease (AD), demonstrating asignificant correlation with AD-related measures of cognitive dysfunction.SynthBA holds the potential to facilitate the broader adoption of brain ageprediction in clinical settings, where re-training or fine-tuning is oftenunfeasible. The SynthBA source code and pre-trained models are publiclyavailable at https://github.com/LemuelPuglisi/SynthBA.</description><author>Lemuel Puglisi, Alessia Rondinella, Linda De Meo, Francesco Guarnera, Sebastiano Battiato, Daniele Rav√¨</author><pubDate>Fri, 19 Jul 2024 16:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00365v2</guid></item><item><title>XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution</title><link>http://arxiv.org/abs/2403.05049v2</link><description>Diffusion-based methods, endowed with a formidable generative prior, havereceived increasing attention in Image Super-Resolution (ISR) recently.However, as low-resolution (LR) images often undergo severe degradation, it ischallenging for ISR models to perceive the semantic and degradationinformation, resulting in restoration images with incorrect content orunrealistic artifacts. To address these issues, we propose a\textit{Cross-modal Priors for Super-Resolution (XPSR)} framework. Within XPSR,to acquire precise and comprehensive semantic conditions for the diffusionmodel, cutting-edge Multimodal Large Language Models (MLLMs) are utilized. Tofacilitate better fusion of cross-modal priors, a \textit{Semantic-FusionAttention} is raised. To distill semantic-preserved information instead ofundesired degradations, a \textit{Degradation-Free Constraint} is attachedbetween LR and its high-resolution (HR) counterpart. Quantitative andqualitative results show that XPSR is capable of generating high-fidelity andhigh-realism images across synthetic and real-world datasets. Codes arereleased at \url{https://github.com/qyp2000/XPSR}.</description><author>Yunpeng Qu, Kun Yuan, Kai Zhao, Qizhi Xie, Jinhua Hao, Ming Sun, Chao Zhou</author><pubDate>Fri, 19 Jul 2024 16:31:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05049v2</guid></item><item><title>GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction</title><link>http://arxiv.org/abs/2407.04237v3</link><description>We present GSD, a diffusion model approach based on Gaussian Splatting (GS)representation for 3D object reconstruction from a single view. Prior workssuffer from inconsistent 3D geometry or mediocre rendering quality due toimproper representations. We take a step towards resolving these shortcomingsby utilizing the recent state-of-the-art 3D explicit representation, GaussianSplatting, and an unconditional diffusion model. This model learns to generate3D objects represented by sets of GS ellipsoids. With these strong generative3D priors, though learning unconditionally, the diffusion model is ready forview-guided reconstruction without further model fine-tuning. This is achievedby propagating fine-grained 2D features through the efficient yet flexiblesplatting function and the guided denoising sampling process. In addition, a 2Ddiffusion model is further employed to enhance rendering fidelity, and improvereconstructed GS quality by polishing and re-using the rendered images. Thefinal reconstructed objects explicitly come with high-quality 3D structure andtexture, and can be efficiently rendered in arbitrary views. Experiments on thechallenging real-world CO3D dataset demonstrate the superiority of ourapproach. Project page: $\href{https://yxmu.foo/GSD/}{\text{this https URL}}$</description><author>Yuxuan Mu, Xinxin Zuo, Chuan Guo, Yilin Wang, Juwei Lu, Xiaofeng Wu, Songcen Xu, Peng Dai, Youliang Yan, Li Cheng</author><pubDate>Fri, 19 Jul 2024 16:30:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04237v3</guid></item><item><title>PACE: A Large-Scale Dataset with Pose Annotations in Cluttered Environments</title><link>http://arxiv.org/abs/2312.15130v3</link><description>We introduce PACE (Pose Annotations in Cluttered Environments), a large-scalebenchmark designed to advance the development and evaluation of pose estimationmethods in cluttered scenarios. PACE provides a large-scale real-worldbenchmark for both instance-level and category-level settings. The benchmarkconsists of 55K frames with 258K annotations across 300 videos, covering 238objects from 43 categories and featuring a mix of rigid and articulated itemsin cluttered scenes. To annotate the real-world data efficiently, we develop aninnovative annotation system with a calibrated 3-camera setup. Additionally, weoffer PACE-Sim, which contains 100K photo-realistic simulated frames with 2.4Mannotations across 931 objects. We test state-of-the-art algorithms in PACEalong two tracks: pose estimation, and object pose tracking, revealing thebenchmark's challenges and research opportunities. Our benchmark code and datais available on https://github.com/qq456cvb/PACE.</description><author>Yang You, Kai Xiong, Zhening Yang, Zhengxiang Huang, Junwei Zhou, Ruoxi Shi, Zhou Fang, Adam W. Harley, Leonidas Guibas, Cewu Lu</author><pubDate>Fri, 19 Jul 2024 16:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15130v3</guid></item><item><title>DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models</title><link>http://arxiv.org/abs/2407.01519v2</link><description>This paper introduces a method for zero-shot video restoration usingpre-trained image restoration diffusion models. Traditional video restorationmethods often need retraining for different settings and struggle with limitedgeneralization across various degradation types and datasets. Our approach usesa hierarchical token merging strategy for keyframes and local frames, combinedwith a hybrid correspondence mechanism that blends optical flow andfeature-based nearest neighbor matching (latent merging). We show that ourmethod not only achieves top performance in zero-shot video restoration butalso significantly surpasses trained models in generalization across diversedatasets and extreme degradations (8$\times$ super-resolution and high-standarddeviation video denoising). We present evidence through quantitative metricsand visual comparisons on various challenging datasets. Additionally, ourtechnique works with any 2D restoration diffusion model, offering a versatileand powerful tool for video enhancement tasks without extensive retraining.This research leads to more efficient and widely applicable video restorationtechnologies, supporting advancements in fields that require high-quality videooutput. See our project page for video results athttps://jimmycv07.github.io/DiffIR2VR_web/.</description><author>Chang-Han Yeh, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Ting-Hsuan Chen, Yu-Lun Liu</author><pubDate>Fri, 19 Jul 2024 16:25:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01519v2</guid></item><item><title>SlowPerception: Physical-World Latency Attack against Visual Perception in Autonomous Driving</title><link>http://arxiv.org/abs/2406.05800v2</link><description>Autonomous Driving (AD) systems critically depend on visual perception forreal-time object detection and multiple object tracking (MOT) to ensure safedriving. However, high latency in these visual perception components can leadto significant safety risks, such as vehicle collisions. While previousresearch has extensively explored latency attacks within the digital realm,translating these methods effectively to the physical world presentschallenges. For instance, existing attacks rely on perturbations that areunrealistic or impractical for AD, such as adversarial perturbations affectingareas like the sky, or requiring large patches that obscure most of a camera'sview, thus making them impossible to be conducted effectively in the realworld. In this paper, we introduce SlowPerception, the first physical-world latencyattack against AD perception, via generating projector-based universalperturbations. SlowPerception strategically creates numerous phantom objects onvarious surfaces in the environment, significantly increasing the computationalload of Non-Maximum Suppression (NMS) and MOT, thereby inducing substantiallatency. Our SlowPerception achieves second-level latency in physical-worldsettings, with an average latency of 2.5 seconds across different AD perceptionsystems, scenarios, and hardware configurations. This performance significantlyoutperforms existing state-of-the-art latency attacks. Additionally, we conductAD system-level impact assessments, such as vehicle collisions, usingindustry-grade AD systems with production-grade AD simulators with a 97%average rate. We hope that our analyses can inspire further research in thiscritical domain, enhancing the robustness of AD systems against emergingvulnerabilities.</description><author>Chen Ma, Ningfei Wang, Zhengyu Zhao, Qi Alfred Chen, Chao Shen</author><pubDate>Fri, 19 Jul 2024 16:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05800v2</guid></item><item><title>Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding</title><link>http://arxiv.org/abs/2407.14439v1</link><description>Cropping high-resolution document images into multiple sub-images is the mostwidely used approach for current Multimodal Large Language Models (MLLMs) to dodocument understanding. Most of current document understanding methods preserveall tokens within sub-images and treat them equally. This neglects theirdifferent informativeness and leads to a significant increase in the number ofimage tokens. To perform a more adaptive and efficient document understanding,we propose Token-level Correlation-guided Compression, a parameter-free andplug-and-play methodology to optimize token processing. Firstly, we propose aninnovative approach for assessing the pattern repetitiveness based on thecorrelation between each patch tokens. This method identifies redundant tokens,allowing for the determination of the sub-image's information density.Secondly, we present a token-level sampling method that efficiently capturesthe most informative tokens by delving into the correlation between the [CLS]token and patch tokens. By integrating these strategies, we develop aplug-and-play adaptive compressor module that can be seamlessly incorporatedinto MLLMs utilizing cropping techniques. This module not only enhances theprocessing speed during training and inference but also maintains comparableperformance. We conduct experiments with the SOTA document understanding modelmPLUG-DocOwl1.5 and the effectiveness is demonstrated through extensivecomparisons with other compression methods.</description><author>Renshan Zhang, Yibo Lyu, Rui Shao, Gongwei Chen, Weili Guan, Liqiang Nie</author><pubDate>Fri, 19 Jul 2024 16:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14439v1</guid></item><item><title>Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics</title><link>http://arxiv.org/abs/2310.17316v5</link><description>Defect inspection is paramount within the closed-loop manufacturing system.However, existing datasets for defect inspection often lack precision andsemantic granularity required for practical applications. In this paper, weintroduce the Defect Spectrum, a comprehensive benchmark that offers precise,semantic-abundant, and large-scale annotations for a wide range of industrialdefects. Building on four key industrial benchmarks, our dataset refinesexisting annotations and introduces rich semantic details, distinguishingmultiple defect types within a single image. Furthermore, we introduceDefect-Gen, a two-stage diffusion-based generator designed to createhigh-quality and diverse defective images, even when working with limiteddatasets. The synthetic images generated by Defect-Gen significantly enhancethe efficacy of defect inspection models. Overall, The Defect Spectrum datasetdemonstrates its potential in defect inspection research, offering a solidplatform for testing and refining advanced models.</description><author>Shuai Yang, Zhifei Chen, Pengguang Chen, Xi Fang, Yixun Liang, Shu Liu, Yingcong Chen</author><pubDate>Fri, 19 Jul 2024 16:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17316v5</guid></item><item><title>MaSS: Multi-attribute Selective Suppression for Utility-preserving Data Transformation from an Information-theoretic Perspective</title><link>http://arxiv.org/abs/2405.14981v2</link><description>The growing richness of large-scale datasets has been crucial in driving therapid advancement and wide adoption of machine learning technologies. Themassive collection and usage of data, however, pose an increasing risk forpeople's private and sensitive information due to either inadvertentmishandling or malicious exploitation. Besides legislative solutions, manytechnical approaches have been proposed towards data privacy protection.However, they bear various limitations such as leading to degraded dataavailability and utility, or relying on heuristics and lacking solidtheoretical bases. To overcome these limitations, we propose a formalinformation-theoretic definition for this utility-preserving privacy protectionproblem, and design a data-driven learnable data transformation framework thatis capable of selectively suppressing sensitive attributes from target datasetswhile preserving the other useful attributes, regardless of whether or not theyare known in advance or explicitly annotated for preservation. We providerigorous theoretical analyses on the operational bounds for our framework, andcarry out comprehensive experimental evaluations using datasets of a variety ofmodalities, including facial images, voice audio clips, and human activitymotion sensor signals. Results demonstrate the effectiveness andgeneralizability of our method under various configurations on a multitude oftasks. Our code is available at https://github.com/jpmorganchase/MaSS.</description><author>Yizhuo Chen, Chun-Fu Chen, Hsiang Hsu, Shaohan Hu, Marco Pistoia, Tarek Abdelzaher</author><pubDate>Fri, 19 Jul 2024 16:10:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14981v2</guid></item><item><title>Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders</title><link>http://arxiv.org/abs/2407.14435v1</link><description>Sparse autoencoders (SAEs) are a promising unsupervised approach foridentifying causally relevant and interpretable linear features in a languagemodel's (LM) activations. To be useful for downstream tasks, SAEs need todecompose LM activations faithfully; yet to be interpretable the decompositionmust be sparse -- two objectives that are in tension. In this paper, weintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelityat a given sparsity level on Gemma 2 9B activations, compared to other recentadvances such as Gated and TopK SAEs. We also show that this improvement doesnot come at the cost of interpretability through manual and automatedinterpretability studies. JumpReLU SAEs are a simple modification of vanilla(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLUactivation function -- and are similarly efficient to train and run. Byutilising straight-through-estimators (STEs) in a principled manner, we showhow it is possible to train JumpReLU SAEs effectively despite the discontinuousJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEsto directly train L0 to be sparse, instead of training on proxies such as L1,avoiding problems like shrinkage.</description><author>Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, J√°nos Kram√°r, Neel Nanda</author><pubDate>Fri, 19 Jul 2024 16:07:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14435v1</guid></item><item><title>Co-synthesis of Histopathology Nuclei Image-Label Pairs using a Context-Conditioned Joint Diffusion Model</title><link>http://arxiv.org/abs/2407.14434v1</link><description>In multi-class histopathology nuclei analysis tasks, the lack of trainingdata becomes a main bottleneck for the performance of learning-based methods.To tackle this challenge, previous methods have utilized generative models toincrease data by generating synthetic samples. However, existing methods oftenoverlook the importance of considering the context of biological tissues (e.g.,shape, spatial layout, and tissue type) in the synthetic data. Moreover, whilegenerative models have shown superior performance in synthesizing realistichistopathology images, none of the existing methods are capable of producingimage-label pairs at the same time. In this paper, we introduce a novelframework for co-synthesizing histopathology nuclei images and paired semanticlabels using a context-conditioned joint diffusion model. We proposeconditioning of a diffusion model using nucleus centroid layouts withstructure-related text prompts to incorporate spatial and structural contextinformation into the generation targets. Moreover, we enhance the granularityof our synthesized semantic labels by generating instance-wise nuclei labelsusing distance maps synthesized concurrently in conjunction with the images andsemantic labels. We demonstrate the effectiveness of our framework ingenerating high-quality samples on multi-institutional, multi-organ, andmulti-modality datasets. Our synthetic data consistently outperforms existingaugmentation methods in the downstream tasks of nuclei segmentation andclassification.</description><author>Seonghui Min, Hyun-Jic Oh, Won-Ki Jeong</author><pubDate>Fri, 19 Jul 2024 16:06:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14434v1</guid></item><item><title>Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data</title><link>http://arxiv.org/abs/2402.04375v2</link><description>The growing use of machine learning (ML) has raised concerns that an ML modelmay reveal private information about an individual who has contributed to thetraining dataset. To prevent leakage of sensitive data, we consider usingdifferentially-private (DP), synthetic training data instead of real trainingdata to train an ML model. A key desirable property of synthetic data is itsability to preserve the low-order marginals of the original distribution. Ourmain contribution comprises novel upper and lower bounds on the excessempirical risk of linear models trained on such synthetic data, for continuousand Lipschitz loss functions. We perform extensive experimentation alongsideour theoretical results.</description><author>Yvonne Zhou, Mingyu Liang, Ivan Brugere, Dana Dachman-Soled, Danial Dervovic, Antigoni Polychroniadou, Min Wu</author><pubDate>Fri, 19 Jul 2024 16:01:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04375v2</guid></item><item><title>The Extrapolation Power of Implicit Models</title><link>http://arxiv.org/abs/2407.14430v1</link><description>In this paper, we investigate the extrapolation capabilities of implicit deeplearning models in handling unobserved data, where traditional deep neuralnetworks may falter. Implicit models, distinguished by their adaptability inlayer depth and incorporation of feedback within their computational graph, areput to the test across various extrapolation scenarios: out-of-distribution,geographical, and temporal shifts. Our experiments consistently demonstratesignificant performance advantage with implicit models. Unlike theirnon-implicit counterparts, which often rely on meticulous architectural designfor each task, implicit models demonstrate the ability to learn complex modelstructures without the need for task-specific design, highlighting theirrobustness in handling unseen data.</description><author>Juliette Decugis, Alicia Y. Tsai, Max Emerling, Ashwin Ganesh, Laurent El Ghaoui</author><pubDate>Fri, 19 Jul 2024 16:01:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14430v1</guid></item><item><title>Dataset Distillation in Medical Imaging: A Feasibility Study</title><link>http://arxiv.org/abs/2407.14429v1</link><description>Data sharing in the medical image analysis field has potential yet remainsunderappreciated. The aim is often to share datasets efficiently with othersites to train models effectively. One possible solution is to avoidtransferring the entire dataset while still achieving similar modelperformance. Recent progress in data distillation within computer scienceoffers promising prospects for sharing medical data efficiently withoutsignificantly compromising model effectiveness. However, it remains uncertainwhether these methods would be applicable to medical imaging, since medical andnatural images are distinct fields. Moreover, it is intriguing to consider whatlevel of performance could be achieved with these methods. To answer thesequestions, we conduct investigations on a variety of leading data distillationmethods, in different contexts of medical imaging. We evaluate the feasibilityof these methods with extensive experiments in two aspects: 1) Assess theimpact of data distillation across multiple datasets characterized by minor orgreat variations. 2) Explore the indicator to predict the distillationperformance. Our extensive experiments across multiple medical datasets revealthat data distillation can significantly reduce dataset size while maintainingcomparable model performance to that achieved with the full dataset, suggestingthat a small, representative sample of images can serve as a reliable indicatorof distillation success. This study demonstrates that data distillation is aviable method for efficient and secure medical data sharing, with the potentialto facilitate enhanced collaborative research and clinical applications.</description><author>Muyang Li, Can Cui, Quan Liu, Ruining Deng, Tianyuan Yao, Marilyn Lionts, Yuankai Huo</author><pubDate>Fri, 19 Jul 2024 15:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14429v1</guid></item><item><title>From Principles to Practices: Lessons Learned from Applying Partnership on AI's (PAI) Synthetic Media Framework to 11 Use Cases</title><link>http://arxiv.org/abs/2407.13025v2</link><description>2023 was the year the world woke up to generative AI, and 2024 is the yearpolicymakers are responding more firmly. Importantly, this policy momentum istaking place alongside real world creation and distribution of synthetic media.Social media platforms, news organizations, dating apps, image generationcompanies, and more are already navigating a world of AI-generated visuals andsounds, already changing hearts and minds, as policymakers try to catch up.How, then, can AI governance capture the complexity of the synthetic medialandscape? How can it attend to synthetic media's myriad uses, ranging fromstorytelling to privacy preservation, to deception, fraud, and defamation,taking into account the many stakeholders involved in its development,creation, and distribution? And what might it mean to govern synthetic media ina manner that upholds the truth while bolstering freedom of expression? Whatfollows is the first known collection of diverse examples of the implementationof synthetic media governance that responds to these questions, specificallythrough Partnership on AI's (PAI) Responsible Practices for Synthetic Media - avoluntary, normative Framework for creating, distributing, and buildingtechnology for synthetic media responsibly, launched in February 2023. In thispaper, we present a case bank of real world examples that help operationalizethe Framework - highlighting areas synthetic media governance can be applied,augmented, expanded, and refined for use, in practice. Read together, the casesemphasize distinct elements of AI policymaking and seven emergent bestpractices supporting transparency, safety, expression, and digital dignityonline: consent, disclosure, and differentiation between harmful and creativeuse cases.</description><author>Claire R. Leibowicz, Christian H. Cardona</author><pubDate>Fri, 19 Jul 2024 15:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13025v2</guid></item><item><title>Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference</title><link>http://arxiv.org/abs/2310.16975v2</link><description>We present two neural network approaches that approximate the solutions ofstatic and dynamic conditional optimal transport (COT) problems. Bothapproaches enable conditional sampling and conditional density estimation,which are core tasks in Bayesian inference$\unicode{x2013}$particularly in thesimulation-based ("likelihood-free") setting. Our methods represent the targetconditional distributions as transformations of a tractable referencedistribution and, therefore, fall into the framework of measure transport.Although many measure transport approaches model the transformation as COTmaps, obtaining the map is computationally challenging, even in moderatedimensions. To improve scalability, our numerical algorithms use neuralnetworks to parameterize COT maps and further exploit the structure of the COTproblem. Our static approach approximates the map as the gradient of apartially input-convex neural network. It uses a novel numerical implementationto increase computational efficiency compared to state-of-the-art alternatives.Our dynamic approach approximates the conditional optimal transport via theflow map of a regularized neural ODE; compared to the static approach, it isslower to train but offers more modeling choices and can lead to fastersampling. We demonstrate both algorithms numerically, comparing them withcompeting state-of-the-art approaches, using benchmark datasets andsimulation-based Bayesian inverse problems.</description><author>Zheyu Oliver Wang, Ricardo Baptista, Youssef Marzouk, Lars Ruthotto, Deepanshu Verma</author><pubDate>Fri, 19 Jul 2024 15:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16975v2</guid></item><item><title>Controllable and Efficient Multi-Class Pathology Nuclei Data Augmentation using Text-Conditioned Diffusion Models</title><link>http://arxiv.org/abs/2407.14426v1</link><description>In the field of computational pathology, deep learning algorithms have madesignificant progress in tasks such as nuclei segmentation and classification.However, the potential of these advanced methods is limited by the lack ofavailable labeled data. Although image synthesis via recent generative modelshas been actively explored to address this challenge, existing works havebarely addressed label augmentation and are mostly limited to single-class andunconditional label generation. In this paper, we introduce a novel two-stageframework for multi-class nuclei data augmentation using text-conditionaldiffusion models. In the first stage, we innovate nuclei label synthesis bygenerating multi-class semantic labels and corresponding instance maps througha joint diffusion model conditioned by text prompts that specify the labelstructure information. In the second stage, we utilize a semantic andtext-conditional latent diffusion model to efficiently generate high-qualitypathology images that align with the generated nuclei label images. Wedemonstrate the effectiveness of our method on large and diverse pathologynuclei datasets, with evaluations including qualitative and quantitativeanalyses, as well as assessments of downstream tasks.</description><author>Hyun-Jic Oh, Won-Ki Jeong</author><pubDate>Fri, 19 Jul 2024 15:53:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14426v1</guid></item><item><title>HOTS3D: Hyper-Spherical Optimal Transport for Semantic Alignment of Text-to-3D Generation</title><link>http://arxiv.org/abs/2407.14419v1</link><description>Recent CLIP-guided 3D generation methods have achieved promising results butstruggle with generating faithful 3D shapes that conform with input text due tothe gap between text and image embeddings. To this end, this paper proposesHOTS3D which makes the first attempt to effectively bridge this gap by aligningtext features to the image features with spherical optimal transport (SOT).However, in high-dimensional situations, solving the SOT remains a challenge.To obtain the SOT map for high-dimensional features obtained from CLIP encodingof two modalities, we mathematically formulate and derive the solution based onVillani's theorem, which can directly align two hyper-sphere distributionswithout manifold exponential maps. Furthermore, we implement it by leveraginginput convex neural networks (ICNNs) for the optimal Kantorovich potential.With the optimally mapped features, a diffusion-based generator and aNerf-based decoder are subsequently utilized to transform them into 3D shapes.Extensive qualitative and qualitative comparisons with state-of-the-artsdemonstrate the superiority of the proposed HOTS3D for 3D shape generation,especially on the consistency with text semantics.</description><author>Zezeng Li, Weimin Wang, WenHai Li, Na Lei, Xianfeng Gu</author><pubDate>Fri, 19 Jul 2024 15:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14419v1</guid></item><item><title>Improving classification of road surface conditions via road area extraction and contrastive learning</title><link>http://arxiv.org/abs/2407.14418v1</link><description>Maintaining roads is crucial to economic growth and citizen well-beingbecause roads are a vital means of transportation. In various countries, theinspection of road surfaces is still done manually, however, to automate it,research interest is now focused on detecting the road surface defects via thevisual data. While, previous research has been focused on deep learning methodswhich tend to process the entire image and leads to heavy computational cost.In this study, we focus our attention on improving the classificationperformance while keeping the computational cost of our solution low. Insteadof processing the whole image, we introduce a segmentation model to only focusthe downstream classification model to the road surface in the image.Furthermore, we employ contrastive learning during model training to improvethe road surface condition classification. Our experiments on the public RTKdataset demonstrate a significant improvement in our proposed method whencompared to previous works.</description><author>Linh Trinh, Ali Anwar, Siegfried Mercelis</author><pubDate>Fri, 19 Jul 2024 15:43:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14418v1</guid></item><item><title>Mixture of Experts with Mixture of Precisions for Tuning Quality of Service</title><link>http://arxiv.org/abs/2407.14417v1</link><description>The increasing demand for deploying large Mixture-of-Experts (MoE) models inresource-constrained environments necessitates efficient approaches to addresstheir high memory and computational requirements challenges. Moreover, giventhat tasks come in different user-defined constraints and the availableresources change over time in multi-tenant environments, it is necessary todesign an approach which provides a flexible configuration space. This paperpresents an adaptive serving approach for the efficient deployment of MoEmodels, capitalizing on partial quantization of the experts. By dynamicallydetermining the number of quantized experts and their distribution across CPUand GPU, our approach explores the Pareto frontier and offers a fine-grainedrange of configurations for tuning throughput and model quality. Our evaluationon an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three languagemodelling benchmarks demonstrates that the throughput of token generation canbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with amarginal perplexity increase of 2.62 to 2.80, 6.48 to 7.24, and 3.24 to 3.53for WikiText2, PTB, and C4 datasets respectively under maximum quantization.These results highlight the practical applicability of our approach in dynamicand accuracy-sensitive applications where both memory usage and output qualityare important.</description><author>HamidReza Imani, Abdolah Amirany, Tarek El-Ghazawi</author><pubDate>Fri, 19 Jul 2024 15:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14417v1</guid></item><item><title>System-1.x: Learning to Balance Fast and Slow Planning with Language Models</title><link>http://arxiv.org/abs/2407.14414v1</link><description>Language models can be used to solve long-horizon planning problems in twodistinct modes: a fast 'System-1' mode, directly generating plans without anyexplicit search or backtracking, and a slow 'System-2' mode, planningstep-by-step by explicitly searching over possible actions. While System-2 istypically more effective, it is also more computationally expensive, making itinfeasible for long plans or large action spaces. Moreover, isolated System-1or 2 ignores the user's end goals, failing to provide ways to control themodel's behavior. To this end, we propose the System-1.x Planner, acontrollable planning framework with LLMs that is capable of generating hybridplans and balancing between the two planning modes based on the difficulty ofthe problem at hand. System-1.x consists of (i) a controller, (ii) a System-1Planner, and (iii) a System-2 Planner. Based on a user-specified hybridizationfactor (x) governing the mixture between System-1 and 2, the controllerdecomposes a problem into sub-goals, and classifies them as easy or hard to besolved by either System-1 or 2, respectively. We fine-tune all three componentson top of a single base LLM, requiring only search traces as supervision.Experiments with two diverse planning tasks -- Maze Navigation and Blocksworld-- show that our System-1.x Planner outperforms a System-1 Planner, a System-2Planner trained to approximate A* search, and also a symbolic planner (A*). Wedemonstrate the following key properties of our planner: (1) controllability:increasing the hybridization factor (e.g., System-1.75 vs 1.5) performs moresearch, improving performance, (2) flexibility: by building a neuro-symbolicvariant with a neural System-1 and a symbolic System-2, we can use existingsymbolic methods, and (3) generalizability: by being able to learn fromdifferent search algorithms, our method is robust to the choice of searchalgorithm.</description><author>Swarnadeep Saha, Archiki Prasad, Justin Chih-Yao Chen, Peter Hase, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Fri, 19 Jul 2024 15:40:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14414v1</guid></item><item><title>DEAL: Disentangle and Localize Concept-level Explanations for VLMs</title><link>http://arxiv.org/abs/2407.14412v1</link><description>Large pre-trained Vision-Language Models (VLMs) have become ubiquitousfoundational components of other models and downstream tasks. Althoughpowerful, our empirical results reveal that such models might not be able toidentify fine-grained concepts. Specifically, the explanations of VLMs withrespect to fine-grained concepts are entangled and mislocalized. To addressthis issue, we propose to DisEntAngle and Localize (DEAL) the concept-levelexplanations for VLMs without human annotations. The key idea is encouragingthe concept-level explanations to be distinct while maintaining consistencywith category-level explanations. We conduct extensive experiments and ablationstudies on a wide range of benchmark datasets and vision-language models. Ourempirical results demonstrate that the proposed method significantly improvesthe concept-level explanations of the model in terms of disentanglability andlocalizability. Surprisingly, the improved explainability alleviates themodel's reliance on spurious correlations, which further benefits theprediction accuracy.</description><author>Tang Li, Mengmeng Ma, Xi Peng</author><pubDate>Fri, 19 Jul 2024 15:39:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14412v1</guid></item><item><title>HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context</title><link>http://arxiv.org/abs/2407.09375v2</link><description>This work explores the in-context learning capabilities of State Space Models(SSMs) and presents, to the best of our knowledge, the first theoreticalexplanation of a possible underlying mechanism. We introduce a novel weightconstruction for SSMs, enabling them to predict the next state of any dynamicalsystem after observing previous states without parameter fine-tuning. This isaccomplished by extending the HiPPO framework to demonstrate that continuousSSMs can approximate the derivative of any input signal. Specifically, we findan explicit weight construction for continuous SSMs and provide an asymptoticerror bound on the derivative approximation. The discretization of thiscontinuous SSM subsequently yields a discrete SSM that predicts the next state.Finally, we demonstrate the effectiveness of our parameterization empirically.This work should be an initial step toward understanding how sequence modelsbased on SSMs learn in context.</description><author>Federico Arangath Joseph, Kilian Konstantin Haefeli, Noah Liniger, Caglar Gulcehre</author><pubDate>Fri, 19 Jul 2024 15:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09375v2</guid></item><item><title>The Vision of Autonomic Computing: Can LLMs Make It a Reality?</title><link>http://arxiv.org/abs/2407.14402v1</link><description>The Vision of Autonomic Computing (ACV), proposed over two decades ago,envisions computing systems that self-manage akin to biological organisms,adapting seamlessly to changing environments. Despite decades of research,achieving ACV remains challenging due to the dynamic and complex nature ofmodern computing systems. Recent advancements in Large Language Models (LLMs)offer promising solutions to these challenges by leveraging their extensiveknowledge, language understanding, and task automation capabilities. This paperexplores the feasibility of realizing ACV through an LLM-based multi-agentframework for microservice management. We introduce a five-level taxonomy forautonomous service maintenance and present an online evaluation benchmark basedon the Sock Shop microservice demo project to assess our framework'sperformance. Our findings demonstrate significant progress towards achievingLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting andresolving issues within microservice architectures. This study contributes toadvancing autonomic computing by pioneering the integration of LLMs intomicroservice management frameworks, paving the way for more adaptive andself-managing computing systems. The code will be made available athttps://aka.ms/ACV-LLM.</description><author>Zhiyang Zhang, Fangkai Yang, Xiaoting Qin, Jue Zhang, Qingwei Lin, Gong Cheng, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</author><pubDate>Fri, 19 Jul 2024 15:30:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14402v1</guid></item><item><title>MoralBERT: A Fine-Tuned Language Model for Capturing Moral Values in Social Discussions</title><link>http://arxiv.org/abs/2403.07678v2</link><description>Moral values play a fundamental role in how we evaluate information, makedecisions, and form judgements around important social issues. Controversialtopics, including vaccination, abortion, racism, and sexual orientation, oftenelicit opinions and attitudes that are not solely based on evidence but ratherreflect moral worldviews. Recent advances in Natural Language Processing (NLP)show that moral values can be gauged in human-generated textual content.Building on the Moral Foundations Theory (MFT), this paper introducesMoralBERT, a range of language representation models fine-tuned to capturemoral sentiment in social discourse. We describe a framework for bothaggregated and domain-adversarial training on multiple heterogeneous MFThuman-annotated datasets sourced from Twitter (now X), Reddit, and Facebookthat broaden textual content diversity in terms of social media audienceinterests, content presentation and style, and spreading patterns. We show thatthe proposed framework achieves an average F1 score that is between 11% and 32%higher than lexicon-based approaches, Word2Vec embeddings, and zero-shotclassification with large language models such as GPT-4 for in-domaininference. Domain-adversarial training yields better out-of domain predictionsthan aggregate training while achieving comparable performance to zero-shotlearning. Our approach contributes to annotation-free and effective moralitylearning, and provides useful insights towards a more comprehensiveunderstanding of moral narratives in controversial social debates using NLP.</description><author>Vjosa Preniqi, Iacopo Ghinassi, Julia Ive, Charalampos Saitis, Kyriaki Kalimeri</author><pubDate>Fri, 19 Jul 2024 15:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07678v2</guid></item><item><title>On the Impact of PRB Load Uncertainty Forecasting for Sustainable Open RAN</title><link>http://arxiv.org/abs/2407.14400v1</link><description>The transition to sustainable Open Radio Access Network (O-RAN) architecturesbrings new challenges for resource management, especially in predicting theutilization of Physical Resource Block (PRB)s. In this paper, we propose anovel approach to characterize the PRB load using probabilistic forecastingtechniques. First, we provide background information on the O-RAN architectureand components and emphasize the importance of energy/power consumption modelsfor sustainable implementations. The problem statement highlights the need foraccurate PRB load prediction to optimize resource allocation and powerefficiency. We then investigate probabilistic forecasting techniques, includingSimple-Feed-Forward (SFF), DeepAR, and Transformers, and discuss theirlikelihood model assumptions. The simulation results show that DeepARestimators predict the PRBs with less uncertainty and effectively capture thetemporal dependencies in the dataset compared to SFF- and Transformer-basedmodels, leading to power savings. Different percentile selections can alsoincrease power savings, but at the cost of over-/under provisioning. At thesame time, the performance of the Long-Short Term Memory (LSTM) is shown to beinferior to the probabilistic estimators with respect to all error metrics.Finally, we outline the importance of probabilistic, prediction-basedcharacterization for sustainable O-RAN implementations and highlight avenuesfor future research.</description><author>Vaishnavi Kasuluru, Luis Blanco, Cristian J. Vaca-Rubio, Engin Zeydan</author><pubDate>Fri, 19 Jul 2024 15:25:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14400v1</guid></item><item><title>Wildfire Risk Prediction: A Review</title><link>http://arxiv.org/abs/2405.01607v2</link><description>Wildfires have significant impacts on global vegetation, wildlife, andhumans. They destroy plant communities and wildlife habitats and contribute toincreased emissions of carbon dioxide, nitrogen oxides, methane, and otherpollutants. The prediction of wildfires relies on various independent variablescombined with regression or machine learning methods. In this technical review,we describe the options for independent variables, data processing techniques,models, independent variables collinearity and importance estimation methods,and model performance evaluation metrics. First, we divide the independentvariables into 4 aspects, including climate and meteorology conditions,socio-economical factors, terrain and hydrological features, and wildfirehistorical records. Second, preprocessing methods are described for differentmagnitudes, different spatial-temporal resolutions, and different formats ofdata. Third, the collinearity and importance evaluation methods of independentvariables are also considered. Fourth, we discuss the application ofstatistical models, traditional machine learning models, and deep learningmodels in wildfire risk prediction. In this subsection, compared with otherreviews, this manuscript particularly discusses the evaluation metrics andrecent advancements in deep learning methods. Lastly, addressing thelimitations of current research, this paper emphasizes the need for moreeffective deep learning time series forecasting algorithms, the utilization ofthree-dimensional data including ground and trunk fuel, extraction of moreaccurate historical fire point data, and improved model evaluation metrics.</description><author>Zhengsen Xu, Jonathan Li, Linlin Xu</author><pubDate>Fri, 19 Jul 2024 15:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01607v2</guid></item><item><title>Regularized KL-Divergence for Well-Defined Function-Space Variational Inference in Bayesian neural networks</title><link>http://arxiv.org/abs/2406.04317v2</link><description>Bayesian neural networks (BNN) promise to combine the predictive performanceof neural networks with principled uncertainty modeling important forsafety-critical systems and decision making. However, posterior uncertaintyestimates depend on the choice of prior, and finding informative priors inweight-space has proven difficult. This has motivated variational inference(VI) methods that pose priors directly on the function generated by the BNNrather than on weights. In this paper, we address a fundamental issue with suchfunction-space VI approaches pointed out by Burt et al. (2020), who showed thatthe objective function (ELBO) is negative infinite for most priors of interest.Our solution builds on generalized VI (Knoblauch et al., 2019) with theregularized KL divergence (Quang, 2019) and is, to the best of our knowledge,the first well-defined variational objective for function-space inference inBNNs with Gaussian process (GP) priors. Experiments show that our methodincorporates the properties specified by the GP prior on synthetic and smallreal-world data sets, and provides competitive uncertainty estimates forregression, classification and out-of-distribution detection compared to BNNbaselines with both function and weight-space priors.</description><author>Tristan Cinquin, Robert Bamler</author><pubDate>Fri, 19 Jul 2024 15:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04317v2</guid></item><item><title>TTT: A Temporal Refinement Heuristic for Tenuously Tractable Discrete Time Reachability Problems</title><link>http://arxiv.org/abs/2407.14394v1</link><description>Reachable set computation is an important tool for analyzing control systems.Simulating a control system can show that the system is generally functioningas desired, but a formal tool like reachability analysis can provide aguarantee of correctness. For linear systems, reachability analysis isstraightforward and fast, but as more complex components are added to thecontrol system such as nonlinear dynamics or a neural network controller,reachability analysis may slow down or become overly conservative. To addressthese challenges, much literature has focused on spatial refinement, e.g.,tuning the discretization of the input sets and intermediate reachable sets.However, this paper addresses a different dimension: temporal refinement. Thebasic idea of temporal refinement is to automatically choose when along thehorizon of the reachability problem to execute slow symbolic queries whichincur less approximation error versus fast concrete queries which incur moreapproximation error. Temporal refinement can be combined with other refinementapproaches and offers an additional ``tuning knob'' with which to trade offtractability and tightness in approximate reachable set computation. Here, weintroduce an automatic framework for performing temporal refinement and wedemonstrate the effectiveness of this technique on computing approximatereachable sets for nonlinear systems with neural network control policies. Wedemonstrate the calculation of reachable sets of varying approximation errorunder varying computational budget and show that our algorithm is able togenerate approximate reachable sets with a similar amount of error to thebaseline approach in 20-70% less time.</description><author>Chelsea Sidrane, Jana Tumova</author><pubDate>Fri, 19 Jul 2024 15:16:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14394v1</guid></item><item><title>The Future of Large Language Model Pre-training is Federated</title><link>http://arxiv.org/abs/2405.10853v2</link><description>Generative pre-trained large language models (LLMs) have demonstratedimpressive performance over a wide range of tasks, thanks to the unprecedentedamount of data they have been trained on. As established scaling laws indicate,LLMs' future performance improvement depends on the amount of computing anddata sources they can leverage for pre-training. Federated learning (FL) hasthe potential to unleash the majority of the planet's data and computationalresources, which are underutilized by the data-center-focused trainingmethodology of current LLM practice. Our work presents a robust, flexible,reproducible FL approach that enables large-scale collaboration acrossinstitutions to train LLMs. We propose a scalable deployment system calledPhoton to enable the investigation and development of this new trainingparadigm for LLM pre-training. We show that Photon can be used by organizationsinterested in collaborating with their private data sources and computationalresources for pre-training LLMs with billions of parameters. This paradigmwould mobilize more computational and data resources while matching orpotentially exceeding centralized performance. We further show theeffectiveness of the federated training scales with model size and present ourapproach for training a billion-scale federated LLM using limited resources.Finally, we show that LLM training is highly resilient to the classicalchallenges of federated statistical and hardware heterogeneity. Furthermore, weshow that convergence is robust to partial participation, opening the avenuefor compute-efficient collaborative training. Photon will help data-rich actorsto become the protagonists of LLMs pre-training instead of leaving the stage tocompute-rich actors alone.</description><author>Lorenzo Sani, Alex Iacob, Zeyu Cao, Bill Marino, Yan Gao, Tomas Paulik, Wanru Zhao, William F. Shen, Preslav Aleksandrov, Xinchi Qiu, Nicholas D. Lane</author><pubDate>Fri, 19 Jul 2024 15:16:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10853v2</guid></item><item><title>Do Parameters Reveal More than Loss for Membership Inference?</title><link>http://arxiv.org/abs/2406.11544v2</link><description>Membership inference attacks aim to infer whether an individual record wasused to train a model, serving as a key tool for disclosure auditing. Whilesuch evaluations are useful to demonstrate risk, they are computationallyexpensive and often make strong assumptions about potential adversaries' accessto models and training environments, and thus do not provide very tight boundson leakage from potential attacks. We show how prior claims around black-boxaccess being sufficient for optimal membership inference do not hold for mostuseful settings such as stochastic gradient descent, and that optimalmembership inference indeed requires white-box access. We validate our findingswith a new white-box inference attack IHA (Inverse Hessian Attack) thatexplicitly uses model parameters by taking advantage of computinginverse-Hessian vector products. Our results show that both audits andadversaries may be able to benefit from access to model parameters, and weadvocate for further research into white-box methods for membership privacyauditing.</description><author>Anshuman Suri, Xiao Zhang, David Evans</author><pubDate>Fri, 19 Jul 2024 15:13:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11544v2</guid></item><item><title>GLAudio Listens to the Sound of the Graph</title><link>http://arxiv.org/abs/2407.14387v1</link><description>We propose GLAudio: Graph Learning on Audio representation of the nodefeatures and the connectivity structure. This novel architecture propagates thenode features through the graph network according to the discrete wave equationand then employs a sequence learning architecture to learn the target nodefunction from the audio wave signal. This leads to a new paradigm of learningon graph-structured data, in which information propagation and informationprocessing are separated into two distinct steps. We theoretically characterizethe expressivity of our model, introducing the notion of the receptive field ofa vertex, and investigate our model's susceptibility to over-smoothing andover-squashing both theoretically as well as experimentally on various graphdatasets.</description><author>Aurelio Sulser, Johann Wenckstern, Clara Kuempel</author><pubDate>Fri, 19 Jul 2024 15:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14387v1</guid></item><item><title>Frontiers of Deep Learning: From Novel Application to Real-World Deployment</title><link>http://arxiv.org/abs/2407.14386v1</link><description>Deep learning continues to re-shape numerous fields, from natural languageprocessing and imaging to data analytics and recommendation systems. Thisreport studies two research papers that represent recent progress on deeplearning from two largely different aspects: The first paper applied thetransformer networks, which are typically used in language models, to improvethe quality of synthetic aperture radar image by effectively reducing thespeckle noise. The second paper presents an in-storage computing designsolution to enable cost-efficient and high-performance implementations of deeplearning recommendation systems. In addition to summarizing each paper in termsof motivation, key ideas and techniques, and evaluation results, this reportalso presents thoughts and discussions about possible future researchdirections. By carrying out in-depth study on these two representative papersand related references, this doctoral candidate has developed betterunderstanding on the far-reaching impact and efficient implementation of deeplearning models.</description><author>Rui Xie</author><pubDate>Fri, 19 Jul 2024 15:11:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14386v1</guid></item><item><title>The Sticky Path to Expressive Querying: Decidability of Navigational Queries under Existential Rules</title><link>http://arxiv.org/abs/2407.14384v1</link><description>Extensive research in the field of ontology-based query answering has led tothe identification of numerous fragments of existential rules (also known astuple-generating dependencies) that exhibit decidable answering of atomic andconjunctive queries. Motivated by the increased theoretical and practicalinterest in navigational queries, this paper considers the question for whichof these fragments decidability of querying extends to regular path queries(RPQs). In fact, decidability of RPQs has recently been shown to generally holdfor the comprehensive family of all fragments that come with the guarantee ofuniversal models being reasonably well-shaped (that is, being of finitecliquewidth). Yet, for the second major family of fragments, known as finiteunification sets (short: fus), which are based on first-order-rewritability,corresponding results have been largely elusive so far. We complete the pictureby showing that RPQ answering over arbitrary fus rulesets is undecidable. Onthe positive side, we establish that the problem is decidable for the prominentfus subclass of sticky rulesets, with the caveat that a very mild extension ofthe RPQ formalism turns the problem undecidable again.</description><author>Piotr Ostropolski-Nalewaja, Sebastian Rudolph</author><pubDate>Fri, 19 Jul 2024 15:11:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14384v1</guid></item><item><title>Improving GBDT Performance on Imbalanced Datasets: An Empirical Study of Class-Balanced Loss Functions</title><link>http://arxiv.org/abs/2407.14381v1</link><description>Class imbalance remains a significant challenge in machine learning,particularly for tabular data classification tasks. While Gradient BoostingDecision Trees (GBDT) models have proven highly effective for such tasks, theirperformance can be compromised when dealing with imbalanced datasets. Thispaper presents the first comprehensive study on adapting class-balanced lossfunctions to three GBDT algorithms across various tabular classification tasks,including binary, multi-class, and multi-label classification. We conductextensive experiments on multiple datasets to evaluate the impact ofclass-balanced losses on different GBDT models, establishing a valuablebenchmark. Our results demonstrate the potential of class-balanced lossfunctions to enhance GBDT performance on imbalanced datasets, offering a robustapproach for practitioners facing class imbalance challenges in real-worldapplications. Additionally, we introduce a Python package that facilitates theintegration of class-balanced loss functions into GBDT workflows, making theseadvanced techniques accessible to a wider audience.</description><author>Jiaqi Luo, Yuan Yuan, Shixin Xu</author><pubDate>Fri, 19 Jul 2024 15:10:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14381v1</guid></item><item><title>Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer</title><link>http://arxiv.org/abs/2208.12527v3</link><description>Neuromorphic spike data, an upcoming modality with high temporal resolution,has shown promising potential in autonomous driving by mitigating thechallenges posed by high-velocity motion blur. However, training the spikedepth estimation network holds significant challenges in two aspects: sparsespatial information for pixel-wise tasks and difficulties in achieving paireddepth labels for temporally intensive spike streams. Therefore, we introduceopen-source RGB data to support spike depth estimation, leveraging itsannotations and spatial information. The inherent differences in modalities anddata distribution make it challenging to directly apply transfer learning fromopen-source RGB to target spike data. To this end, we propose a cross-modalitycross-domain (BiCross) framework to realize unsupervised spike depth estimationby introducing simulated mediate source spike data. Specifically, we design aCoarse-to-Fine Knowledge Distillation (CFKD) approach to facilitatecomprehensive cross-modality knowledge transfer while preserving the uniquestrengths of both modalities, utilizing a spike-oriented uncertainty scheme.Then, we propose a Self-Correcting Teacher-Student (SCTS) mechanism to screenout reliable pixel-wise pseudo labels and ease the domain shift of the studentmodel, which avoids error accumulation in target spike data. To verify theeffectiveness of BiCross, we conduct extensive experiments on four scenarios,including Synthetic to Real, Extreme Weather, Scene Changing, and Real Spike.Our method achieves state-of-the-art (SOTA) performances, compared withRGB-oriented unsupervised depth estimation methods. Code and dataset:https://github.com/Theia-4869/BiCross</description><author>Jiaming Liu, Qizhe Zhang, Xiaoqi Li, Jianing Li, Guanqun Wang, Ming Lu, Tiejun Huang, Shanghang Zhang</author><pubDate>Fri, 19 Jul 2024 15:05:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12527v3</guid></item><item><title>Enhancing Cloud-Native Resource Allocation with Probabilistic Forecasting Techniques in O-RAN</title><link>http://arxiv.org/abs/2407.14377v1</link><description>The need for intelligent and efficient resource provisioning for theproductive management of resources in real-world scenarios is growing with theevolution of telecommunications towards the 6G era. Technologies such as OpenRadio Access Network (O-RAN) can help to build interoperable solutions for themanagement of complex systems. Probabilistic forecasting, in contrast todeterministic single-point estimators, can offer a different approach toresource allocation by quantifying the uncertainty of the generatedpredictions. This paper examines the cloud-native aspects of O-RAN togetherwith the radio App (rApp) deployment options. The integration of probabilisticforecasting techniques as a rApp in O-RAN is also emphasized, along with casestudies of real-world applications. Through a comparative analysis offorecasting models using the error metric, we show the advantages of DeepAutoregressive Recurrent network (DeepAR) over other deterministicprobabilistic estimators. Furthermore, the simplicity of Simple-Feed-Forward(SFF) leads to a fast runtime but does not capture the temporal dependencies ofthe input data. Finally, we present some aspects related to the practicalapplicability of cloud-native O-RAN with probabilistic forecasting.</description><author>Vaishnavi Kasuluru, Luis Blanco, Engin Zeydan, Albert Bel, Angelos Antonopoulos</author><pubDate>Fri, 19 Jul 2024 15:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14377v1</guid></item><item><title>On the use of Probabilistic Forecasting for Network Analysis in Open RAN</title><link>http://arxiv.org/abs/2407.14375v1</link><description>Unlike other single-point Artificial Intelligence (AI)-based predictiontechniques, such as Long-Short Term Memory (LSTM), probabilistic forecastingtechniques (e.g., DeepAR and Transformer) provide a range of possible outcomesand associated probabilities that enable decision makers to make more informedand robust decisions. At the same time, the architecture of Open RAN hasemerged as a revolutionary approach for mobile networks, aiming at openness,interoperability and innovation in the ecosystem of RAN. In this paper, wepropose the use of probabilistic forecasting techniques as a radio App (rApp)within the Open RAN architecture. We investigate and compare differentprobabilistic and single-point forecasting methods and algorithms to estimatethe utilization and resource demands of Physical Resource Blocks (PRBs) ofcellular base stations. Through our evaluations, we demonstrate the numericaladvantages of probabilistic forecasting techniques over traditionalsingle-point forecasting methods and show that they are capable of providingmore accurate and reliable estimates. In particular, DeepAR clearly outperformssingle-point forecasting techniques such as LSTM and Seasonal-Naive (SN)baselines and other probabilistic forecasting techniques such asSimple-Feed-Forward (SFF) and Transformer neural networks.</description><author>Vaishnavi Kasuluru, Luis Blanco, Engin Zeydan</author><pubDate>Fri, 19 Jul 2024 15:03:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14375v1</guid></item><item><title>SCoPE: Evaluating LLMs for Software Vulnerability Detection</title><link>http://arxiv.org/abs/2407.14372v1</link><description>In recent years, code security has become increasingly important, especiallywith the rise of interconnected technologies. Detecting vulnerabilities earlyin the software development process has demonstrated numerous benefits.Consequently, the scientific community started using machine learning forautomated detection of source code vulnerabilities. This work explores andrefines the CVEFixes dataset, which is commonly used to train models forcode-related tasks, specifically the C/C++ subset. To this purpose, the SourceCode Processing Engine (SCoPE), a framework composed of strategized techniquesthat can be used to reduce the size and normalize C/C++ functions is presented.The output generated by SCoPE was used to create a new version of CVEFixes.This refined dataset was then employed in a feature representation analysis toassess the effectiveness of the tool's code processing techniques, consistingof fine-tuning three pre-trained LLMs for software vulnerability detection. Theresults show that SCoPE successfully helped to identify 905 duplicates withinthe evaluated subset. The LLM results corroborate with the literature regardingtheir suitability for software vulnerability detection, with the best modelachieving 53% F1-score.</description><author>Jos√© Gon√ßalves, Tiago Dias, Eva Maia, Isabel Pra√ßa</author><pubDate>Fri, 19 Jul 2024 15:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14372v1</guid></item><item><title>Open Artificial Knowledge</title><link>http://arxiv.org/abs/2407.14371v1</link><description>The tremendous success of chat-based AI systems like ChatGPT, Claude, andGemini stems from Large Language Models (LLMs) trained on vast amount ofdatasets. However, acquiring high-quality, diverse, and ethically sourcedtraining data remains a significant challenge. We introduce the Open ArtificialKnowledge (OAK) dataset, a large-scale resource of over 500 million tokens (atthe moment of writing) designed to address this issue. OAK leverages anensemble of state-of-the-art LLMs, including GPT4o, LLaMa3-70B, LLaMa3-8B,Mixtral-8x7B, Gemma-7B, and Gemma-2-9B , to generate high-quality text acrossdiverse domains, guided by Wikipedia's main categories. Our methodology ensuresbroad knowledge coverage while maintaining coherence and factual accuracy. TheOAK dataset aims to foster the development of more capable and aligned languagemodels while addressing critical issues of data scarcity and privacy in LLMtraining, and it is freely available on www.oakdataset.org.</description><author>Vadim Borisov, Richard H. Schreiber</author><pubDate>Fri, 19 Jul 2024 15:01:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14371v1</guid></item><item><title>Thinking Racial Bias in Fair Forgery Detection: Models, Datasets and Evaluations</title><link>http://arxiv.org/abs/2407.14367v1</link><description>Due to the successful development of deep image generation technology,forgery detection plays a more important role in social and economic security.Racial bias has not been explored thoroughly in the deep forgery detectionfield. In the paper, we first contribute a dedicated dataset called the FairForgery Detection (FairFD) dataset, where we prove the racial bias of publicstate-of-the-art (SOTA) methods. Different from existing forgery detectiondatasets, the self-construct FairFD dataset contains a balanced racial ratioand diverse forgery generation images with the largest-scale subjects.Additionally, we identify the problems with naive fairness metrics whenbenchmarking forgery detection models. To comprehensively evaluate fairness, wedesign novel metrics including Approach Averaged Metric and Utility RegularizedMetric, which can avoid deceptive results. Extensive experiments conducted withnine representative forgery detection models demonstrate the value of theproposed dataset and the reasonability of the designed fairness metrics. Wealso conduct more in-depth analyses to offer more insights to inspireresearchers in the community.</description><author>Decheng Liu, Zongqi Wang, Chunlei Peng, Nannan Wang, Ruimin Hu, Xinbo Gao</author><pubDate>Fri, 19 Jul 2024 14:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14367v1</guid></item><item><title>Towards Assessing Data Replication in Music Generation with Music Similarity Metrics on Raw Audio</title><link>http://arxiv.org/abs/2407.14364v1</link><description>Recent advancements in music generation are raising multiple concerns aboutthe implications of AI in creative music processes, current business models andimpacts related to intellectual property management. A relevant challenge isthe potential replication and plagiarism of the training set in AI-generatedmusic, which could lead to misuse of data and intellectual property rightsviolations. To tackle this issue, we present the Music Replication Assessment(MiRA) tool: a model-independent open evaluation method based on diverse audiomusic similarity metrics to assess data replication of the training set. Weevaluate the ability of five metrics to identify exact replication, byconducting a controlled replication experiment in different music genres basedon synthetic samples. Our results show that the proposed methodology canestimate exact data replication with a proportion higher than 10%. Byintroducing the MiRA tool, we intend to encourage the open evaluation of musicgenerative models by researchers, developers and users concerning datareplication, highlighting the importance of ethical, social, legal and economicconsequences of generative AI in the music domain.</description><author>Roser Batlle-Roca, Wei-Hisang Liao, Xavier Serra, Yuki Mitsufuji, Emilia G√≥mez</author><pubDate>Fri, 19 Jul 2024 14:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14364v1</guid></item><item><title>FuzzTheREST: An Intelligent Automated Black-box RESTful API Fuzzer</title><link>http://arxiv.org/abs/2407.14361v1</link><description>Software's pervasive impact and increasing reliance in the era of digitaltransformation raise concerns about vulnerabilities, emphasizing the need forsoftware security. Fuzzy testing is a dynamic analysis software testingtechnique that consists of feeding faulty input data to a System Under Test(SUT) and observing its behavior. Specifically regarding black-box RESTful APItesting, recent literature has attempted to automate this technique usingheuristics to perform the input search and using the HTTP response status codesfor classification. However, most approaches do not keep track of codecoverage, which is important to validate the solution. This work introduces ablack-box RESTful API fuzzy testing tool that employs Reinforcement Learning(RL) for vulnerability detection. The fuzzer operates via the OpenAPISpecification (OAS) file and a scenarios file, which includes information tocommunicate with the SUT and the sequences of functionalities to test,respectively. To evaluate its effectiveness, the tool was tested on thePetstore API. The tool found a total of six unique vulnerabilities and achieved55\% code coverage.</description><author>Tiago Dias, Eva Maia, Isabel Pra√ßa</author><pubDate>Fri, 19 Jul 2024 14:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14361v1</guid></item><item><title>Stable Audio Open</title><link>http://arxiv.org/abs/2407.14358v1</link><description>Open generative models are vitally important for the community, allowing forfine-tunes and serving as baselines when presenting new models. However, mostcurrent text-to-audio models are private and not accessible for artists andresearchers to build upon. Here we describe the architecture and trainingprocess of a new open-weights text-to-audio model trained with Creative Commonsdata. Our evaluation shows that the model's performance is competitive with thestate-of-the-art across various metrics. Notably, the reported FDopenl3 results(measuring the realism of the generations) showcase its potential forhigh-quality stereo sound synthesis at 44.1kHz.</description><author>Zach Evans, Julian D. Parker, CJ Carr, Zack Zukowski, Josiah Taylor, Jordi Pons</author><pubDate>Fri, 19 Jul 2024 14:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14358v1</guid></item><item><title>Interior Object Geometry via Fitted Frames</title><link>http://arxiv.org/abs/2407.14357v1</link><description>We describe a representation targeted for anatomic objects which is designedto enable strong locational correspondence within object populations and thusto provide powerful object statistics. The method generates fitted frames onthe boundary and in the interior of objects and produces alignment-freegeometric features from them. It accomplishes this by understanding an objectas the diffeomorphic deformation of an ellipsoid and using a skeletalrepresentation fitted throughout the deformation to produce a model of thetarget object, where the object is provided initially in the form of a boundarymesh. Via classification performance on hippocampi shape between individualswith a disorder vs. others, we compare our method to two state-of-the-artmethods for producing object representations that are intended to capturegeometric correspondence across a population of objects and to yield geometricfeatures useful for statistics, and we show improved classification performanceby this new representation, which we call the evolutionary s-rep. The geometricfeatures that are derived from each of the representations, especially viafitted frames, is discussed.</description><author>Stephen M. Pizer, Zhiyuan Liu, Junjie Zhao, Nicholas Tapp-Hughes, James Damon, Miaomiao Zhang, JS Marron, Jared Vicory</author><pubDate>Fri, 19 Jul 2024 14:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14357v1</guid></item><item><title>Vision-Based Power Line Cables and Pylons Detection for Low Flying Aircrafts</title><link>http://arxiv.org/abs/2407.14352v1</link><description>Power lines are dangerous for low-flying aircrafts, especially inlow-visibility conditions. Thus, a vision-based system able to analyze theaircraft's surroundings and to provide the pilots with a "second pair of eyes"can contribute to enhancing their safety. To this end, we have developed a deeplearning approach to jointly detect power line cables and pylons from imagescaptured at distances of several hundred meters by aircraft-mounted cameras. Indoing so, we have combined a modern convolutional architecture with transferlearning and a loss function adapted to curvilinear structure delineation. Weuse a single network for both detection tasks and demonstrated its performanceon two benchmarking datasets. We have integrated it within an onboard systemand run it in flight, and have demonstrated with our experiments that itoutperforms the prior distant cable detection method on both datasets, whilealso successfully detecting pylons, given their annotations are available forthe data.</description><author>Jakub Gwizda≈Ça, Doruk Oner, Soumava Kumar Roy, Mian Akbar Shah, Ad Eberhard, Ivan Egorov, Philipp Kr√ºsi, Grigory Yakushev</author><pubDate>Fri, 19 Jul 2024 14:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14352v1</guid></item><item><title>Improving Retrieval in Sponsored Search by Leveraging Query Context Signals</title><link>http://arxiv.org/abs/2407.14346v1</link><description>Accurately retrieving relevant bid keywords for user queries is critical inSponsored Search but remains challenging, particularly for short, ambiguousqueries. Existing dense and generative retrieval models often fail to capturenuanced user intent in these cases. To address this, we propose an approach toenhance query understanding by augmenting queries with rich contextual signalsderived from web search results and large language models, stored in an onlinecache. Specifically, we use web search titles and snippets to ground queries inreal-world information and utilize GPT-4 to generate query rewrites andexplanations that clarify user intent. These signals are efficiently integratedthrough a Fusion-in-Decoder based Unity architecture, enabling both dense andgenerative retrieval with serving costs on par with traditional context-freemodels. To address scenarios where context is unavailable in the cache, weintroduce context glancing, a curriculum learning strategy that improves modelrobustness and performance even without contextual signals during inference.Extensive offline experiments demonstrate that our context-aware approachsubstantially outperforms context-free models. Furthermore, online A/B testingon a prominent search engine across 160+ countries shows significantimprovements in user engagement and revenue.</description><author>Akash Kumar Mohankumar, Gururaj K, Gagan Madan, Amit Singh</author><pubDate>Fri, 19 Jul 2024 14:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14346v1</guid></item><item><title>Prototype-based Interpretable Breast Cancer Prediction Models: Analysis and Challenges</title><link>http://arxiv.org/abs/2403.20260v3</link><description>Deep learning models have achieved high performance in medical applications,however, their adoption in clinical practice is hindered due to their black-boxnature. Self-explainable models, like prototype-based models, can be especiallybeneficial as they are interpretable by design. However, if the learntprototypes are of low quality then the prototype-based models are as good asblack-box. Having high quality prototypes is a pre-requisite for a trulyinterpretable model. In this work, we propose a prototype evaluation frameworkfor coherence (PEF-C) for quantitatively evaluating the quality of theprototypes based on domain knowledge. We show the use of PEF-C in the contextof breast cancer prediction using mammography. Existing works onprototype-based models on breast cancer prediction using mammography havefocused on improving the classification performance of prototype-based modelscompared to black-box models and have evaluated prototype quality throughanecdotal evidence. We are the first to go beyond anecdotal evidence andevaluate the quality of the mammography prototypes systematically using ourPEF-C. Specifically, we apply three state-of-the-art prototype-based models,ProtoPNet, BRAIxProtoPNet++ and PIP-Net on mammography images for breast cancerprediction and evaluate these models w.r.t. i) classification performance, andii) quality of the prototypes, on three public datasets. Our results show thatprototype-based models are competitive with black-box models in terms ofclassification performance, and achieve a higher score in detecting ROIs.However, the quality of the prototypes are not yet sufficient and can beimproved in aspects of relevance, purity and learning a variety of prototypes.We call the XAI community to systematically evaluate the quality of theprototypes to check their true usability in high stake decisions and improvesuch models further.</description><author>Shreyasi Pathak, J√∂rg Schl√∂tterer, Jeroen Veltman, Jeroen Geerdink, Maurice van Keulen, Christin Seifert</author><pubDate>Fri, 19 Jul 2024 14:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20260v3</guid></item><item><title>Rapid Biomedical Research Classification: The Pandemic PACT Advanced Categorisation Engine</title><link>http://arxiv.org/abs/2407.10086v2</link><description>This paper introduces the Pandemic PACT Advanced Categorisation Engine(PPACE) along with its associated dataset. PPACE is a fine-tuned modeldeveloped to automatically classify research abstracts from funded biomedicalprojects according to WHO-aligned research priorities. This task is crucial formonitoring research trends and identifying gaps in global health preparednessand response. Our approach builds on human-annotated projects, which areallocated one or more categories from a predefined list. A large language modelis then used to generate `rationales' explaining the reasoning behind theseannotations. This augmented data, comprising expert annotations and rationales,is subsequently used to fine-tune a smaller, more efficient model. Developed aspart of the Pandemic PACT project, which aims to track and analyse researchfunding and clinical evidence for a wide range of diseases with outbreakpotential, PPACE supports informed decision-making by research funders,policymakers, and independent researchers. We introduce and release both thetrained model and the instruction-based dataset used for its training. Ourevaluation shows that PPACE significantly outperforms its baselines. Therelease of PPACE and its associated dataset offers valuable resources forresearchers in multilabel biomedical document classification and supportsadvancements in aligning biomedical research with key global health priorities.</description><author>Omid Rohanian, Mohammadmahdi Nouriborji, Olena Seminog, Rodrigo Furst, Thomas Mendy, Shanthi Levanita, Zaharat Kadri-Alabi, Nusrat Jabin, Daniela Toale, Georgina Humphreys, Emilia Antonio, Adrian Bucher, Alice Norton, David A. Clifton</author><pubDate>Fri, 19 Jul 2024 14:28:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10086v2</guid></item><item><title>LLMs left, right, and center: Assessing GPT's capabilities to label political bias from web domains</title><link>http://arxiv.org/abs/2407.14344v1</link><description>This research investigates whether OpenAI's GPT-4, a state-of-the-art largelanguage model, can accurately classify the political bias of news sourcesbased solely on their URLs. Given the subjective nature of political labels,third-party bias ratings like those from Ad Fontes Media, AllSides, and MediaBias/Fact Check (MBFC) are often used in research to analyze news sourcediversity. This study aims to determine if GPT-4 can replicate these humanratings on a seven-degree scale ("far-left" to "far-right"). The analysiscompares GPT-4's classifications against MBFC's, and controls for websitepopularity using Open PageRank scores. Findings reveal a high correlation($\text{Spearman's } \rho = .89$, $n = 5,877$, $p &lt; 0.001$) between GPT-4's andMBFC's ratings, indicating the model's potential reliability. However, GPT-4abstained from classifying approximately $\frac{2}{3}$ of the dataset,particularly less popular and less biased sources. The study also identifies aslight leftward skew in GPT-4's classifications compared to MBFC's. Theanalysis suggests that while GPT-4 can be a scalable, cost-effective tool forpolitical bias classification of news websites, but its use should complementhuman judgment to mitigate biases. Further research is recommended to explorethe model's performance across different settings, languages, and additionaldatasets.</description><author>Raphael Hernandes</author><pubDate>Fri, 19 Jul 2024 14:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14344v1</guid></item><item><title>Do LLMs have Consistent Values?</title><link>http://arxiv.org/abs/2407.12878v2</link><description>Values are a basic driving force underlying human behavior. Large LanguageModels (LLM) technology is constantly improving towards human-like dialogue.However, little research has been done to study the values exhibited in textgenerated by LLMs. Here we study this question by turning to the richliterature on value structure in psychology. We ask whether LLMs exhibit thesame value structure that has been demonstrated in humans, including theranking of values, and correlation between values. We show that the results ofthis analysis strongly depend on how the LLM is prompted, and that under aparticular prompting strategy (referred to as 'Value Anchoring') the agreementwith human data is quite compelling. Our results serve both to improve ourunderstanding of values in LLMs, as well as introduce novel methods forassessing consistency in LLM responses.</description><author>Naama Rozen, Gal Elidan, Amir Globerson, Ella Daniel</author><pubDate>Fri, 19 Jul 2024 14:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12878v2</guid></item><item><title>Quantifying the value of positive transfer: An experimental case study</title><link>http://arxiv.org/abs/2407.14342v1</link><description>In traditional approaches to structural health monitoring, challenges oftenarise associated with the availability of labelled data. Population-basedstructural health monitoring seeks to overcomes these challenges by leveragingdata/information from similar structures via technologies such as transferlearning. The current paper demonstrate a methodology for quantifying the valueof information transfer in the context of operation and maintenancedecision-making. This demonstration, based on a population of laboratory-scaleaircraft models, highlights the steps required to evaluate the expected valueof information transfer including similarity assessment and prediction oftransfer efficacy. Once evaluated for a given population, the value ofinformation transfer can be used to optimise transfer-learning strategies fornewly-acquired target domains.</description><author>Aidan J. Hughes, Giulia Delo, Jack Poole, Nikolaos Dervilis, Keith Worden</author><pubDate>Fri, 19 Jul 2024 14:23:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14342v1</guid></item><item><title>Large Kernel Distillation Network for Efficient Single Image Super-Resolution</title><link>http://arxiv.org/abs/2407.14340v1</link><description>Efficient and lightweight single-image super-resolution (SISR) has achievedremarkable performance in recent years. One effective approach is the use oflarge kernel designs, which have been shown to improve the performance of SISRmodels while reducing their computational requirements. However, currentstate-of-the-art (SOTA) models still face problems such as high computationalcosts. To address these issues, we propose the Large Kernel DistillationNetwork (LKDN) in this paper. Our approach simplifies the model structure andintroduces more efficient attention modules to reduce computational costs whilealso improving performance. Specifically, we employ the reparameterizationtechnique to enhance model performance without adding extra cost. We alsointroduce a new optimizer from other tasks to SISR, which improves trainingspeed and performance. Our experimental results demonstrate that LKDNoutperforms existing lightweight SR methods and achieves SOTA performance.</description><author>Chengxing Xie, Xiaoming Zhang, Linze Li, Haiteng Meng, Tianlin Zhang, Tianrui Li, Xiaole Zhao</author><pubDate>Fri, 19 Jul 2024 14:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14340v1</guid></item><item><title>Decoupling of neural network calibration measures</title><link>http://arxiv.org/abs/2406.02411v2</link><description>A lot of effort is currently invested in safeguarding autonomous drivingsystems, which heavily rely on deep neural networks for computer vision. Weinvestigate the coupling of different neural network calibration measures witha special focus on the Area Under the Sparsification Error curve (AUSE) metric.We elaborate on the well-known inconsistency in determining optimal calibrationusing the Expected Calibration Error (ECE) and we demonstrate similar issuesfor the AUSE, the Uncertainty Calibration Score (UCS), as well as theUncertainty Calibration Error (UCE). We conclude that the current methodologiesleave a degree of freedom, which prevents a unique model calibration for thehomologation of safety-critical functionalities. Furthermore, we propose theAUSE as an indirect measure for the residual uncertainty, which is irreduciblefor a fixed network architecture and is driven by the stochasticity in theunderlying data generation process (aleatoric contribution) as well as thelimitation in the hypothesis space (epistemic contribution).</description><author>Dominik Werner Wolf, Prasannavenkatesh Balaji, Alexander Braun, Markus Ulrich</author><pubDate>Fri, 19 Jul 2024 14:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02411v2</guid></item><item><title>Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models</title><link>http://arxiv.org/abs/2407.11282v3</link><description>Large Language Models (LLMs) are employed across various high-stakes domains,where the reliability of their outputs is crucial. One commonly used method toassess the reliability of LLMs' responses is uncertainty estimation, whichgauges the likelihood of their answers being correct. While many studies focuson improving the accuracy of uncertainty estimations for LLMs, our researchinvestigates the fragility of uncertainty estimation and explores potentialattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,when activated by a specific trigger in the input, manipulates the model'suncertainty without affecting the final output. Specifically, the proposedbackdoor attack method can alter an LLM's output probability distribution,causing the probability distribution to converge towards an attacker-predefineddistribution while ensuring that the top-1 prediction remains unchanged. Ourexperimental results demonstrate that this attack effectively undermines themodel's self-evaluation reliability in multiple-choice questions. For instance,we achieved a 100 attack success rate (ASR) across three different triggeringstrategies in four models. Further, we investigate whether this manipulationgeneralizes across different prompts and domains. This work highlights asignificant threat to the reliability of LLMs and underscores the need forfuture defenses against such attacks. The code is available athttps://github.com/qcznlp/uncertainty_attack.</description><author>Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang</author><pubDate>Fri, 19 Jul 2024 14:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11282v3</guid></item><item><title>Open-Set Recognition in the Age of Vision-Language Models</title><link>http://arxiv.org/abs/2403.16528v2</link><description>Are vision-language models (VLMs) for open-vocabulary perception inherentlyopen-set models because they are trained on internet-scale datasets? We answerthis question with a clear no - VLMs introduce closed-set assumptions via theirfinite query set, making them vulnerable to open-set conditions. Wesystematically evaluate VLMs for open-set recognition and find they frequentlymisclassify objects not contained in their query set, leading to alarmingly lowprecision when tuned for high recall and vice versa. We show that naivelyincreasing the size of the query set to contain more and more classes does notmitigate this problem, but instead causes diminishing task performance andopen-set performance. We establish a revised definition of the open-set problemfor the age of VLMs, define a new benchmark and evaluation protocol tofacilitate standardised evaluation and research in this important area, andevaluate promising baseline approaches based on predictive uncertainty anddedicated negative embeddings on a range of open-vocabulary VLM classifiers andobject detectors.</description><author>Dimity Miller, Niko S√ºnderhauf, Alex Kenna, Keita Mason</author><pubDate>Fri, 19 Jul 2024 14:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16528v2</guid></item><item><title>Straightforward Layer-wise Pruning for More Efficient Visual Adaptation</title><link>http://arxiv.org/abs/2407.14330v1</link><description>Parameter-efficient transfer learning (PETL) aims to adapt large pre-trainedmodels using limited parameters. While most PETL approaches update the addedparameters and freeze pre-trained weights during training, the minimal impactof task-specific deep layers on cross-domain data poses a challenge as PETLcannot modify them, resulting in redundant model structures. Structural pruningeffectively reduces model redundancy; however, common pruning methods oftenlead to an excessive increase in stored parameters due to varying pruningstructures based on pruning rates and data. Recognizing the storage parametervolume issue, we propose a Straightforward layer-wise pruning method, calledSLS, for pruning PETL-transferred models. By evaluating parameters from afeature perspective of each layer and utilizing clustering metrics to assesscurrent parameters based on clustering phenomena in low-dimensional spaceobtained through t-SNE, SLS facilitates informed pruning decisions. Our studyreveals that layer-wise pruning, with a focus on storing pruning indices,addresses storage volume concerns. Notably, mainstream Layer-wise pruningmethods may not be suitable for assessing layer importance in PETL-transferredmodels, where the majority of parameters are pre-trained and have limitedrelevance to downstream datasets. Comparative analysis against state-of-the-artPETL methods demonstrates that the pruned model achieved a notable balancebetween model throughput and accuracy. Moreover, SLS effectively reducesstorage overhead arising from varying pruned structures while enhancing theaccuracy and speed of pruned models compared to conventional pruning methods.</description><author>Ruizi Han, Jinglei Tang</author><pubDate>Fri, 19 Jul 2024 14:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14330v1</guid></item><item><title>Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection</title><link>http://arxiv.org/abs/2403.14270v2</link><description>Visual relationship detection aims to identify objects and theirrelationships in images. Prior methods approach this task by adding separaterelationship modules or decoders to existing object detection architectures.This separation increases complexity and hinders end-to-end training, whichlimits performance. We propose a simple and highly efficient decoder-freearchitecture for open-vocabulary visual relationship detection. Our modelconsists of a Transformer-based image encoder that represents objects as tokensand models their relationships implicitly. To extract relationship information,we introduce an attention mechanism that selects object pairs likely to form arelationship. We provide a single-stage recipe to train this model on a mixtureof object and relationship detection data. Our approach achievesstate-of-the-art relationship detection performance on Visual Genome and on thelarge-vocabulary GQA benchmark at real-time inference speeds. We provideablations, real-world qualitative examples, and analyses of zero-shotperformance.</description><author>Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer</author><pubDate>Fri, 19 Jul 2024 14:07:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14270v2</guid></item><item><title>Modality-Order Matters! A Novel Hierarchical Feature Fusion Method for CoSAm: A Code-Switched Autism Corpus</title><link>http://arxiv.org/abs/2407.14328v1</link><description>Autism Spectrum Disorder (ASD) is a complex neuro-developmental challenge,presenting a spectrum of difficulties in social interaction, communication, andthe expression of repetitive behaviors in different situations. This increasingprevalence underscores the importance of ASD as a major public health concernand the need for comprehensive research initiatives to advance ourunderstanding of the disorder and its early detection methods. This studyintroduces a novel hierarchical feature fusion method aimed at enhancing theearly detection of ASD in children through the analysis of code-switched speech(English and Hindi). Employing advanced audio processing techniques, theresearch integrates acoustic, paralinguistic, and linguistic information usingTransformer Encoders. This innovative fusion strategy is designed to improveclassification robustness and accuracy, crucial for early and precise ASDidentification. The methodology involves collecting a code-switched speechcorpus, CoSAm, from children diagnosed with ASD and a matched control group.The dataset comprises 61 voice recordings from 30 children diagnosed with ASDand 31 from neurotypical children, aged between 3 and 13 years, resulting in atotal of 159.75 minutes of voice recordings. The feature analysis focuses onMFCCs and extensive statistical attributes to capture speech patternvariability and complexity. The best model performance is achieved using ahierarchical fusion technique with an accuracy of 98.75% using a combination ofacoustic and linguistic features first, followed by paralinguistic features ina hierarchical manner.</description><author>Mohd Mujtaba Akhtar, Girish, Muskaan Singh, Orchid Chetia Phukan</author><pubDate>Fri, 19 Jul 2024 14:06:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14328v1</guid></item><item><title>Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model</title><link>http://arxiv.org/abs/2407.14326v1</link><description>Mammography is crucial for breast cancer surveillance and early diagnosis.However, analyzing mammography images is a demanding task for radiologists, whooften review hundreds of mammograms daily, leading to overdiagnosis andovertreatment. Computer-Aided Diagnosis (CAD) systems have been developed toassist in this process, but their capabilities, particularly in lesionsegmentation, remained limited. With the contemporary advances in deep learningtheir performance may be improved. Recently, vision-language diffusion modelsemerged, demonstrating outstanding performance in image generation andtransferability to various downstream tasks. We aim to harness theircapabilities for breast lesion segmentation in a panoptic setting, whichencompasses both semantic and instance-level predictions. Specifically, wepropose leveraging pretrained features from a Stable Diffusion model as inputsto a state-of-the-art panoptic segmentation architecture, resulting in accuratedelineation of individual breast lesions. To bridge the gap between natural andmedical imaging domains, we incorporated a mammography-specific MAM-E diffusionmodel and BiomedCLIP image and text encoders into this framework. We evaluatedour approach on two recently published mammography datasets, CDD-CESM andVinDr-Mammo. For the instance segmentation task, we noted 40.25 AP0.1 and 46.82AP0.05, as well as 25.44 PQ0.1 and 26.92 PQ0.05. For the semantic segmentationtask, we achieved Dice scores of 38.86 and 40.92, respectively.</description><author>Kun Zhao, Jakub Prokop, Javier Montalt Tordera, Sadegh Mohammadi</author><pubDate>Fri, 19 Jul 2024 14:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14326v1</guid></item><item><title>Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach</title><link>http://arxiv.org/abs/2407.12687v2</link><description>A major challenge facing the world is the provision of equitable anduniversal access to quality education. Recent advances in generative AI (genAI) have created excitement about the potential of new technologies to offer apersonal tutor for every learner and a teaching assistant for every teacher.The full extent of this dream, however, has not yet materialised. We argue thatthis is primarily due to the difficulties with verbalising pedagogicalintuitions into gen AI prompts and the lack of good evaluation practices,reinforced by the challenges in defining excellent pedagogy. Here we presentour work collaborating with learners and educators to translate high levelprinciples from learning science into a pragmatic set of seven diverseeducational benchmarks, spanning quantitative, qualitative, automatic and humanevaluations; and to develop a new set of fine-tuning datasets to improve thepedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluationsshow that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini byeducators and learners on a number of pedagogical dimensions. We hope that thiswork can serve as a first step towards developing a comprehensive educationalevaluation framework, and that this can enable rapid progress within the AI andEdTech communities towards maximising the positive impact of gen AI ineducation.</description><author>Irina Jurenka, Markus Kunesch, Kevin R. McKee, Daniel Gillick, Shaojian Zhu, Sara Wiltberger, Shubham Milind Phal, Katherine Hermann, Daniel Kasenberg, Avishkar Bhoopchand, Ankit Anand, Miruna P√Æslar, Stephanie Chan, Lisa Wang, Jennifer She, Parsa Mahmoudieh, Aliya Rysbek, Wei-Jen Ko, Andrea Huber, Brett Wiltshire, Gal Elidan, Roni Rabin, Jasmin Rubinovitz, Amit Pitaru, Mac McAllister, Julia Wilkowski, David Choi, Roee Engelberg, Lidan Hackmon, Adva Levin, Rachel Griffin, Michael Sears, Filip Bar, Mia Mesar, Mana Jabbour, Arslan Chaudhry, James Cohan, Sridhar Thiagarajan, Nir Levine, Ben Brown, Dilan Gorur, Svetlana Grant, Rachel Hashimshoni, Laura Weidinger, Jieru Hu, Dawn Chen, Kuba Dolecki, Canfer Akbulut, Maxwell Bileschi, Laura Culp, Wen-Xin Dong, Nahema Marchal, Kelsie Van Deman, Hem</author><pubDate>Fri, 19 Jul 2024 14:03:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12687v2</guid></item><item><title>Truly No-Regret Learning in Constrained MDPs</title><link>http://arxiv.org/abs/2402.15776v3</link><description>Constrained Markov decision processes (CMDPs) are a common way to modelsafety constraints in reinforcement learning. State-of-the-art methods forefficiently solving CMDPs are based on primal-dual algorithms. For thesealgorithms, all currently known regret bounds allow for error cancellations --one can compensate for a constraint violation in one round with a strictconstraint satisfaction in another. This makes the online learning processunsafe since it only guarantees safety for the final (mixture) policy but notduring learning. As Efroni et al. (2020) pointed out, it is an open questionwhether primal-dual algorithms can provably achieve sublinear regret if we donot allow error cancellations. In this paper, we give the first affirmativeanswer. We first generalize a result on last-iterate convergence of regularizedprimal-dual schemes to CMDPs with multiple constraints. Building upon thisinsight, we propose a model-based primal-dual algorithm to learn in an unknownCMDP. We prove that our algorithm achieves sublinear regret without errorcancellations.</description><author>Adrian M√ºller, Pragnya Alatur, Volkan Cevher, Giorgia Ramponi, Niao He</author><pubDate>Fri, 19 Jul 2024 14:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15776v3</guid></item><item><title>Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models</title><link>http://arxiv.org/abs/2311.04131v5</link><description>While transformer models exhibit strong capabilities on linguistic tasks,their complex architectures make them difficult to interpret. Recent work hasaimed to reverse engineer transformer models into human-readablerepresentations called circuits that implement algorithmic functions. We extendthis research by analyzing and comparing circuits for similar sequencecontinuation tasks, which include increasing sequences of Arabic numerals,number words, and months. By applying circuit interpretability analysis, weidentify a key sub-circuit in both GPT-2 Small and Llama-2-7B responsible fordetecting sequence members and for predicting the next member in a sequence.Our analysis reveals that semantically related sequences rely on shared circuitsubgraphs with analogous roles. Additionally, we show that this sub-circuit haseffects on various math-related prompts, such as on intervaled circuits,Spanish number word and months continuation, and natural language wordproblems. Overall, documenting shared computational structures enables bettermodel behavior predictions, identification of errors, and safer editingprocedures. This mechanistic understanding of transformers is a critical steptowards building more robust, aligned, and interpretable language models.</description><author>Michael Lan, Philip Torr, Fazl Barez</author><pubDate>Fri, 19 Jul 2024 13:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04131v5</guid></item><item><title>Multimodal Misinformation Detection using Large Vision-Language Models</title><link>http://arxiv.org/abs/2407.14321v1</link><description>The increasing proliferation of misinformation and its alarming impact havemotivated both industry and academia to develop approaches for misinformationdetection and fact checking. Recent advances on large language models (LLMs)have shown remarkable performance in various tasks, but whether and how LLMscould help with misinformation detection remains relatively underexplored. Mostof existing state-of-the-art approaches either do not consider evidence andsolely focus on claim related features or assume the evidence to be provided.Few approaches consider evidence retrieval as part of the misinformationdetection but rely on fine-tuning models. In this paper, we investigate thepotential of LLMs for misinformation detection in a zero-shot setting. Weincorporate an evidence retrieval component into the process as it is crucialto gather pertinent information from various sources to detect the veracity ofclaims. To this end, we propose a novel re-ranking approach for multimodalevidence retrieval using both LLMs and large vision-language models (LVLM). Theretrieved evidence samples (images and texts) serve as the input for anLVLM-based approach for multimodal fact verification (LVLM4FV). To enable afair evaluation, we address the issue of incomplete ground truth for evidencesamples in an existing evidence retrieval dataset by annotating a more completeset of evidence samples for both image and text retrieval. Our experimentalresults on two datasets demonstrate the superiority of the proposed approach inboth evidence retrieval and fact verification tasks and also bettergeneralization capability across dataset compared to the supervised baseline.</description><author>Sahar Tahmasebi, Eric M√ºller-Budack, Ralph Ewerth</author><pubDate>Fri, 19 Jul 2024 13:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14321v1</guid></item><item><title>Joint or Disjoint: Mixing Training Regimes for Early-Exit Models</title><link>http://arxiv.org/abs/2407.14320v1</link><description>Early exits are an important efficiency mechanism integrated into deep neuralnetworks that allows for the termination of the network's forward pass beforeprocessing through all its layers. By allowing early halting of the inferenceprocess for less complex inputs that reached high confidence, early exitssignificantly reduce the amount of computation required. Early exit methods addtrainable internal classifiers which leads to more intricacy in the trainingprocess. However, there is no consistent verification of the approaches oftraining of early exit methods, and no unified scheme of training such models.Most early exit methods employ a training strategy that either simultaneouslytrains the backbone network and the exit heads or trains the exit headsseparately. We propose a training approach where the backbone is initiallytrained on its own, followed by a phase where both the backbone and the exitheads are trained together. Thus, we advocate for organizing early-exittraining strategies into three distinct categories, and then validate them fortheir performance and efficiency. In this benchmark, we perform boththeoretical and empirical analysis of early-exit training regimes. We study themethods in terms of information flow, loss landscape and numerical rank ofactivations and gauge the suitability of regimes for various architectures anddatasets.</description><author>Bart≈Çomiej Krzepkowski, Monika Michaluk, Franciszek Szarwacki, Piotr Kubaty, Jary Pomponi, Tomasz Trzci≈Ñski, Bartosz W√≥jcik, Kamil Adamczewski</author><pubDate>Fri, 19 Jul 2024 13:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14320v1</guid></item><item><title>A Curriculum-style Self-training Approach for Source-Free Semantic Segmentation</title><link>http://arxiv.org/abs/2106.11653v5</link><description>Source-free domain adaptation has developed rapidly in recent years, wherethe well-trained source model is adapted to the target domain instead of thesource data, offering the potential for privacy concerns and intellectualproperty protection. However, a number of feature alignment techniques in priordomain adaptation methods are not feasible in this challenging problem setting.Thereby, we resort to probing inherent domain-invariant feature learning andpropose a curriculum-style self-training approach for source-free domainadaptive semantic segmentation. In particular, we introduce a curriculum-styleentropy minimization method to explore the implicit knowledge from the sourcemodel, which fits the trained source model to the target data using certaininformation from easy-to-hard predictions. We then train the segmentationnetwork by the proposed complementary curriculum-style self-training, whichutilizes the negative and positive pseudo labels following thecurriculum-learning manner. Although negative pseudo-labels with highuncertainty cannot be identified with the correct labels, they can definitelyindicate absent classes. Moreover, we employ an information propagation schemeto further reduce the intra-domain discrepancy within the target domain, whichcould act as a standard post-processing method for the domain adaptation field.Furthermore, we extend the proposed method to a more challenging black-boxsource model scenario where only the source model's predictions are available.Extensive experiments validate that our method yields state-of-the-artperformance on source-free semantic segmentation tasks for bothsynthetic-to-real and adverse conditions datasets. The code and correspondingtrained models are released at \url{https://github.com/yxiwang/ATP}.</description><author>Yuxi Wang, Jian Liang, Zhaoxiang Zhang</author><pubDate>Fri, 19 Jul 2024 13:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.11653v5</guid></item><item><title>Cognitive Bias in High-Stakes Decision-Making with LLMs</title><link>http://arxiv.org/abs/2403.00811v2</link><description>Large language models (LLMs) offer significant potential as tools to supportan expanding range of decision-making tasks. Given their training on human(created) data, LLMs have been shown to inherit societal biases againstprotected groups, as well as be subject to bias functionally resemblingcognitive bias. Human-like bias can impede fair and explainable decisions madewith LLM assistance. Our work introduces BiasBuster, a framework designed touncover, evaluate, and mitigate cognitive bias in LLMs, particularly inhigh-stakes decision-making tasks. Inspired by prior research in psychology andcognitive science, we develop a dataset containing 16,800 prompts to evaluatedifferent cognitive biases (e.g., prompt-induced, sequential, inherent). Wetest various bias mitigation strategies, amidst proposing a novel methodutilising LLMs to debias their own prompts. Our analysis provides acomprehensive picture of the presence and effects of cognitive bias acrosscommercial and open-source models. We demonstrate that our self-help debiasingeffectively mitigates model answers that display patterns akin to humancognitive bias without having to manually craft examples for each bias.</description><author>Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, Zexue He</author><pubDate>Fri, 19 Jul 2024 13:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00811v2</guid></item><item><title>EmoCAM: Toward Understanding What Drives CNN-based Emotion Recognition</title><link>http://arxiv.org/abs/2407.14314v1</link><description>Convolutional Neural Networks are particularly suited for image analysistasks, such as Image Classification, Object Recognition or Image Segmentation.Like all Artificial Neural Networks, however, they are "black box" models, andsuffer from poor explainability. This work is concerned with the specificdownstream task of Emotion Recognition from images, and proposes a frameworkthat combines CAM-based techniques with Object Detection on a corpus level tobetter understand on which image cues a particular model, in our case EmoNet,relies to assign a specific emotion to an image. We demonstrate that the modelmostly focuses on human characteristics, but also explore the pronounced effectof specific image modifications.</description><author>Youssef Doulfoukar, Laurent Mertens, Joost Vennekens</author><pubDate>Fri, 19 Jul 2024 13:47:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14314v1</guid></item></channel></rss>