<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 14 Nov 2024 13:00:25 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Scaling Properties of Diffusion Models for Perceptual Tasks</title><link>http://arxiv.org/abs/2411.08034v2</link><description>In this paper, we argue that iterative computation with diffusion modelsoffers a powerful paradigm for not only generation but also visual perceptiontasks. We unify tasks such as depth estimation, optical flow, and amodalsegmentation under the framework of image-to-image translation, and show howdiffusion models benefit from scaling training and test-time compute for theseperceptual tasks. Through a careful analysis of these scaling properties, weformulate compute-optimal training and inference recipes to scale diffusionmodels for visual perception tasks. Our models achieve competitive performanceto state-of-the-art methods using significantly less data and compute. Toaccess our code and models, see https://scaling-diffusion-perception.github.io .</description><author>Rahul Ravishankar, Zeeshan Patel, Jathushan Rajasegaran, Jitendra Malik</author><pubDate>Wed, 13 Nov 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08034v2</guid></item><item><title>4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization</title><link>http://arxiv.org/abs/2411.08879v1</link><description>Novel view synthesis of dynamic scenes is becoming important in variousapplications, including augmented and virtual reality. We propose a novel 4DGaussian Splatting (4DGS) algorithm for dynamic scenes from casually recordedmonocular videos. To overcome the overfitting problem of existing work forthese real-world videos, we introduce an uncertainty-aware regularization thatidentifies uncertain regions with few observations and selectively imposesadditional priors based on diffusion models and depth smoothness on suchregions. This approach improves both the performance of novel view synthesisand the quality of training image reconstruction. We also identify theinitialization problem of 4DGS in fast-moving dynamic regions, where theStructure from Motion (SfM) algorithm fails to provide reliable 3D landmarks.To initialize Gaussian primitives in such regions, we present a dynamic regiondensification method using the estimated depth maps and scene flow. Ourexperiments show that the proposed method improves the performance of 4DGSreconstruction from a video captured by a handheld monocular camera and alsoexhibits promising results in few-shot static scene reconstruction.</description><author>Mijeong Kim, Jongwoo Lim, Bohyung Han</author><pubDate>Wed, 13 Nov 2024 18:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08879v1</guid></item><item><title>A Short Note on Evaluating RepNet for Temporal Repetition Counting in Videos</title><link>http://arxiv.org/abs/2411.08878v1</link><description>We discuss some consistent issues on how RepNet has been evaluated in variouspapers. As a way to mitigate these issues, we report RepNet performance resultson different datasets, and release evaluation code and the RepNet checkpoint toobtain these results. Code URL:https://github.com/google-research/google-research/blob/master/repnet/</description><author>Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman</author><pubDate>Wed, 13 Nov 2024 18:55:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08878v1</guid></item><item><title>Causal Explanations for Image Classifiers</title><link>http://arxiv.org/abs/2411.08875v1</link><description>Existing algorithms for explaining the output of image classifiers usedifferent definitions of explanations and a variety of techniques to extractthem. However, none of the existing tools use a principled approach based onformal definitions of causes and explanations for the explanation extraction.In this paper we present a novel black-box approach to computing explanationsgrounded in the theory of actual causality. We prove relevant theoreticalresults and present an algorithm for computing approximate explanations basedon these definitions. We prove termination of our algorithm and discuss itscomplexity and the amount of approximation compared to the precise definition.We implemented the framework in a tool rex and we present experimental resultsand a comparison with state-of-the-art tools. We demonstrate that rex is themost efficient tool and produces the smallest explanations, in addition tooutperforming other black-box tools on standard quality measures.</description><author>Hana Chockler, David A. Kelly, Daniel Kroening, Youcheng Sun</author><pubDate>Wed, 13 Nov 2024 18:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08875v1</guid></item><item><title>The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models</title><link>http://arxiv.org/abs/2411.08870v1</link><description>Several recent works seek to develop foundation models specifically formedical applications, adapting general-purpose large language models (LLMs) andvision-language models (VLMs) via continued pretraining on publicly availablebiomedical corpora. These works typically claim that such domain-adaptivepretraining (DAPT) improves performance on downstream medical tasks, such asanswering medical licensing exam questions. In this paper, we compare tenpublic "medical" LLMs and two VLMs against their corresponding base models,arriving at a different conclusion: all medical VLMs and nearly all medicalLLMs fail to consistently improve over their base models in the zero-/few-shotprompting and supervised fine-tuning regimes for medical question-answering(QA). For instance, across all tasks and model pairs we consider in the 3-shotsetting, medical LLMs only outperform their base models in 22.7% of cases,reach a (statistical) tie in 36.8% of cases, and are significantly worse thantheir base models in the remaining 40.5% of cases. Our conclusions are based on(i) comparing each medical model head-to-head, directly against thecorresponding base model; (ii) optimizing the prompts for each model separatelyin zero-/few-shot prompting; and (iii) accounting for statistical uncertaintyin comparisons. While these basic practices are not consistently adopted in theliterature, our ablations show that they substantially impact conclusions.Meanwhile, we find that after fine-tuning on specific QA tasks, medical LLMscan show performance improvements, but the benefits do not carry over to tasksbased on clinical notes. Our findings suggest that state-of-the-artgeneral-domain models may already exhibit strong medical knowledge andreasoning capabilities, and offer recommendations to strengthen the conclusionsof future studies.</description><author>Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst</author><pubDate>Wed, 13 Nov 2024 18:50:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08870v1</guid></item><item><title>CamemBERT 2.0: A Smarter French Language Model Aged to Perfection</title><link>http://arxiv.org/abs/2411.08868v1</link><description>French language models, such as CamemBERT, have been widely adopted acrossindustries for natural language processing (NLP) tasks, with models likeCamemBERT seeing over 4 million downloads per month. However, these models facechallenges due to temporal concept drift, where outdated training data leads toa decline in performance, especially when encountering new topics andterminology. This issue emphasizes the need for updated models that reflectcurrent linguistic trends. In this paper, we introduce two new versions of theCamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address thesechallenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes useof the Replaced Token Detection (RTD) objective for better contextualunderstanding, while CamemBERTv2 is built on RoBERTa, which uses the MaskedLanguage Modeling (MLM) objective. Both models are trained on a significantlylarger and more recent dataset with longer context length and an updatedtokenizer that enhances tokenization performance for French. We evaluate theperformance of these models on both general-domain NLP tasks anddomain-specific applications, such as medical field tasks, demonstrating theirversatility and effectiveness across a range of use cases. Our results showthat these updated models vastly outperform their predecessors, making themvaluable tools for modern NLP systems. All our new models, as well asintermediate checkpoints, are made openly available on Huggingface.</description><author>Wissam Antoun, Francis Kulumba, Rian Touchent, Éric de la Clergerie, Benoît Sagot, Djamé Seddah</author><pubDate>Wed, 13 Nov 2024 18:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08868v1</guid></item><item><title>Unsupervised Parameter-free Outlier Detection using HDBSCAN* Outlier Profiles</title><link>http://arxiv.org/abs/2411.08867v1</link><description>In machine learning and data mining, outliers are data points thatsignificantly differ from the dataset and often introduce irrelevantinformation that can induce bias in its statistics and models. Therefore,unsupervised methods are crucial to detect outliers if there is limited or noinformation about them. Global-Local Outlier Scores based on Hierarchies(GLOSH) is an unsupervised outlier detection method within HDBSCAN*, astate-of-the-art hierarchical clustering method. GLOSH estimates outlier scoresfor each data point by comparing its density to the highest density of theregion they reside in the HDBSCAN* hierarchy. GLOSH may be sensitive toHDBSCAN*'s minpts parameter that influences density estimation. With limitedknowledge about the data, choosing an appropriate minpts value beforehand ischallenging as one or some minpts values may better represent the underlyingcluster structure than others. Additionally, in the process of searching for``potential outliers'', one has to define the number of outliers n a datasethas, which may be impractical and is often unknown. In this paper, we proposean unsupervised strategy to find the ``best'' minpts value, leveraging therange of GLOSH scores across minpts values to identify the value for whichGLOSH scores can best identify outliers from the rest of the dataset. Moreover,we propose an unsupervised strategy to estimate a threshold for classifyingpoints into inliers and (potential) outliers without the need to pre-define anyvalue. Our experiments show that our strategies can automatically find theminpts value and threshold that yield the best or near best outlier detectionresults using GLOSH.</description><author>Kushankur Ghosh, Murilo Coelho Naldi, Jörg Sander, Euijin Choo</author><pubDate>Wed, 13 Nov 2024 18:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08867v1</guid></item><item><title>LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs</title><link>http://arxiv.org/abs/2411.08862v1</link><description>We introduce LLMStinger, a novel approach that leverages Large LanguageModels (LLMs) to automatically generate adversarial suffixes for jailbreakattacks. Unlike traditional methods, which require complex prompt engineeringor white-box access, LLMStinger uses a reinforcement learning (RL) loop tofine-tune an attacker LLM, generating new suffixes based on existing attacksfor harmful questions from the HarmBench benchmark. Our method significantlyoutperforms existing red-teaming approaches (we compared against 15 of thelatest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) onLLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known fortheir extensive safety measures. Additionally, we achieved a 94.97% ASR onGPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptabilityof LLMStinger across open and closed-source models.</description><author>Piyush Jha, Arnav Arora, Vijay Ganesh</author><pubDate>Wed, 13 Nov 2024 18:44:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08862v1</guid></item><item><title>Interaction Testing in Variation Analysis</title><link>http://arxiv.org/abs/2411.08861v1</link><description>Relationships of cause and effect are of prime importance for explainingscientific phenomena. Often, rather than just understanding the effects ofcauses, researchers also wish to understand how a cause $X$ affects an outcome$Y$ mechanistically -- i.e., what are the causal pathways that are activatedbetween $X$ and $Y$. For analyzing such questions, a range of methods has beendeveloped over decades under the rubric of causal mediation analysis.Traditional mediation analysis focuses on decomposing the average treatmenteffect (ATE) into direct and indirect effects, and therefore focuses on the ATEas the central quantity. This corresponds to providing explanations forassociations in the interventional regime, such as when the treatment $X$ israndomized. Commonly, however, it is of interest to explain associations in theobservational regime, and not just in the interventional regime. In this paper,we introduce \text{variation analysis}, an extension of mediation analysis thatfocuses on the total variation (TV) measure between $X$ and $Y$, written as$\mathrm{E}[Y \mid X=x_1] - \mathrm{E}[Y \mid X=x_0]$. The TV measureencompasses both causal and confounded effects, as opposed to the ATE whichonly encompasses causal (direct and mediated) variations. In this way, the TVmeasure is suitable for providing explanations in the natural regime andanswering questions such as ``why is $X$ associated with $Y$?''. Our focus ison decomposing the TV measure, in a way that explicitly includes direct,indirect, and confounded variations. Furthermore, we also decompose the TVmeasure to include interaction terms between these different pathways.Subsequently, interaction testing is introduced, involving hypothesis tests todetermine if interaction terms are significantly different from zero. Ifinteractions are not significant, more parsimonious decompositions of the TVmeasure can be used.</description><author>Drago Plecko</author><pubDate>Wed, 13 Nov 2024 18:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08861v1</guid></item><item><title>Forensic Iris Image-Based Post-Mortem Interval Estimation</title><link>http://arxiv.org/abs/2404.10172v3</link><description>Post-mortem iris recognition is an emerging application of iris-based humanidentification in a forensic setup. One factor that may be useful inconditioning iris recognition methods is the tissue decomposition level, whichis correlated with the post-mortem interval (PMI), \ie the number of hours thathave elapsed since death. PMI, however, is not always available, and itsprecise estimation remains one of the core challenges in forensic examination.This paper presents the first known to us method of the PMI estimation directlyfrom iris images captured after death. To assess the feasibility of theiris-based PMI estimation, we designed models predicting the PMI from (a)near-infrared (NIR), (b) visible (RGB), and (c) multispectral (RGB+NIR)forensic iris images. Models were evaluated following a 10-foldcross-validation, in (S1) sample-disjoint, (S2) subject-disjoint, and (S3)cross-dataset scenarios. We explore two data balancing techniques for S3:resampling-based balancing (S3-real), and synthetic data-supplemented balancing(S3-synthetic). We found that using the multispectral data offers aspectacularly low mean absolute error (MAE) of $\approx 3.5$ hours in thescenario (S1), a bit worse MAE $\approx 17.5$ hours in the scenario (S2), andMAE $\approx 45.77$ hours in the scenario (S3). Additionally, supplementing thetraining set with synthetically-generated forensic iris images (S3-synthetic)significantly enhances the models' ability to generalize to new NIR, RGB andmultispectral data collected in a different lab. This suggests that if theenvironmental conditions are favorable (\eg, bodies are kept in lowtemperatures), forensic iris images provide features that are indicative of thePMI and can be automatically estimated.</description><author>Rasel Ahmed Bhuiyan, Adam Czajka</author><pubDate>Wed, 13 Nov 2024 18:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10172v3</guid></item><item><title>Regional Style and Color Transfer</title><link>http://arxiv.org/abs/2404.13880v4</link><description>This paper presents a novel contribution to the field of regional styletransfer. Existing methods often suffer from the drawback of applying stylehomogeneously across the entire image, leading to stylistic inconsistencies orforeground object twisted when applied to image with foreground elements suchas person figures. To address this limitation, we propose a new approach thatleverages a segmentation network to precisely isolate foreground objects withinthe input image. Subsequently, style transfer is applied exclusively to thebackground region. The isolated foreground objects are then carefullyreintegrated into the style-transferred background. To enhance the visualcoherence between foreground and background, a color transfer step is employedon the foreground elements prior to their rein-corporation. Finally, we utilizefeathering techniques to achieve a seamless amalgamation of foreground andbackground, resulting in a visually unified and aesthetically pleasing finalcomposition. Extensive evaluations demonstrate that our proposed approachyields significantly more natural stylistic transformations compared toconventional methods.</description><author>Zhicheng Ding, Panfeng Li, Qikai Yang, Siyang Li, Qingtian Gong</author><pubDate>Wed, 13 Nov 2024 18:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13880v4</guid></item><item><title>Oblique Bayesian additive regression trees</title><link>http://arxiv.org/abs/2411.08849v1</link><description>Current implementations of Bayesian Additive Regression Trees (BART) arebased on axis-aligned decision rules that recursively partition the featurespace using a single feature at a time. Several authors have demonstrated thatoblique trees, whose decision rules are based on linear combinations offeatures, can sometimes yield better predictions than axis-aligned trees andexhibit excellent theoretical properties. We develop an oblique version of BARTthat leverages a data-adaptive decision rule prior that recursively partitionsthe feature space along random hyperplanes. Using several synthetic andreal-world benchmark datasets, we systematically compared our oblique BARTimplementation to axis-aligned BART and other tree ensemble methods, findingthat oblique BART was competitive with -- and sometimes much better than --those methods.</description><author>Paul-Hieu V. Nguyen, Ryan Yee, Sameer K. Deshpande</author><pubDate>Wed, 13 Nov 2024 18:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08849v1</guid></item><item><title>Data-driven Surface Solar Irradiance Estimation using Neural Operators at Global Scale</title><link>http://arxiv.org/abs/2411.08843v1</link><description>Accurate surface solar irradiance (SSI) forecasting is essential foroptimizing renewable energy systems, particularly in the context of long-termenergy planning on a global scale. This paper presents a pioneering approach tosolar radiation forecasting that leverages recent advancements in numericalweather prediction (NWP) and data-driven machine learning weather models. Theseadvances facilitate long, stable rollouts and enable large ensemble forecasts,enhancing the reliability of predictions. Our flexible model utilizes variablesforecast by these NWP and AI weather models to estimate 6-hourly SSI at globalscale. Developed using NVIDIA Modulus, our model represents the first adaptiveglobal framework capable of providing long-term SSI forecasts. Furthermore, itcan be fine-tuned using satellite data, which significantly enhances itsperformance in the fine-tuned regions, while maintaining accuracy elsewhere.The improved accuracy of these forecasts has substantial implications for theintegration of solar energy into power grids, enabling more efficient energymanagement and contributing to the global transition to renewable energysources.</description><author>Alberto Carpentieri, Jussi Leinonen, Jeff Adie, Boris Bonev, Doris Folini, Farah Hariri</author><pubDate>Wed, 13 Nov 2024 18:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08843v1</guid></item><item><title>A Single Transformer for Scalable Vision-Language Modeling</title><link>http://arxiv.org/abs/2407.06438v2</link><description>We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.Current large vision-language models (LVLMs) such as LLaVA mostly employheterogeneous architectures that connect pre-trained visual encoders with largelanguage models (LLMs) to facilitate visual recognition and complex reasoning.Although achieving remarkable performance with relatively lightweight training,we identify four primary scalability limitations: (1) The visual capacity isconstrained by pre-trained visual encoders, which are typically an order ofmagnitude smaller than LLMs. (2) The heterogeneous architecture complicates theuse of established hardware and software infrastructure. (3) Study of scalinglaws on such architecture must consider three separate components - visualencoder, connector, and LLMs, which complicates the analysis. (4) The use ofexisting visual encoders typically requires following a pre-definedspecification of image inputs pre-processing, for example, by reshaping inputsto fixed-resolution square images, which presents difficulties in processingand training on high-resolution images or those with unusual aspect ratio. Aunified single Transformer architecture, like SOLO, effectively addresses thesescalability concerns in LVLMs; however, its limited adoption in the moderncontext likely stems from the absence of reliable training recipes that balanceboth modalities and ensure stable training for billion-scale models. In thispaper, we introduce the first open-source training recipe for developing SOLO,an open-source 7B LVLM using moderate academic resources. The training recipeinvolves initializing from LLMs, sequential pre-training on ImageNet andweb-scale data, and instruction fine-tuning on our curated high-qualitydatasets. On extensive evaluation, SOLO demonstrates performance comparable toLLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.</description><author>Yangyi Chen, Xingyao Wang, Hao Peng, Heng Ji</author><pubDate>Wed, 13 Nov 2024 18:21:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06438v2</guid></item><item><title>AstroM$^3$: A self-supervised multimodal model for astronomy</title><link>http://arxiv.org/abs/2411.08842v1</link><description>While machine-learned models are now routinely employed to facilitateastronomical inquiry, model inputs tend to be limited to a primary data source(namely images or time series) and, in the more advanced approaches, somemetadata. Yet with the growing use of wide-field, multiplexed observationalresources, individual sources of interest often have a broad range ofobservational modes available. Here we construct an astronomical multimodaldataset and propose AstroM$^3$, a self-supervised pre-training approach thatenables a model to learn from multiple modalities simultaneously. Specifically,we extend the CLIP (Contrastive Language-Image Pretraining) model to a trimodalsetting, allowing the integration of time-series photometry data, spectra, andastrophysical metadata. In a fine-tuning supervised setting, our resultsdemonstrate that CLIP pre-training improves classification performance fortime-series photometry, where accuracy increases from 84.6% to 91.5%.Furthermore, CLIP boosts classification accuracy by up to 12.6% when theavailability of labeled data is limited, showing the effectiveness ofleveraging larger corpora of unlabeled data. In addition to fine-tunedclassification, we can use the trained model in other downstream tasks that arenot explicitly contemplated during the construction of the self-supervisedmodel. In particular we show the efficacy of using the learned embeddings formisclassifications identification, similarity search, and anomaly detection.One surprising highlight is the "rediscovery" of Mira subtypes and twoRotational variable subclasses using manifold learning and dimension reductionalgorithm. To our knowledge this is the first construction of an $n&gt;2$ modemodel in astronomy. Extensions to $n&gt;3$ modes is naturally anticipated withthis approach.</description><author>Mariia Rizhko, Joshua S. Bloom</author><pubDate>Wed, 13 Nov 2024 18:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08842v1</guid></item><item><title>Multimodal Instruction Tuning with Hybrid State Space Models</title><link>http://arxiv.org/abs/2411.08840v1</link><description>Handling lengthy context is crucial for enhancing the recognition andunderstanding capabilities of multimodal large language models (MLLMs) inapplications such as processing high-resolution images or high frame ratevideos. The rise in image resolution and frame rate substantially increasescomputational demands due to the increased number of input tokens. Thischallenge is further exacerbated by the quadratic complexity with respect tosequence length of the self-attention mechanism. Most prior works eitherpre-train models with long contexts, overlooking the efficiency problem, orattempt to reduce the context length via downsampling (e.g., identify the keyimage patches or frames) to decrease the context length, which may result ininformation loss. To circumvent this issue while keeping the remarkableeffectiveness of MLLMs, we propose a novel approach using a hybridtransformer-MAMBA model to efficiently handle long contexts in multimodalapplications. Our multimodal model can effectively process long context inputexceeding 100k tokens, outperforming existing models across various benchmarks.Remarkably, our model enhances inference efficiency for high-resolution imagesand high-frame-rate videos by about 4 times compared to current models, withefficiency gains increasing as image resolution or video frames rise.Furthermore, our model is the first to be trained on low-resolution images orlow-frame-rate videos while being capable of inference on high-resolutionimages and high-frame-rate videos, offering flexibility for inference indiverse scenarios.</description><author>Jianing Zhou, Han Li, Shuai Zhang, Ning Xie, Ruijie Wang, Xiaohan Nie, Sheng Liu, Lingyun Wang</author><pubDate>Wed, 13 Nov 2024 18:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08840v1</guid></item><item><title>Offline Adaptation of Quadruped Locomotion using Diffusion Models</title><link>http://arxiv.org/abs/2411.08832v1</link><description>We present a diffusion-based approach to quadrupedal locomotion thatsimultaneously addresses the limitations of learning and interpolating betweenmultiple skills and of (modes) offline adapting to new locomotion behavioursafter training. This is the first framework to apply classifier-free guideddiffusion to quadruped locomotion and demonstrate its efficacy by extractinggoal-conditioned behaviour from an originally unlabelled dataset. We show thatthese capabilities are compatible with a multi-skill policy and can be appliedwith little modification and minimal compute overhead, i.e., running entirelyon the robots onboard CPU. We verify the validity of our approach with hardwareexperiments on the ANYmal quadruped platform.</description><author>Reece O'Mahoney, Alexander L. Mitchell, Wanming Yu, Ingmar Posner, Ioannis Havoutis</author><pubDate>Wed, 13 Nov 2024 18:12:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08832v1</guid></item><item><title>MILU: A Multi-task Indic Language Understanding Benchmark</title><link>http://arxiv.org/abs/2411.02538v2</link><description>Evaluating Large Language Models (LLMs) in low-resource and linguisticallydiverse languages remains a significant challenge in NLP, particularly forlanguages using non-Latin scripts like those spoken in India. Existingbenchmarks predominantly focus on English, leaving substantial gaps inassessing LLM capabilities in these languages. We introduce MILU, a Multi taskIndic Language Understanding Benchmark, a comprehensive evaluation benchmarkdesigned to address this gap. MILU spans 8 domains and 42 subjects across 11Indic languages, reflecting both general and culturally specific knowledge.With an India-centric design, incorporates material from regional andstate-level examinations, covering topics such as local history, arts,festivals, and laws, alongside standard subjects like science and mathematics.We evaluate over 45 LLMs, and find that current LLMs struggle with MILU, withGPT-4o achieving the highest average accuracy at 72 percent. Open multilingualmodels outperform language-specific fine-tuned models, which perform onlyslightly better than random baselines. Models also perform better in highresource languages as compared to low resource ones. Domain-wise analysisindicates that models perform poorly in culturally relevant areas like Arts andHumanities, Law and Governance compared to general fields like STEM. To thebest of our knowledge, MILU is the first of its kind benchmark focused on Indiclanguages, serving as a crucial step towards comprehensive cultural evaluation.All code, benchmarks, and artifacts are publicly available to foster openresearch.</description><author>Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra Murthy, Jaydeep Sen</author><pubDate>Wed, 13 Nov 2024 18:04:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02538v2</guid></item><item><title>Model agnostic local variable importance for locally dependent relationships</title><link>http://arxiv.org/abs/2411.08821v1</link><description>Global variable importance measures are commonly used to interpret machinelearning model results. Local variable importance techniques assess howvariables contribute to individual observations rather than the entire dataset.Current methods typically fail to accurately reflect locally dependentrelationships between variables and instead focus on marginal importancevalues. Additionally, they are not natively adapted for multi-classclassification problems. We propose a new model-agnostic method for calculatinglocal variable importance, CLIQUE, that captures locally dependentrelationships, contains improvements over permutation-based methods, and can bedirectly applied to multi-class classification problems. Simulated andreal-world examples show that CLIQUE emphasizes locally dependent informationand properly reduces bias in regions where variables do not affect theresponse.</description><author>Kelvyn K. Bladen, Adele Cutler, D. Richard Cutler, Kevin R. Moon</author><pubDate>Wed, 13 Nov 2024 17:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08821v1</guid></item><item><title>Process-aware Human Activity Recognition</title><link>http://arxiv.org/abs/2411.08814v1</link><description>Humans naturally follow distinct patterns when conducting their dailyactivities, which are driven by established practices and processes, such asproduction workflows, social norms and daily routines. Human activityrecognition (HAR) algorithms usually use neural networks or machine learningtechniques to analyse inherent relationships within the data. However, theseapproaches often overlook the contextual information in which the data aregenerated, potentially limiting their effectiveness. We propose a novelapproach that incorporates process information from context to enhance the HARperformance. Specifically, we align probabilistic events generated by machinelearning models with process models derived from contextual information. Thisalignment adaptively weighs these two sources of information to optimise HARaccuracy. Our experiments demonstrate that our approach achieves betteraccuracy and Macro F1-score compared to baseline models.</description><author>Jiawei Zheng, Petros Papapanagiotou, Jacques D. Fleuriot, Jane Hillston</author><pubDate>Wed, 13 Nov 2024 17:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08814v1</guid></item><item><title>Rethinking CyberSecEval: An LLM-Aided Approach to Evaluation Critique</title><link>http://arxiv.org/abs/2411.08813v1</link><description>A key development in the cybersecurity evaluations space is the work carriedout by Meta, through their CyberSecEval approach. While this work isundoubtedly a useful contribution to a nascent field, there are notablefeatures that limit its utility. Key drawbacks focus on the insecure codedetection part of Meta's methodology. We explore these limitations, and use ourexploration as a test case for LLM-assisted benchmark analysis.</description><author>Suhas Hariharan, Zainab Ali Majid, Jaime Raldua Veuthey, Jacob Haimes</author><pubDate>Wed, 13 Nov 2024 17:51:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08813v1</guid></item><item><title>Physics-Informed Geometry-Aware Neural Operator</title><link>http://arxiv.org/abs/2408.01600v3</link><description>Engineering design problems often involve solving parametric PartialDifferential Equations (PDEs) under variable PDE parameters and domaingeometry. Recently, neural operators have shown promise in learning PDEoperators and quickly predicting the PDE solutions. However, training theseneural operators typically requires large datasets, the acquisition of whichcan be prohibitively expensive. To overcome this, physics-informed trainingoffers an alternative way of building neural operators, eliminating the highcomputational costs associated with Finite Element generation of training data.Nevertheless, current physics-informed neural operators struggle withlimitations, either in handling varying domain geometries or varying PDEparameters. In this research, we introduce a novel method, the Physics-InformedGeometry-Aware Neural Operator (PI-GANO), designed to simultaneously generalizeacross both PDE parameters and domain geometries. We adopt a geometry encoderto capture the domain geometry features, and design a novel pipeline tointegrate this component within the existing DCON architecture. Numericalresults demonstrate the accuracy and efficiency of the proposed method. All thecodes and data related to this work are available on GitHub:https://github.com/WeihengZ/Physics-informed-Neural-Foundation-Operator.</description><author>Weiheng Zhong, Hadi Meidani</author><pubDate>Wed, 13 Nov 2024 17:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01600v3</guid></item><item><title>FinRobot: AI Agent for Equity Research and Valuation with Large Language Models</title><link>http://arxiv.org/abs/2411.08804v1</link><description>As financial markets grow increasingly complex, there is a rising need forautomated tools that can effectively assist human analysts in equity research,particularly within sell-side research. While Generative AI (GenAI) hasattracted significant attention in this field, existing AI solutions often fallshort due to their narrow focus on technical factors and limited capacity fordiscretionary judgment. These limitations hinder their ability to adapt to newdata in real-time and accurately assess risks, which diminishes their practicalvalue for investors. This paper presents FinRobot, the first AI agent framework specificallydesigned for equity research. FinRobot employs a multi-agent Chain of Thought(CoT) system, integrating both quantitative and qualitative analyses to emulatethe comprehensive reasoning of a human analyst. The system is structured aroundthree specialized agents: the Data-CoT Agent, which aggregates diverse datasources for robust financial integration; the Concept-CoT Agent, which mimicsan analysts reasoning to generate actionable insights; and the Thesis-CoTAgent, which synthesizes these insights into a coherent investment thesis andreport. FinRobot provides thorough company analysis supported by precisenumerical data, industry-appropriate valuation metrics, and realistic riskassessments. Its dynamically updatable data pipeline ensures that researchremains timely and relevant, adapting seamlessly to new financial information.Unlike existing automated research tools, such as CapitalCube and WrightReports, FinRobot delivers insights comparable to those produced by majorbrokerage firms and fundamental research vendors. We open-source FinRobot at\url{https://github. com/AI4Finance-Foundation/FinRobot}.</description><author>Tianyu Zhou, Pinqiao Wang, Yilin Wu, Hongyang Yang</author><pubDate>Wed, 13 Nov 2024 17:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08804v1</guid></item><item><title>OML: Open, Monetizable, and Loyal AI</title><link>http://arxiv.org/abs/2411.03887v2</link><description>Artificial Intelligence (AI) has steadily improved across a wide range oftasks. However, the development and deployment of AI are almost entirelycontrolled by a few powerful organizations that are racing to create ArtificialGeneral Intelligence (AGI). The centralized entities make decisions with littlepublic oversight, shaping the future of humanity, often with unforeseenconsequences. In this paper, we propose OML, which stands for Open,Monetizable, and Loyal AI, an approach designed to democratize AI development.OML is realized through an interdisciplinary framework spanning AI, blockchain,and cryptography. We present several ideas for constructing OML usingtechnologies such as Trusted Execution Environments (TEE), traditionalcryptographic primitives like fully homomorphic encryption and functionalencryption, obfuscation, and AI-native solutions rooted in the samplecomplexity and intrinsic hardness of AI tasks. A key innovation of our work isintroducing a new scientific field: AI-native cryptography. Unlike conventionalcryptography, which focuses on discrete data and binary security guarantees,AI-native cryptography exploits the continuous nature of AI datarepresentations and their low-dimensional manifolds, focusing on improvingapproximate performance. One core idea is to transform AI attack methods, suchas data poisoning, into security tools. This novel approach serves as afoundation for OML 1.0 which uses model fingerprinting to protect the integrityand ownership of AI models. The spirit of OML is to establish a decentralized,open, and transparent platform for AI development, enabling the community tocontribute, monetize, and take ownership of AI models. By decentralizingcontrol and ensuring transparency through blockchain technology, OML preventsthe concentration of power and provides accountability in AI development thathas not been possible before.</description><author>Zerui Cheng, Edoardo Contente, Ben Finch, Oleg Golev, Jonathan Hayase, Andrew Miller, Niusha Moshrefi, Anshul Nasery, Sandeep Nailwal, Sewoong Oh, Himanshu Tyagi, Pramod Viswanath</author><pubDate>Wed, 13 Nov 2024 17:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03887v2</guid></item><item><title>GaussianObject: High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting</title><link>http://arxiv.org/abs/2402.10259v4</link><description>Reconstructing and rendering 3D objects from highly sparse views is ofcritical importance for promoting applications of 3D vision techniques andimproving user experience. However, images from sparse views only contain verylimited 3D information, leading to two significant challenges: 1) Difficulty inbuilding multi-view consistency as images for matching are too few; 2)Partially omitted or highly compressed object information as view coverage isinsufficient. To tackle these challenges, we propose GaussianObject, aframework to represent and render the 3D object with Gaussian splatting thatachieves high rendering quality with only 4 input images. We first introducetechniques of visual hull and floater elimination, which explicitly injectstructure priors into the initial optimization process to help build multi-viewconsistency, yielding a coarse 3D Gaussian representation. Then we construct aGaussian repair model based on diffusion models to supplement the omittedobject information, where Gaussians are further refined. We design aself-generating strategy to obtain image pairs for training the repair model.We further design a COLMAP-free variant, where pre-given accurate camera posesare not required, which achieves competitive quality and facilitates widerapplications. GaussianObject is evaluated on several challenging datasets,including MipNeRF360, OmniObject3D, OpenIllumination, and our-collected unposedimages, achieving superior performance from only four views and significantlyoutperforming previous SOTA methods. Our demo is available athttps://gaussianobject.github.io/, and the code has been released athttps://github.com/GaussianObject/GaussianObject.</description><author>Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</author><pubDate>Wed, 13 Nov 2024 17:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10259v4</guid></item><item><title>Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A Comparative Analysis</title><link>http://arxiv.org/abs/2410.16527v2</link><description>This report presents a comparative analysis of open-source vulnerabilityscanners for conversational large language models (LLMs). As LLMs becomeintegral to various applications, they also present potential attack surfaces,exposed to security risks such as information leakage and jailbreak attacks.Our study evaluates prominent scanners - Garak, Giskard, PyRIT, andCyberSecEval - that adapt red-teaming practices to expose thesevulnerabilities. We detail the distinctive features and practical use of thesescanners, outline unifying principles of their design and perform quantitativeevaluations to compare them. These evaluations uncover significant reliabilityissues in detecting successful attacks, highlighting a fundamental gap forfuture development. Additionally, we contribute a preliminary labelled dataset,which serves as an initial step to bridge this gap. Based on the above, weprovide strategic recommendations to assist organizations choose the mostsuitable scanner for their red-teaming needs, accounting for customizability,test suite comprehensiveness, and industry-specific use cases.</description><author>Jonathan Brokman, Omer Hofman, Oren Rachmil, Inderjeet Singh, Rathina Sabapathy Aishvariya Priya, Vikas Pahuja, Amit Giloni, Roman Vainshtein, Hisashi Kojima</author><pubDate>Wed, 13 Nov 2024 17:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16527v2</guid></item><item><title>$π_0$: A Vision-Language-Action Flow Model for General Robot Control</title><link>http://arxiv.org/abs/2410.24164v3</link><description>Robot learning holds tremendous promise to unlock the full potential offlexible, general, and dexterous robot systems, as well as to address some ofthe deepest questions in artificial intelligence. However, bringing robotlearning to the level of generality required for effective real-world systemsfaces major obstacles in terms of data, generalization, and robustness. In thispaper, we discuss how generalist robot policies (i.e., robot foundation models)can address these challenges, and how we can design effective generalist robotpolicies for complex and highly dexterous tasks. We propose a novel flowmatching architecture built on top of a pre-trained vision-language model (VLM)to inherit Internet-scale semantic knowledge. We then discuss how this modelcan be trained on a large and diverse dataset from multiple dexterous robotplatforms, including single-arm robots, dual-arm robots, and mobilemanipulators. We evaluate our model in terms of its ability to perform tasks inzero shot after pre-training, follow language instructions from people and froma high-level VLM policy, and its ability to acquire new skills via fine-tuning.Our results cover a wide variety of tasks, such as laundry folding, tablecleaning, and assembling boxes.</description><author>Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky</author><pubDate>Wed, 13 Nov 2024 17:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24164v3</guid></item><item><title>Harnessing Smartphone Sensors for Enhanced Road Safety: A Comprehensive Dataset and Review</title><link>http://arxiv.org/abs/2411.07315v2</link><description>Severe collisions can result from aggressive driving and poor roadconditions, emphasizing the need for effective monitoring to ensure safety.Smartphones, with their array of built-in sensors, offer a practical andaffordable solution for road-sensing. However, the lack of reliable,standardized datasets has hindered progress in assessing road conditions anddriving patterns. This study addresses this gap by introducing a comprehensivedataset derived from smartphone sensors, which surpasses existing datasets byincorporating a diverse range of sensors including accelerometer, gyroscope,magnetometer, GPS, gravity, orientation, and uncalibrated sensors. Thesesensors capture extensive parameters such as acceleration force, gravitation,rotation rate, magnetic field strength, and vehicle speed, providing a detailedunderstanding of road conditions and driving behaviors. The dataset is designedto enhance road safety, infrastructure maintenance, traffic management, andurban planning. By making this dataset available to the community, the studyaims to foster collaboration, inspire further research, and facilitate thedevelopment of innovative solutions in intelligent transportation systems.</description><author>Amith Khandakar, David G. Michelson, Mansura Naznine, Abdus Salam, Md. Nahiduzzaman, Khaled M. Khan, Ponnuthurai Nagaratnam Suganthan, Mohamed Arselene Ayari, Hamid Menouar, Julfikar Haider</author><pubDate>Wed, 13 Nov 2024 17:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07315v2</guid></item><item><title>Deep Learning Accelerated Quantum Transport Simulations in Nanoelectronics: From Break Junctions to Field-Effect Transistors</title><link>http://arxiv.org/abs/2411.08800v1</link><description>Quantum transport calculations are essential for understanding and designingnanoelectronic devices, yet the trade-off between accuracy and computationalefficiency has long limited their practical applications. We present a generalframework that combines the deep learning tight-binding Hamiltonian (DeePTB)approach with the non-equilibrium Green's Function (NEGF) method, enablingefficient quantum transport calculations while maintaining first-principlesaccuracy. We demonstrate the capabilities of the DeePTB-NEGF framework throughtwo representative applications: comprehensive simulation of break junctionsystems, where conductance histograms show good agreement with experimentalmeasurements in both metallic contact and single-molecule junction cases; andsimulation of carbon nanotube field effect transistors through self-consistentNEGF-Poisson calculations, capturing essential physics including theelectrostatic potential and transfer characteristic curves under finite biasconditions. This framework bridges the gap between first-principles accuracyand computational efficiency, providing a powerful tool for high-throughputquantum transport simulations across different scales in nanoelectronics.</description><author>Jijie Zou, Zhanghao Zhouyin, Dongying Lin, Linfeng Zhang, Shimin Hou, Qiangqiang Gu</author><pubDate>Wed, 13 Nov 2024 17:27:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08800v1</guid></item><item><title>Physics-informed Discretization-independent Deep Compositional Operator Network</title><link>http://arxiv.org/abs/2404.13646v4</link><description>Solving parametric Partial Differential Equations (PDEs) for a broad range ofparameters is a critical challenge in scientific computing. To this end, neuraloperators, which \textcolor{black}{predicts the PDE solution with variable PDEparameter inputs}, have been successfully used. However, the training of neuraloperators typically demands large training datasets, the acquisition of whichcan be prohibitively expensive. To address this challenge, physics-informedtraining can offer a cost-effective strategy. However, current physics-informedneural operators face limitations, either in handling irregular domain shapesor in in generalizing to various discrete representations of PDE parameters. Inthis research, we introduce a novel physics-informed model architecture whichcan generalize to various discrete representations of PDE parameters andirregular domain shapes. Particularly, inspired by deep operator neuralnetworks, our model involves a discretization-independent learning of parameterembedding repeatedly, and this parameter embedding is integrated with theresponse embeddings through multiple compositional layers, for moreexpressivity. Numerical results demonstrate the accuracy and efficiency of theproposed method. All the codes and data related to this work are available onGitHub: https://github.com/WeihengZ/PI-DCON.</description><author>Weiheng Zhong, Hadi Meidani</author><pubDate>Wed, 13 Nov 2024 17:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13646v4</guid></item><item><title>Learning Gaussian Multi-Index Models with Gradient Flow: Time Complexity and Directional Convergence</title><link>http://arxiv.org/abs/2411.08798v1</link><description>This work focuses on the gradient flow dynamics of a neural network modelthat uses correlation loss to approximate a multi-index function onhigh-dimensional standard Gaussian data. Specifically, the multi-index functionwe consider is a sum of neurons $f^*(x) \!=\! \sum_{j=1}^k \! \sigma^*(v_j^Tx)$ where $v_1, \dots, v_k$ are unit vectors, and $\sigma^*$ lacks the firstand second Hermite polynomials in its Hermite expansion. It is known that, forthe single-index case ($k\!=\!1$), overcoming the search phase requirespolynomial time complexity. We first generalize this result to multi-indexfunctions characterized by vectors in arbitrary directions. After the searchphase, it is not clear whether the network neurons converge to the indexvectors, or get stuck at a sub-optimal solution. When the index vectors areorthogonal, we give a complete characterization of the fixed points and provethat neurons converge to the nearest index vectors. Therefore, using $n \!\asymp \! k \log k$ neurons ensures finding the full set of index vectors withgradient flow with high probability over random initialization. When $ v_i^Tv_j \!=\! \beta \! \geq \! 0$ for all $i \neq j$, we prove the existence of asharp threshold $\beta_c \!=\! c/(c+k)$ at which the fixed point that computesthe average of the index vectors transitions from a saddle point to a minimum.Numerical simulations show that using a correlation loss and a mildoverparameterization suffices to learn all of the index vectors when they arenearly orthogonal, however, the correlation loss fails when the dot productbetween the index vectors exceeds a certain threshold.</description><author>Berfin Simsek, Amire Bendjeddou, Daniel Hsu</author><pubDate>Wed, 13 Nov 2024 17:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08798v1</guid></item><item><title>Evaluating World Models with LLM for Decision Making</title><link>http://arxiv.org/abs/2411.08794v1</link><description>World model emerges as a key module in decision making, where MuZero andDreamer achieve remarkable successes in complex tasks. Recent work leveragesLarge Language Models (LLMs) as general world simulators to simulate thedynamics of the world due to their generalizability. LLMs also serve as theworld model for deliberative reasoning in Reasoning via Planning (RAP) and Treeof Thought (ToT). However, the world models are either evaluated as a generalworld simulator, or as a functional module of the agent, i.e., predicting thetransitions to assist the planning. In this work, we propose a comprehensiveevaluation of the world models with LLMs from the decision making perspective.Specifically, we leverage the 31 diverse environments from (Wang et al.,2023;2024) and curate the rule-based policy of each environment for the diverseevaluation. Then, we design three main tasks, i.e., policy verification, actionproposal, and policy planning, where the world models can be used for decisionmaking solely. Finally, we conduct the comprehensive evaluation of the advancedLLMs, i.e., GPT-4o and GPT-4o-mini, on the environments for the three maintasks under various settings. The key observations include: i) GPT-4osignificantly outperforms GPT-4o-mini on the three main tasks, especially forthe tasks which require the domain knowledge, ii) the performance of the worldmodel with LLM will be decreased for long-term decision-making tasks, and iii)the combination of different functionalities of the world model will bringsadditional unstabilities of the performance.</description><author>Chang Yang, Xinrun Wang, Junzhe Jiang, Qinggang Zhang, Xiao Huang</author><pubDate>Wed, 13 Nov 2024 17:19:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08794v1</guid></item><item><title>Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective</title><link>http://arxiv.org/abs/2403.18346v4</link><description>Recent advancements in Large Language Models (LLMs) have facilitated thedevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,MLLMs often suffer from over-reliance on unimodal biases (e.g., language biasand vision bias), leading to incorrect answers or hallucinations in complexmultimodal tasks. To investigate this issue, we propose a causal framework tointerpret the biases in Visual Question Answering (VQA) problems. Within thisframework, we conduct an in-depth causal analysis to assess the causal effectof these biases on MLLM predictions. Based on the analysis, we introduce 1) anovel MORE dataset with 12,000 challenging VQA instances requiring multi-hopreasoning and overcoming unimodal biases. 2) a causality-enhanced agentframework CAVE that guides models to comprehensively integrate information fromdifferent modalities and mitigate biases. Our experiments show that MLLMsperform poorly on MORE, indicating strong unimodal biases and limited semanticunderstanding. However, when integrated with our CAVE, promising improvementsin reasoning and bias mitigation can be seen. These findings provide importantinsights for the development of more robust MLLMs and contribute to the broadergoal of advancing multimodal AI systems capable of deeper understanding andreasoning. Our project page is at https://github.com/OpenCausaLab/MORE.</description><author>Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu</author><pubDate>Wed, 13 Nov 2024 17:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18346v4</guid></item><item><title>Locally Private Sampling with Public Data</title><link>http://arxiv.org/abs/2411.08791v1</link><description>Local differential privacy (LDP) is increasingly employed inprivacy-preserving machine learning to protect user data before sharing it withan untrusted aggregator. Most LDP methods assume that users possess only asingle data record, which is a significant limitation since users often gatherextensive datasets (e.g., images, text, time-series data) and frequently haveaccess to public datasets. To address this limitation, we propose a locallyprivate sampling framework that leverages both the private and public datasetsof each user. Specifically, we assume each user has two distributions: $p$ and$q$ that represent their private dataset and the public dataset, respectively.The objective is to design a mechanism that generates a private sampleapproximating $p$ while simultaneously preserving $q$. We frame this objectiveas a minimax optimization problem using $f$-divergence as the utility measure.We fully characterize the minimax optimal mechanisms for general$f$-divergences provided that $p$ and $q$ are discrete distributions.Remarkably, we demonstrate that this optimal mechanism is universal across all$f$-divergences. Experiments validate the effectiveness of our minimax optimalsampler compared to the state-of-the-art locally private sampler.</description><author>Behnoosh Zamanlooy, Mario Diaz, Shahab Asoodeh</author><pubDate>Wed, 13 Nov 2024 17:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08791v1</guid></item><item><title>Can sparse autoencoders be used to decompose and interpret steering vectors?</title><link>http://arxiv.org/abs/2411.08790v1</link><description>Steering vectors are a promising approach to control the behaviour of largelanguage models. However, their underlying mechanisms remain poorly understood.While sparse autoencoders (SAEs) may offer a potential method to interpretsteering vectors, recent findings show that SAE-reconstructed vectors oftenlack the steering properties of the original vectors. This paper investigateswhy directly applying SAEs to steering vectors yields misleadingdecompositions, identifying two reasons: (1) steering vectors fall outside theinput distribution for which SAEs are designed, and (2) steering vectors canhave meaningful negative projections in feature directions, which SAEs are notdesigned to accommodate. These limitations hinder the direct use of SAEs forinterpreting steering vectors.</description><author>Harry Mayne, Yushi Yang, Adam Mahdi</author><pubDate>Wed, 13 Nov 2024 17:16:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08790v1</guid></item><item><title>AI Consciousness is Inevitable: A Theoretical Computer Science Perspective</title><link>http://arxiv.org/abs/2403.17101v8</link><description>We look at consciousness through the lens of Theoretical Computer Science, abranch of mathematics that studies computation under resource limitations. Fromthis perspective, we develop a formal machine model for consciousness. Themodel is inspired by Alan Turing's simple yet powerful model of computation andBernard Baars' theater model of consciousness. Though extremely simple, themodel aligns at a high level with many of the major scientific theories ofhuman and animal consciousness, supporting our claim that machine consciousnessis inevitable.</description><author>Lenore Blum, Manuel Blum</author><pubDate>Wed, 13 Nov 2024 17:14:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17101v8</guid></item><item><title>Zero-shot Cross-lingual Transfer Learning with Multiple Source and Target Languages for Information Extraction: Language Selection and Adversarial Training</title><link>http://arxiv.org/abs/2411.08785v1</link><description>The majority of previous researches addressing multi-lingual IE are limitedto zero-shot cross-lingual single-transfer (one-to-one) setting, withhigh-resource languages predominantly as source training data. As a result,these works provide little understanding and benefit for the realistic goal ofdeveloping a multi-lingual IE system that can generalize to as many languagesas possible. Our study aims to fill this gap by providing a detailed analysison Cross-Lingual Multi-Transferability (many-to-many transfer learning), forthe recent IE corpora that cover a diverse set of languages. Specifically, wefirst determine the correlation between single-transfer performance and a widerange of linguistic-based distances. From the obtained insights, a combinedlanguage distance metric can be developed that is not only highly correlatedbut also robust across different tasks and model scales. Next, we investigatethe more general zero-shot multi-lingual transfer settings where multiplelanguages are involved in the training and evaluation processes. Languageclustering based on the newly defined distance can provide directions forachieving the optimal cost-performance trade-off in data (languages) selectionproblem. Finally, a relational-transfer setting is proposed to furtherincorporate multi-lingual unlabeled data based on adversarial training usingthe relation induced from the above linguistic distance.</description><author>Nghia Trung Ngo, Thien Huu Nguyen</author><pubDate>Wed, 13 Nov 2024 17:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08785v1</guid></item><item><title>A Universal Deep Learning Framework for Materials X-ray Absorption Spectra</title><link>http://arxiv.org/abs/2409.19552v2</link><description>X-ray absorption spectroscopy (XAS) is a powerful characterization techniquefor probing the local chemical environment of absorbing atoms. However,analyzing XAS data presents significant challenges, often requiring extensive,computationally intensive simulations, as well as significant domain expertise.These limitations hinder the development of fast, robust XAS analysis pipelinesthat are essential in high-throughput studies and for autonomousexperimentation. We address these challenges with OmniXAS, a framework thatcontains a suite of transfer learning approaches for XAS prediction, eachcontributing to improved accuracy and efficiency, as demonstrated on K-edgespectra database covering eight 3d transition metals (Ti-Cu). The OmniXASframework is built upon three distinct strategies. First, we use M3GNet toderive latent representations of the local chemical environment of absorptionsites as input for XAS prediction, achieving up to order-of-magnitudeimprovements over conventional featurization techniques. Second, we employ ahierarchical transfer learning strategy, training a universal multi-task modelacross elements before fine-tuning for element-specific predictions. Modelsbased on this cascaded approach after element-wise fine-tuning outperformelement-specific models by up to 69%. Third, we implement cross-fidelitytransfer learning, adapting a universal model to predict spectra generated bysimulation of a different fidelity with a higher computational cost. Thisapproach improves prediction accuracy by up to 11% over models trained on thetarget fidelity alone. Our approach boosts the throughput of XAS modeling byorders of magnitude versus first-principles simulations and is extendable toXAS prediction for a broader range of elements. This transfer learningframework is generalizable to enhance deep-learning models that target otherproperties in materials research.</description><author>Shubha R. Kharel, Fanchen Meng, Xiaohui Qu, Matthew R. Carbone, Deyu Lu</author><pubDate>Wed, 13 Nov 2024 17:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19552v2</guid></item><item><title>Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models</title><link>http://arxiv.org/abs/2402.03271v3</link><description>In the face of uncertainty, the ability to *seek information* is offundamental importance. In many practical applications, such as medicaldiagnosis and troubleshooting, the information needed to solve the task is notinitially given and has to be actively sought by asking follow-up questions(for example, a doctor asking a patient for more details about their symptoms).In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm toaugment large language models with the ability to actively seek information byasking effective questions. UoT combines 1) an *uncertainty-aware simulationapproach* which enables the model to simulate possible future scenarios and howlikely they are to occur, 2) *uncertainty-based rewards* motivated byinformation gain which incentivizes the model to seek information, and 3) a*reward propagation scheme* to select the optimal question to ask in a way thatmaximizes the expected reward. In experiments on medical diagnosis,troubleshooting, and the `20 Questions` game, UoT achieves an averageperformance improvement of 38.1% in the rate of successful task completionacross multiple LLMs compared with direct prompting and also improvesefficiency (i.e., the number of questions needed to complete the task). Ourcode has been released [here](https://github.com/zhiyuanhubj/UoT)</description><author>Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi</author><pubDate>Wed, 13 Nov 2024 17:10:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03271v3</guid></item><item><title>Active Inference Meeting Energy-Efficient Control of Parallel and Identical Machines</title><link>http://arxiv.org/abs/2406.09322v2</link><description>We investigate the application of active inference in developingenergy-efficient control agents for manufacturing systems. Active inference,rooted in neuroscience, provides a unified probabilistic framework integratingperception, learning, and action, with inherent uncertainty quantificationelements. Our study explores deep active inference, an emerging field thatcombines deep learning with the active inference decision-making framework.Leveraging a deep active inference agent, we focus on controlling parallel andidentical machine workstations to enhance energy efficiency. We addresschallenges posed by the problem's stochastic nature and delayed policy responseby introducing tailored enhancements to existing agent architectures.Specifically, we introduce multi-step transition and hybrid horizon methods tomitigate the need for complex planning. Our experimental results demonstratethe effectiveness of these enhancements and highlight the potential of theactive inference-based approach.</description><author>Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta</author><pubDate>Wed, 13 Nov 2024 17:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09322v2</guid></item><item><title>Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity</title><link>http://arxiv.org/abs/2407.09733v3</link><description>In this paper, we introduce Textured-GS, an innovative method for renderingGaussian splatting that incorporates spatially defined color and opacityvariations using Spherical Harmonics (SH). This approach enables each Gaussianto exhibit a richer representation by accommodating varying colors andopacities across its surface, significantly enhancing rendering qualitycompared to traditional methods. To demonstrate the merits of our approach, wehave adapted the Mini-Splatting architecture to integrate textured Gaussianswithout increasing the number of Gaussians. Our experiments across multiplereal-world datasets show that Textured-GS consistently outperforms both thebaseline Mini-Splatting and standard 3DGS in terms of visual fidelity. Theresults highlight the potential of Textured-GS to advance Gaussian-basedrendering technologies, promising more efficient and high-quality scenereconstructions. Our implementation is available athttps://github.com/ZhentaoHuang/Textured-GS.</description><author>Zhentao Huang, Minglun Gong</author><pubDate>Wed, 13 Nov 2024 17:07:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09733v3</guid></item><item><title>China and the U.S. produce more impactful AI research when collaborating together</title><link>http://arxiv.org/abs/2304.11123v2</link><description>Artificial Intelligence (AI) has become a disruptive technology, promising togrant a significant economic and strategic advantage to nations that harnessits power. China, with its recent push towards AI adoption, is challenging theU.S.'s position as the global leader in this field. Given AI's massivepotential, as well as the fierce geopolitical tensions between China and theU.S., several recent policies have been put in place to discourage AIscientists from migrating to, or collaborating with, the other nation.Nevertheless, the extent of talent migration and cross-border collaboration arenot fully understood. Here, we analyze a dataset of over 350,000 AI scientistsand 5,000,000 AI papers. We find that since 2000, China and the U.S. have ledthe field in terms of impact, novelty, productivity, and workforce. Most AIscientists who move to China come from the U.S., and most who move to the U.S.come from China, highlighting a notable bidirectional talent migration.Moreover, the vast majority of those moving in either direction have Asianancestry. Upon moving, those scientists continue to collaborate frequently withthose in the origin country. Although the number of collaborations between thetwo countries has increased since the dawn of the millennium, suchcollaborations continue to be relatively rare. A matching experiment revealsthat the two countries have always been more impactful when collaborating thanwhen each works without the other. These findings suggest that instead ofsuppressing cross-border migration and collaboration between the two nations,the science could benefit from promoting such activities.</description><author>Bedoor AlShebli, Shahan Ali Memon, James A. Evans, Talal Rahwan</author><pubDate>Wed, 13 Nov 2024 17:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11123v2</guid></item><item><title>LUDO: Low-Latency Understanding of Highly Deformable Objects using Point Cloud Occupancy Functions</title><link>http://arxiv.org/abs/2411.08777v1</link><description>Accurately determining the shape and location of internal structures withindeformable objects is crucial for medical tasks that require precise targeting,such as robotic biopsies. We introduce LUDO, a method for accurate low-latencyunderstanding of deformable objects. LUDO reconstructs objects in theirdeformed state, including their internal structures, from a single-view pointcloud observation in under 30 ms using occupancy networks. We demonstrateLUDO's abilities for autonomous targeting of internal regions of interest(ROIs) in highly deformable objects. Additionally, LUDO provides uncertaintyestimates and explainability for its predictions, both of which are importantin safety-critical applications such as surgical interventions. We evaluateLUDO in real-world robotic experiments, achieving a success rate of 98.9% forpuncturing various ROIs inside highly deformable objects. LUDO demonstrates thepotential to interact with deformable objects without the need for deformableregistration methods.</description><author>Pit Henrich, Franziska Mathis-Ullrich, Paul Maria Scheikl</author><pubDate>Wed, 13 Nov 2024 17:02:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08777v1</guid></item><item><title>Optimal Oblivious Subspace Embeddings with Near-optimal Sparsity</title><link>http://arxiv.org/abs/2411.08773v1</link><description>An oblivious subspace embedding is a random $m\times n$ matrix $\Pi$ suchthat, for any $d$-dimensional subspace, with high probability $\Pi$ preservesthe norms of all vectors in that subspace within a $1\pm\epsilon$ factor. Inthis work, we give an oblivious subspace embedding with the optimal dimension$m=\Theta(d/\epsilon^2)$ that has a near-optimal sparsity of $\tildeO(1/\epsilon)$ non-zero entries per column of $\Pi$. This is the first resultto nearly match the conjecture of Nelson and Nguyen [FOCS 2013] in terms of thebest sparsity attainable by an optimal oblivious subspace embedding, improvingon a prior bound of $\tilde O(1/\epsilon^6)$ non-zeros per column [Chenakkod etal., STOC 2024]. We further extend our approach to the non-oblivious setting,proposing a new family of Leverage Score Sparsified embeddings with IndependentColumns, which yield faster runtimes for matrix approximation and regressiontasks. In our analysis, we develop a new method which uses a decoupling argumenttogether with the cumulant method for bounding the edge universality error ofisotropic random matrices. To achieve near-optimal sparsity, we combine thisgeneral-purpose approach with new traces inequalities that leverage thespecific structure of our subspace embedding construction.</description><author>Shabarish Chenakkod, Michał Dereziński, Xiaoyu Dong</author><pubDate>Wed, 13 Nov 2024 16:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08773v1</guid></item><item><title>Sharingan: Extract User Action Sequence from Desktop Recordings</title><link>http://arxiv.org/abs/2411.08768v1</link><description>Video recordings of user activities, particularly desktop recordings, offer arich source of data for understanding user behaviors and automating processes.However, despite advancements in Vision-Language Models (VLMs) and theirincreasing use in video analysis, extracting user actions from desktoprecordings remains an underexplored area. This paper addresses this gap byproposing two novel VLM-based methods for user action extraction: the DirectFrame-Based Approach (DF), which inputs sampled frames directly into VLMs, andthe Differential Frame-Based Approach (DiffF), which incorporates explicitframe differences detected via computer vision techniques. We evaluate thesemethods using a basic self-curated dataset and an advanced benchmark adaptedfrom prior work. Our results show that the DF approach achieves an accuracy of70% to 80% in identifying user actions, with the extracted action sequencesbeing re-playable though Robotic Process Automation. We find that while VLMsshow potential, incorporating explicit UI changes can degrade performance,making the DF approach more reliable. This work represents the firstapplication of VLMs for extracting user action sequences from desktoprecordings, contributing new methods, benchmarks, and insights for futureresearch.</description><author>Yanting Chen, Yi Ren, Xiaoting Qin, Jue Zhang, Kehong Yuan, Lu Han, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</author><pubDate>Wed, 13 Nov 2024 16:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08768v1</guid></item><item><title>SANDWICH: Towards an Offline, Differentiable, Fully-Trainable Wireless Neural Ray-Tracing Surrogate</title><link>http://arxiv.org/abs/2411.08767v1</link><description>Wireless ray-tracing (RT) is emerging as a key tool for three-dimensional(3D) wireless channel modeling, driven by advances in graphical rendering.Current approaches struggle to accurately model beyond 5G (B5G) networksignaling, which often operates at higher frequencies and is more susceptibleto environmental conditions and changes. Existing online learning solutionsrequire real-time environmental supervision during training, which is bothcostly and incompatible with GPU-based processing. In response, we propose anovel approach that redefines ray trajectory generation as a sequentialdecision-making problem, leveraging generative models to jointly learn theoptical, physical, and signal properties within each designated environment.Our work introduces the Scene-Aware Neural Decision Wireless Channel RaytracingHierarchy (SANDWICH), an innovative offline, fully differentiable approach thatcan be trained entirely on GPUs. SANDWICH offers superior performance comparedto existing online learning methods, outperforms the baseline by 4e^-2 radianin RT accuracy, and only fades 0.5 dB away from toplined channel gainestimation.</description><author>Yifei Jin, Ali Maatouk, Sarunas Girdzijauskas, Shugong Xu, Leandros Tassiulas, Rex Ying</author><pubDate>Wed, 13 Nov 2024 16:53:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08767v1</guid></item><item><title>Mapping Methane -- The Impact of Dairy Farm Practices on Emissions Through Satellite Data and Machine Learning</title><link>http://arxiv.org/abs/2411.08766v1</link><description>This study investigates the correlation between dairy farm characteristicsand methane concentrations as derived from satellite observations in EasternCanada. Utilizing data from 11 dairy farms collected between January 2020 andDecember 2022, we integrated Sentinel-5P satellite methane data with criticalfarm-level attributes, including herd genetics, feeding practices, andmanagement strategies. Initial analyses revealed significant correlations withmethane concentrations, leading to the application of Variance Inflation Factor(VIF) and Principal Component Analysis (PCA) to address multicollinearity andenhance model stability. Subsequently, machine learning models - specificallyRandom Forest and Neural Networks - were employed to evaluate featureimportance and predict methane emissions. Our findings indicate a strongnegative correlation between the Estimated Breeding Value (EBV) for proteinpercentage and methane concentrations, suggesting that genetic selection forhigher milk protein content could be an effective strategy for emissionsreduction. The integration of atmospheric transport models with satellite datafurther refined our emission estimates, significantly enhancing accuracy andspatial resolution. This research underscores the potential of advancedsatellite monitoring, machine learning techniques, and atmospheric modeling inimproving methane emission assessments within the dairy sector. It emphasizesthe critical role of farm-specific characteristics in developing effectivemitigation strategies. Future investigations should focus on expanding thedataset and incorporating inversion modeling for more precise emissionquantification. Balancing ecological impacts with economic viability will beessential for fostering sustainable dairy farming practices.</description><author>Hanqing Bi, Suresh Neethirajan</author><pubDate>Wed, 13 Nov 2024 16:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08766v1</guid></item><item><title>Flow reconstruction in time-varying geometries using graph neural networks</title><link>http://arxiv.org/abs/2411.08764v1</link><description>The paper presents a Graph Attention Convolutional Network (GACN) for flowreconstruction from very sparse data in time-varying geometries. The modelincorporates a feature propagation algorithm as a preprocessing step to handleextremely sparse inputs, leveraging information from neighboring nodes toinitialize missing features. In addition, a binary indicator is introduced as avalidity mask to distinguish between the original and propagated data points,enabling more effective learning from sparse inputs. Trained on a unique dataset of Direct Numerical Simulations (DNS) of a motored engine at a technicallyrelevant operating condition, the GACN shows robust performance acrossdifferent resolutions and domain sizes and can effectively handle unstructureddata and variable input sizes. The model is tested on previously unseen DNSdata as well as on an experimental data set from Particle Image Velocimetry(PIV) measurements that were not considered during training. A comparativeanalysis shows that the GACN consistently outperforms both a conventionalConvolutional Neural Network (CNN) and cubic interpolation methods on the DNSand PIV test sets by achieving lower reconstruction errors and better capturingfine-scale turbulent structures. In particular, the GACN effectivelyreconstructs flow fields from domains up to 14 times larger than those observedduring training, with the performance advantage increasing for larger domains.</description><author>Bogdan A. Danciu, Vito A. Pagone, Benjamin Böhm, Marius Schmidt, Christos E. Frouzakis</author><pubDate>Wed, 13 Nov 2024 16:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08764v1</guid></item><item><title>HiFi-Syn: Hierarchical Granularity Discrimination for High-Fidelity Synthesis of MR Images with Structure Preservation</title><link>http://arxiv.org/abs/2311.12461v2</link><description>Synthesizing medical images while preserving their structural information iscrucial in medical research. In such scenarios, the preservation of anatomicalcontent becomes especially important. Although recent advances have been madeby incorporating instance-level information to guide translation, these methodsoverlook the spatial coherence of structural-level representation and theanatomical invariance of content during translation. To address these issues,we introduce hierarchical granularity discrimination, which exploits variouslevels of semantic information present in medical images. Our strategy utilizesthree levels of discrimination granularity: pixel-level discrimination using aBrain Memory Bank, structure-level discrimination on each brain structure witha re-weighting strategy to focus on hard samples, and global-leveldiscrimination to ensure anatomical consistency during translation. The imagetranslation performance of our strategy has been evaluated on three independentdatasets (UK Biobank, IXI, and BraTS 2018), and it has outperformedstate-of-the-art algorithms. Particularly, our model excels not only insynthesizing normal structures but also in handling abnormal (pathological)structures, such as brain tumors, despite the variations in contrast observedacross different imaging modalities due to their pathological characteristics.The diagnostic value of synthesized MR images containing brain tumors has beenevaluated by radiologists. This indicates that our model may offer analternative solution in scenarios where specific MR modalities of patients areunavailable. Extensive experiments further demonstrate the versatility of ourmethod, providing unique insights into medical image translation.</description><author>Ziqi Yu, Botao Zhao, Shengjie Zhang, Xiang Chen, Jianfeng Feng, Tingying Peng, Xiao-Yong Zhang</author><pubDate>Wed, 13 Nov 2024 16:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12461v2</guid></item><item><title>Energy Dissipation Preserving Physics Informed Neural Network for Allen-Cahn Equations</title><link>http://arxiv.org/abs/2411.08760v1</link><description>This paper investigates a numerical solution of Allen-Cahn equation withconstant and degenerate mobility, with polynomial and logarithmic energyfunctionals, with deterministic and random initial functions, and withadvective term in one, two, and three spatial dimensions, based on thephysics-informed neural network (PINN). To improve the learning capacity of thePINN, we incorporate the energy dissipation property of the Allen-Cahn equationas a penalty term into the loss function of the network. To facilitate thelearning process of random initials, we employ a continuous analogue of theinitial random condition by utilizing the Fourier series expansion. Adaptivemethods from traditional numerical analysis are also integrated to enhance theeffectiveness of the proposed PINN. Numerical results indicate a consistentdecrease in the discrete energy, while also revealing phenomena such as phaseseparation and metastability.</description><author>Mustafa Kütük, Hamdullah Yücel</author><pubDate>Wed, 13 Nov 2024 16:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08760v1</guid></item><item><title>On Training Survival Models with Scoring Rules</title><link>http://arxiv.org/abs/2403.13150v2</link><description>Scoring rules are an established way of comparing predictive performancesacross model classes. In the context of survival analysis, they requireadaptation in order to accommodate censoring. This work investigates usingscoring rules for model training rather than evaluation. Doing so, we establisha general framework for training survival models that is model agnostic and canlearn event time distributions parametrically or non-parametrically. Inaddition, our framework is not restricted to any specific scoring rule. Whilewe focus on neural network-based implementations, we also provideproof-of-concept implementations using gradient boosting, generalized additivemodels, and trees. Empirical comparisons on synthetic and real-world dataindicate that scoring rules can be successfully incorporated into modeltraining and yield competitive predictive performance with establishedtime-to-event models.</description><author>Philipp Kopper, David Rügamer, Raphael Sonabend, Bernd Bischl, Andreas Bender</author><pubDate>Wed, 13 Nov 2024 16:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13150v2</guid></item><item><title>CGRclust: Chaos Game Representation for Twin Contrastive Clustering of Unlabelled DNA Sequences</title><link>http://arxiv.org/abs/2407.02538v2</link><description>This study proposes CGRclust, a novel combination of unsupervised twincontrastive clustering of Chaos Game Representations (CGR) of DNA sequences,with convolutional neural networks (CNNs). To the best of our knowledge,CGRclust is the first method to use unsupervised learning for imageclassification (herein applied to two-dimensional CGR images) for clusteringdatasets of DNA sequences. CGRclust overcomes the limitations of traditionalsequence classification methods by leveraging unsupervised twin contrastivelearning to detect distinctive sequence patterns, without requiring DNAsequence alignment or biological/taxonomic labels. CGRclust accuratelyclustered twenty-five diverse datasets, with sequence lengths ranging from 664bp to 100 kbp, including mitochondrial genomes of fish, fungi, and protists, aswell as viral whole genome assemblies and synthetic DNA sequences. Comparedwith three recent clustering methods for DNA sequences (DeLUCS, iDeLUCS, andMeShClust v3.0.), CGRclust is the only method that surpasses 81.70% accuracyacross all four taxonomic levels tested for mitochondrial DNA genomes of fish.Moreover, CGRclust also consistently demonstrates superior performance acrossall the viral genomic datasets. The high clustering accuracy of CGRclust onthese twenty-five datasets, which vary significantly in terms of sequencelength, number of genomes, number of clusters, and level of taxonomy,demonstrates its robustness, scalability, and versatility.</description><author>Fatemeh Alipour, Kathleen A. Hill, Lila Kari</author><pubDate>Wed, 13 Nov 2024 16:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02538v2</guid></item><item><title>ScaleNet: Scale Invariance Learning in Directed Graphs</title><link>http://arxiv.org/abs/2411.08758v1</link><description>Graph Neural Networks (GNNs) have advanced relational data analysis but lackinvariance learning techniques common in image classification. In nodeclassification with GNNs, it is actually the ego-graph of the center node thatis classified. This research extends the scale invariance concept to nodeclassification by drawing an analogy to image processing: just as scaleinvariance being used in image classification to capture multi-scale features,we propose the concept of ``scaled ego-graphs''. Scaled ego-graphs generalizetraditional ego-graphs by replacing undirected single-edges with``scaled-edges'', which are ordered sequences of multiple directed edges. Weempirically assess the performance of the proposed scale invariance in graphson seven benchmark datasets, across both homophilic and heterophilicstructures. Our scale-invariance-based graph learning outperforms inceptionmodels derived from random walks by being simpler, faster, and more accurate.The scale invariance explains inception models' success on homophilic graphsand limitations on heterophilic graphs. To ensure applicability of inceptionmodel to heterophilic graphs as well, we further present ScaleNet, anarchitecture that leverages multi-scaled features. ScaleNet achievesstate-of-the-art results on five out of seven datasets (four homophilic and oneheterophilic) and matches top performance on the remaining two, demonstratingits excellent applicability. This represents a significant advance in graphlearning, offering a unified framework that enhances node classification acrossvarious graph types. Our code is available athttps://github.com/Qin87/ScaleNet/tree/July25.</description><author>Qin Jiang, Chengjia Wang, Michael Lones, Wei Pang</author><pubDate>Wed, 13 Nov 2024 16:42:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08758v1</guid></item><item><title>Optimal vintage factor analysis with deflation varimax</title><link>http://arxiv.org/abs/2310.10545v3</link><description>Vintage factor analysis is one important type of factor analysis that aims tofirst find a low-dimensional representation of the original data, and then toseek a rotation such that the rotated low-dimensional representation isscientifically meaningful. The most widely used vintage factor analysis is thePrincipal Component Analysis (PCA) followed by the varimax rotation. Despiteits popularity, little theoretical guarantee can be provided to date mainlybecause varimax rotation requires to solve a non-convex optimization over theset of orthogonal matrices. In this paper, we propose a deflation varimax procedure that solves each rowof an orthogonal matrix sequentially. In addition to its net computational gainand flexibility, we are able to fully establish theoretical guarantees for theproposed procedure in a broader context. Adopting this new deflation varimax asthe second step after PCA, we further analyze this two step procedure under ageneral class of factor models. Our results show that it estimates the factorloading matrix in the minimax optimal rate when the signal-to-noise-ratio (SNR)is moderate or large. In the low SNR regime, we offer possible improvement overusing PCA and the deflation varimax when the additive noise under the factormodel is structured. The modified procedure is shown to be minimax optimal inall SNR regimes. Our theory is valid for finite sample and allows the number ofthe latent factors to grow with the sample size as well as the ambientdimension to grow with, or even exceed, the sample size. Extensive simulationand real data analysis further corroborate our theoretical findings.</description><author>Xin Bing, Dian Jin, Yuqian Zhang</author><pubDate>Wed, 13 Nov 2024 16:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10545v3</guid></item><item><title>On the Effects of Data Scale on UI Control Agents</title><link>http://arxiv.org/abs/2406.03679v6</link><description>Autonomous agents that control computer interfaces to accomplish human tasksare emerging. Leveraging LLMs to power such agents has been of specialinterest, but unless fine-tuned on human-collected task demonstrations,performance is still relatively low. In this work we study whether fine-tuningalone is a viable approach for building real-world computer control agents. Inparticularly, we investigate how performance measured on both high andlow-level tasks in domain and out of domain scales as more training data iscollected. To this end we collect and release a new dataset, AndroidControl,consisting of 15,283 demonstrations of everyday tasks with Android apps.Compared to existing datasets, each AndroidControl task instance includes bothhigh and low-level human-generated instructions, allowing us to explore thelevel of task complexity an agent can handle. Moreover, AndroidControl is themost diverse computer control dataset to date, including 14,548 unique tasksover 833 Android apps, thus allowing us to conduct in-depth analysis of themodel performance in and out of the domain of the training data. Using thedataset, we find that when tested in domain fine-tuned models outperform zeroand few-shot baselines and scale in such a way that robust performance mightfeasibly be obtained simply by collecting more data. Out of domain, performancescales significantly more slowly and suggests that in particular for high-leveltasks, fine-tuning on more data alone may be insufficient for achieving robustout-of-domain performance.</description><author>Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, Oriana Riva</author><pubDate>Wed, 13 Nov 2024 16:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03679v6</guid></item><item><title>AudioProtoPNet: An interpretable deep learning model for bird sound classification</title><link>http://arxiv.org/abs/2404.10420v3</link><description>Deep learning models have significantly advanced acoustic bird monitoring bybeing able to recognize numerous bird species based on their vocalizations.However, traditional deep learning models are black boxes that provide noinsight into their underlying computations, limiting their usefulness toornithologists and machine learning engineers. Explainable models couldfacilitate debugging, knowledge discovery, trust, and interdisciplinarycollaboration. This study introduces AudioProtoPNet, an adaptation of thePrototypical Part Network (ProtoPNet) for multi-label bird soundclassification. It is an inherently interpretable model that uses a ConvNeXtbackbone to extract embeddings, with the classification layer replaced by aprototype learning classifier trained on these embeddings. The classifierlearns prototypical patterns of each bird species' vocalizations fromspectrograms of training instances. During inference, audio recordings areclassified by comparing them to the learned prototypes in the embedding space,providing explanations for the model's decisions and insights into the mostinformative embeddings of each bird species. The model was trained on theBirdSet training dataset, which consists of 9,734 bird species and over 6,800hours of recordings. Its performance was evaluated on the seven test datasetsof BirdSet, covering different geographical regions. AudioProtoPNetoutperformed the state-of-the-art model Perch, achieving an average AUROC of0.90 and a cmAP of 0.42, with relative improvements of 7.1% and 16.7% overPerch, respectively. These results demonstrate that even for the challengingtask of multi-label bird sound classification, it is possible to developpowerful yet inherently interpretable deep learning models that providevaluable insights for ornithologists and machine learning engineers.</description><author>René Heinrich, Lukas Rauch, Bernhard Sick, Christoph Scholz</author><pubDate>Wed, 13 Nov 2024 16:42:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10420v3</guid></item><item><title>Masked Image Modeling Boosting Semi-Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2411.08756v1</link><description>In view of the fact that semi- and self-supervised learning share afundamental principle, effectively modeling knowledge from unlabeled data,various semi-supervised semantic segmentation methods have integratedrepresentative self-supervised learning paradigms for further regularization.However, the potential of the state-of-the-art generative self-supervisedparadigm, masked image modeling, has been scarcely studied. This paradigmlearns the knowledge through establishing connections between the masked andvisible parts of masked image, during the pixel reconstruction process. Byinheriting and extending this insight, we successfully leverage masked imagemodeling to boost semi-supervised semantic segmentation. Specifically, weintroduce a novel class-wise masked image modeling that independentlyreconstructs different image regions according to their respective classes. Inthis way, the mask-induced connections are established within each class,mitigating the semantic confusion that arises from plainly reconstructingimages in basic masked image modeling. To strengthen these intra-classconnections, we further develop a feature aggregation strategy that minimizesthe distances between features corresponding to the masked and visible partswithin the same class. Additionally, in semantic space, we explore theapplication of masked image modeling to enhance regularization. Extensiveexperiments conducted on well-known benchmarks demonstrate that our approachachieves state-of-the-art performance. The code will be available athttps://github.com/haoxt/S4MIM.</description><author>Yangyang Li, Xuanting Hao, Ronghua Shang, Licheng Jiao</author><pubDate>Wed, 13 Nov 2024 16:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08756v1</guid></item><item><title>ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for Scalable Recommendation System</title><link>http://arxiv.org/abs/2402.04032v4</link><description>The personalized recommendation system's continuous size growth poses newchallenges for model inference. Although weight-sharing algorithms have beenproposed to reduce embedding table capacity, they increase memory access.Recent advancements in processing-in-memory (PIM) successfully enhance therecommendation system's throughput by exploiting memory parallelism, but ouranalysis shows that those algorithms introduce CPU-PIM communication overheadinto prior PIM systems, compromising the PIM throughput. We proposeProactivePIM, a specialized memory architecture integrated with PIM technologytailored to accelerate the weight-sharing algorithms. ProacitvePIM integratesan SRAM cache within the PIM with an efficient prefetching scheme to leverage aunique locality of the algorithm and eliminate CPU-PIM communication.</description><author>Youngsuk Kim, Junghwan Lim, Hyuk-Jae Lee, Chae Eun Rhee</author><pubDate>Wed, 13 Nov 2024 16:33:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04032v4</guid></item><item><title>Weakly-Supervised Anomaly Detection in Surveillance Videos Based on Two-Stream I3D Convolution Network</title><link>http://arxiv.org/abs/2411.08755v1</link><description>The widespread implementation of urban surveillance systems has necessitatedmore sophisticated techniques for anomaly detection to ensure enhanced publicsafety. This paper presents a significant advancement in the field of anomalydetection through the application of Two-Stream Inflated 3D (I3D) ConvolutionalNetworks. These networks substantially outperform traditional 3D ConvolutionalNetworks (C3D) by more effectively extracting spatial and temporal featuresfrom surveillance videos, thus improving the precision of anomaly detection.Our research advances the field by implementing a weakly supervised learningframework based on Multiple Instance Learning (MIL), which uniquelyconceptualizes surveillance videos as collections of 'bags' that containinstances (video clips). Each instance is innovatively processed through aranking mechanism that prioritizes clips based on their potential to displayanomalies. This novel strategy not only enhances the accuracy and precision ofanomaly detection but also significantly diminishes the dependency on extensivemanual annotations. Moreover, through meticulous optimization of modelsettings, including the choice of optimizer, our approach not only establishesnew benchmarks in the performance of anomaly detection systems but also offersa scalable and efficient solution for real-world surveillance applications.This paper contributes significantly to the field of computer vision bydelivering a more adaptable, efficient, and context-aware anomaly detectionsystem, which is poised to redefine practices in urban surveillance.</description><author>Sareh Soltani Nejad, Anwar Haque</author><pubDate>Wed, 13 Nov 2024 16:33:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08755v1</guid></item><item><title>Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Videos</title><link>http://arxiv.org/abs/2411.08753v1</link><description>Given a multi-view video, which viewpoint is most informative for a humanobserver? Existing methods rely on heuristics or expensive ``best-view"supervision to answer this question, limiting their applicability. We propose aweakly supervised approach that leverages language accompanying aninstructional multi-view video as a means to recover its most informativeviewpoint(s). Our key hypothesis is that the more accurately an individual viewcan predict a view-agnostic text summary, the more informative it is. To putthis into action, we propose a framework that uses the relative accuracy ofview-dependent caption predictions as a proxy for best view pseudo-labels.Then, those pseudo-labels are used to train a view selector, together with anauxiliary camera pose predictor that enhances view-sensitivity. Duringinference, our model takes as input only a multi-view video -- no language orcamera poses -- and returns the best viewpoint to watch at each timestep. Ontwo challenging datasets comprised of diverse multi-camera setups and how-toactivities, our model consistently outperforms state-of-the-art baselines, bothwith quantitative metrics and human evaluation.</description><author>Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Reina Pradhan, Kristen Grauman</author><pubDate>Wed, 13 Nov 2024 16:31:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08753v1</guid></item><item><title>Multi-Perspective Stance Detection</title><link>http://arxiv.org/abs/2411.08752v1</link><description>Subjective NLP tasks usually rely on human annotations provided by multipleannotators, whose judgments may vary due to their diverse backgrounds and lifeexperiences. Traditional methods often aggregate multiple annotations into asingle ground truth, disregarding the diversity in perspectives that arisesfrom annotator disagreement. In this preliminary study, we examine the effectof including multiple annotations on model accuracy in classification. Ourmethodology investigates the performance of perspective-aware classificationmodels in stance detection task and further inspects if annotator disagreementaffects the model confidence. The results show that multi-perspective approachyields better classification performance outperforming the baseline which usesthe single label. This entails that designing more inclusive perspective-awareAI models is not only an essential first step in implementing responsible andethical AI, but it can also achieve superior results than using the traditionalapproaches.</description><author>Benedetta Muscato, Praveen Bushipaka, Gizem Gezici, Lucia Passaro, Fosca Giannotti</author><pubDate>Wed, 13 Nov 2024 16:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08752v1</guid></item><item><title>Optimal Transport-Based Displacement Interpolation with Data Augmentation for Reduced Order Modeling of Nonlinear Dynamical Systems</title><link>http://arxiv.org/abs/2411.08750v1</link><description>We present a novel reduced-order Model (ROM) that leverages optimal transport(OT) theory and displacement interpolation to enhance the representation ofnonlinear dynamics in complex systems. While traditional ROM techniques facechallenges in this scenario, especially when data (i.e., observationalsnapshots) is limited, our method addresses these issues by introducing a dataaugmentation strategy based on OT principles. The proposed framework generatesinterpolated solutions tracing geodesic paths in the space of probabilitydistributions, enriching the training dataset for the ROM. A key feature of ourapproach is its ability to provide a continuous representation of thesolution's dynamics by exploiting a virtual-to-real time mapping. This enablesthe reconstruction of solutions at finer temporal scales than those provided bythe original data. To further improve prediction accuracy, we employ GaussianProcess Regression to learn the residual and correct the representation betweenthe interpolated snapshots and the physical solution. We demonstrate theeffectiveness of our methodology with atmospheric mesoscale benchmarkscharacterized by highly nonlinear, advection-dominated dynamics. Our resultsshow improved accuracy and efficiency in predicting complex system behaviors,indicating the potential of this approach for a wide range of applications incomputational physics and engineering.</description><author>Moaad Khamlich, Federico Pichi, Michele Girfoglio, Annalisa Quaini, Gianluigi Rozza</author><pubDate>Wed, 13 Nov 2024 16:29:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08750v1</guid></item><item><title>Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models</title><link>http://arxiv.org/abs/2411.07140v2</link><description>New LLM evaluation benchmarks are important to align with the rapiddevelopment of Large Language Models (LLMs). In this work, we present ChineseSimpleQA, the first comprehensive Chinese benchmark to evaluate the factualityability of language models to answer short questions, and Chinese SimpleQAmainly has five properties (i.e., Chinese, Diverse, High-quality, Static,Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6major topics with 99 diverse subtopics. Second, we conduct a comprehensivequality control process to achieve high-quality questions and answers, wherethe reference answers are static and cannot be changed over time. Third,following SimpleQA, the questions and answers are very short, and the gradingprocess is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, weperform a comprehensive evaluation on the factuality abilities of existingLLMs. Finally, we hope that Chinese SimpleQA could guide the developers tobetter understand the Chinese factuality abilities of their models andfacilitate the growth of foundation models.</description><author>Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Zhuoran Lin, Xuepeng Liu, Dekai Sun, Shirong Lin, Zhicheng Zheng, Xiaoyong Zhu, Wenbo Su, Bo Zheng</author><pubDate>Wed, 13 Nov 2024 16:27:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07140v2</guid></item><item><title>Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers</title><link>http://arxiv.org/abs/2411.08745v1</link><description>A central question in multilingual language modeling is whether largelanguage models (LLMs) develop a universal concept representation, disentangledfrom specific languages. In this paper, we address this question by analyzinglatent representations (latents) during a word translation task intransformer-based LLMs. We strategically extract latents from a sourcetranslation prompt and insert them into the forward pass on a targettranslation prompt. By doing so, we find that the output language is encoded inthe latent at an earlier layer than the concept to be translated. Building onthis insight, we conduct two key experiments. First, we demonstrate that we canchange the concept without changing the language and vice versa throughactivation patching alone. Second, we show that patching with the mean overlatents across different languages does not impair and instead improves themodels' performance in translating the concept. Our results provide evidencefor the existence of language-agnostic concept representations within theinvestigated models.</description><author>Clément Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West</author><pubDate>Wed, 13 Nov 2024 16:26:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08745v1</guid></item><item><title>A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models</title><link>http://arxiv.org/abs/2411.08742v1</link><description>With the rise of Speech Large Language Models (Speech LLMs), there has beengrowing interest in discrete speech tokens for their ability to integrate withtext-based tokens seamlessly. Compared to most studies that focus on continuousspeech features, although discrete-token based LLMs have shown promisingresults on certain tasks, the performance gap between these two paradigms israrely explored. In this paper, we present a fair and thorough comparisonbetween discrete and continuous features across a variety of semantic-relatedtasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal thatcontinuous features generally outperform discrete tokens, particularly in tasksrequiring fine-grained semantic understanding. Moreover, this study goes beyondsurface-level comparison by identifying key factors behind theunder-performance of discrete tokens, such as limited token granularity andinefficient information retention. To enhance the performance of discretetokens, we explore potential aspects based on our analysis. We hope our resultscan offer new insights into the opportunities for advancing discrete speechtokens in Speech LLMs.</description><author>Dingdong Wang, Mingyu Cui, Dongchao Yang, Xueyuan Chen, Helen Meng</author><pubDate>Wed, 13 Nov 2024 16:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08742v1</guid></item><item><title>Bayesian Comparisons Between Representations</title><link>http://arxiv.org/abs/2411.08739v1</link><description>Which neural networks are similar is a fundamental question for both machinelearning and neuroscience. Our novel method compares representations based onBayesian statistics about linear readouts from the representations. Concretely,we suggest to use the total variation distance or Jensen-Shannon distancebetween prior predictive distributions to compare representations. The priorpredictive distribution is a full description of the inductive bias andgeneralization of a model in Bayesian statistics, making it a great basis forcomparisons. As Jensen-Shannon distance and total variation distance aremetrics our dissimilarity measures are pseudo-metrics for representations. Fora linear readout, our metrics just depend on the linear kernel matrix of therepresentations. Thus, our metrics connects linear read-out based comparisonsto kernel based metrics like centered kernel alignment and representationalsimilarity analysis. We apply our new metrics to deep neural networks trainedon ImageNet-1k. Our new metrics can be computed efficiently including astochastic gradient without dimensionality reductions of the representations.It broadly agrees with existing metrics, but is more stringent. It varies lessacross different random image samples, and it measures how well tworepresentations could be distinguished based on a linear read out. Thus ourmetric nicely extends our toolkit for comparing representations.</description><author>Heiko H. Schütt</author><pubDate>Wed, 13 Nov 2024 16:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08739v1</guid></item><item><title>New advances in universal approximation with neural networks of minimal width</title><link>http://arxiv.org/abs/2411.08735v1</link><description>Deep neural networks have achieved remarkable success in diverseapplications, prompting the need for a solid theoretical foundation. Recentresearch has identified the minimal width $\max\{2,d_x,d_y\}$ required forneural networks with input dimensions $d_x$ and output dimension $d_y$ that useleaky ReLU activations to universally approximate$L^p(\mathbb{R}^{d_x},\mathbb{R}^{d_y})$ on compacta. Here, we present analternative proof for the minimal width of such neural networks, by directlyconstructing approximating networks using a coding scheme that leverages theproperties of leaky ReLUs and standard $L^p$ results. The obtained constructionhas a minimal interior dimension of $1$, independent of input and outputdimensions, which allows us to show that autoencoders with leaky ReLUactivations are universal approximators of $L^p$ functions. Furthermore, wedemonstrate that the normalizing flow LU-Net serves as a distributionaluniversal approximator. We broaden our results to show that smooth invertibleneural networks can approximate $L^p(\mathbb{R}^{d},\mathbb{R}^{d})$ oncompacta when the dimension $d\geq 2$, which provides a constructive proof of aclassical theorem of Brenier and Gangbo. In addition, we use a topologicalargument to establish that for FNNs with monotone Lipschitz continuousactivations, $d_x+1$ is a lower bound on the minimal width required for theuniform universal approximation of continuous functions$C^0(\mathbb{R}^{d_x},\mathbb{R}^{d_y})$ on compacta when $d_x\geq d_y$.</description><author>Dennis Rochau, Hanno Gottschalk, Robin Chan</author><pubDate>Wed, 13 Nov 2024 16:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08735v1</guid></item><item><title>Recommender systems and reinforcement learning for building control and occupant interaction: A text-mining driven review of scientific literature</title><link>http://arxiv.org/abs/2411.08734v1</link><description>The indoor environment greatly affects health and well-being; enhancinghealth and reducing energy use in these settings is a key research focus. Withadvancing Information and Communication Technology (ICT), recommendationsystems and reinforcement learning have emerged as promising methods to inducebehavioral changes that improve indoor environments and building energyefficiency. This study employs text-mining and Natural Language Processing(NLP) to examine these approaches in building control and occupant interaction.Analyzing approximately 27,000 articles from the ScienceDirect database, wefound extensive use of recommendation systems and reinforcement learning forspace optimization, location recommendations, and personalized controlsuggestions. Despite broad applications, their use in optimizing indoorenvironments and energy efficiency is limited. Traditional recommendationalgorithms are commonly used, but optimizing indoor conditions and energyefficiency often requires advanced machine learning techniques likereinforcement and deep learning. This review highlights the potential forexpanding recommender systems and reinforcement learning applications inbuildings and indoor environments. Areas for innovation include predictivemaintenance, building-related product recommendations, and optimizingenvironments for specific needs like sleep and productivity enhancements basedon user feedback.</description><author>Wenhao Zhang, Matias Quintana, Clayton Miller</author><pubDate>Wed, 13 Nov 2024 16:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08734v1</guid></item><item><title>Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models</title><link>http://arxiv.org/abs/2411.08733v1</link><description>Aligning Large Language Models (LLMs) traditionally relies on costly trainingand human preference annotations. Self-alignment seeks to reduce these expensesby enabling models to align themselves. To further lower costs and achievealignment without any expensive tuning or annotations, we introduce a newtuning-free approach for self-alignment, Dynamic Rewarding with PromptOptimization (\ours). Our approach leverages a search-based optimizationframework that allows LLMs to iteratively self-improve and craft the optimalalignment instructions, all without additional training or human intervention.The core of \ours is a dynamic rewarding mechanism, which identifies andrectifies model-specific alignment weaknesses, allowing LLMs to adaptefficiently to diverse alignment challenges. Empirical evaluations on eightrecent LLMs, both open- and closed-sourced, demonstrate that \ourssignificantly enhances alignment performance, with base models outperformingtheir SFT/RLHF-tuned counterparts. Moreover, the prompts automaticallyoptimized by \ours surpass those curated by human experts, further validatingthe effectiveness of our approach. Our findings highlight the great potentialof current LLMs to achieve adaptive self-alignment through inference-timeoptimization, complementing tuning-based alignment methods.</description><author>Somanshu Singla, Zhen Wang, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, Eric P. Xing</author><pubDate>Wed, 13 Nov 2024 16:15:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08733v1</guid></item><item><title>Polymetis:Large Language Modeling for Multiple Material Domains</title><link>http://arxiv.org/abs/2411.08728v1</link><description>As the application of large language models in various fields continues toexpand, materials science also ushers in opportunities for AI-driveninnovation. The traditional way of relying on manual search for materialsscience-related information is now using artificial intelligence technology asan auxiliary tool to improve the efficiency of materials science research. Toaccelerate researchers' knowledge acquisition and intelligent decision-makingsupport in materials science research, this paper proposes a large languagemodel Polymetis model for a variety of materials fields, aiming to providehighly professional knowledge answers in the field of materials, coveringenergy materials, functional materials, alloy materials, physical chemistry,biology, and other material directions. The model uses a dataset of about 2million material knowledge instructions, and in the process of building thedataset, we developed the Intelligent Extraction Large Model (IELM), which isspecially used to extract and form structured knowledge from scientific texts,avoiding a large number of costs that need to be manually annotated, andimproving efficiency. We inject this data into the GLM4-9B model for learningto enhance its inference capabilities in a variety of material domains. Inaddition, we have introduced enhanced prompt strategies to ensure that theanswers to the model are more organized and comprehensive, providing efficientand comprehensive intelligent support for the diverse needs of materialsscience exploration, and promoting the development of material science.</description><author>Chao Huang, Huichen Xiao, Chen Chen, Chunyan Chen, Yi Zhao, Shiyu Du, Yiming Zhang, He Sha, Ruixin Gu</author><pubDate>Wed, 13 Nov 2024 16:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08728v1</guid></item><item><title>Analyst Reports and Stock Performance: Evidence from the Chinese Market</title><link>http://arxiv.org/abs/2411.08726v1</link><description>This article applies natural language processing (NLP) to extract andquantify textual information to predict stock performance. Using an extensivedataset of Chinese analyst reports and employing a customized BERT deeplearning model for Chinese text, this study categorizes the sentiment of thereports as positive, neutral, or negative. The findings underscore thepredictive capacity of this sentiment indicator for stock volatility, excessreturns, and trading volume. Specifically, analyst reports with strong positivesentiment will increase excess return and intraday volatility, and vice versa,reports with strong negative sentiment also increase volatility and tradingvolume, but decrease future excess return. The magnitude of this effect isgreater for positive sentiment reports than for negative sentiment reports.This article contributes to the empirical literature on sentiment analysis andthe response of the stock market to news in the Chinese stock market.</description><author>Rui Liu, Jiayou Liang, Haolong Chen, Yujia Hu</author><pubDate>Wed, 13 Nov 2024 16:08:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08726v1</guid></item><item><title>Implicit Bias of Mirror Flow on Separable Data</title><link>http://arxiv.org/abs/2406.12763v3</link><description>We examine the continuous-time counterpart of mirror descent, namely mirrorflow, on classification problems which are linearly separable. Such problemsare minimised `at infinity' and have many possible solutions; we study whichsolution is preferred by the algorithm depending on the mirror potential. Forexponential tailed losses and under mild assumptions on the potential, we showthat the iterates converge in direction towards a $\phi_\infty$-maximum marginclassifier. The function $\phi_\infty$ is the \textit{horizon function} of themirror potential and characterises its shape `at infinity'. When the potentialis separable, a simple formula allows to compute this function. We analyseseveral examples of potentials and provide numerical experiments highlightingour results.</description><author>Scott Pesme, Radu-Alexandru Dragomir, Nicolas Flammarion</author><pubDate>Wed, 13 Nov 2024 16:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12763v3</guid></item><item><title>Retrieval Augmented Recipe Generation</title><link>http://arxiv.org/abs/2411.08715v1</link><description>Given the potential applications of generating recipes from food images, thisarea has garnered significant attention from researchers in recent years.Existing works for recipe generation primarily utilize a two-stage trainingmethod, first generating ingredients and then obtaining instructions from boththe image and ingredients. Large Multi-modal Models (LMMs), which have achievednotable success across a variety of vision and language tasks, shed light togenerating both ingredients and instructions directly from images.Nevertheless, LMMs still face the common issue of hallucinations during recipegeneration, leading to suboptimal performance. To tackle this, we propose aretrieval augmented large multimodal model for recipe generation. We firstintroduce Stochastic Diversified Retrieval Augmentation (SDRA) to retrieverecipes semantically related to the image from an existing datastore as asupplement, integrating them into the prompt to add diverse and rich context tothe input image. Additionally, Self-Consistency Ensemble Voting mechanism isproposed to determine the most confident prediction recipes as the finaloutput. It calculates the consistency among generated recipe candidates, whichuse different retrieval recipes as context for generation. Extensiveexperiments validate the effectiveness of our proposed method, whichdemonstrates state-of-the-art (SOTA) performance in recipe generation tasks onthe Recipe1M dataset.</description><author>Guoshan Liu, Hailong Yin, Bin Zhu, Jingjing Chen, Chong-Wah Ngo, Yu-Gang Jiang</author><pubDate>Wed, 13 Nov 2024 15:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08715v1</guid></item><item><title>High-resolution optical and acoustic remote sensing datasets of the Puck Lagoon, Southern Baltic</title><link>http://arxiv.org/abs/2411.08712v1</link><description>The very shallow marine basin of Puck Lagoon in the southern Baltic Sea, onthe Northern coast of Poland, hosts valuable benthic habitats and culturalheritage sites. These include, among others, protected Zostera marina meadows,one of the Baltic's major medieval harbours, a ship graveyard, and likely othersubmerged features that are yet to be discovered. Prior to this project, nocomprehensive high-resolution remote sensing data were available for this area.This article describes the first Digital Elevation Models (DEMs) derived from acombination of airborne bathymetric LiDAR, multibeam echosounder, airbornephotogrammetry and satellite imagery. These datasets also include multibeamechosounder backscatter and LiDAR intensity, allowing determination of thecharacter and properties of the seafloor. Combined, these datasets are a vitalresource for assessing and understanding seafloor morphology, benthic habitats,cultural heritage, and submerged landscapes. Given the significance of PuckLagoon's hydrographical, ecological, geological, and archaeological environs,the high-resolution bathymetry, acquired by our project, can provide thefoundation for sustainable management and informed decision-making for thisarea of interest.</description><author>Łukasz Janowski, Dimitrios Skarlatos, Panagiotis Agrafiotis, Paweł Tysiąc, Andrzej Pydyn, Mateusz Popek, Anna M. Kotarba-Morley, Gottfried Mandlburger, Łukasz Gajewski, Mateusz Kołakowski, Alexandra Papadaki, Juliusz Gajewski</author><pubDate>Wed, 13 Nov 2024 15:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08712v1</guid></item><item><title>Rethinking Distribution Shifts: Empirical Analysis and Inductive Modeling for Tabular Data</title><link>http://arxiv.org/abs/2307.05284v4</link><description>Different distribution shifts require different interventions, and algorithmsmust be grounded in the specific shifts they address. However, methodologicaldevelopment for robust algorithms typically relies on structural assumptionsthat lack empirical validation. Advocating for an empirically groundeddata-driven approach to research, we build an empirical testbed comprisingnatural shifts across 5 tabular datasets and 60,000 method configurationsencompassing imbalanced learning and distributionally robust optimization (DRO)methods. We find $Y|X$-shifts are most prevalent on our testbed, in starkcontrast to the heavy focus on $X$ (covariate)-shifts in the ML literature. Theperformance of robust algorithms varies significantly over shift types, and isno better than that of vanilla methods. To understand why, we conduct anin-depth empirical analysis of DRO methods and find that although oftenneglected by researchers, implementation details -- such as the choice ofunderlying model class (e.g., XGBoost) and hyperparameter selection -- have abigger impact on performance than the ambiguity set or its radius. To furtherbridge that gap between methodological research and practice, we design casestudies that illustrate how such a data-driven, inductive understanding ofdistribution shifts can enhance both data-centric and algorithmicinterventions.</description><author>Jiashuo Liu, Tianyu Wang, Peng Cui, Hongseok Namkoong</author><pubDate>Wed, 13 Nov 2024 15:53:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05284v4</guid></item><item><title>Are Triggers Needed for Document-Level Event Extraction?</title><link>http://arxiv.org/abs/2411.08708v1</link><description>Most existing work on event extraction has focused on sentence-level textsand presumes the identification of a trigger-span -- a word or phrase in theinput that evokes the occurrence of an event of interest. Event arguments arethen extracted with respect to the trigger. Indeed, triggers are treated asintegral to, and trigger detection as an essential component of, eventextraction. In this paper, we provide the first investigation of the role oftriggers for the more difficult and much less studied task of document-levelevent extraction. We analyze their usefulness in multiple end-to-end andpipelined neural event extraction models for three document-level eventextraction datasets, measuring performance using triggers of varying quality(human-annotated, LLM-generated, keyword-based, and random). Our research showsthat trigger effectiveness varies based on the extraction task'scharacteristics and data quality, with basic, automatically-generated triggersserving as a viable alternative to human-annotated ones. Furthermore, providingdetailed event descriptions to the extraction model helps maintain robustperformance even when trigger quality degrades. Perhaps surprisingly, we alsofind that the mere existence of trigger input, even random ones, is importantfor prompt-based LLM approaches to the task.</description><author>Shaden Shaar, Wayne Chen, Maitreyi Chatterjee, Barry Wang, Wenting Zhao, Claire Cardie</author><pubDate>Wed, 13 Nov 2024 15:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08708v1</guid></item><item><title>Searching Latent Program Spaces</title><link>http://arxiv.org/abs/2411.08706v1</link><description>Program synthesis methods aim to automatically generate programs restrictedto a language that can explain a given specification of input-output pairs.While purely symbolic approaches suffer from a combinatorial search space,recent methods leverage neural networks to learn distributions over programstructures to narrow this search space significantly, enabling more efficientsearch. However, for challenging problems, it remains difficult to train modelsto perform program synthesis in one shot, making test-time search essential.Most neural methods lack structured search mechanisms during inference, relyinginstead on stochastic sampling or gradient updates, which can be inefficient.In this work, we propose the Latent Program Network (LPN), a general algorithmfor program induction that learns a distribution over latent programs in acontinuous space, enabling efficient search and test-time adaptation. Weexplore how to train these networks to optimize for test-time computation anddemonstrate the use of gradient-based search both during training and at testtime. We evaluate LPN on ARC-AGI, a program synthesis benchmark that evaluatesperformance by generalizing programs to new inputs rather than explaining theunderlying specification. We show that LPN can generalize beyond its trainingdistribution and adapt to unseen tasks by utilizing test-time computation,outperforming algorithms without test-time adaptation mechanisms.</description><author>Clément Bonnet, Matthew V Macfarlane</author><pubDate>Wed, 13 Nov 2024 15:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08706v1</guid></item><item><title>Calibrating Bayesian Generative Machine Learning for Bayesiamplification</title><link>http://arxiv.org/abs/2408.00838v2</link><description>Recently, combinations of generative and Bayesian machine learning have beenintroduced in particle physics for both fast detector simulation and inferencetasks. These neural networks aim to quantify the uncertainty on the generateddistribution originating from limited training statistics. The interpretationof a distribution-wide uncertainty however remains ill-defined. We show a clearscheme for quantifying the calibration of Bayesian generative machine learningmodels. For a Continuous Normalizing Flow applied to a low-dimensional toyexample, we evaluate the calibration of Bayesian uncertainties from either amean-field Gaussian weight posterior, or Monte Carlo sampling network weights,to gauge their behaviour on unsteady distribution edges. Well calibrateduncertainties can then be used to roughly estimate the number of uncorrelatedtruth samples that are equivalent to the generated sample and clearly indicatedata amplification for smooth features of the distribution.</description><author>Sebastian Bieringer, Sascha Diefenbacher, Gregor Kasieczka, Mathias Trabs</author><pubDate>Wed, 13 Nov 2024 15:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00838v2</guid></item><item><title>BoQ: A Place is Worth a Bag of Learnable Queries</title><link>http://arxiv.org/abs/2405.07364v3</link><description>In visual place recognition, accurately identifying and matching images oflocations under varying environmental conditions and viewpoints remains asignificant challenge. In this paper, we introduce a new technique, calledBag-of-Queries (BoQ), which learns a set of global queries designed to captureuniversal place-specific attributes. Unlike existing methods that employself-attention and generate the queries directly from the input features, BoQemploys distinct learnable global queries, which probe the input features viacross-attention, ensuring consistent information aggregation. In addition, ourtechnique provides an interpretable attention mechanism and integrates withboth CNN and Vision Transformer backbones. The performance of BoQ isdemonstrated through extensive experiments on 14 large-scale benchmarks. Itconsistently outperforms current state-of-the-art techniques including NetVLAD,MixVPR and EigenPlaces. Moreover, as a global retrieval technique (one-stage),BoQ surpasses two-stage retrieval methods, such as Patch-NetVLAD, TransVPR andR2Former, all while being orders of magnitude faster and more efficient. Thecode and model weights are publicly available athttps://github.com/amaralibey/Bag-of-Queries.</description><author>Amar Ali-Bey, Brahim Chaib-draa, Philippe Giguère</author><pubDate>Wed, 13 Nov 2024 15:48:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07364v3</guid></item><item><title>AutoSAT: Automatically Optimize SAT Solvers via Large Language Models</title><link>http://arxiv.org/abs/2402.10705v3</link><description>Conflict-Driven Clause Learning (CDCL) is the mainstream framework forsolving the Satisfiability problem (SAT), and CDCL solvers typically rely onvarious heuristics, which have a significant impact on their performance.Modern CDCL solvers, such as MiniSat and Kissat, commonly incorporate severalheuristics and select one to use according to simple rules, requiringsignificant time and expert effort to fine-tune in practice. The pervasion ofLarge Language Models (LLMs) provides a potential solution to address thisissue. However, generating a CDCL solver from scratch is not effective due tothe complexity and context volume of SAT solvers. Instead, we propose AutoSAT,a framework that automatically optimizes heuristics in a pre-defined modularsearch space based on existing CDCL solvers. Unlike existing automatedalgorithm design approaches focusing on hyperparameter tuning and operatorselection, AutoSAT can generate new efficient heuristics. In this first attemptat optimizing SAT solvers using LLMs, several strategies including the greedyhill climber and (1+1) Evolutionary Algorithm are employed to guide LLMs tosearch for better heuristics. Experimental results demonstrate that LLMs cangenerally enhance the performance of CDCL solvers. A realization of AutoSAToutperforms MiniSat on 9 out of 12 datasets and even surpasses thestate-of-the-art hybrid solver Kissat on 4 datasets.</description><author>Yiwen Sun, Furong Ye, Xianyin Zhang, Shiyu Huang, Bingzhen Zhang, Ke Wei, Shaowei Cai</author><pubDate>Wed, 13 Nov 2024 15:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10705v3</guid></item><item><title>MVKTrans: Multi-View Knowledge Transfer for Robust Multiomics Classification</title><link>http://arxiv.org/abs/2411.08703v1</link><description>The distinct characteristics of multiomics data, including complexinteractions within and across biological layers and disease heterogeneity(e.g., heterogeneity in etiology and clinical symptoms), drive us to developnovel designs to address unique challenges in multiomics prediction. In thispaper, we propose the multi-view knowledge transfer learning (MVKTrans)framework, which transfers intra- and inter-omics knowledge in an adaptivemanner by reviewing data heterogeneity and suppressing bias transfer, therebyenhancing classification performance. Specifically, we design a graphcontrastive module that is trained on unlabeled data to effectively learn andtransfer the underlying intra-omics patterns to the supervised task. Thisunsupervised pretraining promotes learning general and unbiased representationsfor each modality, regardless of the downstream tasks. In light of the varyingdiscriminative capacities of modalities across different diseases and/orsamples, we introduce an adaptive and bi-directional cross-omics distillationmodule. This module automatically identifies richer modalities and facilitatesdynamic knowledge transfer from more informative to less informative omics,thereby enabling a more robust and generalized integration. Extensiveexperiments on four real biomedical datasets demonstrate the superiorperformance and robustness of MVKTrans compared to the state-of-the-art. Codeand data are available at https://github.com/Yaolab-fantastic/MVKTrans.</description><author>Shan Cong, Zhiling Sang, Hongwei Liu, Haoran Luo, Xin Wang, Hong Liang, Jie Hao, Xiaohui Yao</author><pubDate>Wed, 13 Nov 2024 15:45:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08703v1</guid></item><item><title>Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators</title><link>http://arxiv.org/abs/2311.07879v4</link><description>Extensive efforts in automated approaches for content moderation have beenfocused on developing models to identify toxic, offensive, and hateful contentwith the aim of lightening the load for moderators. Yet, it remains uncertainwhether improvements on those tasks have truly addressed moderators' needs inaccomplishing their work. In this paper, we surface gaps between past researchefforts that have aimed to provide automation for aspects of content moderationand the needs of volunteer content moderators, regarding identifying violationsof various moderation rules. To do so, we conduct a model review on HuggingFace to reveal the availability of models to cover various moderation rules andguidelines from three exemplar forums. We further put state-of-the-art LLMs tothe test, evaluating how well these models perform in flagging violations ofplatform rules from one particular forum. Finally, we conduct a user surveystudy with volunteer moderators to gain insight into their perspectives onuseful moderation models. Overall, we observe a non-trivial gap, as missingdeveloped models and LLMs exhibit moderate to low performance on a significantportion of the rules. Moderators' reports provide guides for future work ondeveloping moderation assistant models.</description><author>Yang Trista Cao, Lovely-Frances Domingo, Sarah Ann Gilbert, Michelle Mazurek, Katie Shilton, Hal Daumé III</author><pubDate>Wed, 13 Nov 2024 15:45:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07879v4</guid></item><item><title>TRACE: Transformer-based Risk Assessment for Clinical Evaluation</title><link>http://arxiv.org/abs/2411.08701v1</link><description>We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation),a novel method for clinical risk assessment based on clinical data, leveragingthe self-attention mechanism for enhanced feature interaction and resultinterpretation. Our approach is able to handle different data modalities,including continuous, categorical and multiple-choice (checkbox) attributes.The proposed architecture features a shared representation of the clinical dataobtained by integrating specialized embeddings of each data modality, enablingthe detection of high-risk individuals using Transformer encoder layers. Toassess the effectiveness of the proposed method, a strong baseline based onnon-negative multi-layer perceptrons (MLPs) is introduced. The proposed methodoutperforms various baselines widely used in the domain of clinical riskassessment, while effectively handling missing values. In terms ofexplainability, our Transformer-based method offers easily interpretableresults via attention weights, further enhancing the clinicians'decision-making process.</description><author>Dionysis Christopoulos, Sotiris Spanos, Valsamis Ntouskos, Konstantinos Karantzalos</author><pubDate>Wed, 13 Nov 2024 15:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08701v1</guid></item><item><title>Rethinking negative sampling in content-based news recommendation</title><link>http://arxiv.org/abs/2411.08700v1</link><description>News recommender systems are hindered by the brief lifespan of articles, asthey undergo rapid relevance decay. Recent studies have demonstrated thepotential of content-based neural techniques in tackling this problem. However,these models often involve complex neural architectures and often lackconsideration for negative examples. In this study, we posit that the carefulsampling of negative examples has a big impact on the model's outcome. Wedevise a negative sampling technique that not only improves the accuracy of themodel but also facilitates the decentralization of the recommendation system.The experimental results obtained using the MIND dataset demonstrate that theaccuracy of the method under consideration can compete with that ofState-of-the-Art models. The utilization of the sampling technique is essentialin reducing model complexity and accelerating the training process, whilemaintaining a high level of accuracy. Finally, we discuss how decentralizedmodels can help improve privacy and scalability.</description><author>Miguel Ângelo Rebelo, João Vinagre, Ivo Pereira, Álvaro Figueira</author><pubDate>Wed, 13 Nov 2024 15:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08700v1</guid></item><item><title>FedSub: Introducing class-aware Subnetworks Fusion to Enhance Personalized Federated Learning in Ubiquitous Systems</title><link>http://arxiv.org/abs/2411.08699v1</link><description>Personalized Federated Learning is essential in AI-driven ubiquitous systems,supporting the distributed development of models able to adapt to diverse andevolving user behaviors while safeguarding privacy. Despite addressingheterogeneous user data distributions in collaborative model training, existingmethods often face limitations balancing personalization and generalization,oversimplifying user similarities, or relying heavily on global models. In thispaper, we propose FedSub, a novel federated approach designed to enhancepersonalization through the use of class-aware prototypes and modelsubnetworks. Prototypes serve as compact representations of user data,clustered on the server to identify similarities based on specific labelpatterns. Concurrently, subnetworks -- model components necessary to processeach class -- are extracted locally and fused by the server according to theseclusters, producing highly tailored model updates for each user. Thisfine-grained, class-specific aggregation of clients' models allows FedSub tocapture the unique characteristics of individual user data patterns. Theeffectiveness of FedSub is validated in three real-world scenarioscharacterized by high data heterogeneity, derived from human activityrecognition and mobile health applications. Experimental evaluationsdemonstrate FedSub's performance improvements with respect to thestate-of-the-art and significant advancements in personalization for ubiquitoussystems based on personal mobile and wearable devices.</description><author>Mattia Giovanni Campana, Franca Delmastro</author><pubDate>Wed, 13 Nov 2024 15:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08699v1</guid></item><item><title>Scholarly Wikidata: Population and Exploration of Conference Data in Wikidata using LLMs</title><link>http://arxiv.org/abs/2411.08696v1</link><description>Several initiatives have been undertaken to conceptually model the domain ofscholarly data using ontologies and to create respective Knowledge Graphs. Yet,the full potential seems unleashed, as automated means for automatic populationof said ontologies are lacking, and respective initiatives from the SemanticWeb community are not necessarily connected: we propose to make scholarly datamore sustainably accessible by leveraging Wikidata's infrastructure andautomating its population in a sustainable manner through LLMs by tapping intounstructured sources like conference Web sites and proceedings texts as well asalready existing structured conference datasets. While an initial analysisshows that Semantic Web conferences are only minimally represented in Wikidata,we argue that our methodology can help to populate, evolve and maintainscholarly data as a community within Wikidata. Our main contributions include(a) an analysis of ontologies for representing scholarly data to identify gapsand relevant entities/properties in Wikidata, (b) semi-automated extraction --requiring (minimal) manual validation -- of conference metadata (e.g.,acceptance rates, organizer roles, programme committee members, best paperawards, keynotes, and sponsors) from websites and proceedings texts using LLMs.Finally, we discuss (c) extensions to visualization tools in the Wikidatacontext for data exploration of the generated scholarly data. Our study focuseson data from 105 Semantic Web-related conferences and extends/adds more than6000 entities in Wikidata. It is important to note that the method can be moregenerally applicable beyond Semantic Web-related conferences for enhancingWikidata's utility as a comprehensive scholarly resource. Source Repository: https://github.com/scholarly-wikidata/ DOI: https://doi.org/10.5281/zenodo.10989709 License: Creative Commons CC0 (Data), MIT (Code)</description><author>Nandana Mihindukulasooriya, Sanju Tiwari, Daniil Dobriy, Finn Årup Nielsen, Tek Raj Chhetri, Axel Polleres</author><pubDate>Wed, 13 Nov 2024 15:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08696v1</guid></item><item><title>Neural Persistence Dynamics</title><link>http://arxiv.org/abs/2405.15732v2</link><description>We consider the problem of learning the dynamics in the topology oftime-evolving point clouds, the prevalent spatiotemporal model for systemsexhibiting collective behavior, such as swarms of insects and birds orparticles in physics. In such systems, patterns emerge from (local)interactions among self-propelled entities. While several well-understoodgoverning equations for motion and interaction exist, they are notoriouslydifficult to fit to data, as most prior work requires knowledge aboutindividual motion trajectories, i.e., a requirement that is challenging tosatisfy with an increasing number of entities. To evade such confoundingfactors, we investigate collective behavior from a $\textit{topologicalperspective}$, but instead of summarizing entire observation sequences (as donepreviously), we propose learning a latent dynamical model from topologicalfeatures $\textit{per time point}$. The latter is then used to formulate adownstream regression task to predict the parametrization of some a priorispecified governing equation. We implement this idea based on a latent ODElearned from vectorized (static) persistence diagrams and show that acombination of recent stability results for persistent homology justifies thismodeling choice. Various (ablation) experiments not only demonstrate therelevance of each model component but provide compelling empirical evidencethat our proposed model - $\textit{Neural Persistence Dynamics}$ -substantially outperforms the state-of-the-art across a diverse set ofparameter regression tasks.</description><author>Sebastian Zeng, Florian Graf, Martin Uray, Stefan Huber, Roland Kwitt</author><pubDate>Wed, 13 Nov 2024 15:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15732v2</guid></item><item><title>PICZL: Image-based Photometric Redshifts for AGN</title><link>http://arxiv.org/abs/2411.07305v2</link><description>Computing photo-z for AGN is challenging, primarily due to the interplay ofrelative emissions associated with the SMBH and its host galaxy. SED fittingmethods, effective in pencil-beam surveys, face limitations in all-sky surveyswith fewer bands available, lacking the ability to capture the AGN contributionto the SED accurately. This limitation affects the many 10s of millions of AGNclearly singled out and identified by SRG/eROSITA. Our goal is to significantlyenhance photometric redshift performance for AGN in all-sky surveys whileavoiding the need to merge multiple data sets. Instead, we employ readilyavailable data products from the 10th Data Release of the Imaging Legacy Surveyfor DESI, covering &gt; 20,000 deg$^{2}$ with deep images and catalog-basedphotometry in the grizW1-W4 bands. We introduce PICZL, a machine-learningalgorithm leveraging an ensemble of CNNs. Utilizing a cross-channel approach,the algorithm integrates distinct SED features from images with those obtainedfrom catalog-level data. Full probability distributions are achieved via theintegration of Gaussian mixture models. On a validation sample of 8098 AGN,PICZL achieves a variance $\sigma_{\textrm{NMAD}}$ of 4.5% with an outlierfraction $\eta$ of 5.6%, outperforming previous attempts to compute accuratephoto-z for AGN using ML. We highlight that the model's performance depends onmany variables, predominantly the depth of the data. A thorough evaluation ofthese dependencies is presented in the paper. Our streamlined methodologymaintains consistent performance across the entire survey area when accountingfor differing data quality. The same approach can be adopted for future deepphotometric surveys such as LSST and Euclid, showcasing its potential forwide-scale realisation. With this paper, we release updated photo-z (includingerrors) for the XMM-SERVS W-CDF-S, ELAIS-S1 and LSS fields.</description><author>William Roster, Mara Salvato, Sven Krippendorf, Aman Saxena, Raphael Shirley, Johannes Buchner, Julien Wolf, Tom Dwelly, Franz E. Bauer, James Aird, Claudio Ricci, Roberto J. Assef, Scott F. Anderson, Xin Liu, Andrea Merloni, Jochen Weller, Kirpal Nandra</author><pubDate>Wed, 13 Nov 2024 15:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07305v2</guid></item><item><title>Inferring Parameter Distributions in Heterogeneous Motile Particle Ensembles: A Likelihood Approach for Second Order Langevin Models</title><link>http://arxiv.org/abs/2411.08692v1</link><description>The inherent complexity of biological agents often leads to motility behaviorthat appears to have random components. Robust stochastic inference methods aretherefore required to understand and predict the motion patterns from timediscrete trajectory data provided by experiments. In many cases second orderLangevin models are needed to adequately capture the motility. Additionally,population heterogeneity needs to be taken into account when analyzing datafrom several individual organisms. In this work, we describe a maximumlikelihood approach to infer dynamical, stochastic models and, simultaneously,estimate the heterogeneity in a population of motile active particles fromdiscretely sampled, stochastic trajectories. To this end we propose a newmethod to approximate the likelihood for non-linear second order Langevinmodels. We show that this maximum likelihood ansatz outperforms alternativeapproaches especially for short trajectories. Additionally, we demonstrate howa measure of uncertainty for the heterogeneity estimate can be derived. Wethereby pave the way for the systematic, data-driven inference of dynamicalmodels for actively driven entities based on trajectory data, decipheringtemporal fluctuations and inter-particle variability.</description><author>Jan Albrecht, Manfred Opper, Robert Großmann</author><pubDate>Wed, 13 Nov 2024 15:27:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08692v1</guid></item><item><title>Deep Learning for Economists</title><link>http://arxiv.org/abs/2407.15339v3</link><description>Deep learning provides powerful methods to impute structured information fromlarge-scale, unstructured text and image datasets. For example, economistsmight wish to detect the presence of economic activity in satellite images, orto measure the topics or entities mentioned in social media, the congressionalrecord, or firm filings. This review introduces deep neural networks, coveringmethods such as classifiers, regression models, generative AI, and embeddingmodels. Applications include classification, document digitization, recordlinkage, and methods for data exploration in massive scale text and imagecorpora. When suitable methods are used, deep learning models can be cheap totune and can scale affordably to problems involving millions or billions ofdata points.. The review is accompanied by a companion website, EconDL, withuser-friendly demo notebooks, software resources, and a knowledge base thatprovides technical details and additional applications.</description><author>Melissa Dell</author><pubDate>Wed, 13 Nov 2024 15:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15339v3</guid></item><item><title>Measuring similarity between embedding spaces using induced neighborhood graphs</title><link>http://arxiv.org/abs/2411.08687v1</link><description>Deep Learning techniques have excelled at generating embedding spaces thatcapture semantic similarities between items. Often these representations arepaired, enabling experiments with analogies (pairs within the same domain) andcross-modality (pairs across domains). These experiments are based on specificassumptions about the geometry of embedding spaces, which allow finding paireditems by extrapolating the positional relationships between embedding pairs inthe training dataset, allowing for tasks such as finding new analogies, andmultimodal zero-shot classification. In this work, we propose a metric toevaluate the similarity between paired item representations. Our proposal isbuilt from the structural similarity between the nearest-neighbors inducedgraphs of each representation, and can be configured to compare spaces based ondifferent distance metrics and on different neighborhood sizes. We demonstratethat our proposal can be used to identify similar structures at differentscales, which is hard to achieve with kernel methods such as Centered KernelAlignment (CKA). We further illustrate our method with two case studies: ananalogy task using GloVe embeddings, and zero-shot classification in theCIFAR-100 dataset using CLIP embeddings. Our results show that accuracy in bothanalogy and zero-shot classification tasks correlates with the embeddingsimilarity. These findings can help explain performance differences in thesetasks, and may lead to improved design of paired-embedding models in thefuture.</description><author>Tiago F. Tavares, Fabio Ayres, Paris Smaragdis</author><pubDate>Wed, 13 Nov 2024 15:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08687v1</guid></item><item><title>Extending choice assessments to choice functions: An algorithm for computing the natural extension</title><link>http://arxiv.org/abs/2407.21164v2</link><description>We study how to infer new choices from prior choices using the framework ofchoice functions, a unifying mathematical framework for decision-making basedon sets of preference orders. In particular, we define the natural (mostconservative) extension of a given choice assessment to a coherent choicefunction -- whenever possible -- and use this natural extension to make newchoices. We provide a practical algorithm for computing this natural extensionand various ways to improve scalability. Finally, we test these algorithms fordifferent types of choice assessments.</description><author>Arne Decadt, Alexander Erreygers, Jasper De Bock</author><pubDate>Wed, 13 Nov 2024 15:22:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21164v2</guid></item><item><title>Analogical Reasoning Within a Conceptual Hyperspace</title><link>http://arxiv.org/abs/2411.08684v1</link><description>We propose an approach to analogical inference that marries theneuro-symbolic computational power of complex-sampled hyperdimensionalcomputing (HDC) with Conceptual Spaces Theory (CST), a promising theory ofsemantic meaning. CST sketches, at an abstract level, approaches to analogicalinference that go beyond the standard predicate-based structure mappingtheories. But it does not describe how such an approach can be operationalized.We propose a concrete HDC-based architecture that computes several types ofanalogy classified by CST. We present preliminary proof-of-concept experimentalresults within a toy domain and describe how it can perform category-based andproperty-based analogical reasoning.</description><author>Howard Goldowsky, Vasanth Sarathy</author><pubDate>Wed, 13 Nov 2024 15:20:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08684v1</guid></item><item><title>Circuit design in biology and machine learning. I. Random networks and dimensional reduction</title><link>http://arxiv.org/abs/2408.09604v2</link><description>A biological circuit is a neural or biochemical cascade, taking inputs andproducing outputs. How have biological circuits learned to solve environmentalchallenges over the history of life? The answer certainly follows Dobzhansky'sfamous quote that ``nothing in biology makes sense except in the light ofevolution.'' But that quote leaves out the mechanistic basis by which naturalselection's trial-and-error learning happens, which is exactly what we have tounderstand. How does the learning process that designs biological circuitsactually work? How much insight can we gain about the form and function ofbiological circuits by studying the processes that have made those circuits?Because life's circuits must often solve the same problems as those faced bymachine learning, such as environmental tracking, homeostatic control,dimensional reduction, or classification, we can begin by considering howmachine learning designs computational circuits to solve problems. We can thenask: How much insight do those computational circuits provide about the designof biological circuits? How much does biology differ from computers in theparticular circuit designs that it uses to solve problems? This article stepsthrough two classic machine learning models to set the foundation for analyzingbroad questions about the design of biological circuits. One insight is thesurprising power of randomly connected networks. Another is the central role ofinternal models of the environment embedded within biological circuits,illustrated by a model of dimensional reduction and trend prediction. Overall,many challenges in biology have machine learning analogs, suggesting hypothesesabout how biology's circuits are designed.</description><author>Steven A. Frank</author><pubDate>Wed, 13 Nov 2024 15:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09604v2</guid></item><item><title>No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices</title><link>http://arxiv.org/abs/2402.16187v3</link><description>Advances in generative models have made it possible for AI-generated text,code, and images to mirror human-generated content in many applications.Watermarking, a technique that aims to embed information in the output of amodel to verify its source, is useful for mitigating the misuse of suchAI-generated content. However, we show that common design choices in LLMwatermarking schemes make the resulting systems surprisingly susceptible toattack -- leading to fundamental trade-offs in robustness, utility, andusability. To navigate these trade-offs, we rigorously study a set of simpleyet effective attacks on common watermarking systems, and propose guidelinesand defenses for LLM watermarking in practice.</description><author>Qi Pang, Shengyuan Hu, Wenting Zheng, Virginia Smith</author><pubDate>Wed, 13 Nov 2024 15:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16187v3</guid></item><item><title>Reducing ADC Front-end Costs During Training of On-sensor Printed Multilayer Perceptrons</title><link>http://arxiv.org/abs/2411.08674v1</link><description>Printed electronics technology offers a cost-effectiveand fully-customizablesolution to computational needs beyondthe capabilities of traditional silicontechnologies, offering ad-vantages such as on-demand manufacturing andconformal, low-cost hardware. However, the low-resolution fabrication ofprintedelectronics, which results in large feature sizes, poses a challengeforintegrating complex designs like those of machine learn-ing (ML) classificationsystems. Current literature optimizes onlythe Multilayer Perceptron (MLP)circuit within the classificationsystem, while the cost of analog-to-digitalconverters (ADCs)is overlooked. Printed applications frequently requireon-sensorprocessing, yet while the digital classifier has beenextensivelyoptimized, the analog-to-digital interfacing, specifically theADCs,dominates the total area and energy consumption. In this work,we targetdigital printed MLP classifiers and we propose thedesign of customized ADCs perMLP's input which involvesminimizing the distinct represented numbers for eachinput,simplifying thus the ADC's circuitry. Incorporating this ADCoptimizationin the MLP training, enables eliminating ADC levelsand the respectivecomparators, while still maintaining highclassification accuracy. Our approachachieves 11.2x lower ADCarea for less than 5% accuracy drop across varyingMLPs.</description><author>Florentia Afentaki, Paula Carolina Lozano Duarte, Georgios Zervakis, Mehdi B. Tahoori</author><pubDate>Wed, 13 Nov 2024 15:08:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08674v1</guid></item><item><title>Theoretical Analysis of Byte-Pair Encoding</title><link>http://arxiv.org/abs/2411.08671v1</link><description>Byte-Pair Encoding (BPE) is a widely used method for subword tokenization,with origins in grammar-based text compression. It is employed in a variety oflanguage processing tasks such as machine translation or large language model(LLM) pretraining, to create a token dictionary of a prescribed size. Mostevaluations of BPE to date are empirical, and the reasons for its goodpractical performance are not well understood. In this paper we focus on the optimization problem underlying BPE: finding apair encoding that achieves optimal compression utility. We show that thisproblem is APX-complete, indicating that it is unlikely to admit apolynomial-time approximation scheme. This answers, in a stronger form, aquestion recently raised by Zouhar et al. On the positive side, we show that BPE approximates the compression utilityof the optimal pair encoding to a worst-case factor between $0.333$ and$0.625$. Our results aim to explain the ongoing success of BPE and are, to ourknowledge, the first rigorous guarantees on its compression utility that holdfor all inputs.</description><author>László Kozma, Johannes Voderholzer</author><pubDate>Wed, 13 Nov 2024 15:04:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08671v1</guid></item><item><title>A Machine Learning Algorithm for Finite-Horizon Stochastic Control Problems in Economics</title><link>http://arxiv.org/abs/2411.08668v1</link><description>We propose a machine learning algorithm for solving finite-horizon stochasticcontrol problems based on a deep neural network representation of the optimalpolicy functions. The algorithm has three features: (1) It can solvehigh-dimensional (e.g., over 100 dimensions) and finite-horizontime-inhomogeneous stochastic control problems. (2) It has a monotonicity ofperformance improvement in each iteration, leading to good convergenceproperties. (3) It does not rely on the Bellman equation. To demonstrate theefficiency of the algorithm, it is applied to solve various finite-horizontime-inhomogeneous problems including recursive utility optimization under astochastic volatility model, a multi-sector stochastic growth, and optimalcontrol under a dynamic stochastic integration of climate and economy modelwith eight-dimensional state vectors and 600 time periods.</description><author>Xianhua Peng, Steven Kou, Lekang Zhang</author><pubDate>Wed, 13 Nov 2024 15:02:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08668v1</guid></item><item><title>A Survey on Vision Autoregressive Model</title><link>http://arxiv.org/abs/2411.08666v1</link><description>Autoregressive models have demonstrated great performance in natural languageprocessing (NLP) with impressive scalability, adaptability andgeneralizability. Inspired by their notable success in NLP field,autoregressive models have been intensively investigated recently for computervision, which perform next-token predictions by representing visual data asvisual tokens and enables autoregressive modelling for a wide range of visiontasks, ranging from visual generation and visual understanding to the veryrecent multimodal generation that unifies visual generation and understandingwith a single autoregressive model. This paper provides a systematic review ofvision autoregressive models, including the development of a taxonomy ofexisting methods and highlighting their major contributions, strengths, andlimitations, covering various vision tasks such as image generation, videogeneration, image editing, motion generation, medical image analysis, 3Dgeneration, robotic manipulation, unified multimodal generation, etc. Besides,we investigate and analyze the latest advancements in autoregressive models,including thorough benchmarking and discussion of existing methods acrossvarious evaluation datasets. Finally, we outline key challenges and promisingdirections for future research, offering a roadmap to guide furtheradvancements in vision autoregressive models.</description><author>Kai Jiang, Jiaxing Huang</author><pubDate>Wed, 13 Nov 2024 14:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08666v1</guid></item><item><title>OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with Geometric and Semantic Guidances</title><link>http://arxiv.org/abs/2411.08665v1</link><description>OpenStreetMap (OSM), an online and versatile source of volunteered geographicinformation (VGI), is widely used for human self-localization by matchingnearby visual observations with vectorized map data. However, due to thedivergence in modalities and views, image-to-OSM (I2O) matching andlocalization remain challenging for robots, preventing the full utilization ofVGI data in the unmanned ground vehicles and logistic industry. Inspired by thefact that the human brain relies on geometric and semantic understanding ofsensory information for spatial localization tasks, we propose the OSMLoc inthis paper. OSMLoc is a brain-inspired single-image visual localization methodwith semantic and geometric guidance to improve accuracy, robustness, andgeneralization ability. First, we equip the OSMLoc with the visual foundationalmodel to extract powerful image features. Second, a geometry-guided depthdistribution adapter is proposed to bridge the monocular depth estimation andcamera-to-BEV transform. Thirdly, the semantic embeddings from the OSM data areutilized as auxiliary guidance for image-to-OSM feature matching. To validatethe proposed OSMLoc, we collect a worldwide cross-area and cross-condition (CC)benchmark for extensive evaluation. Experiments on the MGL dataset, CCvalidation benchmark, and KITTI dataset have demonstrated the superiority ofour method. Code, pre-trained models, CC validation benchmark, and additionalresults are available on: https://github.com/WHU-USI3DV/OSMLoc</description><author>Youqi Liao, Xieyuanli Chen, Shuhao Kang, Jianping Li, Zhen Dong, Hongchao Fan, Bisheng Yang</author><pubDate>Wed, 13 Nov 2024 14:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08665v1</guid></item></channel></rss>