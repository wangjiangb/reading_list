<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 03 Apr 2024 06:01:01 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Segment Any 3D Object with Language</title><link>http://arxiv.org/abs/2404.02157v1</link><description>In this paper, we investigate Open-Vocabulary 3D Instance Segmentation(OV-3DIS) with free-form language instructions. Earlier works that rely on onlyannotated base categories for training suffer from limited generalization tounseen novel categories. Recent works mitigate poor generalizability to novelcategories by generating class-agnostic masks or projecting generalized masksfrom 2D to 3D, but disregard semantic or geometry information, leading tosub-optimal performance. Instead, generating generalizable but semantic-relatedmasks directly from 3D point clouds would result in superior outcomes. In thispaper, we introduce Segment any 3D Object with LanguagE (SOLE), which is asemantic and geometric-aware visual-language learning framework with stronggeneralizability by generating semantic-related masks directly from 3D pointclouds. Specifically, we propose a multimodal fusion network to incorporatemultimodal semantics in both backbone and decoder. In addition, to align the 3Dsegmentation model with various language instructions and enhance the maskquality, we introduce three types of multimodal associations as supervision.Our SOLE outperforms previous methods by a large margin on ScanNetv2,ScanNet200, and Replica benchmarks, and the results are even close to thefully-supervised counterpart despite the absence of class annotations in thetraining. Furthermore, extensive qualitative results demonstrate theversatility of our SOLE to language instructions.</description><author>Seungjun Lee, Yuyang Zhao, Gim Hee Lee</author><pubDate>Tue, 02 Apr 2024 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02157v1</guid></item><item><title>Alpha Invariance: On Inverse Scaling Between Distance and Volume Density in Neural Radiance Fields</title><link>http://arxiv.org/abs/2404.02155v1</link><description>Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity ofvolumetric densities in neural radiance fields, i.e., the densities double whenscene size is halved, and vice versa. We call this property alpha invariance.For NeRFs to better maintain alpha invariance, we recommend 1) parameterizingboth distance and volume densities in log space, and 2) adiscretization-agnostic initialization strategy to guarantee high raytransmittance. We revisit a few popular radiance field models and find thatthese systems use various heuristics to deal with issues arising from scenescaling. We test their behaviors and show our recipe to be more robust.</description><author>Joshua Ahn, Haochen Wang, Raymond A. Yeh, Greg Shakhnarovich</author><pubDate>Tue, 02 Apr 2024 18:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02155v1</guid></item><item><title>Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration</title><link>http://arxiv.org/abs/2404.02154v1</link><description>All-in-one image restoration tackles different types of degradations with aunified model instead of having task-specific, non-generic models for eachdegradation. The requirement to tackle multiple degradations using the samemodel can lead to high-complexity designs with fixed configuration that lackthe adaptability to more efficient alternatives. We propose DyNet, a dynamicfamily of networks designed in an encoder-decoder style for all-in-one imagerestoration tasks. Our DyNet can seamlessly switch between its bulkier andlightweight variants, thereby offering flexibility for efficient modeldeployment with a single round of training. This seamless switching is enabledby our weights-sharing mechanism, forming the core of our architecture andfacilitating the reuse of initialized module weights. Further, to establishrobust weights initialization, we introduce a dynamic pre-training strategythat trains variants of the proposed DyNet concurrently, thereby achieving a50% reduction in GPU hours. To tackle the unavailability of large-scale datasetrequired in pre-training, we curate a high-quality, high-resolution imagedataset named Million-IRD having 2M image samples. We validate our DyNet forimage denoising, deraining, and dehazing in all-in-one setting, achievingstate-of-the-art results with 31.34% reduction in GFlops and a 56.75% reductionin parameters compared to baseline models. The source codes and trained modelsare available at https://github.com/akshaydudhane16/DyNet.</description><author>Akshay Dudhane, Omkar Thawakar, Syed Waqas Zamir, Salman Khan, Fahad Shahbaz Khan, Ming-Hsuan Yang</author><pubDate>Tue, 02 Apr 2024 18:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02154v1</guid></item><item><title>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image</title><link>http://arxiv.org/abs/2404.02152v1</link><description>Recently, we have witnessed the explosive growth of various volumetricrepresentations in modeling animatable head avatars. However, due to thediversity of frameworks, there is no practical method to support high-levelapplications like 3D head avatar editing across different representations. Inthis paper, we propose a generic avatar editing approach that can beuniversally applied to various 3DMM driving volumetric head avatars. To achievethis goal, we design a novel expression-aware modification generative model,which enables lift 2D editing from a single image to a consistent 3Dmodification field. To ensure the effectiveness of the generative modificationprocess, we develop several techniques, including an expression-dependentmodification distillation scheme to draw knowledge from the large-scale headavatar model and 2D facial texture editing tools, implicit latent spaceguidance to enhance model convergence, and a segmentation-based loss reweightstrategy for fine-grained texture inversion. Extensive experiments demonstratethat our method delivers high-quality and consistent results across multipleexpression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/</description><author>Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</author><pubDate>Tue, 02 Apr 2024 18:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02152v1</guid></item><item><title>Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</title><link>http://arxiv.org/abs/2404.02151v1</link><description>We show that even the most recent safety-aligned LLMs are not robust tosimple adaptive jailbreaking attacks. First, we demonstrate how to successfullyleverage access to logprobs for jailbreaking: we initially design anadversarial prompt template (sometimes adapted to the target LLM), and then weapply random search on a suffix to maximize the target logprob (e.g., of thetoken "Sure"), potentially with multiple restarts. In this way, we achievenearly 100\% attack success rate -- according to GPT-4 as a judge -- onGPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that wasadversarially trained against the GCG attack. We also show how to jailbreak allClaude models -- that do not expose logprobs -- via either a transfer orprefilling attack with 100\% success rate. In addition, we show how to userandom search on a restricted set of tokens for finding trojan strings inpoisoned models -- a task that shares many similarities with jailbreaking --which is the algorithm that brought us the first place in the SaTML'24 TrojanDetection Competition. The common theme behind these attacks is that adaptivityis crucial: different models are vulnerable to different prompting templates(e.g., R2D2 is very sensitive to in-context learning prompts), some models haveunique vulnerabilities based on their APIs (e.g., prefilling for Claude), andin some settings it is crucial to restrict the token search space based onprior knowledge (e.g., for trojan detection). We provide the code, prompts, andlogs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.</description><author>Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion</author><pubDate>Tue, 02 Apr 2024 18:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02151v1</guid></item><item><title>Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models</title><link>http://arxiv.org/abs/2404.02148v1</link><description>Recent advancements in 3D generation are predominantly propelled byimprovements in 3D-aware image diffusion models which are pretrained onInternet-scale image data and fine-tuned on massive 3D data, offering thecapability of producing highly consistent multi-view images. However, due tothe scarcity of synchronized multi-view video data, it is impractical to adaptthis paradigm to 4D generation directly. Despite that, the available video and3D data are adequate for training video and multi-view diffusion models thatcan provide satisfactory dynamic and geometric priors respectively. In thispaper, we present Diffusion$^2$, a novel framework for dynamic 3D contentcreation that leverages the knowledge about geometric consistency and temporalsmoothness from these models to directly sample dense multi-view andmulti-frame images which can be employed to optimize continuous 4Drepresentation. Specifically, we design a simple yet effective denoisingstrategy via score composition of video and multi-view diffusion models basedon the probability structure of the images to be generated. Owing to the highparallelism of the image generation and the efficiency of the modern 4Dreconstruction pipeline, our framework can generate 4D content within fewminutes. Furthermore, our method circumvents the reliance on 4D data, therebyhaving the potential to benefit from the scalability of the foundation videoand multi-view diffusion models. Extensive experiments demonstrate the efficacyof our proposed framework and its capability to flexibly adapt to various typesof prompts.</description><author>Zeyu Yang, Zijie Pan, Chun Gu, Li Zhang</author><pubDate>Tue, 02 Apr 2024 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02148v1</guid></item><item><title>Iterated Learning Improves Compositionality in Large Vision-Language Models</title><link>http://arxiv.org/abs/2404.02145v1</link><description>A fundamental characteristic common to both human vision and natural languageis their compositional nature. Yet, despite the performance gains contributedby large vision and language pretraining, recent investigations find thatmost-if not all-our state-of-the-art vision-language models struggle atcompositionality. They are unable to distinguish between images of " a girl inwhite facing a man in black" and "a girl in black facing a man in white".Moreover, prior work suggests that compositionality doesn't arise with scale:larger model sizes or training data don't help. This paper develops a newiterated training algorithm that incentivizes compositionality. We draw ondecades of cognitive science research that identifies cultural transmission-theneed to teach a new generation-as a necessary inductive prior that incentivizeshumans to develop compositional languages. Specifically, we reframevision-language contrastive learning as the Lewis Signaling Game between avision agent and a language agent, and operationalize cultural transmission byiteratively resetting one of the agent's weights during training. After everyiteration, this training paradigm induces representations that become "easierto learn", a property of compositional languages: e.g. our model trained onCC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in theSugarCrepe benchmark.</description><author>Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, Ranjay Krishna</author><pubDate>Tue, 02 Apr 2024 18:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02145v1</guid></item><item><title>HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models</title><link>http://arxiv.org/abs/2312.05209v2</link><description>Recent progress in generative AI, including large language models (LLMs) likeChatGPT, has opened up significant opportunities in fields ranging from naturallanguage processing to knowledge discovery and data mining. However, there isalso a growing awareness that the models can be prone to problems such asmaking information up or `hallucinations', and faulty reasoning on seeminglysimple problems. Because of the popularity of models like ChatGPT, bothacademic scholars and citizen scientists have documented hallucinations ofseveral different types and severity. Despite this body of work, a formal modelfor describing and representing these hallucinations (with relevant meta-data)at a fine-grained level, is still lacking. In this paper, we address this gapby presenting the Hallucination Ontology or HALO, a formal, extensible ontologywritten in OWL that currently offers support for six different types ofhallucinations known to arise in LLMs, along with support for provenance andexperimental metadata. We also collect and publish a dataset containinghallucinations that we inductively gathered across multiple independent Websources, and show that HALO can be successfully used to model this dataset andanswer competency questions.</description><author>Navapat Nananukul, Mayank Kejriwal</author><pubDate>Tue, 02 Apr 2024 18:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05209v2</guid></item><item><title>Robustly estimating heterogeneity in factorial data using Rashomon Partitions</title><link>http://arxiv.org/abs/2404.02141v1</link><description>Many statistical analyses, in both observational data and randomized controltrials, ask: how does the outcome of interest vary with combinations ofobservable covariates? How do various drug combinations affect health outcomes,or how does technology adoption depend on incentives and demographics? Our goalis to partition this factorial space into ``pools'' of covariate combinationswhere the outcome differs across the pools (but not within a pool). Existingapproaches (i) search for a single ``optimal'' partition under assumptionsabout the association between covariates or (ii) sample from the entire set ofpossible partitions. Both these approaches ignore the reality that, especiallywith correlation structure in covariates, many ways to partition the covariatespace may be statistically indistinguishable, despite very differentimplications for policy or science. We develop an alternative perspective,called Rashomon Partition Sets (RPSs). Each item in the RPS partitions thespace of covariates using a tree-like geometry. RPSs incorporate all partitionsthat have posterior values near the maximum a posteriori partition, even ifthey offer substantively different explanations, and do so using a prior thatmakes no assumptions about associations between covariates. This prior is the$\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculatethe posterior of any measurable function of the feature effects vector onoutcomes, conditional on being in the RPS. We also characterize approximationerror relative to the entire posterior and provide bounds on the size of theRPS. Simulations demonstrate this framework allows for robust conclusionsrelative to conventional regularization techniques. We apply our method tothree empirical settings: price effects on charitable giving, chromosomalstructure (telomere length), and the introduction of microfinance.</description><author>Aparajithan Venkateswaran, Anirudh Sankar, Arun G. Chandrasekhar, Tyler H. McCormick</author><pubDate>Tue, 02 Apr 2024 18:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02141v1</guid></item><item><title>Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding</title><link>http://arxiv.org/abs/2309.15028v3</link><description>Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) mayseem unnecessary when generating natural language text based onstate-of-the-art reinforcement learning such as Proximal Policy Optimization(PPO). In this paper, we demonstrate that it is possible to get extra mileageout of PPO by integrating MCTS on top. The key idea is not to throw out thevalue network, a byproduct of PPO training for evaluating partial outputsequences, when decoding text out of the policy network. More concretely, wepresent a novel value-guided decoding algorithm called PPO-MCTS, which canintegrate the value network from PPO to work closely with the policy networkduring inference-time generation. Compared to prior approaches based on MCTSfor controlled text generation, the key strength of our approach is to reducethe fundamental mismatch of the scoring mechanisms of the partial outputsbetween training and test. Evaluation on four text generation tasks demonstratethat PPO-MCTS greatly improves the preferability of generated text compared tothe standard practice of using only the PPO policy. Our results demonstrate thepromise of search algorithms even on top of the aligned language models fromPPO, and the under-explored benefit of the value network.</description><author>Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz</author><pubDate>Tue, 02 Apr 2024 18:51:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15028v3</guid></item><item><title>Topic-based Watermarks for LLM-Generated Text</title><link>http://arxiv.org/abs/2404.02138v1</link><description>Recent advancements of large language models (LLMs) have resulted inindistinguishable text outputs comparable to human-generated text. Watermarkingalgorithms are potential tools that offer a way to differentiate between LLM-and human-generated text by embedding detectable signatures withinLLM-generated output. However, current watermarking schemes lack robustnessagainst known attacks against watermarking algorithms. In addition, they areimpractical considering an LLM generates tens of thousands of text outputs perday and the watermarking algorithm needs to memorize each output it generatesfor the detection to work. In this work, focusing on the limitations of currentwatermarking schemes, we propose the concept of a "topic-based watermarkingalgorithm" for LLMs. The proposed algorithm determines how to generate tokensfor the watermarked LLM output based on extracted topics of an input prompt orthe output of a non-watermarked LLM. Inspired from previous work, we proposeusing a pair of lists (that are generated based on the specified extractedtopic(s)) that specify certain tokens to be included or excluded whilegenerating the watermarked output of the LLM. Using the proposed watermarkingalgorithm, we show the practicality of a watermark detection algorithm.Furthermore, we discuss a wide range of attacks that can emerge againstwatermarking algorithms for LLMs and the benefit of the proposed watermarkingscheme for the feasibility of modeling a potential attacker considering itsbenefit vs. loss.</description><author>Alexander Nemecek, Yuzhou Jiang, Erman Ayday</author><pubDate>Tue, 02 Apr 2024 18:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02138v1</guid></item><item><title>Neural Embedding Compression For Efficient Multi-Task Earth Observation Modelling</title><link>http://arxiv.org/abs/2403.17886v2</link><description>As repositories of large scale data in earth observation (EO) have grown, sohave transfer and storage costs for model training and inference, expendingsignificant resources. We introduce Neural Embedding Compression (NEC), basedon the transfer of compressed embeddings to data consumers instead of raw data.We adapt foundation models (FM) through learned neural compression to generatemulti-task embeddings while navigating the tradeoff between compression rateand embedding utility. We update only a small fraction of the FM parameters(10%) for a short training period (1% of the iterations of pre-training). Weevaluate NEC on two EO tasks: scene classification and semantic segmentation.Compared with applying traditional compression to the raw data, NEC achievessimilar accuracy with a 75% to 90% reduction in data. Even at 99.7%compression, performance drops by only 5% on the scene classification task.Overall, NEC is a data-efficient yet performant approach for multi-task EOmodelling.</description><author>Carlos Gomes, Thomas Brunschwiler</author><pubDate>Tue, 02 Apr 2024 18:48:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17886v2</guid></item><item><title>ResNet with Integrated Convolutional Block Attention Module for Ship Classification Using Transfer Learning on Optical Satellite Imagery</title><link>http://arxiv.org/abs/2404.02135v1</link><description>This study proposes a novel transfer learning framework for effective shipclassification using high-resolution optical remote sensing satellite imagery.The framework is based on the deep convolutional neural network model ResNet50and incorporates the Convolutional Block Attention Module (CBAM) to enhanceperformance. CBAM enables the model to attend to salient features in theimages, allowing it to better discriminate between subtle differences betweenships and backgrounds. Furthermore, this study adopts a transfer learningapproach tailored for accurately classifying diverse types of ships byfine-tuning a pre-trained model for the specific task. Experimental resultsdemonstrate the efficacy of the proposed framework in ship classification usingoptical remote sensing imagery, achieving a high classification accuracy of 94%across 5 classes, outperforming existing methods. This research holds potentialapplications in maritime surveillance and management, illegal fishingdetection, and maritime traffic monitoring.</description><author>Ryan Donghan Kwon, Gangjoo Robin Nam, Jisoo Tak, Yeom Hyeok, Junseob Shin, Hyerin Cha, Kim Soo Bin</author><pubDate>Tue, 02 Apr 2024 18:48:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02135v1</guid></item><item><title>ViTamin: Designing Scalable Vision Models in the Vision-Language Era</title><link>http://arxiv.org/abs/2404.02132v1</link><description>Recent breakthroughs in vision-language models (VLMs) start a new page in thevision community. The VLMs provide stronger and more generalizable featureembeddings compared to those from ImageNet-pretrained models, thanks to thetraining on the large-scale Internet image-text pairs. However, despite theamazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remainthe default choice for the image encoder. Although pure transformer proves itseffectiveness in the text encoding area, it remains questionable whether it isalso the case for image encoding, especially considering that various types ofnetworks are proposed on the ImageNet benchmark, which, unfortunately, arerarely studied in VLMs. Due to small data/model scale, the original conclusionsof model design on ImageNet can be limited and biased. In this paper, we aim atbuilding an evaluation protocol of vision models in the vision-language eraunder the contrastive language-image pretraining (CLIP) framework. We provide acomprehensive way to benchmark different vision models, covering theirzero-shot performance and scalability in both model and training data sizes. Tothis end, we introduce ViTamin, a new vision models tailored for VLMs.ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy,when using the same publicly available DataComp-1B dataset and the sameOpenCLIP training scheme. ViTamin-L presents promising results on 60 diversebenchmarks, including classification, retrieval, open-vocabulary detection andsegmentation, and large multi-modal models. When further scaling up the modelsize, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shotaccuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters(4.4B).</description><author>Jienneg Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen</author><pubDate>Tue, 02 Apr 2024 18:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02132v1</guid></item><item><title>Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion</title><link>http://arxiv.org/abs/2308.12469v3</link><description>Producing quality segmentation masks for images is a fundamental problem incomputer vision. Recent research has explored large-scale supervised trainingto enable zero-shot segmentation on virtually any image style and unsupervisedtraining to enable segmentation without dense annotations. However,constructing a model capable of segmenting anything in a zero-shot mannerwithout any annotations is still challenging. In this paper, we propose toutilize the self-attention layers in stable diffusion models to achieve thisgoal because the pre-trained stable diffusion model has learned inherentconcepts of objects within its attention layers. Specifically, we introduce asimple yet effective iterative merging process based on measuring KL divergenceamong attention maps to merge them into valid segmentation masks. The proposedmethod does not require any training or language dependency to extract qualitysegmentation for any images. On COCO-Stuff-27, our method surpasses the priorunsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%in mean IoU. The project page is at\url{https://sites.google.com/view/diffseg/home}.</description><author>Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, Mar Gonzalez-Franco</author><pubDate>Tue, 02 Apr 2024 18:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12469v3</guid></item><item><title>Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models</title><link>http://arxiv.org/abs/2404.00462v2</link><description>A world model creates a surrogate world to train a controller and predictsafety violations by learning the internal dynamic model of systems. However,the existing world models rely solely on statistical learning of howobservations change in response to actions, lacking precise quantification ofhow accurate the surrogate dynamics are, which poses a significant challenge insafety-critical systems. To address this challenge, we propose foundation worldmodels that embed observations into meaningful and causally latentrepresentations. This enables the surrogate dynamics to directly predict causalfuture states by leveraging a training-free large language model. In two commonbenchmarks, this novel model outperforms standard world models in the safetyprediction task and has a performance comparable to supervised learning despitenot using any data. We evaluate its performance with a more specialized andsystem-relevant metric by comparing estimated states instead of aggregatingobservation-wide error.</description><author>Zhenjiang Mao, Siqi Dai, Yuang Geng, Ivan Ruchkin</author><pubDate>Tue, 02 Apr 2024 18:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00462v2</guid></item><item><title>Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models</title><link>http://arxiv.org/abs/2310.05861v2</link><description>An increasing number of vision-language tasks can be handled with little tono training, i.e., in a zero and few-shot manner, by marrying large languagemodels (LLMs) to vision encoders, resulting in large vision-language models(LVLMs). While this has huge upsides, such as not requiring training data orcustom architectures, how an input is presented to an LVLM can have a majorimpact on zero-shot model performance. In particular, inputs phrased in anunderspecified way can result in incorrect answers due to factors like missingvisual information, complex implicit reasoning, or linguistic ambiguity.Therefore, adding visually-grounded information to the input as a preemptiveclarification should improve model performance by reducing underspecification,e.g., by localizing objects and disambiguating references. Similarly, in theVQA setting, changing the way questions are framed can make them easier formodels to answer. To this end, we present Rephrase, Augment and Reason(RepARe), a gradient-free framework that extracts salient details about theimage using the underlying LVLM as a captioner and reasoner, in order topropose modifications to the original question. We then use the LVLM'sconfidence over a generated answer as an unsupervised scoring function toselect the rephrased question most likely to improve zero-shot performance.Focusing on three visual question answering tasks, we show that RepARe canresult in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%,and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, wefind that using gold answers for oracle question candidate selection achieves asubstantial gain in VQA accuracy by up to 14.41%. Through extensive analysis,we demonstrate that outputs from RepARe increase syntactic complexity, andeffectively utilize vision-language interaction and the frozen LLM.</description><author>Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Tue, 02 Apr 2024 18:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05861v2</guid></item><item><title>FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning</title><link>http://arxiv.org/abs/2404.02127v1</link><description>Instruction tuning is an important step in making language models useful fordirect user interaction. However, many legal tasks remain out of reach for mostopen LLMs and there do not yet exist any large scale instruction datasets forthe domain. This critically limits research in this application area. In thiswork, we curate LawInstruct, a large legal instruction dataset, covering 17jurisdictions, 24 languages and a total of 12M examples. We present evidencethat domain-specific pretraining and instruction tuning improve performance onLegalBench, including improving Flan-T5 XL by 8 points or 16\% over thebaseline. However, the effect does not generalize across all tasks, trainingregimes, model sizes, and other factors. LawInstruct is a resource foraccelerating the development of models with stronger information processing anddecision making capabilities in the legal domain.</description><author>Joel Niklaus, Lucia Zheng, Arya D. McCarthy, Christopher Hahn, Brian M. Rosen, Peter Henderson, Daniel E. Ho, Garrett Honke, Percy Liang, Christopher Manning</author><pubDate>Tue, 02 Apr 2024 18:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02127v1</guid></item><item><title>Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity</title><link>http://arxiv.org/abs/2404.02126v1</link><description>Knowledge graphs play a pivotal role in various applications, such asquestion-answering and fact-checking. Abstract Meaning Representation (AMR)represents text as knowledge graphs. Evaluating the quality of these graphsinvolves matching them structurally to each other and semantically to thesource text. Existing AMR metrics are inefficient and struggle to capturesemantic similarity. We also lack a systematic evaluation benchmark forassessing structural similarity between AMR graphs. To overcome theselimitations, we introduce a novel AMR similarity metric, rematch, alongside anew evaluation for structural similarity called RARE. Among state-of-the-artmetrics, rematch ranks second in structural similarity; and first in semanticsimilarity by 1--5 percentage points on the STS-B and SICK-R benchmarks.Rematch is also five times faster than the next most efficient metric.</description><author>Zoher Kachwala, Jisun An, Haewoon Kwak, Filippo Menczer</author><pubDate>Tue, 02 Apr 2024 18:33:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02126v1</guid></item><item><title>3D Congealing: 3D-Aware Image Alignment in the Wild</title><link>http://arxiv.org/abs/2404.02125v1</link><description>We propose 3D Congealing, a novel problem of 3D-aware alignment for 2D imagescapturing semantically similar objects. Given a collection of unlabeledInternet images, our goal is to associate the shared semantic parts from theinputs and aggregate the knowledge from 2D images to a shared 3D canonicalspace. We introduce a general framework that tackles the task without assumingshape templates, poses, or any camera parameters. At its core is a canonical 3Drepresentation that encapsulates geometric and semantic information. Theframework optimizes for the canonical representation together with the pose foreach input image, and a per-image coordinate map that warps 2D pixelcoordinates to the 3D canonical frame to account for the shape matching. Theoptimization procedure fuses prior knowledge from a pre-trained imagegenerative model and semantic information from input images. The formerprovides strong knowledge guidance for this under-constraint task, while thelatter provides the necessary information to mitigate the training data biasfrom the pre-trained model. Our framework can be used for various tasks such ascorrespondence matching, pose estimation, and image editing, achieving strongresults on real-world image datasets under challenging illumination conditionsand on in-the-wild online image collections.</description><author>Yunzhi Zhang, Zizhang Li, Amit Raj, Andreas Engelhardt, Yuanzhen Li, Tingbo Hou, Jiajun Wu, Varun Jampani</author><pubDate>Tue, 02 Apr 2024 18:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02125v1</guid></item><item><title>Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models</title><link>http://arxiv.org/abs/2404.02124v1</link><description>Multiple-choice questions (MCQs) are ubiquitous in almost all levels ofeducation since they are easy to administer, grade, and are a reliable formatin assessments and practices. One of the most important aspects of MCQs is thedistractors, i.e., incorrect options that are designed to target common errorsor misconceptions among real students. To date, the task of craftinghigh-quality distractors largely remains a labor and time-intensive process forteachers and learning content designers, which has limited scalability. In thiswork, we study the task of automated distractor generation in the domain ofmath MCQs and explore a wide variety of large language model (LLM)-basedapproaches, from in-context learning to fine-tuning. We conduct extensiveexperiments using a real-world math MCQ dataset and find that although LLMs cangenerate some mathematically valid distractors, they are less adept atanticipating common errors or misconceptions among real students.</description><author>Wanyong Feng, Jaewook Lee, Hunter McNichols, Alexander Scarlatos, Digory Smith, Simon Woodhead, Nancy Otero Ornelas, Andrew Lan</author><pubDate>Tue, 02 Apr 2024 18:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02124v1</guid></item><item><title>VC dimension of Graph Neural Networks with Pfaffian activation functions</title><link>http://arxiv.org/abs/2401.12362v2</link><description>Graph Neural Networks (GNNs) have emerged in recent years as a powerful toolto learn tasks across a wide range of graph domains in a data-driven fashion;based on a message passing mechanism, GNNs have gained increasing popularitydue to their intuitive formulation, closely linked with the Weisfeiler-Lehman(WL) test for graph isomorphism, to which they have proven equivalent. From atheoretical point of view, GNNs have been shown to be universal approximators,and their generalization capability (namely, bounds on the Vapnik Chervonekis(VC) dimension) has recently been investigated for GNNs with piecewisepolynomial activation functions. The aim of our work is to extend this analysison the VC dimension of GNNs to other commonly used activation functions, suchas sigmoid and hyperbolic tangent, using the framework of Pfaffian functiontheory. Bounds are provided with respect to architecture parameters (depth,number of neurons, input size) as well as with respect to the number of colorsresulting from the 1-WL test applied on the graph domain. The theoreticalanalysis is supported by a preliminary experimental study.</description><author>Giuseppe Alessio D'Inverno, Monica Bianchini, Franco Scarselli</author><pubDate>Tue, 02 Apr 2024 18:30:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12362v2</guid></item><item><title>UINav: A Practical Approach to Train On-Device Automation Agents</title><link>http://arxiv.org/abs/2312.10170v2</link><description>Automation systems that can autonomously drive application user interfaces tocomplete user tasks are of great benefit, especially when users aresituationally or permanently impaired. Prior automation systems do not producegeneralizable models while AI-based automation agents work reliably only insimple, hand-crafted applications or incur high computation costs. We proposeUINav, a demonstration-based approach to train automation agents that fitmobile devices, yet achieving high success rates with modest numbers ofdemonstrations. To reduce the demonstration overhead, UINav uses a refereemodel that provides users with immediate feedback on tasks where the agentfails, and automatically augments human demonstrations to increase diversity intraining data. Our evaluation shows that with only 10 demonstrations UINav canachieve 70% accuracy, and that with enough demonstrations it can surpass 90%accuracy.</description><author>Wei Li, Fu-Lin Hsu, Will Bishop, Folawiyo Campbell-Ajala, Max Lin, Oriana Riva</author><pubDate>Tue, 02 Apr 2024 18:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10170v2</guid></item><item><title>Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners</title><link>http://arxiv.org/abs/2404.02117v1</link><description>Few-Shot Class Incremental Learning (FSCIL) is a task that requires a modelto learn new classes incrementally without forgetting when only a few samplesfor each class are given. FSCIL encounters two significant challenges:catastrophic forgetting and overfitting, and these challenges have driven priorstudies to primarily rely on shallow models, such as ResNet-18. Even thoughtheir limited capacity can mitigate both forgetting and overfitting issues, itleads to inadequate knowledge transfer during few-shot incremental sessions. Inthis paper, we argue that large models such as vision and language transformerspre-trained on large datasets can be excellent few-shot incremental learners.To this end, we propose a novel FSCIL framework called PriViLege, Pre-trainedVision and Language transformers with prompting functions and knowledgedistillation. Our framework effectively addresses the challenges ofcatastrophic forgetting and overfitting in large models through new pre-trainedknowledge tuning (PKT) and two losses: entropy-based divergence loss andsemantic knowledge distillation loss. Experimental results show that theproposed PriViLege significantly outperforms the existing state-of-the-artmethods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and+13.36% in miniImageNet. Our implementation code is available athttps://github.com/KHU-AGI/PriViLege.</description><author>Keon-Hee Park, Kyungwoo Song, Gyeong-Moon Park</author><pubDate>Tue, 02 Apr 2024 18:23:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02117v1</guid></item><item><title>pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction</title><link>http://arxiv.org/abs/2312.12337v3</link><description>We introduce pixelSplat, a feed-forward model that learns to reconstruct 3Dradiance fields parameterized by 3D Gaussian primitives from pairs of images.Our model features real-time and memory-efficient rendering for scalabletraining as well as fast 3D reconstruction at inference time. To overcome localminima inherent to sparse and locally supported representations, we predict adense probability distribution over 3D and sample Gaussian means from thatprobability distribution. We make this sampling operation differentiable via areparameterization trick, allowing us to back-propagate gradients through theGaussian splatting representation. We benchmark our method on wide-baselinenovel view synthesis on the real-world RealEstate10k and ACID datasets, wherewe outperform state-of-the-art light field transformers and acceleraterendering by 2.5 orders of magnitude while reconstructing an interpretable andeditable 3D radiance field.</description><author>David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann</author><pubDate>Tue, 02 Apr 2024 18:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12337v3</guid></item><item><title>GINopic: Topic Modeling with Graph Isomorphism Network</title><link>http://arxiv.org/abs/2404.02115v1</link><description>Topic modeling is a widely used approach for analyzing and exploring largedocument collections. Recent research efforts have incorporated pre-trainedcontextualized language models, such as BERT embeddings, into topic modeling.However, they often neglect the intrinsic informational value conveyed bymutual dependencies between words. In this study, we introduce GINopic, a topicmodeling framework based on graph isomorphism networks to capture thecorrelation between words. By conducting intrinsic (quantitative as well asqualitative) and extrinsic evaluations on diverse benchmark datasets, wedemonstrate the effectiveness of GINopic compared to existing topic models andhighlight its potential for advancing topic modeling.</description><author>Suman Adhya, Debarshi Kumar Sanyal</author><pubDate>Tue, 02 Apr 2024 18:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02115v1</guid></item><item><title>Deployable Reinforcement Learning with Variable Control Rate</title><link>http://arxiv.org/abs/2401.09286v2</link><description>Deploying controllers trained with Reinforcement Learning (RL) on real robotscan be challenging: RL relies on agents' policies being modeled as MarkovDecision Processes (MDPs), which assume an inherently discrete passage of time.The use of MDPs results in that nearly all RL-based control systems employ afixed-rate control strategy with a period (or time step) typically chosen basedon the developer's experience or specific characteristics of the applicationenvironment. Unfortunately, the system should be controlled at the highest,worst-case frequency to ensure stability, which can demand significantcomputational and energy resources and hinder the deployability of thecontroller on onboard hardware. Adhering to the principles of reactiveprogramming, we surmise that applying control actions only when necessaryenables the use of simpler hardware and helps reduce energy consumption. Wechallenge the fixed frequency assumption by proposing a variant of RL withvariable control rate. In this approach, the policy decides the action theagent should take as well as the duration of the time step associated with thataction. In our new setting, we expand Soft Actor-Critic (SAC) to compute theoptimal policy with a variable control rate, introducing the Soft ElasticActor-Critic (SEAC) algorithm. We show the efficacy of SEAC through aproof-of-concept simulation driving an agent with Newtonian kinematics. Ourexperiments show higher average returns, shorter task completion times, andreduced computational resources when compared to fixed rate policies.</description><author>Dong Wang, Giovanni Beltrame</author><pubDate>Tue, 02 Apr 2024 18:18:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09286v2</guid></item><item><title>More Samples or More Prompts? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering</title><link>http://arxiv.org/abs/2311.09782v2</link><description>While most existing works on LLM prompting techniques focus only on how toselect a better set of data samples inside one single prompt input (In-ContextLearning or ICL), why can not we design and leverage multiple prompts togetherto further improve the LLM's performance? In this work, we propose In-ContextSampling (ICS), a low-resource LLM prompting technique to produce confidentpredictions by optimizing the construction of multiple ICL prompt inputs.Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, andMixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI)and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhanceLLMs' performance. An in-depth evaluation with three data similarity-based ICSstrategies suggests that these strategies can further elevate LLM'sperformance, which sheds light on a new yet promising future researchdirection.</description><author>Bingsheng Yao, Guiming Chen, Ruishi Zou, Yuxuan Lu, Jiachen Li, Shao Zhang, Yisi Sang, Sijia Liu, James Hendler, Dakuo Wang</author><pubDate>Tue, 02 Apr 2024 18:16:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09782v2</guid></item><item><title>Measuring and Controlling Instruction (In)Stability in Language Model Dialogs</title><link>http://arxiv.org/abs/2402.10962v2</link><description>System-prompting is a standard tool for customizing language-model chatbots,enabling them to follow a specific instruction. An implicit assumption in theuse of system prompts is that they will be stable, so the chatbot will continueto generate text according to the stipulated instructions for the duration of aconversation. We propose a quantitative benchmark to test this assumption,evaluating instruction stability via self-chats between two instructedchatbots. Testing popular models like LLaMA2-chat-70B and GPT-3.5, we reveal asignificant instruction drift within eight rounds of conversations. Anempirical and theoretical analysis of this phenomenon suggests the transformerattention mechanism plays a role, due to attention decay over long exchanges.To combat attention decay and instruction drift, we propose a lightweightmethod called split-softmax, which compares favorably against two strongbaselines.</description><author>Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg</author><pubDate>Tue, 02 Apr 2024 18:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10962v2</guid></item><item><title>Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL</title><link>http://arxiv.org/abs/2404.02113v1</link><description>In continual or lifelong reinforcement learning access to the environmentshould be limited. If we aspire to design algorithms that can run forlong-periods of time, continually adapting to new, unexpected situations thenwe must be willing to deploy our agents without tuning their hyperparametersover the agent's entire lifetime. The standard practice in deep RL -- and evencontinual RL -- is to assume unfettered access to deployment environment forthe full lifetime of the agent. This paper explores the notion that progress inlifelong RL research has been held back by inappropriate empiricalmethodologies. In this paper we propose a new approach for tuning andevaluating lifelong RL agents where only one percent of the experiment data canbe used for hyperparameter tuning. We then conduct an empirical study of DQNand Soft Actor Critic across a variety of continuing and non-stationarydomains. We find both methods generally perform poorly when restricted toone-percent tuning, whereas several algorithmic mitigations designed tomaintain network plasticity perform surprising well. In addition, we find thatproperties designed to measure the network's ability to learn continuallyindeed correlate with performance under one-percent tuning.</description><author>Golnaz Mesbahi, Olya Mastikhina, Parham Mohammad Panahi, Martha White, Adam White</author><pubDate>Tue, 02 Apr 2024 18:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02113v1</guid></item><item><title>ImageNot: A contrast with ImageNet preserves model rankings</title><link>http://arxiv.org/abs/2404.02112v1</link><description>We introduce ImageNot, a dataset designed to match the scale of ImageNetwhile differing drastically in other aspects. We show that key modelarchitectures developed for ImageNet over the years rank identically whentrained and evaluated on ImageNot to how they rank on ImageNet. This is truewhen training models from scratch or fine-tuning them. Moreover, the relativeimprovements of each model over earlier models strongly correlate in bothdatasets. We further give evidence that ImageNot has a similar utility asImageNet for transfer learning purposes. Our work demonstrates a surprisingdegree of external validity in the relative performance of image classificationmodels. This stands in contrast with absolute accuracy numbers that typicallydrop sharply even under small changes to a dataset.</description><author>Olawale Salaudeen, Moritz Hardt</author><pubDate>Tue, 02 Apr 2024 18:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02112v1</guid></item><item><title>MedMamba: Vision Mamba for Medical Image Classification</title><link>http://arxiv.org/abs/2403.03849v3</link><description>Medical image classification is a very fundamental and crucial task in thefield of computer vision. These years, CNN-based and Transformer-based modelshave been widely used to classify various medical images. Unfortunately, Thelimitation of CNNs in long-range modeling capabilities prevents them fromeffectively extracting features in medical images, while Transformers arehampered by their quadratic computational complexity. Recent research has shownthat the state space model (SSM) represented by Mamba can efficiently modellong-range interactions while maintaining linear computational complexity.Inspired by this, we propose Vision Mamba for medical image classification(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSMcombines the local feature extraction ability of convolutional layers with theability of SSM to capture long-range dependency, thereby modeling medicalimages with different modalities. To demonstrate the potential of MedMamba, weconducted extensive experiments using 14 publicly available medical datasetswith different imaging techniques and two private datasets built by ourselves.Extensive experimental results demonstrate that the proposed MedMamba performswell in detecting lesions in various medical images. To the best of ourknowledge, this is the first Vision Mamba tailored for medical imageclassification. The purpose of this work is to establish a new baseline formedical image classification tasks and provide valuable insights for the futuredevelopment of more efficient and effective SSM-based artificial intelligencealgorithms and application systems in the medical. Source code has beenavailable at https://github.com/YubiaoYue/MedMamba.</description><author>Yubiao Yue, Zhenzhang Li</author><pubDate>Tue, 02 Apr 2024 18:11:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03849v3</guid></item><item><title>GDA: Generalized Diffusion for Robust Test-time Adaptation</title><link>http://arxiv.org/abs/2404.00095v2</link><description>Machine learning models struggle with generalization when encounteringout-of-distribution (OOD) samples with unexpected distribution shifts. Forvision tasks, recent studies have shown that test-time adaptation employingdiffusion models can achieve state-of-the-art accuracy improvements on OODsamples by generating new samples that align with the model's domain withoutthe need to modify the model's weights. Unfortunately, those studies haveprimarily focused on pixel-level corruptions, thereby lacking thegeneralization to adapt to a broader range of OOD types. We introduceGeneralized Diffusion Adaptation (GDA), a novel diffusion-based test-timeadaptation method robust against diverse OOD types. Specifically, GDAiteratively guides the diffusion by applying a marginal entropy loss derivedfrom the model, in conjunction with style and content preservation lossesduring the reverse sampling process. In other words, GDA considers the model'soutput behavior with the semantic information of the samples as a whole, whichcan reduce ambiguity in downstream tasks during the generation process.Evaluation across various popular model architectures and OOD benchmarks showsthat GDA consistently outperforms prior work on diffusion-driven adaptation.Notably, it achieves the highest classification accuracy improvements, rangingfrom 4.4\% to 5.02\% on ImageNet-C and 2.5\% to 7.4\% on Rendition, Sketch, andStylized benchmarks. This performance highlights GDA's generalization to abroader range of OOD benchmarks.</description><author>Yun-Yun Tsai, Fu-Chen Chen, Albert Y. C. Chen, Junfeng Yang, Che-Chun Su, Min Sun, Cheng-Hao Kuo</author><pubDate>Tue, 02 Apr 2024 18:08:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00095v2</guid></item><item><title>Variance-Reduced Policy Gradient Approaches for Infinite Horizon Average Reward Markov Decision Processes</title><link>http://arxiv.org/abs/2404.02108v1</link><description>We present two Policy Gradient-based methods with general parameterization inthe context of infinite horizon average reward Markov Decision Processes. Thefirst approach employs Implicit Gradient Transport for variance reduction,ensuring an expected regret of the order $\tilde{\mathcal{O}}(T^{3/5})$. Thesecond approach, rooted in Hessian-based techniques, ensures an expected regretof the order $\tilde{\mathcal{O}}(\sqrt{T})$. These results significantlyimprove the state of the art of the problem, which achieves a regret of$\tilde{\mathcal{O}}(T^{3/4})$.</description><author>Swetha Ganesh, Washim Uddin Mondal, Vaneet Aggarwal</author><pubDate>Tue, 02 Apr 2024 18:08:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02108v1</guid></item><item><title>Neural Ordinary Differential Equation based Sequential Image Registration for Dynamic Characterization</title><link>http://arxiv.org/abs/2404.02106v1</link><description>Deformable image registration (DIR) is crucial in medical image analysis,enabling the exploration of biological dynamics such as organ motions andlongitudinal changes in imaging. Leveraging Neural Ordinary DifferentialEquations (ODE) for registration, this extension work discusses how thisframework can aid in the characterization of sequential biological processes.Utilizing the Neural ODE's ability to model state derivatives with neuralnetworks, our Neural Ordinary Differential Equation Optimization-based (NODEO)framework considers voxels as particles within a dynamic system, definingdeformation fields through the integration of neural differential equations.This method learns dynamics directly from data, bypassing the need for physicalpriors, making it exceptionally suitable for medical scenarios where suchpriors are unavailable or inapplicable. Consequently, the framework can discernunderlying dynamics and use sequence data to regularize the transformationtrajectory. We evaluated our framework on two clinical datasets: one forcardiac motion tracking and another for longitudinal brain MRI analysis.Demonstrating its efficacy in both 2D and 3D imaging scenarios, our frameworkoffers flexibility and model agnosticism, capable of managing image sequencesand facilitating label propagation throughout these sequences. This studyprovides a comprehensive understanding of how the Neural ODE-based frameworkuniquely benefits the image registration challenge.</description><author>Yifan Wu, Mengjin Dong, Rohit Jena, Chen Qin, James C. Gee</author><pubDate>Tue, 02 Apr 2024 18:04:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02106v1</guid></item><item><title>Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation</title><link>http://arxiv.org/abs/2403.18360v2</link><description>Most domain adaptation (DA) methods are based on either a convolutionalneural networks (CNNs) or a vision transformers (ViTs). They align thedistribution differences between domains as encoders without considering theirunique characteristics. For instance, ViT excels in accuracy due to itssuperior ability to capture global representations, while CNN has an advantagein capturing local representations. This fact has led us to design a hybridmethod to fully take advantage of both ViT and CNN, called ExplicitlyClass-specific Boundaries (ECB). ECB learns CNN on ViT to combine theirdistinct strengths. In particular, we leverage ViT's properties to explicitlyfind class-specific decision boundaries by maximizing the discrepancy betweenthe outputs of the two classifiers to detect target samples far from the sourcesupport. In contrast, the CNN encoder clusters target features based on thepreviously defined class-specific boundaries by minimizing the discrepancybetween the probabilities of the two classifiers. Finally, ViT and CNN mutuallyexchange knowledge to improve the quality of pseudo labels and reduce theknowledge discrepancies of these models. Compared to conventional DA methods,our ECB achieves superior performance, which verifies its effectiveness in thishybrid model. The project website can be foundhttps://dotrannhattuong.github.io/ECB/website/.</description><author>Ba Hung Ngo, Nhat-Tuong Do-Tran, Tuan-Ngoc Nguyen, Hae-Gon Jeon, Tae Jong Choi</author><pubDate>Tue, 02 Apr 2024 18:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18360v2</guid></item><item><title>Lifelong Continual Learning for Anomaly Detection: New Challenges, Perspectives, and Insights</title><link>http://arxiv.org/abs/2303.07557v2</link><description>Anomaly detection is of paramount importance in many real-world domains,characterized by evolving behavior. Lifelong learning represents an emergingtrend, answering the need for machine learning models that continuously adaptto new challenges in dynamic environments while retaining past knowledge.However, limited efforts are dedicated to building foundations for lifelonganomaly detection, which provides intrinsically different challenges comparedto the more widely explored classification setting. In this paper, we face thisissue by exploring, motivating, and discussing lifelong anomaly detection,trying to build foundations for its wider adoption. First, we explain whylifelong anomaly detection is relevant, defining challenges and opportunitiesto design anomaly detection methods that deal with lifelong learningcomplexities. Second, we characterize learning settings and a scenariogeneration procedure that enables researchers to experiment with lifelonganomaly detection using existing datasets. Third, we perform experiments withpopular anomaly detection methods on proposed lifelong scenarios, emphasizingthe gap in performance that could be gained with the adoption of lifelonglearning. Overall, we conclude that the adoption of lifelong anomaly detectionis important to design more robust models that provide a comprehensive view ofthe environment, as well as simultaneous adaptation and knowledge retention.</description><author>Kamil Faber, Roberto Corizzo, Bartlomiej Sniezynski, Nathalie Japkowicz</author><pubDate>Tue, 02 Apr 2024 18:01:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07557v2</guid></item><item><title>CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems</title><link>http://arxiv.org/abs/2404.02103v1</link><description>Retrieval Augmented Generation (RAG) has become a popular application forlarge language models. It is preferable that successful RAG systems provideaccurate answers that are supported by being grounded in a passage without anyhallucinations. While considerable work is required for building a full RAGpipeline, being able to benchmark performance is also necessary. We presentClapNQ, a benchmark Long-form Question Answering dataset for the full RAGpipeline. ClapNQ includes long answers with grounded gold passages from NaturalQuestions (NQ) and a corpus to perform either retrieval, generation, or thefull RAG pipeline. The ClapNQ answers are concise, 3x smaller than the fullpassage, and cohesive, with multiple pieces of the passage that are notcontiguous. RAG models must adapt to these properties to be successful atClapNQ. We present baseline experiments and analysis for ClapNQ that highlightareas where there is still significant room for improvement in grounded RAG.CLAPNQ is publicly available at https://github.com/primeqa/clapnq</description><author>Sara Rosenthal, Avirup Sil, Radu Florian, Salim Roukos</author><pubDate>Tue, 02 Apr 2024 18:00:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02103v1</guid></item><item><title>Sharp bounds for max-sliced Wasserstein distances</title><link>http://arxiv.org/abs/2403.00666v4</link><description>We obtain essentially matching upper and lower bounds for the expectedmax-sliced 1-Wasserstein distance between a probability measure on a separableHilbert space and its empirical distribution from $n$ samples. By proving aBanach space version of this result, we also obtain an upper bound, that issharp up to a log factor, for the expected max-sliced 2-Wasserstein distancebetween a symmetric probability measure $\mu$ on a Euclidean space and itssymmetrized empirical distribution in terms of the operator norm of thecovariance matrix of $\mu$ and the diameter of the support of $\mu$.</description><author>March T. Boedihardjo</author><pubDate>Tue, 02 Apr 2024 17:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00666v4</guid></item><item><title>CameraCtrl: Enabling Camera Control for Text-to-Video Generation</title><link>http://arxiv.org/abs/2404.02101v1</link><description>Controllability plays a crucial role in video generation since it allowsusers to create desired content. However, existing models largely overlookedthe precise control of camera pose that serves as a cinematic language toexpress deeper narrative nuances. To alleviate this issue, we introduceCameraCtrl, enabling accurate camera pose control for text-to-video(T2V)models. After precisely parameterizing the camera trajectory, a plug-and-playcamera module is then trained on a T2V model, leaving others untouched.Additionally, a comprehensive study on the effect of various datasets is alsoconducted, suggesting that videos with diverse camera distribution and similarappearances indeed enhance controllability and generalization. Experimentalresults demonstrate the effectiveness of CameraCtrl in achieving precise anddomain-adaptive camera control, marking a step forward in the pursuit ofdynamic and customized video storytelling from textual and camera pose inputs.Our project website is at: https://hehao13.github.io/projects-CameraCtrl/.</description><author>Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, Ceyuan Yang</author><pubDate>Tue, 02 Apr 2024 17:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02101v1</guid></item><item><title>Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties</title><link>http://arxiv.org/abs/2309.00779v2</link><description>Human values are crucial to human decision-making. Value pluralism is theview that multiple correct values may be held in tension with one another(e.g., when considering lying to a friend to protect their feelings, how doesone balance honesty with friendship?). As statistical learners, AI systems fitto averages by default, washing out these potentially irreducible valueconflicts. To improve AI systems to better reflect value pluralism, thefirst-order challenge is to explore the extent to which AI systems can modelpluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, andduties connected to 31k human-written situations. ValuePrism's contextualizedvalues are generated by GPT-4 and deemed high-quality by human annotators 91%of the time. We conduct a large-scale study with annotators across diversesocial and demographic backgrounds to try to understand whose values arerepresented. With ValuePrism, we build Kaleido, an open, light-weight, and structuredlanguage-based multi-task model that generates, explains, and assesses therelevance and valence (i.e., support or oppose) of human values, rights, andduties within a specific context. Humans prefer the sets of values output byour system over the teacher GPT-4, finding them more accurate and with broadercoverage. In addition, we demonstrate that Kaleido can help explain variabilityin human decision-making by outputting contrasting values. Finally, we showthat Kaleido's representations transfer to other philosophical frameworks anddatasets, confirming the benefit of an explicit, modular, and interpretableapproach to value pluralism. We hope that our work will serve as a step tomaking more explicit the implicit values behind human decision-making and tosteering AI systems to make decisions that are more in accordance with them.</description><author>Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, John Tasioulas, Yejin Choi</author><pubDate>Tue, 02 Apr 2024 17:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00779v2</guid></item><item><title>BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory Speech Recognition</title><link>http://arxiv.org/abs/2404.02098v1</link><description>Self-supervision has recently shown great promise for learning visual andauditory speech representations from unlabelled data. In this work, we proposeBRAVEn, an extension to the recent RAVEn method, which learns speechrepresentations entirely from raw audio-visual data. Our modifications to RAVEnenable BRAVEn to achieve state-of-the-art results among self-supervised methodsin various settings. Moreover, we observe favourable scaling behaviour byincreasing the amount of unlabelled data well beyond other self-supervisedworks. In particular, we achieve 20.0% / 1.7% word error rate for VSR / ASR onthe LRS3 test set, with only 30 hours of labelled data and no external ASRmodels. Our results suggest that readily available unlabelled audio-visual datacan largely replace costly transcribed data.</description><author>Alexandros Haliassos, Andreas Zinonos, Rodrigo Mira, Stavros Petridis, Maja Pantic</author><pubDate>Tue, 02 Apr 2024 17:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02098v1</guid></item><item><title>MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models</title><link>http://arxiv.org/abs/2404.00511v2</link><description>This paper presents our winning submission to Subtask 2 of SemEval 2024 Task3 on multimodal emotion cause analysis in conversations. We propose a novelMultimodal Emotion Recognition and Multimodal Emotion Cause Extraction(MER-MCE) framework that integrates text, audio, and visual modalities usingspecialized emotion encoders. Our approach sets itself apart fromtop-performing teams by leveraging modality-specific features for enhancedemotion understanding and causality inference. Experimental evaluationdemonstrates the advantages of our multimodal approach, with our submissionachieving a competitive weighted F1 score of 0.3435, ranking third with amargin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team.Project: https://github.com/MIPS-COLT/MER-MCE.git</description><author>Zebang Cheng, Fuqiang Niu, Yuxiang Lin, Zhi-Qi Cheng, Bowen Zhang, Xiaojiang Peng</author><pubDate>Tue, 02 Apr 2024 17:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00511v2</guid></item><item><title>LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback</title><link>http://arxiv.org/abs/2311.09336v3</link><description>Recent large language models (LLM) are leveraging human feedback to improvetheir generation quality. However, human feedback is costly to obtain,especially during inference. In this work, we propose LLMRefine, an inferencetime optimization method to refine LLM's output. The core idea is to use alearned fine-grained feedback model to pinpoint defects and guide LLM to refinethem iteratively. Using original LLM as a proposal of edits, LLMRefine searchesfor defect-less text via simulated annealing, trading off the exploration andexploitation. We conduct experiments on three text generation tasks, includingmachine translation, long-form question answering (QA), and topicalsummarization. LLMRefine consistently outperforms all baseline approaches,achieving improvements up to 1.7 MetricX points on translation tasks, 8.1ROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization.</description><author>Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, Markus Freitag</author><pubDate>Tue, 02 Apr 2024 17:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09336v3</guid></item><item><title>Already Moderate Population Sizes Provably Yield Strong Robustness to Noise</title><link>http://arxiv.org/abs/2404.02090v1</link><description>Experience shows that typical evolutionary algorithms can cope well withstochastic disturbances such as noisy function evaluations. In this first mathematical runtime analysis of the $(1+\lambda)$ and$(1,\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise,we show that both algorithms can tolerate constant noise probabilities withoutincreasing the asymptotic runtime on the OneMax benchmark. For this, apopulation size $\lambda$ suffices that is at least logarithmic in the problemsize $n$. The only previous result in this direction regarded the lessrealistic one-bit noise model, required a population size super-linear in theproblem size, and proved a runtime guarantee roughly cubic in the noiselessruntime for the OneMax benchmark. Our significantly stronger results are basedon the novel proof argument that the noiseless offspring can be seen as abiased uniform crossover between the parent and the noisy offspring. We areoptimistic that the technical lemmas resulting from this insight will findapplications also in future mathematical runtime analyses of evolutionaryalgorithms.</description><author>Denis Antipov, Benjamin Doerr, Alexandra Ivanova</author><pubDate>Tue, 02 Apr 2024 17:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02090v1</guid></item><item><title>Immature Green Apple Detection and Sizing in Commercial Orchards using YOLOv8 and Shape Fitting Techniques</title><link>http://arxiv.org/abs/2401.08629v2</link><description>Detecting and estimating size of apples during the early stages of growth iscrucial for predicting yield, pest management, and making informed decisionsrelated to crop-load management, harvest and post-harvest logistics, andmarketing. Traditional fruit size measurement methods are laborious andtimeconsuming. This study employs the state-of-the-art YOLOv8 object detectionand instance segmentation algorithm in conjunction with geometric shape fittingtechniques on 3D point cloud data to accurately determine the size of immaturegreen apples (or fruitlet) in a commercial orchard environment. The methodologyutilized two RGB-D sensors: Intel RealSense D435i and Microsoft Azure KinectDK. Notably, the YOLOv8 instance segmentation models exhibited proficiency inimmature green apple detection, with the YOLOv8m-seg model achieving thehighest AP@0.5 and AP@0.75 scores of 0.94 and 0.91, respectively. Using theellipsoid fitting technique on images from the Azure Kinect, we achieved anRMSE of 2.35 mm, MAE of 1.66 mm, MAPE of 6.15 mm, and an R-squared value of 0.9in estimating the size of apple fruitlets. Challenges such as partial occlusioncaused some error in accurately delineating and sizing green apples using theYOLOv8-based segmentation technique, particularly in fruit clusters. In acomparison with 102 outdoor samples, the size estimation technique performedbetter on the images acquired with Microsoft Azure Kinect than the same withIntel Realsense D435i. This superiority is evident from the metrics: the RMSEvalues (2.35 mm for Azure Kinect vs. 9.65 mm for Realsense D435i), MAE values(1.66 mm for Azure Kinect vs. 7.8 mm for Realsense D435i), and the R-squaredvalues (0.9 for Azure Kinect vs. 0.77 for Realsense D435i).</description><author>Ranjan Sapkota, Dawood Ahmed, Martin Churuvija, Manoj Karkee</author><pubDate>Tue, 02 Apr 2024 17:35:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08629v2</guid></item><item><title>LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task</title><link>http://arxiv.org/abs/2404.02088v1</link><description>Conversation is the most natural form of human communication, where eachutterance can range over a variety of possible emotions. While significant workhas been done towards the detection of emotions in text, relatively little workhas been done towards finding the cause of the said emotions, especially inmultimodal settings. SemEval 2024 introduces the task of Multimodal EmotionCause Analysis in Conversations, which aims to extract emotions reflected inindividual utterances in a conversation involving multiple modalities (textual,audio, and visual modalities) along with the corresponding utterances that werethe cause for the emotion. In this paper, we propose models that tackle thistask as an utterance labeling and a sequence labeling problem and perform acomparative study of these models, involving baselines using differentencoders, using BiLSTM for adding contextual information of the conversation,and finally adding a CRF layer to try to model the inter-dependencies betweenadjacent utterances more effectively. In the official leaderboard for the task,our architecture was ranked 8th, achieving an F1-score of 0.1759 on theleaderboard.</description><author>Suyash Vardhan Mathur, Akshett Rai Jindal, Hardik Mittal, Manish Shrivastava</author><pubDate>Tue, 02 Apr 2024 17:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02088v1</guid></item><item><title>Adaptive Feature Fusion Neural Network for Glaucoma Segmentation on Unseen Fundus Images</title><link>http://arxiv.org/abs/2404.02084v1</link><description>Fundus image segmentation on unseen domains is challenging, especially forthe over-parameterized deep models trained on the small medical datasets. Toaddress this challenge, we propose a method named Adaptive Feature-fusionNeural Network (AFNN) for glaucoma segmentation on unseen domains, which mainlyconsists of three modules: domain adaptor, feature-fusion network, andself-supervised multi-task learning. Specifically, the domain adaptor helps thepretrained-model fast adapt from other image domains to the medical fundusimage domain. Feature-fusion network and self-supervised multi-task learningfor the encoder and decoder are introduced to improve the domain generalizationability. In addition, we also design the weighted-dice-loss to improve modelperformance on complex optic-cup segmentation tasks. Our proposed methodachieves a competitive performance over existing fundus segmentation methods onfour public glaucoma datasets.</description><author>Jiyuan Zhong, Hu Ke, Ming Yan</author><pubDate>Tue, 02 Apr 2024 17:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02084v1</guid></item><item><title>WcDT: World-centric Diffusion Transformer for Traffic Scene Generation</title><link>http://arxiv.org/abs/2404.02082v1</link><description>In this paper, we introduce a novel approach for autonomous drivingtrajectory generation by harnessing the complementary strengths of diffusionprobabilistic models (a.k.a., diffusion models) and transformers. Our proposedframework, termed the "World-Centric Diffusion Transformer" (WcDT), optimizesthe entire trajectory generation process, from feature extraction to modelinference. To enhance the scene diversity and stochasticity, the historicaltrajectory data is first preprocessed and encoded into latent space usingDenoising Diffusion Probabilistic Models (DDPM) enhanced with Diffusion withTransformer (DiT) blocks. Then, the latent features, historical trajectories,HD map features, and historical traffic signal information are fused withvarious transformer-based encoders. The encoded traffic scenes are then decodedby a trajectory decoder to generate multimodal future trajectories.Comprehensive experimental results show that the proposed approach exhibitssuperior performance in generating both realistic and diverse trajectories,showing its potential for integration into automatic driving simulationsystems.</description><author>Chen Yang, Aaron Xuxiang Tian, Dong Chen, Tianyu Shi, Arsalan Heydarian</author><pubDate>Tue, 02 Apr 2024 17:28:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02082v1</guid></item><item><title>Advancing LLM Reasoning Generalists with Preference Trees</title><link>http://arxiv.org/abs/2404.02078v1</link><description>We introduce Eurus, a suite of large language models (LLMs) optimized forreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achievestate-of-the-art results among open-source models on a diverse set ofbenchmarks covering mathematics, code generation, and logical reasoningproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through acomprehensive benchmarking across 12 tests covering five tasks, and achieves a33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challengingbenchmarks, substantially outperforming existing open-source models by marginsmore than 13.3%. The strong performance of Eurus can be primarily attributed toUltraInteract, our newly-curated large-scale, high-quality alignment datasetspecifically designed for complex reasoning tasks. UltraInteract can be used inboth supervised fine-tuning and preference learning. For each instruction, itincludes a preference tree consisting of (1) reasoning chains with diverseplanning strategies in a unified format, (2) multi-turn interactiontrajectories with the environment and the critique, and (3) pairwise data tofacilitate preference learning. UltraInteract allows us to conduct an in-depthexploration of preference learning for reasoning tasks. Our investigationreveals that some well-established preference learning algorithms may be lesssuitable for reasoning tasks compared to their effectiveness in generalconversations. Inspired by this, we derive a novel reward modeling objectivewhich, together with UltraInteract, leads to a strong reward model.</description><author>Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun</author><pubDate>Tue, 02 Apr 2024 17:25:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02078v1</guid></item><item><title>EGTR: Extracting Graph from Transformer for Scene Graph Generation</title><link>http://arxiv.org/abs/2404.02072v1</link><description>Scene Graph Generation (SGG) is a challenging task of detecting objects andpredicting relationships between objects. After DETR was developed, one-stageSGG models based on a one-stage object detector have been actively studied.However, complex modeling is used to predict the relationship between objects,and the inherent relationship between object queries learned in the multi-headself-attention of the object detector has been neglected. We propose alightweight one-stage SGG model that extracts the relation graph from thevarious relationships learned in the multi-head self-attention layers of theDETR decoder. By fully utilizing the self-attention by-products, the relationgraph can be extracted effectively with a shallow relation extraction head.Considering the dependency of the relation extraction task on the objectdetection task, we propose a novel relation smoothing technique that adjuststhe relation label adaptively according to the quality of the detected objects.By the relation smoothing, the model is trained according to the continuouscurriculum that focuses on object detection task at the beginning of trainingand performs multi-task learning as the object detection performance graduallyimproves. Furthermore, we propose a connectivity prediction task that predictswhether a relation exists between object pairs as an auxiliary task of therelation extraction. We demonstrate the effectiveness and efficiency of ourmethod for the Visual Genome and Open Image V6 datasets. Our code is publiclyavailable at https://github.com/naver-ai/egtr .</description><author>Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, Seunghyun Park</author><pubDate>Tue, 02 Apr 2024 17:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02072v1</guid></item><item><title>Semantically-Prompted Language Models Improve Visual Descriptions</title><link>http://arxiv.org/abs/2306.06077v3</link><description>Language-vision models like CLIP have made significant strides in visiontasks, such as zero-shot image classification (ZSIC). However, generatingspecific and expressive visual descriptions remains challenging; descriptionsproduced by current methods are often ambiguous and lacking in granularity. Totackle these issues, we propose V-GLOSS: Visual Glosses, a novel method builtupon two key ideas. The first is Semantic Prompting, which conditions alanguage model on structured semantic knowledge. The second is a newcontrastive algorithm that elicits fine-grained distinctions between similarconcepts. With both ideas, we demonstrate that V-GLOSS improves visualdescriptions and achieves strong results in the zero-shot setting on generaland fine-grained image-classification datasets, including ImageNet, STL-10,FGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilitiescontribute to enhancing image-generation performance. Finally, we introduce aquality-tested silver dataset with descriptions generated with V-GLOSS for allImageNet classes.</description><author>Michael Ogezi, Bradley Hauer, Grzegorz Kondrak</author><pubDate>Tue, 02 Apr 2024 17:19:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06077v3</guid></item><item><title>MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music</title><link>http://arxiv.org/abs/2402.09871v2</link><description>The rapidly evolving multimodal Large Language Models (LLMs) urgently requirenew benchmarks to uniformly evaluate their performance on understanding andtextually describing music. However, due to semantic gaps between MusicInformation Retrieval (MIR) algorithms and human understanding, discrepanciesbetween professionals and the public, and low precision of annotations,existing music description datasets cannot serve as benchmarks. To this end, wepresent MuChin, the first open-source music description benchmark in Chinesecolloquial language, designed to evaluate the performance of multimodal LLMs inunderstanding and describing music. We established the Caichong MusicAnnotation Platform (CaiMAP) that employs an innovative multi-person,multi-stage assurance method, and recruited both amateurs and professionals toensure the precision of annotations and alignment with popular semantics.Utilizing this method, we built a dataset with multi-dimensional,high-precision music annotations, the Caichong Music Dataset (CaiMD), andcarefully selected 1,000 high-quality entries to serve as the test set forMuChin. Based on MuChin, we analyzed the discrepancies between professionalsand amateurs in terms of music description, and empirically demonstrated theeffectiveness of annotated data for fine-tuning LLMs. Ultimately, we employedMuChin to evaluate existing music understanding models on their ability toprovide colloquial descriptions of music. All data related to the benchmark andthe code for scoring have been open-sourced.</description><author>Zihao Wang, Shuyu Li, Tao Zhang, Qi Wang, Pengfei Yu, Jinyang Luo, Yan Liu, Ming Xi, Kejun Zhang</author><pubDate>Tue, 02 Apr 2024 17:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09871v2</guid></item><item><title>Modular Control Architecture for Safe Marine Navigation: Reinforcement Learning and Predictive Safety Filters</title><link>http://arxiv.org/abs/2312.01855v2</link><description>Many autonomous systems face safety challenges, requiring robust closed-loopcontrol to handle physical limitations and safety constraints. Real-worldsystems, like autonomous ships, encounter nonlinear dynamics and environmentaldisturbances. Reinforcement learning is increasingly used to adapt to complexscenarios, but standard frameworks ensuring safety and stability are lacking.Predictive Safety Filters (PSF) offer a promising solution, ensuring constraintsatisfaction in learning-based control without explicit constraint handling.This modular approach allows using arbitrary control policies, with the safetyfilter optimizing proposed actions to meet physical and safety constraints. Weapply this approach to marine navigation, combining RL with PSF on a simulatedCybership II model. The RL agent is trained on path following and collisionavpodance, while the PSF monitors and modifies control actions for safety.Results demonstrate the PSF's effectiveness in maintaining safety withouthindering the RL agent's learning rate and performance, evaluated against astandard RL agent without PSF.</description><author>Aksel Vaaler, Svein Jostein Husa, Daniel Menges, Thomas Nakken Larsen, Adil Rasheed</author><pubDate>Tue, 02 Apr 2024 17:12:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01855v2</guid></item><item><title>Using Interpretation Methods for Model Enhancement</title><link>http://arxiv.org/abs/2404.02068v1</link><description>In the age of neural natural language processing, there are plenty of workstrying to derive interpretations of neural models. Intuitively, when goldrationales exist during training, one can additionally train the model to matchits interpretation with the rationales. However, this intuitive idea has notbeen fully explored. In this paper, we propose a framework of utilizinginterpretation methods and gold rationales to enhance models. Our framework isvery general in the sense that it can incorporate various interpretationmethods. Previously proposed gradient-based methods can be shown as an instanceof our framework. We also propose two novel instances utilizing two other typesof interpretation methods, erasure/replace-based and extractor-based methods,for model enhancement. We conduct comprehensive experiments on a variety oftasks. Experimental results show that our framework is effective especially inlow-resource settings in enhancing models with various interpretation methods,and our two newly-proposed methods outperform gradient-based methods in mostsettings. Code is available at https://github.com/Chord-Chen-30/UIMER.</description><author>Zhuo Chen, Chengyue Jiang, Kewei Tu</author><pubDate>Tue, 02 Apr 2024 17:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02068v1</guid></item><item><title>Red-Teaming Segment Anything Model</title><link>http://arxiv.org/abs/2404.02067v1</link><description>Foundation models have emerged as pivotal tools, tackling many complex tasksthrough pre-training on vast datasets and subsequent fine-tuning for specificapplications. The Segment Anything Model is one of the first and mostwell-known foundation models for computer vision segmentation tasks. This workpresents a multi-faceted red-teaming analysis that tests the Segment AnythingModel against challenging tasks: (1) We analyze the impact of style transfer onsegmentation masks, demonstrating that applying adverse weather conditions andraindrops to dashboard images of city roads significantly distorts generatedmasks. (2) We focus on assessing whether the model can be used for attacks onprivacy, such as recognizing celebrities' faces, and show that the modelpossesses some undesired knowledge in this task. (3) Finally, we check howrobust the model is to adversarial attacks on segmentation masks under textprompts. We not only show the effectiveness of popular white-box attacks andresistance to black-box attacks but also introduce a novel approach - FocusedIterative Gradient Attack (FIGA) that combines white-box approaches toconstruct an efficient attack resulting in a smaller number of modified pixels.All of our testing methods and analyses indicate a need for enhanced safetymeasures in foundation models for image segmentation.</description><author>Krzysztof Jankowski, Bartlomiej Sobieski, Mateusz Kwiatkowski, Jakub Szulc, Michal Janik, Hubert Baniecki, Przemyslaw Biecek</author><pubDate>Tue, 02 Apr 2024 17:07:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02067v1</guid></item><item><title>Multi-Level Label Correction by Distilling Proximate Patterns for Semi-supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2404.02065v1</link><description>Semi-supervised semantic segmentation relieves the reliance on large-scalelabeled data by leveraging unlabeled data. Recent semi-supervised semanticsegmentation approaches mainly resort to pseudo-labeling methods to exploitunlabeled data. However, unreliable pseudo-labeling can undermine thesemi-supervision processes. In this paper, we propose an algorithm calledMulti-Level Label Correction (MLLC), which aims to use graph neural networks tocapture structural relationships in Semantic-Level Graphs (SLGs) andClass-Level Graphs (CLGs) to rectify erroneous pseudo-labels. Specifically,SLGs represent semantic affinities between pairs of pixel features, and CLGsdescribe classification consistencies between pairs of pixel labels. With thesupport of proximate pattern information from graphs, MLLC can rectifyincorrectly predicted pseudo-labels and can facilitate discriminative featurerepresentations. We design an end-to-end network to train and perform thiseffective label corrections mechanism. Experiments demonstrate that MLLC cansignificantly improve supervised baselines and outperforms state-of-the-artapproaches in different scenarios on Cityscapes and PASCAL VOC 2012 datasets.Specifically, MLLC improves the supervised baseline by at least 5% and 2% withDeepLabV2 and DeepLabV3+ respectively under different partition protocols.</description><author>Hui Xiao, Yuting Hong, Li Dong, Diqun Yan, Jiayan Zhuang, Junjie Xiong, Dongtai Liang, Chengbin Peng</author><pubDate>Tue, 02 Apr 2024 17:06:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02065v1</guid></item><item><title>SPMamba: State-space model is all you need in speech separation</title><link>http://arxiv.org/abs/2404.02063v1</link><description>In speech separation, both CNN- and Transformer-based models havedemonstrated robust separation capabilities, garnering significant attentionwithin the research community. However, CNN-based methods have limitedmodelling capability for long-sequence audio, leading to suboptimal separationperformance. Conversely, Transformer-based methods are limited in practicalapplications due to their high computational complexity. Notably, withincomputer vision, Mamba-based methods have been celebrated for their formidableperformance and reduced computational requirements. In this paper, we propose anetwork architecture for speech separation using a state-space model, namelySPMamba. We adopt the TF-GridNet model as the foundational framework andsubstitute its Transformer component with a bidirectional Mamba module, aimingto capture a broader range of contextual information. Our experimental resultsreveal an important role in the performance aspects of Mamba-based models.SPMamba demonstrates superior performance with a significant advantage overexisting separation models in a dataset built on Librispeech. Notably, SPMambaachieves a substantial improvement in separation quality, with a 2.42 dBenhancement in SI-SNRi compared to the TF-GridNet. The source code for SPMambais publicly accessible at https://github.com/JusperLee/SPMamba .</description><author>Kai Li, Guo Chen</author><pubDate>Tue, 02 Apr 2024 17:04:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02063v1</guid></item><item><title>Digital Forgetting in Large Language Models: A Survey of Unlearning Methods</title><link>http://arxiv.org/abs/2404.02062v1</link><description>The objective of digital forgetting is, given a model with undesirableknowledge or behavior, obtain a new model where the detected issues are nolonger present. The motivations for forgetting include privacy protection,copyright protection, elimination of biases and discrimination, and preventionof harmful content generation. Effective digital forgetting has to be effective(meaning how well the new model has forgotten the undesiredknowledge/behavior), retain the performance of the original model on thedesirable tasks, and be scalable (in particular forgetting has to be moreefficient than retraining from scratch on just the tasks/data to be retained).This survey focuses on forgetting in large language models (LLMs). We firstprovide background on LLMs, including their components, the types of LLMs, andtheir usual training pipeline. Second, we describe the motivations, types, anddesired properties of digital forgetting. Third, we introduce the approaches todigital forgetting in LLMs, among which unlearning methodologies stand out asthe state of the art. Fourth, we provide a detailed taxonomy of machineunlearning methods for LLMs, and we survey and compare current approaches.Fifth, we detail datasets, models and metrics used for the evaluation offorgetting, retaining and runtime. Sixth, we discuss challenges in the area.Finally, we provide some concluding remarks.</description><author>Alberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares, David Sánchez, Josep Domingo-Ferrer, Guillem Collell, Kuan Eeik Tan</author><pubDate>Tue, 02 Apr 2024 17:01:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02062v1</guid></item><item><title>Long-context LLMs Struggle with Long In-context Learning</title><link>http://arxiv.org/abs/2404.02060v1</link><description>Large Language Models (LLMs) have made significant strides in handling longsequences exceeding 32K tokens. However, their performance evaluation haslargely been confined to metrics like perplexity and synthetic tasks, which maynot fully capture their abilities in more nuanced, real-world scenarios. Thisstudy introduces a specialized benchmark (LIConBench) focusing on longin-context learning within the realm of extreme-label classification. Wemeticulously selected six datasets with a label range spanning 28 to 174classes covering different input (few-shot demonstration) length from 2K to50K. Our benchmark requires LLMs to comprehend the entire input to recognizethe massive label spaces to make correct prediction. We evaluate 13long-context LLMs on our benchmarks. We find that the long-context LLMs performrelatively well under the token length of 20K and the performance benefits fromutilizing the long context window. However, after the context window exceeds20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gapin current LLM capabilities for processing and understanding long, context-richsequences. Further analysis revealed a tendency among models to favorpredictions for labels presented towards the end at the sequence. Their abilityto reason over multiple pieces in the long sequence is yet to be improved. Ourstudy reveals that long context understanding and reasoning is still achallenging task for the existing LLMs. We believe LIConBench could serve as amore realistic evaluation for the future long context LLMs.</description><author>Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen</author><pubDate>Tue, 02 Apr 2024 16:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02060v1</guid></item><item><title>IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT</title><link>http://arxiv.org/abs/2404.02059v1</link><description>Multimodal foundation models are transformative in sequential recommendersystems, leveraging powerful representation learning capabilities. WhileParameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundationmodels for recommendation tasks, most research prioritizes parameterefficiency, often overlooking critical factors like GPU memory efficiency andtraining speed. Addressing this gap, our paper introduces IISAN (Intra- andInter-modal Side Adapted Network for Multimodal Representation), a simpleplug-and-play architecture using a Decoupled PEFT structure and exploiting bothintra- and inter-modal adaptation. IISAN matches the performance of full fine-tuning (FFT) and state-of-the-artPEFT. More importantly, it significantly reduces GPU memory usage - from 47GBto just 3GB for multimodal sequential recommendation tasks. Additionally, itaccelerates training time per epoch from 443s to 22s compared to FFT. This isalso a notable improvement over the Adapter and LoRA, which require 37-39 GBGPU memory and 350-380 seconds per epoch for training. Furthermore, we propose a new composite efficiency metric, TPME(Training-time, Parameter, and GPU Memory Efficiency) to alleviate theprevalent misconception that "parameter efficiency represents overallefficiency". TPME provides more comprehensive insights into practicalefficiency comparisons between different methods. Besides, we give anaccessible efficiency analysis of all PEFT and FFT approaches, whichdemonstrate the superiority of IISAN. We release our codes and other materialsat https://github.com/jjGenAILab/IISAN.</description><author>Junchen Fu, Xuri Ge, Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Jie Wang, Joemon M Jose</author><pubDate>Tue, 02 Apr 2024 16:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02059v1</guid></item><item><title>Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks</title><link>http://arxiv.org/abs/2404.02058v1</link><description>Quantitative Structure Property Relationship studies aim to define a mappingbetween molecular structure and arbitrary quantities of interest. This washistorically accomplished via the development of descriptors which requiressignificant domain expertise and struggles to generalize. Thus the field hasmorphed into Molecular Property Prediction and been given over to learnedrepresentations which are highly generalizable. The paper introduces fastprop,a DeepQSPR framework which uses a cogent set of molecular level descriptors tomeet and exceed the performance of learned representations on diverse datasetsin dramatically less time. fastprop is freely available on github atgithub.com/JacksonBurns/fastprop.</description><author>Jackson Burns, William Green</author><pubDate>Tue, 02 Apr 2024 16:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02058v1</guid></item><item><title>The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation</title><link>http://arxiv.org/abs/2310.20620v2</link><description>Continuous-output neural machine translation (CoNMT) replaces the discretenext-word prediction problem with an embedding prediction. The semanticstructure of the target embedding space (i.e., closeness of related words) isintuitively believed to be crucial. We challenge this assumption and show thatcompletely random output embeddings can outperform laboriously pretrained ones,especially on larger datasets. Further investigation shows this surprisingeffect is strongest for rare words, due to the geometry of their embeddings. Weshed further light on this finding by designing a mixed strategy that combinesrandom and pre-trained embeddings for different tokens.</description><author>Evgeniia Tokarchuk, Vlad Niculae</author><pubDate>Tue, 02 Apr 2024 16:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20620v2</guid></item><item><title>XLB: A differentiable massively parallel lattice Boltzmann library in Python</title><link>http://arxiv.org/abs/2311.16080v3</link><description>The lattice Boltzmann method (LBM) has emerged as a prominent technique forsolving fluid dynamics problems due to its algorithmic potential forcomputational scalability. We introduce XLB library, a Python-baseddifferentiable LBM library based on the JAX platform. The architecture of XLBis predicated upon ensuring accessibility, extensibility, and computationalperformance, enabling scaling effectively across CPU, TPU, multi-GPU, anddistributed multi-GPU or TPU systems. The library can be readily augmented withnovel boundary conditions, collision models, or multi-physics simulationcapabilities. XLB's differentiability and data structure is compatible with theextensive JAX-based machine learning ecosystem, enabling it to addressphysics-based machine learning, optimization, and inverse problems. XLB hasbeen successfully scaled to handle simulations with billions of cells,achieving giga-scale lattice updates per second. XLB is released under thepermissive Apache-2.0 license and is available on GitHub athttps://github.com/Autodesk/XLB.</description><author>Mohammadmehdi Ataei, Hesam Salehipour</author><pubDate>Tue, 02 Apr 2024 16:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16080v3</guid></item><item><title>Deconstructing In-Context Learning: Understanding Prompts via Corruption</title><link>http://arxiv.org/abs/2404.02054v1</link><description>The ability of large language models (LLMs) to "learn in context" based onthe provided prompt has led to an explosive growth in their use, culminating inthe proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AIassistants are known to be robust to minor prompt modifications, mostly due toalignment techniques that use human feedback. In contrast, the underlyingpre-trained LLMs they use as a backbone are known to be brittle in thisrespect. Building high-quality backbone models remains a core challenge, and acommon approach to assessing their quality is to conduct few-shot evaluation.Such evaluation is notorious for being highly sensitive to minor promptmodifications, as well as the choice of specific in-context examples. Priorwork has examined how modifying different elements of the prompt can affectmodel performance. However, these earlier studies tended to concentrate on alimited number of specific prompt attributes and often produced contradictoryresults. Additionally, previous research either focused on models with fewerthan 15 billion parameters or exclusively examined black-box models like GPT-3or PaLM, making replication challenging. In the present study, we decompose theentire prompt into four components: task description, demonstration inputs,labels, and inline instructions provided for each demonstration. We investigatethe effects of structural and semantic corruptions of these elements on modelperformance. We study models ranging from 1.5B to 70B in size, using tendatasets covering classification and generation tasks. We find that repeatingtext within the prompt boosts model performance, and bigger models ($\geq$30B)are more sensitive to the semantics of the prompt. Finally, we observe thatadding task and inline instructions to the demonstrations enhances modelperformance even when the instructions are semantically corrupted.</description><author>Namrata Shivagunde, Vladislav Lialin, Sherin Muckatira, Anna Rumshisky</author><pubDate>Tue, 02 Apr 2024 16:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02054v1</guid></item><item><title>BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights</title><link>http://arxiv.org/abs/2404.02053v1</link><description>This paper explores the intersection of Natural Language Processing (NLP) andfinancial analysis, focusing on the impact of sentiment analysis in stock priceprediction. We employ BERTopic, an advanced NLP technique, to analyze thesentiment of topics derived from stock market comments. Our methodologyintegrates this sentiment analysis with various deep learning models, renownedfor their effectiveness in time series and stock prediction tasks. Throughcomprehensive experiments, we demonstrate that incorporating topic sentimentnotably enhances the performance of these models. The results indicate thattopics in stock market comments provide implicit, valuable insights into stockmarket volatility and price trends. This study contributes to the field byshowcasing the potential of NLP in enriching financial analysis and opens upavenues for further research into real-time sentiment analysis and theexploration of emotional and contextual aspects of market sentiment. Theintegration of advanced NLP techniques like BERTopic with traditional financialanalysis methods marks a step forward in developing more sophisticated toolsfor understanding and predicting market behaviors.</description><author>Enmin Zhu</author><pubDate>Tue, 02 Apr 2024 16:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02053v1</guid></item><item><title>Noise Masking Attacks and Defenses for Pretrained Speech Models</title><link>http://arxiv.org/abs/2404.02052v1</link><description>Speech models are often trained on sensitive data in order to improve modelperformance, leading to potential privacy leakage. Our work considers noisemasking attacks, introduced by Amid et al. 2022, which attack automatic speechrecognition (ASR) models by requesting a transcript of an utterance which ispartially replaced with noise. They show that when a record has been seen attraining time, the model will transcribe the noisy record with its memorizedsensitive transcript. In our work, we extend these attacks beyond ASR models,to attack pretrained speech encoders. Our method fine-tunes the encoder toproduce an ASR model, and then performs noise masking on this model, which wefind recovers private information from the pretraining data, despite the modelnever having seen transcripts at pretraining time! We show how to improve theprecision of these attacks and investigate a number of countermeasures to ourattacks.</description><author>Matthew Jagielski, Om Thakkar, Lun Wang</author><pubDate>Tue, 02 Apr 2024 16:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02052v1</guid></item><item><title>PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models</title><link>http://arxiv.org/abs/2402.01118v3</link><description>We introduce PokeLLMon, the first LLM-embodied agent that achieveshuman-parity performance in tactical battle games, as demonstrated in Pokemonbattles. The design of PokeLLMon incorporates three key strategies: (i)In-context reinforcement learning that instantly consumes text-based feedbackderived from battles to iteratively refine the policy; (ii) Knowledge-augmentedgeneration that retrieves external knowledge to counteract hallucination andenables the agent to act timely and properly; (iii) Consistent actiongeneration to mitigate the panic switching phenomenon when the agent faces apowerful opponent and wants to elude the battle. We show that online battlesagainst human demonstrates PokeLLMon's human-like battle strategies andjust-in-time decision making, achieving 49% of win rate in the Laddercompetitions and 56% of win rate in the invited battles. Our implementation andplayable battle logs are available at: https://github.com/git-disl/PokeLLMon.</description><author>Sihao Hu, Tiansheng Huang, Ling Liu</author><pubDate>Tue, 02 Apr 2024 16:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01118v3</guid></item><item><title>FISTNet: FusIon of STyle-path generative Networks for Facial Style Transfer</title><link>http://arxiv.org/abs/2307.09020v3</link><description>With the surge in emerging technologies such as Metaverse, spatial computing,and generative AI, the application of facial style transfer has gained a lot ofinterest from researchers as well as startups enthusiasts alike. StyleGANmethods have paved the way for transfer-learning strategies that could reducethe dependency on the huge volume of data that is available for the trainingprocess. However, StyleGAN methods have the tendency of overfitting thatresults in the introduction of artifacts in the facial images. Studies, such asDualStyleGAN, proposed the use of multipath networks but they require thenetworks to be trained for a specific style rather than generating a fusion offacial styles at once. In this paper, we propose a FusIon of STyles (FIST)network for facial images that leverages pre-trained multipath style transfernetworks to eliminate the problem associated with lack of huge data volume inthe training phase along with the fusion of multiple styles at the output. Weleverage pre-trained styleGAN networks with an external style pass that useresidual modulation block instead of a transform coding block. The method alsopreserves facial structure, identity, and details via the gated mapping unitintroduced in this study. The aforementioned components enable us to train thenetwork with very limited amount of data while generating high-quality stylizedimages. Our training process adapts curriculum learning strategy to performefficient, flexible style and model fusion in the generative space. We performextensive experiments to show the superiority of FISTNet in comparison toexisting state-of-the-art methods.</description><author>Sunder Ali Khowaja, Lewis Nkenyereye, Ghulam Mujtaba, Ik Hyun Lee, Giancarlo Fortino, Kapal Dev</author><pubDate>Tue, 02 Apr 2024 16:46:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09020v3</guid></item><item><title>IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations</title><link>http://arxiv.org/abs/2404.01266v2</link><description>Current foundation models exhibit impressive capabilities when promptedeither with text only or with both image and text inputs. But do theircapabilities change depending on the input modality? In this work, we propose$\textbf{IsoBench}$, a benchmark dataset containing problems from four majorareas: math, science, algorithms, and games. Each example is presented withmultiple $\textbf{isomorphic representations}$ of inputs, such as visual,textual, and mathematical presentations. IsoBench provides fine-grainedfeedback to diagnose performance gaps caused by the form of the representation.Across various foundation models, we observe that on the same problem, modelshave a consistent preference towards textual representations. Most prominently,when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 pointsworse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7points worse and Gemini Pro is 14.9 points worse. Finally, we present twoprompting techniques, $\textit{IsoCombination}$ and $\textit{IsoScratchPad}$,which improve model performance by considering combinations of, andtranslations between, different input representations.</description><author>Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, Willie Neiswanger</author><pubDate>Tue, 02 Apr 2024 16:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01266v2</guid></item><item><title>MonoBox: Tightness-free Box-supervised Polyp Segmentation using Monotonicity Constraint</title><link>http://arxiv.org/abs/2404.01188v2</link><description>We propose MonoBox, an innovative box-supervised segmentation methodconstrained by monotonicity to liberate its training from the user-unfriendlybox-tightness assumption. In contrast to conventional box-supervisedsegmentation, where the box edges must precisely touch the target boundaries,MonoBox leverages imprecisely-annotated boxes to achieve robust pixel-wisesegmentation. The 'linchpin' is that, within the noisy zones around box edges,MonoBox discards the traditional misguiding multiple-instance learning loss,and instead optimizes a carefully-designed objective, termed monotonicityconstraint. Along directions transitioning from the foreground to background,this new constraint steers responses to adhere to a trend of monotonicallydecreasing values. Consequently, the originally unreliable learning within thenoisy zones is transformed into a correct and effective monotonicityoptimization. Moreover, an adaptive label correction is introduced, enablingMonoBox to enhance the tightness of box annotations using predicted masks fromthe previous epoch and dynamically shrink the noisy zones as trainingprogresses. We verify MonoBox in the box-supervised segmentation task ofpolyps, where satisfying box-tightness is challenging due to the vagueboundaries between the polyp and normal tissues. Experiments on both publicsynthetic and in-house real noisy datasets demonstrate that MonoBox exceedsother anti-noise state-of-the-arts by improving Dice by at least 5.5% and 3.3%,respectively. Codes are at https://github.com/Huster-Hq/MonoBox.</description><author>Qiang Hu, Zhenyu Yi, Ying Zhou, Ting Li, Fan Huang, Mei Liu, Qiang Li, Zhiwei Wang</author><pubDate>Tue, 02 Apr 2024 16:45:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01188v2</guid></item><item><title>Statistical inference for pairwise comparison models</title><link>http://arxiv.org/abs/2401.08463v2</link><description>Pairwise comparison models have been widely used for utility evaluation andranking across various fields. The increasing scale of problems todayunderscores the need to understand statistical inference in these models whenthe number of subjects diverges, a topic currently lacking in the literatureexcept in a few special instances. To partially address this gap, this paperestablishes a near-optimal asymptotic normality result for the maximumlikelihood estimator in a broad class of pairwise comparison models, as well asa non-asymptotic convergence rate for each individual subject under comparison.The key idea lies in identifying the Fisher information matrix as a weightedgraph Laplacian, which can be studied via a meticulous spectral analysis. Ourfindings provide a unified theory for performing statistical inference in awide range of pairwise comparison models beyond the Bradley--Terry model,benefiting practitioners with theoretical guarantees for their use. Simulationsutilizing synthetic data are conducted to validate the asymptotic normalityresult, followed by a hypothesis test using a tennis competition dataset.</description><author>Ruijian Han, Wenlu Tang, Yiming Xu</author><pubDate>Tue, 02 Apr 2024 16:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08463v2</guid></item><item><title>Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs</title><link>http://arxiv.org/abs/2404.00026v2</link><description>Individuality and personalization comprise the distinctive characteristicsthat make each writer unique and influence their words in order to effectivelyengage readers while conveying authenticity. However, our growing reliance onLLM-based writing assistants risks compromising our creativity andindividuality over time. We often overlook the negative impacts of this trendon our creativity and uniqueness, despite the possible consequences. This studyinvestigates these concerns by performing a brief survey to explore differentperspectives and concepts, as well as trying to understand people's viewpoints,in conjunction with past studies in the area. Addressing these issues isessential for improving human-computer interaction systems and enhancingwriting assistants for personalization and individuality.</description><author>Azmine Toushik Wasi, Raima Islam, Mst Rafia Islam</author><pubDate>Tue, 02 Apr 2024 16:42:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00026v2</guid></item><item><title>LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning</title><link>http://arxiv.org/abs/2404.00027v2</link><description>Sense of ownership in writing confines our investment of thoughts, time, andcontribution, leading to attachment to the output. However, using writingassistants introduces a mental dilemma, as some content isn't directly ourcreation. For instance, we tend to credit Large Language Models (LLMs) more increative tasks, even though all tasks are equal for them. Additionally, whilewe may not claim complete ownership of LLM-generated content, we freely claimauthorship. We conduct a short survey to examine these issues and understandunderlying cognitive processes in order to gain a better knowledge ofhuman-computer interaction in writing and improve writing aid systems.</description><author>Azmine Toushik Wasi, Mst Rafia Islam, Raima Islam</author><pubDate>Tue, 02 Apr 2024 16:40:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00027v2</guid></item><item><title>Samplet basis pursuit: Multiresolution scattered data approximation with sparsity constraints</title><link>http://arxiv.org/abs/2306.10180v4</link><description>We consider scattered data approximation in samplet coordinates with$\ell_1$-regularization. The application of an $\ell_1$-regularization termenforces sparsity of the coefficients with respect to the samplet basis.Samplets are wavelet-type signed measures, which are tailored to scattereddata. Therefore, samplets enable the use of well-established multiresolutiontechniques on general scattered data sets. They provide similar properties aswavelets in terms of localization, multiresolution analysis, and datacompression. By using the Riesz isometry, we embed samplets into reproducingkernel Hilbert spaces and discuss the properties of the resulting functions. Weargue that the class of signals that are sparse with respect to the embeddedsamplet basis is considerably larger than the class of signals that are sparsewith respect to the basis of kernel translates. Vice versa, every signal thatis a linear combination of only a few kernel translates is sparse in sampletcoordinates. We propose the rapid solution of the problem under consideration by combiningsoft-shrinkage with the semi-smooth Newton method. Leveraging on the sparserepresentation of kernel matrices in samplet coordinates, this approachconverges faster than the fast iterative shrinkage thresholding algorithm andis feasible for large-scale data. Numerical benchmarks are presented anddemonstrate the superiority of the multiresolution approach over thesingle-scale approach. As large-scale applications, the surface reconstructionfrom scattered data and the reconstruction of scattered temperature data usinga dictionary of multiple kernels are considered.</description><author>Davide Baroli, Helmut Harbrecht, Michael Multerer</author><pubDate>Tue, 02 Apr 2024 16:40:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10180v4</guid></item><item><title>Universal representations for financial transactional data: embracing local, global, and external contexts</title><link>http://arxiv.org/abs/2404.02047v1</link><description>Effective processing of financial transactions is essential for banking dataanalysis. However, in this domain, most methods focus on specialized solutionsto stand-alone problems instead of constructing universal representationssuitable for many problems. We present a representation learning framework thataddresses diverse business challenges. We also suggest novel generative modelsthat account for data specifics, and a way to integrate external informationinto a client's representation, leveraging insights from other customers'actions. Finally, we offer a benchmark, describing representation qualityglobally, concerning the entire transaction history; locally, reflecting theclient's current state; and dynamically, capturing representation evolutionover time. Our generative approach demonstrates superior performance in localtasks, with an increase in ROC-AUC of up to 14\% for the next MCC predictiontask and up to 46\% for downstream tasks from existing contrastive baselines.Incorporating external information improves the scores by an additional 20\%.</description><author>Alexandra Bazarova, Maria Kovaleva, Ilya Kuleshov, Evgenia Romanenkova, Alexander Stepikin, Alexandr Yugay, Dzhambulat Mollaev, Ivan Kireev, Andrey Savchenko, Alexey Zaytsev</author><pubDate>Tue, 02 Apr 2024 16:39:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02047v1</guid></item><item><title>Causality-based Transfer of Driving Scenarios to Unseen Intersections</title><link>http://arxiv.org/abs/2404.02046v1</link><description>Scenario-based testing of automated driving functions has become a promisingmethod to reduce time and cost compared to real-world testing. Inscenario-based testing automated functions are evaluated in a set ofpre-defined scenarios. These scenarios provide information about vehiclebehaviors, environmental conditions, or road characteristics using parameters.To create realistic scenarios, parameters and parameter dependencies have to befitted utilizing real-world data. However, due to the large variety ofintersections and movement constellations found in reality, data may not beavailable for certain scenarios. This paper proposes a methodology tosystematically analyze relations between parameters of scenarios. Bayesiannetworks are utilized to analyze causal dependencies in order to decrease theamount of required data and to transfer causal patterns creating unseenscenarios. Thereby, infrastructural influences on movement patterns areinvestigated to generate realistic scenarios on unobserved intersections. Forevaluation, scenarios and underlying parameters are extracted from the inDdataset. Movement patterns are estimated, transferred and checked againstrecorded data from those initially unseen intersections.</description><author>Christoph Glasmacher, Michael Schuldes, Sleiman El Masri, Lutz Eckstein</author><pubDate>Tue, 02 Apr 2024 16:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02046v1</guid></item><item><title>Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches</title><link>http://arxiv.org/abs/2404.02043v1</link><description>Despite the extensive amount of labeled datasets in the NLP textclassification field, the persistent imbalance in data availability acrossvarious languages remains evident. Ukrainian, in particular, stands as alanguage that still can benefit from the continued refinement of cross-lingualmethodologies. Due to our knowledge, there is a tremendous lack of Ukrainiancorpora for typical text classification tasks. In this work, we leverage thestate-of-the-art advances in NLP, exploring cross-lingual knowledge transfermethods avoiding manual data curation: large multilingual encoders andtranslation systems, LLMs, and language adapters. We test the approaches onthree text classification tasks -- toxicity classification, formalityclassification, and natural language inference -- providing the "recipe" forthe optimal setups.</description><author>Daryna Dementieva, Valeriia Khylenko, Georg Groh</author><pubDate>Tue, 02 Apr 2024 16:37:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02043v1</guid></item><item><title>SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation</title><link>http://arxiv.org/abs/2404.02041v1</link><description>We present a new self-supervised approach, SelfPose3d, for estimating 3dposes of multiple persons from multiple camera views. Unlike currentstate-of-the-art fully-supervised methods, our approach does not require any 2dor 3d ground-truth poses and uses only the multi-view input images from acalibrated camera setup and 2d pseudo poses generated from an off-the-shelf 2dhuman pose estimator. We propose two self-supervised learning objectives:self-supervised person localization in 3d space and self-supervised 3d poseestimation. We achieve self-supervised 3d person localization by training themodel on synthetically generated 3d points, serving as 3d person rootpositions, and on the projected root-heatmaps in all the views. We then modelthe 3d poses of all the localized persons with a bottleneck representation, mapthem onto all views obtaining 2d joints, and render them using 2d Gaussianheatmaps in an end-to-end differentiable manner. Afterwards, we use thecorresponding 2d joints and heatmaps from the pseudo 2d poses for learning. Toalleviate the intrinsic inaccuracy of the pseudo labels, we propose an adaptivesupervision attention mechanism to guide the self-supervision. Our experimentsand analysis on three public benchmark datasets, including Panoptic, Shelf, andCampus, show the effectiveness of our approach, which is comparable tofully-supervised methods. Code is available at\url{https://github.com/CAMMA-public/SelfPose3D}</description><author>Vinkle Srivastav, Keqi Chen, Nicolas Padoy</author><pubDate>Tue, 02 Apr 2024 16:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02041v1</guid></item><item><title>Transformers as Transducers</title><link>http://arxiv.org/abs/2404.02040v1</link><description>We study the sequence-to-sequence mapping capacity of transformers byrelating them to finite transducers, and find that they can expresssurprisingly large classes of transductions. We do so using variants of RASP, aprogramming language designed to help people "think like transformers," as anintermediate representation. We extend the existing Boolean variant B-RASP tosequence-to-sequence functions and show that it computes exactly thefirst-order rational functions (such as string rotation). Then, we introducetwo new extensions. B-RASP[pos] enables calculations on positions (such ascopying the first half of a string) and contains all first-order regularfunctions. S-RASP adds prefix sum, which enables additional arithmeticoperations (such as squaring a string) and contains all first-order polyregularfunctions. Finally, we show that masked average-hard attention transformers cansimulate S-RASP. A corollary of our results is a new proof that transformerdecoders are Turing-complete.</description><author>Lena Strobl, Dana Angluin, David Chiang, Jonathan Rawski, Ashish Sabharwal</author><pubDate>Tue, 02 Apr 2024 16:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02040v1</guid></item><item><title>A Survey on Large Language Model-Based Game Agents</title><link>http://arxiv.org/abs/2404.02039v1</link><description>The development of game agents holds a critical role in advancing towardsArtificial General Intelligence (AGI). The progress of LLMs and theirmultimodal counterparts (MLLMs) offers an unprecedented opportunity to evolveand empower game agents with human-like decision-making capabilities in complexcomputer game environments. This paper provides a comprehensive overview ofLLM-based game agents from a holistic viewpoint. First, we introduce theconceptual architecture of LLM-based game agents, centered around six essentialfunctional components: perception, memory, thinking, role-playing, action, andlearning. Second, we survey existing representative LLM-based game agentsdocumented in the literature with respect to methodologies and adaptationagility across six genres of games, including adventure, communication,competition, cooperation, simulation, and crafting &amp; exploration games.Finally, we present an outlook of future research and development directions inthis burgeoning field. A curated list of relevant papers is maintained and madeaccessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.</description><author>Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, Ling Liu</author><pubDate>Tue, 02 Apr 2024 16:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02039v1</guid></item><item><title>Joint Multimodal Transformer for Emotion Recognition in the Wild</title><link>http://arxiv.org/abs/2403.10488v2</link><description>Systems for multimodal emotion recognition (MMER) can typically outperformunimodal systems by leveraging the inter- and intra-modal relationshipsbetween, e.g., visual, textual, physiological, and auditory modalities. In thispaper, an MMER method is proposed that relies on a joint multimodal transformerfor fusion with key-based cross-attention. This framework aims to exploit thediverse and complementary nature of different modalities to improve predictiveaccuracy. Separate backbones capture intra-modal spatiotemporal dependencieswithin each modality over video sequences. Subsequently, a joint multimodaltransformer fusion architecture integrates the individual modality embeddings,allowing the model to capture inter-modal and intra-modal relationshipseffectively. Extensive experiments on two challenging expression recognitiontasks: (1) dimensional emotion recognition on the Affwild2 dataset (with faceand voice), and (2) pain estimation on the Biovid dataset (with face andbiosensors), indicate that the proposed method can work effectively withdifferent modalities. Empirical results show that MMER systems with ourproposed fusion method allow us to outperform relevant baseline andstate-of-the-art methods.</description><author>Paul Waligora, Haseeb Aslam, Osama Zeeshan, Soufiane Belharbi, Alessandro Lameiras Koerich, Marco Pedersoli, Simon Bacon, Eric Granger</author><pubDate>Tue, 02 Apr 2024 16:34:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10488v2</guid></item><item><title>MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages</title><link>http://arxiv.org/abs/2404.02037v1</link><description>Text detoxification is a textual style transfer (TST) task where a text isparaphrased from a toxic surface form, e.g. featuring rude words, to theneutral register. Recently, text detoxification methods found theirapplications in various task such as detoxification of Large Language Models(LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxicspeech combating in social networks (Deng et al., 2023; Mun et al., 2023;Agarwal et al., 2023). All these applications are extremely important to ensuresafe communication in modern digital worlds. However, the previous approachesfor parallel text detoxification corpora collection -- ParaDetox (Logacheva etal., 2022) and APPADIA (Atwell et al., 2022) -- were explored only inmonolingual setup. In this work, we aim to extend ParaDetox pipeline tomultiple languages presenting MultiParaDetox to automate paralleldetoxification corpus collection for potentially any language. Then, weexperiment with different text detoxification models -- from unsupervisedbaselines to LLMs and fine-tuned models on the presented parallel corpora --showing the great benefit of parallel corpus presence to obtainstate-of-the-art text detoxification models for any language.</description><author>Daryna Dementieva, Nikolay Babakov, Alexander Panchenko</author><pubDate>Tue, 02 Apr 2024 16:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02037v1</guid></item><item><title>Navigating the Post-API Dilemma | Search Engine Results Pages Present a Biased View of Social Media Data</title><link>http://arxiv.org/abs/2401.15479v3</link><description>Recent decisions to discontinue access to social media APIs are havingdetrimental effects on Internet research and the field of computational socialscience as a whole. This lack of access to data has been dubbed the Post-APIera of Internet research. Fortunately, popular search engines have the means tocrawl, capture, and surface social media data on their Search Engine ResultsPages (SERP) if provided the proper search query, and may provide a solution tothis dilemma. In the present work we ask: does SERP provide a complete andunbiased sample of social media data? Is SERP a viable alternative to directAPI-access? To answer these questions, we perform a comparative analysisbetween (Google) SERP results and nonsampled data from Reddit and Twitter/X. Wefind that SERP results are highly biased in favor of popular posts; againstpolitical, pornographic, and vulgar posts; are more positive in theirsentiment; and have large topical gaps. Overall, we conclude that SERP is not aviable alternative to social media API access.</description><author>Amrit Poudel, Tim Weninger</author><pubDate>Tue, 02 Apr 2024 16:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15479v3</guid></item><item><title>Enhancing wind field resolution in complex terrain through a knowledge-driven machine learning approach</title><link>http://arxiv.org/abs/2309.10172v2</link><description>Atmospheric flows are governed by a broad variety of spatio-temporal scales,thus making real-time numerical modeling of such turbulent flows in complexterrain at high resolution computationally intractable. In this study, wedemonstrate a neural network approach motivated by Enhanced Super-ResolutionGenerative Adversarial Networks to upscale low-resolution wind fields togenerate high-resolution wind fields in an actual wind farm in Bessaker,Norway. The neural network-based model is shown to successfully reconstructfully resolved 3D velocity fields from a coarser scale while respecting thelocal terrain and that it easily outperforms trilinear interpolation. We alsodemonstrate that by using appropriate cost function based on domain knowledge,we can alleviate the use of adversarial training.</description><author>Jacob Wulff Wold, Florian Stadtmann, Adil Rasheed, Mandar Tabib, Omer San, Jan-Tore Horn</author><pubDate>Tue, 02 Apr 2024 16:24:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10172v2</guid></item><item><title>How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities</title><link>http://arxiv.org/abs/2311.09447v2</link><description>The rapid progress in open-source Large Language Models (LLMs) issignificantly driving AI development forward. However, there is still a limitedunderstanding of their trustworthiness. Deploying these models at scale withoutsufficient trustworthiness can pose significant risks, highlighting the need touncover these issues promptly. In this work, we conduct an adversarialassessment of open-source LLMs on trustworthiness, scrutinizing them acrosseight different aspects including toxicity, stereotypes, ethics, hallucination,fairness, sycophancy, privacy, and robustness against adversarialdemonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU)prompting strategy by incorporating carefully crafted malicious demonstrationsfor trustworthiness attack. Our extensive experiments encompass recent andrepresentative series of open-source LLMs, including Vicuna, MPT, Falcon,Mistral, and Llama 2. The empirical outcomes underscore the efficacy of ourattack strategy across diverse aspects. More interestingly, our result analysisreveals that models with superior performance in general NLP tasks do notalways have greater trustworthiness; in fact, larger models can be morevulnerable to attacks. Additionally, models that have undergone instructiontuning, focusing on instruction following, tend to be more susceptible,although fine-tuning LLMs for safety alignment proves effective in mitigatingadversarial trustworthiness attacks.</description><author>Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun</author><pubDate>Tue, 02 Apr 2024 16:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09447v2</guid></item><item><title>A Simple Recipe for Language-guided Domain Generalized Segmentation</title><link>http://arxiv.org/abs/2311.17922v2</link><description>Generalization to new domains not seen during training is one of thelong-standing challenges in deploying neural networks in real-worldapplications. Existing generalization techniques either necessitate externalimages for augmentation, and/or aim at learning invariant representations byimposing various alignment constraints. Large-scale pretraining has recentlyshown promising generalization capabilities, along with the potential ofbinding different modalities. For instance, the advent of vision-languagemodels like CLIP has opened the doorway for vision models to exploit thetextual modality. In this paper, we introduce a simple framework forgeneralizing semantic segmentation networks by employing language as the sourceof randomization. Our recipe comprises three key ingredients: (i) thepreservation of the intrinsic CLIP robustness through minimal fine-tuning, (ii)language-driven local style augmentation, and (iii) randomization by locallymixing the source and augmented styles during training. Extensive experimentsreport state-of-the-art results on various generalization benchmarks. Code isaccessible at https://github.com/astra-vision/FAMix .</description><author>Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</author><pubDate>Tue, 02 Apr 2024 16:20:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17922v2</guid></item><item><title>Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts</title><link>http://arxiv.org/abs/2404.02022v1</link><description>In the era of large language models, applying techniques such as RetrievalAugmented Generation can better address Open-Domain Question-Answeringproblems. Due to constraints including model sizes and computing resources, thelength of context is often limited, and it becomes challenging to empower themodel to cover overlong contexts while answering questions from open domains.This paper proposes a general and convenient method to covering longer contextsin Open-Domain Question-Answering tasks. It leverages a small encoder languagemodel that effectively encodes contexts, and the encoding appliescross-attention with origin inputs. With our method, the origin language modelscan cover several times longer contexts while keeping the computingrequirements close to the baseline. Our experiments demonstrate that afterfine-tuning, there is improved performance across two held-in datasets, fourheld-out datasets, and also in two In Context Learning settings.</description><author>Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu</author><pubDate>Tue, 02 Apr 2024 16:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02022v1</guid></item><item><title>Large Language Models for Orchestrating Bimanual Robots</title><link>http://arxiv.org/abs/2404.02018v1</link><description>Although there has been rapid progress in endowing robots with the ability tosolve complex manipulation tasks, generating control policies for bimanualrobots to solve tasks involving two hands is still challenging because of thedifficulties in effective temporal and spatial coordination. With emergentabilities in terms of step-by-step reasoning and in-context learning, LargeLanguage Models (LLMs) have taken control of a variety of robotic tasks.However, the nature of language communication via a single sequence of discretesymbols makes LLM-based coordination in continuous space a particular challengefor bimanual tasks. To tackle this challenge for the first time by an LLM, wepresent LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizingan LLM to analyze task configurations and devise coordination control policiesfor addressing long-horizon bimanual tasks. In the simulated environment, theLABOR agent is evaluated through several everyday tasks on the NICOL humanoidrobot. Reported success rates indicate that overall coordination efficiency isclose to optimal performance, while the analysis of failure causes, classifiedinto spatial and temporal coordination and skill selection, shows that thesevary over tasks. The project website can be found athttp://labor-agent.github.io</description><author>Kun Chu, Xufeng Zhao, Cornelius Weber, Mengdi Li, Wenhao Lu, Stefan Wermter</author><pubDate>Tue, 02 Apr 2024 16:08:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02018v1</guid></item><item><title>Open-Vocabulary Federated Learning with Multimodal Prototyping</title><link>http://arxiv.org/abs/2404.01232v2</link><description>Existing federated learning (FL) studies usually assume the training labelspace and test label space are identical. However, in real-world applications,this assumption is too ideal to be true. A new user could come up with queriesthat involve data from unseen classes, and such open-vocabulary queries woulddirectly defect such FL systems. Therefore, in this work, we explicitly focuson the under-explored open-vocabulary challenge in FL. That is, for a new user,the global server shall understand her/his query that involves arbitraryunknown classes. To address this problem, we leverage the pre-trainedvision-language models (VLMs). In particular, we present a novel adaptationframework tailored for VLMs in the context of FL, named as Federated MultimodalPrototyping (Fed-MP). Fed-MP adaptively aggregates the local model weightsbased on light-weight client residuals, and makes predictions based on a novelmultimodal prototyping mechanism. Fed-MP exploits the knowledge learned fromthe seen classes, and robustifies the adapted VLM to unseen categories. Ourempirical evaluation on various datasets validates the effectiveness of Fed-MP.</description><author>Huimin Zeng, Zhenrui Yue, Dong Wang</author><pubDate>Tue, 02 Apr 2024 16:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01232v2</guid></item><item><title>Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE</title><link>http://arxiv.org/abs/2402.13604v2</link><description>This paper introduces a new tool, OccCANINE, to automatically transformoccupational descriptions into the HISCO classification system. The manual workinvolved in processing and classifying occupational descriptions iserror-prone, tedious, and time-consuming. We finetune a preexisting languagemodel (CANINE) to do this automatically, thereby performing in seconds andminutes what previously took days and weeks. The model is trained on 14 millionpairs of occupational descriptions and HISCO codes in 13 different languagescontributed by 22 different sources. Our approach is shown to have accuracy,recall, and precision above 90 percent. Our tool breaks the metaphorical HISCObarrier and makes this data readily available for analysis of occupationalstructures with broad applicability in economics, economic history, and variousrelated disciplines.</description><author>Christian Møller Dahl, Torben Johansen, Christian Vedel</author><pubDate>Tue, 02 Apr 2024 15:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13604v2</guid></item><item><title>Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi, Tamil, and Indian English Online Spaces</title><link>http://arxiv.org/abs/2404.02013v1</link><description>Online gender-based harassment is a widespread issue limiting the freeexpression and participation of women and marginalized genders in digitalspaces. Detecting such abusive content can enable platforms to curb thismenace. We participated in the Gendered Abuse Detection in Indic Languagesshared task at ICON2023 that provided datasets of annotated Twitter posts inEnglish, Hindi and Tamil for building classifiers to identify gendered abuse.Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTMnetworks that can effectively model semantic and sequential patterns in textualdata. The CNN captures localized features indicative of abusive languagethrough its convolution filters applied on embedded input text. To determinecontext-based offensiveness, the BiLSTM analyzes this sequence for dependenciesamong words and phrases. Multiple variations were trained using FastText andGloVe word embeddings for each language dataset comprising over 7,600crowdsourced annotations across labels for explicit abuse, targeted minorityattacks and general offences. The validation scores showed strong performanceacross f1-measures, especially for English 0.84. Our experiments reveal howcustomizing embeddings and model hyperparameters can improve detectioncapability. The proposed architecture ranked 1st in the competition, provingits ability to handle real-world noisy text with code-switching. This techniquehas a promising scope as platforms aim to combat cyber harassment facing Indiclanguage internet users. Our Code is athttps://github.com/advaithavetagiri/CNLP-NITS-PP</description><author>Advaitha Vetagiri, Gyandeep Kalita, Eisha Halder, Chetna Taparia, Partha Pakray, Riyanka Manna</author><pubDate>Tue, 02 Apr 2024 15:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02013v1</guid></item><item><title>Preuve de concept d'un bot vocal dialoguant en wolof</title><link>http://arxiv.org/abs/2404.02009v1</link><description>This paper presents the proof-of-concept of the first automatic voiceassistant ever built in Wolof language, the main vehicular language spoken inSenegal. This voicebot is the result of a collaborative research projectbetween Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp,a small IT company based in Dakar, Senegal. The purpose of the voicebot is toprovide information to Orange customers about the Sargal loyalty program ofOrange Senegal by using the most natural mean to communicate: speech. Thevoicebot receives in input the customer's oral request that is then processedby a SLU system to reply to the customer's request using audio recordings. Thefirst results of this proof-of-concept are encouraging as we achieved 22\% ofWER for the ASR task and 78\% of F1-score on the NLU task.</description><author>Elodie Gauthier, Papa-Séga Wade, Thierry Moudenc, Patrice Collen, Emilie De Neef, Oumar Ba, Ndeye Khoyane Cama, Cheikh Ahmadou Bamba Kebe, Ndeye Aissatou Gningue, Thomas Mendo'o Aristide</author><pubDate>Tue, 02 Apr 2024 15:53:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02009v1</guid></item><item><title>JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models</title><link>http://arxiv.org/abs/2403.04798v2</link><description>This paper presents our system development for SemEval-2024 Task 3: "TheCompetition of Multimodal Emotion Cause Analysis in Conversations". Effectivelycapturing emotions in human conversations requires integrating multiplemodalities such as text, audio, and video. However, the complexities of thesediverse modalities pose challenges for developing an efficient multimodalemotion cause analysis (ECA) system. Our proposed approach addresses thesechallenges by a two-step framework. We adopt two different approaches in ourimplementation. In Approach 1, we employ instruction-tuning with two separateLlama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4Vfor conversation-level video description and employ in-context learning withannotated conversation using GPT 3.5. Our system wins rank 4, and systemablation experiments demonstrate that our proposed solutions achievesignificant performance gains. All the experimental codes are available onGithub.</description><author>Arefa, Mohammed Abbas Ansari, Chandni Saxena, Tanvir Ahmad</author><pubDate>Tue, 02 Apr 2024 15:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04798v2</guid></item><item><title>Cross-modality debiasing: using language to mitigate sub-population shifts in imaging</title><link>http://arxiv.org/abs/2403.07888v2</link><description>Sub-population shift is a specific type of domain shift that highlightschanges in data distribution within specific sub-groups or populations betweentraining and testing. Sub-population shift accounts for a significant source ofalgorithmic bias and calls for distributional robustness. Recent studies foundinherent distributional robustness in multi-modality foundation models, such asthe vision-language model CLIP, yet this robustness is vulnerable throughparameter fine-tuning. In this paper, we propose leveraging the connection ofrobustness among different modalities and reshaping the distributionalrobustness of one modality with another. Specifically, in the context of thedistributional robustness of CLIP, we propose to leverage natural languageinputs to debias the image feature representations, to improve worst-caseperformance on sub-populations. Our extensive empirical studies show that imagerepresentations debiased by natural language can achieve significantperformance improvement and reduction of performance instability undersub-population shifts.</description><author>Yijiang Pang, Bao Hoang, Jiayu Zhou</author><pubDate>Tue, 02 Apr 2024 15:47:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07888v2</guid></item><item><title>AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design</title><link>http://arxiv.org/abs/2404.02003v1</link><description>Structure-based drug design (SBDD), which aims to generate molecules that canbind tightly to the target protein, is an essential problem in drug discovery,and previous approaches have achieved initial success. However, most existingmethods still suffer from invalid local structure or unrealistic conformationissues, which are mainly due to the poor leaning of bond angles or torsionalangles. To alleviate these problems, we propose AUTODIFF, a diffusion-basedfragment-wise autoregressive generation model. Specifically, we design a novelmolecule assembly strategy named conformal motif that preserves theconformation of local structures of molecules first, then we encode theinteraction of the protein-ligand complex with an SE(3)-equivariantconvolutional network and generate molecules motif-by-motif with diffusionmodeling. In addition, we also improve the evaluation framework of SBDD byconstraining the molecular weights of the generated molecules in the samerange, together with some new metrics, which make the evaluation more fair andpractical. Extensive experiments on CrossDocked2020 demonstrate that ourapproach outperforms the existing models in generating realistic molecules withvalid structures and conformations while maintaining high binding affinity.</description><author>Xinze Li, Penglei Wang, Tianfan Fu, Wenhao Gao, Chengtao Li, Leilei Shi, Junhong Liu</author><pubDate>Tue, 02 Apr 2024 15:44:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02003v1</guid></item><item><title>Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context</title><link>http://arxiv.org/abs/2404.02000v1</link><description>We present the first self-supervised multilingual speech model trainedexclusively on African speech. The model learned from nearly 60 000 hours ofunlabeled speech segments in 21 languages and dialects spoken in sub-SaharanAfrica. On the SSA subset of the FLEURS-102 dataset, our approach based on aHuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASRdownstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposedin the FLEURS benchmark, while being more efficient by using 7x less data and6x less parameters. Furthermore, in the context of a LID downstream task, ourapproach outperforms FLEURS baselines accuracy by over 22\%.</description><author>Antoine Caubrière, Elodie Gauthier</author><pubDate>Tue, 02 Apr 2024 15:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02000v1</guid></item><item><title>Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2404.01999v1</link><description>Reinforcement learning (RL) is a flexible and efficient method forprogramming micro-robots in complex environments. Here we investigate whetherreinforcement learning can provide insights into biological systems whentrained to perform chemotaxis. Namely, whether we can learn about howintelligent agents process given information in order to swim towards a target.We run simulations covering a range of agent shapes, sizes, and swim speeds todetermine if the physical constraints on biological swimmers, namely Brownianmotion, lead to regions where reinforcement learners' training fails. We findthat the RL agents can perform chemotaxis as soon as it is physically possibleand, in some cases, even before the active swimming overpowers the stochasticenvironment. We study the efficiency of the emergent policy and identifyconvergence in agent size and swim speeds. Finally, we study the strategyadopted by the reinforcement learning algorithm to explain how the agentsperform their tasks. To this end, we identify three emerging dominantstrategies and several rare approaches taken. These strategies, whilstproducing almost identical trajectories in simulation, are distinct and giveinsight into the possible mechanisms behind which biological agents exploretheir environment and respond to changing conditions.</description><author>Samuel Tovey, Christoph Lohrmann, Christian Holm</author><pubDate>Tue, 02 Apr 2024 15:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01999v1</guid></item><item><title>Specularity Factorization for Low-Light Enhancement</title><link>http://arxiv.org/abs/2404.01998v1</link><description>We present a new additive image factorization technique that treats images tobe composed of multiple latent specular components which can be simplyestimated recursively by modulating the sparsity during decomposition. Ourmodel-driven {\em RSFNet} estimates these factors by unrolling the optimizationinto network layers requiring only a few scalars to be learned. The resultantfactors are interpretable by design and can be fused for different imageenhancement tasks via a network or combined directly by the user in acontrollable fashion. Based on RSFNet, we detail a zero-reference Low LightEnhancement (LLE) application trained without paired or unpaired supervision.Our system improves the state-of-the-art performance on standard benchmarks andachieves better generalization on multiple other datasets. We also integrateour factors with other task specific fusion networks for applications likederaining, deblurring and dehazing with negligible overhead therebyhighlighting the multi-domain and multi-task generalizability of our proposedRSFNet. The code and data is released for reproducibility on the projecthomepage.</description><author>Saurabh Saini, P J Narayanan</author><pubDate>Tue, 02 Apr 2024 15:41:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01998v1</guid></item><item><title>A discussion about violin reduction: geometric analysis of contour lines and channel of minima</title><link>http://arxiv.org/abs/2404.01995v1</link><description>Some early violins have been reduced during their history to fit imposedmorphological standards, while more recent ones have been built directly tothese standards. We can observe differences between reduced and unreducedinstruments, particularly in their contour lines and channel of minima. In arecent preliminary work, we computed and highlighted those two features for twoinstruments using triangular 3D meshes acquired by photogrammetry, whosefidelity has been assessed and validated with sub-millimetre accuracy. Wepropose here an extension to a corpus of 38 violins, violas and cellos, andintroduce improved procedures, leading to a stronger discussion of thegeometric analysis. We first recall the material we are working with. We thendiscuss how to derive the best reference plane for the violin alignment, whichis crucial for the computation of contour lines and channel of minima. Finally,we show how to compute efficiently both characteristics and we illustrate ourresults with a few examples.</description><author>Philémon Beghin, Anne-Emmanuelle Ceulemans, François Glineur</author><pubDate>Tue, 02 Apr 2024 15:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01995v1</guid></item></channel></rss>