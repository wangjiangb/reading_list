<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 29 Aug 2023 06:00:27 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Parameter-Efficient Finetuning for Robust Continual Multilingual Learning</title><link>http://arxiv.org/abs/2209.06767v3</link><description>We introduce and study the problem of Continual Multilingual Learning (CML)where a previously trained multilingual model is periodically updated using newdata arriving in stages. If the new data is present only in a subset oflanguages, we find that the resulting model shows improved performance only onthe languages included in the latest update (and a few closely relatedlanguages) while its performance on all the remaining languages degradesignificantly. We address this challenge by proposing LAFT-URIEL, aparameter-efficient finetuning strategy which aims to increase the number oflanguages on which the model improves after an update, while reducing themagnitude of loss in performance for the remaining languages. LAFT-URIEL useslinguistic knowledge to balance overfitting and knowledge sharing acrosslanguages, allowing for an additional 25% of task languages to see animprovement in performance after an update, while also reducing the averagemagnitude of losses on the remaining languages by 78% relative.</description><author>Kartikeya Badola, Shachi Dave, Partha Talukdar</author><pubDate>Mon, 28 Aug 2023 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.06767v3</guid></item><item><title>Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond</title><link>http://arxiv.org/abs/2308.14753v1</link><description>Visual similarities discovery (VSD) is an important task with broade-commerce applications. Given an image of a certain object, the goal of VSD isto retrieve images of different objects with high perceptual visual similarity.Although being a highly addressed problem, the evaluation of proposed methodsfor VSD is often based on a proxy of an identification-retrieval task,evaluating the ability of a model to retrieve different images of the sameobject. We posit that evaluating VSD methods based on identification tasks islimited, and faithful evaluation must rely on expert annotations. In thispaper, we introduce the first large-scale fashion visual similarity benchmarkdataset, consisting of more than 110K expert-annotated image pairs. Besidesthis major contribution, we share insight from the challenges we faced whilecurating this dataset. Based on these insights, we propose a novel andefficient labeling procedure that can be applied to any dataset. Our analysisexamines its limitations and inductive biases, and based on these findings, wepropose metrics to mitigate those limitations. Though our primary focus lies onvisual similarity, the methodologies we present have broader applications fordiscovering and evaluating perceptual similarity across various domains.</description><author>Oren Barkan, Tal Reiss, Jonathan Weill, Ori Katz, Roy Hirsch, Itzik Malkiel, Noam Koenigstein</author><pubDate>Mon, 28 Aug 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14753v1</guid></item><item><title>AI Deception: A Survey of Examples, Risks, and Potential Solutions</title><link>http://arxiv.org/abs/2308.14752v1</link><description>This paper argues that a range of current AI systems have learned how todeceive humans. We define deception as the systematic inducement of falsebeliefs in the pursuit of some outcome other than the truth. We first surveyempirical examples of AI deception, discussing both special-use AI systems(including Meta's CICERO) built for specific competitive situations, andgeneral-purpose AI systems (such as large language models). Next, we detailseveral risks from AI deception, such as fraud, election tampering, and losingcontrol of AI systems. Finally, we outline several potential solutions to theproblems posed by AI deception: first, regulatory frameworks should subject AIsystems that are capable of deception to robust risk-assessment requirements;second, policymakers should implement bot-or-not laws; and finally,policymakers should prioritize the funding of relevant research, includingtools to detect AI deception and to make AI systems less deceptive.Policymakers, researchers, and the broader public should work proactively toprevent AI deception from destabilizing the shared foundations of our society.</description><author>Peter S. Park, Simon Goldstein, Aidan O'Gara, Michael Chen, Dan Hendrycks</author><pubDate>Mon, 28 Aug 2023 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14752v1</guid></item><item><title>MagicEdit: High-Fidelity and Temporally Coherent Video Editing</title><link>http://arxiv.org/abs/2308.14749v1</link><description>In this report, we present MagicEdit, a surprisingly simple yet effectivesolution to the text-guided video editing task. We found that high-fidelity andtemporally coherent video-to-video translation can be achieved by explicitlydisentangling the learning of content, structure and motion signals duringtraining. This is in contradict to most existing methods which attempt tojointly model both the appearance and temporal representation within a singleframework, which we argue, would lead to degradation in per-frame quality.Despite its simplicity, we show that MagicEdit supports various downstreamvideo editing tasks, including video stylization, local editing, video-MagicMixand video outpainting.</description><author>Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, Jiashi Feng</author><pubDate>Mon, 28 Aug 2023 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14749v1</guid></item><item><title>MagicAvatar: Multimodal Avatar Generation and Animation</title><link>http://arxiv.org/abs/2308.14748v1</link><description>This report presents MagicAvatar, a framework for multimodal video generationand animation of human avatars. Unlike most existing methods that generateavatar-centric videos directly from multimodal inputs (e.g., text prompts),MagicAvatar explicitly disentangles avatar video generation into two stages:(1) multimodal-to-motion and (2) motion-to-video generation. The first stagetranslates the multimodal inputs into motion/ control signals (e.g., humanpose, depth, DensePose); while the second stage generates avatar-centric videoguided by these motion signals. Additionally, MagicAvatar supports avataranimation by simply providing a few images of the target person. Thiscapability enables the animation of the provided human identity according tothe specific motion derived from the first stage. We demonstrate theflexibility of MagicAvatar through various applications, including text-guidedand video-guided avatar generation, as well as multimodal avatar animation.</description><author>Jianfeng Zhang, Hanshu Yan, Zhongcong Xu, Jiashi Feng, Jun Hao Liew</author><pubDate>Mon, 28 Aug 2023 18:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14748v1</guid></item><item><title>CoVR: Learning Composed Video Retrieval from Web Video Captions</title><link>http://arxiv.org/abs/2308.14746v1</link><description>Composed Image Retrieval (CoIR) has recently gained popularity as a task thatconsiders both text and image queries together, to search for relevant imagesin a database. Most CoIR approaches require manually annotated datasets,comprising image-text-image triplets, where the text describes a modificationfrom the query image to the target image. However, manual curation of CoIRtriplets is expensive and prevents scalability. In this work, we insteadpropose a scalable automatic dataset creation methodology that generatestriplets given video-caption pairs, while also expanding the scope of the taskto include composed video retrieval (CoVR). To this end, we mine paired videoswith a similar caption from a large database, and leverage a large languagemodel to generate the corresponding modification text. Applying thismethodology to the extensive WebVid2M collection, we automatically constructour WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, weintroduce a new benchmark for CoVR with a manually annotated evaluation set,along with baseline results. Our experiments further demonstrate that traininga CoVR model on our dataset effectively transfers to CoIR, leading to improvedstate-of-the-art performance in the zero-shot setup on both the CIRR andFashionIQ benchmarks. Our code, datasets, and models are publicly available athttps://imagine.enpc.fr/~ventural/covr.</description><author>Lucas Ventura, Antoine Yang, Cordelia Schmid, GÃ¼l Varol</author><pubDate>Mon, 28 Aug 2023 18:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14746v1</guid></item><item><title>Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection</title><link>http://arxiv.org/abs/2304.04688v3</link><description>The goal of spatial-temporal action detection is to determine the time andplace where each person's action occurs in a video and classify thecorresponding action category. Most of the existing methods adoptfully-supervised learning, which requires a large amount of training data,making it very difficult to achieve zero-shot learning. In this paper, wepropose to utilize a pre-trained visual-language model to extract therepresentative image and text features, and model the relationship betweenthese features through different interaction modules to obtain the interactionfeature. In addition, we use this feature to prompt each label to obtain moreappropriate text features. Finally, we calculate the similarity between theinteraction feature and the text feature for each label to determine the actioncategory. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that theproposed interaction module and prompting make the visual-language featuresbetter aligned, thus achieving excellent accuracy for zero-shot spatio-temporalaction detection. The code will be available athttps://github.com/webber2933/iCLIP.</description><author>Wei-Jhe Huang, Jheng-Hsien Yeh, Min-Hung Chen, Gueter Josmy Faure, Shang-Hong Lai</author><pubDate>Mon, 28 Aug 2023 18:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04688v3</guid></item><item><title>Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level</title><link>http://arxiv.org/abs/2308.13506v2</link><description>As research on machine translation moves to translating text beyond thesentence level, it remains unclear how effective automatic evaluation metricsare at scoring longer translations. In this work, we first propose a method forcreating paragraph-level data for training and meta-evaluating metrics fromexisting sentence-level data. Then, we use these new datasets to benchmarkexisting sentence-level metrics as well as train learned metrics at theparagraph level. Interestingly, our experimental results demonstrate that usingsentence-level metrics to score entire paragraphs is equally as effective asusing a metric designed to work at the paragraph level. We speculate thisresult can be attributed to properties of the task of reference-basedevaluation as well as limitations of our datasets with respect to capturing alltypes of phenomena that occur in paragraph-level translations.</description><author>Daniel Deutsch, Juraj Juraska, Mara Finkelstein, Markus Freitag</author><pubDate>Mon, 28 Aug 2023 18:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13506v2</guid></item><item><title>Minimizing Quasi-Self-Concordant Functions by Gradient Regularization of Newton Method</title><link>http://arxiv.org/abs/2308.14742v1</link><description>We study the composite convex optimization problems with aQuasi-Self-Concordant smooth component. This problem class naturallyinterpolates between classic Self-Concordant functions and functions withLipschitz continuous Hessian. Previously, the best complexity bounds for thisproblem class were associated with trust-region schemes and implementations ofa ball-minimization oracle. In this paper, we show that for minimizingQuasi-Self-Concordant functions we can use instead the basic Newton Method withGradient Regularization. For unconstrained minimization, it only involves asimple matrix inversion operation (solving a linear system) at each step. Weprove a fast global linear rate for this algorithm, matching the complexitybound of the trust-region scheme, while our method remains especially simple toimplement. Then, we introduce the Dual Newton Method, and based on it, developthe corresponding Accelerated Newton Scheme for this problem class, whichfurther improves the complexity factor of the basic method. As a directconsequence of our results, we establish fast global linear rates of simplevariants of the Newton Method applied to several practical problems, includingLogistic Regression, Soft Maximum, and Matrix Scaling, without requiringadditional assumptions on strong or uniform convexity for the target objective.</description><author>Nikita Doikov</author><pubDate>Mon, 28 Aug 2023 18:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14742v1</guid></item><item><title>Free Lunch for Gait Recognition: A Novel Relation Descriptor</title><link>http://arxiv.org/abs/2308.11487v2</link><description>Gait recognition is to seek correct matches for query individuals by theirunique walking patterns. However, current methods focus solely on extractingindividual-specific features, overlooking inter-personal relationships. In thispaper, we propose a novel $\textbf{Relation Descriptor}$ that captures not onlyindividual features but also relations between test gaits and pre-selectedanchored gaits. Specifically, we reinterpret classifier weights as anchoredgaits and compute similarity scores between test features and these anchors,which re-expresses individual gait features into a similarity relationdistribution. In essence, the relation descriptor offers a holistic perspectivethat leverages the collective knowledge stored within the classifier's weights,emphasizing meaningful patterns and enhancing robustness. Despite itspotential, relation descriptor poses dimensionality challenges since itsdimension depends on the training set's identity count. To address this, wepropose the Farthest Anchored-gait Selection to identify the mostdiscriminative anchored gaits and an Orthogonal Regularization to increasediversity within anchored gaits. Compared to individual-specific featuresextracted from the backbone, our relation descriptor can boost the performancesnearly without any extra costs. We evaluate the effectiveness of our method onthe popular GREW, Gait3D, CASIA-B, and OU-MVLP, showing that our methodconsistently outperforms the baselines and achieves state-of-the-artperformances.</description><author>Jilong Wang, Saihui Hou, Yan Huang, Chunshui Cao, Xu Liu, Yongzhen Huang, Liang Wang</author><pubDate>Mon, 28 Aug 2023 18:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11487v2</guid></item><item><title>Total Selfie: Generating Full-Body Selfies</title><link>http://arxiv.org/abs/2308.14740v1</link><description>We present a method to generate full-body selfies -- photos that you take ofyourself, but capturing your whole body as if someone else took the photo ofyou from a few feet away. Our approach takes as input a pre-captured video ofyour body, a target pose photo, and a selfie + background pair for eachlocation. We introduce a novel diffusion-based approach to combine all of thisinformation into high quality, well-composed photos of you with the desiredpose and background.</description><author>Bowei Chen, Brian Curless, Ira Kemelmacher-Shlizerman, Steve Seitz</author><pubDate>Mon, 28 Aug 2023 18:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14740v1</guid></item><item><title>Flexible Techniques for Differentiable Rendering with 3D Gaussians</title><link>http://arxiv.org/abs/2308.14737v1</link><description>Fast, reliable shape reconstruction is an essential ingredient in manycomputer vision applications. Neural Radiance Fields demonstrated thatphotorealistic novel view synthesis is within reach, but was gated byperformance requirements for fast reconstruction of real scenes and objects.Several recent approaches have built on alternative shape representations, inparticular, 3D Gaussians. We develop extensions to these renderers, such asintegrating differentiable optical flow, exporting watertight meshes andrendering per-ray normals. Additionally, we show how two of the recent methodsare interoperable with each other. These reconstructions are quick, robust, andeasily performed on GPU or CPU. For code and visual examples, seehttps://leonidk.github.io/fmb-plus</description><author>Leonid Keselman, Martial Hebert</author><pubDate>Mon, 28 Aug 2023 18:38:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14737v1</guid></item><item><title>Bayesian artificial brain with ChatGPT</title><link>http://arxiv.org/abs/2308.14732v1</link><description>This paper aims to investigate the mathematical problem-solving capabilitiesof Chat Generative Pre-Trained Transformer (ChatGPT) in case of Bayesianreasoning. The study draws inspiration from Zhu &amp; Gigerenzer's research in2006, which posed the question: Can children reason the Bayesian way? In thepursuit of answering this question, a set of 10 Bayesian reasoning problemswere presented. The results of their work revealed that children's ability toreason effectively using Bayesian principles is contingent upon awell-structured information representation. In this paper, we present the sameset of 10 Bayesian reasoning problems to ChatGPT. Remarkably, the resultsdemonstrate that ChatGPT provides the right solutions to all problems.</description><author>Renato A. Krohling</author><pubDate>Mon, 28 Aug 2023 18:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14732v1</guid></item><item><title>Distilled GPT for Source Code Summarization</title><link>http://arxiv.org/abs/2308.14731v1</link><description>A code summary is a brief natural language description of source code.Summaries are usually only a single sentence long, and yet form the backbone ofdeveloper documentation. A short descriptions such as "changes all visiblepolygons to the color blue" can give a programmer a high-level idea of whatcode does without the effort of reading the code itself. Recently, productsbased on Large Language Models such as ChatGPT have demonstrated a strongability to write these descriptions automatically. However, to use these tools,programmers must send their code to untrusted third parties for processing(e.g., via an API call). This loss of custody is not acceptable to manyorganizations. In this paper, we present an alternative: we train an opensource model using sample output generated by GPT-3.5 in a process related toknowledge distillation. Our model is small enough (350m parameters) to be runon a single 16gb GPU, yet we show in our evaluation that it is large enough tomimic GPT-3.5 on this task.</description><author>Chia-Yi Su, Collin McMillan</author><pubDate>Mon, 28 Aug 2023 18:34:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14731v1</guid></item><item><title>Decentralized Multi-Agent Reinforcement Learning with Global State Prediction</title><link>http://arxiv.org/abs/2306.12926v2</link><description>Deep reinforcement learning (DRL) has seen remarkable success in the controlof single robots. However, applying DRL to robot swarms presents significantchallenges. A critical challenge is non-stationarity, which occurs when two ormore robots update individual or shared policies concurrently, thereby engagingin an interdependent training process with no guarantees of convergence.Circumventing non-stationarity typically involves training the robots withglobal information about other agents' states and/or actions. In contrast, inthis paper we explore how to remove the need for global information. We poseour problem as a Partially Observable Markov Decision Process, due to theabsence of global knowledge on other agents. Using collective transport as atestbed scenario, we study two approaches to multi-agent training. In thefirst, the robots exchange no messages, and are trained to rely on implicitcommunication through push-and-pull on the object to transport. In the secondapproach, we introduce Global State Prediction (GSP), a network trained toforma a belief over the swarm as a whole and predict its future states. Weprovide a comprehensive study over four well-known deep reinforcement learningalgorithms in environments with obstacles, measuring performance as thesuccessful transport of the object to the goal within a desired time-frame.Through an ablation study, we show that including GSP boosts performance andincreases robustness when compared with methods that use global knowledge.</description><author>Joshua Bloom, Pranjal Paliwal, Apratim Mukherjee, Carlo Pinciroli</author><pubDate>Mon, 28 Aug 2023 18:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12926v2</guid></item><item><title>PanoSwin: a Pano-style Swin Transformer for Panorama Understanding</title><link>http://arxiv.org/abs/2308.14726v1</link><description>In panorama understanding, the widely used equirectangular projection (ERP)entails boundary discontinuity and spatial distortion. It severely deterioratesthe conventional CNNs and vision Transformers on panoramas. In this paper, wepropose a simple yet effective architecture named PanoSwin to learn panoramarepresentations with ERP. To deal with the challenges brought byequirectangular projection, we explore a pano-style shift windowing scheme andnovel pitch attention to address the boundary discontinuity and the spatialdistortion, respectively. Besides, based on spherical distance and Cartesiancoordinates, we adapt absolute positional embeddings and relative positionalbiases for panoramas to enhance panoramic geometry information. Realizing thatplanar image understanding might share some common knowledge with panoramaunderstanding, we devise a novel two-stage learning framework to facilitateknowledge transfer from the planar images to panoramas. We conduct experimentsagainst the state-of-the-art on various panoramic tasks, i.e., panoramic objectdetection, panoramic classification, and panoramic layout estimation. Theexperimental results demonstrate the effectiveness of PanoSwin in panoramaunderstanding.</description><author>Zhixin Ling, Zhen Xing, Xiangdong Zhou, Manliang Cao, Guichun Zhou</author><pubDate>Mon, 28 Aug 2023 18:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14726v1</guid></item><item><title>Revisiting mass-radius relationships for exoplanet populations: a machine learning insight</title><link>http://arxiv.org/abs/2301.07143v3</link><description>The growing number of exoplanet discoveries and advances in machine learningtechniques have opened new avenues for exploring and understanding thecharacteristics of worlds beyond our Solar System. In this study, we employefficient machine learning approaches to analyze a dataset comprising 762confirmed exoplanets and eight Solar System planets, aiming to characterizetheir fundamental quantities. By applying different unsupervised clusteringalgorithms, we classify the data into two main classes: 'small' and 'giant'planets, with cut-off values at $R_{p}=8.13R_{\oplus}$ and$M_{p}=52.48M_{\oplus}$. This classification reveals an intriguing distinction:giant planets have lower densities, suggesting higher H-He mass fractions,while small planets are denser, composed mainly of heavier elements. We applyvarious regression models to uncover correlations between physical parametersand their predictive power for exoplanet radius. Our analysis highlights thatplanetary mass, orbital period, and stellar mass play crucial roles inpredicting exoplanet radius. Among the models evaluated, the Support VectorRegression consistently outperforms others, demonstrating its promise forobtaining accurate planetary radius estimates. Furthermore, we deriveparametric equations using the M5P and Markov Chain Monte Carlo methods.Notably, our study reveals a noteworthy result: small planets exhibit apositive linear mass-radius relation, aligning with previous findings.Conversely, for giant planets, we observe a strong correlation betweenplanetary radius and the mass of their host stars, which might provideintriguing insights into the relationship between giant planet formation andstellar characteristics.</description><author>Mahdiyar Mousavi-Sadr, Davood M. Jassur, Ghassem Gozaliasl</author><pubDate>Mon, 28 Aug 2023 18:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07143v3</guid></item><item><title>Hierarchical Time Series Forecasting with Bayesian Modeling</title><link>http://arxiv.org/abs/2308.14719v1</link><description>We encounter time series data in many domains such as finance, physics,business, and weather. One of the main tasks of time series analysis, one thathelps to take informed decisions under uncertainty, is forecasting. Time seriesare often hierarchically structured, e.g., a company sales might be broken downinto different regions, and each region into different stores. In some casesthe number of series in the hierarchy is too big to fit in a single model toproduce forecasts in relevant time, and a decentralized approach is beneficial. One way to do this is to train independent forecasting models for each seriesand for some summary statistics series implied by the hierarchy (e.g. the sumof all series) and to pass those models to a reconciliation algorithm toimprove those forecasts by sharing information between the series. In this work we focus on the reconciliation step, and propose a method to doso from a Bayesian perspective - Bayesian forecast reconciliation. We alsodefine the common case of linear Gaussian reconciliation, where the forecastsare Gaussian and the hierarchy has linear structure, and show that we cancompute reconciliation in closed form. We evaluate these methods on syntheticand real data sets, and compare them to other work in this field.</description><author>Gal Elgavish</author><pubDate>Mon, 28 Aug 2023 18:20:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14719v1</guid></item><item><title>R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras</title><link>http://arxiv.org/abs/2308.14713v1</link><description>Dense 3D reconstruction and ego-motion estimation are key challenges inautonomous driving and robotics. Compared to the complex, multi-modal systemsdeployed today, multi-camera systems provide a simpler, low-cost alternative.However, camera-based 3D reconstruction of complex dynamic scenes has provenextremely difficult, as existing solutions often produce incomplete orincoherent results. We propose R3D3, a multi-camera system for dense 3Dreconstruction and ego-motion estimation. Our approach iterates betweengeometric estimation that exploits spatial-temporal information from multiplecameras, and monocular depth refinement. We integrate multi-camera featurecorrelation and dense bundle adjustment operators that yield robust geometricdepth and pose estimates. To improve reconstruction where geometric depth isunreliable, e.g. for moving objects or low-textured regions, we introducelearnable scene priors via a depth refinement network. We show that this designenables a dense, consistent 3D reconstruction of challenging, dynamic outdoorenvironments. Consequently, we achieve state-of-the-art dense depth predictionon the DDAD and NuScenes benchmarks.</description><author>Aron Schmied, Tobias Fischer, Martin Danelljan, Marc Pollefeys, Fisher Yu</author><pubDate>Mon, 28 Aug 2023 18:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14713v1</guid></item><item><title>Fast Feedforward Networks</title><link>http://arxiv.org/abs/2308.14711v1</link><description>We break the linear link between the layer size and its inference cost byintroducing the fast feedforward (FFF) architecture, a logarithmic-timealternative to feedforward networks. We show that FFFs give comparable performance to feedforward networks at anexponential fraction of their inference cost, are quicker to deliverperformance compared to mixture-of-expert networks, and can readily take theplace of either in transformers. Pushing FFFs to the absolute limit, we train a vision transformer to performsingle-neuron inferences at the cost of only 5.8% performance decrease againstthe full-width variant. Our implementation is available as a Python package; just use "pip installfastfeedforward".</description><author>Peter Belcak, Roger Wattenhofer</author><pubDate>Mon, 28 Aug 2023 18:11:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14711v1</guid></item><item><title>VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation</title><link>http://arxiv.org/abs/2308.14710v1</link><description>Existing approaches to unsupervised video instance segmentation typicallyrely on motion estimates and experience difficulties tracking small ordivergent motions. We present VideoCutLER, a simple method for unsupervisedmulti-instance video segmentation without using motion-based learning signalslike optical flow or training on natural videos. Our key insight is that usinghigh-quality pseudo masks and a simple video synthesis method for modeltraining is surprisingly sufficient to enable the resulting video model toeffectively segment and track multiple instances across video frames. We showthe first competitive unsupervised learning results on the challengingYouTubeVIS-2019 benchmark, achieving 50.7% APvideo^50 , surpassing the previousstate-of-the-art by a large margin. VideoCutLER can also serve as a strongpretrained model for supervised video instance segmentation tasks, exceedingDINO by 15.9% on YouTubeVIS-2019 in terms of APvideo.</description><author>Xudong Wang, Ishan Misra, Ziyun Zeng, Rohit Girdhar, Trevor Darrell</author><pubDate>Mon, 28 Aug 2023 18:10:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14710v1</guid></item><item><title>Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning</title><link>http://arxiv.org/abs/2308.14705v1</link><description>Ensembling a neural network is a widely recognized approach to enhance modelperformance, estimate uncertainty, and improve robustness in deep supervisedlearning. However, deep ensembles often come with high computational costs andmemory demands. In addition, the efficiency of a deep ensemble is related todiversity among the ensemble members which is challenging for large,over-parameterized deep neural networks. Moreover, ensemble learning has notyet seen such widespread adoption, and it remains a challenging endeavor forself-supervised or unsupervised representation learning. Motivated by thesechallenges, we present a novel self-supervised training regime that leveragesan ensemble of independent sub-networks, complemented by a new loss functiondesigned to encourage diversity. Our method efficiently builds a sub-modelensemble with high diversity, leading to well-calibrated estimates of modeluncertainty, all achieved with minimal computational overhead compared totraditional deep self-supervised ensembles. To evaluate the effectiveness ofour approach, we conducted extensive experiments across various tasks,including in-distribution generalization, out-of-distribution detection,dataset corruption, and semi-supervised settings. The results demonstrate thatour method significantly improves prediction reliability. Our approach not onlyachieves excellent accuracy but also enhances calibration, surpassing baselineperformance across a wide range of self-supervised architectures in computervision, natural language processing, and genomics data.</description><author>Amirhossein Vahidi, Lisa Wimmer, HÃ¼seyin Anil GÃ¼ndÃ¼z, Bernd Bischl, Eyke HÃ¼llermeier, Mina Rezaei</author><pubDate>Mon, 28 Aug 2023 17:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14705v1</guid></item><item><title>TRIVEA: Transparent Ranking Interpretation using Visual Explanation of Black-Box Algorithmic Rankers</title><link>http://arxiv.org/abs/2308.14622v1</link><description>Ranking schemes drive many real-world decisions, like, where to study, whomto hire, what to buy, etc. Many of these decisions often come with highconsequences. For example, a university can be deemed less prestigious if notfeatured in a top-k list, and consumers might not even explore products that donot get recommended to buyers. At the heart of most of these decisions areopaque ranking schemes, which dictate the ordering of data entities, but theirinternal logic is inaccessible or proprietary. Drawing inferences about theranking differences is like a guessing game to the stakeholders, like, therankees (i.e., the entities who are ranked, like product companies) and thedecision-makers (i.e., who use the rankings, like buyers). In this paper, weaim to enable transparency in ranking interpretation by using algorithmicrankers that learn from available data and by enabling human reasoning aboutthe learned ranking differences using explainable AI (XAI) methods. To realizethis aim, we leverage the exploration-explanation paradigm of human-datainteraction to let human stakeholders explore subsets and groupings of complexmulti-attribute ranking data using visual explanations of model fit andattribute influence on rankings. We realize this explanation paradigm fortransparent ranking interpretation in TRIVEA, a visual analytic system that isfueled by: i) visualizations of model fit derived from algorithmic rankers thatlearn the associations between attributes and rankings from available data andii) visual explanations derived from XAI methods that help abstract importantpatterns, like, the relative influence of attributes in different rankingranges. Using TRIVEA, end users not trained in data science have the agency totransparently reason about the global and local behavior of the rankingswithout the need to open black-box ranking models and develop confidence in theresulting attribute-based inferences. We demonstrate the efficacy of TRIVEAusing multiple usage scenarios and subjective feedback from researchers withdiverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank,Explainable ML, Ranking</description><author>Jun Yuan, Kaustav Bhattacharjee, Akm Zahirul Islam, Aritra Dasgupta</author><pubDate>Mon, 28 Aug 2023 17:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14622v1</guid></item><item><title>Assessing Trust in Construction AI-Powered Collaborative Robots using Structural Equation Modeling</title><link>http://arxiv.org/abs/2308.14697v1</link><description>This study aimed to investigate the key technical and psychological factorsthat impact the architecture, engineering, and construction (AEC)professionals' trust in collaborative robots (cobots) powered by artificialintelligence (AI). The study employed a nationwide survey of 600 AEC industrypractitioners to gather in-depth responses and valuable insights into thefuture opportunities for promoting the adoption, cultivation, and training of askilled workforce to leverage this technology effectively. A StructuralEquation Modeling (SEM) analysis revealed that safety and reliability aresignificant factors for the adoption of AI-powered cobots in construction. Fearof being replaced resulting from the use of cobots can have a substantialeffect on the mental health of the affected workers. A lower error rate in jobsinvolving cobots, safety measurements, and security of data collected by cobotsfrom jobsites significantly impact reliability, while the transparency ofcobots' inner workings can benefit accuracy, robustness, security, privacy, andcommunication, and results in higher levels of automation, all of whichdemonstrated as contributors to trust. The study's findings provide criticalinsights into the perceptions and experiences of AEC professionals towardsadoption of cobots in construction and help project teams determine theadoption approach that aligns with the company's goals workers' welfare.</description><author>Newsha Emaminejad, Lisa Kath, Reza Akhavian</author><pubDate>Mon, 28 Aug 2023 17:39:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14697v1</guid></item><item><title>Cross-domain Federated Object Detection</title><link>http://arxiv.org/abs/2206.14996v2</link><description>Detection models trained by one party (including server) may face severeperformance degradation when distributed to other users (clients). Federatedlearning can enable multi-party collaborative learning without leaking clientdata. In this paper, we focus on a special cross-domain scenario in which theserver has large-scale labeled data and multiple clients only have a smallamount of labeled data; meanwhile, there exist differences in datadistributions among the clients. In this case, traditional federated learningmethods can't help a client learn both the global knowledge of all participantsand its own unique knowledge. To make up for this limitation, we propose across-domain federated object detection framework, named FedOD. The proposedframework first performs the federated training to obtain a public globalaggregated model through multi-teacher distillation, and sends the aggregatedmodel back to each client for fine-tuning its personalized local model. After afew rounds of communication, on each client we can perform weighted ensembleinference on the public global model and the personalized local model. Weestablish a federated object detection dataset which has significant backgrounddifferences and instance differences based on multiple public autonomousdriving datasets, and then conduct extensive experiments on the dataset. Theexperimental results validate the effectiveness of the proposed method.</description><author>Shangchao Su, Bin Li, Chengzhi Zhang, Mingzhao Yang, Xiangyang Xue</author><pubDate>Mon, 28 Aug 2023 17:38:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.14996v2</guid></item><item><title>The feasibility of artificial consciousness through the lens of neuroscience</title><link>http://arxiv.org/abs/2306.00915v3</link><description>Interactions with large language models have led to the suggestion that thesemodels may soon be conscious. From the perspective of neuroscience, thisposition is difficult to defend. For one, the inputs to large language modelslack the embodied, embedded information content characteristic of our sensorycontact with the world around us. Secondly, the architecture of large languagemodels is missing key features of the thalamocortical system that have beenlinked to conscious awareness in mammals. Finally, the evolutionary anddevelopmental trajectories that led to the emergence of living consciousorganisms arguably have no parallels in artificial systems as envisioned today.The existence of living organisms depends on their actions, and their survivalis intricately linked to multi-level cellular, inter-cellular, and organismalprocesses culminating in agency and consciousness.</description><author>Jaan Aru, Matthew Larkum, James M. Shine</author><pubDate>Mon, 28 Aug 2023 17:36:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00915v3</guid></item><item><title>Hybrid PLS-ML Authentication Scheme for V2I Communication Networks</title><link>http://arxiv.org/abs/2308.14693v1</link><description>Vehicular communication networks are rapidly emerging as vehicles becomesmarter. However, these networks are increasingly susceptible to variousattacks. The situation is exacerbated by the rise in automated vehiclescomplicates, emphasizing the need for security and authentication measures toensure safe and effective traffic management. In this paper, we propose a novelhybrid physical layer security (PLS)-machine learning (ML) authenticationscheme by exploiting the position of the transmitter vehicle as a devicefingerprint. We use a time-of-arrival (ToA) based localization mechanism wherethe ToA is estimated at roadside units (RSUs), and the coordinates of thetransmitter vehicle are extracted at the base station (BS).Furthermore, totrack the mobility of the moving legitimate vehicle, we use ML model trained onseveral system parameters. We try two ML models for this purpose, i.e., supportvector regression and decision tree. To evaluate our scheme, we conduct binaryhypothesis testing on the estimated positions with the help of the groundtruths provided by the ML model, which classifies the transmitter node aslegitimate or malicious. Moreover, we consider the probability of false alarmand the probability of missed detection as performance metrics resulting fromthe binary hypothesis testing, and mean absolute error (MAE), mean square error(MSE), and coefficient of determination $\text{R}^2$ to further evaluate the MLmodels. We also compare our scheme with a baseline scheme that exploits theangle of arrival at RSUs for authentication. We observe that our proposedposition-based mechanism outperforms the baseline scheme significantly in termsof missed detections.</description><author>Hala Amin, Jawaher Kaldari, Nora Mohamed, Waqas Aman, Saif Al-Kuwari</author><pubDate>Mon, 28 Aug 2023 17:34:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14693v1</guid></item><item><title>MELT: Mining Effective Lightweight Transformations from Pull Requests</title><link>http://arxiv.org/abs/2308.14687v1</link><description>Software developers often struggle to update APIs, leading to manual,time-consuming, and error-prone processes. We introduce MELT, a new approachthat generates lightweight API migration rules directly from pull requests inpopular library repositories. Our key insight is that pull requests merged intoopen-source libraries are a rich source of information sufficient to mine APImigration rules. By leveraging code examples mined from the library source andautomatically generated code examples based on the pull requests, we infertransformation rules in \comby, a language for structural code search andreplace. Since inferred rules from single code examples may be too specific, wepropose a generalization procedure to make the rules more applicable to clientprojects. MELT rules are syntax-driven, interpretable, and easily adaptable.Moreover, unlike previous work, our approach enables rule inference toseamlessly integrate into the library workflow, removing the need to wait forclient code migrations. We evaluated MELT on pull requests from four popularlibraries, successfully mining 461 migration rules from code examples in pullrequests and 114 rules from auto-generated code examples. Our generalizationprocedure increases the number of matches for mined rules by 9x. We appliedthese rules to client projects and ran their tests, which led to an overalldecrease in the number of warnings and fixing some test cases demonstratingMELT's effectiveness in real-world scenarios.</description><author>Daniel Ramos, Hailie Mitchell, InÃªs Lynce, Vasco Manquinho, Ruben Martins, Claire Le Goues</author><pubDate>Mon, 28 Aug 2023 17:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14687v1</guid></item><item><title>360-Degree Panorama Generation from Few Unregistered NFoV Images</title><link>http://arxiv.org/abs/2308.14686v1</link><description>360$^\circ$ panoramas are extensively utilized as environmental light sourcesin computer graphics. However, capturing a 360$^\circ$ $\times$ 180$^\circ$panorama poses challenges due to the necessity of specialized and costlyequipment, and additional human resources. Prior studies develop variouslearning-based generative methods to synthesize panoramas from a single NarrowField-of-View (NFoV) image, but they are limited in alterable input patterns,generation quality, and controllability. To address these issues, we propose anovel pipeline called PanoDiff, which efficiently generates complete360$^\circ$ panoramas using one or more unregistered NFoV images captured fromarbitrary angles. Our approach has two primary components to overcome thelimitations. Firstly, a two-stage angle prediction module to handle variousnumbers of NFoV inputs. Secondly, a novel latent diffusion-based panoramageneration model uses incomplete panorama and text prompts as control signalsand utilizes several geometric augmentation schemes to ensure geometricproperties in generated panoramas. Experiments show that PanoDiff achievesstate-of-the-art panoramic generation quality and high controllability, makingit suitable for applications such as content editing.</description><author>Jionghao Wang, Ziyu Chen, Jun Ling, Rong Xie, Li Song</author><pubDate>Mon, 28 Aug 2023 17:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14686v1</guid></item><item><title>Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual Predatory Chats and Abusive Texts</title><link>http://arxiv.org/abs/2308.14683v1</link><description>Detecting online sexual predatory behaviours and abusive language on socialmedia platforms has become a critical area of research due to the growingconcerns about online safety, especially for vulnerable populations such aschildren and adolescents. Researchers have been exploring various techniquesand approaches to develop effective detection systems that can identify andmitigate these risks. Recent development of large language models (LLMs) hasopened a new opportunity to address this problem more effectively. This paperproposes an approach to detection of online sexual predatory chats and abusivelanguage using the open-source pretrained Llama 2 7B-parameter model, recentlyreleased by Meta GenAI. We fine-tune the LLM using datasets with differentsizes, imbalance degrees, and languages (i.e., English, Roman Urdu and Urdu).Based on the power of LLMs, our approach is generic and automated without amanual search for a synergy between feature extraction and classifier designsteps like conventional methods in this domain. Experimental results show astrong performance of the proposed approach, which performs proficiently andconsistently across three distinct datasets with five sets of experiments. Thisstudy's outcomes indicate that the proposed method can be implemented inreal-world applications (even with non-English languages) for flagging sexualpredators, offensive or toxic content, hate speech, and discriminatory languagein online discussions and comments to maintain respectful internet or digitalcommunities. Furthermore, it can be employed for solving text classificationproblems with other potential applications such as sentiment analysis, spam andphishing detection, sorting legal documents, fake news detection, languageidentification, user intent recognition, text-based product categorization,medical record analysis, and resume screening.</description><author>Thanh Thi Nguyen, Campbell Wilson, Janis Dalins</author><pubDate>Mon, 28 Aug 2023 17:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14683v1</guid></item><item><title>Reconstructing Spatiotemporal Data with C-VAEs</title><link>http://arxiv.org/abs/2307.06243v2</link><description>The continuous representation of spatiotemporal data commonly relies on usingabstract data types, such as \textit{moving regions}, to represent entitieswhose shape and position continuously change over time. Creating thisrepresentation from discrete snapshots of real-world entities requires usinginterpolation methods to compute in-between data representations and estimatethe position and shape of the object of interest at arbitrary temporal points.Existing region interpolation methods often fail to generate smooth andrealistic representations of a region's evolution. However, recent advancementsin deep learning techniques have revealed the potential of deep models trainedon discrete observations to capture spatiotemporal dependencies throughimplicit feature learning. In this work, we explore the capabilities of Conditional VariationalAutoencoder (C-VAE) models to generate smooth and realistic representations ofthe spatiotemporal evolution of moving regions. We evaluate our proposedapproach on a sparsely annotated dataset on the burnt area of a forest fire. Weapply compression operations to sample from the dataset and use the C-VAE modeland other commonly used interpolation algorithms to generate in-between regionrepresentations. To evaluate the performance of the methods, we compare theirinterpolation results with manually annotated data and regions generated by aU-Net model. We also assess the quality of generated data considering temporalconsistency metrics. The proposed C-VAE-based approach demonstrates competitive results ingeometric similarity metrics. It also exhibits superior temporal consistency,suggesting that C-VAE models may be a viable alternative to modelling thespatiotemporal evolution of 2D moving regions.</description><author>Tiago F. R. Ribeiro, Fernando Silva, RogÃ©rio LuÃ­s de C. Costa</author><pubDate>Mon, 28 Aug 2023 17:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06243v2</guid></item><item><title>Video-Based Hand Pose Estimation for Remote Assessment of Bradykinesia in Parkinson's Disease</title><link>http://arxiv.org/abs/2308.14679v1</link><description>There is a growing interest in using pose estimation algorithms forvideo-based assessment of Bradykinesia in Parkinson's Disease (PD) tofacilitate remote disease assessment and monitoring. However, the accuracy ofpose estimation algorithms in videos from video streaming services duringTelehealth appointments has not been studied. In this study, we used sevenoff-the-shelf hand pose estimation models to estimate the movement of the thumband index fingers in videos of the finger-tapping (FT) test recorded fromHealthy Controls (HC) and participants with PD and under two differentconditions: streaming (videos recorded during a live Zoom meeting) andon-device (videos recorded locally with high-quality cameras). The accuracy andreliability of the models were estimated by comparing the models' output withmanual results. Three of the seven models demonstrated good accuracy foron-device recordings, and the accuracy decreased significantly for streamingrecordings. We observed a negative correlation between movement speed and themodel's accuracy for the streaming recordings. Additionally, we evaluated thereliability of ten movement features related to bradykinesia extracted fromvideo recordings of PD patients performing the FT test. While most of thefeatures demonstrated excellent reliability for on-device recordings, most ofthe features demonstrated poor to moderate reliability for streamingrecordings. Our findings highlight the limitations of pose estimationalgorithms when applied to video recordings obtained during Telehealth visits,and demonstrate that on-device recordings can be used for automaticvideo-assessment of bradykinesia in PD.</description><author>Gabriela T. Acevedo Trebbau, Andrea Bandini, Diego L. Guarin</author><pubDate>Mon, 28 Aug 2023 17:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14679v1</guid></item><item><title>Evaluating Open-QA Evaluation</title><link>http://arxiv.org/abs/2305.12421v3</link><description>This study focuses on the evaluation of the Open Question Answering (Open-QA)task, which can directly estimate the factuality of large language models(LLMs). Current automatic evaluation methods have shown limitations, indicatingthat human evaluation still remains the most reliable approach. We introduce anew task, Evaluating QA Evaluation (QA-Eval) and the corresponding datasetEVOUNA, designed to assess the accuracy of AI-generated answers in relation tostandard answers within Open-QA. Our evaluation of these methods utilizeshuman-annotated results to measure their performance. Specifically, the workinvestigates methods that show high correlation with human evaluations, deemingthem more reliable. We also discuss the pitfalls of current methods and methodsto improve LLM-based evaluators. We believe this new QA-Eval task andcorresponding dataset EVOUNA will facilitate the development of more effectiveautomatic evaluation tools and prove valuable for future research in this area.All resources are available at \url{https://github.com/wangcunxiang/QA-Eval}and it is under the Apache-2.0 License.</description><author>Cunxiang Wang, Sirui Cheng, Qipeng Guo, Zhikun Xu, Bowen Ding, Yidong Wang, Xiangkun Hu, Zheng Zhang, Yue Zhang</author><pubDate>Mon, 28 Aug 2023 17:15:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12421v3</guid></item><item><title>Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks</title><link>http://arxiv.org/abs/2211.11869v3</link><description>This effort is focused on examining the behavior of reinforcement learningsystems in personalization environments and detailing the differences in policyentropy associated with the type of learning algorithm utilized. We demonstratethat Policy Optimization agents often possess low-entropy policies duringtraining, which in practice results in agents prioritizing certain actions andavoiding others. Conversely, we also show that Q-Learning agents are far lesssusceptible to such behavior and generally maintain high-entropy policiesthroughout training, which is often preferable in real-world applications. Weprovide a wide range of numerical experiments as well as theoreticaljustification to show that these differences in entropy are due to the type oflearning being employed.</description><author>Anton Dereventsov, Andrew Starnes, Clayton G. Webster</author><pubDate>Mon, 28 Aug 2023 17:14:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11869v3</guid></item><item><title>Wasserstein Geodesic Generator for Conditional Distributions</title><link>http://arxiv.org/abs/2308.10145v3</link><description>Generating samples given a specific label requires estimating conditionaldistributions. We derive a tractable upper bound of the Wasserstein distancebetween conditional distributions to lay the theoretical groundwork to learnconditional distributions. Based on this result, we propose a novel conditionalgeneration algorithm where conditional distributions are fully characterized bya metric space defined by a statistical distance. We employ optimal transporttheory to propose the Wasserstein geodesic generator, a new conditionalgenerator that learns the Wasserstein geodesic. The proposed method learns bothconditional distributions for observed domains and optimal transport mapsbetween them. The conditional distributions given unobserved intermediatedomains are on the Wasserstein geodesic between conditional distributions giventwo observed domain labels. Experiments on face images with light conditions asdomain labels demonstrate the efficacy of the proposed method.</description><author>Young-geun Kim, Kyungbok Lee, Youngwon Choi, Joong-Ho Won, Myunghee Cho Paik</author><pubDate>Mon, 28 Aug 2023 17:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10145v3</guid></item><item><title>Counterpart Fairness -- Addressing Systematic between-group Differences in Fairness Evaluation</title><link>http://arxiv.org/abs/2305.18160v2</link><description>When using machine learning (ML) to aid decision-making, it is critical toensure that an algorithmic decision is fair, i.e., it does not discriminateagainst specific individuals/groups, particularly those from underprivilegedpopulations. Existing group fairness methods require equal group-wise measures,which however fails to consider systematic between-group differences. Theconfounding factors, which are non-sensitive variables but manifest systematicdifferences, can significantly affect fairness evaluation. To tackle thisproblem, we believe that a fairness measurement should be based on thecomparison between counterparts (i.e., individuals who are similar to eachother with respect to the task of interest) from different groups, whose groupidentities cannot be distinguished algorithmically by exploring confoundingfactors. We have developed a propensity-score-based method for identifyingcounterparts, which prevents fairness evaluation from comparing "oranges" with"apples". In addition, we propose a counterpart-based statistical fairnessindex, termed Counterpart-Fairness (CFair), to assess fairness of ML models.Various empirical studies were conducted to validate the effectiveness ofCFair. We publish our code at \url{https://github.com/zhengyjo/CFair}.</description><author>Yifei Wang, Zhengyang Zhou, Liqin Wang, John Laurentiev, Peter Hou, Li Zhou, Pengyu Hong</author><pubDate>Mon, 28 Aug 2023 16:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18160v2</guid></item><item><title>ANER: Arabic and Arabizi Named Entity Recognition using Transformer-Based Approach</title><link>http://arxiv.org/abs/2308.14669v1</link><description>One of the main tasks of Natural Language Processing (NLP), is Named EntityRecognition (NER). It is used in many applications and also can be used as anintermediate step for other tasks. We present ANER, a web-based named entityrecognizer for the Arabic, and Arabizi languages. The model is built upon BERT,which is a transformer-based encoder. It can recognize 50 different entityclasses, covering various fields. We trained our model on the WikiFANE\_Golddataset which consists of Wikipedia articles. We achieved an F1 score of88.7\%, which beats CAMeL Tools' F1 score of 83\% on the ANERcorp dataset,which has only 4 classes. We also got an F1 score of 77.7\% on theNewsFANE\_Gold dataset which contains out-of-domain data from News articles.The system is deployed on a user-friendly web interface that accepts users'inputs in Arabic, or Arabizi. It allows users to explore the entities in thetext by highlighting them. It can also direct users to get information aboutentities through Wikipedia directly. We added the ability to do NER using ourmodel, or CAMeL Tools' model through our website. ANER is publicly accessibleat \url{http://www.aner.online}. We also deployed our model on HuggingFace athttps://huggingface.co/boda/ANER, to allow developers to test and use it.</description><author>Abdelrahman "Boda" Sadallah, Omar Ahmed, Shimaa Mohamed, Omar Hatem, Doaa Hesham, Ahmed H. Yousef</author><pubDate>Mon, 28 Aug 2023 16:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14669v1</guid></item><item><title>Neural Network-Based Histologic Remission Prediction In Ulcerative Colitis</title><link>http://arxiv.org/abs/2308.14667v1</link><description>BACKGROUND &amp; AIMS: Histological remission (HR) is advocated and considered asa new therapeutic target in ulcerative colitis (UC). Diagnosis of histologicremission currently relies on biopsy; during this process, patients are at riskfor bleeding, infection, and post-biopsy fibrosis. In addition, histologicresponse scoring is complex and time-consuming, and there is heterogeneityamong pathologists. Endocytoscopy (EC) is a novel ultra-high magnificationendoscopic technique that can provide excellent in vivo assessment of glands.Based on the EC technique, we propose a neural network model that can assesshistological disease activity in UC using EC images to address the aboveissues. The experiment results demonstrate that the proposed method can assistpatients in precise treatment and prognostic assessment. METHODS: We construct a neural network model for UC evaluation. A total of5105 images of 154 intestinal segments from 87 patients undergoing EC treatmentat a center in China between March 2022 and March 2023 are scored according tothe Geboes score. Subsequently, 103 intestinal segments are used as thetraining set, 16 intestinal segments are used as the validation set for neuralnetwork training, and the remaining 35 intestinal segments are used as the testset to measure the model performance together with the validation set. RESULTS: By treating HR as a negative category and histologic activity as apositive category, the proposed neural network model can achieve an accuracy of0.9, a specificity of 0.95, a sensitivity of 0.75, and an area under the curve(AUC) of 0.81. CONCLUSION: We develop a specific neural network model that can distinguishhistologic remission/activity in EC images of UC, which helps to accelerateclinical histological diagnosis. keywords: ulcerative colitis; Endocytoscopy; Geboes score; neural network.</description><author>Yemin li, Zhongcheng Liu, Xiaoying Lou, Mirigual Kurban, Miao Li, Jie Yang, Kaiwei Che, Jiankun Wang, Max Q. -H Meng, Yan Huang, Qin Guo, Pinjin Hu</author><pubDate>Mon, 28 Aug 2023 16:54:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14667v1</guid></item><item><title>Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward</title><link>http://arxiv.org/abs/2305.02527v2</link><description>We investigate an infinite-horizon average reward Markov Decision Process(MDP) with delayed, composite, and partially anonymous reward feedback. Thedelay and compositeness of rewards mean that rewards generated as a result oftaking an action at a given state are fragmented into different components, andthey are sequentially realized at delayed time instances. The partial anonymityattribute implies that a learner, for each state, only observes the aggregateof past reward components generated as a result of different actions taken atthat state, but realized at the observation instance. We propose an algorithmnamed $\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting andshow that it achieves a regret bound of $\tilde{\mathcal{O}}\left(DS\sqrt{AT} +d (SA)^3\right)$ where $S$ and $A$ are the sizes of the state and actionspaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upperbounded by the maximum reward delay, and $T$ denotes the time horizon. Thisdemonstrates the optimality of the bound in the order of $T$, and an additiveimpact of the delay.</description><author>Washim Uddin Mondal, Vaneet Aggarwal</author><pubDate>Mon, 28 Aug 2023 16:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02527v2</guid></item><item><title>RESTORE: Graph Embedding Assessment Through Reconstruction</title><link>http://arxiv.org/abs/2308.14659v1</link><description>Following the success of Word2Vec embeddings, graph embeddings (GEs) havegained substantial traction. GEs are commonly generated and evaluatedextrinsically on downstream applications, but intrinsic evaluations of theoriginal graph properties in terms of topological structure and semanticinformation have been lacking. Understanding these will help identify thedeficiency of the various families of GE methods when vectorizing graphs interms of preserving the relevant knowledge or learning incorrect knowledge. Toaddress this, we propose RESTORE, a framework for intrinsic GEs assessmentthrough graph reconstruction. We show that reconstructing the original graphfrom the underlying GEs yields insights into the relative amount of informationpreserved in a given vector form. We first introduce the graph reconstructiontask. We generate GEs from three GE families based on factorization methods,random walks, and deep learning (with representative algorithms from eachfamily) on the CommonSense Knowledge Graph (CSKG). We analyze theireffectiveness in preserving the (a) topological structure of node-level graphreconstruction with an increasing number of hops and (b) semantic informationon various word semantic and analogy tests. Our evaluations show deeplearning-based GE algorithm (SDNE) is overall better at preserving (a) with amean average precision (mAP) of 0.54 and 0.35 for 2 and 3-hop reconstructionrespectively, while the factorization-based algorithm (HOPE) is better atencapsulating (b) with an average Euclidean distance of 0.14, 0.17, and 0.11for 1, 2, and 3-hop reconstruction respectively. The modest performance ofthese GEs leaves room for further research avenues on better graphrepresentation learning.</description><author>Hong Yung Yip, Chidaksh Ravuru, Neelabha Banerjee, Shashwat Jha, Amit Sheth, Aman Chadha, Amitava Das</author><pubDate>Mon, 28 Aug 2023 16:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14659v1</guid></item><item><title>Adversarial Predictions of Data Distributions Across Federated Internet-of-Things Devices</title><link>http://arxiv.org/abs/2308.14658v1</link><description>Federated learning (FL) is increasingly becoming the default approach fortraining machine learning models across decentralized Internet-of-Things (IoT)devices. A key advantage of FL is that no raw data are communicated across thenetwork, providing an immediate layer of privacy. Despite this, recent workshave demonstrated that data reconstruction can be done with the locally trainedmodel updates which are communicated across the network. However, many of theseworks have limitations with regard to how the gradients are computed inbackpropagation. In this work, we demonstrate that the model weights shared inFL can expose revealing information about the local data distributions of IoTdevices. This leakage could expose sensitive information to malicious actors ina distributed system. We further discuss results which show that injectingnoise into model weights is ineffective at preventing data leakage withoutseriously harming the global model accuracy.</description><author>Samir Rajani, Dario Dematties, Nathaniel Hudson, Kyle Chard, Nicola Ferrier, Rajesh Sankaran, Peter Beckman</author><pubDate>Mon, 28 Aug 2023 16:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14658v1</guid></item><item><title>DeepHealthNet: Adolescent Obesity Prediction System Based on a Deep Learning Framework</title><link>http://arxiv.org/abs/2308.14657v1</link><description>Childhood and adolescent obesity rates are a global concern because obesityis associated with chronic diseases and long-term health risks. Artificialintelligence technology has emerged as a promising solution to accuratelypredict obesity rates and provide personalized feedback to adolescents. Thisstudy emphasizes the importance of early identification and prevention ofobesity-related health issues. Factors such as height, weight, waistcircumference, calorie intake, physical activity levels, and other relevanthealth information need to be considered for developing robust algorithms forobesity rate prediction and delivering personalized feedback. Hence, bycollecting health datasets from 321 adolescents, we proposed an adolescentobesity prediction system that provides personalized predictions and assistsindividuals in making informed health decisions. Our proposed deep learningframework, DeepHealthNet, effectively trains the model using data augmentationtechniques, even when daily health data are limited, resulting in improvedprediction accuracy (acc: 0.8842). Additionally, the study revealed variationsin the prediction of the obesity rate between boys (acc: 0.9320) and girls(acc: 0.9163), allowing the identification of disparities and the determinationof the optimal time to provide feedback. The proposed system shows significantpotential in effectively addressing childhood and adolescent obesity.</description><author>Ji-Hoon Jeong, In-Gyu Lee, Sung-Kyung Kim, Tae-Eui Kam, Seong-Whan Lee, Euijong Lee</author><pubDate>Mon, 28 Aug 2023 16:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14657v1</guid></item><item><title>Safety Filter Design for Neural Network Systems via Convex Optimization</title><link>http://arxiv.org/abs/2308.08086v2</link><description>With the increase in data availability, it has been widely demonstrated thatneural networks (NN) can capture complex system dynamics precisely in adata-driven manner. However, the architectural complexity and nonlinearity ofthe NNs make it challenging to synthesize a provably safe controller. In thiswork, we propose a novel safety filter that relies on convex optimization toensure safety for a NN system, subject to additive disturbances that arecapable of capturing modeling errors. Our approach leverages tools from NNverification to over-approximate NN dynamics with a set of linear bounds,followed by an application of robust linear MPC to search for controllers thatcan guarantee robust constraint satisfaction. We demonstrate the efficacy ofthe proposed framework numerically on a nonlinear pendulum system.</description><author>Shaoru Chen, Kong Yao Chee, Nikolai Matni, M. Ani Hsieh, George J. Pappas</author><pubDate>Mon, 28 Aug 2023 16:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08086v2</guid></item><item><title>End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear Model Predictive Control</title><link>http://arxiv.org/abs/2308.01674v2</link><description>(Economic) nonlinear model predictive control ((e)NMPC) requires dynamicsystem models that are sufficiently accurate in all relevant state-spaceregions. These models must also be computationally cheap enough to ensurereal-time tractability. Data-driven surrogate models for mechanistic models canbe used to reduce the computational burden of (e)NMPC; however, such models aretypically trained by system identification for maximum average predictionaccuracy on simulation samples and perform suboptimally as part of actual(e)NMPC. We present a method for end-to-end reinforcement learning of dynamicsurrogate models for optimal performance in (e)NMPC applications, resulting inpredictive controllers that strike a favorable balance between controlperformance and computational demand. We validate our method on twoapplications derived from an established nonlinear continuous stirred-tankreactor model. We compare the controller performance to that of MPCs utilizingmodels trained by the prevailing maximum prediction accuracy paradigm, andmodel-free neural network controllers trained using reinforcement learning. Weshow that our method matches the performance of the model-free neural networkcontrollers while consistently outperforming models derived from systemidentification. Additionally, we show that the MPC policies can react tochanges in the control setting without retraining.</description><author>Daniel Mayfrank, Alexander Mitsos, Manuel Dahmen</author><pubDate>Mon, 28 Aug 2023 16:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01674v2</guid></item><item><title>Joint Multiple Intent Detection and Slot Filling with Supervised Contrastive Learning and Self-Distillation</title><link>http://arxiv.org/abs/2308.14654v1</link><description>Multiple intent detection and slot filling are two fundamental and crucialtasks in spoken language understanding. Motivated by the fact that the twotasks are closely related, joint models that can detect intents and extractslots simultaneously are preferred to individual models that perform each taskindependently. The accuracy of a joint model depends heavily on the ability ofthe model to transfer information between the two tasks so that the result ofone task can correct the result of the other. In addition, since a joint modelhas multiple outputs, how to train the model effectively is also challenging.In this paper, we present a method for multiple intent detection and slotfilling by addressing these challenges. First, we propose a bidirectional jointmodel that explicitly employs intent information to recognize slots and slotfeatures to detect intents. Second, we introduce a novel method for trainingthe proposed joint model using supervised contrastive learning andself-distillation. Experimental results on two benchmark datasets MixATIS andMixSNIPS show that our method outperforms state-of-the-art models in bothtasks. The results also demonstrate the contributions of both bidirectionaldesign and the training method to the accuracy improvement. Our source code isavailable at https://github.com/anhtunguyen98/BiSLU</description><author>Nguyen Anh Tu, Hoang Thi Thu Uyen, Tu Minh Phuong, Ngo Xuan Bach</author><pubDate>Mon, 28 Aug 2023 16:36:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14654v1</guid></item><item><title>Learning Visual Tracking and Reaching with Deep Reinforcement Learning on a UR10e Robotic Arm</title><link>http://arxiv.org/abs/2308.14652v1</link><description>As technology progresses, industrial and scientific robots are increasinglybeing used in diverse settings. In many cases, however, programming the robotto perform such tasks is technically complex and costly. To maximize theutility of robots in industrial and scientific settings, they require theability to quickly shift from one task to another. Reinforcement learningalgorithms provide the potential to enable robots to learn optimal solutions tocomplete new tasks without directly reprogramming them. The currentstate-of-the-art in reinforcement learning, however, generally relies on fastsimulations and parallelization to achieve optimal performance. These are oftennot possible in robotics applications. Thus, a significant amount of researchis required to facilitate the efficient and safe, training and deployment ofindustrial and scientific reinforcement learning robots. This technical reportoutlines our initial research into the application of deep reinforcementlearning on an industrial UR10e robot. The report describes the reinforcementlearning environments created to facilitate policy learning with the UR10e, arobotic arm from Universal Robots, and presents our initial results in trainingdeep Q-learning and proximal policy optimization agents on the developedreinforcement learning environments. Our results show that proximal policyoptimization learns a better, more stable policy with less data than deepQ-learning. The corresponding code for this work is available at\url{https://github.com/cbellinger27/bendRL_reacher_tracker}</description><author>Colin Bellinger, Laurence Lamarche-Cliche</author><pubDate>Mon, 28 Aug 2023 16:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14652v1</guid></item><item><title>Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset</title><link>http://arxiv.org/abs/2306.11167v2</link><description>The quest for human imitative AI has been an enduring topic in AI researchsince its inception. The technical evolution and emerging capabilities of thelatest cohort of large language models (LLMs) have reinvigorated the subjectbeyond academia to the cultural zeitgeist. While recent NLP evaluationbenchmark tasks test some aspects of human-imitative behaviour (e.g.,BIG-bench's 'human-like behavior' tasks), few, if not none, examine creativeproblem solving abilities. Creative problem solving in humans is a well-studiedtopic in cognitive neuroscience with standardized tests that predominantly usethe ability to associate (heterogeneous) connections among clue words as ametric for creativity. Exposure to misleading stimuli - distractors dubbed redherrings - impede human performance in such tasks via the fixation effect andEinstellung paradigm. In cognitive neuroscience studies, such fixations areexperimentally induced by pre-exposing participants to orthographically similarincorrect words to subsequent word-fragments or clues. The popular British quizshow Only Connect's Connecting Wall segment essentially mimics Mednick's RemoteAssociates Test (RAT) formulation with built-in, deliberate red herrings, whichmakes it an ideal proxy dataset to explore and study fixation effect andEinstellung paradigm from cognitive neuroscience in LLMs. In this paper wepresent the novel Only Connect Wall (OCW) dataset and report results from ourevaluation of selected pre-trained language models and LLMs on creative problemsolving tasks like grouping clue words by heterogeneous connections, andidentifying correct open knowledge domain connections in respective groups. Wesynthetically generate two additional datasets: OCW-Randomized, OCW-WordNet tofurther analyze our red-herrings hypothesis in language models. The code andlink to the dataset are available at https://github.com/TaatiTeam/OCW.</description><author>Saeid Naeini, Raeid Saqur, Mozhgan Saeidi, John Giorgi, Babak Taati</author><pubDate>Mon, 28 Aug 2023 16:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11167v2</guid></item><item><title>Comparison of automated crater catalogs for Mars from Benedix et al. (2020) and Lee and Hogan (2021)</title><link>http://arxiv.org/abs/2308.14650v1</link><description>Crater mapping using neural networks and other automated methods hasincreased recently with automated Crater Detection Algorithms (CDAs) applied toplanetary bodies throughout the solar system. A recent publication by Benedixet al. (2020) showed high performance at small scales compared to similarautomated CDAs but with a net positive diameter bias in many crater candidates.I compare the publicly available catalogs from Benedix et al. (2020) and Lee &amp;Hogan (2021) and show that the reported performance is sensitive to the metricsused to test the catalogs. I show how the more permissive comparison methodsindicate a higher CDA performance by allowing worse candidate craters to matchground-truth craters. I show that the Benedix et al. (2020) catalog has asubstantial performance loss with increasing latitude and identify an imageprojection issue that might cause this loss. Finally, I suggest futureapplications of neural networks in generating large scientific datasets bevalidated using secondary networks with independent data sources or trainingmethods.</description><author>Christopher Lee</author><pubDate>Mon, 28 Aug 2023 16:22:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14650v1</guid></item><item><title>Edge Generation Scheduling for DAG Tasks using Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2308.14647v1</link><description>Directed acyclic graph (DAG) tasks are currently adopted in the real-timedomain to model complex applications from the automotive, avionics, andindustrial domain that implement their functionalities through chains ofintercommunicating tasks. This paper studies the problem of schedulingreal-time DAG tasks by presenting a novel schedulability test based on theconcept of trivial schedulability. Using this schedulability test, we propose anew DAG scheduling framework (edge generation scheduling -- EGS) that attemptsto minimize the DAG width by iteratively generating edges while guaranteeingthe deadline constraint. We study how to efficiently solve the problem ofgenerating edges by developing a deep reinforcement learning algorithm combinedwith a graph representation neural network to learn an efficient edgegeneration policy for EGS. We evaluate the effectiveness of the proposedalgorithm by comparing it with state-of-the-art DAG scheduling heuristics andan optimal mixed-integer linear programming baseline. Experimental results showthat the proposed algorithm outperforms the state-of-the-art by requiring fewerprocessors to schedule the same DAG tasks.</description><author>Binqi Sun, Mirco Theile, Ziyuan Qin, Daniele Bernardini, Debayan Roy, Andrea Bastoni, Marco Caccamo</author><pubDate>Mon, 28 Aug 2023 16:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14647v1</guid></item><item><title>What's meant by explainable model: A Scoping Review</title><link>http://arxiv.org/abs/2307.09673v2</link><description>We often see the term explainable in the titles of papers that describeapplications based on artificial intelligence (AI). However, the literature inexplainable artificial intelligence (XAI) indicates that explanations in XAIare application- and domain-specific, hence requiring evaluation whenever theyare employed to explain a model that makes decisions for a specific applicationproblem. Additionally, the literature reveals that the performance of post-hocmethods, particularly feature attribution methods, varies substantially hintingthat they do not represent a solution to AI explainability. Therefore, whenusing XAI methods, the quality and suitability of their information outputsshould be evaluated within the specific application. For these reasons, we useda scoping review methodology to investigate papers that apply AI models andadopt methods to generate post-hoc explanations while referring to said modelsas explainable. This paper investigates whether the term explainable model isadopted by authors under the assumption that incorporating a post-hoc XAImethod suffices to characterize a model as explainable. To inspect thisproblem, our review analyzes whether these papers conducted evaluations. Wefound that 81% of the application papers that refer to their approaches as anexplainable model do not conduct any form of evaluation on the XAI method theyused.</description><author>Mallika Mainali, Rosina O Weber</author><pubDate>Mon, 28 Aug 2023 16:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09673v2</guid></item><item><title>Human Comfortability Index Estimation in Industrial Human-Robot Collaboration Task</title><link>http://arxiv.org/abs/2308.14644v1</link><description>Fluent human-robot collaboration requires a robot teammate to understand,learn, and adapt to the human's psycho-physiological state. Such collaborationsrequire a computing system that monitors human physiological signals duringhuman-robot collaboration (HRC) to quantitatively estimate a human's level ofcomfort, which we have termed in this research as comfortability index (CI) anduncomfortability index (unCI). Subjective metrics (surprise, anxiety, boredom,calmness, and comfortability) and physiological signals were collected during ahuman-robot collaboration experiment that varied robot behavior. The emotioncircumplex model is adapted to calculate the CI from the participant'squantitative data as well as physiological data. To estimate CI/unCI fromphysiological signals, time features were extracted from electrocardiogram(ECG), galvanic skin response (GSR), and pupillometry signals. In thisresearch, we successfully adapt the circumplex model to find the location(axis) of 'comfortability' and 'uncomfortability' on the circumplex model, andits location match with the closest emotions on the circumplex model. Finally,the study showed that the proposed approach can estimate humancomfortability/uncomfortability from physiological signals.</description><author>Celal Savur, Jamison Heard, Ferat Sahin</author><pubDate>Mon, 28 Aug 2023 16:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14644v1</guid></item><item><title>Rate-Optimal Policy Optimization for Linear Markov Decision Processes</title><link>http://arxiv.org/abs/2308.14642v1</link><description>We study regret minimization in online episodic linear Markov DecisionProcesses, and obtain rate-optimal $\widetilde O (\sqrt K)$ regret where $K$denotes the number of episodes. Our work is the first to establish the optimal(w.r.t.~$K$) rate of convergence in the stochastic setting with bandit feedbackusing a policy optimization based approach, and the first to establish theoptimal (w.r.t.~$K$) rate in the adversarial setup with full informationfeedback, for which no algorithm with an optimal rate guarantee is currentlyknown.</description><author>Uri Sherman, Alon Cohen, Tomer Koren, Yishay Mansour</author><pubDate>Mon, 28 Aug 2023 16:16:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14642v1</guid></item><item><title>Challenges of GPT-3-based Conversational Agents for Healthca</title><link>http://arxiv.org/abs/2308.14641v1</link><description>The potential to provide patients with faster information access whileallowing medical specialists to concentrate on critical tasks makes medicaldomain dialog agents appealing. However, the integration of large-languagemodels (LLMs) into these agents presents certain limitations that may result inserious consequences. This paper investigates the challenges and risks of usingGPT-3-based models for medical question-answering (MedQA). We perform severalevaluations contextualized in terms of standard medical principles. We providea procedure for manually designing patient queries to stress-test high-risklimitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail torespond adequately to these queries, generating erroneous medical information,unsafe recommendations, and content that may be considered offensive.</description><author>Fabian Lechner, Allison Lahnala, Charles Welch, Lucie Flek</author><pubDate>Mon, 28 Aug 2023 16:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14641v1</guid></item><item><title>Domain Generalization with Correlated Style Uncertainty</title><link>http://arxiv.org/abs/2212.09950v3</link><description>Domain generalization (DG) approaches intend to extract domain invariantfeatures that can lead to a more robust deep learning model. In this regard,style augmentation is a strong DG method taking advantage of instance-specificfeature statistics containing informative style characteristics to syntheticnovel domains. While it is one of the state-of-the-art methods, prior works onstyle augmentation have either disregarded the interdependence amongst distinctfeature channels or have solely constrained style augmentation to linearinterpolation. To address these research gaps, in this work, we introduce anovel augmentation approach, named Correlated Style Uncertainty (CSU),surpassing the limitations of linear interpolation in style statistic space andsimultaneously preserving vital correlation information. Our method's efficacyis established through extensive experimentation on diverse cross-domaincomputer vision and medical imaging classification tasks: PACS, Office-Home,and Camelyon17 datasets, and the Duke-Market1501 instance retrieval task. Theresults showcase a remarkable improvement margin over existing state-of-the-arttechniques. The source code is available https://github.com/freshman97/CSU.</description><author>Zheyuan Zhang, Bin Wang, Debesh Jha, Ugur Demir, Ulas Bagci</author><pubDate>Mon, 28 Aug 2023 16:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09950v3</guid></item><item><title>Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance</title><link>http://arxiv.org/abs/2308.14634v1</link><description>We propose the use of conversational GPT models for easy and quick few-shottext classification in the financial domain using the Banking77 dataset. Ourapproach involves in-context learning with GPT-3.5 and GPT-4, which minimizesthe technical expertise required and eliminates the need for expensive GPUcomputing while yielding quick and accurate results. Additionally, we fine-tuneother pre-trained, masked language models with SetFit, a recent contrastivelearning technique, to achieve state-of-the-art results both in full-data andfew-shot settings. Our findings show that querying GPT-3.5 and GPT-4 canoutperform fine-tuned, non-generative models even with fewer examples. However,subscription fees associated with these solutions may be considered costly forsmall organizations. Lastly, we find that generative models perform better onthe given task when shown representative samples selected by a human expertrather than when shown random ones. We conclude that a) our proposed methodsoffer a practical solution for few-shot tasks in datasets with limited labelavailability, and b) our state-of-the-art results can inspire future work inthe area.</description><author>Lefteris Loukas, Ilias Stogiannidis, Prodromos Malakasiotis, Stavros Vassos</author><pubDate>Mon, 28 Aug 2023 16:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14634v1</guid></item><item><title>Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer</title><link>http://arxiv.org/abs/2207.01964v3</link><description>The increasing capabilities of quantum computing hardware and the challengeof realizing deep quantum circuits require fully automated and efficient toolsfor compiling quantum circuits. To express arbitrary circuits in a sequence ofnative gates specific to the quantum computer architecture, it is necessary tomake algorithms portable across the landscape of quantum hardware providers. Inthis work, we present a compiler capable of transforming and optimizing aquantum circuit targeting a shuttling-based trapped-ion quantum processor. Itconsists of custom algorithms set on top of the quantum circuit frameworkPytket. The performance was evaluated for a wide range of quantum circuits andthe results show that the gate counts can be reduced by factors up to 5.1compared to standard Pytket and up to 2.2 compared to standard Qiskitcompilation.</description><author>Fabian Kreppel, Christian Melzer, Diego Olvera MillÃ¡n, Janis Wagner, Janine Hilder, Ulrich Poschinger, Ferdinand Schmidt-Kaler, AndrÃ© Brinkmann</author><pubDate>Mon, 28 Aug 2023 16:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.01964v3</guid></item><item><title>Comparing AutoML and Deep Learning Methods for Condition Monitoring using Realistic Validation Scenarios</title><link>http://arxiv.org/abs/2308.14632v1</link><description>This study extensively compares conventional machine learning methods anddeep learning for condition monitoring tasks using an AutoML toolbox. Theexperiments reveal consistent high accuracy in random K-fold cross-validationscenarios across all tested models. However, when employing leave-one-group-out(LOGO) cross-validation on the same datasets, no clear winner emerges,indicating the presence of domain shift in real-world scenarios. Additionally,the study assesses the scalability and interpretability of conventional methodsand neural networks. Conventional methods offer explainability with theirmodular structure aiding feature identification. In contrast, neural networksrequire specialized interpretation techniques like occlusion maps to visualizeimportant regions in the input data. Finally, the paper highlights thesignificance of feature selection, particularly in condition monitoring taskswith limited class variations. Low-complexity models prove sufficient for suchtasks, as only a few features from the input signal are typically needed. Insummary, these findings offer crucial insights into the strengths andlimitations of various approaches, providing valuable benchmarks andidentifying the most suitable methods for condition monitoring applications,thereby enhancing their applicability in real-world scenarios.</description><author>Payman Goodarzi, Andreas SchÃ¼tze, Tizian Schneider</author><pubDate>Mon, 28 Aug 2023 15:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14632v1</guid></item><item><title>VesselShot: Few-shot learning for cerebral blood vessel segmentation</title><link>http://arxiv.org/abs/2308.14626v1</link><description>Angiography is widely used to detect, diagnose, and treat cerebrovasculardiseases. While numerous techniques have been proposed to segment the vascularnetwork from different imaging modalities, deep learning (DL) has emerged as apromising approach. However, existing DL methods often depend on proprietarydatasets and extensive manual annotation. Moreover, the availability ofpre-trained networks specifically for medical domains and 3D volumes islimited. To overcome these challenges, we propose a few-shot learning approachcalled VesselShot for cerebrovascular segmentation. VesselShot leveragesknowledge from a few annotated support images and mitigates the scarcity oflabeled data and the need for extensive annotation in cerebral blood vesselsegmentation. We evaluated the performance of VesselShot using the publiclyavailable TubeTK dataset for the segmentation task, achieving a mean Dicecoefficient (DC) of 0.62(0.03).</description><author>Mumu Aktar, Hassan Rivaz, Marta Kersten-Oertel, Yiming Xiao</author><pubDate>Mon, 28 Aug 2023 15:48:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14626v1</guid></item><item><title>Compositional Semantic Mix for Domain Adaptation in Point Cloud Segmentation</title><link>http://arxiv.org/abs/2308.14619v1</link><description>Deep-learning models for 3D point cloud semantic segmentation exhibit limitedgeneralization capabilities when trained and tested on data captured withdifferent sensors or in varying environments due to domain shift. Domainadaptation methods can be employed to mitigate this domain shift, for instance,by simulating sensor noise, developing domain-agnostic generators, or trainingpoint cloud completion networks. Often, these methods are tailored for rangeview maps or necessitate multi-modal input. In contrast, domain adaptation inthe image domain can be executed through sample mixing, which emphasizes inputdata manipulation rather than employing distinct adaptation modules. In thisstudy, we introduce compositional semantic mixing for point cloud domainadaptation, representing the first unsupervised domain adaptation technique forpoint cloud segmentation based on semantic and geometric sample mixing. Wepresent a two-branch symmetric network architecture capable of concurrentlyprocessing point clouds from a source domain (e.g. synthetic) and point cloudsfrom a target domain (e.g. real-world). Each branch operates within one domainby integrating selected data fragments from the other domain and utilizingsemantic information derived from source labels and target (pseudo) labels.Additionally, our method can leverage a limited number of human point-levelannotations (semi-supervised) to further enhance performance. We assess ourapproach in both synthetic-to-real and real-to-real scenarios using LiDARdatasets and demonstrate that it significantly outperforms state-of-the-artmethods in both unsupervised and semi-supervised settings.</description><author>Cristiano Saltori, Fabio Galasso, Giuseppe Fiameni, Nicu Sebe, Fabio Poiesi, Elisa Ricci</author><pubDate>Mon, 28 Aug 2023 15:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14619v1</guid></item><item><title>Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning</title><link>http://arxiv.org/abs/2305.02054v2</link><description>Deep Reinforcement Learning agents often suffer from catastrophic forgetting,forgetting previously found solutions in parts of the input space when trainingon new data. Replay Memories are a common solution to the problem,decorrelating and shuffling old and new training samples. They naively storestate transitions as they come in, without regard for redundancy. We introducea novel cognitive-inspired replay memory approach based on theGrow-When-Required (GWR) self-organizing network, which resembles a map-basedmental model of the world. Our approach organizes stored transitions into aconcise environment-model-like network of state-nodes and transition-edges,merging similar samples to reduce the memory size and increase pair-wisedistance among samples, which increases the relevancy of each sample. Overall,our paper shows that map-based experience replay allows for significant memoryreduction with only small performance decreases.</description><author>Muhammad Burhan Hafez, Tilman Immisch, Tom Weber, Stefan Wermter</author><pubDate>Mon, 28 Aug 2023 15:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02054v2</guid></item><item><title>QuadConv: Quadrature-Based Convolutions with Applications to Non-Uniform PDE Data Compression</title><link>http://arxiv.org/abs/2211.05151v3</link><description>We present a new convolution layer for deep learning architectures which wecall QuadConv -- an approximation to continuous convolution via quadrature. Ouroperator is developed explicitly for use on non-uniform, mesh-based data, andaccomplishes this by learning a continuous kernel that can be sampled atarbitrary locations. Moreover, the construction of our operator admits anefficient implementation which we detail and construct. As an experimentalvalidation of our operator, we consider the task of compressing partialdifferential equation (PDE) simulation data from fixed meshes. We show thatQuadConv can match the performance of standard discrete convolutions on uniformgrid data by comparing a QuadConv autoencoder (QCAE) to a standardconvolutional autoencoder (CAE). Further, we show that the QCAE can maintainthis accuracy even on non-uniform data. In both cases, QuadConv alsooutperforms alternative unstructured convolution methods such as graphconvolution.</description><author>Kevin Doherty, Cooper Simpson, Stephen Becker, Alireza Doostan</author><pubDate>Mon, 28 Aug 2023 15:38:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05151v3</guid></item><item><title>VoroMesh: Learning Watertight Surface Meshes with Voronoi Diagrams</title><link>http://arxiv.org/abs/2308.14616v1</link><description>In stark contrast to the case of images, finding a concise, learnablediscrete representation of 3D surfaces remains a challenge. In particular,while polygon meshes are arguably the most common surface representation usedin geometry processing, their irregular and combinatorial structure often makethem unsuitable for learning-based applications. In this work, we presentVoroMesh, a novel and differentiable Voronoi-based representation of watertight3D shape surfaces. From a set of 3D points (called generators) and theirassociated occupancy, we define our boundary representation through the Voronoidiagram of the generators as the subset of Voronoi faces whose two associated(equidistant) generators are of opposite occupancy: the resulting polygon meshforms a watertight approximation of the target shape's boundary. To learn theposition of the generators, we propose a novel loss function, dubbed VoroLoss,that minimizes the distance from ground truth surface samples to the closestfaces of the Voronoi diagram which does not require an explicit construction ofthe entire Voronoi diagram. A direct optimization of the Voroloss to obtaingenerators on the Thingi32 dataset demonstrates the geometric efficiency of ourrepresentation compared to axiomatic meshing algorithms and recentlearning-based mesh representations. We further use VoroMesh in alearning-based mesh prediction task from input SDF grids on the ABC dataset,and show comparable performance to state-of-the-art methods while guaranteeingclosed output surfaces free of self-intersections.</description><author>Nissim Maruani, Roman Klokov, Maks Ovsjanikov, Pierre Alliez, Mathieu Desbrun</author><pubDate>Mon, 28 Aug 2023 15:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14616v1</guid></item><item><title>SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning</title><link>http://arxiv.org/abs/2305.19442v4</link><description>Federated bilevel optimization (FBO) has shown great potential recently inmachine learning and edge computing due to the emerging nested optimizationstructure in meta-learning, fine-tuning, hyperparameter tuning, etc. However,existing FBO algorithms often involve complicated computations and requiremultiple sub-loops per iteration, each of which contains a number ofcommunication rounds. In this paper, we propose a simple and flexible FBOframework named SimFBO, which is easy to implement without sub-loops, andincludes a generalized server-side aggregation and update for improvingcommunication efficiency. We further propose System-level heterogeneity robustFBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneouslocal computation. We show that SimFBO and ShroFBO provably achieve a linearconvergence speedup with partial client participation and client samplingwithout replacement, as well as improved sample and communication complexities.Experiments demonstrate the effectiveness of the proposed methods over existingFBO algorithms.</description><author>Yifan Yang, Peiyao Xiao, Kaiyi Ji</author><pubDate>Mon, 28 Aug 2023 15:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19442v4</guid></item><item><title>MS-Net: A Multi-modal Self-supervised Network for Fine-Grained Classification of Aircraft in SAR Images</title><link>http://arxiv.org/abs/2308.14613v1</link><description>Synthetic aperture radar (SAR) imaging technology is commonly used to provide24-hour all-weather earth observation. However, it still has some drawbacks inSAR target classification, especially in fine-grained classification ofaircraft: aircrafts in SAR images have large intra-class diversity andinter-class similarity; the number of effective samples is insufficient andit's hard to annotate. To address these issues, this article proposes a novelmulti-modal self-supervised network (MS-Net) for fine-grained classification ofaircraft. Firstly, in order to entirely exploit the potential of multi-modalinformation, a two-sided path feature extraction network (TSFE-N) isconstructed to enhance the image feature of the target and obtain the domainknowledge feature of text mode. Secondly, a contrastive self-supervisedlearning (CSSL) framework is employed to effectively learn usefullabel-independent feature from unbalanced data, a similarity per-ception loss(SPloss) is proposed to avoid network overfitting. Finally, TSFE-N is used asthe encoder of CSSL to obtain the classification results. Through a largenumber of experiments, our MS-Net can effectively reduce the difficulty ofclassifying similar types of aircrafts. In the case of no label, the proposedalgorithm achieves an accuracy of 88.46% for 17 types of air-craftclassification task, which has pioneering significance in the field offine-grained classification of aircraft in SAR images.</description><author>Bingying Yue, Jianhao Li, Hao Shi, Yupei Wang, Honghu Zhong</author><pubDate>Mon, 28 Aug 2023 15:28:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14613v1</guid></item><item><title>A Transformer-Conditioned Neural Fields Pipeline with Polar Coordinate Representation for Astronomical Radio Interferometric Data Reconstruction</title><link>http://arxiv.org/abs/2308.14610v1</link><description>In radio astronomy, visibility data, which are measurements of wave signalsfrom radio telescopes, are transformed into images for observation of distantcelestial objects. However, these resultant images usually contain both realsources and artifacts, due to signal sparsity and other factors. One way toobtain cleaner images is to reconstruct samples into dense forms beforeimaging. Unfortunately, existing visibility reconstruction methods may misssome components of the frequency data, so blurred object edges and persistentartifacts remain in the images. Furthermore, the computation overhead is highon irregular visibility samples due to the data skew. To address theseproblems, we propose PolarRec, a reconstruction method for interferometricvisibility data, which consists of a transformer-conditioned neural fieldspipeline with a polar coordinate representation. This representation matchesthe way in which telescopes observe a celestial area as the Earth rotates. Wefurther propose Radial Frequency Loss function, using radial coordinates in thepolar coordinate system to correlate with the frequency information, to helpreconstruct complete visibility. We also group visibility sample points byangular coordinates in the polar coordinate system, and use groups as thegranularity for subsequent encoding with a Transformer encoder. Consequently,our method can capture the inherent characteristics of visibility dataeffectively and efficiently. Our experiments demonstrate that PolarRec markedlyimproves imaging results by faithfully reconstructing all frequency componentsin the visibility domain while significantly reducing the computation cost.</description><author>Ruoqi Wang, Qiong Luo, Feng Wang</author><pubDate>Mon, 28 Aug 2023 15:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14610v1</guid></item><item><title>AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics</title><link>http://arxiv.org/abs/2308.14608v1</link><description>The introduction of ChatGPT and the subsequent improvement of Large LanguageModels (LLMs) have prompted more and more individuals to turn to the use ofChatBots, both for information and assistance with decision-making. However,the information the user is after is often not formulated by these ChatBotsobjectively enough to be provided with a definite, globally accepted answer. Controversial topics, such as "religion", "gender identity", "freedom ofspeech", and "equality", among others, can be a source of conflict as partisanor biased answers can reinforce preconceived notions or promote disinformation.By exposing ChatGPT to such debatable questions, we aim to understand its levelof awareness and if existing models are subject to socio-political and/oreconomic biases. We also aim to explore how AI-generated answers compare tohuman ones. For exploring this, we use a dataset of a social media platformcreated for the purpose of debating human-generated claims on polemic subjectsamong users, dubbed Kialo. Our results show that while previous versions of ChatGPT have had importantissues with controversial topics, more recent versions of ChatGPT(gpt-3.5-turbo) are no longer manifesting significant explicit biases inseveral knowledge areas. In particular, it is well-moderated regarding economicaspects. However, it still maintains degrees of implicit libertarian leaningtoward right-winged ideals which suggest the need for increased moderation fromthe socio-political point of view. In terms of domain knowledge oncontroversial topics, with the exception of the "Philosophical" category,ChatGPT is performing well in keeping up with the collective human level ofknowledge. Finally, we see that sources of Bing AI have slightly more tendencyto the center when compared to human answers. All the analyses we make aregeneralizable to other types of biases and domains.</description><author>Vahid Ghafouri, Vibhor Agarwal, Yong Zhang, Nishanth Sastry, Jose Such, Guillermo Suarez-Tangil</author><pubDate>Mon, 28 Aug 2023 15:23:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14608v1</guid></item><item><title>On the Tradeoff between Privacy Preservation and Byzantine-Robustness in Decentralized Learning</title><link>http://arxiv.org/abs/2308.14606v1</link><description>This paper jointly considers privacy preservation and Byzantine-robustness indecentralized learning. In a decentralized network, honest-but-curious agentsfaithfully follow the prescribed algorithm, but expect to infer theirneighbors' private data from messages received during the learning process,while dishonest-and-Byzantine agents disobey the prescribed algorithm, anddeliberately disseminate wrong messages to their neighbors so as to bias thelearning process. For this novel setting, we investigate a genericprivacy-preserving and Byzantine-robust decentralized stochastic gradientdescent (SGD) framework, in which Gaussian noise is injected to preserveprivacy and robust aggregation rules are adopted to counteract Byzantineattacks. We analyze its learning error and privacy guarantee, discovering anessential tradeoff between privacy preservation and Byzantine-robustness indecentralized learning -- the learning error caused by defending againstByzantine attacks is exacerbated by the Gaussian noise added to preserveprivacy. Numerical experiments are conducted and corroborate our theoreticalfindings.</description><author>Haoxiang Ye, Heng Zhu, Qing Ling</author><pubDate>Mon, 28 Aug 2023 15:20:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14606v1</guid></item><item><title>Assumption-lean falsification tests of rate double-robustness of double-machine-learning estimators</title><link>http://arxiv.org/abs/2306.10590v4</link><description>The class of doubly-robust (DR) functionals studied by Rotnitzky et al.(2021) is of central importance in economics and biostatistics. It strictlyincludes both (i) the class of mean-square continuous functionals that can bewritten as an expectation of an affine functional of a conditional expectationstudied by Chernozhukov et al. (2022b) and (ii) the class of functionalsstudied by Robins et al. (2008). The present state-of-the-art estimators for DRfunctionals $\psi$ are double-machine-learning (DML) estimators (Chernozhukovet al., 2018). A DML estimator $\widehat{\psi}_{1}$ of $\psi$ depends onestimates $\widehat{p} (x)$ and $\widehat{b} (x)$ of a pair of nuisancefunctions $p(x)$ and $b(x)$, and is said to satisfy "rate double-robustness" ifthe Cauchy--Schwarz upper bound of its bias is $o (n^{- 1/2})$. Were itachievable, our scientific goal would have been to construct valid,assumption-lean (i.e. no complexity-reducing assumptions on $b$ or $p$) testsof the validity of a nominal $(1 - \alpha)$ Wald confidence interval (CI)centered at $\widehat{\psi}_{1}$. But this would require a test of the bias tobe $o (n^{-1/2})$, which can be shown not to exist. We therefore adopt the lessambitious goal of falsifying, when possible, an analyst's justification for herclaim that the reported $(1 - \alpha)$ Wald CI is valid. In many instances, ananalyst justifies her claim by imposing complexity-reducing assumptions on $b$and $p$ to ensure "rate double-robustness". Here we exhibit valid,assumption-lean tests of $H_{0}$: "rate double-robustness holds", withnon-trivial power against certain alternatives. If $H_{0}$ is rejected, we willhave falsified her justification. However, no assumption-lean test of $H_{0}$,including ours, can be a consistent test. Thus, the failure of our test toreject is not meaningful evidence in favor of $H_{0}$.</description><author>Lin Liu, Rajarshi Mukherjee, James M. Robins</author><pubDate>Mon, 28 Aug 2023 15:19:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10590v4</guid></item><item><title>A Generalization of Continuous Relaxation in Structured Pruning</title><link>http://arxiv.org/abs/2308.14605v1</link><description>Deep learning harnesses massive parallel floating-point processing to trainand evaluate large neural networks. Trends indicate that deeper and largerneural networks with an increasing number of parameters achieve higher accuracythan smaller neural networks. This performance improvement, which oftenrequires heavy compute for both training and evaluation, eventually needs totranslate well to resource-constrained hardware for practical value. Structuredpruning asserts that while large networks enable us to find solutions tocomplex computer vision problems, a smaller, computationally efficientsub-network can be derived from the large neural network that retains modelaccuracy but significantly improves computational efficiency. We generalize structured pruning with algorithms for network augmentation,pruning, sub-network collapse and removal. In addition, we demonstrateefficient and stable convergence up to 93% sparsity and 95% FLOPs reductionwithout loss of inference accuracy using with continuous relaxation matching orexceeding the state of the art for all structured pruning methods. Theresulting CNN executes efficiently on GPU hardware without computationallyexpensive sparse matrix operations. We achieve this with routine automatableoperations on classification and segmentation problems using CIFAR-10,ImageNet, and CityScapes datasets with the ResNet and U-NET networkarchitectures.</description><author>Brad Larson, Bishal Upadhyaya, Luke McDermott, Siddha Ganju</author><pubDate>Mon, 28 Aug 2023 15:19:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14605v1</guid></item><item><title>SoGAR: Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition</title><link>http://arxiv.org/abs/2305.06310v3</link><description>This paper introduces a novel approach to Social Group Activity Recognition(SoGAR) using Self-supervised Transformers network that can effectively utilizeunlabeled video data. To extract spatio-temporal information, we created localand global views with varying frame rates. Our self-supervised objectiveensures that features extracted from contrasting views of the same video wereconsistent across spatio-temporal domains. Our proposed approach is efficientin using transformer-based encoders to alleviate the weakly supervised settingof group activity recognition. By leveraging the benefits of transformermodels, our approach can model long-term relationships along spatio-temporaldimensions. Our proposed SoGAR method achieved state-of-the-art results onthree group activity recognition benchmarks, namely JRDB-PAR, NBA, andVolleyball datasets, surpassing the current numbers in terms of F1-score, MCA,and MPCA metrics.</description><author>Naga VS Raviteja Chappa, Pha Nguyen, Alexander H Nelson, Han-Seok Seo, Xin Li, Page Daniel Dobbs, Khoa Luu</author><pubDate>Mon, 28 Aug 2023 15:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06310v3</guid></item><item><title>SAM-PARSER: Fine-tuning SAM Efficiently by Parameter Space Reconstruction</title><link>http://arxiv.org/abs/2308.14604v1</link><description>Segment Anything Model (SAM) has received remarkable attention as it offers apowerful and versatile solution for object segmentation in images. However,fine-tuning SAM for downstream segmentation tasks under different scenariosremains a challenge, as the varied characteristics of different scenariosnaturally requires diverse model parameter spaces. Most existing fine-tuningmethods attempt to bridge the gaps among different scenarios by introducing aset of new parameters to modify SAM's original parameter space. Unlike theseworks, in this paper, we propose fine-tuning SAM efficiently by parameter spacereconstruction (SAM-PARSER), which introduce nearly zero trainable parametersduring fine-tuning. In SAM-PARSER, we assume that SAM's original parameterspace is relatively complete, so that its bases are able to reconstruct theparameter space of a new scenario. We obtain the bases by matrix decomposition,and fine-tuning the coefficients to reconstruct the parameter space tailored tothe new scenario by an optimal linear combination of the bases. Experimentalresults show that SAM-PARSER exhibits superior segmentation performance acrossvarious scenarios, while reducing the number of trainable parameters by$\approx 290$ times compared with current parameter-efficient fine-tuningmethods.</description><author>Zelin Peng, Zhengqin Xu, Zhilin Zeng, Xiaokang Yang, Wei Shen</author><pubDate>Mon, 28 Aug 2023 15:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14604v1</guid></item><item><title>Large Language Models Vote: Prompting for Rare Disease Identification</title><link>http://arxiv.org/abs/2308.12890v2</link><description>The emergence of generative Large Language Models (LLMs) emphasizes the needfor accurate and efficient prompting approaches. LLMs are often applied inFew-Shot Learning (FSL) contexts, where tasks are executed with minimaltraining data. FSL has become popular in many Artificial Intelligence (AI)subdomains, including AI for health. Rare diseases affect a small fraction ofthe population. Rare disease identification from clinical notes inherentlyrequires FSL techniques due to limited data availability. Manual datacollection and annotation is both expensive and time-consuming. In this paper,we propose Models-Vote Prompting (MVP), a flexible prompting approach forimproving the performance of LLM queries in FSL settings. MVP works byprompting numerous LLMs to perform the same tasks and then conducting amajority vote on the resulting outputs. This method achieves improved resultsto any one model in the ensemble on one-shot rare disease identification andclassification tasks. We also release a novel rare disease dataset for FSL,available to those who signed the MIMIC-IV Data Use Agreement (DUA).Furthermore, in using MVP, each model is prompted multiple times, substantiallyincreasing the time needed for manual annotation, and to address this, weassess the feasibility of using JSON for automating generative LLM evaluation.</description><author>David Oniani, Jordan Hilsman, Hang Dong, Fengyi Gao, Shiven Verma, Yanshan Wang</author><pubDate>Mon, 28 Aug 2023 15:16:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12890v2</guid></item><item><title>SPARTAN: Self-supervised Spatiotemporal Transformers Approach to Group Activity Recognition</title><link>http://arxiv.org/abs/2303.12149v4</link><description>In this paper, we propose a new, simple, and effective Self-supervisedSpatio-temporal Transformers (SPARTAN) approach to Group Activity Recognition(GAR) using unlabeled video data. Given a video, we create local and globalSpatio-temporal views with varying spatial patch sizes and frame rates. Theproposed self-supervised objective aims to match the features of thesecontrasting views representing the same video to be consistent with thevariations in spatiotemporal domains. To the best of our knowledge, theproposed mechanism is one of the first works to alleviate the weakly supervisedsetting of GAR using the encoders in video transformers. Furthermore, using theadvantage of transformer models, our proposed approach supports long-termrelationship modeling along spatio-temporal dimensions. The proposed SPARTANapproach performs well on two group activity recognition benchmarks, includingNBA and Volleyball datasets, by surpassing the state-of-the-art results by asignificant margin in terms of MCA and MPCA metrics.</description><author>Naga VS Raviteja Chappa, Pha Nguyen, Alexander H Nelson, Han-Seok Seo, Xin Li, Page Daniel Dobbs, Khoa Luu</author><pubDate>Mon, 28 Aug 2023 15:13:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12149v4</guid></item><item><title>Recent Progress in Energy Management of Connected Hybrid Electric Vehicles Using Reinforcement Learning</title><link>http://arxiv.org/abs/2308.14602v1</link><description>The growing adoption of hybrid electric vehicles (HEVs) presents atransformative opportunity for revolutionizing transportation energy systems.The shift towards electrifying transportation aims to curb environmentalconcerns related to fossil fuel consumption. This necessitates efficient energymanagement systems (EMS) to optimize energy efficiency. The evolution of EMSfrom HEVs to connected hybrid electric vehicles (CHEVs) represent a pivotalshift. For HEVs, EMS now confronts the intricate energy cooperationrequirements of CHEVs, necessitating advanced algorithms for routeoptimization, charging coordination, and load distribution. Challenges persistin both domains, including optimal energy utilization for HEVs, and cooperativeeco-driving control (CED) for CHEVs across diverse vehicle types. Reinforcementlearning (RL) stands out as a promising tool for addressing these challenges athand. Specifically, within the realm of CHEVs, the application of multi-agentreinforcement learning (MARL) emerges as a powerful approach for effectivelytackling the intricacies of CED control. Despite extensive research, fewreviews span from individual vehicles to multi-vehicle scenarios. This reviewbridges the gap, highlighting challenges, advancements, and potentialcontributions of RL-based solutions for future sustainable transportationsystems.</description><author>Min Hua, Bin Shuai, Quan Zhou, Jinhai Wang, Yinglong He, Hongming Xu</author><pubDate>Mon, 28 Aug 2023 15:12:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14602v1</guid></item><item><title>Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery</title><link>http://arxiv.org/abs/2308.14601v1</link><description>As online music platforms grow, music recommender systems play a vital rolein helping users navigate and discover content within their vast musicaldatabases. At odds with this larger goal, is the presence of popularity bias,which causes algorithmic systems to favor mainstream content over, potentiallymore relevant, but niche items. In this work we explore the intrinsicrelationship between music discovery and popularity bias. To mitigate thisissue we propose a domain-aware, individual fairness-based approach whichaddresses popularity bias in graph neural network (GNNs) based recommendersystems. Our approach uses individual fairness to reflect a ground truthlistening experience, i.e., if two songs sound similar, this similarity shouldbe reflected in their representations. In doing so, we facilitate meaningfulmusic discovery that is robust to popularity bias and grounded in the musicdomain. We apply our BOOST methodology to two discovery based tasks, performingrecommendations at both the playlist level and user level. Then, we ground ourevaluation in the cold start setting, showing that our approach outperformsexisting fairness benchmarks in both performance and recommendation oflesser-known content. Finally, our analysis explains why our proposedmethodology is a novel and promising approach to mitigating popularity bias andimproving the discovery of new and niche content in music recommender systems.</description><author>Rebecca Salganik, Fernando Diaz, Golnoosh Farnadi</author><pubDate>Mon, 28 Aug 2023 15:12:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14601v1</guid></item><item><title>An active learning method for solving competitive multi-agent decision-making and control problems</title><link>http://arxiv.org/abs/2212.12561v2</link><description>We propose a scheme based on active learning to reconstruct privatestrategies executed by a population of interacting agents and predict an exactoutcome of the underlying multi-agent interaction process, here identified as astationary action profile. We envision a scenario where an external observer,endowed with a learning procedure, can make queries and observe the agents'reactions through private action-reaction mappings, whose collective fixedpoint corresponds to a stationary profile. By iteratively collecting sensibledata and updating parametric estimates of the action-reaction mappings, weestablish sufficient conditions to assess the asymptotic properties of theproposed active learning methodology so that, if convergence happens, it canonly be towards a stationary action profile. This fact yields two mainconsequences: i) learning locally-exact surrogates of the action-reactionmappings allows the external observer to succeed in its prediction task, andii) working with assumptions so general that a stationary profile is not evenguaranteed to exist, the established sufficient conditions hence act also ascertificates for the existence of such a desirable profile. Extensive numericalsimulations involving typical competitive multi-agent control anddecision-making problems illustrate the practical effectiveness of the proposedlearning-based approach.</description><author>Filippo Fabiani, Alberto Bemporad</author><pubDate>Mon, 28 Aug 2023 15:10:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12561v2</guid></item><item><title>Deep Reinforcement Learning for Wind and Energy Storage Coordination in Wholesale Energy and Ancillary Service Markets</title><link>http://arxiv.org/abs/2212.13368v2</link><description>Wind energy has been increasingly adopted to mitigate climate change.However, the variability of wind energy causes wind curtailment, resulting inconsiderable economic losses for wind farm owners. Wind curtailment can bereduced using battery energy storage systems (BESS) as onsite backup sources.Yet, this auxiliary role may significantly weaken the economic potential ofBESS in energy trading. Ideal BESS scheduling should balance onsite windcurtailment reduction and market bidding, but practical implementation ischallenging due to coordination complexity and the stochastic nature of energyprices and wind generation. We investigate the joint-market bidding strategy ofa co-located wind-battery system in the spot and Regulation Frequency ControlAncillary Service markets. We propose a novel deep reinforcement learning-basedapproach that decouples the system's market participation into two relatedMarkov decision processes for each facility, enabling the BESS to absorb onsitewind curtailment while performing joint-market bidding to maximize overalloperational revenues. Using realistic wind farm data, we validated thecoordinated bidding strategy, with outcomes surpassing the optimization-basedbenchmark in terms of higher revenue by approximately 25\% and more windcurtailment reduction by 2.3 times. Our results show that joint-market biddingcan significantly improve the financial performance of wind-battery systemscompared to participating in each market separately. Simulations also show thatusing curtailed wind generation as a power source for charging the BESS canlead to additional financial gains. The successful implementation of ouralgorithm would encourage co-location of generation and storage assets tounlock wider system benefits.</description><author>Jinhao Li, Changlong Wang, Hao Wang</author><pubDate>Mon, 28 Aug 2023 15:09:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.13368v2</guid></item><item><title>S-TREK: Sequential Translation and Rotation Equivariant Keypoints for local feature extraction</title><link>http://arxiv.org/abs/2308.14598v1</link><description>In this work we introduce S-TREK, a novel local feature extractor thatcombines a deep keypoint detector, which is both translation and rotationequivariant by design, with a lightweight deep descriptor extractor. We trainthe S-TREK keypoint detector within a framework inspired by reinforcementlearning, where we leverage a sequential procedure to maximize a rewarddirectly related to keypoint repeatability. Our descriptor network is trainedfollowing a "detect, then describe" approach, where the descriptor loss isevaluated only at those locations where keypoints have been selected by thealready trained detector. Extensive experiments on multiple benchmarks confirmthe effectiveness of our proposed method, with S-TREK often outperforming otherstate-of-the-art methods in terms of repeatability and quality of the recoveredposes, especially when dealing with in-plane rotations.</description><author>Emanuele Santellani, Christian Sormann, Mattia Rossi, Andreas Kuhn, Friedrich Fraundorfer</author><pubDate>Mon, 28 Aug 2023 15:09:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14598v1</guid></item><item><title>Adversarial Attacks on Foundational Vision Models</title><link>http://arxiv.org/abs/2308.14597v1</link><description>Rapid progress is being made in developing large, pretrained, task-agnosticfoundational vision models such as CLIP, ALIGN, DINOv2, etc. In fact, we areapproaching the point where these models do not have to be finetuneddownstream, and can simply be used in zero-shot or with a lightweight probinghead. Critically, given the complexity of working at this scale, there is abottleneck where relatively few organizations in the world are executing thetraining then sharing the models on centralized platforms such as HuggingFaceand torch.hub. The goal of this work is to identify several key adversarialvulnerabilities of these models in an effort to make future designs morerobust. Intuitively, our attacks manipulate deep feature representations tofool an out-of-distribution (OOD) detector which will be required when usingthese open-world-aware models to solve closed-set downstream tasks. Our methodsreliably make in-distribution (ID) images (w.r.t. a downstream task) bepredicted as OOD and vice versa while existing in extremelylow-knowledge-assumption threat models. We show our attacks to be potent inwhitebox and blackbox settings, as well as when transferred across foundationalmodel types (e.g., attack DINOv2 with CLIP)! This work is only just thebeginning of a long journey towards adversarially robust foundational visionmodels.</description><author>Nathan Inkawhich, Gwendolyn McDonald, Ryan Luley</author><pubDate>Mon, 28 Aug 2023 15:09:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14597v1</guid></item><item><title>LatentDR: Improving Model Generalization Through Sample-Aware Latent Degradation and Restoration</title><link>http://arxiv.org/abs/2308.14596v1</link><description>Despite significant advances in deep learning, models often struggle togeneralize well to new, unseen domains, especially when training data islimited. To address this challenge, we propose a novel approach fordistribution-aware latent augmentation that leverages the relationships acrosssamples to guide the augmentation procedure. Our approach first degrades thesamples stochastically in the latent space, mapping them to augmented labels,and then restores the samples from their corrupted versions during training.This process confuses the classifier in the degradation step and restores theoverall class distribution of the original samples, promoting diverseintra-class/cross-domain variability. We extensively evaluate our approach on adiverse set of datasets and tasks, including domain generalization benchmarksand medical imaging datasets with strong domain shift, where we show ourapproach achieves significant improvements over existing methods for latentspace augmentation. We further show that our method can be flexibly adapted tolong-tail recognition tasks, demonstrating its versatility in building moregeneralizable models. Code is available athttps://github.com/nerdslab/LatentDR.</description><author>Ran Liu, Sahil Khose, Jingyun Xiao, Lakshmi Sathidevi, Keerthan Ramnath, Zsolt Kira, Eva L. Dyer</author><pubDate>Mon, 28 Aug 2023 15:08:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14596v1</guid></item><item><title>Bridging the Gap: Deciphering Tabular Data Using Large Language Model</title><link>http://arxiv.org/abs/2308.11891v2</link><description>In the realm of natural language processing, the understanding of tabulardata has perpetually stood as a focal point of scholarly inquiry. The emergenceof expansive language models, exemplified by the likes of ChatGPT, has usheredin a wave of endeavors wherein researchers aim to harness these models fortasks related to table-based question answering. Central to our investigativepursuits is the elucidation of methodologies that amplify the aptitude of suchlarge language models in discerning both the structural intricacies andinherent content of tables, ultimately facilitating their capacity to provideinformed responses to pertinent queries. To this end, we have architected adistinctive module dedicated to the serialization of tables for seamlessintegration with expansive language models. Additionally, we've instituted acorrective mechanism within the model to rectify potential inaccuracies.Experimental results indicate that, although our proposed method trails theSOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about1.2% in tests on specific datasets. This research marks the first applicationof large language models to table-based question answering tasks, enhancing themodel's comprehension of both table structures and content.</description><author>Hengyuan Zhang, Peng Chang, Zongcheng Ji</author><pubDate>Mon, 28 Aug 2023 15:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11891v2</guid></item><item><title>Neural Network Training Strategy to Enhance Anomaly Detection Performance: A Perspective on Reconstruction Loss Amplification</title><link>http://arxiv.org/abs/2308.14595v1</link><description>Unsupervised anomaly detection (UAD) is a widely adopted approach in industrydue to rare anomaly occurrences and data imbalance. A desirable characteristicof an UAD model is contained generalization ability which excels in thereconstruction of seen normal patterns but struggles with unseen anomalies.Recent studies have pursued to contain the generalization capability of theirUAD models in reconstruction from different perspectives, such as design ofneural network (NN) structure and training strategy. In contrast, we note thatcontaining of generalization ability in reconstruction can also be obtainedsimply from steep-shaped loss landscape. Motivated by this, we propose a losslandscape sharpening method by amplifying the reconstruction loss, dubbed LossAMPlification (LAMP). LAMP deforms the loss landscape into a steep shape so thereconstruction error on unseen anomalies becomes greater. Accordingly, theanomaly detection performance is improved without any change of the NNarchitecture. Our findings suggest that LAMP can be easily applied to anyreconstruction error metrics in UAD settings where the reconstruction model istrained with anomaly-free samples only.</description><author>YeongHyeon Park, Sungho Kang, Myung Jin Kim, Hyeonho Jeong, Hyunkyu Park, Hyeong Seok Kim, Juneho Yi</author><pubDate>Mon, 28 Aug 2023 15:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14595v1</guid></item><item><title>Three-stage binarization of color document images based on discrete wavelet transform and generative adversarial networks</title><link>http://arxiv.org/abs/2211.16098v4</link><description>The efficient segmentation of foreground text information from the backgroundin degraded color document images is a critical challenge in the preservationof ancient manuscripts. The imperfect preservation of ancient manuscripts overtime has led to various types of degradation, such as staining, yellowing, andink seepage, significantly affecting image binarization results. This workproposes a three-stage method using Generative Adversarial Networks (GAN) forenhancing and binarizing degraded color document images through DiscreteWavelet Transform (DWT). Stage-1 involves applying DWT and retaining theLow-Low (LL) subband images for image enhancement. In Stage-2, the originalinput image is divided into four single-channel images (Red, Green, Blue, andGray), and each is trained with independent adversarial networks to extractcolor foreground information. In Stage-3, the output image from Stage-2 and theoriginal input image are used to train independent adversarial networks fordocument binarization, enabling the integration of global and local features.The experimental results demonstrate that our proposed method outperforms otherclassic and state-of-the-art (SOTA) methods on the Document Image BinarizationContest (DIBCO) datasets. We have released our implementation code athttps://github.com/abcpp12383/ThreeStageBinarization.</description><author>Yu-Shian Lin, Rui-Yang Ju, Chih-Chia Chen, Chun-Tse Chien, Jen-Shiun Chiang</author><pubDate>Mon, 28 Aug 2023 15:03:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16098v4</guid></item><item><title>Enhancing Agent Communication and Learning through Action and Language</title><link>http://arxiv.org/abs/2308.10842v2</link><description>We introduce a novel category of GC-agents capable of functioning as bothteachers and learners. Leveraging action-based demonstrations andlanguage-based instructions, these agents enhance communication efficiency. Weinvestigate the incorporation of pedagogy and pragmatism, essential elements inhuman communication and goal achievement, enhancing the agents' teaching andlearning capabilities. Furthermore, we explore the impact of combiningcommunication modes (action and language) on learning outcomes, highlightingthe benefits of a multi-modal approach.</description><author>Hugo Caselles-DuprÃ©, Olivier Sigaud, Mohamed Chetouani</author><pubDate>Mon, 28 Aug 2023 14:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10842v2</guid></item><item><title>Learning to Read Analog Gauges from Synthetic Data</title><link>http://arxiv.org/abs/2308.14583v1</link><description>Manually reading and logging gauge data is time inefficient, and the effortincreases according to the number of gauges available. We present a computervision pipeline that automates the reading of analog gauges. We propose atwo-stage CNN pipeline that identifies the key structural components of ananalog gauge and outputs an angular reading. To facilitate the training of ourapproach, a synthetic dataset is generated thus obtaining a set of realisticanalog gauges with their corresponding annotation. To validate our proposal, anadditional real-world dataset was collected with 4.813 manually curated images.When compared against state-of-the-art methodologies, our method shows asignificant improvement of 4.55 in the average error, which is a 52% relativeimprovement. The resources for this project will be made available at:https://github.com/fuankarion/automatic-gauge-reading.</description><author>Juan Leon-Alcazar, Yazeed Alnumay, Cheng Zheng, Hassane Trigui, Sahejad Patel, Bernard Ghanem</author><pubDate>Mon, 28 Aug 2023 14:49:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14583v1</guid></item><item><title>Referring Image Segmentation Using Text Supervision</title><link>http://arxiv.org/abs/2308.14575v1</link><description>Existing Referring Image Segmentation (RIS) methods typically requireexpensive pixel-level or box-level annotations for supervision. In this paper,we observe that the referring texts used in RIS already provide sufficientinformation to localize the target object. Hence, we propose a novelweakly-supervised RIS framework to formulate the target localization problem asa classification process to differentiate between positive and negative textexpressions. While the referring text expressions for an image are used aspositive expressions, the referring text expressions from other images can beused as negative expressions for this image. Our framework has three mainnovelties. First, we propose a bilateral prompt method to facilitate theclassification process, by harmonizing the domain discrepancy between visualand linguistic features. Second, we propose a calibration method to reducenoisy background information and improve the correctness of the response mapsfor target object localization. Third, we propose a positive response mapselection strategy to generate high-quality pseudo-labels from the enhancedresponse maps, for training a segmentation network for RIS inference. Forevaluation, we propose a new metric to measure localization accuracy.Experiments on four benchmarks show that our framework achieves promisingperformances to existing fully-supervised RIS methods while outperformingstate-of-the-art weakly-supervised methods adapted from related areas. Code isavailable at https://github.com/fawnliu/TRIS.</description><author>Fang Liu, Yuhao Liu, Yuqiu Kong, Ke Xu, Lihe Zhang, Baocai Yin, Gerhard Hancke, Rynson Lau</author><pubDate>Mon, 28 Aug 2023 14:40:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14575v1</guid></item><item><title>SAAN: Similarity-aware attention flow network for change detection with VHR remote sensing images</title><link>http://arxiv.org/abs/2308.14570v1</link><description>Change detection (CD) is a fundamental and important task for monitoring theland surface dynamics in the earth observation field. Existing deeplearning-based CD methods typically extract bi-temporal image features using aweight-sharing Siamese encoder network and identify change regions using adecoder network. These CD methods, however, still perform far fromsatisfactorily as we observe that 1) deep encoder layers focus on irrelevantbackground regions and 2) the models' confidence in the change regions isinconsistent at different decoder stages. The first problem is because deepencoder layers cannot effectively learn from imbalanced change categories usingthe sole output supervision, while the second problem is attributed to the lackof explicit semantic consistency preservation. To address these issues, wedesign a novel similarity-aware attention flow network (SAAN). SAANincorporates a similarity-guided attention flow module with deeply supervisedsimilarity optimization to achieve effective change detection. Specifically, wecounter the first issue by explicitly guiding deep encoder layers to discoversemantic relations from bi-temporal input images using deeply supervisedsimilarity optimization. The extracted features are optimized to besemantically similar in the unchanged regions and dissimilar in the changingregions. The second drawback can be alleviated by the proposedsimilarity-guided attention flow module, which incorporates similarity-guidedattention modules and attention flow mechanisms to guide the model to focus ondiscriminative channels and regions. We evaluated the effectiveness andgeneralization ability of the proposed method by conducting experiments on awide range of CD tasks. The experimental results demonstrate that our methodachieves excellent performance on several CD tasks, with discriminativefeatures and semantic consistency preserved.</description><author>Haonan Guo, Xin Su, Chen Wu, Bo Du, Liangpei Zhang</author><pubDate>Mon, 28 Aug 2023 14:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14570v1</guid></item><item><title>PiClick: Picking the desired mask in click-based interactive segmentation</title><link>http://arxiv.org/abs/2304.11609v3</link><description>Click-based interactive segmentation aims to generate target masks via humanclicking, which facilitates efficient pixel-level annotation and image editing.In such a task, target ambiguity remains a problem hindering the accuracy andefficiency of segmentation. That is, in scenes with rich context, one click maycorrespond to multiple potential targets, while most previous interactivesegmentors only generate a single mask and fail to deal with target ambiguity.In this paper, we propose a novel interactive segmentation network namedPiClick, to yield all potentially reasonable masks and suggest the mostplausible one for the user. Specifically, PiClick utilizes a Transformer-basedarchitecture to generate all potential target masks by mutually interactivemask queries. Moreover, a Target Reasoning module is designed in PiClick toautomatically suggest the user-desired mask from all candidates, relievingtarget ambiguity and extra-human efforts. Extensive experiments on 9interactive segmentation datasets demonstrate PiClick performs favorablyagainst previous state-of-the-arts considering the segmentation results.Moreover, we show that PiClick effectively reduces human efforts in annotatingand picking the desired masks. To ease the usage and inspire future research,we release the source code of PiClick together with a plug-and-play annotationtool at https://github.com/cilinyan/PiClick.</description><author>Cilin Yan, Haochen Wang, Jie Liu, Xiaolong Jiang, Yao Hu, Xu Tang, Guoliang Kang, Efstratios Gavves</author><pubDate>Mon, 28 Aug 2023 14:26:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11609v3</guid></item><item><title>Kernel Limit of Recurrent Neural Networks Trained on Ergodic Data Sequences</title><link>http://arxiv.org/abs/2308.14555v1</link><description>Mathematical methods are developed to characterize the asymptotics ofrecurrent neural networks (RNN) as the number of hidden units, data samples inthe sequence, hidden state updates, and training steps simultaneously grow toinfinity. In the case of an RNN with a simplified weight matrix, we prove theconvergence of the RNN to the solution of an infinite-dimensional ODE coupledwith the fixed point of a random algebraic equation. The analysis requiresaddressing several challenges which are unique to RNNs. In typical mean-fieldapplications (e.g., feedforward neural networks), discrete updates are ofmagnitude $\mathcal{O}(\frac{1}{N})$ and the number of updates is$\mathcal{O}(N)$. Therefore, the system can be represented as an Eulerapproximation of an appropriate ODE/PDE, which it will converge to as $N\rightarrow \infty$. However, the RNN hidden layer updates are$\mathcal{O}(1)$. Therefore, RNNs cannot be represented as a discretization ofan ODE/PDE and standard mean-field techniques cannot be applied. Instead, wedevelop a fixed point analysis for the evolution of the RNN memory states, withconvergence estimates in terms of the number of update steps and the number ofhidden units. The RNN hidden layer is studied as a function in a Sobolev space,whose evolution is governed by the data sequence (a Markov chain), theparameter updates, and its dependence on the RNN hidden layer at the previoustime step. Due to the strong correlation between updates, a Poisson equationmust be used to bound the fluctuations of the RNN around its limit equation.These mathematical methods give rise to the neural tangent kernel (NTK) limitsfor RNNs trained on data sequences as the number of data samples and size ofthe neural network grow to infinity.</description><author>Samuel Chun-Hei Lam, Justin Sirignano, Konstantinos Spiliopoulos</author><pubDate>Mon, 28 Aug 2023 14:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14555v1</guid></item><item><title>Causal Explanations for Sequential Decision-Making in Multi-Agent Systems</title><link>http://arxiv.org/abs/2302.10809v3</link><description>We present CEMA: Causal Explanations in Multi-Agent systems; a generalframework to create causal explanations for an agent's decisions in sequentialmulti-agent systems. The core of CEMA is a novel causal selection methodinspired by how humans select causes for explanations. Unlike prior work thatassumes a specific causal structure, CEMA is applicable whenever aprobabilistic model for predicting future states of the environment isavailable. Given such a model, CEMA samples counterfactual worlds that informus about the salient causes behind the agent's decisions. We evaluate CEMA onthe task of motion planning for autonomous driving and test it in diversesimulated scenarios. We show that CEMA correctly and robustly identifies thecauses behind decisions, even when a large number of agents is present, andshow via a user study that CEMA's explanations have a positive effect onparticipant's trust in AVs and are rated at least as good as high-quality humanexplanations elicited from other participants.</description><author>Balint Gyevnar, Cheng Wang, Christopher G. Lucas, Shay B. Cohen, Stefano V. Albrecht</author><pubDate>Mon, 28 Aug 2023 14:16:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10809v3</guid></item><item><title>Face Presentation Attack Detection by Excavating Causal Clues and Adapting Embedding Statistics</title><link>http://arxiv.org/abs/2308.14551v1</link><description>Recent face presentation attack detection (PAD) leverages domain adaptation(DA) and domain generalization (DG) techniques to address performancedegradation on unknown domains. However, DA-based PAD methods require access tounlabeled target data, while most DG-based PAD solutions rely on a priori,i.e., known domain labels. Moreover, most DA-/DG-based methods arecomputationally intensive, demanding complex model architectures and/ormulti-stage training processes. This paper proposes to model face PAD as acompound DG task from a causal perspective, linking it to model optimization.We excavate the causal factors hidden in the high-level representation viacounterfactual intervention. Moreover, we introduce a class-guided MixStyle toenrich feature-level data distribution within classes instead of focusing ondomain information. Both class-guided MixStyle and counterfactual interventioncomponents introduce no extra trainable parameters and negligible computationalresources. Extensive cross-dataset and analytic experiments demonstrate theeffectiveness and efficiency of our method compared to state-of-the-art PADs.The implementation and the trained weights are publicly available.</description><author>Meiling Fang, Naser Damer</author><pubDate>Mon, 28 Aug 2023 14:11:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14551v1</guid></item><item><title>ReMAV: Reward Modeling of Autonomous Vehicles for Finding Likely Failure Events</title><link>http://arxiv.org/abs/2308.14550v1</link><description>Autonomous vehicles are advanced driving systems that are well known forbeing vulnerable to various adversarial attacks, compromising the vehicle'ssafety, and posing danger to other road users. Rather than actively trainingcomplex adversaries by interacting with the environment, there is a need tofirst intelligently find and reduce the search space to only those states whereautonomous vehicles are found less confident. In this paper, we propose ablackbox testing framework ReMAV using offline trajectories first to analyzethe existing behavior of autonomous vehicles and determine appropriatethresholds for finding the probability of failure events. Our reward modelingtechnique helps in creating a behavior representation that allows us tohighlight regions of likely uncertain behavior even when the baselineautonomous vehicle is performing well. This approach allows for more efficienttesting without the need for computational and inefficient active adversariallearning techniques. We perform our experiments in a high-fidelity urbandriving environment using three different driving scenarios containing singleand multi-agent interactions. Our experiment shows 35%, 23%, 48%, and 50%increase in occurrences of vehicle collision, road objects collision,pedestrian collision, and offroad steering events respectively by theautonomous vehicle under test, demonstrating a significant increase in failureevents. We also perform a comparative analysis with prior testing frameworksand show that they underperform in terms of training-testing efficiency,finding total infractions, and simulation steps to identify the first failurecompared to our approach. The results show that the proposed framework can beused to understand existing weaknesses of the autonomous vehicles under test inorder to only attack those regions, starting with the simplistic perturbationmodels.</description><author>Aizaz Sharif, Dusica Marijan</author><pubDate>Mon, 28 Aug 2023 14:09:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14550v1</guid></item><item><title>Deep graphical regression for jointly moderate and extreme Australian wildfires</title><link>http://arxiv.org/abs/2308.14547v1</link><description>Recent wildfires in Australia have led to considerable economic loss andproperty destruction, and there is increasing concern that climate change mayexacerbate their intensity, duration, and frequency. hazard quantification forextreme wildfires is an important component of wildfire management, as itfacilitates efficient resource distribution, adverse effect mitigation, andrecovery efforts. However, although extreme wildfires are typically the mostimpactful, both small and moderate fires can still be devastating to localcommunities and ecosystems. Therefore, it is imperative to develop robuststatistical methods to reliably model the full distribution of wildfire spread.We do so for a novel dataset of Australian wildfires from 1999 to 2019, andanalyse monthly spread over areas approximately corresponding to StatisticalAreas Level 1 and 2 (SA1/SA2) regions. Given the complex nature of wildfireignition and spread, we exploit recent advances in statistical deep learningand extreme value theory to construct a parametric regression model using graphconvolutional neural networks and the extended generalized Pareto distribution,which allows us to model wildfire spread observed on an irregular spatialdomain. We highlight the efficacy of our newly proposed model and perform awildfire hazard assessment for Australia and population-dense communities,namely Tasmania, Sydney, Melbourne, and Perth.</description><author>Daniela Cisneros, Jordan Richards, Ashok Dahal, Luigi Lombardo, RaphaÃ«l Huser</author><pubDate>Mon, 28 Aug 2023 14:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14547v1</guid></item><item><title>Secure &amp; Private Federated Neuroimaging</title><link>http://arxiv.org/abs/2205.05249v2</link><description>The amount of biomedical data continues to grow rapidly. However, collectingdata from multiple sites for joint analysis remains challenging due tosecurity, privacy, and regulatory concerns. To overcome this challenge, we useFederated Learning, which enables distributed training of neural network modelsover multiple data sources without sharing data. Each site trains the neuralnetwork over its private data for some time, then shares the neural networkparameters (i.e., weights, gradients) with a Federation Controller, which inturn aggregates the local models, sends the resulting community model back toeach site, and the process repeats. Our Federated Learning architecture,MetisFL, provides strong security and privacy. First, sample data never leavesa site. Second, neural network parameters are encrypted before transmission andthe global neural model is computed under fully-homomorphic encryption.Finally, we use information-theoretic methods to limit information leakage fromthe neural model to prevent a curious site from performing model inversion ormembership attacks. We present a thorough evaluation of the performance ofsecure, private federated learning in neuroimaging tasks, including forpredicting Alzheimer's disease and estimating BrainAGE from magnetic resonanceimaging (MRI) studies, in challenging, heterogeneous federated environmentswhere sites have different amounts of data and statistical distributions.</description><author>Dimitris Stripelis, Umang Gupta, Hamza Saleem, Nikhil Dhinagar, Tanmay Ghai, Rafael Chrysovalantis Anastasiou, Armaghan Asghar, Greg Ver Steeg, Srivatsan Ravi, Muhammad Naveed, Paul M. Thompson, Jose Luis Ambite</author><pubDate>Mon, 28 Aug 2023 14:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.05249v2</guid></item><item><title>Multilayer Multiset Neuronal Networks -- MMNNs</title><link>http://arxiv.org/abs/2308.14541v1</link><description>The coincidence similarity index, based on a combination of the Jaccard andoverlap similarity indices, has noticeable properties in comparing andclassifying data, including enhanced selectivity and sensitivity, intrinsicnormalization, and robustness to data perturbations and outliers. Thesefeatures allow multiset neurons, which are based on the coincidence similarityoperation, to perform effective pattern recognition applications, including thechallenging task of image segmentation. A few prototype points have been usedin previous related approaches to represent each pattern to be identified, eachof them being associated with respective multiset neurons. The segmentation ofthe regions can then proceed by taking into account the outputs of theseneurons. The present work describes multilayer multiset neuronal networksincorporating two or more layers of coincidence similarity neurons. Inaddition, as a means to improve performance, this work also explores theutilization of counter-prototype points, which are assigned to the imageregions to be avoided. This approach is shown to allow effective segmentationof complex regions despite considering only one prototype and onecounter-prototype point. As reported here, the balanced accuracy landscapes tobe optimized in order to identify the weight of the neurons in subsequentlayers have been found to be relatively smooth, while typically involving morethan one attraction basin. The use of a simple gradient-based optimizationmethodology has been demonstrated to effectively train the considered neuralnetworks with several architectures, at least for the given data type,configuration of parameters, and network architecture.</description><author>Alexandre Benatti, Luciano da Fontoura Costa</author><pubDate>Mon, 28 Aug 2023 13:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14541v1</guid></item><item><title>EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries</title><link>http://arxiv.org/abs/2212.06969v2</link><description>With the recent advances in video and 3D understanding, novel 4Dspatio-temporal methods fusing both concepts have emerged. Towards thisdirection, the Ego4D Episodic Memory Benchmark proposed a task for VisualQueries with 3D Localization (VQ3D). Given an egocentric video clip and animage crop depicting a query object, the goal is to localize the 3D position ofthe center of that query object with respect to the camera pose of a queryframe. Current methods tackle the problem of VQ3D by unprojecting the 2Dlocalization results of the sibling task Visual Queries with 2D Localization(VQ2D) into 3D predictions. Yet, we point out that the low number of cameraposes caused by camera re-localization from previous VQ3D methods severallyhinders their overall success rate. In this work, we formalize a pipeline (wedub EgoLoc) that better entangles 3D multiview geometry with 2D objectretrieval from egocentric videos. Our approach involves estimating more robustcamera poses and aggregating multi-view 3D displacements by leveraging the 2Ddetection confidence, which enhances the success rate of object queries andleads to a significant improvement in the VQ3D baseline performance.Specifically, our approach achieves an overall success rate of up to 87.12%,which sets a new state-of-the-art result in the VQ3D task. We provide acomprehensive empirical analysis of the VQ3D task and existing solutions, andhighlight the remaining challenges in VQ3D. The code is available athttps://github.com/Wayne-Mai/EgoLoc.</description><author>Jinjie Mai, Abdullah Hamdi, Silvio Giancola, Chen Zhao, Bernard Ghanem</author><pubDate>Mon, 28 Aug 2023 13:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.06969v2</guid></item><item><title>inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data</title><link>http://arxiv.org/abs/2307.03854v3</link><description>The real-time crash likelihood prediction model is an essential component ofthe proactive traffic safety management system. Over the years, numerousstudies have attempted to construct a crash likelihood prediction model inorder to enhance traffic safety, but mostly on freeways. In the majority of theexisting studies, researchers have primarily employed a deep learning-basedframework to identify crash potential. Lately, Transformer has emerged as apotential deep neural network that fundamentally operates throughattention-based mechanisms. Transformer has several functional benefits overextant deep learning models such as LSTM, CNN, etc. Firstly, Transformer canreadily handle long-term dependencies in a data sequence. Secondly,Transformers can parallelly process all elements in a data sequence duringtraining. Finally, a Transformer does not have the vanishing gradient issue.Realizing the immense possibility of Transformers, this paper proposesinTersection-Transformer (inTformer), a time-embedded attention-basedTransformer model that can effectively predict intersection crash likelihood inreal-time. The proposed model was evaluated using connected vehicle dataextracted from Signal Analytics Platform. Acknowledging the complex trafficoperation mechanism at intersection, this study developed zone-specific modelsby dividing the intersection region into two distinct zones:within-intersection and approach zone. The best inTformer models in'within-intersection,' and 'approach' zone achieved a sensitivity of 73%, and70%, respectively. The zone-level models were also compared to earlier studieson crash likelihood prediction at intersections and with several establisheddeep learning models trained on the same connected vehicle dataset.</description><author>B M Tazbiul Hassan Anik, Zubayer Islam, Mohamed Abdel-Aty, Ling Wang</author><pubDate>Mon, 28 Aug 2023 13:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03854v3</guid></item><item><title>Spoken Language Intelligence of Large Language Models for Language Learning</title><link>http://arxiv.org/abs/2308.14536v1</link><description>People have long hoped for a conversational system that can assist inreal-life situations, and recent progress on large language models (LLMs) isbringing this idea closer to reality. While LLMs are often impressive inperformance, their efficacy in real-world scenarios that demand expertknowledge remains unclear. LLMs are believed to hold the most potential andvalue in education, especially in the development of Artificial intelligence(AI) based virtual teachers capable of facilitating language learning. Ourfocus is centered on evaluating the efficacy of LLMs in the realm of education,specifically in the areas of spoken language learning which encompassphonetics, phonology, and second language acquisition. We introduce a newmultiple-choice question dataset to evaluate the effectiveness of LLMs in theaforementioned scenarios, including understanding and application of spokenlanguage knowledge. In addition, we investigate the influence of variousprompting techniques such as zero- and few-shot method (prepending the questionwith question-answer exemplars), chain-of-thought (CoT, think step-by-step),in-domain exampler and external tools (Google, Wikipedia). We conductedlarge-scale evaluation on popular LLMs (20 distinct models) using thesemethods. We achieved significant performance improvements compared to thezero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% -&gt;63.1%; LLaMA2-70B-Chat, 42.2% -&gt; 48.6%). We found that models of differentsizes have good understanding of concepts in phonetics, phonology, and secondlanguage acquisition, but show limitations in reasoning for real-worldproblems. Additionally, we also explore preliminary findings on conversationalcommunication.</description><author>Linkai Peng, Baorian Nuchged, Yingming Gao</author><pubDate>Mon, 28 Aug 2023 13:47:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14536v1</guid></item><item><title>A Multi-Task Semantic Decomposition Framework with Task-specific Pre-training for Few-Shot NER</title><link>http://arxiv.org/abs/2308.14533v1</link><description>The objective of few-shot named entity recognition is to identify namedentities with limited labeled instances. Previous works have primarily focusedon optimizing the traditional token-wise classification framework, whileneglecting the exploration of information based on NER data characteristics. Toaddress this issue, we propose a Multi-Task Semantic Decomposition Frameworkvia Joint Task-specific Pre-training (MSDP) for few-shot NER. Drawinginspiration from demonstration-based and contrastive learning, we introduce twonovel pre-training tasks: Demonstration-based Masked Language Modeling (MLM)and Class Contrastive Discrimination. These tasks effectively incorporateentity boundary information and enhance entity representation in Pre-trainedLanguage Models (PLMs). In the downstream main task, we introduce a multi-taskjoint optimization framework with the semantic decomposing method, whichfacilitates the model to integrate two different semantic information forentity classification. Experimental results of two few-shot NER benchmarksdemonstrate that MSDP consistently outperforms strong baselines by a largemargin. Extensive analyses validate the effectiveness and generalization ofMSDP.</description><author>Guanting Dong, Zechen Wang, Jinxu Zhao, Gang Zhao, Daichi Guo, Dayuan Fu, Tingfeng Hui, Chen Zeng, Keqing He, Xuefeng Li, Liwen Wang, Xinyue Cui, Weiran Xu</author><pubDate>Mon, 28 Aug 2023 13:46:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14533v1</guid></item><item><title>Semi-Supervised Learning for Visual Bird's Eye View Semantic Segmentation</title><link>http://arxiv.org/abs/2308.14525v1</link><description>Visual bird's eye view (BEV) semantic segmentation helps autonomous vehiclesunderstand the surrounding environment only from images, including staticelements (e.g., roads) and dynamic elements (e.g., vehicles, pedestrians).However, the high cost of annotation procedures of full-supervised methodslimits the capability of the visual BEV semantic segmentation, which usuallyneeds HD maps, 3D object bounding boxes, and camera extrinsic matrixes. In thispaper, we present a novel semi-supervised framework for visual BEV semanticsegmentation to boost performance by exploiting unlabeled images during thetraining. A consistency loss that makes full use of unlabeled data is thenproposed to constrain the model on not only semantic prediction but also theBEV feature. Furthermore, we propose a novel and effective data augmentationmethod named conjoint rotation which reasonably augments the dataset whilemaintaining the geometric relationship between the front-view images and theBEV semantic segmentation. Extensive experiments on the nuScenes and Argoversedatasets show that our semi-supervised framework can effectively improveprediction accuracy. To the best of our knowledge, this is the first work thatexplores improving visual BEV semantic segmentation performance using unlabeleddata. The code will be publicly available.</description><author>Junyu Zhu, Lina Liu, Yu Tang, Feng Wen, Wanlong Li, Yong Liu</author><pubDate>Mon, 28 Aug 2023 13:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14525v1</guid></item></channel></rss>