<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 16 Aug 2024 01:00:20 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Problem Solving Through Human-AI Preference-Based Cooperation</title><link>http://arxiv.org/abs/2408.07461v2</link><description>While there is a widespread belief that artificial general intelligence (AGI)-- or even superhuman AI -- is imminent, complex problems in expert domains arefar from being solved. We argue that such problems require human-AI cooperationand that the current state of the art in generative AI is unable to play therole of a reliable partner due to a multitude of shortcomings, includinginability to keep track of a complex solution artifact (e.g., a softwareprogram), limited support for versatile human preference expression and lack ofadapting to human preference in an interactive setting. To address thesechallenges, we propose HAI-Co2, a novel human-AI co-construction framework. Weformalize HAI-Co2 and discuss the difficult open research problems that itfaces. Finally, we present a case study of HAI-Co2 and demonstrate its efficacycompared to monolithic generative AI models.</description><author>Subhabrata Dutta, Timo Kaufmann, Goran Glavaš, Ivan Habernal, Kristian Kersting, Frauke Kreuter, Mira Mezini, Iryna Gurevych, Eyke Hüllermeier, Hinrich Schuetze</author><pubDate>Thu, 15 Aug 2024 15:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07461v2</guid></item><item><title>An Event Structure-aware Generative Model for Biomedical Event Extraction</title><link>http://arxiv.org/abs/2408.06583v3</link><description>Biomedical Event Extraction (BEE) is a challenging task that involvesmodeling complex relationships between fine-grained entities in biomedicaltext. BEE has traditionally been formulated as a classification problem. Withthe recent technological advancements in large language models (LLMs),generation-based models that cast event extraction as a sequence generationproblem have attracted much attention from the NLP research communities.However, current generative models often overlook the importance ofcross-instance information from complex event structures such as nested eventsand overlapping events, which contribute quite significantly in the benchmarkdatasets. In this paper, we propose an event structure-aware generative modelcalled GenBEE, which can capture complex event structures in biomedical textfor biomedical event extraction. In particular, GenBEE constructs event promptsthat distill knowledge from LLMs for incorporating both label semantics andargument dependency relationships into the proposed model. In addition, GenBEEalso generates prefixes with event structural prompts to incorporate structuralfeatures for improving the model's overall performance. We have evaluated theproposed GenBEE model on three widely used biomedical event extractionbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show thatGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,and achieved competitive results when compared to the state-of-the-artclassification-based models on the PHEE dataset.</description><author>Haohan Yuan, Siu Cheung Hui, Haopeng Zhang</author><pubDate>Thu, 15 Aug 2024 15:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06583v3</guid></item><item><title>MagicFace: Training-free Universal-Style Human Image Customized Synthesis</title><link>http://arxiv.org/abs/2408.07433v2</link><description>Existing human image personalized generation methods often require tedioustraining: either fine-tuning with a few images or retraining on large-scaledatasets. In such cases, these methods are prone to overfitting and encounterdifficulties when personalizing individuals of diverse styles. Moreover, thesetraining-based approaches also struggle with multi-concept human imagecustomizing. To this end, we propose MagicFace, the first method foruniversal-style human image personalized synthesis that enablessingle/multi-concept customization for humans of any style in a training-freemanner. MagicFace introduces a coarse-to-fine generation pipeline, involvingtwo sequential stages: semantic scene construction and concept featureinjection. This is achieved by our Reference-aware Self-Attention (RSA) andRegion-grouped Blend Attention (RBA) mechanisms. Specifically, in the firststage, RSA enables the latent image to query features from reference conceptssimultaneously, extracting the coarse-grained overall semantic understanding tofacilitate the initial semantic layout establishment. In the second stage, weemploy an attention-based semantic segmentation method to pinpoint thegenerated regions of all concepts in the latent image at each step. Followingthis, RBA divides the pixels of the latent image into semantic groups, witheach group querying fine-grained features from its reference concept, whichensures precise attribute alignment and feature injection. Throughout thetwo-stage process, a weight mask strategy is employed to ensure the modelfocuses more on the reference concepts. Extensive experiments demonstrate oursuperiority in both human-centric subject-to-image synthesis and multi-concepthuman image customization. Our approach also can be applied to texturetransformation, further enhancing its versatility and applicability.</description><author>Yibin Wang, Weizhong Zhang, Cheng Jin</author><pubDate>Thu, 15 Aug 2024 15:00:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07433v2</guid></item><item><title>Deep Learning: a Heuristic Three-stage Mechanism for Grid Searches to Optimize the Future Risk Prediction of Breast Cancer Metastasis Using EHR-based Clinical Data</title><link>http://arxiv.org/abs/2408.07673v2</link><description>A grid search, at the cost of training and testing a large number of models,is an effective way to optimize the prediction performance of deep learningmodels. A challenging task concerning grid search is the time management.Without a good time management scheme, a grid search can easily be set off as amission that will not finish in our lifetime. In this study, we introduce aheuristic three-stage mechanism for managing the running time of low-budgetgrid searches, and the sweet-spot grid search (SSGS) and randomized grid search(RGS) strategies for improving model prediction performance, in predicting the5-year, 10-year, and 15-year risk of breast cancer metastasis. We develop deepfeedforward neural network (DFNN) models and optimize them through gridsearches. We conduct eight cycles of grid searches by applying our three-stagemechanism and SSGS and RGS strategies. We conduct various SHAP analysesincluding unique ones that interpret the importance of the DFNN-modelhyperparameters. Our results show that grid search can greatly improve modelprediction. The grid searches we conducted improved the risk prediction of5-year, 10-year, and 15-year breast cancer metastasis by 18.6%, 16.3%, and17.3% respectively, over the average performance of all corresponding models wetrained using the RGS strategy. We not only demonstrate best model performancebut also characterize grid searches from various aspects such as theircapabilities of discovering decent models and the unit grid search time. Thethree-stage mechanism worked effectively. It made our low-budget grid searchesfeasible and manageable, and in the meantime helped improve model predictionperformance. Our SHAP analyses identified both clinical risk factors importantfor the prediction of future risk of breast cancer metastasis, and DFNN-modelhyperparameters important to the prediction of performance scores.</description><author>Xia Jiang, Yijun Zhou, Chuhan Xu, Adam Brufsky, Alan Wells</author><pubDate>Thu, 15 Aug 2024 14:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07673v2</guid></item><item><title>A Spitting Image: Modular Superpixel Tokenization in Vision Transformers</title><link>http://arxiv.org/abs/2408.07680v2</link><description>Vision Transformer (ViT) architectures traditionally employ a grid-basedapproach to tokenization independent of the semantic content of an image. Wepropose a modular superpixel tokenization strategy which decouples tokenizationand feature extraction; a shift from contemporary approaches where these aretreated as an undifferentiated whole. Using on-line content-aware tokenizationand scale- and shape-invariant positional embeddings, we perform experimentsand ablations that contrast our approach with patch-based tokenization andrandomized partitions as baselines. We show that our method significantlyimproves the faithfulness of attributions, gives pixel-level granularity onzero-shot unsupervised dense prediction tasks, while maintaining predictiveperformance in classification tasks. Our approach provides a modulartokenization framework commensurable with standard architectures, extending thespace of ViTs to a larger class of semantically-rich models.</description><author>Marius Aasan, Odd Kolbjørnsen, Anne Schistad Solberg, Adín Ramirez Rivera</author><pubDate>Thu, 15 Aug 2024 12:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07680v2</guid></item><item><title>BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt Learning</title><link>http://arxiv.org/abs/2408.07440v2</link><description>Medical foundation models are gaining prominence in the medical community fortheir ability to derive general representations from extensive collections ofmedical image-text pairs. Recent research indicates that these models aresusceptible to backdoor attacks, which allow them to classify clean imagesaccurately but fail when specific triggers are introduced. However, traditionalbackdoor attacks necessitate a considerable amount of additional data tomaliciously pre-train a model. This requirement is often impractical in medicalimaging applications due to the usual scarcity of data. Inspired by the latestdevelopments in learnable prompts, this work introduces a method to embed abackdoor into the medical foundation model during the prompt learning phase. Byincorporating learnable prompts within the text encoder and introducingimperceptible learnable noise trigger to the input images, we exploit the fullcapabilities of the medical foundation models (Med-FM). Our method, BAPLe,requires only a minimal subset of data to adjust the noise trigger and the textprompts for downstream tasks, enabling the creation of an effective backdoorattack. Through extensive experiments with four medical foundation models, eachpre-trained on different modalities and evaluated across six downstreamdatasets, we demonstrate the efficacy of our approach. BAPLe achieves a highbackdoor success rate across all models and datasets, outperforming thebaseline backdoor attack methods. Our work highlights the vulnerability ofMed-FMs towards backdoor attacks and strives to promote the safe adoption ofMed-FMs before their deployment in real-world applications. Code is availableat https://asif-hanif.github.io/baple/.</description><author>Asif Hanif, Fahad Shamshad, Muhammad Awais, Muzammal Naseer, Fahad Shahbaz Khan, Karthik Nandakumar, Salman Khan, Rao Muhammad Anwer</author><pubDate>Thu, 15 Aug 2024 10:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07440v2</guid></item><item><title>A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties</title><link>http://arxiv.org/abs/2312.13764v3</link><description>This paper introduces ProLab, a novel approach using property-level labelspace for creating strong interpretable segmentation models. Instead of relyingsolely on category-specific annotations, ProLab uses descriptive propertiesgrounded in common sense knowledge for supervising segmentation models. It isbased on two core designs. First, we employ Large Language Models (LLMs) andcarefully crafted prompts to generate descriptions of all involved categoriesthat carry meaningful common sense knowledge and follow a structured format.Second, we introduce a description embedding model preserving semanticcorrelation across descriptions and then cluster them into a set of descriptiveproperties (e.g., 256) using K-Means. These properties are based oninterpretable common sense knowledge consistent with theories of humanrecognition. We empirically show that our approach makes segmentation modelsperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, PascalContext, Cityscapes, and BDD). Our method also shows better scalability withextended training steps than category-level supervision. Our interpretablesegmentation framework also emerges with the generalization ability to segmentout-of-domain or unknown categories using only in-domain descriptiveproperties. Code is available at https://github.com/lambert-x/ProLab.</description><author>Junfei Xiao, Ziqi Zhou, Wenxuan Li, Shiyi Lan, Jieru Mei, Zhiding Yu, Alan Yuille, Yuyin Zhou, Cihang Xie</author><pubDate>Thu, 15 Aug 2024 10:03:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13764v3</guid></item><item><title>PsyDI: Towards a Personalized and Progressively In-depth Chatbot for Psychological Measurements</title><link>http://arxiv.org/abs/2408.03337v3</link><description>In the field of psychology, traditional assessment methods, such asstandardized scales, are frequently critiqued for their static nature, lack ofpersonalization, and reduced participant engagement, while comprehensivecounseling evaluations are often inaccessible. The complexity of quantifyingpsychological traits further limits these methods. Despite advances with largelanguage models (LLMs), many still depend on single-round Question-and-Answerinteractions. To bridge this gap, we introduce PsyDI, a personalized andprogressively in-depth chatbot designed for psychological measurements,exemplified by its application in the Myers-Briggs Type Indicator (MBTI)framework. PsyDI leverages user-related multi-modal information and engages incustomized, multi-turn interactions to provide personalized, easily accessiblemeasurements, while ensuring precise MBTI type determination. To address thechallenge of unquantifiable psychological traits, we introduce a novel trainingparadigm that involves learning the ranking of proxy variables associated withthese traits, culminating in a robust score model for MBTI measurements. Thescore model enables PsyDI to conduct comprehensive and precise measurementsthrough multi-turn interactions within a unified estimation context. Throughvarious experiments, we validate the efficacy of both the score model and thePsyDI pipeline, demonstrating its potential to serve as a general framework forpsychological measurements. Furthermore, the online deployment of PsyDI hasgarnered substantial user engagement, with over 3,000 visits, resulting in thecollection of numerous multi-turn dialogues annotated with MBTI types, whichfacilitates further research. The source code for the training and web servicecomponents is publicly available as a part of OpenDILab at:https://github.com/opendilab/PsyDI</description><author>Xueyan Li, Xinyan Chen, Yazhe Niu, Shuai Hu, Yu Liu</author><pubDate>Thu, 15 Aug 2024 06:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03337v3</guid></item><item><title>ConfusedPilot: Confused Deputy Risks in RAG-based LLMs</title><link>http://arxiv.org/abs/2408.04870v3</link><description>Retrieval augmented generation (RAG) is a process where a large languagemodel (LLM) retrieves useful information from a database and then generates theresponses. It is becoming popular in enterprise settings for daily businessoperations. For example, Copilot for Microsoft 365 has accumulated millions ofbusinesses. However, the security implications of adopting such RAG-basedsystems are unclear. In this paper, we introduce ConfusedPilot, a class of securityvulnerabilities of RAG systems that confuse Copilot and cause integrity andconfidentiality violations in its responses. First, we investigate avulnerability that embeds malicious text in the modified prompt in RAG,corrupting the responses generated by the LLM. Second, we demonstrate avulnerability that leaks secret data, which leverages the caching mechanismduring retrieval. Third, we investigate how both vulnerabilities can beexploited to propagate misinformation within the enterprise and ultimatelyimpact its operations, such as sales and manufacturing. We also discuss theroot cause of these attacks by investigating the architecture of a RAG-basedsystem. This study highlights the security vulnerabilities in today's RAG-basedsystems and proposes design guidelines to secure future RAG-based systems.</description><author>Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari</author><pubDate>Thu, 15 Aug 2024 05:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04870v3</guid></item><item><title>Do GPT Language Models Suffer From Split Personality Disorder? The Advent Of Substrate-Free Psychometrics</title><link>http://arxiv.org/abs/2408.07377v2</link><description>Previous research on emergence in large language models shows these displayapparent human-like abilities and psychological latent traits. However, resultsare partly contradicting in expression and magnitude of these latent traits,yet agree on the worrisome tendencies to score high on the Dark Triad ofnarcissism, psychopathy, and Machiavellianism, which, together with a trackrecord of derailments, demands more rigorous research on safety of thesemodels. We provided a state of the art language model with the same personalityquestionnaire in nine languages, and performed Bayesian analysis of GaussianMixture Model, finding evidence for a deeper-rooted issue. Our results suggestboth interlingual and intralingual instabilities, which indicate that currentlanguage models do not develop a consistent core personality. This can lead tounsafe behaviour of artificial intelligence systems that are based on thesefoundation models, and are increasingly integrated in human life. Wesubsequently discuss the shortcomings of modern psychometrics, abstract it, andprovide a framework for its species-neutral, substrate-free formulation.</description><author>Peter Romero, Stephen Fitz, Teruo Nakatsuma</author><pubDate>Thu, 15 Aug 2024 05:15:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07377v2</guid></item><item><title>MathScape: Evaluating MLLMs in multimodal Math Scenarios through a Hierarchical Benchmark</title><link>http://arxiv.org/abs/2408.07543v2</link><description>With the development of Multimodal Large Language Models (MLLMs), theevaluation of multimodal models in the context of mathematical problems hasbecome a valuable research field. Multimodal visual-textual mathematicalreasoning serves as a critical indicator for evaluating the comprehension andcomplex multi-step quantitative reasoning abilities of MLLMs. However, previousmultimodal math benchmarks have not sufficiently integrated visual and textualinformation. To address this gap, we proposed MathScape, a new benchmark thatemphasizes the understanding and application of combined visual and textualinformation. MathScape is designed to evaluate photo-based math problemscenarios, assessing the theoretical understanding and application ability ofMLLMs through a categorical hierarchical approach. We conduct amulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmarkis challenging even for the most sophisticated models. By analyzing theevaluation results, we identify the limitations of MLLMs, offering valuableinsights for enhancing model performance.</description><author>Minxuan Zhou, Hao Liang, Tianpeng Li, Zhiyu Wu, Mingan Lin, Linzhuang Sun, Yaqi Zhou, Yan Zhang, Xiaoqin Huang, Yicong Chen, Yujing Qiao, Weipeng Chen, Bin Cui, Wentao Zhang, Zenan Zhou</author><pubDate>Thu, 15 Aug 2024 04:01:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07543v2</guid></item><item><title>MetaSeg: MetaFormer-based Global Contexts-aware Network for Efficient Semantic Segmentation</title><link>http://arxiv.org/abs/2408.07576v2</link><description>Beyond the Transformer, it is important to explore how to exploit thecapacity of the MetaFormer, an architecture that is fundamental to theperformance improvements of the Transformer. Previous studies have exploited itonly for the backbone network. Unlike previous studies, we explore the capacityof the Metaformer architecture more extensively in the semantic segmentationtask. We propose a powerful semantic segmentation network, MetaSeg, whichleverages the Metaformer architecture from the backbone to the decoder. OurMetaSeg shows that the MetaFormer architecture plays a significant role incapturing the useful contexts for the decoder as well as for the backbone. Inaddition, recent segmentation methods have shown that using a CNN-basedbackbone for extracting the spatial information and a decoder for extractingthe global information is more effective than using a transformer-basedbackbone with a CNN-based decoder. This motivates us to adopt the CNN-basedbackbone using the MetaFormer block and design our MetaFormer-based decoder,which consists of a novel self-attention module to capture the global contexts.To consider both the global contexts extraction and the computationalefficiency of the self-attention for semantic segmentation, we propose aChannel Reduction Attention (CRA) module that reduces the channel dimension ofthe query and key into the one dimension. In this way, our proposed MetaSegoutperforms the previous state-of-the-art methods with more efficientcomputational costs on popular semantic segmentation and a medical imagesegmentation benchmark, including ADE20K, Cityscapes, COCO-stuff, and Synapse.The code is available at https://github.com/hyunwoo137/MetaSeg.</description><author>Beoungwoo Kang, Seunghun Moon, Yubin Cho, Hyunwoo Yu, Suk-Ju Kang</author><pubDate>Thu, 15 Aug 2024 03:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07576v2</guid></item><item><title>DIffSteISR: Harnessing Diffusion Prior for Superior Real-world Stereo Image Super-Resolution</title><link>http://arxiv.org/abs/2408.07516v2</link><description>We introduce DiffSteISR, a pioneering framework for reconstructing real-worldstereo images. DiffSteISR utilizes the powerful prior knowledge embedded inpre-trained text-to-image model to efficiently recover the lost texture detailsin low-resolution stereo images. Specifically, DiffSteISR implements atime-aware stereo cross attention with temperature adapter (TASCATA) to guidethe diffusion process, ensuring that the generated left and right views exhibithigh texture consistency thereby reducing disparity error between thesuper-resolved images and the ground truth (GT) images. Additionally, a stereoomni attention control network (SOA ControlNet) is proposed to enhance theconsistency of super-resolved images with GT images in the pixel, perceptual,and distribution space. Finally, DiffSteISR incorporates a stereo semanticextractor (SSE) to capture unique viewpoint soft semantic information andshared hard tag semantic information, thereby effectively improving thesemantic accuracy and consistency of the generated left and right images.Extensive experimental results demonstrate that DiffSteISR accuratelyreconstructs natural and precise textures from low-resolution stereo imageswhile maintaining a high consistency of semantic and texture between the leftand right views.</description><author>Yuanbo Zhou, Xinlin Zhang, Wei Deng, Tao Wang, Tao Tan, Qinquan Gao, Tong Tong</author><pubDate>Thu, 15 Aug 2024 02:14:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07516v2</guid></item><item><title>Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities</title><link>http://arxiv.org/abs/2408.07666v2</link><description>Model merging is an efficient empowerment technique in the machine learningcommunity that does not require the collection of raw training data and doesnot require expensive computation. As model merging becomes increasinglyprevalent across various fields, it is crucial to understand the availablemodel merging techniques comprehensively. However, there is a significant gapin the literature regarding a systematic and thorough review of thesetechniques. This survey provides a comprehensive overview of model mergingmethods and theories, their applications in various domains and settings, andfuture research directions. Specifically, we first propose a new taxonomicapproach that exhaustively discusses existing model merging methods. Secondly,we discuss the application of model merging techniques in large languagemodels, multimodal large language models, and 10+ machine learning subfields,including continual learning, multi-task learning, few-shot learning, etc.Finally, we highlight the remaining challenges of model merging and discussfuture research directions. A comprehensive list of papers about model mergingis available at\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.</description><author>Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, Dacheng Tao</author><pubDate>Thu, 15 Aug 2024 01:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07666v2</guid></item><item><title>Robust Active Learning (RoAL): Countering Dynamic Adversaries in Active Learning with Elastic Weight Consolidation</title><link>http://arxiv.org/abs/2408.07364v2</link><description>Despite significant advancements in active learning and adversarial attacks,the intersection of these two fields remains underexplored, particularly indeveloping robust active learning frameworks against dynamic adversarialthreats. The challenge of developing robust active learning frameworks underdynamic adversarial attacks is critical, as these attacks can lead tocatastrophic forgetting within the active learning cycle. This paper introducesRobust Active Learning (RoAL), a novel approach designed to address this issueby integrating Elastic Weight Consolidation (EWC) into the active learningprocess. Our contributions are threefold: First, we propose a new dynamicadversarial attack that poses significant threats to active learningframeworks. Second, we introduce a novel method that combines EWC with activelearning to mitigate catastrophic forgetting caused by dynamic adversarialattacks. Finally, we conduct extensive experimental evaluations to demonstratethe efficacy of our approach. The results show that RoAL not only effectivelycounters dynamic adversarial threats but also significantly reduces the impactof catastrophic forgetting, thereby enhancing the robustness and performance ofactive learning systems in adversarial environments.</description><author>Ricky Maulana Fajri, Yulong Pei, Lu Yin, Mykola Pechenizkiy</author><pubDate>Thu, 15 Aug 2024 01:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07364v2</guid></item><item><title>Knowledge Distillation with Refined Logits</title><link>http://arxiv.org/abs/2408.07703v1</link><description>Recent research on knowledge distillation has increasingly focused on logitdistillation because of its simplicity, effectiveness, and versatility in modelcompression. In this paper, we introduce Refined Logit Distillation (RLD) toaddress the limitations of current logit distillation methods. Our approach ismotivated by the observation that even high-performing teacher models can makeincorrect predictions, creating a conflict between the standard distillationloss and the cross-entropy loss. This conflict can undermine the consistency ofthe student model's learning objectives. Previous attempts to use labels toempirically correct teacher predictions may undermine the class correlation. Incontrast, our RLD employs labeling information to dynamically refine teacherlogits. In this way, our method can effectively eliminate misleadinginformation from the teacher while preserving crucial class correlations, thusenhancing the value and efficiency of distilled knowledge. Experimental resultson CIFAR-100 and ImageNet demonstrate its superiority over existing methods.The code is provided at \text{https://github.com/zju-SWJ/RLD}.</description><author>Wujie Sun, Defang Chen, Siwei Lyu, Genlang Chen, Chun Chen, Can Wang</author><pubDate>Wed, 14 Aug 2024 17:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07703v1</guid></item><item><title>The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models</title><link>http://arxiv.org/abs/2408.07702v1</link><description>Schema linking is a crucial step in Text-to-SQL pipelines, which translatenatural language queries into SQL. The goal of schema linking is to retrieverelevant tables and columns (signal) while disregarding irrelevant ones(noise). However, imperfect schema linking can often exclude essential columnsneeded for accurate query generation. In this work, we revisit the need forschema linking when using the latest generation of large language models(LLMs). We find empirically that newer models are adept at identifying relevantschema elements during generation, without the need for explicit schemalinking. This allows Text-to-SQL pipelines to bypass schema linking entirelyand instead pass the full database schema to the LLM, eliminating the risk ofexcluding necessary information. Furthermore, as alternatives to schemalinking, we propose techniques that improve Text-to-SQL accuracy withoutcompromising on essential schema information. Our approach achieves 71.83\%execution accuracy on the BIRD benchmark, ranking first at the time ofsubmission.</description><author>Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, Amine Mhedhbi</author><pubDate>Wed, 14 Aug 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07702v1</guid></item><item><title>Semi-Supervised Laplace Learning on Stiefel Manifolds</title><link>http://arxiv.org/abs/2308.00142v2</link><description>Motivated by the need to address the degeneracy of canonical Laplace learningalgorithms in low label rates, we propose to reformulate graph-basedsemi-supervised learning as a nonconvex generalization of a \emph{Trust-RegionSubproblem} (TRS). This reformulation is motivated by the well-posedness ofLaplacian eigenvectors in the limit of infinite unlabeled data. To solve thisproblem, we first show that a first-order condition implies the solution of amanifold alignment problem and that solutions to the classical \emph{OrthogonalProcrustes} problem can be used to efficiently find good classifiers that areamenable to further refinement. To tackle refinement, we develop the frameworkof Sequential Subspace Optimization for graph-based SSL. Next, we address thecriticality of selecting supervised samples at low-label rates. We characterizeinformative samples with a novel measure of centrality derived from theprincipal eigenvectors of a certain submatrix of the graph Laplacian. Wedemonstrate that our framework achieves lower classification error compared torecent state-of-the-art and classical semi-supervised learning methods atextremely low, medium, and high label rates.</description><author>Chester Holtz, Pengwen Chen, Alexander Cloninger, Chung-Kuan Cheng, Gal Mishne</author><pubDate>Wed, 14 Aug 2024 17:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00142v2</guid></item><item><title>Quantifying over Optimum Answer Sets</title><link>http://arxiv.org/abs/2408.07697v1</link><description>Answer Set Programming with Quantifiers (ASP(Q)) has been introduced toprovide a natural extension of ASP modeling to problems in the polynomialhierarchy (PH). However, ASP(Q) lacks a method for encoding in an elegant andcompact way problems requiring a polynomial number of calls to an oracle in$\Sigma_n^p$ (that is, problems in $\Delta_{n+1}^p$). Such problems include, inparticular, optimization problems. In this paper we propose an extension ofASP(Q), in which component programs may contain weak constraints. Weakconstraints can be used both for expressing local optimization withinquantified component programs and for modeling global optimization criteria. Weshowcase the modeling capabilities of the new formalism through variousapplication scenarios. Further, we study its computational properties obtainingcomplexity results and unveiling non-obvious characteristics of ASP(Q) programswith weak constraints.</description><author>Giuseppe Mazzotta, Francesco Ricca, Mirek Truszczynski</author><pubDate>Wed, 14 Aug 2024 17:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07697v1</guid></item><item><title>End-to-end Semantic-centric Video-based Multimodal Affective Computing</title><link>http://arxiv.org/abs/2408.07694v1</link><description>In the pathway toward Artificial General Intelligence (AGI), understandinghuman's affection is essential to enhance machine's cognition abilities. Forachieving more sensual human-AI interaction, Multimodal Affective Computing(MAC) in human-spoken videos has attracted increasing attention. However,previous methods are mainly devoted to designing multimodal fusion algorithms,suffering from two issues: semantic imbalance caused by diverse pre-processingoperations and semantic mismatch raised by inconsistent affection contentcontained in different modalities comparing with the multimodal ground truth.Besides, the usage of manual features extractors make they fail in buildingend-to-end pipeline for multiple MAC downstream tasks. To address abovechallenges, we propose a novel end-to-end framework named SemanticMAC tocompute multimodal semantic-centric affection for human-spoken videos. Wefirstly employ pre-trained Transformer model in multimodal data pre-processingand design Affective Perceiver module to capture unimodal affectiveinformation. Moreover, we present a semantic-centric approach to unifymultimodal representation learning in three ways, including gated featureinteraction, multi-task pseudo label generation, and intra-/inter-samplecontrastive learning. Finally, SemanticMAC effectively learn specific- andshared-semantic representations in the guidance of semantic-centric labels.Extensive experimental results demonstrate that our approach surpass thestate-of-the-art methods on 7 public datasets in four MAC downstream tasks.</description><author>Ronghao Lin, Ying Zeng, Sijie Mai, Haifeng Hu</author><pubDate>Wed, 14 Aug 2024 17:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07694v1</guid></item><item><title>Detecting Near-Duplicate Face Images</title><link>http://arxiv.org/abs/2408.07689v1</link><description>Near-duplicate images are often generated when applying repeated photometricand geometric transformations that produce imperceptible variants of theoriginal image. Consequently, a deluge of near-duplicates can be circulatedonline posing copyright infringement concerns. The concerns are more severewhen biometric data is altered through such nuanced transformations. In thiswork, we address the challenge of near-duplicate detection in face images by,firstly, identifying the original image from a set of near-duplicates and,secondly, deducing the relationship between the original image and thenear-duplicates. We construct a tree-like structure, called an Image PhylogenyTree (IPT) using a graph-theoretic approach to estimate the relationship, i.e.,determine the sequence in which they have been generated. We further extend ourmethod to create an ensemble of IPTs known as Image Phylogeny Forests (IPFs).We rigorously evaluate our method to demonstrate robustness across othermodalities, unseen transformations by latest generative models and IPTconfigurations, thereby significantly advancing the state-of-the-artperformance by 42% on IPF reconstruction accuracy.</description><author>Sudipta Banerjee, Arun Ross</author><pubDate>Wed, 14 Aug 2024 17:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07689v1</guid></item><item><title>Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation</title><link>http://arxiv.org/abs/2310.08541v2</link><description>We introduce ``Idea to Image,'' a system that enables multimodal iterativeself-refinement with GPT-4V(ision) for automatic image design and generation.Humans can quickly identify the characteristics of different text-to-image(T2I) models via iterative explorations. This enables them to efficientlyconvert their high-level generation ideas into effective T2I prompts that canproduce good images. We investigate if systems based on large multimodal models(LMMs) can develop analogous multimodal self-refinement abilities that enableexploring unknown models or environments via self-refining tries. Idea2Imgcyclically generates revised T2I prompts to synthesize draft images, andprovides directional feedback for prompt revision, both conditioned on itsmemory of the probed T2I model's characteristics. The iterative self-refinementbrings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Imgcan process input ideas with interleaved image-text sequences, follow ideaswith design instructions, and generate images of better semantic and visualqualities. The user preference study validates the efficacy of multimodaliterative self-refinement on automatic image design and generation.</description><author>Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang</author><pubDate>Wed, 14 Aug 2024 17:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08541v2</guid></item><item><title>RSD-DOG : A New Image Descriptor based on Second Order Derivatives</title><link>http://arxiv.org/abs/2408.07687v1</link><description>This paper introduces the new and powerful image patch descriptor based onsecond order image statistics/derivatives. Here, the image patch is treated asa 3D surface with intensity being the 3rd dimension. The considered 3D surfacehas a rich set of second order features/statistics such as ridges, valleys,cliffs and so on, that can be easily captured by using the difference ofrotating semi Gaussian filters. The originality of this method is based onsuccessfully combining the response of the directional filters with that of theDifference of Gaussian (DOG) approach. The obtained descriptor shows a gooddiscriminative power when dealing with the variations in illumination, scale,rotation, blur, viewpoint and compression. The experiments on image matching,demonstrates the advantage of the obtained descriptor when compared to itsfirst order counterparts such as SIFT, DAISY, GLOH, GIST and LIDRIC.</description><author>Darshan Venkatrayappa, Philippe Montesinos, Daniel Diep, Baptiste Magnier</author><pubDate>Wed, 14 Aug 2024 17:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07687v1</guid></item><item><title>Agent Instructs Large Language Models to be General Zero-Shot Reasoners</title><link>http://arxiv.org/abs/2310.03710v2</link><description>We introduce a method to improve the zero-shot reasoning abilities of largelanguage models on general language understanding tasks. Specifically, we buildan autonomous agent to instruct the reasoning process of large language models.We show this approach further unleashes the zero-shot reasoning abilities oflarge language models to more tasks. We study the performance of our method ona wide set of datasets spanning generation, classification, and reasoning. Weshow that our method generalizes to most tasks and obtains state-of-the-artzero-shot performance on 20 of the 29 datasets that we evaluate. For instance,our method boosts the performance of state-of-the-art large language models bya large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), andGPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvementin reasoning is striking, with an average increase of 10.5%. With our method,Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.</description><author>Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, Chenguang Wang</author><pubDate>Wed, 14 Aug 2024 17:39:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03710v2</guid></item><item><title>NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data</title><link>http://arxiv.org/abs/2403.19376v2</link><description>The acquisition of objects outside the Line-of-Sight of cameras is a veryintriguing but also extremely challenging research topic. Recent works showedthe feasibility of this idea exploiting transient imaging data produced bycustom direct Time of Flight sensors. In this paper, for the first time, wetackle this problem using only data from an off-the-shelf indirect Time ofFlight sensor without any further hardware requirement. We introduced a DeepLearning model able to re-frame the surfaces where light bounces happen as avirtual mirror. This modeling makes the task easier to handle and alsofacilitates the construction of annotated training data. From the obtained datait is possible to retrieve the depth information of the hidden scene. We alsoprovide a first-in-its-kind synthetic dataset for the task and demonstrate thefeasibility of the proposed idea over it.</description><author>Matteo Caligiuri, Adriano Simonetto, Pietro Zanuttigh</author><pubDate>Wed, 14 Aug 2024 17:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19376v2</guid></item><item><title>Learning Optimal Signal Temporal Logic Decision Trees for Classification: A Max-Flow MILP Formulation</title><link>http://arxiv.org/abs/2407.21090v2</link><description>This paper presents a novel framework for inferring timed temporal logicproperties from data. The dataset comprises pairs of finite-time system tracesand corresponding labels, denoting whether the traces demonstrate specificdesired behaviors, e.g. whether the ship follows a safe route or not. Ourproposed approach leverages decision-tree-based methods to infer SignalTemporal Logic classifiers using primitive formulae. We formulate the inferenceprocess as a mixed integer linear programming optimization problem, recursivelygenerating constraints to determine both data classification and treestructure. Applying a max-flow algorithm on the resultant tree transforms theproblem into a global optimization challenge, leading to improvedclassification rates compared to prior methodologies. Moreover, we introduce atechnique to reduce the number of constraints by exploiting the symmetryinherent in STL primitives, which enhances the algorithm's time performance andinterpretability. To assess our algorithm's effectiveness and classificationperformance, we conduct three case studies involving two-class, multi-class,and complex formula classification scenarios.</description><author>Kaier Liang, Gustavo A. Cardona, Disha Kamale, Cristian-Ioan Vasile</author><pubDate>Wed, 14 Aug 2024 17:35:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21090v2</guid></item><item><title>A Spitting Image: Modular Superpixel Tokenization in Vision Transformers</title><link>http://arxiv.org/abs/2408.07680v1</link><description>Vision Transformer (ViT) architectures traditionally employ a grid-basedapproach to tokenization independent of the semantic content of an image. Wepropose a modular superpixel tokenization strategy which decouples tokenizationand feature extraction; a shift from contemporary approaches where these aretreated as an undifferentiated whole. Using on-line content-aware tokenizationand scale- and shape-invariant positional embeddings, we perform experimentsand ablations that contrast our approach with patch-based tokenization andrandomized partitions as baselines. We show that our method significantlyimproves the faithfulness of attributions, gives pixel-level granularity onzero-shot unsupervised dense prediction tasks, while maintaining predictiveperformance in classification tasks. Our approach provides a modulartokenization framework commensurable with standard architectures, extending thespace of ViTs to a larger class of semantically-rich models.</description><author>Marius Aasan, Odd Kolbjørnsen, Anne Schistad Solberg, Adín Ramirez Rivera</author><pubDate>Wed, 14 Aug 2024 17:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07680v1</guid></item><item><title>Robust Curve Detection in Volumetric Medical Imaging via Attraction Field</title><link>http://arxiv.org/abs/2408.01159v2</link><description>Understanding body part geometry is crucial for precise medical diagnostics.Curves effectively describe anatomical structures and are widely used inmedical imaging applications related to cardiovascular, respiratory, andskeletal diseases. Traditional curve detection methods are often task-specific,relying heavily on domain-specific features, limiting their broaderapplicability. This paper introduces a novel approach for detectingnon-branching curves, which does not require prior knowledge of the object'sorientation, shape, or position. Our method uses neural networks to predict (1)an attraction field, which offers subpixel accuracy, and (2) a closeness map,which limits the region of interest and essentially eliminates outliers farfrom the desired curve. We tested our curve detector on several clinicallyrelevant tasks with diverse morphologies and achieved impressive subpixel-levelaccuracy results that surpass existing methods, highlighting its versatilityand robustness. Additionally, to support further advancements in this field, weprovide our private annotations of aortic centerlines and masks, which canserve as a benchmark for future research. The dataset can be found athttps://github.com/neuro-ml/curve-detection.</description><author>Farukh Yaushev, Daria Nogina, Valentin Samokhin, Mariya Dugova, Ekaterina Petrash, Dmitry Sevryukov, Mikhail Belyaev, Maxim Pisov</author><pubDate>Wed, 14 Aug 2024 17:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01159v2</guid></item><item><title>Enhanced Detection of Conversational Mental Manipulation Through Advanced Prompting Techniques</title><link>http://arxiv.org/abs/2408.07676v1</link><description>This study presents a comprehensive, long-term project to explore theeffectiveness of various prompting techniques in detecting dialogical mentalmanipulation. We implement Chain-of-Thought prompting with Zero-Shot andFew-Shot settings on a binary mental manipulation detection task, building uponexisting work conducted with Zero-Shot and Few- Shot prompting. Our primaryobjective is to decipher why certain prompting techniques display superiorperformance, so as to craft a novel framework tailored for detection of mentalmanipulation. Preliminary findings suggest that advanced prompting techniquesmay not be suitable for more complex models, if they are not trained throughexample-based learning.</description><author>Ivory Yang, Xiaobo Guo, Sean Xie, Soroush Vosoughi</author><pubDate>Wed, 14 Aug 2024 17:23:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07676v1</guid></item><item><title>G$^2$V$^2$former: Graph Guided Video Vision Transformer for Face Anti-Spoofing</title><link>http://arxiv.org/abs/2408.07675v1</link><description>In videos containing spoofed faces, we may uncover the spoofing evidencebased on either photometric or dynamic abnormality, even a combination of both.Prevailing face anti-spoofing (FAS) approaches generally concentrate on thesingle-frame scenario, however, purely photometric-driven methods overlook thedynamic spoofing clues that may be exposed over time. This may lead FAS systemsto conclude incorrect judgments, especially in cases where it is easilydistinguishable in terms of dynamics but challenging to discern in terms ofphotometrics. To this end, we propose the Graph Guided Video Vision Transformer(G$^2$V$^2$former), which combines faces with facial landmarks for photometricand dynamic feature fusion. We factorize the attention into space and time, andfuse them via a spatiotemporal block. Specifically, we design a novel temporalattention called Kronecker temporal attention, which has a wider receptivefield, and is beneficial for capturing dynamic information. Moreover, weleverage the low-semantic motion of facial landmarks to guide the high-semanticchange of facial expressions based on the motivation that regions containinglandmarks may reveal more dynamic clues. Extensive experiments on ninebenchmark datasets demonstrate that our method achieves superior performanceunder various scenarios. The codes will be released soon.</description><author>Jingyi Yang, Zitong Yu, Xiuming Ni, Jia He, Hui Li</author><pubDate>Wed, 14 Aug 2024 17:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07675v1</guid></item><item><title>Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources</title><link>http://arxiv.org/abs/2308.09766v3</link><description>Prediction of dynamic environmental variables in unmonitored sites remains along-standing challenge for water resources science. The majority of theworld's freshwater resources have inadequate monitoring of criticalenvironmental variables needed for management. Yet, the need to have widespreadpredictions of hydrological variables such as river flow and water quality hasbecome increasingly urgent due to climate and land use change over the pastdecades, and their associated impacts on water resources. Modern machinelearning methods increasingly outperform their process-based and empiricalmodel counterparts for hydrologic time series prediction with their ability toextract information from large, diverse data sets. We review relevantstate-of-the art applications of machine learning for streamflow, waterquality, and other water resources prediction and discuss opportunities toimprove the use of machine learning with emerging methods for incorporatingwatershed characteristics into deep learning models, transfer learning, andincorporating process knowledge into machine learning models. The analysis heresuggests most prior efforts have been focused on deep learning learningframeworks built on many sites for predictions at daily time scales in theUnited States, but that comparisons between different classes of machinelearning methods are few and inadequate. We identify several open questions fortime series predictions in unmonitored sites that include incorporating dynamicinputs and site characteristics, mechanistic understanding and spatial context,and explainable AI techniques in modern machine learning frameworks.</description><author>Jared D. Willard, Charuleka Varadharajan, Xiaowei Jia, Vipin Kumar</author><pubDate>Wed, 14 Aug 2024 17:20:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09766v3</guid></item><item><title>Deep Learning: a Heuristic Three-stage Mechanism for Grid Searches to Optimize the Future Risk Prediction of Breast Cancer Metastasis Using EHR-based Clinical Data</title><link>http://arxiv.org/abs/2408.07673v1</link><description>A grid search, at the cost of training and testing a large number of models,is an effective way to optimize the prediction performance of deep learningmodels. A challenging task concerning grid search is the time management.Without a good time management scheme, a grid search can easily be set off as amission that will not finish in our lifetime. In this study, we introduce aheuristic three-stage mechanism for managing the running time of low-budgetgrid searches, and the sweet-spot grid search (SSGS) and randomized grid search(RGS) strategies for improving model prediction performance, in predicting the5-year, 10-year, and 15-year risk of breast cancer metastasis. We develop deepfeedforward neural network (DFNN) models and optimize them through gridsearches. We conduct eight cycles of grid searches by applying our three-stagemechanism and SSGS and RGS strategies. We conduct various SHAP analysesincluding unique ones that interpret the importance of the DFNN-modelhyperparameters. Our results show that grid search can greatly improve modelprediction. The grid searches we conducted improved the risk prediction of5-year, 10-year, and 15-year breast cancer metastasis by 18.6%, 16.3%, and17.3% respectively, over the average performance of all corresponding models wetrained. We not only demonstrate best model performance but also characterizegrid searches from various aspects such as their capabilities of discoveringdecent models and the unit grid search time. The three-stage mechanism workedeffectively. It made our low-budget grid searches feasible and manageable, andin the meantime helped improve model prediction performance. Our SHAP analysesidentified both clinical risk factors important for the prediction of futurerisk of breast cancer metastasis, and DFNN-model hyperparameters important tothe prediction of performance scores.</description><author>Xia Jiang, Yijun Zhou, Chuhan Xu, Adam Brufsky, Alan Wells</author><pubDate>Wed, 14 Aug 2024 17:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07673v1</guid></item><item><title>Data Science for Geographic Information Systems</title><link>http://arxiv.org/abs/2404.03754v2</link><description>The integration of data science into Geographic Information Systems (GIS) hasfacilitated the evolution of these tools into complete spatial analysisplatforms. The adoption of machine learning and big data techniques hasequipped these platforms with the capacity to handle larger amounts ofincreasingly complex data, transcending the limitations of more traditionalapproaches. This work traces the historical and technical evolution of datascience and GIS as fields of study, highlighting the critical points ofconvergence between domains, and underlining the many sectors that rely on thisintegration. A GIS application is presented as a case study in the disastermanagement sector where we utilize aerial data from Tr\'oia, Portugal, toemphasize the process of insight extraction from raw data. We conclude byoutlining prospects for future research in integration of these fields ingeneral, and the developed application in particular.</description><author>Afonso Oliveira, Nuno Fachada, João P. Matos-Carvalho</author><pubDate>Wed, 14 Aug 2024 17:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03754v2</guid></item><item><title>Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities</title><link>http://arxiv.org/abs/2408.07666v1</link><description>Model merging is an efficient empowerment technique in the machine learningcommunity that does not require the collection of raw training data and doesnot require expensive computation. As model merging becomes increasinglyprevalent across various fields, it is crucial to understand the availablemodel merging techniques comprehensively. However, there is a significant gapin the literature regarding a systematic and thorough review of thesetechniques. This survey provides a comprehensive overview of model mergingmethods and theories, their applications in various domains and settings, andfuture research directions. Specifically, we first propose a new taxonomicapproach that exhaustively discusses existing model merging methods. Secondly,we discuss the application of model merging techniques in large languagemodels, multimodal large language models, and 10+ machine learning subfields,including continual learning, multi-task learning, few-shot learning, etc.Finally, we highlight the remaining challenges of model merging and discussfuture research directions. A comprehensive list of papers about model mergingis available at\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.</description><author>Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, Dacheng Tao</author><pubDate>Wed, 14 Aug 2024 16:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07666v1</guid></item><item><title>Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models</title><link>http://arxiv.org/abs/2408.07665v1</link><description>Warning: This paper may contain texts with uncomfortable content. Large Language Models (LLMs) have achieved remarkable performance in varioustasks, including those involving multimodal data like speech. However, thesemodels often exhibit biases due to the nature of their training data. Recently,more Speech Large Language Models (SLLMs) have emerged, underscoring the urgentneed to address these biases. This study introduces Spoken Stereoset, a datasetspecifically designed to evaluate social biases in SLLMs. By examining howdifferent models respond to speech from diverse demographic groups, we aim toidentify these biases. Our experiments reveal significant insights into theirperformance and bias levels. The findings indicate that while most models showminimal bias, some still exhibit slightly stereotypical or anti-stereotypicaltendencies.</description><author>Yi-Cheng Lin, Wei-Chih Chen, Hung-yi Lee</author><pubDate>Wed, 14 Aug 2024 16:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07665v1</guid></item><item><title>Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining of Probability Distributions</title><link>http://arxiv.org/abs/2408.07663v1</link><description>Large language models are susceptible to jailbreak attacks, which can resultin the generation of harmful content. While prior defenses mitigate these risksby perturbing or inspecting inputs, they ignore competing objectives, theunderlying cause of alignment failures. In this paper, we proposeAlignment-Enhanced Decoding (AED), a novel defense that employs adaptivedecoding to address the root causes of jailbreak issues. We first define theCompetitive Index to quantify alignment failures and utilize feedback fromself-evaluation to compute post-alignment logits. Then, AED adaptively combinesAED and post-alignment logits with the original logits to obtain harmless andhelpful distributions. Consequently, our method enhances safety alignment whilemaintaining helpfulness. We conduct experiments across five models and fourcommon jailbreaks, with the results validating the effectiveness of ourapproach. Code is available at https://github.com/GIGABaozi/AED.git.</description><author>Quan Liu, Zhenhong Zhou, Longzhu He, Yi Liu, Wei Zhang, Sen Su</author><pubDate>Wed, 14 Aug 2024 16:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07663v1</guid></item><item><title>Interpretable Graph Neural Networks for Heterogeneous Tabular Data</title><link>http://arxiv.org/abs/2408.07661v1</link><description>Many machine learning algorithms for tabular data produce black-box models,which prevent users from understanding the rationale behind the modelpredictions. In their unconstrained form, graph neural networks fall into thiscategory, and they have further limited abilities to handle heterogeneous data.To overcome these limitations, an approach is proposed, called IGNH(Interpretable Graph Neural Network for Heterogeneous tabular data), whichhandles both categorical and numerical features, while constraining thelearning process to generate exact feature attributions together with thepredictions. A large-scale empirical investigation is presented, showing thatthe feature attributions provided by IGNH align with Shapley values that arecomputed post hoc. Furthermore, the results show that IGNH outperforms twopowerful machine learning algorithms for tabular data, Random Forests andTabNet, while reaching a similar level of performance as XGBoost.</description><author>Amr Alkhatib, Henrik Boström</author><pubDate>Wed, 14 Aug 2024 16:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07661v1</guid></item><item><title>Off-Policy Reinforcement Learning with High Dimensional Reward</title><link>http://arxiv.org/abs/2408.07660v1</link><description>Conventional off-policy reinforcement learning (RL) focuses on maximizing theexpected return of scalar rewards. Distributional RL (DRL), in contrast,studies the distribution of returns with the distributional Bellman operator ina Euclidean space, leading to highly flexible choices for utility. This paperestablishes robust theoretical foundations for DRL. We prove the contractionproperty of the Bellman operator even when the reward space is aninfinite-dimensional separable Banach space. Furthermore, we demonstrate thatthe behavior of high- or infinite-dimensional returns can be effectivelyapproximated using a lower-dimensional Euclidean space. Leveraging thesetheoretical insights, we propose a novel DRL algorithm that tackles problemswhich have been previously intractable using conventional reinforcementlearning approaches.</description><author>Dong Neuck Lee, Michael R. Kosorok</author><pubDate>Wed, 14 Aug 2024 16:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07660v1</guid></item><item><title>Robust online reconstruction of continuous-time signals from a lean spike train ensemble code</title><link>http://arxiv.org/abs/2408.05950v2</link><description>Sensory stimuli in animals are encoded into spike trains by neurons, offeringadvantages such as sparsity, energy efficiency, and high temporal resolution.This paper presents a signal processing framework that deterministicallyencodes continuous-time signals into biologically feasible spike trains, andaddresses the questions about representable signal classes and reconstructionbounds. The framework considers encoding of a signal through spike trainsgenerated by an ensemble of neurons using a convolve-then-threshold mechanismwith various convolution kernels. A closed-form solution to the inverseproblem, from spike trains to signal reconstruction, is derived in the Hilbertspace of shifted kernel functions, ensuring sparse representation of ageneralized Finite Rate of Innovation (FRI) class of signals. Additionally,inspired by real-time processing in biological systems, an efficient iterativeversion of the optimal reconstruction is formulated that considers only afinite window of past spikes, ensuring robustness of the technique toill-conditioned encoding; convergence guarantees of the windowed reconstructionto the optimal solution are then provided. Experiments on a large audio datasetdemonstrate excellent reconstruction accuracy at spike rates as low asone-fifth of the Nyquist rate, while showing clear competitive advantage incomparison to state-of-the-art sparse coding techniques in the low spike rateregime.</description><author>Anik Chattopadhyay, Arunava Banerjee</author><pubDate>Wed, 14 Aug 2024 16:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05950v2</guid></item><item><title>Implicit Causal Representation Learning via Switchable Mechanisms</title><link>http://arxiv.org/abs/2402.11124v3</link><description>Learning causal representations from observational and interventional data inthe absence of known ground-truth graph structures necessitates implicit latentcausal representation learning. Implicit learning of causal mechanismstypically involves two categories of interventional data: hard and softinterventions. In real-world scenarios, soft interventions are often morerealistic than hard interventions, as the latter require fully controlledenvironments. Unlike hard interventions, which directly force changes in acausal variable, soft interventions exert influence indirectly by affecting thecausal mechanism. However, the subtlety of soft interventions impose severalchallenges for learning causal models. One challenge is that softintervention's effects are ambiguous, since parental relations remain intact.In this paper, we tackle the challenges of learning causal models using softinterventions while retaining implicit modelling. We propose ICLR-SM, whichmodels the effects of soft interventions by employing a causal mechanism switchvariable designed to toggle between different causal mechanisms. In ourexperiments, we consistently observe improved learning of identifiable, causalrepresentations, compared to baseline approaches.</description><author>Shayan Shirahmad Gale Bagi, Zahra Gharaee, Oliver Schulte, Mark Crowley</author><pubDate>Wed, 14 Aug 2024 16:39:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11124v3</guid></item><item><title>Graph Triple Attention Network: A Decoupled Perspective</title><link>http://arxiv.org/abs/2408.07654v1</link><description>Graph Transformers (GTs) have recently achieved significant success in thegraph domain by effectively capturing both long-range dependencies and graphinductive biases. However, these methods face two primary challenges: (1)multi-view chaos, which results from coupling multi-view information(positional, structural, attribute), thereby impeding flexible usage and theinterpretability of the propagation process. (2) local-global chaos, whicharises from coupling local message passing with global attention, leading toissues of overfitting and over-globalizing. To address these challenges, wepropose a high-level decoupled perspective of GTs, breaking them down intothree components and two interaction levels: positional attention, structuralattention, and attribute attention, alongside local and global interaction.Based on this decoupled perspective, we design a decoupled graph tripleattention network named DeGTA, which separately computes multi-view attentionsand adaptively integrates multi-view local and global information. Thisapproach offers three key advantages: enhanced interpretability, flexibledesign, and adaptive integration of local and global information. Throughextensive experiments, DeGTA achieves state-of-the-art performance acrossvarious datasets and tasks, including node classification and graphclassification. Comprehensive ablation studies demonstrate that decoupling isessential for improving performance and enhancing interpretability. Our code isavailable at: https://github.com/wangxiaotang0906/DeGTA</description><author>Xiaotang Wang, Yun Zhu, Haizhou Shi, Yongchao Liu, Chuntao Hong</author><pubDate>Wed, 14 Aug 2024 16:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07654v1</guid></item><item><title>See It All: Contextualized Late Aggregation for 3D Dense Captioning</title><link>http://arxiv.org/abs/2408.07648v1</link><description>3D dense captioning is a task to localize objects in a 3D scene and generatedescriptive sentences for each object. Recent approaches in 3D dense captioninghave adopted transformer encoder-decoder frameworks from object detection tobuild an end-to-end pipeline without hand-crafted components. However, theseapproaches struggle with contradicting objectives where a single queryattention has to simultaneously view both the tightly localized object regionsand contextual environment. To overcome this challenge, we introduce SIA(See-It-All), a transformer pipeline that engages in 3D dense captioning with anovel paradigm called late aggregation. SIA simultaneously decodes two sets ofqueries-context query and instance query. The instance query focuses onlocalization and object attribute descriptions, while the context queryversatilely captures the region-of-interest of relationships between multipleobjects or with the global scene, then aggregated afterwards (i.e., lateaggregation) via simple distance-based measures. To further enhance the qualityof contextualized caption generation, we design a novel aggregator to generatea fully informed caption based on the surrounding context, the globalenvironment, and object instances. Extensive experiments on two of the mostwidely-used 3D dense captioning datasets demonstrate that our proposed methodachieves a significant improvement over prior methods.</description><author>Minjung Kim, Hyung Suk Lim, Seung Hwan Kim, Soonyoung Lee, Bumsoo Kim, Gunhee Kim</author><pubDate>Wed, 14 Aug 2024 16:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07648v1</guid></item><item><title>Adaptive Behavioral AI: Reinforcement Learning to Enhance Pharmacy Services</title><link>http://arxiv.org/abs/2408.07647v1</link><description>Pharmacies are critical in healthcare systems, particularly in low- andmiddle-income countries. Procuring pharmacists with the right behavioralinterventions or nudges can enhance their skills, public health awareness, andpharmacy inventory management, ensuring access to essential medicines thatultimately benefit their patients. We introduce a reinforcement learningoperational system to deliver personalized behavioral interventions throughmobile health applications. We illustrate its potential by discussing a seriesof initial experiments run with SwipeRx, an all-in-one app for pharmacists,including B2B e-commerce, in Indonesia. The proposed method has broaderapplications extending beyond pharmacy operations to optimize healthcaredelivery.</description><author>Ana Fernández del Río, Michael Brennan Leong, Paulo Saraiva, Ivan Nazarov, Aditya Rastogi, Moiz Hassan, Dexian Tang, África Periáñez</author><pubDate>Wed, 14 Aug 2024 16:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07647v1</guid></item><item><title>SigmaRL: A Sample-Efficient and Generalizable Multi-Agent Reinforcement Learning Framework for Motion Planning</title><link>http://arxiv.org/abs/2408.07644v1</link><description>This paper introduces an open-source, decentralized framework named SigmaRL,designed to enhance both sample efficiency and generalization of multi-agentReinforcement Learning (RL) for motion planning of connected and automatedvehicles. Most RL agents exhibit a limited capacity to generalize, oftenfocusing narrowly on specific scenarios, and are usually evaluated in similaror even the same scenarios seen during training. Various methods have beenproposed to address these challenges, including experience replay andregularization. However, how observation design in RL affects sample efficiencyand generalization remains an under-explored area. We address this gap byproposing five strategies to design information-dense observations, focusing ongeneral features that are applicable to most traffic scenarios. We train our RLagents using these strategies on an intersection and evaluate theirgeneralization through numerical experiments across completely unseen trafficscenarios, including a new intersection, an on-ramp, and a roundabout.Incorporating these information-dense observations reduces training times tounder one hour on a single CPU, and the evaluation results reveal that our RLagents can effectively zero-shot generalize. Code:github.com/cas-lab-munich/SigmaRL</description><author>Jianye Xu, Pan Hu, Bassam Alrifaee</author><pubDate>Wed, 14 Aug 2024 16:16:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07644v1</guid></item><item><title>RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completion</title><link>http://arxiv.org/abs/2403.06095v4</link><description>Code Large Language Models (CodeLLMs) have demonstrated impressiveproficiency in code completion tasks. However, they often fall short of fullyunderstanding the extensive context of a project repository, such as theintricacies of relevant files and class hierarchies, which can result in lessprecise completions. To overcome these limitations, we present \tool, amultifaceted framework designed to address the complex challenges associatedwith repository-level code completion. Central to RepoHYPER is the {\emRepo-level Semantic Graph} (RSG), a novel semantic graph structure thatencapsulates the vast context of code repositories. Furthermore, RepoHyperleverages Expand and Refine retrieval method, including a graph expansion and alink prediction algorithm applied to the RSG, enabling the effective retrievaland prioritization of relevant code snippets. Our evaluations show that \toolmarkedly outperforms existing techniques in repository-level code completion,showcasing enhanced accuracy across various datasets when compared to severalstrong baselines. Our implementation of RepoHYPER can be found athttps://github.com/FSoft-AI4Code/RepoHyper.</description><author>Huy N. Phan, Hoang N. Phan, Tien N. Nguyen, Nghi D. Q. Bui</author><pubDate>Wed, 14 Aug 2024 16:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06095v4</guid></item><item><title>Boosting Unconstrained Face Recognition with Targeted Style Adversary</title><link>http://arxiv.org/abs/2408.07642v1</link><description>While deep face recognition models have demonstrated remarkable performance,they often struggle on the inputs from domains beyond their training data.Recent attempts aim to expand the training set by relying on computationallyexpensive and inherently challenging image-space augmentation of imagegeneration modules. In an orthogonal direction, we present a simple yeteffective method to expand the training data by interpolating betweeninstance-level feature statistics across labeled and unlabeled sets. Ourmethod, dubbed Targeted Style Adversary (TSA), is motivated by twoobservations: (i) the input domain is reflected in feature statistics, and (ii)face recognition model performance is influenced by style information. Shiftingtowards an unlabeled style implicitly synthesizes challenging traininginstances. We devise a recognizability metric to constraint our framework topreserve the inherent identity-related information of labeled instances. Theefficacy of our method is demonstrated through evaluations on unconstrainedbenchmarks, outperforming or being on par with its competitors while offeringnearly a 70\% improvement in training speed and 40\% less memory consumption.</description><author>Mohammad Saeed Ebrahimi Saadabadi, Sahar Rahimi Malakshan, Seyed Rasoul Hosseini, Nasser M. Nasrabadi</author><pubDate>Wed, 14 Aug 2024 16:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07642v1</guid></item><item><title>Distilling the Knowledge in Data Pruning</title><link>http://arxiv.org/abs/2403.07854v2</link><description>With the increasing size of datasets used for training neural networks, datapruning becomes an attractive field of research. However, most current datapruning algorithms are limited in their ability to preserve accuracy comparedto models trained on the full data, especially in high pruning regimes. In thispaper we explore the application of data pruning while incorporating knowledgedistillation (KD) when training on a pruned subset. That is, rather thanrelying solely on ground-truth labels, we also use the soft predictions from ateacher network pre-trained on the complete data. By integrating KD intotraining, we demonstrate significant improvement across datasets, pruningmethods, and on all pruning fractions. We first establish a theoreticalmotivation for employing self-distillation to improve training on pruned data.Then, we empirically make a compelling and highly practical observation: usingKD, simple random pruning is comparable or superior to sophisticated pruningmethods across all pruning regimes. On ImageNet for example, we achievesuperior accuracy despite training on a random subset of only 50% of the data.Additionally, we demonstrate a crucial connection between the pruning factorand the optimal knowledge distillation weight. This helps mitigate the impactof samples with noisy labels and low-quality images retained by typical pruningalgorithms. Finally, we make an intriguing observation: when using lowerpruning fractions, larger teachers lead to accuracy degradation, whilesurprisingly, employing teachers with a smaller capacity than the student's mayimprove results. Our code will be made available.</description><author>Emanuel Ben-Baruch, Adam Botach, Igor Kviatkovsky, Manoj Aggarwal, Gérard Medioni</author><pubDate>Wed, 14 Aug 2024 16:08:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07854v2</guid></item><item><title>Hierarchical Working Memory and a New Magic Number</title><link>http://arxiv.org/abs/2408.07637v1</link><description>The extremely limited working memory span, typically around four items,contrasts sharply with our everyday experience of processing much largerstreams of sensory information concurrently. This disparity suggests thatworking memory can organize information into compact representations such aschunks, yet the underlying neural mechanisms remain largely unknown. Here, wepropose a recurrent neural network model for chunking within the framework ofthe synaptic theory of working memory. We showed that by selectivelysuppressing groups of stimuli, the network can maintain and retrieve thestimuli in chunks, hence exceeding the basic capacity. Moreover, we show thatour model can dynamically construct hierarchical representations within workingmemory through hierarchical chunking. A consequence of this proposed mechanismis a new limit on the number of items that can be stored and subsequentlyretrieved from working memory, depending only on the basic working memorycapacity when chunking is not invoked. Predictions from our model wereconfirmed by analyzing single-unit responses in epileptic patients and memoryexperiments with verbal material. Our work provides a novel conceptual andanalytical framework for understanding the on-the-fly organization ofinformation in the brain that is crucial for cognition.</description><author>Weishun Zhong, Mikhail Katkov, Misha Tsodyks</author><pubDate>Wed, 14 Aug 2024 16:03:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07637v1</guid></item><item><title>Fair Enough? A map of the current limitations of the requirements to have fair algorithms</title><link>http://arxiv.org/abs/2311.12435v3</link><description>In recent years, the increase in the usage and efficiency of ArtificialIntelligence and, more in general, of Automated Decision-Making systems hasbrought with it an increasing and welcome awareness of the risks associatedwith such systems. One of such risks is that of perpetuating or even amplifyingbias and unjust disparities present in the data from which many of thesesystems learn to adjust and optimise their decisions. This awareness has on theone hand encouraged several scientific communities to come up with more andmore appropriate ways and methods to assess, quantify, and possibly mitigatesuch biases and disparities. On the other hand, it has prompted more and morelayers of society, including policy makers, to call for fair algorithms. Webelieve that while many excellent and multidisciplinary research is currentlybeing conducted, what is still fundamentally missing is the awareness thathaving fair algorithms is per se a nearly meaningless requirement that needs tobe complemented with many additional social choices to become actionable.Namely, there is a hiatus between what the society is demanding from AutomatedDecision-Making systems, and what this demand actually means in real-worldscenarios. In this work, we outline the key features of such a hiatus andpinpoint a set of crucial open points that we as a society must address inorder to give a concrete meaning to the increasing demand of fairness inAutomated Decision-Making systems.</description><author>Daniele Regoli, Alessandro Castelnovo, Nicole Inverardi, Gabriele Nanino, Ilaria Penco</author><pubDate>Wed, 14 Aug 2024 16:01:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12435v3</guid></item><item><title>Drug Discovery SMILES-to-Pharmacokinetics Diffusion Models with Deep Molecular Understanding</title><link>http://arxiv.org/abs/2408.07636v1</link><description>Artificial intelligence (AI) is increasingly used in every stage of drugdevelopment. One challenge facing drug discovery AI is that drugpharmacokinetic (PK) datasets are often collected independently from eachother, often with limited overlap, creating data overlap sparsity. Datasparsity makes data curation difficult for researchers looking to answerresearch questions in poly-pharmacy, drug combination research, andhigh-throughput screening. We propose Imagand, a novelSMILES-to-Pharmacokinetic (S2PK) diffusion model capable of generating an arrayof PK target properties conditioned on SMILES inputs. We show thatImagand-generated synthetic PK data closely resembles real data univariate andbivariate distributions, and improves performance for downstream tasks. Imagandis a promising solution for data overlap sparsity and allows researchers toefficiently generate ligand PK data for drug discovery research. Code isavailable at \url{https://github.com/bing1100/Imagand}.</description><author>Bing Hu, Anita Layton, Helen Chen</author><pubDate>Wed, 14 Aug 2024 16:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07636v1</guid></item><item><title>Massive Activations in Large Language Models</title><link>http://arxiv.org/abs/2402.17762v2</link><description>We observe an empirical phenomenon in Large Language Models (LLMs) -- veryfew activations exhibit significantly larger values than others (e.g., 100,000times larger). We call them massive activations. First, we demonstrate thewidespread existence of massive activations across various LLMs andcharacterize their locations. Second, we find their values largely stayconstant regardless of the input, and they function as indispensable bias termsin LLMs. Third, these massive activations lead to the concentration ofattention probabilities to their corresponding tokens, and further, implicitbias terms in the self-attention output. Last, we also study massiveactivations in Vision Transformers. Code is available athttps://github.com/locuslab/massive-activations.</description><author>Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu</author><pubDate>Wed, 14 Aug 2024 16:00:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17762v2</guid></item><item><title>A Survey of Open Source User Activity Traces with Applications to User Mobility Characterization and Modeling</title><link>http://arxiv.org/abs/2110.06382v3</link><description>The current state-of-the-art in user mobility research has extensively reliedon open-source mobility traces captured from pedestrian and vehicular activitythrough a variety of communication technologies as users engage in a wide-rangeof applications, including connected healthcare, localization, social media,e-commerce, etc. Most of these traces are feature-rich and diverse, not only inthe information they provide, but also in how they can be used and leveraged.This diversity poses two main challenges for researchers and practitioners whowish to make use of available mobility datasets. First, it is quite difficultto get a bird's eye view of the available traces without spending considerabletime looking them up. Second, once they have found the traces, they still needto figure out whether the traces are adequate to their needs. The purpose of this survey is three-fold. It proposes a taxonomy to classifyopen-source mobility traces including their mobility mode, data source andcollection technology. It then uses the proposed taxonomy to classify existingopen-source mobility traces and finally, highlights three case studies usingpopular publicly available datasets to showcase how our taxonomy can tease outfeature sets in traces to help determine their applicability to specificuse-cases.</description><author>Sinjoni Mukhopadhyay King, Faisal Nawab, Katia Obraczka</author><pubDate>Wed, 14 Aug 2024 15:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.06382v3</guid></item><item><title>Towards Fair and Rigorous Evaluations: Hyperparameter Optimization for Top-N Recommendation Task with Implicit Feedback</title><link>http://arxiv.org/abs/2408.07630v1</link><description>The widespread use of the internet has led to an overwhelming amount of data,which has resulted in the problem of information overload. Recommender systemshave emerged as a solution to this problem by providing personalizedrecommendations to users based on their preferences and historical data.However, as recommendation models become increasingly complex, finding the besthyperparameter combination for different models has become a challenge. Thehigh-dimensional hyperparameter search space poses numerous challenges forresearchers, and failure to disclose hyperparameter settings may impede thereproducibility of research results. In this paper, we investigate the Top-Nimplicit recommendation problem and focus on optimizing the benchmarkrecommendation algorithm commonly used in comparative experiments usinghyperparameter optimization algorithms. We propose a research methodology thatfollows the principles of a fair comparison, employing seven types ofhyperparameter search algorithms to fine-tune six common recommendationalgorithms on three datasets. We have identified the most suitablehyperparameter search algorithms for various recommendation algorithms ondifferent types of datasets as a reference for later study. This studycontributes to algorithmic research in recommender systems based onhyperparameter optimization, providing a fair basis for comparison.</description><author>Hui Fang, Xu Feng, Lu Qin, Zhu Sun</author><pubDate>Wed, 14 Aug 2024 15:56:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07630v1</guid></item><item><title>Optimizing HIV Patient Engagement with Reinforcement Learning in Resource-Limited Settings</title><link>http://arxiv.org/abs/2408.07629v1</link><description>By providing evidence-based clinical decision support, digital tools andelectronic health records can revolutionize patient management, especially inresource-poor settings where fewer health workers are available and often needmore training. When these tools are integrated with AI, they can offerpersonalized support and adaptive interventions, effectively connectingcommunity health workers (CHWs) and healthcare facilities. The CHARM (CommunityHealth Access &amp; Resource Management) app is an AI-native mobile app for CHWs.Developed through a joint partnership of Causal Foundry (CF) andmothers2mothers (m2m), CHARM empowers CHWs, mainly local women, by streamliningcase management, enhancing learning, and improving communication. This paperdetails CHARM's development, integration, and upcoming reinforcementlearning-based adaptive interventions, all aimed at enhancing health workerengagement, efficiency, and patient outcomes, thereby enhancing CHWs'capabilities and community health.</description><author>África Periáñez, Kathrin Schmitz, Lazola Makhupula, Moiz Hassan, Moeti Moleko, Ana Fernández del Río, Ivan Nazarov, Aditya Rastogi, Dexian Tang</author><pubDate>Wed, 14 Aug 2024 15:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07629v1</guid></item><item><title>Battery GraphNets : Relational Learning for Lithium-ion Batteries(LiBs) Life Estimation</title><link>http://arxiv.org/abs/2408.07624v1</link><description>Battery life estimation is critical for optimizing battery performance andguaranteeing minimal degradation for better efficiency and reliability ofbattery-powered systems. The existing methods to predict the Remaining UsefulLife(RUL) of Lithium-ion Batteries (LiBs) neglect the relational dependenciesof the battery parameters to model the nonlinear degradation trajectories. Wepresent the Battery GraphNets framework that jointly learns to incorporate adiscrete dependency graph structure between battery parameters to capture thecomplex interactions and the graph-learning algorithm to model the intrinsicbattery degradation for RUL prognosis. The proposed method outperforms severalpopular methods by a significant margin on publicly available battery datasetsand achieves SOTA performance. We report the ablation studies to support theefficacy of our approach.</description><author>Sakhinana Sagar Srinivas, Rajat Kumar Sarkar, Venkataramana Runkana</author><pubDate>Wed, 14 Aug 2024 15:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07624v1</guid></item><item><title>Latent Anomaly Detection Through Density Matrices</title><link>http://arxiv.org/abs/2408.07623v1</link><description>This paper introduces a novel anomaly detection framework that combines therobust statistical principles of density-estimation-based anomaly detectionmethods with the representation-learning capabilities of deep learning models.The method originated from this framework is presented in two differentversions: a shallow approach employing a density-estimation model based onadaptive Fourier features and density matrices, and a deep approach thatintegrates an autoencoder to learn a low-dimensional representation of thedata. By estimating the density of new samples, both methods are able to findnormality scores. The methods can be seamlessly integrated into an end-to-endarchitecture and optimized using gradient-based optimization techniques. Toevaluate their performance, extensive experiments were conducted on variousbenchmark datasets. The results demonstrate that both versions of the methodcan achieve comparable or superior performance when compared to otherstate-of-the-art methods. Notably, the shallow approach performs better ondatasets with fewer dimensions, while the autoencoder-based approach showsimproved performance on datasets with higher dimensions.</description><author>Joseph Gallego-Mejia, Oscar Bustos-Brinez, Fabio A. González</author><pubDate>Wed, 14 Aug 2024 15:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07623v1</guid></item><item><title>An Event Structure-aware Generative Model for Biomedical Event Extraction</title><link>http://arxiv.org/abs/2408.06583v2</link><description>Biomedical Event Extraction (BEE) is a challenging task that involvesmodeling complex relationships between fine-grained entities in biomedicaltext. Most existing BEE models rely on classification methods that ignore labelsemantics and argument dependencies in the data. Although generative modelsthat use prompts are increasingly being used for event extraction, they facetwo main challenges: creating effective prompts for the biomedical domain anddealing with events with complex structures in the text. To address theselimitations, we propose GenBEE, a generative model enhanced withstructure-aware prefixes for biomedical event extraction. GenBEE constructsevent prompts that leverage knowledge distilled from large language models(LLMs), thereby incorporating both label semantics and argument dependencyrelationships. Additionally, GenBEE introduces a structural prefix learningmodule that generates structure-aware prefixes with structural prompts,enriching the generation process with structural features. Extensiveexperiments on three benchmark datasets demonstrate the effectiveness of GenBEEand it achieves state-of-the-art performance on the MLEE and GE11 datasets.Moreover, our analysis shows that the structural prefixes effectively bridgethe gap between structural prompts and the representation space of generativemodels, enabling better integration of event structural information.</description><author>Haohan Yuan, Siu Cheung Hui, Haopeng Zhang</author><pubDate>Wed, 14 Aug 2024 15:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06583v2</guid></item><item><title>Evolving from Single-modal to Multi-modal Facial Deepfake Detection: A Survey</title><link>http://arxiv.org/abs/2406.06965v3</link><description>This survey addresses the critical challenge of deepfake detection amidst therapid advancements in artificial intelligence. As AI-generated media, includingvideo, audio and text, become more realistic, the risk of misuse to spreadmisinformation and commit identity fraud increases. Focused on face-centricdeepfakes, this work traces the evolution from traditional single-modalitymethods to sophisticated multi-modal approaches that handle audio-visual andtext-visual scenarios. We provide comprehensive taxonomies of detectiontechniques, discuss the evolution of generative methods from auto-encoders andGANs to diffusion models, and categorize these technologies by their uniqueattributes. To our knowledge, this is the first survey of its kind. We alsoexplore the challenges of adapting detection methods to new generative modelsand enhancing the reliability and robustness of deepfake detectors, proposingdirections for future research. This survey offers a detailed roadmap forresearchers, supporting the development of technologies to counter thedeceptive use of AI in media creation, particularly facial forgery. A curatedlist of all related papers can be found at\href{https://github.com/qiqitao77/Comprehensive-Advances-in-Deepfake-Detection-Spanning-Diverse-Modalities}{https://github.com/qiqitao77/Awesome-Comprehensive-Deepfake-Detection}.</description><author>Ping Liu, Qiqi Tao, Joey Tianyi Zhou</author><pubDate>Wed, 14 Aug 2024 15:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06965v3</guid></item><item><title>Exploring LLM Multi-Agents for ICD Coding</title><link>http://arxiv.org/abs/2406.15363v2</link><description>To address the limitations of Large Language Models (LLMs) in theInternational Classification of Diseases (ICD) coding task, where they oftenproduce inaccurate and incomplete prediction results due to thehigh-dimensional and skewed distribution of the ICD codes, and often lackinterpretability and reliability as well. We introduce an innovativemulti-agent approach for ICD coding which mimics the ICD coding assignmentprocedure in real-world settings, comprising five distinct agents: the patient,physician, coder, reviewer, and adjuster. Each agent utilizes an LLM-basedmodel tailored to their specific role within the coding process. We alsointegrate the system with Electronic Health Record (HER)'s SOAP (subjective,objective, assessment and plan) structure to boost the performances. We compareour method with a system of agents designed solely by LLMs and other strongbaselines and evaluate it using the Medical Information Mart for Intensive CareIII (MIMIC-III) dataset. Our multi-agent coding framework significantlyoutperforms Zero-shot Chain of Thought (CoT) prompting and self-consistencywith CoT (CoT-SC) in coding common and rare ICD codes. An ablation studyvalidates the effectiveness of the designated agent roles. it also outperformsthe LLM-designed agent system. Moreover, our method achieves comparable resultsto state-of-the-art ICD coding methods that require extensive pre-training orfine-tuning, and outperforms them in rare code accuracy, and explainability.Additionally, we demonstrate the method's practical applicability by presentingits performance in scenarios not limited by the common or rare ICD codeconstraints.The proposed multi-agent method for ICD coding effectively mimicsthe real-world coding process and improves performance on both common and rarecodes.</description><author>Rumeng Li, Xun Wang, Hong Yu</author><pubDate>Wed, 14 Aug 2024 15:32:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.15363v2</guid></item><item><title>Compact Model Training by Low-Rank Projection with Energy Transfer</title><link>http://arxiv.org/abs/2204.05566v3</link><description>Low-rankness plays an important role in traditional machine learning, but isnot so popular in deep learning. Most previous low-rank network compressionmethods compress networks by approximating pre-trained models and re-training.However, the optimal solution in the Euclidean space may be quite differentfrom the one with low-rank constraint. A well-pre-trained model is not a goodinitialization for the model with low-rank constraints. Thus, the performanceof a low-rank compressed network degrades significantly. Compared with othernetwork compression methods such as pruning, low-rank methods attract lessattention in recent years. In this paper, we devise a new training method,low-rank projection with energy transfer (LRPET), that trains low-rankcompressed networks from scratch and achieves competitive performance. Wepropose to alternately perform stochastic gradient descent training andprojection of each weight matrix onto the corresponding low-rank manifold.Compared to re-training on the compact model, this enables full utilization ofmodel capacity since solution space is relaxed back to Euclidean space afterprojection. The matrix energy (the sum of squares of singular values) reductioncaused by projection is compensated by energy transfer. We uniformly transferthe energy of the pruned singular values to the remaining ones. Wetheoretically show that energy transfer eases the trend of gradient vanishingcaused by projection. In modern networks, a batch normalization (BN) layer canbe merged into the previous convolution layer for inference, therebyinfluencing the optimal low-rank approximation of the previous layer. Wepropose BN rectification to cut off its effect on the optimal low-rankapproximation, which further improves the performance.</description><author>Kailing Guo, Zhenquan Lin, Canyang Chen, Xiaofen Xing, Fang Liu, Xiangmin Xu</author><pubDate>Wed, 14 Aug 2024 15:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.05566v3</guid></item><item><title>Rethinking the Key Factors for the Generalization of Remote Sensing Stereo Matching Networks</title><link>http://arxiv.org/abs/2408.07613v1</link><description>Stereo matching, a critical step of 3D reconstruction, has fully shiftedtowards deep learning due to its strong feature representation of remotesensing images. However, ground truth for stereo matching task relies onexpensive airborne LiDAR data, thus making it difficult to obtain enoughsamples for supervised learning. To improve the generalization ability ofstereo matching networks on cross-domain data from different sensors andscenarios, in this paper, we dedicate to study key training factors from threeperspectives. (1) For the selection of training dataset, it is important toselect data with similar regional target distribution as the test set insteadof utilizing data from the same sensor. (2) For model structure, cascadedstructure that flexibly adapts to different sizes of features is preferred. (3)For training manner, unsupervised methods generalize better than supervisedmethods, and we design an unsupervised early-stop strategy to help retain thebest model with pre-trained weights as the basis. Extensive experiments areconducted to support the previous findings, on the basis of which we present anunsupervised stereo matching network with good generalization performance. Werelease the source code and the datasets athttps://github.com/Elenairene/RKF_RSSM to reproduce the results and encouragefuture work.</description><author>Liting Jiang, Feng Wang, Wenyi Zhang, Peifeng Li, Hongjian You, Yuming Xiang</author><pubDate>Wed, 14 Aug 2024 15:26:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07613v1</guid></item><item><title>Value-Based Rationales Improve Social Experience: A Multiagent Simulation Study</title><link>http://arxiv.org/abs/2408.02117v2</link><description>We propose Exanna, a framework to realize agents that incorporate values indecision making. An Exannaagent considers the values of itself and others whenproviding rationales for its actions and evaluating the rationales provided byothers. Via multiagent simulation, we demonstrate that considering values indecision making and producing rationales, especially for norm-deviatingactions, leads to (1) higher conflict resolution, (2) better social experience,(3) higher privacy, and (4) higher flexibility.</description><author>Sz-Ting Tzeng, Nirav Ajmeri, Munindar P. Singh</author><pubDate>Wed, 14 Aug 2024 15:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02117v2</guid></item><item><title>Amuro &amp; Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models</title><link>http://arxiv.org/abs/2408.06663v2</link><description>The development of large language models leads to the formation of apre-train-then-align paradigm, in which the model is typically pre-trained on alarge text corpus and undergoes a tuning stage to align the model with humanpreference or downstream tasks. In this work, we investigate the relationshipbetween pre-training and fine-tuning by fine-tuning multiple intermediatepre-trained model checkpoints. Our results on 18 datasets suggest that i)continual pre-training improves the model in a latent way that unveils afterfine-tuning; ii) with extra fine-tuning, the datasets that the model does notdemonstrate capability gain much more than those that the model performs wellduring the pre-training stage; iii) although model benefits significantlythrough supervised fine-tuning, it may forget previously known domain knowledgeand the tasks that are not seen during fine-tuning; iv) the model resembleshigh sensitivity to evaluation prompts after supervised fine-tuning, but thissensitivity can be alleviated by more pre-training.</description><author>Kaiser Sun, Mark Dredze</author><pubDate>Wed, 14 Aug 2024 15:23:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06663v2</guid></item><item><title>RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential Recommenders</title><link>http://arxiv.org/abs/2408.02354v3</link><description>Scalability is a major challenge in modern recommender systems. In sequentialrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-artrecommendation quality but consumes excessive GPU memory with large itemcatalogs, limiting its practicality. Using a GPU-efficient locality-sensitivehashing-like algorithm for approximating large tensor of logits, this paperintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantlyreduces memory consumption while allowing one to enjoy the state-of-the-artperformance of full CE loss. Experimental results on various datasets show thatRECE cuts training peak memory usage by up to 12 times compared to existingmethods while retaining or exceeding performance metrics of CE loss. Theapproach also opens up new possibilities for large-scale applications in otherdomains.</description><author>Danil Gusak, Gleb Mezentsev, Ivan Oseledets, Evgeny Frolov</author><pubDate>Wed, 14 Aug 2024 15:19:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02354v3</guid></item><item><title>WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs</title><link>http://arxiv.org/abs/2408.07611v1</link><description>Large Language Models (LLMs) have greatly contributed to the development ofadaptive intelligent agents and are positioned as an important way to achieveArtificial General Intelligence (AGI). However, LLMs are prone to producefactually incorrect information and often produce "phantom" content thatundermines their reliability, which poses a serious challenge for theirdeployment in real-world scenarios. Enhancing LLMs by combining externaldatabases and information retrieval mechanisms is an effective path. To addressthe above challenges, we propose a new approach called WeKnow-RAG, whichintegrates Web search and Knowledge Graphs into a "Retrieval-AugmentedGeneration (RAG)" system. First, the accuracy and reliability of LLM responsesare improved by combining the structured representation of Knowledge Graphswith the flexibility of dense vector retrieval. WeKnow-RAG then utilizesdomain-specific knowledge graphs to satisfy a variety of queries and domains,thereby improving performance on factual information and complex reasoningtasks by employing multi-stage web page retrieval techniques using both sparseand dense retrieval methods. Our approach effectively balances the efficiencyand accuracy of information retrieval, thus improving the overall retrievalprocess. Finally, we also integrate a self-assessment mechanism for the LLM toevaluate the trustworthiness of the answers it generates. Our approach provesits outstanding effectiveness in a wide range of offline experiments and onlinesubmissions.</description><author>Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu</author><pubDate>Wed, 14 Aug 2024 15:19:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07611v1</guid></item><item><title>CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning</title><link>http://arxiv.org/abs/2407.15793v3</link><description>With the emergence of Transformers and Vision-Language Models (VLMs) such asCLIP, fine-tuning large pre-trained models has recently become a prevalentstrategy in Continual Learning. This has led to the development of numerousprompting strategies to adapt transformer-based models without incurringcatastrophic forgetting. However, these strategies often compromise theoriginal zero-shot capabilities of the pre-trained CLIP model and struggle toadapt to domains that significantly deviate from the pre-training data. In thiswork, we propose Continual Generative training for Incremental prompt-Learning,a simple and novel approach to mitigate forgetting while adapting CLIP.Briefly, we employ Variational Autoencoders (VAEs) to learn class-conditioneddistributions within the embedding space of the visual encoder. We then exploitthese distributions to sample new synthetic visual embeddings and train thecorresponding class-specific textual prompts during subsequent tasks. Throughextensive experiments on different domains, we show that such a generativereplay approach can adapt to new tasks while improving zero-shot capabilities,evaluated using a novel metric tailored for CL scenarios. Notably, furtheranalysis reveals that our approach can bridge the gap with joint prompt tuning.The codebase is available at https://github.com/aimagelab/mammoth.</description><author>Emanuele Frascaroli, Aniello Panariello, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</author><pubDate>Wed, 14 Aug 2024 15:12:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15793v3</guid></item><item><title>Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving</title><link>http://arxiv.org/abs/2408.07605v1</link><description>The field of autonomous driving increasingly demands high-quality annotatedvideo training data. In this paper, we propose Panacea+, a powerful anduniversally applicable framework for generating video data in driving scenes.Built upon the foundation of our previous work, Panacea, Panacea+ adopts amulti-view appearance noise prior mechanism and a super-resolution module forenhanced consistency and increased resolution. Extensive experiments show thatthe generated video samples from Panacea+ greatly benefit a wide range of taskson different datasets, including 3D object tracking, 3D object detection, andlane detection tasks on the nuScenes and Argoverse 2 dataset. These resultsstrongly prove Panacea+ to be a valuable data generation framework forautonomous driving.</description><author>Yuqing Wen, Yucheng Zhao, Yingfei Liu, Binyuan Huang, Fan Jia, Yanhui Wang, Chi Zhang, Tiancai Wang, Xiaoyan Sun, Xiangyu Zhang</author><pubDate>Wed, 14 Aug 2024 15:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07605v1</guid></item><item><title>Disentangle and denoise: Tackling context misalignment for video moment retrieval</title><link>http://arxiv.org/abs/2408.07600v1</link><description>Video Moment Retrieval, which aims to locate in-context video momentsaccording to a natural language query, is an essential task for cross-modalgrounding. Existing methods focus on enhancing the cross-modal interactionsbetween all moments and the textual description for video understanding.However, constantly interacting with all locations is unreasonable because ofuneven semantic distribution across the timeline and noisy visual backgrounds.This paper proposes a cross-modal Context Denoising Network (CDNet) foraccurate moment retrieval by disentangling complex correlations and denoisingirrelevant dynamics.Specifically, we propose a query-guided semanticdisentanglement (QSD) to decouple video moments by estimating alignment levelsaccording to the global and fine-grained correlation. A Context-aware DynamicDenoisement (CDD) is proposed to enhance understanding of alignedspatial-temporal details by learning a group of query-relevant offsets.Extensive experiments on public benchmarks demonstrate that the proposed CDNetachieves state-of-the-art performances.</description><author>Kaijing Ma, Han Fang, Xianghao Zang, Chao Ban, Lanxiang Zhou, Zhongjiang He, Yongxiang Li, Hao Sun, Zerun Feng, Xingsong Hou</author><pubDate>Wed, 14 Aug 2024 15:00:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07600v1</guid></item><item><title>Assessing the Role of Lexical Semantics in Cross-lingual Transfer through Controlled Manipulations</title><link>http://arxiv.org/abs/2408.07599v1</link><description>While cross-linguistic model transfer is effective in many settings, there isstill limited understanding of the conditions under which it works. In thispaper, we focus on assessing the role of lexical semantics in cross-lingualtransfer, as we compare its impact to that of other language properties.Examining each language property individually, we systematically analyze howdifferences between English and a target language influence the capacity toalign the language with an English pretrained representation space. We do so byartificially manipulating the English sentences in ways that mimic specificcharacteristics of the target language, and reporting the effect of eachmanipulation on the quality of alignment with the representation space. We showthat while properties such as the script or word order only have a limitedimpact on alignment quality, the degree of lexical matching between the twolanguages, which we define using a measure of translation entropy, greatlyaffects it.</description><author>Roy Ilani, Taelin Karidi, Omri Abend</author><pubDate>Wed, 14 Aug 2024 14:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07599v1</guid></item><item><title>Progressive Radiance Distillation for Inverse Rendering with Gaussian Splatting</title><link>http://arxiv.org/abs/2408.07595v1</link><description>We propose progressive radiance distillation, an inverse rendering methodthat combines physically-based rendering with Gaussian-based radiance fieldrendering using a distillation progress map. Taking multi-view images as input,our method starts from a pre-trained radiance field guidance, and distillsphysically-based light and material parameters from the radiance field using animage-fitting process. The distillation progress map is initialized to a smallvalue, which favors radiance field rendering. During early iterations whenfitted light and material parameters are far from convergence, the radiancefield fallback ensures the sanity of image loss gradients and avoids localminima that attracts under-fit states. As fitted parameters converge, thephysical model gradually takes over and the distillation progress increasescorrespondingly. In presence of light paths unmodeled by the physical model,the distillation progress never finishes on affected pixels and the learnedradiance field stays in the final rendering. With this designed tolerance forphysical model limitations, we prevent unmodeled color components from leakinginto light and material parameters, alleviating relighting artifacts.Meanwhile, the remaining radiance field compensates for the limitations of thephysical model, guaranteeing high-quality novel views synthesis. Experimentalresults demonstrate that our method significantly outperforms state-of-the-arttechniques quality-wise in both novel view synthesis and relighting. The ideaof progressive radiance distillation is not limited to Gaussian splatting. Weshow that it also has positive effects for prominently specular scenes whenadapted to a mesh-based inverse rendering method.</description><author>Keyang Ye, Qiming Hou, Kun Zhou</author><pubDate>Wed, 14 Aug 2024 14:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07595v1</guid></item><item><title>Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space</title><link>http://arxiv.org/abs/2407.11917v2</link><description>We propose a new uncertainty estimator for gradient-free optimisation ofblack-box simulators using deep generative surrogate models. Optimisation ofthese simulators is especially challenging for stochastic simulators and higherdimensions. To address these issues, we utilise a deep generative surrogateapproach to model the black box response for the entire parameter space. Wethen leverage this knowledge to estimate the proposed uncertainty based on theWasserstein distance - the Wasserstein uncertainty. This approach is employedin a posterior agnostic gradient-free optimisation algorithm that minimisesregret over the entire parameter space. A series of tests were conducted todemonstrate that our method is more robust to the shape of both the black boxfunction and the stochastic response of the black box than state-of-the-artmethods, such as efficient global optimisation with a deep Gaussian processsurrogate.</description><author>Tigran Ramazyan, Mikhail Hushchyn, Denis Derkach</author><pubDate>Wed, 14 Aug 2024 14:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11917v2</guid></item><item><title>Natural Language to Verilog: Design of a Recurrent Spiking Neural Network using Large Language Models and ChatGPT</title><link>http://arxiv.org/abs/2405.01419v2</link><description>This paper investigates the use of Large Language Models (LLMs) forautomating the generation of hardware description code, aiming to explore theirpotential in supporting and enhancing the development of efficient neuromorphiccomputing architectures. Building on our prior work, we employ OpenAI'sChatGPT4 and natural language prompts to synthesize a RTL Verilog module of aprogrammable recurrent spiking neural network, while also generating testbenches to assess the system's correctness. The resultant design was validatedin three case studies, the exclusive OR,the IRIS flower classification and theMNIST hand-written digit classification, achieving accuracies of up to 96.6%.To verify its synthesizability and implementability, the design was prototypedon a field-programmable gate array and implemented on SkyWater 130 nmtechnology by using an open-source electronic design automation flow.Additionally, we have submitted it to Tiny Tapeout 6 chip fabrication programto further evaluate the system on-chip performance in the future.</description><author>Paola Vitolo, George Psaltakis, Michael Tomlinson, Gian Domenico Licciardo, Andreas G. Andreou</author><pubDate>Wed, 14 Aug 2024 14:42:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01419v2</guid></item><item><title>"How Big is Big Enough?" Adjusting Model Size in Continual Gaussian Processes</title><link>http://arxiv.org/abs/2408.07588v1</link><description>For many machine learning methods, creating a model requires setting aparameter that controls the model's capacity before training, e.g.~number ofneurons in DNNs, or inducing points in GPs. Increasing capacity improvesperformance until all the information from the dataset is captured. After thispoint, computational cost keeps increasing, without improved performance. Thisleads to the question ``How big is big enough?'' We investigate this problemfor Gaussian processes (single-layer neural networks) in continual learning.Here, data becomes available incrementally, and the final dataset size willtherefore not be known before training, preventing the use of heuristics forsetting the model size. We provide a method that automatically adjusts this,while maintaining near-optimal performance, and show that a singlehyperparameter setting for our method performs well across datasets with a widerange of properties.</description><author>Guiomar Pescador-Barrios, Sarah Filippi, Mark van der Wilk</author><pubDate>Wed, 14 Aug 2024 14:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07588v1</guid></item><item><title>FedQUIT: On-Device Federated Unlearning via a Quasi-Competent Virtual Teacher</title><link>http://arxiv.org/abs/2408.07587v1</link><description>Federated Learning (FL) promises better privacy guarantees for individuals'data when machine learning models are collaboratively trained. When an FLparticipant exercises its right to be forgotten, i.e., to detach from the FLframework it has participated and to remove its past contributions to theglobal model, the FL solution should perform all the necessary steps to make itpossible without sacrificing the overall performance of the global model, whichis not supported in state-of-the-art related solutions nowadays. In this paper,we propose FedQUIT, a novel algorithm that uses knowledge distillation to scrubthe contribution of the forgetting data from an FL global model whilepreserving its generalization ability. FedQUIT directly works on clients'devices and does not require sharing additional information if compared with aregular FL process, nor does it assume the availability of publicly availableproxy data. Our solution is efficient, effective, and applicable in bothcentralized and federated settings. Our experimental results show that, onaverage, FedQUIT requires less than 2.5% additional communication rounds torecover generalization performances after unlearning, obtaining a sanitizedglobal model whose predictions are comparable to those of a global model thathas never seen the data to be forgotten.</description><author>Alessio Mora, Lorenzo Valerio, Paolo Bellavista, Andrea Passarella</author><pubDate>Wed, 14 Aug 2024 14:36:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07587v1</guid></item><item><title>DeepFace-Attention: Multimodal Face Biometrics for Attention Estimation with Application to e-Learning</title><link>http://arxiv.org/abs/2408.05523v2</link><description>This work introduces an innovative method for estimating attention levels(cognitive load) using an ensemble of facial analysis techniques applied towebcam videos. Our method is particularly useful, among others, in e-learningapplications, so we trained, evaluated, and compared our approach on the mEBAL2database, a public multi-modal database acquired in an e-learning environment.mEBAL2 comprises data from 60 users who performed 8 different tasks. Thesetasks varied in difficulty, leading to changes in their cognitive loads. Ourapproach adapts state-of-the-art facial analysis technologies to quantify theusers' cognitive load in the form of high or low attention. Several behavioralsignals and physiological processes related to the cognitive load are used,such as eyeblink, heart rate, facial action units, and head pose, among others.Furthermore, we conduct a study to understand which individual features obtainbetter results, the most efficient combinations, explore local and globalfeatures, and how temporary time intervals affect attention level estimation,among other aspects. We find that global facial features are more appropriatefor multimodal systems using score-level fusion, particularly as the temporalwindow increases. On the other hand, local features are more suitable forfusion through neural network training with score-level fusion approaches. Ourmethod outperforms existing state-of-the-art accuracies using the public mEBAL2benchmark.</description><author>Roberto Daza, Luis F. Gomez, Julian Fierrez, Aythami Morales, Ruben Tolosana, Javier Ortega-Garcia</author><pubDate>Wed, 14 Aug 2024 14:34:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05523v2</guid></item><item><title>SAT Encoding of Partial Ordering Models for Graph Coloring Problems</title><link>http://arxiv.org/abs/2403.15961v3</link><description>In this paper, we suggest new SAT encodings of the partial-ordering based ILPmodel for the graph coloring problem (GCP) and the bandwidth coloring problem(BCP). The GCP asks for the minimum number of colors that can be assigned tothe vertices of a given graph such that each two adjacent vertices getdifferent colors. The BCP is a generalization, where each edge has a weightthat enforces a minimal "distance" between the assigned colors, and the goal isto minimize the "largest" color used. For the widely studied GCP, weexperimentally compare our new SAT encoding to the state-of-the-art approacheson the DIMACS benchmark set. Our evaluation confirms that this SAT encoding iseffective for sparse graphs and even outperforms the state-of-the-art on someDIMACS instances. For the BCP, our theoretical analysis shows that thepartial-ordering based SAT and ILP formulations have an asymptotically smallersize than that of the classical assignment-based model. Our practicalevaluation confirms not only a dominance compared to the assignment-basedencodings but also to the state-of-the-art approaches on a set of benchmarkinstances. Up to our knowledge, we have solved several open instances of theBCP from the literature for the first time.</description><author>Daniel Faber, Adalat Jabrayilov, Petra Mutzel</author><pubDate>Wed, 14 Aug 2024 14:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15961v3</guid></item><item><title>InternVideo2: Scaling Foundation Models for Multimodal Video Understanding</title><link>http://arxiv.org/abs/2403.15377v4</link><description>We introduce InternVideo2, a new family of video foundation models (ViFM)that achieve the state-of-the-art results in video recognition, video-texttasks, and video-centric dialogue. Our core design is a progressive trainingapproach that unifies the masked video modeling, crossmodal contrastivelearning, and next token prediction, scaling up the video encoder size to 6Bparameters. At the data level, we prioritize spatiotemporal consistency bysemantically segmenting videos and generating video-audio-speech captions. Thisimproves the alignment between video and text. Through extensive experiments,we validate our designs and demonstrate superior performance on over 60 videoand audio tasks. Notably, our model outperforms others on various video-relateddialogue and long video understanding benchmarks, highlighting its ability toreason and comprehend longer contexts. Code and models are available athttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.</description><author>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Chenting Wang, Guo Chen, Baoqi Pei, Ziang Yan, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Wed, 14 Aug 2024 14:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15377v4</guid></item><item><title>Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey</title><link>http://arxiv.org/abs/2408.07583v1</link><description>With significant advancements in Transformers LLMs, NLP has extended itsreach into many research fields due to its enhanced capabilities in textgeneration and user interaction. One field benefiting greatly from theseadvancements is cybersecurity. In cybersecurity, many parameters that need tobe protected and exchanged between senders and receivers are in the form oftext and tabular data, making NLP a valuable tool in enhancing the securitymeasures of communication protocols. This survey paper provides a comprehensiveanalysis of the utilization of Transformers and LLMs in cyber-threat detectionsystems. The methodology of paper selection and bibliometric analysis isoutlined to establish a rigorous framework for evaluating existing research.The fundamentals of Transformers are discussed, including backgroundinformation on various cyber-attacks and datasets commonly used in this field.The survey explores the application of Transformers in IDSs, focusing ondifferent architectures such as Attention-based models, LLMs like BERT and GPT,CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.Furthermore, it explores the diverse environments and applications whereTransformers and LLMs-based IDS have been implemented, including computernetworks, IoT devices, critical infrastructure protection, cloud computing,SDN, as well as in autonomous vehicles. The paper also addresses researchchallenges and future directions in this area, identifying key issues such asinterpretability, scalability, and adaptability to evolving threats, and more.Finally, the conclusion summarizes the findings and highlights the significanceof Transformers and LLMs in enhancing cyber-threat detection capabilities,while also outlining potential avenues for further research and development.</description><author>Hamza Kheddar</author><pubDate>Wed, 14 Aug 2024 14:28:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07583v1</guid></item><item><title>Theoretical and Practical Progress in Hyperspectral Pixel Unmixing with Large Spectral Libraries from a Sparse Perspective</title><link>http://arxiv.org/abs/2408.07580v1</link><description>Hyperspectral unmixing is the process of determining the presence ofindividual materials and their respective abundances from an observed pixelspectrum. Unmixing is a fundamental process in hyperspectral image analysis,and is growing in importance as increasingly large spectral libraries arecreated and used. Unmixing is typically done with ordinary least squares (OLS)regression. However, unmixing with large spectral libraries where the materialspresent in a pixel are not a priori known, solving for the coefficients in OLSrequires inverting a non-invertible matrix from a large spectral library. Anumber of regression methods are available that can produce a numericalsolution using regularization, but with considerably varied effectiveness.Also, simple methods that are unpopular in the statistics literature (i.e.step-wise regression) are used with some level of effectiveness inhyperspectral analysis. In this paper, we provide a thorough performanceevaluation of the methods considered, evaluating methods based on how oftenthey select the correct materials in the models. Investigated methods includeordinary least squares regression, non-negative least squares regression, ridgeregression, lasso regression, step-wise regression and Bayesian modelaveraging. We evaluated these unmixing approaches using multiple criteria:incorporation of non-negative abundances, model size, accurate mineraldetection and root mean squared error (RMSE). We provide a taxonomy of theregression methods, showing that most methods can be understood as Bayesianmethods with specific priors. We conclude that methods that can be derived withpriors that correspond to the phenomenology of hyperspectral imagery outperformthose with priors that are optimal for prediction performance under theassumptions of ordinary least squares linear regression.</description><author>Jade Preston, William Basener</author><pubDate>Wed, 14 Aug 2024 14:24:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07580v1</guid></item><item><title>TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases</title><link>http://arxiv.org/abs/2408.07579v1</link><description>While adversarial robustness in computer vision is a mature research field,fewer researchers have tackled the evasion attacks against tabular deeplearning, and even fewer investigated robustification mechanisms and reliabledefenses. We hypothesize that this lag in the research on tabular adversarialattacks is in part due to the lack of standardized benchmarks. To fill thisgap, we propose TabularBench, the first comprehensive benchmark of robustnessof tabular deep learning classification models. We evaluated adversarialrobustness with CAA, an ensemble of gradient and search attacks which wasrecently demonstrated as the most effective attack against a tabular model. Inaddition to our open benchmark (https://github.com/serval-uni-lu/tabularbench)where we welcome submissions of new models and defenses, we implement 7robustification mechanisms inspired by state-of-the-art defenses in computervision and propose the largest benchmark of robust tabular deep learning over200 models across five critical scenarios in finance, healthcare and security.We curated real datasets for each use case, augmented with hundreds ofthousands of realistic synthetic inputs, and trained and assessed our modelswith and without data augmentations. We open-source our library that providesAPI access to all our pre-trained robust tabular models, and the largestdatasets of real and synthetic tabular inputs. Finally, we analyze the impactof various defenses on the robustness and provide actionable insights to designnew defenses and robustification mechanisms.</description><author>Thibault Simonetto, Salah Ghamizi, Maxime Cordy</author><pubDate>Wed, 14 Aug 2024 14:23:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07579v1</guid></item><item><title>A Nested Graph Reinforcement Learning-based Decision-making Strategy for Eco-platooning</title><link>http://arxiv.org/abs/2408.07578v1</link><description>Platooning technology is renowned for its precise vehicle control, trafficflow optimization, and energy efficiency enhancement. However, in large-scalemixed platoons, vehicle heterogeneity and unpredictable traffic conditions leadto virtual bottlenecks. These bottlenecks result in reduced traffic throughputand increased energy consumption within the platoon. To address thesechallenges, we introduce a decision-making strategy based on nested graphreinforcement learning. This strategy improves collaborative decision-making,ensuring energy efficiency and alleviating congestion. We propose a theory ofnested traffic graph representation that maps dynamic interactions betweenvehicles and platoons in non-Euclidean spaces. By incorporating spatio-temporalweighted graph into a multi-head attention mechanism, we further enhance themodel's capacity to process both local and global data. Additionally, we havedeveloped a nested graph reinforcement learning framework to enhance theself-iterative learning capabilities of platooning. Using the I-24 dataset, wedesigned and conducted comparative algorithm experiments, generalizabilitytesting, and permeability ablation experiments, thereby validating the proposedstrategy's effectiveness. Compared to the baseline, our strategy increasesthroughput by 10% and decreases energy use by 9%. Specifically, increasing thepenetration rate of CAVs significantly enhances traffic throughput, though italso increases energy consumption.</description><author>Xin Gao, Xueyuan Li, Hao Liu, Ao Li, Zhaoyang Ma, Zirui Li</author><pubDate>Wed, 14 Aug 2024 14:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07578v1</guid></item><item><title>MetaSeg: MetaFormer-based Global Contexts-aware Network for Efficient Semantic Segmentation</title><link>http://arxiv.org/abs/2408.07576v1</link><description>Beyond the Transformer, it is important to explore how to exploit thecapacity of the MetaFormer, an architecture that is fundamental to theperformance improvements of the Transformer. Previous studies have exploited itonly for the backbone network. Unlike previous studies, we explore the capacityof the Metaformer architecture more extensively in the semantic segmentationtask. We propose a powerful semantic segmentation network, MetaSeg, whichleverages the Metaformer architecture from the backbone to the decoder. OurMetaSeg shows that the MetaFormer architecture plays a significant role incapturing the useful contexts for the decoder as well as for the backbone. Inaddition, recent segmentation methods have shown that using a CNN-basedbackbone for extracting the spatial information and a decoder for extractingthe global information is more effective than using a transformer-basedbackbone with a CNN-based decoder. This motivates us to adopt the CNN-basedbackbone using the MetaFormer block and design our MetaFormer-based decoder,which consists of a novel self-attention module to capture the global contexts.To consider both the global contexts extraction and the computationalefficiency of the self-attention for semantic segmentation, we propose aChannel Reduction Attention (CRA) module that reduces the channel dimension ofthe query and key into the one dimension. In this way, our proposed MetaSegoutperforms the previous state-of-the-art methods with more efficientcomputational costs on popular semantic segmentation and a medical imagesegmentation benchmark, including ADE20K, Cityscapes, COCO-stuff, and Synapse.The code is available at \url{https://github.com/hyunwoo137/MetaSeg}.</description><author>Beoungwoo Kang, Seunghun Moon, Yubin Cho, Hyunwoo Yu, Suk-Ju Kang</author><pubDate>Wed, 14 Aug 2024 14:16:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07576v1</guid></item><item><title>A General Framework for Constraint-based Causal Learning</title><link>http://arxiv.org/abs/2408.07575v1</link><description>By representing any constraint-based causal learning algorithm via aplaceholder property, we decompose the correctness condition into a partrelating the distribution and the true causal graph, and a part that dependssolely on the distribution. This provides a general framework to obtaincorrectness conditions for causal learning, and has the following implications.We provide exact correctness conditions for the PC algorithm, which are thenrelated to correctness conditions of some other existing causal discoveryalgorithms. We show that the sparsest Markov representation condition is theweakest correctness condition resulting from existing notions of minimality formaximal ancestral graphs and directed acyclic graphs. We also reason thatadditional knowledge than just Pearl-minimality is necessary for causallearning beyond faithfulness.</description><author>Kai Z. Teh, Kayvan Sadeghi, Terry Soo</author><pubDate>Wed, 14 Aug 2024 14:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07575v1</guid></item><item><title>Optimal Baseline Corrections for Off-Policy Contextual Bandits</title><link>http://arxiv.org/abs/2405.05736v2</link><description>The off-policy learning paradigm allows for recommender systems and generalranking applications to be framed as decision-making problems, where we aim tolearn decision policies that optimize an unbiased offline estimate of an onlinereward metric. With unbiasedness comes potentially high variance, and prevalentmethods exist to reduce estimation variance. These methods typically make useof control variates, either additive (i.e., baseline corrections or doublyrobust methods) or multiplicative (i.e., self-normalisation). Our work unifiesthese approaches by proposing a single framework built on their equivalence inlearning scenarios. The foundation of our framework is the derivation of anequivalent baseline correction for all of the existing control variates.Consequently, our framework enables us to characterize the variance-optimalunbiased estimator and provide a closed-form solution for it. This optimalestimator brings significantly improved performance in both evaluation andlearning, and minimizes data requirements. Empirical observations corroborateour theoretical findings.</description><author>Shashank Gupta, Olivier Jeunen, Harrie Oosterhuis, Maarten de Rijke</author><pubDate>Wed, 14 Aug 2024 14:14:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05736v2</guid></item><item><title>Disentangled Representation Learning with Transmitted Information Bottleneck</title><link>http://arxiv.org/abs/2311.01686v2</link><description>Encoding only the task-related information from the raw data, \ie,disentangled representation learning, can greatly contribute to the robustnessand generalizability of models. Although significant advances have been made byregularizing the information in representations with information theory, twomajor challenges remain: 1) the representation compression inevitably leads toperformance drop; 2) the disentanglement constraints on representations are incomplicated optimization. To these issues, we introduce Bayesian networks withtransmitted information to formulate the interaction among input andrepresentations during disentanglement. Building upon this framework, wepropose \textbf{DisTIB} (\textbf{T}ransmitted \textbf{I}nformation\textbf{B}ottleneck for \textbf{Dis}entangled representation learning), a novelobjective that navigates the balance between information compression andpreservation. We employ variational inference to derive a tractable estimationfor DisTIB. This estimation can be simply optimized via standard gradientdescent with a reparameterization trick. Moreover, we theoretically prove thatDisTIB can achieve optimal disentanglement, underscoring its superior efficacy.To solidify our claims, we conduct extensive experiments on various downstreamtasks to demonstrate the appealing efficacy of DisTIB and validate ourtheoretical analyses.</description><author>Zhuohang Dang, Minnan Luo, Chengyou Jia, Guang Dai, Jihong Wang, Xiaojun Chang, Jingdong Wang</author><pubDate>Wed, 14 Aug 2024 14:11:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01686v2</guid></item><item><title>MEEG and AT-DGNN: Improving EEG Emotion Recognition with Music Introducing and Graph-based Learning</title><link>http://arxiv.org/abs/2407.05550v3</link><description>We present the MEEG dataset, a multi-modal collection of music-inducedelectroencephalogram (EEG) recordings designed to capture emotional responsesto various musical stimuli across different valence and arousal levels. Thispublic dataset facilitates an in-depth examination of brainwave patterns withinmusical contexts, providing a robust foundation for studying brain networktopology during emotional processing. Leveraging the MEEG dataset, we introducethe Attention-based Temporal Learner with Dynamic Graph Neural Network(AT-DGNN), a novel framework for EEG-based emotion recognition. This modelcombines an attention mechanism with a dynamic graph neural network (DGNN) tocapture intricate EEG dynamics. The AT-DGNN achieves state-of-the-art (SOTA)performance with an accuracy of 83.74% in arousal recognition and 86.01% invalence recognition, outperforming existing SOTA methods. Comparative analysiswith traditional datasets, such as DEAP, further validates the model'seffectiveness and underscores the potency of music as an emotional stimulus.This study advances graph-based learning methodology in brain-computerinterfaces (BCI), significantly improving the accuracy of EEG-based emotionrecognition. The MEEG dataset and source code are publicly available athttps://github.com/xmh1011/AT-DGNN.</description><author>Minghao Xiao, Zhengxi Zhu, Bin Jiang, Meixia Qu, Wenyu Wang</author><pubDate>Wed, 14 Aug 2024 14:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05550v3</guid></item><item><title>Multi-task Heterogeneous Graph Learning on Electronic Health Records</title><link>http://arxiv.org/abs/2408.07569v1</link><description>Learning electronic health records (EHRs) has received emerging attentionbecause of its capability to facilitate accurate medical diagnosis. Since theEHRs contain enriched information specifying complex interactions betweenentities, modeling EHRs with graphs is shown to be effective in practice. TheEHRs, however, present a great degree of heterogeneity, sparsity, andcomplexity, which hamper the performance of most of the models applied to them.Moreover, existing approaches modeling EHRs often focus on learning therepresentations for a single task, overlooking the multi-task nature of EHRanalysis problems and resulting in limited generalizability across differenttasks. In view of these limitations, we propose a novel framework for EHRmodeling, namely MulT-EHR (Multi-Task EHR), which leverages a heterogeneousgraph to mine the complex relations and model the heterogeneity in the EHRs. Tomitigate the large degree of noise, we introduce a denoising module based onthe causal inference framework to adjust for severe confounding effects andreduce noise in the EHR data. Additionally, since our model adopts a singlegraph neural network for simultaneous multi-task prediction, we design amulti-task learning module to leverage the inter-task knowledge to regularizethe training process. Extensive empirical studies on MIMIC-III and MIMIC-IVdatasets validate that the proposed method consistently outperforms thestate-of-the-art designs in four popular EHR analysis tasks -- drugrecommendation, and predictions of the length of stay, mortality, andreadmission. Thorough ablation studies demonstrate the robustness of our methodupon variations to key components and hyperparameters.</description><author>Tsai Hor Chan, Guosheng Yin, Kyongtae Bae, Lequan Yu</author><pubDate>Wed, 14 Aug 2024 14:06:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07569v1</guid></item><item><title>$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation</title><link>http://arxiv.org/abs/2402.19457v3</link><description>Assessing the quality of summarizers poses significant challenges. Inresponse, we propose a novel task-oriented evaluation approach that assessessummarizers based on their capacity to produce summaries that are useful fordownstream tasks, while preserving task outcomes. We theoretically establish adirect relationship between the resulting error probability of these tasks andthe mutual information between source texts and generated summaries. Weintroduce $\texttt{COSMIC}$ as a practical implementation of this metric,demonstrating its strong correlation with human judgment-based metrics and itseffectiveness in predicting downstream task performance. Comparative analysesagainst established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$highlight the competitive performance of $\texttt{COSMIC}$.</description><author>Maxime Darrin, Philippe Formont, Jackie Chi Kit Cheung, Pablo Piantanida</author><pubDate>Wed, 14 Aug 2024 14:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19457v3</guid></item><item><title>Sonic: Fast and Transferable Data Poisoning on Clustering Algorithms</title><link>http://arxiv.org/abs/2408.07558v1</link><description>Data poisoning attacks on clustering algorithms have received limitedattention, with existing methods struggling to scale efficiently as datasetsizes and feature counts increase. These attacks typically requirere-clustering the entire dataset multiple times to generate predictions andassess the attacker's objectives, significantly hindering their scalability.This paper addresses these limitations by proposing Sonic, a novel genetic datapoisoning attack that leverages incremental and scalable clustering algorithms,e.g., FISHDBC, as surrogates to accelerate poisoning attacks againstgraph-based and density-based clustering methods, such as HDBSCAN. Weempirically demonstrate the effectiveness and efficiency of Sonic in poisoningthe target clustering algorithms. We then conduct a comprehensive analysis ofthe factors affecting the scalability and transferability of poisoning attacksagainst clustering algorithms, and we conclude by examining the robustness ofhyperparameters in our attack strategy Sonic.</description><author>Francesco Villani, Dario Lazzaro, Antonio Emanuele Cinà, Matteo Dell'Amico, Battista Biggio, Fabio Roli</author><pubDate>Wed, 14 Aug 2024 13:43:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07558v1</guid></item><item><title>GS-Pose: Generalizable Segmentation-based 6D Object Pose Estimation with 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2403.10683v2</link><description>This paper introduces GS-Pose, a unified framework for localizing andestimating the 6D pose of novel objects. GS-Pose begins with a set of posed RGBimages of a previously unseen object and builds three distinct representationsstored in a database. At inference, GS-Pose operates sequentially by locatingthe object in the input image, estimating its initial 6D pose using a retrievalapproach, and refining the pose with a render-and-compare method. The keyinsight is the application of the appropriate object representation at eachstage of the process. In particular, for the refinement step, we leverage 3DGaussian splatting, a novel differentiable rendering technique that offers highrendering speed and relatively low optimization time. Off-the-shelf toolchainsand commodity hardware, such as mobile phones, can be used to capture newobjects to be added to the database. Extensive evaluations on the LINEMOD andOnePose-LowTexture datasets demonstrate excellent performance, establishing thenew state-of-the-art. Project page: https://dingdingcai.github.io/gs-pose.</description><author>Dingding Cai, Janne Heikkilä, Esa Rahtu</author><pubDate>Wed, 14 Aug 2024 13:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10683v2</guid></item><item><title>PolyCL: Contrastive Learning for Polymer Representation Learning via Explicit and Implicit Augmentations</title><link>http://arxiv.org/abs/2408.07556v1</link><description>Polymers play a crucial role in a wide array of applications due to theirdiverse and tunable properties. Establishing the relationship between polymerrepresentations and their properties is crucial to the computational design andscreening of potential polymers via machine learning. The quality of therepresentation significantly influences the effectiveness of thesecomputational methods. Here, we present a self-supervised contrastive learningparadigm, PolyCL, for learning high-quality polymer representation without theneed for labels. Our model combines explicit and implicit augmentationstrategies for improved learning performance. The results demonstrate that ourmodel achieves either better, or highly competitive, performances on transferlearning tasks as a feature extractor without an overcomplicated trainingstrategy or hyperparameter optimisation. Further enhancing the efficacy of ourmodel, we conducted extensive analyses on various augmentation combinationsused in contrastive learning. This led to identifying the most effectivecombination to maximise PolyCL's performance.</description><author>Jiajun Zhou, Yijie Yang, Austin M. Mroz, Kim E. Jelfs</author><pubDate>Wed, 14 Aug 2024 13:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07556v1</guid></item><item><title>R2Human: Real-Time 3D Human Appearance Rendering from a Single Image</title><link>http://arxiv.org/abs/2312.05826v4</link><description>Rendering 3D human appearance from a single image in real-time is crucial forachieving holographic communication and immersive VR/AR. Existing methodseither rely on multi-camera setups or are constrained to offline operations. Inthis paper, we propose R2Human, the first approach for real-time inference andrendering of photorealistic 3D human appearance from a single image. The coreof our approach is to combine the strengths of implicit texture fields andexplicit neural rendering with our novel representation, namely Z-map. Based onthis, we present an end-to-end network that performs high-fidelity colorreconstruction of visible areas and provides reliable color inference foroccluded regions. To further enhance the 3D perception ability of our network,we leverage the Fourier occupancy field as a prior for generating the texturefield and providing a sampling surface in the rendering stage. We also proposea consistency loss and a spatial fusion strategy to ensure the multi-viewcoherence. Experimental results show that our method outperforms thestate-of-the-art methods on both synthetic data and challenging real-worldimages, in real-time. The project page can be found athttp://cic.tju.edu.cn/faculty/likun/projects/R2Human.</description><author>Yuanwang Yang, Qiao Feng, Yu-Kun Lai, Kun Li</author><pubDate>Wed, 14 Aug 2024 13:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05826v4</guid></item><item><title>LLM Voting: Human Choices and AI Collective Decision Making</title><link>http://arxiv.org/abs/2402.01766v3</link><description>This paper investigates the voting behaviors of Large Language Models (LLMs),specifically GPT-4 and LLaMA-2, their biases, and how they align with humanvoting patterns. Our methodology involved using a dataset from a human votingexperiment to establish a baseline for human preferences and conducting acorresponding experiment with LLM agents. We observed that the choice of votingmethods and the presentation order influenced LLM voting outcomes. We foundthat varying the persona can reduce some of these biases and enhance alignmentwith human choices. While the Chain-of-Thought approach did not improveprediction accuracy, it has potential for AI explainability in the votingprocess. We also identified a trade-off between preference diversity andalignment accuracy in LLMs, influenced by different temperature settings. Ourfindings indicate that LLMs may lead to less diverse collective outcomes andbiased assumptions when used in voting scenarios, emphasizing the need forcautious integration of LLMs into democratic processes.</description><author>Joshua C. Yang, Damian Dailisan, Marcin Korecki, Carina I. Hausladen, Dirk Helbing</author><pubDate>Wed, 14 Aug 2024 13:41:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01766v3</guid></item><item><title>A Probabilistic Approach to Learning the Degree of Equivariance in Steerable CNNs</title><link>http://arxiv.org/abs/2406.03946v2</link><description>Steerable convolutional neural networks (SCNNs) enhance task performance bymodelling geometric symmetries through equivariance constraints on weights.Yet, unknown or varying symmetries can lead to overconstrained weights anddecreased performance. To address this, this paper introduces a probabilisticmethod to learn the degree of equivariance in SCNNs. We parameterise the degreeof equivariance as a likelihood distribution over the transformation groupusing Fourier coefficients, offering the option to model layer-wise and sharedequivariance. These likelihood distributions are regularised to ensure aninterpretable degree of equivariance across the network. Advantages include theapplicability to many types of equivariant networks through the flexibleframework of SCNNs and the ability to learn equivariance with respect to anysubgroup of any compact group without requiring additional layers. Ourexperiments reveal competitive performance on datasets with mixed symmetries,with learnt likelihood distributions that are representative of the underlyingdegree of equivariance.</description><author>Lars Veefkind, Gabriele Cesa</author><pubDate>Wed, 14 Aug 2024 13:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03946v2</guid></item><item><title>A Data-Driven Defense against Edge-case Model Poisoning Attacks on Federated Learning</title><link>http://arxiv.org/abs/2305.02022v2</link><description>Federated Learning systems are increasingly subjected to a multitude of modelpoisoning attacks from clients. Among these, edge-case attacks that target asmall fraction of the input space are nearly impossible to detect usingexisting defenses, leading to a high attack success rate. We propose aneffective defense using an external defense dataset, which provides informationabout the attack target. The defense dataset contains a mix of poisoned andclean examples, with only a few known to be clean. The proposed method,DataDefense, uses this dataset to learn a poisoned data detector model whichmarks each example in the defense dataset as poisoned or clean. It also learnsa client importance model that estimates the probability of a client updatebeing malicious. The global model is then updated as a weighted average of theclient models' updates. The poisoned data detector and the client importancemodel parameters are updated using an alternating minimization strategy overthe Federated Learning rounds. Extensive experiments on standard attackscenarios demonstrate that DataDefense can defend against model poisoningattacks where other state-of-the-art defenses fail. In particular, DataDefenseis able to reduce the attack success rate by at least ~ 40% on standard attacksetups and by more than 80% on some setups. Furthermore, DataDefense requiresvery few defense examples (as few as five) to achieve a near-optimal reductionin attack success rate.</description><author>Kiran Purohit, Soumi Das, Sourangshu Bhattacharya, Santu Rana</author><pubDate>Wed, 14 Aug 2024 13:37:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02022v2</guid></item><item><title>PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation</title><link>http://arxiv.org/abs/2408.07547v1</link><description>Recently, universal waveform generation tasks have been investigatedconditioned on various out-of-distribution scenarios. Although GAN-basedmethods have shown their strength in fast waveform generation, they arevulnerable to train-inference mismatch scenarios such as two-stagetext-to-speech. Meanwhile, diffusion-based models have shown their powerfulgenerative performance in other domains; however, they stay out of thelimelight due to slow inference speed in waveform generation tasks. Above all,there is no generator architecture that can explicitly disentangle the naturalperiodic features of high-resolution waveform signals. In this paper, wepropose PeriodWave, a novel universal waveform generation model. First, weintroduce a period-aware flow matching estimator that can capture the periodicfeatures of the waveform signal when estimating the vector fields.Additionally, we utilize a multi-period estimator that avoids overlaps tocapture different periodic features of waveform signals. Although increasingthe number of periods can improve the performance significantly, this requiresmore computational costs. To reduce this issue, we also propose a singleperiod-conditional universal estimator that can feed-forward parallel byperiod-wise batch inference. Additionally, we utilize discrete wavelettransform to losslessly disentangle the frequency information of waveformsignals for high-frequency modeling, and introduce FreeU to reduce thehigh-frequency noise for waveform generation. The experimental resultsdemonstrated that our model outperforms the previous models both inMel-spectrogram reconstruction and text-to-speech tasks. All source code willbe available at \url{https://github.com/sh-lee-prml/PeriodWave}.</description><author>Sang-Hoon Lee, Ha-Yeong Choi, Seong-Whan Lee</author><pubDate>Wed, 14 Aug 2024 13:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07547v1</guid></item><item><title>$χ$SPN: Characteristic Interventional Sum-Product Networks for Causal Inference in Hybrid Domains</title><link>http://arxiv.org/abs/2408.07545v1</link><description>Causal inference in hybrid domains, characterized by a mixture of discreteand continuous variables, presents a formidable challenge. We take a steptowards this direction and propose Characteristic Interventional Sum-ProductNetwork ($\chi$SPN) that is capable of estimating interventional distributionsin presence of random variables drawn from mixed distributions. $\chi$SPN usescharacteristic functions in the leaves of an interventional SPN (iSPN) therebyproviding a unified view for discrete and continuous random variables throughthe Fourier-Stieltjes transform of the probability measures. A neural networkis used to estimate the parameters of the learned iSPN using the interveneddata. Our experiments on 3 synthetic heterogeneous datasets suggest that$\chi$SPN can effectively capture the interventional distributions for bothdiscrete and continuous variables while being expressive and causally adequate.We also show that $\chi$SPN generalize to multiple interventions while beingtrained only on a single intervention data.</description><author>Harsh Poonia, Moritz Willig, Zhongjie Yu, Matej Zečević, Kristian Kersting, Devendra Singh Dhami</author><pubDate>Wed, 14 Aug 2024 13:31:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07545v1</guid></item><item><title>Planning with OWL-DL Ontologies (Extended Version)</title><link>http://arxiv.org/abs/2408.07544v1</link><description>We introduce ontology-mediated planning, in which planning problems arecombined with an ontology. Our formalism differs from existing ones in that wefocus on a strong separation of the formalisms for describing planning problemsand ontologies, which are only losely coupled by an interface. Moreover, wepresent a black-box algorithm that supports the full expressive power of OWLDL. This goes beyond what existing approaches combining automated planning withontologies can do, which only support limited description logics such asDL-Lite and description logics that are Horn. Our main algorithm relies onrewritings of the ontology-mediated planning specifications into PDDL, so thatexisting planning systems can be used to solve them. The algorithm relies onjustifications, which allows for a generic approach that is independent of theexpressivity of the ontology language. However, dedicated optimizations forcomputing justifications need to be implemented to enable an efficientrewriting procedure. We evaluated our implementation on benchmark sets fromseveral domains. The evaluation shows that our procedure works in practice andthat tailoring the reasoning procedure has significant impact on theperformance.</description><author>Tobias John, Patrick Koopmann</author><pubDate>Wed, 14 Aug 2024 13:27:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07544v1</guid></item><item><title>MathScape: Evaluating MLLMs in multimodal Math Scenarios through a Hierarchical Benchmark</title><link>http://arxiv.org/abs/2408.07543v1</link><description>With the development of Multimodal Large Language Models (MLLMs), theevaluation of multimodal models in the context of mathematical problems hasbecome a valuable research field. Multimodal visual-textual mathematicalreasoning serves as a critical indicator for evaluating the comprehension andcomplex multi-step quantitative reasoning abilities of MLLMs. However, previousmultimodal math benchmarks have not sufficiently integrated visual and textualinformation. To address this gap, we proposed MathScape, a new benchmark thatemphasizes the understanding and application of combined visual and textualinformation. MathScape is designed to evaluate photo-based math problemscenarios, assessing the theoretical understanding and application ability ofMLLMs through a categorical hierarchical approach. We conduct amulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmarkis challenging even for the most sophisticated models. By analyzing theevaluation results, we identify the limitations of MLLMs, offering valuableinsights for enhancing model performance.</description><author>Minxuan Zhou, Hao Liang, Tianpeng Li, Zhiyu Wu, Mingan Lin, Linzhuang Sun, Yaqi Zhou, Yan Zhang, Xiaoqin Huang, Yicong Chen, Yujing Qiao, Weipeng Chen, Bin Cui, Wentao Zhang, Zenan Zhou</author><pubDate>Wed, 14 Aug 2024 13:23:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07543v1</guid></item><item><title>On the Utility of 3D Hand Poses for Action Recognition</title><link>http://arxiv.org/abs/2403.09805v2</link><description>3D hand pose is an underexplored modality for action recognition. Poses arecompact yet informative and can greatly benefit applications with limitedcompute budgets. However, poses alone offer an incomplete understanding ofactions, as they cannot fully capture objects and environments with whichhumans interact. We propose HandFormer, a novel multimodal transformer, toefficiently model hand-object interactions. HandFormer combines 3D hand posesat a high temporal resolution for fine-grained motion modeling with sparselysampled RGB frames for encoding scene semantics. Observing the uniquecharacteristics of hand poses, we temporally factorize hand modeling andrepresent each joint by its short-term trajectories. This factorized poserepresentation combined with sparse RGB samples is remarkably efficient andhighly accurate. Unimodal HandFormer with only hand poses outperforms existingskeleton-based methods at 5x fewer FLOPs. With RGB, we achieve newstate-of-the-art performance on Assembly101 and H2O with significantimprovements in egocentric action recognition.</description><author>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</author><pubDate>Wed, 14 Aug 2024 13:22:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09805v2</guid></item></channel></rss>