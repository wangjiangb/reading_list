<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 27 Oct 2023 14:00:02 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model</title><link>http://arxiv.org/abs/2310.17653v1</link><description>Training deep networks requires various design decisions regarding forinstance their architecture, data augmentation, or optimization. In this work,we find these training variations to result in networks learning unique featuresets from the data. Using public model libraries comprising thousands of modelstrained on canonical datasets like ImageNet, we observe that for arbitrarypairings of pretrained models, one model extracts significant data contextunavailable in the other -- independent of overall performance. Given anyarbitrary pairing of pretrained models and no external rankings (such asseparate test sets, e.g. due to data privacy), we investigate if it is possibleto transfer such "complementary" knowledge from one model to another withoutperformance degradation -- a task made particularly difficult as additionalknowledge can be contained in stronger, equiperformant or weaker models. Yetfacilitating robust transfer in scenarios agnostic to pretrained model pairingswould unlock auxiliary gains and knowledge fusion from any model repositorywithout restrictions on model and problem specifics - including from weaker,lower-performance models. This work therefore provides an initial, in-depthexploration on the viability of such general-purpose knowledge transfer. Acrosslarge-scale experiments, we first reveal the shortcomings of standard knowledgedistillation techniques, and then propose a much more general extension throughdata partitioning for successful transfer between nearly all pretrained models,which we show can also be done unsupervised. Finally, we assess both thescalability and impact of fundamental model properties on successfulmodel-agnostic knowledge transfer.</description><author>Karsten Roth, Lukas Thede, Almut Sophia Koepke, Oriol Vinyals, Olivier HÃ©naff, Zeynep Akata</author><pubDate>Thu, 26 Oct 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17653v1</guid></item><item><title>TabR: Tabular Deep Learning Meets Nearest Neighbors in 2023</title><link>http://arxiv.org/abs/2307.14338v2</link><description>Deep learning (DL) models for tabular data problems (e.g. classification,regression) are currently receiving increasingly more attention fromresearchers. However, despite the recent efforts, the non-DL algorithms basedon gradient-boosted decision trees (GBDT) remain a strong go-to solution forthese problems. One of the research directions aimed at improving the positionof tabular DL involves designing so-called retrieval-augmented models. For atarget object, such models retrieve other objects (e.g. the nearest neighbors)from the available training data and use their features and labels to make abetter prediction. In this work, we present TabR -- essentially, a feed-forward network with acustom k-Nearest-Neighbors-like component in the middle. On a set of publicbenchmarks with datasets up to several million objects, TabR marks a big stepforward for tabular DL: it demonstrates the best average performance amongtabular DL models, becomes the new state-of-the-art on several datasets, andeven outperforms GBDT models on the recently proposed "GBDT-friendly" benchmark(see Figure 1). Among the important findings and technical details poweringTabR, the main ones lie in the attention-like mechanism that is responsible forretrieving the nearest neighbors and extracting valuable signal from them. Inaddition to the much higher performance, TabR is simple and significantly moreefficient compared to prior retrieval-based tabular DL models.</description><author>Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, Artem Babenko</author><pubDate>Thu, 26 Oct 2023 18:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14338v2</guid></item><item><title>High-Dimensional Prediction for Sequential Decision Making</title><link>http://arxiv.org/abs/2310.17651v1</link><description>We study the problem of making predictions of an adversarially chosenhigh-dimensional state that are unbiased subject to an arbitrary collection ofconditioning events, with the goal of tailoring these events to downstreamdecision makers. We give efficient algorithms for solving this problem, as wellas a number of applications that stem from choosing an appropriate set ofconditioning events.</description><author>Georgy Noarov, Ramya Ramalingam, Aaron Roth, Stephan Xie</author><pubDate>Thu, 26 Oct 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17651v1</guid></item><item><title>A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection</title><link>http://arxiv.org/abs/2310.17650v1</link><description>Detection of anomalous events in videos is an important problem inapplications such as surveillance. Video anomaly detection (VAD) iswell-studied in the one-class classification (OCC) and weakly supervised (WS)settings. However, fully unsupervised (US) video anomaly detection methods,which learn a complete system without any annotation or human supervision, havenot been explored in depth. This is because the lack of any ground truthannotations significantly increases the magnitude of the VAD challenge. Toaddress this challenge, we propose a simple-but-effective two-stagepseudo-label generation framework that produces segment-level (normal/anomaly)pseudo-labels, which can be further used to train a segment-level anomalydetector in a supervised manner. The proposed coarse-to-fine pseudo-label(C2FPL) generator employs carefully-designed hierarchical divisive clusteringand statistical hypothesis testing to identify anomalous video segments from aset of completely unlabeled videos. The trained anomaly detector can bedirectly applied on segments of an unseen test video to obtain segment-level,and subsequently, frame-level anomaly predictions. Extensive studies on twolarge-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate thatthe proposed unsupervised approach achieves superior performance compared toall existing OCC and US methods , while yielding comparable performance to thestate-of-the-art WS methods.</description><author>Anas Al-lahham, Nurbek Tastan, Zaigham Zaheer, Karthik Nandakumar</author><pubDate>Thu, 26 Oct 2023 18:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17650v1</guid></item><item><title>6-DoF Stability Field via Diffusion Models</title><link>http://arxiv.org/abs/2310.17649v1</link><description>A core capability for robot manipulation is reasoning over where and how tostably place objects in cluttered environments. Traditionally, robots haverelied on object-specific, hand-crafted heuristics in order to perform suchreasoning, with limited generalizability beyond a small number of objectinstances and object interaction patterns. Recent approaches instead learnnotions of physical interaction, namely motion prediction, but requiresupervision in the form of labeled object information or come at the cost ofhigh sample complexity, and do not directly reason over stability or objectplacement. We present 6-DoFusion, a generative model capable of generating 3Dposes of an object that produces a stable configuration of a given scene.Underlying 6-DoFusion is a diffusion model that incrementally refines arandomly initialized SE(3) pose to generate a sample from a learned,context-dependent distribution over stable poses. We evaluate our model ondifferent object placement and stacking tasks, demonstrating its ability toconstruct stable scenes that involve novel object classes as well as to improvethe accuracy of state-of-the-art 3D pose estimation methods.</description><author>Takuma Yoneda, Tianchong Jiang, Gregory Shakhnarovich, Matthew R. Walter</author><pubDate>Thu, 26 Oct 2023 18:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17649v1</guid></item><item><title>Do Graph Neural Networks Dream of Landau Damping? Insights from Kinetic Simulations of a Plasma Sheet Model</title><link>http://arxiv.org/abs/2310.17646v1</link><description>We explore the possibility of fully replacing a plasma physics kineticsimulator with a graph neural network-based simulator. We focus on this classof surrogate models given the similarity between their message-passing updatemechanism and the traditional physics solver update, and the possibility ofenforcing known physical priors into the graph construction and update. We showthat our model learns the kinetic plasma dynamics of the one-dimensional plasmamodel, a predecessor of contemporary kinetic plasma simulation codes, andrecovers a wide range of well-known kinetic plasma processes, including plasmathermalization, electrostatic fluctuations about thermal equilibrium, and thedrag on a fast sheet and Landau damping. We compare the performance against theoriginal plasma model in terms of run-time, conservation laws, and temporalevolution of key physical quantities. The limitations of the model arepresented and possible directions for higher-dimensional surrogate models forkinetic plasmas are discussed.</description><author>Diogo D Carvalho, Diogo R Ferreira, Luis O Silva</author><pubDate>Thu, 26 Oct 2023 18:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17646v1</guid></item><item><title>Defending Against Transfer Attacks From Public Models</title><link>http://arxiv.org/abs/2310.17645v1</link><description>Adversarial attacks have been a looming and unaddressed threat in theindustry. However, through a decade-long history of the robustness evaluationliterature, we have learned that mounting a strong or optimal attack ischallenging. It requires both machine learning and domain expertise. In otherwords, the white-box threat model, religiously assumed by a large majority ofthe past literature, is unrealistic. In this paper, we propose a new practicalthreat model where the adversary relies on transfer attacks through publiclyavailable surrogate models. We argue that this setting will become the mostprevalent for security-sensitive applications in the future. We evaluate thetransfer attacks in this setting and propose a specialized defense method basedon a game-theoretic perspective. The defenses are evaluated under 24 publicmodels and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, andImageNet). Under this threat model, our defense, PubDef, outperforms thestate-of-the-art white-box adversarial training by a large margin with almostno loss in the normal accuracy. For instance, on ImageNet, our defense achieves62% accuracy under the strongest transfer attack vs only 36% of the bestadversarially trained model. Its accuracy when not under attack is only 2%lower than that of an undefended model (78% vs 80%). We release our code athttps://github.com/wagner-group/pubdef.</description><author>Chawin Sitawarin, Jaewon Chang, David Huang, Wesson Altoyan, David Wagner</author><pubDate>Thu, 26 Oct 2023 18:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17645v1</guid></item><item><title>torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP</title><link>http://arxiv.org/abs/2310.17644v1</link><description>Reproducibility in scientific work has been becoming increasingly importantin research communities such as machine learning, natural language processing,and computer vision communities due to the rapid development of the researchdomains supported by recent advances in deep learning. In this work, we presenta significantly upgraded version of torchdistill, a modular-driven coding-freedeep learning framework significantly upgraded from the initial release, whichsupports only image classification and object detection tasks for reproducibleknowledge distillation experiments. To demonstrate that the upgraded frameworkcan support more tasks with third-party libraries, we reproduce the GLUEbenchmark results of BERT models using a script based on the upgradedtorchdistill, harmonizing with various Hugging Face libraries. All the 27fine-tuned BERT models and configurations to reproduce the results arepublished at Hugging Face, and the model weights have already been widely usedin research communities. We also reimplement popular small-sized models and newknowledge distillation methods and perform additional experiments for computervision tasks.</description><author>Yoshitomo Matsubara</author><pubDate>Thu, 26 Oct 2023 18:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17644v1</guid></item><item><title>Where you go is who you are -- A study on machine learning based semantic privacy attacks</title><link>http://arxiv.org/abs/2310.17643v1</link><description>Concerns about data privacy are omnipresent, given the increasing usage ofdigital applications and their underlying business model that includes sellinguser data. Location data is particularly sensitive since they allow us to inferactivity patterns and interests of users, e.g., by categorizing visitedlocations based on nearby points of interest (POI). On top of that, machinelearning methods provide new powerful tools to interpret big data. In light ofthese considerations, we raise the following question: What is the actual riskthat realistic, machine learning based privacy attacks can obtain meaningfulsemantic information from raw location data, subject to inaccuracies in thedata? In response, we present a systematic analysis of two attack scenarios,namely location categorization and user profiling. Experiments on theFoursquare dataset and tracking data demonstrate the potential for abuse ofhigh-quality spatial information, leading to a significant privacy loss evenwith location inaccuracy of up to 200m. With location obfuscation of more than1 km, spatial information hardly adds any value, but a high privacy risk solelyfrom temporal information remains. The availability of public context data suchas POIs plays a key role in inference based on spatial information. Ourfindings point out the risks of ever-growing databases of tracking data andspatial context data, which policymakers should consider for privacyregulations, and which could guide individuals in their personal locationprotection measures.</description><author>Nina Wiedemann, Ourania Kounadi, Martin Raubal, Krzysztof Janowicz</author><pubDate>Thu, 26 Oct 2023 18:56:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17643v1</guid></item><item><title>Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models</title><link>http://arxiv.org/abs/2310.17642v1</link><description>As autonomous driving technology matures, end-to-end methodologies haveemerged as a leading strategy, promising seamless integration from perceptionto control via deep learning. However, existing systems grapple with challengessuch as unexpected open set environments and the complexity of black-boxmodels. At the same time, the evolution of deep learning introduces larger,multimodal foundational models, offering multi-modal visual and textualunderstanding. In this paper, we harness these multimodal foundation models toenhance the robustness and adaptability of autonomous driving systems, enablingout-of-distribution, end-to-end, multimodal, and more explainable autonomy.Specifically, we present an approach to apply end-to-end open-set (anyenvironment/scene) autonomous driving that is capable of providing drivingdecisions from representations queryable by image and text. To do so, weintroduce a method to extract nuanced spatial (pixel/patch-aligned) featuresfrom transformers to enable the encapsulation of both spatial and semanticfeatures. Our approach (i) demonstrates unparalleled results in diverse testswhile achieving significantly greater robustness in out-of-distributionsituations, and (ii) allows the incorporation of latent space simulation (viatext) for improved training (data augmentation via text) and policy debugging.We encourage the reader to check our explainer video athttps://www.youtube.com/watch?v=4n-DJf8vXxo&amp;feature=youtu.be and to view thecode and demos on our project webpage at https://drive-anywhere.github.io/.</description><author>Tsun-Hsuan Wang, Alaa Maalouf, Wei Xiao, Yutong Ban, Alexander Amini, Guy Rosman, Sertac Karaman, Daniela Rus</author><pubDate>Thu, 26 Oct 2023 18:56:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17642v1</guid></item><item><title>In-Context Learning Dynamics with Random Binary Sequences</title><link>http://arxiv.org/abs/2310.17639v1</link><description>Large language models (LLMs) trained on huge corpora of text datasetsdemonstrate complex, emergent capabilities, achieving state-of-the-artperformance on tasks they were not explicitly trained for. The precise natureof LLM capabilities is often mysterious, and different prompts can elicitdifferent capabilities through in-context learning. We propose a CognitiveInterpretability framework that enables us to analyze in-context learningdynamics to understand latent concepts in LLMs underlying behavioral patterns.This provides a more nuanced understanding than success-or-failure evaluationbenchmarks, but does not require observing internal activations as amechanistic interpretation of circuits would. Inspired by the cognitive scienceof human randomness perception, we use random binary sequences as context andstudy dynamics of in-context learning by manipulating properties of contextdata, such as sequence length. In the latest GPT-3.5+ models, we find emergentabilities to generate pseudo-random numbers and learn basic formal languages,with striking in-context learning dynamics where model outputs transitionsharply from pseudo-random behaviors to deterministic repetition.</description><author>Eric J. Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Tomer D. Ullman</author><pubDate>Thu, 26 Oct 2023 18:54:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17639v1</guid></item><item><title>Generative Fractional Diffusion Models</title><link>http://arxiv.org/abs/2310.17638v1</link><description>We generalize the continuous time framework for score-based generative modelsfrom an underlying Brownian motion (BM) to an approximation of fractionalBrownian motion (FBM). We derive a continuous reparameterization trick and thereverse time model by representing FBM as a stochastic integral over a familyof Ornstein-Uhlenbeck processes to define generative fractional diffusionmodels (GFDM) with driving noise converging to a non-Markovian process ofinfinite quadratic variation. The Hurst index $H\in(0,1)$ of FBM enablescontrol of the roughness of the distribution transforming path. To the best ofour knowledge, this is the first attempt to build a generative model upon astochastic process with infinite quadratic variation.</description><author>Gabriel Nobis, Marco Aversa, Maximilian Springenberg, Michael Detzel, Stefano Ermon, Shinichi Nakajima, Roderick Murray-Smith, Sebastian Lapuschkin, Christoph Knochenhauer, Luis Oala, Wojciech Samek</author><pubDate>Thu, 26 Oct 2023 18:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17638v1</guid></item><item><title>Grow Your Limits: Continuous Improvement with Real-World RL for Robotic Locomotion</title><link>http://arxiv.org/abs/2310.17634v1</link><description>Deep reinforcement learning (RL) can enable robots to autonomously acquirecomplex behaviors, such as legged locomotion. However, RL in the real world iscomplicated by constraints on efficiency, safety, and overall trainingstability, which limits its practical applicability. We present APRL, a policyregularization framework that modulates the robot's exploration over the courseof training, striking a balance between flexible improvement potential andfocused, efficient exploration. APRL enables a quadrupedal robot to efficientlylearn to walk entirely in the real world within minutes and continue to improvewith more training where prior work saturates in performance. We demonstratethat continued training with APRL results in a policy that is substantiallymore capable of navigating challenging situations and is able to adapt tochanges in dynamics with continued training.</description><author>Laura Smith, Yunhao Cao, Sergey Levine</author><pubDate>Thu, 26 Oct 2023 18:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17634v1</guid></item><item><title>Online Estimation and Community Detection of Network Point Processes for Event Streams</title><link>http://arxiv.org/abs/2009.01742v3</link><description>A common goal in network modeling is to uncover the latent communitystructure present among nodes. For many real-world networks, the trueconnections consist of events arriving as streams, which are then aggregated toform edges, ignoring the dynamic temporal component. A natural way to takeaccount of these temporal dynamics of interactions is to use point processes asthe foundation of network models for community detection. Computationalcomplexity hampers the scalability of such approaches to large sparse networks.To circumvent this challenge, we propose a fast online variational inferencealgorithm for estimating the latent structure underlying dynamic event arrivalson a network, using continuous-time point process latent network models. Wedescribe this procedure for networks models capturing community structure. Thisstructure can be learned as new events are observed on the network, updatingthe inferred community assignments. We investigate the theoretical propertiesof such an inference scheme, and provide regret bounds on the loss function ofthis procedure. The proposed inference procedure is then thoroughly compared,using both simulation studies and real data, to non-online variants. Wedemonstrate that online inference can obtain comparable performance, in termsof community recovery, to non-online variants, while realising computationalgains. Our proposed inference framework can also be readily modified toincorporate other popular network structures.</description><author>Guanhua Fang, Owen G. Ward, Tian Zheng</author><pubDate>Thu, 26 Oct 2023 18:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.01742v3</guid></item><item><title>DeepShaRM: Multi-View Shape and Reflectance Map Recovery Under Unknown Lighting</title><link>http://arxiv.org/abs/2310.17632v1</link><description>Geometry reconstruction of textureless, non-Lambertian objects under unknownnatural illumination (i.e., in the wild) remains challenging as correspondencescannot be established and the reflectance cannot be expressed in simpleanalytical forms. We derive a novel multi-view method, DeepShaRM, that achievesstate-of-the-art accuracy on this challenging task. Unlike past methods thatformulate this as inverse-rendering, i.e., estimation of reflectance,illumination, and geometry from images, our key idea is to realize thatreflectance and illumination need not be disentangled and instead estimated asa compound reflectance map. We introduce a novel deep reflectance mapestimation network that recovers the camera-view reflectance maps from thesurface normals of the current geometry estimate and the input multi-viewimages. The network also explicitly estimates per-pixel confidence scores tohandle global light transport effects. A deep shape-from-shading network thenupdates the geometry estimate expressed with a signed distance function usingthe recovered reflectance maps. By alternating between these two, and, mostimportant, by bypassing the ill-posed problem of reflectance and illuminationdecomposition, the method accurately recovers object geometry in thesechallenging settings. Extensive experiments on both synthetic and real-worlddata clearly demonstrate its state-of-the-art accuracy.</description><author>Kohei Yamashita, Shohei Nobuhara, Ko Nishino</author><pubDate>Thu, 26 Oct 2023 18:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17632v1</guid></item><item><title>JudgeLM: Fine-tuned Large Language Models are Scalable Judges</title><link>http://arxiv.org/abs/2310.17631v1</link><description>Evaluating Large Language Models (LLMs) in open-ended scenarios ischallenging because existing benchmarks and metrics can not measure themcomprehensively. To address this problem, we propose to fine-tune LLMs asscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively inopen-ended benchmarks. We first propose a comprehensive, large-scale,high-quality dataset containing task seeds, LLMs-generated answers, andGPT-4-generated judgments for fine-tuning high-performance judges, as well as anew benchmark for evaluating the judges. We train JudgeLM at different scalesfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of itscapabilities and behaviors. We then analyze the key biases in fine-tuning LLMas a judge and consider them as position bias, knowledge bias, and format bias.To address these issues, JudgeLM introduces a bag of techniques including swapaugmentation, reference support, and reference drop, which clearly enhance thejudge's performance. JudgeLM obtains the state-of-the-art judge performance onboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLMis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving anagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLMalso demonstrates extended capabilities in being judges of the single answer,multimodal models, multiple answers, and multi-turn chat.</description><author>Lianghui Zhu, Xinggang Wang, Xinlong Wang</author><pubDate>Thu, 26 Oct 2023 18:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17631v1</guid></item><item><title>InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators</title><link>http://arxiv.org/abs/2310.17630v1</link><description>Instruction-based language modeling has received significant attention inpretrained language models. However, the efficiency of instruction engineeringremains low and hinders the development of instruction studies. Recent studieshave focused on automating instruction generation, but they primarily aim toimprove performance without considering other crucial objectives that impactinstruction quality, such as instruction length and perplexity. Therefore, wepropose a novel approach (i.e., InstOptima) that treats instruction generationas an evolutionary multi-objective optimization problem. In contrast to textedition-based methods, our approach utilizes a large language model (LLM) tosimulate instruction operators, including mutation and crossover. Furthermore,we introduce an objective-guided mechanism for these operators, allowing theLLM to comprehend the objectives and enhance the quality of the generatedinstructions. Experimental results demonstrate improved fine-tuning performanceand the generation of a diverse set of high-quality instructions.</description><author>Heng Yang, Ke Li</author><pubDate>Thu, 26 Oct 2023 18:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17630v1</guid></item><item><title>Approximate Leave-one-out Cross Validation for Regression with $\ell_1$ Regularizers (extended version)</title><link>http://arxiv.org/abs/2310.17629v1</link><description>The out-of-sample error (OO) is the main quantity of interest in riskestimation and model selection. Leave-one-out cross validation (LO) offers a(nearly) distribution-free yet computationally demanding approach to estimateOO. Recent theoretical work showed that approximate leave-one-out crossvalidation (ALO) is a computationally efficient and statistically reliableestimate of LO (and OO) for generalized linear models with differentiableregularizers. For problems involving non-differentiable regularizers, despitesignificant empirical evidence, the theoretical understanding of ALO's errorremains unknown. In this paper, we present a novel theory for a wide class ofproblems in the generalized linear model family with non-differentiableregularizers. We bound the error |ALO - LO| in terms of intuitive metrics suchas the size of leave-i-out perturbations in active sets, sample size n, numberof features p and regularization parameters. As a consequence, for the$\ell_1$-regularized problems, we show that |ALO - LO| goes to zero as p goesto infinity while n/p and SNR are fixed and bounded.</description><author>Arnab Auddy, Haolin Zou, Kamiar Rahnama Rad, Arian Maleki</author><pubDate>Thu, 26 Oct 2023 18:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17629v1</guid></item><item><title>Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions</title><link>http://arxiv.org/abs/2310.02987v2</link><description>Machine learning approaches relying on such criteria as adversarialrobustness or multi-agent settings have raised the need for solvinggame-theoretic equilibrium problems. Of particular relevance to theseapplications are methods targeting finite-sum structure, which genericallyarises in empirical variants of learning problems in these contexts. Further,methods with computable approximation errors are highly desirable, as theyprovide verifiable exit criteria. Motivated by these applications, we studyfinite-sum monotone inclusion problems, which model broad classes ofequilibrium problems. Our main contributions are variants of the classicalHalpern iteration that employ variance reduction to obtain improved complexityguarantees in which $n$ component operators in the finite sum are ``onaverage'' either cocoercive or Lipschitz continuous and monotone, withparameter $L$. The resulting oracle complexity of our methods, which provideguarantees for the last iterate and for a (computable) operator norm residual,is $\widetilde{\mathcal{O}}( n + \sqrt{n}L\varepsilon^{-1})$, which improvesupon existing methods by a factor up to $\sqrt{n}$. This constitutes the firstvariance reduction-type result for general finite-sum monotone inclusions andfor more specific problems such as convex-concave optimization when operatornorm residual is the optimality measure. We further argue that, up topoly-logarithmic factors, this complexity is unimprovable in the monotoneLipschitz setting; i.e., the provided result is near-optimal.</description><author>Xufeng Cai, Ahmet Alacaoglu, Jelena Diakonikolas</author><pubDate>Thu, 26 Oct 2023 18:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02987v2</guid></item><item><title>A Survey on Transferability of Adversarial Examples across Deep Neural Networks</title><link>http://arxiv.org/abs/2310.17626v1</link><description>The emergence of Deep Neural Networks (DNNs) has revolutionized variousdomains, enabling the resolution of complex tasks spanning image recognition,natural language processing, and scientific problem-solving. However, thisprogress has also exposed a concerning vulnerability: adversarial examples.These crafted inputs, imperceptible to humans, can manipulate machine learningmodels into making erroneous predictions, raising concerns for safety-criticalapplications. An intriguing property of this phenomenon is the transferabilityof adversarial examples, where perturbations crafted for one model can deceiveanother, often with a different architecture. This intriguing property enables"black-box" attacks, circumventing the need for detailed knowledge of thetarget model. This survey explores the landscape of the adversarialtransferability of adversarial examples. We categorize existing methodologiesto enhance adversarial transferability and discuss the fundamental principlesguiding each approach. While the predominant body of research primarilyconcentrates on image classification, we also extend our discussion toencompass other vision tasks and beyond. Challenges and future prospects arediscussed, highlighting the importance of fortifying DNNs against adversarialvulnerabilities in an evolving landscape.</description><author>Jindong Gu, Xiaojun Jia, Pau de Jorge, Wenqain Yu, Xinwei Liu, Avery Ma, Yuan Xun, Anjun Hu, Ashkan Khakzar, Zhijiang Li, Xiaochun Cao, Philip Torr</author><pubDate>Thu, 26 Oct 2023 18:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17626v1</guid></item><item><title>Proving Test Set Contamination in Black Box Language Models</title><link>http://arxiv.org/abs/2310.17623v1</link><description>Large language models are trained on vast amounts of internet data, promptingconcerns and speculation that they have memorized public benchmarks. Going fromspeculation to proof of contamination is challenging, as the pretraining dataused by proprietary models are often not publicly accessible. We show that itis possible to provide provable guarantees of test set contamination inlanguage models without access to pretraining data or model weights. Ourapproach leverages the fact that when there is no data contamination, allorderings of an exchangeable benchmark should be equally likely. In contrast,the tendency for language models to memorize example order means that acontaminated language model will find certain canonical orderings to be muchmore likely than others. Our test flags potential contamination whenever thelikelihood of a canonically ordered benchmark dataset is significantly higherthan the likelihood after shuffling the examples. We demonstrate that ourprocedure is sensitive enough to reliably prove test set contamination inchallenging situations, including models as small as 1.4 billion parameters, onsmall test sets of only 1000 examples, and datasets that appear only a fewtimes in the pretraining corpus. Using our test, we audit five popular publiclyaccessible language models for test set contamination and find little evidencefor pervasive contamination.</description><author>Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B. Hashimoto</author><pubDate>Thu, 26 Oct 2023 18:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17623v1</guid></item><item><title>Combating Representation Learning Disparity with Geometric Harmonization</title><link>http://arxiv.org/abs/2310.17622v1</link><description>Self-supervised learning (SSL) as an effective paradigm of representationlearning has achieved tremendous success on various curated datasets in diversescenarios. Nevertheless, when facing the long-tailed distribution in real-worldapplications, it is still hard for existing methods to capture transferable androbust representation. Conventional SSL methods, pursuing sample-leveluniformity, easily leads to representation learning disparity where headclasses dominate the feature regime but tail classes passively collapse. Toaddress this problem, we propose a novel Geometric Harmonization (GH) method toencourage category-level uniformity in representation learning, which is morebenign to the minority and almost does not hurt the majority under long-taileddistribution. Specially, GH measures the population statistics of the embeddingspace on top of self-supervised learning, and then infer an fine-grainedinstance-wise calibration to constrain the space expansion of head classes andavoid the passive collapse of tail classes. Our proposal does not alter thesetting of SSL and can be easily integrated into existing methods in a low-costmanner. Extensive results on a range of benchmark datasets show theeffectiveness of GH with high tolerance to the distribution skewness. Our codeis available at https://github.com/MediaBrain-SJTU/Geometric-Harmonization.</description><author>Zhihan Zhou, Jiangchao Yao, Feng Hong, Ya Zhang, Bo Han, Yanfeng Wang</author><pubDate>Thu, 26 Oct 2023 18:41:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17622v1</guid></item><item><title>Uncovering Meanings of Embeddings via Partial Orthogonality</title><link>http://arxiv.org/abs/2310.17611v1</link><description>Machine learning tools often rely on embedding text as vectors of realnumbers. In this paper, we study how the semantic structure of language isencoded in the algebraic structure of such embeddings. Specifically, we look ata notion of ``semantic independence'' capturing the idea that, e.g.,``eggplant'' and ``tomato'' are independent given ``vegetable''. Although suchexamples are intuitive, it is difficult to formalize such a notion of semanticindependence. The key observation here is that any sensible formalizationshould obey a set of so-called independence axioms, and thus any algebraicencoding of this structure should also obey these axioms. This leads usnaturally to use partial orthogonality as the relevant algebraic structure. Wedevelop theory and methods that allow us to demonstrate that partialorthogonality does indeed capture semantic independence. Complementary to this,we also introduce the concept of independence preserving embeddings whereembeddings preserve the conditional independence structures of a distribution,and we prove the existence of such embeddings and approximations to them.</description><author>Yibo Jiang, Bryon Aragam, Victor Veitch</author><pubDate>Thu, 26 Oct 2023 18:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17611v1</guid></item><item><title>A qualitative difference between gradient flows of convex functions in finite- and infinite-dimensional Hilbert spaces</title><link>http://arxiv.org/abs/2310.17610v1</link><description>We consider gradient flow/gradient descent and heavy ball/acceleratedgradient descent optimization for convex objective functions. In the gradientflow case, we prove the following: 1. If $f$ does not have a minimizer, the convergence $f(x_t)\to \inf f$ canbe arbitrarily slow. 2. If $f$ does have a minimizer, the excess energy $f(x_t) - \inf f$ isintegrable/summable in time. In particular, $f(x_t) - \inf f = o(1/t)$ as$t\to\infty$. 3. In Hilbert spaces, this is optimal: $f(x_t) - \inf f$ can decay to $0$ asslowly as any given function which is monotone decreasing and integrable at$\infty$, even for a fixed quadratic objective. 4. In finite dimension (or more generally, for all gradient flow curves offinite length), this is not optimal: We prove that there are convex monotonedecreasing integrable functions $g(t)$ which decrease to zero slower than$f(x_t)-\inf f$ for the gradient flow of any convex function on $\mathbb R^d$.For instance, we show that any gradient flow $x_t$ of a convex function $f$ infinite dimension satisfies $\liminf_{t\to\infty} \big(t\cdot \log^2(t)\cdot\big\{f(x_t) -\inf f\big\}\big)=0$. This improves on the commonly reported $O(1/t)$ rate and provides a sharpcharacterization of the energy decay law. We also note that it is impossible toestablish a rate $O(1/(t\phi(t))$ for any function $\phi$ which satisfies$\lim_{t\to\infty}\phi(t) = \infty$, even asymptotically. Similar results are obtained in related settings for (1) discrete timegradient descent, (2) stochastic gradient descent with multiplicative noise and(3) the heavy ball ODE. In the case of stochastic gradient descent, thesummability of $\mathbb E[f(x_n) - \inf f]$ is used to prove that $f(x_n)\to\inf f$ almost surely - an improvement on the convergence almost surely up to asubsequence which follows from the $O(1/n)$ decay estimate.</description><author>Jonathan W. Siegel, Stephan Wojtowytsch</author><pubDate>Thu, 26 Oct 2023 18:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17610v1</guid></item><item><title>LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset</title><link>http://arxiv.org/abs/2310.17609v1</link><description>As an important component of intelligent legal systems, legal case retrievalplays a critical role in ensuring judicial justice and fairness. However, thedevelopment of legal case retrieval technologies in the Chinese legal system isrestricted by three problems in existing datasets: limited data size, narrowdefinitions of legal relevance, and naive candidate pooling strategies used indata sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scaleLegal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192candidates extracted from 4.3 million criminal case documents. To the best ofour knowledge, LeCaRDv2 is one of the largest Chinese legal case retrievaldatasets, providing extensive coverage of criminal charges. Additionally, weenrich the existing relevance criteria by considering three key aspects:characterization, penalty, procedure. This comprehensive criteria enriches thedataset and may provides a more holistic perspective. Furthermore, we propose atwo-level candidate set pooling strategy that effectively identify potentialcandidates for each query case. It's important to note that all cases in thedataset have been annotated by multiple legal experts specializing in criminallaw. Their expertise ensures the accuracy and reliability of the annotations.We evaluate several state-of-the-art retrieval models at LeCaRDv2,demonstrating that there is still significant room for improvement in legalcase retrieval. The details of LeCaRDv2 can be found at the anonymous websitehttps://github.com/anonymous1113243/LeCaRDv2.</description><author>Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, Yiqun Liu</author><pubDate>Thu, 26 Oct 2023 18:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17609v1</guid></item><item><title>Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in Ghana</title><link>http://arxiv.org/abs/2310.17606v1</link><description>This paper reports on a set of three recent experiments utilizing large-scalespeech models to evaluate the oral reading fluency (ORF) of students in Ghana.While ORF is a well-established measure of foundational literacy, assessing ittypically requires one-on-one sessions between a student and a trainedevaluator, a process that is time-consuming and costly. Automating theevaluation of ORF could support better literacy instruction, particularly ineducation contexts where formative assessment is uncommon due to large classsizes and limited resources. To our knowledge, this research is among the firstto examine the use of the most recent versions of large-scale speech models(Whisper V2 wav2vec2.0) for ORF assessment in the Global South. We find that Whisper V2 produces transcriptions of Ghanaian students readingaloud with a Word Error Rate of 13.5. This is close to the model's average WERon adult speech (12.8) and would have been considered state-of-the-art forchildren's speech transcription only a few years ago. We also find that whenthese transcriptions are used to produce fully automated ORF scores, theyclosely align with scores generated by expert human graders, with a correlationcoefficient of 0.96. Importantly, these results were achieved on arepresentative dataset (i.e., students with regional accents, recordings takenin actual classrooms), using a free and publicly available speech model out ofthe box (i.e., no fine-tuning). This suggests that using large-scale speechmodels to assess ORF may be feasible to implement and scale in lower-resource,linguistically diverse educational contexts.</description><author>Owen Henkel, Hannah Horne-Robinson, Libby Hills, Bill Roberts, Joshua McGrane</author><pubDate>Thu, 26 Oct 2023 18:30:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17606v1</guid></item><item><title>CEIL: Generalized Contextual Imitation Learning</title><link>http://arxiv.org/abs/2306.14534v2</link><description>In this paper, we present \textbf{C}ont\textbf{E}xtual \textbf{I}mitation\textbf{L}earning~(CEIL), a general and broadly applicable algorithm forimitation learning (IL). Inspired by the formulation of hindsight informationmatching, we derive CEIL by explicitly learning a hindsight embedding functiontogether with a contextual policy using the hindsight embeddings. To achievethe expert matching objective for IL, we advocate for optimizing a contextualvariable such that it biases the contextual policy towards mimicking expertbehaviors. Beyond the typical learning from demonstrations (LfD) setting, CEILis a generalist that can be effectively applied to multiple settings including:1)~learning from observations (LfO), 2)~offline IL, 3)~cross-domain IL(mismatched experts), and 4) one-shot IL settings. Empirically, we evaluateCEIL on the popular MuJoCo tasks (online) and the D4RL dataset (offline).Compared to prior state-of-the-art baselines, we show that CEIL is moresample-efficient in most online IL tasks and achieves better or competitiveperformances in offline tasks.</description><author>Jinxin Liu, Li He, Yachen Kang, Zifeng Zhuang, Donglin Wang, Huazhe Xu</author><pubDate>Thu, 26 Oct 2023 18:27:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14534v2</guid></item><item><title>Gaussian Membership Inference Privacy</title><link>http://arxiv.org/abs/2306.07273v2</link><description>We propose a novel and practical privacy notion called $f$-MembershipInference Privacy ($f$-MIP), which explicitly considers the capabilities ofrealistic adversaries under the membership inference attack threat model.Consequently, $f$-MIP offers interpretable privacy guarantees and improvedutility (e.g., better classification accuracy). In particular, we derive aparametric family of $f$-MIP guarantees that we refer to as $\mu$-GaussianMembership Inference Privacy ($\mu$-GMIP) by theoretically analyzing likelihoodratio-based membership inference attacks on stochastic gradient descent (SGD).Our analysis highlights that models trained with standard SGD already offer anelementary level of MIP. Additionally, we show how $f$-MIP can be amplified byadding noise to gradient updates. Our analysis further yields an analyticalmembership inference attack that offers two distinct advantages over previousapproaches. First, unlike existing state-of-the-art attacks that requiretraining hundreds of shadow models, our attack does not require any shadowmodel. Second, our analytical attack enables straightforward auditing of ourprivacy notion $f$-MIP. Finally, we quantify how various hyperparameters (e.g.,batch size, number of model parameters) and specific data characteristicsdetermine an attacker's ability to accurately infer a point's membership in thetraining set. We demonstrate the effectiveness of our method on models trainedon vision and tabular datasets.</description><author>Tobias Leemann, Martin Pawelczyk, Gjergji Kasneci</author><pubDate>Thu, 26 Oct 2023 18:24:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07273v2</guid></item><item><title>Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms</title><link>http://arxiv.org/abs/2302.13534v2</link><description>We study the problem of designing adaptive multi-armed bandit algorithms thatperform optimally in both the stochastic setting and the adversarial settingsimultaneously (often known as a best-of-both-world guarantee). A line ofrecent works shows that when configured and analyzed properly, theFollow-the-Regularized-Leader (FTRL) algorithm, originally designed for theadversarial setting, can in fact optimally adapt to the stochastic setting aswell. Such results, however, critically rely on an assumption that there existsone unique optimal arm. Recently, Ito (2021) took the first step to remove suchan undesirable uniqueness assumption for one particular FTRL algorithm with the$\frac{1}{2}$-Tsallis entropy regularizer. In this work, we significantlyimprove and generalize this result, showing that uniqueness is unnecessary forFTRL with a broad family of regularizers and a new learning rate schedule. Forsome regularizers, our regret bounds also improve upon prior results even whenuniqueness holds. We further provide an application of our results to thedecoupled exploration and exploitation problem, demonstrating that ourtechniques are broadly applicable.</description><author>Tiancheng Jin, Junyan Liu, Haipeng Luo</author><pubDate>Thu, 26 Oct 2023 18:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.13534v2</guid></item><item><title>MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations</title><link>http://arxiv.org/abs/2310.17596v1</link><description>Imitation learning from a large set of human demonstrations has proved to bean effective paradigm for building capable robot agents. However, thedemonstrations can be extremely costly and time-consuming to collect. Weintroduce MimicGen, a system for automatically synthesizing large-scale, richdatasets from only a small number of human demonstrations by adapting them tonew contexts. We use MimicGen to generate over 50K demonstrations across 18tasks with diverse scene configurations, object instances, and robot arms fromjust ~200 human demonstrations. We show that robot agents can be effectivelytrained on this generated dataset by imitation learning to achieve strongperformance in long-horizon and high-precision tasks, such as multi-partassembly and coffee preparation, across broad initial state distributions. Wefurther demonstrate that the effectiveness and utility of MimicGen data comparefavorably to collecting additional human demonstrations, making it a powerfuland economical approach towards scaling up robot learning. Datasets, simulationenvironments, videos, and more at https://mimicgen.github.io .</description><author>Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, Dieter Fox</author><pubDate>Thu, 26 Oct 2023 18:17:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17596v1</guid></item><item><title>Multi-scale Diffusion Denoised Smoothing</title><link>http://arxiv.org/abs/2310.16779v2</link><description>Along with recent diffusion models, randomized smoothing has become one of afew tangible approaches that offers adversarial robustness to models at scale,e.g., those of large pre-trained models. Specifically, one can performrandomized smoothing on any classifier via a simple "denoise-and-classify"pipeline, so-called denoised smoothing, given that an accurate denoiser isavailable - such as diffusion model. In this paper, we present scalable methodsto address the current trade-off between certified robustness and accuracy indenoised smoothing. Our key idea is to "selectively" apply smoothing amongmultiple noise scales, coined multi-scale smoothing, which can be efficientlyimplemented with a single diffusion model. This approach also suggests a newobjective to compare the collective robustness of multi-scale smoothedclassifiers, and questions which representation of diffusion model wouldmaximize the objective. To address this, we propose to further fine-tunediffusion model (a) to perform consistent denoising whenever the original imageis recoverable, but (b) to generate rather diverse outputs otherwise. Ourexperiments show that the proposed multi-scale smoothing scheme combined withdiffusion fine-tuning enables strong certified robustness available with highnoise level while maintaining its accuracy closer to non-smoothed classifiers.</description><author>Jongheon Jeong, Jinwoo Shin</author><pubDate>Thu, 26 Oct 2023 18:15:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16779v2</guid></item><item><title>SPA: A Graph Spectral Alignment Perspective for Domain Adaptation</title><link>http://arxiv.org/abs/2310.17594v1</link><description>Unsupervised domain adaptation (UDA) is a pivotal form in machine learning toextend the in-domain model to the distinctive target domains where the datadistributions differ. Most prior works focus on capturing the inter-domaintransferability but largely overlook rich intra-domain structures, whichempirically results in even worse discriminability. In this work, we introducea novel graph SPectral Alignment (SPA) framework to tackle the tradeoff. Thecore of our method is briefly condensed as follows: (i)-by casting the DAproblem to graph primitives, SPA composes a coarse graph alignment mechanismwith a novel spectral regularizer towards aligning the domain graphs ineigenspaces; (ii)-we further develop a fine-grained message propagation module-- upon a novel neighbor-aware self-training mechanism -- in order for enhanceddiscriminability in the target domain. On standardized benchmarks, theextensive experiments of SPA demonstrate that its performance has surpassed theexisting cutting-edge DA methods. Coupled with dense model analysis, weconclude that our approach indeed possesses superior efficacy, robustness,discriminability, and transferability. Code and data are available at:https://github.com/CrownX/SPA.</description><author>Zhiqing Xiao, Haobo Wang, Ying Jin, Lei Feng, Gang Chen, Fei Huang, Junbo Zhao</author><pubDate>Thu, 26 Oct 2023 18:13:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17594v1</guid></item><item><title>Lil-Bevo: Explorations of Strategies for Training Language Models in More Humanlike Ways</title><link>http://arxiv.org/abs/2310.17591v1</link><description>We present Lil-Bevo, our submission to the BabyLM Challenge. We pretrainedour masked language models with three ingredients: an initial pretraining withmusic data, training on shorter sequences before training on longer ones, andmasking specific tokens to target some of the BLiMP subtasks. Overall, ourbaseline models performed above chance, but far below the performance levels oflarger LLMs trained on more data. We found that training on short sequencesperformed better than training on longer sequences.Pretraining on music mayhelp performance marginally, but, if so, the effect seems small. Our targetedMasked Language Modeling augmentation did not seem to improve model performancein general, but did seem to help on some of the specific BLiMP tasks that wewere targeting (e.g., Negative Polarity Items). Training performant LLMs onsmall amounts of data is a difficult but potentially informative task. Whilesome of our techniques showed some promise, more work is needed to explorewhether they can improve performance more than the modest gains here. Our codeis available at https://github.com/venkatasg/Lil-Bevo and out models athttps://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a</description><author>Venkata S Govindarajan, Juan Diego Rodriguez, Kaj Bostrom, Kyle Mahowald</author><pubDate>Thu, 26 Oct 2023 18:13:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17591v1</guid></item><item><title>No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions</title><link>http://arxiv.org/abs/2305.17380v3</link><description>Existing online learning algorithms for adversarial Markov Decision Processesachieve ${O}(\sqrt{T})$ regret after $T$ rounds of interactions even if theloss functions are chosen arbitrarily by an adversary, with the caveat that thetransition function has to be fixed. This is because it has been shown thatadversarial transition functions make no-regret learning impossible. Despitesuch impossibility results, in this work, we develop algorithms that can handleboth adversarial losses and adversarial transitions, with regret increasingsmoothly in the degree of maliciousness of the adversary. More concretely, wefirst propose an algorithm that enjoys $\widetilde{{O}}(\sqrt{T} +C^{\textsf{P}})$ regret where $C^{\textsf{P}}$ measures how adversarial thetransition functions are and can be at most ${O}(T)$. While this algorithmitself requires knowledge of $C^{\textsf{P}}$, we further develop a black-boxreduction approach that removes this requirement. Moreover, we also show thatfurther refinements of the algorithm not only maintains the same regret bound,but also simultaneously adapts to easier environments (where losses aregenerated in a certain stochastically constrained manner as in Jin et al.[2021]) and achieves $\widetilde{{O}}(U + \sqrt{UC^{\textsf{L}}} +C^{\textsf{P}})$ regret, where $U$ is some standard gap-dependent coefficientand $C^{\textsf{L}}$ is the amount of corruption on losses.</description><author>Tiancheng Jin, Junyan Liu, ChloÃ© Rouyer, William Chang, Chen-Yu Wei, Haipeng Luo</author><pubDate>Thu, 26 Oct 2023 18:12:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17380v3</guid></item><item><title>Noise-Free Score Distillation</title><link>http://arxiv.org/abs/2310.17590v1</link><description>Score Distillation Sampling (SDS) has emerged as the de facto approach fortext-to-content generation in non-image domains. In this paper, we reexaminethe SDS process and introduce a straightforward interpretation that demystifiesthe necessity for large Classifier-Free Guidance (CFG) scales, rooted in thedistillation of an undesired noise term. Building upon our interpretation, wepropose a novel Noise-Free Score Distillation (NFSD) process, which requiresminimal modifications to the original SDS framework. Through this streamlineddesign, we achieve more effective distillation of pre-trained text-to-imagediffusion models while using a nominal CFG scale. This strategic choice allowsus to prevent the over-smoothing of results, ensuring that the generated datais both realistic and complies with the desired prompt. To demonstrate theefficacy of NFSD, we provide qualitative examples that compare NFSD and SDS, aswell as several other methods.</description><author>Oren Katzir, Or Patashnik, Daniel Cohen-Or, Dani Lischinski</author><pubDate>Thu, 26 Oct 2023 18:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17590v1</guid></item><item><title>An Open Source Data Contamination Report for Llama Series Models</title><link>http://arxiv.org/abs/2310.17589v1</link><description>Data contamination in language model evaluation is increasingly prevalent asthe popularity of large language models. It allows models to "cheat" viamemorisation instead of displaying true capabilities. Therefore, contaminationanalysis has became an crucial part of reliable model evaluation to validateresults. However, existing contamination analysis is usually conductedinternally by LLM developers and often lacks transparency and completeness.This paper present an open source data contamination reports for the Llamaseries models. We analyse six popular multi-choice QA benchmarks and quantifytheir overlapping with the training set of Llama. Various levels ofcontamination ranging from 1\% to 8.7\% are found across benchmarks. Ourcomparison also reveals that Llama models can gain over 5\% higher accuracy oncontaminated subsets versus clean subsets. Data and code are available at:https://github.com/liyucheng09/Contamination_Detector.</description><author>Yucheng Li</author><pubDate>Thu, 26 Oct 2023 18:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17589v1</guid></item><item><title>PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven Perturbed Gradient Descent</title><link>http://arxiv.org/abs/2310.17588v1</link><description>Fine-tuning pretrained language models (PLMs) for downstream tasks is alarge-scale optimization problem, in which the choice of the training algorithmcritically determines how well the trained model can generalize to unseen testdata, especially in the context of few-shot learning. To achieve goodgeneralization performance and avoid overfitting, techniques such as dataaugmentation and pruning are often applied. However, adding theseregularizations necessitates heavy tuning of the hyperparameters ofoptimization algorithms, such as the popular Adam optimizer. In this paper, wepropose a two-stage fine-tuning method, PAC-tuning, to address thisoptimization challenge. First, based on PAC-Bayes training, PAC-tuning directlyminimizes the PAC-Bayes generalization bound to learn proper parameterdistribution. Second, PAC-tuning modifies the gradient by injecting noise withthe variance learned in the first stage into the model parameters duringtraining, resulting in a variant of perturbed gradient descent (PGD). In thepast, the few-shot scenario posed difficulties for PAC-Bayes training becausethe PAC-Bayes bound, when applied to large models with limited training data,might not be stringent. Our experimental results across 5 GLUE benchmark tasksdemonstrate that PAC-tuning successfully handles the challenges of fine-tuningtasks and outperforms strong baseline methods by a visible margin, furtherconfirming the potential to apply PAC training for any other settings where theAdam optimizer is currently used for training.</description><author>Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Marie Johnson, Rongrong Wang</author><pubDate>Thu, 26 Oct 2023 18:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17588v1</guid></item><item><title>Segment Any Building</title><link>http://arxiv.org/abs/2310.01164v4</link><description>The task of identifying and segmenting buildings within remote sensingimagery has perennially stood at the forefront of scholarly investigations.This manuscript accentuates the potency of harnessing diversified datasets intandem with cutting-edge representation learning paradigms for buildingsegmentation in such images. Through the strategic amalgamation of disparatedatasets, we have not only expanded the informational horizon accessible formodel training but also manifested unparalleled performance metrics acrossmultiple datasets. Our avant-garde joint training regimen underscores the meritof our approach, bearing significant implications in pivotal domains such asurban infrastructural development, disaster mitigation strategies, andecological surveillance. Our methodology, predicated upon the fusion ofdatasets and gleaning insights from pre-trained models, carves a new benchmarkin the annals of building segmentation endeavors. The outcomes of this researchboth fortify the foundations for ensuing scholarly pursuits and presage ahorizon replete with innovative applications in the discipline of buildingsegmentation.</description><author>Lei Li</author><pubDate>Thu, 26 Oct 2023 18:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01164v4</guid></item><item><title>Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning</title><link>http://arxiv.org/abs/2301.12593v2</link><description>Many real-world domains require safe decision making in uncertainenvironments. In this work, we introduce a deep reinforcement learningframework for approaching this important problem. We consider a distributionover transition models, and apply a risk-averse perspective towards modeluncertainty through the use of coherent distortion risk measures. We providerobustness guarantees for this framework by showing it is equivalent to aspecific class of distributionally robust safe reinforcement learning problems.Unlike existing approaches to robustness in deep reinforcement learning,however, our formulation does not involve minimax optimization. This leads toan efficient, model-free implementation of our approach that only requiresstandard data collection from a single training environment. In experiments oncontinuous control tasks with safety constraints, we demonstrate that ourframework produces robust performance and safety at deployment time across arange of perturbed test environments.</description><author>James Queeney, Mouhacine Benosman</author><pubDate>Thu, 26 Oct 2023 18:07:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12593v2</guid></item><item><title>Global Voices, Local Biases: Socio-Cultural Prejudices across Languages</title><link>http://arxiv.org/abs/2310.17586v1</link><description>Human biases are ubiquitous but not uniform: disparities exist acrosslinguistic, cultural, and societal borders. As large amounts of recentliterature suggest, language models (LMs) trained on human data can reflect andoften amplify the effects of these social biases. However, the vast majority ofexisting studies on bias are heavily skewed towards Western and Europeanlanguages. In this work, we scale the Word Embedding Association Test (WEAT) to24 languages, enabling broader studies and yielding interesting findings aboutLM bias. We additionally enhance this data with culturally relevant informationfor each language, capturing local contexts on a global scale. Further, toencompass more widely prevalent societal biases, we examine new bias dimensionsacross toxicity, ableism, and more. Moreover, we delve deeper into the Indianlinguistic landscape, conducting a comprehensive regional bias analysis acrosssix prevalent Indian languages. Finally, we highlight the significance of thesesocial biases and the new dimensions through an extensive comparison ofembedding methods, reinforcing the need to address them in pursuit of moreequitable language models. All code, data and results are available here:https://github.com/iamshnoo/weathub.</description><author>Anjishnu Mukherjee, Chahat Raj, Ziwei Zhu, Antonios Anastasopoulos</author><pubDate>Thu, 26 Oct 2023 18:07:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17586v1</guid></item><item><title>A minimax optimal control approach for robust neural ODEs</title><link>http://arxiv.org/abs/2310.17584v1</link><description>In this paper, we address the adversarial training of neural ODEs from arobust control perspective. This is an alternative to the classical trainingvia empirical risk minimization, and it is widely used to enforce reliableoutcomes for input perturbations. Neural ODEs allow the interpretation of deepneural networks as discretizations of control systems, unlocking powerful toolsfrom control theory for the development and the understanding of machinelearning. In this specific case, we formulate the adversarial training withperturbed data as a minimax optimal control problem, for which we derive firstorder optimality conditions in the form of Pontryagin's Maximum Principle. Weprovide a novel interpretation of robust training leading to an alternativeweighted technique, which we test on a low-dimensional classification task.</description><author>Cristina Cipriani, Alessandro Scagliotti, Tobias WÃ¶hrer</author><pubDate>Thu, 26 Oct 2023 18:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17584v1</guid></item><item><title>Convergence of flow-based generative models via proximal gradient descent in Wasserstein space</title><link>http://arxiv.org/abs/2310.17582v1</link><description>Flow-based generative models enjoy certain advantages in computing the datageneration and the likelihood, and have recently shown competitive empiricalperformance. Compared to the accumulating theoretical studies on relatedscore-based diffusion models, analysis of flow-based models, which aredeterministic in both forward (data-to-noise) and reverse (noise-to-data)directions, remain sparse. In this paper, we provide a theoretical guarantee ofgenerating data distribution by a progressive flow model, the so-called JKOflow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in anormalizing flow network. Leveraging the exponential convergence of theproximal gradient descent (GD) in Wasserstein space, we prove theKullback-Leibler (KL) guarantee of data generation by a JKO flow model to be$O(\varepsilon^2)$ when using $N \lesssim \log (1/\varepsilon)$ many JKO steps($N$ Residual Blocks in the flow) where $\varepsilon $ is the error in theper-step first-order condition. The assumption on data density is merely afinite second moment, and the theory extends to data distributions withoutdensity and when there are inversion errors in the reverse process where weobtain KL-$W_2$ mixed error guarantees. The non-asymptotic convergence rate ofthe JKO-type $W_2$-proximal GD is proved for a general class of convexobjective functionals that includes the KL divergence as a special case, whichcan be of independent interest.</description><author>Xiuyuan Cheng, Jianfeng Lu, Yixin Tan, Yao Xie</author><pubDate>Thu, 26 Oct 2023 18:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17582v1</guid></item><item><title>BLIS-Net: Classifying and Analyzing Signals on Graphs</title><link>http://arxiv.org/abs/2310.17579v1</link><description>Graph neural networks (GNNs) have emerged as a powerful tool for tasks suchas node classification and graph classification. However, much less work hasbeen done on signal classification, where the data consists of many functions(referred to as signals) defined on the vertices of a single graph. These tasksrequire networks designed differently from those designed for traditional GNNtasks. Indeed, traditional GNNs rely on localized low-pass filters, and signalsof interest may have intricate multi-frequency behavior and exhibit long rangeinteractions. This motivates us to introduce the BLIS-Net (Bi-LipschitzScattering Net), a novel GNN that builds on the previously introduced geometricscattering transform. Our network is able to capture both local and globalsignal structure and is able to capture both low-frequency and high-frequencyinformation. We make several crucial changes to the original geometricscattering architecture which we prove increase the ability of our network tocapture information about the input signal and show that BLIS-Net achievessuperior performance on both synthetic and real-world data sets based ontraffic flow and fMRI data.</description><author>Charles Xu, Laney Goldman, Valentina Guo, Benjamin Hollander-Bodie, Maedee Trank-Greene, Ian Adelstein, Edward De Brouwer, Rex Ying, Smita Krishnaswamy, Michael Perlmutter</author><pubDate>Thu, 26 Oct 2023 18:03:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17579v1</guid></item><item><title>Detecting and Mitigating Hallucinations in Multilingual Summarisation</title><link>http://arxiv.org/abs/2305.13632v2</link><description>Hallucinations pose a significant challenge to the reliability of neuralmodels for abstractive summarisation. While automatically generated summariesmay be fluent, they often lack faithfulness to the original document. Thisissue becomes even more pronounced in low-resource settings, such ascross-lingual transfer. With the existing faithful metrics focusing on English,even measuring the extent of this phenomenon in cross-lingual settings is hard.To address this, we first develop a novel metric, mFACT, evaluating thefaithfulness of non-English summaries, leveraging translation-based transferfrom multiple English faithfulness metrics. We then propose a simple buteffective method to reduce hallucinations with a cross-lingual transfer, whichweighs the loss of each training example by its faithfulness score. Throughextensive experiments in multiple languages, we demonstrate that mFACT is themetric that is most suited to detect hallucinations. Moreover, we find that ourproposed loss weighting method drastically increases both performance andfaithfulness according to both automatic and human evaluation when compared tostrong baselines for cross-lingual transfer such as MAD-X. Our code and datasetare available at https://github.com/yfqiu-nlp/mfact-summ.</description><author>Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, Shay B. Cohen</author><pubDate>Thu, 26 Oct 2023 18:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13632v2</guid></item><item><title>Optimal Scoring Rule Design under Partial Knowledge</title><link>http://arxiv.org/abs/2107.07420v2</link><description>This paper studies the design of optimal proper scoring rules when theprincipal has partial knowledge of an agent's signal distribution. Recent workcharacterizes the proper scoring rules that maximize the increase of an agent'spayoff when the agent chooses to access a costly signal to refine a posteriorbelief from her prior prediction, under the assumption that the agent's signaldistribution is fully known to the principal. In our setting, the principalonly knows about a set of distributions where the agent's signal distributionbelongs. We formulate the scoring rule design problem as a max-min optimizationthat maximizes the worst-case increase in payoff across the set ofdistributions. We propose an efficient algorithm to compute an optimal scoring rule when theset of distributions is finite, and devise a fully polynomial-timeapproximation scheme that accommodates various infinite sets of distributions.We further remark that widely used scoring rules, such as the quadratic and logrules, as well as previously identified optimal scoring rules under fullknowledge, can be far from optimal in our partial knowledge settings.</description><author>Yiling Chen, Fang-Yi Yu</author><pubDate>Thu, 26 Oct 2023 18:02:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.07420v2</guid></item><item><title>Global Structure-Aware Diffusion Process for Low-Light Image Enhancement</title><link>http://arxiv.org/abs/2310.17577v1</link><description>This paper studies a diffusion-based framework to address the low-light imageenhancement problem. To harness the capabilities of diffusion models, we delveinto this intricate process and advocate for the regularization of its inherentODE-trajectory. To be specific, inspired by the recent research that lowcurvature ODE-trajectory results in a stable and effective diffusion process,we formulate a curvature regularization term anchored in the intrinsicnon-local structures of image data, i.e., global structure-awareregularization, which gradually facilitates the preservation of complicateddetails and the augmentation of contrast during the diffusion process. Thisincorporation mitigates the adverse effects of noise and artifacts resultingfrom the diffusion process, leading to a more precise and flexible enhancement.To additionally promote learning in challenging regions, we introduce anuncertainty-guided regularization technique, which wisely relaxes constraintson the most extreme regions of the image. Experimental evaluations reveal thatthe proposed diffusion-based framework, complemented by rank-informedregularization, attains distinguished performance in low-light enhancement. Theoutcomes indicate substantial advancements in image quality, noise suppression,and contrast amplification in comparison with state-of-the-art methods. Webelieve this innovative approach will stimulate further exploration andadvancement in low-light image processing, with potential implications forother applications of diffusion models. The code is publicly available athttps://github.com/jinnh/GSAD.</description><author>Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, Hui Yuan</author><pubDate>Thu, 26 Oct 2023 18:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17577v1</guid></item><item><title>1D-Touch: NLP-Assisted Coarse Text Selection via a Semi-Direct Gesture</title><link>http://arxiv.org/abs/2310.17576v1</link><description>Existing text selection techniques on touchscreen focus on improving thecontrol for moving the carets. Coarse-grained text selection on word and phraselevels has not received much support beyond word-snapping and entityrecognition. We introduce 1D-Touch, a novel text selection method thatcomplements the carets-based sub-word selection by facilitating the selectionof semantic units of words and above. This method employs a simple verticalslide gesture to expand and contract a selection area from a word. Theexpansion can be by words or by semantic chunks ranging from sub-phrases tosentences. This technique shifts the concept of text selection, from defining arange by locating the first and last words, towards a dynamic process ofexpanding and contracting a textual semantic entity. To understand the effectsof our approach, we prototyped and tested two variants: WordTouch, which offersa straightforward word-by-word expansion, and ChunkTouch, which leverages NLPto chunk text into syntactic units, allowing the selection to grow bysemantically meaningful units in response to the sliding gesture. Ourevaluation, focused on the coarse-grained selection tasks handled by 1D-Touch,shows a 20% improvement over the default word-snapping selection method onAndroid.</description><author>Peiling Jiang, Li Feng, Fuling Sun, Parakrant Sarkar, Haijun Xia, Can Liu</author><pubDate>Thu, 26 Oct 2023 18:01:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17576v1</guid></item><item><title>Inside the black box: Neural network-based real-time prediction of US recessions</title><link>http://arxiv.org/abs/2310.17571v1</link><description>Feedforward neural network (FFN) and two specific types of recurrent neuralnetwork, long short-term memory (LSTM) and gated recurrent unit (GRU), are usedfor modeling US recessions in the period from 1967 to 2021. The estimatedmodels are then employed to conduct real-time predictions of the GreatRecession and the Covid-19 recession in US. Their predictive performances arecompared to those of the traditional linear models, the logistic regressionmodel both with and without the ridge penalty. The out-of-sample performancesuggests the application of LSTM and GRU in the area of recession forecasting,especially for the long-term forecasting tasks. They outperform other types ofmodels across 5 forecasting horizons with respect to different types ofstatistical performance metrics. Shapley additive explanations (SHAP) method isapplied to the fitted GRUs across different forecasting horizons to gaininsight into the feature importance. The evaluation of predictor importancediffers between the GRU and ridge logistic regression models, as reflected inthe variable order determined by SHAP values. When considering the top 5predictors, key indicators such as the S\&amp;P 500 index, real GDP, and privateresidential fixed investment consistently appear for short-term forecasts (upto 3 months). In contrast, for longer-term predictions (6 months or more), theterm spread and producer price index become more prominent. These findings aresupported by both local interpretable model-agnostic explanations (LIME) andmarginal effects.</description><author>Seulki Chung</author><pubDate>Thu, 26 Oct 2023 17:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17571v1</guid></item><item><title>DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation</title><link>http://arxiv.org/abs/2310.17570v1</link><description>While Diffusion Generative Models have achieved great success on imagegeneration tasks, how to efficiently and effectively incorporate them intospeech generation especially translation tasks remains a non-trivial problem.Specifically, due to the low information density of speech data, thetransformed discrete speech unit sequence is much longer than the correspondingtext transcription, posing significant challenges to existing auto-regressivemodels. Furthermore, it is not optimal to brutally apply discrete diffusion onthe speech unit sequence while disregarding the continuous space structure,which will degrade the generation performance significantly. In this paper, wepropose a novel diffusion model by applying the diffusion forward process inthe \textit{continuous} speech representation space, while employing thediffusion backward process in the \textit{discrete} speech unit space. In thisway, we preserve the semantic structure of the continuous speech representationspace in the diffusion process and integrate the continuous and discretediffusion models. We conduct extensive experiments on the textless directspeech-to-speech translation task, where the proposed method achievescomparable results to the computationally intensive auto-regressive baselines(500 steps on average) with significantly fewer decoding steps (50 steps).</description><author>Yongxin Zhu, Zhujin Gao, Xinyuan Zhou, Zhongyi Ye, Linli Xu</author><pubDate>Thu, 26 Oct 2023 17:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17570v1</guid></item><item><title>SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching</title><link>http://arxiv.org/abs/2310.17569v1</link><description>In this paper, we address the challenge of matching semantically similarkeypoints across image pairs. Existing research indicates that the intermediateoutput of the UNet within the Stable Diffusion (SD) can serve as robust imagefeature maps for such a matching task. We demonstrate that by employing a basicprompt tuning technique, the inherent potential of Stable Diffusion can beharnessed, resulting in a significant enhancement in accuracy over previousapproaches. We further introduce a novel conditional prompting module thatconditions the prompt on the local details of the input image pairs, leading toa further improvement in performance. We designate our approach as SD4Match,short for Stable Diffusion for Semantic Matching. Comprehensive evaluations ofSD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it setsnew benchmarks in accuracy across all these datasets. Particularly, SD4Matchoutperforms the previous state-of-the-art by a margin of 12 percentage pointson the challenging SPair-71k dataset.</description><author>Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu</author><pubDate>Thu, 26 Oct 2023 17:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17569v1</guid></item><item><title>Navigating to Success in Multi-Modal Human-Robot Collaboration: Analysis and Corpus Release</title><link>http://arxiv.org/abs/2310.17568v1</link><description>Human-guided robotic exploration is a useful approach to gatheringinformation at remote locations, especially those that might be too risky,inhospitable, or inaccessible for humans. Maintaining common ground between theremotely-located partners is a challenge, one that can be facilitated bymulti-modal communication. In this paper, we explore how participants utilizedmultiple modalities to investigate a remote location with the help of a roboticpartner. Participants issued spoken natural language instructions and receivedfrom the robot: text-based feedback, continuous 2D LIDAR mapping, andupon-request static photographs. We noticed that different strategies wereadopted in terms of use of the modalities, and hypothesize that thesedifferences may be correlated with success at several exploration sub-tasks. Wefound that requesting photos may have improved the identification and countingof some key entities (doorways in particular) and that this strategy did nothinder the amount of overall area exploration. Future work with larger samplesmay reveal the effects of more nuanced photo and dialogue strategies, which caninform the training of robotic agents. Additionally, we announce the release ofour unique multi-modal corpus of human-robot communication in an explorationcontext: SCOUT, the Situated Corpus on Understanding Transactions.</description><author>Stephanie M. Lukin, Kimberly A. Pollard, Claire Bonial, Taylor Hudson, Ron Arstein, Clare Voss, David Traum</author><pubDate>Thu, 26 Oct 2023 17:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17568v1</guid></item><item><title>Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models</title><link>http://arxiv.org/abs/2310.17567v1</link><description>With LLMs shifting their role from statistical modeling of language toserving as general-purpose AI agents, how should LLM evaluations change?Arguably, a key ability of an AI agent is to flexibly combine, as needed, thebasic skills it has learned. The capability to combine skills plays animportant role in (human) pedagogy and also in a paper on emergence phenomena(Arora &amp; Goyal, 2023). This work introduces Skill-Mix, a new evaluation to measure ability tocombine skills. Using a list of $N$ skills the evaluator repeatedly picksrandom subsets of $k$ skills and asks the LLM to produce text combining thatsubset of skills. Since the number of subsets grows like $N^k$, for even modest$k$ this evaluation will, with high probability, require the LLM to producetext significantly different from any text in the training set. The paperdevelops a methodology for (a) designing and administering such an evaluation,and (b) automatic grading (plus spot-checking by humans) of the results usingGPT-4 as well as the open LLaMA-2 70B model. Administering a version of to popular chatbots gave results that, whilegenerally in line with prior expectations, contained surprises. Sizeabledifferences exist among model capabilities that are not captured by theirranking on popular LLM leaderboards ("cramming for the leaderboard").Furthermore, simple probability calculations indicate that GPT-4's reasonableperformance on $k=5$ is suggestive of going beyond "stochastic parrot" behavior(Bender et al., 2021), i.e., it combines skills in ways that it had not seenduring training. We sketch how the methodology can lead to a Skill-Mix based eco-system ofopen evaluations for AI capabilities of future models.</description><author>Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, Sanjeev Arora</author><pubDate>Thu, 26 Oct 2023 17:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17567v1</guid></item><item><title>SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models</title><link>http://arxiv.org/abs/2305.14267v2</link><description>A potent class of generative models known as Diffusion Probabilistic Models(DPMs) has become prominent. A forward diffusion process adds gradually noiseto data, while a model learns to gradually denoise. Sampling from pre-trainedDPMs is obtained by solving differential equations (DE) defined by the learntmodel, a process which has shown to be prohibitively slow. Numerous efforts onspeeding-up this process have consisted on crafting powerful ODE solvers.Despite being quick, such solvers do not usually reach the optimal qualityachieved by available slow SDE solvers. Our goal is to propose SDE solvers thatreach optimal quality without requiring several hundreds or thousands of NFEsto achieve that goal. We propose Stochastic Explicit ExponentialDerivative-free Solvers (SEEDS), improving and generalizing ExponentialIntegrator approaches to the stochastic case on several frameworks. Aftercarefully analyzing the formulation of exact solutions of diffusion SDEs, wecraft SEEDS to analytically compute the linear part of such solutions. Inspiredby the Exponential Time-Differencing method, SEEDS use a novel treatment of thestochastic components of solutions, enabling the analytical computation oftheir variance, and contains high-order terms allowing to reach optimal qualitysampling $\sim3$-$5\times$ faster than previous SDE methods. We validate ourapproach on several image generation benchmarks, showing that SEEDS outperformor are competitive with previous SDE solvers. Contrary to the latter, SEEDS arederivative and training free, and we fully prove strong convergence guaranteesfor them.</description><author>Martin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, Nader Masmoudi</author><pubDate>Thu, 26 Oct 2023 17:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14267v2</guid></item><item><title>Bifurcations and loss jumps in RNN training</title><link>http://arxiv.org/abs/2310.17561v1</link><description>Recurrent neural networks (RNNs) are popular machine learning tools formodeling and forecasting sequential data and for inferring dynamical systems(DS) from observed time series. Concepts from DS theory (DST) have variouslybeen used to further our understanding of both, how trained RNNs solve complextasks, and the training process itself. Bifurcations are particularly importantphenomena in DS, including RNNs, that refer to topological (qualitative)changes in a system's dynamical behavior as one or more of its parameters arevaried. Knowing the bifurcation structure of an RNN will thus allow to deducemany of its computational and dynamical properties, like its sensitivity toparameter variations or its behavior during training. In particular,bifurcations may account for sudden loss jumps observed in RNN training thatcould severely impede the training process. Here we first mathematically provefor a particular class of ReLU-based RNNs that certain bifurcations are indeedassociated with loss gradients tending toward infinity or zero. We thenintroduce a novel heuristic algorithm for detecting all fixed points andk-cycles in ReLU-based RNNs and their existence and stability regions, hencebifurcation manifolds in parameter space. In contrast to previous numericalalgorithms for finding fixed points and common continuation methods, ouralgorithm provides exact results and returns fixed points and cycles up to highorders with surprisingly good scaling behavior. We exemplify the algorithm onthe analysis of the training process of RNNs, and find that the recentlyintroduced technique of generalized teacher forcing completely avoids certaintypes of bifurcations in training. Thus, besides facilitating the DST analysisof trained RNNs, our algorithm provides a powerful instrument for analyzing thetraining process itself.</description><author>Lukas Eisenmann, Zahra Monfared, Niclas Alexander GÃ¶ring, Daniel Durstewitz</author><pubDate>Thu, 26 Oct 2023 17:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17561v1</guid></item><item><title>Instability of computer vision models is a necessary result of the task itself</title><link>http://arxiv.org/abs/2310.17559v1</link><description>Adversarial examples resulting from instability of current computer visionmodels are an extremely important topic due to their potential to compromiseany application. In this paper we demonstrate that instability is inevitabledue to a) symmetries (translational invariance) of the data, b) the categoricalnature of the classification task, and c) the fundamental discrepancy ofclassifying images as objects themselves. The issue is further exacerbated bynon-exhaustive labelling of the training data. Therefore we conclude thatinstability is a necessary result of how the problem of computer vision iscurrently formulated. While the problem cannot be eliminated, through theanalysis of the causes, we have arrived at ways how it can be partiallyalleviated. These include i) increasing the resolution of images, ii) providingcontextual information for the image, iii) exhaustive labelling of trainingdata, and iv) preventing attackers from frequent access to the computer visionsystem.</description><author>Oliver Turnbull, George Cevora</author><pubDate>Thu, 26 Oct 2023 17:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17559v1</guid></item><item><title>Towards Matching Phones and Speech Representations</title><link>http://arxiv.org/abs/2310.17558v1</link><description>Learning phone types from phone instances has been a long-standing problem,while still being open. In this work, we revisit this problem in the context ofself-supervised learning, and pose it as the problem of matching clustercentroids to phone embeddings. We study two key properties that enablematching, namely, whether cluster centroids of self-supervised representationsreduce the variability of phone instances and respect the relationship amongphones. We then use the matching result to produce pseudo-labels and introducea new loss function for improving self-supervised representations. Ourexperiments show that the matching result captures the relationship amongphones. Training the new loss function jointly with the regular self-supervisedlosses, such as APC and CPC, significantly improves the downstream phoneclassification.</description><author>Gene-Ping Yang, Hao Tang</author><pubDate>Thu, 26 Oct 2023 17:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17558v1</guid></item><item><title>Efficient Numerical Algorithm for Large-Scale Damped Natural Gradient Descent</title><link>http://arxiv.org/abs/2310.17556v1</link><description>We propose a new algorithm for efficiently solving the damped Fisher matrixin large-scale scenarios where the number of parameters significantly exceedsthe number of available samples. This problem is fundamental for naturalgradient descent and stochastic reconfiguration. Our algorithm is based onCholesky decomposition and is generally applicable. Benchmark results show thatthe algorithm is significantly faster than existing methods.</description><author>Yixiao Chen, Hao Xie, Han Wang</author><pubDate>Thu, 26 Oct 2023 17:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17556v1</guid></item><item><title>Interactive Robot Learning from Verbal Correction</title><link>http://arxiv.org/abs/2310.17555v1</link><description>The ability to learn and refine behavior after deployment has become evermore important for robots as we design them to operate in unstructuredenvironments like households. In this work, we design a new learning systembased on large language model (LLM), OLAF, that allows everyday users to teacha robot using verbal corrections when the robot makes mistakes, e.g., by saying"Stop what you're doing. You should move closer to the cup." A key feature ofOLAF is its ability to update the robot's visuomotor neural policy based on theverbal feedback to avoid repeating mistakes in the future. This is in contrastto existing LLM-based robotic systems, which only follow verbal commands orcorrections but not learn from them. We demonstrate the efficacy of our designin experiments where a user teaches a robot to perform long-horizonmanipulation tasks both in simulation and on physical hardware, achieving onaverage 20.0% improvement in policy success rate. Videos and more results areat https://ut-austin-rpl.github.io/olaf/</description><author>Huihan Liu, Alice Chen, Yuke Zhu, Adith Swaminathan, Andrey Kolobov, Ching-An Cheng</author><pubDate>Thu, 26 Oct 2023 17:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17555v1</guid></item><item><title>Model-Based Runtime Monitoring with Interactive Imitation Learning</title><link>http://arxiv.org/abs/2310.17552v1</link><description>Robot learning methods have recently made great strides, but generalizationand robustness challenges still hinder their widespread deployment. Failing todetect and address potential failures renders state-of-the-art learning systemsnot combat-ready for high-stakes tasks. Recent advances in interactiveimitation learning have presented a promising framework for human-robotteaming, enabling the robots to operate safely and continually improve theirperformances over long-term deployments. Nonetheless, existing methodstypically require constant human supervision and preemptive feedback, limitingtheir practicality in realistic domains. This work aims to endow a robot withthe ability to monitor and detect errors during task execution. We introduce amodel-based runtime monitoring algorithm that learns from deployment data todetect system anomalies and anticipate failures. Unlike prior work that cannotforesee future failures or requires failure experiences for training, ourmethod learns a latent-space dynamics model and a failure classifier, enablingour method to simulate future action outcomes and detect out-of-distributionand high-risk states preemptively. We train our method within an interactiveimitation learning framework, where it continually updates the model from theexperiences of the human-robot team collected using trustworthy deployments.Consequently, our method reduces the human workload needed over time whileensuring reliable task execution. Our method outperforms the baselines acrosssystem-level and unit-test metrics, with 23% and 40% higher success rates insimulation and on physical hardware, respectively. More information athttps://ut-austin-rpl.github.io/sirius-runtime-monitor/</description><author>Huihan Liu, Shivin Dass, Roberto MartÃ­n-MartÃ­n, Yuke Zhu</author><pubDate>Thu, 26 Oct 2023 17:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17552v1</guid></item><item><title>Unpacking the Ethical Value Alignment in Big Models</title><link>http://arxiv.org/abs/2310.17551v1</link><description>Big models have greatly advanced AI's ability to understand, generate, andmanipulate information and content, enabling numerous applications. However, asthese models become increasingly integrated into everyday life, their inherentethical values and potential biases pose unforeseen risks to society. Thispaper provides an overview of the risks and challenges associated with bigmodels, surveys existing AI ethics guidelines, and examines the ethicalimplications arising from the limitations of these models. Taking a normativeethics perspective, we propose a reassessment of recent normative guidelines,highlighting the importance of collaborative efforts in academia to establish aunified and universal AI ethics framework. Furthermore, we investigate themoral inclinations of current mainstream LLMs using the Moral Foundationtheory, analyze existing alignment algorithms, and outline the uniquechallenges encountered in aligning ethical values within them. To address thesechallenges, we introduce a novel conceptual paradigm for aligning the ethicalvalues of big models and discuss promising research directions for alignmentcriteria, evaluation, and method, representing an initial step towards theinterdisciplinary construction of the ethically aligned AI This paper is a modified English version of our Chinese paperhttps://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intendedto help non-Chinese native speakers better understand our work.</description><author>Xiaoyuan Yi, Jing Yao, Xiting Wang, Xing Xie</author><pubDate>Thu, 26 Oct 2023 17:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17551v1</guid></item><item><title>Human-Guided Complexity-Controlled Abstractions</title><link>http://arxiv.org/abs/2310.17550v1</link><description>Neural networks often learn task-specific latent representations that fail togeneralize to novel settings or tasks. Conversely, humans learn discreterepresentations (i.e., concepts or words) at a variety of abstraction levels(e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction basedon task. Inspired by this, we train neural models to generate a spectrum ofdiscrete representations, and control the complexity of the representations(roughly, how many bits are allocated for encoding inputs) by tuning theentropy of the distribution over representations. In finetuning experiments,using only a small number of labeled examples for a new task, we show that (1)tuning the representation to a task-appropriate complexity level supports thehighest finetuning performance, and (2) in a human-participant study, userswere able to identify the appropriate complexity level for a downstream taskusing visualizations of discrete representations. Our results indicate apromising direction for rapid model finetuning by leveraging human insight.</description><author>Andi Peng, Mycal Tucker, Eoin Kenny, Noga Zaslavsky, Pulkit Agrawal, Julie Shah</author><pubDate>Thu, 26 Oct 2023 17:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17550v1</guid></item><item><title>AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models</title><link>http://arxiv.org/abs/2305.15064v3</link><description>Recent large language models (LLMs) are promising for making decisions ingrounded environments. However, LLMs frequently fail in complex decision-makingtasks due to the misalignment between the pre-trained knowledge in LLMs and theactual rules in the environment. Existing methods require either costlygradient computation or lengthy in-context demonstrations. In this paper, wepropose AutoPlan, an approach to guide LLM-based agents to accomplishinteractive decision-making tasks. AutoPlan augments the LLM prompt with atask-solving plan and optimizes it through iterative experience collection andreflection. Our experiments show that AutoPlan, though using no in-contextdemonstrations, achieves success rates on par with the baselines usinghuman-written demonstrations on ALFWorld and even outperforms them by 8% onHotpotQA. The code is available at https://github.com/owaski/AutoPlan.</description><author>Siqi Ouyang, Lei Li</author><pubDate>Thu, 26 Oct 2023 17:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15064v3</guid></item><item><title>Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting</title><link>http://arxiv.org/abs/2310.17544v1</link><description>We study a novel ensemble approach for feature selection based onhierarchical stacking in cases of non-stationarity and limited number ofsamples with large number of features. Our approach exploits the co-dependencybetween features using a hierarchical structure. Initially, a machine learningmodel is trained using a subset of features, and then the model's output isupdated using another algorithm with the remaining features to minimize thetarget loss. This hierarchical structure allows for flexible depth and featureselection. By exploiting feature co-dependency hierarchically, our proposedapproach overcomes the limitations of traditional feature selection methods andfeature importance scores. The effectiveness of the approach is demonstrated onsynthetic and real-life datasets, indicating improved performance withscalability and stability compared to the traditional methods andstate-of-the-art approaches.</description><author>Aysin Tumay, Mustafa E. Aydin, Suleyman S. Kozat</author><pubDate>Thu, 26 Oct 2023 17:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17544v1</guid></item><item><title>EqDrive: Efficient Equivariant Motion Forecasting with Multi-Modality for Autonomous Driving</title><link>http://arxiv.org/abs/2310.17540v1</link><description>Forecasting vehicular motions in autonomous driving requires a deepunderstanding of agent interactions and the preservation of motion equivarianceunder Euclidean geometric transformations. Traditional models often lack thesophistication needed to handle the intricate dynamics inherent to autonomousvehicles and the interaction relationships among agents in the scene. As aresult, these models have a lower model capacity, which then leads to higherprediction errors and lower training efficiency. In our research, we employEqMotion, a leading equivariant particle, and human prediction model that alsoaccounts for invariant agent interactions, for the task of multi-agent vehiclemotion forecasting. In addition, we use a multi-modal prediction mechanism toaccount for multiple possible future paths in a probabilistic manner. Byleveraging EqMotion, our model achieves state-of-the-art (SOTA) performancewith fewer parameters (1.2 million) and a significantly reduced training time(less than 2 hours).</description><author>Yuping Wang, Jier Chen</author><pubDate>Thu, 26 Oct 2023 17:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17540v1</guid></item><item><title>Integrating View Conditions for Image Synthesis</title><link>http://arxiv.org/abs/2310.16002v2</link><description>In the field of image processing, applying intricate semantic modificationswithin existing images remains an enduring challenge. This paper introduces apioneering framework that integrates viewpoint information to enhance thecontrol of image editing tasks. By surveying existing object editingmethodologies, we distill three essential criteria, consistency,controllability, and harmony, that should be met for an image editing method.In contrast to previous approaches, our method takes the lead in satisfying allthree requirements for addressing the challenge of image synthesis. Throughcomprehensive experiments, encompassing both quantitative assessments andqualitative comparisons with contemporary state-of-the-art methods, we presentcompelling evidence of our framework's superior performance across multipledimensions. This work establishes a promising avenue for advancing imagesynthesis techniques and empowering precise object modifications whilepreserving the visual coherence of the entire composition.</description><author>Jinbin Bai, Zhen Dong, Aosong Feng, Xiao Zhang, Tian Ye, Kaicheng Zhou, Mike Zheng Shou</author><pubDate>Thu, 26 Oct 2023 17:30:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16002v2</guid></item><item><title>TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for Gaze Estimation</title><link>http://arxiv.org/abs/2307.07813v4</link><description>Intelligent edge vision tasks encounter the critical challenge of ensuringpower and latency efficiency due to the typically heavy computational load theyimpose on edge platforms.This work leverages one of the first "AI in sensor"vision platforms, IMX500 by Sony, to achieve ultra-fast and ultra-low-powerend-to-end edge vision applications. We evaluate the IMX500 and compare it toother edge platforms, such as the Google Coral Dev Micro and Sony Spresense, byexploring gaze estimation as a case study. We propose TinyTracker, a highlyefficient, fully quantized model for 2D gaze estimation designed to maximizethe performance of the edge vision systems considered in this study.TinyTracker achieves a 41x size reduction (600Kb) compared to iTracker [1]without significant loss in gaze estimation accuracy (maximum of 0.16 cm whenfully quantized). TinyTracker's deployment on the Sony IMX500 vision sensorresults in end-to-end latency of around 19ms. The camera takes around 17.9ms toread, process and transmit the pixels to the accelerator. The inference time ofthe network is 0.86ms with an additional 0.24 ms for retrieving the resultsfrom the sensor. The overall energy consumption of the end-to-end system is 4.9mJ, including 0.06 mJ for inference. The end-to-end study shows that IMX500 is1.7x faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJVS 34.2mJ)</description><author>Pietro Bonazzi, Thomas Ruegg, Sizhen Bian, Yawei Li, Michele Magno</author><pubDate>Thu, 26 Oct 2023 17:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07813v4</guid></item><item><title>Little Exploration is All You Need</title><link>http://arxiv.org/abs/2310.17538v1</link><description>The prevailing principle of "Optimism in the Face of Uncertainty" advocatesfor the incorporation of an exploration bonus, generally assumed to beproportional to the inverse square root of the visit count ($1/\sqrt{n}$),where $n$ is the number of visits to a particular state-action pair. Thisapproach, however, exclusively focuses on "uncertainty," neglecting theinherent "difficulty" of different options. To address this gap, we introduce anovel modification of standard UCB algorithm in the multi-armed bandit problem,proposing an adjusted bonus term of $1/n^\tau$, where $\tau &gt; 1/2$, thataccounts for task difficulty. Our proposed algorithm, denoted as UCB$^\tau$, issubstantiated through comprehensive regret and risk analyses, confirming itstheoretical robustness. Comparative evaluations with standard UCB and ThompsonSampling algorithms on synthetic datasets demonstrate that UCB$^\tau$ not onlyoutperforms in efficacy but also exhibits lower risk across variousenvironmental conditions and hyperparameter settings.</description><author>Henry H. H. Chen, Jiaming Lu</author><pubDate>Thu, 26 Oct 2023 17:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17538v1</guid></item><item><title>Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity</title><link>http://arxiv.org/abs/2310.17537v1</link><description>Deep reinforcement learning methods exhibit impressive performance on a rangeof tasks but still struggle on hard exploration tasks in large environmentswith sparse rewards. To address this, intrinsic rewards can be generated usingforward model prediction errors that decrease as the environment becomes known,and incentivize an agent to explore novel states. While prediction-basedintrinsic rewards can help agents solve hard exploration tasks, they can sufferfrom catastrophic forgetting and actually increase at visited states. We firstexamine the conditions and causes of catastrophic forgetting in grid worldenvironments. We then propose a new method FARCuriosity, inspired by how humansand animals learn. The method depends on fragmentation and recall: an agentfragments an environment based on surprisal, and uses different local curiositymodules (prediction-based intrinsic reward functions) for each fragment so thatmodules are not trained on the entire environment. At each fragmentation event,the agent stores the current module in long-term memory (LTM) and eitherinitializes a new module or recalls a previously stored module based on itsmatch with the current state. With fragmentation and recall, FARCuriosityachieves less forgetting and better overall performance in games with variedand heterogeneous environments in the Atari benchmark suite of tasks. Thus,this work highlights the problem of catastrophic forgetting in prediction-basedcuriosity methods and proposes a solution.</description><author>Jaedong Hwang, Zhang-Wei Hong, Eric Chen, Akhilan Boopathy, Pulkit Agrawal, Ila Fiete</author><pubDate>Thu, 26 Oct 2023 17:28:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17537v1</guid></item><item><title>SoK: Pitfalls in Evaluating Black-Box Attacks</title><link>http://arxiv.org/abs/2310.17534v1</link><description>Numerous works study black-box attacks on image classifiers. However, theseworks make different assumptions on the adversary's knowledge and currentliterature lacks a cohesive organization centered around the threat model. Tosystematize knowledge in this area, we propose a taxonomy over the threat spacespanning the axes of feedback granularity, the access of interactive queries,and the quality and quantity of the auxiliary data available to the attacker.Our new taxonomy provides three key insights. 1) Despite extensive literature,numerous under-explored threat spaces exist, which cannot be trivially solvedby adapting techniques from well-explored settings. We demonstrate this byestablishing a new state-of-the-art in the less-studied setting of access totop-k confidence scores by adapting techniques from well-explored settings ofaccessing the complete confidence vector, but show how it still falls short ofthe more restrictive setting that only obtains the prediction label,highlighting the need for more research. 2) Identification the threat model ofdifferent attacks uncovers stronger baselines that challenge priorstate-of-the-art claims. We demonstrate this by enhancing an initially weakerbaseline (under interactive query access) via surrogate models, effectivelyoverturning claims in the respective paper. 3) Our taxonomy revealsinteractions between attacker knowledge that connect well to related areas,such as model inversion and extraction attacks. We discuss how advances inother areas can enable potentially stronger black-box attacks. Finally, weemphasize the need for a more realistic assessment of attack success byfactoring in local attack runtime. This approach reveals the potential forcertain attacks to achieve notably higher success rates and the need toevaluate attacks in diverse and harder settings, highlighting the need forbetter selection criteria.</description><author>Fnu Suya, Anshuman Suri, Tingwei Zhang, Jingtao Hong, Yuan Tian, David Evans</author><pubDate>Thu, 26 Oct 2023 17:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17534v1</guid></item><item><title>DocumentNet: Bridging the Data Gap in Document Pre-Training</title><link>http://arxiv.org/abs/2306.08937v3</link><description>Document understanding tasks, in particular, Visually-rich Document EntityRetrieval (VDER), have gained significant attention in recent years thanks totheir broad applications in enterprise AI. However, publicly available datahave been scarce for these tasks due to strict privacy constraints and highannotation costs. To make things worse, the non-overlapping entity spaces fromdifferent datasets hinder the knowledge transfer between document types. Inthis paper, we propose a method to collect massive-scale and weakly labeleddata from the web to benefit the training of VDER models. The collecteddataset, named DocumentNet, does not depend on specific document types orentity sets, making it universally applicable to all VDER tasks. The currentDocumentNet consists of 30M documents spanning nearly 400 document typesorganized in a four-level ontology. Experiments on a set of broadly adoptedVDER tasks show significant improvements when DocumentNet is incorporated intothe pre-training for both classic and few-shot learning settings. With therecent emergence of large language models (LLMs), DocumentNet provides a largedata source to extend their multi-modal capabilities for VDER.</description><author>Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, Alexander G. Hauptmann, Hanjun Dai, Wei Wei</author><pubDate>Thu, 26 Oct 2023 17:23:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08937v3</guid></item><item><title>Learning Regularized Graphon Mean-Field Games with Unknown Graphons</title><link>http://arxiv.org/abs/2310.17531v1</link><description>We design and analyze reinforcement learning algorithms for GraphonMean-Field Games (GMFGs). In contrast to previous works that require theprecise values of the graphons, we aim to learn the Nash Equilibrium (NE) ofthe regularized GMFGs when the graphons are unknown. Our contributions arethreefold. First, we propose the Proximal Policy Optimization for GMFG(GMFG-PPO) algorithm and show that it converges at a rate of $O(T^{-1/3})$after $T$ iterations with an estimation oracle, improving on a previous work byXie et al. (ICML, 2021). Second, using kernel embedding of distributions, wedesign efficient algorithms to estimate the transition kernels, rewardfunctions, and graphons from sampled agents. Convergence rates are then derivedwhen the positions of the agents are either known or unknown. Results for thecombination of the optimization algorithm GMFG-PPO and the estimation algorithmare then provided. These algorithms are the first specifically designed forlearning graphons from sampled agents. Finally, the efficacy of the proposedalgorithms are corroborated through simulations. These simulations demonstratethat learning the unknown graphons reduces the exploitability effectively.</description><author>Fengzhuo Zhang, Vincent Y. F. Tan, Zhaoran Wang, Zhuoran Yang</author><pubDate>Thu, 26 Oct 2023 17:19:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17531v1</guid></item><item><title>Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models</title><link>http://arxiv.org/abs/2310.17530v1</link><description>Pretrained machine learning models are known to perpetuate and even amplifyexisting biases in data, which can result in unfair outcomes that ultimatelyimpact user experience. Therefore, it is crucial to understand the mechanismsbehind those prejudicial biases to ensure that model performance does notresult in discriminatory behaviour toward certain groups or populations. Inthis work, we define gender bias as our case study. We quantify biasamplification in pretraining and after fine-tuning on three families ofvision-and-language models. We investigate the connection, if any, between thetwo learning stages, and evaluate how bias amplification reflects on modelperformance. Overall, we find that bias amplification in pretraining and afterfine-tuning are independent. We then examine the effect of continuedpretraining on gender-neutral data, finding that this reduces groupdisparities, i.e., promotes fairness, on VQAv2 and retrieval tasks withoutsignificantly compromising task performance.</description><author>Laura Cabello, Emanuele Bugliarello, Stephanie Brandl, Desmond Elliott</author><pubDate>Thu, 26 Oct 2023 17:19:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17530v1</guid></item><item><title>Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction</title><link>http://arxiv.org/abs/2310.17527v1</link><description>In this paper, we propose the Masked Space-Time Hash encoding (MSTH), a novelmethod for efficiently reconstructing dynamic 3D scenes from multi-view ormonocular videos. Based on the observation that dynamic scenes often containsubstantial static areas that result in redundancy in storage and computations,MSTH represents a dynamic scene as a weighted combination of a 3D hash encodingand a 4D hash encoding. The weights for the two components are represented by alearnable mask which is guided by an uncertainty-based objective to reflect thespatial and temporal importance of each 3D position. With this design, ourmethod can reduce the hash collision rate by avoiding redundant queries andmodifications on static areas, making it feasible to represent a large numberof space-time voxels by hash tables with small size.Besides, without therequirements to fit the large numbers of temporally redundant featuresindependently, our method is easier to optimize and converge rapidly with onlytwenty minutes of training for a 300-frame dynamic scene.As a result, MSTHobtains consistently better results than previous methods with only 20 minutesof training time and 130 MB of memory storage. Code is available athttps://github.com/masked-spacetime-hashing/msth</description><author>Feng Wang, Zilong Chen, Guokang Wang, Yafei Song, Huaping Liu</author><pubDate>Thu, 26 Oct 2023 17:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17527v1</guid></item><item><title>Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages</title><link>http://arxiv.org/abs/2310.17526v1</link><description>Systematic reviews are vital for guiding practice, research, and policy, yetthey are often slow and labour-intensive. Large language models (LLMs) couldoffer a way to speed up and automate systematic reviews, but their performancein such tasks has not been comprehensively evaluated against humans, and nostudy has tested GPT-4, the biggest LLM so far. This pre-registered studyevaluates GPT-4's capability in title/abstract screening, full-text review, anddata extraction across various literature types and languages using a'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with humanperformance in most tasks, results were skewed by chance agreement and datasetimbalance. After adjusting for these, there was a moderate level of performancefor data extraction, and - barring studies that used highly reliable prompts -screening performance levelled at none to moderate for different stages andlanguages. When screening full-text literature using highly reliable prompts,GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing keystudies using highly reliable prompts improved its performance even more. Ourfindings indicate that, currently, substantial caution should be used if LLMsare being used to conduct systematic reviews, but suggest that, for certainsystematic review tasks delivered under reliable prompts, LLMs can rival humanperformance.</description><author>Qusai Khraisha, Sophie Put, Johanna Kappenberg, Azza Warraitch, Kristin Hadfield</author><pubDate>Thu, 26 Oct 2023 17:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17526v1</guid></item><item><title>FLARE: Fast Learning of Animatable and Relightable Mesh Avatars</title><link>http://arxiv.org/abs/2310.17519v1</link><description>Our goal is to efficiently learn personalized animatable 3D head avatars fromvideos that are geometrically accurate, realistic, relightable, and compatiblewith current rendering systems. While 3D meshes enable efficient processing andare highly portable, they lack realism in terms of shape and appearance. Neuralrepresentations, on the other hand, are realistic but lack compatibility andare slow to train and render. Our key insight is that it is possible toefficiently learn high-fidelity 3D mesh representations via differentiablerendering by exploiting highly-optimized methods from traditional computergraphics and approximating some of the components with neural networks. To thatend, we introduce \moniker, a technique that enables the creation of animatableand relightable mesh avatars from a single monocular video. First, we learn acanonical geometry using a mesh representation, enabling efficientdifferentiable rasterization and straightforward animation via learnedblendshapes and linear blend skinning weights. Second, we followphysically-based rendering and factor observed colors into intrinsic albedo,roughness, and a neural representation of the illumination, allowing thelearned avatars to be relit in novel scenes. Since our input videos arecaptured on a single device with a narrow field of view, modeling thesurrounding environment light is non-trivial. Based on the split-sumapproximation for modeling specular reflections, we address this byapproximating the pre-filtered environment map with a multi-layer perceptron(MLP) modulated by the surface roughness, eliminating the need to explicitlymodel the light. We demonstrate that our mesh-based avatar formulation,combined with learned deformation, material, and lighting MLPs, producesavatars with high-quality geometry and appearance, while also being efficientto train and render compared to existing approaches.</description><author>Shrisha Bharadwaj, Yufeng Zheng, Otmar Hilliges, Michael J. Black, Victoria Fernandez-Abrevaya</author><pubDate>Thu, 26 Oct 2023 17:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17519v1</guid></item><item><title>The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks</title><link>http://arxiv.org/abs/2310.17514v1</link><description>NLP models have progressed drastically in recent years, according to numerousdatasets proposed to evaluate performance. Questions remain, however, about howparticular dataset design choices may impact the conclusions we draw aboutmodel capabilities. In this work, we investigate this question in the domain ofcompositional generalization. We examine the performance of six modelingapproaches across 4 datasets, split according to 8 compositional splittingstrategies, ranking models by 18 compositional generalization splits in total.Our results show that: i) the datasets, although all designed to evaluatecompositional generalization, rank modeling approaches differently; ii)datasets generated by humans align better with each other than they withsynthetic datasets, or than synthetic datasets among themselves; iii)generally, whether datasets are sampled from the same source is more predictiveof the resulting model ranking than whether they maintain the sameinterpretation of compositionality; and iv) which lexical items are used in thedata can strongly impact conclusions. Overall, our results demonstrate thatmuch work remains to be done when it comes to assessing whether popularevaluation datasets measure what they intend to measure, and suggest thatelucidating more rigorous standards for establishing the validity of evaluationsets could benefit the field.</description><author>Kaiser Sun, Adina Williams, Dieuwke Hupkes</author><pubDate>Thu, 26 Oct 2023 17:11:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17514v1</guid></item><item><title>What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement</title><link>http://arxiv.org/abs/2303.11249v4</link><description>The question of what makes a data distribution suitable for deep learning isa fundamental open problem. Focusing on locally connected neural networks (aprevalent family of architectures that includes convolutional and recurrentneural networks as well as local self-attention models), we address thisproblem by adopting theoretical tools from quantum physics. Our maintheoretical result states that a certain locally connected neural network iscapable of accurate prediction over a data distribution if and only if the datadistribution admits low quantum entanglement under certain canonical partitionsof features. As a practical application of this result, we derive apreprocessing method for enhancing the suitability of a data distribution tolocally connected neural networks. Experiments with widespread models overvarious datasets demonstrate our findings. We hope that our use of quantumentanglement will encourage further adoption of tools from physics for formallyreasoning about the relation between deep learning and real-world data.</description><author>Yotam Alexander, Nimrod De La Vega, Noam Razin, Nadav Cohen</author><pubDate>Thu, 26 Oct 2023 17:10:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11249v4</guid></item><item><title>The Expressive Power of Low-Rank Adaptation</title><link>http://arxiv.org/abs/2310.17513v1</link><description>Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method thatleverages low-rank adaptation of weight matrices, has emerged as a prevalenttechnique for fine-tuning pre-trained models such as large language models anddiffusion models. Despite its huge success in practice, the theoreticalunderpinnings of LoRA have largely remained unexplored. This paper takes thefirst step to bridge this gap by theoretically analyzing the expressive powerof LoRA. We prove that, for fully connected neural networks, LoRA can adapt anymodel $f$ to accurately represent any smaller target model $\overline{f}$ ifLoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of}\overline{f}}{\text{depth of }f}$. We also quantify the approximation errorwhen LoRA-rank is lower than the threshold. For Transformer networks, we showany model can be adapted to a target model of the same size withrank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.</description><author>Yuchen Zeng, Kangwook Lee</author><pubDate>Thu, 26 Oct 2023 17:08:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17513v1</guid></item><item><title>SourceP: Detecting Ponzi Schemes on Ethereum with Source Code</title><link>http://arxiv.org/abs/2306.01665v3</link><description>As blockchain technology becomes more and more popular, a typical financialscam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.This Ponzi scheme deployed through smart contracts, also known as the smartPonzi scheme, has caused a lot of economic losses and negative impacts.Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely onbytecode features, opcode features, account features, and transaction behaviorfeatures of smart contracts, and the performance of identifying schemes isinsufficient. In this paper, we propose SourceP, a method to detect smart Ponzischemes on the Ethereum platform using pre-trained models and data flow, whichonly requires using the source code of smart contracts as features to explorethe possibility of detecting smart Ponzi schemes from another direction.SourceP reduces the difficulty of data acquisition and feature extraction ofexisting detection methods while increasing the interpretability of the model.Specifically, we first convert the source code of a smart contract into a dataflow graph and then introduce a pre-trained model based on learning coderepresentations to build a classification model to identify Ponzi schemes insmart contracts. The experimental results show that SourceP achieves 87.2\%recall and 90.7\% F-score for detecting smart Ponzi schemes within Ethereum'ssmart contract dataset, outperforming state-of-the-art methods in terms ofperformance and sustainability. We also demonstrate through additionalexperiments that pre-trained models and data flow play an importantcontribution to SourceP, as well as proving that SourceP has a goodgeneralization ability.</description><author>Pengcheng Lu, Liang Cai, Keting Yin</author><pubDate>Thu, 26 Oct 2023 17:06:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01665v3</guid></item><item><title>CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents</title><link>http://arxiv.org/abs/2310.17512v1</link><description>Large language models (LLMs) have been widely used as agents to completedifferent tasks, such as personal assistance or event planning. While most workhas focused on cooperation and collaboration between agents, little workexplores competition, another important mechanism that fosters the developmentof society and economy. In this paper, we seek to examine the competitionbehaviors in LLM-based agents. We first propose a general framework to studythe competition between agents. Then, we implement a practical competitiveenvironment using GPT-4 to simulate a virtual town with two types of agents,including restaurant agents and customer agents. Specifically, restaurantagents compete with each other to attract more customers, where the competitionfosters them to transform, such as cultivating new operating strategies. Theresults of our experiments reveal several interesting findings ranging fromsocial learning to Matthew Effect, which aligns well with existing sociologicaland economic theories. We believe that competition between agents deservesfurther investigation to help us understand society better. The code will bereleased soon.</description><author>Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, Xing Xie</author><pubDate>Thu, 26 Oct 2023 17:06:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17512v1</guid></item><item><title>Spontaneous Symmetry Breaking in Generative Diffusion Models</title><link>http://arxiv.org/abs/2305.19693v3</link><description>Generative diffusion models have recently emerged as a leading approach forgenerating high-dimensional data. In this paper, we show that the dynamics ofthese models exhibit a spontaneous symmetry breaking that divides thegenerative dynamics into two distinct phases: 1) A linear steady-state dynamicsaround a central fixed-point and 2) an attractor dynamics directed towards thedata manifold. These two "phases" are separated by the change in stability ofthe central fixed-point, with the resulting window of instability beingresponsible for the diversity of the generated samples. Using both theoreticaland empirical evidence, we show that an accurate simulation of the earlydynamics does not significantly contribute to the final generation, since earlyfluctuations are reverted to the central fixed point. To leverage this insight,we propose a Gaussian late initialization scheme, which significantly improvesmodel performance, achieving up to 3x FID improvements on fast samplers, whilealso increasing sample diversity (e.g., racial composition of generated CelebAimages). Our work offers a new way to understand the generative dynamics ofdiffusion models that has the potential to bring about higher performance andless biased fast-samplers.</description><author>Gabriel Raya, Luca Ambrogioni</author><pubDate>Thu, 26 Oct 2023 17:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19693v3</guid></item><item><title>Out-of-Distribution Detection in Time-Series Domain: A Novel Seasonal Ratio Scoring Approach</title><link>http://arxiv.org/abs/2207.04306v3</link><description>Safe deployment of time-series classifiers for real-world applications relieson the ability to detect the data which is not generated from the samedistribution as training data. This task is referred to as out-of-distribution(OOD) detection. We consider the novel problem of OOD detection for thetime-series domain. We discuss the unique challenges posed by time-series dataand explain why prior methods from the image domain will perform poorly.Motivated by these challenges, this paper proposes a novel {\em Seasonal RatioScoring (SRS)} approach. SRS consists of three key algorithmic steps. First,each input is decomposed into class-wise semantic component and remainder.Second, this decomposition is employed to estimate the class-wise conditionallikelihoods of the input and remainder using deep generative models. Theseasonal ratio score is computed from these estimates. Third, a thresholdinterval is identified from the in-distribution data to detect OOD examples.Experiments on diverse real-world benchmarks demonstrate that the SRS method iswell-suited for time-series OOD detection when compared to baseline methods.Open-source code for SRS method is provided athttps://github.com/tahabelkhouja/SRS</description><author>Taha Belkhouja, Yan Yan, Janardhan Rao Doppa</author><pubDate>Thu, 26 Oct 2023 16:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.04306v3</guid></item><item><title>Ponder: Point Cloud Pre-training via Neural Rendering</title><link>http://arxiv.org/abs/2301.00157v2</link><description>We propose a novel approach to self-supervised learning of point cloudrepresentations by differentiable neural rendering. Motivated by the fact thatinformative point cloud features should be able to encode rich geometry andappearance cues and render realistic images, we train a point-cloud encoderwithin a devised point-based neural renderer by comparing the rendered imageswith real images on massive RGB-D data. The learned point-cloud encoder can beeasily integrated into various downstream tasks, including not only high-leveltasks like 3D detection and segmentation, but low-level tasks like 3Dreconstruction and image synthesis. Extensive experiments on various tasksdemonstrate the superiority of our approach compared to existing pre-trainingmethods.</description><author>Di Huang, Sida Peng, Tong He, Honghui Yang, Xiaowei Zhou, Wanli Ouyang</author><pubDate>Thu, 26 Oct 2023 16:56:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00157v2</guid></item><item><title>Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving</title><link>http://arxiv.org/abs/2310.17504v1</link><description>Self-supervised image networks can be used to address complex 2D tasks (e.g.,semantic segmentation, object discovery) very efficiently and with little or nodownstream supervision. However, self-supervised 3D networks on lidar data donot perform as well for now. A few methods therefore propose to distillhigh-quality self-supervised 2D features into 3D networks. The most recent onesdoing so on autonomous driving data show promising results. Yet, a performancegap persists between these distilled features and fully-supervised ones. Inthis work, we revisit 2D-to-3D distillation. First, we propose, for semanticsegmentation, a simple approach that leads to a significant improvementcompared to prior 3D distillation methods. Second, we show that distillation inhigh capacity 3D networks is key to reach high quality 3D features. Thisactually allows us to significantly close the gap between unsuperviseddistilled 3D features and fully-supervised ones. Last, we show that ourhigh-quality distilled representations can also be used for open-vocabularysegmentation and background/foreground discovery.</description><author>Gilles Puy, Spyros Gidaris, Alexandre Boulch, Oriane SimÃ©oni, Corentin Sautier, Patrick PÃ©rez, Andrei Bursuc, Renaud Marlet</author><pubDate>Thu, 26 Oct 2023 16:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17504v1</guid></item><item><title>Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions</title><link>http://arxiv.org/abs/2310.17502v1</link><description>Customizing voice and speaking style in a speech synthesis system withintuitive and fine-grained controls is challenging, given that little data withappropriate labels is available. Furthermore, editing an existing human's voicealso comes with ethical concerns. In this paper, we propose a method togenerate artificial speaker embeddings that cannot be linked to a real humanwhile offering intuitive and fine-grained control over the voice and speakingstyle of the embeddings, without requiring any labels for speaker or style. Theartificial and controllable embeddings can be fed to a speech synthesis system,conditioned on embeddings of real humans during training, without sacrificingprivacy during inference.</description><author>Florian Lux, Pascal Tilli, Sarina Meyer, Ngoc Thang Vu</author><pubDate>Thu, 26 Oct 2023 16:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17502v1</guid></item><item><title>The IMS Toucan System for the Blizzard Challenge 2023</title><link>http://arxiv.org/abs/2310.17499v1</link><description>For our contribution to the Blizzard Challenge 2023, we improved on thesystem we submitted to the Blizzard Challenge 2021. Our approach entails arule-based text-to-phoneme processing system that includes rule-baseddisambiguation of homographs in the French language. It then transforms thephonemes to spectrograms as intermediate representations using a fast andefficient non-autoregressive synthesis architecture based on Conformer andGlow. A GAN based neural vocoder that combines recent state-of-the-artapproaches converts the spectrogram to the final wave. We carefully designedthe data processing, training, and inference procedures for the challenge data.Our system identifier is G. Open source code and demo are available.</description><author>Florian Lux, Julia Koch, Sarina Meyer, Thomas Bott, Nadja Schauffler, Pavel Denisov, Antje Schweitzer, Ngoc Thang Vu</author><pubDate>Thu, 26 Oct 2023 16:53:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17499v1</guid></item><item><title>CBD: A Certified Backdoor Detector Based on Local Dominant Probability</title><link>http://arxiv.org/abs/2310.17498v1</link><description>Backdoor attack is a common threat to deep neural networks. During testing,samples embedded with a backdoor trigger will be misclassified as anadversarial target by a backdoored model, while samples without the backdoortrigger will be correctly classified. In this paper, we present the firstcertified backdoor detector (CBD), which is based on a novel, adjustableconformal prediction scheme based on our proposed statistic local dominantprobability. For any classifier under inspection, CBD provides 1) a detectioninference, 2) the condition under which the attacks are guaranteed to bedetectable for the same classification domain, and 3) a probabilistic upperbound for the false positive rate. Our theoretical results show that attackswith triggers that are more resilient to test-time noise and have smallerperturbation magnitudes are more likely to be detected with guarantees.Moreover, we conduct extensive experiments on four benchmark datasetsconsidering various backdoor types, such as BadNet, CB, and Blend. CBD achievescomparable or even higher detection accuracy than state-of-the-art detectors,and it in addition provides detection certification. Notably, for backdoorattacks with random perturbation triggers bounded by $\ell_2\leq0.75$ whichachieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\%(84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection truepositive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, andTinyImageNet, respectively, with low false positive rates.</description><author>Zhen Xiang, Zidi Xiong, Bo Li</author><pubDate>Thu, 26 Oct 2023 16:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17498v1</guid></item><item><title>Adaptive whitening with fast gain modulation and slow synaptic plasticity</title><link>http://arxiv.org/abs/2308.13633v2</link><description>Neurons in early sensory areas rapidly adapt to changing sensory statistics,both by normalizing the variance of their individual responses and by reducingcorrelations between their responses. Together, these transformations may beviewed as an adaptive form of statistical whitening. Existing mechanisticmodels of adaptive whitening exclusively use either synaptic plasticity or gainmodulation as the biological substrate for adaptation; however, on their own,each of these models has significant limitations. In this work, we unify theseapproaches in a normative multi-timescale mechanistic model that adaptivelywhitens its responses with complementary computational roles for synapticplasticity and gain modulation. Gains are modified on a fast timescale to adaptto the current statistical context, whereas synapses are modified on a slowtimescale to match structural properties of the input statistics that areinvariant across contexts. Our model is derived from a novel multi-timescalewhitening objective that factorizes the inverse whitening matrix into basisvectors, which correspond to synaptic weights, and a diagonal matrix, whichcorresponds to neuronal gains. We test our model on synthetic and naturaldatasets and find that the synapses learn optimal configurations over longtimescales that enable adaptive whitening on short timescales using gainmodulation.</description><author>Lyndon R. Duong, Eero P. Simoncelli, Dmitri B. Chklovskii, David Lipshutz</author><pubDate>Thu, 26 Oct 2023 16:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13633v2</guid></item><item><title>Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach</title><link>http://arxiv.org/abs/2310.17496v1</link><description>In modern recommendation systems, the standard pipeline involves trainingmachine learning models on historical data to predict user behaviors andimprove recommendations continuously. However, these data training loops canintroduce interference in A/B tests, where data generated by control andtreatment algorithms, potentially with different distributions, are combined.To address these challenges, we introduce a novel approach called weightedtraining. This approach entails training a model to predict the probability ofeach data point appearing in either the treatment or control data andsubsequently applying weighted losses during model training. We demonstratethat this approach achieves the least variance among all estimators withoutcausing shifts in the training distributions. Through simulation studies, wedemonstrate the lower bias and variance of our approach compared to othermethods.</description><author>Nian Si</author><pubDate>Thu, 26 Oct 2023 16:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17496v1</guid></item><item><title>Multi-grained Hypergraph Interest Modeling for Conversational Recommendation</title><link>http://arxiv.org/abs/2305.04798v2</link><description>Conversational recommender system (CRS) interacts with users throughmulti-turn dialogues in natural language, which aims to provide high-qualityrecommendations for user's instant information need. Although great effortshave been made to develop effective CRS, most of them still focus on thecontextual information from the current dialogue, usually suffering from thedata scarcity issue. Therefore, we consider leveraging historical dialogue datato enrich the limited contexts of the current dialogue session. In this paper, we propose a novel multi-grained hypergraph interest modelingapproach to capture user interest beneath intricate historical data fromdifferent perspectives. As the core idea, we employ hypergraph to representcomplicated semantic relations underlying historical dialogues. In ourapproach, we first employ the hypergraph structure to model users' historicaldialogue sessions and form a session-based hypergraph, which capturescoarse-grained, session-level relations. Second, to alleviate the issue of datascarcity, we use an external knowledge graph and construct a knowledge-basedhypergraph considering fine-grained, entity-level semantics. We further conductmulti-grained hypergraph convolution on the two kinds of hypergraphs, andutilize the enhanced representations to develop interest-aware CRS. Extensiveexperiments on two benchmarks ReDial and TG-ReDial validate the effectivenessof our approach on both recommendation and conversation tasks. Code isavailable at: https://github.com/RUCAIBox/MHIM.</description><author>Chenzhan Shang, Yupeng Hou, Wayne Xin Zhao, Yaliang Li, Jing Zhang</author><pubDate>Thu, 26 Oct 2023 16:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04798v2</guid></item><item><title>A Hybrid Graph Network for Complex Activity Detection in Video</title><link>http://arxiv.org/abs/2310.17493v1</link><description>Interpretation and understanding of video presents a challenging computervision task in numerous fields - e.g. autonomous driving and sports analytics.Existing approaches to interpreting the actions taking place within a videoclip are based upon Temporal Action Localisation (TAL), which typicallyidentifies short-term actions. The emerging field of Complex Activity Detection(CompAD) extends this analysis to long-term activities, with a deeperunderstanding obtained by modelling the internal structure of a complexactivity taking place within the video. We address the CompAD problem using ahybrid graph neural network which combines attention applied to a graphencoding the local (short-term) dynamic scene with a temporal graph modellingthe overall long-duration activity. Our approach is as follows: i) Firstly, wepropose a novel feature extraction technique which, for each video snippet,generates spatiotemporal `tubes' for the active elements (`agents') in the(local) scene by detecting individual objects, tracking them and thenextracting 3D features from all the agent tubes as well as the overall scene.ii) Next, we construct a local scene graph where each node (representing eitheran agent tube or the scene) is connected to all other nodes. Attention is thenapplied to this graph to obtain an overall representation of the local dynamicscene. iii) Finally, all local scene graph representations are interconnectedvia a temporal graph, to estimate the complex activity class together with itsstart and end time. The proposed framework outperforms all previousstate-of-the-art methods on all three datasets including ActivityNet-1.3,Thumos-14, and ROAD.</description><author>Salman Khan, Izzeddin Teeti, Andrew Bradley, Mohamed Elhoseiny, Fabio Cuzzolin</author><pubDate>Thu, 26 Oct 2023 16:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17493v1</guid></item><item><title>Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach</title><link>http://arxiv.org/abs/2310.17492v1</link><description>The efficient deployment and fine-tuning of foundation models are pivotal incontemporary artificial intelligence. In this study, we present agroundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundationmodels, specifically designed to enhance local task performance on userequipment (UE). Central to our approach is the innovative Emulator-Adapterarchitecture, segmenting the foundation model into two cohesive modules. Thisdesign not only conserves computational resources but also ensures adaptabilityand fine-tuning efficiency for downstream tasks. Additionally, we introduce anadvanced resource allocation mechanism that is fine-tuned to the needs of theEmulator-Adapter structure in decentralized settings. To address the challengespresented by this system, we employ a hybrid multi-agent Deep ReinforcementLearning (DRL) strategy, adept at handling mixed discrete-continuous actionspaces, ensuring dynamic and optimal resource allocations. Our comprehensivesimulations and validations underscore the practical viability of our approach,demonstrating its robustness, efficiency, and scalability. Collectively, thiswork offers a fresh perspective on deploying foundation models and balancingcomputational efficiency with task proficiency.</description><author>Wenhan Yu, Terence Jie Chua, Jun Zhao</author><pubDate>Thu, 26 Oct 2023 16:47:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17492v1</guid></item><item><title>FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing</title><link>http://arxiv.org/abs/2310.17491v1</link><description>The emergence of foundation models, including language and vision models, hasreshaped AI's landscape, offering capabilities across various applications.Deploying and fine-tuning these large models, like GPT-3 and BERT, presentschallenges, especially in the current foundation model era. We introduceEmulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, weexpand this into federated learning as Federated PEAT (FedPEAT). FedPEAT usesadapters, emulators, and PEFT for federated model tuning, enhancing modelprivacy and memory efficiency. Adapters adjust pre-trained models, whileemulators give a compact representation of original models, addressing bothprivacy and efficiency. Adaptable to various neural networks, our approach alsouses deep reinforcement learning for hyper-parameter optimization. We testedFedPEAT in a unique scenario with a server participating in collaborativefederated tuning, showcasing its potential in tackling foundation modelchallenges.</description><author>Terence Jie Chua, Wenhan Yu, Jun Zhao, Kwok-Yan Lam</author><pubDate>Thu, 26 Oct 2023 16:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17491v1</guid></item><item><title>Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering</title><link>http://arxiv.org/abs/2310.17490v1</link><description>Large language models (LLMs) enable zero-shot approaches in open-domainquestion answering (ODQA), yet with limited advancements as the reader iscompared to the retriever. This study aims at the feasibility of a zero-shotreader that addresses the challenges of computational cost and the need forlabeled data. We find that LLMs are distracted due to irrelevant documents inthe retrieved set and the overconfidence of the generated answers when they areexploited as zero-shot readers. To tackle these problems, we mitigate theimpact of such documents via Distraction-aware Answer Selection (DAS) with anegation-based instruction and score adjustment for proper answer selection.Experimental results show that our approach successfully handles distractionacross diverse scenarios, enhancing the performance of zero-shot readers.Furthermore, unlike supervised readers struggling with unseen data, zero-shotreaders demonstrate outstanding transferability without any training.</description><author>Sukmin Cho, Jeong yeon Seo, Soyeong Jeong, Jong C. Park</author><pubDate>Thu, 26 Oct 2023 16:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17490v1</guid></item><item><title>Bias in Evaluation Processes: An Optimization-Based Model</title><link>http://arxiv.org/abs/2310.17489v1</link><description>Biases with respect to socially-salient attributes of individuals have beenwell documented in evaluation processes used in settings such as admissions andhiring. We view such an evaluation process as a transformation of adistribution of the true utility of an individual for a task to an observeddistribution and model it as a solution to a loss minimization problem subjectto an information constraint. Our model has two parameters that have beenidentified as factors leading to biases: the resource-information trade-offparameter in the information constraint and the risk-averseness parameter inthe loss function. We characterize the distributions that arise from our modeland study the effect of the parameters on the observed distribution. Theoutputs of our model enrich the class of distributions that can be used tocapture variation across groups in the observed evaluations. We empiricallyvalidate our model by fitting real-world datasets and use it to study theeffect of interventions in a downstream selection task. These resultscontribute to an understanding of the emergence of bias in evaluation processesand provide tools to guide the deployment of interventions to mitigate biases.</description><author>L. Elisa Celis, Amit Kumar, Anay Mehrotra, Nisheeth K. Vishnoi</author><pubDate>Thu, 26 Oct 2023 16:45:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17489v1</guid></item><item><title>LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation</title><link>http://arxiv.org/abs/2310.17488v1</link><description>This paper presents LightLM, a lightweight Transformer-based language modelfor generative recommendation. While Transformer-based generative modeling hasgained importance in various AI sub-fields such as NLP and vision, generativerecommendation is still in its infancy due to its unique demand on personalizedgenerative modeling. Existing works on generative recommendation often useNLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which areheavy-weight and are not specifically designed for recommendation tasks.LightLM tackles the issue by introducing a light-weight deep and narrowTransformer architecture, which is specifically tailored for direct generationof recommendation items. This structure is especially apt for straightforwardgenerative recommendation and stems from the observation that language modeldoes not have to be too wide for this task, as the input predominantly consistsof short tokens that are well-suited for the model's capacity. We also showthat our devised user and item ID indexing methods, i.e., SpectralCollaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), enablesthe deep and narrow Transformer architecture to outperform large-scale languagemodels for recommendation. Besides, to address the hallucination problem ofgenerating items as output, we propose the constrained generation process forgenerative recommenders. Experiments on real-world datasets show that LightLMoutperforms various competitive baselines in terms of both recommendationaccuracy and efficiency. The code can be found athttps://github.com/dongyuanjushi/LightLM.</description><author>Kai Mei, Yongfeng Zhang</author><pubDate>Thu, 26 Oct 2023 16:44:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17488v1</guid></item><item><title>Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach</title><link>http://arxiv.org/abs/2310.17485v1</link><description>Collaborative vehicle routing occurs when carriers collaborate throughsharing their transportation requests and performing transportation requests onbehalf of each other. This achieves economies of scale, thus reducing cost,greenhouse gas emissions and road congestion. But which carrier should partnerwith whom, and how much should each carrier be compensated? Traditional gametheoretic solution concepts are expensive to calculate as the characteristicfunction scales exponentially with the number of agents. This would requiresolving the vehicle routing problem (NP-hard) an exponential number of times.We therefore propose to model this problem as a coalitional bargaining gamesolved using deep multi-agent reinforcement learning, where - crucially -agents are not given access to the characteristic function. Instead, weimplicitly reason about the characteristic function; thus, when deployed inproduction, we only need to evaluate the expensive post-collaboration vehiclerouting problem once. Our contribution is that we are the first to considerboth the route allocation problem and gain sharing problem simultaneously -without access to the expensive characteristic function. Through decentralisedmachine learning, our agents bargain with each other and agree to outcomes thatcorrelate well with the Shapley value - a fair profit allocation mechanism.Importantly, we are able to achieve a reduction in run-time of 88%.</description><author>Stephen Mak, Liming Xu, Tim Pearce, Michael Ostroumov, Alexandra Brintrup</author><pubDate>Thu, 26 Oct 2023 16:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17485v1</guid></item><item><title>Sequential Memory with Temporal Predictive Coding</title><link>http://arxiv.org/abs/2305.11982v2</link><description>Forming accurate memory of sequential stimuli is a fundamental function ofbiological agents. However, the computational mechanism underlying sequentialmemory in the brain remains unclear. Inspired by neuroscience theories andrecent successes in applying predictive coding (PC) to \emph{static} memorytasks, in this work we propose a novel PC-based model for \emph{sequential}memory, called \emph{temporal predictive coding} (tPC). We show that our tPCmodels can memorize and retrieve sequential inputs accurately with abiologically plausible neural implementation. Importantly, our analytical studyreveals that tPC can be viewed as a classical Asymmetric Hopfield Network (AHN)with an implicit statistical whitening process, which leads to more stableperformance in sequential memory tasks of structured inputs. Moreover, we findthat tPC exhibits properties consistent with behavioral observations andtheories in neuroscience, thereby strengthening its biological relevance. Ourwork establishes a possible computational mechanism underlying sequentialmemory in the brain that can also be theoretically interpreted using existingmemory model frameworks.</description><author>Mufeng Tang, Helen Barron, Rafal Bogacz</author><pubDate>Thu, 26 Oct 2023 16:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11982v2</guid></item><item><title>Editing Common Sense in Transformers</title><link>http://arxiv.org/abs/2305.14956v3</link><description>Editing model parameters directly in Transformers makes updating open-sourcetransformer-based models possible without re-training (Meng et al., 2023).However, these editing methods have only been evaluated on statements aboutencyclopedic knowledge with a single correct answer. Commonsense knowledge withmultiple correct answers, e.g., an apple can be green or red but nottransparent, has not been studied but is as essential for enhancingtransformers' reliability and usefulness. In this paper, we investigate whethercommonsense judgments are causally associated with localized, editableparameters in Transformers, and we provide an affirmative answer. We find thatdirectly applying the MEMIT editing algorithm results in sub-par performanceand improve it for the commonsense domain by varying edit tokens and improvingthe layer selection strategy, i.e., $MEMIT_{CSK}$. GPT-2 Large and XL modelsedited using $MEMIT_{CSK}$ outperform best-fine-tuned baselines by 10.97% and10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novelevaluation dataset, PROBE SET, that contains unaffected and affectedneighborhoods, affected paraphrases, and affected reasoning challenges.$MEMIT_{CSK}$ performs well across the metrics while fine-tuning baselines showsignificant trade-offs between unaffected and affected metrics. These resultssuggest a compelling future direction for incorporating feedback about commonsense into Transformers through direct model editing.</description><author>Anshita Gupta, Debanjan Mondal, Akshay Krishna Sheshadri, Wenlong Zhao, Xiang Lorraine Li, Sarah Wiegreffe, Niket Tandon</author><pubDate>Thu, 26 Oct 2023 16:38:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14956v3</guid></item><item><title>Improving Neural Additive Models with Bayesian Principles</title><link>http://arxiv.org/abs/2305.16905v2</link><description>Neural additive models (NAMs) can improve the interpretability of deep neuralnetworks by handling input features in separate additive sub-networks. However,they lack inherent mechanisms that provide calibrated uncertainties and enableselection of relevant features and interactions. Approaching NAMs from aBayesian perspective, we enhance them in three primary ways, namely by a)providing credible intervals for the individual additive sub-networks; b)estimating the marginal likelihood to perform an implicit selection of featuresvia an empirical Bayes procedure; and c) enabling a ranking of feature pairs ascandidates for second-order interaction in fine-tuned models. In particular, wedevelop Laplace-approximated NAMs (LA-NAMs), which show improved empiricalperformance on tabular datasets and challenging real-world medical tasks.</description><author>Kouroche Bouchiat, Alexander Immer, Hugo YÃ¨che, Gunnar RÃ¤tsch, Vincent Fortuin</author><pubDate>Thu, 26 Oct 2023 16:37:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16905v2</guid></item></channel></rss>