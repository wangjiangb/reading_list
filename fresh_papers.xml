<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 28 Nov 2024 13:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Textured Gaussians for Enhanced 3D Scene Appearance Modeling</title><link>http://arxiv.org/abs/2411.18625v1</link><description>3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3Dreconstruction and rendering technique due to its high-quality results and fasttraining and rendering time. However, pixels covered by the same Gaussian arealways shaded in the same color up to a Gaussian falloff scaling factor.Furthermore, the finest geometric detail any individual Gaussian can representis a simple ellipsoid. These properties of 3DGS greatly limit the expressivityof individual Gaussian primitives. To address these issues, we draw inspirationfrom texture and alpha mapping in traditional graphics and integrate it with3DGS. Specifically, we propose a new generalized Gaussian appearancerepresentation that augments each Gaussian with alpha~(A), RGB, or RGBA texturemaps to model spatially varying color and opacity across the extent of eachGaussian. As such, each Gaussian can represent a richer set of texture patternsand geometric structures, instead of just a single color and ellipsoid as innaive Gaussian Splatting. Surprisingly, we found that the expressivity ofGaussians can be greatly improved by using alpha-only texture maps, and furtheraugmenting Gaussians with RGB texture maps achieves the highest expressivity.We validate our method on a wide variety of standard benchmark datasets and ourown custom captures at both the object and scene levels. We demonstrate imagequality improvements over existing methods while using a similar or lowernumber of Gaussians.</description><author>Brian Chao, Hung-Yu Tseng, Lorenzo Porzi, Chen Gao, Tuotuo Li, Qinbo Li, Ayush Saraf, Jia-Bin Huang, Johannes Kopf, Gordon Wetzstein, Changil Kim</author><pubDate>Wed, 27 Nov 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18625v1</guid></item><item><title>GeneMAN: Generalizable Single-Image 3D Human Reconstruction from Multi-Source Human Data</title><link>http://arxiv.org/abs/2411.18624v1</link><description>Given a single in-the-wild human photo, it remains a challenging task toreconstruct a high-fidelity 3D human model. Existing methods face difficultiesincluding a) the varying body proportions captured by in-the-wild human images;b) diverse personal belongings within the shot; and c) ambiguities in humanpostures and inconsistency in human textures. In addition, the scarcity ofhigh-quality human data intensifies the challenge. To address these problems,we propose a Generalizable image-to-3D huMAN reconstruction framework, dubbedGeneMAN, building upon a comprehensive multi-source collection of high-qualityhuman data, including 3D scans, multi-view videos, single photos, and ourgenerated synthetic human data. GeneMAN encompasses three key modules. 1)Without relying on parametric human models (e.g., SMPL), GeneMAN first trains ahuman-specific text-to-image diffusion model and a view-conditioned diffusionmodel, serving as GeneMAN 2D human prior and 3D human prior for reconstruction,respectively. 2) With the help of the pretrained human prior models, theGeometry Initialization-&amp;-Sculpting pipeline is leveraged to recoverhigh-quality 3D human geometry given a single image. 3) To achievehigh-fidelity 3D human textures, GeneMAN employs the Multi-Space TextureRefinement pipeline, consecutively refining textures in the latent and thepixel spaces. Extensive experimental results demonstrate that GeneMAN couldgenerate high-quality 3D human models from a single image input, outperformingprior state-of-the-art methods. Notably, GeneMAN could reveal much bettergeneralizability in dealing with in-the-wild images, often yieldinghigh-quality 3D human models in natural poses with common items, regardless ofthe body proportions in the input images.</description><author>Wentao Wang, Hang Ye, Fangzhou Hong, Xue Yang, Jianfu Zhang, Yizhou Wang, Ziwei Liu, Liang Pan</author><pubDate>Wed, 27 Nov 2024 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18624v1</guid></item><item><title>Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation</title><link>http://arxiv.org/abs/2411.18623v1</link><description>3D geometric information is essential for manipulation tasks, as robots needto perceive the 3D environment, reason about spatial relationships, andinteract with intricate spatial configurations. Recent research hasincreasingly focused on the explicit extraction of 3D features, while stillfacing challenges such as the lack of large-scale robotic 3D data and thepotential loss of spatial geometry. To address these limitations, we proposethe Lift3D framework, which progressively enhances 2D foundation models withimplicit and explicit 3D robotic representations to construct a robust 3Dmanipulation policy. Specifically, we first design a task-aware maskedautoencoder that masks task-relevant affordance patches and reconstructs depthinformation, enhancing the 2D foundation model's implicit 3D roboticrepresentation. After self-supervised fine-tuning, we introduce a 2Dmodel-lifting strategy that establishes a positional mapping between the input3D points and the positional embeddings of the 2D model. Based on the mapping,Lift3D utilizes the 2D foundation model to directly encode point cloud data,leveraging large-scale pretrained knowledge to construct explicit 3D roboticrepresentations while minimizing spatial information loss. In experiments,Lift3D consistently outperforms previous state-of-the-art methods acrossseveral simulation benchmarks and real-world scenarios.</description><author>Yueru Jia, Jiaming Liu, Sixiang Chen, Chenyang Gu, Zhilue Wang, Longzan Luo, Lily Lee, Pengwei Wang, Zhongyuan Wang, Renrui Zhang, Shanghang Zhang</author><pubDate>Wed, 27 Nov 2024 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18623v1</guid></item><item><title>Leveraging Semi-Supervised Learning to Enhance Data Mining for Image Classification under Limited Labeled Data</title><link>http://arxiv.org/abs/2411.18622v1</link><description>In the 21st-century information age, with the development of big datatechnology, effectively extracting valuable information from massive data hasbecome a key issue. Traditional data mining methods are inadequate when facedwith large-scale, high-dimensional and complex data. Especially when labeleddata is scarce, their performance is greatly limited. This study optimizes datamining algorithms by introducing semi-supervised learning methods, aiming toimprove the algorithm's ability to utilize unlabeled data, thereby achievingmore accurate data analysis and pattern recognition under limited labeled dataconditions. Specifically, we adopt a self-training method and combine it with aconvolutional neural network (CNN) for image feature extraction andclassification, and continuously improve the model prediction performancethrough an iterative process. The experimental results demonstrate that theproposed method significantly outperforms traditional machine learningtechniques such as Support Vector Machine (SVM), XGBoost, and Multi-LayerPerceptron (MLP) on the CIFAR-10 image classification dataset. Notableimprovements were observed in key performance metrics, including accuracy,recall, and F1 score. Furthermore, the robustness and noise-resistancecapabilities of the semi-supervised CNN model were validated throughexperiments under varying noise levels, confirming its practical applicabilityin real-world scenarios.</description><author>Aoran Shen, Minghao Dai, Jiacheng Hu, Yingbin Liang, Shiru Wang, Junliang Du</author><pubDate>Wed, 27 Nov 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18622v1</guid></item><item><title>XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</title><link>http://arxiv.org/abs/2411.15100v2</link><description>The applications of LLM Agents are becoming increasingly complex and diverse,leading to a high demand for structured outputs that can be parsed into code,structured function calls, and embodied agent commands. These developmentsbring significant demands for structured generation in LLM inference.Context-free grammar is a flexible approach to enable structured generation viaconstrained decoding. However, executing context-free grammar requires goingthrough several stack states over all tokens in vocabulary during runtime,bringing non-negligible overhead for structured generation. In this paper, wepropose XGrammar, a flexible and efficient structure generation engine forlarge language models. XGrammar accelerates context-free grammar execution bydividing the vocabulary into context-independent tokens that can be precheckedand context-dependent tokens that need to be interpreted during runtime. Wefurther build transformations to expand the grammar context and reduce thenumber of context-independent tokens. Additionally, we build an efficientpersistent stack to accelerate the context-dependent token checks. Finally, weco-design the grammar engine with LLM inference engine to overlap grammarcomputation with GPU executions. Evaluation results show that XGrammar canachieve up to 100x speedup over existing solutions. Combined with an LLMinference engine, it can generate near-zero overhead structure generation inend-to-end low-LLM serving.</description><author>Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen</author><pubDate>Wed, 27 Nov 2024 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15100v2</guid></item><item><title>Cross-modal Information Flow in Multimodal Large Language Models</title><link>http://arxiv.org/abs/2411.18620v1</link><description>The recent advancements in auto-regressive multimodal large language models(MLLMs) have demonstrated promising progress for vision-language tasks. Whilethere exists a variety of studies investigating the processing of linguisticinformation within large language models, little is currently known about theinner working mechanism of MLLMs and how linguistic and visual informationinteract within these models. In this study, we aim to fill this gap byexamining the information flow between different modalities -- language andvision -- in MLLMs, focusing on visual question answering. Specifically, givenan image-question pair as input, we investigate where in the model and how thevisual and linguistic information are combined to generate the finalprediction. Conducting experiments with a series of models from the LLaVAseries, we find that there are two distinct stages in the process ofintegration of the two modalities. In the lower layers, the model firsttransfers the more general visual features of the whole image into therepresentations of (linguistic) question tokens. In the middle layers, it onceagain transfers visual information about specific objects relevant to thequestion to the respective token positions of the question. Finally, in thehigher layers, the resulting multimodal representation is propagated to thelast position of the input sequence for the final prediction. Overall, ourfindings provide a new and comprehensive perspective on the spatial andfunctional aspects of image and language processing in the MLLMs, therebyfacilitating future research into multimodal information localization andediting.</description><author>Zhi Zhang, Srishti Yadav, Fengze Han, Ekaterina Shutova</author><pubDate>Wed, 27 Nov 2024 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18620v1</guid></item><item><title>Diffusion Self-Distillation for Zero-Shot Customized Image Generation</title><link>http://arxiv.org/abs/2411.18616v1</link><description>Text-to-image diffusion models produce impressive results but are frustratingtools for artists who desire fine-grained control. For example, a common usecase is to create images of a specific instance in novel contexts, i.e.,"identity-preserving generation". This setting, along with many other tasks(e.g., relighting), is a natural fit for image+text-conditional generativemodels. However, there is insufficient high-quality paired data to train such amodel directly. We propose Diffusion Self-Distillation, a method for using apre-trained text-to-image model to generate its own dataset fortext-conditioned image-to-image tasks. We first leverage a text-to-imagediffusion model's in-context generation ability to create grids of images andcurate a large paired dataset with the help of a Visual-Language Model. We thenfine-tune the text-to-image model into a text+image-to-image model using thecurated paired dataset. We demonstrate that Diffusion Self-Distillationoutperforms existing zero-shot methods and is competitive with per-instancetuning techniques on a wide range of identity-preservation generation tasks,without requiring test-time optimization.</description><author>Shengqu Cai, Eric Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, Gordon Wetzstein</author><pubDate>Wed, 27 Nov 2024 18:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18616v1</guid></item><item><title>DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring</title><link>http://arxiv.org/abs/2411.07976v5</link><description>Coronary artery disease (CAD), one of the leading causes of mortalityworldwide, necessitates effective risk assessment strategies, with coronaryartery calcium (CAC) scoring via computed tomography (CT) being a key methodfor prevention. Traditional methods, primarily based on UNET architecturesimplemented on pre-built models, face challenges like the scarcity of annotatedCT scans containing CAC and imbalanced datasets, leading to reduced performancein segmentation and scoring tasks. In this study, we address these limitationsby incorporating the self-supervised learning (SSL) technique of DINO(self-distillation with no labels), which trains without requiring CAC-specificannotations, enhancing its robustness in generating distinct features. TheDINO-LG model, which leverages label guidance to focus on calcified areas,achieves significant improvements, with a sensitivity of 89% and specificity of90% for detecting CAC-containing CT slices, compared to the standard DINOmodel's sensitivity of 79% and specificity of 77%. Additionally, false-negativeand false-positive rates are reduced by 49% and 59%, respectively, instillinggreater confidence in clinicians when ruling out calcification in low-riskpatients and minimizing unnecessary imaging reviews by radiologists. Further,CAC scoring and segmentation tasks are conducted using a basic UNETarchitecture, applied specifically to CT slices identified by the DINO-LG modelas containing calcified areas. This targeted approach enhances CAC scoringaccuracy by feeding the UNET model with relevant slices, significantlyimproving diagnostic precision, reducing both false positives and falsenegatives, and ultimately lowering overall healthcare costs by minimizingunnecessary tests and treatments, presenting a valuable advancement in CAD riskassessment.</description><author>Mahmut S. Gokmen, Caner Ozcan, Moneera N. Haque, Cody Bumgardner</author><pubDate>Wed, 27 Nov 2024 18:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07976v5</guid></item><item><title>Proactive Gradient Conflict Mitigation in Multi-Task Learning: A Sparse Training Perspective</title><link>http://arxiv.org/abs/2411.18615v1</link><description>Advancing towards generalist agents necessitates the concurrent processing ofmultiple tasks using a unified model, thereby underscoring the growingsignificance of simultaneous model training on multiple downstream tasks. Acommon issue in multi-task learning is the occurrence of gradient conflict,which leads to potential competition among different tasks during jointtraining. This competition often results in improvements in one task at theexpense of deterioration in another. Although several optimization methods havebeen developed to address this issue by manipulating task gradients for bettertask balancing, they cannot decrease the incidence of gradient conflict. Inthis paper, we systematically investigate the occurrence of gradient conflictacross different methods and propose a strategy to reduce such conflictsthrough sparse training (ST), wherein only a portion of the model's parametersare updated during training while keeping the rest unchanged. Our extensiveexperiments demonstrate that ST effectively mitigates conflicting gradients andleads to superior performance. Furthermore, ST can be easily integrated withgradient manipulation techniques, thus enhancing their effectiveness.</description><author>Zhi Zhang, Jiayi Shen, Congfeng Cao, Gaole Dai, Shiji Zhou, Qizhe Zhang, Shanghang Zhang, Ekaterina Shutova</author><pubDate>Wed, 27 Nov 2024 18:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18615v1</guid></item><item><title>CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models</title><link>http://arxiv.org/abs/2411.18613v1</link><description>We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocularvideo. CAT4D leverages a multi-view video diffusion model trained on a diversecombination of datasets to enable novel view synthesis at any specified cameraposes and timestamps. Combined with a novel sampling approach, this model cantransform a single monocular video into a multi-view video, enabling robust 4Dreconstruction via optimization of a deformable 3D Gaussian representation. Wedemonstrate competitive performance on novel view synthesis and dynamic scenereconstruction benchmarks, and highlight the creative capabilities for 4D scenegeneration from real or generated videos. See our project page for results andinteractive demos: \url{cat-4d.github.io}.</description><author>Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan T. Barron, Aleksander Holynski</author><pubDate>Wed, 27 Nov 2024 18:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18613v1</guid></item><item><title>Robust Offline Reinforcement Learning with Linearly Structured $f$-Divergence Regularization</title><link>http://arxiv.org/abs/2411.18612v1</link><description>The Distributionally Robust Markov Decision Process (DRMDP) is a popularframework for addressing dynamics shift in reinforcement learning by learningpolicies robust to the worst-case transition dynamics within a constrained set.However, solving its dual optimization oracle poses significant challenges,limiting theoretical analysis and computational efficiency. The recentlyproposed Robust Regularized Markov Decision Process (RRMDP) replaces theuncertainty set constraint with a regularization term on the value function,offering improved scalability and theoretical insights. Yet, existing RRMDPmethods rely on unstructured regularization, often leading to overlyconservative policies by considering transitions that are unrealistic. Toaddress these issues, we propose a novel framework, the $d$-rectangular linearrobust regularized Markov decision process ($d$-RRMDP), which introduces alinear latent structure into both transition kernels and regularization. Forthe offline RL setting, where an agent learns robust policies from apre-collected dataset in the nominal environment, we develop a family ofalgorithms, Robust Regularized Pessimistic Value Iteration (R2PVI), employinglinear function approximation and $f$-divergence based regularization terms ontransition kernels. We provide instance-dependent upper bounds on thesuboptimality gap of R2PVI policies, showing these bounds depend on how wellthe dataset covers state-action spaces visited by the optimal robust policyunder robustly admissible transitions. This term is further shown to befundamental to $d$-RRMDPs via information-theoretic lower bounds. Finally,numerical experiments validate that R2PVI learns robust policies and iscomputationally more efficient than methods for constrained DRMDPs.</description><author>Cheng Tang, Zhishuai Liu, Pan Xu</author><pubDate>Wed, 27 Nov 2024 18:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18612v1</guid></item><item><title>Task Arithmetic Through The Lens Of One-Shot Federated Learning</title><link>http://arxiv.org/abs/2411.18607v1</link><description>Task Arithmetic is a model merging technique that enables the combination ofmultiple models' capabilities into a single model through simple arithmetic inthe weight space, without the need for additional fine-tuning or access to theoriginal training data. However, the factors that determine the success of TaskArithmetic remain unclear. In this paper, we examine Task Arithmetic formulti-task learning by framing it as a one-shot Federated Learning problem. Wedemonstrate that Task Arithmetic is mathematically equivalent to the commonlyused algorithm in Federated Learning, called Federated Averaging (FedAvg). Byleveraging well-established theoretical results from FedAvg, we identify twokey factors that impact the performance of Task Arithmetic: data heterogeneityand training heterogeneity. To mitigate these challenges, we adapt severalalgorithms from Federated Learning to improve the effectiveness of TaskArithmetic. Our experiments demonstrate that applying these algorithms canoften significantly boost performance of the merged model compared to theoriginal Task Arithmetic approach. This work bridges Task Arithmetic andFederated Learning, offering new theoretical perspectives on Task Arithmeticand improved practical methodologies for model merging.</description><author>Zhixu Tao, Ian Mason, Sanjeev Kulkarni, Xavier Boix</author><pubDate>Wed, 27 Nov 2024 18:53:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18607v1</guid></item><item><title>Evaluating and Improving the Effectiveness of Synthetic Chest X-Rays for Medical Image Analysis</title><link>http://arxiv.org/abs/2411.18602v1</link><description>Purpose: To explore best-practice approaches for generating synthetic chestX-ray images and augmenting medical imaging datasets to optimize theperformance of deep learning models in downstream tasks like classification andsegmentation. Materials and Methods: We utilized a latent diffusion model tocondition the generation of synthetic chest X-rays on text prompts and/orsegmentation masks. We explored methods like using a proxy model and usingradiologist feedback to improve the quality of synthetic data. These syntheticimages were then generated from relevant disease information or geometricallytransformed segmentation masks and added to ground truth training set imagesfrom the CheXpert, CANDID-PTX, SIIM, and RSNA Pneumonia datasets to measureimprovements in classification and segmentation model performance on the testsets. F1 and Dice scores were used to evaluate classification and segmentationrespectively. One-tailed t-tests with Bonferroni correction assessed thestatistical significance of performance improvements with synthetic data.Results: Across all experiments, the synthetic data we generated resulted in amaximum mean classification F1 score improvement of 0.150453 (CI:0.099108-0.201798; P=0.0031) compared to using only real data. Forsegmentation, the maximum Dice score improvement was 0.14575 (CI:0.108267-0.183233; P=0.0064). Conclusion: Best practices for generatingsynthetic chest X-ray images for downstream tasks include conditioning onsingle-disease labels or geometrically transformed segmentation masks, as wellas potentially using proxy modeling for fine-tuning such generations.</description><author>Eva Prakash, Jeya Maria Jose Valanarasu, Zhihong Chen, Eduardo Pontes Reis, Andrew Johnston, Anuj Pareek, Christian Bluethgen, Sergios Gatidis, Cameron Olsen, Akshay Chaudhari, Andrew Ng, Curtis Langlotz</author><pubDate>Wed, 27 Nov 2024 18:47:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18602v1</guid></item><item><title>Structured light with a million light planes per second</title><link>http://arxiv.org/abs/2411.18597v1</link><description>We introduce a structured light system that captures full-frame depth atrates of a thousand frames per second, four times faster than the previousstate of the art. Our key innovation to this end is the design of anacousto-optic light scanning device that can scan light planes at rates up totwo million planes per second. We combine this device with an event camera forstructured light, using the sparse events triggered on the camera as we sweep alight plane on the scene for depth triangulation. In contrast to prior work,where light scanning is the bottleneck towards faster structured lightoperation, our light scanning device is three orders of magnitude faster thanthe event camera's full-frame bandwidth, thus allowing us to take fulladvantage of the event camera's fast operation. To surpass this bandwidth, weadditionally demonstrate adaptive scanning of only regions of interest, atspeeds an order of magnitude faster than the theoretical full-frame limit forevent cameras.</description><author>Dhawal Sirikonda, Praneeth Chakravarthula, Ioannis Gkioulekas, Adithya Pediredla</author><pubDate>Wed, 27 Nov 2024 18:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18597v1</guid></item><item><title>Data Readiness for AI: A 360-Degree Survey</title><link>http://arxiv.org/abs/2404.05779v2</link><description>Artificial Intelligence (AI) applications critically depend on data. Poorquality data produces inaccurate and ineffective AI models that may lead toincorrect or unsafe use. Evaluation of data readiness is a crucial step inimproving the quality and appropriateness of data usage for AI. R&amp;D effortshave been spent on improving data quality. However, standardized metrics forevaluating data readiness for use in AI training are still evolving. In thisstudy, we perform a comprehensive survey of metrics used to verify datareadiness for AI training. This survey examines more than 140 papers publishedby ACM Digital Library, IEEE Xplore, journals such as Nature, Springer, andScience Direct, and online articles published by prominent AI experts. Thissurvey aims to propose a taxonomy of data readiness for AI (DRAI) metrics forstructured and unstructured datasets. We anticipate that this taxonomy willlead to new standards for DRAI metrics that will be used for enhancing thequality, accuracy, and fairness of AI training and inference.</description><author>Kaveen Hiniduma, Suren Byna, Jean Luca Bez</author><pubDate>Wed, 27 Nov 2024 18:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05779v2</guid></item><item><title>Biomolecular Analysis of Soil Samples and Rock Imagery for Tracing Evidence of Life Using a Mobile Robot</title><link>http://arxiv.org/abs/2411.18594v1</link><description>The search for evidence of past life on Mars presents a tremendous challengethat requires the usage of very advanced robotic technologies to overcome it.Current digital microscopic imagers and spectrometers used for astrobiologicalexamination suffer from limitations such as insufficient resolution, narrowdetection range, and lack of portability. To overcome these challenges, thisresearch study presents modifications to the Phoenix rover to expand itscapability for detecting biosignatures on Mars. This paper examines themodifications implemented on the Phoenix rover to enhance its capability todetect a broader spectrum of biosignatures. One of the notable improvementscomprises the integration of advanced digital microscopic imagers andspectrometers, enabling high-resolution examination of soil samples.Additionally, the mechanical components of the device have been reinforced toenhance maneuverability and optimize subsurface sampling capabilities.Empirical investigations have demonstrated that Phoenix has the capability tonavigate diverse geological environments and procure samples for the purpose ofbiomolecular analysis. The biomolecular instrumentation and hybrid analyticalmethods showcased in this study demonstrate considerable potential for futureastrobiology missions on Mars. The potential for enhancing the system lies inthe possibility of broadening the range of detectable biomarkers andbiosignatures.</description><author>Shah Md Ahasan Siddique, Ragib Tahshin Rinath, Shakil Mosharrof, Syed Tanjib Mahmud, Sakib Ahmed</author><pubDate>Wed, 27 Nov 2024 18:38:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18594v1</guid></item><item><title>Hierarchical Information Flow for Generalized Efficient Image Restoration</title><link>http://arxiv.org/abs/2411.18588v1</link><description>While vision transformers show promise in numerous image restoration (IR)tasks, the challenge remains in efficiently generalizing and scaling up a modelfor multiple IR tasks. To strike a balance between efficiency and modelcapacity for a generalized transformer-based IR method, we propose ahierarchical information flow mechanism for image restoration, dubbed Hi-IR,which progressively propagates information among pixels in a bottom-up manner.Hi-IR constructs a hierarchical information tree representing the degradedimage across three levels. Each level encapsulates different types ofinformation, with higher levels encompassing broader objects and concepts andlower levels focusing on local details. Moreover, the hierarchical treearchitecture removes long-range self-attention, improves the computationalefficiency and memory utilization, thus preparing it for effective modelscaling. Based on that, we explore model scaling to improve our method'scapabilities, which is expected to positively impact IR in large-scale trainingsettings. Extensive experimental results show that Hi-IR achievesstate-of-the-art performance in seven common image restoration tasks, affirmingits effectiveness and generalizability.</description><author>Yawei Li, Bin Ren, Jingyun Liang, Rakesh Ranjan, Mengyuan Liu, Nicu Sebe, Ming-Hsuan Yang, Luca Benini</author><pubDate>Wed, 27 Nov 2024 18:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18588v1</guid></item><item><title>Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2411.18583v1</link><description>This research presents and compares multiple approaches to automate thegeneration of literature reviews using several Natural Language Processing(NLP) techniques and retrieval-augmented generation (RAG) with a Large LanguageModel (LLM). The ever-increasing number of research articles provides a hugechallenge for manual literature review. It has resulted in an increased demandfor automation. Developing a system capable of automatically generating theliterature reviews from only the PDF files as input is the primary objective ofthis research work. The effectiveness of several Natural Language Processing(NLP) strategies, such as the frequency-based method (spaCy), the transformermodel (Simple T5), and retrieval-augmented generation (RAG) with Large LanguageModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDRdataset is chosen for this research experiment and three distinct techniquesare utilized to implement three different systems for auto-generating theliterature reviews. The ROUGE scores are used for the evaluation of all threesystems. Based on the evaluation, the Large Language Model GPT-3.5-turboachieved the highest ROUGE-1 score, 0.364. The transformer model comes insecond place and spaCy is at the last position. Finally, a graphical userinterface is created for the best system based on the large language model.</description><author>Nurshat Fateh Ali, Md. Mahdi Mohtasim, Shakil Mosharrof, T. Gopi Krishna</author><pubDate>Wed, 27 Nov 2024 18:27:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18583v1</guid></item><item><title>A Suite for Acoustic Language Model Evaluation</title><link>http://arxiv.org/abs/2409.07437v2</link><description>Speech language models have recently demonstrated great potential asuniversal speech processing systems. Such models have the ability to model therich acoustic information existing in audio signals, beyond spoken content,such as emotion, background noise, etc. Despite this, evaluation benchmarkswhich evaluate awareness to a wide range of acoustic aspects, are lacking. Tohelp bridge this gap, we introduce SALMon, a novel evaluation suiteencompassing background noise, emotion, speaker identity and room impulseresponse. The proposed benchmarks both evaluate the consistency of theinspected element and how much it matches the spoken text. We follow amodelling based approach, measuring whether a model gives correct sampleshigher scores than incorrect ones. This approach makes the benchmark fast tocompute even for large models. We evaluated several speech language models onSALMon, thus highlighting the strengths and weaknesses of each evaluatedmethod. We make the code and data publicly available athttps://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .</description><author>Gallil Maimon, Amit Roth, Yossi Adi</author><pubDate>Wed, 27 Nov 2024 18:24:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07437v2</guid></item><item><title>Surveying the space of descriptions of a composite system with machine learning</title><link>http://arxiv.org/abs/2411.18579v1</link><description>Multivariate information theory provides a general and principled frameworkfor understanding how the components of a complex system are connected.Existing analyses are coarse in nature -- built up from characterizations ofdiscrete subsystems -- and can be computationally prohibitive. In this work, wepropose to study the continuous space of possible descriptions of a compositesystem as a window into its organizational structure. A description consists ofspecific information conveyed about each of the components, and the space ofpossible descriptions is equivalent to the space of lossy compression schemesof the components. We introduce a machine learning framework to optimizedescriptions that extremize key information theoretic quantities used tocharacterize organization, such as total correlation and O-information. Throughcase studies on spin systems, Sudoku boards, and letter sequences from naturallanguage, we identify extremal descriptions that reveal how system-widevariation emerges from individual components. By integrating machine learninginto a fine-grained information theoretic analysis of composite randomvariables, our framework opens a new avenues for probing the structure ofreal-world complex systems.</description><author>Kieran A. Murphy, Yujing Zhang, Dani S. Bassett</author><pubDate>Wed, 27 Nov 2024 18:24:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18579v1</guid></item><item><title>Pruning Deep Convolutional Neural Network Using Conditional Mutual Information</title><link>http://arxiv.org/abs/2411.18578v1</link><description>Convolutional Neural Networks (CNNs) achieve high performance in imageclassification tasks but are challenging to deploy on resource-limited hardwaredue to their large model sizes. To address this issue, we leverage MutualInformation, a metric that provides valuable insights into how deep learningmodels retain and process information through measuring the shared informationbetween input features or output labels and network layers. In this study, wepropose a structured filter-pruning approach for CNNs that identifies andselectively retains the most informative features in each layer. Our approachsuccessively evaluates each layer by ranking the importance of its feature mapsbased on Conditional Mutual Information (CMI) values, computed using amatrix-based Renyi {\alpha}-order entropy numerical method. We propose severalformulations of CMI to capture correlation among features across differentlayers. We then develop various strategies to determine the cutoff point forCMI values to prune unimportant features. This approach allows parallel pruningin both forward and backward directions and significantly reduces model sizewhile preserving accuracy. Tested on the VGG16 architecture with the CIFAR-10dataset, the proposed method reduces the number of filters by more than athird, with only a 0.32% drop in test accuracy.</description><author>Tien Vu-Van, Dat Du Thanh, Nguyen Ho, Mai Vu</author><pubDate>Wed, 27 Nov 2024 18:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18578v1</guid></item><item><title>On Importance of Code-Mixed Embeddings for Hate Speech Identification</title><link>http://arxiv.org/abs/2411.18577v1</link><description>Code-mixing is the practice of using two or more languages in a singlesentence, which often occurs in multilingual communities such as India wherepeople commonly speak multiple languages. Classic NLP tools, trained onmonolingual data, face challenges when dealing with code-mixed data. Extractingmeaningful information from sentences containing multiple languages becomesdifficult, particularly in tasks like hate speech detection, due to linguisticvariation, cultural nuances, and data sparsity. To address this, we aim toanalyze the significance of code-mixed embeddings and evaluate the performanceof BERT and HingBERT models (trained on a Hindi-English corpus) in hate speechdetection. Our study demonstrates that HingBERT models, benefiting fromtraining on the extensive Hindi-English dataset L3Cube-HingCorpus, outperformBERT models when tested on hate speech text datasets. We also found thatcode-mixed Hing-FastText performs better than standard English FastText andvanilla BERT models.</description><author>Shruti Jagdale, Omkar Khade, Gauri Takalikar, Mihir Inamdar, Raviraj Joshi</author><pubDate>Wed, 27 Nov 2024 18:23:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18577v1</guid></item><item><title>Functional relevance based on the continuous Shapley value</title><link>http://arxiv.org/abs/2411.18575v1</link><description>The presence of Artificial Intelligence (AI) in our society is increasing,which brings with it the need to understand the behaviour of AI mechanisms,including machine learning predictive algorithms fed with tabular data, text,or images, among other types of data. This work focuses on interpretability ofpredictive models based on functional data. Designing interpretability methodsfor functional data models implies working with a set of features whose size isinfinite. In the context of scalar on function regression, we propose aninterpretability method based on the Shapley value for continuous games, amathematical formulation that allows to fairly distribute a global payoff amonga continuous set players. The method is illustrated through a set ofexperiments with simulated and real data sets. The open source Python packageShapleyFDA is also presented.</description><author>Pedro Delicado, Cristian Pachón-García</author><pubDate>Wed, 27 Nov 2024 18:20:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18575v1</guid></item><item><title>Exploring Depth Information for Detecting Manipulated Face Videos</title><link>http://arxiv.org/abs/2411.18572v1</link><description>Face manipulation detection has been receiving a lot of attention for thereliability and security of the face images/videos. Recent studies focus onusing auxiliary information or prior knowledge to capture robust manipulationtraces, which are shown to be promising. As one of the important face features,the face depth map, which has shown to be effective in other areas such as facerecognition or face detection, is unfortunately paid little attention to inliterature for face manipulation detection. In this paper, we explore thepossibility of incorporating the face depth map as auxiliary information forrobust face manipulation detection. To this end, we first propose a Face DepthMap Transformer (FDMT) to estimate the face depth map patch by patch from anRGB face image, which is able to capture the local depth anomaly created due tomanipulation. The estimated face depth map is then considered as auxiliaryinformation to be integrated with the backbone features using a Multi-headDepth Attention (MDA) mechanism that is newly designed. We also propose anRGB-Depth Inconsistency Attention (RDIA) module to effectively capture theinter-frame inconsistency for multi-frame input. Various experimentsdemonstrate the advantage of our proposed method for face manipulationdetection.</description><author>Haoyue Wang, Sheng Li, Ji He, Zhenxing Qian, Xinpeng Zhang, Shaolin Fan</author><pubDate>Wed, 27 Nov 2024 18:16:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18572v1</guid></item><item><title>Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning</title><link>http://arxiv.org/abs/2411.18571v1</link><description>Large Language Models (LLMs) have demonstrated remarkable multilingualcapabilities, yet challenges persist in adapting these models for low-resourcelanguages. In this study, we investigate the effects of Low-Rank Adaptation(LoRA) Parameter-Efficient Fine-Tuning (PEFT) on multilingual Gemma models forMarathi, a language with limited resources. Using a translated Alpaca datasetwith 52,000 instruction-response pairs, our findings reveal that whileevaluation metrics often show a performance decline post-fine-tuning, manualassessments frequently suggest that the fine-tuned models outperform theiroriginal counterparts. The observations indicate improvements in targetlanguage generation capabilities but a reduction in reasoning abilitiesfollowing language adaptation. These results underscore the need for improvedevaluation methodologies and the creation of high-quality native datasets toaccurately assess language-specific model performance in low-resource settings.</description><author>Omkar Khade, Shruti Jagdale, Abhishek Phaltankar, Gauri Takalikar, Raviraj Joshi</author><pubDate>Wed, 27 Nov 2024 18:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18571v1</guid></item><item><title>A Flexible Defense Against the Winner's Curse</title><link>http://arxiv.org/abs/2411.18569v1</link><description>Across science and policy, decision-makers often need to draw conclusionsabout the best candidate among competing alternatives. For instance,researchers may seek to infer the effectiveness of the most successfultreatment or determine which demographic group benefits most from a specifictreatment. Similarly, in machine learning, practitioners are often interestedin the population performance of the model that performs best empirically.However, cherry-picking the best candidate leads to the winner's curse: theobserved performance for the winner is biased upwards, rendering conclusionsbased on standard measures of uncertainty invalid. We introduce the zoomcorrection, a novel approach for valid inference on the winner. Our method isflexible: it can be employed in both parametric and nonparametric settings, canhandle arbitrary dependencies between candidates, and automatically adapts tothe level of selection bias. The method easily extends to important relatedproblems, such as inference on the top k winners, inference on the value andidentity of the population winner, and inference on "near-winners."</description><author>Tijana Zrnic, William Fithian</author><pubDate>Wed, 27 Nov 2024 18:14:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18569v1</guid></item><item><title>CanFields: Consolidating 4D Dynamic Shapes from Raw Scans</title><link>http://arxiv.org/abs/2406.18582v2</link><description>We introduce Canonical Consolidation Fields (CanFields), a new method forreconstructing a time series of independently captured 3D scans into a single,coherent deforming shape. This 4D representation enables continuous refinementacross both space and time. Unlike prior methods that often over-smooth thegeometry or produce topological and geometric artifacts, CanFields effectivelylearns geometry and deformation in an unsupervised way by incorporating twogeometric priors. First, we introduce a dynamic consolidator module thatadjusts the input and assigns confidence scores, balancing the learning of thecanonical shape and its deformations. Second, we use low-frequency velocityfields to guide deformation while preserving fine details in canonical shapesthrough high-frequency bias. We validate the robustness and accuracy ofCanFields on diverse raw scans, demonstrating its superior performance evenwith missing regions, sparse frames, and noise. Code is available in thesupplementary materials and will be released publicly upon acceptance.</description><author>Miaowei Wang, Changjian Li, Amir Vaxman</author><pubDate>Wed, 27 Nov 2024 18:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18582v2</guid></item><item><title>Learning to Project for Cross-Task Knowledge Distillation</title><link>http://arxiv.org/abs/2403.14494v2</link><description>Traditional knowledge distillation (KD) relies on a proficient teachertrained on the target task, which is not always available. In this setting,cross-task distillation can be used, enabling the use of any teacher modeltrained on a different task. However, many KD methods prove ineffective whenapplied to this cross-task setting. To address this limitation, we propose asimple modification: the use of an inverted projection. We show that thisdrop-in replacement for a standard projector is effective by learning todisregard any task-specific features which might degrade the student'sperformance. We find that this simple modification is sufficient for extendingmany KD methods to the cross-task setting, where the teacher and student taskscan be very different. In doing so, we obtain up to a 1.9% improvement in thecross-task setting compared to the traditional projection, at no additionalcost. Our method can obtain significant performance improvements (up to 7%)when using even a randomly-initialised teacher on various tasks such as depthestimation, image translation, and semantic segmentation, despite the lack ofany learned knowledge to transfer. To provide conceptual and analyticalinsights into this result, we show that using an inverted projection allows thedistillation loss to be decomposed into a knowledge transfer and a spectralregularisation component. Through this analysis we are additionally able topropose a novel regularisation loss that allows teacher-free distillation,enabling performance improvements of up to 8.57% on ImageNet with no additionaltraining costs.</description><author>Dylan Auty, Roy Miles, Benedikt Kolbeinsson, Krystian Mikolajczyk</author><pubDate>Wed, 27 Nov 2024 18:12:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14494v2</guid></item><item><title>cedar: Optimized and Unified Machine Learning Input Data Pipelines</title><link>http://arxiv.org/abs/2401.08895v4</link><description>The input data pipeline is an essential component of each machine learning(ML) training job. It is responsible for reading massive amounts of trainingdata, processing batches of samples using complex transformations, and loadingthem onto training nodes at low latency and high throughput. Performant inputdata systems are becoming increasingly critical, driven by skyrocketing datavolumes and training throughput demands. Unfortunately, current input datasystems cannot fully leverage key performance optimizations, resulting inhugely inefficient infrastructures that require significant resources - orworse - underutilize expensive accelerators. To address these demands, we present cedar, an optimized and unifiedprogramming framework for ML input data pipelines. cedar allows users to defineinput data pipelines using composable operators that support arbitrary MLframeworks and libraries. cedar introduces an extensible optimizer thatsystematically applies a complex combination of optimizations (e.g.,offloading, caching, prefetching, fusion, and reordering). It orchestratesprocessing across a customizable set of local and distributed compute resourcesin order to improve processing performance and efficiency, all without userinput. Across eight pipelines, cedar improves performance by up to 1.87x to10.65x compared to state-of-the-art input data systems.</description><author>Mark Zhao, Emanuel Adamiak, Christos Kozyrakis</author><pubDate>Wed, 27 Nov 2024 18:05:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08895v4</guid></item><item><title>A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2411.18564v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities acrossvarious tasks. However, LLMs often struggle with spatial reasoning which is oneessential part of reasoning and inference and requires understanding complexrelationships between objects in space. This paper proposes a novelneural-symbolic framework that enhances LLMs' spatial reasoning abilities. Weevaluate our approach on two benchmark datasets: StepGame and SparQA,implementing three distinct strategies: (1) ASP (Answer Set Programming)-basedsymbolic reasoning, (2) LLM + ASP pipeline using DSPy, and (3) Fact + Logicalrules. Our experiments demonstrate significant improvements over the baselineprompting methods, with accuracy increases of 40-50% on StepGame} dataset and3-13% on the more complex SparQA dataset. The "LLM + ASP" pipeline achievesparticularly strong results on the tasks of Finding Relations (FR) and FindingBlock (FB) questions, though performance varies across different questiontypes. The impressive results suggest that while neural-symbolic approachesoffer promising directions for enhancing spatial reasoning in LLMs, theireffectiveness depends heavily on the specific task characteristics andimplementation strategies. We propose an integrated, simple yet effective setof strategies using a neural-symbolic pipeline to boost spatial reasoningabilities in LLMs. This pipeline and its strategies demonstrate strong andbroader applicability to other reasoning domains in LLMs, such as temporalreasoning, deductive inference etc.</description><author>Rong Wang, Kun Sun, Jonas Kuhn</author><pubDate>Wed, 27 Nov 2024 18:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18564v1</guid></item><item><title>DexDiffuser: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation</title><link>http://arxiv.org/abs/2411.18562v1</link><description>Dexterous manipulation with contact-rich interactions is crucial for advancedrobotics. While recent diffusion-based planning approaches show promise forsimpler manipulation tasks, they often produce unrealistic ghost states (e.g.,the object automatically moves without hand contact) or lack adaptability whenhandling complex sequential interactions. In this work, we introduceDexDiffuser, an interaction-aware diffusion planning framework for adaptivedexterous manipulation. DexDiffuser models joint state-action dynamics througha dual-phase diffusion process which consists of pre-interaction contactalignment and post-contact goal-directed control, enabling goal-adaptivegeneralizable dexterous manipulation. Additionally, we incorporate dynamicsmodel-based dual guidance and leverage large language models for automatedguidance function generation, enhancing generalizability for physicalinteractions and facilitating diverse goal adaptation through language cues.Experiments on physical interaction tasks such as door opening, pen and blockre-orientation, and hammer striking demonstrate DexDiffuser's effectiveness ongoals outside training distributions, achieving over twice the average successrate (59.2% vs. 29.5%) compared to existing methods. Our framework achieves70.0% success on 30-degree door opening, 40.0% and 36.7% on pen and blockhalf-side re-orientation respectively, and 46.7% on hammer nail half drive,highlighting its robustness and flexibility in contact-rich manipulation.</description><author>Zhixuan Liang, Yao Mu, Yixiao Wang, Fei Ni, Tianxing Chen, Wenqi Shao, Wei Zhan, Masayoshi Tomizuka, Ping Luo, Mingyu Ding</author><pubDate>Wed, 27 Nov 2024 18:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18562v1</guid></item><item><title>Retrofitting (Large) Language Models with Dynamic Tokenization</title><link>http://arxiv.org/abs/2411.18553v1</link><description>Current language models (LMs) use a fixed, static subword tokenizer. Thischoice, often taken for granted, typically results in degraded efficiency andcapabilities in languages other than English, and makes it challenging to applyLMs to new domains or languages. To address these issues, we proposeretrofitting LMs with dynamic tokenization: a way to dynamically decide ontoken boundaries based on the input text. For encoder-style models, weintroduce a subword-merging algorithm inspired by byte-pair encoding (BPE), butat a batch level. We merge frequent subword sequences in a batch, then apply apretrained embedding-prediction hypernetwork to compute the token embeddingson-the-fly. When applied with word-level boundaries, this on average reducestoken sequence lengths by &gt;20% across 14 languages on XNLI with XLM-R whiledegrading its task performance by less than 2%. For decoder-style models, weapply dynamic tokenization in two ways: 1) for prefilling, maintainingperformance of Mistral-7B almost completely with up to 40% sequence reduction -relative to the word-level; and 2) via an approximate nearest neighbor index,achieving fast generation with a one million token vocabulary, demonstratingscalability to even larger, dynamic vocabularies. Overall, our findings showthat dynamic tokenization substantially improves inference speed and promotesfairness across languages, making a leap towards overcoming the limitations ofstatic tokenization and enabling more equitable and adaptable LMs.</description><author>Darius Feher, Benjamin Minixhofer, Ivan Vulić</author><pubDate>Wed, 27 Nov 2024 17:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18553v1</guid></item><item><title>FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion</title><link>http://arxiv.org/abs/2411.18552v1</link><description>Diffusion models are proficient at generating high-quality images. They arehowever effective only when operating at the resolution used during training.Inference at a scaled resolution leads to repetitive patterns and structuraldistortions. Retraining at higher resolutions quickly becomes prohibitive.Thus, methods enabling pre-existing diffusion models to operate at flexibletest-time resolutions are highly desirable. Previous works suffer from frequentartifacts and often introduce large latency overheads. We propose two simplemodules that combine to solve these issues. We introduce a Frequency Modulation(FM) module that leverages the Fourier domain to improve the global structureconsistency, and an Attention Modulation (AM) module which improves theconsistency of local texture patterns, a problem largely ignored in priorworks. Our method, coined Fam diffusion, can seamlessly integrate into anylatent diffusion model and requires no additional training. Extensivequalitative results highlight the effectiveness of our method in addressingstructural and local artifacts, while quantitative results showstate-of-the-art performance. Also, our method avoids redundant inferencetricks for improved consistency such as patch-based or progressive generation,leading to negligible latency overheads.</description><author>Haosen Yang, Adrian Bulat, Isma Hadji, Hai X. Pham, Xiatian Zhu, Georgios Tzimiropoulos, Brais Martinez</author><pubDate>Wed, 27 Nov 2024 17:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18552v1</guid></item><item><title>Concentration of Cumulative Reward in Markov Decision Processes</title><link>http://arxiv.org/abs/2411.18551v1</link><description>In this paper, we investigate the concentration properties of cumulativerewards in Markov Decision Processes (MDPs), focusing on both asymptotic andnon-asymptotic settings. We introduce a unified approach to characterize rewardconcentration in MDPs, covering both infinite-horizon settings (i.e., averageand discounted reward frameworks) and finite-horizon setting. Our asymptoticresults include the law of large numbers, the central limit theorem, and thelaw of iterated logarithms, while our non-asymptotic bounds includeAzuma-Hoeffding-type inequalities and a non-asymptotic version of the law ofiterated logarithms. Additionally, we explore two key implications of ourresults. First, we analyze the sample path behavior of the difference inrewards between any two stationary policies. Second, we show that twoalternative definitions of regret for learning policies proposed in theliterature are rate-equivalent. Our proof techniques rely on a novel martingaledecomposition of cumulative rewards, properties of the solution to the policyevaluation fixed-point equation, and both asymptotic and non-asymptoticconcentration results for martingale difference sequences.</description><author>Borna Sayedana, Peter E. Caines, Aditya Mahajan</author><pubDate>Wed, 27 Nov 2024 17:51:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18551v1</guid></item><item><title>PhyCAGE: Physically Plausible Compositional 3D Asset Generation from a Single Image</title><link>http://arxiv.org/abs/2411.18548v1</link><description>We present PhyCAGE, the first approach for physically plausible compositional3D asset generation from a single image. Given an input image, we firstgenerate consistent multi-view images for components of the assets. Theseimages are then fitted with 3D Gaussian Splatting representations. To ensurethat the Gaussians representing objects are physically compatible with eachother, we introduce a Physical Simulation-Enhanced Score Distillation Sampling(PSE-SDS) technique to further optimize the positions of the Gaussians. It isachieved by setting the gradient of the SDS loss as the initial velocity of thephysical simulation, allowing the simulator to act as a physics-guidedoptimizer that progressively corrects the Gaussians' positions to a physicallycompatible state. Experimental results demonstrate that the proposed method cangenerate physically plausible compositional 3D assets given a single image.</description><author>Han Yan, Mingrui Zhang, Yang Li, Chao Ma, Pan Ji</author><pubDate>Wed, 27 Nov 2024 17:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18548v1</guid></item><item><title>Markov Equivalence and Consistency in Differentiable Structure Learning</title><link>http://arxiv.org/abs/2410.06163v3</link><description>Existing approaches to differentiable structure learning of directed acyclicgraphs (DAGs) rely on strong identifiability assumptions in order to guaranteethat global minimizers of the acyclicity-constrained optimization problemidentifies the true DAG. Moreover, it has been observed empirically that theoptimizer may exploit undesirable artifacts in the loss function. We explainand remedy these issues by studying the behavior of differentiableacyclicity-constrained programs under general likelihoods with multiple globalminimizers. By carefully regularizing the likelihood, it is possible toidentify the sparsest model in the Markov equivalence class, even in theabsence of an identifiable parametrization. We first study the Gaussian case indetail, showing how proper regularization of the likelihood defines a scorethat identifies the sparsest model. Assuming faithfulness, it also recovers theMarkov equivalence class. These results are then generalized to general modelsand likelihoods, where the same claims hold. These theoretical results arevalidated empirically, showing how this can be done using standardgradient-based optimizers, thus paving the way for differentiable structurelearning under general models and losses.</description><author>Chang Deng, Kevin Bello, Pradeep Ravikumar, Bryon Aragam</author><pubDate>Wed, 27 Nov 2024 17:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06163v3</guid></item><item><title>A Novel Word Pair-based Gaussian Sentence Similarity Algorithm For Bengali Extractive Text Summarization</title><link>http://arxiv.org/abs/2411.17181v2</link><description>Extractive Text Summarization is the process of selecting the mostrepresentative parts of a larger text without losing any key information.Recent attempts at extractive text summarization in Bengali, either relied onstatistical techniques like TF-IDF or used naive sentence similarity measureslike the word averaging technique. All of these strategies suffer fromexpressing semantic relationships correctly. Here, we propose a novel Wordpair-based Gaussian Sentence Similarity (WGSS) algorithm for calculating thesemantic relation between two sentences. WGSS takes the geometric means ofindividual Gaussian similarity values of word embedding vectors to get thesemantic relationship between sentences. It compares two sentences on aword-to-word basis which rectifies the sentence representation problem faced bythe word averaging method. The summarization process extracts key sentences bygrouping semantically similar sentences into clusters using the SpectralClustering algorithm. After clustering, we use TF-IDF ranking to pick the bestsentence from each cluster. The proposed method is validated using fourdifferent datasets, and it outperformed other recent models by 43.2% on averageROUGE scores (ranging from 2.5% to 95.4%). It is also experimented on otherlow-resource languages i.e. Turkish, Marathi, and Hindi language, where we findthat the proposed method performs as similar as Bengali for these languages. Inaddition, a new high-quality Bengali dataset is curated which contains 250articles and a pair of summaries for each of them. We believe this research isa crucial addition to Bengali Natural Language Processing (NLP) research and itcan easily be extended into other low-resource languages. We made theimplementation of the proposed model and data public onhttps://github.com/FMOpee/WGSS.</description><author>Fahim Morshed, Md. Abdur Rahman, Sumon Ahmed</author><pubDate>Wed, 27 Nov 2024 17:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17181v2</guid></item><item><title>DataVisT5: A Pre-trained Language Model for Jointly Understanding Text and Data Visualization</title><link>http://arxiv.org/abs/2408.07401v2</link><description>Data visualization (DV) is the fundamental and premise tool to improve theefficiency in conveying the insights behind the big data, which has been widelyaccepted in existing data-driven world. Task automation in DV, such asconverting natural language queries to visualizations (i.e., text-to-vis),generating explanations from visualizations (i.e., vis-to-text), answeringDV-related questions in free form (i.e. FeVisQA), and explicating tabular data(i.e., table-to-text), is vital for advancing the field. Despite theirpotential, the application of pre-trained language models (PLMs) like T5 andBERT in DV has been limited by high costs and challenges in handlingcross-modal information, leading to few studies on PLMs for DV. We introduceDataVisT5, a novel PLM tailored for DV that enhances the T5 architecturethrough a hybrid objective pre-training and multi-task fine-tuning strategy,integrating text and DV datasets to effectively interpret cross-modalsemantics. Extensive evaluations on public datasets show that DataVisT5consistently outperforms current state-of-the-art models on various DV-relatedtasks. We anticipate that DataVisT5 will not only inspire further research onvertical PLMs but also expand the range of applications for PLMs.</description><author>Zhuoyue Wan, Yuanfeng Song, Shuaimin Li, Chen Jason Zhang, Raymond Chi-Wing Wong</author><pubDate>Wed, 27 Nov 2024 17:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07401v2</guid></item><item><title>AdaVLN: Towards Visual Language Navigation in Continuous Indoor Environments with Moving Humans</title><link>http://arxiv.org/abs/2411.18539v1</link><description>Visual Language Navigation is a task that challenges robots to navigate inrealistic environments based on natural language instructions. While previousresearch has largely focused on static settings, real-world navigation mustoften contend with dynamic human obstacles. Hence, we propose an extension tothe task, termed Adaptive Visual Language Navigation (AdaVLN), which seeks tonarrow this gap. AdaVLN requires robots to navigate complex 3D indoorenvironments populated with dynamically moving human obstacles, adding a layerof complexity to navigation tasks that mimic the real-world. To supportexploration of this task, we also present AdaVLN simulator and AdaR2R datasets.The AdaVLN simulator enables easy inclusion of fully animated human modelsdirectly into common datasets like Matterport3D. We also introduce a"freeze-time" mechanism for both the navigation task and simulator, whichpauses world state updates during agent inference, enabling fair comparisonsand experimental reproducibility across different hardware. We evaluate severalbaseline models on this task, analyze the unique challenges introduced byAdaVLN, and demonstrate its potential to bridge the sim-to-real gap in VLNresearch.</description><author>Dillon Loh, Tomasz Bednarz, Xinxing Xia, Frank Guan</author><pubDate>Wed, 27 Nov 2024 17:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18539v1</guid></item><item><title>Utilizing the Mean Teacher with Supcontrast Loss for Wafer Pattern Recognition</title><link>http://arxiv.org/abs/2411.18533v1</link><description>The patterns on wafer maps play a crucial role in helping engineers identifythe causes of production issues during semiconductor manufacturing. In order toreduce costs and improve accuracy, automation technology is essential, andrecent developments in deep learning have led to impressive results in wafermap pattern recognition. In this context, inspired by the effectiveness ofsemi-supervised learning and contrastive learning methods, we introduce aninnovative approach that integrates the Mean Teacher framework with thesupervised contrastive learning loss for enhanced wafer map patternrecognition. Our methodology not only addresses the nuances of wafer patternsbut also tackles challenges arising from limited labeled data. To furtherrefine the process, we address data imbalance in the wafer dataset by employingSMOTE and under-sampling techniques. We conduct a comprehensive analysis of ourproposed method and demonstrate its effectiveness through experiments usingreal-world dataset WM811K obtained from semiconductor manufacturers. Comparedto the baseline method, our method has achieved 5.46%, 6.68%, 5.42%, and 4.53%improvements in Accuracy, Precision, Recall, and F1 score, respectively.</description><author>Qiyu Wei, Xun Xu, Zeng Zeng, Xulei Yang</author><pubDate>Wed, 27 Nov 2024 17:24:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18533v1</guid></item><item><title>Emergence of Self-Identity in AI: A Mathematical Framework and Empirical Study with Generative Large Language Models</title><link>http://arxiv.org/abs/2411.18530v1</link><description>This paper introduces a mathematical framework for defining and quantifyingself-identity in artificial intelligence (AI) systems, addressing a criticalgap in the theoretical foundations of artificial consciousness. While existingapproaches to artificial self-awareness often rely on heuristic implementationsor philosophical abstractions, we present a formal framework grounded in metricspace theory, measure theory, and functional analysis. Our framework positsthat self-identity emerges from two mathematically quantifiable conditions: theexistence of a connected continuum of memories $C \subseteq \mathcal{M}$ in ametric space $(\mathcal{M}, d_{\mathcal{M}})$, and a continuous mapping $I:\mathcal{M} \to \mathcal{S}$ that maintains consistent self-recognition acrossthis continuum, where $(\mathcal{S}, d_{\mathcal{S}})$ represents the metricspace of possible self-identities. To validate this theoretical framework, weconducted empirical experiments using the Llama 3.2 1B model, employingLow-Rank Adaptation (LoRA) for efficient fine-tuning. The model was trained ona synthetic dataset containing temporally structured memories, designed tocapture the complexity of coherent self-identity formation. Our evaluationmetrics included quantitative measures of self-awareness, response consistency,and linguistic precision. The experimental results demonstrate substantialimprovements in measurable self-awareness metrics, with the primaryself-awareness score increasing from 0.276 to 0.801. This enables thestructured creation of AI systems with validated self-identity features. Theimplications of our study are immediately relevant to the fields of humanoidrobotics and autonomous systems.</description><author>Minhyeok Lee</author><pubDate>Wed, 27 Nov 2024 17:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18530v1</guid></item><item><title>NeuroAI for AI Safety</title><link>http://arxiv.org/abs/2411.18526v1</link><description>As AI systems become increasingly powerful, the need for safe AI has becomemore pressing. Humans are an attractive model for AI safety: as the only knownagents capable of general intelligence, they perform robustly even underconditions that deviate significantly from prior experiences, explore the worldsafely, understand pragmatics, and can cooperate to meet their intrinsic goals.Intelligence, when coupled with cooperation and safety mechanisms, can drivesustained progress and well-being. These properties are a function of thearchitecture of the brain and the learning algorithms it implements.Neuroscience may thus hold important keys to technical AI safety that arecurrently underexplored and underutilized. In this roadmap, we highlight andcritically evaluate several paths toward AI safety inspired by neuroscience:emulating the brain's representations, information processing, andarchitecture; building robust sensory and motor systems from imitating braindata and bodies; fine-tuning AI systems on brain data; advancinginterpretability using neuroscience methods; and scaling upcognitively-inspired architectures. We make several concrete recommendationsfor how neuroscience can positively impact AI safety.</description><author>Patrick Mineault, Niccolò Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias</author><pubDate>Wed, 27 Nov 2024 17:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18526v1</guid></item><item><title>Perturbation Ontology based Graph Attention Networks</title><link>http://arxiv.org/abs/2411.18520v1</link><description>In recent years, graph representation learning has undergone a paradigmshift, driven by the emergence and proliferation of graph neural networks(GNNs) and their heterogeneous counterparts. Heterogeneous GNNs have shownremarkable success in extracting low-dimensional embeddings from complex graphsthat encompass diverse entity types and relationships. While meta-path-basedtechniques have long been recognized for their ability to capture semanticaffinities among nodes, their dependence on manual specification poses asignificant limitation. In contrast, matrix-focused methods accelerateprocessing by utilizing structural cues but often overlook contextual richness.In this paper, we challenge the current paradigm by introducing ontology as afundamental semantic primitive within complex graphs. Our goal is to integratethe strengths of both matrix-centric and meta-path-based approaches into aunified framework. We propose perturbation Ontology-based Graph AttentionNetworks (POGAT), a novel methodology that combines ontology subgraphs with anadvanced self-supervised learning paradigm to achieve a deep contextualunderstanding. The core innovation of POGAT lies in our enhanced homogeneousperturbing scheme designed to generate rigorous negative samples, encouragingthe model to explore minimal contextual features more thoroughly. Throughextensive empirical evaluations, we demonstrate that POGAT significantlyoutperforms state-of-the-art baselines, achieving a groundbreaking improvementof up to 10.78\% in F1-score for the critical task of link prediction and12.01\% in Micro-F1 for the critical task of node classification.</description><author>Yichen Wang, Jie Wang, Fulin Wang, Xiang Li, Hao Yin, Bhiksha Raj</author><pubDate>Wed, 27 Nov 2024 17:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18520v1</guid></item><item><title>Lusifer: LLM-based User SImulated Feedback Environment for online Recommender systems</title><link>http://arxiv.org/abs/2405.13362v2</link><description>Training reinforcement learning-based recommender systems is often hinderedby the lack of dynamic and realistic user interactions. To address thislimitation, we introduce Lusifer, a novel environment leveraging Large LanguageModels (LLMs) to generate simulated user feedback. Lusifer synthesizes userprofiles and interaction histories to simulate responses and behaviors towardrecommended items, with profiles updated after each rating to reflect evolvinguser characteristics. Utilizing the MovieLens dataset as a proof of concept, welimited our implementation to the last 40 interactions for each user,representing approximately 39% and 22% of the training sets, to focus on recentuser behavior. For consistency and to gain insights into the performance oftraditional methods with limited data, we implemented baseline approaches usingthe same data subset. Our results demonstrate that Lusifer accurately emulatesuser behavior and preferences, even with reduced training data having an RMSEof 1.3 across various test sets. This paper presents Lusifer's operationalpipeline, including prompt generation and iterative user profile updates, andcompares its performance against baseline methods. The findings validateLusifer's ability to produce realistic dynamic feedback and suggest that itoffers a scalable and adjustable framework for user simulation in onlinereinforcement learning recommender systems for future studies, particularlywhen training data is limited.</description><author>Danial Ebrat, Eli Paradalis, Luis Rueda</author><pubDate>Wed, 27 Nov 2024 17:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13362v2</guid></item><item><title>Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data</title><link>http://arxiv.org/abs/2407.14985v4</link><description>The impressive capabilities of large language models (LLMs) have sparkeddebate over whether these models genuinely generalize to unseen tasks orpredominantly rely on memorizing vast amounts of pretraining data. To explorethis issue, we introduce an extended concept of memorization, distributionalmemorization, which measures the correlation between the LLM outputprobabilities and the pretraining data frequency. To effectively capturetask-specific pretraining data frequency, we propose a novel task-gram languagemodel, which is built by counting the co-occurrence of semantically related$n$-gram pairs from task inputs and outputs in the pretraining corpus. Usingthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:machine translation, factual question answering, world knowledge understanding,and math reasoning. Our findings reveal varying levels of memorization, withthe strongest effect observed in factual question answering. Furthermore, whilemodel performance improves across all tasks as LLM size increases, only factualquestion answering shows an increase in memorization, whereas machinetranslation and reasoning tasks exhibit greater generalization, producing morenovel outputs. This study demonstrates that memorization plays a larger role insimpler, knowledge-intensive tasks, while generalization is the key for harder,reasoning-based tasks, providing a scalable method for analyzing largepretraining corpora in greater depth. We also show the practical implicationsof our analysis through a novel prompt optimization algorithm.</description><author>Xinyi Wang, Antonis Antoniades, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, William Yang Wang</author><pubDate>Wed, 27 Nov 2024 17:05:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14985v4</guid></item><item><title>Factorized Visual Tokenization and Generation</title><link>http://arxiv.org/abs/2411.16681v2</link><description>Visual tokenizers are fundamental to image generation. They convert visualdata into discrete tokens, enabling transformer-based models to excel at imagegeneration. Despite their success, VQ-based tokenizers like VQGAN facesignificant limitations due to constrained vocabulary sizes. Simply expandingthe codebook often leads to training instability and diminishing performancegains, making scalability a critical challenge. In this work, we introduceFactorized Quantization (FQ), a novel approach that revitalizes VQ-basedtokenizers by decomposing a large codebook into multiple independentsub-codebooks. This factorization reduces the lookup complexity of largecodebooks, enabling more efficient and scalable visual tokenization. To ensureeach sub-codebook captures distinct and complementary information, we propose adisentanglement regularization that explicitly reduces redundancy, promotingdiversity across the sub-codebooks. Furthermore, we integrate representationlearning into the training process, leveraging pretrained vision models likeCLIP and DINO to infuse semantic richness into the learned representations.This design ensures our tokenizer captures diverse semantic levels, leading tomore expressive and disentangled representations. Experiments show that theproposed FQGAN model substantially improves the reconstruction quality ofvisual tokenizers, achieving state-of-the-art performance. We furtherdemonstrate that this tokenizer can be effectively adapted into auto-regressiveimage generation. https://showlab.github.io/FQGAN</description><author>Zechen Bai, Jianxiong Gao, Ziteng Gao, Pichao Wang, Zheng Zhang, Tong He, Mike Zheng Shou</author><pubDate>Wed, 27 Nov 2024 17:04:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16681v2</guid></item><item><title>Living off the Analyst: Harvesting Features from Yara Rules for Malware Detection</title><link>http://arxiv.org/abs/2411.18516v1</link><description>A strategy used by malicious actors is to "live off the land," where benignsystems and tools already available on a victim's systems are used andrepurposed for the malicious actor's intent. In this work, we ask if there is away for anti-virus developers to similarly re-purpose existing work to improvetheir malware detection capability. We show that this is plausible via YARArules, which use human-written signatures to detect specific malware families,functionalities, or other markers of interest. By extracting sub-signaturesfrom publicly available YARA rules, we assembled a set of features that canmore effectively discriminate malicious samples from benign ones. Ourexperiments demonstrate that these features add value beyond traditionalfeatures on the EMBER 2018 dataset. Manual analysis of the added sub-signaturesshows a power-law behavior in a combination of features that are specific andunique, as well as features that occur often. A prior expectation may be thatthe features would be limited in being overly specific to unique malwarefamilies. This behavior is observed, and is apparently useful in practice. Inaddition, we also find sub-signatures that are dual-purpose (e.g., detectingvirtual machine environments) or broadly generic (e.g., DLL imports).</description><author>Siddhant Gupta, Fred Lu, Andrew Barlow, Edward Raff, Francis Ferraro, Cynthia Matuszek, Charles Nicholas, James Holt</author><pubDate>Wed, 27 Nov 2024 17:03:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18516v1</guid></item><item><title>Enhancing weed detection performance by means of GenAI-based image augmentation</title><link>http://arxiv.org/abs/2411.18513v1</link><description>Precise weed management is essential for sustaining crop productivity andecological balance. Traditional herbicide applications face economic andenvironmental challenges, emphasizing the need for intelligent weed controlsystems powered by deep learning. These systems require vast amounts ofhigh-quality training data. The reality of scarcity of well-annotated trainingdata, however, is often addressed through generating more data using dataaugmentation. Nevertheless, conventional augmentation techniques such as randomflipping, color changes, and blurring lack sufficient fidelity and diversity.This paper investigates a generative AI-based augmentation technique that usesthe Stable Diffusion model to produce diverse synthetic images that improve thequantity and quality of training datasets for weed detection models. Moreover,this paper explores the impact of these synthetic images on the performance ofreal-time detection systems, thus focusing on compact CNN-based models such asYOLO nano for edge devices. The experimental results show substantialimprovements in mean Average Precision (mAP50 and mAP50-95) scores for YOLOmodels trained with generative AI-augmented datasets, demonstrating thepromising potential of synthetic data to enhance model robustness and accuracy.</description><author>Sourav Modak, Anthony Stein</author><pubDate>Wed, 27 Nov 2024 17:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18513v1</guid></item><item><title>Federated Low-Rank Adaptation with Differential Privacy over Wireless Networks</title><link>http://arxiv.org/abs/2411.07806v2</link><description>Fine-tuning large pre-trained foundation models (FMs) on distributed edgedevices presents considerable computational and privacy challenges. Federatedfine-tuning (FedFT) mitigates some privacy issues by facilitating collaborativemodel training without the need to share raw data. To lessen the computationalburden on resource-limited devices, combining low-rank adaptation (LoRA) withfederated learning enables parameter-efficient fine-tuning. Additionally, thesplit FedFT architecture partitions an FM between edge devices and a centralserver, reducing the necessity for complete model deployment on individualdevices. However, the risk of privacy eavesdropping attacks in FedFT remains aconcern, particularly in sensitive areas such as healthcare and finance. Inthis paper, we propose a split FedFT framework with differential privacy (DP)over wireless networks, where the inherent wireless channel noise in the uplinktransmission is utilized to achieve DP guarantees without adding an extraartificial noise. We shall investigate the impact of the wireless noise onconvergence performance of the proposed framework. We will also show that byupdating only one of the low-rank matrices in the split FedFT with DP, theproposed method can mitigate the noise amplification effect. Simulation resultswill demonstrate that the proposed framework achieves higher accuracy understrict privacy budgets compared to baseline methods.</description><author>Tianqu Kang, Zixin Wang, Hengtao He, Jun Zhang, Shenghui Song, Khaled B. Letaief</author><pubDate>Wed, 27 Nov 2024 16:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07806v2</guid></item><item><title>An iterated learning model of language change that mixes supervised and unsupervised learning</title><link>http://arxiv.org/abs/2405.20818v3</link><description>The iterated learning model is an agent model which simulates thetransmission of of language from generation to generation. It is used to studyhow the language adapts to pressures imposed by transmission. In eachiteration, a language tutor exposes a na\"ive pupil to a limited training setof utterances, each pairing a random meaning with the signal that conveys it.Then the pupil becomes a tutor for a new na\"ive pupil in the next iteration.The transmission bottleneck ensures that tutors must generalize beyond thetraining set that they experienced. Repeated cycles of learning andgeneralization can result in a language that is expressive, compositional andstable. Previously, the agents in the iterated learning model mapped signals tomeanings using an artificial neural network but relied on an unrealistic andcomputationally expensive process of obversion to map meanings to signals.Here, both maps are neural networks, trained separately through supervisedlearning and together through unsupervised learning in the form of anautoencoder. This avoids the computational burden entailed in obversion andintroduces a mixture of supervised and unsupervised learning as observed duringlanguage learning in children. The new model demonstrates a linear relationshipbetween the dimensionality of meaning-signal space and effective bottlenecksize and suggests that internal reflection on potential utterances is importantin language learning and evolution.</description><author>Jack Bunyan, Seth Bullock, Conor Houghton</author><pubDate>Wed, 27 Nov 2024 16:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20818v3</guid></item><item><title>Simulation-based inference with scattering representations: scattering is all you need</title><link>http://arxiv.org/abs/2410.11883v2</link><description>We demonstrate the successful use of scattering representations withoutfurther compression for simulation-based inference (SBI) with images (i.e.field-level), illustrated with a cosmological case study. Scatteringrepresentations provide a highly effective representational space forsubsequent learning tasks, although the higher dimensional compressed spaceintroduces challenges. We overcome these through spatial averaging, coupledwith more expressive density estimators. Compared to alternative methods, suchan approach does not require additional simulations for either training orcomputing derivatives, is interpretable, and resilient to covariate shift. Asexpected, we show that a scattering only approach extracts more informationthan traditional second order summary statistics.</description><author>Kiyam Lin, Benjamin Joachimi, Jason D. McEwen</author><pubDate>Wed, 27 Nov 2024 16:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11883v2</guid></item><item><title>DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection</title><link>http://arxiv.org/abs/2404.03015v2</link><description>The perception of autonomous vehicles has to be efficient, robust, andcost-effective. However, cameras are not robust against severe weatherconditions, lidar sensors are expensive, and the performance of radar-basedperception is still inferior to the others. Camera-radar fusion methods havebeen proposed to address this issue, but these are constrained by the typicalsparsity of radar point clouds and often designed for radars without elevationinformation. We propose a novel camera-radar fusion approach called DualPerspective Fusion Transformer (DPFT), designed to overcome these limitations.Our method leverages lower-level radar data (the radar cube) instead of theprocessed point clouds to preserve as much information as possible and employsprojections in both the camera and ground planes to effectively use radars withelevation information and simplify the fusion with camera data. As a result,DPFT has demonstrated state-of-the-art performance on the K-Radar dataset whileshowing remarkable robustness against adverse weather conditions andmaintaining a low inference time. The code is made available as open-sourcesoftware under https://github.com/TUMFTM/DPFT.</description><author>Felix Fent, Andras Palffy, Holger Caesar</author><pubDate>Wed, 27 Nov 2024 16:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03015v2</guid></item><item><title>LLM-ABBA: Understand time series via symbolic approximation</title><link>http://arxiv.org/abs/2411.18506v1</link><description>The success of large language models (LLMs) for time series has beendemonstrated in previous work. Utilizing a symbolic time series representation,one can efficiently bridge the gap between LLMs and time series. However, theremaining challenge is to exploit the semantic information hidden in timeseries by using symbols or existing tokens of LLMs, while aligning theembedding space of LLMs according to the hidden information of time series. Thesymbolic time series approximation (STSA) method called adaptive Brownianbridge-based symbolic aggregation (ABBA) shows outstanding efficacy inpreserving salient time series features by modeling time series patterns interms of amplitude and period while using existing tokens of LLMs. In this paper, we introduce a method, called LLM-ABBA, that integrates ABBAinto large language models for various downstream time series tasks. Bysymbolizing time series, LLM-ABBA compares favorably to the recentstate-of-the-art (SOTA) in UCR and three medical time series classificationtasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\kc{avoid obvious drifting} during prediction tasks by significantly mitigatingthe effects of cumulative error arising from misused symbols during thetransition from symbols to numerical values. In time series regression tasks,LLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)benchmarks. LLM-ABBA also shows competitive prediction capability compared torecent SOTA time series prediction results. We believe this framework can alsoseamlessly extend to other time series tasks.</description><author>Erin Carson, Xinye Chen, Cheng Kang</author><pubDate>Wed, 27 Nov 2024 16:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18506v1</guid></item><item><title>Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault Diagnosis</title><link>http://arxiv.org/abs/2312.02826v2</link><description>Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be aneffective and flexible solution, attracting extensive research. Deep neuralnetworks can learn rich representations from vast amounts of representativelabeled data for various applications. In IFD, they achieve high classificationperformance from signals in an end-to-end manner, without requiring extensivedomain knowledge. However, deep learning models usually only perform well onthe data distribution they have been trained on. When applied to a differentdistribution, they may experience performance drops. This is also observed inIFD, where assets are often operated in working conditions different from thosein which labeled data have been collected. Unsupervised domain adaptation (UDA)deals with the scenario where labeled data are available in a source domain,and only unlabeled data are available in a target domain, where domains maycorrespond to operating conditions. Recent methods rely on training withconfident pseudo-labels for target samples. However, the confidence-basedselection of pseudo-labels is hindered by poorly calibrated confidenceestimates in the target domain, primarily due to over-confident predictions,which limits the quality of pseudo-labels and leads to error accumulation. Inthis paper, we propose a novel UDA method called Calibrated Adaptive Teacher(CAT), where we propose to calibrate the predictions of the teacher networkthroughout the self-training process, leveraging post-hoc calibrationtechniques. We evaluate CAT on domain-adaptive IFD and perform extensiveexperiments on the Paderborn benchmark for bearing fault diagnosis undervarying operating conditions. Our proposed method achieves state-of-the-artperformance on most transfer tasks.</description><author>Florent Forest, Olga Fink</author><pubDate>Wed, 27 Nov 2024 16:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02826v2</guid></item><item><title>Unveiling the optimization process of Physics Informed Neural Networks: How accurate and competitive can PINNs be?</title><link>http://arxiv.org/abs/2405.04230v2</link><description>This study investigates the potential accuracy boundaries of physics-informedneural networks, contrasting their approach with previous similar works andtraditional numerical methods. We find that selecting improved optimizationalgorithms significantly enhances the accuracy of the results. Simplemodifications to the loss function may also improve precision, offering anadditional avenue for enhancement. Despite optimization algorithms having agreater impact on convergence than adjustments to the loss function, practicalconsiderations often favor tweaking the latter due to ease of implementation.On a global scale, the integration of an enhanced optimizer and a marginallyadjusted loss function enables a reduction in the loss function by severalorders of magnitude across diverse physical problems. Consequently, our resultsobtained using compact networks (typically comprising 2 or 3 layers of 20-30neurons) achieve accuracies comparable to finite difference schemes employingthousands of grid points. This study encourages the continued advancement ofPINNs and associated optimization techniques for broader applications acrossvarious fields.</description><author>Jorge F. Urbán, Petros Stefanou, José A. Pons</author><pubDate>Wed, 27 Nov 2024 16:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04230v2</guid></item><item><title>Isometry pursuit</title><link>http://arxiv.org/abs/2411.18502v1</link><description>Isometry pursuit is a convex algorithm for identifying orthonormalcolumn-submatrices of wide matrices. It consists of a novel normalizationmethod followed by multitask basis pursuit. Applied to Jacobians of putativecoordinate functions, it helps identity isometric embeddings from withininterpretable dictionaries. We provide theoretical and experimental resultsjustifying this method. For problems involving coordinate selection anddiversification, it offers a synergistic alternative to greedy and brute forcesearch.</description><author>Samson Koelle, Marina Meila</author><pubDate>Wed, 27 Nov 2024 16:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18502v1</guid></item><item><title>GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation</title><link>http://arxiv.org/abs/2411.18499v1</link><description>Multimodal Large Language Models (MLLMs) have made significant strides invisual understanding and generation tasks. However, generating interleavedimage-text content remains a challenge, which requires integrated multimodalunderstanding and generation abilities. While the progress in unified modelsoffers new solutions, existing benchmarks are insufficient for evaluating thesemethods due to data size and diversity limitations. To bridge this gap, weintroduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400high-quality human-annotated instances across 56 real-world tasks. OpenINGcovers diverse daily scenarios such as travel guide, design, and brainstorming,offering a robust platform for challenging interleaved generation methods. Inaddition, we present IntJudge, a judge model for evaluating open-endedmultimodal generation methods. Trained with a novel data pipeline, our IntJudgeachieves an agreement rate of 82. 42% with human judgments, outperformingGPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal thatcurrent interleaved generation methods still have substantial room forimprovement. Key findings on interleaved image-text generation are furtherpresented to guide the development of next-generation models. The OpenING isopen-sourced at https://opening.github.io.</description><author>Pengfei Zhou, Xiaopeng Peng, Jiajun Song, Chuanhao Li, Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin, Yefei He, Lirui Zhao, Shuo Liu, Tianhua Li, Yuxuan Xie, Xiaojun Chang, Yu Qiao, Wenqi Shao, Kaipeng Zhang</author><pubDate>Wed, 27 Nov 2024 16:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18499v1</guid></item><item><title>Multiple Choice Learning for Efficient Speech Separation with Many Speakers</title><link>http://arxiv.org/abs/2411.18497v1</link><description>Training speech separation models in the supervised setting raises apermutation problem: finding the best assignation between the model predictionsand the ground truth separated signals. This inherently ambiguous task iscustomarily solved using Permutation Invariant Training (PIT). In this article,we instead consider using the Multiple Choice Learning (MCL) framework, whichwas originally introduced to tackle ambiguous tasks. We demonstrateexperimentally on the popular WSJ0-mix and LibriMix benchmarks that MCL matchesthe performances of PIT, while being computationally advantageous. This opensthe door to a promising research direction, as MCL can be naturally extended tohandle a variable number of speakers, or to tackle speech separation in theunsupervised setting.</description><author>David Perera, François Derrida, Théo Mariotte, Gaël Richard, Slim Essid</author><pubDate>Wed, 27 Nov 2024 16:38:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18497v1</guid></item><item><title>Agent Skill Acquisition for Large Language Models via CycleQD</title><link>http://arxiv.org/abs/2410.14735v2</link><description>Training large language models to acquire specific skills remains achallenging endeavor. Conventional training approaches often struggle with datadistribution imbalances and inadequacies in objective functions that do notalign well with task-specific performance. To address these challenges, weintroduce CycleQD, a novel approach that leverages the Quality Diversityframework through a cyclic adaptation of the algorithm, along with a modelmerging based crossover and an SVD-based mutation. In CycleQD, each task'sperformance metric is alternated as the quality measure while the others serveas the behavioral characteristics. This cyclic focus on individual tasks allowsfor concentrated effort on one task at a time, eliminating the need for dataratio tuning and simplifying the design of the objective function. Empiricalresults from AgentBench indicate that applying CycleQD to LLAMA3-8B-INSTRUCTbased models not only enables them to surpass traditional fine-tuning methodsin coding, operating systems, and database tasks, but also achieves performanceon par with GPT-3.5-TURBO, which potentially contains much more parameters,across these domains. Crucially, this enhanced performance is achieved whileretaining robust language capabilities, as evidenced by its performance onwidely adopted language benchmark tasks. We highlight the key design choices inCycleQD, detailing how these contribute to its effectiveness. Furthermore, ourmethod is general and can be applied to image segmentation models, highlightingits applicability across different domains.</description><author>So Kuroki, Taishi Nakamura, Takuya Akiba, Yujin Tang</author><pubDate>Wed, 27 Nov 2024 16:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14735v2</guid></item><item><title>Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale</title><link>http://arxiv.org/abs/2409.15637v2</link><description>LLMs can now act as autonomous agents that interact with digital environmentsand complete specific objectives (e.g., arranging an online meeting). However,accuracy is still far from satisfactory, partly due to a lack of large-scale,direct demonstrations for digital tasks. Obtaining supervised data from humansis costly, and automatic data collection through exploration or reinforcementlearning relies on complex environmental and content setup, resulting indatasets that lack comprehensive coverage of various scenarios. On the otherhand, there is abundant knowledge that may indirectly assist task completion,such as online tutorials that were created for human consumption. In this work,we present Synatra, an approach that effectively transforms this indirectknowledge into direct supervision at scale. We define different types ofindirect knowledge, and carefully study the available sources to obtain it,methods to encode the structure of direct demonstrations, and finally methodsto transform indirect knowledge into direct demonstrations. We use 100k suchsynthetically-created demonstrations to finetune a 7B CodeLlama, anddemonstrate that the resulting agent surpasses all comparably sized models onthree web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well assurpassing GPT-3.5 on WebArena and Mind2Web. In addition, while syntheticdemonstrations prove to be only 3% the cost of human demonstrations (at $0.031each), we show that the synthetic demonstrations can be more effective than anidentical number of human demonstrations collected from limited domains.</description><author>Tianyue Ou, Frank F. Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sengupta, Dan Roth, Graham Neubig, Shuyan Zhou</author><pubDate>Wed, 27 Nov 2024 16:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15637v2</guid></item><item><title>3D-free meets 3D priors: Novel View Synthesis from a Single Image with Pretrained Diffusion Guidance</title><link>http://arxiv.org/abs/2408.06157v4</link><description>Recent 3D novel view synthesis (NVS) methods often require extensive 3D datafor training, and also typically lack generalization beyond the trainingdistribution. Moreover, they tend to be object centric and struggle withcomplex and intricate scenes. Conversely, 3D-free methods can generatetext-controlled views of complex, in-the-wild scenes using a pretrained stablediffusion model without the need for a large amount of 3D-based training data,but lack camera control. In this paper, we introduce a method capable ofgenerating camera-controlled viewpoints from a single input image, by combiningthe benefits of 3D-free and 3D-based approaches. Our method excels in handlingcomplex and diverse scenes without extensive training or additional 3D andmultiview data. It leverages widely available pretrained NVS models for weakguidance, integrating this knowledge into a 3D-free view synthesis styleapproach, along with enriching the CLIP vision-language space with 3D cameraangle information, to achieve the desired results. Experimental resultsdemonstrate that our method outperforms existing models in both qualitative andquantitative evaluations, achieving high-fidelity, consistent novel viewsynthesis at desired camera angles across a wide variety of scenes whilemaintaining accurate, natural detail representation and image clarity acrossvarious viewpoints. We also support our method with a comprehensive analysis of2D image generation models and the 3D space, providing a solid foundation andrationale for our solution.</description><author>Taewon Kang, Divya Kothandaraman, Dinesh Manocha, Ming C. Lin</author><pubDate>Wed, 27 Nov 2024 16:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.06157v4</guid></item><item><title>SPTTE: A Spatiotemporal Probabilistic Framework for Travel Time Estimation</title><link>http://arxiv.org/abs/2411.18484v1</link><description>Accurate travel time estimation is essential for navigation and itineraryplanning. While existing research employs probabilistic modeling to assesstravel time uncertainty and account for correlations between multiple trips,modeling the temporal variability of multi-trip travel time distributionsremains a significant challenge. Capturing the evolution of joint distributionsrequires large, well-organized datasets; however, real-world trip data areoften temporally sparse and spatially unevenly distributed. To address thisissue, we propose SPTTE, a spatiotemporal probabilistic framework that modelsthe evolving joint distribution of multi-trip travel times by formulating theestimation task as a spatiotemporal stochastic process regression problem withfragmented observations. SPTTE incorporates an RNN-based temporal Gaussianprocess parameterization to regularize sparse observations and capture temporaldependencies. Additionally, it employs a prior-based heterogeneity smoothingstrategy to correct unreliable learning caused by unevenly distributed trips,effectively modeling temporal variability under sparse and uneven datadistributions. Evaluations on real-world datasets demonstrate that SPTTEoutperforms state-of-the-art deterministic and probabilistic methods by over10.13%. Ablation studies and visualizations further confirm the effectivenessof the model components.</description><author>Chen Xu, Qiang Wang, Lijun Sun</author><pubDate>Wed, 27 Nov 2024 16:28:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18484v1</guid></item><item><title>Navigating the Post-API Dilemma | Search Engine Results Pages Present a Biased View of Social Media Data</title><link>http://arxiv.org/abs/2401.15479v4</link><description>Recent decisions to discontinue access to social media APIs are havingdetrimental effects on Internet research and the field of computational socialscience as a whole. This lack of access to data has been dubbed the Post-APIera of Internet research. Fortunately, popular search engines have the means tocrawl, capture, and surface social media data on their Search Engine ResultsPages (SERP) if provided the proper search query, and may provide a solution tothis dilemma. In the present work we ask: does SERP provide a complete andunbiased sample of social media data? Is SERP a viable alternative to directAPI-access? To answer these questions, we perform a comparative analysisbetween (Google) SERP results and nonsampled data from Reddit and Twitter/X. Wefind that SERP results are highly biased in favor of popular posts; againstpolitical, pornographic, and vulgar posts; are more positive in theirsentiment; and have large topical gaps. Overall, we conclude that SERP is not aviable alternative to social media API access.</description><author>Amrit Poudel, Tim Weninger</author><pubDate>Wed, 27 Nov 2024 16:27:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15479v4</guid></item><item><title>SoK: Watermarking for AI-Generated Content</title><link>http://arxiv.org/abs/2411.18479v1</link><description>As the outputs of generative AI (GenAI) techniques improve in quality, itbecomes increasingly challenging to distinguish them from human-createdcontent. Watermarking schemes are a promising approach to address the problemof distinguishing between AI and human-generated content. These schemes embedhidden signals within AI-generated content to enable reliable detection. Whilewatermarking is not a silver bullet for addressing all risks associated withGenAI, it can play a crucial role in enhancing AI safety and trustworthiness bycombating misinformation and deception. This paper presents a comprehensiveoverview of watermarking techniques for GenAI, beginning with the need forwatermarking from historical and regulatory perspectives. We formalize thedefinitions and desired properties of watermarking schemes and examine the keyobjectives and threat models for existing approaches. Practical evaluationstrategies are also explored, providing insights into the development of robustwatermarking techniques capable of resisting various attacks. Additionally, wereview recent representative works, highlight open challenges, and discusspotential directions for this emerging field. By offering a thoroughunderstanding of watermarking in GenAI, this work aims to guide researchers inadvancing watermarking methods and applications, and support policymakers inaddressing the broader implications of GenAI.</description><author>Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tramer, Somesh Jha, Lei Li, Yu-Xiang Wang, Dawn Song</author><pubDate>Wed, 27 Nov 2024 16:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18479v1</guid></item><item><title>Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS</title><link>http://arxiv.org/abs/2411.18478v1</link><description>In-context Learning (ICL) enables large language models (LLMs) to tackledownstream tasks through sophisticated prompting and high-qualitydemonstrations. However, this traditional ICL paradigm shows limitations whenfacing complex mathematical reasoning tasks, primarily due to its heavydependence on example quality and the necessity for human intervention inchallenging scenarios. To address these limitations, this paper presentsHiAR-ICL, a \textbf{Hi}gh-level \textbf{A}utomated \textbf{R}easoning paradigmin \textbf{ICL} that shifts focus from specific examples to abstract thinkingpatterns, extending the conventional concept of context in ICL. HiAR-ICLintroduces five atomic reasoning actions as fundamental components forconstructing chain-structured patterns. Using Monte Carlo Tree Search, weexplore reasoning paths and construct thought cards to guide subsequentinference. We then develop a cognitive complexity framework that dynamicallymatches problems with appropriate thought cards. Experimental resultsdemonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy(79.6$\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o(76.6$\%$) and Claude 3.5 (71.1$\%$).</description><author>Jinyang Wu, Mingkuan Feng, Shuai Zhang, Feihu Che, Zengqi Wen, Jianhua Tao</author><pubDate>Wed, 27 Nov 2024 16:19:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18478v1</guid></item><item><title>A comparison of extended object tracking with multi-modal sensors in indoor environment</title><link>http://arxiv.org/abs/2411.18476v1</link><description>This paper presents a preliminary study of an efficient object trackingapproach, comparing the performance of two different 3D point cloud sensorysources: LiDAR and stereo cameras, which have significant price differences. Inthis preliminary work, we focus on single object tracking. We first developed afast heuristic object detector that utilizes prior information about theenvironment and target. The resulting target points are subsequently fed intoan extended object tracking framework, where the target shape is parameterizedusing a star-convex hypersurface model. Experimental results show that ourobject tracking method using a stereo camera achieves performance similar tothat of a LiDAR sensor, with a cost difference of more than tenfold.</description><author>Jiangtao Shuai, Martin Baerveldt, Manh Nguyen-Duc, Anh Le-Tuan, Manfred Hauswirth, Danh Le-Phuoc</author><pubDate>Wed, 27 Nov 2024 16:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18476v1</guid></item><item><title>Weakly Supervised Framework Considering Multi-temporal Information for Large-scale Cropland Mapping with Satellite Imagery</title><link>http://arxiv.org/abs/2411.18475v1</link><description>Accurately mapping large-scale cropland is crucial for agriculturalproduction management and planning. Currently, the combination of remotesensing data and deep learning techniques has shown outstanding performance incropland mapping. However, those approaches require massive precise labels,which are labor-intensive. To reduce the label cost, this study presented aweakly supervised framework considering multi-temporal information forlarge-scale cropland mapping. Specifically, we extract high-quality labelsaccording to their consistency among global land cover (GLC) products toconstruct the supervised learning signal. On the one hand, to alleviate theoverfitting problem caused by the model's over-trust of remaining errors inhigh-quality labels, we encode the similarity/aggregation of cropland in thevisual/spatial domain to construct the unsupervised learning signal, and takeit as the regularization term to constrain the supervised part. On the otherhand, to sufficiently leverage the plentiful information in the samples withouthigh-quality labels, we also incorporate the unsupervised learning signal inthese samples, enriching the diversity of the feature space. After that, tocapture the phenological features of croplands, we introduce dense satelliteimage time series (SITS) to extend the proposed framework in the temporaldimension. We also visualized the high dimensional phenological features touncover how multi-temporal information benefits cropland extraction, andassessed the method's robustness under conditions of data scarcity. Theproposed framework has been experimentally validated for strong adaptabilityacross three study areas (Hunan Province, Southeast France, and Kansas) inlarge-scale cropland mapping, and the internal mechanism and temporalgeneralizability are also investigated.</description><author>Yuze Wang, Aoran Hu, Ji Qi, Yang Liu, Chao Tao</author><pubDate>Wed, 27 Nov 2024 16:11:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18475v1</guid></item><item><title>HEMGS: A Hybrid Entropy Model for 3D Gaussian Splatting Data Compression</title><link>http://arxiv.org/abs/2411.18473v1</link><description>Fast progress in 3D Gaussian Splatting (3DGS) has made 3D Gaussians popularfor 3D modeling and image rendering, but this creates big challenges in datastorage and transmission. To obtain a highly compact 3DGS representation, wepropose a hybrid entropy model for Gaussian Splatting (HEMGS) data compression,which comprises two primary components, a hyperprior network and anautoregressive network. To effectively reduce structural redundancy acrossattributes, we apply a progressive coding algorithm to generate hyperpriorfeatures, in which we use previously compressed attributes and location asprior information. In particular, to better extract the location features fromthese compressed attributes, we adopt a domain-aware and instance-awarearchitecture to respectively capture domain-aware structural relations withoutadditional storage costs and reveal scene-specific features through MLPs.Additionally, to reduce redundancy within each attribute, we leveragerelationships between neighboring compressed elements within the attributesthrough an autoregressive network. Given its unique structure, we propose anadaptive context coding algorithm with flexible receptive fields to effectivelycapture adjacent compressed elements. Overall, we integrate our HEMGS into anend-to-end optimized 3DGS compression framework and the extensive experimentalresults on four benchmarks indicate that our method achieves about 40\% averagereduction in size while maintaining the rendering quality over our baselinemethod and achieving state-of-the-art compression results.</description><author>Lei Liu, Zhenghao Chen, Dong Xu</author><pubDate>Wed, 27 Nov 2024 16:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18473v1</guid></item><item><title>Isolating authorship from content with semantic embeddings and contrastive learning</title><link>http://arxiv.org/abs/2411.18472v1</link><description>Authorship has entangled style and content inside. Authors frequently writeabout the same topics in the same style, so when different authors write aboutthe exact same topic the easiest way out to distinguish them is byunderstanding the nuances of their style. Modern neural models for authorshipcan pick up these features using contrastive learning, however, some amount ofcontent leakage is always present. Our aim is to reduce the inevitable impactand correlation between content and authorship. We present a technique to usecontrastive learning (InfoNCE) with additional hard negatives syntheticallycreated using a semantic similarity model. This disentanglement technique aimsto distance the content embedding space from the style embedding space, leadingto embeddings more informed by style. We demonstrate the performance withablations on two different datasets and compare them on out-of-domainchallenges. Improvements are clearly shown on challenging evaluations onprolific authors with up to a 10% increase in accuracy when the settings areparticularly hard. Trials on challenges also demonstrate the preservation ofzero-shot capabilities of this method as fine tuning.</description><author>Javier Huertas-Tato, Adrián Girón-Jiménez, Alejandro Martín, David Camacho</author><pubDate>Wed, 27 Nov 2024 16:08:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18472v1</guid></item><item><title>Parole de présidents (1958-2022)</title><link>http://arxiv.org/abs/2411.18468v1</link><description>En plus de soixante ans, huit pr\'esidents se sont succ\'ed\'e \`a la t\^etede la Ve R\'epublique fran\c{c}aise (de Gaulle, Pompidou, Giscard d'Estaing,Mitterrand, Chirac, Sarkozy, Hollande, Macron). Apr\`es avoir pr\'esent\'e lecorpus de leurs discours -- soit 9202 textes et plus de 20 millions de mots\'etiquet\'es -- le style de chacun des pr\'esidents sera caract\'eris\'e \`al'aide de leurs vocabulaire (vocables et cat\'egories grammaticales). Uneanalyse plus approfondie r\'ev\`ele les s\'equences typiques de chaquelocataire de l'\'Elys\'ee. Bas\'ee sur les distances entre l'ensemble desallocutions, une figure illustre les similitudes et diff\'erences entre lesdiff\'erents pr\'esidents. Over the past sixty-six years, eight presidents successively headed the FifthFrench Republic (de Gaulle, Pompidou, Giscard d'Estaing, Mitterrand, Chirac,Sarkozy, Holland, Macron). After presenting the corpus of their speeches --9,202 texts and more than 20 million labelled words -- the style of each ofthem will be characterized by their vocabulary (lemmas and part-of-speech). Adeeper analysis reveals the typical sequences of each tenant of the Elys\'ee.Based on an intertextual distance between all presidential speeches, asynthesis can be drawn reflecting the similarities and differences betweenpresidents.</description><author>Dominique Labbé, Jacques Savoy</author><pubDate>Wed, 27 Nov 2024 16:01:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18468v1</guid></item><item><title>ReforesTree: A Dataset for Estimating Tropical Forest Carbon Stock with Deep Learning and Aerial Imagery</title><link>http://arxiv.org/abs/2201.11192v2</link><description>Forest biomass is a key influence for future climate, and the world urgentlyneeds highly scalable financing schemes, such as carbon offsettingcertifications, to protect and restore forests. Current manual forest carbonstock inventory methods of measuring single trees by hand are time, labour, andcost-intensive and have been shown to be subjective. They can lead tosubstantial overestimation of the carbon stock and ultimately distrust inforest financing. The potential for impact and scale of leveraging advancementsin machine learning and remote sensing technologies is promising but needs tobe of high quality in order to replace the current forest stock protocols forcertifications. In this paper, we present ReforesTree, a benchmark dataset of forest carbonstock in six agro-forestry carbon offsetting sites in Ecuador. Furthermore, weshow that a deep learning-based end-to-end model using individual treedetection from low cost RGB-only drone imagery is accurately estimating forestcarbon stock within official carbon offsetting certification standards.Additionally, our baseline CNN model outperforms state-of-the-artsatellite-based forest biomass and carbon stock estimates for this type ofsmall-scale, tropical agro-forestry sites. We present this dataset to encouragemachine learning research in this area to increase accountability andtransparency of monitoring, verification and reporting (MVR) in carbonoffsetting projects, as well as scaling global reforestation financing throughaccurate remote sensing.</description><author>Gyri Reiersen, David Dao, Björn Lütjens, Konstantin Klemmer, Kenza Amara, Attila Steinegger, Ce Zhang, Xiaoxiang Zhu</author><pubDate>Wed, 27 Nov 2024 15:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.11192v2</guid></item><item><title>Complexity Experts are Task-Discriminative Learners for Any Image Restoration</title><link>http://arxiv.org/abs/2411.18466v1</link><description>Recent advancements in all-in-one image restoration models haverevolutionized the ability to address diverse degradations through a unifiedframework. However, parameters tied to specific tasks often remain inactive forother tasks, making mixture-of-experts (MoE) architectures a natural extension.Despite this, MoEs often show inconsistent behavior, with some expertsunexpectedly generalizing across tasks while others struggle within theirintended scope. This hinders leveraging MoEs' computational benefits bybypassing irrelevant experts during inference. We attribute this undesiredbehavior to the uniform and rigid architecture of traditional MoEs. To addressthis, we introduce ``complexity experts" -- flexible expert blocks with varyingcomputational complexity and receptive fields. A key challenge is assigningtasks to each expert, as degradation complexity is unknown in advance. Thus, weexecute tasks with a simple bias toward lower complexity. To our surprise, thispreference effectively drives task-specific allocation, assigning tasks toexperts with the appropriate complexity. Extensive experiments validate ourapproach, demonstrating the ability to bypass irrelevant experts duringinference while maintaining superior performance. The proposed MoCE-IR modeloutperforms state-of-the-art methods, affirming its efficiency and practicalapplicability. The source will be publicly made available at\href{https://eduardzamfir.github.io/moceir/}{\texttt{eduardzamfir.github.io/MoCE-IR/}}</description><author>Eduard Zamfir, Zongwei Wu, Nancy Mehta, Yuedong Tan, Danda Pani Paudel, Yulun Zhang, Radu Timofte</author><pubDate>Wed, 27 Nov 2024 15:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18466v1</guid></item><item><title>Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding</title><link>http://arxiv.org/abs/2411.18462v1</link><description>Speculative Decoding (SD) has become an important technique in acceleratingthe inference speed of large language models. Conventional SD methods employ afixed draft length, which ignores the token generation difficulty across tasks.Consequently, in this paper, we address such an issue and introduce SVIP - adifficulty-aware dynamic draft length policy for speculative decoding systems.Based on a theoretical lower bound of draft token acceptance rate and itsinference-time approximation, SVIP adaptively determines the lengths of draftsequences based on the entropy of each draft token distribution. Experimentalresults on mainstream SD benchmarks and frameworks demonstrate the superiorperformance of SVIP, achieving up to 20\% walltime speedup on SpecBench overbaseline SD methods and 60\% speedup on MT-Bench for long-form generation of upto 8K tokens. Moreover, SVIP is totally training-free and compatible with anyexisting SD methods that generate draft tokens autoregressively. Experimentalresults also show that SVIP yields consistent walltime improvement on top ofGliDe &amp; CaPE and EAGLE-2.</description><author>Ziyin Zhang, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Rui Wang, Zhaopeng Tu</author><pubDate>Wed, 27 Nov 2024 15:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18462v1</guid></item><item><title>STOP: Spatiotemporal Orthogonal Propagation for Weight-Threshold-Leakage Synergistic Training of Deep Spiking Neural Networks</title><link>http://arxiv.org/abs/2411.11082v2</link><description>The prevailing of artificial intelligence-of-things calls for higherenergy-efficient edge computing paradigms, such as neuromorphic agentsleveraging brain-inspired spiking neural network (SNN) models based onspatiotemporally sparse binary spikes. However, the lack of efficient andhigh-accuracy deep SNN learning algorithms prevents them from practical edgedeployments at a strictly bounded cost. In this paper, we propose thespatiotemporal orthogonal propagation (STOP) algorithm to tackle thischallenge. Our algorithm enables fully synergistic learning of synaptic weightsas well as firing thresholds and leakage factors in spiking neurons to improveSNN accuracy, in a unified temporally-forward trace-based framework to mitigatethe huge memory requirement for storing neural states across all time-steps inthe forward pass. Characteristically, the spatially-backward neuronal errorsand temporally-forward traces propagate orthogonally to and independently ofeach other, substantially reducing computational complexity. Our STOP algorithmobtained high recognition accuracies of 94.84%, 74.92%, 98.26% and 77.10% onthe CIFAR-10, CIFAR-100, DVS-Gesture and DVS-CIFAR10 datasets with adequatedeep convolutional SNNs of VGG-11 or ResNet-18 structures. Compared with otherdeep SNN training algorithms, our method is more plausible for edge intelligentscenarios where resources are limited but high-accuracy in-situ learning isdesired.</description><author>Haoran Gao, Xichuan Zhou, Yingcheng Lin, Min Tian, Liyuan Liu, Cong Shi</author><pubDate>Wed, 27 Nov 2024 15:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11082v2</guid></item><item><title>What do physics-informed DeepONets learn? Understanding and improving training for scientific computing applications</title><link>http://arxiv.org/abs/2411.18459v1</link><description>Physics-informed deep operator networks (DeepONets) have emerged as apromising approach toward numerically approximating the solution of partialdifferential equations (PDEs). In this work, we aim to develop furtherunderstanding of what is being learned by physics-informed DeepONets byassessing the universality of the extracted basis functions and demonstratingtheir potential toward model reduction with spectral methods. Results provideclarity about measuring the performance of a physics-informed DeepONet throughthe decays of singular values and expansion coefficients. In addition, wepropose a transfer learning approach for improving training forphysics-informed DeepONets between parameters of the same PDE as well as acrossdifferent, but related, PDEs where these models struggle to train well. Thisapproach results in significant error reduction and learned basis functionsthat are more effective in representing the solution of a PDE.</description><author>Emily Williams, Amanda Howard, Brek Meuris, Panos Stinis</author><pubDate>Wed, 27 Nov 2024 15:48:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18459v1</guid></item><item><title>GSE: Group-wise Sparse and Explainable Adversarial Attacks</title><link>http://arxiv.org/abs/2311.17434v4</link><description>Sparse adversarial attacks fool deep neural networks (DNNs) through minimalpixel perturbations, often regularized by the $\ell_0$ norm. Recent effortshave replaced this norm with a structural sparsity regularizer, such as thenuclear group norm, to craft group-wise sparse adversarial attacks. Theresulting perturbations are thus explainable and hold significant practicalrelevance, shedding light on an even greater vulnerability of DNNs. However,crafting such attacks poses an optimization challenge, as it involves computingnorms for groups of pixels within a non-convex objective. We address this bypresenting a two-phase algorithm that generates group-wise sparse attackswithin semantically meaningful areas of an image. Initially, we optimize aquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailoredfor non-convex programming. Subsequently, the algorithm transitions to aprojected Nesterov's accelerated gradient descent with $2-$norm regularizationapplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 andImageNet datasets demonstrate a remarkable increase in group-wise sparsity,e.g., $50.9\%$ on CIFAR-10 and $38.4\%$ on ImageNet (average case, targetedattack). This performance improvement is accompanied by significantly fastercomputation times, improved explainability, and a $100\%$ attack success rate.</description><author>Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta</author><pubDate>Wed, 27 Nov 2024 15:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17434v4</guid></item><item><title>Synthetic ECG Generation for Data Augmentation and Transfer Learning in Arrhythmia Classification</title><link>http://arxiv.org/abs/2411.18456v1</link><description>Deep learning models need a sufficient amount of data in order to be able tofind the hidden patterns in it. It is the purpose of generative modeling tolearn the data distribution, thus allowing us to sample more data and augmentthe original dataset. In the context of physiological data, and morespecifically electrocardiogram (ECG) data, given its sensitive nature andexpensive data collection, we can exploit the benefits of generative models inorder to enlarge existing datasets and improve downstream tasks, in our case,classification of heart rhythm. In this work, we explore the usefulness of synthetic data generated withdifferent generative models from Deep Learning namely Diffweave, Time-Diffusionand Time-VQVAE in order to obtain better classification results for two opensource multivariate ECG datasets. Moreover, we also investigate the effects oftransfer learning, by fine-tuning a synthetically pre-trained model and thenprogressively adding increasing proportions of real data. We conclude thatalthough the synthetic samples resemble the real ones, the classificationimprovement when simply augmenting the real dataset is barely noticeable onindividual datasets, but when both datasets are merged the results show anincrease across all metrics for the classifiers when using synthetic samples asaugmented data. From the fine-tuning results the Time-VQVAE generative modelhas shown to be superior to the others but not powerful enough to achieveresults close to a classifier trained with real data only. In addition, methodsand metrics for measuring closeness between synthetic data and the real onehave been explored as a side effect of the main research questions of thisstudy.</description><author>José Fernando Núñez, Jamie Arjona, Javier Béjar</author><pubDate>Wed, 27 Nov 2024 15:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18456v1</guid></item><item><title>ViTOC: Vision Transformer and Object-aware Captioner</title><link>http://arxiv.org/abs/2411.07265v3</link><description>This paper presents ViTOC (Vision Transformer and Object-aware Captioner), anovel vision-language model for image captioning that addresses the challengesof accuracy and diversity in generated descriptions. Unlike conventionalapproaches, ViTOC employs a dual-path architecture based on Vision Transformerand object detector, effectively fusing global visual features and local objectinformation through learnable vectors. The model introduces an innovativeobject-aware prompting strategy that significantly enhances its capability inhandling long-tail data. Experiments on the standard COCO dataset demonstratethat ViTOC outperforms baseline models across all evaluation metrics.Additionally, we propose a reference-free evaluation method based on CLIP tofurther validate the model's effectiveness. By utilizing pretrained visualmodel parameters, ViTOC achieves efficient end-to-end training.</description><author>Feiyang Huang</author><pubDate>Wed, 27 Nov 2024 15:45:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07265v3</guid></item><item><title>S-CFE: Simple Counterfactual Explanations</title><link>http://arxiv.org/abs/2410.15723v3</link><description>We study the problem of finding optimal sparse, manifold-alignedcounterfactual explanations for classifiers. Canonically, this can beformulated as an optimization problem with multiple non-convex components,including classifier loss functions and manifold alignment (or\emph{plausibility}) metrics. The added complexity of enforcing\emph{sparsity}, or shorter explanations, complicates the problem further.Existing methods often focus on specific models and plausibility measures,relying on convex $\ell_1$ regularizers to enforce sparsity. In this paper, wetackle the canonical formulation using the accelerated proximal gradient (APG)method, a simple yet efficient first-order procedure capable of handling smoothnon-convex objectives and non-smooth $\ell_p$ (where $0 \leq p &lt; 1$)regularizers. This enables our approach to seamlessly incorporate variousclassifiers and plausibility measures while producing sparser solutions. Ouralgorithm only requires differentiable data-manifold regularizers and supportsbox constraints for bounded feature ranges, ensuring the generatedcounterfactuals remain \emph{actionable}. Finally, experiments on real-worlddatasets demonstrate that our approach effectively produces sparse,manifold-aligned counterfactual explanations while maintaining proximity to thefactual data and computational efficiency.</description><author>Shpresim Sadiku, Moritz Wagner, Sai Ganesh Nagarajan, Sebastian Pokutta</author><pubDate>Wed, 27 Nov 2024 15:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15723v3</guid></item><item><title>Advancements in Myocardial Infarction Detection and Classification Using Wearable Devices: A Comprehensive Review</title><link>http://arxiv.org/abs/2411.18451v1</link><description>Myocardial infarction (MI), commonly known as a heart attack, is a criticalhealth condition caused by restricted blood flow to the heart. Early-stagedetection through continuous ECG monitoring is essential to minimizeirreversible damage. This review explores advancements in MI classificationmethodologies for wearable devices, emphasizing their potential in real-timemonitoring and early diagnosis. It critically examines traditional approaches,such as morphological filtering and wavelet decomposition, alongsidecutting-edge techniques, including Convolutional Neural Networks (CNNs) andVLSI-based methods. By synthesizing findings on machine learning, deeplearning, and hardware innovations, this paper highlights their strengths,limitations, and future prospects. The integration of these techniques intowearable devices offers promising avenues for efficient, accurate, andenergy-aware MI detection, paving the way for next-generation wearablehealthcare solutions.</description><author>Abhijith S, Arjun Rajesh, Mansi Manoj, Sandra Davis Kollannur, Sujitta R V, Jerrin Thomas Panachakel</author><pubDate>Wed, 27 Nov 2024 15:42:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18451v1</guid></item><item><title>Continuous Autoregressive Models with Noise Augmentation Avoid Error Accumulation</title><link>http://arxiv.org/abs/2411.18447v1</link><description>Autoregressive models are typically applied to sequences of discrete tokens,but recent research indicates that generating sequences of continuousembeddings in an autoregressive manner is also feasible. However, suchContinuous Autoregressive Models (CAMs) can suffer from a decline in generationquality over extended sequences due to error accumulation during inference. Weintroduce a novel method to address this issue by injecting random noise intothe input embeddings during training. This procedure makes the model robustagainst varying error levels at inference. We further reduce error accumulationthrough an inference procedure that introduces low-level noise. Experiments onmusical audio generation show that CAM substantially outperforms existingautoregressive and non-autoregressive approaches while preserving audio qualityover extended sequences. This work paves the way for generating continuousembeddings in a purely autoregressive setting, opening new possibilities forreal-time and interactive generative applications.</description><author>Marco Pasini, Javier Nistal, Stefan Lattner, George Fazekas</author><pubDate>Wed, 27 Nov 2024 15:38:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18447v1</guid></item><item><title>Is my Meeting Summary Good? Estimating Quality with a Multi-LLM Evaluator</title><link>http://arxiv.org/abs/2411.18444v1</link><description>The quality of meeting summaries generated by natural language generation(NLG) systems is hard to measure automatically. Established metrics such asROUGE and BERTScore have a relatively low correlation with human judgments andfail to capture nuanced errors. Recent studies suggest using large languagemodels (LLMs), which have the benefit of better context understanding andadaption of error definitions without training on a large number of humanpreference judgments. However, current LLM-based evaluators risk masking errorsand can only serve as a weak proxy, leaving human evaluation the gold standarddespite being costly and hard to compare across studies. In this work, wepresent MESA, an LLM-based framework employing a three-step assessment ofindividual error types, multi-agent discussion for decision refinement, andfeedback-based self-training to refine error definition understanding andalignment with human judgment. We show that MESA's components enable thorougherror detection, consistent rating, and adaptability to custom errorguidelines. Using GPT-4o as its backbone, MESA achieves mid to highPoint-Biserial correlation with human judgment in error detection and midSpearman and Kendall correlation in reflecting error impact on summary quality,on average 0.25 higher than previous methods. The framework's flexibility inadapting to custom error guidelines makes it suitable for various tasks withlimited human-labeled data.</description><author>Frederic Kirstein, Terry Ruas, Bela Gipp</author><pubDate>Wed, 27 Nov 2024 15:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18444v1</guid></item><item><title>Multiscale Hodge Scattering Networks for Data Analysis</title><link>http://arxiv.org/abs/2311.10270v5</link><description>We propose new scattering networks for signals measured on simplicialcomplexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs).Our construction is based on multiscale basis dictionaries on simplicialcomplexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recentlydeveloped for simplices of dimension $\kappa \in \mathbb{N}$ in a givensimplicial complex by generalizing the node-based Generalized Haar-WalshTransform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The$\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e.,dictionaries) of multiscale basis vectors and the corresponding expansioncoefficients of a given signal. Our MHSNs use a layered structure analogous toa convolutional neural network (CNN) to cascade the moments of the modulus ofthe dictionary coefficients. The resulting features are invariant to reorderingof the simplices (i.e., node permutation of the underlying graphs).Importantly, the use of multiscale basis dictionaries in our MHSNs admits anatural pooling operation that is akin to local pooling in CNNs, and which maybe performed either locally or per-scale. These pooling operations are harderto define in both traditional scattering networks based on Morlet wavelets, andgeometric scattering networks based on Diffusion Wavelets. As a result, we areable to extract a rich set of descriptive yet robust features that can be usedalong with very simple machine learning methods (i.e., logistic regression orsupport vector machines) to achieve high-accuracy classification systems withfar fewer parameters to train than most modern graph neural networks. Finally,we demonstrate the usefulness of our MHSNs in three distinct types of problems:signal classification, domain (i.e., graph/simplex) classification, andmolecular dynamics prediction.</description><author>Naoki Saito, Stefan C. Schonsheck, Eugene Shvarts</author><pubDate>Wed, 27 Nov 2024 15:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10270v5</guid></item><item><title>Metric-DST: Mitigating Selection Bias Through Diversity-Guided Semi-Supervised Metric Learning</title><link>http://arxiv.org/abs/2411.18442v1</link><description>Selection bias poses a critical challenge for fairness in machine learning,as models trained on data that is less representative of the population mightexhibit undesirable behavior for underrepresented profiles. Semi-supervisedlearning strategies like self-training can mitigate selection bias byincorporating unlabeled data into model training to gain further insight intothe distribution of the population. However, conventional self-training seeksto include high-confidence data samples, which may reinforce existing modelbias and compromise effectiveness. We propose Metric-DST, a diversity-guidedself-training strategy that leverages metric learning and its implicitembedding space to counter confidence-based bias through the inclusion of morediverse samples. Metric-DST learned more robust models in the presence ofselection bias for generated and real-world datasets with induced bias, as wellas a molecular biology prediction task with intrinsic bias. The Metric-DSTlearning strategy offers a flexible and widely applicable solution to mitigateselection bias and enhance fairness of machine learning models.</description><author>Yasin I. Tepeli, Mathijs de Wolf, Joana P. Goncalves</author><pubDate>Wed, 27 Nov 2024 15:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18442v1</guid></item><item><title>Learning the Evolution of Physical Structure of Galaxies via Diffusion Models</title><link>http://arxiv.org/abs/2411.18440v1</link><description>In astrophysics, understanding the evolution of galaxies in primarily throughimaging data is fundamental to comprehending the formation of the Universe.This paper introduces a novel approach to conditioning Denoising DiffusionProbabilistic Models (DDPM) on redshifts for generating galaxy images. Weexplore whether this advanced generative model can accurately capture thephysical characteristics of galaxies based solely on their images and redshiftmeasurements. Our findings demonstrate that this model not only producesvisually realistic galaxy images but also encodes the underlying changes inphysical properties with redshift that are the result of galaxy evolution. Thisapproach marks a significant advancement in using generative models to enhanceour scientific insight into cosmic phenomena.</description><author>Andrew Lizarraga, Eric Hanchen Jiang, Jacob Nowack, Yun Qi Li, Ying Nian Wu, Bernie Boscoe, Tuan Do</author><pubDate>Wed, 27 Nov 2024 15:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18440v1</guid></item><item><title>MROVSeg: Breaking the Resolution Curse of Vision-Language Models in Open-Vocabulary Image Segmentation</title><link>http://arxiv.org/abs/2408.14776v2</link><description>Pretrained vision-language models (VLMs), \eg CLIP, are increasingly used tobridge the gap between open- and close-vocabulary recognition inopen-vocabulary image segmentation. As VLMs are generally pretrained withlow-resolution images (e.g. $224\times224$), most previous methods operate onlyon downscaled images. We question this design as low resolution features oftenfail to preserve fine details. A typical solution is to employ additional imagebackbones for high-resolution inputs, but it also introduce significantcomputation overhead. Therefore, we propose MROVSeg, a multi-resolutiontraining framework for open-vocabulary image segmentation with a singlepretrained CLIP backbone, that uses sliding windows to slice thehigh-resolution input into uniform patches, each matching the input size of thewell-trained image encoder. Its key components include a Multi-Res Adapter,which restores the spatial geometry and grasps local-global correspondencesacross patches by interacting with multi-resolution features. To achieveaccurate segmentation, we introduce Multi-grained Masked Attention scheme toaggregate multi-grained semantics from multi-resolution CLIP features to objectqueries. Through comprehensive experiments, we demonstrate the superiority ofMROVSeg on well-established open-vocabulary image segmentation benchmarks,establishing new standards for open-vocabulary image segmentation.</description><author>Yuanbing Zhu, Bingke Zhu, Yingying Chen, Yunfang Niu, Ming Tang, Jinqiao Wang</author><pubDate>Wed, 27 Nov 2024 15:26:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14776v2</guid></item><item><title>Creativity in AI: Progresses and Challenges</title><link>http://arxiv.org/abs/2410.17218v3</link><description>Creativity is the ability to produce novel, useful, and surprising ideas, andhas been widely studied as a crucial aspect of human cognition. Machinecreativity on the other hand has been a long-standing challenge. With the riseof advanced generative AI, there has been renewed interest and debate regardingAI's creative capabilities. Therefore, it is imperative to revisit the state ofcreativity in AI and identify key progresses and remaining challenges. In thiswork, we survey leading works studying the creative capabilities of AI systems,focusing on creative problem-solving, linguistic, artistic, and scientificcreativity. Our review suggests that while the latest AI models are largelycapable of producing linguistically and artistically creative outputs such aspoems, images, and musical pieces, they struggle with tasks that requirecreative problem-solving, abstract thinking and compositionality and theirgenerations suffer from a lack of diversity, originality, long-rangeincoherence and hallucinations. We also discuss key questions concerningcopyright and authorship issues with generative models. Furthermore, wehighlight the need for a comprehensive evaluation of creativity that isprocess-driven and considers several dimensions of creativity. Finally, wepropose future research directions to improve the creativity of AI outputs,drawing inspiration from cognitive science and psychology.</description><author>Mete Ismayilzada, Debjit Paul, Antoine Bosselut, Lonneke van der Plas</author><pubDate>Wed, 27 Nov 2024 15:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17218v3</guid></item><item><title>EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction</title><link>http://arxiv.org/abs/2307.16082v5</link><description>Social platforms have emerged as crucial platforms for disseminatinginformation and discussing real-life social events, offering researchers anexcellent opportunity to design and implement novel event detection frameworks.However, most existing approaches only exploit keyword burstiness or networkstructures to detect unspecified events. Thus, they often need help identifyingunknown events regarding the challenging nature of events and social data.Social data, e.g., tweets, is characterized by misspellings, incompleteness,word sense ambiguation, irregular language, and variation in aspects ofopinions. Moreover, extracting discriminative features and patterns forevolving events by exploiting the limited structural knowledge is almostinfeasible. To address these challenges, in this paper, we propose a novelframework, namely EnrichEvent, that leverages the linguistic and contextualrepresentations of streaming social data. In particular, we leverage contextualand linguistic knowledge to detect semantically related tweets and enhance theeffectiveness of the event detection approaches. Eventually, our proposedframework produces cluster chains for each event to show the evolving variationof the event through time. We conducted extensive experiments to evaluate ourframework, validating its high performance and effectiveness in detecting anddistinguishing unspecified social events.</description><author>Mohammadali Sefidi Esfahani, Mohammad Akbari</author><pubDate>Wed, 27 Nov 2024 15:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16082v5</guid></item><item><title>An End-to-End Smart Predict-then-Optimize Framework for Vehicle Relocation Problems in Large-Scale Vehicle Crowd Sensing</title><link>http://arxiv.org/abs/2411.18432v1</link><description>Ubiquitous mobile devices have catalyzed the development of vehicle crowdsensing (VCS). In particular, vehicle sensing systems show great potential inthe flexible acquisition of spatio-temporal urban data through built-in sensorsunder diverse sensing scenarios. However, vehicle systems often exhibit biasedcoverage due to the heterogeneous nature of trip requests and routes. Toachieve a high sensing coverage, a critical challenge lies in optimallyrelocating vehicles to minimize the divergence between vehicle distributionsand target sensing distributions. Conventional approaches typically employ atwo-stage predict-then-optimize (PTO) process: first predicting real-timevehicle distributions and subsequently generating an optimal relocationstrategy based on the predictions. However, this approach can lead tosuboptimal decision-making due to the propagation of errors from upstreamprediction. To this end, we develop an end-to-end Smart Predict-then-Optimize(SPO) framework by integrating optimization into prediction within the deeplearning architecture, and the entire framework is trained by minimizing thetask-specific matching divergence rather than the upstream prediction error.Methodologically, we formulate the vehicle relocation problem by quadraticprogramming (QP) and incorporate a novel unrolling approach based on theAlternating Direction Method of Multipliers (ADMM) within the SPO framework tocompute gradients of the QP layer, facilitating backpropagation andgradient-based optimization for end-to-end learning. The effectiveness of theproposed framework is validated by real-world taxi datasets in Hong Kong.Utilizing the alternating differentiation method, the general SPO frameworkpresents a novel concept of addressing decision-making problems withuncertainty, demonstrating significant potential for advancing applications inintelligent transportation systems.</description><author>Xinyu Wang, Yiyang Peng, Wei Ma</author><pubDate>Wed, 27 Nov 2024 15:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18432v1</guid></item><item><title>How Does Variance Shape the Regret in Contextual Bandits?</title><link>http://arxiv.org/abs/2410.12713v2</link><description>We consider realizable contextual bandits with general functionapproximation, investigating how small reward variance can lead tobetter-than-minimax regret bounds. Unlike in minimax bounds, we show that theeluder dimension $d_\text{elu}$$-$a complexity measure of the functionclass$-$plays a crucial role in variance-dependent bounds. We consider twotypes of adversary: (1) Weak adversary: The adversary sets the reward variance before observingthe learner's action. In this setting, we prove that a regret of$\Omega(\sqrt{\min\{A,d_\text{elu}\}\Lambda}+d_\text{elu})$ is unavoidable when$d_{\text{elu}}\leq\sqrt{AT}$, where $A$ is the number of actions, $T$ is thetotal number of rounds, and $\Lambda$ is the total variance over $T$ rounds.For the $A\leq d_\text{elu}$ regime, we derive a nearly matching upper bound$\tilde{O}(\sqrt{A\Lambda}+d_\text{elu})$ for the special case where thevariance is revealed at the beginning of each round. (2) Strong adversary: The adversary sets the reward variance after observingthe learner's action. We show that a regret of$\Omega(\sqrt{d_\text{elu}\Lambda}+d_\text{elu})$ is unavoidable when$\sqrt{d_\text{elu}\Lambda}+d_\text{elu}\leq\sqrt{AT}$. In this setting, weprovide an upper bound of order$\tilde{O}(d_\text{elu}\sqrt{\Lambda}+d_\text{elu})$. Furthermore, we examine the setting where the function class additionallyprovides distributional information of the reward, as studied by Wang et al.(2024). We demonstrate that the regret bound$\tilde{O}(\sqrt{d_\text{elu}\Lambda}+d_\text{elu})$ established in their workis unimprovable when $\sqrt{d_{\text{elu}}\Lambda}+d_\text{elu}\leq\sqrt{AT}$.However, with a slightly different definition of the total variance and withthe assumption that the reward follows a Gaussian distribution, one can achievea regret of $\tilde{O}(\sqrt{A\Lambda}+d_\text{elu})$.</description><author>Zeyu Jia, Jian Qian, Alexander Rakhlin, Chen-Yu Wei</author><pubDate>Wed, 27 Nov 2024 15:14:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.12713v2</guid></item><item><title>Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A Benchmark and Empirical Study</title><link>http://arxiv.org/abs/2409.13694v2</link><description>Retrieval-augmented generation (RAG) is increasingly recognized as aneffective approach for mitigating the hallucination of large language models(LLMs) through the integration of external knowledge. While numerous efforts,most studies focus on a single type of externeal knowledge source. However, inreal-world applications, most situations involve diverse knowledge from varioussources, yet this area has been less explored. The main dilemma is the lack ofa suitable dataset containing multiple knowledge sources and pre-exploration ofthe associated issues. To address these challenges, we standardize a benchmarkdataset that combines structured and unstructured knowledge across diverse andcomplementary domains. Based on this dataset, we further develop aplug-and-play RAG framework, PruningRAG, whose main characteristic is to employmulti-granularity pruning strategies for optimizing the integration of relevantinformation and minimizing misleading context. Building upon the standardizeddataset and PruningRAG, we also report a series of experimental results, aswell as insightful findings. Our dataset and code are publiclyavailable\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim ofadvancing future research in the RAG community.</description><author>Shuo Yu, Mingyue Cheng, Jiqian Yang, Jie Ouyang, Yucong Luo, Chenyi Lei, Qi Liu, Enhong Chen</author><pubDate>Wed, 27 Nov 2024 15:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13694v2</guid></item><item><title>MM-Path: Multi-modal, Multi-granularity Path Representation Learning -- Extended Version</title><link>http://arxiv.org/abs/2411.18428v1</link><description>Developing effective path representations has become increasingly essentialacross various fields within intelligent transportation. Although pre-trainedpath representation learning models have shown improved performance, theypredominantly focus on the topological structures from single modality data,i.e., road networks, overlooking the geometric and contextual featuresassociated with path-related images, e.g., remote sensing images. Similar tohuman understanding, integrating information from multiple modalities canprovide a more comprehensive view, enhancing both representation accuracy andgeneralization. However, variations in information granularity impede thesemantic alignment of road network-based paths (road paths) and image-basedpaths (image paths), while the heterogeneity of multi-modal data posessubstantial challenges for effective fusion and utilization. In this paper, wepropose a novel Multi-modal, Multi-granularity Path Representation LearningFramework (MM-Path), which can learn a generic path representation byintegrating modalities from both road paths and image paths. To enhance thealignment of multi-modal data, we develop a multi-granularity alignmentstrategy that systematically associates nodes, road sub-paths, and road pathswith their corresponding image patches, ensuring the synchronization of bothdetailed local information and broader global contexts. To address theheterogeneity of multi-modal data effectively, we introduce a graph-basedcross-modal residual fusion component designed to comprehensively fuseinformation across different modalities and granularities. Finally, we conductextensive experiments on two large-scale real-world datasets under twodownstream tasks, validating the effectiveness of the proposed MM-Path. This isan extended version of the paper accepted by KDD 2025.</description><author>Ronghui Xu, Hanyin Cheng, Chenjuan Guo, Hongfan Gao, Jilin Hu, Sean Bin Yang, Bin Yang</author><pubDate>Wed, 27 Nov 2024 15:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18428v1</guid></item><item><title>Improved Noise Schedule for Diffusion Training</title><link>http://arxiv.org/abs/2407.03297v2</link><description>Diffusion models have emerged as the de facto choice for generatinghigh-quality visual signals across various domains. However, training a singlemodel to predict noise across various levels poses significant challenges,necessitating numerous iterations and incurring significant computationalcosts. Various approaches, such as loss weighting strategy design andarchitectural refinements, have been introduced to expedite convergence andimprove model performance. In this study, we propose a novel approach to designthe noise schedule for enhancing the training of diffusion models. Our keyinsight is that the importance sampling of the logarithm of the Signal-to-Noiseratio ($\log \text{SNR}$), theoretically equivalent to a modified noiseschedule, is particularly beneficial for training efficiency when increasingthe sample frequency around $\log \text{SNR}=0$. This strategic sampling allowsthe model to focus on the critical transition point between signal dominanceand noise dominance, potentially leading to more robust and accuratepredictions.We empirically demonstrate the superiority of our noise scheduleover the standard cosine schedule.Furthermore, we highlight the advantages ofour noise schedule design on the ImageNet benchmark, showing that the designedschedule consistently benefits different prediction targets. Our findingscontribute to the ongoing efforts to optimize diffusion models, potentiallypaving the way for more efficient and effective training paradigms in the fieldof generative AI.</description><author>Tiankai Hang, Shuyang Gu, Xin Geng, Baining Guo</author><pubDate>Wed, 27 Nov 2024 15:10:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03297v2</guid></item><item><title>A Unified Framework for 3D Scene Understanding</title><link>http://arxiv.org/abs/2407.03263v2</link><description>We propose UniSeg3D, a unified 3D scene understanding framework that achievespanoptic, semantic, instance, interactive, referring, and open-vocabularysegmentation tasks within a single model. Most previous 3D segmentationapproaches are typically tailored to a specific task, limiting theirunderstanding of 3D scenes to a task-specific perspective. In contrast, theproposed method unifies six tasks into unified representations processed by thesame Transformer. It facilitates inter-task knowledge sharing, therebypromoting comprehensive 3D scene understanding. To take advantage of multi-taskunification, we enhance performance by establishing explicit inter-taskassociations. Specifically, we design knowledge distillation and contrastivelearning methods to transfer task-specific knowledge across different tasks.Experiments on three benchmarks, including ScanNet20, ScanRefer, andScanNet200, demonstrate that the UniSeg3D consistently outperforms current SOTAmethods, even those specialized for individual tasks. We hope UniSeg3D canserve as a solid unified baseline and inspire future work. Code and models areavailable at https://github.com/dk-liang/UniSeg3D.</description><author>Wei Xu, Chunsheng Shi, Sifan Tu, Xin Zhou, Dingkang Liang, Xiang Bai</author><pubDate>Wed, 27 Nov 2024 15:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03263v2</guid></item><item><title>Streamlining Prediction in Bayesian Deep Learning</title><link>http://arxiv.org/abs/2411.18425v1</link><description>The rising interest in Bayesian deep learning (BDL) has led to a plethora ofmethods for estimating the posterior distribution. However, efficientcomputation of inferences, such as predictions, has been largely overlookedwith Monte Carlo integration remaining the standard. In this work we examinestreamlining prediction in BDL through a single forward pass without sampling.For this we use local linearisation on activation functions and local Gaussianapproximations at linear layers. Thus allowing us to analytically compute anapproximation to the posterior predictive distribution. We showcase ourapproach for both MLP and transformers, such as ViT and GPT-2, and assess itsperformance on regression and classification tasks.</description><author>Rui Li, Marcus Klasson, Arno Solin, Martin Trapp</author><pubDate>Wed, 27 Nov 2024 15:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18425v1</guid></item><item><title>FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware Large Language Model Serving</title><link>http://arxiv.org/abs/2411.18424v1</link><description>Serving numerous users and requests concurrently requires good fairness inLarge Language Models (LLMs) serving system. This ensures that, at the samecost, the system can meet the Service Level Objectives (SLOs) of more users ,such as time to first token (TTFT) and time between tokens (TBT), rather thanallowing a few users to experience performance far exceeding the SLOs. Toachieve better fairness, the preemption-based scheduling policy dynamicallyadjusts the priority of each request to maintain balance during runtime.However, existing systems tend to overly prioritize throughput, overlooking theoverhead caused by preemption-induced context switching, which is crucial formaintaining fairness through priority adjustments. In this work, we identifythree main challenges that result in this overhead. 1) Inadequate I/Outilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turnconversations. Our key insight is that the block-based KV cache memory policyin existing systems, while achieving near-zero memory waste, leads todiscontinuity and insufficient granularity in the KV cache memory. To respond,we introduce FastSwitch, a fairness-aware serving system that not only alignswith existing KV cache memory allocation policy but also mitigates contextswitching overhead. Our evaluation shows that FastSwitch outperforms thestate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x acrossdifferent tail TTFT and TBT.</description><author>Ao Shen, Zhiyao Li, Mingyu Gao</author><pubDate>Wed, 27 Nov 2024 15:07:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18424v1</guid></item><item><title>Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention</title><link>http://arxiv.org/abs/2405.11616v3</link><description>In this paper, we introduce Era3D, a novel multiview diffusion method thatgenerates high-resolution multiview images from a single-view image. Despitesignificant advancements in multiview generation, existing methods still sufferfrom camera prior mismatch, inefficacy, and low resolution, resulting inpoor-quality multiview images. Specifically, these methods assume that theinput images should comply with a predefined camera type, e.g. a perspectivecamera with a fixed focal length, leading to distorted shapes when theassumption fails. Moreover, the full-image or dense multiview attention theyemploy leads to an exponential explosion of computational complexity as imageresolution increases, resulting in prohibitively expensive training costs. Tobridge the gap between assumption and reality, Era3D first proposes adiffusion-based camera prediction module to estimate the focal length andelevation of the input image, which allows our method to generate imageswithout shape distortions. Furthermore, a simple but efficient attention layer,named row-wise attention, is used to enforce epipolar priors in the multiviewdiffusion, facilitating efficient cross-view information fusion. Consequently,compared with state-of-the-art methods, Era3D generates high-quality multiviewimages with up to a 512*512 resolution while reducing computation complexity by12x times. Comprehensive experiments demonstrate that Era3D can reconstructhigh-quality and detailed 3D meshes from diverse single-view input images,significantly outperforming baseline multiview diffusion methods. Project page:https://penghtyx.github.io/Era3D/.</description><author>Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, Wenping Wang, Qifeng Liu, Yike Guo</author><pubDate>Wed, 27 Nov 2024 15:01:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11616v3</guid></item><item><title>Probabilistic size-and-shape functional mixed models</title><link>http://arxiv.org/abs/2411.18416v1</link><description>The reliable recovery and uncertainty quantification of a fixed effectfunction $\mu$ in a functional mixed model, for modelling population- andobject-level variability in noisily observed functional data, is a notoriouslychallenging task: variations along the $x$ and $y$ axes are confounded withadditive measurement error, and cannot in general be disentangled. The questionthen as to what properties of $\mu$ may be reliably recovered becomesimportant. We demonstrate that it is possible to recover the size-and-shape ofa square-integrable $\mu$ under a Bayesian functional mixed model. Thesize-and-shape of $\mu$ is a geometric property invariant to a family ofspace-time unitary transformations, viewed as rotations of the Hilbert space,that jointly transform the $x$ and $y$ axes. A random object-level unitarytransformation then captures size-and-shape \emph{preserving} deviations of$\mu$ from an individual function, while a random linear term and measurementerror capture size-and-shape \emph{altering} deviations. The model isregularized by appropriate priors on the unitary transformations, posteriorsummaries of which may then be suitably interpreted as optimal data-drivenrotations of a fixed orthonormal basis for the Hilbert space. Our numericalexperiments demonstrate utility of the proposed model, and superiority over thecurrent state-of-the-art.</description><author>Fangyi Wang, Karthik Bharath, Oksana Chkrebtii, Sebastian Kurtek</author><pubDate>Wed, 27 Nov 2024 15:00:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18416v1</guid></item><item><title>SuperMat: Physically Consistent PBR Material Estimation at Interactive Rates</title><link>http://arxiv.org/abs/2411.17515v2</link><description>Decomposing physically-based materials from images into their constituentproperties remains challenging, particularly when maintaining bothcomputational efficiency and physical consistency. While recent diffusion-basedapproaches have shown promise, they face substantial computational overhead dueto multiple denoising steps and separate models for different materialproperties. We present SuperMat, a single-step framework that achieveshigh-quality material decomposition with one-step inference. This enablesend-to-end training with perceptual and re-render losses while decomposingalbedo, metallic, and roughness maps at millisecond-scale speeds. We furtherextend our framework to 3D objects through a UV refinement network, enablingconsistent material estimation across viewpoints while maintaining efficiency.Experiments demonstrate that SuperMat achieves state-of-the-art PBR materialdecomposition quality while reducing inference time from seconds tomilliseconds per image, and completes PBR material estimation for 3D objects inapproximately 3 seconds. The project page is athttps://hyj542682306.github.io/SuperMat/.</description><author>Yijia Hong, Yuan-Chen Guo, Ran Yi, Yulong Chen, Yan-Pei Cao, Lizhuang Ma</author><pubDate>Wed, 27 Nov 2024 14:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17515v2</guid></item><item><title>Neural Image Unfolding: Flattening Sparse Anatomical Structures using Neural Fields</title><link>http://arxiv.org/abs/2411.18415v1</link><description>Tomographic imaging reveals internal structures of 3D objects and is crucialfor medical diagnoses. Visualizing the morphology and appearance of non-planarsparse anatomical structures that extend over multiple 2D slices in tomographicvolumes is inherently difficult but valuable for decision-making and reporting.Hence, various organ-specific unfolding techniques exist to map their denselysampled 3D surfaces to a distortion-minimized 2D representation. However, thereis no versatile framework to flatten complex sparse structures includingvascular, duct or bone systems. We deploy a neural field to fit thetransformation of the anatomy of interest to a 2D overview image. We furtherpropose distortion regularization strategies and combine geometric withintensity-based loss formulations to also display non-annotated and auxiliarytargets. In addition to improved versatility, our unfolding techniqueoutperforms mesh-based baselines for sparse structures w.r.t. peak distortionand our regularization scheme yields smoother transformations compared toJacobian formulations from neural field-based image registration.</description><author>Leonhard Rist, Pluvio Stephan, Noah Maul, Linda Vorberg, Hendrik Ditt, Michael Sühling, Andreas Maier, Bernhard Egger, Oliver Taubmann</author><pubDate>Wed, 27 Nov 2024 14:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.18415v1</guid></item></channel></rss>