<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 19 Mar 2024 06:00:02 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap Priors</title><link>http://arxiv.org/abs/2403.10521v1</link><description>Autonomous vehicles are gradually entering city roads today, with the help ofhigh-definition maps (HDMaps). However, the reliance on HDMaps preventsautonomous vehicles from stepping into regions without this expensive digitalinfrastructure. This fact drives many researchers to study online HDMapgeneration algorithms, but the performance of these algorithms at far regionsis still unsatisfying. We present P-MapNet, in which the letter P highlightsthe fact that we focus on incorporating map priors to improve modelperformance. Specifically, we exploit priors in both SDMap and HDMap. On onehand, we extract weakly aligned SDMap from OpenStreetMap, and encode it as anadditional conditioning branch. Despite the misalignment challenge, ourattention-based architecture adaptively attends to relevant SDMap skeletons andsignificantly improves performance. On the other hand, we exploit a maskedautoencoder to capture the prior distribution of HDMap, which can serve as arefinement module to mitigate occlusions and artifacts. We benchmark on thenuScenes and Argoverse2 datasets. Through comprehensive experiments, we showthat: (1) our SDMap prior can improve online map generation performance, usingboth rasterized (by up to $+18.73$ $\rm mIoU$) and vectorized (by up to $+8.50$$\rm mAP$) output representations. (2) our HDMap prior can improve mapperceptual metrics by up to $6.34\%$. (3) P-MapNet can be switched intodifferent inference modes that covers different regions of theaccuracy-efficiency trade-off landscape. (4) P-MapNet is a far-seeing solutionthat brings larger improvements on longer ranges. Codes and models are publiclyavailable at https://jike5.github.io/P-MapNet.</description><author>Zhou Jiang, Zhenxin Zhu, Pengfei Li, Huan-ang Gao, Tianyuan Yuan, Yongliang Shi, Hang Zhao, Hao Zhao</author><pubDate>Fri, 15 Mar 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10521v1</guid></item><item><title>Strong and Controllable Blind Image Decomposition</title><link>http://arxiv.org/abs/2403.10520v1</link><description>Blind image decomposition aims to decompose all components present in animage, typically used to restore a multi-degraded input image. While fullyrecovering the clean image is appealing, in some scenarios, users might want toretain certain degradations, such as watermarks, for copyright protection. Toaddress this need, we add controllability to the blind image decompositionprocess, allowing users to enter which types of degradation to remove orretain. We design an architecture named controllable blind image decompositionnetwork. Inserted in the middle of U-Net structure, our method first decomposesthe input feature maps and then recombines them according to user instructions.Advantageously, this functionality is implemented at minimal computationalcost: decomposition and recombination are all parameter-free. Experimentally,our system excels in blind image decomposition tasks and can outputs partiallyor fully restored images that well reflect user intentions. Furthermore, weevaluate and configure different options for the network structure and lossfunctions. This, combined with the proposed decomposition-and-recombinationmethod, yields an efficient and competitive system for blind imagedecomposition, compared with current state-of-the-art methods.</description><author>Zeyu Zhang, Junlin Han, Chenhui Gou, Hongdong Li, Liang Zheng</author><pubDate>Fri, 15 Mar 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10520v1</guid></item><item><title>Frozen Feature Augmentation for Few-Shot Image Classification</title><link>http://arxiv.org/abs/2403.10519v1</link><description>Training a linear classifier or lightweight model on top of pretrained visionmodel outputs, so-called 'frozen features', leads to impressive performance ona number of downstream few-shot tasks. Currently, frozen features are notmodified during training. On the other hand, when networks are trained directlyon images, data augmentation is a standard recipe that improves performancewith no substantial overhead. In this paper, we conduct an extensive pilotstudy on few-shot image classification that explores applying dataaugmentations in the frozen feature space, dubbed 'frozen feature augmentation(FroFA)', covering twenty augmentations in total. Our study demonstrates thatadopting a deceptively simple pointwise FroFA, such as brightness, can improvefew-shot performance consistently across three network architectures, threelarge pretraining datasets, and eight transfer datasets.</description><author>Andreas BÃ¤r, Neil Houlsby, Mostafa Dehghani, Manoj Kumar</author><pubDate>Fri, 15 Mar 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10519v1</guid></item><item><title>Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives</title><link>http://arxiv.org/abs/2403.10518v1</link><description>We propose Lodge, a network capable of generating extremely long dancesequences conditioned on given music. We design Lodge as a two-stage coarse tofine diffusion architecture, and propose the characteristic dance primitivesthat possess significant expressiveness as intermediate representations betweentwo diffusion models. The first stage is global diffusion, which focuses oncomprehending the coarse-level music-dance correlation and productioncharacteristic dance primitives. In contrast, the second-stage is the localdiffusion, which parallelly generates detailed motion sequences under theguidance of the dance primitives and choreographic rules. In addition, wepropose a Foot Refine Block to optimize the contact between the feet and theground, enhancing the physical realism of the motion. Our approach canparallelly generate dance sequences of extremely long length, striking abalance between global choreographic patterns and local motion quality andexpressiveness. Extensive experiments validate the efficacy of our method.</description><author>Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li</author><pubDate>Fri, 15 Mar 2024 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10518v1</guid></item><item><title>Extraction of Sleep Information from Clinical Notes of Patients with Alzheimer's Disease Using Natural Language Processing</title><link>http://arxiv.org/abs/2204.09601v2</link><description>Alzheimer's Disease (AD) is the most common form of dementia in the UnitedStates. Sleep is one of the lifestyle-related factors that has been showncritical for optimal cognitive function in old age. However, there is a lack ofresearch studying the association between sleep and AD incidence. A majorbottleneck for conducting such research is that the traditional way to acquiresleep information is time-consuming, inefficient, non-scalable, and limited topatients' subjective experience. A gold standard dataset is created from manualannotation of 570 randomly sampled clinical note documents from the adSLEEP, acorpus of 192,000 de-identified clinical notes of 7,266 AD patients retrievedfrom the University of Pittsburgh Medical Center (UPMC). We developed arule-based Natural Language Processing (NLP) algorithm, machine learningmodels, and Large Language Model(LLM)-based NLP algorithms to automate theextraction of sleep-related concepts, including snoring, napping, sleepproblem, bad sleep quality, daytime sleepiness, night wakings, and sleepduration, from the gold standard dataset. Rule-based NLP algorithm achieved thebest performance of F1 across all sleep-related concepts. In terms of PositivePredictive Value (PPV), rule-based NLP algorithm achieved 1.00 for daytimesleepiness and sleep duration, machine learning models: 0.95 and for napping,0.86 for bad sleep quality and 0.90 for snoring; and LLAMA2 with finetuningachieved PPV of 0.93 for Night Wakings, 0.89 for sleep problem, and 1.00 forsleep duration. The results show that the rule-based NLP algorithm consistentlyachieved the best performance for all sleep concepts. This study focused on theclinical notes of patients with AD, but could be extended to general sleepinformation extraction for other diseases.</description><author>Sonish Sivarajkumar, Thomas Yu CHow Tam, Haneef Ahamed Mohammad, Samual Viggiano, David Oniani, Shyam Visweswaran, Yanshan Wang</author><pubDate>Fri, 15 Mar 2024 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.09601v2</guid></item><item><title>How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries</title><link>http://arxiv.org/abs/2402.15302v4</link><description>In this study, we tackle a growing concern around the safety and ethical useof large language models (LLMs). Despite their potential, these models can betricked into producing harmful or unethical content through varioussophisticated methods, including 'jailbreaking' techniques and targetedmanipulation. Our work zeroes in on a specific issue: to what extent LLMs canbe led astray by asking them to generate responses that are instruction-centricsuch as a pseudocode, a program or a software snippet as opposed to vanillatext. To investigate this question, we introduce TechHazardQA, a datasetcontaining complex queries which should be answered in both text andinstruction-centric formats (e.g., pseudocodes), aimed at identifying triggersfor unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b,Mistral-V2 and Mistral 8X7B -- and ask them to generate both text andinstruction-centric responses. For evaluation we report the harmfulness scoremetric as well as judgements from GPT-4 and humans. Overall, we observe thatasking LLMs to produce instruction-centric responses enhances the unethicalresponse generation by ~2-38% across the models. As an additional objective, weinvestigate the impact of model editing using the ROME technique, which furtherincreases the propensity for generating undesirable content. In particular,asking edited LLMs to generate instruction-centric responses further increasesthe unethical response generation by ~3-16% across the different models.</description><author>Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee</author><pubDate>Fri, 15 Mar 2024 18:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15302v4</guid></item><item><title>VideoAgent: Long-form Video Understanding with Large Language Model as Agent</title><link>http://arxiv.org/abs/2403.10517v1</link><description>Long-form video understanding represents a significant challenge withincomputer vision, demanding a model capable of reasoning over long multi-modalsequences. Motivated by the human cognitive process for long-form videounderstanding, we emphasize interactive reasoning and planning over the abilityto process lengthy visual inputs. We introduce a novel agent-based system,VideoAgent, that employs a large language model as a central agent toiteratively identify and compile crucial information to answer a question, withvision-language foundation models serving as tools to translate and retrievevisual information. Evaluated on the challenging EgoSchema and NExT-QAbenchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only8.4 and 8.2 frames used on average. These results demonstrate superioreffectiveness and efficiency of our method over the current state-of-the-artmethods, highlighting the potential of agent-based approaches in advancinglong-form video understanding.</description><author>Xiaohan Wang, Yuhui Zhang, Orr Zohar, Serena Yeung-Levy</author><pubDate>Fri, 15 Mar 2024 18:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10517v1</guid></item><item><title>FeatUp: A Model-Agnostic Framework for Features at Any Resolution</title><link>http://arxiv.org/abs/2403.10516v1</link><description>Deep features are a cornerstone of computer vision research, capturing imagesemantics and enabling the community to solve downstream tasks even in thezero- or few-shot regime. However, these features often lack the spatialresolution to directly perform dense prediction tasks like segmentation anddepth prediction because models aggressively pool information over large areas.In this work, we introduce FeatUp, a task- and model-agnostic framework torestore lost spatial information in deep features. We introduce two variants ofFeatUp: one that guides features with high-resolution signal in a singleforward pass, and one that fits an implicit model to a single image toreconstruct features at any resolution. Both approaches use a multi-viewconsistency loss with deep analogies to NeRFs. Our features retain theiroriginal semantics and can be swapped into existing applications to yieldresolution and performance gains even without re-training. We show that FeatUpsignificantly outperforms other feature upsampling and image super-resolutionapproaches in class activation map generation, transfer learning forsegmentation and depth prediction, and end-to-end training for semanticsegmentation.</description><author>Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong Zhang, William T. Freeman</author><pubDate>Fri, 15 Mar 2024 18:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10516v1</guid></item><item><title>Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study</title><link>http://arxiv.org/abs/2303.13466v2</link><description>Post-stroke patient rehabilitation requires precise, personalized treatmentplans. Natural Language Processing (NLP) offers potential to extract valuableexercise information from clinical notes, aiding in the development of moreeffective rehabilitation strategies. Objective: This study aims to develop andevaluate a variety of NLP algorithms to extract and categorize physicalrehabilitation exercise information from the clinical notes of post-strokepatients treated at the University of Pittsburgh Medical Center. A cohort of13,605 patients diagnosed with stroke was identified, and their clinical notescontaining rehabilitation therapy notes were retrieved. A comprehensiveclinical ontology was created to represent various aspects of physicalrehabilitation exercises. State-of-the-art NLP algorithms were then developedand compared, including rule-based, machine learning-based algorithms, andlarge language model (LLM)-based algorithms (ChatGPT). Analysis was conductedon a dataset comprising 23,724 notes with detailed demographic and clinicalcharacteristics. The rule-based NLP algorithm demonstrated superior performancein most areas, particularly in detecting the 'Right Side' location with an F1score of 0.975, outperforming Gradient Boosting by 0.063. Gradient Boostingexcelled in 'Lower Extremity' location detection (F1 score: 0.978), surpassingrule-based NLP by 0.023. It also showed notable performance in 'Passive Rangeof Motion' with an F1 score of 0.970, a 0.032 improvement over rule-based NLP.The rule-based algorithm efficiently handled 'Duration', 'Sets', and 'Reps'with F1 scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shotprompts, achieved high recall but generally lower precision and F1 scores.However, it notably excelled in 'Backward Plane' motion detection, achieving anF1 score of 0.846, surpassing the rule-based algorithm's 0.720.</description><author>Sonish Sivarajkumar, Fengyi Gao, Parker E. Denny, Bayan M. Aldhahwani, Shyam Visweswaran, Allyn Bove, Yanshan Wang</author><pubDate>Fri, 15 Mar 2024 18:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13466v2</guid></item><item><title>A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction</title><link>http://arxiv.org/abs/2403.10511v1</link><description>Gaze following and social gaze prediction are fundamental tasks providinginsights into human communication behaviors, intent, and social interactions.Most previous approaches addressed these tasks separately, either by designinghighly specialized social gaze models that do not generalize to other socialgaze tasks or by considering social gaze inference as an ad-hoc post-processingof the gaze following task. Furthermore, the vast majority of gaze followingapproaches have proposed static models that can handle only one person at atime, therefore failing to take advantage of social interactions and temporaldynamics. In this paper, we address these limitations and introduce a novelframework to jointly predict the gaze target and social gaze label for allpeople in the scene. The framework comprises of: (i) a temporal,transformer-based architecture that, in addition to image tokens, handlesperson-specific tokens capturing the gaze information related to eachindividual; (ii) a new dataset, VSGaze, that unifies annotation types acrossmultiple gaze following and social gaze datasets. We show that our modeltrained on VSGaze can address all tasks jointly, and achieves state-of-the-artresults for multi-person gaze following and social gaze prediction.</description><author>Anshul Gupta, Samy Tafasca, Arya Farkhondeh, Pierre Vuillecard, Jean-Marc Odobez</author><pubDate>Fri, 15 Mar 2024 18:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10511v1</guid></item><item><title>Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models</title><link>http://arxiv.org/abs/2403.06199v3</link><description>Multimodal Large Language Models (MLLMs) have showcased impressive skills intasks related to visual understanding and reasoning. Yet, their widespreadapplication faces obstacles due to the high computational demands during boththe training and inference phases, restricting their use to a limited audiencewithin the research and user communities. In this paper, we investigate thedesign aspects of Multimodal Small Language Models (MSLMs) and propose anefficient multimodal assistant named Mipha, which is designed to create synergyamong various aspects: visual representation, language models, and optimizationstrategies. We show that without increasing the volume of training data, ourMipha-3B outperforms the state-of-the-art large MLLMs, especiallyLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provideinsights and guidelines for developing strong MSLMs that rival the capabilitiesof MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.</description><author>Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang</author><pubDate>Fri, 15 Mar 2024 18:48:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06199v3</guid></item><item><title>HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation</title><link>http://arxiv.org/abs/2403.10506v1</link><description>Humanoid robots hold great promise in assisting humans in diverseenvironments and tasks, due to their flexibility and adaptability leveraginghuman-like morphology. However, research in humanoid robots is oftenbottlenecked by the costly and fragile hardware setups. To acceleratealgorithmic research in humanoid robots, we present a high-dimensional,simulated robot learning benchmark, HumanoidBench, featuring a humanoid robotequipped with dexterous hands and a variety of challenging whole-bodymanipulation and locomotion tasks. Our findings reveal that state-of-the-artreinforcement learning algorithms struggle with most tasks, whereas ahierarchical learning baseline achieves superior performance when supported byrobust low-level policies, such as walking or reaching. With HumanoidBench, weprovide the robotics community with a platform to identify the challengesarising when solving diverse tasks with humanoid robots, facilitating promptverification of algorithms and ideas. The open-source code is available athttps://sferrazza.cc/humanoidbench_site.</description><author>Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, Pieter Abbeel</author><pubDate>Fri, 15 Mar 2024 18:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10506v1</guid></item><item><title>Belief Change based on Knowledge Measures</title><link>http://arxiv.org/abs/2403.10502v1</link><description>Knowledge Measures (KMs) aim at quantifying the amount ofknowledge/information that a knowledge base carries. On the other hand, BeliefChange (BC) is the process of changing beliefs (in our case, in terms ofcontraction, expansion and revision) taking into account a new piece ofknowledge, which possibly may be in contradiction with the current belief. Wepropose a new quantitative BC framework that is based on KMs by defining beliefchange operators that try to minimise, from an information-theoretic point ofview, the surprise that the changed belief carries. To this end, we introducethe principle of minimal surprise. In particular, our contributions are (i) ageneral information-theoretic approach to KMs for which [1] is a special case;(ii) KM-based BC operators that satisfy the so-called AGM postulates; and (iii)a characterisation of any BC operator that satisfies the AGM postulates as aKM-based BC operator, i.e., any BC operator satisfying the AGM postulates canbe encoded within our quantitative BC framework. We also introduce quantitativemeasures that account for the information loss of contraction, information gainof expansion and information change of revision. We also give a succinct lookinto the problem of iterated revision, which deals with the application of asequence of revision operations in our framework, and also illustrate how onemay build from our KM-based contraction operator also one not satisfying the(in)famous recovery postulate, by focusing on the so-called severe withdrawalmodel as an illustrative example.</description><author>Umberto Straccia, Giovanni Casini</author><pubDate>Fri, 15 Mar 2024 18:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10502v1</guid></item><item><title>Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study</title><link>http://arxiv.org/abs/2403.10499v1</link><description>Pre-training image representations from the raw text about images enableszero-shot vision transfer to downstream tasks. Through pre-training on millionsof samples collected from the internet, multimodal foundation models, such asCLIP, produce state-of-the-art zero-shot results that often reachcompetitiveness with fully supervised methods without the need fortask-specific training. Besides the encouraging performance on classificationaccuracy, it is reported that these models close the robustness gap by matchingthe performance of supervised models trained on ImageNet under naturaldistribution shift. Because robustness is critical to real-world applications,especially safety-critical ones, in this paper, we present a comprehensiveevaluation based on a large-scale robustness benchmark covering 7 natural, 3synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as apilot study. We show that CLIP leads to a significant robustness drop comparedto supervised ImageNet models on our benchmark, especially under syntheticdistribution shift and adversarial attacks. Furthermore, data overlap analysissuggests that the observed robustness under natural distribution shifts couldbe attributed, at least in part, to data overlap. In summary, our evaluationshows a comprehensive evaluation of robustness is necessary; and there is asignificant need to improve the robustness of zero-shot multimodal models.</description><author>Chenguang Wang, Ruoxi Jia, Xin Liu, Dawn Song</author><pubDate>Fri, 15 Mar 2024 18:33:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10499v1</guid></item><item><title>Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings</title><link>http://arxiv.org/abs/2403.10497v1</link><description>Algorithmic verification of realistic systems to satisfy safety and othertemporal requirements has suffered from poor scalability of the employed formalapproaches. To design systems with rigorous guarantees, many approaches stillrely on exact models of the underlying systems. Since this assumption canrarely be met in practice, models have to be inferred from measurement data orare bypassed completely. Whilst former usually requires the model structure tobe known a-priori and immense amounts of data to be available, latter givesrise to a plethora of restrictive mathematical assumptions about the unknowndynamics. In a pursuit of developing scalable formal verification algorithmswithout shifting the problem to unrealistic assumptions, we employ the conceptof barrier certificates, which can guarantee safety of the system, and learnthe certificate directly from a compact set of system trajectories. We useconditional mean embeddings to embed data from the system into a reproducingkernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can beinflated to robustify the result w.r.t. a set of plausible transition kernels.We show how to solve the resulting program efficiently using sum-of-squaresoptimization and a Gaussian process envelope. Our approach lifts the need forrestrictive assumptions on the system dynamics and uncertainty, and suggests animprovement in the sample complexity of verifying the safety of a system on atested case study compared to a state-of-the-art approach.</description><author>Oliver SchÃ¶n, Zhengang Zhong, Sadegh Soudjani</author><pubDate>Fri, 15 Mar 2024 18:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10497v1</guid></item><item><title>Mitigating Dialogue Hallucination for Large Multi-modal Models via Adversarial Instruction Tuning</title><link>http://arxiv.org/abs/2403.10492v1</link><description>Mitigating hallucinations of Large Multi-modal Models(LMMs) is crucial toenhance their reliability for general-purpose assistants. This paper shows thatsuch hallucinations of LMMs can be significantly exacerbated by precedinguser-system dialogues. To precisely measure this, we first present anevaluation benchmark by extending popular multi-modal benchmark datasets withprepended hallucinatory dialogues generated by our novel Adversarial QuestionGenerator, which can automatically generate image-related yet adversarialdialogues by adopting adversarial attacks on LMMs. On our benchmark, thezero-shot performance of state-of-the-art LMMs dropped significantly for boththe VQA and Captioning tasks. Next, we further reveal this hallucination ismainly due to the prediction bias toward preceding dialogues rather than visualcontent. To reduce this bias, we propose Adversarial Instruction Tuning thatrobustly fine-tunes LMMs on augmented multi-modal instruction-followingdatasets with hallucinatory dialogues. Extensive experiments show that ourproposed approach successfully reduces dialogue hallucination while maintainingor even improving performance.</description><author>Dongmin Park, Zhaofang Qian, Guangxing Han, Ser-Nam Lim</author><pubDate>Fri, 15 Mar 2024 18:27:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10492v1</guid></item><item><title>HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models</title><link>http://arxiv.org/abs/2312.06553v2</link><description>We address the problem of generating realistic 3D human-object interactions(HOIs) driven by textual prompts. To this end, we take a modular design anddecompose the complex task into simpler sub-tasks. We first develop adual-branch diffusion model (HOI-DM) to generate both human and object motionsconditioned on the input text, and encourage coherent motions by across-attention communication module between the human and object motiongeneration branches. We also develop an affordance prediction diffusion model(APDM) to predict the contacting area between the human and object during theinteractions driven by the textual prompt. The APDM is independent of theresults by the HOI-DM and thus can correct potential errors by the latter.Moreover, it stochastically generates the contacting points to diversify thegenerated motions. Finally, we incorporate the estimated contacting points intothe classifier-guidance to achieve accurate and close contact between humansand objects. To train and evaluate our approach, we annotate BEHAVE datasetwith text descriptions. Experimental results on BEHAVE and OMOMO demonstratethat our approach produces realistic HOIs with various interactions anddifferent types of objects.</description><author>Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, Huaizu Jiang</author><pubDate>Fri, 15 Mar 2024 18:24:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06553v2</guid></item><item><title>Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild</title><link>http://arxiv.org/abs/2403.10488v1</link><description>Audiovisual emotion recognition (ER) in videos has immense potential overunimodal performance. It effectively leverages the inter- and intra-modaldependencies between visual and auditory modalities. This work proposes a novelaudio-visual emotion recognition system utilizing a joint multimodaltransformer architecture with key-based cross-attention. This framework aims toexploit the complementary nature of audio and visual cues (facial expressionsand vocal patterns) in videos, leading to superior performance compared tosolely relying on a single modality. The proposed model leverages separatebackbones for capturing intra-modal temporal dependencies within each modality(audio and visual). Subsequently, a joint multimodal transformer architectureintegrates the individual modality embeddings, enabling the model toeffectively capture inter-modal (between audio and visual) and intra-modal(within each modality) relationships. Extensive evaluations on the challengingAffwild2 dataset demonstrate that the proposed model significantly outperformsbaseline and state-of-the-art methods in ER tasks.</description><author>Paul Waligora, Osama Zeeshan, Haseeb Aslam, Soufiane Belharbi, Alessandro Lameiras Koerich, Marco Pedersoli, Simon Bacon, Eric Granger</author><pubDate>Fri, 15 Mar 2024 18:23:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10488v1</guid></item><item><title>Metrics on Markov Equivalence Classes for Evaluating Causal Discovery Algorithms</title><link>http://arxiv.org/abs/2402.04952v2</link><description>Many state-of-the-art causal discovery methods aim to generate an outputgraph that encodes the graphical separation and connection statements of thecausal graph that underlies the data-generating process. In this work, we arguethat an evaluation of a causal discovery method against synthetic data shouldinclude an analysis of how well this explicit goal is achieved by measuring howclosely the separations/connections of the method's output align with those ofthe ground truth. We show that established evaluation measures do notaccurately capture the difference in separations/connections of two causalgraphs, and we introduce three new measures of distance called s/c-distance,Markov distance and Faithfulness distance that address this shortcoming. Wecomplement our theoretical analysis with toy examples, empirical experimentsand pseudocode.</description><author>Jonas Wahl, Jakob Runge</author><pubDate>Fri, 15 Mar 2024 18:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04952v2</guid></item><item><title>Stimulate the Potential of Robots via Competition</title><link>http://arxiv.org/abs/2403.10487v1</link><description>It is common for us to feel pressure in a competition environment, whicharises from the desire to obtain success comparing with other individuals oropponents. Although we might get anxious under the pressure, it could also be adrive for us to stimulate our potentials to the best in order to keep up withothers. Inspired by this, we propose a competitive learning framework which isable to help individual robot to acquire knowledge from the competition, fullystimulating its dynamics potential in the race. Specifically, the competitioninformation among competitors is introduced as the additional auxiliary signalto learn advantaged actions. We further build a Multiagent-Race environment,and extensive experiments are conducted, demonstrating that robots trained incompetitive environments outperform ones that are trained with SoTA algorithmsin single robot environment.</description><author>Kangyao Huang, Di Guo, Xinyu Zhang, Xiangyang Ji, Huaping Liu</author><pubDate>Fri, 15 Mar 2024 18:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10487v1</guid></item><item><title>Geometry of the Visual Cortex with Applications to Image Inpainting and Enhancement</title><link>http://arxiv.org/abs/2308.07652v3</link><description>Equipping the rototranslation group $SE(2)$ with a sub-Riemannian structureinspired by the visual cortex V1, we propose algorithms for image inpaintingand enhancement based on hypoelliptic diffusion. We innovate on previousimplementations of the methods by Citti, Sarti, and Boscain et al., byproposing an alternative that prevents fading and is capable of producingsharper results in a procedure that we call WaxOn-WaxOff. We also exploit thesub-Riemannian structure to define a completely new unsharp filter using$SE(2)$, analogous to the classical unsharp filter for 2D image processing. Wedemonstrate our method on blood vessels enhancement in retinal scans.</description><author>Francesco Ballerin, Erlend Grong</author><pubDate>Fri, 15 Mar 2024 18:20:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07652v3</guid></item><item><title>Differentiable Euler Characteristic Transforms for Shape Classification</title><link>http://arxiv.org/abs/2310.07630v2</link><description>The Euler Characteristic Transform (ECT) has proven to be a powerfulrepresentation, combining geometrical and topological characteristics of shapesand graphs. However, the ECT was hitherto unable to learn task-specificrepresentations. We overcome this issue and develop a novel computational layerthat enables learning the ECT in an end-to-end fashion. Our method, theDifferentiable Euler Characteristic Transform (DECT), is fast andcomputationally efficient, while exhibiting performance on a par with morecomplex models in both graph and point cloud classification tasks. Moreover, weshow that this seemingly simple statistic provides the same topologicalexpressivity as more complex topological deep learning layers.</description><author>Ernst Roell, Bastian Rieck</author><pubDate>Fri, 15 Mar 2024 18:17:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07630v2</guid></item><item><title>Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?</title><link>http://arxiv.org/abs/2403.10482v1</link><description>Performance attribution analysis, defined as the process of explaining thedrivers of the excess performance of an investment portfolio against abenchmark, stands as a significant aspect of portfolio management and plays acrucial role in the investment decision-making process, particularly within thefund management industry. Rooted in a solid financial and mathematicalframework, the importance and methodologies of this analytical technique areextensively documented across numerous academic research papers and books. Theintegration of large language models (LLMs) and AI agents marks agroundbreaking development in this field. These agents are designed to automateand enhance the performance attribution analysis by accurately calculating andanalyzing portfolio performances against benchmarks. In this study, weintroduce the application of an AI Agent for a variety of essential performanceattribution tasks, including the analysis of performance drivers and utilizingLLMs as calculation engine for multi-level attribution analysis andquestion-answer (QA) exercises. Leveraging advanced prompt engineeringtechniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), andemploying a standard agent framework from LangChain, the research achievespromising results: it achieves accuracy rates exceeding 93% in analyzingperformance drivers, attains 100% in multi-level attribution calculations, andsurpasses 84% accuracy in QA exercises that simulate official examinationstandards. These findings affirm the impactful role of AI agents, promptengineering and evaluation in advancing portfolio management processes,highlighting a significant advancement in the practical application andevaluation of AI technologies within the domain.</description><author>Bruno de Melo</author><pubDate>Fri, 15 Mar 2024 18:12:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10482v1</guid></item><item><title>Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity</title><link>http://arxiv.org/abs/2401.07348v4</link><description>The advent of Generative AI, particularly through Large Language Models(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AIlandscape. Advanced LLMs exhibit multimodality, handling diverse data formats,thereby broadening their application scope. However, the complexity andemergent autonomy of these models introduce challenges in predictability andlegal compliance. This paper delves into the legal and regulatory implicationsof Generative AI and LLMs in the European Union context, analyzing aspects ofliability, privacy, intellectual property, and cybersecurity. It criticallyexamines the adequacy of the existing and proposed EU legislation, includingthe Artificial Intelligence Act (AIA) draft, in addressing the uniquechallenges posed by Generative AI in general and LLMs in particular. The paperidentifies potential gaps and shortcomings in the legislative framework andproposes recommendations to ensure the safe and compliant deployment ofgenerative models, ensuring they align with the EU's evolving digital landscapeand legal standards.</description><author>Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, Luciano Floridi</author><pubDate>Fri, 15 Mar 2024 18:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07348v4</guid></item><item><title>MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation</title><link>http://arxiv.org/abs/2403.08019v2</link><description>We propose a single-shot approach to determining 6-DoF pose of an object withavailable 3D computer-aided design (CAD) model from a single RGB image. Ourmethod, dubbed MRC-Net, comprises two stages. The first performs poseclassification and renders the 3D object in the classified pose. The secondstage performs regression to predict fine-grained residual pose within class.Connecting the two stages is a novel multi-scale residual correlation (MRC)layer that captures high-and-low level correspondences between the input imageand rendering from first stage. MRC-Net employs a Siamese network with sharedweights between both stages to learn embeddings for input and rendered images.To mitigate ambiguity when predicting discrete pose class labels on symmetricobjects, we use soft probabilistic labels to define pose class in the firststage. We demonstrate state-of-the-art accuracy, outperforming all competingRGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O,YCB-V, and ITODD. Our method is non-iterative and requires no complexpost-processing.</description><author>Yuelong Li, Yafei Mao, Raja Bala, Sunil Hadap</author><pubDate>Fri, 15 Mar 2024 18:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08019v2</guid></item><item><title>Approximate Nullspace Augmented Finetuning for Robust Vision Transformers</title><link>http://arxiv.org/abs/2403.10476v1</link><description>Enhancing the robustness of deep learning models, particularly in the realmof vision transformers (ViTs), is crucial for their real-world deployment. Inthis work, we provide a finetuning approach to enhance the robustness of visiontransformers inspired by the concept of nullspace from linear algebra. Ourinvestigation centers on whether a vision transformer can exhibit resilience toinput variations akin to the nullspace property in linear mappings, implyingthat perturbations sampled from this nullspace do not influence the model'soutput when added to the input. Firstly, we show that for many pretrained ViTs,a non-trivial nullspace exists due to the presence of the patch embeddinglayer. Secondly, as nullspace is a concept associated with linear algebra, wedemonstrate that it is possible to synthesize approximate nullspace elementsfor the non-linear blocks of ViTs employing an optimisation strategy. Finally,we propose a fine-tuning strategy for ViTs wherein we augment the training datawith synthesized approximate nullspace noise. After finetuning, we find thatthe model demonstrates robustness to adversarial and natural image perbutationsalike.</description><author>Haoyang Liu, Aditya Singh, Yijiang Li, Haohan Wang</author><pubDate>Fri, 15 Mar 2024 18:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10476v1</guid></item><item><title>Guess &amp; Sketch: Language Model Guided Transpilation</title><link>http://arxiv.org/abs/2309.14396v2</link><description>Maintaining legacy software requires many software and systems engineeringhours. Assembly code programs, which demand low-level control over the computermachine state and have no variable names, are particularly difficult for humansto analyze. Existing conventional program translators guarantee correctness,but are hand-engineered for the source and target programming languages inquestion. Learned transpilation, i.e. automatic translation of code, offers analternative to manual re-writing and engineering efforts. Automated symbolicprogram translation approaches guarantee correctness but struggle to scale tolonger programs due to the exponentially large search space. Their rigidrule-based systems also limit their expressivity, so they can only reason abouta reduced space of programs. Probabilistic neural language models (LMs) produceplausible outputs for every input, but do so at the cost of guaranteedcorrectness. In this work, we leverage the strengths of LMs and symbolicsolvers in a neurosymbolic approach to learned transpilation for assembly code.Assembly code is an appropriate setting for a neurosymbolic approach, sinceassembly code can be divided into shorter non-branching basic blocks amenableto the use of symbolic methods. Guess &amp; Sketch extracts alignment andconfidence information from features of the LM then passes it to a symbolicsolver to resolve semantic equivalence of the transpilation input and output.We test Guess &amp; Sketch on three different test sets of assembly transpilationtasks, varying in difficulty, and show that it successfully transpiles 57.6%more examples than GPT-4 and 39.6% more examples than an engineered transpiler.We also share a training and evaluation dataset for this task.</description><author>Celine Lee, Abdulrahman Mahmoud, Michal Kurek, Simone Campanoni, David Brooks, Stephen Chong, Gu-Yeon Wei, Alexander M. Rush</author><pubDate>Fri, 15 Mar 2024 18:03:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14396v2</guid></item><item><title>DeepRepViz: Identifying Confounders in Deep Learning Model Predictions</title><link>http://arxiv.org/abs/2309.15551v2</link><description>Deep Learning (DL) models have gained popularity in neuroimaging studies forpredicting psychological behaviors, cognitive traits, and brain pathologies.However, these models can be biased by confounders such as age, sex, or imagingartifacts from the acquisition process. To address this, we introduce'DeepRepViz', a two-part framework designed to identify confounders in DL modelpredictions. The first component is a visualization tool that can be used toqualitatively examine the final latent representation of the DL model. Thesecond component is a metric called 'Con-score' that quantifies the confounderrisk associated with a variable, using the final latent representation of theDL model. We demonstrate the effectiveness of the Con-score using a simplesimulated setup by iteratively altering the strength of a simulated confounderand observing the corresponding change in the Con-score. Next, we validate theDeepRepViz framework on a large-scale neuroimaging dataset (n=12000) byperforming three MRI-phenotype prediction tasks that include (a) predictingchronic alcohol users, (b) classifying participant sex, and (c) predictingperformance speed on a cognitive task called 'trail making'. DeepRepVizidentifies sex as a significant confounder in the DL model predicting chronicalcohol users (Con-score=0.35) and age as a confounder in the model predictingcognitive task performance (Con-score=0.3). In conclusion, the DeepRepVizframework provides a systematic approach to test for potential confounders suchas age, sex, and imaging artifacts and improves the transparency of DL modelsfor neuroimaging studies.</description><author>Roshan Prakash Rane, JiHoon Kim, Arjun Umesha, Didem Stark, Marc-AndrÃ© Schulz, Kerstin Ritter</author><pubDate>Fri, 15 Mar 2024 18:01:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15551v2</guid></item><item><title>LILO: Learning Interpretable Libraries by Compressing and Documenting Code</title><link>http://arxiv.org/abs/2310.19791v4</link><description>While large language models (LLMs) now excel at code generation, a key aspectof software development is the art of refactoring: consolidating code intolibraries of reusable and readable programs. In this paper, we introduce LILO,a neurosymbolic framework that iteratively synthesizes, compresses, anddocuments code to build libraries tailored to particular problem domains. LILOcombines LLM-guided program synthesis with recent algorithmic advances inautomated refactoring from Stitch: a symbolic compression system thatefficiently identifies optimal lambda abstractions across large code corpora.To make these abstractions interpretable, we introduce an auto-documentation(AutoDoc) procedure that infers natural language names and docstrings based oncontextual examples of usage. In addition to improving human readability, wefind that AutoDoc boosts performance by helping LILO's synthesizer to interpretand deploy learned abstractions. We evaluate LILO on three inductive programsynthesis benchmarks for string editing, scene reasoning, and graphicscomposition. Compared to existing neural and symbolic methods - including thestate-of-the-art library learning algorithm DreamCoder - LILO solves morecomplex tasks and learns richer libraries that are grounded in linguisticknowledge.</description><author>Gabriel Grand, Lionel Wong, Maddy Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas</author><pubDate>Fri, 15 Mar 2024 17:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19791v4</guid></item><item><title>Safety Cases: Justifying the Safety of Advanced AI Systems</title><link>http://arxiv.org/abs/2403.10462v1</link><description>As AI systems become more advanced, companies and regulators will makedifficult decisions about whether it is safe to train and deploy them. Toprepare for these decisions, we investigate how developers could make a 'safetycase,' which is a structured rationale that AI systems are unlikely to cause acatastrophe. We propose a framework for organizing a safety case and discussfour categories of arguments to justify safety: total inability to cause acatastrophe, sufficiently strong control measures, trustworthiness despitecapability to cause harm, and deference to credible AI advisors. We evaluateconcrete examples of arguments in each category and outline how arguments couldbe combined to justify that AI systems are safe to deploy.</description><author>Joshua Clymer, Nick Gabrieli, David Krueger, Thomas Larsen</author><pubDate>Fri, 15 Mar 2024 17:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10462v1</guid></item><item><title>Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance ML Robustness</title><link>http://arxiv.org/abs/2403.10461v1</link><description>Machine Learning (ML) is susceptible to adversarial attacks that aim to trickML models, making them produce faulty predictions. Adversarial training wasfound to increase the robustness of ML models against these attacks. However,in network and cybersecurity, obtaining labeled training and adversarialtraining data is challenging and costly. Furthermore, concept drift deepens thechallenge, particularly in dynamic domains like network and cybersecurity, andrequires various models to conduct periodic retraining. This letter introducesAdaptive Continuous Adversarial Training (ACAT) to continuously integrateadversarial training samples into the model during ongoing learning sessions,using real-world detected adversarial data, to enhance model resilience againstevolving adversarial threats. ACAT is an adaptive defense mechanism thatutilizes periodic retraining to effectively counter adversarial attacks whilemitigating catastrophic forgetting. Our approach also reduces the total timerequired for adversarial sample detection, especially in environments such asnetwork security where the rate of attacks could be very high. Traditionaldetection processes that involve two stages may result in lengthy procedures.Experimental results using a SPAM detection dataset demonstrate that with ACAT,the accuracy of the SPAM filter increased from 69% to over 88% after just threeretraining sessions. Furthermore, ACAT outperforms conventional adversarialsample detectors, providing faster decision times, up to four times faster insome cases.</description><author>Mohamed elShehaby, Aditya Kotha, Ashraf Matrawy</author><pubDate>Fri, 15 Mar 2024 17:52:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10461v1</guid></item><item><title>Online Concurrent Multi-Robot Coverage Path Planning</title><link>http://arxiv.org/abs/2403.10460v1</link><description>Recently, centralized receding horizon online multi-robot coverage pathplanning algorithms have shown remarkable scalability in thoroughly exploringlarge, complex, unknown workspaces with many robots. In a horizon, the pathplanning and the path execution interleave, meaning when the path planningoccurs for robots with no paths, the robots with outstanding paths do notexecute, and subsequently, when the robots with new or outstanding pathsexecute to reach respective goals, path planning does not occur for thoserobots yet to get new paths, leading to wastage of both the robotic and thecomputation resources. As a remedy, we propose a centralized algorithm that isnot horizon-based. It plans paths at any time for a subset of robots with nopaths, i.e., who have reached their previously assigned goals, while the restexecute their outstanding paths, thereby enabling concurrent planning andexecution. We formally prove that the proposed algorithm ensures completecoverage of an unknown workspace and analyze its time complexity. Todemonstrate scalability, we evaluate our algorithm to cover eight large $2$Dgrid benchmark workspaces with up to 512 aerial and ground robots,respectively. A comparison with a state-of-the-art horizon-based algorithmshows its superiority in completing the coverage with up to 1.6x speedup. Forvalidation, we perform ROS + Gazebo simulations in six 2D grid benchmarkworkspaces with 10 quadcopters and TurtleBots, respectively. We alsosuccessfully conducted one outdoor experiment with three quadcopters and oneindoor with two TurtleBots.</description><author>Ratijit Mitra, Indranil Saha</author><pubDate>Fri, 15 Mar 2024 17:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10460v1</guid></item><item><title>Understanding the Double Descent Phenomenon in Deep Learning</title><link>http://arxiv.org/abs/2403.10459v1</link><description>Combining empirical risk minimization with capacity control is a classicalstrategy in machine learning when trying to control the generalization gap andavoid overfitting, as the model class capacity gets larger. Yet, in modern deeplearning practice, very large over-parameterized models (e.g. neural networks)are optimized to fit perfectly the training data and still obtain greatgeneralization performance. Past the interpolation point, increasing modelcomplexity seems to actually lower the test error. In this tutorial, we explain the concept of double descent and itsmechanisms. The first section sets the classical statistical learning frameworkand introduces the double descent phenomenon. By looking at a number ofexamples, section 2 introduces inductive biases that appear to have a key rolein double descent by selecting, among the multiple interpolating solutions, asmooth empirical risk minimizer. Finally, section 3 explores the double descentwith two linear models, and gives other points of view from recent relatedworks.</description><author>Marc Lafon, Alexandre Thomas</author><pubDate>Fri, 15 Mar 2024 17:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10459v1</guid></item><item><title>SimPLR: A Simple and Plain Transformer for Scaling-Efficient Object Detection and Segmentation</title><link>http://arxiv.org/abs/2310.05920v3</link><description>The ability to detect objects in images at varying scales has played apivotal role in the design of modern object detectors. Despite considerableprogress in removing hand-crafted components and simplifying the architecturewith transformers, multi-scale feature maps and/or pyramid design remain a keyfactor for their empirical success. In this paper, we show that this relianceon either feature pyramids or an hierarchical backbone is unnecessary and atransformer-based detector with scale-aware attention enables the plaindetector `SimPLR' whose backbone and detection head are both non-hierarchicaland operate on single-scale features. We find through our experiments thatSimPLR with scale-aware attention is plain and simple, yet competitive withmulti-scale vision transformer alternatives. Compared to the multi-scale andsingle-scale state-of-the-art, our model scales much better with biggercapacity (self-supervised) models and more pre-training data, allowing us toreport a consistently better accuracy and faster runtime for object detection,instance segmentation as well as panoptic segmentation. Code will be released.</description><author>Duy-Kien Nguyen, Martin R. Oswald, Cees G. M. Snoek</author><pubDate>Fri, 15 Mar 2024 17:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05920v3</guid></item><item><title>Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness</title><link>http://arxiv.org/abs/2403.10454v1</link><description>Integrated task and motion planning (TAMP) has proven to be a valuableapproach to generalizable long-horizon robotic manipulation and navigationproblems. However, the typical TAMP problem formulation assumes fullobservability and deterministic action effects. These assumptions limit theability of the planner to gather information and make decisions that arerisk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness(TAMPURA) that is capable of efficiently solving long-horizon planning problemswith initial-state and action outcome uncertainty, including problems thatrequire information gathering and avoiding undesirable and irreversibleoutcomes. Our planner reasons under uncertainty at both the abstract task leveland continuous controller level. Given a set of closed-loop goal-conditionedcontrollers operating in the primitive action space and a description of theirpreconditions and potential capabilities, we learn a high-level abstractionthat can be solved efficiently and then refined to continuous actions forexecution. We demonstrate our approach on several robotics problems whereuncertainty is a crucial factor and show that reasoning under uncertainty inthese problems outperforms previously proposed determinized planning, directsearch, and reinforcement learning strategies. Lastly, we demonstrate ourplanner on two real-world robotics problems using recent advancements inprobabilistic perception.</description><author>Aidan Curtis, George Matheos, Nishad Gothoskar, Vikash Mansinghka, Joshua Tenenbaum, TomÃ¡s Lozano-PÃ©rez, Leslie Pack Kaelbling</author><pubDate>Fri, 15 Mar 2024 17:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10454v1</guid></item><item><title>Debiasing Algorithm through Model Adaptation</title><link>http://arxiv.org/abs/2310.18913v3</link><description>Large language models are becoming the go-to solution for the ever-growingnumber of tasks. However, with growing capacity, models are prone to rely onspurious correlations stemming from biases and stereotypes present in thetraining data. This work proposes a novel method for detecting and mitigatinggender bias in language models. We perform causal analysis to identifyproblematic model components and discover that mid-upper feed-forward layersare most prone to convey bias. Based on the analysis results, we intervene inthe model by applying a linear projection to the weight matrices of theselayers. Our titular method, DAMA, significantly decreases bias as measured bydiverse metrics while maintaining the model's performance on downstream tasks.We release code for our method and models, which retrain LLaMA'sstate-of-the-art performance while being significantly less biased.</description><author>Tomasz Limisiewicz, David MareÄek, TomÃ¡Å¡ Musil</author><pubDate>Fri, 15 Mar 2024 17:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18913v3</guid></item><item><title>Robust Shape Fitting for 3D Scene Abstraction</title><link>http://arxiv.org/abs/2403.10452v1</link><description>Humans perceive and construct the world as an arrangement of simpleparametric models. In particular, we can often describe man-made environmentsusing volumetric primitives such as cuboids or cylinders. Inferring theseprimitives is important for attaining high-level, abstract scene descriptions.Previous approaches for primitive-based abstraction estimate shape parametersdirectly and are only able to reproduce simple objects. In contrast, we proposea robust estimator for primitive fitting, which meaningfully abstracts complexreal-world environments using cuboids. A RANSAC estimator guided by a neuralnetwork fits these primitives to a depth map. We condition the network onpreviously detected parts of the scene, parsing it one-by-one. To obtaincuboids from single RGB images, we additionally optimise a depth estimation CNNend-to-end. Naively minimising point-to-primitive distances leads to large orspurious cuboids occluding parts of the scene. We thus propose an improvedocclusion-aware distance metric correctly handling opaque scenes. Furthermore,we present a neural network based cuboid solver which provides moreparsimonious scene abstractions while also reducing inference time. Theproposed algorithm does not require labour-intensive labels, such as cuboidannotations, for training. Results on the NYU Depth v2 dataset demonstrate thatthe proposed algorithm successfully abstracts cluttered real-world 3D scenelayouts.</description><author>Florian Kluger, Eric Brachmann, Michael Ying Yang, Bodo Rosenhahn</author><pubDate>Fri, 15 Mar 2024 17:37:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10452v1</guid></item><item><title>Goodness of fit by Neyman-Pearson testing</title><link>http://arxiv.org/abs/2305.14137v2</link><description>The Neyman-Pearson strategy for hypothesis testing can be employed forgoodness of fit if the alternative hypothesis is selected from data byexploring a rich parametrised family of models, while controlling the impact ofstatistical fluctuations. The New Physics Learning Machine (NPLM) methodologyhas been developed as a concrete implementation of this idea, to target thedetection of new physical effects in the context of high energy physicscollider experiments. In this paper we conduct a comparison of this approach togoodness of fit with others, in particular with classifier-based strategiesthat share strong similarities with NPLM. From our comparison, NPLM emerges asthe more sensitive test to small departures of the data from the expecteddistribution and not biased towards detecting specific types of anomalies.These features make it suited for agnostic searches for new physics at colliderexperiments. Its deployment in other scientific and industrial scenarios shouldbe investigated.</description><author>Gaia Grosso, Marco Letizia, Maurizio Pierini, Andrea Wulzer</author><pubDate>Fri, 15 Mar 2024 17:30:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14137v2</guid></item><item><title>Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases</title><link>http://arxiv.org/abs/2403.10446v1</link><description>We proposed an end-to-end system design towards utilizing Retrieval AugmentedGeneration (RAG) to improve the factual accuracy of Large Language Models(LLMs) for domain-specific and time-sensitive queries related to privateknowledge-bases. Our system integrates RAG pipeline with upstream datasetsprocessing and downstream performance evaluation. Addressing the challenge ofLLM hallucinations, we finetune models with a curated dataset which originatesfrom CMU's extensive resources and annotated with the teacher model. Ourexperiments demonstrate the system's effectiveness in generating more accurateanswers to domain-specific and time-sensitive inquiries. The results alsorevealed the limitations of fine-tuning LLMs with small-scale and skeweddatasets. This research highlights the potential of RAG systems in augmentingLLMs with external datasets for improved performance in knowledge-intensivetasks. Our code and models are available on Github.</description><author>Jiarui Li, Ye Yuan, Zehua Zhang</author><pubDate>Fri, 15 Mar 2024 17:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10446v1</guid></item><item><title>Optimal Block-Level Draft Verification for Accelerating Speculative Decoding</title><link>http://arxiv.org/abs/2403.10444v1</link><description>Speculative decoding has shown to be an effective method for losslessacceleration of large language models (LLMs) during inference. In eachiteration, the algorithm first uses a smaller model to draft a block of tokens.The tokens are then verified by the large model in parallel and only a subsetof tokens will be kept to guarantee that the final output follows thedistribution of the large model. In all of the prior speculative decodingworks, the draft verification is performed token-by-token independently. Inthis work, we propose a better draft verification algorithm that providesadditional wall-clock speedup without incurring additional computation cost anddraft tokens. We first formulate the draft verification step as a block-leveloptimal transport problem. The block-level formulation allows us to consider awider range of draft verification algorithms and obtain a higher number ofaccepted tokens in expectation in one draft block. We propose a verificationalgorithm that achieves the optimal accepted length for the block-leveltransport problem. We empirically evaluate our proposed block-levelverification algorithm in a wide range of tasks and datasets, and observeconsistent improvements in wall-clock speedup when compared to token-levelverification algorithm. To the best of our knowledge, our work is the first toestablish improvement over speculative decoding through a better draftverification algorithm.</description><author>Ziteng Sun, Jae Hun Ro, Ahmad Beirami, Ananda Theertha Suresh</author><pubDate>Fri, 15 Mar 2024 17:28:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10444v1</guid></item><item><title>Data Ethics Emergency Drill: A Toolbox for Discussing Responsible AI for Industry Teams</title><link>http://arxiv.org/abs/2403.10438v1</link><description>Researchers urge technology practitioners such as data scientists to considerthe impacts and ethical implications of algorithmic decisions. However, unlikeprogramming, statistics, and data management, discussion of ethicalimplications is rarely included in standard data science training. To begin toaddress this gap, we designed and tested a toolbox called the data ethicsemergency drill (DEED) to help data science teams discuss and reflect on theethical implications of their work. The DEED is a roleplay of a fictionalethical emergency scenario that is contextually situated in the team's specificworkplace and applications. This paper outlines the DEED toolbox and describesthree studies carried out with two different data science teams thatiteratively shaped its design. Our findings show that practitioners can applylessons learnt from the roleplay to real-life situations, and how the DEEDopened up conversations around ethics and values.</description><author>Vanessa Aisyahsari Hanschke, Dylan Rees, Merve Alanyali, David Hopkinson, Paul Marshall</author><pubDate>Fri, 15 Mar 2024 17:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10438v1</guid></item><item><title>ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs</title><link>http://arxiv.org/abs/2306.03204v2</link><description>This paper explores the concept of leveraging generative AI as a mappingassistant for enhancing the efficiency of collaborative mapping. We presentresults of an experiment that combines multiple sources of volunteeredgeographic information (VGI) and large language models (LLMs). Three analystsdescribed the content of crowdsourced Mapillary street-level photographs takenalong roads in a small test area in Miami, Florida. GPT-3.5-turbo wasinstructed to suggest the most appropriate tagging for each road inOpenStreetMap (OSM). The study also explores the utilization of BLIP-2, astate-of-the-art multimodal pre-training method as an artificial analyst ofstreet-level photographs in addition to human analysts. Results demonstrate twoways to effectively increase the accuracy of mapping suggestions withoutmodifying the underlying AI models: by (1) providing a more detaileddescription of source photographs, and (2) combining prompt engineering withadditional context (e.g. location and objects detected along a road). The firstapproach increases the suggestion accuracy by up to 29%, and the second one byup to 20%.</description><author>Levente JuhÃ¡sz, Peter Mooney, Hartwig H. Hochmair, Boyuan Guan</author><pubDate>Fri, 15 Mar 2024 17:15:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03204v2</guid></item><item><title>Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for Visual Insect Understanding</title><link>http://arxiv.org/abs/2311.15206v2</link><description>In precision agriculture, the detection and recognition of insects play anessential role in the ability of crops to grow healthy and produce ahigh-quality yield. The current machine vision model requires a large volume ofdata to achieve high performance. However, there are approximately 5.5 milliondifferent insect species in the world. None of the existing insect datasets cancover even a fraction of them due to varying geographic locations andacquisition costs. In this paper, we introduce a novel "Insect-1M" dataset, agame-changing resource poised to revolutionize insect-related foundation modeltraining. Covering a vast spectrum of insect species, our dataset, including 1million images with dense identification labels of taxonomy hierarchy andinsect descriptions, offers a panoramic view of entomology, enabling foundationmodels to comprehend visual and semantic information about insects like neverbefore. Then, to efficiently establish an Insect Foundation Model, we develop amicro-feature self-supervised learning method with a Patch-wise RelevantAttention mechanism capable of discerning the subtle differences among insectimages. In addition, we introduce Description Consistency loss to improvemicro-feature modeling via insect descriptions. Through our experiments, weillustrate the effectiveness of our proposed approach in insect modeling andachieve State-of-the-Art performance on standard benchmarks of insect-relatedtasks. Our Insect Foundation Model and Dataset promise to empower the nextgeneration of insect-related vision models, bringing them closer to theultimate goal of precision agriculture.</description><author>Hoang-Quan Nguyen, Thanh-Dat Truong, Xuan Bac Nguyen, Ashley Dowling, Xin Li, Khoa Luu</author><pubDate>Fri, 15 Mar 2024 17:15:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15206v2</guid></item><item><title>Using an LLM to Turn Sign Spottings into Spoken Language Sentences</title><link>http://arxiv.org/abs/2403.10434v1</link><description>Sign Language Translation (SLT) is a challenging task that aims to generatespoken language sentences from sign language videos. In this paper, weintroduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter anda pretrained large language model to improve SLT performance. Our method buildsupon the strengths of both components. The videos are first processed by thespotter, which is trained on a linguistic sign language dataset, to identifyindividual signs. These spotted signs are then passed to the powerful languagemodel, which transforms them into coherent and contextually appropriate spokenlanguage sentences.</description><author>Ozge Mercanoglu Sincan, Necati Cihan Camgoz, Richard Bowden</author><pubDate>Fri, 15 Mar 2024 17:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10434v1</guid></item><item><title>Weight fluctuations in (deep) linear neural networks and a derivation of the inverse-variance flatness relation</title><link>http://arxiv.org/abs/2311.14120v2</link><description>We investigate the stationary (late-time) training regime of single- andtwo-layer linear underparameterized neural networks within the continuum limitof stochastic gradient descent (SGD) for synthetic Gaussian data. In the caseof a single-layer network in the weakly underparameterized regime, the spectrumof the noise covariance matrix deviates notably from the Hessian, which can beattributed to the broken detailed balance of SGD dynamics. The weightfluctuations are in this case generally anisotropic, but are subject to anisotropic loss. For a two-layer network, we obtain the stochastic dynamics ofthe weights in each layer and analyze the associated stationary covariances. Weidentify the inter-layer coupling as a new source of anisotropy for the weightfluctuations. In contrast to the single-layer case, the weight fluctuationsexperience an anisotropic loss, the flatness of which is inversely related tothe fluctuation variance. We thereby provide an analytical derivation of therecently observed inverse variance-flatness relation in a model of a deeplinear neural network.</description><author>Markus Gross, Arne P. Raulf, Christoph RÃ¤th</author><pubDate>Fri, 15 Mar 2024 17:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14120v2</guid></item><item><title>AI-enhanced Collective Intelligence: The State of the Art and Prospects</title><link>http://arxiv.org/abs/2403.10433v1</link><description>The current societal challenges exceed the capacity of human individual orcollective effort alone. As AI evolves, its role within human collectives ispoised to vary from an assistive tool to a participatory member. Humans and AIpossess complementary capabilities that, when synergized, can achieve a levelof collective intelligence that surpasses the collective capabilities of eitherhumans or AI in isolation. However, the interactions in human-AI systems areinherently complex, involving intricate processes and interdependencies. Thisreview incorporates perspectives from network science to conceptualize amultilayer representation of human-AI collective intelligence, comprising acognition layer, a physical layer, and an information layer. Within thismultilayer network, humans and AI agents exhibit varying characteristics;humans differ in diversity from surface-level to deep-level attributes, whileAI agents range in degrees of functionality and anthropomorphism. The interplayamong these agents shapes the overall structure and dynamics of the system. Weexplore how agents' diversity and interactions influence the system'scollective intelligence. Furthermore, we present an analysis of real-worldinstances of AI-enhanced collective intelligence. We conclude by addressing thepotential challenges in AI-enhanced collective intelligence and offerperspectives on future developments in this field.</description><author>Hao Cui, Taha Yasseri</author><pubDate>Fri, 15 Mar 2024 17:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10433v1</guid></item><item><title>Transferring climate change knowledge</title><link>http://arxiv.org/abs/2309.14780v3</link><description>Accurate and precise climate projections are required for climate adaptationand mitigation, but Earth system models still exhibit great uncertainties.Several approaches have been developed to reduce the spread of climateprojections and feedbacks, yet those methods cannot capture the non-linearcomplexity inherent in the climate system. Using a Transfer Learning approach,we show that Machine Learning can be used to optimally leverage and merge theknowledge gained from Earth system models simulations and historicalobservations to more accurately project global surface air temperature fieldsin the 21st century. We reach an uncertainty reduction of more than 50% withrespect to state-of-the-art approaches. We give evidence that our novel methodprovides narrower projection uncertainty together with more accurate meanclimate projections, urgently required for climate adaptation.</description><author>Francesco Immorlano, Veronika Eyring, Thomas le Monnier de Gouville, Gabriele Accarino, Donatello Elia, Giovanni Aloisio, Pierre Gentine</author><pubDate>Fri, 15 Mar 2024 17:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14780v3</guid></item><item><title>CinPatent: Datasets for Patent Classification</title><link>http://arxiv.org/abs/2212.12192v3</link><description>Patent classification is the task that assigns each input patent into severalcodes (classes). Due to its high demand, several datasets and methods have beenintroduced. However, the lack of both systematic performance comparison ofbaselines and access to some datasets creates a gap for the task. To fill thegap, we introduce two new datasets in English and Japanese collected by usingCPC codes. The English dataset includes 45,131 patent documents with 425 labelsand the Japanese dataset contains 54,657 documents with 523 labels. Tofacilitate the next studies, we compare the performance of strong multi-labeltext classification methods on the two datasets. Experimental results show thatAttentionXML is consistently better than other strong baselines. The ablationstudy is also conducted in two aspects: the contribution of different parts(title, abstract, description, and claims) of a patent and the behavior ofbaselines in terms of performance with different training data segmentation. Werelease the two new datasets with the code of the baselines.</description><author>Minh-Tien Nguyen, Nhung Bui, Manh Tran-Tien, Linh Le, Huy-The Vu</author><pubDate>Fri, 15 Mar 2024 17:03:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12192v3</guid></item><item><title>Unprocessing Seven Years of Algorithmic Fairness</title><link>http://arxiv.org/abs/2306.07261v5</link><description>Seven years ago, researchers proposed a postprocessing method to equalize theerror rates of a model across different demographic groups. The work launchedhundreds of papers purporting to improve over the postprocessing baseline. Weempirically evaluate these claims through thousands of model evaluations onseveral tabular datasets. We find that the fairness-accuracy Pareto frontierachieved by postprocessing contains all other methods we were feasibly able toevaluate. In doing so, we address two common methodological errors that haveconfounded previous observations. One relates to the comparison of methods withdifferent unconstrained base models. The other concerns methods achievingdifferent levels of constraint relaxation. At the heart of our study is asimple idea we call unprocessing that roughly corresponds to the inverse ofpostprocessing. Unprocessing allows for a direct comparison of methods usingdifferent underlying models and levels of relaxation.</description><author>AndrÃ© F. Cruz, Moritz Hardt</author><pubDate>Fri, 15 Mar 2024 17:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07261v5</guid></item><item><title>SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians</title><link>http://arxiv.org/abs/2403.10427v1</link><description>Implicit neural representation methods have shown impressive advancements inlearning 3D scenes from unstructured in-the-wild photo collections but arestill limited by the large computational cost of volumetric rendering. Morerecently, 3D Gaussian Splatting emerged as a much faster alternative withsuperior rendering quality and training efficiency, especially for small-scaleand object-centric scenarios. Nevertheless, this technique suffers from poorperformance on unstructured in-the-wild data. To tackle this, we extend over 3DGaussian Splatting to handle unstructured image collections. We achieve this bymodeling appearance to seize photometric variations in the rendered images.Additionally, we introduce a new mechanism to train transient Gaussians tohandle the presence of scene occluders in an unsupervised manner. Experimentson diverse photo collection scenes and multi-pass acquisition of outdoorlandmarks show the effectiveness of our method over prior works achievingstate-of-the-art results with improved efficiency.</description><author>Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou</author><pubDate>Fri, 15 Mar 2024 17:00:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10427v1</guid></item><item><title>NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices</title><link>http://arxiv.org/abs/2403.10425v1</link><description>Real-time high-accuracy optical flow estimation is a crucial component invarious applications, including localization and mapping in robotics, objecttracking, and activity recognition in computer vision. While recentlearning-based optical flow methods have achieved high accuracy, they oftencome with heavy computation costs. In this paper, we propose a highly efficientoptical flow architecture, called NeuFlow, that addresses both high accuracyand computational cost concerns. The architecture follows a global-to-localscheme. Given the features of the input images extracted at different spatialresolutions, global matching is employed to estimate an initial optical flow onthe 1/16 resolution, capturing large displacement, which is then refined on the1/8 resolution with lightweight CNN layers for better accuracy. We evaluate ourapproach on Jetson Orin Nano and RTX 2080 to demonstrate efficiencyimprovements across different computing platforms. We achieve a notable 10x-80xspeedup compared to several state-of-the-art methods, while maintainingcomparable accuracy. Our approach achieves around 30 FPS on edge computingplatforms, which represents a significant breakthrough in deploying complexcomputer vision tasks such as SLAM on small robots like drones. The fulltraining and evaluation code is available athttps://github.com/neufieldrobotics/NeuFlow.</description><author>Zhiyong Zhang, Huaizu Jiang, Hanumant Singh</author><pubDate>Fri, 15 Mar 2024 16:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10425v1</guid></item><item><title>Structured Evaluation of Synthetic Tabular Data</title><link>http://arxiv.org/abs/2403.10424v1</link><description>Tabular data is common yet typically incomplete, small in volume, andaccess-restricted due to privacy concerns. Synthetic data generation offerspotential solutions. Many metrics exist for evaluating the quality of synthetictabular data; however, we lack an objective, coherent interpretation of themany metrics. To address this issue, we propose an evaluation framework with asingle, mathematical objective that posits that the synthetic data should bedrawn from the same distribution as the observed data. Through variousstructural decomposition of the objective, this framework allows us to reasonfor the first time the completeness of any set of metrics, as well as unifiesexisting metrics, including those that stem from fidelity considerations,downstream application, and model-based approaches. Moreover, the frameworkmotivates model-free baselines and a new spectrum of metrics. We evaluatestructurally informed synthesizers and synthesizers powered by deep learning.Using our structured framework, we show that synthetic data generators thatexplicitly represent tabular structure outperform other methods, especially onsmaller datasets.</description><author>Scott Cheng-Hsin Yang, Baxter Eaves, Michael Schmidt, Ken Swanson, Patrick Shafto</author><pubDate>Fri, 15 Mar 2024 16:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10424v1</guid></item><item><title>Quantization Avoids Saddle Points in Distributed Optimization</title><link>http://arxiv.org/abs/2403.10423v1</link><description>Distributed nonconvex optimization underpins key functionalities of numerousdistributed systems, ranging from power systems, smart buildings, cooperativerobots, vehicle networks to sensor networks. Recently, it has also merged as apromising solution to handle the enormous growth in data and model sizes indeep learning. A fundamental problem in distributed nonconvex optimization isavoiding convergence to saddle points, which significantly degrade optimizationaccuracy. We discover that the process of quantization, which is necessary forall digital communications, can be exploited to enable saddle-point avoidance.More specifically, we propose a stochastic quantization scheme and prove thatit can effectively escape saddle points and ensure convergence to asecond-order stationary point in distributed nonconvex optimization. With aneasily adjustable quantization granularity, the approach allows a user tocontrol the number of bits sent per iteration and, hence, to aggressivelyreduce the communication overhead. Numerical experimental results usingdistributed optimization and learning problems on benchmark datasets confirmthe effectiveness of the approach.</description><author>Yanan Bo, Yongqiang Wang</author><pubDate>Fri, 15 Mar 2024 16:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10423v1</guid></item><item><title>Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination</title><link>http://arxiv.org/abs/2403.10416v1</link><description>We study Gaussian sparse estimation tasks in Huber's contamination model witha focus on mean estimation, PCA, and linear regression. For each of thesetasks, we give the first sample and computationally efficient robust estimatorswith optimal error guarantees, within constant factors. All prior efficientalgorithms for these tasks incur quantitatively suboptimal error. Concretely,for Gaussian robust $k$-sparse mean estimation on $\mathbb{R}^d$ withcorruption rate $\epsilon&gt;0$, our algorithm has sample complexity$(k^2/\epsilon^2)\mathrm{polylog}(d/\epsilon)$, runs in sample polynomial time,and approximates the target mean within $\ell_2$-error $O(\epsilon)$. Previousefficient algorithms inherently incur error $\Omega(\epsilon\sqrt{\log(1/\epsilon)})$. At the technical level, we develop a novelmultidimensional filtering method in the sparse regime that may find otherapplications.</description><author>Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia, Thanasis Pittas</author><pubDate>Fri, 15 Mar 2024 16:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10416v1</guid></item><item><title>Gradient based Feature Attribution in Explainable AI: A Technical Review</title><link>http://arxiv.org/abs/2403.10415v1</link><description>The surge in black-box AI models has prompted the need to explain theinternal mechanism and justify their reliability, especially in high-stakesapplications, such as healthcare and autonomous driving. Due to the lack of arigorous definition of explainable AI (XAI), a plethora of research related toexplainability, interpretability, and transparency has been developed toexplain and analyze the model from various perspectives. Consequently, with anexhaustive list of papers, it becomes challenging to have a comprehensiveoverview of XAI research from all aspects. Considering the popularity of neuralnetworks in AI research, we narrow our focus to a specific area of XAIresearch: gradient based explanations, which can be directly adopted for neuralnetwork models. In this review, we systematically explore gradient basedexplanation methods to date and introduce a novel taxonomy to categorize theminto four distinct classes. Then, we present the essence of technique detailsin chronological order and underscore the evolution of algorithms. Next, weintroduce both human and quantitative evaluations to measure algorithmperformance. More importantly, we demonstrate the general challenges in XAI andspecific challenges in gradient based explanations. We hope that this surveycan help researchers understand state-of-the-art progress and theircorresponding disadvantages, which could spark their interest in addressingthese issues in future work.</description><author>Yongjie Wang, Tong Zhang, Xu Guo, Zhiqi Shen</author><pubDate>Fri, 15 Mar 2024 16:49:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10415v1</guid></item><item><title>Real-Time Image Segmentation via Hybrid Convolutional-Transformer Architecture Search</title><link>http://arxiv.org/abs/2403.10413v1</link><description>Image segmentation is one of the most fundamental problems in computer visionand has drawn a lot of attentions due to its vast applications in imageunderstanding and autonomous driving. However, designing effective andefficient segmentation neural architectures is a labor-intensive process thatmay require lots of trials by human experts. In this paper, we address thechallenge of integrating multi-head self-attention into high resolutionrepresentation CNNs efficiently, by leveraging architecture search. Manuallyreplacing convolution layers with multi-head self-attention is non-trivial dueto the costly overhead in memory to maintain high resolution. By contrast, wedevelop a multi-target multi-branch supernet method, which not only fullyutilizes the advantages of high-resolution features, but also finds the properlocation for placing multi-head self-attention module. Our search algorithm isoptimized towards multiple objective s (e.g., latency and mIoU) and capable offinding architectures on Pareto frontier with arbitrary number of branches in asingle search. We further present a series of model via HybridConvolutional-Transformer Architecture Search (HyCTAS) method that searched forthe best hybrid combination of light-weight convolution layers andmemory-efficient self-attention layers between branches from differentresolutions and fuse to high resolution for both efficiency and effectiveness.Extensive experiments demonstrate that HyCTAS outperforms previous methods onsemantic segmentation task. Code and models are available at\url{https://github.com/MarvinYu1995/HyCTAS}.</description><author>Hongyuan Yu, Cheng Wan, Mengchen Liu, Dongdong Chen, Bin Xiao, Xiyang Dai</author><pubDate>Fri, 15 Mar 2024 16:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10413v1</guid></item><item><title>HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes</title><link>http://arxiv.org/abs/2403.02769v2</link><description>Human-centric 3D scene understanding has recently drawn increasing attention,driven by its critical impact on robotics. However, human-centric real-lifescenarios are extremely diverse and complicated, and humans have intricatemotions and interactions. With limited labeled data, supervised methods aredifficult to generalize to general scenarios, hindering real-life applications.Mimicking human intelligence, we propose an unsupervised 3D detection methodfor human-centric scenarios by transferring the knowledge from synthetic humaninstances to real scenes. To bridge the gap between the distinct datarepresentations and feature distributions of synthetic models and real pointclouds, we introduce novel modules for effective instance-to-scenerepresentation transfer and synthetic-to-real feature alignment. Remarkably,our method exhibits superior performance compared to current state-of-the-arttechniques, achieving 87.8% improvement in mAP and closely approaching theperformance of fully supervised methods (62.15 mAP vs. 69.02 mAP) on HuCenLifeDataset.</description><author>Yichen Yao, Zimo Jiang, Yujing Sun, Zhencai Zhu, Xinge Zhu, Runnan Chen, Yuexin Ma</author><pubDate>Fri, 15 Mar 2024 16:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02769v2</guid></item><item><title>Fast and Simple Explainability for Point Cloud Networks</title><link>http://arxiv.org/abs/2403.07706v2</link><description>We propose a fast and simple explainable AI (XAI) method for point clouddata. It computes pointwise importance with respect to a trained networkdownstream task. This allows better understanding of the network properties,which is imperative for safety-critical applications. In addition to debuggingand visualization, our low computational complexity facilitates online feedbackto the network at inference. This can be used to reduce uncertainty and toincrease robustness. In this work, we introduce \emph{Feature BasedInterpretability} (FBI), where we compute the features' norm, per point, beforethe bottleneck. We analyze the use of gradients and post- and pre-bottleneckstrategies, showing pre-bottleneck is preferred, in terms of smoothness andranking. We obtain at least three orders of magnitude speedup, compared tocurrent XAI methods, thus, scalable for big point clouds or large-scalearchitectures. Our approach achieves SOTA results, in terms of classificationexplainability. We demonstrate how the proposed measure is helpful in analyzingand characterizing various aspects of 3D learning, such as rotation invariance,robustness to out-of-distribution (OOD) outliers or domain shift and datasetbias.</description><author>Meir Yossef Levi, Guy Gilboa</author><pubDate>Fri, 15 Mar 2024 16:46:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07706v2</guid></item><item><title>Cognitive Architectures for Language Agents</title><link>http://arxiv.org/abs/2309.02427v3</link><description>Recent efforts have augmented large language models (LLMs) with externalresources (e.g., the Internet) or internal control flows (e.g., promptchaining) for tasks requiring grounding or reasoning, leading to a new class oflanguage agents. While these agents have achieved substantial empiricalsuccess, we lack a systematic framework to organize existing agents and planfuture developments. In this paper, we draw on the rich history of cognitivescience and symbolic artificial intelligence to propose Cognitive Architecturesfor Language Agents (CoALA). CoALA describes a language agent with modularmemory components, a structured action space to interact with internal memoryand external environments, and a generalized decision-making process to chooseactions. We use CoALA to retrospectively survey and organize a large body ofrecent work, and prospectively identify actionable directions towards morecapable agents. Taken together, CoALA contextualizes today's language agentswithin the broader history of AI and outlines a path towards language-basedgeneral intelligence.</description><author>Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths</author><pubDate>Fri, 15 Mar 2024 16:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02427v3</guid></item><item><title>SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores</title><link>http://arxiv.org/abs/2403.10408v1</link><description>We present SocialGenPod, a decentralised and privacy-friendly way ofdeploying generative AI Web applications. Unlike centralised Web and dataarchitectures that keep user data tied to application and service providers, weshow how one can use Solid -- a decentralised Web specification -- to decoupleuser data from generative AI applications. We demonstrate SocialGenPod using aprototype that allows users to converse with different Large Language Models,optionally leveraging Retrieval Augmented Generation to generate answersgrounded in private documents stored in any Solid Pod that the user is allowedto access, directly or indirectly. SocialGenPod makes use of Solid accesscontrol mechanisms to give users full control of determining who has access todata stored in their Pods. SocialGenPod keeps all user data (chat history, appconfiguration, personal documents, etc) securely in the user's personal Pod;separate from specific model or application providers. Besides better privacycontrols, this approach also enables portability across different services andapplications. Finally, we discuss challenges, posed by the large computerequirements of state-of-the-art models, that future research in this areashould address. Our prototype is open-source and available at:https://github.com/Vidminas/socialgenpod/.</description><author>Vidminas Vizgirda, Rui Zhao, Naman Goel</author><pubDate>Fri, 15 Mar 2024 16:43:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10408v1</guid></item><item><title>zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models</title><link>http://arxiv.org/abs/2311.10112v2</link><description>Modeling evolving knowledge over temporal knowledge graphs (TKGs) has becomea heated topic. Various methods have been proposed to forecast links on TKGs.Most of them are embedding-based, where hidden representations are learned torepresent knowledge graph (KG) entities and relations based on the observedgraph contexts. Although these methods show strong performance on traditionalTKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling theunseen zero-shot relations that have no prior graph context. In this paper, wetry to mitigate this problem as follows. We first input the text descriptionsof KG relations into large language models (LLMs) for generating relationrepresentations, and then introduce them into embedding-based TKGF methods.LLM-empowered representations can capture the semantic information in therelation descriptions. This makes the relations, whether seen or unseen, withsimilar semantic meanings stay close in the embedding space, enabling TKGFmodels to recognize zero-shot relations even without any observed graphcontext. Experimental results show that our approach helps TKGF models toachieve much better performance in forecasting the facts with previously unseenrelations, while still maintaining their ability in link forecasting regardingseen relations.</description><author>Zifeng Ding, Heling Cai, Jingpei Wu, Yunpu Ma, Ruotong Liao, Bo Xiong, Volker Tresp</author><pubDate>Fri, 15 Mar 2024 16:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10112v2</guid></item><item><title>A comparative study on machine learning approaches for rock mass classification using drilling data</title><link>http://arxiv.org/abs/2403.10404v1</link><description>Current rock engineering design in drill and blast tunnelling primarilyrelies on engineers' observational assessments. Measure While Drilling (MWD)data, a high-resolution sensor dataset collected during tunnel excavation, isunderutilised, mainly serving for geological visualisation. This study aims toautomate the translation of MWD data into actionable metrics for rockengineering. It seeks to link data to specific engineering actions, thusproviding critical decision support for geological challenges ahead of thetunnel face. Leveraging a large and geologically diverse dataset of 500,000drillholes from 15 tunnels, the research introduces models for accurate rockmass quality classification in a real-world tunnelling context. Bothconventional machine learning and image-based deep learning are explored toclassify MWD data into Q-classes and Q-values, examples of metrics describingthe stability of the rock mass, using both tabular and image data. The resultsindicate that the K-nearest neighbours algorithm in an ensemble with tree-basedmodels using tabular data, effectively classifies rock mass quality. Itachieves a cross-validated balanced accuracy of 0.86 in classifying rock massinto the Q-classes A, B, C, D, E1, E2, and 0.95 for a binary classificationwith E versus the rest. Classification using a CNN with MWD-images for eachblasting round resulted in a balanced accuracy of 0.82 for binaryclassification. Regressing the Q-value from tabular MWD-data achievedcross-validated R2 and MSE scores of 0.80 and 0.18 for a similar ensemble modelas in classification. High performance in regression and classification boostsconfidence in automated rock mass assessment. Applying advanced modelling on aunique dataset demonstrates MWD data's value in improving rock massclassification accuracy and advancing data-driven rock engineering design,reducing manual intervention.</description><author>Tom F. Hansen, Georg H. Erharter, Zhongqiang Liu, Jim Torresen</author><pubDate>Fri, 15 Mar 2024 16:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10404v1</guid></item><item><title>Energy Correction Model in the Feature Space for Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2403.10403v1</link><description>In this work, we study the out-of-distribution (OOD) detection problemthrough the use of the feature space of a pre-trained deep classifier. We showthat learning the density of in-distribution (ID) features with an energy-basedmodels (EBM) leads to competitive detection results. However, we found that thenon-mixing of MCMC sampling during the EBM's training undermines its detectionperformance. To overcome this an energy-based correction of a mixture ofclass-conditional Gaussian distributions. We obtains favorable results whencompared to a strong baseline like the KNN detector on the CIFAR-10/CIFAR-100OOD detection benchmarks.</description><author>Marc Lafon, ClÃ©ment Rambour, Nicolas Thome</author><pubDate>Fri, 15 Mar 2024 16:37:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10403v1</guid></item><item><title>SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal Conditioned Diffusion Policy</title><link>http://arxiv.org/abs/2403.10401v1</link><description>Manipulating deformable objects remains a challenge within robotics due tothe difficulties of state estimation, long-horizon planning, and predicting howthe object will deform given an interaction. These challenges are the mostpronounced with 3D deformable objects. We propose SculptDiff, agoal-conditioned diffusion-based imitation learning framework that works withpoint cloud state observations to directly learn clay sculpting policies for avariety of target shapes. To the best of our knowledge this is the firstreal-world method that successfully learns manipulation policies for 3Ddeformable objects. For sculpting videos and access to our dataset and hardwareCAD models, see the project website:https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home</description><author>Alison Bartsch, Arvind Car, Charlotte Avra, Amir Barati Farimani</author><pubDate>Fri, 15 Mar 2024 16:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10401v1</guid></item><item><title>DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training</title><link>http://arxiv.org/abs/2310.02025v4</link><description>Zeroth-order (ZO) optimization has become a popular technique for solvingmachine learning (ML) problems when first-order (FO) information is difficultor impossible to obtain. However, the scalability of ZO optimization remains anopen problem: Its use has primarily been limited to relatively small-scale MLproblems, such as sample-wise adversarial attack generation. To our bestknowledge, no prior work has demonstrated the effectiveness of ZO optimizationin training deep neural networks (DNNs) without a significant decrease inperformance. To overcome this roadblock, we develop DeepZero, a principled ZOdeep learning (DL) framework that can scale ZO optimization to DNN trainingfrom scratch through three primary innovations. First, we demonstrate theadvantages of coordinatewise gradient estimation (CGE) over randomizedvector-wise gradient estimation in training accuracy and computationalefficiency. Second, we propose a sparsityinduced ZO training protocol thatextends the model pruning methodology using only finite differences to exploreand exploit the sparse DL prior in CGE. Third, we develop the methods offeature reuse and forward parallelization to advance the practicalimplementations of ZO training. Our extensive experiments show that DeepZeroachieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10,approaching FO training performance for the first time. Furthermore, we showthe practical utility of DeepZero in applications of certified adversarialdefense and DL-based partial differential equation error correction, achieving10-20% improvement over SOTA. We believe our results will inspire futureresearch on scalable ZO optimization and contribute to advancing DL with blackbox. Codes are available at https://github.com/OPTML-Group/DeepZero.</description><author>Aochuan Chen, Yimeng Zhang, Jinghan Jia, James Diffenderfer, Jiancheng Liu, Konstantinos Parasyris, Yihua Zhang, Zheng Zhang, Bhavya Kailkhura, Sijia Liu</author><pubDate>Fri, 15 Mar 2024 16:28:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02025v4</guid></item><item><title>Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding</title><link>http://arxiv.org/abs/2403.10395v1</link><description>Encouraged by the growing availability of pre-trained 2D diffusion models,image-to-3D generation by leveraging Score Distillation Sampling (SDS) ismaking remarkable progress. Most existing methods combine novel-view liftingfrom 2D diffusion models which usually take the reference image as a conditionwhile applying hard L2 image supervision at the reference view. Yet heavilyadhering to the image is prone to corrupting the inductive knowledge of the 2Ddiffusion model leading to flat or distorted 3D generation frequently. In thiswork, we reexamine image-to-3D in a novel perspective and present Isotropic3D,an image-to-3D generation pipeline that takes only an image CLIP embedding asinput. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuthangle by solely resting on the SDS loss. The core of our framework lies in atwo-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3Ddiffusion model by substituting its text encoder with an image encoder, bywhich the model preliminarily acquires image-to-image capabilities. Secondly,we perform fine-tuning using our Explicit Multi-view Attention (EMA) whichcombines noisy multi-view images with the noise-free reference image as anexplicit condition. CLIP embedding is sent to the diffusion model throughoutthe whole process while reference images are discarded once after fine-tuning.As a result, with a single image CLIP embedding, Isotropic3D is capable ofgenerating multi-view mutually consistent images and also a 3D model with moresymmetrical and neat content, well-proportioned geometry, rich colored texture,and less distortion compared with existing image-to-3D methods while stillpreserving the similarity to the reference image to a large extent. The projectpage is available at https://isotropic3d.github.io/. The code and models areavailable at https://github.com/pkunliu/Isotropic3D.</description><author>Pengkun Liu, Yikai Wang, Fuchun Sun, Jiafang Li, Hang Xiao, Hongxiang Xue, Xinzhou Wang</author><pubDate>Fri, 15 Mar 2024 16:27:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10395v1</guid></item><item><title>Signed Diverse Multiplex Networks: Clustering and Inference</title><link>http://arxiv.org/abs/2402.10242v2</link><description>The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG)model, which is a variant of the Generalized Random Dot Product Graph (GRDPG),where, in addition, edges can be positive or negative. The setting is extendedto a multiplex version, where all layers have the same collection of nodes andfollow the SGRDPG. The only common feature of the layers of the network is thatthey can be partitioned into groups with common subspace structures, whileotherwise matrices of connection probabilities can be all different. Thesetting above is extremely flexible and includes a variety of existingmultiplex network models as its particular cases. The paper fulfills twoobjectives. First, it shows that keeping signs of the edges in the process ofnetwork construction leads to a better precision of estimation and clusteringand, hence, is beneficial for tackling real world problems such as, forexample, analysis of brain networks. Second, by employing novel algorithms, ourpaper ensures strongly consistent clustering of layers and high accuracy ofsubspace estimation. In addition to theoretical guarantees, both of thosefeatures are demonstrated using numerical simulations and a real data example.</description><author>Marianna Pensky</author><pubDate>Fri, 15 Mar 2024 16:22:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10242v2</guid></item><item><title>CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning</title><link>http://arxiv.org/abs/2403.10391v1</link><description>Pseudo-label-based semi-supervised learning (SSL) algorithms trained on aclass-imbalanced set face two cascading challenges: 1) Classifiers tend to bebiased towards majority classes, and 2) Biased pseudo-labels are used fortraining. It is difficult to appropriately re-balance the classifiers in SSLbecause the class distribution of an unlabeled set is often unknown and couldbe mismatched with that of a labeled set. We propose a novel class-imbalancedSSL algorithm called class-distribution-mismatch-aware debiasing (CDMAD). Foreach iteration of training, CDMAD first assesses the classifier's biased degreetowards each class by calculating the logits on an image without any patterns(e.g., solid color image), which can be considered irrelevant to the trainingset. CDMAD then refines biased pseudo-labels of the base SSL algorithm byensuring the classifier's neutrality. CDMAD uses these refined pseudo-labelsduring the training of the base SSL algorithm to improve the quality of therepresentations. In the test phase, CDMAD similarly refines biased classpredictions on test samples. CDMAD can be seen as an extension of post-hoclogit adjustment to address a challenge of incorporating the unknown classdistribution of the unlabeled set for re-balancing the biased classifier underclass distribution mismatch. CDMAD ensures Fisher consistency for the balancederror. Extensive experiments verify the effectiveness of CDMAD.</description><author>Hyuck Lee, Heeyoung Kim</author><pubDate>Fri, 15 Mar 2024 16:22:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10391v1</guid></item><item><title>Evaluating Perceptual Distances by Fitting Binomial Distributions to Two-Alternative Forced Choice Data</title><link>http://arxiv.org/abs/2403.10390v1</link><description>The two-alternative forced choice (2AFC) experimental setup is popular in thevisual perception literature, where practitioners aim to understand how humanobservers perceive distances within triplets that consist of a reference imageand two distorted versions of that image. In the past, this had been conductedin controlled environments, with a tournament-style algorithm dictating whichimages are shown to each participant to rank the distorted images. Recently,crowd-sourced perceptual datasets have emerged, with no images shared betweentriplets, making ranking impossible. Evaluating perceptual distances using thisdata is non-trivial, relying on reducing the collection of judgements on atriplet to a binary decision -- which is suboptimal and prone to misleadingconclusions. Instead, we statistically model the underlying decision-makingprocess during 2AFC experiments using a binomial distribution. We use maximumlikelihood estimation to fit a distribution to the perceptual judgements,conditioned on the perceptual distance to test and impose consistency andsmoothness between our empirical estimates of the density. This way, we canevaluate a different number of judgements per triplet, and can calculatemetrics such as likelihoods of judgements according to a set of distances --key ingredients that neural network counterparts lack.</description><author>Alexander Hepburn, Raul Santos-Rodriguez, Javier Portilla</author><pubDate>Fri, 15 Mar 2024 16:21:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10390v1</guid></item><item><title>An Ecosystem for Personal Knowledge Graphs: A Survey and Research Roadmap</title><link>http://arxiv.org/abs/2304.09572v2</link><description>This paper presents an ecosystem for personal knowledge graphs (PKGs),commonly defined as resources of structured information about entities relatedto an individual, their attributes, and the relations between them. PKGs are akey enabler of secure and sophisticated personal data management andpersonalized services. However, there are challenges that need to be addressedbefore PKGs can achieve widespread adoption. One of the fundamental challengesis the very definition of what constitutes a PKG, as there are multipleinterpretations of the term. We propose our own definition of a PKG,emphasizing the aspects of (1) data ownership by a single individual and (2)the delivery of personalized services as the primary purpose. We further arguethat a holistic view of PKGs is needed to unlock their full potential, andpropose a unified framework for PKGs, where the PKG is a part of a largerecosystem with clear interfaces towards data services and data sources. Acomprehensive survey and synthesis of existing work is conducted, with amapping of the surveyed work into the proposed unified ecosystem. Finally, weidentify open challenges and research opportunities for the ecosystem as awhole, as well as for the specific aspects of PKGs, which include population,representation and management, and utilization.</description><author>Martin G. SkjÃ¦veland, Krisztian Balog, Nolwenn Bernard, Weronika Åajewska, Trond Linjordet</author><pubDate>Fri, 15 Mar 2024 16:20:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09572v2</guid></item><item><title>Monotonic Representation of Numeric Properties in Language Models</title><link>http://arxiv.org/abs/2403.10381v1</link><description>Language models (LMs) can express factual knowledge involving numericproperties such as Karl Popper was born in 1902. However, how this informationis encoded in the model's internal representations is not understood well.Here, we introduce a simple method for finding and editing representations ofnumeric properties such as an entity's birth year. Empirically, we findlow-dimensional subspaces that encode numeric properties monotonically, in aninterpretable and editable fashion. When editing representations alongdirections in these subspaces, LM output changes accordingly. For example, bypatching activations along a "birthyear" direction we can make the LM expressan increasingly late birthyear: Karl Popper was born in 1929, Karl Popper wasborn in 1957, Karl Popper was born in 1968. Property-encoding directions existacross several numeric properties in all models under consideration, suggestingthe possibility that monotonic representation of numeric propertiesconsistently emerges during LM pretraining. Code:https://github.com/bheinzerling/numeric-property-repr</description><author>Benjamin Heinzerling, Kentaro Inui</author><pubDate>Fri, 15 Mar 2024 16:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10381v1</guid></item><item><title>BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics</title><link>http://arxiv.org/abs/2403.10380v1</link><description>Deep learning (DL) models have emerged as a powerful tool in avianbioacoustics to diagnose environmental health and biodiversity. However,inconsistencies in research pose notable challenges hindering progress in thisdomain. Reliable DL models need to analyze bird calls flexibly across variousspecies and environments to fully harness the potential of bioacoustics in acost-effective passive acoustic monitoring scenario. Data fragmentation andopacity across studies complicate a comprehensive evaluation of general modelperformance. To overcome these challenges, we present the BirdSet benchmark, aunified framework consolidating research efforts with a holistic approach forclassifying bird vocalizations in avian bioacoustics. BirdSet harmonizesopen-source bird recordings into a curated dataset collection. This unifiedapproach provides an in-depth understanding of model performance and identifiespotential shortcomings across different tasks. By establishing baseline resultsof current models, BirdSet aims to facilitate comparability, guide subsequentdata collection, and increase accessibility for newcomers to avianbioacoustics.</description><author>Lukas Rauch, Raphael Schwinger, Moritz Wirth, RenÃ© Heinrich, Jonas Lange, Stefan Kahl, Bernhard Sick, Sven Tomforde, Christoph Scholz</author><pubDate>Fri, 15 Mar 2024 16:10:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10380v1</guid></item><item><title>Post-hoc Bias Scoring Is Optimal For Fair Classification</title><link>http://arxiv.org/abs/2310.05725v3</link><description>We consider a binary classification problem under group fairness constraints,which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), orEqualized Odds (EO). We propose an explicit characterization of Bayes optimalclassifier under the fairness constraints, which turns out to be a simplemodification rule of the unconstrained classifier. Namely, we introduce a novelinstance-level measure of bias, which we call bias score, and the modificationrule is a simple linear rule on top of the finite amount of bias scores.Basedon this characterization, we develop a post-hoc approach that allows us toadapt to fairness constraints while maintaining high accuracy. In the case ofDP and EOp constraints, the modification rule is thresholding a single biasscore, while in the case of EO constraints we are required to fit a linearmodification rule with 2 parameters. The method can also be applied forcomposite group-fairness criteria, such as ones involving several sensitiveattributes.</description><author>Wenlong Chen, Yegor Klochkov, Yang Liu</author><pubDate>Fri, 15 Mar 2024 16:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05725v3</guid></item><item><title>Regret Minimization via Saddle Point Optimization</title><link>http://arxiv.org/abs/2403.10379v1</link><description>A long line of works characterizes the sample complexity of regretminimization in sequential decision-making by min-max programs. In thecorresponding saddle-point game, the min-player optimizes the samplingdistribution against an adversarial max-player that chooses confusing modelsleading to large regret. The most recent instantiation of this idea is thedecision-estimation coefficient (DEC), which was shown to provide nearly tightlower and upper bounds on the worst-case expected regret in structured banditsand reinforcement learning. By re-parametrizing the offset DEC with theconfidence radius and solving the corresponding min-max program, we derive ananytime variant of the Estimation-To-Decisions (E2D) algorithm. Importantly,the algorithm optimizes the exploration-exploitation trade-off online insteadof via the analysis. Our formulation leads to a practical algorithm for finitemodel classes and linear feedback models. We further point out connections tothe information ratio, decoupling coefficient and PAC-DEC, and numericallyevaluate the performance of E2D on simple examples.</description><author>Johannes Kirschner, Seyed Alireza Bakhtiari, Kushagra Chandak, Volodymyr Tkachuk, Csaba SzepesvÃ¡ri</author><pubDate>Fri, 15 Mar 2024 16:09:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10379v1</guid></item><item><title>EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models</title><link>http://arxiv.org/abs/2403.10378v1</link><description>We introduce EXAMS-V, a new challenging multi-discipline multimodalmultilingual exam benchmark for evaluating vision language models. It consistsof 20,932 multiple-choice questions across 20 school disciplines coveringnatural science, social science, and other miscellaneous studies, e.g.,religion, fine arts, business, etc. EXAMS-V includes a variety of multimodalfeatures such as text, images, tables, figures, diagrams, maps, scientificsymbols, and equations. The questions come in 11 languages from 7 languagefamilies. Unlike existing benchmarks, EXAMS-V is uniquely curated by gatheringschool exam questions from various countries, with a variety of educationsystems. This distinctive approach calls for intricate reasoning across diverselanguages and relies on region-specific knowledge. Solving the problems in thedataset requires advanced perception and joint reasoning over the text and thevisual content of the image. Our evaluation results demonstrate that this is achallenging dataset, which is difficult even for advanced vision-text modelssuch as GPT-4V and Gemini; this underscores the inherent complexity of thedataset and its significance as a future benchmark.</description><author>Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, Preslav Nakov</author><pubDate>Fri, 15 Mar 2024 16:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10378v1</guid></item><item><title>GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence</title><link>http://arxiv.org/abs/2311.14155v2</link><description>We present GigaPose, a fast, robust, and accurate method for CAD-based novelobject pose estimation in RGB images. GigaPose first leverages discriminative"templates", rendered images of the CAD models, to recover the out-of-planerotation and then uses patch correspondences to estimate the four remainingparameters. Our approach samples templates in only a two-degrees-of-freedomspace instead of the usual three and matches the input image to the templatesusing fast nearest-neighbor search in feature space, results in a speedupfactor of 35x compared to the state of the art. Moreover, GigaPose issignificantly more robust to segmentation errors. Our extensive evaluation onthe seven core datasets of the BOP challenge demonstrates that it achievesstate-of-the-art accuracy and can be seamlessly integrated with existingrefinement methods. Additionally, we show the potential of GigaPose with 3Dmodels predicted by recent work on 3D reconstruction from a single image,relaxing the need for CAD models and making 6D pose object estimation much moreconvenient. Our source code and trained models are publicly available athttps://github.com/nv-nguyen/gigaPose</description><author>Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, Vincent Lepetit</author><pubDate>Fri, 15 Mar 2024 16:05:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14155v2</guid></item><item><title>PASTA: Towards Flexible and Efficient HDR Imaging Via Progressively Aggregated Spatio-Temporal Aligment</title><link>http://arxiv.org/abs/2403.10376v1</link><description>Leveraging Transformer attention has led to great advancements in HDRdeghosting. However, the intricate nature of self-attention introducespractical challenges, as existing state-of-the-art methods often demandhigh-end GPUs or exhibit slow inference speeds, especially for high-resolutionimages like 2K. Striking an optimal balance between performance and latencyremains a critical concern. In response, this work presents PASTA, a novelProgressively Aggregated Spatio-Temporal Alignment framework for HDRdeghosting. Our approach achieves effectiveness and efficiency by harnessinghierarchical representation during feature distanglement. Through theutilization of diverse granularities within the hierarchical structure, ourmethod substantially boosts computational speed and optimizes the HDR imagingworkflow. In addition, we explore within-scale feature modeling with local andglobal attention, gradually merging and refining them in a coarse-to-finefashion. Experimental results showcase PASTA's superiority over current SOTAmethods in both visual quality and performance metrics, accompanied by asubstantial 3-fold (x3) increase in inference speed.</description><author>Xiaoning Liu, Ao Li, Zongwei Wu, Yapeng Du, Le Zhang, Yulun Zhang, Radu Timofte, Ce Zhu</author><pubDate>Fri, 15 Mar 2024 16:05:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10376v1</guid></item><item><title>Overcoming Distribution Shifts in Plug-and-Play Methods with Test-Time Training</title><link>http://arxiv.org/abs/2403.10374v1</link><description>Plug-and-Play Priors (PnP) is a well-known class of methods for solvinginverse problems in computational imaging. PnP methods combine physical forwardmodels with learned prior models specified as image denoisers. A common issuewith the learned models is that of a performance drop when there is adistribution shift between the training and testing data. Test-time training(TTT) was recently proposed as a general strategy for improving the performanceof learned models when training and testing data come from differentdistributions. In this paper, we propose PnP-TTT as a new method for overcomingdistribution shifts in PnP. PnP-TTT uses deep equilibrium learning (DEQ) foroptimizing a self-supervised loss at the fixed points of PnP iterations.PnP-TTT can be directly applied on a single test sample to improve thegeneralization of PnP. We show through simulations that given a sufficientnumber of measurements, PnP-TTT enables the use of image priors trained onnatural images for image reconstruction in magnetic resonance imaging (MRI).</description><author>Edward P. Chandler, Shirin Shoushtari, Jiaming Liu, M. Salman Asif, Ulugbek S. Kamilov</author><pubDate>Fri, 15 Mar 2024 16:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10374v1</guid></item><item><title>Towards a general framework for improving the performance of classifiers using XAI methods</title><link>http://arxiv.org/abs/2403.10373v1</link><description>Modern Artificial Intelligence (AI) systems, especially Deep Learning (DL)models, poses challenges in understanding their inner workings by AIresearchers. eXplainable Artificial Intelligence (XAI) inspects internalmechanisms of AI models providing explanations about their decisions. Whilecurrent XAI research predominantly concentrates on explaining AI systems, thereis a growing interest in using XAI techniques to automatically improve theperformance of AI systems themselves. This paper proposes a general frameworkfor automatically improving the performance of pre-trained DL classifiers usingXAI methods, avoiding the computational overhead associated with retrainingcomplex models from scratch. In particular, we outline the possibility of twodifferent learning strategies for implementing this architecture, which we willcall auto-encoder-based and encoder-decoder-based, and discuss their keyaspects.</description><author>Andrea Apicella, Salvatore Giugliano, Francesco IsgrÃ², Roberto Prevete</author><pubDate>Fri, 15 Mar 2024 16:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10373v1</guid></item><item><title>Price-Discrimination Game for Distributed Resource Management in Federated Learning</title><link>http://arxiv.org/abs/2308.13838v5</link><description>In vanilla federated learning (FL) such as FedAvg, the parameter server (PS)and multiple distributed clients can form a typical buyer's market, where thenumber of PS/buyers of FL services is far less than the number ofclients/sellers. In order to improve the performance of FL and reduce the costof motivating clients to participate in FL, this paper proposes todifferentiate the pricing for services provided by different clients ratherthan simply providing the same service pricing for different clients. The priceis differentiated based on the performance improvements brought to FL and theirheterogeneity in computing and communication capabilities. To this end, aprice-discrimination game (PDG) is formulated to comprehensively address thedistributed resource management problems in FL, including multi-objectivetrade-off, client selection, and incentive mechanism. As the PDG is amixed-integer nonlinear programming (MINLP) problem, a distributedsemi-heuristic algorithm with low computational complexity and lowcommunication overhead is designed to solve it. The simulation result verifiesthe effectiveness of the proposed approach.</description><author>Han Zhang, Halvin Yang, Guopeng Zhang</author><pubDate>Fri, 15 Mar 2024 16:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13838v5</guid></item><item><title>Voting-based Multimodal Automatic Deception Detection</title><link>http://arxiv.org/abs/2307.07516v3</link><description>Automatic Deception Detection has been a hot research topic for a long time,using machine learning and deep learning to automatically detect deception,brings new light to this old field. In this paper, we proposed a voting-basedmethod for automatic deception detection from videos using audio, visual andlexical features. Experiments were done on two datasets, the Real-life trialdataset by Michigan University and the Miami University deception detectiondataset. Video samples were split into frames of images, audio, andmanuscripts. Our Voting-based Multimodal proposed solution consists of threemodels. The first model is CNN for detecting deception from images, the secondmodel is Support Vector Machine (SVM) on Mel spectrograms for detectingdeception from audio and the third model is Word2Vec on Support Vector Machine(SVM) for detecting deception from manuscripts. Our proposed solutionoutperforms state of the art. Best results achieved on images, audio and textwere 97%, 96%, 92% respectively on Real-Life Trial Dataset, and 97%, 82%, 73%on video, audio and text respectively on Miami University Deception Detection.</description><author>Lana Touma, Mohammad Al Horani, Manar Tailouni, Anas Dahabiah, Khloud Al Jallad</author><pubDate>Fri, 15 Mar 2024 16:03:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07516v3</guid></item><item><title>An Energy-Efficient Ensemble Approach for Mitigating Data Incompleteness in IoT Applications</title><link>http://arxiv.org/abs/2403.10371v1</link><description>Machine Learning (ML) is becoming increasingly important for IoT-basedapplications. However, the dynamic and ad-hoc nature of many IoT ecosystemsposes unique challenges to the efficacy of ML algorithms. One such challenge isdata incompleteness, which is manifested as missing sensor readings. Manyfactors, including sensor failures and/or network disruption, can cause dataincompleteness. Furthermore, most IoT systems are severely power-constrained.It is important that we build IoT-based ML systems that are robust against dataincompleteness while simultaneously being energy efficient. This paper presentsan empirical study of SECOE - a recent technique for alleviating dataincompleteness in IoT - with respect to its energy bottlenecks. Towardsaddressing the energy bottlenecks of SECOE, we propose ENAMLE - a proactive,energy-aware technique for mitigating the impact of concurrent missing data.ENAMLE is unique in the sense that it builds an energy-aware ensemble ofsub-models, each trained with a subset of sensors chosen carefully based ontheir correlations. Furthermore, at inference time, ENAMLE adaptively altersthe number of the ensemble of models based on the amount of missing data rateand the energy-accuracy trade-off. ENAMLE's design includes several novelmechanisms for minimizing energy consumption while maintaining accuracy. Wepresent extensive experimental studies on two distinct datasets thatdemonstrate the energy efficiency of ENAMLE and its ability to alleviate sensorfailures.</description><author>Yousef AlShehri, Lakshmish Ramaswamy</author><pubDate>Fri, 15 Mar 2024 16:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10371v1</guid></item><item><title>Open Stamped Parts Dataset</title><link>http://arxiv.org/abs/2403.10369v1</link><description>We present the Open Stamped Parts Dataset (OSPD), featuring synthetic andreal images of stamped metal sheets for auto manufacturing. The real partimages, captured from 7 cameras, consist of 7,980 unlabeled images and 1,680labeled images. In addition, we have compiled a defect dataset by overlayingsynthetically generated masks on 10% of the holes. The synthetic datasetreplicates the real manufacturing environment in terms of lighting and partplacement relative to the cameras. The synthetic data includes 7,980 trainingimages, 1,680 validation images and 1,680 test images, each with bounding boxand segmentation mask annotations around all holes. 10% of the holes in thesynthetic data mimic defects generated in the real image dataset. We trained ahole-detection model on the synthetic-OSPD, achieving a modified recall scoreof 67.2% and a precision of 94.4% . We anticipate researchers in the automanufacturing and broader machine learning and computer vision communitiesusing OSPD to advance the state of the art in defect detection of stamped holesin the metalsheet stamping process. The dataset is available for download at:https://tinyurl.com/hm6xatd7</description><author>Sara Antiles, Sachin S. Talathi</author><pubDate>Fri, 15 Mar 2024 16:00:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10369v1</guid></item><item><title>Conformal Predictions for Probabilistically Robust Scalable Machine Learning Classification</title><link>http://arxiv.org/abs/2403.10368v1</link><description>Conformal predictions make it possible to define reliable and robust learningalgorithms. But they are essentially a method for evaluating whether analgorithm is good enough to be used in practice. To define a reliable learningframework for classification from the very beginning of its design, the conceptof scalable classifier was introduced to generalize the concept of classicalclassifier by linking it to statistical order theory and probabilistic learningtheory. In this paper, we analyze the similarities between scalable classifiersand conformal predictions by introducing a new definition of a score functionand defining a special set of input variables, the conformal safety set, whichcan identify patterns in the input space that satisfy the error coverageguarantee, i.e., that the probability of observing the wrong (possibly unsafe)label for points belonging to this set is bounded by a predefined $\varepsilon$error level. We demonstrate the practical implications of this frameworkthrough an application in cybersecurity for identifying DNS tunneling attacks.Our work contributes to the development of probabilistically robust andreliable machine learning models.</description><author>Alberto Carlevaro, Teodoro Alamo Cantarero, Fabrizio Dabbene, Maurizio Mongelli</author><pubDate>Fri, 15 Mar 2024 15:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10368v1</guid></item><item><title>Testing MediaPipe Holistic for Linguistic Analysis of Nonmanual Markers in Sign Languages</title><link>http://arxiv.org/abs/2403.10367v1</link><description>Advances in Deep Learning have made possible reliable landmark tracking ofhuman bodies and faces that can be used for a variety of tasks. We test arecent Computer Vision solution, MediaPipe Holistic (MPH), to find out if itstracking of the facial features is reliable enough for a linguistic analysis ofdata from sign languages, and compare it to an older solution (OpenFace, OF).We use an existing data set of sentences in Kazakh-Russian Sign Language and anewly created small data set of videos with head tilts and eyebrow movements.We find that MPH does not perform well enough for linguistic analysis ofeyebrow movement -- but in a different way from OF, which is also performingpoorly without correction. We reiterate a previous proposal to train additionalcorrection models to overcome these limitations.</description><author>Anna Kuznetsova, Vadim Kimmelman</author><pubDate>Fri, 15 Mar 2024 15:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10367v1</guid></item><item><title>Scalable Algorithms for Individual Preference Stable Clustering</title><link>http://arxiv.org/abs/2403.10365v1</link><description>In this paper, we study the individual preference (IP) stability, which is annotion capturing individual fairness and stability in clustering. Within thissetting, a clustering is $\alpha$-IP stable when each data point's averagedistance to its cluster is no more than $\alpha$ times its average distance toany other cluster. In this paper, we study the natural local search algorithmfor IP stable clustering. Our analysis confirms a $O(\log n)$-IP stabilityguarantee for this algorithm, where $n$ denotes the number of points in theinput. Furthermore, by refining the local search approach, we show it runs inan almost linear time, $\tilde{O}(nk)$.</description><author>Ron Mosenzon, Ali Vakilian</author><pubDate>Fri, 15 Mar 2024 15:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10365v1</guid></item><item><title>CPGA: Coding Priors-Guided Aggregation Network for Compressed Video Quality Enhancement</title><link>http://arxiv.org/abs/2403.10362v1</link><description>Recently, numerous approaches have achieved notable success in compressedvideo quality enhancement (VQE). However, these methods usually ignore theutilization of valuable coding priors inherently embedded in compressed videos,such as motion vectors and residual frames, which carry abundant temporal andspatial information. To remedy this problem, we propose the CodingPriors-Guided Aggregation (CPGA) network to utilize temporal and spatialinformation from coding priors. The CPGA mainly consists of an inter-frametemporal aggregation (ITA) module and a multi-scale non-local aggregation (MNA)module. Specifically, the ITA module aggregates temporal information fromconsecutive frames and coding priors, while the MNA module globally capturesspatial information guided by residual frames. In addition, to facilitateresearch in VQE task, we newly construct the Video Coding Priors (VCP) dataset,comprising 300 videos with various coding priors extracted from correspondingbitstreams. It remedies the shortage of previous datasets on the lack of codinginformation. Experimental results demonstrate the superiority of our methodcompared to existing state-of-the-art methods. The code and dataset will bereleased at https://github.com/CPGA/CPGA.git.</description><author>Qiang Zhu, Jinhua Hao, Yukang Ding, Yu Liu, Qiao Mo, Ming Sun, Chao Zhou, Shuyuan Zhu</author><pubDate>Fri, 15 Mar 2024 15:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10362v1</guid></item><item><title>Neur2RO: Neural Two-Stage Robust Optimization</title><link>http://arxiv.org/abs/2310.04345v2</link><description>Robust optimization provides a mathematical framework for modeling andsolving decision-making problems under worst-case uncertainty. This workaddresses two-stage robust optimization (2RO) problems (also called adjustablerobust optimization), wherein first-stage and second-stage decisions are madebefore and after uncertainty is realized, respectively. This results in anested min-max-min optimization problem which is extremely challengingcomputationally, especially when the decisions are discrete. We proposeNeur2RO, an efficient machine learning-driven instantiation ofcolumn-and-constraint generation (CCG), a classical iterative algorithm for2RO. Specifically, we learn to estimate the value function of the second-stageproblem via a novel neural network architecture that is easy to optimize overby design. Embedding our neural network into CCG yields high-quality solutionsquickly as evidenced by experiments on two 2RO benchmarks, knapsack and capitalbudgeting. For knapsack, Neur2RO finds solutions that are within roughly $2\%$of the best-known values in a few seconds compared to the three hours of thestate-of-the-art exact branch-and-price algorithm; for larger and more complexinstances, Neur2RO finds even better solutions. For capital budgeting, Neur2ROoutperforms three variants of the $k$-adaptability algorithm, particularly onthe largest instances, with a 10 to 100-fold reduction in solution time. Ourcode and data are available at https://github.com/khalil-research/Neur2RO.</description><author>Justin Dumouchelle, Esther Julien, Jannis Kurtz, Elias B. Khalil</author><pubDate>Fri, 15 Mar 2024 15:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04345v2</guid></item><item><title>ANIM: Accurate Neural Implicit Model for Human Reconstruction from a single RGB-D image</title><link>http://arxiv.org/abs/2403.10357v1</link><description>Recent progress in human shape learning, shows that neural implicit modelsare effective in generating 3D human surfaces from limited number of views, andeven from a single RGB image. However, existing monocular approaches stillstruggle to recover fine geometric details such as face, hands or clothwrinkles. They are also easily prone to depth ambiguities that result indistorted geometries along the camera optical axis. In this paper, we explorethe benefits of incorporating depth observations in the reconstruction processby introducing ANIM, a novel method that reconstructs arbitrary 3D human shapesfrom single-view RGB-D images with an unprecedented level of accuracy. Ourmodel learns geometric details from both multi-resolution pixel-aligned andvoxel-aligned features to leverage depth information and enable spatialrelationships, mitigating depth ambiguities. We further enhance the quality ofthe reconstructed shape by introducing a depth-supervision strategy, whichimproves the accuracy of the signed distance field estimation of points thatlie on the reconstructed surface. Experiments demonstrate that ANIM outperformsstate-of-the-art works that use RGB, surface normals, point cloud or RGB-D dataas input. In addition, we introduce ANIM-Real, a new multi-modal datasetcomprising high-quality scans paired with consumer-grade RGB-D camera, and ourprotocol to fine-tune ANIM, enabling high-quality reconstruction fromreal-world human capture.</description><author>Marco Pesavento, Yuanlu Xu, Nikolaos Sarafianos, Robert Maier, Ziyan Wang, Chun-Han Yao, Marco Volino, Edmond Boyer, Adrian Hilton, Tony Tung</author><pubDate>Fri, 15 Mar 2024 15:45:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10357v1</guid></item><item><title>ScoreCL: Augmentation-Adaptive Contrastive Learning via Score-Matching Function</title><link>http://arxiv.org/abs/2306.04175v2</link><description>Self-supervised contrastive learning (CL) has achieved state-of-the-artperformance in representation learning by minimizing the distance betweenpositive pairs while maximizing that of negative ones. Recently, it has beenverified that the model learns better representation with diversely augmentedpositive pairs because they enable the model to be more view-invariant.However, only a few studies on CL have considered the difference betweenaugmented views, and have not gone beyond the hand-crafted findings. In thispaper, we first observe that the score-matching function can measure how muchdata has changed from the original through augmentation. With the observedproperty, every pair in CL can be weighted adaptively by the difference ofscore values, resulting in boosting the performance of the existing CL method.We show the generality of our method, referred to as ScoreCL, by consistentlyimproving various CL methods, SimCLR, SimSiam, W-MSE, and VICReg, up to 3%p ink-NN evaluation on CIFAR-10, CIFAR-100, and ImageNet-100. Moreover, we haveconducted exhaustive experiments and ablations, including results on diversedownstream tasks, comparison with possible baselines, and improvement when usedwith other proposed augmentation methods. We hope our exploration will inspiremore research in exploiting the score matching for CL.</description><author>Jin-Young Kim, Soonwoo Kwon, Hyojun Go, Yunsung Lee, Seungtaek Choi</author><pubDate>Fri, 15 Mar 2024 15:44:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04175v2</guid></item><item><title>SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras</title><link>http://arxiv.org/abs/2403.10353v1</link><description>The field of autonomous driving has attracted considerable interest inapproaches that directly infer 3D objects in the Bird's Eye View (BEV) frommultiple cameras. Some attempts have also explored utilizing 2D detectors fromsingle images to enhance the performance of 3D detection. However, theseapproaches rely on a two-stage process with separate detectors, where the 2Ddetection results are utilized only once for token selection or queryinitialization. In this paper, we present a single model termed SimPB, whichsimultaneously detects 2D objects in the perspective view and 3D objects in theBEV space from multiple cameras. To achieve this, we introduce a hybrid decoderconsisting of several multi-view 2D decoder layers and several 3D decoderlayers, specifically designed for their respective detection tasks. A DynamicQuery Allocation module and an Adaptive Query Aggregation module are proposedto continuously update and refine the interaction between 2D and 3D results, ina cyclic 3D-2D-3D manner. Additionally, Query-group Attention is utilized tostrengthen the interaction among 2D queries within each camera group. In theexperiments, we evaluate our method on the nuScenes dataset and demonstratepromising results for both 2D and 3D detection tasks. Our code is available at:https://github.com/nullmax-vision/SimPB.</description><author>Yingqi Tang, Zhaotie Meng, Guoliang Chen, Erkang Cheng</author><pubDate>Fri, 15 Mar 2024 15:39:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10353v1</guid></item><item><title>TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale</title><link>http://arxiv.org/abs/2403.10351v1</link><description>The advent of large language models (LLMs) has significantly advanced naturallanguage processing tasks like text summarization. However, their large sizeand computational demands, coupled with privacy concerns in data transmission,limit their use in resource-constrained and privacy-centric settings. Toovercome this, we introduce TriSum, a framework for distilling LLMs' textsummarization abilities into a compact, local model. Initially, LLMs extract aset of aspect-triple rationales and summaries, which are refined using adual-scoring method for quality. Next, a smaller local model is trained withthese tasks, employing a curriculum learning strategy that evolves from simpleto complex tasks. Our method enhances local model performance on variousbenchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability byproviding insights into the summarization rationale.</description><author>Pengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder Bhatia, Jimeng Sun, Jiawei Han</author><pubDate>Fri, 15 Mar 2024 15:36:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10351v1</guid></item><item><title>Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks</title><link>http://arxiv.org/abs/2403.05407v2</link><description>In the investigation of any causal mechanisms, such as the brain's causalnetworks, the assumption of causal sufficiency plays a critical role. Notably,neglecting this assumption can result in significant errors, a fact that isoften disregarded in the causal analysis of brain networks. In this study, wepropose an algorithmic identification approach for determining essentialexogenous nodes that satisfy the critical need for causal sufficiency to adhereto it in such inquiries. Our approach consists of three main steps: First, bycapturing the essence of the Peter-Clark (PC) algorithm, we conductindependence tests for pairs of regions within a network, as well as for thesame pairs conditioned on nodes from other networks. Next, we distinguishcandidate confounders by analyzing the differences between the conditional andunconditional results, using the Kolmogorov-Smirnov test. Subsequently, weutilize Non-Factorized identifiable Variational Autoencoders (NF-iVAE) alongwith the Correlation Coefficient index (CCI) metric to identify the confoundingvariables within these candidate nodes. Applying our method to the HumanConnectome Projects (HCP) movie-watching task data, we demonstrate that whileinteractions exist between dorsal and ventral regions, only dorsal regionsserve as confounders for the visual networks, and vice versa. These findingsalign consistently with those resulting from the neuroscientific perspective.Finally, we show the reliability of our results by testing 30 independent runsfor NF-iVAE initialization.</description><author>Abdolmahdi Bagheri, Mahdi Dehshiri, Babak Nadjar Araabi, Alireza Akhondi Asl</author><pubDate>Fri, 15 Mar 2024 15:35:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05407v2</guid></item><item><title>ParaPoint: Learning Global Free-Boundary Surface Parameterization of 3D Point Clouds</title><link>http://arxiv.org/abs/2403.10349v1</link><description>Surface parameterization is a fundamental geometry processing problem withrich downstream applications. Traditional approaches are designed to operate onwell-behaved mesh models with high-quality triangulations that are laboriouslyproduced by specialized 3D modelers, and thus unable to meet the processingdemand for the current explosion of ordinary 3D data. In this paper, we seek toperform UV unwrapping on unstructured 3D point clouds. Technically, we proposeParaPoint, an unsupervised neural learning pipeline for achieving globalfree-boundary surface parameterization by building point-wise mappings betweengiven 3D points and 2D UV coordinates with adaptively deformed boundaries. Weingeniously construct several geometrically meaningful sub-networks withspecific functionalities, and assemble them into a bi-directional cycle mappingframework. We also design effective loss functions and auxiliary differentialgeometric constraints for the optimization of the neural mapping process. Tothe best of our knowledge, this work makes the first attempt to investigateneural point cloud parameterization that pursues both global mappings and freeboundaries. Experiments demonstrate the effectiveness and inspiring potentialof our proposed learning paradigm. The code will be publicly available.</description><author>Qijian Zhang, Junhui Hou, Ying He</author><pubDate>Fri, 15 Mar 2024 15:35:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10349v1</guid></item><item><title>Denoising Task Difficulty-based Curriculum for Training Diffusion Models</title><link>http://arxiv.org/abs/2403.10348v1</link><description>Diffusion-based generative models have emerged as powerful tools in the realmof generative modeling. Despite extensive research on denoising across varioustimesteps and noise levels, a conflict persists regarding the relativedifficulties of the denoising tasks. While various studies argue that lowertimesteps present more challenging tasks, others contend that higher timestepsare more difficult. To address this conflict, our study undertakes acomprehensive examination of task difficulties, focusing on convergencebehavior and changes in relative entropy between consecutive probabilitydistributions across timesteps. Our observational study reveals that denoisingat earlier timesteps poses challenges characterized by slower convergence andhigher relative entropy, indicating increased task difficulty at these lowertimesteps. Building on these observations, we introduce an easy-to-hardlearning scheme, drawing from curriculum learning, to enhance the trainingprocess of diffusion models. By organizing timesteps or noise levels intoclusters and training models with descending orders of difficulty, wefacilitate an order-aware training regime, progressing from easier to harderdenoising tasks, thereby deviating from the conventional approach of trainingdiffusion models simultaneously across all timesteps. Our approach leads toimproved performance and faster convergence by leveraging the benefits ofcurriculum learning, while maintaining orthogonality with existing improvementsin diffusion training techniques. We validate these advantages throughcomprehensive experiments in image generation tasks, including unconditional,class-conditional, and text-to-image generation.</description><author>Jin-Young Kim, Hyojun Go, Soonwoo Kwon, Hyun-Gyoon Kim</author><pubDate>Fri, 15 Mar 2024 15:34:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10348v1</guid></item><item><title>End-to-end Adaptive Dynamic Subsampling and Reconstruction for Cardiac MRI</title><link>http://arxiv.org/abs/2403.10346v1</link><description>Accelerating dynamic MRI is essential for enhancing clinical applications,such as adaptive radiotherapy, and improving patient comfort. Traditional deeplearning (DL) approaches for accelerated dynamic MRI reconstruction typicallyrely on predefined or random subsampling patterns, applied uniformly across alltemporal phases. This standard practice overlooks the potential benefits ofleveraging temporal correlations and lacks the adaptability required forcase-specific subsampling optimization, which holds the potential formaximizing reconstruction quality. Addressing this gap, we present a novelend-to-end framework for adaptive dynamic MRI subsampling and reconstruction.Our pipeline integrates a DL-based adaptive sampler, generating case-specificdynamic subsampling patterns, trained end-to-end with a state-of-the-art 2Ddynamic reconstruction network, namely vSHARP, which effectively reconstructsthe adaptive dynamic subsampled data into a moving image. Our method isassessed using dynamic cine cardiac MRI data, comparing its performance againstvSHARP models that employ common subsampling trajectories, and pipelinestrained to optimize dataset-specific sampling schemes alongside vSHARPreconstruction. Our results indicate superior reconstruction quality,particularly at high accelerations.</description><author>George Yiasemis, Jan-Jakob Sonke, Jonas Teuwen</author><pubDate>Fri, 15 Mar 2024 15:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10346v1</guid></item><item><title>SelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency</title><link>http://arxiv.org/abs/2303.07033v3</link><description>This work presents an effective depth-consistency self-prompt Transformer forimage dehazing. It is motivated by an observation that the estimated depths ofan image with haze residuals and its clear counterpart vary. Enforcing thedepth consistency of dehazed images with clear ones, therefore, is essentialfor dehazing. For this purpose, we develop a prompt based on the features ofdepth differences between the hazy input images and corresponding clearcounterparts that can guide dehazing models for better restoration.Specifically, we first apply deep features extracted from the input images tothe depth difference features for generating the prompt that contains the hazeresidual information in the input. Then we propose a prompt embedding modulethat is designed to perceive the haze residuals, by linearly adding the promptto the deep features. Further, we develop an effective prompt attention moduleto pay more attention to haze residuals for better removal. By incorporatingthe prompt, prompt embedding, and prompt attention into an encoder-decodernetwork based on VQGAN, we can achieve better perception quality. As the depthsof clear images are not available at inference, and the dehazed images withone-time feed-forward execution may still contain a portion of haze residuals,we propose a new continuous self-prompt inference that can iteratively correctthe dehazing model towards better haze-free image generation. Extensiveexperiments show that our method performs favorably against thestate-of-the-art approaches on both synthetic and real-world datasets in termsof perception metrics including NIQE, PI, and PIQE.</description><author>Cong Wang, Jinshan Pan, Wanyu Lin, Jiangxin Dong, Xiao-Ming Wu</author><pubDate>Fri, 15 Mar 2024 15:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07033v3</guid></item><item><title>SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric hybrid solution</title><link>http://arxiv.org/abs/2403.10344v1</link><description>Neural implicit surface representation methods have recently shown impressive3D reconstruction results. However, existing solutions struggle to reconstructurban outdoor scenes due to their large, unbounded, and highly detailed nature.Hence, to achieve accurate reconstructions, additional supervision data such asLiDAR, strong geometric priors, and long training times are required. To tacklesuch issues, we present SCILLA, a new hybrid implicit surface learning methodto reconstruct large driving scenes from 2D images. SCILLA's hybridarchitecture models two separate implicit fields: one for the volumetricdensity and another for the signed distance to the surface. To accuratelyrepresent urban outdoor scenarios, we introduce a novel volume-renderingstrategy that relies on self-supervised probabilistic density estimation tosample points near the surface and transition progressively from volumetric tosurface representation. Our solution permits a proper and fast initializationof the signed distance field without relying on any geometric prior on thescene, compared to concurrent methods. By conducting extensive experiments onfour outdoor driving datasets, we show that SCILLA can learn an accurate anddetailed 3D surface scene representation in various urban scenarios while beingtwo times faster to train compared to previous state-of-the-art solutions.</description><author>Hala Djeghim, Nathan Piasco, Moussab Bennehar, Luis RoldÃ£o, Dzmitry Tsishkou, DÃ©sirÃ© SidibÃ©</author><pubDate>Fri, 15 Mar 2024 15:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10344v1</guid></item><item><title>Thermal-NeRF: Neural Radiance Fields from an Infrared Camera</title><link>http://arxiv.org/abs/2403.10340v1</link><description>In recent years, Neural Radiance Fields (NeRFs) have demonstrated significantpotential in encoding highly-detailed 3D geometry and environmental appearance,positioning themselves as a promising alternative to traditional explicitrepresentation for 3D scene reconstruction. However, the predominant relianceon RGB imaging presupposes ideal lighting conditions: a premise frequentlyunmet in robotic applications plagued by poor lighting or visual obstructions.This limitation overlooks the capabilities of infrared (IR) cameras, whichexcel in low-light detection and present a robust alternative under suchadverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the firstmethod that estimates a volumetric scene representation in the form of a NeRFsolely from IR imaging. By leveraging a thermal mapping and structural thermalconstraint derived from the thermal characteristics of IR imaging, our methodshowcasing unparalleled proficiency in recovering NeRFs in visually degradedscenes where RGB-based methods fall short. We conduct extensive experiments todemonstrate that Thermal-NeRF can achieve superior quality compared to existingmethods. Furthermore, we contribute a dataset for IR-based NeRF applications,paving the way for future research in IR NeRF reconstruction.</description><author>Tianxiang Ye, Qi Wu, Junyuan Deng, Guoqing Liu, Liu Liu, Songpengcheng Xia, Liang Pang, Wenxian Yu, Ling Pei</author><pubDate>Fri, 15 Mar 2024 15:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10340v1</guid></item><item><title>Robust Identity Perceptual Watermark Against Deepfake Face Swapping</title><link>http://arxiv.org/abs/2311.01357v2</link><description>Notwithstanding offering convenience and entertainment to society, Deepfakeface swapping has caused critical privacy issues with the rapid development ofdeep generative models. Due to imperceptible artifacts in high-qualitysynthetic images, passive detection models against face swapping in recentyears usually suffer performance damping regarding the generalizability issue.Therefore, several studies have been attempted to proactively protect theoriginal images against malicious manipulations by inserting invisible signalsin advance. However, the existing proactive defense approaches demonstrateunsatisfactory results with respect to visual quality, detection accuracy, andsource tracing ability. In this study, to fulfill the research gap, we proposethe first robust identity perceptual watermarking framework that concurrentlyperforms detection and source tracing against Deepfake face swappingproactively. We assign identity semantics regarding the image contents to thewatermarks and devise an unpredictable and nonreversible chaotic encryptionsystem to ensure watermark confidentiality. The watermarks are encoded andrecovered by jointly training an encoder-decoder framework along withadversarial image manipulations. Falsification and source tracing areaccomplished by justifying the consistency between the content-matched identityperceptual watermark and the recovered robust watermark from the image.Extensive experiments demonstrate state-of-the-art detection performance onDeepfake face swapping under both cross-dataset and cross-manipulationsettings.</description><author>Tianyi Wang, Mengxiao Huang, Harry Cheng, Bin Ma, Yinglong Wang</author><pubDate>Fri, 15 Mar 2024 15:27:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01357v2</guid></item></channel></rss>