<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 25 May 2023 06:00:27 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>On Consistency of Signatures Using Lasso</title><link>http://arxiv.org/abs/2305.10413v2</link><description>Signature transforms are iterated path integrals of continuous anddiscrete-time time series data, and their universal nonlinearity linearizes theproblem of feature selection. This paper revisits the consistency issue ofLasso regression for the signature transform, both theoretically andnumerically. Our study shows that, for processes and time series that arecloser to Brownian motion or random walk with weaker inter-dimensionalcorrelations, the Lasso regression is more consistent for their signaturesdefined by It\^o integrals; for mean reverting processes and time series, theirsignatures defined by Stratonovich integrals have more consistency in the Lassoregression. Our findings highlight the importance of choosing appropriatedefinitions of signatures and stochastic models in statistical inference andmachine learning.</description><author>Xin Guo, Ruixun Zhang, Chaoyi Zhao</author><pubDate>Wed, 24 May 2023 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10413v2</guid></item><item><title>Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective</title><link>http://arxiv.org/abs/2305.15408v1</link><description>Recent studies have discovered that Chain-of-Thought prompting (CoT) candramatically improve the performance of Large Language Models (LLMs),particularly when dealing with complex tasks involving mathematics orreasoning. Despite the enormous empirical success, the underlying mechanismsbehind CoT and how it unlocks the potential of LLMs remain elusive. In thispaper, we take a first step towards theoretically answering these questions.Specifically, we examine the capacity of LLMs with CoT in solving fundamentalmathematical and decision-making problems. We start by giving an impossibilityresult showing that any bounded-depth Transformer cannot directly outputcorrect answers for basic arithmetic/equation tasks unless the model size growssuper-polynomially with respect to the input length. In contrast, we then proveby construction that autoregressive Transformers of a constant size suffice tosolve both tasks by generating CoT derivations using a commonly-used mathlanguage format. Moreover, we show LLMs with CoT are capable of solving ageneral class of decision-making problems known as Dynamic Programming, thusjustifying its power in tackling complex real-world tasks. Finally, extensiveexperiments on four tasks show that, while Transformers always fail to predictthe answers directly, they can consistently learn to generate correct solutionsstep-by-step given sufficient CoT demonstrations.</description><author>Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, Liwei Wang</author><pubDate>Wed, 24 May 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15408v1</guid></item><item><title>Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic Contrast Sets</title><link>http://arxiv.org/abs/2305.15407v1</link><description>Vision-language models are growing in popularity and public visibility togenerate, edit, and caption images at scale; but their outputs can perpetuateand amplify societal biases learned during pre-training on uncurated image-textpairs from the internet. Although debiasing methods have been proposed, weargue that these measurements of model bias lack validity due to dataset bias.We demonstrate there are spurious correlations in COCO Captions, the mostcommonly used dataset for evaluating bias, between background context and thegender of people in-situ. This is problematic because commonly-used biasmetrics (such as Bias@K) rely on per-gender base rates. To address this issue,we propose a novel dataset debiasing pipeline to augment the COCO dataset withsynthetic, gender-balanced contrast sets, where only the gender of the subjectis edited and the background is fixed. However, existing image editing methodshave limitations and sometimes produce low-quality images; so, we introduce amethod to automatically filter the generated images based on their similarityto real images. Using our balanced synthetic contrast sets, we benchmark biasin multiple CLIP-based models, demonstrating how metrics are skewed byimbalance in the original COCO images. Our results indicate that the proposedapproach improves the validity of the evaluation, ultimately contributing tomore realistic understanding of bias in vision-language models.</description><author>Brandon Smith, Miguel Farinha, Siobhan Mackenzie Hall, Hannah Rose Kirk, Aleksandar Shtedritski, Max Bain</author><pubDate>Wed, 24 May 2023 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15407v1</guid></item><item><title>Unit-based Speech-to-Speech Translation Without Parallel Data</title><link>http://arxiv.org/abs/2305.15405v1</link><description>We propose an unsupervised speech-to-speech translation (S2ST) system thatdoes not rely on parallel data between the source and target languages. Ourapproach maps source and target language speech signals into automaticallydiscovered, discrete units and reformulates the problem as unsupervisedunit-to-unit machine translation. We develop a three-step training procedurethat involves (a) pre-training an unit-based encoder-decoder language modelwith a denoising objective (b) training it with word-by-word translatedutterance pairs created by aligning monolingual text embedding spaces and (c)running unsupervised backtranslation bootstrapping off of the initialtranslation model. Our approach avoids mapping the speech signal into text anduses speech-to-unit and unit-to-speech models instead of automatic speechrecognition and text to speech models. We evaluate our model onsynthetic-speaker Europarl-ST English-German and German-English evaluationsets, finding that unit-based translation is feasible under this constrainedscenario, achieving 9.29 ASR-BLEU in German to English and 8.07 in English toGerman.</description><author>Anuj Diwan, Anirudh Srinivasan, David Harwath, Eunsol Choi</author><pubDate>Wed, 24 May 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15405v1</guid></item><item><title>RoMa: Revisiting Robust Losses for Dense Feature Matching</title><link>http://arxiv.org/abs/2305.15404v1</link><description>Dense feature matching is an important computer vision task that involvesestimating all correspondences between two images of a 3D scene. In this paper,we revisit robust losses for matching from a Markov chain perspective, yieldingtheoretical insights and large gains in performance. We begin by constructing aunifying formulation of matching as a Markov chain, based on which we identifytwo key stages which we argue should be decoupled for matching. The first isthe coarse stage, where the estimated result needs to be globally consistent.The second is the refinement stage, where the model needs precise localizationcapabilities. Inspired by the insight that these stages concern distinctissues, we propose a coarse matcher following the regression-by-classificationparadigm that provides excellent globally consistent, albeit not exactlylocalized, matches. This is followed by a local feature refinement stage usingwell-motivated robust regression losses, yielding extremely precise matches.Our proposed approach, which we call RoMa, achieves significant improvementscompared to the state-of-the-art. Code is available athttps://github.com/Parskatt/RoMa</description><author>Johan Edstedt, Qiyu Sun, Georg Bökman, Mårten Wadenbäck, Michael Felsberg</author><pubDate>Wed, 24 May 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15404v1</guid></item><item><title>AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation</title><link>http://arxiv.org/abs/2305.15403v1</link><description>Direct speech-to-speech translation (S2ST) aims to convert speech from onelanguage into another, and has demonstrated significant progress to date.Despite the recent success, current S2ST models still suffer from distinctdegradation in noisy environments and fail to translate visual speech (i.e.,the movement of lips and teeth). In this work, we present AV-TranSpeech, thefirst audio-visual speech-to-speech (AV-S2ST) translation model without relyingon intermediate text. AV-TranSpeech complements the audio stream with visualinformation to promote system robustness and opens up a host of practicalapplications: dictation or dubbing archival films. To mitigate the datascarcity with limited parallel AV-S2ST data, we 1) explore self-supervisedpre-training with unlabeled audio-visual data to learn contextualrepresentation, and 2) introduce cross-modal distillation with S2ST modelstrained on the audio-only corpus to further reduce the requirements of visualdata. Experimental results on two language pairs demonstrate that AV-TranSpeechoutperforms audio-only models under all settings regardless of the type ofnoise. With low-resource audio-visual data (10h, 30h), cross-modal distillationyields an improvement of 7.6 BLEU on average compared with baselines. Audiosamples are available at https://AV-TranSpeech.github.io</description><author>Rongjie Huang, Huadai Liu, Xize Cheng, Yi Ren, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin, Zhou Zhao</author><pubDate>Wed, 24 May 2023 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15403v1</guid></item><item><title>Beyond Invariance: Test-Time Label-Shift Adaptation for Distributions with "Spurious" Correlations</title><link>http://arxiv.org/abs/2211.15646v3</link><description>Changes in the data distribution at test time can have deleterious effects onthe performance of predictive models $p(y|x)$. We consider situations wherethere are additional meta-data labels (such as group labels), denoted by $z$,that can account for such changes in the distribution. In particular, we assumethat the prior distribution $p(y, z)$, which models the dependence between theclass label $y$ and the "nuisance" factors $z$, may change across domains,either due to a change in the correlation between these terms, or a change inone of their marginals. However, we assume that the generative model forfeatures $p(x|y, z)$ is invariant across domains. We note that this correspondsto an expanded version of the widely used "label shift" assumption, where thelabels now also include the nuisance factors $z$. Based on this observation, wepropose a test-time label shift correction that adapts to changes in the jointdistribution $p(y, z)$ using EM applied to unlabeled samples from the targetdomain distribution, $p_t(x)$. Importantly, we are able to avoid fitting agenerative model $p(x|y,z)$, and merely need to reweight the outputs of adiscriminative model $p_s(y,z|x)$ trained on the source distribution. Weevaluate our method, which we call "Test-Time Label-Shift Adaptation" (TTLSA),on several standard image and text datasets, as well as the CheXpert chestX-ray dataset, and show that it improves performance over methods that targetinvariance to changes in the distribution, as well as baseline empirical riskminimization methods. Code for reproducing experiments is available athttps://github.com/nalzok/test-time-label-shift .</description><author>Qingyao Sun, Kevin Murphy, Sayna Ebrahimi, Alexander D'Amour</author><pubDate>Wed, 24 May 2023 18:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15646v3</guid></item><item><title>TextDiffuser: Diffusion Models as Text Painters</title><link>http://arxiv.org/abs/2305.10855v2</link><description>Diffusion models have gained increasing attention for their impressivegeneration abilities but currently struggle with rendering accurate andcoherent text. To address this issue, we introduce TextDiffuser, focusing ongenerating images with visually appealing text that is coherent withbackgrounds. TextDiffuser consists of two stages: first, a Transformer modelgenerates the layout of keywords extracted from text prompts, and thendiffusion models generate images conditioned on the text prompt and thegenerated layout. Additionally, we contribute the first large-scale text imagesdataset with OCR annotations, MARIO-10M, containing 10 million image-text pairswith text recognition, detection, and character-level segmentation annotations.We further collect the MARIO-Eval benchmark to serve as a comprehensive toolfor evaluating text rendering quality. Through experiments and user studies, weshow that TextDiffuser is flexible and controllable to create high-quality textimages using text prompts alone or together with text template images, andconduct text inpainting to reconstruct incomplete images with text. The code,model, and dataset will be available at \url{https://aka.ms/textdiffuser}.</description><author>Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei</author><pubDate>Wed, 24 May 2023 18:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10855v2</guid></item><item><title>Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape</title><link>http://arxiv.org/abs/2305.15399v1</link><description>Synthesizing novel 3D models that resemble the input example has long beenpursued by researchers and artists in computer graphics. In this paper, wepresent Sin3DM, a diffusion model that learns the internal patch distributionfrom a single 3D textured shape and generates high-quality variations with finegeometry and texture details. Training a diffusion model directly in 3D wouldinduce large memory and computational cost. Therefore, we first compress theinput into a lower-dimensional latent space and then train a diffusion model onit. Specifically, we encode the input 3D textured shape into triplane featuremaps that represent the signed distance and texture fields of the input. Thedenoising network of our diffusion model has a limited receptive field to avoidoverfitting, and uses triplane-aware 2D convolution blocks to improve theresult quality. Aside from randomly generating new samples, our model alsofacilitates applications such as retargeting, outpainting and local editing.Through extensive qualitative and quantitative evaluation, we show that ourmodel can generate 3D shapes of various types with better quality than priormethods.</description><author>Rundi Wu, Ruoshi Liu, Carl Vondrick, Changxi Zheng</author><pubDate>Wed, 24 May 2023 18:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15399v1</guid></item><item><title>Differentially-Private Decision Trees with Probabilistic Robustness to Data Poisoning</title><link>http://arxiv.org/abs/2305.15394v1</link><description>Decision trees are interpretable models that are well-suited to non-linearlearning problems. Much work has been done on extending decision tree learningalgorithms with differential privacy, a system that guarantees the privacy ofsamples within the training data. However, current state-of-the-art algorithmsfor this purpose sacrifice much utility for a small privacy benefit. Thesesolutions create random decision nodes that reduce decision tree accuracy orspend an excessive share of the privacy budget on labeling leaves. Moreover,many works do not support or leak information about feature values when data iscontinuous. We propose a new method called PrivaTree based on privatehistograms that chooses good splits while consuming a small privacy budget. Theresulting trees provide a significantly better privacy-utility trade-off andaccept mixed numerical and categorical data without leaking additionalinformation. Finally, while it is notoriously hard to give robustnessguarantees against data poisoning attacks, we prove bounds for the expectedsuccess rates of backdoor attacks against differentially-private learners. Ourexperimental results show that PrivaTree consistently outperforms previousworks on predictive accuracy and significantly improves robustness againstbackdoor attacks compared to regular decision trees.</description><author>Daniël Vos, Jelle Vos, Tianyu Li, Zekeriya Erkin, Sicco Verwer</author><pubDate>Wed, 24 May 2023 18:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15394v1</guid></item><item><title>LayoutGPT: Compositional Visual Planning and Generation with Large Language Models</title><link>http://arxiv.org/abs/2305.15393v1</link><description>Attaining a high degree of user controllability in visual generation oftenrequires intricate, fine-grained inputs like layouts. However, such inputsimpose a substantial burden on users when compared to simple text inputs. Toaddress the issue, we study how Large Language Models (LLMs) can serve asvisual planners by generating layouts from text conditions, and thuscollaborate with visual generative models. We propose LayoutGPT, a method tocompose in-context visual demonstrations in style sheet language to enhance thevisual planning skills of LLMs. LayoutGPT can generate plausible layouts inmultiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT alsoshows superior performance in converting challenging language concepts likenumerical and spatial relations to layout arrangements for faithfultext-to-image generation. When combined with a downstream image generationmodel, LayoutGPT outperforms text-to-image models/systems by 20-40% andachieves comparable performance as human users in designing visual layouts fornumerical and spatial correctness. Lastly, LayoutGPT achieves comparableperformance to supervised methods in 3D indoor scene synthesis, demonstratingits effectiveness and potential in multiple visual domains.</description><author>Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, William Yang Wang</author><pubDate>Wed, 24 May 2023 18:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15393v1</guid></item><item><title>A Neural Space-Time Representation for Text-to-Image Personalization</title><link>http://arxiv.org/abs/2305.15391v1</link><description>A key aspect of text-to-image personalization methods is the manner in whichthe target concept is represented within the generative process. This choicegreatly affects the visual fidelity, downstream editability, and disk spaceneeded to store the learned concept. In this paper, we explore a newtext-conditioning space that is dependent on both the denoising processtimestep (time) and the denoising U-Net layers (space) and showcase itscompelling properties. A single concept in the space-time representation iscomposed of hundreds of vectors, one for each combination of time and space,making this space challenging to optimize directly. Instead, we propose toimplicitly represent a concept in this space by optimizing a small neuralmapper that receives the current time and space parameters and outputs thematching token embedding. In doing so, the entire personalized concept isrepresented by the parameters of the learned mapper, resulting in a compact,yet expressive, representation. Similarly to other personalization methods, theoutput of our neural mapper resides in the input space of the text encoder. Weobserve that one can significantly improve the convergence and visual fidelityof the concept by introducing a textual bypass, where our neural mapperadditionally outputs a residual that is added to the output of the textencoder. Finally, we show how one can impose an importance-based ordering overour implicit representation, providing users control over the reconstructionand editability of the learned concept using a single trained model. Wedemonstrate the effectiveness of our approach over a range of concepts andprompts, showing our method's ability to generate high-quality and controllablecompositions without fine-tuning any parameters of the generative model itself.</description><author>Yuval Alaluf, Elad Richardson, Gal Metzer, Daniel Cohen-Or</author><pubDate>Wed, 24 May 2023 18:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15391v1</guid></item><item><title>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2304.09842v2</link><description>Large language models (LLMs) have achieved remarkable progress in solvingvarious natural language processing tasks due to emergent reasoning abilities.However, LLMs have inherent limitations as they are incapable of accessingup-to-date information (stored on the Web or in task-specific knowledge bases),using external tools, and performing precise mathematical and logicalreasoning. In this paper, we present Chameleon, an AI system that mitigatesthese limitations by augmenting LLMs with plug-and-play modules forcompositional reasoning. Chameleon synthesizes programs by composing varioustools (e.g., LLMs, off-the-shelf vision models, web search engines, Pythonfunctions, and heuristic-based modules) for accomplishing complex reasoningtasks. At the heart of Chameleon is an LLM-based planner that assembles asequence of tools to execute to generate the final response. We showcase theeffectiveness of Chameleon on two multi-modal knowledge-intensive reasoningtasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%overall accuracy on ScienceQA, improving the best published few-shot result by11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,lifting the state of the art to 98.78%. Our analysis also shows that theGPT-4-powered planner exhibits more consistent and rational tool selection viainferring potential constraints from instructions, compared to aChatGPT-powered planner.</description><author>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao</author><pubDate>Wed, 24 May 2023 18:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09842v2</guid></item><item><title>Comparing Humans and Models on a Similar Scale: Towards Cognitive Gender Bias Evaluation in Coreference Resolution</title><link>http://arxiv.org/abs/2305.15389v1</link><description>Spurious correlations were found to be an important factor explaining modelperformance in various NLP tasks (e.g., gender or racial artifacts), oftenconsidered to be ''shortcuts'' to the actual task. However, humans tend tosimilarly make quick (and sometimes wrong) predictions based on societal andcognitive presuppositions. In this work we address the question: can wequantify the extent to which model biases reflect human behaviour? Answeringthis question will help shed light on model performance and provide meaningfulcomparisons against humans. We approach this question through the lens of thedual-process theory for human decision-making. This theory differentiatesbetween an automatic unconscious (and sometimes biased) ''fast system'' and a''slow system'', which when triggered may revisit earlier automatic reactions.We make several observations from two crowdsourcing experiments of gender biasin coreference resolution, using self-paced reading to study the ''fast''system, and question answering to study the ''slow'' system under a constrainedtime setting. On real-world data humans make $\sim$3\% more gender-biaseddecisions compared to models, while on synthetic data models are $\sim$12\%more biased.</description><author>Gili Lior, Gabriel Stanovsky</author><pubDate>Wed, 24 May 2023 18:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15389v1</guid></item><item><title>Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering</title><link>http://arxiv.org/abs/2305.15387v1</link><description>The integration of multi-document pre-training objectives into languagemodels has resulted in remarkable improvements in multi-document downstreamtasks. In this work, we propose extending this idea by pre-training a genericmulti-document model from a novel cross-document question answeringpre-training objective. To that end, given a set (or cluster) oftopically-related documents, we systematically generate semantically-orientedquestions from a salient sentence in one document and challenge the model,during pre-training, to answer these questions while "peeking" into othertopically-related documents. In a similar manner, the model is also challengedto recover the sentence from which the question was generated, again whileleveraging cross-document information. This novel multi-document QA formulationdirects the model to better recover cross-text informational relations, andintroduces a natural augmentation that artificially increases the pre-trainingdata. Further, unlike prior multi-document models that focus on eitherclassification or summarization tasks, our pre-training objective formulationenables the model to perform tasks that involve both short text generation(e.g., QA) and long text generation (e.g., summarization). Following thisscheme, we pre-train our model -- termed QAmden -- and evaluate its performanceacross several multi-document tasks, including multi-document QA,summarization, and query-focused summarization, yielding improvements of up to7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4.</description><author>Avi Caciularu, Matthew E. Peters, Jacob Goldberger, Ido Dagan, Arman Cohan</author><pubDate>Wed, 24 May 2023 18:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15387v1</guid></item><item><title>Assessor360: Multi-sequence Network for Blind Omnidirectional Image Quality Assessment</title><link>http://arxiv.org/abs/2305.10983v2</link><description>Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectivelyassess the human perceptual quality of omnidirectional images (ODIs) withoutrelying on pristine-quality image information. It is becoming more significantwith the increasing advancement of virtual reality (VR) technology. However,the quality assessment of ODIs is severely hampered by the fact that theexisting BOIQA pipeline lacks the modeling of the observer's browsing process.To tackle this issue, we propose a novel multi-sequence network for BOIQAcalled Assessor360, which is derived from the realistic multi-assessor ODIquality assessment procedure. Specifically, we propose a generalized RecursiveProbability Sampling (RPS) method for the BOIQA task, combining content anddetailed information to generate multiple pseudo viewport sequences from agiven starting point. Additionally, we design a Multi-scale Feature Aggregation(MFA) module with Distortion-aware Block (DAB) to fuse distorted and semanticfeatures of each viewport. We also devise TMM to learn the viewport transitionin the temporal domain. Extensive experimental results demonstrate thatAssessor360 outperforms state-of-the-art methods on multiple OIQA datasets.</description><author>Tianhe Wu, Shuwei Shi, Haoming Cai, Mingdeng Cao, Jing Xiao, Yinqiang Zheng, Yujiu Yang</author><pubDate>Wed, 24 May 2023 18:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10983v2</guid></item><item><title>Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR</title><link>http://arxiv.org/abs/2305.15386v1</link><description>Improving ASR systems is necessary to make new LLM-based use-cases accessibleto people across the globe. In this paper, we focus on Indian languages, andmake the case that diverse benchmarks are required to evaluate and improve ASRsystems for Indian languages. To address this, we collate Vistaar as a set of59 benchmarks across various language and domain combinations, on which weevaluate 3 publicly available ASR systems and 2 commercial systems. We alsotrain IndicWhisper models by fine-tuning the Whisper models on publiclyavailable training datasets across 12 Indian languages totalling to 10.7Khours. We show that IndicWhisper significantly improves on considered ASRsystems on the Vistaar benchmark. Indeed, IndicWhisper has the lowest WER in 39out of the 59 benchmarks, with an average reduction of 4.1 WER. We open-sourceall datasets, code and models.</description><author>Kaushal Santosh Bhogale, Sai Sundaresan, Abhigyan Raman, Tahir Javed, Mitesh M. Khapra, Pratyush Kumar</author><pubDate>Wed, 24 May 2023 18:46:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15386v1</guid></item><item><title>Behavior quantification as the missing link between fields: Tools for digital psychiatry and their role in the future of neurobiology</title><link>http://arxiv.org/abs/2305.15385v1</link><description>The great behavioral heterogeneity observed between individuals with the samepsychiatric disorder and even within one individual over time complicates bothclinical practice and biomedical research. However, modern technologies are anexciting opportunity to improve behavioral characterization. Existingpsychiatry methods that are qualitative or unscalable, such as patient surveysor clinical interviews, can now be collected at a greater capacity and analyzedto produce new quantitative measures. Furthermore, recent capabilities forcontinuous collection of passive sensor streams, such as phone GPS orsmartwatch accelerometer, open avenues of novel questioning that werepreviously entirely unrealistic. Their temporally dense nature enables acohesive study of real-time neural and behavioral signals. To develop comprehensive neurobiological models of psychiatric disease, itwill be critical to first develop strong methods for behavioral quantification.There is huge potential in what can theoretically be captured by currenttechnologies, but this in itself presents a large computational challenge --one that will necessitate new data processing tools, new machine learningtechniques, and ultimately a shift in how interdisciplinary work is conducted.In my thesis, I detail research projects that take different perspectives ondigital psychiatry, subsequently tying ideas together with a concludingdiscussion on the future of the field. I also provide software infrastructurewhere relevant, with extensive documentation. Major contributions include scientific arguments and proof of concept resultsfor daily free-form audio journals as an underappreciated psychiatry researchdatatype, as well as novel stability theorems and pilot empirical success for aproposed multi-area recurrent neural network architecture.</description><author>Michaela Ennis</author><pubDate>Wed, 24 May 2023 18:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15385v1</guid></item><item><title>Augmentation-Aware Self-Supervision for Data-Efficient GAN Training</title><link>http://arxiv.org/abs/2205.15677v2</link><description>Training generative adversarial networks (GANs) with limited data ischallenging because discriminator is prone to overfitting. Previously proposeddifferentiable augmentation demonstrates improved data efficiency of trainingGANs. However, the augmentation implicitly introduces undesired invariance toaugmentation for the discriminator since it ignores the change of semantics inthe label space caused by data transformation, which may limit therepresentation learning ability of the discriminator and ultimately affect thegenerative modeling performance of the generator. To mitigate the negativeimpact of invariance while inheriting the benefits of data augmentation, wepropose a novel augmentation-aware self-supervised discriminator that predictsthe augmentation parameter of the augmented data. Particularly, the predictiontargets of real data and generated data are required to be distinguished sincethey are different during training. We further encourage the generator toadversarially learn from the self-supervised discriminator by generatingaugmentation-predictable real but not fake data. This formulation connects thelearning objective of the generator and the arithmetic$-$harmonic meandivergence under certain assumptions. We compare our method withstate-of-the-art (SOTA) methods using the class-conditional BigGAN andunconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100,FFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstratesignificant improvements of our method over SOTA methods in trainingdata-efficient GANs.</description><author>Liang Hou, Qi Cao, Yige Yuan, Songtao Zhao, Chongyang Ma, Siyuan Pan, Pengfei Wan, Zhongyuan Wang, Huawei Shen, Xueqi Cheng</author><pubDate>Wed, 24 May 2023 18:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.15677v2</guid></item><item><title>Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference</title><link>http://arxiv.org/abs/2305.13484v2</link><description>In the rapidly evolving field of deep learning, the performance of modelinference has become a pivotal aspect as models become more complex and aredeployed in diverse applications. Among these, autoregressive models stand outdue to their state-of-the-art performance in numerous generative tasks. Thesemodels, by design, harness a temporal dependency structure, where the currenttoken's probability distribution is conditioned on preceding tokens. Thisinherently sequential characteristic, however, adheres to the Markov Chainassumption and lacks temporal parallelism, which poses unique challenges.Particularly in industrial contexts where inference requests, following aPoisson time distribution, necessitate diverse response lengths, this absenceof parallelism is more profound. Existing solutions, such as dynamic batchingand concurrent model instances, nevertheless, come with severe overheads and alack of flexibility, these coarse-grained methods fall short of achievingoptimal latency and throughput. To address these shortcomings, we proposeFlavor -- a temporal fusion framework for efficient inference in autoregressivemodels, eliminating the need for heuristic settings and applies to a wide rangeof inference scenarios. By providing more fine-grained parallelism on thetemporality of requests and employing an efficient memory shuffle algorithm,Flover achieves up to 11x faster inference on GPT models compared to thecutting-edge solutions provided by NVIDIA Triton FasterTransformer. Crucially,by leveraging the advanced tensor parallel technique, Flover proves efficaciousacross diverse computational landscapes, from single-GPU setups to multi-nodescenarios, thereby offering robust performance optimization that transcendshardware boundaries.</description><author>Jinghan Yao, Nawras Alnaasan, Tian Chen, Aamir Shafi, Hari Subramoni, Dhabaleswar K., Panda</author><pubDate>Wed, 24 May 2023 18:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13484v2</guid></item><item><title>Follower Agnostic Methods for Stackelberg Games</title><link>http://arxiv.org/abs/2302.01421v2</link><description>We propose an algorithm to solve a class of Stackelberg games (possibly withmultiple followers) in a follower agnostic manner. Particularly, unlike othercontemporary works, our algorithm does not require the use of an oracleestimator for the gradient of the leader's objective or knowledge about thefollower's utility function or strategy space. Instead, we design two-loopalgorithm where the leader updates its strategies using specially constructedgradient estimator obtained by probing followers with specially designedstrategies. Upon receiving the followers engage in an adaptation rule such thatthe joint strategy of followers converges near equilibrium which is the onlyinformation observed by leader to construct the aforementioned gradientestimator. We provide non-asymptotic convergence rates to stationary points ofthe leader's objective in the absence of convexity of the closed-loop functionand further show asymptotic convergence to a local minima of the leader'sobjective.</description><author>Chinmay Maheshwari, S. Shankar Sasty, Lillian Ratliff, Eric Mazumdar</author><pubDate>Wed, 24 May 2023 18:43:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01421v2</guid></item><item><title>On the Minimax Regret for Online Learning with Feedback Graphs</title><link>http://arxiv.org/abs/2305.15383v1</link><description>In this work, we improve on the upper and lower bounds for the regret ofonline learning with strongly observable undirected feedback graphs. The bestknown upper bound for this problem is $\mathcal{O}\bigl(\sqrt{\alpha T\lnK}\bigr)$, where $K$ is the number of actions, $\alpha$ is the independencenumber of the graph, and $T$ is the time horizon. The $\sqrt{\ln K}$ factor isknown to be necessary when $\alpha = 1$ (the experts case). On the other hand,when $\alpha = K$ (the bandits case), the minimax rate is known to be$\Theta\bigl(\sqrt{KT}\bigr)$, and a lower bound $\Omega\bigl(\sqrt{\alphaT}\bigr)$ is known to hold for any $\alpha$. Our improved upper bound$\mathcal{O}\bigl(\sqrt{\alpha T(1+\ln(K/\alpha))}\bigr)$ holds for any$\alpha$ and matches the lower bounds for bandits and experts, whileinterpolating intermediate cases. To prove this result, we use FTRL with$q$-Tsallis entropy for a carefully chosen value of $q \in [1/2, 1)$ thatvaries with $\alpha$. The analysis of this algorithm requires a new bound onthe variance term in the regret. We also show how to extend our techniques totime-varying graphs, without requiring prior knowledge of their independencenumbers. Our upper bound is complemented by an improved$\Omega\bigl(\sqrt{\alpha T(\ln K)/(\ln\alpha)}\bigr)$ lower bound for all$\alpha &gt; 1$, whose analysis relies on a novel reduction to multitask learning.This shows that a logarithmic factor is necessary as soon as $\alpha &lt; K$.</description><author>Khaled Eldowa, Emmanuel Esposito, Tommaso Cesari, Nicolò Cesa-Bianchi</author><pubDate>Wed, 24 May 2023 18:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15383v1</guid></item><item><title>Theorem Proving in Dependently-Typed Higher-Order Logic -- Extended Preprint</title><link>http://arxiv.org/abs/2305.15382v1</link><description>Higher-order logic HOL offers a very simple syntax and semantics forrepresenting and reasoning about typed data structures. But its type systemlacks advanced features where types may depend on terms. Dependent type theoryoffers such a rich type system, but has rather substantial conceptualdifferences to HOL, as well as comparatively poor proof automation support. Weintroduce a dependently-typed extension DHOL of HOL that retains the style andconceptual framework of HOL. Moreover, we build a translation from DHOL to HOLand implement it as a preprocessor to a HOL theorem prover, thereby obtaining atheorem prover for DHOL.</description><author>Colin Rothgang, Florian Rabe, Christoph Benzmüller</author><pubDate>Wed, 24 May 2023 18:40:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15382v1</guid></item><item><title>Sentiment Analysis Using Aligned Word Embeddings for Uralic Languages</title><link>http://arxiv.org/abs/2305.15380v1</link><description>In this paper, we present an approach for translating word embeddings from amajority language into 4 minority languages: Erzya, Moksha, Udmurt andKomi-Zyrian. Furthermore, we align these word embeddings and present a novelneural network model that is trained on English data to conduct sentimentanalysis and then applied on endangered language data through the aligned wordembeddings. To test our model, we annotated a small sentiment analysis corpusfor the 4 endangered languages and Finnish. Our method reached at least 56\%accuracy for each endangered language. The models and the sentiment corpus willbe released together with this paper. Our research shows that state-of-the-artneural models can be used with endangered languages with the only requirementbeing a dictionary between the endangered language and a majority language.</description><author>Khalid Alnajjar, Mika Hämäläinen, Jack Rueter</author><pubDate>Wed, 24 May 2023 18:40:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15380v1</guid></item><item><title>Uncovering and Quantifying Social Biases in Code Generation</title><link>http://arxiv.org/abs/2305.15377v1</link><description>With the popularity of automatic code generation tools, such as Copilot, thestudy of the potential hazards of these tools is gaining importance. In thiswork, we explore the social bias problem in pre-trained code generation models.We propose a new paradigm to construct code prompts and successfully uncoversocial biases in code generation models. To quantify the severity of socialbiases in generated code, we develop a dataset along with three metrics toevaluate the overall social bias and fine-grained unfairness across differentdemographics. Experimental results on three pre-trained code generation models(Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases.Moreover, we conduct analysis to provide useful insights for further choice ofcode generation models with low social bias. (This work contains examples thatpotentially implicate stereotypes, associations, and other harms that could beoffensive to individuals in certain social groups.)</description><author>Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu Chen, Tsung-Yi Ho</author><pubDate>Wed, 24 May 2023 18:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15377v1</guid></item><item><title>ASPER: Answer Set Programming Enhanced Neural Network Models for Joint Entity-Relation Extraction</title><link>http://arxiv.org/abs/2305.15374v1</link><description>A plethora of approaches have been proposed for joint entity-relation (ER)extraction. Most of these methods largely depend on a large amount of manuallyannotated training data. However, manual data annotation is time consuming,labor intensive, and error prone. Human beings learn using both data (throughinduction) and knowledge (through deduction). Answer Set Programming (ASP) hasbeen a widely utilized approach for knowledge representation and reasoning thatis elaboration tolerant and adept at reasoning with incomplete information.This paper proposes a new approach, ASP-enhanced Entity-Relation extraction(ASPER), to jointly recognize entities and relations by learning from both dataand domain knowledge. In particular, ASPER takes advantage of the factualknowledge (represented as facts in ASP) and derived knowledge (represented asrules in ASP) in the learning process of neural network models. We haveconducted experiments on two real datasets and compare our method with threebaselines. The results show that our ASPER model consistently outperforms thebaselines.</description><author>Trung Hoang Le, Huiping Cao, Tran Cao Son</author><pubDate>Wed, 24 May 2023 18:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15374v1</guid></item><item><title>Efficient Transformers with Dynamic Token Pooling</title><link>http://arxiv.org/abs/2211.09761v2</link><description>Transformers achieve unrivalled performance in modelling language, but remaininefficient in terms of memory and time complexity. A possible remedy is toreduce the sequence length in the intermediate layers by pooling fixed-lengthsegments of tokens. Nevertheless, natural units of meaning, such as words orphrases, display varying sizes. To address this mismatch, we equip languagemodels with a dynamic-pooling mechanism, which predicts segment boundaries inan autoregressive fashion. We compare several methods to infer boundaries,including end-to-end learning through stochastic re-parameterisation,supervised learning (based on segmentations from subword tokenizers or spikesin conditional entropy), as well as linguistically motivated boundaries. Weperform character-level evaluation on texts from multiple datasets andmorphologically diverse languages. The results demonstrate that dynamicpooling, which jointly segments and models language, is both faster and moreaccurate than vanilla Transformers and fixed-length pooling within the samecomputational budget.</description><author>Piotr Nawrot, Jan Chorowski, Adrian Łańcucki, Edoardo M. Ponti</author><pubDate>Wed, 24 May 2023 18:32:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09761v2</guid></item><item><title>Cost-aware learning of relevant contextual variables within Bayesian optimization</title><link>http://arxiv.org/abs/2305.14120v2</link><description>Contextual Bayesian Optimization (CBO) is a powerful framework for optimizingblack-box, expensive-to-evaluate functions with respect to design variables,while simultaneously efficiently integrating relevant contextual informationregarding the environment, such as experimental conditions. However, in manypractical scenarios, the relevance of contextual variables is not necessarilyknown beforehand. Moreover, the contextual variables can sometimes be optimizedthemselves, a setting that current CBO algorithms do not take into account.Optimizing contextual variables may be costly, which raises the question ofdetermining a minimal relevant subset. In this paper, we frame this problem asa cost-aware model selection BO task and address it using a novel method,Sensitivity-Analysis-Driven Contextual BO (SADCBO). We learn the relevance ofcontext variables by sensitivity analysis of the posterior surrogate model atspecific input points, whilst minimizing the cost of optimization by leveragingrecent developments on early stopping for BO. We empirically evaluate ourproposed SADCBO against alternatives on synthetic experiments together withextensive ablation studies, and demonstrate a consistent improvement acrossexamples.</description><author>Julien Martinelli, Ayush Bharti, S. T. John, Armi Tiihonen, Sabina Sloman, Louis Filstroff, Samuel Kaski</author><pubDate>Wed, 24 May 2023 18:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14120v2</guid></item><item><title>What can generic neural networks learn from a child's visual experience?</title><link>http://arxiv.org/abs/2305.15372v1</link><description>Young children develop sophisticated internal models of the world based ontheir egocentric visual experience. How much of this is driven by innateconstraints and how much is driven by their experience? To investigate thesequestions, we train state-of-the-art neural networks on a realistic proxy of achild's visual experience without any explicit supervision or domain-specificinductive biases. Specifically, we train both embedding models and generativemodels on 200 hours of headcam video from a single child collected over twoyears. We train a total of 72 different models, exploring a range of modelarchitectures and self-supervised learning algorithms, and comprehensivelyevaluate their performance in downstream tasks. The best embedding modelsperform at 70% of a highly performant ImageNet-trained model on average. Theyalso learn broad semantic categories without any labeled examples and learn tolocalize semantic categories in an image without any location supervision.However, these models are less object-centric and more background-sensitivethan comparable ImageNet-trained models. Generative models trained with thesame data successfully extrapolate simple properties of partially maskedobjects, such as their texture, color, orientation, and rough outline, butstruggle with finer object details. We replicate our experiments with two otherchildren and find very similar results. Broadly useful high-level visualrepresentations are thus robustly learnable from a representative sample of achild's visual experience without strong inductive biases.</description><author>A. Emin Orhan, Brenden M. Lake</author><pubDate>Wed, 24 May 2023 18:26:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15372v1</guid></item><item><title>Stochastic Unrolled Federated Learning</title><link>http://arxiv.org/abs/2305.15371v1</link><description>Algorithm unrolling has emerged as a learning-based optimization paradigmthat unfolds truncated iterative algorithms in trainable neural-networkoptimizers. We introduce Stochastic UnRolled Federated learning (SURF), amethod that expands algorithm unrolling to a federated learning scenario. Ourproposed method tackles two challenges of this expansion, namely the need tofeed whole datasets to the unrolled optimizers to find a descent direction andthe decentralized nature of federated learning. We circumvent the formerchallenge by feeding stochastic mini-batches to each unrolled layer andimposing descent constraints to mitigate the randomness induced by usingmini-batches. We address the latter challenge by unfolding the distributedgradient descent (DGD) algorithm in a graph neural network (GNN)-based unrolledarchitecture, which preserves the decentralized nature of training in federatedlearning. We theoretically prove that our proposed unrolled optimizer convergesto a near-optimal region infinitely often. Through extensive numericalexperiments, we also demonstrate the effectiveness of the proposed framework incollaborative training of image classifiers.</description><author>Samar Hadou, Navid NaderiAlizadeh, Alejandro Ribeiro</author><pubDate>Wed, 24 May 2023 18:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15371v1</guid></item><item><title>IdEALS: Idiomatic Expressions for Advancement of Language Skills</title><link>http://arxiv.org/abs/2305.13637v2</link><description>Although significant progress has been made in developing methods forGrammatical Error Correction (GEC), addressing word choice improvements hasbeen notably lacking and enhancing sentence expressivity by replacing phraseswith advanced expressions is an understudied aspect. In this paper, we focus onthis area and present our investigation into the task of incorporating theusage of idiomatic expressions in student writing. To facilitate our study, wecurate extensive training sets and expert-annotated testing sets usingreal-world data and evaluate various approaches and compare their performanceagainst human experts.</description><author>Narutatsu Ri, Bill Sun, Sam Davidson, Zhou Yu</author><pubDate>Wed, 24 May 2023 18:25:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13637v2</guid></item><item><title>SAMScore: A Semantic Structural Similarity Metric for Image Translation Evaluation</title><link>http://arxiv.org/abs/2305.15367v1</link><description>Image translation has wide applications, such as style transfer and modalityconversion, usually aiming to generate images having both high degrees ofrealism and faithfulness. These problems remain difficult, especially when itis important to preserve semantic structures. Traditional image-levelsimilarity metrics are of limited use, since the semantics of an image arehigh-level, and not strongly governed by pixel-wise faithfulness to an originalimage. Towards filling this gap, we introduce SAMScore, a generic semanticstructural similarity metric for evaluating the faithfulness of imagetranslation models. SAMScore is based on the recent high-performance SegmentAnything Model (SAM), which can perform semantic similarity comparisons withstandout accuracy. We applied SAMScore on 19 image translation tasks, and foundthat it is able to outperform all other competitive metrics on all of thetasks. We envision that SAMScore will prove to be a valuable tool that willhelp to drive the vibrant field of image translation, by allowing for moreprecise evaluations of new and evolving translation models. The code isavailable at https://github.com/Kent0n-Li/SAMScore.</description><author>Yunxiang Li, Meixu Chen, Wenxuan Yang, Kai Wang, Jun Ma, Alan C. Bovik, You Zhang</author><pubDate>Wed, 24 May 2023 18:22:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15367v1</guid></item><item><title>Boundary Attention Mapping (BAM): Fine-grained saliency maps for segmentation of Burn Injuries</title><link>http://arxiv.org/abs/2305.15365v1</link><description>Burn injuries can result from mechanisms such as thermal, chemical, andelectrical insults. A prompt and accurate assessment of burns is essential fordeciding definitive clinical treatments. Currently, the primary approach forburn assessments, via visual and tactile observations, is approximately 60%-80%accurate. The gold standard is biopsy and a close second would be non-invasivemethods like Laser Doppler Imaging (LDI) assessments, which have up to 97%accuracy in predicting burn severity and the required healing time. In thispaper, we introduce a machine learning pipeline for assessing burn severitiesand segmenting the regions of skin that are affected by burn. Segmenting 2Dcolour images of burns allows for the injured versus non-injured skin to bedelineated, clearly marking the extent and boundaries of the localizedburn/region-of-interest, even during remote monitoring of a burn patient. Wetrained a convolutional neural network (CNN) to classify four severities ofburns. We built a saliency mapping method, Boundary Attention Mapping (BAM),that utilises this trained CNN for the purpose of accurately localizing andsegmenting the burn regions from skin burn images. We demonstrated theeffectiveness of our proposed pipeline through extensive experiments andevaluations using two datasets; 1) A larger skin burn image dataset consistingof 1684 skin burn images of four burn severities, 2) An LDI dataset thatconsists of a total of 184 skin burn images with their associated LDI scans.The CNN trained using the first dataset achieved an average F1-Score of 78% andmicro/macro- average ROC of 85% in classifying the four burn severities.Moreover, a comparison between the BAM results and LDI results for measuringinjury boundary showed that the segmentations generated by our method achieved91.60% accuracy, 78.17% sensitivity, and 93.37% specificity.</description><author>Mahla Abdolahnejad, Justin Lee, Hannah Chan, Alex Morzycki, Olivier Ethier, Anthea Mo, Peter X. Liu, Joshua N. Wong, Colin Hong, Rakesh Joshi</author><pubDate>Wed, 24 May 2023 18:15:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15365v1</guid></item><item><title>Inverse Preference Learning: Preference-based RL without a Reward Function</title><link>http://arxiv.org/abs/2305.15363v1</link><description>Reward functions are difficult to design and often hard to align with humanintent. Preference-based Reinforcement Learning (RL) algorithms address theseproblems by learning reward functions from human feedback. However, themajority of preference-based RL methods na\"ively combine supervised rewardmodels with off-the-shelf RL algorithms. Contemporary approaches have sought toimprove performance and query complexity by using larger and more complexreward architectures such as transformers. Instead of using highly complexarchitectures, we develop a new and parameter-efficient algorithm, InversePreference Learning (IPL), specifically designed for learning from offlinepreference data. Our key insight is that for a fixed policy, the $Q$-functionencodes all information about the reward function, effectively making theminterchangeable. Using this insight, we completely eliminate the need for alearned reward function. Our resulting algorithm is simpler and moreparameter-efficient. Across a suite of continuous control and roboticsbenchmarks, IPL attains competitive performance compared to more complexapproaches that leverage transformer-based and non-Markovian reward functionswhile having fewer algorithmic hyperparameters and learned network parameters.Our code is publicly released.</description><author>Joey Hejna, Dorsa Sadigh</author><pubDate>Wed, 24 May 2023 18:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15363v1</guid></item><item><title>MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering</title><link>http://arxiv.org/abs/2305.12820v2</link><description>Recent advances in tabular question answering (QA) with large language modelsare constrained in their coverage and only answer questions over a singletable. However, real-world queries are complex in nature, often over multipletables in a relational database or web page. Single table questions do notinvolve common table operations such as set operations, Cartesian products(joins), or nested queries. Furthermore, multi-table operations often result ina tabular output, which necessitates table generation capabilities of tabularQA models. To fill this gap, we propose a new task of answering questions overmultiple tables. Our model, MultiTabQA, not only answers questions overmultiple tables, but also generalizes to generate tabular answers. To enableeffective training, we build a pre-training dataset comprising of 132,645 SQLqueries and tabular answers. Further, we evaluate the generated tables byintroducing table-specific metrics of varying strictness assessing variouslevels of granularity of the table structure. MultiTabQA outperformsstate-of-the-art single table QA models adapted to a multi-table QA setting byfinetuning on three datasets: Spider, Atis and GeoQuery.</description><author>Vaishali Pal, Andrew Yates, Evangelos Kanoulas, Maarten de Rijke</author><pubDate>Wed, 24 May 2023 18:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12820v2</guid></item><item><title>Context-Aware Transformer Pre-Training for Answer Sentence Selection</title><link>http://arxiv.org/abs/2305.15358v1</link><description>Answer Sentence Selection (AS2) is a core component for building an accurateQuestion Answering pipeline. AS2 models rank a set of candidate sentences basedon how likely they answer a given question. The state of the art in AS2exploits pre-trained transformers by transferring them on large annotateddatasets, while using local contextual information around the candidatesentence. In this paper, we propose three pre-training objectives designed tomimic the downstream fine-tuning task of contextual AS2. This allows forspecializing LMs when fine-tuning for contextual AS2. Our experiments on threepublic and two large-scale industrial datasets show that our pre-trainingapproaches (applied to RoBERTa and ELECTRA) can improve baseline contextual AS2accuracy by up to 8% on some datasets.</description><author>Luca Di Liello, Siddhant Garg, Alessandro Moschitti</author><pubDate>Wed, 24 May 2023 18:10:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15358v1</guid></item><item><title>Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution</title><link>http://arxiv.org/abs/2305.15357v1</link><description>Diffusion models, as a kind of powerful generative model, have givenimpressive results on image super-resolution (SR) tasks. However, due to therandomness introduced in the reverse process of diffusion models, theperformances of diffusion-based SR models are fluctuating at every time ofsampling, especially for samplers with few resampled steps. This inherentrandomness of diffusion models results in ineffectiveness and instability,making it challenging for users to guarantee the quality of SR results.However, our work takes this randomness as an opportunity: fully analyzing andleveraging it leads to the construction of an effective plug-and-play samplingmethod that owns the potential to benefit a series of diffusion-based SRmethods. More in detail, we propose to steadily sample high-quality SR imagesfrom pretrained diffusion-based SR models by solving diffusion ordinarydifferential equations (diffusion ODEs) with optimal boundary conditions (BCs)and analyze the characteristics between the choices of BCs and theircorresponding SR results. Our analysis shows the route to obtain anapproximately optimal BC via an efficient exploration in the whole space. Thequality of SR results sampled by the proposed method with fewer stepsoutperforms the quality of results sampled by current methods with randomnessfrom the same pretrained diffusion-based SR model, which means that oursampling method ``boosts'' current diffusion-based SR models without anyadditional training.</description><author>Yiyang Ma, Huan Yang, Wenhan Yang, Jianlong Fu, Jiaying Liu</author><pubDate>Wed, 24 May 2023 18:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15357v1</guid></item><item><title>Mitigating Biased Activation in Weakly-supervised Object Localization via Counterfactual Learning</title><link>http://arxiv.org/abs/2305.15354v1</link><description>In this paper, we focus on an under-explored issue of biased activation inprior weakly-supervised object localization methods based on Class ActivationMapping (CAM). We analyze the cause of this problem from a causal view andattribute it to the co-occurring background confounders. Following thisinsight, we propose a novel Counterfactual Co-occurring Learning (CCL) paradigmto synthesize the counterfactual representations via coupling constantforeground and unrealized backgrounds in order to cut off their co-occurringrelationship. Specifically, we design a new network structure calledCounterfactual-CAM, which embeds the counterfactual representation perturbationmechanism into the vanilla CAM-based model. This mechanism is responsible fordecoupling foreground as well as background and synthesizing the counterfactualrepresentations. By training the detection model with these synthesizedrepresentations, we compel the model to focus on the constant foregroundcontent while minimizing the influence of distracting co-occurring background.To our best knowledge, it is the first attempt in this direction. Extensiveexperiments on several benchmarks demonstrate that Counterfactual-CAMsuccessfully mitigates the biased activation problem, achieving improved objectlocalization accuracy.</description><author>Feifei Shao, Yawei Luo, Lei Chen, Ping Liu, Yi Yang, Jun Xiao</author><pubDate>Wed, 24 May 2023 18:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15354v1</guid></item><item><title>A Virtual Reality Tool for Representing, Visualizing and Updating Deep Learning Models</title><link>http://arxiv.org/abs/2305.15353v1</link><description>Deep learning is ubiquitous, but its lack of transparency limits its impacton several potential application areas. We demonstrate a virtual reality toolfor automating the process of assigning data inputs to different categories. Adataset is represented as a cloud of points in virtual space. The user exploresthe cloud through movement and uses hand gestures to categorise portions of thecloud. This triggers gradual movements in the cloud: points of the samecategory are attracted to each other, different groups are pushed apart, whilepoints are globally distributed in a way that utilises the entire space. Thespace, time, and forces observed in virtual reality can be mapped towell-defined machine learning concepts, namely the latent space, the trainingepochs and the backpropagation. Our tool illustrates how the inner workings ofdeep neural networks can be made tangible and transparent. We expect thisapproach to accelerate the autonomous development of deep learning applicationsby end users in novel areas.</description><author>Hannes Kath, Bengt Lüers, Thiago S. Gouvêa, Daniel Sonntag</author><pubDate>Wed, 24 May 2023 18:06:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15353v1</guid></item><item><title>Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages</title><link>http://arxiv.org/abs/2212.05409v3</link><description>Building Natural Language Understanding (NLU) capabilities for Indiclanguages, which have a collective speaker base of more than one billionspeakers is absolutely crucial. In this work, we aim to improve the NLUcapabilities of Indic languages by making contributions along 3 important axes(i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing onIndic languages. Specifically, we curate the largest monolingual corpora,IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a2.3x increase over prior work, while supporting 12 additional languages. Next,we create a human-supervised benchmark, IndicXTREME, consisting of nine diverseNLU tasks covering 20 languages. Across languages and tasks, IndicXTREMEcontains a total of 105 evaluation sets, of which 52 are new contributions tothe literature. To the best of our knowledge, this is the first effort towardscreating a standard benchmark for Indic languages that aims to test themultilingual zero-shot capabilities of pretrained language models. Finally, wetrain IndicBERT v2, a state-of-the-art model supporting all the languages.Averaged across languages and tasks, the model achieves an absolute improvementof 2 points over a strong baseline. The data and models are available athttps://github.com/AI4Bharat/IndicBERT.</description><author>Sumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, Pratyush Kumar</author><pubDate>Wed, 24 May 2023 18:05:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05409v3</guid></item><item><title>Optimal Rates for Bandit Nonstochastic Control</title><link>http://arxiv.org/abs/2305.15352v1</link><description>Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) controlare foundational and extensively researched problems in optimal control. Weinvestigate LQR and LQG problems with semi-adversarial perturbations andtime-varying adversarial bandit loss functions. The best-known sublinear regretalgorithm of~\cite{gradu2020non} has a $T^{\frac{3}{4}}$ time horizondependence, and its authors posed an open question about whether a tight rateof $\sqrt{T}$ could be achieved. We answer in the affirmative, giving analgorithm for bandit LQR and LQG which attains optimal regret (up tologarithmic factors) for both known and unknown systems. A central component ofour method is a new scheme for bandit convex optimization with memory, which isof independent interest.</description><author>Y. Jennifer Sun, Stephen Newman, Elad Hazan</author><pubDate>Wed, 24 May 2023 18:02:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15352v1</guid></item><item><title>Learning Meta Representations of One-shot Relations for Temporal Knowledge Graph Link Prediction</title><link>http://arxiv.org/abs/2205.10621v2</link><description>Few-shot relational learning for static knowledge graphs (KGs) has drawngreater interest in recent years, while few-shot learning for temporalknowledge graphs (TKGs) has hardly been studied. Compared to KGs, TKGs containrich temporal information, thus requiring temporal reasoning techniques formodeling. This poses a greater challenge in learning few-shot relations in thetemporal context. In this paper, we follow the previous work that focuses onfew-shot relational learning on static KGs and extend two fundamental TKGreasoning tasks, i.e., interpolated and extrapolated link prediction, to theone-shot setting. We propose four new large-scale benchmark datasets anddevelop a TKG reasoning model for learning one-shot relations in TKGs.Experimental results show that our model can achieve superior performance onall datasets in both TKG link prediction tasks.</description><author>Zifeng Ding, Bailan He, Yunpu Ma, Zhen Han, Volker Tresp</author><pubDate>Wed, 24 May 2023 18:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.10621v2</guid></item><item><title>Black-Box Variational Inference Converges</title><link>http://arxiv.org/abs/2305.15349v1</link><description>We provide the first convergence guarantee for full black-box variationalinference (BBVI), also known as Monte Carlo variational inference. Whilepreliminary investigations worked on simplified versions of BBVI (e.g., boundeddomain, bounded support, only optimizing for the scale, and such), our setupdoes not need any such algorithmic modifications. Our results hold forlog-smooth posterior densities with and without strong log-concavity and thelocation-scale variational family. Also, our analysis reveals that certainalgorithm design choices commonly employed in practice, particularly, nonlinearparameterizations of the scale of the variational approximation, can result insuboptimal convergence rates. Fortunately, running BBVI with proximalstochastic gradient descent fixes these limitations, and thus achieves thestrongest known convergence rate guarantees. We evaluate this theoreticalinsight by comparing proximal SGD against other standard implementations ofBBVI on large-scale Bayesian inference problems.</description><author>Kyurae Kim, Kaiwen Wu, Jisu Oh, Yian Ma, Jacob R. Gardner</author><pubDate>Wed, 24 May 2023 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15349v1</guid></item><item><title>READ: Recurrent Adaptation of Large Transformers</title><link>http://arxiv.org/abs/2305.15348v1</link><description>Fine-tuning large-scale Transformers has led to the explosion of many AIapplications across Natural Language Processing and Computer Vision tasks.However, fine-tuning all pre-trained model parameters becomes impractical asthe model size and number of tasks increase. Parameter-efficient transferlearning (PETL) methods aim to address these challenges. While effective inreducing the number of trainable parameters, PETL methods still requiresignificant energy and computational resources to fine-tune. In this paper, weintroduce \textbf{RE}current \textbf{AD}aption (READ) -- a lightweight andmemory-efficient fine-tuning method -- to overcome the limitations of thecurrent PETL approaches. Specifically, READ inserts a small RNN networkalongside the backbone model so that the model does not have to back-propagatethrough the large backbone network. Through comprehensive empirical evaluationof the GLUE benchmark, we demonstrate READ can achieve a $56\%$ reduction inthe training memory consumption and an $84\%$ reduction in the GPU energy usagewhile retraining high model quality compared to full-tuning. Additionally, themodel size of READ does not grow with the backbone model size, making it ahighly scalable solution for fine-tuning large Transformers.</description><author>Sid Wang, John Nguyen, Ke Li, Carole-Jean Wu</author><pubDate>Wed, 24 May 2023 17:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15348v1</guid></item><item><title>A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</title><link>http://arxiv.org/abs/2305.15347v1</link><description>Text-to-image diffusion models have made significant advances in generatingand editing high-quality images. As a result, numerous approaches have exploredthe ability of diffusion model features to understand and process single imagesfor downstream tasks, e.g., classification, semantic segmentation, andstylization. However, significantly less is known about what these featuresreveal across multiple, different images and objects. In this work, we exploitStable Diffusion (SD) features for semantic and dense correspondence anddiscover that with simple post-processing, SD features can performquantitatively similar to SOTA representations. Interestingly, the qualitativeanalysis reveals that SD features have very different properties compared toexisting representation learning features, such as the recently releasedDINOv2: while DINOv2 provides sparse but accurate matches, SD features providehigh-quality spatial information but sometimes inaccurate semantic matches. Wedemonstrate that a simple fusion of these two features works surprisingly well,and a zero-shot evaluation using nearest neighbors on these fused featuresprovides a significant performance gain over state-of-the-art methods onbenchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS. We also show thatthese correspondences can enable interesting applications such as instanceswapping in two images.</description><author>Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, Ming-Hsuan Yang</author><pubDate>Wed, 24 May 2023 17:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15347v1</guid></item><item><title>Embedding Contextual Information through Reward Shaping in Multi-Agent Learning: A Case Study from Google Football</title><link>http://arxiv.org/abs/2303.15471v2</link><description>Artificial Intelligence has been used to help human complete difficult tasksin complicated environments by providing optimized strategies fordecision-making or replacing the manual labour. In environments includingmultiple agents, such as football, the most common methods to train agents areImitation Learning and Multi-Agent Reinforcement Learning (MARL). However, theagents trained by Imitation Learning cannot outperform the expert demonstrator,which makes humans hardly get new insights from the learnt policy. Besides,MARL is prone to the credit assignment problem. In environments with sparsereward signal, this method can be inefficient. The objective of our research isto create a novel reward shaping method by embedding contextual information inreward function to solve the aforementioned challenges. We demonstrate this inthe Google Research Football (GRF) environment. We quantify the contextualinformation extracted from game state observation and use this quantificationtogether with original sparse reward to create the shaped reward. Theexperiment results in the GRF environment prove that our reward shaping methodis a useful addition to state-of-the-art MARL algorithms for training agents inenvironments with sparse reward signal.</description><author>Chaoyi Gu, Varuna De Silva, Corentin Artaud, Rafael Pina</author><pubDate>Wed, 24 May 2023 17:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15471v2</guid></item><item><title>Learning Answer Generation using Supervision from Automatic Question Answering Evaluators</title><link>http://arxiv.org/abs/2305.15344v1</link><description>Recent studies show that sentence-level extractive QA, i.e., based on AnswerSentence Selection (AS2), is outperformed by Generation-based QA (GenQA)models, which generate answers using the top-k answer sentences ranked by AS2models (a la retrieval-augmented generation style). In this paper, we propose anovel training paradigm for GenQA using supervision from automatic QAevaluation models (GAVA). Specifically, we propose three strategies to transferknowledge from these QA evaluation models to a GenQA model: (i) augmentingtraining data with answers generated by the GenQA model and labelled by GAVA(either statically, before training, or (ii) dynamically, at every trainingepoch); and (iii) using the GAVA score for weighting the generator loss duringthe learning of the GenQA model. We evaluate our proposed methods on twoacademic and one industrial dataset, obtaining a significant improvement inanswering accuracy over the previous state of the art.</description><author>Matteo Gabburo, Siddhant Garg, Rik Koncel-Kedziorski, Alessandro Moschitti</author><pubDate>Wed, 24 May 2023 17:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15344v1</guid></item><item><title>Is Your Model "MADD"? A Novel Metric to Evaluate Algorithmic Fairness for Predictive Student Models</title><link>http://arxiv.org/abs/2305.15342v1</link><description>Predictive student models are increasingly used in learning environments dueto their ability to enhance educational outcomes and support stakeholders inmaking informed decisions. However, predictive models can be biased and produceunfair outcomes, leading to potential discrimination against some students andpossible harmful long-term implications. This has prompted research on fairnessmetrics meant to capture and quantify such biases. Nonetheless, so far,existing fairness metrics used in education are predictiveperformance-oriented, focusing on assessing biased outcomes across groups ofstudents, without considering the behaviors of the models nor the severity ofthe biases in the outcomes. Therefore, we propose a novel metric, the ModelAbsolute Density Distance (MADD), to analyze models' discriminatory behaviorsindependently from their predictive performance. We also provide acomplementary visualization-based analysis to enable fine-grained humanassessment of how the models discriminate between groups of students. Weevaluate our approach on the common task of predicting student success inonline courses, using several common predictive classification models on anopen educational dataset. We also compare our metric to the only predictiveperformance-oriented fairness metric developed in education, ABROCA. Results onthis dataset show that: (1) fair predictive performance does not guarantee fairmodels' behaviors and thus fair outcomes, (2) there is no direct relationshipbetween data bias and predictive performance bias nor discriminatory behaviorsbias, and (3) trained on the same data, models exhibit different discriminatorybehaviors, according to different sensitive features too. We thus recommendusing the MADD on models that show satisfying predictive performance, to gain afiner-grained understanding on how they behave and to refine models selectionand their usage.</description><author>Mélina Verger, Sébastien Lallé, François Bouchet, Vanda Luengo</author><pubDate>Wed, 24 May 2023 17:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15342v1</guid></item><item><title>Bayesian calibration of differentiable agent-based models</title><link>http://arxiv.org/abs/2305.15340v1</link><description>Agent-based modelling (ABMing) is a powerful and intuitive approach tomodelling complex systems; however, the intractability of ABMs' likelihoodfunctions and the non-differentiability of the mathematical operationscomprising these models present a challenge to their use in the real world.These difficulties have in turn generated research on approximate Bayesianinference methods for ABMs and on constructing differentiable approximations toarbitrary ABMs, but little work has been directed towards designing approximateBayesian inference techniques for the specific case of differentiable ABMs. Inthis work, we aim to address this gap and discuss how generalised variationalinference procedures may be employed to provide misspecification-robustBayesian parameter inferences for differentiable ABMs. We demonstrate withexperiments on a differentiable ABM of the COVID-19 pandemic that our approachcan result in accurate inferences, and discuss avenues for future work.</description><author>Arnau Quera-Bofarull, Ayush Chopra, Anisoara Calinescu, Michael Wooldridge, Joel Dyer</author><pubDate>Wed, 24 May 2023 17:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15340v1</guid></item><item><title>Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing</title><link>http://arxiv.org/abs/2305.15338v1</link><description>In executable task-oriented semantic parsing, the system aims to translateusers' utterances in natural language to machine-interpretable programs (APIcalls) that can be executed according to pre-defined API specifications. Withthe popularity of Large Language Models (LLMs), in-context learning offers astrong baseline for such scenarios, especially in data-limited regimes.However, LLMs are known to hallucinate and therefore pose a formidablechallenge in constraining generated content. Thus, it remains uncertain if LLMscan effectively perform task-oriented utterance-to-API generation whererespecting API's structural and task-specific constraints is crucial. In this work, we seek to measure, analyze and mitigate such constraintsviolations. First, we identify the categories of various constraints inobtaining API-semantics from task-oriented utterances, and define fine-grainedmetrics that complement traditional ones. Second, we leverage these metrics toconduct a detailed error analysis of constraints violations seen instate-of-the-art LLMs, which motivates us to investigate two mitigationstrategies: Semantic-Retrieval of Demonstrations (SRD) and API-awareConstrained Decoding (API-CD). Our experiments show that these strategies areeffective at reducing constraints violations and improving the quality of thegenerated API calls, but require careful consideration given theirimplementation complexity and latency.</description><author>Shufan Wang, Sebastien Jean, Sailik Sengupta, James Gung, Nikolaos Pappas, Yi Zhang</author><pubDate>Wed, 24 May 2023 17:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15338v1</guid></item><item><title>A Deep Generative Model for Interactive Data Annotation through Direct Manipulation in Latent Space</title><link>http://arxiv.org/abs/2305.15337v1</link><description>The impact of machine learning (ML) in many fields of application isconstrained by lack of annotated data. Among existing tools for ML-assisteddata annotation, one little explored tool type relies on an analogy between thecoordinates of a graphical user interface and the latent space of a neuralnetwork for interaction through direct manipulation. In the present work, we 1)expand the paradigm by proposing two new analogies: time and force asreflecting iterations and gradients of network training; 2) propose a networkmodel for learning a compact graphical representation of the data that takesinto account both its internal structure and user provided annotations; and 3)investigate the impact of model hyperparameters on the learned graphicalrepresentations of the data, identifying candidate model variants for a futureuser study.</description><author>Hannes Kath, Thiago S. Gouvêa, Daniel Sonntag</author><pubDate>Wed, 24 May 2023 17:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15337v1</guid></item><item><title>Gorilla: Large Language Model Connected with Massive APIs</title><link>http://arxiv.org/abs/2305.15334v1</link><description>Large Language Models (LLMs) have seen an impressive wave of advancesrecently, with models now excelling in a variety of tasks, such as mathematicalreasoning and program synthesis. However, their potential to effectively usetools via API calls remains unfulfilled. This is a challenging task even fortoday's state-of-the-art LLMs such as GPT-4, largely due to their inability togenerate accurate input arguments and their tendency to hallucinate the wrongusage of an API call. We release Gorilla, a finetuned LLaMA-based model thatsurpasses the performance of GPT-4 on writing API calls. When combined with adocument retriever, Gorilla demonstrates a strong capability to adapt totest-time document changes, enabling flexible user updates or version changes.It also substantially mitigates the issue of hallucination, commonlyencountered when prompting LLMs directly. To evaluate the model's ability, weintroduce APIBench, a comprehensive dataset consisting of HuggingFace,TorchHub, and TensorHub APIs. The successful integration of the retrievalsystem with Gorilla demonstrates the potential for LLMs to use tools moreaccurately, keep up with frequently updated documentation, and consequentlyincrease the reliability and applicability of their outputs. Gorilla's code,model, data, and demo are available at https://gorilla.cs.berkeley.edu</description><author>Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez</author><pubDate>Wed, 24 May 2023 17:48:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15334v1</guid></item><item><title>Agnostic proper learning of monotone functions: beyond the black-box correction barrier</title><link>http://arxiv.org/abs/2304.02700v3</link><description>We give the first agnostic, efficient, proper learning algorithm for monotoneBoolean functions. Given $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$ uniformly randomexamples of an unknown function $f:\{\pm 1\}^n \rightarrow \{\pm 1\}$, ouralgorithm outputs a hypothesis $g:\{\pm 1\}^n \rightarrow \{\pm 1\}$ that ismonotone and $(\mathrm{opt} + \varepsilon)$-close to $f$, where $\mathrm{opt}$is the distance from $f$ to the closest monotone function. The running time ofthe algorithm (and consequently the size and evaluation time of the hypothesis)is also $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$, nearly matching the lower boundof Blais et al (RANDOM '15). We also give an algorithm for estimating up toadditive error $\varepsilon$ the distance of an unknown function $f$ tomonotone using a run-time of $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$. Previously,for both of these problems, sample-efficient algorithms were known, but thesealgorithms were not run-time efficient. Our work thus closes this gap in ourknowledge between the run-time and sample complexity. This work builds upon the improper learning algorithm of Bshouty and Tamon(JACM '96) and the proper semiagnostic learning algorithm of Lange, Rubinfeld,and Vasilyan (FOCS '22), which obtains a non-monotone Boolean-valuedhypothesis, then ``corrects'' it to monotone using query-efficient localcomputation algorithms on graphs. This black-box correction approach canachieve no error better than $2\mathrm{opt} + \varepsilon$information-theoretically; we bypass this barrier by a) augmenting the improper learner with a convex optimization step, and b) learning and correcting a real-valued function before rounding its valuesto Boolean. Our real-valued correction algorithm solves the ``poset sorting'' problem of[LRV22] for functions over general posets with non-Boolean labels.</description><author>Jane Lange, Arsen Vasilyan</author><pubDate>Wed, 24 May 2023 17:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02700v3</guid></item><item><title>Breaking the Curse of Quality Saturation with User-Centric Ranking</title><link>http://arxiv.org/abs/2305.15333v1</link><description>A key puzzle in search, ads, and recommendation is that the ranking model canonly utilize a small portion of the vastly available user interaction data. Asa result, increasing data volume, model size, or computation FLOPs will quicklysuffer from diminishing returns. We examined this problem and found that one ofthe root causes may lie in the so-called ``item-centric'' formulation, whichhas an unbounded vocabulary and thus uncontrolled model complexity. To mitigatequality saturation, we introduce an alternative formulation named``user-centric ranking'', which is based on a transposed view of the dyadicuser-item interaction data. We show that this formulation has a promisingscaling property, enabling us to train better-converged models on substantiallylarger data sets.</description><author>Zhuokai Zhao, Yang Yang, Wenyu Wang, Chihuang Liu, Yu Shi, Wenjie Hu, Haotian Zhang, Shuang Yang</author><pubDate>Wed, 24 May 2023 17:45:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15333v1</guid></item><item><title>A feature selection method based on Shapley values robust to concept shift in regression</title><link>http://arxiv.org/abs/2304.14774v2</link><description>Feature selection is one of the most relevant processes in any methodologyfor creating a statistical learning model. Generally, existing algorithmsestablish some criterion to select the most influential variables, discardingthose that do not contribute any relevant information to the model. Thismethodology makes sense in a classical static situation where the jointdistribution of the data does not vary over time. However, when dealing withreal data, it is common to encounter the problem of the dataset shift and,specifically, changes in the relationships between variables (concept shift).In this case, the influence of a variable cannot be the only indicator of itsquality as a regressor of the model, since the relationship learned in thetraning phase may not correspond to the current situation. Thus, we propose anew feature selection methodology for regression problems that takes this factinto account, using Shapley values to study the effect that each variable hason the predictions. Five examples are analysed: four correspond to typicalsituations where the method matches the state of the art and one examplerelated to electricity price forecasting where a concept shift phenomenon hasoccurred in the Iberian market. In this case the proposed algorithm improvesthe results significantly.</description><author>Carlos Sebastián, Carlos E. González-Guillén</author><pubDate>Wed, 24 May 2023 17:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14774v2</guid></item><item><title>No-Regret Online Prediction with Strategic Experts</title><link>http://arxiv.org/abs/2305.15331v1</link><description>We study a generalization of the online binary prediction with expert adviceframework where at each round, the learner is allowed to pick $m\geq 1$ expertsfrom a pool of $K$ experts and the overall utility is a modular or submodularfunction of the chosen experts. We focus on the setting in which experts actstrategically and aim to maximize their influence on the algorithm'spredictions by potentially misreporting their beliefs about the events. Amongothers, this setting finds applications in forecasting competitions where thelearner seeks not only to make predictions by aggregating different forecastersbut also to rank them according to their relative performance. Our goal is todesign algorithms that satisfy the following two requirements: 1)$\textit{Incentive-compatible}$: Incentivize the experts to report theirbeliefs truthfully, and 2) $\textit{No-regret}$: Achieve sublinear regret withrespect to the true beliefs of the best fixed set of $m$ experts in hindsight.Prior works have studied this framework when $m=1$ and providedincentive-compatible no-regret algorithms for the problem. We first show that asimple reduction of our problem to the $m=1$ setting is neither efficient noreffective. Then, we provide algorithms that utilize the specific structure ofthe utility functions to achieve the two desired goals.</description><author>Omid Sadeghi, Maryam Fazel</author><pubDate>Wed, 24 May 2023 17:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15331v1</guid></item><item><title>Visual Programming for Text-to-Image Generation and Evaluation</title><link>http://arxiv.org/abs/2305.15328v1</link><description>As large language models have demonstrated impressive performance in manydomains, recent works have adopted language models (LMs) as controllers ofvisual modules for vision-and-language tasks. While existing work focuses onequipping LMs with visual understanding, we propose two novelinterpretable/explainable visual programming frameworks for text-to-image (T2I)generation and evaluation. First, we introduce VPGen, an interpretablestep-by-step T2I generation framework that decomposes T2I generation into threesteps: object/count generation, layout generation, and image generation. Weemploy an LM to handle the first two steps (object/count generation and layoutgeneration), by finetuning it on text-layout pairs. Our step-by-step T2Igeneration framework provides stronger spatial control than end-to-end models,the dominant approach for this task. Furthermore, we leverage the worldknowledge of pretrained LMs, overcoming the limitation of previouslayout-guided T2I works that can only handle predefined object classes. Wedemonstrate that our VPGen has improved control in counts/spatialrelations/scales of objects than state-of-the-art T2I generation models.Second, we introduce VPEval, an interpretable and explainable evaluationframework for T2I generation based on visual programming. Unlike previous T2Ievaluations with a single scoring model that is accurate in some skills butunreliable in others, VPEval produces evaluation programs that invoke a set ofvisual modules that are experts in different skills, and also providesvisual+textual explanations of the evaluation results. Our analysis showsVPEval provides a more human-correlated evaluation for skill-specific andopen-ended prompts than widely used single model-based evaluation. We hope ourwork encourages future progress on interpretable/explainable generation andevaluation for T2I models. Website: https://vp-t2i.github.io</description><author>Jaemin Cho, Abhay Zala, Mohit Bansal</author><pubDate>Wed, 24 May 2023 17:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15328v1</guid></item><item><title>Statistical post-processing of visibility ensemble forecasts</title><link>http://arxiv.org/abs/2305.15325v1</link><description>To be able to produce accurate and reliable predictions of visibility hascrucial importance in aviation meteorology, as well as in water- and roadtransportation. Nowadays, several meteorological services provide ensembleforecasts of visibility; however, the skill, and reliability of visibilitypredictions are far reduced compared to other variables, such as temperature orwind speed. Hence, some form of calibration is strongly advised, which usuallymeans estimation of the predictive distribution of the weather quantity at handeither by parametric or non-parametric approaches, including also machinelearning-based techniques. As visibility observations - according to thesuggestion of the World Meteorological Organization - are usually reported indiscrete values, the predictive distribution for this particular variable is adiscrete probability law, hence calibration can be reduced to a classificationproblem. Based on visibility ensemble forecasts of the European Centre forMedium-Range Weather Forecasts covering two slightly overlapping domains inCentral and Western Europe and two different time periods, we investigate thepredictive performance of locally, semi-locally and regionally trainedproportional odds logistic regression (POLR) and multilayer perceptron (MLP)neural network classifiers. We show that while climatological forecastsoutperform the raw ensemble by a wide margin, post-processing results infurther substantial improvement in forecast skill and in general, POLR modelsare superior to their MLP counterparts.</description><author>Sándor Baran, Mária Lakatos</author><pubDate>Wed, 24 May 2023 17:41:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15325v1</guid></item><item><title>Model evaluation for extreme risks</title><link>http://arxiv.org/abs/2305.15324v1</link><description>Current approaches to building general-purpose AI systems tend to producesystems with both beneficial and harmful capabilities. Further progress in AIdevelopment could lead to capabilities that pose extreme risks, such asoffensive cyber capabilities or strong manipulation skills. We explain whymodel evaluation is critical for addressing extreme risks. Developers must beable to identify dangerous capabilities (through "dangerous capabilityevaluations") and the propensity of models to apply their capabilities for harm(through "alignment evaluations"). These evaluations will become critical forkeeping policymakers and other stakeholders informed, and for makingresponsible decisions about model training, deployment, and security.</description><author>Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul Christiano, Allan Dafoe</author><pubDate>Wed, 24 May 2023 17:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15324v1</guid></item><item><title>AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment</title><link>http://arxiv.org/abs/2305.04476v4</link><description>The speech-to-singing (STS) voice conversion task aims to generate singingsamples corresponding to speech recordings while facing a major challenge: thealignment between the target (singing) pitch contour and the source (speech)content is difficult to learn in a text-free situation. This paper proposesAlignSTS, an STS model based on explicit cross-modal alignment, which viewsspeech variance such as pitch and content as different modalities. Inspired bythe mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1)adopts a novel rhythm adaptor to predict the target rhythm representation tobridge the modality gap between content and pitch, where the rhythmrepresentation is computed in a simple yet effective way and is quantized intoa discrete space; and 2) uses the predicted rhythm representation to re-alignthe content based on cross-attention and conducts a cross-modal fusion forre-synthesize. Extensive experiments show that AlignSTS achieves superiorperformance in terms of both objective and subjective metrics. Audio samplesare available at https://alignsts.github.io.</description><author>Ruiqi Li, Rongjie Huang, Lichao Zhang, Jinglin Liu, Zhou Zhao</author><pubDate>Wed, 24 May 2023 17:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04476v4</guid></item><item><title>Towards Foundation Models for Relational Databases [Vision Paper]</title><link>http://arxiv.org/abs/2305.15321v1</link><description>Tabular representation learning has recently gained a lot of attention.However, existing approaches only learn a representation from a single table,and thus ignore the potential to learn from the full structure of relationaldatabases, including neighboring tables that can contain important informationfor a contextualized representation. Moreover, current models are significantlylimited in scale, which prevents that they learn from large databases. In thispaper, we thus introduce our vision of relational representation learning, thatcan not only learn from the full relational structure, but also can scale tolarger database sizes that are commonly found in real-world. Moreover, we alsodiscuss opportunities and challenges we see along the way to enable this visionand present initial very promising results. Overall, we argue that thisdirection can lead to foundation models for relational databases that are todayonly available for text and images.</description><author>Liane Vogel, Benjamin Hilprecht, Carsten Binnig</author><pubDate>Wed, 24 May 2023 17:37:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15321v1</guid></item><item><title>"What if?" in Probabilistic Logic Programming</title><link>http://arxiv.org/abs/2305.15318v1</link><description>A ProbLog program is a logic program with facts that only hold with aspecified probability. In this contribution we extend this ProbLog language bythe ability to answer "What if" queries. Intuitively, a ProbLog program definesa distribution by solving a system of equations in terms of mutuallyindependent predefined Boolean random variables. In the theory of causality,Judea Pearl proposes a counterfactual reasoning for such systems of equations.Based on Pearl's calculus, we provide a procedure for processing thesecounterfactual queries on ProbLog programs, together with a proof ofcorrectness and a full implementation. Using the latter, we provide insightsinto the influence of different parameters on the scalability of inference.Finally, we also show that our approach is consistent with CP-logic, i.e. withthe causal semantics for logic programs with annotated with disjunctions.</description><author>Rafael Kiesel, Kilian Rückschloß, Felix Weitkämper</author><pubDate>Wed, 24 May 2023 17:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15318v1</guid></item><item><title>Training on Thin Air: Improve Image Classification with Generated Data</title><link>http://arxiv.org/abs/2305.15316v1</link><description>Acquiring high-quality data for training discriminative models is a crucialyet challenging aspect of building effective predictive systems. In this paper,we present Diffusion Inversion, a simple yet effective method that leveragesthe pre-trained generative model, Stable Diffusion, to generate diverse,high-quality training data for image classification. Our approach captures theoriginal data distribution and ensures data coverage by inverting images to thelatent space of Stable Diffusion, and generates diverse novel training imagesby conditioning the generative model on noisy versions of these vectors. Weidentify three key components that allow our generated images to successfullysupplant the original dataset, leading to a 2-3x enhancement in samplecomplexity and a 6.5x decrease in sampling time. Moreover, our approachconsistently outperforms generic prompt-based steering methods and KNNretrieval baseline across a wide range of datasets. Additionally, wedemonstrate the compatibility of our approach with widely-used dataaugmentation techniques, as well as the reliability of the generated data insupporting various neural architectures and enhancing few-shot learning.</description><author>Yongchao Zhou, Hshmat Sahak, Jimmy Ba</author><pubDate>Wed, 24 May 2023 17:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15316v1</guid></item><item><title>Personalized Dictionary Learning for Heterogeneous Datasets</title><link>http://arxiv.org/abs/2305.15311v1</link><description>We introduce a relevant yet challenging problem named Personalized DictionaryLearning (PerDL), where the goal is to learn sparse linear representations fromheterogeneous datasets that share some commonality. In PerDL, we model eachdataset's shared and unique features as global and local dictionaries.Challenges for PerDL not only are inherited from classical dictionary learning(DL), but also arise due to the unknown nature of the shared and uniquefeatures. In this paper, we rigorously formulate this problem and provideconditions under which the global and local dictionaries can be provablydisentangled. Under these conditions, we provide a meta-algorithm calledPersonalized Matching and Averaging (PerMA) that can recover both global andlocal dictionaries from heterogeneous datasets. PerMA is highly efficient; itconverges to the ground truth at a linear rate under suitable conditions.Moreover, it automatically borrows strength from strong learners to improve theprediction of weak learners. As a general framework for extracting global andlocal dictionaries, we show the application of PerDL in different learningtasks, such as training with imbalanced datasets and video surveillance.</description><author>Geyu Liang, Naichen Shi, Raed Al Kontar, Salar Fattahi</author><pubDate>Wed, 24 May 2023 17:31:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15311v1</guid></item><item><title>Multi-Modal Mutual Attention and Iterative Interaction for Referring Image Segmentation</title><link>http://arxiv.org/abs/2305.15302v1</link><description>We address the problem of referring image segmentation that aims to generatea mask for the object specified by a natural language expression. Many recentworks utilize Transformer to extract features for the target object byaggregating the attended visual regions. However, the generic attentionmechanism in Transformer only uses the language input for attention weightcalculation, which does not explicitly fuse language features in its output.Thus, its output feature is dominated by vision information, which limits themodel to comprehensively understand the multi-modal information, and bringsuncertainty for the subsequent mask decoder to extract the output mask. Toaddress this issue, we propose Multi-Modal Mutual Attention ($\mathrm{M^3Att}$)and Multi-Modal Mutual Decoder ($\mathrm{M^3Dec}$) that better fuse informationfrom the two input modalities. Based on {$\mathrm{M^3Dec}$}, we further proposeIterative Multi-modal Interaction ($\mathrm{IMI}$) to allow continuous andin-depth interactions between language and vision features. Furthermore, weintroduce Language Feature Reconstruction ($\mathrm{LFR}$) to prevent thelanguage information from being lost or distorted in the extracted feature.Extensive experiments show that our proposed approach significantly improvesthe baseline and outperforms state-of-the-art referring image segmentationmethods on RefCOCO series datasets consistently.</description><author>Chang Liu, Henghui Ding, Yulun Zhang, Xudong Jiang</author><pubDate>Wed, 24 May 2023 17:26:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15302v1</guid></item><item><title>Science in the Era of ChatGPT, Large Language Models and AI: Challenges for Research Ethics Review and How to Respond</title><link>http://arxiv.org/abs/2305.15299v1</link><description>Large language models of artificial intelligence (AI) such as ChatGPT findremarkable but controversial applicability in science and research. This paperreviews epistemological challenges, ethical and integrity risks in scienceconduct. This is with the aim to lay new timely foundations for a high-qualityresearch ethics review in the era of AI. The role of AI language models as aresearch instrument and subject is scrutinized along with ethical implicationsfor scientists, participants and reviewers. Ten recommendations shape aresponse for a more responsible research conduct with AI language models.</description><author>Evangelos Pournaras</author><pubDate>Wed, 24 May 2023 17:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15299v1</guid></item><item><title>MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation</title><link>http://arxiv.org/abs/2305.15296v1</link><description>The recent popularity of text-to-image diffusion models (DM) can largely beattributed to the intuitive interface they provide to users. The intendedgeneration can be expressed in natural language, with the model producingfaithful interpretations of text prompts. However, expressing complex ornuanced ideas in text alone can be difficult. To ease image generation, wepropose MultiFusion that allows one to express complex and nuanced conceptswith arbitrarily interleaved inputs of multiple modalities and languages.MutliFusion leverages pre-trained models and aligns them for integration into acohesive system, thereby avoiding the need for extensive training from scratch.Our experimental results demonstrate the efficient transfer of capabilitiesfrom individual modules to the downstream model. Specifically, the fusion ofall independent components allows the image generation module to utilizemultilingual, interleaved multimodal inputs despite being trained solely onmonomodal data in a single language.</description><author>Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Björn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Koen Oostermeijer, Andres Felipe Cruz-Salinas, Patrick Schramowski, Kristian Kersting, Samuel Weinbach</author><pubDate>Wed, 24 May 2023 17:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15296v1</guid></item><item><title>Highly over-parameterized classifiers generalize since bad solutions are rare</title><link>http://arxiv.org/abs/2211.03570v2</link><description>We study the generalization of over-parameterized classifiers where EmpiricalRisk Minimization (ERM) for learning leads to zero training error. In theseover-parameterized settings there are many global minima with zero trainingerror, some of which generalize better than others. We show that under certainconditions the fraction of "bad" global minima with a true error larger than{\epsilon} decays to zero exponentially fast with the number of training datan. The bound depends on the distribution of the true error over the set ofclassifier functions used for the given classification problem, and does notnecessarily depend on the size or complexity (e.g. the number of parameters) ofthe classifier function set. This might explain the unexpectedly goodgeneralization even of highly over-parameterized Neural Networks. We supportour mathematical framework with experiments on a synthetic data set and asubset of MNIST.</description><author>Julius Martinetz, Thomas Martinetz</author><pubDate>Wed, 24 May 2023 17:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03570v2</guid></item><item><title>Measuring The Impact Of Programming Language Distribution</title><link>http://arxiv.org/abs/2302.01973v3</link><description>Current benchmarks for evaluating neural code models focus on only a smallsubset of programming languages, excluding many popular languages such as Go orRust. To ameliorate this issue, we present the BabelCode framework forexecution-based evaluation of any benchmark in any language. BabelCode enablesnew investigations into the qualitative performance of models' memory, runtime,and individual test case results. Additionally, we present a new codetranslation dataset called Translating Python Programming Puzzles (TP3) fromthe Python Programming Puzzles (Schuster et al. 2021) benchmark that involvestranslating expert-level python functions to any language. With both BabelCodeand the TP3 benchmark, we investigate if balancing the distributions of 14languages in a training dataset improves a large language model's performanceon low-resource languages. Training a model on a balanced corpus results in, onaverage, 12.34% higher $pass@k$ across all tasks and languages compared to thebaseline. We find that this strategy achieves 66.48% better $pass@k$ onlow-resource languages at the cost of only a 12.94% decrease to high-resourcelanguages. In our three translation tasks, this strategy yields, on average,30.77% better low-resource $pass@k$ while having 19.58% worse high-resource$pass@k$.</description><author>Gabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui, Joshua Howland, Jonathan Malmaud, Jacob Austin, Rishabh Singh, Michele Catasta</author><pubDate>Wed, 24 May 2023 17:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01973v3</guid></item><item><title>ZITS++: Image Inpainting by Improving the Incremental Transformer on Structural Priors</title><link>http://arxiv.org/abs/2210.05950v3</link><description>Image inpainting involves filling missing areas of a corrupted image. Despiteimpressive results have been achieved recently, restoring images with bothvivid textures and reasonable structures remains a significant challenge.Previous methods have primarily addressed regular textures while disregardingholistic structures due to the limited receptive fields of Convolutional NeuralNetworks (CNNs). To this end, we study learning a Zero-initialized residualaddition based Incremental Transformer on Structural priors (ZITS++), animproved model upon our conference work, ZITS. Specifically, given one corruptimage, we present the Transformer Structure Restorer (TSR) module to restoreholistic structural priors at low image resolution, which are further upsampledby Simple Structure Upsampler (SSU) module to higher image resolution. Torecover image texture details, we use the Fourier CNN Texture Restoration (FTR)module, which is strengthened by Fourier and large-kernel attentionconvolutions. Furthermore, to enhance the FTR, the upsampled structural priorsfrom TSR are further processed by Structure Feature Encoder (SFE) and optimizedwith the Zero-initialized Residual Addition (ZeroRA) incrementally. Besides, anew masking positional encoding is proposed to encode the large irregularmasks. Compared with ZITS, ZITS++ improves the FTR's stability and inpaintingability with several techniques. More importantly, we comprehensively explorethe effects of various image priors for inpainting and investigate how toutilize them to address high-resolution image inpainting with extensiveexperiments. This investigation is orthogonal to most inpainting approaches andcan thus significantly benefit the community. Codes and models will be releasedin https://github.com/ewrfcas/ZITS-PlusPlus.</description><author>Chenjie Cao, Qiaole Dong, Yanwei Fu</author><pubDate>Wed, 24 May 2023 17:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05950v3</guid></item><item><title>Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy</title><link>http://arxiv.org/abs/2305.15294v1</link><description>Large language models are powerful text processors and reasoners, but arestill subject to limitations including outdated knowledge and hallucinations,which necessitates connecting them to the world. Retrieval-augmented largelanguage models have raised extensive attention for grounding model generationon external knowledge. However, retrievers struggle to capture relevance,especially for queries with complex information needs. Recent work has proposedto improve relevance modeling by having large language models actively involvedin retrieval, i.e., to improve retrieval with generation. In this paper, weshow that strong performance can be achieved by a method we call Iter-RetGen,which synergizes retrieval and generation in an iterative manner. A modeloutput shows what might be needed to finish a task, and thus provides aninformative context for retrieving more relevant knowledge which in turn helpsgenerate a better output in the next iteration. Compared with recent work whichinterleaves retrieval with generation when producing an output, Iter-RetGenprocesses all retrieved knowledge as a whole and largely preserves theflexibility in generation without structural constraints. We evaluateIter-RetGen on multi-hop question answering, fact verification, and commonsensereasoning, and show that it can flexibly leverage parametric knowledge andnon-parametric knowledge, and is superior to or competitive withstate-of-the-art retrieval-augmented baselines while causing fewer overheads ofretrieval and generation. We can further improve performance viageneration-augmented retrieval adaptation.</description><author>Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen</author><pubDate>Wed, 24 May 2023 17:17:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15294v1</guid></item><item><title>Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling</title><link>http://arxiv.org/abs/2212.02908v6</link><description>Autonomous cars are indispensable when humans go further down the hands-freeroute. Although existing literature highlights that the acceptance of theautonomous car will increase if it drives in a human-like manner, sparseresearch offers the naturalistic experience from a passenger's seat perspectiveto examine the humanness of current autonomous cars. The present study testedwhether the AI driver could create a human-like ride experience for passengersbased on 69 participants' feedback in a real-road scenario. We designed a rideexperience-based version of the non-verbal Turing test for automated driving.Participants rode in autonomous cars (driven by either human or AI drivers) asa passenger and judged whether the driver was human or AI. The AI driver failedto pass our test because passengers detected the AI driver above chance. Incontrast, when the human driver drove the car, the passengers' judgement wasaround chance. We further investigated how human passengers ascribe humannessin our test. Based on Lewin's field theory, we advanced a computational modelcombining signal detection theory with pre-trained language models to predictpassengers' humanness rating behaviour. We employed affective transitionbetween pre-study baseline emotions and corresponding post-stage emotions asthe signal strength of our model. Results showed that the passengers'ascription of humanness would increase with the greater affective transition.Our study suggested an important role of affective transition in passengers'ascription of humanness, which might become a future direction for autonomousdriving.</description><author>Zhaoning Li, Qiaoli Jiang, Zhengming Wu, Anqi Liu, Haiyan Wu, Miner Huang, Kai Huang, Yixuan Ku</author><pubDate>Wed, 24 May 2023 17:16:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02908v6</guid></item><item><title>Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective</title><link>http://arxiv.org/abs/2302.01735v4</link><description>For medical image segmentation, contrastive learning is the dominant practiceto improve the quality of visual representations by contrasting semanticallysimilar and dissimilar pairs of samples. This is enabled by the observationthat without accessing ground truth labels, negative examples with trulydissimilar anatomical features, if sampled, can significantly improve theperformance. In reality, however, these samples may come from similaranatomical regions and the models may struggle to distinguish the minoritytail-class samples, making the tail classes more prone to misclassification,both of which typically lead to model collapse. In this paper, we propose ARCO,a semi-supervised contrastive learning (CL) framework with stratified grouptheory for medical image segmentation. In particular, we first propose buildingARCO through the concept of variance-reduced estimation and show that certainvariance-reduction techniques are particularly beneficial in pixel/voxel-levelsegmentation tasks with extremely limited labels. Furthermore, we theoreticallyprove these sampling techniques are universal in variance reduction. Finally,we experimentally validate our approaches on eight benchmarks, i.e., five 2D/3Dmedical and three semantic segmentation datasets, with different labelsettings, and our methods consistently outperform state-of-the-artsemi-supervised methods. Additionally, we augment the CL frameworks with thesesampling techniques and demonstrate significant gains over previous methods. Webelieve our work is an important step towards semi-supervised medical imagesegmentation by quantifying the limitation of current self-supervisionobjectives for accomplishing such challenging safety-critical tasks.</description><author>Chenyu You, Weicheng Dai, Yifei Min, Fenglin Liu, David A. Clifton, S Kevin Zhou, Lawrence Hamilton Staib, James S Duncan</author><pubDate>Wed, 24 May 2023 17:11:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01735v4</guid></item><item><title>The Crucial Role of Normalization in Sharpness-Aware Minimization</title><link>http://arxiv.org/abs/2305.15287v1</link><description>Sharpness-Aware Minimization (SAM) is a recently proposed gradient-basedoptimizer (Foret et al., ICLR 2021) that greatly improves the predictionperformance of deep neural networks. Consequently, there has been a surge ofinterest in explaining its empirical success. We focus, in particular, onunderstanding the role played by normalization, a key component of the SAMupdates. We theoretically and empirically study the effect of normalization inSAM for both convex and non-convex functions, revealing two key roles played bynormalization: i) it helps in stabilizing the algorithm; and ii) it enables thealgorithm to drift along a continuum (manifold) of minima -- a propertyidentified by recent theoretical works that is the key to better performance.We further argue that these two properties of normalization make SAM robustagainst the choice of hyper-parameters, supporting the practicality of SAM. Ourconclusions are backed by various experiments.</description><author>Yan Dai, Kwangjun Ahn, Suvrit Sra</author><pubDate>Wed, 24 May 2023 17:09:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15287v1</guid></item><item><title>Replicable Reinforcement Learning</title><link>http://arxiv.org/abs/2305.15284v1</link><description>The replicability crisis in the social, behavioral, and data sciences has ledto the formulation of algorithm frameworks for replicability -- i.e., arequirement that an algorithm produce identical outputs (with high probability)when run on two different samples from the same underlying distribution. Whilestill in its infancy, provably replicable algorithms have been developed formany fundamental tasks in machine learning and statistics, includingstatistical query learning, the heavy hitters problem, and distributiontesting. In this work we initiate the study of replicable reinforcementlearning, providing a provably replicable algorithm for parallel valueiteration, and a provably replicable version of R-max in the episodic setting.These are the first formal replicability results for control problems, whichpresent different challenges for replication than batch learning settings.</description><author>Eric Eaton, Marcel Hussing, Michael Kearns, Jessica Sorrell</author><pubDate>Wed, 24 May 2023 17:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15284v1</guid></item><item><title>Learning Large-scale Neural Fields via Context Pruned Meta-Learning</title><link>http://arxiv.org/abs/2302.00617v2</link><description>We introduce an efficient optimization-based meta-learning technique forlarge-scale neural field training by realizing significant memory savingsthrough automated online context point selection. This is achieved by focusingeach learning step on the subset of data with the highest expected immediateimprovement in model quality, resulting in the almost instantaneous modeling ofglobal structure and subsequent refinement of high-frequency details. Wefurther improve the quality of our meta-learned initialization by introducing abootstrap correction resulting in the minimization of any error introduced byreduced context sets while simultaneously mitigating the well-known myopia ofoptimization-based meta-learning. Finally, we show how gradient re-scaling atmeta-test time allows the learning of extremely high-quality neural fields insignificantly shortened optimization procedures. Our framework ismodel-agnostic, intuitive, straightforward to implement, and shows significantreconstruction improvements for a wide range of signals. We provide anextensive empirical evaluation on nine datasets across multiple multiplemodalities, demonstrating state-of-the-art results while providing additionalinsight through careful analysis of the algorithmic components constituting ourmethod. Code is available at https://github.com/jihoontack/GradNCP</description><author>Jihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, Jonathan Richard Schwarz</author><pubDate>Wed, 24 May 2023 17:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00617v2</guid></item><item><title>High Speed Human Action Recognition using a Photonic Reservoir Computer</title><link>http://arxiv.org/abs/2305.15283v1</link><description>The recognition of human actions in videos is one of the most active researchfields in computer vision. The canonical approach consists in a more or lesscomplex preprocessing stages of the raw video data, followed by a relativelysimple classification algorithm. Here we address recognition of human actionsusing the reservoir computing algorithm, which allows us to focus on theclassifier stage. We introduce a new training method for the reservoircomputer, based on "Timesteps Of Interest", which combines in a simple wayshort and long time scales. We study the performance of this algorithm usingboth numerical simulations and a photonic implementation based on a singlenon-linear node and a delay line on the well known KTH dataset. We solve thetask with high accuracy and speed, to the point of allowing for processingmultiple video streams in real time. The present work is thus an important steptowards developing efficient dedicated hardware for video processing.</description><author>Enrico Picco, Piotr Antonik, Serge Massar</author><pubDate>Wed, 24 May 2023 17:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15283v1</guid></item><item><title>A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification</title><link>http://arxiv.org/abs/2305.15282v1</link><description>In recent years, large language models (LLMs) have achieved strongperformance on benchmark tasks, especially in zero or few-shot settings.However, these benchmarks often do not adequately address the challenges posedin the real-world, such as that of hierarchical classification. In order toaddress this challenge, we propose refactoring conventional tasks onhierarchical datasets into a more indicative long-tail prediction task. Weobserve LLMs are more prone to failure in these cases. To address theselimitations, we propose the use of entailment-contradiction prediction inconjunction with LLMs, which allows for strong performance in a strictzero-shot setting. Importantly, our method does not require any parameterupdates, a resource-intensive process and achieves strong performance acrossmultiple datasets.</description><author>Rohan Bhambhoria, Lei Chen, Xiaodan Zhu</author><pubDate>Wed, 24 May 2023 17:04:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15282v1</guid></item><item><title>CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language</title><link>http://arxiv.org/abs/2211.01427v4</link><description>Recent works have demonstrated that natural language can be used to generateand edit 3D shapes. However, these methods generate shapes with limitedfidelity and diversity. We introduce CLIP-Sculptor, a method to address theseconstraints by producing high-fidelity and diverse 3D shapes without the needfor (text, shape) pairs during training. CLIP-Sculptor achieves this in amulti-resolution approach that first generates in a low-dimensional latentspace and then upscales to a higher resolution for improved shape fidelity. Forimproved shape diversity, we use a discrete latent space which is modeled usinga transformer conditioned on CLIP's image-text embedding space. We also presenta novel variant of classifier-free guidance, which improves theaccuracy-diversity trade-off. Finally, we perform extensive experimentsdemonstrating that CLIP-Sculptor outperforms state-of-the-art baselines. Thecode is available at https://ivl.cs.brown.edu/#/projects/clip-sculptor.</description><author>Aditya Sanghi, Rao Fu, Vivian Liu, Karl Willis, Hooman Shayani, Amir Hosein Khasahmadi, Srinath Sridhar, Daniel Ritchie</author><pubDate>Wed, 24 May 2023 17:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01427v4</guid></item><item><title>Enhancing Cross-lingual Prompting with Dual Prompt Augmentation</title><link>http://arxiv.org/abs/2202.07255v2</link><description>Prompting shows promising results in few-shot scenarios. However, itsstrength for multilingual/cross-lingual problems has not been fully exploited.Zhao and Sch\"utze (2021) made initial explorations in this direction bypresenting that cross-lingual prompting outperforms cross-lingual finetuning.In this paper, we conduct an empirical exploration on the effect of eachcomponent in cross-lingual prompting and derive language-agnostic UniversalPrompting, which helps alleviate the discrepancies between source-languagetraining and target-language inference. Based on this, we propose DPA, a dualprompt augmentation framework, aiming at relieving the data scarcity issue infew-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54%with only 16 English training examples per class, significantly better than34.99% of finetuning. Our code is available athttps://github.com/DAMO-NLP-SG/DPA.</description><author>Meng Zhou, Xin Li, Yue Jiang, Lidong Bing</author><pubDate>Wed, 24 May 2023 17:03:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.07255v2</guid></item><item><title>Successor-Predecessor Intrinsic Exploration</title><link>http://arxiv.org/abs/2305.15277v1</link><description>Exploration is essential in reinforcement learning, particularly inenvironments where external rewards are sparse. Here we focus on explorationwith intrinsic rewards, where the agent transiently augments the externalrewards with self-generated intrinsic rewards. Although the study of intrinsicrewards has a long history, existing methods focus on composing the intrinsicreward based on measures of future prospects of states, ignoring theinformation contained in the retrospective structure of transition sequences.Here we argue that the agent can utilise retrospective information to generateexplorative behaviour with structure-awareness, facilitating efficientexploration based on global instead of local information. We proposeSuccessor-Predecessor Intrinsic Exploration (SPIE), an exploration algorithmbased on a novel intrinsic reward combining prospective and retrospectiveinformation. We show that SPIE yields more efficient and ethologicallyplausible exploratory behaviour in environments with sparse rewards andbottleneck states than competing methods. We also implement SPIE in deepreinforcement learning agents, and show that the resulting agent achievesstronger empirical performance than existing methods on sparse-reward Atarigames.</description><author>Changmin Yu, Neil Burgess, Maneesh Sahani, Sam Gershman</author><pubDate>Wed, 24 May 2023 17:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15277v1</guid></item><item><title>Robust Sparse Mean Estimation via Incremental Learning</title><link>http://arxiv.org/abs/2305.15276v1</link><description>In this paper, we study the problem of robust sparse mean estimation, wherethe goal is to estimate a $k$-sparse mean from a collection of partiallycorrupted samples drawn from a heavy-tailed distribution. Existing estimatorsface two critical challenges in this setting. First, they are limited by aconjectured computational-statistical tradeoff, implying that anycomputationally efficient algorithm needs $\tilde\Omega(k^2)$ samples, whileits statistically-optimal counterpart only requires $\tilde O(k)$ samples.Second, the existing estimators fall short of practical use as they scalepoorly with the ambient dimension. This paper presents a simple mean estimatorthat overcomes both challenges under moderate conditions: it runs innear-linear time and memory (both with respect to the ambient dimension) whilerequiring only $\tilde O(k)$ samples to recover the true mean. At the core ofour method lies an incremental learning phenomenon: we introduce a simplenonconvex framework that can incrementally learn the top-$k$ nonzero elementsof the mean while keeping the zero elements arbitrarily small. Unlike existingestimators, our method does not need any prior knowledge of the sparsity level$k$. We prove the optimality of our estimator by providing a matchinginformation-theoretic lower bound. Finally, we conduct a series of simulationsto corroborate our theoretical findings. Our code is available athttps://github.com/huihui0902/Robust_mean_estimation.</description><author>Jianhao Ma, Rui Ray Chen, Yinghui He, Salar Fattahi, Wei Hu</author><pubDate>Wed, 24 May 2023 17:02:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15276v1</guid></item><item><title>Self-Evolution Learning for Discriminative Language Model Pretraining</title><link>http://arxiv.org/abs/2305.15275v1</link><description>Masked language modeling, widely used in discriminative language model (e.g.,BERT) pretraining, commonly adopts a random masking strategy. However, randommasking does not consider the importance of the different words in the sentencemeaning, where some of them are more worthy to be predicted. Therefore, variousmasking strategies (e.g., entity-level masking) are proposed, but most of themrequire expensive prior knowledge and generally train from scratch withoutreusing existing model weights. In this paper, we present Self-Evolutionlearning (SE), a simple and effective token masking and learning method tofully and wisely exploit the knowledge from data. SE focuses on learning theinformative yet under-explored tokens and adaptively regularizes the trainingby introducing a novel Token-specific Label Smoothing approach. Experiments on10 tasks show that our SE brings consistent and significant improvements(+1.43~2.12 average scores) upon different PLMs. In-depth analyses demonstratethat SE improves linguistic knowledge learning and generalization.</description><author>Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao</author><pubDate>Wed, 24 May 2023 17:00:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15275v1</guid></item><item><title>Revisiting Token Dropping Strategy in Efficient BERT Pretraining</title><link>http://arxiv.org/abs/2305.15273v1</link><description>Token dropping is a recently-proposed strategy to speed up the pretraining ofmasked language models, such as BERT, by skipping the computation of a subsetof the input tokens at several middle layers. It can effectively reduce thetraining time without degrading much performance on downstream tasks. However,we empirically find that token dropping is prone to a semantic loss problem andfalls short in handling semantic-intense tasks. Motivated by this, we propose asimple yet effective semantic-consistent learning method (ScTD) to improve thetoken dropping. ScTD aims to encourage the model to learn how to preserve thesemantic information in the representation space. Extensive experiments on 12tasks show that, with the help of our ScTD, token dropping can achieveconsistent and significant performance gains across all task types and modelsizes. More encouragingly, ScTD saves up to 57% of pretraining time and bringsup to +1.56% average improvement over the vanilla token dropping.</description><author>Qihuang Zhong, Liang Ding, Juhua Liu, Xuebo Liu, Min Zhang, Bo Du, Dacheng Tao</author><pubDate>Wed, 24 May 2023 16:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15273v1</guid></item><item><title>ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers</title><link>http://arxiv.org/abs/2305.15272v1</link><description>Recently, plain vision Transformers (ViTs) have shown impressive performanceon various computer vision tasks, thanks to their strong modeling capacity andlarge-scale pretraining. However, they have not yet conquered the problem ofimage matting. We hypothesize that image matting could also be boosted by ViTsand present a new efficient and robust ViT-based matting system, namedViTMatte. Our method utilizes (i) a hybrid attention mechanism combined with aconvolution neck to help ViTs achieve an excellent performance-computationtrade-off in matting tasks. (ii) Additionally, we introduce the detail capturemodule, which just consists of simple lightweight convolutions to complementthe detailed information required by matting. To the best of our knowledge,ViTMatte is the first work to unleash the potential of ViT on image mattingwith concise adaptation. It inherits many superior properties from ViT tomatting, including various pretraining strategies, concise architecture design,and flexible inference strategies. We evaluate ViTMatte on Composition-1k andDistinctions-646, the most commonly used benchmark for image matting, ourmethod achieves state-of-the-art performance and outperforms prior mattingworks by a large margin.</description><author>Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang</author><pubDate>Wed, 24 May 2023 16:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15272v1</guid></item><item><title>Reversible Graph Neural Network-based Reaction Distribution Learning for Multiple Appropriate Facial Reactions Generation</title><link>http://arxiv.org/abs/2305.15270v1</link><description>Generating facial reactions in a human-human dyadic interaction is complexand highly dependent on the context since more than one facial reactions can beappropriate for the speaker's behaviour. This has challenged existing machinelearning (ML) methods, whose training strategies enforce models to reproduce aspecific (not multiple) facial reaction from each input speaker behaviour. Thispaper proposes the first multiple appropriate facial reaction generationframework that re-formulates the one-to-many mapping facial reaction generationproblem as a one-to-one mapping problem. This means that we approach thisproblem by considering the generation of a distribution of the listener'sappropriate facial reactions instead of multiple different appropriate facialreactions, i.e., 'many' appropriate facial reaction labels are summarised as'one' distribution label during training. Our model consists of a perceptualprocessor, a cognitive processor, and a motor processor. The motor processor isimplemented with a novel Reversible Multi-dimensional Edge Graph Neural Network(REGNN). This allows us to obtain a distribution of appropriate real facialreactions during the training process, enabling the cognitive processor to betrained to predict the appropriate facial reaction distribution. At theinference stage, the REGNN decodes an appropriate facial reaction by using thisdistribution as input. Experimental results demonstrate that our approachoutperforms existing models in generating more appropriate, realistic, andsynchronized facial reactions. The improved performance is largely attributedto the proposed appropriate facial reaction distribution learning strategy andthe use of a REGNN. The code is available athttps://github.com/TongXu-05/REGNN-Multiple-Appropriate-Facial-Reaction-Generation.</description><author>Tong Xu, Micol Spitale, Hao Tang, Lu Liu, Hatice Gunes, Siyang Song</author><pubDate>Wed, 24 May 2023 16:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15270v1</guid></item><item><title>PopulAtion Parameter Averaging (PAPA)</title><link>http://arxiv.org/abs/2304.03094v2</link><description>Ensemble methods combine the predictions of multiple models to improveperformance, but they require significantly higher computation costs atinference time. To avoid these costs, multiple neural networks can be combinedinto one by averaging their weights. However, this usually performssignificantly worse than ensembling. Weight averaging is only beneficial whendifferent enough to benefit from combining them, but similar enough to averagewell. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): amethod that combines the generality of ensembling with the efficiency of weightaveraging. PAPA leverages a population of diverse models (trained on differentdata orders, augmentations, and regularizations) while slowly pushing theweights of the networks toward the population average of the weights. PAPAreduces the performance gap between averaging and ensembling, increasing theaverage accuracy of a population of models by up to 0.8% on CIFAR-10, 1.9% onCIFAR-100, and 1.6% on ImageNet when compared to training independent(non-averaged) models.</description><author>Alexia Jolicoeur-Martineau, Emy Gervais, Kilian Fatras, Yan Zhang, Simon Lacoste-Julien</author><pubDate>Wed, 24 May 2023 16:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03094v2</guid></item><item><title>Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples</title><link>http://arxiv.org/abs/2305.15269v1</link><description>Given the intractably large size of the space of proofs, any model that iscapable of general deductive reasoning must generalize to proofs of greatercomplexity. Recent studies have shown that large language models (LLMs) possesssome abstract deductive reasoning ability given chain-of-thought prompts.However, they have primarily been tested on proofs using modus ponens or of aspecific size, and from the same distribution as the in-context examples. Tomeasure the general deductive reasoning ability of LLMs, we test on a broad setof deduction rules and measure their ability to generalize to more complexproofs from simpler demonstrations from multiple angles: depth-, width-, andcompositional generalization. To facilitate systematic exploration, weconstruct a new synthetic and programmable reasoning dataset that enablescontrol over deduction rules and proof complexity. Our experiments on four LLMsof various sizes and training objectives show that they are able to generalizeto longer and compositional proofs. However, they require explicitdemonstrations to produce hypothetical subproofs, specifically in proof bycases and proof by contradiction.</description><author>Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He</author><pubDate>Wed, 24 May 2023 16:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15269v1</guid></item><item><title>EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models</title><link>http://arxiv.org/abs/2305.15268v1</link><description>Events serve as fundamental units of occurrence within various contexts. Theprocessing of event semantics in textual information forms the basis ofnumerous natural language processing (NLP) applications. Recent studies havebegun leveraging large language models (LLMs) to address event semanticprocessing. However, the extent that LLMs can effectively tackle thesechallenges remains uncertain. Furthermore, the lack of a comprehensiveevaluation framework for event semantic processing poses a significantchallenge in evaluating these capabilities. In this paper, we propose anoverarching framework for event semantic processing, encompassingunderstanding, reasoning, and prediction, along with their fine-grainedaspects. To comprehensively evaluate the event semantic processing abilities ofmodels, we introduce a novel benchmark called EVEVAL. We collect 8 datasetsthat cover all aspects of event semantic processing. Extensive experiments areconducted on EVEVAL, leading to several noteworthy findings based on theobtained results.</description><author>Zhengwei Tao, Zhi Jin, Xiaoying Bai, Haiyan Zhao, Yanlin Feng, Jia Li, Wenpeng Hu</author><pubDate>Wed, 24 May 2023 16:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15268v1</guid></item><item><title>Training Energy-Based Normalizing Flow with Score-Matching Objectives</title><link>http://arxiv.org/abs/2305.15267v1</link><description>In this paper, we establish a connection between the parameterization offlow-based and energy-based generative models, and present a new flow-basedmodeling approach called energy-based normalizing flow (EBFlow). We demonstratethat by optimizing EBFlow with score-matching objectives, the computation ofJacobian determinants for linear transformations can be entirely bypassed. Thisfeature enables the use of arbitrary linear layers in the construction offlow-based models without increasing the computational time complexity of eachtraining iteration from $\mathcal{O}(D^2L)$ to $\mathcal{O}(D^3L)$ for an$L$-layered model that accepts $D$-dimensional inputs. This makes the trainingof EBFlow more efficient than the commonly-adopted maximum likelihood trainingmethod. In addition to the reduction in runtime, we enhance the trainingstability and empirical performance of EBFlow through a number of techniquesdeveloped based on our analysis on the score-matching methods. The experimentalresults demonstrate that our approach achieves a significant speedup comparedto maximum likelihood estimation, while outperforming prior efficient trainingtechniques with a noticeable margin in terms of negative log-likelihood (NLL).</description><author>Chen-Hao Chao, Wei-Fang Sun, Yen-Chang Hsu, Zsolt Kira, Chun-Yi Lee</author><pubDate>Wed, 24 May 2023 16:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15267v1</guid></item><item><title>Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model</title><link>http://arxiv.org/abs/2305.15265v1</link><description>With the rapid growth in model size, fine-tuning the large pre-trainedlanguage model has become increasingly difficult due to its extensive memoryusage. Previous works usually focus on reducing the number of trainableparameters in the network. While the model parameters do contribute to memoryusage, the primary memory bottleneck during training arises from storingfeature maps, also known as activations, as they are crucial for gradientcalculation. Notably, neural networks are usually trained using stochasticgradient descent. We argue that in stochastic optimization, models can handlenoisy gradients as long as the gradient estimator is unbiased with reasonablevariance. Following this motivation, we propose a new family of unbiasedestimators called WTA-CRS, for matrix production with reduced variance, whichonly requires storing the sub-sampled activations for calculating the gradient.Our work provides both theoretical and experimental evidence that, in thecontext of tuning transformers, our proposed estimators exhibit lower variancecompared to existing ones. By replacing the linear operation with ourapproximated one in transformers, we can achieve up to 2.7$\times$ peak memoryreduction with almost no accuracy drop and enables up to $6.4\times$ largerbatch size. Under the same hardware, WTA-CRS enables better down-streaming taskperformance by applying larger models and/or faster training speed with largerbatch sizes.</description><author>Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu, Xia Hu</author><pubDate>Wed, 24 May 2023 16:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15265v1</guid></item><item><title>Error Feedback Shines when Features are Rare</title><link>http://arxiv.org/abs/2305.15264v1</link><description>We provide the first proof that gradient descent $\left({\color{green}\sfGD}\right)$ with greedy sparsification $\left({\color{green}\sf TopK}\right)$and error feedback $\left({\color{green}\sf EF}\right)$ can obtain bettercommunication complexity than vanilla ${\color{green}\sf GD}$ when solving thedistributed optimization problem $\min_{x\in \mathbb{R}^d}{f(x)=\frac{1}{n}\sum_{i=1}^n f_i(x)}$, where $n$ = # of clients, $d$ = # offeatures, and $f_1,\dots,f_n$ are smooth nonconvex functions. Despite intensiveresearch since 2014 when ${\color{green}\sf EF}$ was first proposed by Seide etal., this problem remained open until now. We show that ${\color{green}\sf EF}$shines in the regime when features are rare, i.e., when each feature is presentin the data owned by a small number of clients only. To illustrate our mainresult, we show that in order to find a random vector $\hat{x}$ such that$\lVert {\nabla f(\hat{x})} \rVert^2 \leq \varepsilon$ in expectation,${\color{green}\sf GD}$ with the ${\color{green}\sf Top1}$ sparsifier and${\color{green}\sf EF}$ requires ${\cal O} \left(\left( L+{\color{blue}r}\sqrt{ \frac{{\color{red}c}}{n} \min \left( \frac{{\color{red}c}}{n} \max_iL_i^2, \frac{1}{n}\sum_{i=1}^n L_i^2 \right) }\right) \frac{1}{\varepsilon}\right)$ bits to be communicated by each worker to the server only, where $L$is the smoothness constant of $f$, $L_i$ is the smoothness constant of $f_i$,${\color{red}c}$ is the maximal number of clients owning any feature ($1\leq{\color{red}c} \leq n$), and ${\color{blue}r}$ is the maximal number offeatures owned by any client ($1\leq {\color{blue}r} \leq d$). Clearly, thecommunication complexity improves as ${\color{red}c}$ decreases (i.e., asfeatures become more rare), and can be much better than the ${\calO}({\color{blue}r} L \frac{1}{\varepsilon})$ communication complexity of${\color{green}\sf GD}$ in the same regime.</description><author>Peter Richtárik, Elnur Gasanov, Konstantin Burlachenko</author><pubDate>Wed, 24 May 2023 16:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15264v1</guid></item><item><title>Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration</title><link>http://arxiv.org/abs/2305.15262v1</link><description>We identify two crucial limitations in the evaluation of recentparallel-integrated method Parallel Context Windows (PCW), which extends themaximum context lengths of language models, e.g., 2048 for LLaMA, by harnessingwindow-wise attention and positional embedding techniques. We first show that asimple yet strong baseline, weighted sum ensemble, is missing for thein-context few-shot classification. Moreover, on more challengingChain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpecteddeterioration regarding question miscomprehension and false inference. Based onour findings, we suggest that the existing PCW design may not guaranteesufficient improvement and practicality in handling lengthy documents inreal-world applications. More community efforts on enabling language models'long context understanding ability should be paid.</description><author>Kejuan Yang, Xiao Liu, Kaiwen Men, Aohan Zeng, Yuxiao Dong, Jie Tang</author><pubDate>Wed, 24 May 2023 16:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15262v1</guid></item><item><title>Collaborative World Models: An Online-Offline Transfer RL Approach</title><link>http://arxiv.org/abs/2305.15260v1</link><description>Training visual reinforcement learning (RL) models in offline datasets ischallenging due to overfitting issues in representation learning andoverestimation problems in value function. In this paper, we propose a transferlearning method called Collaborative World Models (CoWorld) to improve theperformance of visual RL under offline conditions. The core idea is to use aneasy-to-interact, off-the-shelf simulator to train an auxiliary RL model as theonline ``test bed'' for the offline policy learned in the target domain, whichprovides a flexible constraint for the value function -- Intuitively, we wantto mitigate the overestimation problem of value functions outside the offlinedata distribution without impeding the exploration of actions with potentialadvantages. Specifically, CoWorld performs domain-collaborative representationlearning to bridge the gap between online and offline hidden statedistributions. Furthermore, it performs domain-collaborative behavior learningthat enables the source RL agent to provide target-aware value estimation,allowing for effective offline policy regularization. Experiments show thatCoWorld significantly outperforms existing methods in offline visual controltasks in DeepMind Control and Meta-World.</description><author>Qi Wang, Junming Yang, Yunbo Wang, Xin Jin, Wenjun Zeng, Xiaokang Yang</author><pubDate>Wed, 24 May 2023 16:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15260v1</guid></item><item><title>Discounting in Strategy Logic</title><link>http://arxiv.org/abs/2305.15256v1</link><description>Discounting is an important dimension in multi-agent systems as long as wewant to reason about strategies and time. It is a key aspect in economics as itcaptures the intuition that the far-away future is not as important as the nearfuture. Traditional verification techniques allow to check whether there is awinning strategy for a group of agents but they do not take into account thefact that satisfying a goal sooner is different from satisfying it after a longwait. In this paper, we augment Strategy Logic with future discounting over aset of discounted functions D, denoted SLdisc[D]. We consider "until" operatorswith discounting functions: the satisfaction value of a specification inSLdisc[D] is a value in [0, 1], where the longer it takes to fulfillrequirements, the smaller the satisfaction value is. We motivate our approachwith classical examples from Game Theory and study the complexity ofmodel-checking SLdisc[D]-formulas.</description><author>Munyque Mittelmann, Aniello Murano, Laurent Perrussel</author><pubDate>Wed, 24 May 2023 16:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15256v1</guid></item><item><title>LMs with a Voice: Spoken Language Modeling beyond Speech Tokens</title><link>http://arxiv.org/abs/2305.15255v1</link><description>We present SPECTRON, a novel approach to adapting pre-trained language models(LMs) to perform speech continuation. By leveraging pre-trained speechencoders, our model generates both text and speech outputs with the entiresystem being trained end-to-end operating directly on spectrograms. Trainingthe entire model in the spectrogram domain simplifies our speech continuationsystem versus existing cascade methods which use discrete speechrepresentations. We further show our method surpasses existing spoken languagemodels both in semantic content and speaker preservation while also benefitingfrom the knowledge transferred from pre-existing models. Audio samples can befound in our website https://michelleramanovich.github.io/spectron/spectron</description><author>Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad, RJ Skerry-Ryan, Michelle Tadmor Ramanovich</author><pubDate>Wed, 24 May 2023 16:39:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15255v1</guid></item><item><title>A DNN Framework for Learning Lagrangian Drift With Uncertainty</title><link>http://arxiv.org/abs/2204.05891v2</link><description>Reconstructions of Lagrangian drift, for example for objects lost at sea, areoften uncertain due to unresolved physical phenomena within the data.Uncertainty is usually overcome by introducing stochasticity into the drift,but this approach requires specific assumptions for modelling uncertainty. Weremove this constraint by presenting a purely data-driven framework formodelling probabilistic drift in flexible environments. Using ocean circulationmodel simulations, we generate probabilistic trajectories of object location bysimulating uncertainty in the initial object position. We train an emulator ofprobabilistic drift over one day given perfectly known velocities and observegood agreement with numerical simulations. Several loss functions are tested.Then, we strain our framework by training models where the input information isimperfect. On these harder scenarios, we observe reasonable predictionsalthough the effects of data drift become noticeable when evaluating the modelsagainst unseen flow scenarios.</description><author>Joseph Jenkins, Adeline Paiement, Yann Ourmières, Julien Le Sommer, Jacques Verron, Clément Ubelmann, Hervé Glotin</author><pubDate>Wed, 24 May 2023 16:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.05891v2</guid></item><item><title>Attention to Mean-Fields for Particle Cloud Generation</title><link>http://arxiv.org/abs/2305.15254v1</link><description>The generation of collider data using machine learning has emerged as aprominent research topic in particle physics due to the increasingcomputational challenges associated with traditional Monte Carlo simulationmethods, particularly for future colliders with higher luminosity. Althoughgenerating particle clouds is analogous to generating point clouds, accuratelymodelling the complex correlations between the particles presents aconsiderable challenge. Additionally, variable particle cloud sizes furtherexacerbate these difficulties, necessitating more sophisticated models. In thiswork, we propose a novel model that utilizes an attention-based aggregationmechanism to address these challenges. The model is trained in an adversarialtraining paradigm, ensuring that both the generator and critic exhibitpermutation equivariance/invariance with respect to their input. A novelfeature matching loss in the critic is introduced to stabilize the training.The proposed model performs competitively to the state-of-art whilst havingsignificantly fewer parameters.</description><author>Benno Käch, Isabell Melzer-Pellmann</author><pubDate>Wed, 24 May 2023 16:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15254v1</guid></item><item><title>Rethinking the Evaluation Protocol of Domain Generalization</title><link>http://arxiv.org/abs/2305.15253v1</link><description>Domain generalization aims to solve the challenge of Out-of-Distribution(OOD) generalization by leveraging common knowledge learned from multipletraining domains to generalize to unseen test domains. To accurately evaluatethe OOD generalization ability, it is necessary to ensure that test datainformation is unavailable. However, the current domain generalization protocolmay still have potential test data information leakage. This paper examines thepotential risks of test data information leakage in two aspects of the currentprotocol: pretraining on ImageNet and oracle model selection. We propose thattraining from scratch and using multiple test domains would result in a moreprecise evaluation of OOD generalization ability. We also rerun the algorithmswith the modified protocol and introduce a new leaderboard to encourage futureresearch in domain generalization with a fairer comparison.</description><author>Han Yu, Xingxuan Zhang, Renzhe Xu, Jiashuo Liu, Yue He, Peng Cui</author><pubDate>Wed, 24 May 2023 16:36:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15253v1</guid></item><item><title>PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</title><link>http://arxiv.org/abs/2305.10415v4</link><description>In this paper, we focus on the problem of Medical Visual Question Answering(MedVQA), which is crucial in efficiently interpreting medical images withvital clinic-relevant information. Firstly, we reframe the problem of MedVQA asa generation task that naturally follows the human-machine interaction, wepropose a generative-based model for medical visual understanding by aligningvisual information from a pre-trained vision encoder with a large languagemodel. Secondly, we establish a scalable pipeline to construct a large-scalemedical visual question-answering dataset, named PMC-VQA, which contains 227kVQA pairs of 149k images that cover various modalities or diseases. Thirdly, wepre-train our proposed model on PMC-VQA and then fine-tune it on multiplepublic benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by alarge margin. Additionally, we propose a test set that has undergone manualverification, which is significantly more challenging, even the best modelsstruggle to solve.</description><author>Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Wed, 24 May 2023 16:35:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10415v4</guid></item></channel></rss>