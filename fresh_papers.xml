<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 25 Apr 2024 06:00:51 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>TOP-Nav: Legged Navigation Integrating Terrain, Obstacle and Proprioception Estimation</title><link>http://arxiv.org/abs/2404.15256v2</link><description>Legged navigation is typically examined within open-world, off-road, andchallenging environments. In these scenarios, estimating external disturbancesrequires a complex synthesis of multi-modal information. This underlines amajor limitation in existing works that primarily focus on avoiding obstacles.In this work, we propose TOP-Nav, a novel legged navigation framework thatintegrates a comprehensive path planner with Terrain awareness, Obstacleavoidance and close-loop Proprioception. TOP-Nav underscores the synergiesbetween vision and proprioception in both path and motion planning. Within thepath planner, we present and integrate a terrain estimator that enables therobot to select waypoints on terrains with higher traversability whileeffectively avoiding obstacles. In the motion planning level, we not onlyimplement a locomotion controller to track the navigation commands, but alsoconstruct a proprioception advisor to provide motion evaluations for the pathplanner. Based on the close-loop motion feedback, we make online correctionsfor the vision-based terrain and obstacle estimations. Consequently, TOP-Navachieves open-world navigation that the robot can handle terrains ordisturbances beyond the distribution of prior knowledge and overcomesconstraints imposed by visual conditions. Building upon extensive experimentsconducted in both simulation and real-world environments, TOP-Nav demonstratessuperior performance in open-world navigation compared to existing methods.</description><author>Junli Ren, Yikai Liu, Yingru Dai, Guijin Wang</author><pubDate>Wed, 24 Apr 2024 17:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15256v2</guid></item><item><title>Leverage Variational Graph Representation For Model Poisoning on Federated Learning</title><link>http://arxiv.org/abs/2404.15042v2</link><description>This paper puts forth a new training data-untethered model poisoning (MP)attack on federated learning (FL). The new MP attack extends an adversarialvariational graph autoencoder (VGAE) to create malicious local models basedsolely on the benign local models overheard without any access to the trainingdata of FL. Such an advancement leads to the VGAE-MP attack that is not onlyefficacious but also remains elusive to detection. VGAE-MP attack extractsgraph structural correlations among the benign local models and the trainingdata features, adversarially regenerates the graph structure, and generatesmalicious local models using the adversarial graph structure and benign models'features. Moreover, a new attacking algorithm is presented to train themalicious local models using VGAE and sub-gradient descent, while enabling anoptimal selection of the benign local models for training the VGAE. Experimentsdemonstrate a gradual drop in FL accuracy under the proposed VGAE-MP attack andthe ineffectiveness of existing defense mechanisms in detecting the attack,posing a severe threat to FL.</description><author>Kai Li, Xin Yuan, Jingjing Zheng, Wei Ni, Falko Dressler, Abbas Jamalipour</author><pubDate>Wed, 24 Apr 2024 17:08:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15042v2</guid></item><item><title>An Economic Solution to Copyright Challenges of Generative AI</title><link>http://arxiv.org/abs/2404.13964v3</link><description>Generative artificial intelligence (AI) systems are trained on large datacorpora to generate new pieces of text, images, videos, and other media. Thereis growing concern that such systems may infringe on the copyright interests oftraining data contributors. To address the copyright challenges of generativeAI, we propose a framework that compensates copyright owners proportionally totheir contributions to the creation of AI-generated content. The metric forcontributions is quantitatively determined by leveraging the probabilisticnature of modern generative AI models and using techniques from cooperativegame theory in economics. This framework enables a platform where AI developersbenefit from access to high-quality training data, thus improving modelperformance. Meanwhile, copyright owners receive fair compensation, driving thecontinued provision of relevant data for generative model training. Experimentsdemonstrate that our framework successfully identifies the most relevant datasources used in artwork generation, ensuring a fair and interpretabledistribution of revenues among copyright owners.</description><author>Jiachen T. Wang, Zhun Deng, Hiroaki Chiba-Okabe, Boaz Barak, Weijie J. Su</author><pubDate>Wed, 24 Apr 2024 17:04:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13964v3</guid></item><item><title>Direct Zernike Coefficient Prediction from Point Spread Functions and Extended Images using Deep Learning</title><link>http://arxiv.org/abs/2404.15231v2</link><description>Optical imaging quality can be severely degraded by system and sample inducedaberrations. Existing adaptive optics systems typically rely on iterativesearch algorithm to correct for aberrations and improve images. This studydemonstrates the application of convolutional neural networks to characterisethe optical aberration by directly predicting the Zernike coefficients from twoto three phase-diverse optical images. We evaluated our network on 600,000simulated Point Spread Function (PSF) datasets randomly generated within therange of -1 to 1 radians using the first 25 Zernike coefficients. The resultsshow that using only three phase-diverse images captured above, below and atthe focal plane with an amplitude of 1 achieves a low RMSE of 0.10 radians onthe simulated PSF dataset. Furthermore, this approach directly predicts Zernikemodes simulated extended 2D samples, while maintaining a comparable RMSE of0.15 radians. We demonstrate that this approach is effective using only asingle prediction step, or can be iterated a small number of times. This simpleand straightforward technique provides rapid and accurate method for predictingthe aberration correction using three or less phase-diverse images, paving theway for evaluation on real-world dataset.</description><author>Yong En Kok, Alexander Bentley, Andrew Parkes, Amanda J. Wright, Michael G. Somekh, Michael Pound</author><pubDate>Wed, 24 Apr 2024 16:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15231v2</guid></item><item><title>Exploring Feedback Generation in Automated Skeletal Movement Assessment: A Comprehensive Overview</title><link>http://arxiv.org/abs/2404.09359v3</link><description>The application of machine-learning solutions to movement assessment fromskeleton videos has attracted significant research attention in recent years.This advancement has made rehabilitation at home more accessible, utilizingmovement assessment algorithms that can operate on affordable equipment forhuman pose detection and analysis from 2D or 3D videos. While the primaryobjective of automatic assessment tasks is to score movements, the automaticgeneration of feedback highlighting key movement issues has the potential tosignificantly enhance and accelerate the rehabilitation process. While numerousresearch works exist in the field of automatic movement assessment, only ahandful address feedback generation. In this study, we explain the types offeedback that can be generated, review existing solutions for automaticfeedback generation, and discuss future research directions. To our knowledge,this is the first comprehensive review of feedback generation in skeletalmovement assessment.</description><author>Tal Hakim</author><pubDate>Wed, 24 Apr 2024 16:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09359v3</guid></item><item><title>MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets</title><link>http://arxiv.org/abs/2404.13923v2</link><description>Driven by powerful image diffusion models, recent research has achieved theautomatic creation of 3D objects from textual or visual guidance. By performingscore distillation sampling (SDS) iteratively across different views, thesemethods succeed in lifting 2D generative prior to the 3D space. However, such a2D generative image prior bakes the effect of illumination and shadow into thetexture. As a result, material maps optimized by SDS inevitably involvespurious correlated components. The absence of precise material definitionmakes it infeasible to relight the generated assets reasonably in novel scenes,which limits their application in downstream scenarios. In contrast, humans caneffortlessly circumvent this ambiguity by deducing the material of the objectfrom its appearance and semantics. Motivated by this insight, we proposeMaterialSeg3D, a 3D asset material generation framework to infer underlyingmaterial from the 2D semantic prior. Based on such a prior model, we devise amechanism to parse material in 3D space. We maintain a UV stack, each map ofwhich is unprojected from a specific viewpoint. After traversing allviewpoints, we fuse the stack through a weighted voting scheme and then employregion unification to ensure the coherence of the object parts. To fuel thelearning of semantics prior, we collect a material dataset, named MaterializedIndividual Objects (MIO), which features abundant images, diverse categories,and accurate annotations. Extensive quantitative and qualitative experimentsdemonstrate the effectiveness of our method.</description><author>Zeyu Li, Ruitong Gan, Chuanchen Luo, Yuxi Wang, Jiaheng Liu, Ziwei Zhu Man Zhang, Qing Li, Xucheng Yin, Zhaoxiang Zhang, Junran Peng</author><pubDate>Wed, 24 Apr 2024 15:21:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13923v2</guid></item><item><title>Better Synthetic Data by Retrieving and Transforming Existing Datasets</title><link>http://arxiv.org/abs/2404.14361v2</link><description>Despite recent advances in large language models, building dependable anddeployable NLP models typically requires abundant, high-quality training data.However, task-specific data is not available for many use cases, and manuallycurating task-specific data is labor-intensive. Recent work has studiedprompt-driven synthetic data generation using large language models, but thesegenerated datasets tend to lack complexity and diversity. To address theselimitations, we introduce a method, DataTune, to make better use of existing,publicly available datasets to improve automatic dataset generation. DataTuneperforms dataset transformation, enabling the repurposing of publicly availabledatasets into a format that is directly aligned with the specific requirementsof target tasks. On a diverse set of language-based tasks from the BIG-Benchbenchmark, we find that finetuning language models via DataTune improves over afew-shot prompting baseline by 49% and improves over existing methods that usesynthetic or retrieved training data by 34%. We find that datasettransformation significantly increases the diversity and difficulty ofgenerated data on many tasks. We integrate DataTune into an open-sourcerepository to make this method accessible to the community:https://github.com/neulab/prompt2model.</description><author>Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, Graham Neubig</author><pubDate>Wed, 24 Apr 2024 11:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14361v2</guid></item><item><title>Effective Decision Boundary Learning for Class Incremental Learning</title><link>http://arxiv.org/abs/2301.05180v3</link><description>Rehearsal approaches in class incremental learning (CIL) suffer from decisionboundary overfitting to new classes, which is mainly caused by two factors:insufficiency of old classes data for knowledge distillation and imbalanceddata learning between the learned and new classes because of the limitedstorage memory. In this work, we present a simple but effective approach totackle these two factors. First, we employ a re-sampling strategy and MixupK}nowledge D}istillation (Re-MKD) to improve the performances of KD, whichwould greatly alleviate the overfitting problem. Specifically, we combine mixupand re-sampling strategies to synthesize adequate data used in KD training thatare more consistent with the latent distribution between the learned and newclasses. Second, we propose a novel incremental influence balance (IIB) methodfor CIL to tackle the classification of imbalanced data by extending theinfluence balance method into the CIL setting, which re-weights samples bytheir influences to create a proper decision boundary. With these twoimprovements, we present the effective decision boundary learning algorithm(EDBL) which improves the performance of KD and deals with the imbalanced datalearning simultaneously. Experiments show that the proposed EDBL achievesstate-of-the-art performances on several CIL benchmarks.</description><author>Chaoyue Ding, Kunchi Li, Jun Wan, Shan Yu</author><pubDate>Wed, 24 Apr 2024 10:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.05180v3</guid></item><item><title>Probabilistic forecasting of power system imbalance using neural network-based ensembles</title><link>http://arxiv.org/abs/2404.14836v2</link><description>Keeping the balance between electricity generation and consumption isbecoming increasingly challenging and costly, mainly due to the rising share ofrenewables, electric vehicles and heat pumps and electrification of industrialprocesses. Accurate imbalance forecasts, along with reliable uncertaintyestimations, enable transmission system operators (TSOs) to dispatchappropriate reserve volumes, reducing balancing costs. Further, market partiescan use these probabilistic forecasts to design strategies that exploit assetflexibility to help balance the grid, generating revenue with known risks.Despite its importance, literature regarding system imbalance (SI) forecastingis limited. Further, existing methods do not focus on situations with highimbalance magnitude, which are crucial to forecast accurately for both TSOs andmarket parties. Hence, we propose an ensemble of C-VSNs, which are ouradaptation of variable selection networks (VSNs). Each minute, our modelpredicts the imbalance of the current and upcoming two quarter-hours, alongwith uncertainty estimations on these forecasts. We evaluate our approach byforecasting the imbalance of Belgium, where high imbalance magnitude is definedas $|$SI$| &gt; 500\,$MW (occurs 1.3% of the time in Belgium). For high imbalancemagnitude situations, our model outperforms the state-of-the-art by 23.4% (interms of continuous ranked probability score (CRPS), which evaluatesprobabilistic forecasts), while also attaining a 6.5% improvement in overallCRPS. Similar improvements are achieved in terms of root-mean-squared error.Additionally, we developed a fine-tuning methodology to effectively include newinputs with limited history in our model. This work was performed incollaboration with Elia (the Belgian TSO) to further improve their imbalanceforecasts, demonstrating the relevance of our work.</description><author>Jonas Van Gompel, Bert Claessens, Chris Develder</author><pubDate>Wed, 24 Apr 2024 09:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14836v2</guid></item><item><title>CORE-BEHRT: A Carefully Optimized and Rigorously Evaluated BEHRT</title><link>http://arxiv.org/abs/2404.15201v2</link><description>BERT-based models for Electronic Health Records (EHR) have surged inpopularity following the release of BEHRT and Med-BERT. Subsequent models havelargely built on these foundations despite the fundamental design choices ofthese pioneering models remaining underexplored. To address this issue, weintroduce CORE-BEHRT, a Carefully Optimized and Rigorously Evaluated BEHRT.Through incremental optimization, we isolate the sources of improvement for keydesign choices, giving us insights into the effect of data representation andindividual technical components on performance. Evaluating this across a set ofgeneric tasks (death, pain treatment, and general infection), we showed thatimproving data representation can increase the average downstream performancefrom 0.785 to 0.797 AUROC, primarily when including medication and timestamps.Improving the architecture and training protocol on top of this increasedaverage downstream performance to 0.801 AUROC. We then demonstrated theconsistency of our optimization through a rigorous evaluation across 25 diverseclinical prediction tasks. We observed significant performance increases in 17out of 25 tasks and improvements in 24 tasks, highlighting the generalizabilityof our findings. Our findings provide a strong foundation for future work andaim to increase the trustworthiness of BERT-based EHR models.</description><author>Mikkel Odgaard, Kiril Vadimovic Klein, Sanne Møller Thysen, Espen Jimenez-Solem, Martin Sillesen, Mads Nielsen</author><pubDate>Wed, 24 Apr 2024 09:02:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15201v2</guid></item><item><title>BotDGT: Dynamicity-aware Social Bot Detection with Dynamic Graph Transformers</title><link>http://arxiv.org/abs/2404.15070v2</link><description>Detecting social bots has evolved into a pivotal yet intricate task, aimed atcombating the dissemination of misinformation and preserving the authenticityof online interactions. While earlier graph-based approaches, which leveragetopological structure of social networks, yielded notable outcomes, theyoverlooked the inherent dynamicity of social networks -- In reality, theylargely depicted the social network as a static graph and solely relied on itsmost recent state. Due to the absence of dynamicity modeling, such approachesare vulnerable to evasion, particularly when advanced social bots interact withother users to camouflage identities and escape detection. To tackle thesechallenges, we propose BotDGT, a novel framework that not only considers thetopological structure, but also effectively incorporates dynamic nature ofsocial network. Specifically, we characterize a social network as a dynamicgraph. A structural module is employed to acquire topological information fromeach historical snapshot. Additionally, a temporal module is proposed tointegrate historical context and model the evolving behavior patterns exhibitedby social bots and legitimate users. Experimental results demonstrate thesuperiority of BotDGT against the leading methods that neglected the dynamicnature of social networks in terms of accuracy, recall, and F1-score.</description><author>Buyun He, Yingguang Yang, Qi Wu, Hao Liu, Renyu Yang, Hao Peng, Xiang Wang, Yong Liao, Pengyuan Zhou</author><pubDate>Wed, 24 Apr 2024 09:01:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15070v2</guid></item><item><title>FlashSpeech: Efficient Zero-Shot Speech Synthesis</title><link>http://arxiv.org/abs/2404.14700v2</link><description>Recent progress in large-scale zero-shot speech synthesis has beensignificantly advanced by language models and diffusion models. However, thegeneration process of both methods is slow and computationally intensive.Efficient speech synthesis using a lower computing budget to achieve quality onpar with previous work remains a significant challenge. In this paper, wepresent FlashSpeech, a large-scale zero-shot speech synthesis system withapproximately 5\% of the inference time compared with previous work.FlashSpeech is built on the latent consistency model and applies a noveladversarial consistency training approach that can train from scratch withoutthe need for a pre-trained diffusion model as the teacher. Furthermore, a newprosody generator module enhances the diversity of prosody, making the rhythmof the speech sound more natural. The generation processes of FlashSpeech canbe achieved efficiently with one or two sampling steps while maintaining highaudio quality and high similarity to the audio prompt for zero-shot speechgeneration. Our experimental results demonstrate the superior performance ofFlashSpeech. Notably, FlashSpeech can be about 20 times faster than otherzero-shot speech synthesis systems while maintaining comparable performance interms of voice quality and similarity. Furthermore, FlashSpeech demonstratesits versatility by efficiently performing tasks like voice conversion, speechediting, and diverse speech sampling. Audio samples can be found inhttps://flashspeech.github.io/.</description><author>Zhen Ye, Zeqian Ju, Haohe Liu, Xu Tan, Jianyi Chen, Yiwen Lu, Peiwen Sun, Jiahao Pan, Weizhen Bian, Shulin He, Qifeng Liu, Yike Guo, Wei Xue</author><pubDate>Wed, 24 Apr 2024 08:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14700v2</guid></item><item><title>DAWN: Domain-Adaptive Weakly Supervised Nuclei Segmentation via Cross-Task Interactions</title><link>http://arxiv.org/abs/2404.14956v2</link><description>Weakly supervised segmentation methods have gained significant attention dueto their ability to reduce the reliance on costly pixel-level annotationsduring model training. However, the current weakly supervised nucleisegmentation approaches typically follow a two-stage pseudo-label generationand network training process. The performance of the nuclei segmentationheavily relies on the quality of the generated pseudo-labels, thereby limitingits effectiveness. This paper introduces a novel domain-adaptive weaklysupervised nuclei segmentation framework using cross-task interactionstrategies to overcome the challenge of pseudo-label generation. Specifically,we utilize weakly annotated data to train an auxiliary detection task, whichassists the domain adaptation of the segmentation network. To enhance theefficiency of domain adaptation, we design a consistent feature constraintmodule integrating prior knowledge from the source domain. Furthermore, wedevelop pseudo-label optimization and interactive training methods to improvethe domain transfer capability. To validate the effectiveness of our proposedmethod, we conduct extensive comparative and ablation experiments on sixdatasets. The results demonstrate the superiority of our approach over existingweakly supervised approaches. Remarkably, our method achieves comparable oreven better performance than fully supervised methods. Our code will bereleased in https://github.com/zhangye-zoe/DAWN.</description><author>Ye Zhang, Yifeng Wang, Zijie Fang, Hao Bian, Linghan Cai, Ziyue Wang, Yongbing Zhang</author><pubDate>Wed, 24 Apr 2024 07:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14956v2</guid></item><item><title>Zero-Shot Character Identification and Speaker Prediction in Comics via Iterative Multimodal Fusion</title><link>http://arxiv.org/abs/2404.13993v2</link><description>Recognizing characters and predicting speakers of dialogue are critical forcomic processing tasks, such as voice generation or translation. However,because characters vary by comic title, supervised learning approaches liketraining character classifiers which require specific annotations for eachcomic title are infeasible. This motivates us to propose a novel zero-shotapproach, allowing machines to identify characters and predict speaker namesbased solely on unannotated comic images. In spite of their importance inreal-world applications, these task have largely remained unexplored due tochallenges in story comprehension and multimodal integration. Recent largelanguage models (LLMs) have shown great capability for text understanding andreasoning, while their application to multimodal content analysis is still anopen problem. To address this problem, we propose an iterative multimodalframework, the first to employ multimodal information for both characteridentification and speaker prediction tasks. Our experiments demonstrate theeffectiveness of the proposed framework, establishing a robust baseline forthese tasks. Furthermore, since our method requires no training data orannotations, it can be used as-is on any comic series.</description><author>Yingxuan Li, Ryota Hinami, Kiyoharu Aizawa, Yusuke Matsui</author><pubDate>Wed, 24 Apr 2024 07:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13993v2</guid></item><item><title>A sensitivity analysis to quantify the impact of neuroimaging preprocessing strategies on subsequent statistical analyses</title><link>http://arxiv.org/abs/2404.14882v2</link><description>Even though novel imaging techniques have been successful in studying brainstructure and function, the measured biological signals are often contaminatedby multiple sources of noise, arising due to e.g. head movements of theindividual being scanned, limited spatial/temporal resolution, or other issuesspecific to each imaging technology. Data preprocessing (e.g. denoising) istherefore critical. Preprocessing pipelines have become increasingly complexover the years, but also more flexible, and this flexibility can have asignificant impact on the final results and conclusions of a given study. Thislarge parameter space is often referred to as multiverse analyses. Here, weprovide conceptual and practical tools for statistical analyses that canaggregate multiple pipeline results along with a new sensitivity analysistesting for hypotheses across pipelines such as "no effect across allpipelines" or "at least one pipeline with no effect". The proposed framework isgeneric and can be applied to any multiverse scenario, but we illustrate itsuse based on positron emission tomography data.</description><author>Brice Ozenne, Martin Norgaard, Cyril Pernet, Melanie Ganz</author><pubDate>Wed, 24 Apr 2024 06:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14882v2</guid></item><item><title>Towards smaller, faster decoder-only transformers: Architectural variants and their implications</title><link>http://arxiv.org/abs/2404.14462v2</link><description>Research on Large Language Models (LLMs) has recently seen exponentialgrowth, largely focused on transformer-based architectures, as introduced by[1] and further advanced by the decoder-only variations in [2]. Contemporarystudies typically aim to improve model capabilities by increasing both thearchitecture's complexity and the volume of training data. However, researchexploring how to reduce model sizes while maintaining performance is limited.This study introduces three modifications to the decoder-only transformerarchitecture: ParallelGPT (p-gpt), LinearlyCompressedGPT (lc-gpt), andConvCompressedGPT (cc-gpt). These variants achieve comparable performance toconventional architectures in code generation tasks while benefiting fromreduced model sizes and faster training times. We open-source the model weightsand codebase to support future research and development in this domain.</description><author>Sathya Krishnan Suresh, Shunmugapriya P</author><pubDate>Wed, 24 Apr 2024 04:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14462v2</guid></item><item><title>Biologically-Informed Excitatory and Inhibitory Balance for Robust Spiking Neural Network Training</title><link>http://arxiv.org/abs/2404.15627v1</link><description>Spiking neural networks drawing inspiration from biological constraints ofthe brain promise an energy-efficient paradigm for artificial intelligence.However, challenges exist in identifying guiding principles to train thesenetworks in a robust fashion. In addition, training becomes an even moredifficult problem when incorporating biological constraints of excitatory andinhibitory connections. In this work, we identify several key factors, such aslow initial firing rates and diverse inhibitory spiking patterns, thatdetermine the overall ability to train spiking networks with various ratios ofexcitatory to inhibitory neurons on AI-relevant datasets. The results indicatenetworks with the biologically realistic 80:20 excitatory:inhibitory balancecan reliably train at low activity levels and in noisy environments.Additionally, the Van Rossum distance, a measure of spike train synchrony,provides insight into the importance of inhibitory neurons to increase networkrobustness to noise. This work supports further biologically-informedlarge-scale networks and energy efficient hardware implementations.</description><author>Joseph A. Kilgore, Jeffrey D. Kopsick, Giorgio A. Ascoli, Gina C. Adam</author><pubDate>Wed, 24 Apr 2024 04:29:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15627v1</guid></item><item><title>Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models</title><link>http://arxiv.org/abs/2404.15625v1</link><description>The open-world test dataset is often mixed with out-of-distribution (OOD)samples, where the deployed models will struggle to make accurate predictions.Traditional detection methods need to trade off OOD detection andin-distribution (ID) classification performance since they share the samerepresentation learning model. In this work, we propose to detect OOD moleculesby adopting an auxiliary diffusion model-based framework, which comparessimilarities between input molecules and reconstructed graphs. Due to thegenerative bias towards reconstructing ID training samples, the similarityscores of OOD molecules will be much lower to facilitate detection. Although itis conceptually simple, extending this vanilla framework to practical detectionapplications is still limited by two significant challenges. First, the popularsimilarity metrics based on Euclidian distance fail to consider the complexgraph structure. Second, the generative model involving iterative denoisingsteps is time-consuming especially when it runs on the enormous pool of drugs.To address these challenges, our research pioneers an approach of PrototypicalGraph Reconstruction for Molecular OOD Detection, dubbed as PGR-MOOD and hingeson three innovations: i) An effective metric to comprehensively quantify thematching degree of input and reconstructed molecules; ii) A creative graphgenerator to construct prototypical graphs that are in line with ID but awayfrom OOD; iii) An efficient and scalable OOD detector to compare the similaritybetween test samples and pre-constructed prototypical graphs and omit thegenerative process on every new molecule. Extensive experiments on tenbenchmark datasets and six baselines are conducted to demonstrate oursuperiority.</description><author>Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, Xin Wang</author><pubDate>Wed, 24 Apr 2024 04:25:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15625v1</guid></item><item><title>FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural Architecture Search</title><link>http://arxiv.org/abs/2404.15622v1</link><description>Neural Architecture Search (NAS) has emerged as a key tool in identifyingoptimal configurations of deep neural networks tailored to specific tasks.However, training and assessing numerous architectures introduces considerablecomputational overhead. One method to mitigating this is through performancepredictors, which offer a means to estimate the potential of an architecturewithout exhaustive training. Given that neural architectures fundamentallyresemble Directed Acyclic Graphs (DAGs), Graph Neural Networks (GNNs) become anapparent choice for such predictive tasks. Nevertheless, the scarcity oftraining data can impact the precision of GNN-based predictors. To addressthis, we introduce a novel GNN predictor for NAS. This predictor renders neuralarchitectures into vector representations by combining both the conventionaland inverse graph views. Additionally, we incorporate a customized trainingloss within the GNN predictor to ensure efficient utilization of both types ofrepresentations. We subsequently assessed our method through experiments onbenchmark datasets including NAS-Bench-101, NAS-Bench-201, and the DARTS searchspace, with a training dataset ranging from 50 to 400 samples. Benchmarkedagainst leading GNN predictors, the experimental results showcase a significantimprovement in prediction accuracy, with a 3%--16% increase in Kendall-taucorrelation. Source codes are available at https://github.com/EMI-Group/fr-nas.</description><author>Haoming Zhang, Ran Cheng</author><pubDate>Wed, 24 Apr 2024 04:22:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15622v1</guid></item><item><title>Layer Ensemble Averaging for Improving Memristor-Based Artificial Neural Network Performance</title><link>http://arxiv.org/abs/2404.15621v1</link><description>Artificial neural networks have advanced due to scaling dimensions, butconventional computing faces inefficiency due to the von Neumann bottleneck.In-memory computation architectures, like memristors, offer promise but facechallenges due to hardware non-idealities. This work proposes andexperimentally demonstrates layer ensemble averaging, a technique to mappre-trained neural network solutions from software to defective hardwarecrossbars of emerging memory devices and reliably attain near-softwareperformance on inference. The approach is investigated using a custom20,000-device hardware prototyping platform on a continual learning problemwhere a network must learn new tasks without catastrophically forgettingpreviously learned information. Results demonstrate that by trading off thenumber of devices required for layer mapping, layer ensemble averaging canreliably boost defective memristive network performance up to the softwarebaseline. For the investigated problem, the average multi-task classificationaccuracy improves from 61 % to 72 % (&lt; 1 % of software baseline) using theproposed approach.</description><author>Osama Yousuf, Brian Hoskins, Karthick Ramu, Mitchell Fream, William A. Borders, Advait Madhavan, Matthew W. Daniels, Andrew Dienstfrey, Jabez J. McClelland, Martin Lueker-Boden, Gina C. Adam</author><pubDate>Wed, 24 Apr 2024 04:19:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15621v1</guid></item><item><title>Neural Operator induced Gaussian Process framework for probabilistic solution of parametric partial differential equations</title><link>http://arxiv.org/abs/2404.15618v1</link><description>The study of neural operators has paved the way for the development ofefficient approaches for solving partial differential equations (PDEs) comparedwith traditional methods. However, most of the existing neural operators lackthe capability to provide uncertainty measures for their predictions, a crucialaspect, especially in data-driven scenarios with limited available data. Inthis work, we propose a novel Neural Operator-induced Gaussian Process (NOGaP),which exploits the probabilistic characteristics of Gaussian Processes (GPs)while leveraging the learning prowess of operator learning. The proposedframework leads to improved prediction accuracy and offers a quantifiablemeasure of uncertainty. The proposed framework is extensively evaluated throughexperiments on various PDE examples, including Burger's equation, Darcy flow,non-homogeneous Poisson, and wave-advection equations. Furthermore, acomparative study with state-of-the-art operator learning algorithms ispresented to highlight the advantages of NOGaP. The results demonstratesuperior accuracy and expected uncertainty characteristics, suggesting thepromising potential of the proposed framework.</description><author>Sawan Kumar, Rajdip Nayek, Souvik Chakraborty</author><pubDate>Wed, 24 Apr 2024 04:16:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15618v1</guid></item><item><title>Few-shot point cloud reconstruction and denoising via learned Guassian splats renderings and fine-tuned diffusion features</title><link>http://arxiv.org/abs/2404.01112v4</link><description>Existing deep learning methods for the reconstruction and denoising of pointclouds rely on small datasets of 3D shapes. We circumvent the problem byleveraging deep learning methods trained on billions of images. We propose amethod to reconstruct point clouds from few images and to denoise point cloudsfrom their rendering by exploiting prior knowledge distilled from image-baseddeep learning models. To improve reconstruction in constraint settings, weregularize the training of a differentiable renderer with hybrid surface andappearance by introducing semantic consistency supervision. In addition, wepropose a pipeline to finetune Stable Diffusion to denoise renderings of noisypoint clouds and we demonstrate how these learned filters can be used to removepoint cloud noise coming without 3D supervision. We compare our method with DSSand PointRadiance and achieved higher quality 3D reconstruction on theSketchfab Testset and SCUT Dataset.</description><author>Pietro Bonazzi, Marie-Julie Rakatosaona, Marco Cannici, Federico Tombari, Davide Scaramuzza</author><pubDate>Wed, 24 Apr 2024 04:14:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01112v4</guid></item><item><title>3D scene generation from scene graphs and self-attention</title><link>http://arxiv.org/abs/2404.01887v3</link><description>Synthesizing realistic and diverse indoor 3D scene layouts in a controllablefashion opens up applications in simulated navigation and virtual reality. Asconcise and robust representations of a scene, scene graphs have proven to bewell-suited as the semantic control on the generated layout. We present avariant of the conditional variational autoencoder (cVAE) model to synthesize3D scenes from scene graphs and floor plans. We exploit the properties ofself-attention layers to capture high-level relationships between objects in ascene, and use these as the building blocks of our model. Our model, leveragesgraph transformers to estimate the size, dimension and orientation of theobjects in a room while satisfying relationships in the given scene graph. Ourexperiments shows self-attention layers leads to sparser (7.9x compared toGraphto3D) and more diverse scenes (16%).</description><author>Pietro Bonazzi, Mengqi Wang, Diego Martin Arroyo, Fabian Manhardt, Nico Messikomer, Federico Tombari, Davide Scaramuzza</author><pubDate>Wed, 24 Apr 2024 04:13:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01887v3</guid></item><item><title>DPO: Differential reinforcement learning with application to optimal configuration search</title><link>http://arxiv.org/abs/2404.15617v1</link><description>Reinforcement learning (RL) with continuous state and action spaces remainsone of the most challenging problems within the field. Most current learningmethods focus on integral identities such as value functions to derive anoptimal strategy for the learning agent. In this paper, we instead study thedual form of the original RL formulation to propose the first differential RLframework that can handle settings with limited training samples andshort-length episodes. Our approach introduces Differential Policy Optimization(DPO), a pointwise and stage-wise iteration method that optimizes policiesencoded by local-movement operators. We prove a pointwise convergence estimatefor DPO and provide a regret bound comparable with current theoretical works.Such pointwise estimate ensures that the learned policy matches the optimalpath uniformly across different steps. We then apply DPO to a class ofpractical RL problems which search for optimal configurations with Lagrangianrewards. DPO is easy to implement, scalable, and shows competitive results onbenchmarking experiments against several popular RL methods.</description><author>Chandrajit Bajaj, Minh Nguyen</author><pubDate>Wed, 24 Apr 2024 04:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15617v1</guid></item><item><title>A Bi-directional Quantum Search Algorithm</title><link>http://arxiv.org/abs/2404.15616v1</link><description>Grover's search algorithms, including various partial Grover searches,experience scaling problems as the number of iterations rises with increasedqubits, making implementation more computationally expensive. This papercombines Partial Grover's search algorithm and Bi-directional Search to createa fast Grover's quantum search algorithm, referred to as Bi-Directional GroverSearch (BDGS). We incorporated a bi-directional search tactic with a partialGrover search, starting from an initial state and a single marked state inparallel. We have shown in this article that our novel approach requires$\frac{\pi}{4\sqrt{2}}\sqrt{N}(1-\sqrt{\frac{1}{b^{r/2k}}})$ iterations overregular Grover Search and Partial Grover Search (PGS), which takes$\frac{\pi}{4}\sqrt{N}\sqrt{1-\frac{1}{b}}$ (here, $N=2^r$ elements, $b$ is thebranching factor of partial search, and $k= \lceil\log_2b \rceil$). Theproposed BDGS algorithm is benchmarked against the state-of-the-art Depth-FirstGrover's Search (DFGS) and generic Grover's Search (GS) implementations for $2$to $20$ qubits and provides promising results. The Qiskit Python implementationof the proposed BDGS algorithm is available on Github(https://github.com/hafeezzwiz21/DFGS-BDGS).</description><author>Debanjan Konar, Zain Hafeez, Vaneet Aggarwal</author><pubDate>Wed, 24 Apr 2024 04:11:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15616v1</guid></item><item><title>MDDD: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition</title><link>http://arxiv.org/abs/2404.15615v1</link><description>Emotion decoding using Electroencephalography (EEG)-based affectivebrain-computer interfaces represents a significant area within the field ofaffective computing. In the present study, we propose a novel non-deep transferlearning method, termed as Manifold-based Domain adaptation with DynamicDistribution (MDDD). The proposed MDDD includes four main modules: manifoldfeature transformation, dynamic distribution alignment, classifier learning,and ensemble learning. The data undergoes a transformation onto an optimalGrassmann manifold space, enabling dynamic alignment of the source and targetdomains. This process prioritizes both marginal and conditional distributionsaccording to their significance, ensuring enhanced adaptation efficiency acrossvarious types of data. In the classifier learning, the principle of structuralrisk minimization is integrated to develop robust classification models. Thisis complemented by dynamic distribution alignment, which refines the classifieriteratively. Additionally, the ensemble learning module aggregates theclassifiers obtained at different stages of the optimization process, whichleverages the diversity of the classifiers to enhance the overall predictionaccuracy. The experimental results indicate that MDDD outperforms traditionalnon-deep learning methods, achieving an average improvement of 3.54%, and iscomparable to deep learning methods. This suggests that MDDD could be apromising method for enhancing the utility and applicability of aBCIs inreal-world scenarios.</description><author>Ting Luo, Jing Zhang, Yingwei Qiu, Li Zhang, Yaohua Hu, Zhuliang Yu, Zhen Liang</author><pubDate>Wed, 24 Apr 2024 04:08:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15615v1</guid></item><item><title>Talk Too Much: Poisoning Large Language Models under Token Limit</title><link>http://arxiv.org/abs/2404.14795v2</link><description>Mainstream poisoning attacks on large language models (LLMs) typically set afixed trigger in the input instance and specific responses for triggeredqueries. However, the fixed trigger setting (e.g., unusual words) may be easilydetected by human detection, limiting the effectiveness and practicality inreal-world scenarios. To enhance the stealthiness of the trigger, we present apoisoning attack against LLMs that is triggered by a generation/outputcondition-token limitation, which is a commonly adopted strategy by users forreducing costs. The poisoned model performs normally for output without tokenlimitation, while becomes harmful for output with limited tokens. To achievethis objective, we introduce BrieFool, an efficient attack framework. Itleverages the characteristics of generation limitation by efficient instructionsampling and poisoning data generation, thereby influencing the behavior ofLLMs under target conditions. Our experiments demonstrate that BrieFool iseffective across safety domains and knowledge domains. For instance, with only20 generated poisoning examples against GPT-3.5-turbo, BrieFool achieves a 100%Attack Success Rate (ASR) and a 9.28/10 average Harmfulness Score (HS) undertoken limitation conditions while maintaining the benign performance.</description><author>Jiaming He, Wenbo Jiang, Guanyu Hou, Wenshu Fan, Rui Zhang, Hongwei Li</author><pubDate>Wed, 24 Apr 2024 03:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14795v2</guid></item><item><title>Regional Style and Color Transfer</title><link>http://arxiv.org/abs/2404.13880v2</link><description>This paper presents a novel contribution to the field of regional styletransfer. Existing methods often suffer from the drawback of applying stylehomogeneously across the entire image, leading to stylistic inconsistencies orforeground object twisted when applied to image with foreground elements suchas person figures. To address this limitation, we propose a new approach thatleverages a segmentation network to precisely isolate foreground objects withinthe input image. Subsequently, style transfer is applied exclusively to thebackground region. The isolated foreground objects are then carefullyreintegrated into the style-transferred background. To enhance the visualcoherence between foreground and background, a color transfer step is employedon the foreground elements prior to their rein-corporation. Finally, we utilizefeathering techniques to achieve a seamless amalgamation of foreground andbackground, resulting in a visually unified and aesthetically pleasing finalcomposition. Extensive evaluations demonstrate that our proposed approachyields significantly more natural stylistic transformations compared toconventional methods.</description><author>Zhicheng Ding, Panfeng Li, Qikai Yang, Siyang Li, Qingtian Gong</author><pubDate>Wed, 24 Apr 2024 03:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13880v2</guid></item><item><title>Understanding and Improving CNNs with Complex Structure Tensor: A Biometrics Study</title><link>http://arxiv.org/abs/2404.15608v1</link><description>Our study provides evidence that CNNs struggle to effectively extractorientation features. We show that the use of Complex Structure Tensor, whichcontains compact orientation features with certainties, as input to CNNsconsistently improves identification accuracy compared to using grayscaleinputs alone. Experiments also demonstrated that our inputs, which wereprovided by mini complex conv-nets, combined with reduced CNN sizes,outperformed full-fledged, prevailing CNN architectures. This suggests that theupfront use of orientation features in CNNs, a strategy seen in mammalianvision, not only mitigates their limitations but also enhances theirexplainability and relevance to thin-clients. Experiments were done on publiclyavailable data sets comprising periocular images for biometric identificationand verification (Close and Open World) using 6 State of the Art CNNarchitectures. We reduced SOA Equal Error Rate (EER) on the PolyU dataset by5-26% depending on data and scenario.</description><author>Kevin Hernandez-Diaz, Josef Bigun, Fernando Alonso-Fernandez</author><pubDate>Wed, 24 Apr 2024 03:51:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15608v1</guid></item><item><title>Motion Code: Robust Time series Classification and Forecasting via Sparse Variational Multi-Stochastic Processes Learning</title><link>http://arxiv.org/abs/2402.14081v2</link><description>Despite being extensively studied, time series classification and forecastingon noisy data remain highly difficult. The main challenges lie in findingsuitable mathematical concepts to describe time series and effectivelyseparating noise from the true signals. Instead of treating time series as astatic vector or a data sequence as often seen in previous methods, weintroduce a novel framework that considers each time series, not necessarily offixed length, as a sample realization of a continuous-time stochastic process.Such mathematical model explicitly captures the data dependence across severaltimestamps and detects the hidden time-dependent signals from noise. However,since the underlying data is often composed of several distinct dynamics,modeling using a single stochastic process is not sufficient. To handle suchsettings, we first assign each dynamics a signature vector. We then propose theabstract concept of the most informative timestamps to infer a sparseapproximation of the individual dynamics based on their assigned vectors. Thefinal model, referred to as Motion Code, contains parameters that can fullycapture different underlying dynamics in an integrated manner. This allowsunmixing classification and generation of specific sub-type forecastingsimultaneously. Extensive experiments on sensors and devices noisy time seriesdata demonstrate Motion Code's competitiveness against time seriesclassification and forecasting benchmarks.</description><author>Chandrajit Bajaj, Minh Nguyen</author><pubDate>Wed, 24 Apr 2024 03:45:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14081v2</guid></item><item><title>A Comparative Study on Enhancing Prediction in Social Network Advertisement through Data Augmentation</title><link>http://arxiv.org/abs/2404.13812v2</link><description>In the ever-evolving landscape of social network advertising, the volume andaccuracy of data play a critical role in the performance of predictive models.However, the development of robust predictive algorithms is often hampered bythe limited size and potential bias present in real-world datasets. This studypresents and explores a generative augmentation framework of social networkadvertising data. Our framework explores three generative models for dataaugmentation - Generative Adversarial Networks (GANs), Variational Autoencoders(VAEs), and Gaussian Mixture Models (GMMs) - to enrich data availability anddiversity in the context of social network advertising analytics effectiveness.By performing synthetic extensions of the feature space, we find that throughdata augmentation, the performance of various classifiers has beenquantitatively improved. Furthermore, we compare the relative performance gainsbrought by each data augmentation technique, providing insights forpractitioners to select appropriate techniques to enhance model performance.This paper contributes to the literature by showing that synthetic dataaugmentation alleviates the limitations imposed by small or imbalanced datasetsin the field of social network advertising. At the same time, this article alsoprovides a comparative perspective on the practicality of different dataaugmentation methods, thereby guiding practitioners to choose appropriatetechniques to enhance model performance.</description><author>Qikai Yang, Panfeng Li, Xinhe Xu, Zhicheng Ding, Wenjing Zhou, Yi Nian</author><pubDate>Wed, 24 Apr 2024 03:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13812v2</guid></item><item><title>Hybrid LLM/Rule-based Approaches to Business Insights Generation from Structured Data</title><link>http://arxiv.org/abs/2404.15604v1</link><description>In the field of business data analysis, the ability to extract actionableinsights from vast and varied datasets is essential for informeddecision-making and maintaining a competitive edge. Traditional rule-basedsystems, while reliable, often fall short when faced with the complexity anddynamism of modern business data. Conversely, Artificial Intelligence (AI)models, particularly Large Language Models (LLMs), offer significant potentialin pattern recognition and predictive analytics but can lack the precisionnecessary for specific business applications. This paper explores the efficacyof hybrid approaches that integrate the robustness of rule-based systems withthe adaptive power of LLMs in generating actionable business insights.</description><author>Aliaksei Vertsel, Mikhail Rumiantsau</author><pubDate>Wed, 24 Apr 2024 03:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15604v1</guid></item><item><title>Deepfakes and Higher Education: A Research Agenda and Scoping Review of Synthetic Media</title><link>http://arxiv.org/abs/2404.15601v1</link><description>The availability of software which can produce convincing yet synthetic mediaposes both threats and benefits to tertiary education globally. While otherforms of synthetic media exist, this study focuses on deepfakes, which areadvanced Generative AI (GenAI) fakes of real people. This conceptual paperassesses the current literature on deepfakes across multiple disciplines byconducting an initial scoping review of 182 peer-reviewed publications. The review reveals three major trends: detection methods, maliciousapplications, and potential benefits, although no specific studies on deepfakesin the tertiary educational context were found. Following a discussion of thesetrends, this study applies the findings to postulate the major risks andpotential mitigation strategies of deepfake technologies in higher education,as well as potential beneficial uses to aid the teaching and learning of bothdeepfakes and synthetic media. This culminates in the proposal of a researchagenda to build a comprehensive, cross-cultural approach to investigatedeepfakes in higher education.</description><author>Jasper Roe, Mike Perkins</author><pubDate>Wed, 24 Apr 2024 03:40:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15601v1</guid></item><item><title>A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2211.11760v3</link><description>In recent years, spiking neural networks (SNNs) have been used inreinforcement learning (RL) due to their low power consumption and event-drivenfeatures. However, spiking reinforcement learning (SRL), which suffers fromfixed coding methods, still faces the problems of high latency and poorversatility. In this paper, we use learnable matrix multiplication to encodeand decode spikes, improving the flexibility of the coders and thus reducinglatency. Meanwhile, we train the SNNs using the direct training method and usetwo different structures for online and offline RL algorithms, which gives ourmodel a wider range of applications. Extensive experiments have revealed thatour method achieves optimal performance with ultra-low latency (as low as 0.8%of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) indifferent algorithms and different environments.</description><author>Lang Qin, Rui Yan, Huajin Tang</author><pubDate>Wed, 24 Apr 2024 03:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11760v3</guid></item><item><title>A Lightweight Randomized Nonlinear Dictionary Learning Method using Random Vector Functional Link</title><link>http://arxiv.org/abs/2402.03833v2</link><description>Kernel-based nonlinear dictionary learning methods operate in a feature spaceobtained by an implicit feature map, and they are not independent ofcomputationally expensive operations like Singular Value Decomposition (SVD).This paper presents an SVD-free lightweight approach to learning a nonlineardictionary using a randomized functional link called a Random Vector FunctionalLink (RVFL). The proposed RVFL-based nonlinear Dictionary Learning (RVFLDL)learns a dictionary as a sparse-to-dense feature map from nonlinear sparsecoefficients to the dense input features. Sparse coefficients w.r.t an initialrandom dictionary are derived by assuming Horseshoe prior are used as inputsmaking it a lightweight network. Training the RVFL-based dictionary is freefrom SVD computation as RVFL generates weights from the input to the outputlayer analytically. Higher-order dependencies between the input sparsecoefficients and the dictionary atoms are incorporated into the trainingprocess by nonlinearly transforming the sparse coefficients and adding them asenhanced features. Thus the method projects sparse coefficients to a higherdimensional space while inducing nonlinearities into the dictionary. Forclassification using RVFL-net, a classifier matrix is learned as a transformthat maps nonlinear sparse coefficients to the labels. The empirical evidenceof the method illustrated in image classification and reconstructionapplications shows that RVFLDL is scalable and provides a solution better thanthose obtained using other nonlinear dictionary learning methods.</description><author>G. Madhuri, Atul Negi</author><pubDate>Wed, 24 Apr 2024 03:39:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03833v2</guid></item><item><title>The Open-World Lottery Ticket Hypothesis for OOD Intent Classification</title><link>http://arxiv.org/abs/2210.07071v3</link><description>Most existing methods of Out-of-Domain (OOD) intent classification rely onextensive auxiliary OOD corpora or specific training paradigms. However, theyare underdeveloped in the underlying principle that the models should havedifferentiated confidence in In- and Out-of-domain intent. In this work, weshed light on the fundamental cause of model overconfidence on OOD anddemonstrate that calibrated subnetworks can be uncovered by pruning theoverparameterized model. Calibrated confidence provided by the subnetwork canbetter distinguish In- and Out-of-domain, which can be a benefit for almost allpost hoc methods. In addition to bringing fundamental insights, we also extendthe Lottery Ticket Hypothesis to open-world scenarios. We conduct extensiveexperiments on four real-world datasets to demonstrate our approach canestablish consistent improvements compared with a suite of competitivebaselines.</description><author>Yunhua Zhou, Pengyu Wang, Peiju Liu, Yuxin Wang, Xipeng Qiu</author><pubDate>Wed, 24 Apr 2024 03:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07071v3</guid></item><item><title>A Multilevel Guidance-Exploration Network and Behavior-Scene Matching Method for Human Behavior Anomaly Detection</title><link>http://arxiv.org/abs/2312.04119v2</link><description>Human behavior anomaly detection aims to identify unusual human actions,playing a crucial role in intelligent surveillance and other areas. The currentmainstream methods still adopt reconstruction or future frame predictiontechniques. However, reconstructing or predicting low-level pixel featureseasily enables the network to achieve overly strong generalization ability,allowing anomalies to be reconstructed or predicted as effectively as normaldata. Different from their methods, inspired by the Student-Teacher Network, wepropose a novel framework called the Multilevel Guidance-ExplorationNetwork(MGENet), which detects anomalies through the difference in high-levelrepresentation between the Guidance and Exploration network. Specifically, wefirst utilize the pre-trained Normalizing Flow that takes skeletal keypoints asinput to guide an RGB encoder, which takes unmasked RGB frames as input, toexplore motion latent features. Then, the RGB encoder guides the mask encoder,which takes masked RGB frames as input, to explore the latent appearancefeature. Additionally, we design a Behavior-Scene Matching Module(BSMM) todetect scene-related behavioral anomalies. Extensive experiments demonstratethat our proposed method achieves state-of-the-art performance on ShanghaiTechand UBnormal datasets, with AUC of 86.9 % and 73.5 %, respectively. The codewill be available on https://github.com/molu-ggg/GENet.</description><author>Guoqing Yang, Zhiming Luo, Jianzhe Gao, Yingxin Lai, Kun Yang, Yifan He, Shaozi Li</author><pubDate>Wed, 24 Apr 2024 03:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04119v2</guid></item><item><title>Human-in-the-loop Learning for Dynamic Congestion Games</title><link>http://arxiv.org/abs/2404.15599v1</link><description>Today mobile users learn and share their traffic observations viacrowdsourcing platforms (e.g., Waze). Yet such platforms simply cater toselfish users' myopic interests to recommend the shortest path, and do notencourage enough users to travel and learn other paths for future others. Priorstudies focus on one-shot congestion games without considering users'information learning, while our work studies how users learn and alter trafficconditions on stochastic paths in a human-in-the-loop manner. Our analysisshows that the myopic routing policy leads to severe under-exploration ofstochastic paths. This results in a price of anarchy (PoA) greater than $2$, ascompared to the socially optimal policy in minimizing the long-term socialcost. Besides, the myopic policy fails to ensure the correct learningconvergence about users' traffic hazard beliefs. To address this, we focus oninformational (non-monetary) mechanisms as they are easier to implement thanpricing. We first show that existing information-hiding mechanisms anddeterministic path-recommendation mechanisms in Bayesian persuasion literaturedo not work with even (\text{PoA}=\infty). Accordingly, we propose a newcombined hiding and probabilistic recommendation (CHAR) mechanism to hide allinformation from a selected user group and provide state-dependentprobabilistic recommendations to the other user group. Our CHAR successfullyensures PoA less than (\frac{5}{4}), which cannot be further reduced by anyother informational (non-monetary) mechanism. Besides the parallel network, wefurther extend our analysis and CHAR to more general linear path graphs withmultiple intermediate nodes, and we prove that the PoA results remainunchanged. Additionally, we carry out experiments with real-world datasets tofurther extend our routing graphs and verify the close-to-optimal performanceof our CHAR.</description><author>Hongbo Li, Lingjie Duan</author><pubDate>Wed, 24 Apr 2024 03:23:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15599v1</guid></item><item><title>Federated Learning with Only Positive Labels by Exploring Label Correlations</title><link>http://arxiv.org/abs/2404.15598v1</link><description>Federated learning aims to collaboratively learn a model by using the datafrom multiple users under privacy constraints. In this paper, we study themulti-label classification problem under the federated learning setting, wheretrivial solution and extremely poor performance may be obtained, especiallywhen only positive data w.r.t. a single class label are provided for eachclient. This issue can be addressed by adding a specially designed regularizeron the server-side. Although effective sometimes, the label correlations aresimply ignored and thus sub-optimal performance may be obtained. Besides, it isexpensive and unsafe to exchange user's private embeddings between server andclients frequently, especially when training model in the contrastive way. Toremedy these drawbacks, we propose a novel and generic method termed FederatedAveraging by exploring Label Correlations (FedALC). Specifically, FedALCestimates the label correlations in the class embedding learning for differentlabel pairs and utilizes it to improve the model training. To further improvethe safety and also reduce the communication overhead, we propose a variant tolearn fixed class embedding for each client, so that the server and clientsonly need to exchange class embeddings once. Extensive experiments on multiplepopular datasets demonstrate that our FedALC can significantly outperformexisting counterparts.</description><author>Xuming An, Dui Wang, Li Shen, Yong Luo, Han Hu, Bo Du, Yonggang Wen, Dacheng Tao</author><pubDate>Wed, 24 Apr 2024 03:22:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15598v1</guid></item><item><title>GRSN: Gated Recurrent Spiking Neurons for POMDPs and MARL</title><link>http://arxiv.org/abs/2404.15597v1</link><description>Spiking neural networks (SNNs) are widely applied in various fields due totheir energy-efficient and fast-inference capabilities. Applying SNNs toreinforcement learning (RL) can significantly reduce the computational resourcerequirements for agents and improve the algorithm's performance underresource-constrained conditions. However, in current spiking reinforcementlearning (SRL) algorithms, the simulation results of multiple time steps canonly correspond to a single-step decision in RL. This is quite different fromthe real temporal dynamics in the brain and also fails to fully exploit thecapacity of SNNs to process temporal data. In order to address this temporalmismatch issue and further take advantage of the inherent temporal dynamics ofspiking neurons, we propose a novel temporal alignment paradigm (TAP) thatleverages the single-step update of spiking neurons to accumulate historicalstate information in RL and introduces gated units to enhance the memorycapacity of spiking neurons. Experimental results show that our method cansolve partially observable Markov decision processes (POMDPs) and multi-agentcooperation problems with similar performance as recurrent neural networks(RNNs) but with about 50% power consumption.</description><author>Lang Qin, Ziming Wang, Runhao Jiang, Rui Yan, Huajin Tang</author><pubDate>Wed, 24 Apr 2024 03:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15597v1</guid></item><item><title>Variational Deep Survival Machines: Survival Regression with Censored Outcomes</title><link>http://arxiv.org/abs/2404.15595v1</link><description>Survival regression aims to predict the time when an event of interest willtake place, typically a death or a failure. A fully parametric method [18] isproposed to estimate the survival function as a mixture of individualparametric distributions in the presence of censoring. In this paper, Wepresent a novel method to predict the survival time by better clustering thesurvival data and combine primitive distributions. We propose two variants ofvariational auto-encoder (VAE), discrete and continuous, to generate the latentvariables for clustering input covariates. The model is trained end to end byjointly optimizing the VAE loss and regression loss. Thorough experiments ondataset SUPPORT and FLCHAIN show that our method can effectively improve theclustering result and reach competitive scores with previous methods. Wedemonstrate the superior result of our model prediction in the long-term. Ourcode is available at https://github.com/qinzzz/auton-survival-785.</description><author>Qinxin Wang, Jiayuan Huang, Junhui Li, Jiaming Liu</author><pubDate>Wed, 24 Apr 2024 03:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15595v1</guid></item><item><title>A Survey of Deep Long-Tail Classification Advancements</title><link>http://arxiv.org/abs/2404.15593v1</link><description>Many data distributions in the real world are hardly uniform. Instead, skewedand long-tailed distributions of various kinds are commonly observed. Thisposes an interesting problem for machine learning, where most algorithms assumeor work well with uniformly distributed data. The problem is furtherexacerbated by current state-of-the-art deep learning models requiring largevolumes of training data. As such, learning from imbalanced data remains achallenging research problem and a problem that must be solved as we movetowards more real-world applications of deep learning. In the context of classimbalance, state-of-the-art (SOTA) accuracies on standard benchmark datasetsfor classification typically fall less than 75%, even for less challengingdatasets such as CIFAR100. Nonetheless, there has been progress in this nichearea of deep learning. To this end, in this survey, we provide a taxonomy ofvarious methods proposed for addressing the problem of long-tailclassification, focusing on works that happened in the last few years under asingle mathematical framework. We also discuss standard performance metrics,convergence studies, feature distribution and classifier analysis. We alsoprovide a quantitative comparison of the performance of different SOTA methodsand conclude the survey by discussing the remaining challenges and futureresearch direction.</description><author>Charika de Alvis, Suranga Seneviratne</author><pubDate>Wed, 24 Apr 2024 02:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15593v1</guid></item><item><title>DEFT: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection</title><link>http://arxiv.org/abs/2310.16776v4</link><description>Recent advances have led to the availability of many pre-trained languagemodels (PLMs); however, a question that remains is how much data is trulyneeded to fine-tune PLMs for downstream tasks? In this work, we introduceDEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervisedcore-set selection to identify a smaller, representative dataset that reducesthe amount of data needed to fine-tune PLMs for downstream tasks. We examinethe efficacy of DEFT-UCS in the context of text-editing LMs, and compare to thestate-of-the art text-editing model, CoEDIT. Our results demonstrate thatDEFT-UCS models are just as accurate as CoEDIT, across eight different datasetsconsisting of six different editing tasks, while finetuned on 70% less data.</description><author>Devleena Das, Vivek Khetan</author><pubDate>Wed, 24 Apr 2024 02:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16776v4</guid></item><item><title>ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction</title><link>http://arxiv.org/abs/2404.15592v1</link><description>Existing datasets for attribute value extraction (AVE) predominantly focus onexplicit attribute values while neglecting the implicit ones, lack productimages, are often not publicly available, and lack an in-depth human inspectionacross diverse domains. To address these limitations, we present ImplicitAVE,the first, publicly available multimodal dataset for implicit attribute valueextraction. ImplicitAVE, sourced from the MAVE dataset, is carefully curatedand expanded to include implicit AVE and multimodality, resulting in a refineddataset of 68k training and 1.6k testing data across five domains. We alsoexplore the application of multimodal large language models (MLLMs) to implicitAVE, establishing a comprehensive benchmark for MLLMs on the ImplicitAVEdataset. Six recent MLLMs with eleven variants are evaluated across diversesettings, revealing that implicit value extraction remains a challenging taskfor MLLMs. The contributions of this work include the development and releaseof ImplicitAVE, and the exploration and benchmarking of various MLLMs forimplicit AVE, providing valuable insights and potential future researchdirections. Dataset and code are available athttps://github.com/HenryPengZou/ImplicitAVE</description><author>Henry Peng Zou, Vinay Samuel, Yue Zhou, Weizhi Zhang, Liancheng Fang, Zihe Song, Philip S. Yu, Cornelia Caragea</author><pubDate>Wed, 24 Apr 2024 02:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15592v1</guid></item><item><title>Domain Adaptation for Learned Image Compression with Supervised Adapters</title><link>http://arxiv.org/abs/2404.15591v1</link><description>In Learned Image Compression (LIC), a model is trained at encoding anddecoding images sampled from a source domain, often outperforming traditionalcodecs on natural images; yet its performance may be far from optimal on imagessampled from different domains. In this work, we tackle the problem of adaptinga pre-trained model to multiple target domains by plugging into the decoder anadapter module for each of them, including the source one. Each adapterimproves the decoder performance on a specific domain, without the modelforgetting about the images seen at training time. A gate network computes theweights to optimally blend the contributions from the adapters when thebitstream is decoded. We experimentally validate our method over twostate-of-the-art pre-trained models, observing improved rate-distortionefficiency on the target domains without penalties on the source domain.Furthermore, the gate's ability to find similarities with the learned targetdomains enables better encoding efficiency also for images outside them.</description><author>Alberto Presta, Gabriele Spadaro, Enzo Tartaglione, Attilio Fiandrotti, Marco Grangetto</author><pubDate>Wed, 24 Apr 2024 02:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15591v1</guid></item><item><title>Minimal Evidence Group Identification for Claim Verification</title><link>http://arxiv.org/abs/2404.15588v1</link><description>Claim verification in real-world settings (e.g. against a large collection ofcandidate evidences retrieved from the web) typically requires identifying andaggregating a complete set of evidence pieces that collectively provide fullsupport to the claim. The problem becomes particularly challenging when thereexists distinct sets of evidence that could be used to verify the claim fromdifferent perspectives. In this paper, we formally define and study the problemof identifying such minimal evidence groups (MEGs) for claim verification. Weshow that MEG identification can be reduced from Set Cover problem, based onentailment inference of whether a given evidence group provides full/partialsupport to a claim. Our proposed approach achieves 18.4% and 34.8% absoluteimprovements on the WiCE and SciFact datasets over LLM prompting. Finally, wedemonstrate the benefits of MEGs in downstream applications such as claimgeneration.</description><author>Xiangci Li, Sihao Chen, Rajvi Kapadia, Jessica Ouyang, Fan Zhang</author><pubDate>Wed, 24 Apr 2024 02:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15588v1</guid></item><item><title>Brain Storm Optimization Based Swarm Learning for Diabetic Retinopathy Image Classification</title><link>http://arxiv.org/abs/2404.15585v1</link><description>The application of deep learning techniques to medical problems has garneredwidespread research interest in recent years, such as applying convolutionalneural networks to medical image classification tasks. However, data in themedical field is often highly private, preventing different hospitals fromsharing data to train an accurate model. Federated learning, as aprivacy-preserving machine learning architecture, has shown promisingperformance in balancing data privacy and model utility by keeping private dataon the client's side and using a central server to coordinate a set of clientsfor model training through aggregating their uploaded model parameters. Yet,this architecture heavily relies on a trusted third-party server, which ischallenging to achieve in real life. Swarm learning, as a specializeddecentralized federated learning architecture that does not require a centralserver, utilizes blockchain technology to enable direct parameter exchangesbetween clients. However, the mining of blocks requires significantcomputational resources, limiting its scalability. To address this issue, thispaper integrates the brain storm optimization algorithm into the swarm learningframework, named BSO-SL. This approach clusters similar clients into differentgroups based on their model distributions. Additionally, leveraging thearchitecture of BSO, clients are given the probability to engage incollaborative learning both within their cluster and with clients outside theircluster, preventing the model from converging to local optima. The proposedmethod has been validated on a real-world diabetic retinopathy imageclassification dataset, and the experimental results demonstrate theeffectiveness of the proposed approach.</description><author>Liang Qu, Cunze Wang, Yuhui Shi</author><pubDate>Wed, 24 Apr 2024 02:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15585v1</guid></item><item><title>Multi-Agent Reinforcement Learning for Energy Networks: Computational Challenges, Progress and Open Problems</title><link>http://arxiv.org/abs/2404.15583v1</link><description>The rapidly changing architecture and functionality of electrical networksand the increasing penetration of renewable and distributed energy resourceshave resulted in various technological and managerial challenges. These haverendered traditional centralized energy-market paradigms insufficient due totheir inability to support the dynamic and evolving nature of the network. Thissurvey explores how multi-agent reinforcement learning (MARL) can support thedecentralization and decarbonization of energy networks and mitigate the 12associated challenges. This is achieved by specifying key computationalchallenges in managing energy networks, reviewing recent research progress onaddressing them, and highlighting open challenges that may be addressed usingMARL.</description><author>Sarah Keren, Chaimaa Essayeh, Stefano V. Albrecht, Thomas Mortsyn</author><pubDate>Wed, 24 Apr 2024 02:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15583v1</guid></item><item><title>Embed-Search-Align: DNA Sequence Alignment using Transformer Models</title><link>http://arxiv.org/abs/2309.11087v4</link><description>DNA sequence alignment involves assigning short DNA reads to the mostprobable locations on an extensive reference genome. This process is crucialfor various genomic analyses, including variant calling, transcriptomics, andepigenomics. Conventional methods, refined over decades, tackle this challengein two steps: genome indexing followed by efficient search to locate likelypositions for given reads. Building on the success of Large Language Models(LLM) in encoding text into embeddings, where the distance metric capturessemantic similarity, recent efforts have explored whether the same Transformerarchitecture can produce numerical representations for DNA sequences. Suchmodels have shown early promise in tasks involving classification of short DNAsequences, such as the detection of coding vs non-coding regions, as well asthe identification of enhancer and promoter sequences. Performance at sequenceclassification tasks does not, however, translate to sequence alignment, whereit is necessary to conduct a genome-wide search to successfully align everyread. We address this open problem by framing it as an Embed-Search-Align task.In this framework, a novel encoder model DNA-ESA generates representations ofreads and fragments of the reference, which are projected into a shared vectorspace where the read-fragment distance is used as surrogate for alignment. Inparticular, DNA-ESA introduces: (1) Contrastive loss for self-supervisedtraining of DNA sequence representations, facilitating rich sequence-levelembeddings, and (2) a DNA vector store to enable search across fragments on aglobal scale. DNA-ESA is &gt;97% accurate when aligning 250-length reads onto ahuman reference genome of 3 gigabases (single-haploid), far exceeds theperformance of 6 recent DNA-Transformer model baselines and shows task transferacross chromosomes and species.</description><author>Pavan Holur, K. C. Enevoldsen, Shreyas Rajesh, Lajoyce Mboning, Thalia Georgiou, Louis-S. Bouchard, Matteo Pellegrini, Vwani Roychowdhury</author><pubDate>Wed, 24 Apr 2024 02:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11087v4</guid></item><item><title>Comparison of Methods in Human Skin Decomposition</title><link>http://arxiv.org/abs/2404.00552v2</link><description>Decomposition of skin pigment plays an important role in medical fields.Human skin can be decomposed into two primitive components, hemoglobin andmelanin. It is our goal to apply these results for diagnosis of skin cancer. Inthis paper, various methods for skin pigment decomposition are reviewedcomparatively and the performance of each method is evaluated boththeoretically and experimentally. In addition, isometric feature mapping(Isomap) is introduced in order to improve the dimensionality reductionperformance in context of skin decomposition.</description><author>Hao Gong, Michel Desvignes</author><pubDate>Wed, 24 Apr 2024 02:16:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00552v2</guid></item><item><title>Adaptive Prompt Learning with Negative Textual Semantics and Uncertainty Modeling for Universal Multi-Source Domain Adaptation</title><link>http://arxiv.org/abs/2404.14696v2</link><description>Universal Multi-source Domain Adaptation (UniMDA) transfers knowledge frommultiple labeled source domains to an unlabeled target domain under domainshifts (different data distribution) and class shifts (unknown target classes).Existing solutions focus on excavating image features to detect unknownsamples, ignoring abundant information contained in textual semantics. In thispaper, we propose an Adaptive Prompt learning with Negative textual semanticsand uncErtainty modeling method based on Contrastive Language-ImagePre-training (APNE-CLIP) for UniMDA classification tasks. Concretely, weutilize the CLIP with adaptive prompts to leverage textual information of classsemantics and domain representations, helping the model identify unknownsamples and address domain shifts. Additionally, we design a novel globalinstance-level alignment objective by utilizing negative textual semantics toachieve more precise image-text pair alignment. Furthermore, we propose anenergy-based uncertainty modeling strategy to enlarge the margin distancebetween known and unknown samples. Extensive experiments demonstrate thesuperiority of our proposed method.</description><author>Yuxiang Yang, Lu Wen, Yuanyuan Xu, Jiliu Zhou, Yan Wang</author><pubDate>Wed, 24 Apr 2024 02:14:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14696v2</guid></item><item><title>MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image Analysis</title><link>http://arxiv.org/abs/2404.15580v1</link><description>The Vision Transformer (ViT) has demonstrated remarkable performance inSelf-Supervised Learning (SSL) for 3D medical image analysis. Mask AutoEncoder(MAE) for feature pre-training can further unleash the potential of ViT onvarious medical vision tasks. However, due to large spatial sizes with muchhigher dimensions of 3D medical images, the lack of hierarchical design for MAEmay hinder the performance of downstream tasks. In this paper, we propose anovel \textit{Mask in Mask (MiM)} pre-training framework for 3D medical images,which aims to advance MAE by learning discriminative representation fromhierarchical visual tokens across varying scales. We introduce multiple levelsof granularity for masked inputs from the volume, which are then reconstructedsimultaneously ranging at both fine and coarse levels. Additionally, across-level alignment mechanism is applied to adjacent level volumes to enforceanatomical similarity hierarchically. Furthermore, we adopt a hybrid backboneto enhance the hierarchical representation learning efficiently during thepre-training. MiM was pre-trained on a large scale of available 3D volumetricimages, \textit{i.e.,} Computed Tomography (CT) images containing various bodyparts. Extensive experiments on thirteen public datasets demonstrate thesuperiority of MiM over other SSL methods in organ/lesion/tumor segmentationand disease classification. We further scale up the MiM to large pre-trainingdatasets with more than 10k volumes, showing that large-scale pre-training canfurther enhance the performance of downstream tasks. The improvement alsoconcluded that the research community should pay more attention to the scale ofthe pre-training dataset towards the healthcare foundation model for 3D medicalimages.</description><author>Jiaxin Zhuang, Linshan Wu, Qiong Wang, Varut Vardhanabhuti, Lin Luo, Hao Chen</author><pubDate>Wed, 24 Apr 2024 02:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15580v1</guid></item><item><title>ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Image</title><link>http://arxiv.org/abs/2310.17994v2</link><description>We introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel viewsynthesis for in-the-wild scenes. While existing methods are designed forsingle objects with masked backgrounds, we propose new techniques to addresschallenges introduced by in-the-wild multi-object scenes with complexbackgrounds. Specifically, we train a generative prior on a mixture of datasources that capture object-centric, indoor, and outdoor scenes. To addressissues from data mixture such as depth-scale ambiguity, we propose a novelcamera conditioning parameterization and normalization scheme. Further, weobserve that Score Distillation Sampling (SDS) tends to truncate thedistribution of complex backgrounds during distillation of 360-degree scenes,and propose "SDS anchoring" to improve the diversity of synthesized novelviews. Our model sets a new state-of-the-art result in LPIPS on the DTU datasetin the zero-shot setting, even outperforming methods specifically trained onDTU. We further adapt the challenging Mip-NeRF 360 dataset as a new benchmarkfor single-image novel view synthesis, and demonstrate strong performance inthis setting. Our code and data are at http://kylesargent.github.io/zeronvs/</description><author>Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, Jiajun Wu</author><pubDate>Wed, 24 Apr 2024 02:08:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17994v2</guid></item><item><title>Learning with Unmasked Tokens Drives Stronger Vision Learners</title><link>http://arxiv.org/abs/2310.13593v2</link><description>Masked image modeling (MIM) has become a leading self-supervised learningstrategy. MIMs such as Masked Autoencoder (MAE) learn strong representations byrandomly masking input tokens for the encoder to process, with the decoderreconstructing the masked tokens to the input. However, MIM pre-trainedencoders often exhibit a limited attention span, attributed to MIM's sole focuson regressing masked tokens only, which may impede the encoder's broadercontext learning. To tackle the limitation, we improve MIM by explicitlyincorporating unmasked tokens into the training process. Specifically, ourmethod enables the encoder to learn from broader context supervision, allowingunmasked tokens to experience broader contexts while the decoder reconstructsmasked tokens. Thus, the encoded unmasked tokens are equipped with extensivecontextual information, empowering masked tokens to leverage the enhancedunmasked tokens for MIM. As a result, our simple remedy trains morediscriminative representations revealed by achieving 84.2% top-1 accuracy withViT-B on ImageNet-1K with 0.6%p gain. We attribute the success to the enhancedpre-training method, as evidenced by the singular value spectrum and attentionanalyses. Finally, our models achieve significant performance gains at thedownstream semantic segmentation and fine-grained visual classification tasks;and on diverse robust evaluation metrics. Code is available athttps://github.com/naver-ai/lut</description><author>Taekyung Kim, Sanghyuk Chun, Byeongho Heo, Dongyoon Han</author><pubDate>Wed, 24 Apr 2024 01:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13593v2</guid></item><item><title>Can Foundational Large Language Models Assist with Conducting Pharmaceuticals Manufacturing Investigations?</title><link>http://arxiv.org/abs/2404.15578v1</link><description>General purpose Large Language Models (LLM) such as the Generative PretrainedTransformer (GPT) and Large Language Model Meta AI (LLaMA) have attracted muchattention in recent years. There is strong evidence that these models canperform remarkably well in various natural language processing tasks. However,how to leverage them to approach domain-specific use cases and drive valueremains an open question. In this work, we focus on a specific use case,pharmaceutical manufacturing investigations, and propose that leveraginghistorical records of manufacturing incidents and deviations in an organizationcan be beneficial for addressing and closing new cases, or de-risking newmanufacturing campaigns. Using a small but diverse dataset of realmanufacturing deviations selected from different product lines, we evaluate andquantify the power of three general purpose LLMs (GPT-3.5, GPT-4, and Claude-2)in performing tasks related to the above goal. In particular, (1) the abilityof LLMs in automating the process of extracting specific information such asroot cause of a case from unstructured data, as well as (2) the possibility ofidentifying similar or related deviations by performing semantic search on thedatabase of historical records are examined. While our results point to thehigh accuracy of GPT-4 and Claude-2 in the information extraction task, wediscuss cases of complex interplay between the apparent reasoning andhallucination behavior of LLMs as a risk factor. Furthermore, we show thatsemantic search on vector embedding of deviation descriptions can be used toidentify similar records, such as those with a similar type of defect, with ahigh level of accuracy. We discuss further improvements to enhance the accuracyof similar record identification.</description><author>Hossein Salami, Brandye Smith-Goettler, Vijay Yadav</author><pubDate>Wed, 24 Apr 2024 01:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15578v1</guid></item><item><title>Seeing Text in the Dark: Algorithm and Benchmark</title><link>http://arxiv.org/abs/2404.08965v3</link><description>Localizing text in low-light environments is challenging due to visualdegradations. Although a straightforward solution involves a two-stage pipelinewith low-light image enhancement (LLE) as the initial step followed bydetector, LLE is primarily designed for human vision instead of machine and canaccumulate errors. In this work, we propose an efficient and effectivesingle-stage approach for localizing text in dark that circumvents the need forLLE. We introduce a constrained learning module as an auxiliary mechanismduring the training stage of the text detector. This module is designed toguide the text detector in preserving textual spatial features amidst featuremap resizing, thus minimizing the loss of spatial information in texts underlow-light visual degradations. Specifically, we incorporate spatialreconstruction and spatial semantic constraints within this module to ensurethe text detector acquires essential positional and contextual range knowledge.Our approach enhances the original text detector's ability to identify text'slocal topological features using a dynamic snake feature pyramid network andadopts a bottom-up contour shaping strategy with a novel rectangularaccumulation technique for accurate delineation of streamlined text features.In addition, we present a comprehensive low-light dataset for arbitrary-shapedtext, encompassing diverse scenes and languages. Notably, our method achievesstate-of-the-art results on this low-light dataset and exhibits comparableperformance on standard normal light datasets. The code and dataset will bereleased.</description><author>Chengpei Xu, Hao Fu, Long Ma, Wenjing Jia, Chengqi Zhang, Feng Xia, Xiaoyu Ai, Binghao Li, Wenjie Zhang</author><pubDate>Wed, 24 Apr 2024 01:40:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08965v3</guid></item><item><title>Retrieval Head Mechanistically Explains Long-Context Factuality</title><link>http://arxiv.org/abs/2404.15574v1</link><description>Despite the recent progress in long-context language models, it remainselusive how transformer-based models exhibit the capability to retrieverelevant information from arbitrary locations within the long context. Thispaper aims to address this question. Our systematic investigation across a widespectrum of models reveals that a special type of attention heads are largelyresponsible for retrieving information, which we dub retrieval heads. Weidentify intriguing properties of retrieval heads:(1) universal: all theexplored models with long-context capability have a set of retrieval heads; (2)sparse: only a small portion (less than 5\%) of the attention heads areretrieval. (3) intrinsic: retrieval heads already exist in models pretrainedwith short context. When extending the context length by continual pretraining,it is still the same set of heads that perform information retrieval. (4)dynamically activated: take Llama-2 7B for example, 12 retrieval heads alwaysattend to the required information no matter how the context is changed. Therest of the retrieval heads are activated in different contexts. (5) causal:completely pruning retrieval heads leads to failure in retrieving relevantinformation and results in hallucination, while pruning random non-retrievalheads does not affect the model's retrieval ability. We further show thatretrieval heads strongly influence chain-of-thought (CoT) reasoning, where themodel needs to frequently refer back the question and previously-generatedcontext. Conversely, tasks where the model directly generates the answer usingits intrinsic knowledge are less impacted by masking out retrieval heads. Theseobservations collectively explain which internal part of the model seeksinformation from the input tokens. We believe our insights will foster futureresearch on reducing hallucination, improving reasoning, and compressing the KVcache.</description><author>Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, Yao Fu</author><pubDate>Wed, 24 Apr 2024 01:24:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15574v1</guid></item><item><title>Structure-Guided Image Completion with Image-level and Object-level Semantic Discriminators</title><link>http://arxiv.org/abs/2212.06310v2</link><description>Structure-guided image completion aims to inpaint a local region of an imageaccording to an input guidance map from users. While such a task enables manypractical applications for interactive editing, existing methods often struggleto hallucinate realistic object instances in complex natural scenes. Such alimitation is partially due to the lack of semantic-level constraints insidethe hole region as well as the lack of a mechanism to enforce realistic objectgeneration. In this work, we propose a learning paradigm that consists ofsemantic discriminators and object-level discriminators for improving thegeneration of complex semantics and objects. Specifically, the semanticdiscriminators leverage pretrained visual features to improve the realism ofthe generated visual concepts. Moreover, the object-level discriminators takealigned instances as inputs to enforce the realism of individual objects. Ourproposed scheme significantly improves the generation quality and achievesstate-of-the-art results on various tasks, including segmentation-guidedcompletion, edge-guided manipulation and panoptically-guided manipulation onPlaces2 datasets. Furthermore, our trained model is flexible and can supportmultiple editing use cases, such as object insertion, replacement, removal andstandard inpainting. In particular, our trained model combined with a novelautomatic image completion pipeline achieves state-of-the-art results on thestandard inpainting task.</description><author>Haitian Zheng, Zhe Lin, Jingwan Lu, Scott Cohen, Eli Shechtman, Connelly Barnes, Jianming Zhang, Qing Liu, Yuqian Zhou, Sohrab Amirghodsi, Jiebo Luo</author><pubDate>Wed, 24 Apr 2024 01:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.06310v2</guid></item><item><title>Utility-Fairness Trade-Offs and How to Find Them</title><link>http://arxiv.org/abs/2404.09454v2</link><description>When building classification systems with demographic fairnessconsiderations, there are two objectives to satisfy: 1) maximizing utility forthe specific task and 2) ensuring fairness w.r.t. a known demographicattribute. These objectives often compete, so optimizing both can lead to atrade-off between utility and fairness. While existing works acknowledge thetrade-offs and study their limits, two questions remain unanswered: 1) What arethe optimal trade-offs between utility and fairness? and 2) How can wenumerically quantify these trade-offs from data for a desired prediction taskand demographic attribute of interest? This paper addresses these questions. Weintroduce two utility-fairness trade-offs: the Data-Space and Label-SpaceTrade-off. The trade-offs reveal three regions within the utility-fairnessplane, delineating what is fully and partially possible and impossible. Wepropose U-FaTE, a method to numerically quantify the trade-offs for a givenprediction task and group fairness definition from data samples. Based on thetrade-offs, we introduce a new scheme for evaluating representations. Anextensive evaluation of fair representation learning methods andrepresentations from over 1000 pre-trained models revealed that most currentapproaches are far from the estimated and achievable fairness-utilitytrade-offs across multiple datasets and prediction tasks.</description><author>Sepehr Dehdashtian, Bashir Sadeghi, Vishnu Naresh Boddeti</author><pubDate>Wed, 24 Apr 2024 01:08:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09454v2</guid></item><item><title>Understanding Addition in Transformers</title><link>http://arxiv.org/abs/2310.13121v9</link><description>Understanding the inner workings of machine learning models like Transformersis vital for their safe and ethical use. This paper provides a comprehensiveanalysis of a one-layer Transformer model trained to perform n-digit integeraddition. Our findings suggest that the model dissects the task into parallelstreams dedicated to individual digits, employing varied algorithms tailored todifferent positions within the digits. Furthermore, we identify a rare scenariocharacterized by high loss, which we explain. By thoroughly elucidating themodel's algorithm, we provide new insights into its functioning. These findingsare validated through rigorous testing and mathematical modeling, therebycontributing to the broader fields of model understanding and interpretability.Our approach opens the door for analyzing more complex tasks and multi-layerTransformer models.</description><author>Philip Quirke, Fazl Barez</author><pubDate>Wed, 24 Apr 2024 00:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13121v9</guid></item><item><title>CASPR: Automated Evaluation Metric for Contrastive Summarization</title><link>http://arxiv.org/abs/2404.15565v1</link><description>Summarizing comparative opinions about entities (e.g., hotels, phones) from aset of source reviews, often referred to as contrastive summarization, canconsiderably aid users in decision making. However, reliably measuring thecontrastiveness of the output summaries without relying on human evaluationsremains an open problem. Prior work has proposed token-overlap based metrics,Distinctiveness Score, to measure contrast which does not take into account thesensitivity to meaning-preserving lexical variations. In this work, we proposean automated evaluation metric CASPR to better measure contrast between a pairof summaries. Our metric is based on a simple and light-weight method thatleverages natural language inference (NLI) task to measure contrast bysegmenting reviews into single-claim sentences and carefully aggregating NLIscores between them to come up with a summary-level score. We compare CASPRwith Distinctiveness Score and a simple yet powerful baseline based onBERTScore. Our results on a prior dataset CoCoTRIP demonstrate that CASPR canmore reliably capture the contrastiveness of the summary pairs compared to thebaselines.</description><author>Nirupan Ananthamurugan, Dat Duong, Philip George, Ankita Gupta, Sandeep Tata, Beliz Gunel</author><pubDate>Wed, 24 Apr 2024 00:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15565v1</guid></item><item><title>Guided AbsoluteGrad: Magnitude of Gradients Matters to Explanation's Localization and Saliency</title><link>http://arxiv.org/abs/2404.15564v1</link><description>This paper proposes a new gradient-based XAI method called GuidedAbsoluteGrad for saliency map explanations. We utilize both positive andnegative gradient magnitudes and employ gradient variance to distinguish theimportant areas for noise deduction. We also introduce a novel evaluationmetric named ReCover And Predict (RCAP), which considers the Localization andVisual Noise Level objectives of the explanations. We propose two propositionsfor these two objectives and prove the necessity of evaluating them. Weevaluate Guided AbsoluteGrad with seven gradient-based XAI methods using theRCAP metric and other SOTA metrics in three case studies: (1) ImageNet datasetwith ResNet50 model; (2) International Skin Imaging Collaboration (ISIC)dataset with EfficientNet model; (3) the Places365 dataset with DenseNet161model. Our method surpasses other gradient-based approaches, showcasing thequality of enhanced saliency map explanations through gradient magnitude.</description><author>Jun Huang, Yan Liu</author><pubDate>Wed, 24 Apr 2024 00:26:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15564v1</guid></item><item><title>Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation</title><link>http://arxiv.org/abs/2306.16699v3</link><description>Implicit Neural Representation (INR) is an innovative approach forrepresenting complex shapes or objects without explicitly defining theirgeometry or surface structure. Instead, INR represents objects as continuousfunctions. Previous research has demonstrated the effectiveness of using neuralnetworks as INR for image compression, showcasing comparable performance totraditional methods such as JPEG. However, INR holds potential for variousapplications beyond image compression. This paper introduces Rapid-INR, a novelapproach that utilizes INR for encoding and compressing images, therebyaccelerating neural network training in computer vision tasks. Our methodologyinvolves storing the whole dataset directly in INR format on a GPU, mitigatingthe significant data communication overhead between the CPU and GPU duringtraining. Additionally, the decoding process from INR to RGB format is highlyparallelized and executed on-the-fly. To further enhance compression, wepropose iterative and dynamic pruning, as well as layer-wise quantization,building upon previous work. We evaluate our framework on the imageclassification task, utilizing the ResNet-18 backbone network and threecommonly used datasets with varying image sizes. Rapid-INR reduces memoryconsumption to only about 5% of the original dataset size in RGB format andachieves a maximum 6$\times$ speedup over the PyTorch training pipeline, aswell as a maximum 1.2x speedup over the DALI training pipeline, with only amarginal decrease in accuracy. Importantly, Rapid-INR can be readily applied toother computer vision tasks and backbone networks with reasonable engineeringefforts. Our implementation code is publicly available athttps://github.com/sharc-lab/Rapid-INR.</description><author>Hanqiu Chen, Hang Yang, Stephen Fitzmeyer, Cong Hao</author><pubDate>Wed, 24 Apr 2024 00:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16699v3</guid></item><item><title>Large Language Models in Biomedical and Health Informatics: A Bibliometric Review</title><link>http://arxiv.org/abs/2403.16303v3</link><description>Large Language Models (LLMs) have rapidly become important tools inBiomedical and Health Informatics (BHI), enabling new ways to analyze data,treat patients, and conduct research. This bibliometric review aims to providea panoramic view of how LLMs have been used in BHI by examining researcharticles and collaboration networks from 2022 to 2023. It further explores howLLMs can improve Natural Language Processing (NLP) applications in various BHIareas like medical diagnosis, patient engagement, electronic health recordmanagement, and personalized medicine. To do this, our bibliometric reviewidentifies key trends, maps out research networks, and highlights majordevelopments in this fast-moving field. Lastly, it discusses the ethicalconcerns and practical challenges of using LLMs in BHI, such as data privacyand reliable medical recommendations. Looking ahead, we consider how LLMs couldfurther transform biomedical research as well as healthcare delivery andpatient outcomes. This bibliometric review serves as a resource forstakeholders in healthcare, including researchers, clinicians, andpolicymakers, to understand the current state and future potential of LLMs inBHI.</description><author>Huizi Yu, Lizhou Fan, Lingyao Li, Jiayan Zhou, Zihui Ma, Lu Xian, Wenyue Hua, Sijia He, Mingyu Jin, Yongfeng Zhang, Ashvin Gandhi, Xin Ma</author><pubDate>Wed, 24 Apr 2024 00:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16303v3</guid></item><item><title>Can LLM-Generated Misinformation Be Detected?</title><link>http://arxiv.org/abs/2309.13788v5</link><description>The advent of Large Language Models (LLMs) has made a transformative impact.However, the potential that LLMs such as ChatGPT can be exploited to generatemisinformation has posed a serious concern to online safety and public trust. Afundamental research question is: will LLM-generated misinformation cause moreharm than human-written misinformation? We propose to tackle this question fromthe perspective of detection difficulty. We first build a taxonomy ofLLM-generated misinformation. Then we categorize and validate the potentialreal-world methods for generating misinformation with LLMs. Then, throughextensive empirical investigation, we discover that LLM-generatedmisinformation can be harder to detect for humans and detectors compared tohuman-written misinformation with the same semantics, which suggests it canhave more deceptive styles and potentially cause more harm. We also discuss theimplications of our discovery on combating misinformation in the age of LLMsand the countermeasures.</description><author>Canyu Chen, Kai Shu</author><pubDate>Tue, 23 Apr 2024 23:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13788v5</guid></item><item><title>Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures</title><link>http://arxiv.org/abs/2402.01055v3</link><description>There has been much interest in recent years in learning good classifiersfrom data with noisy labels. Most work on learning from noisy labels hasfocused on standard loss-based performance measures. However, many machinelearning problems require using non-decomposable performance measures whichcannot be expressed as the expectation or sum of a loss on individual examples;these include for example the H-mean, Q-mean and G-mean in class imbalancesettings, and the Micro $F_1$ in information retrieval. In this paper, wedesign algorithms to learn from noisy labels for two broad classes ofmulticlass non-decomposable performance measures, namely, monotonic convex andratio-of-linear, which encompass all the above examples. Our work builds on theFrank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In bothcases, we develop noise-corrected versions of the algorithms under the widelystudied family of class-conditional noise models. We provide regret (excessrisk) bounds for our algorithms, establishing that even though they are trainedon noisy data, they are Bayes consistent in the sense that their performanceconverges to the optimal performance w.r.t. the clean (non-noisy) distribution.Our experiments demonstrate the effectiveness of our algorithms in handlinglabel noise.</description><author>Mingyuan Zhang, Shivani Agarwal</author><pubDate>Tue, 23 Apr 2024 23:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01055v3</guid></item><item><title>Cross-Temporal Spectrogram Autoencoder (CTSAE): Unsupervised Dimensionality Reduction for Clustering Gravitational Wave Glitches</title><link>http://arxiv.org/abs/2404.15552v1</link><description>The advancement of The Laser Interferometer Gravitational-Wave Observatory(LIGO) has significantly enhanced the feasibility and reliability ofgravitational wave detection. However, LIGO's high sensitivity makes itsusceptible to transient noises known as glitches, which necessitate effectivedifferentiation from real gravitational wave signals. Traditional approachespredominantly employ fully supervised or semi-supervised algorithms for thetask of glitch classification and clustering. In the future task of identifyingand classifying glitches across main and auxiliary channels, it is impracticalto build a dataset with manually labeled ground-truth. In addition, thepatterns of glitches can vary with time, generating new glitches without manuallabels. In response to this challenge, we introduce the Cross-TemporalSpectrogram Autoencoder (CTSAE), a pioneering unsupervised method for thedimensionality reduction and clustering of gravitational wave glitches. CTSAEintegrates a novel four-branch autoencoder with a hybrid of ConvolutionalNeural Networks (CNN) and Vision Transformers (ViT). To further extractfeatures across multi-branches, we introduce a novel multi-branch fusion methodusing the CLS (Class) token. Our model, trained and evaluated on the GravitySpyO3 dataset on the main channel, demonstrates superior performance in clusteringtasks when compared to state-of-the-art semi-supervised learning methods. Tothe best of our knowledge, CTSAE represents the first unsupervised approachtailored specifically for clustering LIGO data, marking a significant stepforward in the field of gravitational wave research. The code of this paper isavailable at https://github.com/Zod-L/CTSAE</description><author>Yi Li, Yunan Wu, Aggelos K. Katsaggelos</author><pubDate>Tue, 23 Apr 2024 23:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15552v1</guid></item><item><title>PRISM: Patient Records Interpretation for Semantic Clinical Trial Matching using Large Language Models</title><link>http://arxiv.org/abs/2404.15549v1</link><description>Clinical trial matching is the task of identifying trials for which patientsmay be potentially eligible. Typically, this task is labor-intensive andrequires detailed verification of patient electronic health records (EHRs)against the stringent inclusion and exclusion criteria of clinical trials. Thisprocess is manual, time-intensive, and challenging to scale up, resulting inmany patients missing out on potential therapeutic options. Recent advancementsin Large Language Models (LLMs) have made automating patient-trial matchingpossible, as shown in multiple concurrent research studies. However, thecurrent approaches are confined to constrained, often synthetic datasets thatdo not adequately mirror the complexities encountered in real-world medicaldata. In this study, we present the first, end-to-end large-scale empiricalevaluation of clinical trial matching using real-world EHRs. Our studyshowcases the capability of LLMs to accurately match patients with appropriateclinical trials. We perform experiments with proprietary LLMs, including GPT-4and GPT-3.5, as well as our custom fine-tuned model called OncoLLM and showthat OncoLLM, despite its significantly smaller size, not only outperformsGPT-3.5 but also matches the performance of qualified medical doctors. Allexperiments were carried out on real-world EHRs that include clinical notes andavailable clinical trials from a single cancer center in the United States.</description><author>Shashi Kant Gupta, Aditya Basu, Mauro Nievas, Jerrin Thomas, Nathan Wolfrath, Adhitya Ramamurthi, Bradley Taylor, Anai N. Kothari, Therica M. Miller, Sorena Nadaf-Rahrov, Yanshan Wang, Hrituraj Singh</author><pubDate>Tue, 23 Apr 2024 23:33:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15549v1</guid></item><item><title>Springs and a stopwatch: neural units with time-dependent multifunctionality</title><link>http://arxiv.org/abs/2404.15545v1</link><description>Several branches of computing use a system's physical dynamics to docomputation. We show that the dynamics of an underdamped harmonic oscillatorcan perform multifunctional computation, solving distinct problems at distincttimes within a single dynamical trajectory. Oscillator computing usuallyfocuses on the oscillator's phase as the information-carrying component. Herewe focus on the time-resolved amplitude of an oscillator whose inputs influenceits frequency, which has a natural parallel as the activity of a time-dependentneural unit. Because the activity of the unit at fixed time is a nonmonotonicfunction of the input, the unit can solve nonlinearly-separable problems suchas XOR. Because the activity of the unit at fixed input is a nonmonotonicfunction of time, the unit is multifunctional in a temporal sense, able tocarry out distinct nonlinear computations at distinct times within the samedynamical trajectory. Time-resolved computing of this nature can be done in orout of equilibrium, with the natural time evolution of the system giving usmultiple computations for the price of one.</description><author>Stephen Whitelam</author><pubDate>Tue, 23 Apr 2024 23:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15545v1</guid></item><item><title>Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance</title><link>http://arxiv.org/abs/2310.03722v3</link><description>In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$of a Gaussian distribution with unknown variance $\sigma^2$. Curiously, heemployed both an improper (right Haar) mixture over $\sigma$ and an improper(flat) mixture over $\mu$. Here, we elaborate carefully on the details of hisconstruction, which use generalized nonintegrable martingales and an extendedVille's inequality. While this does yield a sequential t-test, it does notyield an "e-process" (due to the nonintegrability of his martingale). In thispaper, we develop two new e-processes and confidence sequences for the samesetting: one is a test martingale in a reduced filtration, while the other isan e-process in the canonical data filtration. These are respectively obtainedby swapping Lai's flat mixture for a Gaussian mixture, and swapping the rightHaar mixture over $\sigma$ with the maximum likelihood estimate under the null,as done in universal inference. We also analyze the width of resultingconfidence sequences, which have a curious polynomial dependence on the errorprobability $\alpha$ that we prove to be not only unavoidable, but (foruniversal inference) even better than the classical fixed-sample t-test.Numerical experiments are provided along the way to compare and contrast thevarious approaches, including some recent suboptimal ones.</description><author>Hongjian Wang, Aaditya Ramdas</author><pubDate>Tue, 23 Apr 2024 23:00:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03722v3</guid></item><item><title>DreamCraft: Text-Guided Generation of Functional 3D Environments in Minecraft</title><link>http://arxiv.org/abs/2404.15538v1</link><description>Procedural Content Generation (PCG) algorithms enable the automaticgeneration of complex and diverse artifacts. However, they don't providehigh-level control over the generated content and typically require domainexpertise. In contrast, text-to-3D methods allow users to specify desiredcharacteristics in natural language, offering a high amount of flexibility andexpressivity. But unlike PCG, such approaches cannot guarantee functionality,which is crucial for certain applications like game design. In this paper, wepresent a method for generating functional 3D artifacts from free-form textprompts in the open-world game Minecraft. Our method, DreamCraft, trainsquantized Neural Radiance Fields (NeRFs) to represent artifacts that, whenviewed in-game, match given text descriptions. We find that DreamCraft producesmore aligned in-game artifacts than a baseline that post-processes the outputof an unconstrained NeRF. Thanks to the quantized representation of theenvironment, functional constraints can be integrated using specialized lossterms. We show how this can be leveraged to generate 3D structures that match atarget distribution or obey certain adjacency rules over the block types.DreamCraft inherits a high degree of expressivity and controllability from theNeRF, while still being able to incorporate functional constraints throughdomain-specific objectives.</description><author>Sam Earle, Filippos Kokkinos, Yuhe Nie, Julian Togelius, Roberta Raileanu</author><pubDate>Tue, 23 Apr 2024 22:57:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15538v1</guid></item><item><title>BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis</title><link>http://arxiv.org/abs/2404.15532v1</link><description>This paper presents BattleAgent, an emulation system that combines the LargeVision-Language Model and Multi-agent System. This novel system aims tosimulate complex dynamic interactions among multiple agents, as well as betweenagents and their environments, over a period of time. It emulates both thedecision-making processes of leaders and the viewpoints of ordinaryparticipants, such as soldiers. The emulation showcases the currentcapabilities of agents, featuring fine-grained multi-modal interactions betweenagents and landscapes. It develops customizable agent structures to meetspecific situational requirements, for example, a variety of battle-relatedactivities like scouting and trench digging. These components collaborate torecreate historical events in a lively and comprehensive manner while offeringinsights into the thoughts and feelings of individuals from diverse viewpoints.The technological foundations of BattleAgent establish detailed and immersivesettings for historical battles, enabling individual agents to partake in,observe, and dynamically respond to evolving battle scenarios. This methodologyholds the potential to substantially deepen our understanding of historicalevents, particularly through individual accounts. Such initiatives can also aidhistorical research, as conventional historical narratives often lackdocumentation and prioritize the perspectives of decision-makers, therebyoverlooking the experiences of ordinary individuals. BattelAgent illustratesAI's potential to revitalize the human aspect in crucial social events, therebyfostering a more nuanced collective understanding and driving the progressivedevelopment of human society.</description><author>Shuhang Lin, Wenyue Hua, Lingyao Li, Che-Jui Chang, Lizhou Fan, Jianchao Ji, Hang Hua, Mingyu Jin, Jiebo Luo, Yongfeng Zhang</author><pubDate>Tue, 23 Apr 2024 22:37:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15532v1</guid></item><item><title>Unsupervised Solution Operator Learning for Mean-Field Games via Sampling-Invariant Parametrizations</title><link>http://arxiv.org/abs/2401.15482v2</link><description>Recent advances in deep learning has witnessed many innovative frameworksthat solve high dimensional mean-field games (MFG) accurately and efficiently.These methods, however, are restricted to solving single-instance MFG anddemands extensive computational time per instance, limiting practicality. Toovercome this, we develop a novel framework to learn the MFG solution operator.Our model takes a MFG instances as input and output their solutions with oneforward pass. To ensure the proposed parametrization is well-suited foroperator learning, we introduce and prove the notion of sampling invariance forour model, establishing its convergence to a continuous operator in thesampling limit. Our method features two key advantages. First, it isdiscretization-free, making it particularly suitable for learning operators ofhigh-dimensional MFGs. Secondly, it can be trained without the need for accessto supervised labels, significantly reducing the computational overheadassociated with creating training datasets in existing operator learningmethods. We test our framework on synthetic and realistic datasets with varyingcomplexity and dimensionality to substantiate its robustness.</description><author>Han Huang, Rongjie Lai</author><pubDate>Tue, 23 Apr 2024 22:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15482v2</guid></item><item><title>Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications</title><link>http://arxiv.org/abs/2404.13506v2</link><description>The rise of deep learning has marked significant progress in fields such ascomputer vision, natural language processing, and medical imaging, primarilythrough the adaptation of pre-trained models for specific tasks. Traditionalfine-tuning methods, involving adjustments to all parameters, face challengesdue to high computational and memory demands. This has led to the developmentof Parameter Efficient Fine-Tuning (PEFT) techniques, which selectively updateparameters to balance computational efficiency with performance. This reviewexamines PEFT approaches, offering a detailed comparison of various strategieshighlighting applications across different domains, including text generation,medical imaging, protein modeling, and speech synthesis. By assessing theeffectiveness of PEFT methods in reducing computational load, speeding uptraining, and lowering memory usage, this paper contributes to making deeplearning more accessible and adaptable, facilitating its wider application andencouraging innovation in model optimization. Ultimately, the paper aims tocontribute towards insights into PEFT's evolving landscape, guiding researchersand practitioners in overcoming the limitations of conventional fine-tuningapproaches.</description><author>Charith Chandra Sai Balne, Sreyoshi Bhaduri, Tamoghna Roy, Vinija Jain, Aman Chadha</author><pubDate>Tue, 23 Apr 2024 22:28:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13506v2</guid></item><item><title>Understanding Hyperbolic Metric Learning through Hard Negative Sampling</title><link>http://arxiv.org/abs/2404.15523v1</link><description>In recent years, there has been a growing trend of incorporating hyperbolicgeometry methods into computer vision. While these methods have achievedstate-of-the-art performance on various metric learning tasks using hyperbolicdistance measurements, the underlying theoretical analysis supporting thissuperior performance remains under-exploited. In this study, we investigate theeffects of integrating hyperbolic space into metric learning, particularly whentraining with contrastive loss. We identify a need for a comprehensivecomparison between Euclidean and hyperbolic spaces regarding the temperatureeffect in the contrastive loss within the existing literature. To address thisgap, we conduct an extensive investigation to benchmark the results of VisionTransformers (ViTs) using a hybrid objective function that combines loss fromEuclidean and hyperbolic spaces. Additionally, we provide a theoreticalanalysis of the observed performance improvement. We also reveal thathyperbolic metric learning is highly related to hard negative sampling,providing insights for future work. This work will provide valuable data pointsand experience in understanding hyperbolic image embeddings. To shed more lighton problem-solving and encourage further investigation into our approach, ourcode is available online (https://github.com/YunYunY/HypMix).</description><author>Yun Yue, Fangzhou Lin, Guanyi Mou, Ziming Zhang</author><pubDate>Tue, 23 Apr 2024 22:11:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15523v1</guid></item><item><title>Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models</title><link>http://arxiv.org/abs/2404.15522v1</link><description>Recently developed large language models (LLMs) have been shown to performremarkably well on a wide range of language understanding tasks. But, can theyreally "reason" over the natural language? This question has been receivingsignificant research attention and many reasoning skills such as commonsense,numerical, and qualitative have been studied. However, the crucial skillpertaining to 'logical reasoning' has remained underexplored. Existing workinvestigating this reasoning ability of LLMs has focused only on a couple ofinference rules (such as modus ponens and modus tollens) of propositional andfirst-order logic. Addressing the above limitation, we comprehensively evaluatethe logical reasoning ability of LLMs on 25 different reasoning patternsspanning over propositional, first-order, and non-monotonic logics. To enablesystematic evaluation, we introduce LogicBench, a natural languagequestion-answering dataset focusing on the use of a single inference rule. Weconduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini,Llama-2, and Mistral using chain-of-thought prompting. Experimental resultsshow that existing LLMs do not fare well on LogicBench; especially, theystruggle with instances involving complex reasoning and negations. Furthermore,they sometimes overlook contextual information necessary for reasoning toarrive at the correct conclusion. We believe that our work and findingsfacilitate future research for evaluating and enhancing the logical reasoningability of LLMs. Data and code are available athttps://github.com/Mihir3009/LogicBench.</description><author>Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral</author><pubDate>Tue, 23 Apr 2024 22:08:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15522v1</guid></item><item><title>An adaptation of InfoMap to absorbing random walks using absorption-scaled graphs</title><link>http://arxiv.org/abs/2112.10953v4</link><description>InfoMap is a popular approach to detect densely connected "communities" ofnodes in networks. To detect such communities, InfoMap uses random walks andideas from information theory. Motivated by the dynamics of disease spread onnetworks, whose nodes can have heterogeneous disease-removal rates, we adaptInfoMap to absorbing random walks. To do this, we use absorption-scaled graphs(in which edge weights are scaled according to absorption rates) and Markovtime sweeping. One of our adaptations of InfoMap converges to the standardversion of InfoMap in the limit in which the node-absorption rates approach$0$. We demonstrate that the community structure that one obtains using ouradaptations of InfoMap can differ markedly from the community structure thatone detects using methods that do not account for node-absorption rates. Wealso illustrate that the community structure that is induced by heterogeneousabsorption rates can have important implications forsusceptible-infected-recovered (SIR) dynamics on ring-lattice networks. Forexample, in some situations, the outbreak duration is maximized when a moderatenumber of nodes have large node-absorption rates.</description><author>Esteban Vargas Bernal, Mason A. Porter, Joseph H. Tien</author><pubDate>Tue, 23 Apr 2024 22:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.10953v4</guid></item><item><title>An MRP Formulation for Supervised Learning: Generalized Temporal Difference Learning Models</title><link>http://arxiv.org/abs/2404.15518v1</link><description>In traditional statistical learning, data points are usually assumed to beindependently and identically distributed (i.i.d.) following an unknownprobability distribution. This paper presents a contrasting viewpoint,perceiving data points as interconnected and employing a Markov reward process(MRP) for data modeling. We reformulate the typical supervised learning as anon-policy policy evaluation problem within reinforcement learning (RL),introducing a generalized temporal difference (TD) learning algorithm as aresolution. Theoretically, our analysis draws connections between the solutionsof linear TD learning and ordinary least squares (OLS). We also show that underspecific conditions, particularly when noises are correlated, the TD's solutionproves to be a more effective estimator than OLS. Furthermore, we establish theconvergence of our generalized TD algorithms under linear functionapproximation. Empirical studies verify our theoretical results, examine thevital design of our TD algorithm and show practical utility across variousdatasets, encompassing tasks such as regression and image classification withdeep learning.</description><author>Yangchen Pan, Junfeng Wen, Chenjun Xiao, Philip Torr</author><pubDate>Tue, 23 Apr 2024 22:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15518v1</guid></item><item><title>SegFormer3D: an Efficient Transformer for 3D Medical Image Segmentation</title><link>http://arxiv.org/abs/2404.10156v2</link><description>The adoption of Vision Transformers (ViTs) based architectures represents asignificant advancement in 3D Medical Image (MI) segmentation, surpassingtraditional Convolutional Neural Network (CNN) models by enhancing globalcontextual understanding. While this paradigm shift has significantly enhanced3D segmentation performance, state-of-the-art architectures require extremelylarge and complex architectures with large scale computing resources fortraining and deployment. Furthermore, in the context of limited datasets, oftenencountered in medical imaging, larger models can present hurdles in both modelgeneralization and convergence. In response to these challenges and todemonstrate that lightweight models are a valuable area of research in 3Dmedical imaging, we present SegFormer3D, a hierarchical Transformer thatcalculates attention across multiscale volumetric features. Additionally,SegFormer3D avoids complex decoders and uses an all-MLP decoder to aggregatelocal and global attention features to produce highly accurate segmentationmasks. The proposed memory efficient Transformer preserves the performancecharacteristics of a significantly larger model in a compact design.SegFormer3D democratizes deep learning for 3D medical image segmentation byoffering a model with 33x less parameters and a 13x reduction in GFLOPScompared to the current state-of-the-art (SOTA). We benchmark SegFormer3Dagainst the current SOTA models on three widely used datasets Synapse, BRaTs,and ACDC, achieving competitive results. Code:https://github.com/OSUPCVLab/SegFormer3D.git</description><author>Shehan Perera, Pouyan Navard, Alper Yilmaz</author><pubDate>Tue, 23 Apr 2024 22:00:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10156v2</guid></item><item><title>Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval</title><link>http://arxiv.org/abs/2404.15516v1</link><description>Composed Image Retrieval (CIR) is a task that retrieves images similar to aquery, based on a provided textual modification. Current techniques rely onsupervised learning for CIR models using labeled triplets of the referenceimage, text, target image. These specific triplets are not as commonlyavailable as simple image-text pairs, limiting the widespread use of CIR andits scalability. On the other hand, zero-shot CIR can be relatively easilytrained with image-caption pairs without considering the image-to-imagerelation, but this approach tends to yield lower accuracy. We propose a newsemi-supervised CIR approach where we search for a reference and its relatedtarget images in auxiliary data and learn our large language model-based VisualDelta Generator (VDG) to generate text describing the visual difference (i.e.,visual delta) between the two. VDG, equipped with fluent language knowledge andbeing model agnostic, can generate pseudo triplets to boost the performance ofCIR models. Our approach significantly improves the existing supervisedlearning approaches and achieves state-of-the-art results on the CIRbenchmarks.</description><author>Young Kyun Jang, Donghyun Kim, Zihang Meng, Dat Huynh, Ser-Nam Lim</author><pubDate>Tue, 23 Apr 2024 22:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15516v1</guid></item><item><title>ToM-LM: Delegating Theory Of Mind Reasoning to External Symbolic Executors in Large Language Models</title><link>http://arxiv.org/abs/2404.15515v1</link><description>Theory of Mind (ToM) refers to the ability of individuals to attribute mentalstates to others. While Large Language Models (LLMs) have shown some promisewith ToM ability, they still struggle with complex ToM reasoning. Our approachleverages an external symbolic executor, specifically the SMCDEL model checker,and fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,an LLM is first fine-tuned through pairs of natural language and symbolicformulation representation of ToM problems and is then instructed to generatethe symbolic formulation with a one-shot in-context example. The generatedsymbolic formulation is then executed by the SMCDEL model checker to performtransparent and verifiable ToM reasoning and give the final result. Wedemonstrate that our approach, ToM-LM, shows a significant improvement over allthe constructed baselines. Our study proposes a novel view about externalizinga particular component of ToM reasoning, mainly reasoning about beliefs, andsuggests generalizing it to other aspects of ToM reasoning.</description><author>Weizhi Tang, Vaishak Belle</author><pubDate>Tue, 23 Apr 2024 21:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15515v1</guid></item><item><title>NeuraChip: Accelerating GNN Computations with a Hash-based Decoupled Spatial Accelerator</title><link>http://arxiv.org/abs/2404.15510v1</link><description>Graph Neural Networks (GNNs) are emerging as a formidable tool for processingnon-euclidean data across various domains, ranging from social network analysisto bioinformatics. Despite their effectiveness, their adoption has not beenpervasive because of scalability challenges associated with large-scale graphdatasets, particularly when leveraging message passing. To tackle these challenges, we introduce NeuraChip, a novel GNN spatialaccelerator based on Gustavson's algorithm. NeuraChip decouples themultiplication and addition computations in sparse matrix multiplication. Thisseparation allows for independent exploitation of their unique datadependencies, facilitating efficient resource allocation. We introduce arolling eviction strategy to mitigate data idling in on-chip memory as well asaddress the prevalent issue of memory bloat in sparse graph computations.Furthermore, the compute resource load balancing is achieved through a dynamicreseeding hash-based mapping, ensuring uniform utilization of computingresources agnostic of sparsity patterns. Finally, we present NeuraSim, anopen-source, cycle-accurate, multi-threaded, modular simulator forcomprehensive performance analysis. Overall, NeuraChip presents a significant improvement, yielding an averagespeedup of 22.1x over Intel's MKL, 17.1x over NVIDIA's cuSPARSE, 16.7x overAMD's hipSPARSE, and 1.5x over prior state-of-the-art SpGEMM accelerator and1.3x over GNN accelerator. The source code for our open-sourced simulator andperformance visualizer is publicly accessible on GitHub https://neurachip.us</description><author>Kaustubh Shivdikar, Nicolas Bohm Agostini, Malith Jayaweera, Gilbert Jonatan, Jose L. Abellan, Ajay Joshi, John Kim, David Kaeli</author><pubDate>Tue, 23 Apr 2024 21:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15510v1</guid></item><item><title>FedGreen: Carbon-aware Federated Learning with Model Size Adaptation</title><link>http://arxiv.org/abs/2404.15503v1</link><description>Federated learning (FL) provides a promising collaborative framework to builda model from distributed clients, and this work investigates the carbonemission of the FL process. Cloud and edge servers hosting FL clients mayexhibit diverse carbon footprints influenced by their geographical locationswith varying power sources, offering opportunities to reduce carbon emissionsby training local models with adaptive computations and communications. In thispaper, we propose FedGreen, a carbon-aware FL approach to efficiently trainmodels by adopting adaptive model sizes shared with clients based on theircarbon profiles and locations using ordered dropout as a model compressiontechnique. We theoretically analyze the trade-offs between the produced carbonemissions and the convergence accuracy, considering the carbon intensitydiscrepancy across countries to choose the parameters optimally. Empiricalstudies show that FedGreen can substantially reduce the carbon footprints of FLcompared to the state-of-the-art while maintaining competitive model accuracy.</description><author>Ali Abbasi, Fan Dong, Xin Wang, Henry Leung, Jiayu Zhou, Steve Drew</author><pubDate>Tue, 23 Apr 2024 21:37:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15503v1</guid></item><item><title>Runtime Stealthy Perception Attacks against DNN-based Adaptive Cruise Control Systems</title><link>http://arxiv.org/abs/2307.08939v3</link><description>Adaptive Cruise Control (ACC) is a widely used driver assistance technologyfor maintaining the desired speed and safe distance to the leading vehicle.This paper evaluates the security of the deep neural network (DNN) based ACCsystems under runtime stealthy perception attacks that strategically injectperturbations into camera data to cause forward collisions. We present acontext-aware strategy for the selection of the most critical times fortriggering the attacks and a novel optimization-based method for the adaptivegeneration of image perturbations at runtime. We evaluate the effectiveness ofthe proposed attack using an actual vehicle, a publicly available drivingdataset, and a realistic simulation platform with the control software from aproduction ACC system, a physical-world driving simulator, and interventions bythe human driver and safety features such as Advanced Emergency Braking System(AEBS). Experimental results show that the proposed attack achieves 142.9 timeshigher success rate in causing hazards and 89.6% higher evasion rate thanbaselines, while being stealthy and robust to real-world factors and dynamicchanges in the environment. This study highlights the role of human drivers andbasic safety mechanisms in preventing attacks.</description><author>Xugui Zhou, Anqi Chen, Maxfield Kouzel, Haotian Ren, Morgan McCarty, Cristina Nita-Rotaru, Homa Alemzadeh</author><pubDate>Tue, 23 Apr 2024 21:33:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08939v3</guid></item><item><title>Killkan: The Automatic Speech Recognition Dataset for Kichwa with Morphosyntactic Information</title><link>http://arxiv.org/abs/2404.15501v1</link><description>This paper presents Killkan, the first dataset for automatic speechrecognition (ASR) in the Kichwa language, an indigenous language of Ecuador.Kichwa is an extremely low-resource endangered language, and there have been noresources before Killkan for Kichwa to be incorporated in applications ofnatural language processing. The dataset contains approximately 4 hours ofaudio with transcription, translation into Spanish, and morphosyntacticannotation in the format of Universal Dependencies. The audio data wasretrieved from a publicly available radio program in Kichwa. This paper alsoprovides corpus-linguistic analyses of the dataset with a special focus on theagglutinative morphology of Kichwa and frequent code-switching with Spanish.The experiments show that the dataset makes it possible to develop the firstASR system for Kichwa with reliable quality despite its small dataset size.This dataset, the ASR model, and the code used to develop them will be publiclyavailable. Thus, our study positively showcases resource building and itsapplications for low-resource languages and their community.</description><author>Chihiro Taguchi, Jefferson Saransig, Dayana Velásquez, David Chiang</author><pubDate>Tue, 23 Apr 2024 21:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15501v1</guid></item><item><title>GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots</title><link>http://arxiv.org/abs/2404.15500v1</link><description>Geospatial Copilots unlock unprecedented potential for performing EarthObservation (EO) applications through natural language instructions. However,existing agents rely on overly simplified single tasks and template-basedprompts, creating a disconnect with real-world scenarios. In this work, wepresent GeoLLM-Engine, an environment for tool-augmented agents with intricatetasks routinely executed by analysts on remote sensing platforms. We enrich ourenvironment with geospatial API tools, dynamic maps/UIs, and externalmultimodal knowledge bases to properly gauge an agent's proficiency ininterpreting realistic high-level natural language commands and its functionalcorrectness in task completions. By alleviating overheads typically associatedwith human-in-the-loop benchmark curation, we harness our massively parallelengine across 100 GPT-4-Turbo nodes, scaling to over half a million diversemulti-tool tasks and across 1.1 million satellite images. By moving beyondtraditional single-task image-caption paradigms, we investigatestate-of-the-art agents and prompting techniques against long-horizon prompts.</description><author>Simranjit Singh, Michael Fore, Dimitrios Stamoulis</author><pubDate>Tue, 23 Apr 2024 21:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15500v1</guid></item><item><title>Deep-learning Optical Flow Outperforms PIV in Obtaining Velocity Fields from Active Nematics</title><link>http://arxiv.org/abs/2404.15497v1</link><description>Deep learning-based optical flow (DLOF) extracts features in adjacent videoframes with deep convolutional neural networks. It uses those features toestimate the inter-frame motions of objects at the pixel level. In thisarticle, we evaluate the ability of optical flow to quantify the spontaneousflows of MT-based active nematics under different labeling conditions. Wecompare DLOF against the commonly used technique, particle imaging velocimetry(PIV). We obtain flow velocity ground truths either by performingsemi-automated particle tracking on samples with sparsely labeled filaments, orfrom passive tracer beads. We find that DLOF produces significantly moreaccurate velocity fields than PIV for densely labeled samples. We show that thebreakdown of PIV arises because the algorithm cannot reliably distinguishcontrast variations at high densities, particularly in directions parallel tothe nematic director. DLOF overcomes this limitation. For sparsely labeledsamples, DLOF and PIV produce results with similar accuracy, but DLOF giveshigher-resolution fields. Our work establishes DLOF as a versatile tool formeasuring fluid flows in a broad class of active, soft, and biophysicalsystems.</description><author>Phu N. Tran, Sattvic Ray, Linnea Lemma, Yunrui Li, Reef Sweeney, Aparna Baskaran, Zvonimir Dogic, Pengyu Hong, Michael F. Hagan</author><pubDate>Tue, 23 Apr 2024 21:20:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15497v1</guid></item><item><title>Multi-scale Intervention Planning based on Generative Design</title><link>http://arxiv.org/abs/2404.15492v1</link><description>The scarcity of green spaces, in urban environments, consists a criticalchallenge. There are multiple adverse effects, impacting the health andwell-being of the citizens. Small scale interventions, e.g. pocket parks, is aviable solution, but comes with multiple constraints, involving the design andimplementation over a specific area. In this study, we harness the capabilitiesof generative AI for multi-scale intervention planning, focusing on naturebased solutions. By leveraging image-to-image and image inpainting algorithms,we propose a methodology to address the green space deficit in urban areas.Focusing on two alleys in Thessaloniki, where greenery is lacking, wedemonstrate the efficacy of our approach in visualizing NBS interventions. Ourfindings underscore the transformative potential of emerging technologies inshaping the future of urban intervention planning processes.</description><author>Ioannis Kavouras, Ioannis Rallis, Emmanuel Sardis, Eftychios Protopapadakis, Anastasios Doulamis, Nikolaos Doulamis</author><pubDate>Tue, 23 Apr 2024 21:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15492v1</guid></item><item><title>IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection &amp; Correction Task On the Shoulders of Medical Agents</title><link>http://arxiv.org/abs/2404.15488v1</link><description>In natural language processing applied to the clinical domain, utilizinglarge language models has emerged as a promising avenue for error detection andcorrection on clinical notes, a knowledge-intensive task for which annotateddata is scarce. This paper presents MedReAct'N'MedReFlex, which leverages asuite of four LLM-based medical agents. The MedReAct agent initiates theprocess by observing, analyzing, and taking action, generating trajectories toguide the search to target a potential error in the clinical notes.Subsequently, the MedEval agent employs five evaluators to assess the targetederror and the proposed correction. In cases where MedReAct's actions proveinsufficient, the MedReFlex agent intervenes, engaging in reflective analysisand proposing alternative strategies. Finally, the MedFinalParser agent formatsthe final output, preserving the original style while ensuring the integrity ofthe error correction process. One core component of our method is our RAGpipeline based on our ClinicalCorp corpora. Among other well-known sourcescontaining clinical guidelines and information, we preprocess and release theopen-source MedWiki dataset for clinical RAG application. Our resultsdemonstrate the central role of our RAG approach with ClinicalCorp leveragedthrough the MedReAct'N'MedReFlex framework. It achieved the ninth rank on theMEDIQA-CORR 2024 final leaderboard.</description><author>Jean-Philippe Corbeil</author><pubDate>Tue, 23 Apr 2024 21:00:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15488v1</guid></item><item><title>Large Language Models Spot Phishing Emails with Surprising Accuracy: A Comparative Analysis of Performance</title><link>http://arxiv.org/abs/2404.15485v1</link><description>Phishing, a prevalent cybercrime tactic for decades, remains a significantthreat in today's digital world. By leveraging clever social engineeringelements and modern technology, cybercrime targets many individuals,businesses, and organizations to exploit trust and security. Thesecyber-attackers are often disguised in many trustworthy forms to appear aslegitimate sources. By cleverly using psychological elements like urgency,fear, social proof, and other manipulative strategies, phishers can lureindividuals into revealing sensitive and personalized information. Building onthis pervasive issue within modern technology, this paper aims to analyze theeffectiveness of 15 Large Language Models (LLMs) in detecting phishingattempts, specifically focusing on a randomized set of "419 Scam" emails. Theobjective is to determine which LLMs can accurately detect phishing emails byanalyzing a text file containing email metadata based on predefined criteria.The experiment concluded that the following models, ChatGPT 3.5,GPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detectingphishing emails.</description><author>Het Patel, Umair Rehman, Farkhund Iqbal</author><pubDate>Tue, 23 Apr 2024 20:55:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15485v1</guid></item><item><title>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</title><link>http://arxiv.org/abs/2306.00978v4</link><description>Large language models (LLMs) have fundamentally transformed the capabilitiesof numerous applications, from natural language processing to more intricatedomain-specific tasks in robotics and autonomous driving. Moreover, theimportance of on-device LLMs has grown significantly in the recent years.Running LLMs on edge devices not only promises reduced latency and improveduser experience but also aligns with the increasing need for user privacy, asdata processing can occur locally. However, the astronomical model sizes ofmodern LLMs and constraints of the edge devices, primarily in terms of memorysize and bandwidth, pose significant deployment challenges. In this paper, wepropose Activation-aware Weight Quantization (AWQ), a hardware-friendlyapproach for LLM low-bit weight-only quantization. Our method is based on theobservation that weights are not equally important: protecting only 1% ofsalient weights can greatly reduce quantization error. We then propose tosearch for the optimal per-channel scaling that protects the salient weights byobserving the activation, not weights. AWQ does not rely on any backpropagationor reconstruction, so it can well preserve LLMs' generalization ability ondifferent domains and modalities, without overfitting to the calibration set.AWQ outperforms existing work on various language modeling and domain-specificbenchmarks (coding and math). Thanks to better generalization, it achievesexcellent quantization performance for instruction-tuned LMs and, for the firsttime, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient andflexible inference framework tailored for on-device LLM/VLMs, offering morethan 3x speedup over the Huggingface FP16 implementation on both desktop andmobile GPUs. It also democratizes the deployment of the 70B Llama-2 model onmobile GPUs.</description><author>Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han</author><pubDate>Tue, 23 Apr 2024 20:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00978v4</guid></item><item><title>A Non-Expert's Introduction to Data Ethics for Mathematicians</title><link>http://arxiv.org/abs/2201.07794v4</link><description>I give a short introduction to data ethics. I begin with some backgroundinformation and societal context for data ethics. I then discuss data ethics inmathematical-science education and indicate some available course material. Ibriefly highlight a few efforts -- at my home institution and elsewhere -- ondata ethics, society, and social good. I then discuss open data in research,research replicability and some other ethical issues in research, and thetension between privacy and open data and code, and a few controversial studiesand reactions to studies. I then discuss ethical principles, institutionalreview boards, and a few other considerations in the scientific use of humandata. I then briefly survey a variety of research and lay articles that arerelevant to data ethics and data privacy. I conclude with a brief summary andsome closing remarks. My focal audience is mathematicians, but I hope that this chapter will alsobe useful to others. I am not an expert about data ethics, and this chapterprovides only a starting point on this wide-ranging topic. I encourage you toexamine the resources that I discuss and to reflect carefully on data ethics,its role in mathematics education, and the societal implications of data anddata analysis. As data and technology continue to evolve, I hope that suchcareful reflection will continue throughout your life.</description><author>Mason A. Porter</author><pubDate>Tue, 23 Apr 2024 20:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.07794v4</guid></item><item><title>AV-RIR: Audio-Visual Room Impulse Response Estimation</title><link>http://arxiv.org/abs/2312.00834v2</link><description>Accurate estimation of Room Impulse Response (RIR), which captures anenvironment's acoustic properties, is important for speech processing and AR/VRapplications. We propose AV-RIR, a novel multi-modal multi-task learningapproach to accurately estimate the RIR from a given reverberant speech signaland the visual cues of its corresponding environment. AV-RIR builds on a novelneural codec-based architecture that effectively captures environment geometryand materials properties and solves speech dereverberation as an auxiliary taskby using multi-task learning. We also propose Geo-Mat features that augmentmaterial information into visual cues and CRIP that improves late reverberationcomponents in the estimated RIR via image-to-RIR retrieval by 86%. Empiricalresults show that AV-RIR quantitatively outperforms previous audio-only andvisual-only approaches by achieving 36% - 63% improvement across variousacoustic metrics in RIR estimation. Additionally, it also achieves higherpreference scores in human evaluation. As an auxiliary benefit, dereverbedspeech from AV-RIR shows competitive performance with the state-of-the-art invarious spoken language processing tasks and outperforms reverberation timeerror score in the real-world AVSpeech dataset. Qualitative examples of bothsynthesized reverberant speech and enhanced speech can be found athttps://www.youtube.com/watch?v=tTsKhviukAE.</description><author>Anton Ratnarajah, Sreyan Ghosh, Sonal Kumar, Purva Chiniya, Dinesh Manocha</author><pubDate>Tue, 23 Apr 2024 20:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00834v2</guid></item><item><title>KDAS: Knowledge Distillation via Attention Supervision Framework for Polyp Segmentation</title><link>http://arxiv.org/abs/2312.08555v3</link><description>Polyp segmentation, a contentious issue in medical imaging, has seen numerousproposed methods aimed at improving the quality of segmented masks. Whilecurrent state-of-the-art techniques yield impressive results, the size andcomputational cost of these models create challenges for practical industryapplications. To address this challenge, we present KDAS, a KnowledgeDistillation framework that incorporates attention supervision, and ourproposed Symmetrical Guiding Module. This framework is designed to facilitate acompact student model with fewer parameters, allowing it to learn the strengthsof the teacher model and mitigate the inconsistency between teacher featuresand student features, a common challenge in Knowledge Distillation, via theSymmetrical Guiding Module. Through extensive experiments, our compact modelsdemonstrate their strength by achieving competitive results withstate-of-the-art methods, offering a promising approach to creating compactmodels with high accuracy for polyp segmentation and in the medical imagingfield. The implementation is available on https://github.com/huyquoctrinh/KDAS.</description><author>Quoc-Huy Trinh, Minh-Van Nguyen, Phuoc-Thao Vo Thi</author><pubDate>Tue, 23 Apr 2024 20:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08555v3</guid></item><item><title>Training all-mechanical neural networks for task learning through in situ backpropagation</title><link>http://arxiv.org/abs/2404.15471v1</link><description>Recent advances unveiled physical neural networks as promising machinelearning platforms, offering faster and more energy-efficient informationprocessing. Compared with extensively-studied optical neural networks, thedevelopment of mechanical neural networks (MNNs) remains nascent and facessignificant challenges, including heavy computational demands and learning withapproximate gradients. Here, we introduce the mechanical analogue of in situbackpropagation to enable highly efficient training of MNNs. We demonstratethat the exact gradient can be obtained locally in MNNs, enabling learningthrough their immediate vicinity. With the gradient information, we showcasethe successful training of MNNs for behavior learning and machine learningtasks, achieving high accuracy in regression and classification. Furthermore,we present the retrainability of MNNs involving task-switching and damage,demonstrating the resilience. Our findings, which integrate the theory fortraining MNNs and experimental and numerical validations, pave the way formechanical machine learning hardware and autonomous self-learning materialsystems.</description><author>Shuaifeng Li, Xiaoming Mao</author><pubDate>Tue, 23 Apr 2024 20:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15471v1</guid></item><item><title>IDD-X: A Multi-View Dataset for Ego-relative Important Object Localization and Explanation in Dense and Unstructured Traffic</title><link>http://arxiv.org/abs/2404.08561v2</link><description>Intelligent vehicle systems require a deep understanding of the interplaybetween road conditions, surrounding entities, and the ego vehicle's drivingbehavior for safe and efficient navigation. This is particularly critical indeveloping countries where traffic situations are often dense and unstructuredwith heterogeneous road occupants. Existing datasets, predominantly gearedtowards structured and sparse traffic scenarios, fall short of capturing thecomplexity of driving in such environments. To fill this gap, we present IDD-X,a large-scale dual-view driving video dataset. With 697K bounding boxes, 9Kimportant object tracks, and 1-12 objects per video, IDD-X offers comprehensiveego-relative annotations for multiple important road objects covering 10categories and 19 explanation label categories. The dataset also incorporatesrearview information to provide a more complete representation of the drivingenvironment. We also introduce custom-designed deep networks aimed at multipleimportant object localization and per-object explanation prediction. Overall,our dataset and introduced prediction models form the foundation for studyinghow road conditions and surrounding entities affect driving behavior in complextraffic situations.</description><author>Chirag Parikh, Rohit Saluja, C. V. Jawahar, Ravi Kiran Sarvadevabhatla</author><pubDate>Tue, 23 Apr 2024 20:19:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08561v2</guid></item><item><title>Private Optimal Inventory Policy Learning for Feature-based Newsvendor with Unknown Demand</title><link>http://arxiv.org/abs/2404.15466v1</link><description>The data-driven newsvendor problem with features has recently emerged as asignificant area of research, driven by the proliferation of data acrossvarious sectors such as retail, supply chains, e-commerce, and healthcare.Given the sensitive nature of customer or organizational data often used infeature-based analysis, it is crucial to ensure individual privacy to upholdtrust and confidence. Despite its importance, privacy preservation in thecontext of inventory planning remains unexplored. A key challenge is thenonsmoothness of the newsvendor loss function, which sets it apart fromexisting work on privacy-preserving algorithms in other settings. This paperintroduces a novel approach to estimate a privacy-preserving optimal inventorypolicy within the f-differential privacy framework, an extension of theclassical $(\epsilon, \delta)$-differential privacy with several appealingproperties. We develop a clipped noisy gradient descent algorithm based onconvolution smoothing for optimal inventory estimation to simultaneouslyaddress three main challenges: (1) unknown demand distribution and nonsmoothloss function; (2) provable privacy guarantees for individual-level data; and(3) desirable statistical precision. We derive finite-sample high-probabilitybounds for optimal policy parameter estimation and regret analysis. Byleveraging the structure of the newsvendor problem, we attain a faster excesspopulation risk bound compared to that obtained from an indiscriminateapplication of existing results for general nonsmooth convex loss. Our boundaligns with that for strongly convex and smooth loss function. Our numericalexperiments demonstrate that the proposed new method can achieve desirableprivacy protection with a marginal increase in cost.</description><author>Tuoyi Zhao, Wen-xin Zhou, Lan Wang</author><pubDate>Tue, 23 Apr 2024 20:15:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15466v1</guid></item><item><title>Can Large Language Models Learn the Physics of Metamaterials? An Empirical Study with ChatGPT</title><link>http://arxiv.org/abs/2404.15458v1</link><description>Large language models (LLMs) such as ChatGPT, Gemini, LlaMa, and Claude aretrained on massive quantities of text parsed from the internet and have shown aremarkable ability to respond to complex prompts in a manner oftenindistinguishable from humans. We present a LLM fine-tuned on up to 40,000 datathat can predict electromagnetic spectra over a range of frequencies given atext prompt that only specifies the metasurface geometry. Results are comparedto conventional machine learning approaches including feed-forward neuralnetworks, random forest, linear regression, and K-nearest neighbor (KNN).Remarkably, the fine-tuned LLM (FT-LLM) achieves a lower error across alldataset sizes explored compared to all machine learning approaches including adeep neural network. We also demonstrate the LLM's ability to solve inverseproblems by providing the geometry necessary to achieve a desired spectrum.LLMs possess some advantages over humans that may give them benefits forresearch, including the ability to process enormous amounts of data, findhidden patterns in data, and operate in higher-dimensional spaces. We proposethat fine-tuning LLMs on large datasets specific to a field allows them tograsp the nuances of that domain, making them valuable tools for research andanalysis.</description><author>Darui Lu, Yang Deng, Jordan M. Malof, Willie J. Padilla</author><pubDate>Tue, 23 Apr 2024 20:05:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15458v1</guid></item><item><title>CFPFormer: Feature-pyramid like Transformer Decoder for Segmentation and Detection</title><link>http://arxiv.org/abs/2404.15451v1</link><description>Feature pyramids have been widely adopted in convolutional neural networks(CNNs) and transformers for tasks like medical image segmentation and objectdetection. However, the currently existing models generally focus on theEncoder-side Transformer to extract features, from which decoder improvementcan bring further potential with well-designed architecture. We proposeCFPFormer, a novel decoder block that integrates feature pyramids andtransformers. Specifically, by leveraging patch embedding, cross-layer featureconcatenation, and Gaussian attention mechanisms, CFPFormer enhances featureextraction capabilities while promoting generalization across diverse tasks.Benefiting from Transformer structure and U-shaped Connections, our introducedmodel gains the ability to capture long-range dependencies and effectivelyup-sample feature maps. Our model achieves superior performance in detectingsmall objects compared to existing methods. We evaluate CFPFormer on medicalimage segmentation datasets and object detection benchmarks (VOC 2007, VOC2012,MS-COCO), demonstrating its effectiveness and versatility. On the ACDCPost-2017-MICCAI-Challenge online test set, our model reaches exceptionallyimpressive accuracy, and performed well compared with the original decodersetting in Synapse multi-organ segmentation dataset.</description><author>Hongyi Cai, Mohammad Mahdinur Rahman, Jingyu Wu, Yulun Deng</author><pubDate>Tue, 23 Apr 2024 19:46:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15451v1</guid></item><item><title>PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image Classification Accuracy for AIs and Humans</title><link>http://arxiv.org/abs/2308.13651v3</link><description>Nearest neighbors (NN) are traditionally used to compute final decisions,e.g., in Support Vector Machines or k-NN classifiers, and to provide users withexplanations for the model's decision. In this paper, we show a novel utilityof nearest neighbors: To improve predictions of a frozen, pretrained classifierC. We leverage an image comparator S that (1) compares the input image with NNimages from the top-K most probable classes; and (2) uses S's output scores toweight the confidence scores of C. Our method consistently improvesfine-grained image classification accuracy on CUB-200, Cars-196, and Dogs-120.Also, a human study finds that showing lay users our probable-class nearestneighbors (PCNN) improves their decision accuracy over prior work which onlyshows only the top-1 class examples.</description><author>Giang Nguyen, Valerie Chen, Mohammad Reza Taesiri, Anh Totti Nguyen</author><pubDate>Tue, 23 Apr 2024 19:45:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13651v3</guid></item></channel></rss>