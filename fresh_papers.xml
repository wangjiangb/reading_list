<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 12 Nov 2023 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Window Attention is Bugged: How not to Interpolate Position Embeddings</title><link>http://arxiv.org/abs/2311.05613v1</link><description>Window attention, position embeddings, and high resolution finetuning arecore concepts in the modern transformer era of computer vision. However, wefind that naively combining these near ubiquitous components can have adetrimental effect on performance. The issue is simple: interpolating positionembeddings while using window attention is wrong. We study two state-of-the-artmethods that have these three components, namely Hiera and ViTDet, and findthat both do indeed suffer from this bug. To fix it, we introduce a simpleabsolute window position embedding strategy, which solves the bug outright inHiera and allows us to increase both speed and performance of the model inViTDet. We finally combine the two to obtain HieraDet, which achieves 61.7 boxmAP on COCO, making it state-of-the-art for models that only use ImageNet-1kpretraining. This all stems from what is essentially a 3 line bug fix, which wename "absolute win".</description><author>Daniel Bolya, Chaitanya Ryali, Judy Hoffman, Christoph Feichtenhofer</author><pubDate>Thu, 09 Nov 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05613v1</guid></item><item><title>Efficient Parallelization Layouts for Large-Scale Distributed Model Training</title><link>http://arxiv.org/abs/2311.05610v1</link><description>Efficiently training large language models requires parallelizing acrosshundreds of hardware accelerators and invoking various compute and memoryoptimizations. When combined, many of these strategies have complexinteractions regarding the final training efficiency. Prior work tackling thisproblem did not have access to the latest set of optimizations, such asFlashAttention or sequence parallelism. In this work, we conduct acomprehensive ablation study of possible training configurations for largelanguage models. We distill this large study into several key recommendationsfor the most efficient training. For instance, we find that using a micro-batchsize of 1 usually enables the most efficient training layouts. Largermicro-batch sizes necessitate activation checkpointing or higher degrees ofmodel parallelism and also lead to larger pipeline bubbles. Our most efficientconfigurations enable us to achieve state-of-the-art training efficiencyresults over a range of model sizes, most notably a Model FLOPs utilization of70.5% when training a 13B model.</description><author>Johannes Hagemann, Samuel Weinbach, Konstantin Dobler, Maximilian Schall, Gerard de Melo</author><pubDate>Thu, 09 Nov 2023 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05610v1</guid></item><item><title>What Do I Hear? Generating Sounds for Visuals with ChatGPT</title><link>http://arxiv.org/abs/2311.05609v1</link><description>This short paper introduces a workflow for generating realistic soundscapesfor visual media. In contrast to prior work, which primarily focus on matchingsounds for on-screen visuals, our approach extends to suggesting sounds thatmay not be immediately visible but are essential to crafting a convincing andimmersive auditory environment. Our key insight is leveraging the reasoningcapabilities of language models, such as ChatGPT. In this paper, we describeour workflow, which includes creating a scene context, brainstorming sounds,and generating the sounds.</description><author>David Chuan-En Lin, Nikolas Martelaro</author><pubDate>Thu, 09 Nov 2023 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05609v1</guid></item><item><title>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</title><link>http://arxiv.org/abs/2311.05608v1</link><description>Large vision-language models (VLMs) like GPT-4V represent an unprecedentedrevolution in the field of artificial intelligence (AI). Compared tosingle-modal large language models (LLMs), VLMs possess more versatilecapabilities by incorporating additional modalities (e.g., images). Meanwhile,there's a rising enthusiasm in the AI community to develop open-source VLMs,such as LLaVA and MiniGPT4, which, however, have not undergone rigorous safetyassessment. In this paper, to demonstrate that more modalities lead tounforeseen AI safety issues, we propose FigStep, a novel jailbreaking frameworkagainst VLMs. FigStep feeds harmful instructions into VLMs through the imagechannel and then uses benign text prompts to induce VLMs to output contentsthat violate common AI safety policies. Our experimental results show thatFigStep can achieve an average attack success rate of 94.8% across 2 familiesof popular open-source VLMs, LLaVA and MiniGPT4 (a total of 5 VLMs). Moreover,we demonstrate that the methodology of FigStep can even jailbreak GPT-4V, whichalready leverages several system-level mechanisms to filter harmful queries.Above all, our experimental results reveal that VLMs are vulnerable tojailbreaking attacks, which highlights the necessity of novel safety alignmentsbetween visual and textual modalities.</description><author>Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang</author><pubDate>Thu, 09 Nov 2023 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05608v1</guid></item><item><title>Real-Time Neural Rasterization for Large Scenes</title><link>http://arxiv.org/abs/2311.05607v1</link><description>We propose a new method for realistic real-time novel-view synthesis (NVS) oflarge scenes. Existing neural rendering methods generate realistic results, butprimarily work for small scale scenes (&lt;50 square meters) and have difficultyat large scale (&gt;10000 square meters). Traditional graphics-based rasterizationrendering is fast for large scenes but lacks realism and requires expensivemanually created assets. Our approach combines the best of both worlds bytaking a moderate-quality scaffold mesh as input and learning a neural texturefield and shader to model view-dependant effects to enhance realism, whilestill using the standard graphics pipeline for real-time rendering. Our methodoutperforms existing neural rendering methods, providing at least 30x fasterrendering with comparable or better realism for large self-driving and dronescenes. Our work is the first to enable real-time rendering of large real-worldscenes.</description><author>Jeffrey Yunfan Liu, Yun Chen, Ze Yang, Jingkang Wang, Sivabalan Manivasagam, Raquel Urtasun</author><pubDate>Thu, 09 Nov 2023 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05607v1</guid></item><item><title>Diffusion-Generative Multi-Fidelity Learning for Physical Simulation</title><link>http://arxiv.org/abs/2311.05606v1</link><description>Multi-fidelity surrogate learning is important for physical simulationrelated applications in that it avoids running numerical solvers from scratch,which is known to be costly, and it uses multi-fidelity examples for trainingand greatly reduces the cost of data collection. Despite the variety ofexisting methods, they all build a model to map the input parameters outrightto the solution output. Inspired by the recent breakthrough in generativemodels, we take an alternative view and consider the solution output asgenerated from random noises. We develop a diffusion-generative multi-fidelity(DGMF) learning method based on stochastic differential equations (SDE), wherethe generation is a continuous denoising process. We propose a conditionalscore model to control the solution generation by the input parameters and thefidelity. By conditioning on additional inputs (temporal or spacial variables),our model can efficiently learn and predict multi-dimensional solution arrays.Our method naturally unifies discrete and continuous fidelity modeling. Theadvantage of our method in several typical applications shows a promising newdirection for multi-fidelity learning.</description><author>Zheng Wang, Shibo Li, Shikai Fang, Shandian Zhe</author><pubDate>Thu, 09 Nov 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05606v1</guid></item><item><title>3D-QAE: Fully Quantum Auto-Encoding of 3D Point Clouds</title><link>http://arxiv.org/abs/2311.05604v1</link><description>Existing methods for learning 3D representations are deep neural networkstrained and tested on classical hardware. Quantum machine learningarchitectures, despite their theoretically predicted advantages in terms ofspeed and the representational capacity, have so far not been considered forthis problem nor for tasks involving 3D data in general. This paper thusintroduces the first quantum auto-encoder for 3D point clouds. Our 3D-QAEapproach is fully quantum, i.e. all its data processing components are designedfor quantum hardware. It is trained on collections of 3D point clouds toproduce their compressed representations. Along with finding a suitablearchitecture, the core challenges in designing such a fully quantum modelinclude 3D data normalisation and parameter optimisation, and we proposesolutions for both these tasks. Experiments on simulated gate-based quantumhardware demonstrate that our method outperforms simple classical baselines,paving the way for a new research direction in 3D computer vision. The sourcecode is available at https://4dqv.mpi-inf.mpg.de/QAE3D/.</description><author>Lakshika Rathi, Edith Tretschk, Christian Theobalt, Rishabh Dabral, Vladislav Golyanik</author><pubDate>Thu, 09 Nov 2023 18:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05604v1</guid></item><item><title>Reconstructing Objects in-the-wild for Realistic Sensor Simulation</title><link>http://arxiv.org/abs/2311.05602v1</link><description>Reconstructing objects from real world data and rendering them at novel viewsis critical to bringing realism, diversity and scale to simulation for roboticstraining and testing. In this work, we present NeuSim, a novel approach thatestimates accurate geometry and realistic appearance from sparse in-the-wilddata captured at distance and at limited viewpoints. Towards this goal, werepresent the object surface as a neural signed distance function and leverageboth LiDAR and camera sensor data to reconstruct smooth and accurate geometryand normals. We model the object appearance with a robust physics-inspiredreflectance representation effective for in-the-wild data. Our experiments showthat NeuSim has strong view synthesis performance on challenging scenarios withsparse training views. Furthermore, we showcase composing NeuSim assets into avirtual world and generating realistic multi-sensor data for evaluatingself-driving perception models.</description><author>Ze Yang, Sivabalan Manivasagam, Yun Chen, Jingkang Wang, Rui Hu, Raquel Urtasun</author><pubDate>Thu, 09 Nov 2023 18:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05602v1</guid></item><item><title>FAMuS: Frames Across Multiple Sources</title><link>http://arxiv.org/abs/2311.05601v1</link><description>Understanding event descriptions is a central aspect of language processing,but current approaches focus overwhelmingly on single sentences or documents.Aggregating information about an event \emph{across documents} can offer a muchricher understanding. To this end, we present FAMuS, a new corpus of Wikipediapassages that \emph{report} on some event, paired with underlying,genre-diverse (non-Wikipedia) \emph{source} articles for the same event. Eventsand (cross-sentence) arguments in both report and source are annotated againstFrameNet, providing broad coverage of different event types. We present resultson two key event understanding tasks enabled by FAMuS: \emph{source validation}-- determining whether a document is a valid source for a target report event-- and \emph{cross-document argument extraction} -- full-document argumentextraction for a target event from both its report and the correct sourcearticle. We release both FAMuS and our models to support further research.</description><author>Siddharth Vashishtha, Alexander Martin, William Gantt, Benjamin Van Durme, Aaron Steven White</author><pubDate>Thu, 09 Nov 2023 18:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05601v1</guid></item><item><title>SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers</title><link>http://arxiv.org/abs/2311.05599v1</link><description>Vision-based human-to-robot handover is an important and challenging task inhuman-robot interaction. Recent work has attempted to train robot policies byinteracting with dynamic virtual humans in simulated environments, where thepolicies can later be transferred to the real world. However, a majorbottleneck is the reliance on human motion capture data, which is expensive toacquire and difficult to scale to arbitrary objects and human grasping motions.In this paper, we introduce a framework that can generate plausible humangrasping motions suitable for training the robot. To achieve this, we propose ahand-object synthesis method that is designed to generate handover-friendlymotions similar to humans. This allows us to generate synthetic training andtesting data with 100x more objects than previous work. In our experiments, weshow that our method trained purely with synthetic data is competitive withstate-of-the-art methods that rely on real human motion data both in simulationand on a real system. In addition, we can perform evaluations on a larger scalecompared to prior work. With our newly introduced test set, we show that ourmodel can better scale to a large variety of unseen objects and human motionscompared to the baselines. Project page:https://eth-ait.github.io/synthetic-handovers/</description><author>Sammy Christen, Lan Feng, Wei Yang, Yu-Wei Chao, Otmar Hilliges, Jie Song</author><pubDate>Thu, 09 Nov 2023 18:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05599v1</guid></item><item><title>Neuro-GPT: Developing A Foundation Model for EEG</title><link>http://arxiv.org/abs/2311.03764v2</link><description>To handle the scarcity and heterogeneity of electroencephalography (EEG) datain Brain-Computer Interface (BCI) tasks, and to harness the vast public data,we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPTmodel. The foundation model is pre-trained on a large-scale public EEG dataset,using a self-supervised task which learns how to reconstruct the masked chunkin EEG. We then fine-tune the foundation model on a Motor ImageryClassification task where only 9 subjects are available. Experimentsdemonstrated that applying foundation model can significantly improveclassification performance compared to the model trained from scratch, whichprovides evidence for the advanced generalizability of foundation model and theability to address the challenges of data scarcity and heterogeneity.</description><author>Wenhui Cui, Woojae Jeong, Philipp Thölke, Takfarinas Medani, Karim Jerbi, Anand A. Joshi, Richard M. Leahy</author><pubDate>Thu, 09 Nov 2023 18:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03764v2</guid></item><item><title>Sorting Out Quantum Monte Carlo</title><link>http://arxiv.org/abs/2311.05598v1</link><description>Molecular modeling at the quantum level requires choosing a parameterizationof the wavefunction that both respects the required particle symmetries, and isscalable to systems of many particles. For the simulation of fermions, validparameterizations must be antisymmetric with respect to the exchange ofparticles. Typically, antisymmetry is enforced by leveraging the anti-symmetryof determinants with respect to the exchange of matrix rows, but this involvescomputing a full determinant each time the wavefunction is evaluated. Instead,we introduce a new antisymmetrization layer derived from sorting, the$\textit{sortlet}$, which scales as $O(N \log N)$ with regards to the number ofparticles -- in contrast to $O(N^3)$ for the determinant. We show numericallythat applying this anti-symmeterization layer on top of an attention basedneural-network backbone yields a flexible wavefunction parameterization capableof reaching chemical accuracy when approximating the ground state of first-rowatoms and small molecules.</description><author>Jack Richter-Powell, Luca Thiede, Alán Asparu-Guzik, David Duvenaud</author><pubDate>Thu, 09 Nov 2023 18:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05598v1</guid></item><item><title>LLM Augmented Hierarchical Agents</title><link>http://arxiv.org/abs/2311.05596v1</link><description>Solving long-horizon, temporally-extended tasks using Reinforcement Learning(RL) is challenging, compounded by the common practice of learning withoutprior knowledge (or tabula rasa learning). Humans can generate and executeplans with temporally-extended actions and quickly learn to perform new tasksbecause we almost never solve problems from scratch. We want autonomous agentsto have this same ability. Recently, LLMs have been shown to encode atremendous amount of knowledge about the world and to perform impressivein-context learning and reasoning. However, using LLMs to solve real worldproblems is hard because they are not grounded in the current task. In thispaper we exploit the planning capabilities of LLMs while using RL to providelearning from the environment, resulting in a hierarchical agent that uses LLMsto solve long-horizon tasks. Instead of completely relying on LLMs, they guidea high-level policy, making learning significantly more sample efficient. Thisapproach is evaluated in simulation environments such as MiniGrid, SkillHack,and Crafter, and on a real robot arm in block manipulation tasks. We show thatagents trained using our approach outperform other baselines methods and, oncetrained, don't need access to LLMs during deployment.</description><author>Bharat Prakash, Tim Oates, Tinoosh Mohsenin</author><pubDate>Thu, 09 Nov 2023 18:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05596v1</guid></item><item><title>Towards Avoiding the Data Mess: Industry Insights from Data Mesh Implementations</title><link>http://arxiv.org/abs/2302.01713v3</link><description>With the increasing importance of data and artificial intelligence,organizations strive to become more data-driven. However, current dataarchitectures are not necessarily designed to keep up with the scale and scopeof data and analytics use cases. In fact, existing architectures often fail todeliver the promised value associated with them. Data mesh is asocio-technical, decentralized, distributed concept for enterprise datamanagement. As the concept of data mesh is still novel, it lacks empiricalinsights from the field. Specifically, an understanding of the motivationalfactors for introducing data mesh, the associated challenges, implementationstrategies, its business impact, and potential archetypes is missing. Toaddress this gap, we conduct 15 semi-structured interviews with industryexperts. Our results show, among other insights, that organizations havedifficulties with the transition toward federated governance associated withthe data mesh concept, the shift of responsibility for the development,provision, and maintenance of data products, and the comprehension of theoverall concept. In our work, we derive multiple implementation strategies andsuggest organizations introduce a cross-domain steering unit, observe the dataproduct usage, create quick wins in the early phases, and favor small dedicatedteams that prioritize data products. While we acknowledge that organizationsneed to apply implementation strategies according to their individual needs, wealso deduct two archetypes that provide suggestions in more detail. Ourfindings synthesize insights from industry experts and provide researchers andprofessionals with preliminary guidelines for the successful adoption of datamesh.</description><author>Jan Bode, Niklas Kühl, Dominik Kreuzberger, Sebastian Hirschl, Carsten Holtmann</author><pubDate>Thu, 09 Nov 2023 18:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01713v3</guid></item><item><title>Accuracy of a Vision-Language Model on Challenging Medical Cases</title><link>http://arxiv.org/abs/2311.05591v1</link><description>Background: General-purpose large language models that utilize both text andimages have not been evaluated on a diverse array of challenging medical cases. Methods: Using 934 cases from the NEJM Image Challenge published between 2005and 2023, we evaluated the accuracy of the recently released GenerativePre-trained Transformer 4 with Vision model (GPT-4V) compared to humanrespondents overall and stratified by question difficulty, image type, and skintone. We further conducted a physician evaluation of GPT-4V on 69 NEJMclinicopathological conferences (CPCs). Analyses were conducted for modelsutilizing text alone, images alone, and both text and images. Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58 to 64%)compared to 49% (95% CI, 49 to 50%) for humans. GPT-4V outperformed humans atall levels of difficulty and disagreement, skin tones, and image types; theexception was radiographic images, where performance was equivalent betweenGPT-4V and human respondents. Longer, more informative captions were associatedwith improved performance for GPT-4V but similar performance for humanrespondents. GPT-4V included the correct diagnosis in its differential for 80%(95% CI, 68 to 88%) of CPCs when using text alone, compared to 58% (95% CI, 45to 70%) of CPCs when using both images and text. Conclusions: GPT-4V outperformed human respondents on challenging medicalcases and was able to synthesize information from both images and text, butperformance deteriorated when images were added to highly informative text.Overall, our results suggest that multimodal AI models may be useful in medicaldiagnostic reasoning but that their accuracy may depend heavily on context.</description><author>Thomas Buckley, James A. Diao, Adam Rodman, Arjun K. Manrai</author><pubDate>Thu, 09 Nov 2023 18:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05591v1</guid></item><item><title>Conversational AI Threads for Visualizing Multidimensional Datasets</title><link>http://arxiv.org/abs/2311.05590v1</link><description>Generative Large Language Models (LLMs) show potential in data analysis, yettheir full capabilities remain uncharted. Our work explores the capabilities ofLLMs for creating and refining visualizations via conversational interfaces. Weused an LLM to conduct a re-analysis of a prior Wizard-of-Oz study examiningthe use of chatbots for conducting visual analysis. We surfaced the strengthsand weaknesses of LLM-driven analytic chatbots, finding that they fell short insupporting progressive visualization refinements. From these findings, wedeveloped AI Threads, a multi-threaded analytic chatbot that enables analyststo proactively manage conversational context and improve the efficacy of itsoutputs. We evaluate its usability through a crowdsourced study (n=40) andin-depth interviews with expert analysts (n=10). We further demonstrate thecapabilities of AI Threads on a dataset outside the LLM's training corpus. Ourfindings show the potential of LLMs while also surfacing challenges andfruitful avenues for future research.</description><author>Matt-Heun Hong, Anamaria Crisan</author><pubDate>Thu, 09 Nov 2023 18:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05590v1</guid></item><item><title>A Coefficient Makes SVRG Effective</title><link>http://arxiv.org/abs/2311.05589v1</link><description>Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson &amp; Zhang(2013), is a theoretically compelling optimization method. However, as Defazio&amp; Bottou (2019) highlights, its effectiveness in deep learning is yet to beproven. In this work, we demonstrate the potential of SVRG in optimizingreal-world neural networks. Our analysis finds that, for deeper networks, thestrength of the variance reduction term in SVRG should be smaller and decreaseas training progresses. Inspired by this, we introduce a multiplicativecoefficient $\alpha$ to control the strength and adjust it through a lineardecay schedule. We name our method $\alpha$-SVRG. Our results show$\alpha$-SVRG better optimizes neural networks, consistently reducing trainingloss compared to both baseline and the standard SVRG across variousarchitectures and image classification datasets. We hope our findings encouragefurther exploration into variance reduction techniques in deep learning. Codeis available at https://github.com/davidyyd/alpha-SVRG.</description><author>Yida Yin, Zhiqiu Xu, Zhiyuan Li, Trevor Darrell, Zhuang Liu</author><pubDate>Thu, 09 Nov 2023 18:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05589v1</guid></item><item><title>Bayesian Methods for Media Mix Modelling with shape and funnel effects</title><link>http://arxiv.org/abs/2311.05587v1</link><description>In recent years, significant progress in generative AI has highlighted theimportant role of physics-inspired models that utilize advanced mathematicalconcepts based on fundamental physics principles to enhance artificialintelligence capabilities. Among these models, those based on diffusionequations have greatly improved image quality. This study aims to explore thepotential uses of Maxwell-Boltzmann equation, which forms the basis of thekinetic theory of gases, and the Michaelis-Menten model in Marketing MixModelling (MMM) applications. We propose incorporating these equations intoHierarchical Bayesian models to analyse consumer behaviour in the context ofadvertising. These equation sets excel in accurately describing the randomdynamics in complex systems like social interactions and consumer-advertisinginteractions.</description><author>Javier Marin</author><pubDate>Thu, 09 Nov 2023 18:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05587v1</guid></item><item><title>Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations</title><link>http://arxiv.org/abs/2311.05584v1</link><description>Large language models (LLMs) have emerged as powerful and general solutionsto many natural language tasks. However, many of the most importantapplications of language generation are interactive, where an agent has to talkto a person to reach a desired outcome. For example, a teacher might try tounderstand their student's current comprehension level to tailor theirinstruction accordingly, and a travel agent might ask questions of theircustomer to understand their preferences in order to recommend activities theymight enjoy. LLMs trained with supervised fine-tuning or "single-step" RL, aswith standard RLHF, might struggle which tasks that require such goal-directedbehavior, since they are not trained to optimize for overall conversationaloutcomes after multiple turns of interaction. In this work, we explore a newmethod for adapting LLMs with RL for such goal-directed dialogue. Our keyinsight is that, though LLMs might not effectively solve goal-directed dialoguetasks out of the box, they can provide useful data for solving such tasks bysimulating suboptimal but human-like behaviors. Given a textual description ofa goal-directed dialogue task, we leverage LLMs to sample diverse syntheticrollouts of hypothetical in-domain human-human interactions. Our algorithm thenutilizes this dataset with offline reinforcement learning to train aninteractive conversational agent that can optimize goal-directed objectivesover multiple turns. In effect, the LLM produces examples of possibleinteractions, and RL then processes these examples to learn to perform moreoptimal interactions. Empirically, we show that our proposed approach achievesstate-of-the-art performance in various goal-directed dialogue tasks thatinclude teaching and preference elicitation.</description><author>Joey Hong, Sergey Levine, Anca Dragan</author><pubDate>Thu, 09 Nov 2023 18:45:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05584v1</guid></item><item><title>Training neural operators to preserve invariant measures of chaotic attractors</title><link>http://arxiv.org/abs/2306.01187v2</link><description>Chaotic systems make long-horizon forecasts difficult because smallperturbations in initial conditions cause trajectories to diverge at anexponential rate. In this setting, neural operators trained to minimize squarederror losses, while capable of accurate short-term forecasts, often fail toreproduce statistical or structural properties of the dynamics over longer timehorizons and can yield degenerate results. In this paper, we propose analternative framework designed to preserve invariant measures of chaoticattractors that characterize the time-invariant statistical properties of thedynamics. Specifically, in the multi-environment setting (where each sampletrajectory is governed by slightly different dynamics), we consider two novelapproaches to training with noisy data. First, we propose a loss based on theoptimal transport distance between the observed dynamics and the neuraloperator outputs. This approach requires expert knowledge of the underlyingphysics to determine what statistical features should be included in theoptimal transport loss. Second, we show that a contrastive learning framework,which does not require any specialized prior knowledge, can preservestatistical properties of the dynamics nearly as well as the optimal transportapproach. On a variety of chaotic systems, our method is shown empirically topreserve invariant measures of chaotic attractors.</description><author>Ruoxi Jiang, Peter Y. Lu, Elena Orlova, Rebecca Willett</author><pubDate>Thu, 09 Nov 2023 18:40:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01187v2</guid></item><item><title>Inference for Probabilistic Dependency Graphs</title><link>http://arxiv.org/abs/2311.05580v1</link><description>Probabilistic dependency graphs (PDGs) are a flexible class of probabilisticgraphical models, subsuming Bayesian Networks and Factor Graphs. They can alsocapture inconsistent beliefs, and provide a way of measuring the degree of thisinconsistency. We present the first tractable inference algorithm for PDGs withdiscrete variables, making the asymptotic complexity of PDG inference similarthat of the graphical models they generalize. The key components are: (1) theobservation that, in many cases, the distribution a PDG specifies can beformulated as a convex optimization problem (with exponential coneconstraints), (2) a construction that allows us to express these problemscompactly for PDGs of boundeed treewidth, (3) contributions to the theory ofPDGs that justify the construction, and (4) an appeal to interior point methodsthat can solve such problems in polynomial time. We verify the correctness andcomplexity of our approach, and provide an implementation of it. We thenevaluate our implementation, and demonstrate that it outperforms baselineapproaches. Our code is available athttp://github.com/orichardson/pdg-infer-uai.</description><author>Oliver E. Richardson, Joseph Y. Halpern, Christopher De Sa</author><pubDate>Thu, 09 Nov 2023 18:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05580v1</guid></item><item><title>SigScatNet: A Siamese + Scattering based Deep Learning Approach for Signature Forgery Detection and Similarity Assessment</title><link>http://arxiv.org/abs/2311.05579v1</link><description>The surge in counterfeit signatures has inflicted widespread inconveniencesand formidable challenges for both individuals and organizations. Thisgroundbreaking research paper introduces SigScatNet, an innovative solution tocombat this issue by harnessing the potential of a Siamese deep learningnetwork, bolstered by Scattering wavelets, to detect signature forgery andassess signature similarity. The Siamese Network empowers us to ascertain theauthenticity of signatures through a comprehensive similarity index, enablingprecise validation and comparison. Remarkably, the integration of Scatteringwavelets endows our model with exceptional efficiency, rendering it lightenough to operate seamlessly on cost-effective hardware systems. To validatethe efficacy of our approach, extensive experimentation was conducted on twoopen-sourced datasets: the ICDAR SigComp Dutch dataset and the CEDAR dataset.The experimental results demonstrate the practicality and resounding success ofour proposed SigScatNet, yielding an unparalleled Equal Error Rate of 3.689%with the ICDAR SigComp Dutch dataset and an astonishing 0.0578% with the CEDARdataset. Through the implementation of SigScatNet, our research spearheads anew state-of-the-art in signature analysis in terms of EER scores andcomputational efficiency, offering an advanced and accessible solution fordetecting forgery and quantifying signature similarities. By employingcutting-edge Siamese deep learning and Scattering wavelets, we provide a robustframework that paves the way for secure and efficient signature verificationsystems.</description><author>Anmol Chokshi, Vansh Jain, Rajas Bhope, Sudhir Dhage</author><pubDate>Thu, 09 Nov 2023 18:38:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05579v1</guid></item><item><title>Antenna Response Consistency Driven Self-supervised Learning for WIFI-based Human Activity Recognition</title><link>http://arxiv.org/abs/2310.06328v2</link><description>Self-supervised learning (SSL) for WiFi-based human activity recognition(HAR) holds great promise due to its ability to address the challenge ofinsufficient labeled data. However, directly transplanting SSL algorithms,especially contrastive learning, originally designed for other domains to CSIdata, often fails to achieve the expected performance. We attribute this issueto the inappropriate alignment criteria, which disrupt the semantic distanceconsistency between the feature space and the input space. To address thischallenge, we introduce \textbf{A}ntenna \textbf{R}esponse \textbf{C}onsistency(ARC) as a solution to define proper alignment criteria. ARC is designed toretain semantic information from the input space while introducing robustnessto real-world noise. Moreover, we substantiate the effectiveness of ARC througha comprehensive set of experiments, demonstrating its capability to enhance theperformance of self-supervised learning for WiFi-based HAR by achieving anincrease of over 5\% in accuracy in most cases and achieving a best accuracy of94.97\%.</description><author>Ke Xu, Jiangtao Wang, Hongyuan Zhu, Dingchang Zheng</author><pubDate>Thu, 09 Nov 2023 18:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06328v2</guid></item><item><title>Outlier-Robust Wasserstein DRO</title><link>http://arxiv.org/abs/2311.05573v1</link><description>Distributionally robust optimization (DRO) is an effective approach fordata-driven decision-making in the presence of uncertainty. Geometricuncertainty due to sampling or localized perturbations of data points iscaptured by Wasserstein DRO (WDRO), which seeks to learn a model that performsuniformly well over a Wasserstein ball centered around the observed datadistribution. However, WDRO fails to account for non-geometric perturbationssuch as adversarial outliers, which can greatly distort the Wassersteindistance measurement and impede the learned model. We address this gap byproposing a novel outlier-robust WDRO framework for decision-making under bothgeometric (Wasserstein) perturbations and non-geometric (total variation (TV))contamination that allows an $\varepsilon$-fraction of data to be arbitrarilycorrupted. We design an uncertainty set using a certain robust Wasserstein ballthat accounts for both perturbation types and derive minimax optimal excessrisk bounds for this procedure that explicitly capture the Wasserstein and TVrisks. We prove a strong duality result that enables tractable convexreformulations and efficient computation of our outlier-robust WDRO problem.When the loss function depends only on low-dimensional features of the data, weeliminate certain dimension dependencies from the risk bounds that areunavoidable in the general setting. Finally, we present experiments validatingour theory on standard regression and classification tasks.</description><author>Sloan Nietert, Ziv Goldfeld, Soroosh Shafiee</author><pubDate>Thu, 09 Nov 2023 18:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05573v1</guid></item><item><title>Progressive Ensemble Distillation: Building Ensembles for Efficient Inference</title><link>http://arxiv.org/abs/2302.10093v2</link><description>We study the problem of progressive ensemble distillation: Given a large,pretrained teacher model $g$, we seek to decompose the model into smaller,low-inference cost student models $f_i$, such that progressively evaluatingadditional models in this ensemble leads to improved predictions. The resultingensemble allows for flexibly tuning accuracy vs. inference cost at runtime,which is useful for a number of applications in on-device inference. The methodwe propose, B-DISTIL , relies on an algorithmic procedure that uses functioncomposition over intermediate activations to construct expressive ensembleswith similar performance as $g$ , but with smaller student models. Wedemonstrate the effectiveness of B-DISTIL by decomposing pretrained modelsacross standard image, speech, and sensor datasets. We also provide theoreticalguarantees in terms of convergence and generalization.</description><author>Don Kurian Dennis, Abhishek Shetty, Anish Sevekari, Kazuhito Koishida, Virginia Smith</author><pubDate>Thu, 09 Nov 2023 18:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10093v2</guid></item><item><title>A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions</title><link>http://arxiv.org/abs/2310.19218v2</link><description>With the development of trustworthy Federated Learning (FL), the requirementof implementing right to be forgotten gives rise to the area of FederatedUnlearning (FU). Comparing to machine unlearning, a major challenge of FU liesin the decentralized and privacy-preserving nature of FL, in which clientsjointly train a global model without sharing their raw data, making itsubstantially more intricate to selectively unlearn specific information. Inthat regard, many efforts have been made to tackle the challenges of FU andhave achieved significant progress. In this paper, we present a comprehensivesurvey of FU. Specially, we provide the existing algorithms, objectives,evaluation metrics, and identify some challenges of FU. By reviewing andcomparing some studies, we summarize them into a taxonomy for various schemes,potential applications and future directions.</description><author>Jiaxi Yang, Yang Zhao</author><pubDate>Thu, 09 Nov 2023 18:26:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19218v2</guid></item><item><title>Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models</title><link>http://arxiv.org/abs/2310.09949v2</link><description>A Retrieval-Augmented Language Model (RALM) augments a generative languagemodel by retrieving context-specific knowledge from an external database. Thisstrategy facilitates impressive text generation quality even with smallermodels, thus reducing orders of magnitude of computational demands. However,RALMs introduce unique system design challenges due to (a) the diverse workloadcharacteristics between LM inference and retrieval and (b) the various systemrequirements and bottlenecks for different RALM configurations such as modelsizes, database sizes, and retrieval frequencies. We propose Chameleon, aheterogeneous accelerator system that integrates both LM and retrievalaccelerators in a disaggregated architecture. The heterogeneity ensuresefficient acceleration of both LM inference and retrieval, while theaccelerator disaggregation enables the system to independently scale both typesof accelerators to fulfill diverse RALM requirements. Our Chameleon prototypeimplements retrieval accelerators on FPGAs and assigns LM inference to GPUs,with a CPU server orchestrating these accelerators over the network. Comparedto CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72xspeedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleonexhibits up to 2.16x reduction in latency and 3.18x speedup in throughputcompared to the hybrid CPU-GPU architecture. These promising results pave theway for bringing accelerator heterogeneity and disaggregation into future RALMsystems.</description><author>Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, Gustavo Alonso</author><pubDate>Thu, 09 Nov 2023 18:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09949v2</guid></item><item><title>Exploring Emotion Expression Recognition in Older Adults Interacting with a Virtual Coach</title><link>http://arxiv.org/abs/2311.05567v1</link><description>The EMPATHIC project aimed to design an emotionally expressive virtual coachcapable of engaging healthy seniors to improve well-being and promoteindependent aging. One of the core aspects of the system is its human sensingcapabilities, allowing for the perception of emotional states to provide apersonalized experience. This paper outlines the development of the emotionexpression recognition module of the virtual coach, encompassing datacollection, annotation design, and a first methodological approach, alltailored to the project requirements. With the latter, we investigate the roleof various modalities, individually and combined, for discrete emotionexpression recognition in this context: speech from audio, and facialexpressions, gaze, and head dynamics from video. The collected corpus includesusers from Spain, France, and Norway, and was annotated separately for theaudio and video channels with distinct emotional labels, allowing for aperformance comparison across cultures and label types. Results confirm theinformative power of the modalities studied for the emotional categoriesconsidered, with multimodal methods generally outperforming others (around 68%accuracy with audio labels and 72-74% with video labels). The findings areexpected to contribute to the limited literature on emotion recognition appliedto older adults in conversational human-machine interaction.</description><author>Cristina Palmero, Mikel deVelasco, Mohamed Amine Hmani, Aymen Mtibaa, Leila Ben Letaifa, Pau Buch-Cardona, Raquel Justo, Terry Amorese, Eduardo González-Fraile, Begoña Fernández-Ruanova, Jofre Tenorio-Laranga, Anna Torp Johansen, Micaela Rodrigues da Silva, Liva Jenny Martinussen, Maria Stylianou Korsnes, Gennaro Cordasco, Anna Esposito, Mounim A. El-Yacoubi, Dijana Petrovska-Delacrétaz, M. Inés Torres, Sergio Escalera</author><pubDate>Thu, 09 Nov 2023 18:22:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05567v1</guid></item><item><title>High-Performance Transformers for Table Structure Recognition Need Early Convolutions</title><link>http://arxiv.org/abs/2311.05565v1</link><description>Table structure recognition (TSR) aims to convert tabular images into amachine-readable format, where a visual encoder extracts image features and atextual decoder generates table-representing tokens. Existing approaches useclassic convolutional neural network (CNN) backbones for the visual encoder andtransformers for the textual decoder. However, this hybrid CNN-Transformerarchitecture introduces a complex visual encoder that accounts for nearly halfof the total model parameters, markedly reduces both training and inferencespeed, and hinders the potential for self-supervised learning in TSR. In thiswork, we design a lightweight visual encoder for TSR without sacrificingexpressive power. We discover that a convolutional stem can match classic CNNbackbone performance, with a much simpler model. The convolutional stem strikesan optimal balance between two crucial factors for high-performance TSR: ahigher receptive field (RF) ratio and a longer sequence length. This allows itto "see" an appropriate portion of the table and "store" the complex tablestructure within sufficient context length for the subsequent transformer. Weconducted reproducible ablation studies and open-sourced our code athttps://github.com/poloclub/tsr-convstem to enhance transparency, inspireinnovations, and facilitate fair comparisons in our domain as tables are apromising modality for representation learning.</description><author>ShengYun Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari Balasubramaniyan, Duen Horng Chau</author><pubDate>Thu, 09 Nov 2023 18:20:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05565v1</guid></item><item><title>Aspects of human memory and Large Language Models</title><link>http://arxiv.org/abs/2311.03839v2</link><description>Large Language Models (LLMs) are huge artificial neural networks whichprimarily serve to generate text, but also provide a very sophisticatedprobabilistic model of language use. Since generating a semantically consistenttext requires a form of effective memory, we investigate the memory propertiesof LLMs and find surprising similarities with key characteristics of humanmemory. We argue that the human-like memory properties of the Large LanguageModel do not follow automatically from the LLM architecture but are ratherlearned from the statistics of the training textual data. These resultsstrongly suggest that the biological features of human memory leave an imprinton the way that we structure our textual narratives.</description><author>Romuald A. Janik</author><pubDate>Thu, 09 Nov 2023 18:16:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03839v2</guid></item><item><title>Teaching AI to Teach: Leveraging Limited Human Salience Data Into Unlimited Saliency-Based Training</title><link>http://arxiv.org/abs/2306.05527v2</link><description>Machine learning models have shown increased accuracy in classification taskswhen the training process incorporates human perceptual information. However, achallenge in training human-guided models is the cost associated withcollecting image annotations for human salience. Collecting annotation data forall images in a large training set can be prohibitively expensive. In thiswork, we utilize "teacher" models (trained on a small amount of human-annotateddata) to annotate additional data by means of teacher models' saliency maps.Then, "student" models are trained using the larger amount of annotatedtraining data. This approach makes it possible to supplement a limited numberof human-supplied annotations with an arbitrarily large number ofmodel-generated image annotations. We compare the accuracy achieved by ourteacher-student training paradigm with (1) training using all available humansalience annotations, and (2) using all available training data without humansalience annotations. We use synthetic face detection and fake iris detectionas example challenging problems, and report results across four modelarchitectures (DenseNet, ResNet, Xception, and Inception), and two saliencyestimation methods (CAM and RISE). Results show that our teacher-studenttraining paradigm results in models that significantly exceed the performanceof both baselines, demonstrating that our approach can usefully leverage asmall amount of human annotations to generate salience maps for an arbitraryamount of additional training data.</description><author>Colton R. Crum, Aidan Boyd, Kevin Bowyer, Adam Czajka</author><pubDate>Thu, 09 Nov 2023 18:15:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05527v2</guid></item><item><title>Disentangling Quantum and Classical Contributions in Hybrid Quantum Machine Learning Architectures</title><link>http://arxiv.org/abs/2311.05559v1</link><description>Quantum computing offers the potential for superior computationalcapabilities, particularly for data-intensive tasks. However, the current stateof quantum hardware puts heavy restrictions on input size. To address this,hybrid transfer learning solutions have been developed, merging pre-trainedclassical models, capable of handling extensive inputs, with variationalquantum circuits. Yet, it remains unclear how much each component - classicaland quantum - contributes to the model's results. We propose a novel hybridarchitecture: instead of utilizing a pre-trained network for compression, weemploy an autoencoder to derive a compressed version of the input data. Thiscompressed data is then channeled through the encoder part of the autoencoderto the quantum component. We assess our model's classification capabilitiesagainst two state-of-the-art hybrid transfer learning architectures, two purelyclassical architectures and one quantum architecture. Their accuracy iscompared across four datasets: Banknote Authentication, Breast CancerWisconsin, MNIST digits, and AudioMNIST. Our research suggests that classicalcomponents significantly influence classification in hybrid transfer learning,a contribution often mistakenly ascribed to the quantum element. Theperformance of our model aligns with that of a variational quantum circuitusing amplitude embedding, positioning it as a feasible alternative.</description><author>Michael Kölle, Jonas Maurer, Philipp Altmann, Leo Sünkel, Jonas Stein, Claudia Linnhoff-Popien</author><pubDate>Thu, 09 Nov 2023 18:13:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05559v1</guid></item><item><title>Using Early Exits for Fast Inference in Automatic Modulation Classification</title><link>http://arxiv.org/abs/2308.11100v2</link><description>Automatic modulation classification (AMC) plays a critical role in wirelesscommunications by autonomously classifying signals transmitted over the radiospectrum. Deep learning (DL) techniques are increasingly being used for AMC dueto their ability to extract complex wireless signal features. However, DLmodels are computationally intensive and incur high inference latencies. Thispaper proposes the application of early exiting (EE) techniques for DL modelsused for AMC to accelerate inference. We present and analyze four early exitingarchitectures and a customized multi-branch training algorithm for thisproblem. Through extensive experimentation, we show that signals with moderateto high signal-to-noise ratios (SNRs) are easier to classify, do not requiredeep architectures, and can therefore leverage the proposed EE architectures.Our experimental results demonstrate that EE techniques can significantlyreduce the inference speed of deep neural networks without sacrificingclassification accuracy. We also thoroughly study the trade-off betweenclassification accuracy and inference time when using these architectures. Tothe best of our knowledge, this work represents the first attempt to applyearly exiting methods to AMC, providing a foundation for future research inthis area.</description><author>Elsayed Mohammed, Omar Mashaal, Hatem Abou-Zeid</author><pubDate>Thu, 09 Nov 2023 18:10:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11100v2</guid></item><item><title>Exploiting Neural-Network Statistics for Low-Power DNN Inference</title><link>http://arxiv.org/abs/2311.05557v1</link><description>Specialized compute blocks have been developed for efficient DNN execution.However, due to the vast amount of data and parameter movements, theinterconnects and on-chip memories form another bottleneck, impairing power andperformance. This work addresses this bottleneck by contributing a low-powertechnique for edge-AI inference engines that combines overhead-free coding witha statistical analysis of the data and parameters of neural networks. Ourapproach reduces the interconnect and memory power consumption by up to 80% forstate-of-the-art benchmarks while providing additional power savings for thecompute blocks by up to 39%. These power improvements are achieved with no lossof accuracy and negligible hardware cost.</description><author>Lennart Bamberg, Ardalan Najafi, Alberto Garcia-Ortiz</author><pubDate>Thu, 09 Nov 2023 18:05:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05557v1</guid></item><item><title>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</title><link>http://arxiv.org/abs/2311.05556v1</link><description>Latent Consistency Models (LCMs) have achieved impressive performance inaccelerating text-to-image generative tasks, producing high-quality images withminimal inference steps. LCMs are distilled from pre-trained latent diffusionmodels (LDMs), requiring only ~32 A100 GPU training hours. This report furtherextends LCMs' potential in two aspects: First, by applying LoRA distillation toStable-Diffusion models including SD-V1.5, SSD-1B, and SDXL, we have expandedLCM's scope to larger models with significantly less memory consumption,achieving superior image generation quality. Second, we identify the LoRAparameters obtained through LCM distillation as a universal Stable-Diffusionacceleration module, named LCM-LoRA. LCM-LoRA can be directly plugged intovarious Stable-Diffusion fine-tuned models or LoRAs without training, thusrepresenting a universally applicable accelerator for diverse image generationtasks. Compared with previous numerical PF-ODE solvers such as DDIM,DPM-Solver, LCM-LoRA can be viewed as a plug-in neural PF-ODE solver thatpossesses strong generalization abilities. Project page:https://github.com/luosiallen/latent-consistency-model.</description><author>Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian Li, Hang Zhao</author><pubDate>Thu, 09 Nov 2023 18:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05556v1</guid></item><item><title>Leveraging Human Feedback to Scale Educational Datasets: Combining Crowdworkers and Comparative Judgement</title><link>http://arxiv.org/abs/2305.12894v2</link><description>Machine Learning models have many potentially beneficial applications ineducation settings, but a key barrier to their development is securing enoughdata to train these models. Labelling educational data has traditionally reliedon highly skilled raters using complex, multi-class rubrics, making the processexpensive and difficult to scale. An alternative, more scalable approach couldbe to use non-expert crowdworkers to evaluate student work, however,maintaining sufficiently high levels of accuracy and inter-rater reliabilitywhen using non-expert workers is challenging. This paper reports on twoexperiments investigating using non-expert crowdworkers and comparativejudgement to evaluate complex student data. Crowdworkers were hired to evaluatestudent responses to open-ended reading comprehension questions. Crowdworkerswere randomly assigned to one of two conditions: the control, where they wereasked to decide whether answers were correct or incorrect (i.e., a categoricaljudgement), or the treatment, where they were shown the same question andanswers, but were instead asked to decide which of two candidate answers wasmore correct (i.e., a comparative/preference-based judgement). We found thatusing comparative judgement substantially improved inter-rater reliability onboth tasks. These results are in-line with well-established literature on thebenefits of comparative judgement in the field of educational assessment, aswell as with recent trends in artificial intelligence research, wherecomparative judgement is becoming the preferred method for providing humanfeedback on model outputs when working with non-expert crowdworkers. However,to our knowledge, these results are novel and important in demonstrating thebeneficial effects of using the combination of comparative judgement andcrowdworkers to evaluate educational data.</description><author>Owen Henkel, Libby Hills</author><pubDate>Thu, 09 Nov 2023 18:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12894v2</guid></item><item><title>Removing RLHF Protections in GPT-4 via Fine-Tuning</title><link>http://arxiv.org/abs/2311.05553v1</link><description>As large language models (LLMs) have increased in their capabilities, so doestheir potential for dual use. To reduce harmful outputs, produces and vendorsof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,LLM vendors have been increasingly enabling fine-tuning of their most powerfulmodels. However, concurrent work has shown that fine-tuning can remove RLHFprotections. We may expect that the most powerful models currently available(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the contrary: fine-tuning allows attackers to removeRLHF protections with as few as 340 examples and a 95% success rate. Thesetraining examples can be automatically generated with weaker models. We furthershow that removing RLHF protections does not decrease usefulness onnon-censored outputs, providing evidence that our fine-tuning strategy does notdecrease usefulness despite using weaker models to generate training data. Ourresults show the need for further research on protections on LLMs.</description><author>Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, Daniel Kang</author><pubDate>Thu, 09 Nov 2023 17:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05553v1</guid></item><item><title>The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation</title><link>http://arxiv.org/abs/2311.05552v1</link><description>Human evaluation is often considered to be the gold standard method ofevaluating a Natural Language Generation system. However, whilst its importanceis accepted by the community at large, the quality of its execution is oftenbrought into question. In this position paper, we argue that the generation ofmore esoteric forms of language - humour, irony and sarcasm - constitutes asubdomain where the characteristics of selected evaluator panels are of utmostimportance, and every effort should be made to report demographiccharacteristics wherever possible, in the interest of transparency andreplicability. We support these claims with an overview of each language formand an analysis of examples in terms of how their interpretation is affected bydifferent participant variables. We additionally perform a critical survey ofrecent works in NLG to assess how well evaluation procedures are reported inthis subdomain, and note a severe lack of open reporting of evaluatordemographic information, and a significant reliance on crowdsourcing platformsfor recruitment.</description><author>Tyler Loakman, Aaron Maladry, Chenghua Lin</author><pubDate>Thu, 09 Nov 2023 17:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05552v1</guid></item><item><title>Towards End-to-End Spoken Grammatical Error Correction</title><link>http://arxiv.org/abs/2311.05550v1</link><description>Grammatical feedback is crucial for L2 learners, teachers, and testers.Spoken grammatical error correction (GEC) aims to supply feedback to L2learners on their use of grammar when speaking. This process usually relies ona cascaded pipeline comprising an ASR system, disfluency removal, and GEC, withthe associated concern of propagating errors between these individual modules.In this paper, we introduce an alternative "end-to-end" approach to spoken GEC,exploiting a speech recognition foundation model, Whisper. This foundationmodel can be used to replace the whole framework or part of it, e.g., ASR anddisfluency removal. These end-to-end approaches are compared to more standardcascaded approaches on the data obtained from a free-speaking spoken languageassessment test, Linguaskill. Results demonstrate that end-to-end spoken GEC ispossible within this architecture, but the lack of available data limitscurrent performance compared to a system using large quantities of text-basedGEC data. Conversely, end-to-end disfluency detection and removal, which iseasier for the attention-based Whisper to learn, does outperform cascadedapproaches. Additionally, the paper discusses the challenges of providingfeedback to candidates when using end-to-end systems for spoken GEC.</description><author>Stefano Bannò, Rao Ma, Mengjie Qian, Kate M. Knill, Mark J. F. Gales</author><pubDate>Thu, 09 Nov 2023 17:49:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05550v1</guid></item><item><title>Prediction-Powered Inference</title><link>http://arxiv.org/abs/2301.09633v4</link><description>Prediction-powered inference is a framework for performing valid statisticalinference when an experimental dataset is supplemented with predictions from amachine-learning system. The framework yields simple algorithms for computingprovably valid confidence intervals for quantities such as means, quantiles,and linear and logistic regression coefficients, without making any assumptionson the machine-learning algorithm that supplies the predictions. Furthermore,more accurate predictions translate to smaller confidence intervals.Prediction-powered inference could enable researchers to draw valid and moredata-efficient conclusions using machine learning. The benefits ofprediction-powered inference are demonstrated with datasets from proteomics,astronomy, genomics, remote sensing, census analysis, and ecology.</description><author>Anastasios N. Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I. Jordan, Tijana Zrnic</author><pubDate>Thu, 09 Nov 2023 17:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09633v4</guid></item><item><title>L-WaveBlock: A Novel Feature Extractor Leveraging Wavelets for Generative Adversarial Networks</title><link>http://arxiv.org/abs/2311.05548v1</link><description>Generative Adversarial Networks (GANs) have risen to prominence in the fieldof deep learning, facilitating the generation of realistic data from randomnoise. The effectiveness of GANs often depends on the quality of featureextraction, a critical aspect of their architecture. This paper introducesL-WaveBlock, a novel and robust feature extractor that leverages thecapabilities of the Discrete Wavelet Transform (DWT) with deep learningmethodologies. L-WaveBlock is catered to quicken the convergence of GANgenerators while simultaneously enhancing their performance. The paperdemonstrates the remarkable utility of L-WaveBlock across three datasets, aroad satellite imagery dataset, the CelebA dataset and the GoPro dataset,showcasing its ability to ease feature extraction and make it more efficient.By utilizing DWT, L-WaveBlock efficiently captures the intricate details ofboth structural and textural details, and further partitions feature maps intoorthogonal subbands across multiple scales while preserving essentialinformation at the same time. Not only does it lead to faster convergence, butalso gives competent results on every dataset by employing the L-WaveBlock. Theproposed method achieves an Inception Score of 3.6959 and a StructuralSimilarity Index of 0.4261 on the maps dataset, a Peak Signal-to-Noise Ratio of29.05 and a Structural Similarity Index of 0.874 on the CelebA dataset. Theproposed method performs competently to the state-of-the-art for the imagedenoising dataset, albeit not better, but still leads to faster convergencethan conventional methods. With this, L-WaveBlock emerges as a robust andefficient tool for enhancing GAN-based image generation, demonstrating superiorconvergence speed and competitive performance across multiple datasets forimage resolution, image generation and image denoising.</description><author>Mirat Shah, Vansh Jain, Anmol Chokshi, Guruprasad Parasnis, Pramod Bide</author><pubDate>Thu, 09 Nov 2023 17:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05548v1</guid></item><item><title>Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization</title><link>http://arxiv.org/abs/2311.05546v1</link><description>Multi-Agent Reinforcement Learning is becoming increasingly more important intimes of autonomous driving and other smart industrial applications.Simultaneously a promising new approach to Reinforcement Learning arises usingthe inherent properties of quantum mechanics, reducing the trainable parametersof a model significantly. However, gradient-based Multi-Agent QuantumReinforcement Learning methods often have to struggle with barren plateaus,holding them back from matching the performance of classical approaches. Webuild upon a existing approach for gradient free Quantum Reinforcement Learningand propose tree approaches with Variational Quantum Circuits for Multi-AgentReinforcement Learning using evolutionary optimization. We evaluate ourapproach in the Coin Game environment and compare them to classical approaches.We showed that our Variational Quantum Circuit approaches perform significantlybetter compared to a neural network with a similar amount of trainableparameters. Compared to the larger neural network, our approaches archivesimilar results using $97.88\%$ less parameters.</description><author>Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien</author><pubDate>Thu, 09 Nov 2023 17:45:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05546v1</guid></item><item><title>Bayesian Prognostic Covariate Adjustment With Additive Mixture Priors</title><link>http://arxiv.org/abs/2310.18027v2</link><description>Effective and rapid decision-making from randomized controlled trials (RCTs)requires unbiased and precise treatment effect inferences. Two strategies toaddress this requirement are to adjust for covariates that are highlycorrelated with the outcome, and to leverage historical control information viaBayes' theorem. We propose a new Bayesian prognostic covariate adjustmentmethodology, referred to as Bayesian PROCOVA, that combines these twostrategies. Covariate adjustment in Bayesian PROCOVA is based on generativeartificial intelligence (AI) algorithms that construct a digital twin generator(DTG) for RCT participants. The DTG is trained on historical control data andyields a digital twin (DT) probability distribution for each RCT participant'soutcome under the control treatment. The expectation of the DT distribution,referred to as the prognostic score, defines the covariate for adjustment.Historical control information is leveraged via an additive mixture prior withtwo components: an informative prior probability distribution specified basedon historical control data, and a weakly informative prior distribution. Themixture weight determines the extent to which posterior inferences are drawnfrom the informative component, versus the weakly informative component. Thisweight has a prior distribution as well, and so the entire additive mixtureprior is completely pre-specifiable without involving any RCT information. Weestablish an efficient Gibbs algorithm for sampling from the posteriordistribution, and derive closed-form expressions for the posterior mean andvariance of the treatment effect parameter conditional on the weight, inBayesian PROCOVA. We evaluate efficiency gains of Bayesian PROCOVA via its biascontrol and variance reduction compared to frequentist PROCOVA in simulationstudies that encompass different discrepancies. These gains translate tosmaller RCTs.</description><author>Alyssa M. Vanderbeek, Arman Sabbaghi, Jon R. Walsh, Charles K. Fisher</author><pubDate>Thu, 09 Nov 2023 17:39:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18027v2</guid></item><item><title>Hopfield-Enhanced Deep Neural Networks for Artifact-Resilient Brain State Decoding</title><link>http://arxiv.org/abs/2311.03421v2</link><description>The study of brain states, ranging from highly synchronous to asynchronousneuronal patterns like the sleep-wake cycle, is fundamental for assessing thebrain's spatiotemporal dynamics and their close connection to behavior.However, the development of new techniques to accurately identify them stillremains a challenge, as these are often compromised by the presence of noise,artifacts, and suboptimal recording quality. In this study, we propose atwo-stage computational framework combining Hopfield Networks for artifact datapreprocessing with Convolutional Neural Networks (CNNs) for classification ofbrain states in rat neural recordings under different levels of anesthesia. Toevaluate the robustness of our framework, we deliberately introduced noiseartifacts into the neural recordings. We evaluated our hybrid Hopfield-CNNpipeline by benchmarking it against two comparative models: a standalone CNNhandling the same noisy inputs, and another CNN trained and tested onartifact-free data. Performance across various levels of data compression andnoise intensities showed that our framework can effectively mitigate artifacts,allowing the model to reach parity with the clean-data CNN at lower noiselevels. Although this study mainly benefits small-scale experiments, thefindings highlight the necessity for advanced deep learning and HopfieldNetwork models to improve scalability and robustness in diverse real-worldsettings.</description><author>Arnau Marin-Llobet, Arnau Manasanch, Maria V. Sanchez-Vives</author><pubDate>Thu, 09 Nov 2023 17:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03421v2</guid></item><item><title>A Deep Learning Method for Simultaneous Denoising and Missing Wedge Reconstruction in Cryogenic Electron Tomography</title><link>http://arxiv.org/abs/2311.05539v1</link><description>Cryogenic electron tomography (cryo-ET) is a technique for imaging biologicalsamples such as viruses, cells, and proteins in 3D. A microscope collects aseries of 2D projections of the sample, and the goal is to reconstruct the 3Ddensity of the sample called the tomogram. This is difficult as the 2Dprojections have a missing wedge of information and are noisy. Tomogramsreconstructed with conventional methods, such as filtered back-projection,suffer from the noise, and from artifacts and anisotropic resolution due to themissing wedge of information. To improve the visual quality and resolution ofsuch tomograms, we propose a deep-learning approach for simultaneous denoisingand missing wedge reconstruction called DeepDeWedge. DeepDeWedge is based onfitting a neural network to the 2D projections with a self-supervised lossinspired by noise2noise-like methods. The algorithm requires no training orground truth data. Experiments on synthetic and real cryo-ET data show thatDeepDeWedge achieves competitive performance for deep learning-based denoisingand missing wedge reconstruction of cryo-ET tomograms.</description><author>Simon Wiedemann, Reinhard Heckel</author><pubDate>Thu, 09 Nov 2023 17:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05539v1</guid></item><item><title>Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples</title><link>http://arxiv.org/abs/2311.05538v1</link><description>Mixup refers to interpolation-based data augmentation, originally motivatedas a way to go beyond empirical risk minimization (ERM). Its extensions mostlyfocus on the definition of interpolation and the space (input or feature) whereit takes place, while the augmentation process itself is less studied. In mostmethods, the number of generated examples is limited to the mini-batch size andthe number of examples being interpolated is limited to two (pairs), in theinput space. We make progress in this direction by introducing MultiMix, which generatesan arbitrarily large number of interpolated examples beyond the mini-batch sizeand interpolates the entire mini-batch in the embedding space. Effectively, wesample on the entire convex hull of the mini-batch rather than along linearsegments between pairs of examples. On sequence data, we further extend to Dense MultiMix. We densely interpolatefeatures and target labels at each spatial location and also apply the lossdensely. To mitigate the lack of dense labels, we inherit labels from examplesand weight interpolation factors by attention as a measure of confidence. Overall, we increase the number of loss terms per mini-batch by orders ofmagnitude at little additional cost. This is only possible because ofinterpolating in the embedding space. We empirically show that our solutionsyield significant improvement over state-of-the-art mixup methods on fourdifferent benchmarks, despite interpolation being only linear. By analyzing theembedding space, we show that the classes are more tightly clustered anduniformly spread over the embedding space, thereby explaining the improvedbehavior.</description><author>Shashanka Venkataramanan, Ewa Kijak, Laurent Amsaleg, Yannis Avrithis</author><pubDate>Thu, 09 Nov 2023 17:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05538v1</guid></item><item><title>Information-theoretic generalization bounds for learning from quantum data</title><link>http://arxiv.org/abs/2311.05529v1</link><description>Learning tasks play an increasingly prominent role in quantum information andcomputation. They range from fundamental problems such as state discriminationand metrology over the framework of quantum probably approximately correct(PAC) learning, to the recently proposed shadow variants of state tomography.However, the many directions of quantum learning theory have so far evolvedseparately. We propose a general mathematical formalism for describing quantumlearning by training on classical-quantum data and then testing how well thelearned hypothesis generalizes to new data. In this framework, we prove boundson the expected generalization error of a quantum learner in terms of classicaland quantum information-theoretic quantities measuring how strongly thelearner's hypothesis depends on the specific data seen during training. To achieve this, we use tools from quantum optimal transport and quantumconcentration inequalities to establish non-commutative versions of decouplinglemmas that underlie recent information-theoretic generalization bounds forclassical machine learning. Our framework encompasses and gives intuitively accessible generalizationbounds for a variety of quantum learning scenarios such as quantum statediscrimination, PAC learning quantum states, quantum parameter estimation, andquantumly PAC learning classical functions. Thereby, our work lays a foundationfor a unifying quantum information-theoretic perspective on quantum learning.</description><author>Matthias Caro, Tom Gur, Cambyse Rouzé, Daniel Stilck França, Sathyawageeswar Subramanian</author><pubDate>Thu, 09 Nov 2023 17:21:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05529v1</guid></item><item><title>Differentiable Cutting-plane Layers for Mixed-integer Linear Optimization</title><link>http://arxiv.org/abs/2311.03350v3</link><description>We consider the problem of solving a family of parametric mixed-integerlinear optimization problems where some entries in the input data change. Weintroduce the concept of cutting-plane layer (CPL), i.e., a differentiablecutting-plane generator mapping the problem data and previous iterates tocutting planes. We propose a CPL implementation to generate split cuts, and bycombining several CPLs, we devise a differentiable cutting-plane algorithm thatexploits the repeated nature of parametric instances. In an offline phase, wetrain our algorithm by updating the internal parameters controlling the CPLs,thus altering cut generation. Once trained, our algorithm computes, withpredictable execution times and a fixed number of cuts, solutions with lowintegrality gaps. Preliminary computational tests show that our algorithmgeneralizes on unseen instances and captures underlying parametric structures.</description><author>Gabriele Dragotto, Stefan Clarke, Jaime Fernández Fisac, Bartolomeo Stellato</author><pubDate>Thu, 09 Nov 2023 17:19:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03350v3</guid></item><item><title>Gaussian Cooling and Dikin Walks: The Interior-Point Method for Logconcave Sampling</title><link>http://arxiv.org/abs/2307.12943v3</link><description>The connections between (convex) optimization and (logconcave) sampling havebeen considerably enriched in the past decade with many conceptual andmathematical analogies. For instance, the Langevin algorithm can be viewed as asampling analogue of gradient descent and has condition-number-dependentguarantees on its performance. In the early 1990s, Nesterov and Nemirovskideveloped the Interior-Point Method (IPM) for convex optimization based onself-concordant barriers, providing efficient algorithms for structured convexoptimization, often faster than the general method. This raises the followingquestion: can we develop an analogous IPM for structured sampling problems? In 2012, Kannan and Narayanan proposed the Dikin walk for uniformly samplingpolytopes, and an improved analysis was given in 2020 by Laddha-Lee-Vempala.The Dikin walk uses a local metric defined by a self-concordant barrier forlinear constraints. Here we generalize this approach by developing and adaptingIPM machinery together with the Dikin walk for poly-time sampling algorithms.Our IPM-based sampling framework provides an efficient warm start and goesbeyond uniform distributions and linear constraints. We illustrate the approachon important special cases, in particular giving the fastest algorithms tosample uniform, exponential, or Gaussian distributions on a truncated PSD cone.The framework is general and can be applied to other sampling algorithms.</description><author>Yunbum Kook, Santosh S. Vempala</author><pubDate>Thu, 09 Nov 2023 17:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12943v3</guid></item><item><title>Fine-Tune Language Models as Multi-Modal Differential Equation Solvers</title><link>http://arxiv.org/abs/2308.05061v3</link><description>In the growing domain of scientific machine learning, in-context operatorlearning has shown notable potential in learning operators and solvingdifferential equations using prompted data, during the inference stage withoutweight updates. However, the current model's overdependence on function data,may inadvertently overlook the invaluable human insight into the operator. Toaddress this, we present a transformation of in-context operator learning intoa multi-modal paradigm. In particular, we take inspiration from the recentsuccess of large language models, and propose using "captions" to integratehuman knowledge about the operator, expressed through natural languagedescriptions and equations. Also, we introduce a novel approach to train alanguage-model-like architecture, or directly fine-tune existing languagemodels, for in-context operator learning. We beat the baseline on single-modallearning tasks, and also demonstrated the effectiveness of multi-modal learningin enhancing performance and reducing function data requirements. The proposedmethod not only significantly improves in-context operator learning, but alsocreates a new path for the application of language models.</description><author>Liu Yang, Siting Liu, Stanley J. Osher</author><pubDate>Thu, 09 Nov 2023 17:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05061v3</guid></item><item><title>SeaTurtleID2022: A long-span dataset for reliable sea turtle re-identification</title><link>http://arxiv.org/abs/2311.05524v1</link><description>This paper introduces the first public large-scale, long-span dataset withsea turtle photographs captured in the wild -- SeaTurtleID2022(https://www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022). The datasetcontains 8729 photographs of 438 unique individuals collected within 13 years,making it the longest-spanned dataset for animal re-identification. Allphotographs include various annotations, e.g., identity, encounter timestamp,and body parts segmentation masks. Instead of standard "random" splits, thedataset allows for two realistic and ecologically motivated splits: (i) atime-aware closed-set with training, validation, and test data from differentdays/years, and (ii) a time-aware open-set with new unknown individuals in testand validation sets. We show that time-aware splits are essential forbenchmarking re-identification methods, as random splits lead to performanceoverestimation. Furthermore, a baseline instance segmentation andre-identification performance over various body parts is provided. Finally, anend-to-end system for sea turtle re-identification is proposed and evaluated.The proposed system based on Hybrid Task Cascade for head instance segmentationand ArcFace-trained feature-extractor achieved an accuracy of 86.8%.</description><author>Lukáš Adam, Vojtěch Čermák, Kostas Papafitsoros, Lukáš Picek</author><pubDate>Thu, 09 Nov 2023 17:10:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05524v1</guid></item><item><title>BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis</title><link>http://arxiv.org/abs/2311.05521v1</link><description>Synthesizing photorealistic 4D human head avatars from videos is essentialfor VR/AR, telepresence, and video game applications. Although existing NeuralRadiance Fields (NeRF)-based methods achieve high-fidelity results, thecomputational expense limits their use in real-time applications. To overcomethis limitation, we introduce BakedAvatar, a novel representation for real-timeneural head avatar synthesis, deployable in a standard polygon rasterizationpipeline. Our approach extracts deformable multi-layer meshes from learnedisosurfaces of the head and computes expression-, pose-, and view-dependentappearances that can be baked into static textures for efficient rasterization.We thus propose a three-stage pipeline for neural head avatar synthesis, whichincludes learning continuous deformation, manifold, and radiance fields,extracting layered meshes and textures, and fine-tuning texture details withdifferential rasterization. Experimental results demonstrate that ourrepresentation generates synthesis results of comparable quality to otherstate-of-the-art methods while significantly reducing the inference timerequired. We further showcase various head avatar synthesis results frommonocular videos, including view synthesis, face reenactment, expressionediting, and pose editing, all at interactive frame rates.</description><author>Hao-Bin Duan, Miao Wang, Jin-Chuan Shi, Xu-Chuan Chen, Yan-Pei Cao</author><pubDate>Thu, 09 Nov 2023 17:05:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05521v1</guid></item><item><title>HIC-YOLOv5: Improved YOLOv5 For Small Object Detection</title><link>http://arxiv.org/abs/2309.16393v2</link><description>Small object detection has been a challenging problem in the field of objectdetection. There has been some works that proposes improvements for this task,such as adding several attention blocks or changing the whole structure offeature fusion networks. However, the computation cost of these models islarge, which makes deploying a real-time object detection system unfeasible,while leaving room for improvement. To this end, an improved YOLOv5 model:HIC-YOLOv5 is proposed to address the aforementioned problems. Firstly, anadditional prediction head specific to small objects is added to provide ahigher-resolution feature map for better prediction. Secondly, an involutionblock is adopted between the backbone and neck to increase channel informationof the feature map. Moreover, an attention mechanism named CBAM is applied atthe end of the backbone, thus not only decreasing the computation cost comparedwith previous works but also emphasizing the important information in bothchannel and spatial domain. Our result shows that HIC-YOLOv5 has improvedmAP@[.5:.95] by 6.42% and mAP@0.5 by 9.38% on VisDrone-2019-DET dataset.</description><author>Shiyi Tang, Shu Zhang, Yini Fang</author><pubDate>Thu, 09 Nov 2023 17:01:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16393v2</guid></item><item><title>Metropolis Sampling for Constrained Diffusion Models</title><link>http://arxiv.org/abs/2307.05439v2</link><description>Denoising diffusion models have recently emerged as the predominant paradigmfor generative modelling on image domains. In addition, their extension toRiemannian manifolds has facilitated a range of applications across the naturalsciences. While many of these problems stand to benefit from the ability tospecify arbitrary, domain-informed constraints, this setting is not covered bythe existing (Riemannian) diffusion model methodology. Recent work hasattempted to address this issue by constructing novel noising processes basedon the reflected Brownian motion and logarithmic barrier methods. However, theassociated samplers are either computationally burdensome or only apply toconvex subsets of Euclidean space. In this paper, we introduce an alternative,simple noising scheme based on Metropolis sampling that affords substantialgains in computational efficiency and empirical performance compared to theearlier samplers. Of independent interest, we prove that this new processcorresponds to a valid discretisation of the reflected Brownian motion. Wedemonstrate the scalability and flexibility of our approach on a range ofproblem settings with convex and non-convex constraints, including applicationsfrom geospatial modelling, robotics and protein design.</description><author>Nic Fishman, Leo Klarner, Emile Mathieu, Michael Hutchinson, Valentin de Bortoli</author><pubDate>Thu, 09 Nov 2023 16:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05439v2</guid></item><item><title>FDAPT: Federated Domain-adaptive Pre-training for Language Models</title><link>http://arxiv.org/abs/2307.06933v2</link><description>Foundation models (FMs) have shown prominent success in a wide range oftasks. Their applicability to specific domain-task pairings relies on theavailability of, both, high-quality data and significant computationalresources. These challenges are not new to the field and, indeed, FederatedLearning (FL) has been shown to be a promising solution in similar setups. Thispaper tackles the specific case of Domain-Adaptive Pre-Training (DAPT), a keystep in the application of FMs. We conduct the first comprehensive empiricalstudy to evaluate the performance of Federated Domain-Adaptive Pre-Training(FDAPT). We demonstrate that FDAPT can maintain competitive downstream taskperformance to the centralized baseline in both IID and non-IID situations.Finally, we propose a novel algorithm, Frozen Federated Domain-AdaptivePre-Training (FFDAPT). FFDAPT improves the computational efficiency by 12.1% onaverage and exhibits similar downstream task performance to vanilla FDAPT, withgeneral performance fluctuations remaining less than 1%.</description><author>Lekang Jiang, Filip Svoboda, Nicholas D. Lane</author><pubDate>Thu, 09 Nov 2023 16:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06933v2</guid></item><item><title>From Learning Management System to Affective Tutoring system: a preliminary study</title><link>http://arxiv.org/abs/2311.05513v1</link><description>In this study, we investigate the combination of indicators, includingperformance, behavioral engagement, and emotional engagement, to identifystudents experiencing difficulties. We analyzed data from two primary sources:digital traces extracted from th e Learning Management System (LMS) and imagescaptured by students' webcams. The digital traces provided insights intostudents' interactions with the educational content, while the images wereutilized to analyze their emotional expressions during learnin g activities. Byutilizing real data collected from students at a French engineering school,recorded during the 2022 2023 academic year, we observed a correlation betweenpositive emotional states and improved academic outcomes. These preliminaryfindings support the notion that emotions play a crucial role indifferentiating between high achieving and low achieving students.</description><author>Nadaud Edouard, Geoffroy Thibault, Khelifi Tesnim, Yaacoub Antoun, Haidar Siba, Ben Rabah NourhÈne, Aubin Jean Pierre, Prevost Lionel, Le Grand Benedicte</author><pubDate>Thu, 09 Nov 2023 16:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05513v1</guid></item><item><title>Anytime-Constrained Reinforcement Learning</title><link>http://arxiv.org/abs/2311.05511v1</link><description>We introduce and study constrained Markov Decision Processes (cMDPs) withanytime constraints. An anytime constraint requires the agent to never violateits budget at any point in time, almost surely. Although Markovian policies areno longer sufficient, we show that there exist optimal deterministic policiesaugmented with cumulative costs. In fact, we present a fixed-parametertractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Ourreduction yields planning and learning algorithms that are time andsample-efficient for tabular cMDPs so long as the precision of the costs islogarithmic in the size of the cMDP. However, we also show that computingnon-trivial approximately optimal policies is NP-hard in general. To circumventthis bottleneck, we design provable approximation algorithms that efficientlycompute or learn an approximately feasible policy with optimal value so long asthe maximum supported cost is bounded by a polynomial in the cMDP or by theabsolute budget. Given our hardness results, our approximation guarantees arethe best possible in terms of tractability under worst-case analysis.</description><author>Jeremy McMahan, Xiaojin Zhu</author><pubDate>Thu, 09 Nov 2023 16:51:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05511v1</guid></item><item><title>Facing Off World Model Backbones: RNNs, Transformers, and S4</title><link>http://arxiv.org/abs/2307.02064v2</link><description>World models are a fundamental component in model-based reinforcementlearning (MBRL). To perform temporally extended and consistent simulations ofthe future in partially observable environments, world models need to possesslong-term memory. However, state-of-the-art MBRL agents, such as Dreamer,predominantly employ recurrent neural networks (RNNs) as their world modelbackbone, which have limited memory capacity. In this paper, we seek to explorealternative world model backbones for improving long-term memory. Inparticular, we investigate the effectiveness of Transformers and StructuredState Space Sequence (S4) models, motivated by their remarkable ability tocapture long-range dependencies in low-dimensional sequences and theircomplementary strengths. We propose S4WM, the first world model compatible withparallelizable SSMs including S4 and its variants. By incorporating latentvariable modeling, S4WM can efficiently generate high-dimensional imagesequences through latent imagination. Furthermore, we extensively compare RNN-,Transformer-, and S4-based world models across four sets of environments, whichwe have tailored to assess crucial memory capabilities of world models,including long-term imagination, context-dependent recall, reward prediction,and memory-based reasoning. Our findings demonstrate that S4WM outperformsTransformer-based world models in terms of long-term memory, while exhibitinggreater efficiency during training and imagination. These results pave the wayfor the development of stronger MBRL agents.</description><author>Fei Deng, Junyeong Park, Sungjin Ahn</author><pubDate>Thu, 09 Nov 2023 16:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02064v2</guid></item><item><title>A Generalized Variable Importance Metric and Estimator for Black Box Machine Learning Models</title><link>http://arxiv.org/abs/2212.09931v2</link><description>The aim of this study is to define importance of predictors for black boxmachine learning methods, where the prediction function can be complex andcannot be represented by statistical parameters. In this paper we defined a``Generalized Variable Importance Metric (GVIM)'' using the true conditionalexpectation function for a continuous or a binary response variable. We furthershowed that the defined GVIM can be represented as a function of theConditional Average Treatment Effect (CATE) for multinomial and continuouspredictors. Then we propose how the metric can be estimated using using anymachine learning models. Finally using simulations we evaluated the propertiesof the estimator when estimated from XGBoost, Random Forest and a mis-specifiedgeneralized additive model.</description><author>Mohammad Kaviul Anam Khan, Rafal Kustra</author><pubDate>Thu, 09 Nov 2023 16:43:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09931v2</guid></item><item><title>Dirichlet Active Learning</title><link>http://arxiv.org/abs/2311.05501v1</link><description>This work introduces Dirichlet Active Learning (DiAL), a Bayesian-inspiredapproach to the design of active learning algorithms. Our framework modelsfeature-conditional class probabilities as a Dirichlet random field and lendsobservational strength between similar features in order to calibrate therandom field. This random field can then be utilized in learning tasks: inparticular, we can use current estimates of mean and variance to conductclassification and active learning in the context where labeled data is scarce.We demonstrate the applicability of this model to low-label rate graph learningby constructing ``propagation operators'' based upon the graph Laplacian, andoffer computational studies demonstrating the method's competitiveness with thestate of the art. Finally, we provide rigorous guarantees regarding the abilityof this approach to ensure both exploration and exploitation, expressedrespectively in terms of cluster exploration and increased attention todecision boundaries.</description><author>Kevin Miller, Ryan Murray</author><pubDate>Thu, 09 Nov 2023 16:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05501v1</guid></item><item><title>Object-centric Cross-modal Feature Distillation for Event-based Object Detection</title><link>http://arxiv.org/abs/2311.05494v1</link><description>Event cameras are gaining popularity due to their unique properties, such astheir low latency and high dynamic range. One task where these benefits can becrucial is real-time object detection. However, RGB detectors still outperformevent-based detectors due to the sparsity of the event data and missing visualdetails. In this paper, we develop a novel knowledge distillation approach toshrink the performance gap between these two modalities. To this end, wepropose a cross-modality object detection distillation method that by designcan focus on regions where the knowledge distillation works best. We achievethis by using an object-centric slot attention mechanism that can iterativelydecouple features maps into object-centric features and correspondingpixel-features used for distillation. We evaluate our novel distillationapproach on a synthetic and a real event dataset with aligned grayscale imagesas a teacher modality. We show that object-centric distillation allows tosignificantly improve the performance of the event-based student objectdetector, nearly halving the performance gap with respect to the teacher.</description><author>Lei Li, Alexander Liniger, Mario Millhaeusler, Vagia Tsiminaki, Yuanyou Li, Dengxin Dai</author><pubDate>Thu, 09 Nov 2023 16:33:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05494v1</guid></item><item><title>General Policies, Subgoal Structure, and Planning Width</title><link>http://arxiv.org/abs/2311.05490v1</link><description>It has been observed that many classical planning domains with atomic goalscan be solved by means of a simple polynomial exploration procedure, called IW,that runs in time exponential in the problem width, which in these cases isbounded and small. Yet, while the notion of width has become part ofstate-of-the-art planning algorithms such as BFWS, there is no good explanationfor why so many benchmark domains have bounded width when atomic goals areconsidered. In this work, we address this question by relating bounded widthwith the existence of general optimal policies that in each planning instanceare represented by tuples of atoms of bounded size. We also define the notionsof (explicit) serializations and serialized width that have a broader scope asmany domains have a bounded serialized width but no bounded width. Suchproblems are solved non-optimally in polynomial time by a suitable variant ofthe Serialized IW algorithm. Finally, the language of general policies and thesemantics of serializations are combined to yield a simple, meaningful, andexpressive language for specifying serializations in compact form in the formof sketches, which can be used for encoding domain control knowledge by hand orfor learning it from small examples. Sketches express general problemdecompositions in terms of subgoals, and sketches of bounded width expressproblem decompositions that can be solved in polynomial time.</description><author>Blai Bonet, Hector Geffner</author><pubDate>Thu, 09 Nov 2023 16:30:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05490v1</guid></item><item><title>Disease Gene Prioritization With Quantum Walks</title><link>http://arxiv.org/abs/2311.05486v1</link><description>Disease gene prioritization assigns scores to genes or proteins according totheir likely relevance for a given disease based on a provided set of seedgenes. Here, we describe a new algorithm for disease gene prioritization basedon continuous-time quantum walks using the adjacency matrix of aprotein-protein interaction (PPI) network. Our algorithm can be seen as aquantum version of a previous method known as the diffusion kernel, but,importantly, has higher performance in predicting disease genes, and alsopermits the encoding of seed node self-loops into the underlying Hamiltonian,which offers yet another boost in performance. We demonstrate the success ofour proposed method by comparing it to several well-known gene prioritizationmethods on three disease sets, across seven different PPI networks. In order tocompare these methods, we use cross-validation and examine the mean reciprocalranks and recall values. We further validate our method by performing anenrichment analysis of the predicted genes for coronary artery disease. We alsoinvestigate the impact of adding self-loops to the seeds, and argue that theyallow the quantum walker to remain more local to low-degree seed nodes.</description><author>Harto Saarinen, Mark Goldsmith, Rui-Sheng Wang, Joseph Loscalzo, Sabrina Maniscalco</author><pubDate>Thu, 09 Nov 2023 16:21:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05486v1</guid></item><item><title>meta4: semantically-aligned generation of metaphoric gestures using self-supervised text and speech representation</title><link>http://arxiv.org/abs/2311.05481v1</link><description>Image Schemas are repetitive cognitive patterns that influence the way weconceptualize and reason about various concepts present in speech. Thesepatterns are deeply embedded within our cognitive processes and are reflectedin our bodily expressions including gestures. Particularly, metaphoric gesturespossess essential characteristics and semantic meanings that align with ImageSchemas, to visually represent abstract concepts. The shape and form ofgestures can convey abstract concepts, such as extending the forearm and handor tracing a line with hand movements to visually represent the image schema ofPATH. Previous behavior generation models have primarily focused on utilizingspeech (acoustic features and text) to drive the generation model of virtualagents. They have not considered key semantic information as those carried byImage Schemas to effectively generate metaphoric gestures. To address thislimitation, we introduce META4, a deep learning approach that generatesmetaphoric gestures from both speech and Image Schemas. Our approach has twoprimary goals: computing Image Schemas from input text to capture theunderlying semantic and metaphorical meaning, and generating metaphoricgestures driven by speech and the computed image schemas. Our approach is thefirst method for generating speech driven metaphoric gestures while leveragingthe potential of Image Schemas. We demonstrate the effectiveness of ourapproach and highlight the importance of both speech and image schemas inmodeling metaphoric gestures.</description><author>Mireille Fares, Catherine Pelachaud, Nicolas Obin</author><pubDate>Thu, 09 Nov 2023 16:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05481v1</guid></item><item><title>Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models</title><link>http://arxiv.org/abs/2104.07505v2</link><description>Recent research has demonstrated that large pre-trained language modelsreflect societal biases expressed in natural language. The present paperintroduces a simple method for probing language models to conduct amultilingual study of gender bias towards politicians. We quantify the usage ofadjectives and verbs generated by language models surrounding the names ofpoliticians as a function of their gender. To this end, we curate a dataset of250k politicians worldwide, including their names and gender. Our study isconducted in seven languages across six different language modelingarchitectures. The results demonstrate that pre-trained language models' stancetowards politicians varies strongly across analyzed languages. We find thatwhile some words such as dead, and designated are associated with both male andfemale politicians, a few specific words such as beautiful and divorced arepredominantly associated with female politicians. Finally, and contrary toprevious findings, our study suggests that larger language models do not tendto be significantly more gender-biased than smaller ones.</description><author>Karolina Stańczak, Sagnik Ray Choudhury, Tiago Pimentel, Ryan Cotterell, Isabelle Augenstein</author><pubDate>Thu, 09 Nov 2023 16:15:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.07505v2</guid></item><item><title>Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation</title><link>http://arxiv.org/abs/2305.14734v2</link><description>Grammatical error correction (GEC) is a well-explored problem in English withmany existing models and datasets. However, research on GEC in morphologicallyrich languages has been limited due to challenges such as data scarcity andlanguage complexity. In this paper, we present the first results on Arabic GECusing two newly developed Transformer-based pretrained sequence-to-sequencemodels. We also define the task of multi-class Arabic grammatical errordetection (GED) and present the first results on multi-class Arabic GED. Weshow that using GED information as an auxiliary input in GEC models improvesGEC performance across three datasets spanning different genres. Moreover, wealso investigate the use of contextual morphological preprocessing in aidingGEC systems. Our models achieve SOTA results on two Arabic GEC shared taskdatasets and establish a strong benchmark on a recently created dataset. Wemake our code, data, and pretrained models publicly available.</description><author>Bashar Alhafni, Go Inoue, Christian Khairallah, Nizar Habash</author><pubDate>Thu, 09 Nov 2023 16:10:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14734v2</guid></item><item><title>Retinal OCT Synthesis with Denoising Diffusion Probabilistic Models for Layer Segmentation</title><link>http://arxiv.org/abs/2311.05479v1</link><description>Modern biomedical image analysis using deep learning often encounters thechallenge of limited annotated data. To overcome this issue, deep generativemodels can be employed to synthesize realistic biomedical images. In thisregard, we propose an image synthesis method that utilizes denoising diffusionprobabilistic models (DDPMs) to automatically generate retinal opticalcoherence tomography (OCT) images. By providing rough layer sketches, thetrained DDPMs can generate realistic circumpapillary OCT images. We furtherfind that more accurate pseudo labels can be obtained through knowledgeadaptation, which greatly benefits the segmentation task. Through this, weobserve a consistent improvement in layer segmentation accuracy, which isvalidated using various neural networks. Furthermore, we have discovered that alayer segmentation model trained solely with synthesized images can achievecomparable results to a model trained exclusively with real images. Thesefindings demonstrate the promising potential of DDPMs in reducing the need formanual annotations of retinal OCT images.</description><author>Yuli Wu, Weidong He, Dennis Eschweiler, Ningxin Dou, Zixin Fan, Shengli Mi, Peter Walter, Johannes Stegmaier</author><pubDate>Thu, 09 Nov 2023 16:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05479v1</guid></item><item><title>Robust Retraining-free GAN Fingerprinting via Personalized Normalization</title><link>http://arxiv.org/abs/2311.05478v1</link><description>In recent years, there has been significant growth in the commercialapplications of generative models, licensed and distributed by model developersto users, who in turn use them to offer services. In this scenario, there is aneed to track and identify the responsible user in the presence of a violationof the license agreement or any kind of malicious usage. Although there aremethods enabling Generative Adversarial Networks (GANs) to include invisiblewatermarks in the images they produce, generating a model with a differentwatermark, referred to as a fingerprint, for each user is time- andresource-consuming due to the need to retrain the model to include the desiredfingerprint. In this paper, we propose a retraining-free GAN fingerprintingmethod that allows model developers to easily generate model copies with thesame functionality but different fingerprints. The generator is modified byinserting additional Personalized Normalization (PN) layers whose parameters(scaling and bias) are generated by two dedicated shallow networks (ParamGenNets) taking the fingerprint as input. A watermark decoder is trainedsimultaneously to extract the fingerprint from the generated images. Theproposed method can embed different fingerprints inside the GAN by justchanging the input of the ParamGen Nets and performing a feedforward pass,without finetuning or retraining. The performance of the proposed method interms of robustness against both model-level and image-level attacks is alsosuperior to the state-of-the-art.</description><author>Jianwei Fei, Zhihua Xia, Benedetta Tondi, Mauro Barni</author><pubDate>Thu, 09 Nov 2023 16:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05478v1</guid></item><item><title>Using ResNet to Utilize 4-class T2-FLAIR Slice Classification Based on the Cholinergic Pathways Hyperintensities Scale for Pathological Aging</title><link>http://arxiv.org/abs/2311.05477v1</link><description>The Cholinergic Pathways Hyperintensities Scale (CHIPS) is a visual ratingscale used to assess the extent of cholinergic white matter hyperintensities inT2-FLAIR images, serving as an indicator of dementia severity. However, themanual selection of four specific slices for rating throughout the entire brainis a time-consuming process. Our goal was to develop a deep learning-basedmodel capable of automatically identifying the four slices relevant to CHIPS.To achieve this, we trained a 4-class slice classification model (BSCA) usingthe ADNI T2-FLAIR dataset (N=150) with the assistance of ResNet. Subsequently,we tested the model's performance on a local dataset (N=30). The resultsdemonstrated the efficacy of our model, with an accuracy of 99.82% and anF1-score of 99.83%. This achievement highlights the potential impact of BSCA asan automatic screening tool, streamlining the selection of four specificT2-FLAIR slices that encompass white matter landmarks along the cholinergicpathways. Clinicians can leverage this tool to assess the risk of clinicaldementia development efficiently.</description><author>Wei-Chun Kevin Tsai, Yi-Chien Liu, Ming-Chun Yu, Chia-Ju Chou, Sui-Hing Yan, Yang-Teng Fan, Yan-Hsiang Huang, Yen-Ling Chiu, Yi-Fang Chuang, Ran-Zan Wang, Yao-Chia Shih</author><pubDate>Thu, 09 Nov 2023 16:08:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05477v1</guid></item><item><title>Do Ensembling and Meta-Learning Improve Outlier Detection in Randomized Controlled Trials?</title><link>http://arxiv.org/abs/2311.05473v1</link><description>Modern multi-centre randomized controlled trials (MCRCTs) collect massiveamounts of tabular data, and are monitored intensively for irregularities byhumans. We began by empirically evaluating 6 modern machine learning-basedoutlier detection algorithms on the task of identifying irregular data in 838datasets from 7 real-world MCRCTs with a total of 77,001 patients from over 44countries. Our results reinforce key findings from prior work in the outlierdetection literature on data from other domains. Existing algorithms oftensucceed at identifying irregularities without any supervision, with at leastone algorithm exhibiting positive performance 70.6% of the time. However,performance across datasets varies substantially with no single algorithmperforming consistently well, motivating new techniques for unsupervised modelselection or other means of aggregating potentially discordant predictions frommultiple candidate models. We propose the Meta-learned Probabilistic Ensemble(MePE), a simple algorithm for aggregating the predictions of multipleunsupervised models, and show that it performs favourably compared to recentmeta-learning approaches for outlier detection model selection. Whilemeta-learning shows promise, small ensembles outperform all forms ofmeta-learning on average, a negative result that may guide the application ofcurrent outlier detection approaches in healthcare and other real-worlddomains.</description><author>Walter Nelson, Jonathan Ranisau, Jeremy Petch</author><pubDate>Thu, 09 Nov 2023 16:05:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05473v1</guid></item><item><title>Text Representation Distillation via Information Bottleneck Principle</title><link>http://arxiv.org/abs/2311.05472v1</link><description>Pre-trained language models (PLMs) have recently shown great success in textrepresentation field. However, the high computational cost and high-dimensionalrepresentation of PLMs pose significant challenges for practical applications.To make models more accessible, an effective method is to distill large modelsinto smaller representation models. In order to relieve the issue ofperformance degradation after distillation, we propose a novel KnowledgeDistillation method called IBKD. This approach is motivated by the InformationBottleneck principle and aims to maximize the mutual information between thefinal representation of the teacher and student model, while simultaneouslyreducing the mutual information between the student model's representation andthe input data. This enables the student model to preserve important learnedinformation while avoiding unnecessary information, thus reducing the risk ofover-fitting. Empirical studies on two main downstream applications of textrepresentation (Semantic Textual Similarity and Dense Retrieval tasks)demonstrate the effectiveness of our proposed approach.</description><author>Yanzhao Zhang, Dingkun Long, Zehan Li, Pengjun Xie</author><pubDate>Thu, 09 Nov 2023 16:04:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05472v1</guid></item><item><title>GAN-generated Faces Detection: A Survey and New Perspectives</title><link>http://arxiv.org/abs/2202.07145v6</link><description>Generative Adversarial Networks (GAN) have led to the generation of veryrealistic face images, which have been used in fake social media accounts andother disinformation matters that can generate profound impacts. Therefore, thecorresponding GAN-face detection techniques are under active development thatcan examine and expose such fake faces. In this work, we aim to provide acomprehensive review of recent progress in GAN-face detection. We focus onmethods that can detect face images that are generated or synthesized from GANmodels. We classify the existing detection works into four categories: (1) deeplearning-based, (2) physical-based, (3) physiological-based methods, and (4)evaluation and comparison against human visual performance. For each category,we summarize the key ideas and connect them with method implementations. Wealso discuss open problems and suggest future research directions.</description><author>Xin Wang, Hui Guo, Shu Hu, Ming-Ching Chang, Siwei Lyu</author><pubDate>Thu, 09 Nov 2023 16:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.07145v6</guid></item><item><title>Interpreting CNN Predictions using Conditional Generative Adversarial Networks</title><link>http://arxiv.org/abs/2301.08067v3</link><description>We propose a novel method that trains a conditional Generative AdversarialNetwork (GAN) to generate visual interpretations of a Convolutional NeuralNetwork (CNN). To comprehend a CNN, the GAN is trained with information on howthe CNN processes an image when making predictions. Supplying that informationhas two main challenges: how to represent this information in a form that isfeedable to the GANs and how to effectively feed the representation to the GAN.To address these issues, we developed a suitable representation of CNNarchitectures by cumulatively averaging intermediate interpretation maps. Wealso propose two alternative approaches to feed the representations to the GANand to choose an effective training strategy. Our approach learned the generalaspects of CNNs and was agnostic to datasets and CNN architectures. The studyincludes both qualitative and quantitative evaluations and compares theproposed GANs with state-of-the-art approaches. We found that the initiallayers of CNNs and final layers are equally crucial for interpreting CNNs uponinterpreting the proposed GAN. We believe training a GAN to interpret CNNswould open doors for improved interpretations by leveraging fast-paced deeplearning advancements. The code used for experimentation is publicly availableat https://github.com/Akash-guna/Explain-CNN-With-GANS</description><author>R T Akash Guna, Raul Benitez, O K Sikha</author><pubDate>Thu, 09 Nov 2023 15:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08067v3</guid></item><item><title>3DStyle-Diffusion: Pursuing Fine-grained Text-driven 3D Stylization with 2D Diffusion Models</title><link>http://arxiv.org/abs/2311.05464v1</link><description>3D content creation via text-driven stylization has played a fundamentalchallenge to multimedia and graphics community. Recent advances of cross-modalfoundation models (e.g., CLIP) have made this problem feasible. Thoseapproaches commonly leverage CLIP to align the holistic semantics of stylizedmesh with the given text prompt. Nevertheless, it is not trivial to enable morecontrollable stylization of fine-grained details in 3D meshes solely based onsuch semantic-level cross-modal supervision. In this work, we propose a new3DStyle-Diffusion model that triggers fine-grained stylization of 3D mesheswith additional controllable appearance and geometric guidance from 2DDiffusion models. Technically, 3DStyle-Diffusion first parameterizes thetexture of 3D mesh into reflectance properties and scene lighting usingimplicit MLP networks. Meanwhile, an accurate depth map of each sampled view isachieved conditioned on 3D mesh. Then, 3DStyle-Diffusion leverages apre-trained controllable 2D Diffusion model to guide the learning of renderedimages, encouraging the synthesized image of each view semantically alignedwith text prompt and geometrically consistent with depth map. This wayelegantly integrates both image rendering via implicit MLP networks anddiffusion process of image synthesis in an end-to-end fashion, enabling ahigh-quality fine-grained stylization of 3D meshes. We also build a new datasetderived from Objaverse and the evaluation protocol for this task. Through bothqualitative and quantitative experiments, we validate the capability of our3DStyle-Diffusion. Source code and data are available at\url{https://github.com/yanghb22-fdu/3DStyle-Diffusion-Official}.</description><author>Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Tao Mei</author><pubDate>Thu, 09 Nov 2023 15:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05464v1</guid></item><item><title>ControlStyle: Text-Driven Stylized Image Generation Using Diffusion Priors</title><link>http://arxiv.org/abs/2311.05463v1</link><description>Recently, the multimedia community has witnessed the rise of diffusion modelstrained on large-scale multi-modal data for visual content creation,particularly in the field of text-to-image generation. In this paper, wepropose a new task for ``stylizing'' text-to-image models, namely text-drivenstylized image generation, that further enhances editability in contentcreation. Given input text prompt and style image, this task aims to producestylized images which are both semantically relevant to input text prompt andmeanwhile aligned with the style image in style. To achieve this, we present anew diffusion model (ControlStyle) via upgrading a pre-trained text-to-imagemodel with a trainable modulation network enabling more conditions of textprompts and style images. Moreover, diffusion style and content regularizationsare simultaneously introduced to facilitate the learning of this modulationnetwork with these diffusion priors, pursuing high-quality stylizedtext-to-image generation. Extensive experiments demonstrate the effectivenessof our ControlStyle in producing more visually pleasing and artistic results,surpassing a simple combination of text-to-image model and conventional styletransfer techniques.</description><author>Jingwen Chen, Yingwei Pan, Ting Yao, Tao Mei</author><pubDate>Thu, 09 Nov 2023 15:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05463v1</guid></item><item><title>Control3D: Towards Controllable Text-to-3D Generation</title><link>http://arxiv.org/abs/2311.05461v1</link><description>Recent remarkable advances in large-scale text-to-image diffusion models haveinspired a significant breakthrough in text-to-3D generation, pursuing 3Dcontent creation solely from a given text prompt. However, existing text-to-3Dtechniques lack a crucial ability in the creative process: interactivelycontrol and shape the synthetic 3D contents according to users' desiredspecifications (e.g., sketch). To alleviate this issue, we present the firstattempt for text-to-3D generation conditioning on the additional hand-drawnsketch, namely Control3D, which enhances controllability for users. Inparticular, a 2D conditioned diffusion model (ControlNet) is remoulded to guidethe learning of 3D scene parameterized as NeRF, encouraging each view of 3Dscene aligned with the given text prompt and hand-drawn sketch. Moreover, weexploit a pre-trained differentiable photo-to-sketch model to directly estimatethe sketch of the rendered image over synthetic 3D scene. Such estimated sketchalong with each sampled view is further enforced to be geometrically consistentwith the given sketch, pursuing better controllable text-to-3D generation.Through extensive experiments, we demonstrate that our proposal can generateaccurate and faithful 3D scenes that align closely with the input text promptsand sketches.</description><author>Yang Chen, Yingwei Pan, Yehao Li, Ting Yao, Tao Mei</author><pubDate>Thu, 09 Nov 2023 15:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05461v1</guid></item><item><title>Topological Learning in Multi-Class Data Sets</title><link>http://arxiv.org/abs/2301.09734v2</link><description>We specialize techniques from topological data analysis to the problem ofcharacterizing the topological complexity (as defined in the body of the paper)of a multi-class data set. As a by-product, a topological classifier is definedthat uses an open sub-covering of the data set. This sub-covering can be usedto construct a simplicial complex whose topological features (e.g., Bettinumbers) provide information about the classification problem. We use thesetopological constructs to study the impact of topological complexity onlearning in feedforward deep neural networks (DNNs). We hypothesize thattopological complexity is negatively correlated with the ability of a fullyconnected feedforward deep neural network to learn to classify data correctly.We evaluate our topological classification algorithm on multiple constructedand open source data sets. We also validate our hypothesis regarding therelationship between topological complexity and learning in DNN's on multipledata sets.</description><author>Christopher Griffin, Trevor Karn, Benjamin Apple</author><pubDate>Thu, 09 Nov 2023 15:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09734v2</guid></item><item><title>AP$n$P: A Less-constrained P$n$P Solver for Pose Estimation with Unknown Anisotropic Scaling or Focal Lengths</title><link>http://arxiv.org/abs/2310.09982v3</link><description>Perspective-$n$-Point (P$n$P) stands as a fundamental algorithm for poseestimation in various applications. In this paper, we present a new approach tothe P$n$P problem with relaxed constraints, eliminating the need for precise 3Dcoordinates or complete calibration data. We refer to it as AP$n$P due to itsability to handle unknown anisotropic scaling factors of 3D coordinates oralternatively two distinct focal lengths in addition to the conventional rigidtransformation. Through algebraic manipulations and a novel parametrization,both cases are brought into similar forms that distinguish themselves primarilyby the order of a rotation and an anisotropic scaling operation. AP$n$P thenboils down to one unique polynomial problem, which is solved by the Gr\"obnerbasis approach. Experimental results on both simulated and real datasetsdemonstrate the effectiveness of AP$n$P as a more flexible and practicalsolution to camera pose estimation. Code: https://github.com/goldoak/APnP.</description><author>Jiaxin Wei, Stefan Leutenegger, Laurent Kneip</author><pubDate>Thu, 09 Nov 2023 15:46:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09982v3</guid></item><item><title>Transformer-based Model for Oral Epithelial Dysplasia Segmentation</title><link>http://arxiv.org/abs/2311.05452v1</link><description>Oral epithelial dysplasia (OED) is a premalignant histopathological diagnosisgiven to lesions of the oral cavity. OED grading is subject to largeinter/intra-rater variability, resulting in the under/over-treatment ofpatients. We developed a new Transformer-based pipeline to improve detectionand segmentation of OED in haematoxylin and eosin (H&amp;E) stained whole slideimages (WSIs). Our model was trained on OED cases (n = 260) and controls (n =105) collected using three different scanners, and validated on test data fromthree external centres in the United Kingdom and Brazil (n = 78). Our internalexperiments yield a mean F1-score of 0.81 for OED segmentation, which reducedslightly to 0.71 on external testing, showing good generalisability, andgaining state-of-the-art results. This is the first externally validated studyto use Transformers for segmentation in precancerous histology images. Ourpublicly available model shows great promise to be the first step of afully-integrated pipeline, allowing earlier and more efficient OED diagnosis,ultimately benefiting patient outcomes.</description><author>Adam J Shephard, Hanya Mahmood, Shan E Ahmed Raza, Anna Luiza Damaceno Araujo, Alan Roger Santos-Silva, Marcio Ajudarte Lopes, Pablo Agustin Vargas, Kris McCombe, Stephanie Craig, Jacqueline James, Jill Brooks, Paul Nankivell, Hisham Mehanna, Syed Ali Khurram, Nasir M Rajpoot</author><pubDate>Thu, 09 Nov 2023 15:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05452v1</guid></item><item><title>All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation</title><link>http://arxiv.org/abs/2311.05451v1</link><description>Fairness in Language Models (LMs) remains a longstanding challenge, given theinherent biases in training data that can be perpetuated by models and affectthe downstream tasks. Recent methods employ expensive retraining or attemptdebiasing during inference by constraining model outputs to contrast from areference set of biased templates or exemplars. Regardless, they dont addressthe primary goal of fairness to maintain equitability across differentdemographic groups. In this work, we posit that inferencing LMs to generateunbiased output for one demographic under a context ensues from being aware ofoutputs for other demographics under the same context. To this end, we proposeCounterfactually Aware Fair InferencE (CAFIE), a framework that dynamicallycompares the model understanding of diverse demographics to generate moreequitable sentences. We conduct an extensive empirical evaluation using baseLMs of varying sizes and across three diverse datasets and found that CAFIEoutperforms strong baselines. CAFIE produces fairer text and strikes the bestbalance between fairness and language modeling capability</description><author>Pragyan Banerjee, Abhinav Java, Surgan Jandial, Simra Shahid, Shaz Furniturewala, Balaji Krishnamurthy, Sumit Bhatia</author><pubDate>Thu, 09 Nov 2023 15:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05451v1</guid></item><item><title>Cognitively Inspired Components for Social Conversational Agents</title><link>http://arxiv.org/abs/2311.05450v1</link><description>Current conversational agents (CA) have seen improvement in conversationalquality in recent years due to the influence of large language models (LLMs)like GPT3. However, two key categories of problem remain. Firstly there are theunique technical problems resulting from the approach taken in creating the CA,such as scope with retrieval agents and the often nonsensical answers of formergenerative agents. Secondly, humans perceive CAs as social actors, and as aresult expect the CA to adhere to social convention. Failure on the part of theCA in this respect can lead to a poor interaction and even the perception ofthreat by the user. As such, this paper presents a survey highlighting apotential solution to both categories of problem through the introduction ofcognitively inspired additions to the CA. Through computational facsimiles ofsemantic and episodic memory, emotion, working memory, and the ability tolearn, it is possible to address both the technical and social problemsencountered by CAs.</description><author>Alex Clay, Eduardo Alonso, Esther Mondragón</author><pubDate>Thu, 09 Nov 2023 15:38:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05450v1</guid></item><item><title>Variational Denoising for Variational Quantum Eigensolver</title><link>http://arxiv.org/abs/2304.00549v2</link><description>The variational quantum eigensolver (VQE) is a hybrid algorithm that has thepotential to provide a quantum advantage in practical chemistry problems thatare currently intractable on classical computers. VQE trains parameterizedquantum circuits using a classical optimizer to approximate the eigenvalues andeigenstates of a given Hamiltonian. However, VQE faces challenges intask-specific design and machine-specific architecture, particularly whenrunning on noisy quantum devices. This can have a negative impact on itstrainability, accuracy, and efficiency, resulting in noisy quantum data. Wepropose variational denoising, an unsupervised learning method that employs aparameterized quantum neural network to improve the solution of VQE by learningfrom noisy VQE outputs. Our approach can significantly decrease energyestimation errors and increase fidelities with ground states compared to noisyinput data for the $\text{H}_2$, LiH, and $\text{BeH}_2$ molecularHamiltonians, and the transverse field Ising model. Surprisingly, it onlyrequires noisy data for training. Variational denoising can be integrated intoquantum hardware, increasing its versatility as an end-to-end quantumprocessing for quantum data.</description><author>Quoc Hoan Tran, Shinji Kikuchi, Hirotaka Oshima</author><pubDate>Thu, 09 Nov 2023 15:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00549v2</guid></item><item><title>Multi-modal Graph Learning over UMLS Knowledge Graphs</title><link>http://arxiv.org/abs/2307.04461v2</link><description>Clinicians are increasingly looking towards machine learning to gain insightsabout patient evolutions. We propose a novel approach named Multi-Modal UMLSGraph Learning (MMUGL) for learning meaningful representations of medicalconcepts using graph neural networks over knowledge graphs based on the unifiedmedical language system. These representations are aggregated to represententire patient visits and then fed into a sequence model to perform predictionsat the granularity of multiple hospital visits of a patient. We improveperformance by incorporating prior medical knowledge and considering multiplemodalities. We compare our method to existing architectures proposed to learnrepresentations at different granularities on the MIMIC-III dataset and showthat our approach outperforms these methods. The results demonstrate thesignificance of multi-modal medical concept representations based on priormedical knowledge.</description><author>Manuel Burger, Gunnar Rätsch, Rita Kuznetsova</author><pubDate>Thu, 09 Nov 2023 15:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04461v2</guid></item><item><title>On Characterizing the Evolution of Embedding Space of Neural Networks using Algebraic Topology</title><link>http://arxiv.org/abs/2311.04592v2</link><description>We study how the topology of feature embedding space changes as it passesthrough the layers of a well-trained deep neural network (DNN) through Bettinumbers. Motivated by existing studies using simplicial complexes on shallowfully connected networks (FCN), we present an extended analysis using Cubicalhomology instead, with a variety of popular deep architectures and real imagedatasets. We demonstrate that as depth increases, a topologically complicateddataset is transformed into a simple one, resulting in Betti numbers attainingtheir lowest possible value. The rate of decay in topological complexity (as ametric) helps quantify the impact of architectural choices on thegeneralization ability. Interestingly from a representation learningperspective, we highlight several invariances such as topological invariance of(1) an architecture on similar datasets; (2) embedding space of a dataset forarchitectures of variable depth; (3) embedding space to input resolution/size,and (4) data sub-sampling. In order to further demonstrate the link betweenexpressivity \&amp; the generalization capability of a network, we consider thetask of ranking pre-trained models for downstream classification task (transferlearning). Compared to existing approaches, the proposed metric has a bettercorrelation to the actually achievable accuracy via fine-tuning the pre-trainedmodel.</description><author>Suryaka Suresh, Bishshoy Das, Vinayak Abrol, Sumantra Dutta Roy</author><pubDate>Thu, 09 Nov 2023 15:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04592v2</guid></item><item><title>A Practical Approach to Novel Class Discovery in Tabular Data</title><link>http://arxiv.org/abs/2311.05440v1</link><description>The problem of Novel Class Discovery (NCD) consists in extracting knowledgefrom a labeled set of known classes to accurately partition an unlabeled set ofnovel classes. While NCD has recently received a lot of attention from thecommunity, it is often solved on computer vision problems and under unrealisticconditions. In particular, the number of novel classes is usually assumed to beknown in advance, and their labels are sometimes used to tune hyperparameters.Methods that rely on these assumptions are not applicable in real-worldscenarios. In this work, we focus on solving NCD in tabular data when no priorknowledge of the novel classes is available. To this end, we propose to tunethe hyperparameters of NCD methods by adapting the $k$-fold cross-validationprocess and hiding some of the known classes in each fold. Since we have foundthat methods with too many hyperparameters are likely to overfit these hiddenclasses, we define a simple deep NCD model. This method is composed of only theessential elements necessary for the NCD problem and performs impressively wellunder realistic conditions. Furthermore, we find that the latent space of thismethod can be used to reliably estimate the number of novel classes.Additionally, we adapt two unsupervised clustering algorithms ($k$-means andSpectral Clustering) to leverage the knowledge of the known classes. Extensiveexperiments are conducted on 7 tabular datasets and demonstrate theeffectiveness of the proposed method and hyperparameter tuning process, andshow that the NCD problem can be solved without relying on knowledge from thenovel classes.</description><author>Colin Troisemaine, Alexandre Reiffers-Masson, Stéphane Gosselin, Vincent Lemaire, Sandrine Vaton</author><pubDate>Thu, 09 Nov 2023 15:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05440v1</guid></item><item><title>LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents</title><link>http://arxiv.org/abs/2311.05437v1</link><description>LLaVA-Plus is a general-purpose multimodal assistant that expands thecapabilities of large multimodal models. It maintains a skill repository ofpre-trained vision and vision-language models and can activate relevant toolsbased on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained onmultimodal instruction-following data to acquire the ability to use tools,covering visual understanding, generation, external knowledge retrieval, andcompositions. Empirical results show that LLaVA-Plus outperforms LLaVA inexisting capabilities and exhibits new ones. It is distinct in that the imagequery is directly grounded and actively engaged throughout the entire human-AIinteraction sessions, significantly improving tool use performance and enablingnew scenarios.</description><author>Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li</author><pubDate>Thu, 09 Nov 2023 15:22:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05437v1</guid></item><item><title>Fair Wasserstein Coresets</title><link>http://arxiv.org/abs/2311.05436v1</link><description>Recent technological advancements have given rise to the ability ofcollecting vast amounts of data, that often exceed the capacity of commonlyused machine learning algorithms. Approaches such as coresets and syntheticdata distillation have emerged as frameworks to generate a smaller, yetrepresentative, set of samples for downstream training. As machine learning isincreasingly applied to decision-making processes, it becomes imperative formodelers to consider and address biases in the data concerning subgroupsdefined by factors like race, gender, or other sensitive attributes. Currentapproaches focus on creating fair synthetic representative samples byoptimizing local properties relative to the original samples. These methods,however, are not guaranteed to positively affect the performance or fairness ofdownstream learning processes. In this work, we present Fair WassersteinCoresets (FWC), a novel coreset approach which generates fair syntheticrepresentative samples along with sample-level weights to be used in downstreamlearning tasks. FWC aims to minimize the Wasserstein distance between theoriginal datasets and the weighted synthetic samples while enforcing (anempirical version of) demographic parity, a prominent criterion for algorithmicfairness, via a linear constraint. We show that FWC can be thought of as aconstrained version of Lloyd's algorithm for k-medians or k-means clustering.Our experiments, conducted on both synthetic and real datasets, demonstrate thescalability of our approach and highlight the competitive performance of FWCcompared to existing fair clustering approaches, even when attempting toenhance the fairness of the latter through fair pre-processing techniques.</description><author>Zikai Xiong, Niccolò Dalmasso, Vamsi K. Potluru, Tucker Balch, Manuela Veloso</author><pubDate>Thu, 09 Nov 2023 15:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05436v1</guid></item><item><title>Parkinson's Disease Detection through Vocal Biomarkers and Advanced Machine Learning Algorithms: A Comprehensive Study</title><link>http://arxiv.org/abs/2311.05435v1</link><description>Parkinson's disease (PD) is a prevalent neurodegenerative disorder known forits impact on motor neurons, causing symptoms like tremors, stiffness, and gaitdifficulties. This study explores the potential of vocal feature alterations inPD patients as a means of early disease prediction. This research aims topredict the onset of Parkinson's disease. Utilizing a variety of advancedmachine-learning algorithms, including XGBoost, LightGBM, Bagging, AdaBoost,and Support Vector Machine, among others, the study evaluates the predictiveperformance of these models using metrics such as accuracy, area under thecurve (AUC), sensitivity, and specificity. The findings of this comprehensiveanalysis highlight LightGBM as the most effective model, achieving animpressive accuracy rate of 96%, alongside a matching AUC of 96%. LightGBMexhibited a remarkable sensitivity of 100% and specificity of 94.43%,surpassing other machine learning algorithms in accuracy and AUC scores. Giventhe complexities of Parkinson's disease and its challenges in early diagnosis,this study underscores the significance of leveraging vocal biomarkers coupledwith advanced machine-learning techniques for precise and timely PD detection.</description><author>Md Abu Sayed, Sabbir Ahamed, Duc M Cao, Md Eyasin Ul Islam Pavel, Malay Sarkar, Md Tuhin Mia</author><pubDate>Thu, 09 Nov 2023 15:21:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05435v1</guid></item><item><title>Price Interpretability of Prediction Markets: A Convergence Analysis</title><link>http://arxiv.org/abs/2205.08913v2</link><description>Prediction markets are long known for prediction accuracy. This studysystematically explores the fundamental properties of prediction markets,addressing questions about their information aggregation process and thefactors contributing to their remarkable efficacy. We propose a novelmultivariate utility (MU) based mechanism that unifies several existingautomated market-making schemes. Using this mechanism, we establish theconvergence results for markets comprised of risk-averse traders who haveheterogeneous beliefs and repeatedly interact with the market maker. Wedemonstrate that the resulting limiting wealth distribution aligns with thePareto efficient frontier defined by the utilities of all market participants.With the help of this result, we establish analytical and numerical results forthe limiting price in different market models. Specifically, we show that thelimiting price converges to the geometric mean of agent beliefs in exponentialutility-based markets. In risk-measure-based markets, we construct a family ofrisk measures that satisfy the convergence criteria and prove that the pricecan converge to a unique level represented by the weighted power mean of agentbeliefs. In broader markets with Constant Relative Risk Aversion (CRRA)utilities, we reveal that the limiting price can be characterized by systems ofequations that encapsulate agent beliefs, risk parameters, and wealth. Despitethe potential impact of traders' trading sequences on the limiting price, weestablish a price invariance result for markets with a large trader population.Using this result, we propose an efficient approximation scheme for thelimiting price.</description><author>Dian Yu, Jianjun Gao, Weiping Wu, Zizhuo Wang</author><pubDate>Thu, 09 Nov 2023 15:19:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.08913v2</guid></item><item><title>Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT</title><link>http://arxiv.org/abs/2308.06294v2</link><description>We hypothesize that large language models (LLMs) based on the transformerarchitecture can enable automated detection of clinical phenotype terms,including terms not documented in the HPO. In this study, we developed twotypes of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERTas its pre-trained model, and PhenoGPT, a GPT-based model that can beinitialized from diverse GPT models, including open-source versions such asGPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 andGPT-3.5. We compared our methods with PhenoTagger, a recently developed HPOrecognition tool that combines rule-based and deep learning methods. We foundthat our methods can extract more phenotype concepts, including novel ones notcharacterized by HPO. We also performed case studies on biomedical literatureto illustrate how new phenotype information can be recognized and extracted. Wecompared current BERT-based versus GPT-based models for phenotype tagging, inmultiple aspects including model architecture, memory usage, speed, accuracy,and privacy protection. We also discussed the addition of a negation step andan HPO normalization layer to the transformer models for improved HPO termtagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discoveryof phenotype terms from clinical notes and biomedical literature, facilitatingautomated downstream tasks to derive new biological insights on human diseases.</description><author>Jingye Yang, Cong Liu, Wendy Deng, Da Wu, Chunhua Weng, Yunyun Zhou, Kai Wang</author><pubDate>Thu, 09 Nov 2023 15:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06294v2</guid></item><item><title>AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban Planning via Consensus-based Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2310.16772v2</link><description>In urban planning, land use readjustment plays a pivotal role in aligningland use configurations with the current demands for sustainable urbandevelopment. However, present-day urban planning practices face two mainissues. Firstly, land use decisions are predominantly dependent on humanexperts. Besides, while resident engagement in urban planning can promote urbansustainability and livability, it is challenging to reconcile the diverseinterests of stakeholders. To address these challenges, we introduce aConsensus-based Multi-Agent Reinforcement Learning framework for real-worldland use readjustment. This framework serves participatory urban planning,allowing diverse intelligent agents as stakeholder representatives to vote forpreferred land use types. Within this framework, we propose a novel consensusmechanism in reward design to optimize land utilization through collectivedecision making. To abstract the structure of the complex urban system, thegeographic information of cities is transformed into a spatial graph structureand then processed by graph neural networks. Comprehensive experiments on bothtraditional top-down planning and participatory planning methods fromreal-world communities indicate that our computational framework enhancesglobal benefits and accommodates diverse interests, leading to improvedsatisfaction across different demographic groups. By integrating Multi-AgentReinforcement Learning, our framework ensures that participatory urban planningdecisions are more dynamic and adaptive to evolving community needs andprovides a robust platform for automating complex real-world urban planningprocesses.</description><author>Kejiang Qian, Lingjun Mao, Xin Liang, Yimin Ding, Jin Gao, Xinran Wei, Ziyi Guo, Jiajie Li</author><pubDate>Thu, 09 Nov 2023 15:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16772v2</guid></item><item><title>Convolutional Neural Networks for Automatic Detection of Intact Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles</title><link>http://arxiv.org/abs/2310.19630v3</link><description>Regular monitoring of the primary particles and purity profiles of a drugproduct during development and manufacturing processes is essential formanufacturers to avoid product variability and contamination. Transmissionelectron microscopy (TEM) imaging helps manufacturers predict how changesaffect particle characteristics and purity for virus-based gene therapy vectorproducts and intermediates. Since intact particles can characterize efficaciousproducts, it is beneficial to automate the detection of intact adenovirusagainst a non-intact-viral background mixed with debris, broken, and artefactparticles. In the presence of such particles, detecting intact adenovirusesbecomes more challenging. To overcome the challenge, due to such a presence, wedeveloped a software tool for semi-automatic annotation and segmentation ofadenoviruses and a software tool for automatic segmentation and detection ofintact adenoviruses in TEM imaging systems. The developed semi-automatic toolexploited conventional image analysis techniques while the automatic tool wasbuilt based on convolutional neural networks and image analysis techniques. Ourquantitative and qualitative evaluations showed outstanding true positivedetection rates compared to false positive and negative rates whereadenoviruses were nicely detected without mistaking them for real debris,broken adenoviruses, and/or staining artefacts.</description><author>Olivier Rukundo, Andrea Behanova, Riccardo De Feo, Seppo Ronkko, Joni Oja, Jussi Tohka</author><pubDate>Thu, 09 Nov 2023 15:18:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19630v3</guid></item><item><title>Dual Pipeline Style Transfer with Input Distribution Differentiation</title><link>http://arxiv.org/abs/2311.05432v1</link><description>The color and texture dual pipeline architecture (CTDP) suppresses texturerepresentation and artifacts through masked total variation loss (Mtv), andfurther experiments have shown that smooth input can almost completelyeliminate texture representation. We have demonstrated through experiments thatsmooth input is not the key reason for removing texture representations, butrather the distribution differentiation of the training dataset. Based on this,we propose an input distribution differentiation training strategy (IDD), whichforces the generation of textures to be completely dependent on the noisedistribution, while the smooth distribution will not produce textures at all.Overall, our proposed distribution differentiation training strategy allows fortwo pre-defined input distributions to be responsible for two generation tasks,with noise distribution responsible for texture generation and smoothdistribution responsible for color smooth transfer. Finally, we choose a smoothdistribution as the input for the forward inference stage to completelyeliminate texture representations and artifacts in color transfer tasks.</description><author>ShiQi Jiang, JunJie Kang, YuJian Li</author><pubDate>Thu, 09 Nov 2023 15:17:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05432v1</guid></item><item><title>Taxonomy for Resident Space Objects in LEO: A Deep Learning Approach</title><link>http://arxiv.org/abs/2311.05430v1</link><description>The increasing number of RSOs has raised concerns about the risk ofcollisions and catastrophic incidents for all direct and indirect users ofspace. To mitigate this issue, it is essential to have a good understanding ofthe various RSOs in orbit and their behaviour. A well-established taxonomydefining several classes of RSOs is a critical step in achieving thisunderstanding. This taxonomy helps assign objects to specific categories basedon their main characteristics, leading to better tracking services.Furthermore, a well-established taxonomy can facilitate research and analysisprocesses by providing a common language and framework for better understandingthe factors that influence RSO behaviour in space. These factors, in turn, helpdesign more efficient and effective strategies for space traffic management.Our work proposes a new taxonomy for RSOs focusing on the low Earth orbitregime to enhance space traffic management. In addition, we present a deeplearning-based model that uses an autoencoder architecture to reduce thefeatures representing the characteristics of the RSOs. The autoencodergenerates a lower-dimensional space representation that is then explored usingtechniques such as Uniform Manifold Approximation and Projection to identifyfundamental clusters of RSOs based on their unique characteristics. Thisapproach captures the complex and non-linear relationships between the featuresand the RSOs' classes identified. Our proposed taxonomy and model offer asignificant contribution to the ongoing efforts to mitigate the overall risksposed by the increasing number of RSOs in orbit.</description><author>Marta Guimarães, Cláudia Soares, Chiara Manfletti</author><pubDate>Thu, 09 Nov 2023 15:14:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05430v1</guid></item><item><title>Principles from Clinical Research for NLP Model Generalization</title><link>http://arxiv.org/abs/2311.03663v2</link><description>The NLP community typically relies on performance of a model on a held-outtest set to assess generalization. Performance drops observed in datasetsoutside of official test sets are generally attributed to"out-of-distribution'' effects. Here, we explore the foundations ofgeneralizability and study the various factors that affect it, articulatinggeneralizability lessons from clinical studies. In clinical researchgeneralizability depends on (a) internal validity of experiments to ensurecontrolled measurement of cause and effect, and (b) external validity ortransportability of the results to the wider population. We present the need toensure internal validity when building machine learning models in naturallanguage processing, especially where results may be impacted by spuriouscorrelations in the data. We demonstrate how spurious factors, such as thedistance between entities in relation extraction tasks, can affect modelinternal validity and in turn adversely impact generalization. We also offerguidance on how to analyze generalization failures.</description><author>Aparna Elangovan, Jiayuan He, Yuan Li, Karin Verspoor</author><pubDate>Thu, 09 Nov 2023 15:09:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03663v2</guid></item><item><title>Anthropomorphic Grasping with Neural Object Shape Completion</title><link>http://arxiv.org/abs/2311.02510v2</link><description>The progressive prevalence of robots in human-suited environments has givenrise to a myriad of object manipulation techniques, in which dexterity plays aparamount role. It is well-established that humans exhibit extraordinarydexterity when handling objects. Such dexterity seems to derive from a robustunderstanding of object properties (such as weight, size, and shape), as wellas a remarkable capacity to interact with them. Hand postures commonlydemonstrate the influence of specific regions on objects that need to begrasped, especially when objects are partially visible. In this work, weleverage human-like object understanding by reconstructing and completing theirfull geometry from partial observations, and manipulating them using a 7-DoFanthropomorphic robot hand. Our approach has significantly improved thegrasping success rates of baselines with only partial reconstruction by nearly30% and achieved over 150 successful grasps with three different objectcategories. This demonstrates our approach's consistent ability to predict andexecute grasping postures based on the completed object shapes from variousdirections and positions in real-world scenarios. Our work opens up newpossibilities for enhancing robotic applications that require precise graspingand manipulation skills of real-world reconstructed objects.</description><author>Diego Hidalgo-Carvajal, Hanzhi Chen, Gemma C. Bettelani, Jaesug Jung, Melissa Zavaglia, Laura Busse, Abdeldjallil Naceri, Stefan Leutenegger, Sami Haddadin</author><pubDate>Thu, 09 Nov 2023 15:06:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02510v2</guid></item><item><title>Statistical Learning of Conjunction Data Messages Through a Bayesian Non-Homogeneous Poisson Process</title><link>http://arxiv.org/abs/2311.05426v1</link><description>Current approaches for collision avoidance and space traffic management facemany challenges, mainly due to the continuous increase in the number of objectsin orbit and the lack of scalable and automated solutions. To avoidcatastrophic incidents, satellite owners/operators must be aware of theirassets' collision risk to decide whether a collision avoidance manoeuvre needsto be performed. This process is typically executed through the use of warningsissued in the form of CDMs which contain information about the event, such asthe expected TCA and the probability of collision. Our previous work presenteda statistical learning model that allowed us to answer two important questions:(1) Will any new conjunctions be issued in the next specified time interval?(2) When and with what uncertainty will the next CDM arrive? However, the modelwas based on an empirical Bayes homogeneous Poisson process, which assumes thatthe arrival rates of CDMs are constant over time. In fact, the rate at whichthe CDMs are issued depends on the behaviour of the objects as well as on thescreening process performed by third parties. Thus, in this work, we extend theprevious study and propose a Bayesian non-homogeneous Poisson processimplemented with high precision using a Probabilistic Programming Language tofully describe the underlying phenomena. We compare the proposed solution witha baseline model to demonstrate the added value of our approach. The resultsshow that this problem can be successfully modelled by our Bayesiannon-homogeneous Poisson Process with greater accuracy, contributing to thedevelopment of automated collision avoidance systems and helping operatorsreact timely but sparingly with satellite manoeuvres.</description><author>Marta Guimarães, Cláudia Soares, Chiara Manfletti</author><pubDate>Thu, 09 Nov 2023 15:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05426v1</guid></item><item><title>Active Mining Sample Pair Semantics for Image-text Matching</title><link>http://arxiv.org/abs/2311.05425v1</link><description>Recently, commonsense learning has been a hot topic in image-text matching.Although it can describe more graphic correlations, commonsense learning stillhas some shortcomings: 1) The existing methods are based on triplet semanticsimilarity measurement loss, which cannot effectively match the intractablenegative in image-text sample pairs. 2) The weak generalization ability of themodel leads to the poor effect of image and text matching on large-scaledatasets. According to these shortcomings. This paper proposes a novelimage-text matching model, called Active Mining Sample Pair Semanticsimage-text matching model (AMSPS). Compared with the single semantic learningmode of the commonsense learning model with triplet loss function, AMSPS is anactive learning idea. Firstly, the proposed Adaptive Hierarchical ReinforcementLoss (AHRL) has diversified learning modes. Its active learning mode enablesthe model to more focus on the intractable negative samples to enhance thediscriminating ability. In addition, AMSPS can also adaptively mine more hiddenrelevant semantic representations from uncommented items, which greatlyimproves the performance and generalization ability of the model. Experimentalresults on Flickr30K and MSCOCO universal datasets show that our proposedmethod is superior to advanced comparison methods.</description><author>Yongfeng Chena, Jin Liua, Zhijing Yang, Ruihan Chena, Junpeng Tan</author><pubDate>Thu, 09 Nov 2023 15:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05425v1</guid></item><item><title>Diffusion Based Causal Representation Learning</title><link>http://arxiv.org/abs/2311.05421v1</link><description>Causal reasoning can be considered a cornerstone of intelligent systems.Having access to an underlying causal graph comes with the promise ofcause-effect estimation and the identification of efficient and safeinterventions. However, learning causal representations remains a majorchallenge, due to the complexity of many real-world systems. Previous works oncausal representation learning have mostly focused on Variational Auto-Encoders(VAE). These methods only provide representations from a point estimate, andthey are unsuitable to handle high dimensions. To overcome these problems, weproposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm.This algorithm uses diffusion-based representations for causal discovery. DCRLoffers access to infinite dimensional latent codes, which encode differentlevels of information in the latent code. In a first proof of principle, weinvestigate the use of DCRL for causal representation learning. We furtherdemonstrate experimentally that this approach performs comparably well inidentifying the causal structure and causal variables.</description><author>Amir Mohammad Karimi Mamaghan, Andrea Dittadi, Stefan Bauer, Karl Henrik Johansson, Francesco Quinzan</author><pubDate>Thu, 09 Nov 2023 14:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05421v1</guid></item><item><title>Counterfactually Fair Representation</title><link>http://arxiv.org/abs/2311.05420v1</link><description>The use of machine learning models in high-stake applications (e.g.,healthcare, lending, college admission) has raised growing concerns due topotential biases against protected social groups. Various fairness notions andmethods have been proposed to mitigate such biases. In this work, we focus onCounterfactual Fairness (CF), a fairness notion that is dependent on anunderlying causal graph and first proposed by Kusner \textit{etal.}~\cite{kusner2017counterfactual}; it requires that the outcome anindividual perceives is the same in the real world as it would be in a"counterfactual" world, in which the individual belongs to another socialgroup. Learning fair models satisfying CF can be challenging. It was shown in\cite{kusner2017counterfactual} that a sufficient condition for satisfying CFis to \textbf{not} use features that are descendants of sensitive attributes inthe causal graph. This implies a simple method that learns CF models only usingnon-descendants of sensitive attributes while eliminating all descendants.Although several subsequent works proposed methods that use all features fortraining CF models, there is no theoretical guarantee that they can satisfy CF.In contrast, this work proposes a new algorithm that trains models using allthe available features. We theoretically and empirically show that modelstrained with this method can satisfy CF\footnote{The code repository for thiswork can be found in\url{https://github.com/osu-srml/CF_Representation_Learning}}.</description><author>Zhiqun Zuo, Mohammad Mahdi Khalili, Xueru Zhang</author><pubDate>Thu, 09 Nov 2023 14:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05420v1</guid></item></channel></rss>