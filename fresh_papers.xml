<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 22 Apr 2024 06:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MoVA: Adapting Mixture of Vision Experts to Multimodal Context</title><link>http://arxiv.org/abs/2404.13046v1</link><description>As the key component in multimodal large language models (MLLMs), the abilityof the visual encoder greatly affects MLLM's understanding on diverse imagecontent. Although some large-scale pretrained vision encoders such as visionencoders in CLIP and DINOv2 have brought promising performance, we found thatthere is still no single vision encoder that can dominate various image contentunderstanding, e.g., the CLIP vision encoder leads to outstanding results ongeneral image understanding but poor performance on document or chart content.To alleviate the bias of CLIP vision encoder, we first delve into the inherentbehavior of different pre-trained vision encoders and then propose the MoVA, apowerful and novel MLLM, adaptively routing and fusing task-specific visionexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we designa context-aware expert routing strategy to dynamically select the most suitablevision experts according to the user instruction, input image, and expertise ofvision experts. This benefits from the powerful model function understandingability of the large language model (LLM) equipped with expert-routing low-rankadaptation (LoRA). In the fine-grained stage, we elaborately conduct themixture-of-vision-expert adapter (MoV-Adapter) to extract and fusetask-specific knowledge from various experts. This coarse-to-fine paradigmeffectively leverages representations from experts based on multimodal contextand model expertise, further enhancing the generalization ability. We conductextensive experiments to evaluate the effectiveness of the proposed approach.Without any bells and whistles, MoVA can achieve significant performance gainsover current state-of-the-art methods in a wide range of challenging multimodalbenchmarks. Codes and models will be available athttps://github.com/TempleX98/MoVA.</description><author>Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, Yu Liu</author><pubDate>Fri, 19 Apr 2024 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13046v1</guid></item><item><title>Unified Scene Representation and Reconstruction for 3D Large Language Models</title><link>http://arxiv.org/abs/2404.13044v1</link><description>Enabling Large Language Models (LLMs) to interact with 3D environments ischallenging. Existing approaches extract point clouds either from ground truth(GT) geometry or 3D scenes reconstructed by auxiliary models. Text-imagealigned 2D features from CLIP are then lifted to point clouds, which serve asinputs for LLMs. However, this solution lacks the establishment of 3Dpoint-to-point connections, leading to a deficiency of spatial structureinformation. Concurrently, the absence of integration and unification betweenthe geometric and semantic representations of the scene culminates in adiminished level of 3D scene understanding. In this paper, we demonstrate theimportance of having a unified scene representation and reconstructionframework, which is essential for LLMs in 3D scenes. Specifically, we introduceUni3DR^2 extracts 3D geometric and semantic aware representation features viathe frozen pre-trained 2D foundation models (e.g., CLIP and SAM) and amulti-scale aggregate 3D decoder. Our learned 3D representations not onlycontribute to the reconstruction process but also provide valuable knowledgefor LLMs. Experimental results validate that our Uni3DR^2 yields convincinggains over the baseline on the 3D reconstruction dataset ScanNet (increasingF-Score by +1.8\%). When applied to LLMs, our Uni3DR^2-LLM exhibits superiorperformance over the baseline on the 3D vision-language understanding datasetScanQA (increasing BLEU-1 by +4.0\% and +4.2\% on the val set and test set,respectively). Furthermore, it outperforms the state-of-the-art method thatuses additional GT point clouds on both ScanQA and 3DMV-VQA.</description><author>Tao Chu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Qiong Liu, Jiaqi Wang</author><pubDate>Fri, 19 Apr 2024 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13044v1</guid></item><item><title>Data Alignment for Zero-Shot Concept Generation in Dermatology AI</title><link>http://arxiv.org/abs/2404.13043v1</link><description>AI in dermatology is evolving at a rapid pace but the major limitation totraining trustworthy classifiers is the scarcity of data with ground-truthconcept level labels, which are meta-labels semantically meaningful to humans.Foundation models like CLIP providing zero-shot capabilities can help alleviatethis challenge by leveraging vast amounts of image-caption pairs available onthe internet. CLIP can be fine-tuned using domain specific image-caption pairsto improve classification performance. However, CLIP's pre-training data is notwell-aligned with the medical jargon that clinicians use to perform diagnoses.The development of large language models (LLMs) in recent years has led to thepossibility of leveraging the expressive nature of these models to generaterich text. Our goal is to use these models to generate caption text that alignswell with both the clinical lexicon and with the natural human language used inCLIP's pre-training data. Starting with captions used for images in PubMedarticles, we extend them by passing the raw captions through an LLM fine-tunedon the field's several textbooks. We find that using captions generated by anexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot conceptclassification performance.</description><author>Soham Gadgil, Mahtab Bigverdi</author><pubDate>Fri, 19 Apr 2024 18:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13043v1</guid></item><item><title>Analysis of Classifier-Free Guidance Weight Schedulers</title><link>http://arxiv.org/abs/2404.13040v1</link><description>Classifier-Free Guidance (CFG) enhances the quality and condition adherenceof text-to-image diffusion models. It operates by combining the conditional andunconditional predictions using a fixed weight. However, recent works vary theweights throughout the diffusion process, reporting superior results butwithout providing any rationale or analysis. By conducting comprehensiveexperiments, this paper provides insights into CFG weight schedulers. Ourfindings suggest that simple, monotonically increasing weight schedulersconsistently lead to improved performances, requiring merely a single line ofcode. In addition, more complex parametrized schedulers can be optimized forfurther improvement, but do not generalize across different models and tasks.</description><author>Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fernandez Abrevaya, David Picard, Vicky Kalogeiton</author><pubDate>Fri, 19 Apr 2024 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13040v1</guid></item><item><title>LaPA: Latent Prompt Assist Model For Medical Visual Question Answering</title><link>http://arxiv.org/abs/2404.13039v1</link><description>Medical visual question answering (Med-VQA) aims to automate the predictionof correct answers for medical images and questions, thereby assistingphysicians in reducing repetitive tasks and alleviating their workload.Existing approaches primarily focus on pre-training models using additional andcomprehensive datasets, followed by fine-tuning to enhance performance indownstream tasks. However, there is also significant value in exploringexisting models to extract clinically relevant information. In this paper, wepropose the Latent Prompt Assist model (LaPA) for medical visual questionanswering. Firstly, we design a latent prompt generation module to generate thelatent prompt with the constraint of the target answer. Subsequently, wepropose a multi-modal fusion block with latent prompt fusion module thatutilizes the latent prompt to extract clinical-relevant information fromuni-modal and multi-modal features. Additionally, we introduce a priorknowledge fusion module to integrate the relationship between diseases andorgans with the clinical-relevant information. Finally, we combine the finalintegrated information with image-language cross-modal information to predictthe final answers. Experimental results on three publicly available Med-VQAdatasets demonstrate that LaPA outperforms the state-of-the-art model ARL,achieving improvements of 1.83%, 0.63%, and 1.80% on VQA-RAD, SLAKE, andVQA-2019, respectively. The code is publicly available athttps://github.com/GaryGuTC/LaPA_model.</description><author>Tiancheng Gu, Kaicheng Yang, Dongnan Liu, Weidong Cai</author><pubDate>Fri, 19 Apr 2024 18:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13039v1</guid></item><item><title>Mapping Social Choice Theory to RLHF</title><link>http://arxiv.org/abs/2404.13038v1</link><description>Recent work on the limitations of using reinforcement learning from humanfeedback (RLHF) to incorporate human preferences into model behavior oftenraises social choice theory as a reference point. Social choice theory'sanalysis of settings such as voting mechanisms provides technicalinfrastructure that can inform how to aggregate human preferences amiddisagreement. We analyze the problem settings of social choice and RLHF,identify key differences between them, and discuss how these differences mayaffect the RLHF interpretation of well-known technical results in socialchoice.</description><author>Jessica Dai, Eve Fleisig</author><pubDate>Fri, 19 Apr 2024 18:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13038v1</guid></item><item><title>Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs</title><link>http://arxiv.org/abs/2404.13033v1</link><description>In the burgeoning field of Large Language Models (LLMs) like ChatGPT andLLaMA, Prompt Engineering (PE) is renowned for boosting zero-shot or in-contextlearning (ICL) through prompt modifications. Yet, the realm of the sampledesign for downstream fine-tuning, crucial for task-specific LLM adaptation, islargely unexplored. This paper introduces Sample Design Engineering (SDE), amethodical approach to enhancing LLMs' post-tuning performance by refininginput, output, and reasoning designs. We conduct a series of in-domain (ID) andout-of-domain (OOD) experiments to assess the impact of various design optionson LLMs' downstream performance, revealing several intriguing patterns thathold consistently across different LLMs. Based on these insights, we propose anintegrated SDE strategy, combining the most effective options, and validate itsconsistent superiority over heuristic sample designs in complex downstreamtasks like multi-aspect sentiment analysis, event extraction, and nested entityrecognition. Additionally, analyses of LLMs' inherent prompt/output perplexity,zero-shot, and ICL abilities illustrate that good PE strategies may not alwaystranslate to good SDE strategies. Code available athttps://github.com/beyondguo/LLM-Tuning.</description><author>Biyang Guo, He Wang, Wenyilin Xiao, Hong Chen, Zhuxin Lee, Songqiao Han, Hailiang Huang</author><pubDate>Fri, 19 Apr 2024 18:47:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13033v1</guid></item><item><title>OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies</title><link>http://arxiv.org/abs/2404.09305v2</link><description>The paper tackles the issue of mapping logic axioms formalised in theOntology Web Language (OWL) within the Object-Oriented Programming (OOP)paradigm. The issues of mapping OWL axioms hierarchies and OOP objectshierarchies are due to OWL-based reasoning algorithms, which might change anOWL hierarchy at runtime; instead, OOP hierarchies are usually defined asstatic structures. Although programming paradigms based on reflection allowchanging the OOP hierarchies at runtime and mapping OWL axioms dynamically,there are no currently available mechanisms that do not limit the reasoningalgorithms. Thus, the factory-based paradigm is typically used since itdecouples the OWL and OOP hierarchies. However, the factory inhibits OOPpolymorphism and introduces a paradigm shift with respect to widely acceptedOOP paradigms. We present the OWLOOP API, which exploits the factory to notlimit reasoning algorithms, and it provides novel OOP interfaces concerning theaxioms in an ontology. OWLOOP is designed to limit the paradigm shift requiredfor using ontologies while improving, through OOP-like polymorphism, themodularity of software architectures that exploit logic reasoning. The paperdetails our OWL to OOP mapping mechanism, and it shows the benefits andlimitations of OWLOOP through examples concerning a robot in a smartenvironment.</description><author>Luca Buoncompagni, Fulvio Mastrogiovanni</author><pubDate>Fri, 19 Apr 2024 18:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09305v2</guid></item><item><title>When Life gives you LLMs, make LLM-ADE: Large Language Models with Adaptive Data Engineering</title><link>http://arxiv.org/abs/2404.13028v1</link><description>This paper presents the LLM-ADE framework, a novel methodology for continuedpre-training of large language models (LLMs) that addresses the challenges ofcatastrophic forgetting and double descent. LLM-ADE employs dynamicarchitectural adjustments, including selective block freezing and expansion,tailored to specific datasets. This strategy enhances model adaptability to newdata while preserving previously acquired knowledge. We demonstrate LLM-ADE'seffectiveness on the TinyLlama model across various general knowledgebenchmarks, showing significant performance improvements without the drawbacksof traditional continuous training methods. This approach promises a moreversatile and robust way to keep LLMs current and efficient in real-worldapplications.</description><author>Stephen Choi, William Gazeley</author><pubDate>Fri, 19 Apr 2024 18:43:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13028v1</guid></item><item><title>PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation</title><link>http://arxiv.org/abs/2404.13026v1</link><description>Realistic object interactions are crucial for creating immersive virtualexperiences, yet synthesizing realistic 3D object dynamics in response to novelinteractions remains a significant challenge. Unlike unconditional ortext-conditioned dynamics generation, action-conditioned dynamics requiresperceiving the physical material properties of objects and grounding the 3Dmotion prediction on these properties, such as object stiffness. However,estimating physical material properties is an open problem due to the lack ofmaterial ground-truth data, as measuring these properties for real objects ishighly difficult. We present PhysDreamer, a physics-based approach that endowsstatic 3D objects with interactive dynamics by leveraging the object dynamicspriors learned by video generation models. By distilling these priors,PhysDreamer enables the synthesis of realistic object responses to novelinteractions, such as external forces or agent manipulations. We demonstrateour approach on diverse examples of elastic objects and evaluate the realism ofthe synthesized interactions through a user study. PhysDreamer takes a steptowards more engaging and realistic virtual experiences by enabling static 3Dobjects to dynamically respond to interactive stimuli in a physically plausiblemanner. See our project page at https://physdreamer.github.io/.</description><author>Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, William T. Freeman</author><pubDate>Fri, 19 Apr 2024 18:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13026v1</guid></item><item><title>BANF: Band-limited Neural Fields for Levels of Detail Reconstruction</title><link>http://arxiv.org/abs/2404.13024v1</link><description>Largely due to their implicit nature, neural fields lack a direct mechanismfor filtering, as Fourier analysis from discrete signal processing is notdirectly applicable to these representations. Effective filtering of neuralfields is critical to enable level-of-detail processing in downstreamapplications, and support operations that involve sampling the field on regulargrids (e.g. marching cubes). Existing methods that attempt to decompose neuralfields in the frequency domain either resort to heuristics or require extensivemodifications to the neural field architecture. We show that via a simplemodification, one can obtain neural fields that are low-pass filtered, and inturn show how this can be exploited to obtain a frequency decomposition of theentire signal. We demonstrate the validity of our technique by investigatinglevel-of-detail reconstruction, and showing how coarser representations can becomputed effectively.</description><author>Ahan Shabanov, Shrisudhan Govindarajan, Cody Reading, Lily Goli, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi</author><pubDate>Fri, 19 Apr 2024 18:39:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13024v1</guid></item><item><title>Stronger Random Baselines for In-Context Learning</title><link>http://arxiv.org/abs/2404.13020v1</link><description>Evaluating the in-context learning classification performance of languagemodels poses challenges due to small dataset sizes, extensive prompt-selectionusing the validation set, and intentionally difficult tasks that lead tonear-random performance. The standard random baseline -- the expected accuracyof guessing labels uniformly at random -- is stable when the evaluation set isused only once or when the dataset is large. We account for the common practiceof validation set reuse and existing small datasets with a stronger randombaseline: the expected maximum accuracy across multiple random classifiers.When choosing the best prompt demonstrations across six quantized languagemodels applied to 16 BIG-bench Lite tasks, more than 20\% of the few-shotresults that exceed the standard baseline do not exceed this stronger randombaseline. When held-out test sets are available, this stronger baseline is alsoa better predictor of held-out performance than the standard baseline, avoidingunnecessary test set evaluations. This maximum random baseline provides aneasily calculated drop-in replacement for the standard baseline.</description><author>Gregory Yauney, David Mimno</author><pubDate>Fri, 19 Apr 2024 18:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13020v1</guid></item><item><title>AI Consciousness is Inevitable: A Theoretical Computer Science Perspective</title><link>http://arxiv.org/abs/2403.17101v2</link><description>We look at consciousness through the lens of Theoretical Computer Science, abranch of mathematics that studies computation under resource limitations. Fromthis perspective, we develop a formal machine model for consciousness. Themodel is inspired by Alan Turing's simple yet powerful model of computation andBernard Baars' theater model of consciousness. Though extremely simple, themodel aligns at a high level with many of the major scientific theories ofhuman and animal consciousness, supporting our claim that machine consciousnessis inevitable.</description><author>Lenore Blum, Manuel Blum</author><pubDate>Fri, 19 Apr 2024 18:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17101v2</guid></item><item><title>Optimizing Calibration by Gaining Aware of Prediction Correctness</title><link>http://arxiv.org/abs/2404.13016v1</link><description>Model calibration aims to align confidence with prediction correctness. TheCross-Entropy CE) loss is widely used for calibrator training, which enforcesthe model to increase confidence on the ground truth class. However, we findthe CE loss has intrinsic limitations. For example, for a narrowmisclassification, a calibrator trained by the CE loss often produces highconfidence on the wrongly predicted class (e.g., a test sample is wronglyclassified and its softmax score on the ground truth class is around 0.4),which is undesirable. In this paper, we propose a new post-hoc calibrationobjective derived from the aim of calibration. Intuitively, the proposedobjective function asks that the calibrator decrease model confidence onwrongly predicted samples and increase confidence on correctly predictedsamples. Because a sample itself has insufficient ability to indicatecorrectness, we use its transformed versions (e.g., rotated, greyscaled andcolor-jittered) during calibrator training. Trained on an in-distributionvalidation set and tested with isolated, individual test samples, our methodachieves competitive calibration performance on both in-distribution andout-of-distribution test sets compared with the state of the art. Further, ouranalysis points out the difference between our method and commonly usedobjectives such as CE loss and mean square error loss, where the latterssometimes deviates from the calibration aim.</description><author>Yuchi Liu, Lei Wang, Yuli Zou, James Zou, Liang Zheng</author><pubDate>Fri, 19 Apr 2024 18:25:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13016v1</guid></item><item><title>Using Graph Neural Networks to Predict Local Culture</title><link>http://arxiv.org/abs/2402.17905v2</link><description>Urban research has long recognized that neighbourhoods are dynamic andrelational. However, lack of data, methodologies, and computer processing powerhave hampered a formal quantitative examination of neighbourhood relationaldynamics. To make progress on this issue, this study proposes a graph neuralnetwork (GNN) approach that permits combining and evaluating multiple sourcesof information about internal characteristics of neighbourhoods, their pastcharacteristics, and flows of groups among them, potentially providing greaterexpressive power in predictive models. By exploring a public large-scaledataset from Yelp, we show the potential of our approach for consideringstructural connectedness in predicting neighbourhood attributes, specificallyto predict local culture. Results are promising from a substantive andmethodologically point of view. Substantively, we find that either local areainformation (e.g. area demographics) or group profiles (tastes of Yelpreviewers) give the best results in predicting local culture, and they arenearly equivalent in all studied cases. Methodologically, exploring groupprofiles could be a helpful alternative where finding local information forspecific areas is challenging, since they can be extracted automatically frommany forms of online data. Thus, our approach could empower researchers andpolicy-makers to use a range of data sources when other local area informationis lacking.</description><author>Thiago H Silva, Daniel Silver</author><pubDate>Fri, 19 Apr 2024 18:25:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17905v2</guid></item><item><title>Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models</title><link>http://arxiv.org/abs/2404.13013v1</link><description>We introduce Groma, a Multimodal Large Language Model (MLLM) with groundedand fine-grained visual perception ability. Beyond holistic imageunderstanding, Groma is adept at region-level tasks such as region captioningand visual grounding. Such capabilities are built upon a localized visualtokenization mechanism, where an image input is decomposed into regions ofinterest and subsequently encoded into region tokens. By integrating regiontokens into user instructions and model responses, we seamlessly enable Gromato understand user-specified region inputs and ground its textual output toimages. Besides, to enhance the grounded chat ability of Groma, we curate avisually grounded instruction dataset by leveraging the powerful GPT-4V andvisual prompting techniques. Compared with MLLMs that rely on the languagemodel or external module for localization, Groma consistently demonstratessuperior performances in standard referring and grounding benchmarks,highlighting the advantages of embedding localization into image tokenization.Project page: https://groma-mllm.github.io/.</description><author>Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, Xiaojuan Qi</author><pubDate>Fri, 19 Apr 2024 18:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13013v1</guid></item><item><title>FinLangNet: A Novel Deep Learning Framework for Credit Risk Prediction Using Linguistic Analogy in Financial Data</title><link>http://arxiv.org/abs/2404.13004v1</link><description>Recent industrial applications in risk prediction still heavily rely onextensively manually-tuned, statistical learning methods. Real-world financialdata, characterized by its high-dimensionality, sparsity, high noise levels,and significant imbalance, poses unique challenges for the effectiveapplication of deep neural network models. In this work, we introduce a noveldeep learning risk prediction framework, FinLangNet, which conceptualizescredit loan trajectories in a structure that mirrors linguistic constructs.This framework is tailored for credit risk prediction using real-worldfinancial data, drawing on structural similarities to language by adaptingnatural language processing techniques. It focuses on analyzing the evolutionand predictability of credit histories through detailed financial eventsequences. Our research demonstrates that FinLangNet surpasses traditionalstatistical methods in predicting credit risk and that its integration withthese methods enhances credit card fraud prediction models, achieving asignificant improvement of over 1.5 points in the Kolmogorov-Smirnov metric.</description><author>Yu Lei, Zixuan Wang, Chu Liu, Tongyao Wang, Dongyang Lee</author><pubDate>Fri, 19 Apr 2024 18:01:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13004v1</guid></item><item><title>Towards Robust Ferrous Scrap Material Classification with Deep Learning and Conformal Prediction</title><link>http://arxiv.org/abs/2404.13002v1</link><description>In the steel production domain, recycling ferrous scrap is essential forenvironmental and economic sustainability, as it reduces both energyconsumption and greenhouse gas emissions. However, the classification of scrapmaterials poses a significant challenge, requiring advancements in automationtechnology. Additionally, building trust among human operators is a majorobstacle. Traditional approaches often fail to quantify uncertainty and lackclarity in model decision-making, which complicates acceptance. In thisarticle, we describe how conformal prediction can be employed to quantifyuncertainty and add robustness in scrap classification. We have adapted theSplit Conformal Prediction technique to seamlessly integrate withstate-of-the-art computer vision models, such as the Vision Transformer (ViT),Swin Transformer, and ResNet-50, while also incorporating ExplainableArtificial Intelligence (XAI) methods. We evaluate the approach using acomprehensive dataset of 8147 images spanning nine ferrous scrap classes. Theapplication of the Split Conformal Prediction method allowed for thequantification of each model's uncertainties, which enhanced the understandingof predictions and increased the reliability of the results. Specifically, theSwin Transformer model demonstrated more reliable outcomes than the others, asevidenced by its smaller average size of prediction sets and achieving anaverage classification accuracy exceeding 95%. Furthermore, the Score-CAMmethod proved highly effective in clarifying visual features, significantlyenhancing the explainability of the classification decisions.</description><author>Paulo Henrique dos Santos, Valéria de Carvalho Santos, Eduardo José da Silva Luz</author><pubDate>Fri, 19 Apr 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13002v1</guid></item><item><title>RadRotator: 3D Rotation of Radiographs with Diffusion Models</title><link>http://arxiv.org/abs/2404.13000v1</link><description>Transforming two-dimensional (2D) images into three-dimensional (3D) volumesis a well-known yet challenging problem for the computer vision community. Inthe medical domain, a few previous studies attempted to convert two or moreinput radiographs into computed tomography (CT) volumes. Following theireffort, we introduce a diffusion model-based technology that can rotate theanatomical content of any input radiograph in 3D space, potentially enablingthe visualization of the entire anatomical content of the radiograph from anyviewpoint in 3D. Similar to previous studies, we used CT volumes to createDigitally Reconstructed Radiographs (DRRs) as the training data for our model.However, we addressed two significant limitations encountered in previousstudies: 1. We utilized conditional diffusion models with classifier-freeguidance instead of Generative Adversarial Networks (GANs) to achieve highermode coverage and improved output image quality, with the only trade-off beingslower inference time, which is often less critical in medical applications;and 2. We demonstrated that the unreliable output of style transfer deeplearning (DL) models, such as Cycle-GAN, to transfer the style of actualradiographs to DRRs could be replaced with a simple yet effective trainingtransformation that randomly changes the pixel intensity histograms of theinput and ground-truth imaging data during training. This transformation makesthe diffusion model agnostic to any distribution variations of the input datapixel intensity, enabling the reliable training of a DL model on input DRRs andapplying the exact same model to conventional radiographs (or DRRs) duringinference.</description><author>Pouria Rouzrokh, Bardia Khosravi, Shahriar Faghani, Kellen L. Mulford, Michael J. Taunton, Bradley J. Erickson, Cody C. Wyles</author><pubDate>Fri, 19 Apr 2024 17:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13000v1</guid></item><item><title>Goal Exploration via Adaptive Skill Distribution for Goal-Conditioned Reinforcement Learning</title><link>http://arxiv.org/abs/2404.12999v1</link><description>Exploration efficiency poses a significant challenge in goal-conditionedreinforcement learning (GCRL) tasks, particularly those with long horizons andsparse rewards. A primary limitation to exploration efficiency is the agent'sinability to leverage environmental structural patterns. In this study, weintroduce a novel framework, GEASD, designed to capture these patterns throughan adaptive skill distribution during the learning process. This distributionoptimizes the local entropy of achieved goals within a contextual horizon,enhancing goal-spreading behaviors and facilitating deep exploration in statescontaining familiar structural patterns. Our experiments reveal markedimprovements in exploration efficiency using the adaptive skill distributioncompared to a uniform skill distribution. Additionally, the learned skilldistribution demonstrates robust generalization capabilities, achievingsubstantial exploration progress in unseen tasks containing similar localstructures.</description><author>Lisheng Wu, Ke Chen</author><pubDate>Fri, 19 Apr 2024 17:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12999v1</guid></item><item><title>QGen: On the Ability to Generalize in Quantization Aware Training</title><link>http://arxiv.org/abs/2404.11769v2</link><description>Quantization lowers memory usage, computational requirements, and latency byutilizing fewer bits to represent model weights and activations. In this work,we investigate the generalization properties of quantized neural networks, acharacteristic that has received little attention despite its implications onmodel performance. In particular, first, we develop a theoretical model forquantization in neural networks and demonstrate how quantization functions as aform of regularization. Second, motivated by recent work connecting thesharpness of the loss landscape and generalization, we derive an approximatebound for the generalization of quantized models conditioned on the amount ofquantization noise. We then validate our hypothesis by experimenting with over2000 models trained on CIFAR-10, CIFAR-100, and ImageNet datasets onconvolutional and transformer-based models.</description><author>MohammadHossein AskariHemmat, Ahmadreza Jeddi, Reyhane Askari Hemmat, Ivan Lazarevich, Alexander Hoffman, Sudhakar Sah, Ehsan Saboori, Yvon Savaria, Jean-Pierre David</author><pubDate>Fri, 19 Apr 2024 17:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11769v2</guid></item><item><title>Towards a Foundation Model for Partial Differential Equations: Multi-Operator Learning and Extrapolation</title><link>http://arxiv.org/abs/2404.12355v2</link><description>Foundation models, such as large language models, have demonstrated successin addressing various language and image processing tasks. In this work, weintroduce a multi-modal foundation model for scientific problems, namedPROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is amulti-operator learning approach which can predict future states ofspatiotemporal systems while concurrently learning the underlying governingequations of the physical system. Specifically, we focus on multi-operatorlearning by training distinct one-dimensional time-dependent nonlinear constantcoefficient partial differential equations, with potential applications to manyphysical applications including physics, geology, and biology. Moreimportantly, we provide three extrapolation studies to demonstrate thatPROSE-PDE can generalize physical features through the robust training ofmultiple operators and that the proposed model can extrapolate to predict PDEsolutions whose models or data were unseen during the training. Furthermore, weshow through systematic numerical experiments that the utilization of thesymbolic modality in our model effectively resolves the well-posedness problemswith training multiple operators and thus enhances our model's predictivecapabilities.</description><author>Jingmin Sun, Yuxuan Liu, Zecheng Zhang, Hayden Schaeffer</author><pubDate>Fri, 19 Apr 2024 17:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12355v2</guid></item><item><title>Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback on Crowdworkers and LLMs</title><link>http://arxiv.org/abs/2404.12994v1</link><description>In ad-hoc retrieval, evaluation relies heavily on user actions, includingimplicit feedback. In a conversational setting such signals are usuallyunavailable due to the nature of the interactions, and, instead, the evaluationoften relies on crowdsourced evaluation labels. The role of user feedback inannotators' assessment of turns in a conversational perception has been littlestudied. We focus on how the evaluation of task-oriented dialogue systems(TDSs), is affected by considering user feedback, explicit or implicit, asprovided through the follow-up utterance of a turn being evaluated. We exploreand compare two methodologies for assessing TDSs: one includes the user'sfollow-up utterance and one without. We use both crowdworkers and largelanguage models (LLMs) as annotators to assess system responses across fouraspects: relevance, usefulness, interestingness, and explanation quality. Ourfindings indicate that there is a distinct difference in ratings assigned byboth annotator groups in the two setups, indicating user feedback doesinfluence system evaluation. Workers are more susceptible to user feedback onusefulness and interestingness compared to LLMs on interestingness andrelevance. User feedback leads to a more personalized assessment of usefulnessby workers, aligning closely with the user's explicit feedback. Additionally,in cases of ambiguous or complex user requests, user feedback improvesagreement among crowdworkers. These findings emphasize the significance of userfeedback in refining system evaluations and suggest the potential for automatedfeedback integration in future research. We publicly release the annotated datato foster research in this area.</description><author>Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke</author><pubDate>Fri, 19 Apr 2024 17:45:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12994v1</guid></item><item><title>Nuclei Instance Segmentation of Cryosectioned H&amp;E Stained Histological Images using Triple U-Net Architecture</title><link>http://arxiv.org/abs/2404.12986v1</link><description>Nuclei instance segmentation is crucial in oncological diagnosis and cancerpathology research. H&amp;E stained images are commonly used for medical diagnosis,but pre-processing is necessary before using them for image processing tasks.Two principal pre-processing methods are formalin-fixed paraffin-embeddedsamples (FFPE) and frozen tissue samples (FS). While FFPE is widely used, it istime-consuming, while FS samples can be processed quickly. Analyzing H&amp;Estained images derived from fast sample preparation, staining, and scanning canpose difficulties due to the swift process, which can result in the degradationof image quality. This paper proposes a method that leverages the uniqueoptical characteristics of H&amp;E stained images. A three-branch U-Netarchitecture has been implemented, where each branch contributes to the finalsegmentation results. The process includes applying watershed algorithm toseparate overlapping regions and enhance accuracy. The Triple U-Netarchitecture comprises an RGB branch, a Hematoxylin branch, and a Segmentationbranch. This study focuses on a novel dataset named CryoNuSeg. The resultsobtained through robust experiments outperform the state-of-the-art resultsacross various metrics. The benchmark score for this dataset is AJI 52.5 and PQ47.7, achieved through the implementation of U-Net Architecture. However, theproposed Triple U-Net architecture achieves an AJI score of 67.41 and PQ of50.56. The proposed architecture improves more on AJI than other evaluationmetrics, which further justifies the superiority of the Triple U-Netarchitecture over the baseline U-Net model, as AJI is a more strict evaluationmetric. The use of the three-branch U-Net model, followed by watershedpost-processing, significantly surpasses the benchmark scores, showingsubstantial improvement in the AJI score</description><author>Zarif Ahmed, Chowdhury Nur E Alam Siddiqi, Fardifa Fathmiul Alam, Tasnim Ahmed, Tareque Mohmud Chowdhury</author><pubDate>Fri, 19 Apr 2024 17:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12986v1</guid></item><item><title>Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative Diseases</title><link>http://arxiv.org/abs/2404.12984v1</link><description>Parkinson's disease ranks as the second most prevalent neurodegenerativedisorder globally. This research aims to develop a system leveraging MixedReality capabilities for tracking and assessing eye movements. In this paper,we present a medical scenario and outline the development of an applicationdesigned to capture eye-tracking signals through Mixed Reality technology forthe evaluation of neurodegenerative diseases. Additionally, we introduce apipeline for extracting clinically relevant features from eye-gaze analysis,describing the capabilities of the proposed system from a medical perspective.The study involved a cohort of healthy control individuals and patientssuffering from Parkinson's disease, showcasing the feasibility and potential ofthe proposed technology for non-intrusive monitoring of eye movement patternsfor the diagnosis of neurodegenerative diseases. Clinical relevance - Developing a non-invasive biomarker for Parkinson'sdisease is urgently needed to accurately detect the disease's onset. This wouldallow for the timely introduction of neuroprotective treatment at the earlieststage and enable the continuous monitoring of intervention outcomes. Theability to detect subtle changes in eye movements allows for early diagnosis,offering a critical window for intervention before more pronounced symptomsemerge. Eye tracking provides objective and quantifiable biomarkers, ensuringreliable assessments of disease progression and cognitive function. The eyegaze analysis using Mixed Reality glasses is wireless, facilitating convenientassessments in both home and hospital settings. The approach offers theadvantage of utilizing hardware that requires no additional specializedattachments, enabling examinations through personal eyewear.</description><author>Mateusz Daniol, Daria Hemmerling, Jakub Sikora, Pawel Jemiolo, Marek Wodzinski, Magdalena Wojcik-Pedziwiatr</author><pubDate>Fri, 19 Apr 2024 17:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12984v1</guid></item><item><title>Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation</title><link>http://arxiv.org/abs/2403.16427v4</link><description>Large Language Models (LLMs) are emerging as promising approaches to enhancesession-based recommendation (SBR), where both prompt-based andfine-tuning-based methods have been widely investigated to align LLMs with SBR.However, the former methods struggle with optimal prompts to elicit the correctreasoning of LLMs due to the lack of task-specific feedback, leading tounsatisfactory recommendations. Although the latter methods attempt tofine-tune LLMs with domain-specific knowledge, they face limitations such ashigh computational costs and reliance on open-source backbones. To address suchissues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) forSBR, guiding LLMs to focus on specialized knowledge essential for more accuraterecommendations effectively and efficiently. In particular, we first design theReflective Exploration Module to effectively extract knowledge that is readilyunderstandable and digestible by LLMs. To be specific, we direct LLMs toexamine recommendation errors through self-reflection and construct a knowledgebase (KB) comprising hints capable of rectifying these errors. To efficientlyelicit the correct reasoning of LLMs, we further devise the ReinforcementUtilization Module to train a lightweight retrieval agent. It learns to selecthints from the constructed KB based on the task-specific feedback, where thehints can serve as guidance to help correct LLMs reasoning for betterrecommendations. Extensive experiments on multiple real-world datasetsdemonstrate that our method consistently outperforms state-of-the-art methods.</description><author>Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya Wang, Jie Zhang</author><pubDate>Fri, 19 Apr 2024 17:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16427v4</guid></item><item><title>Blind Federated Learning via Over-the-Air q-QAM</title><link>http://arxiv.org/abs/2311.04253v2</link><description>In this work, we investigate federated edge learning over a fading multipleaccess channel. To alleviate the communication burden between the edge devicesand the access point, we introduce a pioneering digital over-the-aircomputation strategy employing q-ary quadrature amplitude modulation,culminating in a low latency communication scheme. Indeed, we propose a newfederated edge learning framework in which edge devices use digital modulationfor over-the-air uplink transmission to the edge server while they have noaccess to the channel state information. Furthermore, we incorporate multipleantennas at the edge server to overcome the fading inherent in wirelesscommunication. We analyze the number of antennas required to mitigate thefading impact effectively. We prove a non-asymptotic upper bound for the meansquared error for the proposed federated learning with digital over-the-airuplink transmissions under both noisy and fading conditions. Leveraging thederived upper bound, we characterize the convergence rate of the learningprocess of a non-convex loss function in terms of the mean square error ofgradients due to the fading channel. Furthermore, we substantiate thetheoretical assurances through numerical experiments concerning mean squareerror and the convergence efficacy of the digital federated edge learningframework. Notably, the results demonstrate that augmenting the number ofantennas at the edge server and adopting higher-order modulations improve themodel accuracy up to 60\%.</description><author>Saeed Razavikia, José Mairton Barros Da Silva Júnior, Carlo Fischione</author><pubDate>Fri, 19 Apr 2024 17:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04253v2</guid></item><item><title>What Generative Artificial Intelligence Means for Terminological Definitions</title><link>http://arxiv.org/abs/2402.16139v3</link><description>This paper examines the impact of Generative Artificial Intelligence (GenAI)tools like ChatGPT on the creation and consumption of terminologicaldefinitions. From the terminologist's point of view, the strategic use of GenAItools can streamline the process of crafting definitions, reducing both timeand effort, while potentially enhancing quality. GenAI tools enable AI-assistedterminography, notably post-editing terminography, where the machine produces adefinition that the terminologist then corrects or refines. However, thepotential of GenAI tools to fulfill all the terminological needs of a user,including term definitions, challenges the very existence of terminologicaldefinitions and resources as we know them. Unlike terminological definitions,GenAI tools can describe the knowledge activated by a term in a specificcontext. However, a main drawback of these tools is that their output cancontain errors. For this reason, users requiring reliability will likely stillresort to terminological resources for definitions. Nevertheless, with theinevitable integration of AI into terminology work, the distinction betweenhuman-created and AI-created content will become increasingly blurred.</description><author>Antonio San Martín</author><pubDate>Fri, 19 Apr 2024 17:13:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16139v3</guid></item><item><title>One-shot skill assessment in high-stakes domains with limited data via meta learning</title><link>http://arxiv.org/abs/2301.00812v5</link><description>Deep Learning (DL) has achieved robust competency assessment in varioushigh-stakes fields. However, the applicability of DL models is often hamperedby their substantial data requirements and confinement to specific trainingdomains. This prevents them from transitioning to new tasks where data isscarce. Therefore, domain adaptation emerges as a critical element for thepractical implementation of DL in real-world scenarios. Herein, we introduceA-VBANet, a novel meta-learning model capable of delivering domain-agnosticskill assessment via one-shot learning. Our methodology has been tested byassessing surgical skills on five laparoscopic and robotic simulators andreal-life laparoscopic cholecystectomy. Our model successfully adapted withaccuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulatedtasks and 89.7% for laparoscopic cholecystectomy. This study marks the firstinstance of a domain-agnostic methodology for skill assessment in criticalfields setting a precedent for the broad application of DL across diversereal-life domains with limited data.</description><author>Erim Yanik, Steven Schwaitzberg, Gene Yang, Xavier Intes, Jack Norfleet, Matthew Hackett, Suvranu De</author><pubDate>Fri, 19 Apr 2024 17:10:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00812v5</guid></item><item><title>TRNet: Two-level Refinement Network leveraging Speech Enhancement for Noise Robust Speech Emotion Recognition</title><link>http://arxiv.org/abs/2404.12979v1</link><description>One persistent challenge in Speech Emotion Recognition (SER) is theubiquitous environmental noise, which frequently results in diminished SERperformance in practical use. In this paper, we introduce a Two-levelRefinement Network, dubbed TRNet, to address this challenge. Specifically, apre-trained speech enhancement module is employed for front-end noise reductionand noise level estimation. Later, we utilize clean speech spectrograms andtheir corresponding deep representations as reference signals to refine thespectrogram distortion and representation shift of enhanced speech during modeltraining. Experimental results validate that the proposed TRNet substantiallyincreases the system's robustness in both matched and unmatched noisyenvironments, without compromising its performance in clean environments.</description><author>Chengxin Chen, Pengyuan Zhang</author><pubDate>Fri, 19 Apr 2024 17:09:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12979v1</guid></item><item><title>FineRec:Exploring Fine-grained Sequential Recommendation</title><link>http://arxiv.org/abs/2404.12975v1</link><description>Sequential recommendation is dedicated to offering items of interest forusers based on their history behaviors. The attribute-opinion pairs, expressedby users in their reviews for items, provide the potentials to capture userpreferences and item characteristics at a fine-grained level. To this end, wepropose a novel framework FineRec that explores the attribute-opinion pairs ofreviews to finely handle sequential recommendation. Specifically, we utilize alarge language model to extract attribute-opinion pairs from reviews. For eachattribute, a unique attribute-specific user-opinion-item graph is created,where corresponding opinions serve as the edges linking heterogeneous user anditem nodes. To tackle the diversity of opinions, we devise a diversity-awareconvolution operation to aggregate information within the graphs, enablingattribute-specific user and item representation learning. Ultimately, wepresent an interaction-driven fusion mechanism to integrate attribute-specificuser/item representations across all attributes for generating recommendations.Extensive experiments conducted on several realworld datasets demonstrate thesuperiority of our FineRec over existing state-of-the-art methods. Furtheranalysis also verifies the effectiveness of our fine-grained manner in handlingthe task.</description><author>Xiaokun Zhang, Bo Xu, Youlin Wu, Yuan Zhong, Hongfei Lin, Fenglong Ma</author><pubDate>Fri, 19 Apr 2024 17:04:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12975v1</guid></item><item><title>Cross-modal Diffusion Modelling for Super-resolved Spatial Transcriptomics</title><link>http://arxiv.org/abs/2404.12973v1</link><description>The recent advancement of spatial transcriptomics (ST) allows to characterizespatial gene expression within tissue for discovery research. However, currentST platforms suffer from low resolution, hindering in-depth understanding ofspatial gene expression. Super-resolution approaches promise to enhance ST mapsby integrating histology images with gene expressions of profiled tissue spots.However, current super-resolution methods are limited by restorationuncertainty and mode collapse. Although diffusion models have shown promise incapturing complex interactions between multi-modal conditions, it remains achallenge to integrate histology images and gene expression for super-resolvedST maps. This paper proposes a cross-modal conditional diffusion model forsuper-resolving ST maps with the guidance of histology images. Specifically, wedesign a multi-modal disentangling network with cross-modal adaptive modulationto utilize complementary information from histology images and spatial geneexpression. Moreover, we propose a dynamic cross-attention modelling strategyto extract hierarchical cell-to-tissue information from histology images.Lastly, we propose a co-expression-based gene-correlation graph network tomodel the co-expression relationship of multiple genes. Experiments show thatour method outperforms other state-of-the-art methods in ST super-resolution onthree public datasets.</description><author>Xiaofei Wang, Xingxu Huang, Stephen J. Price, Chao Li</author><pubDate>Fri, 19 Apr 2024 17:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12973v1</guid></item><item><title>Disentangling ID and Modality Effects for Session-based Recommendation</title><link>http://arxiv.org/abs/2404.12969v1</link><description>Session-based recommendation aims to predict intents of anonymous users basedon their limited behaviors. Modeling user behaviors involves two distinctrationales: co-occurrence patterns reflected by item IDs, and fine-grainedpreferences represented by item modalities (e.g., text and images). However,existing methods typically entangle these causes, leading to their failure inachieving accurate and explainable recommendations. To this end, we propose anovel framework DIMO to disentangle the effects of ID and modality in the task.At the item level, we introduce a co-occurrence representation schema toexplicitly incorporate cooccurrence patterns into ID representations.Simultaneously, DIMO aligns different modalities into a unified semantic spaceto represent them uniformly. At the session level, we present a multi-viewself-supervised disentanglement, including proxy mechanism and counterfactualinference, to disentangle ID and modality effects without supervised signals.Leveraging these disentangled causes, DIMO provides recommendations via causalinference and further creates two templates for generating explanations.Extensive experiments on multiple real-world datasets demonstrate theconsistent superiority of DIMO over existing methods. Further analysis alsoconfirms DIMO's effectiveness in generating explanations.</description><author>Xiaokun Zhang, Bo Xu, Zhaochun Ren, Xiaochen Wang, Hongfei Lin, Fenglong Ma</author><pubDate>Fri, 19 Apr 2024 16:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12969v1</guid></item><item><title>Scalable Data Assimilation with Message Passing</title><link>http://arxiv.org/abs/2404.12968v1</link><description>Data assimilation is a core component of numerical weather predictionsystems. The large quantity of data processed during assimilation requires thecomputation to be distributed across increasingly many compute nodes, yetexisting approaches suffer from synchronisation overhead in this setting. Inthis paper, we exploit the formulation of data assimilation as a Bayesianinference problem and apply a message-passing algorithm to solve the spatialinference problem. Since message passing is inherently based on localcomputations, this approach lends itself to parallel and distributedcomputation. In combination with a GPU-accelerated implementation, we can scalethe algorithm to very large grid sizes while retaining good accuracy andcompute and memory requirements.</description><author>Oscar Key, So Takao, Daniel Giles, Marc Peter Deisenroth</author><pubDate>Fri, 19 Apr 2024 16:54:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12968v1</guid></item><item><title>Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2404.12966v1</link><description>Counterfactual reasoning, as a crucial manifestation of human intelligence,refers to making presuppositions based on established facts and extrapolatingpotential outcomes. Existing multimodal large language models (MLLMs) haveexhibited impressive cognitive and reasoning capabilities, which have beenexamined across a wide range of Visual Question Answering (VQA) benchmarks.Nevertheless, how will existing MLLMs perform when faced with counterfactualquestions? To answer this question, we first curate a novel\textbf{C}ounter\textbf{F}actual \textbf{M}ulti\textbf{M}odal reasoningbenchmark, abbreviated as \textbf{CFMM}, to systematically assess thecounterfactual reasoning capabilities of MLLMs. Our CFMM comprises sixchallenging tasks, each including hundreds of carefully human-labeledcounterfactual questions, to evaluate MLLM's counterfactual reasoningcapabilities across diverse aspects. Through experiments, interestingly, wefind that existing MLLMs prefer to believe what they see, but ignore thecounterfactual presuppositions presented in the question, thereby leading toinaccurate responses. Furthermore, we evaluate a wide range of prevalent MLLMson our proposed CFMM. The significant gap between their performance on our CFMMand that on several VQA benchmarks indicates that there is still considerableroom for improvement in existing MLLMs toward approaching human-levelintelligence. On the other hand, through boosting MLLMs performances on ourCFMM in the future, potential avenues toward developing MLLMs with advancedintelligence can be explored.</description><author>Yian Li, Wentao Tian, Yang Jiao, Jingjing Chen, Yu-Gang Jiang</author><pubDate>Fri, 19 Apr 2024 16:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12966v1</guid></item><item><title>Event-Based Contrastive Learning for Medical Time Series</title><link>http://arxiv.org/abs/2312.10308v3</link><description>In clinical practice, one often needs to identify whether a patient is athigh risk of adverse outcomes after some key medical event. For example,quantifying the risk of adverse outcomes after an acute cardiovascular eventhelps healthcare providers identify those patients at the highest risk of pooroutcomes; i.e., patients who benefit from invasive therapies that can lowertheir risk. Assessing the risk of adverse outcomes, however, is challenging dueto the complexity, variability, and heterogeneity of longitudinal medical data,especially for individuals suffering from chronic diseases like heart failure.In this paper, we introduce Event-Based Contrastive Learning (EBCL) - a methodfor learning embeddings of heterogeneous patient data that preserves temporalinformation before and after key index events. We demonstrate that EBCL can beused to construct models that yield improved performance on importantdownstream tasks relative to other pretraining methods. We develop and test themethod using a cohort of heart failure patients obtained from a large hospitalnetwork and the publicly available MIMIC-IV dataset consisting of patients inan intensive care unit at a large tertiary care center. On both cohorts, EBCLpretraining yields models that are performant with respect to a number ofdownstream tasks, including mortality, hospital readmission, and length ofstay. In addition, unsupervised EBCL embeddings effectively cluster heartfailure patients into subgroups with distinct outcomes, thereby providinginformation that helps identify new heart failure phenotypes. The contrastiveframework around the index event can be adapted to a wide array of time-seriesdatasets and provides information that can be used to guide personalized care.</description><author>Hyewon Jeong, Nassim Oufattole, Matthew Mcdermott, Aparna Balagopalan, Bryan Jangeesingh, Marzyeh Ghassemi, Collin Stultz</author><pubDate>Fri, 19 Apr 2024 16:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10308v3</guid></item><item><title>Interpretable Graph Neural Networks for Tabular Data</title><link>http://arxiv.org/abs/2308.08945v2</link><description>Data in tabular format is frequently occurring in real-world applications.Graph Neural Networks (GNNs) have recently been extended to effectively handlesuch data, allowing feature interactions to be captured through representationlearning. However, these approaches essentially produce black-box models, inthe form of deep neural networks, precluding users from following the logicbehind the model predictions. We propose an approach, called IGNNet(Interpretable Graph Neural Network for tabular data), which constrains thelearning algorithm to produce an interpretable model, where the model shows howthe predictions are exactly computed from the original input features. Alarge-scale empirical investigation is presented, showing that IGNNet isperforming on par with state-of-the-art machine-learning algorithms that targettabular data, including XGBoost, Random Forests, and TabNet. At the same time,the results show that the explanations obtained from IGNNet are aligned withthe true Shapley values of the features without incurring any additionalcomputational overhead.</description><author>Amr Alkhatib, Sofiane Ennadir, Henrik Boström, Michalis Vazirgiannis</author><pubDate>Fri, 19 Apr 2024 16:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08945v2</guid></item><item><title>Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts</title><link>http://arxiv.org/abs/2311.08097v2</link><description>Reasoning methods, best exemplified by the well-known Chain-of-Thought (CoT),empower the reasoning abilities of Large Language Models (LLMs) by elicitingthem to solve complex tasks in a step-by-step manner. Although they areachieving significant success, the ability to deliver multi-step reasoningremains limited to English because of the imbalance in the distribution ofpre-training data, which makes other languages a barrier. In this paper, wepropose Cross-lingual Tree-of-Thoughts (Cross-ToT), a method for aligningCross-lingual CoT reasoning across languages. The proposed method, through aself-consistent cross-lingual prompting mechanism inspired by theTree-of-Thoughts approach, provides multi-step reasoning paths in differentlanguages that, during the steps, lead to the final solution. Experimentalevaluations show that our method significantly outperforms existing promptingmethods by reducing the number of interactions and achieving state-of-the-artperformance.</description><author>Leonardo Ranaldi, Giulia Pucci, Federico Ranaldi, Elena Sofia Ruzzetti, Fabio Massimo Zanzotto</author><pubDate>Fri, 19 Apr 2024 16:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08097v2</guid></item><item><title>Improving Pediatric Pneumonia Diagnosis with Adult Chest X-ray Images Utilizing Contrastive Learning and Embedding Similarity</title><link>http://arxiv.org/abs/2404.12958v1</link><description>Despite the advancement of deep learning-based computer-aided diagnosis (CAD)methods for pneumonia from adult chest x-ray (CXR) images, the performance ofCAD methods applied to pediatric images remains suboptimal, mainly due to thelack of large-scale annotated pediatric imaging datasets. Establishing a properframework to leverage existing adult large-scale CXR datasets can thus enhancepediatric pneumonia detection performance. In this paper, we propose athree-branch parallel path learning-based framework that utilizes both adultand pediatric datasets to improve the performance of deep learning models onpediatric test datasets. The paths are trained with pediatric only, adult only,and both types of CXRs, respectively. Our proposed framework utilizes themulti-positive contrastive loss to cluster the classwise embeddings and theembedding similarity loss among these three parallel paths to make theclasswise embeddings as close as possible to reduce the effect of domain shift.Experimental evaluations on open-access adult and pediatric CXR datasets showthat the proposed method achieves a superior AUROC score of 0.8464 compared to0.8348 obtained using the conventional approach of join training on bothdatasets. The proposed approach thus paves the way for generalized CAD modelsthat are effective for both adult and pediatric age groups.</description><author>Mohammad Zunaed, Anwarul Hasan, Taufiq Hasan</author><pubDate>Fri, 19 Apr 2024 16:40:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12958v1</guid></item><item><title>Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction</title><link>http://arxiv.org/abs/2404.12957v1</link><description>We propose an approach for estimating the latent knowledge embedded insidelarge language models (LLMs). We leverage the in-context learning (ICL)abilities of LLMs to estimate the extent to which an LLM knows the facts storedin a knowledge base. Our knowledge estimator avoids reliability concerns withprevious prompting-based methods, is both conceptually simpler and easier toapply, and we demonstrate that it can surface more of the latent knowledgeembedded in LLMs. We also investigate how different design choices affect theperformance of ICL-based knowledge estimation. Using the proposed estimator, weperform a large-scale evaluation of the factual knowledge of a variety of opensource LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large setof relations and facts from the Wikidata knowledge base. We observe differencesin the factual knowledge between different model families and models ofdifferent sizes, that some relations are consistently better known than othersbut that models differ in the precise facts they know, and differences in theknowledge of base models and their finetuned counterparts.</description><author>Qinyuan Wu, Mohammad Aflah Khan, Soumi Das, Vedant Nanda, Bishwamittra Ghosh, Camila Kolling, Till Speicher, Laurent Bindschaedler, Krishna P. Gummadi, Evimaria Terzi</author><pubDate>Fri, 19 Apr 2024 16:40:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12957v1</guid></item><item><title>The maximum capability of a topological feature in link prediction</title><link>http://arxiv.org/abs/2206.15101v3</link><description>Networks offer a powerful approach to modeling complex systems byrepresenting the underlying set of pairwise interactions. Link prediction isthe task that predicts links of a network that are not directly visible, withprofound applications in biological, social, and other complex systems. Despiteintensive utilization of the topological feature in this task, it is unclear towhat extent a feature can be leveraged to infer missing links. Here, we aim tounveil the capability of a topological feature in link prediction byidentifying its prediction performance upper bound. We introduce a theoreticalframework that is compatible with different indexes to gauge the feature,different prediction approaches to utilize the feature, and different metricsto quantify the prediction performance. The maximum capability of a topologicalfeature follows a simple yet theoretically validated expression, which onlydepends on the extent to which the feature is held in missing and nonexistentlinks. Because a family of indexes based on the same feature shares the sameupper bound, the potential of all others can be estimated from one singleindex. Furthermore, a feature's capability is lifted in the supervisedprediction, which can be mathematically quantified, allowing us to estimate thebenefit of applying machine learning algorithms. The universality of thepattern uncovered is empirically verified by 550 structurally diverse networks.The findings have applications in feature and method selection, and shed lighton network characteristics that make a topological feature effective in linkprediction.</description><author>Yijun Ran, Xiao-Ke Xu, Tao Jia</author><pubDate>Fri, 19 Apr 2024 16:40:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.15101v3</guid></item><item><title>When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour</title><link>http://arxiv.org/abs/2311.09410v2</link><description>Large Language Models have been demonstrating the ability to solve complextasks by delivering answers that are positively evaluated by humans due in partto the intensive use of human feedback that refines responses. However, thesuggestibility transmitted through human feedback increases the inclination toproduce responses that correspond to the users' beliefs or misleading promptsas opposed to true facts, a behaviour known as sycophancy. This phenomenondecreases the bias, robustness, and, consequently, their reliability. In thispaper, we shed light on the suggestibility of Large Language Models (LLMs) tosycophantic behaviour, demonstrating these tendencies via human-influencedprompts over different tasks. Our investigation reveals that LLMs showsycophantic tendencies when responding to queries involving subjective opinionsand statements that should elicit a contrary response based on facts. Incontrast, when confronted with mathematical tasks or queries that have anobjective answer, these models at various scales seem not to follow the users'hints by demonstrating confidence in delivering the correct answers.</description><author>Leonardo Ranaldi, Giulia Pucci</author><pubDate>Fri, 19 Apr 2024 16:36:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09410v2</guid></item><item><title>Learning Machine Morality through Experience and Interaction</title><link>http://arxiv.org/abs/2312.01818v2</link><description>Increasing interest in ensuring safety of next-generation ArtificialIntelligence (AI) systems calls for novel approaches to embedding morality intoautonomous agents. Traditionally, this has been done by imposing explicittop-down rules or hard constraints on systems, for example by filtering systemoutputs through pre-defined ethical rules. Recently, instead, entirelybottom-up methods for learning implicit preferences from human behavior havebecome increasingly popular, such as those for training and fine-tuning LargeLanguage Models. In this paper, we provide a systematization of existingapproaches to the problem of introducing morality in machines - modeled as acontinuum, and argue that the majority of popular techniques lie at theextremes - either being fully hard-coded, or entirely learned, where noexplicit statement of any moral principle is required. Given the relativestrengths and weaknesses of each type of methodology, we argue that more hybridsolutions are needed to create adaptable and robust, yet more controllable andinterpretable agents. In particular, we present three case studies of recent works which uselearning from experience (i.e., Reinforcement Learning) to explicitly providemoral principles to learning agents - either as intrinsic rewards, morallogical constraints or textual principles for language models. For example,using intrinsic rewards in Social Dilemma games, we demonstrate how it ispossible to represent classical moral frameworks for agents. We also present anoverview of the existing work in this area in order to provide empiricalevidence for the potential of this hybrid approach. We then discuss strategiesfor evaluating the effectiveness of moral learning agents. Finally, we presentopen research questions and implications for the future of AI safety and ethicswhich are emerging from this framework.</description><author>Elizaveta Tennant, Stephen Hailes, Mirco Musolesi</author><pubDate>Fri, 19 Apr 2024 16:34:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01818v2</guid></item><item><title>HiLo: Detailed and Robust 3D Clothed Human Reconstruction with High-and Low-Frequency Information of Parametric Models</title><link>http://arxiv.org/abs/2404.04876v2</link><description>Reconstructing 3D clothed human involves creating a detailed geometry ofindividuals in clothing, with applications ranging from virtual try-on, movies,to games. To enable practical and widespread applications, recent advancespropose to generate a clothed human from an RGB image. However, they struggleto reconstruct detailed and robust avatars simultaneously. We empirically findthat the high-frequency (HF) and low-frequency (LF) information from aparametric model has the potential to enhance geometry details and improverobustness to noise, respectively. Based on this, we propose HiLo, namelyclothed human reconstruction with high- and low-frequency information, whichcontains two components. 1) To recover detailed geometry using HF information,we propose a progressive HF Signed Distance Function to enhance the detailed 3Dgeometry of a clothed human. We analyze that our progressive learning manneralleviates large gradients that hinder model convergence. 2) To achieve robustreconstruction against inaccurate estimation of the parametric model by usingLF information, we propose a spatial interaction implicit function. Thisfunction effectively exploits the complementary spatial information from alow-resolution voxel grid of the parametric model. Experimental resultsdemonstrate that HiLo outperforms the state-of-the-art methods by 10.43% and9.54% in terms of Chamfer distance on the Thuman2.0 and CAPE datasets,respectively. Additionally, HiLo demonstrates robustness to noise from theparametric model, challenging poses, and various clothing styles.</description><author>Yifan Yang, Dong Liu, Shuhai Zhang, Zeshuai Deng, Zixiong Huang, Mingkui Tan</author><pubDate>Fri, 19 Apr 2024 16:33:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04876v2</guid></item><item><title>Next Generation Loss Function for Image Classification</title><link>http://arxiv.org/abs/2404.12948v1</link><description>Neural networks are trained by minimizing a loss function that defines thediscrepancy between the predicted model output and the target value. Theselection of the loss function is crucial to achieve task-specific behaviourand highly influences the capability of the model. A variety of loss functionshave been proposed for a wide range of tasks affecting training and modelperformance. For classification tasks, the cross entropy is the de-factostandard and usually the first choice. Here, we try to experimentally challengethe well-known loss functions, including cross entropy (CE) loss, by utilizingthe genetic programming (GP) approach, a population-based evolutionaryalgorithm. GP constructs loss functions from a set of operators and leaf nodesand these functions are repeatedly recombined and mutated to find an optimalstructure. Experiments were carried out on different small-sized datasetsCIFAR-10, CIFAR-100 and Fashion-MNIST using an Inception model. The 5 bestfunctions found were evaluated for different model architectures on a set ofstandard datasets ranging from 2 to 102 classes and very different sizes. Onefunction, denoted as Next Generation Loss (NGL), clearly stood out showing sameor better performance for all tested datasets compared to CE. To evaluate theNGL function on a large-scale dataset, we tested its performance on theImagenet-1k dataset where it showed improved top-1 accuracy compared to modelstrained with identical settings and other losses. Finally, the NGL was trainedon a segmentation downstream task for Pascal VOC 2012 and COCO-Stuff164kdatasets improving the underlying model performance.</description><author>Shakhnaz Akhmedova, Nils Körber</author><pubDate>Fri, 19 Apr 2024 16:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12948v1</guid></item><item><title>QDFormer: Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition</title><link>http://arxiv.org/abs/2310.00132v3</link><description>Audiovisual segmentation (AVS) is a challenging task that aims to segmentvisual objects in videos according to their associated acoustic cues. Withmultiple sound sources and background disturbances involved, establishingrobust correspondences between audio and visual contents poses uniquechallenges due to (1) complex entanglement across sound sources and (2)frequent changes in the occurrence of distinct sound events. Assuming soundevents occur independently, the multi-source semantic space can be representedas the Cartesian product of single-source sub-spaces. We are motivated todecompose the multi-source audio semantics into single-source semantics formore effective interactions with visual content. We propose a semanticdecomposition method based on product quantization, where the multi-sourcesemantics can be decomposed and represented by several disentangled andnoise-suppressed single-source semantics. Furthermore, we introduce aglobal-to-local quantization mechanism, which distills knowledge from stableglobal (clip-level) features into local (frame-level) ones, to handle frequentchanges in audio semantics. Extensive experiments demonstrate that oursemantically decomposed audio representation significantly improves AVSperformance, e.g., +21.2% mIoU on the challenging AVS-Semantic benchmark withResNet50 backbone. https://github.com/lxa9867/QSD.</description><author>Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj</author><pubDate>Fri, 19 Apr 2024 16:23:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00132v3</guid></item><item><title>Purposer: Putting Human Motion Generation in Context</title><link>http://arxiv.org/abs/2404.12942v1</link><description>We present a novel method to generate human motion to populate 3D indoorscenes. It can be controlled with various combinations of conditioning signalssuch as a path in a scene, target poses, past motions, and scenes representedas 3D point clouds. State-of-the-art methods are either models specialized toone single setting, require vast amounts of high-quality and diverse trainingdata, or are unconditional models that do not integrate scene or othercontextual information. As a consequence, they have limited applicability andrely on costly training data. To address these limitations, we propose a newmethod ,dubbed Purposer, based on neural discrete representation learning. Ourmodel is capable of exploiting, in a flexible manner, different types ofinformation already present in open access large-scale datasets such as AMASS.First, we encode unconditional human motion into a discrete latent space.Second, an autoregressive generative model, conditioned with key contextualinformation, either with prompting or additive tokens, and trained fornext-step prediction in this space, synthesizes sequences of latent indices. Wefurther design a novel conditioning block to handle future conditioninginformation in such a causal model by using a network with two branches tocompute separate stacks of features. In this manner, Purposer can generaterealistic motion sequences in diverse test scenes. Through exhaustiveevaluation, we demonstrate that our multi-contextual solution outperformsexisting specialized approaches for specific contextual information, both interms of quality and diversity. Our model is trained with short sequences, buta byproduct of being able to use various conditioning signals is that at testtime different combinations can be used to chain short sequences together andgenerate long motions within a context scene.</description><author>Nicolas Ugrinovic, Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, Gregory Rogez, Francesc Moreno-Noguer</author><pubDate>Fri, 19 Apr 2024 16:16:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12942v1</guid></item><item><title>Quantum Normalizing Flows for Anomaly Detection</title><link>http://arxiv.org/abs/2402.02866v2</link><description>A Normalizing Flow computes a bijective mapping from an arbitrarydistribution to a predefined (e.g. normal) distribution. Such a flow can beused to address different tasks, e.g. anomaly detection, once such a mappinghas been learned. In this work we introduce Normalizing Flows for Quantumarchitectures, describe how to model and optimize such a flow and evaluate ourmethod on example datasets. Our proposed models show competitive performancefor anomaly detection compared to classical methods, esp. those ones wherethere are already quantum inspired algorithms available. In the experiments wecompare our performance to isolation forests (IF), the local outlier factor(LOF) or single-class SVMs.</description><author>Bodo Rosenhahn, Christoph Hirche</author><pubDate>Fri, 19 Apr 2024 16:12:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02866v2</guid></item><item><title>Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling</title><link>http://arxiv.org/abs/2404.12940v1</link><description>Conventional diffusion models typically relies on a fixed forward process,which implicitly defines complex marginal distributions over latent variables.This can often complicate the reverse process' task in learning generativetrajectories, and results in costly inference for diffusion models. To addressthese limitations, we introduce Neural Flow Diffusion Models (NFDM), a novelframework that enhances diffusion models by supporting a broader range offorward processes beyond the fixed linear Gaussian. We also propose a novelparameterization technique for learning the forward process. Our frameworkprovides an end-to-end, simulation-free optimization objective, effectivelyminimizing a variational upper bound on the negative log-likelihood.Experimental results demonstrate NFDM's strong performance, evidenced bystate-of-the-art likelihood estimation. Furthermore, we investigate NFDM'scapacity for learning generative dynamics with specific characteristics, suchas deterministic straight lines trajectories. This exploration underscoresNFDM's versatility and its potential for a wide range of applications.</description><author>Grigory Bartosh, Dmitry Vetrov, Christian A. Naesseth</author><pubDate>Fri, 19 Apr 2024 16:10:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12940v1</guid></item><item><title>MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews</title><link>http://arxiv.org/abs/2404.12938v1</link><description>Deceptive reviews are becoming increasingly common, especially given theincrease in performance and the prevalence of LLMs. While work to date hasaddressed the development of models to differentiate between truthful anddeceptive human reviews, much less is known about the distinction between realreviews and AI-authored fake reviews. Moreover, most of the research so far hasfocused primarily on English, with very little work dedicated to otherlanguages. In this paper, we compile and make publicly available the MAiDE-updataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews,balanced across ten languages. Using this dataset, we conduct extensivelinguistic analyses to (1) compare the AI fake hotel reviews to real hotelreviews, and (2) identify the factors that influence the deception detectionmodel performance. We explore the effectiveness of several models for deceptiondetection in hotel reviews across three main dimensions: sentiment, location,and language. We find that these dimensions influence how well we can detectAI-generated fake reviews.</description><author>Oana Ignat, Xiaomeng Xu, Rada Mihalcea</author><pubDate>Fri, 19 Apr 2024 16:08:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12938v1</guid></item><item><title>Cross-cultural Inspiration Detection and Analysis in Real and LLM-generated Social Media Data</title><link>http://arxiv.org/abs/2404.12933v1</link><description>Inspiration is linked to various positive outcomes, such as increasedcreativity, productivity, and happiness. Although inspiration has greatpotential, there has been limited effort toward identifying content that isinspiring, as opposed to just engaging or positive. Additionally, most researchhas concentrated on Western data, with little attention paid to other cultures.This work is the first to study cross-cultural inspiration through machinelearning methods. We aim to identify and analyze real and AI-generatedcross-cultural inspiring posts. To this end, we compile and make publiclyavailable the InspAIred dataset, which consists of 2,000 real inspiring posts,2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenlydistributed across India and the UK. The real posts are sourced from Reddit,while the generated posts are created using the GPT-4 model. Using thisdataset, we conduct extensive computational linguistic analyses to (1) compareinspiring content across cultures, (2) compare AI-generated inspiring posts toreal inspiring posts, and (3) determine if detection models can accuratelydistinguish between inspiring content across cultures and data sources.</description><author>Oana Ignat, Gayathri Ganesh Lakshmy, Rada Mihalcea</author><pubDate>Fri, 19 Apr 2024 16:04:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12933v1</guid></item><item><title>The Positivity of the Neural Tangent Kernel</title><link>http://arxiv.org/abs/2404.12928v1</link><description>The Neural Tangent Kernel (NTK) has emerged as a fundamental concept in thestudy of wide Neural Networks. In particular, it is known that the positivityof the NTK is directly related to the memorization capacity of sufficientlywide networks, i.e., to the possibility of reaching zero loss in training, viagradient descent. Here we will improve on previous works and obtain a sharpresult concerning the positivity of the NTK of feedforward networks of anydepth. More precisely, we will show that, for any non-polynomial activationfunction, the NTK is strictly positive definite. Our results are based on anovel characterization of polynomial functions which is of independentinterest.</description><author>Luís Carvalho, João L. Costa, José Mourão, Gonçalo Oliveira</author><pubDate>Fri, 19 Apr 2024 15:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12928v1</guid></item><item><title>MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering</title><link>http://arxiv.org/abs/2404.12926v1</link><description>Recent advancements in LLMs have shown their significant potential in taskslike text summarization and generation. Yet, they often encounter difficultywhile solving complex physics problems that require arithmetic calculation anda good understanding of concepts. Moreover, many physics problems includeimages that contain important details required to understand the problem'scontext. We propose an LMM-based chatbot to answer multimodal physics MCQs. Fordomain adaptation, we utilize the MM-PhyQA dataset comprising Indian highschool-level multimodal physics problems. To improve the LMM's performance, weexperiment with two techniques, RLHF (Reinforcement Learning from HumanFeedback) and Image Captioning. In image captioning, we add a detailedexplanation of the diagram in each image, minimizing hallucinations and imageprocessing errors. We further explore the integration of Reinforcement Learningfrom Human Feedback (RLHF) methodology inspired by the ranking approach in RLHFto enhance the human-like problem-solving abilities of the models. The RLHFapproach incorporates human feedback into the learning process of LLMs,improving the model's problem-solving skills, truthfulness, and reasoningcapabilities, minimizing the hallucinations in the answers, and improving thequality instead of using vanilla-supervised fine-tuned models. We employ theLLaVA open-source model to answer multimodal physics MCQs and compare theperformance with and without using RLHF.</description><author>Avinash Anand, Janak Kapuriya, Chhavi Kirtani, Apoorv Singh, Jay Saraf, Naman Lal, Jatin Kumar, Adarsh Raj Shivam, Astha Verma, Rajiv Ratn Shah, Roger Zimmermann</author><pubDate>Fri, 19 Apr 2024 15:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12926v1</guid></item><item><title>A Hybrid Generative and Discriminative PointNet on Unordered Point Sets</title><link>http://arxiv.org/abs/2404.12925v1</link><description>As point cloud provides a natural and flexible representation usable inmyriad applications (e.g., robotics and self-driving cars), the ability tosynthesize point clouds for analysis becomes crucial. Recently, Xie et al.propose a generative model for unordered point sets in the form of anenergy-based model (EBM). Despite the model achieving an impressive performancefor point cloud generation, one separate model needs to be trained for eachcategory to capture the complex point set distributions. Besides, their methodis unable to classify point clouds directly and requires additional fine-tuningfor classification. One interesting question is: Can we train a single networkfor a hybrid generative and discriminative model of point clouds? A similarquestion has recently been answered in the affirmative for images, introducingthe framework of Joint Energy-based Model (JEM), which achieves highperformance in image classification and generation simultaneously. This paperproposes GDPNet, the first hybrid Generative and Discriminative PointNet thatextends JEM for point cloud classification and generation. Our GDPNet retainsstrong discriminative power of modern PointNet classifiers, while generatingpoint cloud samples rivaling state-of-the-art generative approaches.</description><author>Yang Ye, Shihao Ji</author><pubDate>Fri, 19 Apr 2024 15:52:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12925v1</guid></item><item><title>Probabilistic-Numeric SMC Sampling for Bayesian Nonlinear System Identification in Continuous Time</title><link>http://arxiv.org/abs/2404.12923v1</link><description>In engineering, accurately modeling nonlinear dynamic systems from datacontaminated by noise is both essential and complex. Established SequentialMonte Carlo (SMC) methods, used for the Bayesian identification of thesesystems, facilitate the quantification of uncertainty in the parameteridentification process. A significant challenge in this context is thenumerical integration of continuous-time ordinary differential equations(ODEs), crucial for aligning theoretical models with discretely sampled data.This integration introduces additional numerical uncertainty, a factor that isoften over looked. To address this issue, the field of probabilistic numericscombines numerical methods, such as numerical integration, with probabilisticmodeling to offer a more comprehensive analysis of total uncertainty. Byretaining the accuracy of classical deterministic methods, these probabilisticapproaches offer a deeper understanding of the uncertainty inherent in theinference process. This paper demonstrates the application of a probabilisticnumerical method for solving ODEs in the joint parameter-state identificationof nonlinear dynamic systems. The presented approach efficiently identifieslatent states and system parameters from noisy measurements. Simultaneouslyincorporating probabilistic solutions to the ODE in the identificationchallenge. The methodology's primary advantage lies in its capability toproduce posterior distributions over system parameters, thereby representingthe inherent uncertainties in both the data and the identification process.</description><author>Joe D. Longbottom, Max D. Champneys, Timothy J. Rogers</author><pubDate>Fri, 19 Apr 2024 15:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12923v1</guid></item><item><title>LitSumm: Large language models for literature summarisation of non-coding RNAs</title><link>http://arxiv.org/abs/2311.03056v3</link><description>Motivation: Curation of literature in life sciences is a growing challenge.The continued increase in the rate of publication, coupled with the relativelyfixed number of curators worldwide presents a major challenge to developers ofbiomedical knowledgebases. Very few knowledgebases have resources to scale tothe whole relevant literature and all have to prioritise their efforts. Results: In this work, we take a first step to alleviating the lack ofcurator time in RNA science by generating summaries of literature fornon-coding RNAs using large language models (LLMs). We demonstrate thathigh-quality, factually accurate summaries with accurate references can beautomatically generated from the literature using a commercial LLM and a chainof prompts and checks. Manual assessment was carried out for a subset ofsummaries, with the majority being rated extremely high quality. We alsoapplied the most commonly used automated evaluation approaches, finding thatthey do not correlate with human assessment. Finally, we apply our tool to aselection of over 4,600 ncRNAs and make the generated summaries available viathe RNAcentral resource. We conclude that automated literature summarization isfeasible with the current generation of LLMs, provided careful prompting andautomated checking are applied. Availability: Code used to produce these summaries can be found here:https://github.com/RNAcentral/litscan-summarization and the dataset of contextsand summaries can be found here:https://huggingface.co/datasets/RNAcentral/litsumm-v1. Summaries are alsodisplayed on the RNA report pages in RNAcentral (https://rnacentral.org/)</description><author>Andrew Green, Carlos Ribas, Nancy Ontiveros-Palacios, Sam Griffiths-Jones, Anton I. Petrov, Alex Bateman, Blake Sweeney</author><pubDate>Fri, 19 Apr 2024 15:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03056v3</guid></item><item><title>Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images</title><link>http://arxiv.org/abs/2404.12922v1</link><description>In this paper, we introduce Selective-distillation for Class andArchitecture-agnostic unleaRning (SCAR), a novel approximate unlearning method.SCAR efficiently eliminates specific information while preserving the model'stest accuracy without using a retain set, which is a key component instate-of-the-art approximate unlearning algorithms. Our approach utilizes amodified Mahalanobis distance to guide the unlearning of the feature vectors ofthe instances to be forgotten, aligning them to the nearest wrong classdistribution. Moreover, we propose a distillation-trick mechanism that distillsthe knowledge of the original model into the unlearning model without-of-distribution images for retaining the original model's test performancewithout using any retain set. Importantly, we propose a self-forget version ofSCAR that unlearns without having access to the forget set. We experimentallyverified the effectiveness of our method, on three public datasets, comparingit with state-of-the-art methods. Our method obtains performance higher thanmethods that operate without the retain set and comparable w.r.t the bestmethods that rely on the retain set.</description><author>Jacopo Bonato, Marco Cotogni, Luigi Sabetta</author><pubDate>Fri, 19 Apr 2024 15:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12922v1</guid></item><item><title>Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</title><link>http://arxiv.org/abs/2404.12920v1</link><description>Localizing the exact pathological regions in a given medical scan is animportant imaging problem that requires a large amount of bounding box groundtruth annotations to be accurately solved. However, there exist alternative,potentially weaker, forms of supervision, such as accompanying free-textreports, which are readily available. The task of performing localization withtextual guidance is commonly referred to as phrase grounding. In this work, weuse a publicly available Foundation Model, namely the Latent Diffusion Model,to solve this challenging task. This choice is supported by the fact that theLatent Diffusion Model, despite being generative in nature, contains mechanisms(cross-attention) that implicitly align visual and textual features, thusleading to intermediate representations that are suitable for the task at hand.In addition, we aim to perform this task in a zero-shot manner, i.e., withoutany further training on target data, meaning that the model's weights remainfrozen. To this end, we devise strategies to select features and also refinethem via post-processing without extra learnable parameters. We compare ourproposed method with state-of-the-art approaches which explicitly enforceimage-text alignment in a joint embedding space via contrastive learning.Results on a popular chest X-ray benchmark indicate that our method iscompetitive wih SOTA on different types of pathology, and even outperforms themon average in terms of two metrics (mean IoU and AUC-ROC). Source code will bereleased upon acceptance.</description><author>Konstantinos Vilouras, Pedro Sanchez, Alison Q. O'Neil, Sotirios A. Tsaftaris</author><pubDate>Fri, 19 Apr 2024 15:43:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12920v1</guid></item><item><title>Zero-Shot Stitching in Reinforcement Learning using Relative Representations</title><link>http://arxiv.org/abs/2404.12917v1</link><description>Visual Reinforcement Learning is a popular and powerful framework that takesfull advantage of the Deep Learning breakthrough. However, it is also knownthat variations in the input (e.g., different colors of the panorama due to theseason of the year) or the task (e.g., changing the speed limit for a car torespect) could require complete retraining of the agents. In this work, weleverage recent developments in unifying latent representations to demonstratethat it is possible to combine the components of an agent, rather than retrainit from scratch. We build upon the recent relative representations frameworkand adapt it for Visual RL. This allows us to create completely new agentscapable of handling environment-task combinations never seen during training.Our work paves the road toward a more accessible and flexible use ofreinforcement learning.</description><author>Antonio Pio Ricciardi, Valentino Maiorca, Luca Moschella, Riccardo Marin, Emanuele Rodolà</author><pubDate>Fri, 19 Apr 2024 15:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12917v1</guid></item><item><title>Negative impact of heavy-tailed uncertainty and error distributions on the reliability of calibration statistics for machine learning regression tasks</title><link>http://arxiv.org/abs/2402.10043v3</link><description>Average calibration of the prediction uncertainties of machine learningregression tasks can be tested in two ways: one is to estimate the calibrationerror (CE) as the difference between the mean absolute error (MSE) and the meanvariance (MV) or mean squared uncertainty; the alternative is to compare themean squared z-scores (ZMS) or scaled errors to 1. The problem is that bothapproaches might lead to different conclusions, as illustrated in this studyfor an ensemble of datasets from the recent machine learning uncertaintyquantification (ML-UQ) literature. It is shown that the estimation of MV, MSEand their confidence intervals can become unreliable for heavy-taileduncertainty and error distributions, which seems to be a common issue for ML-UQdatasets. By contrast, the ZMS statistic is less sensitive and offers the mostreliable approach in this context. Unfortunately, the same problem affects alsoconditional calibrations statistics, such as the popular ENCE, and very likelypost-hoc calibration methods based on similar statistics. As not much can bedone to relieve this issue, except for a change of paradigm to intervals- ordistribution-based UQ metrics, robust tailedness metrics are proposed to detectthe potentially problematic datasets.</description><author>Pascal Pernot</author><pubDate>Fri, 19 Apr 2024 15:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10043v3</guid></item><item><title>An Embodied Generalist Agent in 3D World</title><link>http://arxiv.org/abs/2311.12871v2</link><description>Leveraging massive knowledge and learning schemes from large language models(LLMs), recent machine learning models show notable successes in buildinggeneralist agents that exhibit the capability of general-purpose task solvingin diverse domains, including natural language processing, computer vision, androbotics. However, a significant challenge remains as these models exhibitlimited ability in understanding and interacting with the 3D world. We arguethis limitation significantly hinders the current models from performingreal-world tasks and further achieving general intelligence. To this end, weintroduce an embodied multi-modal and multi-task generalist agent that excelsin perceiving, grounding, reasoning, planning, and acting in the 3D world. Ourproposed agent, referred to as LEO, is trained with shared LLM-based modelarchitectures, objectives, and weights in two stages: (i) 3D vision-languagealignment and (ii) 3D vision-language-action instruction tuning. To facilitatethe training, we meticulously curate and generate an extensive datasetcomprising object-level and scene-level multi-modal tasks with exceeding scaleand complexity, necessitating a deep understanding of and interaction with the3D world. Through rigorous experiments, we demonstrate LEO's remarkableproficiency across a wide spectrum of tasks, including 3D captioning, questionanswering, embodied reasoning, embodied navigation, and robotic manipulation.Our ablation results further provide valuable insights for the development offuture embodied generalist agents.</description><author>Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang</author><pubDate>Fri, 19 Apr 2024 15:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12871v2</guid></item><item><title>Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images</title><link>http://arxiv.org/abs/2404.12908v1</link><description>Diffusion models (DMs) have revolutionized image generation, producinghigh-quality images with applications spanning various fields. However, theirability to create hyper-realistic images poses significant challenges indistinguishing between real and synthetic content, raising concerns aboutdigital authenticity and potential misuse in creating deepfakes. This workintroduces a robust detection framework that integrates image and text featuresextracted by CLIP model with a Multilayer Perceptron (MLP) classifier. Wepropose a novel loss that can improve the detector's robustness and handleimbalanced datasets. Additionally, we flatten the loss landscape during themodel training to improve the detector's generalization capabilities. Theeffectiveness of our method, which outperforms traditional detectiontechniques, is demonstrated through extensive experiments, underscoring itspotential to set a new state-of-the-art approach in DM-generated imagedetection. The code is available athttps://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection.</description><author>Santosh, Li Lin, Irene Amerini, Xin Wang, Shu Hu</author><pubDate>Fri, 19 Apr 2024 15:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12908v1</guid></item><item><title>Mitigating Open-Vocabulary Caption Hallucinations</title><link>http://arxiv.org/abs/2312.03631v3</link><description>While recent years have seen rapid progress in image-conditioned textgeneration, image captioning still suffers from the fundamental issue ofhallucinations, namely, the generation of spurious details that cannot beinferred from the given image. Existing methods largely use closed-vocabularyobject lists to mitigate or evaluate hallucinations in image captioning,ignoring the long-tailed nature of hallucinations that occur in practice. Tothis end, we propose a framework for addressing hallucinations in imagecaptioning in the open-vocabulary setting. Our framework includes a newbenchmark, OpenCHAIR, that leverages generative foundation models to evaluateopen-vocabulary object hallucinations for image captioning, surpassing thepopular and similarly-sized CHAIR benchmark in both diversity and accuracy.Furthermore, to mitigate open-vocabulary hallucinations without using a closedobject list, we propose MOCHa, an approach harnessing advancements inreinforcement learning. Our multi-objective reward function explicitly targetsthe trade-off between fidelity and adequacy in generations without requiringany strong supervision. MOCHa improves a large variety of image captioningmodels, as captured by our OpenCHAIR benchmark and other existing metrics. Wewill release our code and models.</description><author>Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor</author><pubDate>Fri, 19 Apr 2024 15:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03631v3</guid></item><item><title>KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced Multi-Vehicle Trajectory Forecasting at Signalized Intersections</title><link>http://arxiv.org/abs/2404.11181v2</link><description>Reliable prediction of vehicle trajectories at signalized intersections iscrucial to urban traffic management and autonomous driving systems. However, itpresents unique challenges, due to the complex roadway layout at intersections,involvement of traffic signal controls, and interactions among different typesof road users. To address these issues, we present in this paper a novel modelcalled Knowledge-Informed Generative Adversarial Network (KI-GAN), whichintegrates both traffic signal information and multi-vehicle interactions topredict vehicle trajectories accurately. Additionally, we propose a specializedattention pooling method that accounts for vehicle orientation and proximity atintersections. Based on the SinD dataset, our KI-GAN model is able to achievean Average Displacement Error (ADE) of 0.05 and a Final Displacement Error(FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. Whenthe prediction window is extended to 9 seconds, the ADE and FDE values arefurther reduced to 0.11 and 0.26, respectively. These results demonstrate theeffectiveness of the proposed KI-GAN model in vehicle trajectory predictionunder complex scenarios at signalized intersections, which represents asignificant advancement in the target field.</description><author>Chuheng Wei, Guoyuan Wu, Matthew J. Barth, Amr Abdelraouf, Rohit Gupta, Kyungtae Han</author><pubDate>Fri, 19 Apr 2024 15:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11181v2</guid></item><item><title>A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches</title><link>http://arxiv.org/abs/2404.02817v3</link><description>Task and Motion Planning (TAMP) integrates high-level task planning andlow-level motion planning to equip robots with the autonomy to effectivelyreason over long-horizon, dynamic tasks. Optimization-based TAMP focuses onhybrid optimization approaches that define goal conditions via objectivefunctions and are capable of handling open-ended goals, robotic dynamics, andphysical interaction between the robot and the environment. Therefore,optimization-based TAMP is particularly suited to solve highly complex,contact-rich locomotion and manipulation problems. This survey provides acomprehensive review on optimization-based TAMP, covering (i) planning domainrepresentations, including action description languages and temporal logic,(ii) individual solution strategies for components of TAMP, including AIplanning and trajectory optimization (TO), and (iii) the dynamic interplaybetween logic-based task planning and model-based TO. A particular focus ofthis survey is to highlight the algorithm structures to efficiently solve TAMP,especially hierarchical and distributed approaches. Additionally, the surveyemphasizes the synergy between the classical methods and contemporarylearning-based innovations such as large language models. Furthermore, thefuture research directions for TAMP is discussed in this survey, highlightingboth algorithmic and application-specific challenges.</description><author>Zhigen Zhao, Shuo Cheng, Yan Ding, Ziyi Zhou, Shiqi Zhang, Danfei Xu, Ye Zhao</author><pubDate>Fri, 19 Apr 2024 15:26:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02817v3</guid></item><item><title>Feature Corrective Transfer Learning: End-to-End Solutions to Object Detection in Non-Ideal Visual Conditions</title><link>http://arxiv.org/abs/2404.11214v2</link><description>A significant challenge in the field of object detection lies in the system'sperformance under non-ideal imaging conditions, such as rain, fog, lowillumination, or raw Bayer images that lack ISP processing. Our studyintroduces "Feature Corrective Transfer Learning", a novel approach thatleverages transfer learning and a bespoke loss function to facilitate theend-to-end detection of objects in these challenging scenarios without the needto convert non-ideal images into their RGB counterparts. In our methodology, weinitially train a comprehensive model on a pristine RGB image dataset.Subsequently, non-ideal images are processed by comparing their feature mapsagainst those from the initial ideal RGB model. This comparison employs theExtended Area Novel Structural Discrepancy Loss (EANSDL), a novel loss functiondesigned to quantify similarities and integrate them into the detection loss.This approach refines the model's ability to perform object detection acrossvarying conditions through direct feature map correction, encapsulating theessence of Feature Corrective Transfer Learning. Experimental validation onvariants of the KITTI dataset demonstrates a significant improvement in meanAverage Precision (mAP), resulting in a 3.8-8.1% relative enhancement indetection under non-ideal conditions compared to the baseline model, and a lessmarginal performance difference within 1.3% of the mAP@[0.5:0.95] achievedunder ideal conditions by the standard Faster RCNN algorithm.</description><author>Chuheng Wei, Guoyuan Wu, Matthew J. Barth</author><pubDate>Fri, 19 Apr 2024 15:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11214v2</guid></item><item><title>Learning Symbolic Task Representation from a Human-Led Demonstration: A Memory to Store, Retrieve, Consolidate, and Forget Experiences</title><link>http://arxiv.org/abs/2404.10591v2</link><description>We present a symbolic learning framework inspired by cognitive-like memoryfunctionalities (i.e., storing, retrieving, consolidating and forgetting) togenerate task representations to support high-level task planning and knowledgebootstrapping. We address a scenario involving a non-expert human, who performsa single task demonstration, and a robot, which online learns structuredknowledge to re-execute the task based on experiences, i.e., observations. Weconsider a one-shot learning process based on non-annotated data to store anintelligible representation of the task, which can be refined throughinteraction, e.g., via verbal or visual communication. Our general-purposeframework relies on fuzzy Description Logic, which has been used to extend thepreviously developed Scene Identification and Tagging algorithm. In this paper,we exploit such an algorithm to implement cognitive-like memory functionalitiesemploying scores that rank memorised observations over time based on simpleheuristics. Our main contribution is the formalisation of a framework that canbe used to systematically investigate different heuristics for bootstrappinghierarchical knowledge representations based on robot observations. Through anillustrative assembly task scenario, the paper presents the performance of ourframework to discuss its benefits and limitations.</description><author>Luca Buoncompagni, Fulvio Mastrogiovanni</author><pubDate>Fri, 19 Apr 2024 15:21:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10591v2</guid></item><item><title>Large Language Models for Networking: Workflow, Advances and Challenges</title><link>http://arxiv.org/abs/2404.12901v1</link><description>The networking field is characterized by its high complexity and rapiditeration, requiring extensive expertise to accomplish network tasks, rangingfrom network design, diagnosis, configuration and security. The inherentcomplexity of these tasks, coupled with the ever-changing landscape ofnetworking technologies and protocols, poses significant hurdles fortraditional machine learning-based methods. These methods often struggle togeneralize and automate complex tasks in networking, as they require extensivelabeled data, domain-specific feature engineering, and frequent retraining toadapt to new scenarios. However, the recent emergence of large language models(LLMs) has sparked a new wave of possibilities in addressing these challenges.LLMs have demonstrated remarkable capabilities in natural languageunderstanding, generation, and reasoning. These models, trained on extensivedata, can benefit the networking domain. Some efforts have already explored theapplication of LLMs in the networking domain and revealed promising results. Byreviewing recent advances, we present an abstract workflow to describe thefundamental process involved in applying LLM for Networking. We introduce thehighlights of existing works by category and explain in detail how they operateat different stages of the workflow. Furthermore, we delve into the challengesencountered, discuss potential solutions, and outline future researchprospects. We hope that this survey will provide insight for researchers andpractitioners, promoting the development of this interdisciplinary researchfield.</description><author>Chang Liu, Xiaohui Xie, Xinggong Zhang, Yong Cui</author><pubDate>Fri, 19 Apr 2024 15:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12901v1</guid></item><item><title>RANRAC: Robust Neural Scene Representations via Random Ray Consensus</title><link>http://arxiv.org/abs/2312.09780v2</link><description>Learning-based scene representations such as neural radiance fields or lightfield networks, that rely on fitting a scene model to image observations,commonly encounter challenges in the presence of inconsistencies within theimages caused by occlusions, inaccurately estimated camera parameters oreffects like lens flare. To address this challenge, we introduce RANdom RAyConsensus (RANRAC), an efficient approach to eliminate the effect ofinconsistent data, thereby taking inspiration from classical RANSAC basedoutlier detection for model fitting. In contrast to the down-weighting of theeffect of outliers based on robust loss formulations, our approach reliablydetects and excludes inconsistent perspectives, resulting in clean imageswithout floating artifacts. For this purpose, we formulate a fuzzy adaption ofthe RANSAC paradigm, enabling its application to large scale models. Weinterpret the minimal number of samples to determine the model parameters as atunable hyperparameter, investigate the generation of hypotheses withdata-driven models, and analyze the validation of hypotheses in noisyenvironments. We demonstrate the compatibility and potential of our solutionfor both photo-realistic robust multi-view reconstruction from real-worldimages based on neural radiance fields and for single-shot reconstruction basedon light-field networks. In particular, the results indicate significantimprovements compared to state-of-the-art robust methods for novel-viewsynthesis on both synthetic and captured scenes with various inconsistenciesincluding occlusions, noisy camera pose estimates, and unfocused perspectives.The results further indicate significant improvements for single-shotreconstruction from occluded images. Project Page:https://bennobuschmann.com/ranrac/</description><author>Benno Buschmann, Andreea Dogaru, Elmar Eisemann, Michael Weinmann, Bernhard Egger</author><pubDate>Fri, 19 Apr 2024 15:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09780v2</guid></item><item><title>RefinedFields: Radiance Fields Refinement for Unconstrained Scenes</title><link>http://arxiv.org/abs/2312.00639v3</link><description>Modeling large scenes from unconstrained images has proven to be a majorchallenge in computer vision. Existing methods tackling in-the-wild scenemodeling operate in closed-world settings, where no conditioning on priorsacquired from real-world images is present. We propose RefinedFields, which is,to the best of our knowledge, the first method leveraging pre-trained models toimprove in-the-wild scene modeling. We employ pre-trained networks to refineK-Planes representations via optimization guidance using an alternatingtraining procedure. We carry out extensive experiments and verify the merit ofour method on synthetic data and real tourism photo collections. RefinedFieldsenhances rendered scenes with richer details and improves upon its baserepresentation on the task of novel view synthesis in the wild. Our projectpage can be found at https://refinedfields.github.io.</description><author>Karim Kassab, Antoine Schnepf, Jean-Yves Franceschi, Laurent Caraffa, Jeremie Mary, Valérie Gouet-Brunet</author><pubDate>Fri, 19 Apr 2024 15:14:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00639v3</guid></item><item><title>Training-and-prompt-free General Painterly Harmonization Using Image-wise Attention Sharing</title><link>http://arxiv.org/abs/2404.12900v1</link><description>Painterly Image Harmonization aims at seamlessly blending disparate visualelements within a single coherent image. However, previous approaches oftenencounter significant limitations due to training data constraints, the needfor time-consuming fine-tuning, or reliance on additional prompts. To surmountthese hurdles, we design a Training-and-prompt-Free General PainterlyHarmonization method using image-wise attention sharing (TF-GPH), whichintegrates a novel "share-attention module". This module redefines thetraditional self-attention mechanism by allowing for comprehensive image-wiseattention, facilitating the use of a state-of-the-art pretrained latentdiffusion model without the typical training data limitations. Additionally, wefurther introduce "similarity reweighting" mechanism enhances performance byeffectively harnessing cross-image information, surpassing the capabilities offine-tuning or prompt-based approaches. At last, we recognize the deficienciesin existing benchmarks and propose the "General Painterly HarmonizationBenchmark", which employs range-based evaluation metrics to more accuratelyreflect real-world application. Extensive experiments demonstrate the superiorefficacy of our method across various benchmarks. The code and web demo areavailable at https://github.com/BlueDyee/TF-GPH.</description><author>Teng-Fang Hsiao, Bo-Kai Ruan, Hong-Han Shuai</author><pubDate>Fri, 19 Apr 2024 15:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12900v1</guid></item><item><title>On the Pitfalls of Batch Normalization for End-to-End Video Learning: A Study on Surgical Workflow Analysis</title><link>http://arxiv.org/abs/2203.07976v5</link><description>Batch Normalization's (BN) unique property of depending on other samples in abatch is known to cause problems in several tasks, including sequence modeling.Yet, BN-related issues are hardly studied for long video understanding, despitethe ubiquitous use of BN in CNNs (Convolutional Neural Networks) for featureextraction. Especially in surgical workflow analysis, where the lack ofpretrained feature extractors has led to complex, multi-stage trainingpipelines, limited awareness of BN issues may have hidden the benefits oftraining CNNs and temporal models end to end. In this paper, we analyzepitfalls of BN in video learning, including issues specific to online taskssuch as a 'cheating' effect in anticipation. We observe that BN's propertiescreate major obstacles for end-to-end learning. However, using BN-freebackbones, even simple CNN-LSTMs beat the state of the art{\color{\colorrevtwo}on three surgical workflow benchmarks} by utilizingadequate end-to-end training strategies which maximize temporal context. Weconclude that awareness of BN's pitfalls is crucial for effective end-to-endlearning in surgical tasks. By reproducing results on natural-video datasets,we hope our insights will benefit other areas of video learning as well. Codeis available at: \url{https://gitlab.com/nct_tso_public/pitfalls_bn}</description><author>Dominik Rivoir, Isabel Funke, Stefanie Speidel</author><pubDate>Fri, 19 Apr 2024 15:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.07976v5</guid></item><item><title>Bayesian Co-navigation: Dynamic Designing of the Materials Digital Twins via Active Learning</title><link>http://arxiv.org/abs/2404.12899v1</link><description>Scientific advancement is universally based on the dynamic interplay betweentheoretical insights, modelling, and experimental discoveries. However, thisfeedback loop is often slow, including delayed community interactions and thegradual integration of experimental data into theoretical frameworks. Thischallenge is particularly exacerbated in domains dealing with high-dimensionalobject spaces, such as molecules and complex microstructures. Hence, theintegration of theory within automated and autonomous experimental setups, ortheory in the loop automated experiment, is emerging as a crucial objective foraccelerating scientific research. The critical aspect is not only to use theorybut also on-the-fly theory updates during the experiment. Here, we introduce amethod for integrating theory into the loop through Bayesian co-navigation oftheoretical model space and experimentation. Our approach leverages theconcurrent development of surrogate models for both simulation and experimentaldomains at the rates determined by latencies and costs of experiments andcomputation, alongside the adjustment of control parameters within theoreticalmodels to minimize epistemic uncertainty over the experimental object spaces.This methodology facilitates the creation of digital twins of materialstructures, encompassing both the surrogate model of behavior that includes thecorrelative part and the theoretical model itself. While demonstrated herewithin the context of functional responses in ferroelectric materials, ourapproach holds promise for broader applications, the exploration of opticalproperties in nanoclusters, microstructure-dependent properties in complexmaterials, and properties of molecular systems. The analysis code that supportsthe funding is publicly available athttps://github.com/Slautin/2024_Co-navigation/tree/main</description><author>Boris N. Slautin, Yongtao Liu, Hiroshi Funakubo, Rama K. Vasudevan, Maxim A. Ziatdinov, Sergei V. Kalinin</author><pubDate>Fri, 19 Apr 2024 15:11:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12899v1</guid></item><item><title>Enabling Natural Zero-Shot Prompting on Encoder Models via Statement-Tuning</title><link>http://arxiv.org/abs/2404.12897v1</link><description>While Large Language Models (LLMs) exhibit remarkable capabilities inzero-shot and few-shot scenarios, they often require computationallyprohibitive sizes. Conversely, smaller Masked Language Models (MLMs) like BERTand RoBERTa achieve state-of-the-art results through fine-tuning but strugglewith extending to few-shot and zero-shot settings due to their architecturalconstraints. Hence, we propose Statement-Tuning, a technique that modelsdiscriminative tasks as a set of finite statements and trains an Encoder modelto discriminate between the potential statements to determine the label. We doStatement-Tuning on multiple tasks to enable cross-task generalization.Experimental results demonstrate that Statement Tuning achieves competitiveperformance compared to state-of-the-art LLMs with significantly fewerparameters. Moreover, the study investigates the impact of several designchoices on few-shot and zero-shot generalization, revealing that StatementTuning can achieve sufficient performance with modest training data andbenefits from task and statement diversity for unseen task generalizability.</description><author>Ahmed Elshabrawy, Yongix Huang, Iryna Gurevych, Alham Fikri Aji</author><pubDate>Fri, 19 Apr 2024 15:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12897v1</guid></item><item><title>Leveraging Automatic Personalised Nutrition: Food Image Recognition Benchmark and Dataset based on Nutrition Taxonomy</title><link>http://arxiv.org/abs/2211.07440v4</link><description>Maintaining a healthy lifestyle has become increasingly challenging intoday's sedentary society marked by poor eating habits. To address this issue,both national and international organisations have made numerous efforts topromote healthier diets and increased physical activity. However, implementingthese recommendations in daily life can be difficult, as they are often genericand not tailored to individuals. This study presents the AI4Food-NutritionDBdatabase, the first nutrition database that incorporates food images and anutrition taxonomy based on recommendations by national and internationalhealth authorities. The database offers a multi-level categorisation,comprising 6 nutritional levels, 19 main categories (e.g., "Meat"), 73subcategories (e.g., "White Meat"), and 893 specific food products (e.g.,"Chicken"). The AI4Food-NutritionDB opens the doors to new food computingapproaches in terms of food intake frequency, quality, and categorisation.Also, we present a standardised experimental protocol and benchmark includingthree tasks based on the nutrition taxonomy (i.e., category, subcategory, andfinal product recognition). These resources are available to the researchcommunity, including our deep learning models trained on AI4Food-NutritionDB,which can serve as pre-trained models, achieving accurate recognition resultsfor challenging food image databases.</description><author>Sergio Romero-Tapiador, Ruben Tolosana, Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez, Isabel Espinosa-Salinas, Gala Freixer, Enrique Carrillo de Santa Pau, Ana Ramírez de Molina, Javier Ortega-Garcia</author><pubDate>Fri, 19 Apr 2024 15:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07440v4</guid></item><item><title>Multi-modal vision-language model for generalizable annotation-free pathological lesions localization and clinical diagnosis</title><link>http://arxiv.org/abs/2401.02044v3</link><description>Defining pathologies automatically from medical images aids the understandingof the emergence and progression of diseases, and such an ability is crucial inclinical diagnostics. However, existing deep learning models heavily rely onexpert annotations and lack generalization capabilities in open clinicalenvironments. In this study, we present a generalizable vision-languagepre-training model for Annotation-Free pathological lesions Localization(AFLoc). The core strength of AFLoc lies in its extensive multi-level semanticstructure-based contrastive learning, which comprehensively alignsmulti-granularity medical concepts from reports with abundant image features,to adapt to the diverse expressions of pathologies and unseen pathologieswithout the reliance on image annotations from experts. We demonstrate theproof of concept on CXR images, with extensive experimental validation across 4distinct external datasets, encompassing 11 types of chest pathologies. Theresults demonstrate that AFLoc surpasses state-of-the-art methods inpathological lesions localization and disease classification, and evenoutperforms the human benchmark in locating 5 different pathologies.Additionally, we further verify its generalization ability by applying it toretinal fundus images. Our approach showcases AFoc versatilities andunderscores its suitability for clinical diagnoses in complex clinicalenvironments.</description><author>Hao Yang, Hong-Yu Zhou, Zhihuan Li, Yuanxu Gao, Cheng Li, Weijian Huang, Jiarun Liu, Hairong Zheng, Kang Zhang, Shanshan Wang</author><pubDate>Fri, 19 Apr 2024 15:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02044v3</guid></item><item><title>A Machine Learning-Based Error Mitigation Approach For Reliable Software Development On IBM'S Quantum Computers</title><link>http://arxiv.org/abs/2404.12892v1</link><description>Quantum computers have the potential to outperform classical computers forsome complex computational problems. However, current quantum computers (e.g.,from IBM and Google) have inherent noise that results in errors in the outputsof quantum software executing on the quantum computers, affecting thereliability of quantum software development. The industry is increasinglyinterested in machine learning (ML)--based error mitigation techniques, giventheir scalability and practicality. However, existing ML-based techniques havelimitations, such as only targeting specific noise types or specific quantumcircuits. This paper proposes a practical ML-based approach, called Q-LEAR,with a novel feature set, to mitigate noise errors in quantum software outputs.We evaluated Q-LEAR on eight quantum computers and their corresponding noisysimulators, all from IBM, and compared Q-LEAR with a state-of-the-art ML-basedapproach taken as baseline. Results show that, compared to the baseline, Q-LEARachieved a 25% average improvement in error mitigation on both real quantumcomputers and simulators. We also discuss the implications and practicality ofQ-LEAR, which, we believe, is valuable for practitioners.</description><author>Asmar Muqeet, Shaukat Ali, Tao Yue, Paolo Arcaini</author><pubDate>Fri, 19 Apr 2024 14:51:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12892v1</guid></item><item><title>Succinct Interaction-Aware Explanations</title><link>http://arxiv.org/abs/2402.05566v2</link><description>SHAP is a popular approach to explain black-box models by revealing theimportance of individual features. As it ignores feature interactions, SHAPexplanations can be confusing up to misleading. NSHAP, on the other hand,reports the additive importance for all subsets of features. While this doesinclude all interacting sets of features, it also leads to an exponentiallysized, difficult to interpret explanation. In this paper, we propose to combinethe best of these two worlds, by partitioning the features into parts thatsignificantly interact, and use these parts to compose a succinct,interpretable, additive explanation. We derive a criterion by which to measurethe representativeness of such a partition for a models behavior, traded offagainst the complexity of the resulting explanation. To efficiently find thebest partition out of super-exponentially many, we show how to prunesub-optimal solutions using a statistical test, which not only improves runtimebut also helps to detect spurious interactions. Experiments on synthetic andreal world data show that our explanations are both more accurate resp. moreeasily interpretable than those of SHAP and NSHAP.</description><author>Sascha Xu, Joscha Cüppers, Jilles Vreeken</author><pubDate>Fri, 19 Apr 2024 14:47:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05566v2</guid></item><item><title>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</title><link>http://arxiv.org/abs/2404.12888v1</link><description>Speech-driven facial animation methods usually contain two main classes, 3Dand 2D talking face, both of which attract considerable research attention inrecent years. However, to the best of our knowledge, the research on 3D talkingface does not go deeper as 2D talking face, in the aspect oflip-synchronization (lip-sync) and speech perception. To mind the gap betweenthe two sub-fields, we propose a learning framework named Learn2Talk, which canconstruct a better 3D talking face network by exploiting two expertise pointsfrom the field of 2D talking face. Firstly, inspired by the audio-video syncnetwork, a 3D sync-lip expert model is devised for the pursuit of lip-syncbetween audio and 3D facial motion. Secondly, a teacher model selected from 2Dtalking face methods is used to guide the training of the audio-to-3D motionsregression network to yield more 3D vertex accuracy. Extensive experiments showthe advantages of the proposed framework in terms of lip-sync, vertex accuracyand speech perception, compared with state-of-the-arts. Finally, we show twoapplications of the proposed framework: audio-visual speech recognition andspeech-driven 3D Gaussian Splatting based avatar animation.</description><author>Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</author><pubDate>Fri, 19 Apr 2024 14:45:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12888v1</guid></item><item><title>3D Multi-frame Fusion for Video Stabilization</title><link>http://arxiv.org/abs/2404.12887v1</link><description>In this paper, we present RStab, a novel framework for video stabilizationthat integrates 3D multi-frame fusion through volume rendering. Departing fromconventional methods, we introduce a 3D multi-frame perspective to generatestabilized images, addressing the challenge of full-frame generation whilepreserving structure. The core of our approach lies in Stabilized Rendering(SR), a volume rendering module, which extends beyond the image fusion byincorporating feature fusion. The core of our RStab framework lies inStabilized Rendering (SR), a volume rendering module, fusing multi-frameinformation in 3D space. Specifically, SR involves warping features and colorsfrom multiple frames by projection, fusing them into descriptors to render thestabilized image. However, the precision of warped information depends on theprojection accuracy, a factor significantly influenced by dynamic regions. Inresponse, we introduce the Adaptive Ray Range (ARR) module to integrate depthpriors, adaptively defining the sampling range for the projection process.Additionally, we propose Color Correction (CC) assisting geometric constraintswith optical flow for accurate color aggregation. Thanks to the three modules,our RStab demonstrates superior performance compared with previous stabilizersin the field of view (FOV), image quality, and video stability across variousdatasets.</description><author>Zhan Peng, Xinyi Ye, Weiyue Zhao, Tianqi Liu, Huiqiang Sun, Baopu Li, Zhiguo Cao</author><pubDate>Fri, 19 Apr 2024 14:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12887v1</guid></item><item><title>MCM: Multi-condition Motion Synthesis Framework</title><link>http://arxiv.org/abs/2404.12886v1</link><description>Conditional human motion synthesis (HMS) aims to generate human motionsequences that conform to specific conditions. Text and audio represent the twopredominant modalities employed as HMS control conditions. While existingresearch has primarily focused on single conditions, the multi-condition humanmotion synthesis remains underexplored. In this study, we propose amulti-condition HMS framework, termed MCM, based on a dual-branch structurecomposed of a main branch and a control branch. This framework effectivelyextends the applicability of the diffusion model, which is initially predicatedsolely on textual conditions, to auditory conditions. This extensionencompasses both music-to-dance and co-speech HMS while preserving theintrinsic quality of motion and the capabilities for semantic associationinherent in the original model. Furthermore, we propose the implementation of aTransformer-based diffusion model, designated as MWNet, as the main branch.This model adeptly apprehends the spatial intricacies and inter-jointcorrelations inherent in motion sequences, facilitated by the integration ofmulti-wise self-attention modules. Extensive experiments show that our methodachieves competitive results in single-condition and multi-condition HMS tasks.</description><author>Zeyu Ling, Bo Han, Yongkang Wongkan, Han Lin, Mohan Kankanhalli, Weidong Geng</author><pubDate>Fri, 19 Apr 2024 14:40:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12886v1</guid></item><item><title>Conditional Diffusion Models for Semantic 3D Brain MRI Synthesis</title><link>http://arxiv.org/abs/2305.18453v5</link><description>Artificial intelligence (AI) in healthcare, especially in medical imaging,faces challenges due to data scarcity and privacy concerns. Addressing these,we introduce Med-DDPM, a diffusion model designed for 3D semantic brain MRIsynthesis. This model effectively tackles data scarcity and privacy issues byintegrating semantic conditioning. This involves the channel-wise concatenationof a conditioning image to the model input, enabling control in imagegeneration. Med-DDPM demonstrates superior stability and performance comparedto existing 3D brain imaging synthesis methods. It generates diverse,anatomically coherent images with high visual fidelity. In terms of dice scoreaccuracy in the tumor segmentation task, Med-DDPM achieves 0.6207, close to the0.6531 accuracy of real images, and outperforms baseline models. Combined withreal images, it further increases segmentation accuracy to 0.6675, showing thepotential of our proposed method for data augmentation. This model representsthe first use of a diffusion model in 3D semantic brain MRI synthesis,producing high-quality images. Its semantic conditioning feature also showspotential for image anonymization in biomedical imaging, addressing data andprivacy issues. We provide the code and model weights for Med-DDPM on ourGitHub repository (https://github.com/mobaidoctor/med-ddpm/) to supportreproducibility.</description><author>Zolnamar Dorjsembe, Hsing-Kuo Pao, Sodtavilan Odonchimed, Furen Xiao</author><pubDate>Fri, 19 Apr 2024 14:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18453v5</guid></item><item><title>Modeling Hierarchical Structural Distance for Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2211.11424v2</link><description>Unsupervised domain adaptation (UDA) aims to estimate a transferable modelfor unlabeled target domains by exploiting labeled source data. OptimalTransport (OT) based methods have recently been proven to be a promisingsolution for UDA with a solid theoretical foundation and competitiveperformance. However, most of these methods solely focus on domain-level OTalignment by leveraging the geometry of domains for domain-invariant featuresbased on the global embeddings of images. However, global representations ofimages may destroy image structure, leading to the loss of local details thatoffer category-discriminative information. This study proposes an end-to-endDeep Hierarchical Optimal Transport method (DeepHOT), which aims to learn bothdomain-invariant and category-discriminative representations by mininghierarchical structural relations among domains. The main idea is toincorporate a domain-level OT and image-level OT into a unified OT framework,hierarchical optimal transport, to model the underlying geometry in both domainspace and image space. In DeepHOT framework, an image-level OT serves as theground distance metric for the domain-level OT, leading to the hierarchicalstructural distance. Compared with the ground distance of the conventionaldomain-level OT, the image-level OT captures structural associations amonglocal regions of images that are beneficial to classification. In this way,DeepHOT, a unified OT framework, not only aligns domains by domain-level OT,but also enhances the discriminative power through image-level OT. Moreover, toovercome the limitation of high computational complexity, we propose a robustand efficient implementation of DeepHOT by approximating origin OT with slicedWasserstein distance in image-level OT and accomplishing the mini-batchunbalanced domain-level OT.</description><author>Yingxue Xu, Guihua Wen, Yang Hu, Pei Yang</author><pubDate>Fri, 19 Apr 2024 14:31:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11424v2</guid></item><item><title>Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2404.12879v1</link><description>While Retrieval-Augmented Generation (RAG) plays a crucial role in theapplication of Large Language Models (LLMs), existing retrieval methods inknowledge-dense domains like law and medicine still suffer from a lack ofmulti-perspective views, which are essential for improving interpretability andreliability. Previous research on multi-view retrieval often focused solely ondifferent semantic forms of queries, neglecting the expression of specificdomain knowledge perspectives. This paper introduces a novel multi-view RAGframework, MVRAG, tailored for knowledge-dense domains that utilizesintention-aware query rewriting from multiple domain viewpoints to enhanceretrieval precision, thereby improving the effectiveness of the finalinference. Experiments conducted on legal and medical case retrievaldemonstrate significant improvements in recall and precision rates with ourframework. Our multi-perspective retrieval approach unleashes the potential ofmulti-view information enhancing RAG tasks, accelerating the furtherapplication of LLMs in knowledge-intensive fields.</description><author>Guanhua Chen, Wenhan Yu, Lei Sha</author><pubDate>Fri, 19 Apr 2024 14:27:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12879v1</guid></item><item><title>A Large-scale Medical Visual Task Adaptation Benchmark</title><link>http://arxiv.org/abs/2404.12876v1</link><description>Visual task adaptation has been demonstrated to be effective in adaptingpre-trained Vision Transformers (ViTs) to general downstream visual tasks usingspecialized learnable layers or tokens. However, there is yet a large-scalebenchmark to fully explore the effect of visual task adaptation on therealistic and important medical domain, particularly across diverse medicalvisual modalities, such as color images, X-ray, and CT. To close this gap, wepresent Med-VTAB, a large-scale Medical Visual Task Adaptation Benchmarkconsisting of 1.68 million medical images for diverse organs, modalities, andadaptation approaches. Based on Med-VTAB, we explore the scaling law of medicalprompt tuning concerning tunable parameters and the generalizability of medicalvisual adaptation using non-medical/medical pre-train weights. Besides, westudy the impact of patient ID out-of-distribution on medical visualadaptation, which is a real and challenging scenario. Furthermore, results fromMed-VTAB indicate that a single pre-trained model falls short in medical taskadaptation. Therefore, we introduce GMoE-Adapter, a novel method that combinesmedical and general pre-training weights through a gated mixture-of-expertsadapter, achieving state-of-the-art results in medical visual task adaptation.</description><author>Shentong Mo, Xufang Luo, Yansen Wang, Dongsheng Li</author><pubDate>Fri, 19 Apr 2024 14:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12876v1</guid></item><item><title>Interactive Question Answering Systems: Literature Review</title><link>http://arxiv.org/abs/2209.01621v3</link><description>Question answering systems are recognized as popular and frequently effectivemeans of information seeking on the web. In such systems, information seekerscan receive a concise response to their query by presenting their questions innatural language. Interactive question answering is a recently proposed andincreasingly popular solution that resides at the intersection of questionanswering and dialogue systems. On the one hand, the user can ask questions innormal language and locate the actual response to her inquiry; on the otherhand, the system can prolong the question-answering session into a dialogue ifthere are multiple probable replies, very few, or ambiguities in the initialrequest. By permitting the user to ask more questions, interactive questionanswering enables users to dynamically interact with the system and receivemore precise results. This survey offers a detailed overview of the interactivequestion-answering methods that are prevalent in current literature. It beginsby explaining the foundational principles of question-answering systems, hencedefining new notations and taxonomies to combine all identified works inside aunified framework. The reviewed published work on interactivequestion-answering systems is then presented and examined in terms of itsproposed methodology, evaluation approaches, and dataset/application domain. Wealso describe trends surrounding specific tasks and issues raised by thecommunity, so shedding light on the future interests of scholars. Our work isfurther supported by a GitHub page with a synthesis of all the major topicscovered in this literature study.https://sisinflab.github.io/interactive-question-answering-systems-survey/</description><author>Giovanni Maria Biancofiore, Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, Fedelucio Narducci</author><pubDate>Fri, 19 Apr 2024 14:21:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01621v3</guid></item><item><title>MultiModal-Learning for Predicting Molecular Properties: A Framework Based on Image and Graph Structures</title><link>http://arxiv.org/abs/2311.16666v2</link><description>The quest for accurate prediction of drug molecule properties poses afundamental challenge in the realm of Artificial Intelligence Drug Discovery(AIDD). An effective representation of drug molecules emerges as a pivotalcomponent in this pursuit. Contemporary leading-edge research predominantlyresorts to self-supervised learning (SSL) techniques to extract meaningfulstructural representations from large-scale, unlabeled molecular data,subsequently fine-tuning these representations for an array of downstreamtasks. However, an inherent shortcoming of these studies lies in their singularreliance on one modality of molecular information, such as molecule image orSMILES representations, thus neglecting the potential complementarity ofvarious molecular modalities. In response to this limitation, we propose MolIG,a novel MultiModaL molecular pre-training framework for predicting molecularproperties based on Image and Graph structures. MolIG model innovativelyleverages the coherence and correlation between molecule graph and moleculeimage to execute self-supervised tasks, effectively amalgamating the strengthsof both molecular representation forms. This holistic approach allows for thecapture of pivotal molecular structural characteristics and high-level semanticinformation. Upon completion of pre-training, Graph Neural Network (GNN)Encoder is used for the prediction of downstream tasks. In comparison toadvanced baseline models, MolIG exhibits enhanced performance in downstreamtasks pertaining to molecular property prediction within benchmark groups suchas MoleculeNet Benchmark Group and ADMET Benchmark Group.</description><author>Zhuoyuan Wang, Jiacong Mi, Shan Lu, Jieyue He</author><pubDate>Fri, 19 Apr 2024 14:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16666v2</guid></item><item><title>Monocular 3D lane detection for Autonomous Driving: Recent Achievements, Challenges, and Outlooks</title><link>http://arxiv.org/abs/2404.06860v2</link><description>3D lane detection is essential in autonomous driving as it extractsstructural and traffic information from the road in three-dimensional space,aiding self-driving cars in logical, safe, and comfortable path planning andmotion control. Given the cost of sensors and the advantages of visual data incolor information, 3D lane detection based on monocular vision is an importantresearch direction in the realm of autonomous driving, increasingly gainingattention in both industry and academia. Regrettably, recent advancements invisual perception seem inadequate for the development of fully reliable 3D lanedetection algorithms, which also hampers the progress of vision-based fullyautonomous vehicles. We believe that there is still considerable room forimprovement in 3D lane detection algorithms for autonomous vehicles usingvisual sensors, and significant enhancements are needed. This review looks backand analyzes the current state of achievements in the field of 3D lanedetection research. It covers all current monocular-based 3D lane detectionprocesses, discusses the performance of these cutting-edge algorithms, analyzesthe time complexity of various algorithms, and highlights the main achievementsand limitations of ongoing research efforts. The survey also includes acomprehensive discussion of available 3D lane detection datasets and thechallenges that researchers face but have not yet resolved. Finally, our workoutlines future research directions and invites researchers and practitionersto join this exciting field.</description><author>Fulong Ma, Weiqing Qi, Guoyang Zhao, Linwei Zheng, Sheng Wang, Yuxuan Liu, Ming Liu</author><pubDate>Fri, 19 Apr 2024 14:18:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06860v2</guid></item><item><title>LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency</title><link>http://arxiv.org/abs/2404.12872v1</link><description>Query rewrite, which aims to generate more efficient queries by altering aSQL query's structure without changing the query result, has been an importantresearch problem. In order to maintain equivalence between the rewritten queryand the original one during rewriting, traditional query rewrite methods alwaysrewrite the queries following certain rewrite rules. However, some problemsstill remain. Firstly, existing methods of finding the optimal choice orsequence of rewrite rules are still limited and the process always costs a lotof resources. Methods involving discovering new rewrite rules typically requirecomplicated proofs of structural logic or extensive user interactions.Secondly, current query rewrite methods usually rely highly on DBMS costestimators which are often not accurate. In this paper, we address theseproblems by proposing a novel method of query rewrite named LLM-R2, adopting alarge language model (LLM) to propose possible rewrite rules for a databaserewrite system. To further improve the inference ability of LLM in recommendingrewrite rules, we train a contrastive model by curriculum to learn queryrepresentations and select effective query demonstrations for the LLM.Experimental results have shown that our method can significantly improve thequery execution efficiency and outperform the baseline methods. In addition,our method enjoys high robustness across different datasets.</description><author>Zhaodonghui Li, Haitao Yuan, Huiming Wang, Gao Cong, Lidong Bing</author><pubDate>Fri, 19 Apr 2024 14:17:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12872v1</guid></item><item><title>FedGiA: An Efficient Hybrid Algorithm for Federated Learning</title><link>http://arxiv.org/abs/2205.01438v6</link><description>Federated learning has shown its advances recently but is still facing manychallenges, such as how algorithms save communication resources and reducecomputational costs, and whether they converge. To address these criticalissues, we propose a hybrid federated learning algorithm (FedGiA) that combinesthe gradient descent and the inexact alternating direction method ofmultipliers. The proposed algorithm is more communication- andcomputation-efficient than several state-of-the-art algorithms theoreticallyand numerically. Moreover, it also converges globally under mild conditions.</description><author>Shenglong Zhou, Geoffrey Ye Li</author><pubDate>Fri, 19 Apr 2024 14:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.01438v6</guid></item><item><title>FipTR: A Simple yet Effective Transformer Framework for Future Instance Prediction in Autonomous Driving</title><link>http://arxiv.org/abs/2404.12867v1</link><description>The future instance prediction from a Bird's Eye View(BEV) perspective is avital component in autonomous driving, which involves future instancesegmentation and instance motion prediction. Existing methods usually rely on aredundant and complex pipeline which requires multiple auxiliary outputs andpost-processing procedures. Moreover, estimated errors on each of the auxiliarypredictions will lead to degradation of the prediction performance. In thispaper, we propose a simple yet effective fully end-to-end framework namedFuture Instance Prediction Transformer(FipTR), which views the task as BEVinstance segmentation and prediction for future frames. We propose to adoptinstance queries representing specific traffic participants to directlyestimate the corresponding future occupied masks, and thus get rid of complexpost-processing procedures. Besides, we devise a flow-aware BEV predictor forfuture BEV feature prediction composed of a flow-aware deformable attentionthat takes backward flow guiding the offset sampling. A novel future instancematching strategy is also proposed to further improve the temporal coherence.Extensive experiments demonstrate the superiority of FipTR and itseffectiveness under different temporal BEV encoders.</description><author>Xingtai Gui, Tengteng Huang, Haonan Shao, Haotian Yao, Chi Zhang</author><pubDate>Fri, 19 Apr 2024 14:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12867v1</guid></item><item><title>MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks via Text Prompts</title><link>http://arxiv.org/abs/2401.11403v2</link><description>Deep learning is now widely used in drug discovery, providing significantacceleration and cost reduction. As the most fundamental building block,molecular representation is essential for predicting molecular properties toenable various downstream applications. Most existing methods attempt toincorporate more information to learn better representations. However, not allfeatures are equally important for a specific task. Ignoring this wouldpotentially compromise the training efficiency and predictive accuracy. Toaddress this issue, we propose a novel approach, which treats language modelsas an agent and molecular pretraining models as a knowledge base. The agentaccentuates task-relevant features in the molecular representation byunderstanding the natural language description of the task, just as a tailorcustomizes clothes for clients. Thus, we call this approach MolTailor.Evaluations demonstrate MolTailor's superior performance over baselines,validating the efficacy of enhancing relevance for molecular representationlearning. This illustrates the potential of language model guided optimizationto better exploit and unleash the capabilities of existing powerful molecularrepresentation methods. Our code is available athttps://github.com/SCIR-HI/MolTailor.</description><author>Haoqiang Guo, Sendong Zhao, Haochun Wang, Yanrui Du, Bing Qin</author><pubDate>Fri, 19 Apr 2024 14:07:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11403v2</guid></item><item><title>How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?</title><link>http://arxiv.org/abs/2404.12866v1</link><description>The increase in parameter size of multimodal large language models (MLLMs)introduces significant capabilities, particularly in-context learning, whereMLLMs enhance task performance without updating pre-trained parameters. Thiseffectiveness, however, hinges on the appropriate selection of in-contextexamples, a process that is currently biased towards visual data, overlookingtextual information. Furthermore, the area of supervised retrievers for MLLMs,crucial for optimal in-context example selection, continues to beuninvestigated. Our study offers an in-depth evaluation of the impact oftextual information on the unsupervised selection of in-context examples inmultimodal contexts, uncovering a notable sensitivity of retriever performanceto the employed modalities. Responding to this, we introduce a novel supervisedMLLM-retriever MSIER that employs a neural network to select examples thatenhance multimodal in-context learning efficiency. This approach is validatedthrough extensive testing across three distinct tasks, demonstrating themethod's effectiveness. Additionally, we investigate the influence ofmodalities on our supervised retrieval method's training and pinpoint factorscontributing to our model's success. This exploration paves the way for futureadvancements, highlighting the potential for refined in-context learning inMLLMs through the strategic use of multimodal data.</description><author>Yang Luo, Zangwei Zheng, Zirui Zhu, Yang You</author><pubDate>Fri, 19 Apr 2024 14:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12866v1</guid></item><item><title>A Guide to Feature Importance Methods for Scientific Inference</title><link>http://arxiv.org/abs/2404.12862v1</link><description>While machine learning (ML) models are increasingly used due to their highpredictive power, their use in understanding the data-generating process (DGP)is limited. Understanding the DGP requires insights into feature-targetassociations, which many ML models cannot directly provide, due to their opaqueinternal mechanisms. Feature importance (FI) methods provide useful insightsinto the DGP under certain conditions. Since the results of different FImethods have different interpretations, selecting the correct FI method for aconcrete use case is crucial and still requires expert knowledge. This paperserves as a comprehensive guide to help understand the differentinterpretations of FI methods. Through an extensive review of FI methods andproviding new proofs regarding their interpretation, we facilitate a thoroughunderstanding of these methods and formulate concrete recommendations forscientific inference. We conclude by discussing options for FI uncertaintyestimation and point to directions for future research aiming at fullstatistical inference from black-box ML models.</description><author>Fiona Katharina Ewald, Ludwig Bothmann, Marvin N. Wright, Bernd Bischl, Giuseppe Casalicchio, Gunnar König</author><pubDate>Fri, 19 Apr 2024 14:01:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12862v1</guid></item><item><title>Foundation Model assisted Weakly Supervised LiDAR Semantic Segmentation</title><link>http://arxiv.org/abs/2404.12861v1</link><description>Current point cloud semantic segmentation has achieved great advances whengiven sufficient labels. However, the dense annotation of LiDAR point cloudsremains prohibitively expensive and time-consuming, unable to keep up with thecontinuously growing volume of data. In this paper, we propose annotatingimages with scattered points, followed by utilizing SAM (a Foundation model) togenerate semantic segmentation labels for the images. Finally, by mapping thesegmentation labels of the images to the LiDAR space using the intrinsic andextrinsic parameters of the camera and LiDAR, we obtain labels for point cloudsemantic segmentation, and release Scatter-KITTI and Scatter-nuScenes, whichare the first works to utilize image segmentation-based SAM for weaklysupervised point cloud semantic segmentation. Furthermore, to mitigate theinfluence of erroneous pseudo labels obtained from sparse annotations on pointcloud features, we propose a multi-modal weakly supervised network for LiDARsemantic segmentation, called MM-ScatterNet. This network combines featuresfrom both point cloud and image modalities, enhancing the representationlearning of point clouds by introducing consistency constraints betweenmulti-modal features and point cloud features. On the SemanticKITTI dataset, weachieve 66\% of fully supervised performance using only 0.02% of annotateddata, and on the NuScenes dataset, we achieve 95% of fully supervisedperformance using only 0.1% labeled points.</description><author>Yilong Chen, Zongyi Xu, xiaoshui Huang, Ruicheng Zhang, Xinqi Jiang, Xinbo Gao</author><pubDate>Fri, 19 Apr 2024 14:01:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12861v1</guid></item><item><title>Language-Driven Active Learning for Diverse Open-Set 3D Object Detection</title><link>http://arxiv.org/abs/2404.12856v1</link><description>Object detection is crucial for ensuring safe autonomous driving. However,data-driven approaches face challenges when encountering minority or novelobjects in the 3D driving scene. In this paper, we propose VisLED, alanguage-driven active learning framework for diverse open-set 3D ObjectDetection. Our method leverages active learning techniques to query diverse andinformative data samples from an unlabeled pool, enhancing the model's abilityto detect underrepresented or novel objects. Specifically, we introduce theVision-Language Embedding Diversity Querying (VisLED-Querying) algorithm, whichoperates in both open-world exploring and closed-world mining settings. Inopen-world exploring, VisLED-Querying selects data points most novel relativeto existing data, while in closed-world mining, it mines new instances of knownclasses. We evaluate our approach on the nuScenes dataset and demonstrate itseffectiveness compared to random sampling and entropy-querying methods. Ourresults show that VisLED-Querying consistently outperforms random sampling andoffers competitive performance compared to entropy-querying despite thelatter's model-optimality, highlighting the potential of VisLED for improvingobject detection in autonomous driving scenarios.</description><author>Ross Greer, Bjørk Antoniussen, Andreas Møgelmose, Mohan Trivedi</author><pubDate>Fri, 19 Apr 2024 13:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12856v1</guid></item><item><title>Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions</title><link>http://arxiv.org/abs/2403.12007v3</link><description>Digital Behavior Change Interventions (DBCIs) are supporting development ofnew health behaviors. Evaluating their effectiveness is crucial for theirimprovement and understanding of success factors. However, comprehensiveguidance for developers, particularly in small-scale studies with ethicalconstraints, is limited. Building on the CAPABLE project, this study aims todefine effective engagement with DBCIs for supporting cancer patients inenhancing their quality of life. We identify metrics for measuring engagement,explore the interest of both patients and clinicians in DBCIs, and proposehypotheses for assessing the impact of DBCIs in such contexts. Our findingssuggest that clinician prescriptions significantly increase sustainedengagement with mobile DBCIs. In addition, while one weekly engagement with aDBCI is sufficient to maintain well-being, transitioning from extrinsic tointrinsic motivation may require a higher level of engagement.</description><author>Aneta Lisowska, Szymon Wilk, Laura Locati, Mimma Rizzo, Lucia Sacchi, Silvana Quaglini, Matteo Terzaghi, Valentina Tibollo, Mor Peleg</author><pubDate>Fri, 19 Apr 2024 13:47:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12007v3</guid></item><item><title>Factorized Motion Fields for Fast Sparse Input Dynamic View Synthesis</title><link>http://arxiv.org/abs/2404.11669v2</link><description>Designing a 3D representation of a dynamic scene for fast optimization andrendering is a challenging task. While recent explicit representations enablefast learning and rendering of dynamic radiance fields, they require a denseset of input viewpoints. In this work, we focus on learning a fastrepresentation for dynamic radiance fields with sparse input viewpoints.However, the optimization with sparse input is under-constrained andnecessitates the use of motion priors to constrain the learning. Existing fastdynamic scene models do not explicitly model the motion, making them difficultto be constrained with motion priors. We design an explicit motion model as afactorized 4D representation that is fast and can exploit the spatio-temporalcorrelation of the motion field. We then introduce reliable flow priorsincluding a combination of sparse flow priors across cameras and dense flowpriors within cameras to regularize our motion model. Our model is fast,compact and achieves very good performance on popular multi-view dynamic scenedatasets with sparse input viewpoints. The source code for our model can befound on our project page:https://nagabhushansn95.github.io/publications/2024/RF-DeRF.html.</description><author>Nagabhushan Somraj, Kapil Choudhary, Sai Harsha Mupparaju, Rajiv Soundararajan</author><pubDate>Fri, 19 Apr 2024 13:46:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11669v2</guid></item><item><title>LSP Framework: A Compensatory Model for Defeating Trigger Reverse Engineering via Label Smoothing Poisoning</title><link>http://arxiv.org/abs/2404.12852v1</link><description>Deep neural networks are vulnerable to backdoor attacks. Among the existingbackdoor defense methods, trigger reverse engineering based approaches, whichreconstruct the backdoor triggers via optimizations, are the most versatile andeffective ones compared to other types of methods. In this paper, we summarizeand construct a generic paradigm for the typical trigger reverse engineeringprocess. Based on this paradigm, we propose a new perspective to defeat triggerreverse engineering by manipulating the classification confidence of backdoorsamples. To determine the specific modifications of classification confidence,we propose a compensatory model to compute the lower bound of the modification.With proper modifications, the backdoor attack can easily bypass the triggerreverse engineering based methods. To achieve this objective, we propose aLabel Smoothing Poisoning (LSP) framework, which leverages label smoothing tospecifically manipulate the classification confidences of backdoor samples.Extensive experiments demonstrate that the proposed work can defeat thestate-of-the-art trigger reverse engineering based methods, and possess goodcompatibility with a variety of existing backdoor attacks.</description><author>Beichen Li, Yuanfang Guo, Heqi Peng, Yangxi Li, Yunhong Wang</author><pubDate>Fri, 19 Apr 2024 13:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12852v1</guid></item><item><title>Differentiating Viral and Bacterial Infections: A Machine Learning Model Based on Routine Blood Test Values</title><link>http://arxiv.org/abs/2305.07877v2</link><description>The growing threat of antibiotic resistance necessitates accuratedifferentiation between bacterial and viral infections for proper antibioticadministration. In this study, a Virus vs. Bacteria machine learning model wasdeveloped to distinguish between these infection types using 16 routine bloodtest results, C-reactive protein concentration (CRP), biological sex, and age.With a dataset of 44,120 cases from a single medical center, the model achievedan accuracy of 82.2 %, a sensitivity of 79.7 %, a specificity of 84.5 %, aBrier score of 0.129, and an area under the ROC curve (AUC) of 0.905,outperforming a CRP-based decision rule. Notably, the machine learning modelenhanced accuracy within the CRP range of 10-40 mg/L, a range where CRP aloneis less informative. These results highlight the advantage of integratingmultiple blood parameters in diagnostics. The "Virus vs. Bacteria" model pavesthe way for advanced diagnostic tools, leveraging machine learning to optimizeinfection management.</description><author>Gregor Gunčar, Matjaž Kukar, Tim Smole, Sašo Moškon, Tomaž Vovko, Simon Podnar, Peter Černelč, Miran Brvar, Mateja Notar, Manca Köster, Marjeta Tušek Jelenc, Marko Notar</author><pubDate>Fri, 19 Apr 2024 13:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07877v2</guid></item></channel></rss>