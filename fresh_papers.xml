<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 20 Sep 2023 06:00:18 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SlimPajama-DC: Understanding Data Combinations for LLM Training</title><link>http://arxiv.org/abs/2309.10818v1</link><description>This paper aims to understand the impacts of various data combinations (e.g.,web text, wikipedia, github, books) on the training of large language modelsusing SlimPajama. SlimPajama is a rigorously deduplicated, multi-sourcedataset, which has been refined and further deduplicated to 627B tokens fromthe extensive 1.2T tokens RedPajama dataset contributed by Together. We'vetermed our research as SlimPajama-DC, an empirical analysis designed to uncoverfundamental characteristics and best practices associated with employingSlimPajama in the training of large language models. During our research withSlimPajama, two pivotal observations emerged: (1) Global deduplication vs.local deduplication. We analyze and discuss how global (across differentsources of datasets) and local (within the single source of dataset)deduplications affect the performance of trained models. (2) Proportions ofhigh-quality/highly-deduplicated multi-source datasets in the combination. Tostudy this, we construct six configurations of SlimPajama dataset and trainindividual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our bestconfiguration outperforms the 1.3B model trained on RedPajama using the samenumber of training tokens by a significant margin. All our 1.3B models aretrained on Cerebras 16$\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16mixed precision. We further extend our discoveries (such as increasing datadiversity is crucial after global deduplication) on a 7B model with largebatch-size training. Our models and the separate SlimPajama-DC datasets areavailable at: https://huggingface.co/MBZUAI-LLM andhttps://huggingface.co/datasets/cerebras/SlimPajama-627B.</description><author>Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, Eric Xing</author><pubDate>Tue, 19 Sep 2023 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10818v1</guid></item><item><title>Assessing the capacity of a denoising diffusion probabilistic model to reproduce spatial context</title><link>http://arxiv.org/abs/2309.10817v1</link><description>Diffusion models have emerged as a popular family of deep generative models(DGMs). In the literature, it has been claimed that one class of diffusionmodels -- denoising diffusion probabilistic models (DDPMs) -- demonstratesuperior image synthesis performance as compared to generative adversarialnetworks (GANs). To date, these claims have been evaluated using eitherensemble-based methods designed for natural images, or conventional measures ofimage quality such as structural similarity. However, there remains animportant need to understand the extent to which DDPMs can reliably learnmedical imaging domain-relevant information, which is referred to as `spatialcontext' in this work. To address this, a systematic assessment of the abilityof DDPMs to learn spatial context relevant to medical imaging applications isreported for the first time. A key aspect of the studies is the use ofstochastic context models (SCMs) to produce training data. In this way, theability of the DDPMs to reliably reproduce spatial context can bequantitatively assessed by use of post-hoc image analyses. Error-rates inDDPM-generated ensembles are reported, and compared to those corresponding to amodern GAN. The studies reveal new and important insights regarding thecapacity of DDPMs to learn spatial context. Notably, the results demonstratethat DDPMs hold significant capacity for generating contextually correct imagesthat are `interpolated' between training samples, which may benefitdata-augmentation tasks in ways that GANs cannot.</description><author>Rucha Deshpande, Muzaffer Özbey, Hua Li, Mark A. Anastasio, Frank J. Brooks</author><pubDate>Tue, 19 Sep 2023 18:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10817v1</guid></item><item><title>PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes</title><link>http://arxiv.org/abs/2309.10815v1</link><description>Training perception systems for self-driving cars requires substantialannotations. However, manual labeling in 2D images is highly labor-intensive.While existing datasets provide rich annotations for pre-recorded sequences,they fall short in labeling rarely encountered viewpoints, potentiallyhampering the generalization ability for perception models. In this paper, wepresent PanopticNeRF-360, a novel approach that combines coarse 3D annotationswith noisy 2D semantic cues to generate consistent panoptic labels andhigh-quality images from any viewpoint. Our key insight lies in exploiting thecomplementarity of 3D and 2D priors to mutually enhance geometry and semantics.Specifically, we propose to leverage noisy semantic and instance labels in both3D and 2D spaces to guide geometry optimization. Simultaneously, the improvedgeometry assists in filtering noise present in the 3D and 2D annotations bymerging them in 3D space via a learned semantic field. To further enhanceappearance, we combine MLP and hash grids to yield hybrid scene features,striking a balance between high-frequency appearance and predominantlycontiguous semantics. Our experiments demonstrate PanopticNeRF-360'sstate-of-the-art performance over existing label transfer methods on thechallenging urban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360enables omnidirectional rendering of high-fidelity, multi-view andspatiotemporally consistent appearance, semantic and instance labels. We makeour code and data available at https://github.com/fuxiao0719/PanopticNeRF</description><author>Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu, Xiaowei Zhou, Andreas Geiger, Yiyi Liao</author><pubDate>Tue, 19 Sep 2023 18:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10815v1</guid></item><item><title>Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning</title><link>http://arxiv.org/abs/2309.10814v1</link><description>How can we perform computations over natural language representations tosolve tasks that require symbolic and numeric reasoning? We propose naturallanguage embedded programs (NLEP) as a unifying framework for addressingmath/symbolic reasoning, natural language understanding, and instructionfollowing tasks. Our approach prompts a language model to generate full Pythonprograms that define functions over data structures which contain naturallanguage representations of structured knowledge. A Python interpreter thenexecutes the generated code and prints the output. Despite using a task-generalprompt, we find that this approach can improve upon strong baselines across arange of different tasks including math and symbolic reasoning, textclassification, question answering, and instruction following. We further findthe generated programs are often interpretable and enable post-hoc verificationof the intermediate reasoning steps.</description><author>Tianhua Zhang, Jiaxin Ge, Hongyin Luo, Yung-Sung Chuang, Mingye Gao, Yuan Gong, Xixin Wu, Yoon Kim, Helen Meng, James Glass</author><pubDate>Tue, 19 Sep 2023 18:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10814v1</guid></item><item><title>Modeling interdisciplinary interactions among Physics, Mathematics &amp; Computer Science</title><link>http://arxiv.org/abs/2309.10811v1</link><description>Interdisciplinarity has over the recent years have gained tremendousimportance and has become one of the key ways of doing cutting edge research.In this paper we attempt to model the citation flow across three differentfields -- Physics (PHY), Mathematics (MA) and Computer Science (CS). Forinstance, is there a specific pattern in which these fields cite one another?We carry out experiments on a dataset comprising more than 1.2 million articlestaken from these three fields. We quantify the citation interactions amongthese three fields through temporal bucket signatures. We present numericalmodels based on variants of the recently proposed relay-linking framework toexplain the citation dynamics across the three disciplines. These models make amodest attempt to unfold the underlying principles of how citation links couldhave been formed across the three fields over time.</description><author>Rima Hazra, Mayank Singh, Pawan Goyal, Bibhas Adhikari, Animesh Mukherjee</author><pubDate>Tue, 19 Sep 2023 18:52:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10811v1</guid></item><item><title>Artificial Intelligence and Spontaneous Collusion</title><link>http://arxiv.org/abs/2202.05946v5</link><description>We develop a tractable model for studying strategic interactions betweenlearning algorithms. We uncover a mechanism responsible for the emergence ofalgorithmic collusion. We observe that algorithms periodically coordinate onactions that are more profitable than static Nash equilibria. This novelcollusive channel relies on an endogenous statistical linkage in thealgorithms' estimates which we call spontaneous coupling. The model'sparameters predict whether the statistical linkage will appear, and what marketstructures facilitate algorithmic collusion. We show that spontaneous couplingcan sustain collusion in prices and market shares, complementing experimentalfindings in the literature. Finally, we apply our results to design algorithmicmarkets.</description><author>Martino Banchio, Giacomo Mantegazza</author><pubDate>Tue, 19 Sep 2023 18:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05946v5</guid></item><item><title>PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance</title><link>http://arxiv.org/abs/2309.10810v1</link><description>Exploiting pre-trained diffusion models for restoration has recently become afavored alternative to the traditional task-specific training approach.Previous works have achieved noteworthy success by limiting the solution spaceusing explicit degradation models. However, these methods often fall short whenfaced with complex degradations as they generally cannot be precisely modeled.In this paper, we propose PGDiff by introducing partial guidance, a freshperspective that is more adaptable to real-world degradations compared toexisting works. Rather than specifically defining the degradation process, ourapproach models the desired properties, such as image structure and colorstatistics of high-quality images, and applies this guidance during the reversediffusion process. These properties are readily available and make noassumptions about the degradation process. When combined with a diffusionprior, this partial guidance can deliver appealing results across a range ofrestoration tasks. Additionally, PGDiff can be extended to handle compositetasks by consolidating multiple high-quality image properties, achieved byintegrating the guidance from respective tasks. Experimental resultsdemonstrate that our method not only outperforms existing diffusion-prior-basedapproaches but also competes favorably with task-specific models.</description><author>Peiqing Yang, Shangchen Zhou, Qingyi Tao, Chen Change Loy</author><pubDate>Tue, 19 Sep 2023 18:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10810v1</guid></item><item><title>Semantic Text Compression for Classification</title><link>http://arxiv.org/abs/2309.10809v1</link><description>We study semantic compression for text where meanings contained in the textare conveyed to a source decoder, e.g., for classification. The main motivatorto move to such an approach of recovering the meaning without requiring exactreconstruction is the potential resource savings, both in storage and inconveying the information to another node. Towards this end, we proposesemantic quantization and compression approaches for text where we utilizesentence embeddings and the semantic distortion metric to preserve the meaning.Our results demonstrate that the proposed semantic approaches result insubstantial (orders of magnitude) savings in the required number of bits formessage representation at the expense of very modest accuracy loss compared tothe semantic agnostic baseline. We compare the results of proposed approachesand observe that resource savings enabled by semantic quantization can befurther amplified by semantic clustering. Importantly, we observe thegeneralizability of the proposed methodology which produces excellent resultson many benchmark text classification datasets with a diverse array ofcontexts.</description><author>Emrecan Kutay, Aylin Yener</author><pubDate>Tue, 19 Sep 2023 18:50:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10809v1</guid></item><item><title>AI Foundation Models for Weather and Climate: Applications, Design, and Implementation</title><link>http://arxiv.org/abs/2309.10808v1</link><description>Machine learning and deep learning methods have been widely explored inunderstanding the chaotic behavior of the atmosphere and furthering weatherforecasting. There has been increasing interest from technology companies,government institutions, and meteorological agencies in building digital twinsof the Earth. Recent approaches using transformers, physics-informed machinelearning, and graph neural networks have demonstrated state-of-the-artperformance on relatively narrow spatiotemporal scales and specific tasks. Withthe recent success of generative artificial intelligence (AI) using pre-trainedtransformers for language modeling and vision with prompt engineering andfine-tuning, we are now moving towards generalizable AI. In particular, we arewitnessing the rise of AI foundation models that can perform competitively onmultiple domain-specific downstream tasks. Despite this progress, we are stillin the nascent stages of a generalizable AI model for global Earth systemmodels, regional climate models, and mesoscale weather models. Here, we reviewcurrent state-of-the-art AI approaches, primarily from transformer and operatorlearning literature in the context of meteorology. We provide our perspectiveon criteria for success towards a family of foundation models for nowcastingand forecasting weather and climate predictions. We also discuss how suchmodels can perform competitively on downstream tasks such as downscaling(super-resolution), identifying conditions conducive to the occurrence ofwildfires, and predicting consequential meteorological phenomena across variousspatiotemporal scales such as hurricanes and atmospheric rivers. In particular,we examine current AI methodologies and contend they have matured enough todesign and implement a weather foundation model.</description><author>S. Karthik Mukkavilli, Daniel Salles Civitarese, Johannes Schmude, Johannes Jakubik, Anne Jones, Nam Nguyen, Christopher Phillips, Sujit Roy, Shraddha Singh, Campbell Watson, Raghu Ganti, Hendrik Hamann, Udaysankar Nair, Rahul Ramachandran, Kommy Weldemariam</author><pubDate>Tue, 19 Sep 2023 18:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10808v1</guid></item><item><title>Homology-Preserving Multi-Scale Graph Skeletonization Using Mapper on Graphs</title><link>http://arxiv.org/abs/1804.11242v5</link><description>Node-link diagrams are a popular method for representing graphs that capturerelationships between individuals, businesses, proteins, and telecommunicationendpoints. However, node-link diagrams may fail to convey insights regardinggraph structures, even for moderately sized data of a few hundred nodes, due tovisual clutter. We propose to apply the mapper construction -- a popular toolin topological data analysis -- to graph visualization, which provides a strongtheoretical basis for summarizing the data while preserving their corestructures. We develop a variation of the mapper construction targetingweighted, undirected graphs, called {\mog}, which generates homology-preservingskeletons of graphs. We further show how the adjustment of a single parameterenables multi-scale skeletonization of the input graph. We provide a softwaretool that enables interactive explorations of such skeletons and demonstratethe effectiveness of our method for synthetic and real-world data.</description><author>Paul Rosen, Mustafa Hajij, Bei Wang</author><pubDate>Tue, 19 Sep 2023 18:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1804.11242v5</guid></item><item><title>Multi-Context Dual Hyper-Prior Neural Image Compression</title><link>http://arxiv.org/abs/2309.10799v1</link><description>Transform and entropy models are the two core components in deep imagecompression neural networks. Most existing learning-based image compressionmethods utilize convolutional-based transform, which lacks the ability to modellong-range dependencies, primarily due to the limited receptive field of theconvolution operation. To address this limitation, we propose aTransformer-based nonlinear transform. This transform has the remarkableability to efficiently capture both local and global information from the inputimage, leading to a more decorrelated latent representation. In addition, weintroduce a novel entropy model that incorporates two different hyperpriors tomodel cross-channel and spatial dependencies of the latent representation. Tofurther improve the entropy model, we add a global context that leveragesdistant relationships to predict the current latent more accurately. Thisglobal context employs a causal attention mechanism to extract long-rangeinformation in a content-dependent manner. Our experiments show that ourproposed framework performs better than the state-of-the-art methods in termsof rate-distortion performance.</description><author>Atefeh Khoshkhahtinat, Ali Zafari, Piyush M. Mehta, Mohammad Akyash, Hossein Kashiani, Nasser M. Nasrabadi</author><pubDate>Tue, 19 Sep 2023 18:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10799v1</guid></item><item><title>Heuristic Search for Path Finding with Refuelling</title><link>http://arxiv.org/abs/2309.10796v1</link><description>This paper considers a generalization of the Path Finding (PF) with refuelingconstraints referred to as the Refuelling Path Finding (RF-PF) problem. Justlike PF, the RF-PF problem is defined over a graph, where vertices are gasstations with known fuel prices, and edge costs depend on the gas consumptionbetween the corresponding vertices. RF-PF seeks a minimum-cost path from thestart to the goal vertex for a robot with a limited gas tank and a limitednumber of refuelling stops. While RF-PF is polynomial-time solvable, it remainsa challenge to quickly compute an optimal solution in practice since the robotneeds to simultaneously determine the path, where to make the stops, and theamount to refuel at each stop. This paper develops a heuristic search algorithmcalled Refuel A* (RF-A* ) that iteratively constructs partial solution pathsfrom the start to the goal guided by a heuristic function while leveragingdominance rules for state pruning during planning. RF-A* is guaranteed to findan optimal solution and runs more than an order of magnitude faster than theexisting state of the art (a polynomial time algorithm) when tested in largecity maps with hundreds of gas stations.</description><author>Anushtup Nandy, Zhongqiang Ren, Sivakumar Rathinam, Howie Choset</author><pubDate>Tue, 19 Sep 2023 18:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10796v1</guid></item><item><title>Multi-spectral Entropy Constrained Neural Compression of Solar Imagery</title><link>http://arxiv.org/abs/2309.10791v1</link><description>Missions studying the dynamic behaviour of the Sun are defined to capturemulti-spectral images of the sun and transmit them to the ground station in adaily basis. To make transmission efficient and feasible, image compressionsystems need to be exploited. Recently successful end-to-end optimized neuralnetwork-based image compression systems have shown great potential to be usedin an ad-hoc manner. In this work we have proposed a transformer-basedmulti-spectral neural image compressor to efficiently capture redundancies bothintra/inter-wavelength. To unleash the locality of window-based self attentionmechanism, we propose an inter-window aggregated token multi head selfattention. Additionally to make the neural compressor autoencoder shiftinvariant, a randomly shifted window attention mechanism is used which makesthe transformer blocks insensitive to translations in their input domain. Wedemonstrate that the proposed approach not only outperforms the conventionalcompression algorithms but also it is able to better decorrelates images alongthe multiple wavelengths compared to single spectral compression.</description><author>Ali Zafari, Atefeh Khoshkhahtinat, Piyush M. Mehta, Nasser M. Nasrabadi, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva</author><pubDate>Tue, 19 Sep 2023 18:40:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10791v1</guid></item><item><title>Guide Your Agent with Adaptive Multimodal Rewards</title><link>http://arxiv.org/abs/2309.10790v1</link><description>Developing an agent capable of adapting to unseen environments remains adifficult challenge in imitation learning. In this work, we present AdaptiveReturn-conditioned Policy (ARP), an efficient framework designed to enhance theagent's generalization ability using natural language task descriptions andpre-trained multimodal encoders. Our key idea is to calculate a similaritybetween visual observations and natural language instructions in thepre-trained multimodal embedding space (such as CLIP) and use it as a rewardsignal. We then train a return-conditioned policy using expert demonstrationslabeled with multimodal rewards. Because the multimodal rewards provideadaptive signals at each timestep, our ARP effectively mitigates the goalmisgeneralization. This results in superior generalization performances evenwhen faced with unseen text instructions, compared to existing text-conditionedpolicies. To improve the quality of rewards, we also introduce a fine-tuningmethod for pre-trained multimodal encoders, further enhancing the performance.Video demonstrations and source code are available on the project website:https://sites.google.com/view/2023arp.</description><author>Changyeon Kim, Younggyo Seo, Hao Liu, Lisa Lee, Jinwoo Shin, Honglak Lee, Kimin Lee</author><pubDate>Tue, 19 Sep 2023 18:39:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10790v1</guid></item><item><title>AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models</title><link>http://arxiv.org/abs/2309.10787v1</link><description>Audio-visual representation learning aims to develop systems with human-likeperception by utilizing correlation between auditory and visual information.However, current models often focus on a limited set of tasks, andgeneralization abilities of learned representations are unclear. To this end,we propose the AV-SUPERB benchmark that enables general-purpose evaluation ofunimodal audio/visual and bimodal fusion representations on 7 datasets covering5 audio-visual tasks in speech and audio processing. We evaluate 5 recentself-supervised models and show that none of these models generalize to alltasks, emphasizing the need for future study on improving universal modelperformance. In addition, we show that representations may be improved withintermediate-task fine-tuning and audio event classification with AudioSetserves as a strong intermediate task. We release our benchmark with evaluationcode and a model submission platform to encourage further research inaudio-visual learning.</description><author>Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee</author><pubDate>Tue, 19 Sep 2023 18:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10787v1</guid></item><item><title>Context-Aware Neural Video Compression on Solar Dynamics Observatory</title><link>http://arxiv.org/abs/2309.10784v1</link><description>NASA's Solar Dynamics Observatory (SDO) mission collects large data volumesof the Sun's daily activity. Data compression is crucial for space missions toreduce data storage and video bandwidth requirements by eliminatingredundancies in the data. In this paper, we present a novel neuralTransformer-based video compression approach specifically designed for the SDOimages. Our primary objective is to efficiently exploit the temporal andspatial redundancies inherent in solar images to obtain a high compressionratio. Our proposed architecture benefits from a novel Transformer block calledFused Local-aware Window (FLaWin), which incorporates window-basedself-attention modules and an efficient fused local-aware feed-forward (FLaFF)network. This architectural design allows us to simultaneously captureshort-range and long-range information while facilitating the extraction ofrich and diverse contextual representations. Moreover, this design choiceresults in reduced computational complexity. Experimental results demonstratethe significant contribution of the FLaWin Transformer block to the compressionperformance, outperforming conventional hand-engineered video codecs such asH.264 and H.265 in terms of rate-distortion trade-off.</description><author>Atefeh Khoshkhahtinat, Ali Zafari, Piyush M. Mehta, Nasser M. Nasrabadi, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva</author><pubDate>Tue, 19 Sep 2023 18:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10784v1</guid></item><item><title>Language as the Medium: Multimodal Video Classification through text only</title><link>http://arxiv.org/abs/2309.10783v1</link><description>Despite an exciting new wave of multimodal machine learning models, currentapproaches still struggle to interpret the complex contextual relationshipsbetween the different modalities present in videos. Going beyond existingmethods that emphasize simple activities or objects, we propose a newmodel-agnostic approach for generating detailed textual descriptions thatcaptures multimodal video information. Our method leverages the extensiveknowledge learnt by large language models, such as GPT-3.5 or Llama2, to reasonabout textual descriptions of the visual and aural modalities, obtained fromBLIP-2, Whisper and ImageBind. Without needing additional finetuning ofvideo-text models or datasets, we demonstrate that available LLMs have theability to use these multimodal textual descriptions as proxies for ``sight''or ``hearing'' and perform zero-shot multimodal classification of videosin-context. Our evaluations on popular action recognition benchmarks, such asUCF-101 or Kinetics, show these context-rich descriptions can be successfullyused in video understanding tasks. This method points towards a promising newresearch direction in multimodal classification, demonstrating how an interplaybetween textual, visual and auditory machine learning models can enable moreholistic video understanding.</description><author>Laura Hanu, Anita L. Verő, James Thewlis</author><pubDate>Tue, 19 Sep 2023 18:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10783v1</guid></item><item><title>$O(k)$-Equivariant Dimensionality Reduction on Stiefel Manifolds</title><link>http://arxiv.org/abs/2309.10775v1</link><description>Many real-world datasets live on high-dimensional Stiefel and Grassmannianmanifolds, $V_k(\mathbb{R}^N)$ and $Gr(k, \mathbb{R}^N)$ respectively, andbenefit from projection onto lower-dimensional Stiefel (respectively,Grassmannian) manifolds. In this work, we propose an algorithm called PrincipalStiefel Coordinates (PSC) to reduce data dimensionality from $V_k(\mathbb{R}^N)$ to $V_k(\mathbb{R}^n)$ in an $O(k)$-equivariant manner ($k\leq n \ll N$). We begin by observing that each element $\alpha \inV_n(\mathbb{R}^N)$ defines an isometric embedding of $V_k(\mathbb{R}^n)$ into$V_k(\mathbb{R}^N)$. Next, we optimize for such an embedding map that minimizesdata fit error by warm-starting with the output of principal component analysis(PCA) and applying gradient descent. Then, we define a continuous and$O(k)$-equivariant map $\pi_\alpha$ that acts as a ``closest point operator''to project the data onto the image of $V_k(\mathbb{R}^n)$ in$V_k(\mathbb{R}^N)$ under the embedding determined by $\alpha$, whileminimizing distortion. Because this dimensionality reduction is$O(k)$-equivariant, these results extend to Grassmannian manifolds as well.Lastly, we show that the PCA output globally minimizes projection error in anoiseless setting, but that our algorithm achieves a meaningfully different andimproved outcome when the data does not lie exactly on the image of a linearlyembedded lower-dimensional Stiefel manifold as above. Multiple numericalexperiments using synthetic and real-world data are performed.</description><author>Andrew Lee, Harlin Lee, Jose A. Perea, Nikolas Schonsheck, Madeleine Weinstein</author><pubDate>Tue, 19 Sep 2023 18:21:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10775v1</guid></item><item><title>Semi-supervised Domain Adaptation in Graph Transfer Learning</title><link>http://arxiv.org/abs/2309.10773v1</link><description>As a specific case of graph transfer learning, unsupervised domain adaptationon graphs aims for knowledge transfer from label-rich source graphs tounlabeled target graphs. However, graphs with topology and attributes usuallyhave considerable cross-domain disparity and there are numerous real-worldscenarios where merely a subset of nodes are labeled in the source graph. Thisimposes critical challenges on graph transfer learning due to serious domainshifts and label scarcity. To address these challenges, we propose a methodnamed Semi-supervised Graph Domain Adaptation (SGDA). To deal with the domainshift, we add adaptive shift parameters to each of the source nodes, which aretrained in an adversarial manner to align the cross-domain distributions ofnode embedding, thus the node classifier trained on labeled source nodes can betransferred to the target nodes. Moreover, to address the label scarcity, wepropose pseudo-labeling on unlabeled nodes, which improves classification onthe target graph via measuring the posterior influence of nodes based on theirrelative position to the class centroids. Finally, extensive experiments on arange of publicly accessible datasets validate the effectiveness of ourproposed SGDA in different experimental settings.</description><author>Ziyue Qiao, Xiao Luo, Meng Xiao, Hao Dong, Yuanchun Zhou, Hui Xiong</author><pubDate>Tue, 19 Sep 2023 18:20:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10773v1</guid></item><item><title>Sparse Autoencoders Find Highly Interpretable Features in Language Models</title><link>http://arxiv.org/abs/2309.08600v2</link><description>One of the roadblocks to a better understanding of neural networks' internalsis \textit{polysemanticity}, where neurons appear to activate in multiple,semantically distinct contexts. Polysemanticity prevents us from identifyingconcise, human-understandable explanations for what neural networks are doinginternally. One hypothesised cause of polysemanticity is\textit{superposition}, where neural networks represent more features than theyhave neurons by assigning features to an overcomplete set of directions inactivation space, rather than to individual neurons. Here, we attempt toidentify those directions, using sparse autoencoders to reconstruct theinternal activations of a language model. These autoencoders learn sets ofsparsely activating features that are more interpretable and monosemantic thandirections identified by alternative approaches, where interpretability ismeasured by automated methods. Ablating these features enables precise modelediting, for example, by removing capabilities such as pronoun prediction,while disrupting model behaviour less than prior techniques. This workindicates that it is possible to resolve superposition in language models usinga scalable, unsupervised method. Our method may serve as a foundation forfuture mechanistic interpretability work, which we hope will enable greatermodel transparency and steerability.</description><author>Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, Lee Sharkey</author><pubDate>Tue, 19 Sep 2023 18:20:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08600v2</guid></item><item><title>Interactive Distillation of Large Single-Topic Corpora of Scientific Papers</title><link>http://arxiv.org/abs/2309.10772v1</link><description>Highly specific datasets of scientific literature are important for bothresearch and education. However, it is difficult to build such datasets atscale. A common approach is to build these datasets reductively by applyingtopic modeling on an established corpus and selecting specific topics. A morerobust but time-consuming approach is to build the dataset constructively inwhich a subject matter expert (SME) handpicks documents. This method does notscale and is prone to error as the dataset grows. Here we showcase a new tool,based on machine learning, for constructively generating targeted datasets ofscientific literature. Given a small initial "core" corpus of papers, we builda citation network of documents. At each step of the citation network, wegenerate text embeddings and visualize the embeddings through dimensionalityreduction. Papers are kept in the dataset if they are "similar" to the core orare otherwise pruned through human-in-the-loop selection. Additional insightinto the papers is gained through sub-topic modeling using SeNMFk. Wedemonstrate our new tool for literature review by applying it to two differentfields in machine learning.</description><author>Nicholas Solovyev, Ryan Barron, Manish Bhattarai, Maksim E. Eren, Kim O. Rasmussen, Boian S. Alexandrov</author><pubDate>Tue, 19 Sep 2023 18:18:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10772v1</guid></item><item><title>FRASIMED: a Clinical French Annotated Resource Produced through Crosslingual BERT-Based Annotation Projection</title><link>http://arxiv.org/abs/2309.10770v1</link><description>Natural language processing (NLP) applications such as named entityrecognition (NER) for low-resource corpora do not benefit from recent advancesin the development of large language models (LLMs) where there is still a needfor larger annotated datasets. This research article introduces a methodologyfor generating translated versions of annotated datasets through crosslingualannotation projection. Leveraging a language agnostic BERT-based approach, itis an efficient solution to increase low-resource corpora with few humanefforts and by only using already available open data resources. Quantitativeand qualitative evaluations are often lacking when it comes to evaluating thequality and effectiveness of semi-automatic data generation strategies. Theevaluation of our crosslingual annotation projection approach showed botheffectiveness and high accuracy in the resulting dataset. As a practicalapplication of this methodology, we present the creation of French AnnotatedResource with Semantic Information for Medical Entities Detection (FRASIMED),an annotated corpus comprising 2'051 synthetic clinical cases in French. Thecorpus is now available for researchers and practitioners to develop and refineFrench natural language processing (NLP) applications in the clinical field(https://zenodo.org/record/8355629), making it the largest open annotatedcorpus with linked medical concepts in French.</description><author>Jamil Zaghir, Mina Bjelogrlic, Jean-Philippe Goldman, Soukaïna Aananou, Christophe Gaudet-Blavignac, Christian Lovis</author><pubDate>Tue, 19 Sep 2023 18:17:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10770v1</guid></item><item><title>MAGIC-TBR: Multiview Attention Fusion for Transformer-based Bodily Behavior Recognition in Group Settings</title><link>http://arxiv.org/abs/2309.10765v1</link><description>Bodily behavioral language is an important social cue, and its automatedanalysis helps in enhancing the understanding of artificial intelligencesystems. Furthermore, behavioral language cues are essential for activeengagement in social agent-based user interactions. Despite the progress madein computer vision for tasks like head and body pose estimation, there is stilla need to explore the detection of finer behaviors such as gesturing, grooming,or fumbling. This paper proposes a multiview attention fusion method namedMAGIC-TBR that combines features extracted from videos and their correspondingDiscrete Cosine Transform coefficients via a transformer-based approach. Theexperiments are conducted on the BBSI dataset and the results demonstrate theeffectiveness of the proposed feature fusion with multiview attention. The codeis available at: https://github.com/surbhimadan92/MAGIC-TBR</description><author>Surbhi Madan, Rishabh Jain, Gulshan Sharma, Ramanathan Subramanian, Abhinav Dhall</author><pubDate>Tue, 19 Sep 2023 18:04:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10765v1</guid></item><item><title>A Blueprint for Precise and Fault-Tolerant Analog Neural Networks</title><link>http://arxiv.org/abs/2309.10759v1</link><description>Analog computing has reemerged as a promising avenue for accelerating deepneural networks (DNNs) due to its potential to overcome the energy efficiencyand scalability challenges posed by traditional digital architectures. However,achieving high precision and DNN accuracy using such technologies ischallenging, as high-precision data converters are costly and impractical. Inthis paper, we address this challenge by using the residue number system (RNS).RNS allows composing high-precision operations from multiple low-precisionoperations, thereby eliminating the information loss caused by the limitedprecision of the data converters. Our study demonstrates that analogaccelerators utilizing the RNS-based approach can achieve ${\geq}99\%$ of FP32accuracy for state-of-the-art DNN inference using data converters with only$6$-bit precision whereas a conventional analog core requires more than $8$-bitprecision to achieve the same accuracy in the same DNNs. The reduced precisionrequirements imply that using RNS can reduce the energy consumption of analogaccelerators by several orders of magnitude while maintaining the samethroughput and precision. Our study extends this approach to DNN training,where we can efficiently train DNNs using $7$-bit integer arithmetic whileachieving accuracy comparable to FP32 precision. Lastly, we present afault-tolerant dataflow using redundant RNS error-correcting codes to protectthe computation against noise and errors inherent within an analog accelerator.</description><author>Cansu Demirkiran, Lakshmi Nair, Darius Bunandar, Ajay Joshi</author><pubDate>Tue, 19 Sep 2023 18:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10759v1</guid></item><item><title>SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction</title><link>http://arxiv.org/abs/2309.10748v1</link><description>Recent hand-object interaction datasets show limited real object variabilityand rely on fitting the MANO parametric model to obtain groundtruth handshapes. To go beyond these limitations and spur further research, we introducethe SHOWMe dataset which consists of 96 videos, annotated with real anddetailed hand-object 3D textured meshes. Following recent work, we consider arigid hand-object scenario, in which the pose of the hand with respect to theobject remains constant during the whole video sequence. This assumption allowsus to register sub-millimetre-precise groundtruth 3D scans to the imagesequences in SHOWMe. Although simpler, this hypothesis makes sense in terms ofapplications where the required accuracy and level of detail is important eg.,object hand-over in human-robot collaboration, object scanning, or manipulationand contact point analysis. Importantly, the rigidity of the hand-objectsystems allows to tackle video-based 3D reconstruction of unknown hand-heldobjects using a 2-stage pipeline consisting of a rigid registration stepfollowed by a multi-view reconstruction (MVR) part. We carefully evaluate a setof non-trivial baselines for these two stages and show that it is possible toachieve promising object-agnostic 3D hand-object reconstructions employing anSfM toolbox or a hand pose estimator to recover the rigid transforms andoff-the-shelf MVR algorithms. However, these methods remain sensitive to theinitial camera pose estimates which might be imprecise due to lack of textureson the objects or heavy occlusions of the hands, leaving room for improvementsin the reconstruction. Code and dataset are available athttps://europe.naverlabs.com/research/showme</description><author>Anilkumar Swamy, Vincent Leroy, Philippe Weinzaepfel, Fabien Baradel, Salma Galaaoui, Romain Bregier, Matthieu Armando, Jean-Sebastien Franco, Gregory Rogez</author><pubDate>Tue, 19 Sep 2023 17:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10748v1</guid></item><item><title>RouteNet-Fermi: Network Modeling with Graph Neural Networks</title><link>http://arxiv.org/abs/2212.12070v2</link><description>Network models are an essential block of modern networks. For example, theyare widely used in network planning and optimization. However, as networksincrease in scale and complexity, some models present limitations, such as theassumption of Markovian traffic in queuing theory models, or the highcomputational cost of network simulators. Recent advances in machine learning,such as Graph Neural Networks (GNN), are enabling a new generation of networkmodels that are data-driven and can learn complex non-linear behaviors. In thispaper, we present RouteNet-Fermi, a custom GNN model that shares the same goalsas Queuing Theory, while being considerably more accurate in the presence ofrealistic traffic models. The proposed model predicts accurately the delay,jitter, and packet loss of a network. We have tested RouteNet-Fermi in networksof increasing size (up to 300 nodes), including samples with mixed trafficprofiles -- e.g., with complex non-Markovian models -- and arbitrary routingand queue scheduling configurations. Our experimental results show thatRouteNet-Fermi achieves similar accuracy as computationally-expensivepacket-level simulators and scales accurately to larger networks. Our modelproduces delay estimates with a mean relative error of 6.24% when applied to atest dataset of 1,000 samples, including network topologies one order ofmagnitude larger than those seen during training. Finally, we have alsoevaluated RouteNet-Fermi with measurements from a physical testbed and packettraces from a real-life network.</description><author>Miquel Ferriol-Galmés, Jordi Paillisse, José Suárez-Varela, Krzysztof Rusek, Shihan Xiao, Xiang Shi, Xiangle Cheng, Pere Barlet-Ros, Albert Cabellos-Aparicio</author><pubDate>Tue, 19 Sep 2023 17:47:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12070v2</guid></item><item><title>A preferential interpretation of MultiLayer Perceptrons in a conditional logic with typicality</title><link>http://arxiv.org/abs/2305.00304v3</link><description>In this paper we investigate the relationships between a multipreferentialsemantics for defeasible reasoning in knowledge representation and a multilayerneural network model. Weighted knowledge bases for a simple description logicwith typicality are considered under a (many-valued) ``concept-wise"multipreference semantics. The semantics is used to provide a preferentialinterpretation of MultiLayer Perceptrons (MLPs). A model checking and anentailment based approach are exploited in the verification of conditionalproperties of MLPs.</description><author>Mario Alviano, Francesco Bartoli, Marco Botta, Roberto Esposito, Laura Giordano, Daniele Theseider Dupré</author><pubDate>Tue, 19 Sep 2023 17:45:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00304v3</guid></item><item><title>Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome</title><link>http://arxiv.org/abs/2309.10744v1</link><description>Metaphors and sarcasm are precious fruits of our highly-evolved socialcommunication skills. However, children with Asperger syndrome are known tohave difficulties in comprehending sarcasm, even if they possess a certainlevel of verbal IQ sufficient for understanding metaphors. Given that, ascreening test that scores the ability to understand metaphor and sarcasm hasbeen used to differentiate Asperger syndrome from other symptoms exhibitingakin external behaviors (e.g., attention-deficit/hyperactivity disorder). Thisstudy uses the standardized test to examine the capability of recent largelanguage models (LLMs) in understanding human nuanced communication. Theresults divulged that, whereas their ability to comprehend metaphors has beenimproved with the increase of the number of model parameters, the improvementin sarcasm understanding was not observed. This implies that an alternativeapproach is imperative to imbue LLMs with the capacity to grasp sarcasm, whichhas been associated with the amygdala, a pivotal cerebral region for emotionallearning, in the case of humans.</description><author>Hiromu Yakura</author><pubDate>Tue, 19 Sep 2023 17:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10744v1</guid></item><item><title>Reliable Federated Disentangling Network for Non-IID Domain Feature</title><link>http://arxiv.org/abs/2301.12798v3</link><description>Federated learning (FL), as an effective decentralized distributed learningapproach, enables multiple institutions to jointly train a model withoutsharing their local data. However, the domain feature shift caused by differentacquisition devices/clients substantially degrades the performance of the FLmodel. Furthermore, most existing FL approaches aim to improve accuracy withoutconsidering reliability (e.g., confidence or uncertainty). The predictions arethus unreliable when deployed in safety-critical applications. Therefore,aiming at improving the performance of FL in non-Domain feature issues whileenabling the model more reliable. In this paper, we propose a novel reliablefederated disentangling network, termed RFedDis, which utilizes featuredisentangling to enable the ability to capture the global domain-invariantcross-client representation and preserve local client-specific featurelearning. Meanwhile, to effectively integrate the decoupled features, anuncertainty-aware decision fusion is also introduced to guide the network fordynamically integrating the decoupled features at the evidence level, whileproducing a reliable prediction with an estimated uncertainty. To the best ofour knowledge, our proposed RFedDis is the first work to develop an FL approachbased on evidential uncertainty combined with feature disentangling, whichenhances the performance and reliability of FL in non-IID domain features.Extensive experimental results show that our proposed RFedDis providesoutstanding performance with a high degree of reliability as compared to otherstate-of-the-art FL approaches.</description><author>Meng Wang, Kai Yu, Chun-Mei Feng, Yiming Qian, Ke Zou, Lianyu Wang, Rick Siow Mong Goh, Yong Liu, Huazhu Fu</author><pubDate>Tue, 19 Sep 2023 17:38:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12798v3</guid></item><item><title>Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation</title><link>http://arxiv.org/abs/2309.10740v1</link><description>Diffusion models power a vast majority of text-to-audio (TTA) generationmethods. Unfortunately, these models suffer from slow inference speed due toiterative queries to the underlying denoising network, thus unsuitable forscenarios with inference time or computational constraints. This work modifiesthe recently proposed consistency distillation framework to train TTA modelsthat require only a single neural network query. In addition to incorporatingclassifier-free guidance into the distillation process, we leverage theavailability of generated audio during distillation training to fine-tune theconsistency TTA model with novel loss functions in the audio space, such as theCLAP score. Our objective and subjective evaluation results on the AudioCapsdataset show that consistency models retain diffusion models' high generationquality and diversity while reducing the number of queries by a factor of 400.</description><author>Yatong Bai, Trung Dang, Dung Tran, Kazuhito Koishida, Somayeh Sojoudi</author><pubDate>Tue, 19 Sep 2023 17:36:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10740v1</guid></item><item><title>Controllable Speaking Styles Using a Large Language Model</title><link>http://arxiv.org/abs/2305.10321v2</link><description>Reference-based Text-to-Speech (TTS) models can generate multiple,prosodically-different renditions of the same target text. Such models jointlylearn a latent acoustic space during training, which can be sampled from duringinference. Controlling these models during inference typically requires findingan appropriate reference utterance, which is non-trivial. Large generative language models (LLMs) have shown excellent performance invarious language-related tasks. Given only a natural language query text (theprompt), such models can be used to solve specific, context-dependent tasks.Recent work in TTS has attempted similar prompt-based control of novel speakingstyle generation. Those methods do not require a reference utterance and can,under ideal conditions, be controlled with only a prompt. But existing methodstypically require a prompt-labelled speech corpus for jointly training aprompt-conditioned encoder. In contrast, we instead employ an LLM to directly suggest prosodicmodifications for a controllable TTS model, using contextual informationprovided in the prompt. The prompt can be designed for a multitude of tasks.Here, we give two demonstrations: control of speaking style; prosodyappropriate for a given dialogue context. The proposed method is rated mostappropriate in 50% of cases vs. 31% for a baseline model.</description><author>Atli Thor Sigurgeirsson, Simon King</author><pubDate>Tue, 19 Sep 2023 17:35:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10321v2</guid></item><item><title>MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation</title><link>http://arxiv.org/abs/2309.10738v1</link><description>Pre-trained language models have achieved impressive results in various musicunderstanding and generation tasks. However, existing pre-training methods forsymbolic melody generation struggle to capture multi-scale, multi-dimensionalstructural information in note sequences, due to the domain knowledgediscrepancy between text and music. Moreover, the lack of available large-scalesymbolic melody datasets limits the pre-training improvement. In this paper, wepropose MelodyGLM, a multi-task pre-training framework for generating melodieswith long-term structure. We design the melodic n-gram and long span samplingstrategies to create local and global blank infilling tasks for modeling thelocal and global structures in melodies. Specifically, we incorporate pitchn-grams, rhythm n-grams, and their combined n-grams into the melodic n-gramblank infilling tasks for modeling the multi-dimensional structures inmelodies. To this end, we have constructed a large-scale symbolic melodydataset, MelodyNet, containing more than 0.4 million melody pieces. MelodyNetis utilized for large-scale pre-training and domain-specific n-gram lexiconconstruction. Both subjective and objective evaluations demonstrate thatMelodyGLM surpasses the standard and previous pre-training methods. Inparticular, subjective evaluations show that, on the melody continuation task,MelodyGLM achieves average improvements of 0.82, 0.87, 0.78, and 0.94 inconsistency, rhythmicity, structure, and overall quality, respectively.Notably, MelodyGLM nearly matches the quality of human-composed melodies on themelody inpainting task.</description><author>Xinda Wu, Zhijie Huang, Kejun Zhang, Jiaxing Yu, Xu Tan, Tieyao Zhang, Zihao Wang, Lingyun Sun</author><pubDate>Tue, 19 Sep 2023 17:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10738v1</guid></item><item><title>Monte-Carlo tree search with uncertainty propagation via optimal transport</title><link>http://arxiv.org/abs/2309.10737v1</link><description>This paper introduces a novel backup strategy for Monte-Carlo Tree Search(MCTS) designed for highly stochastic and partially observable Markov decisionprocesses. We adopt a probabilistic approach, modeling both value andaction-value nodes as Gaussian distributions. We introduce a novel backupoperator that computes value nodes as the Wasserstein barycenter of theiraction-value children nodes; thus, propagating the uncertainty of the estimateacross the tree to the root node. We study our novel backup operator when usinga novel combination of $L^1$-Wasserstein barycenter with $\alpha$-divergence,by drawing a notable connection to the generalized mean backup operator. Wecomplement our probabilistic backup operator with two sampling strategies,based on optimistic selection and Thompson sampling, obtaining our WassersteinMCTS algorithm. We provide theoretical guarantees of asymptotic convergence tothe optimal policy, and an empirical evaluation on several stochastic andpartially observable environments, where our approach outperforms well-knownrelated baselines.</description><author>Tuan Dam, Pascal Stenger, Lukas Schneider, Joni Pajarinen, Carlo D'Eramo, Odalric-Ambrym Maillard</author><pubDate>Tue, 19 Sep 2023 17:32:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10737v1</guid></item><item><title>Mixture Weight Estimation and Model Prediction in Multi-source Multi-target Domain Adaptation</title><link>http://arxiv.org/abs/2309.10736v1</link><description>We consider the problem of learning a model from multiple heterogeneoussources with the goal of performing well on a new target distribution. The goalof learner is to mix these data sources in a target-distribution aware way andsimultaneously minimize the empirical risk on the mixed source. The literaturehas made some tangible advancements in establishing theory of learning onmixture domain. However, there are still two unsolved problems. Firstly, how toestimate the optimal mixture of sources, given a target domain; Secondly, whenthere are numerous target domains, how to solve empirical risk minimization(ERM) for each target using possibly unique mixture of data sources in acomputationally efficient manner. In this paper we address both problemsefficiently and with guarantees. We cast the first problem, mixture weightestimation, as a convex-nonconcave compositional minimax problem, and proposean efficient stochastic algorithm with provable stationarity guarantees. Next,for the second problem, we identify that for certain regimes, solving ERM foreach target domain individually can be avoided, and instead parameters for atarget optimal model can be viewed as a non-linear function on a space of themixture coefficients. Building upon this, we show that in the offline setting,a GD-trained overparameterized neural network can provably learn such functionto predict the model of target domain instead of solving a designated ERMproblem. Finally, we also consider an online setting and propose a labelefficient online algorithm, which predicts parameters for new targets given anarbitrary sequence of mixing coefficients, while enjoying regret guarantees.</description><author>Yuyang Deng, Ilja Kuzborskij, Mehrdad Mahdavi</author><pubDate>Tue, 19 Sep 2023 17:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10736v1</guid></item><item><title>A State-Space Perspective on Modelling and Inference for Online Skill Rating</title><link>http://arxiv.org/abs/2308.02414v2</link><description>This paper offers a comprehensive review of the main methodologies used forskill rating in competitive sports. We advocate for a state-space modelperspective, wherein players' skills are represented as time-varying, and matchresults serve as the sole observed quantities. The state-space modelperspective facilitates the decoupling of modeling and inference, enabling amore focused approach highlighting model assumptions, while also fostering thedevelopment of general-purpose inference tools. We explore the essential stepsinvolved in constructing a state-space model for skill rating before turning toa discussion on the three stages of inference: filtering, smoothing andparameter estimation. Throughout, we examine the computational challenges ofscaling up to high-dimensional scenarios involving numerous players andmatches, highlighting approximations and reductions used to address thesechallenges effectively. We provide concise summaries of popular methodsdocumented in the literature, along with their inferential paradigms andintroduce new approaches to skill rating inference based on sequential MonteCarlo and finite state-spaces. We close with numerical experimentsdemonstrating a practical workflow on real data across different sports.</description><author>Samuel Duffield, Samuel Power, Lorenzo Rimella</author><pubDate>Tue, 19 Sep 2023 17:26:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02414v2</guid></item><item><title>MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer</title><link>http://arxiv.org/abs/2309.09067v2</link><description>Precise crop yield prediction provides valuable information for agriculturalplanning and decision-making processes. However, timely predicting crop yieldsremains challenging as crop growth is sensitive to growing season weathervariation and climate change. In this work, we develop a deep learning-basedsolution, namely Multi-Modal Spatial-Temporal Vision Transformer (MMST-ViT),for predicting crop yields at the county level across the United States, byconsidering the effects of short-term meteorological variations during thegrowing season and the long-term climate change on crops. Specifically, ourMMST-ViT consists of a Multi-Modal Transformer, a Spatial Transformer, and aTemporal Transformer. The Multi-Modal Transformer leverages both visual remotesensing data and short-term meteorological data for modeling the effect ofgrowing season weather variations on crop growth. The Spatial Transformerlearns the high-resolution spatial dependency among counties for accurateagricultural tracking. The Temporal Transformer captures the long-rangetemporal dependency for learning the impact of long-term climate change oncrops. Meanwhile, we also devise a novel multi-modal contrastive learningtechnique to pre-train our model without extensive human supervision. Hence,our MMST-ViT captures the impacts of both short-term weather variations andlong-term climate change on crops by leveraging both satellite images andmeteorological data. We have conducted extensive experiments on over 200counties in the United States, with the experimental results exhibiting thatour MMST-ViT outperforms its counterparts under three performance metrics ofinterest.</description><author>Fudong Lin, Summer Crawford, Kaleb Guillot, Yihe Zhang, Yan Chen, Xu Yuan, Li Chen, Shelby Williams, Robert Minvielle, Xiangming Xiao, Drew Gholson, Nicolas Ashwell, Tri Setiyono, Brenda Tubana, Lu Peng, Magdy Bayoumi, Nian-Feng Tzeng</author><pubDate>Tue, 19 Sep 2023 17:24:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09067v2</guid></item><item><title>Promoting Fairness in GNNs: A Characterization of Stability</title><link>http://arxiv.org/abs/2309.03648v2</link><description>The Lipschitz bound, a technique from robust statistics, can limit themaximum changes in the output concerning the input, taking into accountassociated irrelevant biased factors. It is an efficient and provable methodfor examining the output stability of machine learning models without incurringadditional computation costs. Recently, Graph Neural Networks (GNNs), whichoperate on non-Euclidean data, have gained significant attention. However, noprevious research has investigated the GNN Lipschitz bounds to shed light onstabilizing model outputs, especially when working on non-Euclidean data withinherent biases. Given the inherent biases in common graph data used for GNNtraining, it poses a serious challenge to constraining the GNN outputperturbations induced by input biases, thereby safeguarding fairness duringtraining. Recently, despite the Lipschitz constant's use in controlling thestability of Euclideanneural networks, the calculation of the precise Lipschitzconstant remains elusive for non-Euclidean neural networks like GNNs,especially within fairness contexts. To narrow this gap, we begin with thegeneral GNNs operating on an attributed graph, and formulate a Lipschitz boundto limit the changes in the output regarding biases associated with the input.Additionally, we theoretically analyze how the Lipschitz constant of a GNNmodel could constrain the output perturbations induced by biases learned fromdata for fairness training. We experimentally validate the Lipschitz bound'seffectiveness in limiting biases of the model output. Finally, from a trainingdynamics perspective, we demonstrate why the theoretical Lipschitz bound caneffectively guide the GNN training to better trade-off between accuracy andfairness.</description><author>Yaning Jia, Chunhui Zhang</author><pubDate>Tue, 19 Sep 2023 17:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03648v2</guid></item><item><title>GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models</title><link>http://arxiv.org/abs/2309.10730v1</link><description>The remarkable capabilities and intricate nature of Artificial Intelligence(AI) have dramatically escalated the imperative for specialized AIaccelerators. Nonetheless, designing these accelerators for various AIworkloads remains both labor- and time-intensive. While existing designexploration and automation tools can partially alleviate the need for extensivehuman involvement, they still demand substantial hardware expertise, posing abarrier to non-experts and stifling AI accelerator development. Motivated bythe astonishing potential of large language models (LLMs) for generatinghigh-quality content in response to human language instructions, we embark onthis work to examine the possibility of harnessing LLMs to automate AIaccelerator design. Through this endeavor, we develop GPT4AIGChip, a frameworkintended to democratize AI accelerator design by leveraging human naturallanguages instead of domain-specific languages. Specifically, we first performan in-depth investigation into LLMs' limitations and capabilities for AIaccelerator design, thus aiding our understanding of our current position andgarnering insights into LLM-powered automated AI accelerator design.Furthermore, drawing inspiration from the above insights, we develop aframework called GPT4AIGChip, which features an automated demo-augmentedprompt-generation pipeline utilizing in-context learning to guide LLMs towardscreating high-quality AI accelerator design. To our knowledge, this work is thefirst to demonstrate an effective pipeline for LLM-powered automated AIaccelerator generation. Accordingly, we anticipate that our insights andframework can serve as a catalyst for innovations in next-generationLLM-powered design automation tools.</description><author>Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, Yingyan Lin</author><pubDate>Tue, 19 Sep 2023 17:14:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10730v1</guid></item><item><title>PAMS: Platform for Artificial Market Simulations</title><link>http://arxiv.org/abs/2309.10729v1</link><description>This paper presents a new artificial market simulation platform, PAMS:Platform for Artificial Market Simulations. PAMS is developed as a Python-basedsimulator that is easily integrated with deep learning and enabling varioussimulation that requires easy users' modification. In this paper, wedemonstrate PAMS effectiveness through a study using agents predicting futureprices by deep learning.</description><author>Masanori Hirano, Ryosuke Takata, Kiyoshi Izumi</author><pubDate>Tue, 19 Sep 2023 17:14:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10729v1</guid></item><item><title>Few-Shot Panoptic Segmentation With Foundation Models</title><link>http://arxiv.org/abs/2309.10726v1</link><description>Current state-of-the-art methods for panoptic segmentation require an immenseamount of annotated training data that is both arduous and expensive to obtainposing a significant challenge for their widespread adoption. Concurrently,recent breakthroughs in visual representation learning have sparked a paradigmshift leading to the advent of large foundation models that can be trained withcompletely unlabeled images. In this work, we propose to leverage suchtask-agnostic image features to enable few-shot panoptic segmentation bypresenting Segmenting Panoptic Information with Nearly 0 labels (SPINO). Indetail, our method combines a DINOv2 backbone with lightweight network headsfor semantic segmentation and boundary estimation. We show that our approach,albeit being trained with only ten annotated images, predicts high-qualitypseudo-labels that can be used with any existing panoptic segmentation method.Notably, we demonstrate that SPINO achieves competitive results compared tofully supervised baselines while using less than 0.3% of the ground truthlabels, paving the way for learning complex visual recognition tasks leveragingfoundation models. To illustrate its general applicability, we further deploySPINO on real-world robotic vision systems for both outdoor and indoorenvironments. To foster future research, we make the code and trained modelspublicly available at http://spino.cs.uni-freiburg.de.</description><author>Markus Käppeler, Kürsat Petek, Niclas Vödisch, Wolfram Burgard, Abhinav Valada</author><pubDate>Tue, 19 Sep 2023 17:09:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10726v1</guid></item><item><title>Causality-Driven One-Shot Learning for Prostate Cancer Grading from MRI</title><link>http://arxiv.org/abs/2309.10725v1</link><description>In this paper, we present a novel method to automatically classify medicalimages that learns and leverages weak causal signals in the image. Ourframework consists of a convolutional neural network backbone and acausality-extractor module that extracts cause-effect relationships betweenfeature maps that can inform the model on the appearance of a feature in oneplace of the image, given the presence of another feature within some otherplace of the image. To evaluate the effectiveness of our approach in low-datascenarios, we train our causality-driven architecture in a One-shot learningscheme, where we propose a new meta-learning procedure entailing meta-trainingand meta-testing tasks that are designed using related classes but at differentlevels of granularity. We conduct binary and multi-class classificationexperiments on a publicly available dataset of prostate MRI images. To validatethe effectiveness of the proposed causality-driven module, we perform anablation study and conduct qualitative assessments using class activation mapsto highlight regions strongly influencing the network's decision-makingprocess. Our findings show that causal relationships among features play acrucial role in enhancing the model's ability to discern relevant informationand yielding more reliable and interpretable predictions. This would make it apromising approach for medical image classification tasks.</description><author>Gianluca Carloni, Eva Pachetti, Sara Colantonio</author><pubDate>Tue, 19 Sep 2023 17:08:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10725v1</guid></item><item><title>Sound Source Localization is All about Cross-Modal Alignment</title><link>http://arxiv.org/abs/2309.10724v1</link><description>Humans can easily perceive the direction of sound sources in a visual scene,termed sound source localization. Recent studies on learning-based sound sourcelocalization have mainly explored the problem from a localization perspective.However, prior arts and existing benchmarks do not account for a more importantaspect of the problem, cross-modal semantic understanding, which is essentialfor genuine sound source localization. Cross-modal semantic understanding isimportant in understanding semantically mismatched audio-visual events, e.g.,silent objects, or off-screen sounds. To account for this, we propose across-modal alignment task as a joint task with sound source localization tobetter learn the interaction between audio and visual modalities. Thereby, weachieve high localization performance with strong cross-modal semanticunderstanding. Our method outperforms the state-of-the-art approaches in bothsound source localization and cross-modal retrieval. Our work suggests thatjointly tackling both tasks is necessary to conquer genuine sound sourcelocalization.</description><author>Arda Senocak, Hyeonggon Ryu, Junsik Kim, Tae-Hyun Oh, Hanspeter Pfister, Joon Son Chung</author><pubDate>Tue, 19 Sep 2023 17:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10724v1</guid></item><item><title>An Empirical Study of NetOps Capability of Pre-Trained Large Language Models</title><link>http://arxiv.org/abs/2309.05557v3</link><description>Nowadays, the versatile capabilities of Pre-trained Large Language Models(LLMs) have attracted much attention from the industry. However, some verticaldomains are more interested in the in-domain capabilities of LLMs. For theNetworks domain, we present NetEval, an evaluation set for measuring thecomprehensive capabilities of LLMs in Network Operations (NetOps). NetEval isdesigned for evaluating the commonsense knowledge and inference ability inNetOps in a multi-lingual context. NetEval consists of 5,732 questions aboutNetOps, covering five different sub-domains of NetOps. With NetEval, wesystematically evaluate the NetOps capability of 26 publicly available LLMs.The results show that only GPT-4 can achieve a performance competitive tohumans. However, some open models like LLaMA 2 demonstrate significantpotential.</description><author>Yukai Miao, Yu Bai, Li Chen, Dan Li, Haifeng Sun, Xizheng Wang, Ziqiu Luo, Yanyu Ren, Dapeng Sun, Xiuting Xu, Qi Zhang, Chao Xiang, Xinchi Li</author><pubDate>Tue, 19 Sep 2023 17:04:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05557v3</guid></item><item><title>LEA*: An A* Variant Algorithm with Improved Edge Efficiency for Robot Motion Planning</title><link>http://arxiv.org/abs/2309.10722v1</link><description>In this work, we introduce a new graph search algorithm, lazy edged based A*(LEA*), for robot motion planning. By using an edge queue and exploiting theidea of lazy search, LEA* is optimally vertex efficient similar to A*, and hasimproved edge efficiency compared to A*. LEA* is simple and easy to implementwith minimum modification to A*, resulting in a very small overhead compared toprevious lazy search algorithms. We also explore the effect of inflatedheuristics, which results in the weighted LEA* (wLEA*). We show that the edgeefficiency of wLEA* becomes close to LazySP and, thus is near-optimal. We testLEA* and wLEA* on 2D planning problems and planning of a 7-DOF manipulator. Weperform a thorough comparison with previous algorithms by considering sparse,medium, and cluttered random worlds and small, medium, and large graph sizes.Our results show that LEA* and wLEA* are the fastest algorithms to find theplan compared to previous algorithms.</description><author>Dongliang Zheng, Panagiotis Tsiotras</author><pubDate>Tue, 19 Sep 2023 17:04:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10722v1</guid></item><item><title>Reconstruct-and-Generate Diffusion Model for Detail-Preserving Image Denoising</title><link>http://arxiv.org/abs/2309.10714v1</link><description>Image denoising is a fundamental and challenging task in the field ofcomputer vision. Most supervised denoising methods learn to reconstruct cleanimages from noisy inputs, which have intrinsic spectral bias and tend toproduce over-smoothed and blurry images. Recently, researchers have exploreddiffusion models to generate high-frequency details in image restoration tasks,but these models do not guarantee that the generated texture aligns with realimages, leading to undesirable artifacts. To address the trade-off betweenvisual appeal and fidelity of high-frequency details in denoising tasks, wepropose a novel approach called the Reconstruct-and-Generate Diffusion Model(RnG). Our method leverages a reconstructive denoising network to recover themajority of the underlying clean signal, which serves as the initial estimationfor subsequent steps to maintain fidelity. Additionally, it employs a diffusionalgorithm to generate residual high-frequency details, thereby enhancing visualquality. We further introduce a two-stage training scheme to ensure effectivecollaboration between the reconstructive and generative modules of RnG. Toreduce undesirable texture introduced by the diffusion model, we also proposean adaptive step controller that regulates the number of inverse steps appliedby the diffusion model, allowing control over the level of high-frequencydetails added to each patch as well as saving the inference computational cost.Through our proposed RnG, we achieve a better balance between perception anddistortion. We conducted extensive experiments on both synthetic and realdenoising datasets, validating the superiority of the proposed approach.</description><author>Yujin Wang, Lingen Li, Tianfan Xue, Jinwei Gu</author><pubDate>Tue, 19 Sep 2023 17:01:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10714v1</guid></item><item><title>Interpret Vision Transformers as ConvNets with Dynamic Convolutions</title><link>http://arxiv.org/abs/2309.10713v1</link><description>There has been a debate about the superiority between vision Transformers andConvNets, serving as the backbone of computer vision models. Although they areusually considered as two completely different architectures, in this paper, weinterpret vision Transformers as ConvNets with dynamic convolutions, whichenables us to characterize existing Transformers and dynamic ConvNets in aunified framework and compare their design choices side by side. In addition,our interpretation can also guide the network design as researchers now canconsider vision Transformers from the design space of ConvNets and vice versa.We demonstrate such potential through two specific studies. First, we inspectthe role of softmax in vision Transformers as the activation function and findit can be replaced by commonly used ConvNets modules, such as ReLU and LayerNormalization, which results in a faster convergence rate and betterperformance. Second, following the design of depth-wise convolution, we createa corresponding depth-wise vision Transformer that is more efficient withcomparable performance. The potential of the proposed unified interpretation isnot limited to the given examples and we hope it can inspire the community andgive rise to more advanced network architectures.</description><author>Chong Zhou, Chen Change Loy, Bo Dai</author><pubDate>Tue, 19 Sep 2023 17:00:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10713v1</guid></item><item><title>Latent Space Energy-based Model for Fine-grained Open Set Recognition</title><link>http://arxiv.org/abs/2309.10711v1</link><description>Fine-grained open-set recognition (FineOSR) aims to recognize imagesbelonging to classes with subtle appearance differences while rejecting imagesof unknown classes. A recent trend in OSR shows the benefit of generativemodels to discriminative unknown detection. As a type of generative model,energy-based models (EBM) are the potential for hybrid modeling of generativeand discriminative tasks. However, most existing EBMs suffer from densityestimation in high-dimensional space, which is critical to recognizing imagesfrom fine-grained classes. In this paper, we explore the low-dimensional latentspace with energy-based prior distribution for OSR in a fine-grained visualworld. Specifically, based on the latent space EBM, we propose anattribute-aware information bottleneck (AIB), a residual attribute featureaggregation (RAFA) module, and an uncertainty-based virtual outlier synthesis(UVOS) module to improve the expressivity, granularity, and density of thesamples in fine-grained classes, respectively. Our method is flexible to takeadvantage of recent vision transformers for powerful visual classification andgeneration. The method is validated on both fine-grained and general visualclassification datasets while preserving the capability of generatingphoto-realistic fake images with high resolution.</description><author>Wentao Bao, Qi Yu, Yu Kong</author><pubDate>Tue, 19 Sep 2023 17:00:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10711v1</guid></item><item><title>Harnessing Collective Intelligence Under a Lack of Cultural Consensus</title><link>http://arxiv.org/abs/2309.09787v2</link><description>Harnessing collective intelligence to drive effective decision-making andcollaboration benefits from the ability to detect and characterizeheterogeneity in consensus beliefs. This is particularly true in domains suchas technology acceptance or leadership perception, where a consensus defines anintersubjective truth, leading to the possibility of multiple "ground truths"when subsets of respondents sustain mutually incompatible consensuses. CulturalConsensus Theory (CCT) provides a statistical framework for detecting andcharacterizing these divergent consensus beliefs. However, it is unworkable inmodern applications because it lacks the ability to generalize across evenhighly similar beliefs, is ineffective with sparse data, and can leverageneither external knowledge bases nor learned machine representations. Here, weovercome these limitations through Infinite Deep Latent Construct CulturalConsensus Theory (iDLC-CCT), a nonparametric Bayesian model that extends CCTwith a latent construct that maps between pretrained deep neural networkembeddings of entities and the consensus beliefs regarding those entities amongone or more subsets of respondents. We validate the method across domainsincluding perceptions of risk sources, food healthiness, leadership, firstimpressions, and humor. We find that iDLC-CCT better predicts the degree ofconsensus, generalizes well to out-of-sample entities, and is effective evenwith sparse data. To improve scalability, we introduce an efficienthard-clustering variant of the iDLC-CCT using an algorithm derived from asmall-variance asymptotic analysis of the model. The iDLC-CCT, therefore,provides a workable computational foundation for harnessing collectiveintelligence under a lack of cultural consensus and may potentially form thebasis of consensus-aware information technologies.</description><author>Necdet Gürkan, Jordan W. Suchow</author><pubDate>Tue, 19 Sep 2023 16:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09787v2</guid></item><item><title>Unified Brain MR-Ultrasound Synthesis using Multi-Modal Hierarchical Representations</title><link>http://arxiv.org/abs/2309.08747v2</link><description>We introduce MHVAE, a deep hierarchical variational auto-encoder (VAE) thatsynthesizes missing images from various modalities. Extending multi-modal VAEswith a hierarchical latent structure, we introduce a probabilistic formulationfor fusing multi-modal images in a common latent representation while havingthe flexibility to handle incomplete image sets as input. Moreover, adversariallearning is employed to generate sharper images. Extensive experiments areperformed on the challenging problem of joint intra-operative ultrasound (iUS)and Magnetic Resonance (MR) synthesis. Our model outperformed multi-modal VAEs,conditional GANs, and the current state-of-the-art unified method (ResViT) forsynthesizing missing images, demonstrating the advantage of using ahierarchical latent representation and a principled probabilistic fusionoperation. Our code is publicly available\url{https://github.com/ReubenDo/MHVAE}.</description><author>Reuben Dorent, Nazim Haouchine, Fryderyk Kögl, Samuel Joutard, Parikshit Juvekar, Erickson Torio, Alexandra Golby, Sebastien Ourselin, Sarah Frisken, Tom Vercauteren, Tina Kapur, William M. Wells</author><pubDate>Tue, 19 Sep 2023 16:48:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08747v2</guid></item><item><title>OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch</title><link>http://arxiv.org/abs/2309.10706v1</link><description>Large language models (LLMs) with billions of parameters have demonstratedoutstanding performance on various natural language processing tasks. Thisreport presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model,to contribute an LLM variant to the Chinese-oriented open-source modelcommunity. We enhance OpenBA with effective and efficient techniques as well asadopt a three-stage training strategy to train the model from scratch. Oursolution can also achieve very competitive performance with only 380B tokens,which is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on theMMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report providesthe main details to pre-train an analogous model, including pre-training dataprocessing, Bilingual Flan data collection, the empirical observations thatinspire our model architecture design, training objectives of different stages,and other enhancement techniques. We have refactored our code to follow thedesign principles of the Huggingface Transformers Library, making it moreconvenient for developers to use, and released checkpoints of differenttraining stages at https://huggingface.co/openBA. More details of our projectare available at https://github.com/OpenNLG/openBA.git.</description><author>Juntao Li, Zecheng Tang, Yuyang Ding, Pinzheng Wang, Pei Guo, Wangjie You, Dan Qiao, Wenliang Chen, Guohong Fu, Qiaoming Zhu, Guodong Zhou, Min Zhang</author><pubDate>Tue, 19 Sep 2023 16:46:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10706v1</guid></item><item><title>How to Data in Datathons</title><link>http://arxiv.org/abs/2309.09770v2</link><description>The rise of datathons, also known as data or data science hackathons, hasprovided a platform to collaborate, learn, and innovate in a short timeframe.Despite their significant potential benefits, organizations often struggle toeffectively work with data due to a lack of clear guidelines and best practicesfor potential issues that might arise. Drawing on our own experiences andinsights from organizing &gt;80 datathon challenges with &gt;60 partnershiporganizations since 2016, we provide guidelines and recommendations that serveas a resource for organizers to navigate the data-related complexities ofdatathons. We apply our proposed framework to 10 case studies.</description><author>Carlos Mougan, Richard Plant, Clare Teng, Marya Bazzi, Alvaro Cabregas Ejea, Ryan Sze-Yin Chan, David Salvador Jasin, Martin Stoffel, Kirstie Jane Whitaker, Jules Manser</author><pubDate>Tue, 19 Sep 2023 16:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09770v2</guid></item><item><title>Accelerating Globally Optimal Consensus Maximization in Geometric Vision</title><link>http://arxiv.org/abs/2304.05156v2</link><description>Branch-and-bound-based consensus maximization stands out due to its importantability of retrieving the globally optimal solution to outlier-affectedgeometric problems. However, while the discovery of such solutions caries highscientific value, its application in practical scenarios is often prohibited byits computational complexity growing exponentially as a function of thedimensionality of the problem at hand. In this work, we convey a novel, generaltechnique that allows us to branch over an n-1 dimensional space for ann-dimensional problem. The remaining degree of freedom can be solved globallyoptimally within each bound calculation by applying the efficient intervalstabbing technique. While each individual bound derivation is harder to computeowing to the additional need for solving a sorting problem, the reduced numberof intervals and tighter bounds in practice lead to a significant reduction inthe overall number of required iterations. Besides an abstract introduction ofthe approach, we present applications to four fundamental geometric computervision problems: camera resectioning, relative camera pose estimation, pointset registration, and rotation and focal length estimation. Through ourexhaustive tests, we demonstrate significant speed-up factors at timesexceeding two orders of magnitude, thereby increasing the viability of globallyoptimal consensus maximizers in online application scenarios.</description><author>Xinyue Zhang, Liangzu Peng, Wanting Xu, Laurent Kneip</author><pubDate>Tue, 19 Sep 2023 16:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05156v2</guid></item><item><title>Unfolding Local Growth Rate Estimates for (Almost) Perfect Adversarial Detection</title><link>http://arxiv.org/abs/2212.06776v2</link><description>Convolutional neural networks (CNN) define the state-of-the-art solution onmany perceptual tasks. However, current CNN approaches largely remainvulnerable against adversarial perturbations of the input that have beencrafted specifically to fool the system while being quasi-imperceptible to thehuman eye. In recent years, various approaches have been proposed to defendCNNs against such attacks, for example by model hardening or by adding explicitdefence mechanisms. Thereby, a small "detector" is included in the network andtrained on the binary classification task of distinguishing genuine data fromdata containing adversarial perturbations. In this work, we propose a simpleand light-weight detector, which leverages recent findings on the relationbetween networks' local intrinsic dimensionality (LID) and adversarial attacks.Based on a re-interpretation of the LID measure and several simple adaptations,we surpass the state-of-the-art on adversarial detection by a significantmargin and reach almost perfect results in terms of F1-score for severalnetworks and datasets. Sources available at:https://github.com/adverML/multiLID</description><author>Peter Lorenz, Margret Keuper, Janis Keuper</author><pubDate>Tue, 19 Sep 2023 16:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.06776v2</guid></item><item><title>Measurement Simplification in ρ-POMDP with Performance Guarantees</title><link>http://arxiv.org/abs/2309.10701v1</link><description>Decision making under uncertainty is at the heart of any autonomous systemacting with imperfect information. The cost of solving the decision makingproblem is exponential in the action and observation spaces, thus rendering itunfeasible for many online systems. This paper introduces a novel approach toefficient decision-making, by partitioning the high-dimensional observationspace. Using the partitioned observation space, we formulate analytical boundson the expected information-theoretic reward, for general belief distributions.These bounds are then used to plan efficiently while keeping performanceguarantees. We show that the bounds are adaptive, computationally efficient,and that they converge to the original solution. We extend the partitioningparadigm and present a hierarchy of partitioned spaces that allows greaterefficiency in planning. We then propose a specific variant of these bounds forGaussian beliefs and show a theoretical performance improvement of at least afactor of 4. Finally, we compare our novel method to other state of the artalgorithms in active SLAM scenarios, in simulation and in real experiments. Inboth cases we show a significant speed-up in planning with performanceguarantees.</description><author>Tom Yotam, Vadim Indelman</author><pubDate>Tue, 19 Sep 2023 16:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10701v1</guid></item><item><title>Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey</title><link>http://arxiv.org/abs/2307.04370v2</link><description>End-to-End driving is a promising paradigm as it circumvents the drawbacksassociated with modular systems, such as their overwhelming complexity andpropensity for error propagation. Autonomous driving transcends conventionaltraffic patterns by proactively recognizing critical events in advance,ensuring passengers' safety and providing them with comfortable transportation,particularly in highly stochastic and variable traffic settings. This paperpresents a comprehensive review of the End-to-End autonomous driving stack. Itprovides a taxonomy of automated driving tasks wherein neural networks havebeen employed in an End-to-End manner, encompassing the entire driving processfrom perception to control, while addressing key challenges encountered inreal-world applications. Recent developments in End-to-End autonomous drivingare analyzed, and research is categorized based on underlying principles,methodologies, and core functionality. These categories encompass sensorialinput, main and auxiliary output, learning approaches ranging from imitation toreinforcement learning, and model evaluation techniques. The surveyincorporates a detailed discussion of the explainability and safety aspects.Furthermore, it assesses the state-of-the-art, identifies challenges, andexplores future possibilities. We maintained the latest advancements and theircorresponding open-source implementations athttps://github.com/Pranav-chib/Recent-Advancements-in-End-to-End-Autonomous-Driving-using-Deep-Learning.</description><author>Pranav Singh Chib, Pravendra Singh</author><pubDate>Tue, 19 Sep 2023 16:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04370v2</guid></item><item><title>Collaborative causal inference on distributed data</title><link>http://arxiv.org/abs/2208.07898v3</link><description>In recent years, the development of technologies for causal inference withprivacy preservation of distributed data has gained considerable attention.Many existing methods for distributed data focus on resolving the lack ofsubjects (samples) and can only reduce random errors in estimating treatmenteffects. In this study, we propose a data collaboration quasi-experiment(DC-QE) that resolves the lack of both subjects and covariates, reducing randomerrors and biases in the estimation. Our method involves constructingdimensionality-reduced intermediate representations from private data fromlocal parties, sharing intermediate representations instead of private data forprivacy preservation, estimating propensity scores from the shared intermediaterepresentations, and finally, estimating the treatment effects from propensityscores. Through numerical experiments on both artificial and real-world data,we confirm that our method leads to better estimation results than individualanalyses. While dimensionality reduction loses some information in the privatedata and causes performance degradation, we observe that sharing intermediaterepresentations with many parties to resolve the lack of subjects andcovariates sufficiently improves performance to overcome the degradation causedby dimensionality reduction. Although external validity is not necessarilyguaranteed, our results suggest that DC-QE is a promising method. With thewidespread use of our method, intermediate representations can be published asopen data to help researchers find causalities and accumulate a knowledge base.</description><author>Yuji Kawamata, Ryoki Motai, Yukihiko Okada, Akira Imakura, Tetsuya Sakurai</author><pubDate>Tue, 19 Sep 2023 16:30:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07898v3</guid></item><item><title>From "Let's Google" to "Let's ChatGPT": Student and Instructor Perspectives on the influence of LLMs on Undergraduate Engineering Education</title><link>http://arxiv.org/abs/2309.10694v1</link><description>The rise in popularity of Large Language Models (LLMs) has prompteddiscussions in academic circles, with students exploring LLM-based tools forcoursework inquiries and instructors exploring them for teaching and research.Even though a lot of work is underway to create LLM-based tools tailored forstudents and instructors, there is a lack of comprehensive user studies thatcapture the perspectives of students and instructors regarding LLMs. This paperaddresses this gap by conducting surveys and interviews within undergraduateengineering universities in India. Using 1306 survey responses among students,112 student interviews, and 27 instructor interviews around the academic usageof ChatGPT (a popular LLM), this paper offers insights into the current usagepatterns, perceived benefits, threats, and challenges, as well asrecommendations for enhancing the adoption of LLMs among students andinstructors. These insights are further utilized to discuss the practicalimplications of LLMs in undergraduate engineering education and beyond.</description><author>Ishika Joshi, Ritvik Budhiraja, Pranav Deepak Tanna, Lovenya Jain, Mihika Deshpande, Arjun Srivastava, Srinivas Rallapalli, Harshal D Akolekar, Jagat Sesh Challa, Dhruv Kumar</author><pubDate>Tue, 19 Sep 2023 16:29:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10694v1</guid></item><item><title>MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback</title><link>http://arxiv.org/abs/2309.10691v1</link><description>To solve complex tasks, large language models (LLMs) often require multiplerounds of interactions with the user, sometimes assisted by external tools.However, current evaluation paradigms often focus solely on benchmarkperformance with single-turn exchanges, neglecting the intricate interactionsamong the user, LLMs, and external tools, creating a discrepancy betweenbenchmark evaluation and real-world use cases. We introduce MINT benchmark toevaluate LLMs' ability to solve tasks with multi-turn interactions by (1) usingtools and (2) leveraging natural language feedback. To ensure reproducibility,we provide an evaluation framework where LLMs can access tools by executingPython code and receive natural language feedback from the user simulated withGPT-4. We repurpose a diverse set of established datasets and tasks focusing onreasoning, coding, and decision-making and carefully curate them into a compactsubset of instances for efficient evaluation. Our analysis of 20 open- andclosed-source LLMs offers intriguing findings. (1) LLMs generally benefit fromtool interactions and language feedback, with performance gains (absolute, samebelow) of 1--8% per additional turn with tool use and 2--17% with naturallanguage feedback. (2) Better single-turn performance does not guarantee bettermulti-turn performance. (3) Surprisingly, on LLMs we evaluated, we foundsupervised instruction-finetuning (SIFT) and reinforcement learning from humanfeedback (RLHF) generally hurt multi-turn capabilities. We hope MINT can helpmeasure progress and incentivize research in improving LLMs' capabilities inmulti-turn interactions, especially for open-source communities wheremulti-turn human evaluation has been less accessible compared to commercialLLMs with a larger user base.</description><author>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji</author><pubDate>Tue, 19 Sep 2023 16:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10691v1</guid></item><item><title>ReShader: View-Dependent Highlights for Single Image View-Synthesis</title><link>http://arxiv.org/abs/2309.10689v1</link><description>In recent years, novel view synthesis from a single image has seensignificant progress thanks to the rapid advancements in 3D scenerepresentation and image inpainting techniques. While the current approachesare able to synthesize geometrically consistent novel views, they often do nothandle the view-dependent effects properly. Specifically, the highlights intheir synthesized images usually appear to be glued to the surfaces, making thenovel views unrealistic. To address this major problem, we make a keyobservation that the process of synthesizing novel views requires changing theshading of the pixels based on the novel camera, and moving them to appropriatelocations. Therefore, we propose to split the view synthesis process into twoindependent tasks of pixel reshading and relocation. During the reshadingprocess, we take the single image as the input and adjust its shading based onthe novel camera. This reshaded image is then used as the input to an existingview synthesis method to relocate the pixels and produce the final novel viewimage. We propose to use a neural network to perform reshading and generate alarge set of synthetic input-reshaded pairs to train our network. Wedemonstrate that our approach produces plausible novel view images withrealistic moving highlights on a variety of real world scenes.</description><author>Avinash Paliwal, Brandon Nguyen, Andrii Tsarov, Nima Khademi Kalantari</author><pubDate>Tue, 19 Sep 2023 16:23:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10689v1</guid></item><item><title>On the different regimes of Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2309.10688v1</link><description>Modern deep networks are trained with stochastic gradient descent (SGD) whosekey parameters are the number of data considered at each step or batch size$B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$,SGD corresponds to a stochastic evolution of the parameters, whose noiseamplitude is governed by the `temperature' $T\equiv \eta/B$. Yet thisdescription is observed to break down for sufficiently large batches $B\geqB^*$, or simplifies to gradient descent (GD) when the temperature issufficiently small. Understanding where these cross-overs take place remains acentral challenge. Here we resolve these questions for a teacher-studentperceptron classification model, and show empirically that our key predictionsstill apply to deep networks. Specifically, we obtain a phase diagram in the$B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ anoise-dominated SGD governed by temperature, $\textit{(ii)}$ alarge-first-step-dominated SGD and $\textit{(iii)}$ GD. These different phasesalso corresponds to different regimes of generalization error. Remarkably, ouranalysis reveals that the batch size $B^*$ separating regimes $\textit{(i)}$and $\textit{(ii)}$ scale with the size $P$ of the training set, with anexponent that characterizes the hardness of the classification problem.</description><author>Antonio Sclocchi, Matthieu Wyart</author><pubDate>Tue, 19 Sep 2023 16:23:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10688v1</guid></item><item><title>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title><link>http://arxiv.org/abs/1910.10683v4</link><description>Transfer learning, where a model is first pre-trained on a data-rich taskbefore being fine-tuned on a downstream task, has emerged as a powerfultechnique in natural language processing (NLP). The effectiveness of transferlearning has given rise to a diversity of approaches, methodology, andpractice. In this paper, we explore the landscape of transfer learningtechniques for NLP by introducing a unified framework that converts alltext-based language problems into a text-to-text format. Our systematic studycompares pre-training objectives, architectures, unlabeled data sets, transferapproaches, and other factors on dozens of language understanding tasks. Bycombining the insights from our exploration with scale and our new ``ColossalClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarkscovering summarization, question answering, text classification, and more. Tofacilitate future work on transfer learning for NLP, we release our data set,pre-trained models, and code.</description><author>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu</author><pubDate>Tue, 19 Sep 2023 16:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1910.10683v4</guid></item><item><title>Is RobustBench/AutoAttack a suitable Benchmark for Adversarial Robustness?</title><link>http://arxiv.org/abs/2112.01601v3</link><description>Recently, RobustBench (Croce et al. 2020) has become a widely recognizedbenchmark for the adversarial robustness of image classification networks. Inits most commonly reported sub-task, RobustBench evaluates and ranks theadversarial robustness of trained neural networks on CIFAR10 under AutoAttack(Croce and Hein 2020b) with l-inf perturbations limited to eps = 8/255. Withleading scores of the currently best performing models of around 60% of thebaseline, it is fair to characterize this benchmark to be quite challenging.Despite its general acceptance in recent literature, we aim to fosterdiscussion about the suitability of RobustBench as a key indicator forrobustness which could be generalized to practical applications. Our line ofargumentation against this is two-fold and supported by excessive experimentspresented in this paper: We argue that I) the alternation of data by AutoAttackwith l-inf, eps = 8/255 is unrealistically strong, resulting in close toperfect detection rates of adversarial samples even by simple detectionalgorithms and human observers. We also show that other attack methods are muchharder to detect while achieving similar success rates. II) That results onlow-resolution data sets like CIFAR10 do not generalize well to higherresolution images as gradient-based attacks appear to become even moredetectable with increasing resolutions.</description><author>Peter Lorenz, Dominik Strassel, Margret Keuper, Janis Keuper</author><pubDate>Tue, 19 Sep 2023 16:11:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.01601v3</guid></item><item><title>Locally Stylized Neural Radiance Fields</title><link>http://arxiv.org/abs/2309.10684v1</link><description>In recent years, there has been increasing interest in applying stylizationon 3D scenes from a reference style image, in particular onto neural radiancefields (NeRF). While performing stylization directly on NeRF guaranteesappearance consistency over arbitrary novel views, it is a challenging problemto guide the transfer of patterns from the style image onto different parts ofthe NeRF scene. In this work, we propose a stylization framework for NeRF basedon local style transfer. In particular, we use a hash-grid encoding to learnthe embedding of the appearance and geometry components, and show that themapping defined by the hash table allows us to control the stylization to acertain extent. Stylization is then achieved by optimizing the appearancebranch while keeping the geometry branch fixed. To support local styletransfer, we propose a new loss function that utilizes a segmentation networkand bipartite matching to establish region correspondences between the styleimage and the content images obtained from volume rendering. Our experimentsshow that our method yields plausible stylization results with novel viewsynthesis while having flexible controllability via manipulating andcustomizing the region correspondences.</description><author>Hong-Wing Pang, Binh-Son Hua, Sai-Kit Yeung</author><pubDate>Tue, 19 Sep 2023 16:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10684v1</guid></item><item><title>Learning-Initialized Trajectory Planning in Unknown Environments</title><link>http://arxiv.org/abs/2309.10683v1</link><description>Autonomous flight in unknown environments requires precise planning for boththe spatial and temporal profiles of trajectories, which generally involvesnonconvex optimization, leading to high time costs and susceptibility to localoptima. To address these limitations, we introduce the Learning-InitializedTrajectory Planner (LIT-Planner), a novel approach that guides optimizationusing a Neural Network (NN) Planner to provide initial values. We firstleverage the spatial-temporal optimization with batch sampling to generatetraining cases, aiming to capture multimodality in trajectories. Based on thesedata, the NN-Planner maps visual and inertial observations to trajectoryparameters for handling unknown environments. The network outputs are thenoptimized to enhance both reliability and explainability, ensuring robustperformance. Furthermore, we propose a framework that supports robust onlinereplanning with tolerance to planning latency. Comprehensive simulationsvalidate the LIT-Planner's time efficiency without compromising trajectoryquality compared to optimization-based methods. Real-world experiments furtherdemonstrate its practical suitability for autonomous drone navigation.</description><author>Yicheng Chen, Jinjie Li, Wenyuan Qin, Yongzhao Hua, Xiwang Dong, Qingdong Li</author><pubDate>Tue, 19 Sep 2023 16:07:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10683v1</guid></item><item><title>Oracle Complexity Reduction for Model-free LQR: A Stochastic Variance-Reduced Policy Gradient Approach</title><link>http://arxiv.org/abs/2309.10679v1</link><description>We investigate the problem of learning an $\epsilon$-approximate solution forthe discrete-time Linear Quadratic Regulator (LQR) problem via a StochasticVariance-Reduced Policy Gradient (SVRPG) approach. Whilst policy gradientmethods have proven to converge linearly to the optimal solution of themodel-free LQR problem, the substantial requirement for two-point cost queriesin gradient estimations may be intractable, particularly in applications whereobtaining cost function evaluations at two distinct control inputconfigurations is exceptionally costly. To this end, we propose anoracle-efficient approach. Our method combines both one-point and two-pointestimations in a dual-loop variance-reduced algorithm. It achieves anapproximate optimal solution with only$O\left(\log\left(1/\epsilon\right)^{\beta}\right)$ two-point cost informationfor $\beta \in (0,1)$.</description><author>Leonardo F. Toso, Han Wang, James Anderson</author><pubDate>Tue, 19 Sep 2023 16:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10679v1</guid></item><item><title>Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation</title><link>http://arxiv.org/abs/2309.10677v1</link><description>Data contamination in model evaluation is getting increasingly prevalent asthe massive training corpora of large language models often unintentionallyinclude benchmark samples. Therefore, contamination analysis has became aninevitable part of reliable model evaluation. However, existing method ofcontamination analysis requires the access of the entire training data which isoften confidential for recent models. This prevent the community to rigorouslyaudit these models and conduct accurate assessment of their capability. In thispaper, we propose a novel method to quantify contamination without the accessof the full training set, that measure the extent of contamination withperplexity. Our analysis provides evidence of significant memorisation ofrecent foundation models in popular reading comprehension, summarisationbenchmarks, while multiple choice appears less contaminated.</description><author>Yucheng Li</author><pubDate>Tue, 19 Sep 2023 16:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10677v1</guid></item><item><title>Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification</title><link>http://arxiv.org/abs/2309.09976v2</link><description>Decision trees are widely used in machine learning due to their simplicity inconstruction and interpretability. However, as data sizes grow, traditionalmethods for constructing and retraining decision trees become increasinglyslow, scaling polynomially with the number of training examples. In this work,we introduce a novel quantum algorithm, named Des-q, for constructing andretraining decision trees in regression and binary classification tasks.Assuming the data stream produces small increments of new training examples, wedemonstrate that our Des-q algorithm significantly reduces the time requiredfor tree retraining, achieving a poly-logarithmic time complexity in the numberof training examples, even accounting for the time needed to load the newexamples into quantum-accessible memory. Our approach involves building adecision tree algorithm to perform k-piecewise linear tree splits at eachinternal node. These splits simultaneously generate multiple hyperplanes,dividing the feature space into k distinct regions. To determine the k suitableanchor points for these splits, we develop an efficient quantum-supervisedclustering method, building upon the q-means algorithm of Kerenidis et al.Des-q first efficiently estimates each feature weight using a novel quantumtechnique to estimate the Pearson correlation. Subsequently, we employ weighteddistance estimation to cluster the training examples in k disjoint regions andthen proceed to expand the tree using the same procedure. We benchmark theperformance of the simulated version of our algorithm against thestate-of-the-art classical decision tree for regression and binaryclassification on multiple data sets with numerical features. Further, weshowcase that the proposed algorithm exhibits similar performance to thestate-of-the-art decision tree while significantly speeding up the periodictree retraining.</description><author>Niraj Kumar, Romina Yalovetzky, Changhao Li, Pierre Minssen, Marco Pistoia</author><pubDate>Tue, 19 Sep 2023 16:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09976v2</guid></item><item><title>Detecting AutoAttack Perturbations in the Frequency Domain</title><link>http://arxiv.org/abs/2111.08785v2</link><description>Recently, adversarial attacks on image classification networks by theAutoAttack (Croce and Hein, 2020b) framework have drawn a lot of attention.While AutoAttack has shown a very high attack success rate, most defenseapproaches are focusing on network hardening and robustness enhancements, likeadversarial training. This way, the currently best-reported method canwithstand about 66% of adversarial examples on CIFAR10. In this paper, weinvestigate the spatial and frequency domain properties of AutoAttack andpropose an alternative defense. Instead of hardening a network, we detectadversarial attacks during inference, rejecting manipulated inputs. Based on arather simple and fast analysis in the frequency domain, we introduce twodifferent detection algorithms. First, a black box detector that only operateson the input images and achieves a detection accuracy of 100% on the AutoAttackCIFAR10 benchmark and 99.3% on ImageNet, for epsilon = 8/255 in both cases.Second, a whitebox detector using an analysis of CNN feature maps, leading to adetection rate of also 100% and 98.7% on the same benchmarks.</description><author>Peter Lorenz, Paula Harder, Dominik Strassel, Margret Keuper, Janis Keuper</author><pubDate>Tue, 19 Sep 2023 16:01:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.08785v2</guid></item><item><title>A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications</title><link>http://arxiv.org/abs/2308.16375v3</link><description>Graph Neural Networks (GNNs) have gained significant attention owing to theirability to handle graph-structured data and the improvement in practicalapplications. However, many of these models prioritize high utilityperformance, such as accuracy, with a lack of privacy consideration, which is amajor concern in modern society where privacy attacks are rampant. To addressthis issue, researchers have started to develop privacy-preserving GNNs.Despite this progress, there is a lack of a comprehensive overview of theattacks and the techniques for preserving privacy in the graph domain. In thissurvey, we aim to address this gap by summarizing the attacks on graph dataaccording to the targeted information, categorizing the privacy preservationtechniques in GNNs, and reviewing the datasets and applications that could beused for analyzing/solving privacy issues in GNNs. We also outline potentialdirections for future research in order to build better privacy-preservingGNNs.</description><author>Yi Zhang, Yuying Zhao, Zhaoqing Li, Xueqi Cheng, Yu Wang, Olivera Kotevska, Philip S. Yu, Tyler Derr</author><pubDate>Tue, 19 Sep 2023 16:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16375v3</guid></item><item><title>Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality</title><link>http://arxiv.org/abs/2307.02347v6</link><description>Diffusion models recently have been successfully applied for the visualsynthesis of strikingly realistic appearing images. This raises strong concernsabout their potential for malicious purposes. In this paper, we propose usingthe lightweight multi Local Intrinsic Dimensionality (multiLID), which has beenoriginally developed in context of the detection of adversarial examples, forthe automatic detection of synthetic images and the identification of theaccording generator networks. In contrast to many existing detectionapproaches, which often only work for GAN-generated images, the proposed methodprovides close to perfect detection results in many realistic use cases.Extensive experiments on known and newly created datasets demonstrate that theproposed multiLID approach exhibits superiority in diffusion detection andmodel identification. Since the empirical evaluations of recent publications onthe detection of generated images are often mainly focused on the"LSUN-Bedroom" dataset, we further establish a comprehensive benchmark for thedetection of diffusion-generated images, including samples from severaldiffusion models with different image sizes.</description><author>Peter Lorenz, Ricard Durall, Janis Keuper</author><pubDate>Tue, 19 Sep 2023 15:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02347v6</guid></item><item><title>BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture</title><link>http://arxiv.org/abs/2309.08036v2</link><description>This paper introduces the Budding Ensemble Architecture (BEA), a novelreduced ensemble architecture for anchor-based object detection models. Objectdetection models are crucial in vision-based tasks, particularly in autonomoussystems. They should provide precise bounding box detections while alsocalibrating their predicted confidence scores, leading to higher-qualityuncertainty estimates. However, current models may make erroneous decisions dueto false positives receiving high scores or true positives being discarded dueto low scores. BEA aims to address these issues. The proposed loss functions inBEA improve the confidence score calibration and lower the uncertainty error,which results in a better distinction of true and false positives and,eventually, higher accuracy of the object detection models. Both Base-YOLOv3and SSD models were enhanced using the BEA method and its proposed lossfunctions. The BEA on Base-YOLOv3 trained on the KITTI dataset results in a 6%and 3.7% increase in mAP and AP50, respectively. Utilizing a well-balanceduncertainty estimation threshold to discard samples in real-time even leads toa 9.6% higher AP50 than its base model. This is attributed to a 40% increase inthe area under the AP50-based retention curve used to measure the quality ofcalibration of confidence scores. Furthermore, BEA-YOLOV3 trained on KITTIprovides superior out-of-distribution detection on Citypersons, BDD100K, andCOCO datasets compared to the ensembles and vanilla models of YOLOv3 andGaussian-YOLOv3.</description><author>Syed Sha Qutub, Neslihan Kose, Rafael Rosales, Michael Paulitsch, Korbinian Hagn, Florian Geissler, Yang Peng, Gereon Hinz, Alois Knoll</author><pubDate>Tue, 19 Sep 2023 15:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08036v2</guid></item><item><title>Weakly-supervised positional contrastive learning: application to cirrhosis classification</title><link>http://arxiv.org/abs/2307.04617v3</link><description>Large medical imaging datasets can be cheaply and quickly annotated withlow-confidence, weak labels (e.g., radiological scores). Access tohigh-confidence labels, such as histology-based diagnoses, is rare and costly.Pretraining strategies, like contrastive learning (CL) methods, can leverageunlabeled or weakly-annotated datasets. These methods typically require largebatch sizes, which poses a difficulty in the case of large 3D images at fullresolution, due to limited GPU memory. Nevertheless, volumetric positionalinformation about the spatial context of each 2D slice can be very importantfor some medical applications. In this work, we propose an efficientweakly-supervised positional (WSP) contrastive learning strategy where weintegrate both the spatial context of each 2D slice and a weak label via ageneric kernel-based loss function. We illustrate our method on cirrhosisprediction using a large volume of weakly-labeled images, namely radiologicallow-confidence annotations, and small strongly-labeled (i.e., high-confidence)datasets. The proposed model improves the classification AUC by 5% with respectto a baseline model on our internal dataset, and by 26% on the public LIHCdataset from the Cancer Genome Atlas. The code is available at:https://github.com/Guerbet-AI/wsp-contrastive.</description><author>Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch</author><pubDate>Tue, 19 Sep 2023 15:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04617v3</guid></item><item><title>ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks</title><link>http://arxiv.org/abs/2306.06196v2</link><description>Electrocardiograms (ECGs) are commonly used by cardiologists to detectheart-related pathological conditions. Reliable collections of ECGs are crucialfor precise diagnosis. However, in clinical practice, the assignment ofcaptured ECG recordings to incorrect patients can occur inadvertently. Incollaboration with a clinical and research facility which recognized thischallenge and reached out to us, we present a study that addresses this issue.In this work, we propose a small and efficient neural-network based model fordetermining whether two ECGs originate from the same patient. Our modeldemonstrates great generalization capabilities and achieves state-of-the-artperformance in gallery-probe patient identification on PTB-XL while utilizing760x fewer parameters. Furthermore, we present a technique leveraging our modelfor detection of recording-assignment mistakes, showcasing its applicability ina realistic scenario. Finally, we evaluate our model on a newly collected ECGdataset specifically curated for this study, and make it public for theresearch community.</description><author>Michal Seják, Jakub Sido, David Žahour</author><pubDate>Tue, 19 Sep 2023 15:51:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06196v2</guid></item><item><title>Language Modeling Is Compression</title><link>http://arxiv.org/abs/2309.10668v1</link><description>It has long been established that predictive models can be transformed intolossless compressors and vice versa. Incidentally, in recent years, the machinelearning community has focused on training increasingly large and powerfulself-supervised (language) models. Since these large language models exhibitimpressive predictive capabilities, they are well-positioned to be strongcompressors. In this work, we advocate for viewing the prediction problemthrough the lens of compression and evaluate the compression capabilities oflarge (foundation) models. We show that large language models are powerfulgeneral-purpose predictors and that the compression viewpoint provides novelinsights into scaling laws, tokenization, and in-context learning. For example,Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to43.4% and LibriSpeech samples to 16.4% of their raw size, beatingdomain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively.Finally, we show that the prediction-compression equivalence allows us to useany compressor (like gzip) to build a conditional generative model.</description><author>Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, Joel Veness</author><pubDate>Tue, 19 Sep 2023 15:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10668v1</guid></item><item><title>Learning Tri-modal Embeddings for Zero-Shot Soundscape Mapping</title><link>http://arxiv.org/abs/2309.10667v1</link><description>We focus on the task of soundscape mapping, which involves predicting themost probable sounds that could be perceived at a particular geographiclocation. We utilise recent state-of-the-art models to encode geotagged audio,a textual description of the audio, and an overhead image of its capturelocation using contrastive pre-training. The end result is a shared embeddingspace for the three modalities, which enables the construction of soundscapemaps for any geographic region from textual or audio queries. Using theSoundingEarth dataset, we find that our approach significantly outperforms theexisting SOTA, with an improvement of image-to-audio Recall@100 from 0.256 to0.450. Our code is available at https://github.com/mvrl/geoclap.</description><author>Subash Khanal, Srikumar Sastry, Aayush Dhakal, Nathan Jacobs</author><pubDate>Tue, 19 Sep 2023 15:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10667v1</guid></item><item><title>NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages</title><link>http://arxiv.org/abs/2309.10661v1</link><description>Democratizing access to natural language processing (NLP) technology iscrucial, especially for underrepresented and extremely low-resource languages.Previous research has focused on developing labeled and unlabeled corpora forthese languages through online scraping and document translation. While thesemethods have proven effective and cost-efficient, we have identifiedlimitations in the resulting corpora, including a lack of lexical diversity andcultural relevance to local communities. To address this gap, we conduct a casestudy on Indonesian local languages. We compare the effectiveness of onlinescraping, human translation, and paragraph writing by native speakers inconstructing datasets. Our findings demonstrate that datasets generated throughparagraph writing by native speakers exhibit superior quality in terms oflexical diversity and cultural content. In addition, we present the\datasetname{} benchmark, encompassing 12 underrepresented and extremelylow-resource languages spoken by millions of individuals in Indonesia. Ourempirical experiment results using existing multilingual large language modelsconclude the need to extend these models to more underrepresented languages. Werelease the NusaWrites dataset at https://github.com/IndoNLP/nusa-writes.</description><author>Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Dea Adhista, Emmanuel Dave, Sarah Oktavianti, Salsabil Maulana Akbar, Jhonson Lee, Nuur Shadieq, Tjeng Wawan Cenggoro, Hanung Wahyuning Linuwih, Bryan Wilie, Galih Pradipta Muridan, Genta Indra Winata, David Moeljadi, Alham Fikri Aji, Ayu Purwarianti, Pascale Fung</author><pubDate>Tue, 19 Sep 2023 15:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10661v1</guid></item><item><title>Conformal Off-Policy Evaluation in Markov Decision Processes</title><link>http://arxiv.org/abs/2304.02574v2</link><description>Reinforcement Learning aims at identifying and evaluating efficient controlpolicies from data. In many real-world applications, the learner is not allowedto experiment and cannot gather data in an online manner (this is the case whenexperimenting is expensive, risky or unethical). For such applications, thereward of a given policy (the target policy) must be estimated using historicaldata gathered under a different policy (the behavior policy). Most methods forthis learning task, referred to as Off-Policy Evaluation (OPE), do not comewith accuracy and certainty guarantees. We present a novel OPE method based onConformal Prediction that outputs an interval containing the true reward of thetarget policy with a prescribed level of certainty. The main challenge in OPEstems from the distribution shift due to the discrepancies between the targetand the behavior policies. We propose and empirically evaluate different waysto deal with this shift. Some of these methods yield conformalized intervalswith reduced length compared to existing approaches, while maintaining the samecertainty level.</description><author>Daniele Foffano, Alessio Russo, Alexandre Proutiere</author><pubDate>Tue, 19 Sep 2023 15:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02574v2</guid></item><item><title>Implementing a new fully stepwise decomposition-based sampling technique for the hybrid water level forecasting model in real-world application</title><link>http://arxiv.org/abs/2309.10658v1</link><description>Various time variant non-stationary signals need to be pre-processed properlyin hydrological time series forecasting in real world, for example, predictionsof water level. Decomposition method is a good candidate and widely used insuch a pre-processing problem. However, decomposition methods with aninappropriate sampling technique may introduce future data which is notavailable in practical applications, and result in incorrectdecomposition-based forecasting models. In this work, a novel Fully StepwiseDecomposition-Based (FSDB) sampling technique is well designed for thedecomposition-based forecasting model, strictly avoiding introducing futureinformation. This sampling technique with decomposition methods, such asVariational Mode Decomposition (VMD) and Singular spectrum analysis (SSA), isapplied to predict water level time series in three different stations ofGuoyang and Chaohu basins in China. Results of VMD-based hybrid model usingFSDB sampling technique show that Nash-Sutcliffe Efficiency (NSE) coefficientis increased by 6.4%, 28.8% and 7.0% in three stations respectively, comparedwith those obtained from the currently most advanced sampling technique. In themeantime, for series of SSA-based experiments, NSE is increased by 3.2%, 3.1%and 1.1% respectively. We conclude that the newly developed FSDB samplingtechnique can be used to enhance the performance of decomposition-based hybridmodel in water level time series forecasting in real world.</description><author>Ziqian Zhang, Nana Bao, Xingting Yan, Aokai Zhu, Chenyang Li, Mingyu Liu</author><pubDate>Tue, 19 Sep 2023 15:40:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10658v1</guid></item><item><title>Learning Adaptive Safety for Multi-Agent Systems</title><link>http://arxiv.org/abs/2309.10657v1</link><description>Ensuring safety in dynamic multi-agent systems is challenging due to limitedinformation about the other agents. Control Barrier Functions (CBFs) areshowing promise for safety assurance but current methods make strongassumptions about other agents and often rely on manual tuning to balancesafety, feasibility, and performance. In this work, we delve into the problemof adaptive safe learning for multi-agent systems with CBF. We show howemergent behavior can be profoundly influenced by the CBF configuration,highlighting the necessity for a responsive and dynamic approach to CBF design.We present ASRL, a novel adaptive safe RL framework, to fully automate theoptimization of policy and CBF coefficients, to enhance safety and long-termperformance through reinforcement learning. By directly interacting with theother agents, ASRL learns to cope with diverse agent behaviours and maintainsthe cost violations below a desired limit. We evaluate ASRL in a multi-robotsystem and a competitive multi-agent racing scenario, against learning-basedand control-theoretic approaches. We empirically demonstrate the efficacy andflexibility of ASRL, and assess generalization and scalability toout-of-distribution scenarios. Code and supplementary material are publiconline.</description><author>Luigi Berducci, Shuo Yang, Rahul Mangharam, Radu Grosu</author><pubDate>Tue, 19 Sep 2023 15:39:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10657v1</guid></item><item><title>A spectrum of physics-informed Gaussian processes for regression in engineering</title><link>http://arxiv.org/abs/2309.10656v1</link><description>Despite the growing availability of sensing and data in general, we remainunable to fully characterise many in-service engineering systems and structuresfrom a purely data-driven approach. The vast data and resources available tocapture human activity are unmatched in our engineered world, and, even incases where data could be referred to as ``big,'' they will rarely holdinformation across operational windows or life spans. This paper pursues thecombination of machine learning technology and physics-based reasoning toenhance our ability to make predictive models with limited data. By explicitlylinking the physics-based view of stochastic processes with a data-basedregression approach, a spectrum of possible Gaussian process models areintroduced that enable the incorporation of different levels of expertknowledge of a system. Examples illustrate how these approaches cansignificantly reduce reliance on data collection whilst also increasing theinterpretability of the model, another important consideration in this context.</description><author>Elizabeth J Cross, Timothy J Rogers, Daniel J Pitchforth, Samuel J Gibson, Matthew R Jones</author><pubDate>Tue, 19 Sep 2023 15:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10656v1</guid></item><item><title>Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics</title><link>http://arxiv.org/abs/2302.00600v2</link><description>Coarse-grained (CG) molecular dynamics enables the study of biologicalprocesses at temporal and spatial scales that would be intractable at anatomistic resolution. However, accurately learning a CG force field remains achallenge. In this work, we leverage connections between score-based generativemodels, force fields and molecular dynamics to learn a CG force field withoutrequiring any force inputs during training. Specifically, we train a diffusiongenerative model on protein structures from molecular dynamics simulations, andwe show that its score function approximates a force field that can directly beused to simulate CG molecular dynamics. While having a vastly simplifiedtraining setup compared to previous work, we demonstrate that our approachleads to improved performance across several small- to medium-sized proteinsimulations, reproducing the CG equilibrium distribution, and preservingdynamics of all-atom simulations such as protein folding events.</description><author>Marloes Arts, Victor Garcia Satorras, Chin-Wei Huang, Daniel Zuegner, Marco Federici, Cecilia Clementi, Frank Noé, Robert Pinsler, Rianne van den Berg</author><pubDate>Tue, 19 Sep 2023 15:37:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00600v2</guid></item><item><title>MindAgent: Emergent Gaming Interaction</title><link>http://arxiv.org/abs/2309.09971v2</link><description>Large Language Models (LLMs) have the capacity of performing complexscheduling in a multi-agent system and can coordinate these agents intocompleting sophisticated tasks that require extensive collaboration. However,despite the introduction of numerous gaming frameworks, the community hasinsufficient benchmarks towards building general multi-agents collaborationinfrastructure that encompass both LLM and human-NPCs collaborations. In thiswork, we propose a novel infrastructure - MindAgent - to evaluate planning andcoordination emergent capabilities for gaming interaction. In particular, ourinfrastructure leverages existing gaming framework, to i) require understandingof the coordinator for a multi-agent system, ii) collaborate with human playersvia un-finetuned proper instructions, and iii) establish an in-context learningon few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a newgaming scenario and related benchmark that dispatch a multi-agent collaborationefficiency and supervise multiple agents playing the game simultaneously. Weconduct comprehensive evaluations with new auto-metric CoS for calculating thecollaboration efficiency. Finally, our infrastructure can be deployed intoreal-world gaming scenarios in a customized VR version of CUISINEWORLD andadapted in existing broader Minecraft gaming domain. We hope our findings onLLMs and the new infrastructure for general-purpose scheduling and coordinationcan help shed light on how such skills can be obtained by learning from largelanguage corpora.</description><author>Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao</author><pubDate>Tue, 19 Sep 2023 15:36:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09971v2</guid></item><item><title>CFGPT: Chinese Financial Assistant with Large Language Model</title><link>http://arxiv.org/abs/2309.10654v1</link><description>Large language models (LLMs) have demonstrated great potential in naturallanguage processing tasks within the financial domain. In this work, we presenta Chinese Financial Generative Pre-trained Transformer framework, named CFGPT,which includes a dataset~(CFData) for pre-training and supervised fine-tuning,a financial LLM~(CFLLM) to adeptly manage financial texts, and a deploymentframework~(CFAPP) designed to navigate real-world financial applications. TheCFData comprising both a pre-training dataset and a supervised fine-tuningdataset, where the pre-training dataset collates Chinese financial data andanalytics, alongside a smaller subset of general-purpose text with 584Mdocuments and 141B tokens in total, and the supervised fine-tuning dataset istailored for six distinct financial tasks, embodying various facets offinancial analysis and decision-making with 1.5M instruction pairs and 1.5Btokens in total. The CFLLM, which is based on InternLM-7B to balance the modelcapability and size, is trained on CFData in two stage, continued pre-trainingand supervised fine-tuning. The CFAPP is centered on large language models(LLMs) and augmented with additional modules to ensure multifacetedfunctionality in real-world application. Our codes are released athttps://github.com/TongjiFinLab/CFGPT.</description><author>Jiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun Ding, Changjun Jiang</author><pubDate>Tue, 19 Sep 2023 15:34:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10654v1</guid></item><item><title>Multi-Stain Self-Attention Graph Multiple Instance Learning Pipeline for Histopathology Whole Slide Images</title><link>http://arxiv.org/abs/2309.10650v1</link><description>Whole Slide Images (WSIs) present a challenging computer vision task due totheir gigapixel size and presence of numerous artefacts. Yet they are avaluable resource for patient diagnosis and stratification, often representingthe gold standard for diagnostic tasks. Real-world clinical datasets tend tocome as sets of heterogeneous WSIs with labels present at the patient-level,with poor to no annotations. Weakly supervised attention-based multipleinstance learning approaches have been developed in recent years to addressthese challenges, but can fail to resolve both long and short-rangedependencies. Here we propose an end-to-end multi-stain self-attention graph(MUSTANG) multiple instance learning pipeline, which is designed to solve aweakly-supervised gigapixel multi-image classification task, where the label isassigned at the patient-level, but no slide-level labels or region annotationsare available. The pipeline uses a self-attention based approach by restrictingthe operations to a highly sparse k-Nearest Neighbour Graph of embedded WSIpatches based on the Euclidean distance. We show this approach achieves astate-of-the-art F1-score/AUC of 0.89/0.92, outperforming the widely used CLAMmodel. Our approach is highly modular and can easily be modified to suitdifferent clinical datasets, as it only requires a patient-level label withoutannotations and accepts WSI sets of different sizes, as the graphs can be ofvarying sizes and structures. The source code can be found athttps://github.com/AmayaGS/MUSTANG.</description><author>Amaya Gallagher-Syed, Luca Rossi, Felice Rivellese, Costantino Pitzalis, Myles Lewis, Michael Barnes, Gregory Slabaugh</author><pubDate>Tue, 19 Sep 2023 15:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10650v1</guid></item><item><title>Cross-modal and Cross-domain Knowledge Transfer for Label-free 3D Segmentation</title><link>http://arxiv.org/abs/2309.10649v1</link><description>Current state-of-the-art point cloud-based perception methods usually rely onlarge-scale labeled data, which requires expensive manual annotations. Anatural option is to explore the unsupervised methodology for 3D perceptiontasks. However, such methods often face substantial performance-dropdifficulties. Fortunately, we found that there exist amounts of image-baseddatasets and an alternative can be proposed, i.e., transferring the knowledgein the 2D images to 3D point clouds. Specifically, we propose a novel approachfor the challenging cross-modal and cross-domain adaptation task by fullyexploring the relationship between images and point clouds and designingeffective feature alignment strategies. Without any 3D labels, our methodachieves state-of-the-art performance for 3D point cloud semantic segmentationon SemanticKITTI by using the knowledge of KITTI360 and GTA5, compared toexisting unsupervised and weakly-supervised baselines.</description><author>Jingyu Zhang, Huitong Yang, Daijie Wu, Xuesong Li, Xinge Zhu, Yuexin Ma</author><pubDate>Tue, 19 Sep 2023 15:29:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10649v1</guid></item><item><title>Self-Supervised Super-Resolution Approach for Isotropic Reconstruction of 3D Electron Microscopy Images from Anisotropic Acquisition</title><link>http://arxiv.org/abs/2309.10646v1</link><description>Three-dimensional electron microscopy (3DEM) is an essential technique toinvestigate volumetric tissue ultra-structure. Due to technical limitations andhigh imaging costs, samples are often imaged anisotropically, where resolutionin the axial direction ($z$) is lower than in the lateral directions $(x,y)$.This anisotropy 3DEM can hamper subsequent analysis and visualization tasks. Toovercome this limitation, we propose a novel deep-learning (DL)-basedself-supervised super-resolution approach that computationally reconstructsisotropic 3DEM from the anisotropic acquisition. The proposed DL-basedframework is built upon the U-shape architecture incorporatingvision-transformer (ViT) blocks, enabling high-capability learning of local andglobal multi-scale image dependencies. To train the tailored network, we employa self-supervised approach. Specifically, we generate pairs of anisotropic andisotropic training datasets from the given anisotropic 3DEM data. By feedingthe given anisotropic 3DEM dataset in the trained network through our proposedframework, the isotropic 3DEM is obtained. Importantly, this isotropicreconstruction approach relies solely on the given anisotropic 3DEM dataset anddoes not require pairs of co-registered anisotropic and isotropic 3DEM trainingdatasets. To evaluate the effectiveness of the proposed method, we conductedexperiments using three 3DEM datasets acquired from brain. The experimentalresults demonstrated that our proposed framework could successfully reconstructisotropic 3DEM from the anisotropic acquisition.</description><author>Mohammad Khateri, Morteza Ghahremani, Alejandra Sierra, Jussi Tohka</author><pubDate>Tue, 19 Sep 2023 15:28:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10646v1</guid></item><item><title>Towards Energy-Aware Federated Traffic Prediction for Cellular Networks</title><link>http://arxiv.org/abs/2309.10645v1</link><description>Cellular traffic prediction is a crucial activity for optimizing networks infifth-generation (5G) networks and beyond, as accurate forecasting is essentialfor intelligent network design, resource allocation and anomaly mitigation.Although machine learning (ML) is a promising approach to effectively predictnetwork traffic, the centralization of massive data in a single data centerraises issues regarding confidentiality, privacy and data transfer demands. Toaddress these challenges, federated learning (FL) emerges as an appealing MLtraining framework which offers high accurate predictions through paralleldistributed computations. However, the environmental impact of these methods isoften overlooked, which calls into question their sustainability. In thispaper, we address the trade-off between accuracy and energy consumption in FLby proposing a novel sustainability indicator that allows assessing thefeasibility of ML models. Then, we comprehensively evaluate state-of-the-artdeep learning (DL) architectures in a federated scenario using real-worldmeasurements from base station (BS) sites in the area of Barcelona, Spain. Ourfindings indicate that larger ML models achieve marginally improved performancebut have a significant environmental impact in terms of carbon footprint, whichmake them impractical for real-world applications.</description><author>Vasileios Perifanis, Nikolaos Pavlidis, Selim F. Yilmaz, Francesc Wilhelmi, Elia Guerra, Marco Miozzo, Pavlos S. Efraimidis, Paolo Dini, Remous-Aris Koutsiamanis</author><pubDate>Tue, 19 Sep 2023 15:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10645v1</guid></item><item><title>ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs</title><link>http://arxiv.org/abs/2305.03513v2</link><description>ChatGPT, as a recently launched large language model (LLM), has shownsuperior performance in various natural language processing (NLP) tasks.However, two major limitations hinder its potential applications: (1) theinflexibility of finetuning on downstream tasks and (2) the lack ofinterpretability in the decision-making process. To tackle these limitations,we propose a novel framework that leverages the power of ChatGPT for specifictasks, such as text classification, while improving its interpretability. Theproposed framework conducts a knowledge graph extraction task to extractrefined and structural knowledge from the raw data using ChatGPT. The richknowledge is then converted into a graph, which is further used to train aninterpretable linear classifier to make predictions. To evaluate theeffectiveness of our proposed method, we conduct experiments on four datasets.The result shows that our method can significantly improve the performancecompared to directly utilizing ChatGPT for text classification tasks. And ourmethod provides a more transparent decision-making process compared withprevious text classification methods.</description><author>Yucheng Shi, Hehuan Ma, Wenliang Zhong, Qiaoyu Tan, Gengchen Mai, Xiang Li, Tianming Liu, Junzhou Huang</author><pubDate>Tue, 19 Sep 2023 15:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03513v2</guid></item><item><title>KFC: Kinship Verification with Fair Contrastive Loss and Multi-Task Learning</title><link>http://arxiv.org/abs/2309.10641v1</link><description>Kinship verification is an emerging task in computer vision with multiplepotential applications. However, there's no large enough kinship dataset totrain a representative and robust model, which is a limitation for achievingbetter performance. Moreover, face verification is known to exhibit bias, whichhas not been dealt with by previous kinship verification works and sometimeseven results in serious issues. So we first combine existing kinship datasetsand label each identity with the correct race in order to take race informationinto consideration and provide a larger and complete dataset, called KinRacedataset. Secondly, we propose a multi-task learning model structure withattention module to enhance accuracy, which surpasses state-of-the-artperformance. Lastly, our fairness-aware contrastive loss function withadversarial learning greatly mitigates racial bias. We introduce a debias terminto traditional contrastive loss and implement gradient reverse in raceclassification task, which is an innovative idea to mix two fairness methods toalleviate bias. Exhaustive experimental evaluation demonstrates theeffectiveness and superior performance of the proposed KFC in both standarddeviation and accuracy at the same time.</description><author>Jia Luo Peng, Keng Wei Chang, Shang-Hong Lai</author><pubDate>Tue, 19 Sep 2023 15:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10641v1</guid></item><item><title>Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers</title><link>http://arxiv.org/abs/2309.10639v1</link><description>In this paper, we provide a geometric interpretation of the structure of DeepLearning (DL) networks, characterized by $L$ hidden layers, a ramp activationfunction, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) costfunction, and input and output spaces ${\mathbb R}^Q$ with equal dimension$Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well.We apply our recent results on shallow neural networks to construct an explicitfamily of minimizers for the global minimum of the cost function in the case$L\geq Q$, which we show to be degenerate. In the context presented here, thehidden layers of the DL network "curate" the training inputs by recursiveapplication of a truncation map that minimizes the noise to signal ratio of thetraining inputs. Moreover, we determine a set of $2^Q-1$ distinct degeneratelocal minima of the cost function.</description><author>Thomas Chen, Patricia Muñoz Ewald</author><pubDate>Tue, 19 Sep 2023 15:20:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10639v1</guid></item><item><title>Exploring the Influence of Information Entropy Change in Learning Systems</title><link>http://arxiv.org/abs/2309.10625v1</link><description>In this work, we explore the influence of entropy change in deep learningsystems by adding noise to the inputs/latent features. The applications in thispaper focus on deep learning tasks within computer vision, but the proposedtheory can be further applied to other fields. Noise is conventionally viewedas a harmful perturbation in various deep learning architectures, such asconvolutional neural networks (CNNs) and vision transformers (ViTs), as well asdifferent learning tasks like image classification and transfer learning.However, this paper aims to rethink whether the conventional proposition alwaysholds. We demonstrate that specific noise can boost the performance of variousdeep architectures under certain conditions. We theoretically prove theenhancement gained from positive noise by reducing the task complexity definedby information entropy and experimentally show the significant performance gainin large image datasets, such as the ImageNet. Herein, we use the informationentropy to define the complexity of the task. We categorize the noise into twotypes, positive noise (PN) and harmful noise (HN), based on whether the noisecan help reduce the complexity of the task. Extensive experiments of CNNs andViTs have shown performance improvements by proactively injecting positivenoise, where we achieved an unprecedented top 1 accuracy of over 95% onImageNet. Both theoretical analysis and empirical evidence have confirmed thatthe presence of positive noise can benefit the learning process, while thetraditionally perceived harmful noise indeed impairs deep learning models. Thedifferent roles of noise offer new explanations for deep models on specifictasks and provide a new paradigm for improving model performance. Moreover, itreminds us that we can influence the performance of learning systems viainformation entropy change.</description><author>Xiaowei Yu, Yao Xue, Lu Zhang, Li Wang, Tianming Liu, Dajiang Zhu</author><pubDate>Tue, 19 Sep 2023 15:04:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10625v1</guid></item><item><title>MultiTalent: A Multi-Dataset Approach to Medical Image Segmentation</title><link>http://arxiv.org/abs/2303.14444v2</link><description>The medical imaging community generates a wealth of datasets, many of whichare openly accessible and annotated for specific diseases and tasks such asmulti-organ or lesion segmentation. Current practices continue to limit modeltraining and supervised pre-training to one or a few similar datasets,neglecting the synergistic potential of other available annotated data. Wepropose MultiTalent, a method that leverages multiple CT datasets with diverseand conflicting class definitions to train a single model for a comprehensivestructure segmentation. Our results demonstrate improved segmentationperformance compared to previous related approaches, systematically, alsocompared to single dataset training using state-of-the-art methods, especiallyfor lesion segmentation and other challenging structures. We show thatMultiTalent also represents a powerful foundation model that offers a superiorpre-training for various segmentation tasks compared to commonly usedsupervised or unsupervised pre-training baselines. Our findings offer a newdirection for the medical imaging community to effectively utilize the wealthof available data for improved segmentation performance. The code and modelweights will be published here: [tba]</description><author>Constantin Ulrich, Fabian Isensee, Tassilo Wald, Maximilian Zenk, Michael Baumgartner, Klaus H. Maier-Hein</author><pubDate>Tue, 19 Sep 2023 15:03:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14444v2</guid></item><item><title>A multiobjective continuation method to compute the regularization path of deep neural networks</title><link>http://arxiv.org/abs/2308.12044v3</link><description>Sparsity is a highly desired feature in deep neural networks (DNNs) since itensures numerical efficiency, improves the interpretability of models (due tothe smaller number of relevant features), and robustness. In machine learningapproaches based on linear models, it is well known that there exists aconnecting path between the sparsest solution in terms of the $\ell^1$ norm(i.e., zero weights) and the non-regularized solution, which is called theregularization path. Very recently, there was a first attempt to extend theconcept of regularization paths to DNNs by means of treating the empirical lossand sparsity ($\ell^1$ norm) as two conflicting criteria and solving theresulting multiobjective optimization problem. However, due to thenon-smoothness of the $\ell^1$ norm and the high number of parameters, thisapproach is not very efficient from a computational perspective. To overcomethis limitation, we present an algorithm that allows for the approximation ofthe entire Pareto front for the above-mentioned objectives in a very efficientmanner. We present numerical examples using both deterministic and stochasticgradients. We furthermore demonstrate that knowledge of the regularization pathallows for a well-generalizing network parametrization.</description><author>Augustina C. Amakor, Konstantin Sonntag, Sebastian Peitz</author><pubDate>Tue, 19 Sep 2023 14:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12044v3</guid></item><item><title>Large language models can accurately predict searcher preferences</title><link>http://arxiv.org/abs/2309.10621v1</link><description>Relevance labels, which indicate whether a search result is valuable to asearcher, are key to evaluating and optimising search systems. The best way tocapture the true preferences of users is to ask them for their careful feedbackon which results would be useful, but this approach does not scale to produce alarge number of labels. Getting relevance labels at scale is usually done withthird-party labellers, who judge on behalf of the user, but there is a risk oflow-quality data if the labeller doesn't understand user needs. To improvequality, one standard approach is to study real users through interviews, userstudies and direct feedback, find areas where labels are systematicallydisagreeing with users, then educate labellers about user needs through judgingguidelines, training and monitoring. This paper introduces an alternateapproach for improving label quality. It takes careful feedback from realusers, which by definition is the highest-quality first-party gold data thatcan be derived, and develops an large language model prompt that agrees withthat data. We present ideas and observations from deploying language models forlarge-scale relevance labelling at Bing, and illustrate with data from TREC. Wehave found large language models can be effective, with accuracy as good ashuman labellers and similar capability to pick the hardest queries, best runs,and best groups. Systematic changes to the prompts make a difference inaccuracy, but so too do simple paraphrases. To measure agreement with realsearchers needs high-quality ``gold'' labels, but with these we find thatmodels produce better labels than third-party workers, for a fraction of thecost, and these labels let us train notably better rankers.</description><author>Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra</author><pubDate>Tue, 19 Sep 2023 14:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10621v1</guid></item><item><title>Deep Kernel Methods Learn Better: From Cards to Process Optimization</title><link>http://arxiv.org/abs/2303.14554v2</link><description>The ability of deep learning methods to perform classification and regressiontasks relies heavily on their capacity to uncover manifolds in high-dimensionaldata spaces and project them into low-dimensional representation spaces. Inthis study, we investigate the structure and character of the manifoldsgenerated by classical variational autoencoder (VAE) approaches and deep kernellearning (DKL). In the former case, the structure of the latent space isdetermined by the properties of the input data alone, while in the latter, thelatent manifold forms as a result of an active learning process that balancesthe data distribution and target functionalities. We show that DKL with activelearning can produce a more compact and smooth latent space which is moreconducive to optimization compared to previously reported methods, such as theVAE. We demonstrate this behavior using a simple cards data set and extend itto the optimization of domain-generated trajectories in physical systems. Ourfindings suggest that latent manifolds constructed through active learning havea more beneficial structure for optimization problems, especially infeature-rich target-poor scenarios that are common in domain sciences, such asmaterials synthesis, energy storage, and molecular discovery. The jupyternotebooks that encapsulate the complete analysis accompany the article.</description><author>Mani Valleti, Rama K. Vasudevan, Maxim A. Ziatdinov, Sergei V. Kalinin</author><pubDate>Tue, 19 Sep 2023 14:53:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14554v2</guid></item><item><title>Source-free Active Domain Adaptation for Diabetic Retinopathy Grading Based on Ultra-wide-field Fundus Image</title><link>http://arxiv.org/abs/2309.10619v1</link><description>Domain adaptation (DA) has been widely applied in the diabetic retinopathy(DR) grading of unannotated ultra-wide-field (UWF) fundus images, which cantransfer annotated knowledge from labeled color fundus images. However,suffering from huge domain gaps and complex real-world scenarios, the DRgrading performance of most mainstream DA is far from that of clinicaldiagnosis. To tackle this, we propose a novel source-free active domainadaptation (SFADA) in this paper. Specifically, we focus on DR grading problemitself and propose to generate features of color fundus images withcontinuously evolving relationships of DRs, actively select a few valuable UWFfundus images for labeling with local representation matching, and adapt modelon UWF fundus images with DR lesion prototypes. Notably, the SFADA also takesdata privacy and computational efficiency into consideration. Extensiveexperimental results demonstrate that our proposed SFADA achievesstate-of-the-art DR grading performance, increasing accuracy by 20.9% andquadratic weighted kappa by 18.63% compared with baseline and reaching 85.36%and 92.38% respectively. These investigations show that the potential of ourapproach for real clinical practice is promising.</description><author>Jinye Ran, Guanghua Zhang, Ximei Zhang, Juan Xie, Fan Xia, Hao Zhang</author><pubDate>Tue, 19 Sep 2023 14:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10619v1</guid></item><item><title>A Dynamic Linear Bias Incorporation Scheme for Nonnegative Latent Factor Analysis</title><link>http://arxiv.org/abs/2309.10618v1</link><description>High-Dimensional and Incomplete (HDI) data is commonly encountered in bigdata-related applications like social network services systems, which areconcerning the limited interactions among numerous nodes. Knowledge acquisitionfrom HDI data is a vital issue in the domain of data science due to theirembedded rich patterns like node behaviors, where the fundamental task is toperform HDI data representation learning. Nonnegative Latent Factor Analysis(NLFA) models have proven to possess the superiority to address this issue,where a linear bias incorporation (LBI) scheme is important in present thetraining overshooting and fluctuation, as well as preventing the model frompremature convergence. However, existing LBI schemes are all statistic oneswhere the linear biases are fixed, which significantly restricts thescalability of the resultant NLFA model and results in loss of representationlearning ability to HDI data. Motivated by the above discoveries, this paperinnovatively presents the dynamic linear bias incorporation (DLBI) scheme. Itfirstly extends the linear bias vectors into matrices, and then builds a binaryweight matrix to switch the active/inactive states of the linear biases. Theweight matrix's each entry switches between the binary states dynamicallycorresponding to the linear bias value variation, thereby establishing thedynamic linear biases for an NLFA model. Empirical studies on three HDIdatasets from real applications demonstrate that the proposed DLBI-based NLFAmodel obtains higher representation accuracy several than state-of-the-artmodels do, as well as highly-competitive computational efficiency.</description><author>Yurong Zhong, Zhe Xie, Weiling Li, Xin Luo</author><pubDate>Tue, 19 Sep 2023 14:48:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10618v1</guid></item><item><title>Intelligent Debris Mass Estimation Model for Autonomous Underwater Vehicle</title><link>http://arxiv.org/abs/2309.10617v1</link><description>Marine debris poses a significant threat to the survival of marine wildlife,often leading to entanglement and starvation, ultimately resulting in death.Therefore, removing debris from the ocean is crucial to restore the naturalbalance and allow marine life to thrive. Instance segmentation is an advancedform of object detection that identifies objects and precisely locates andseparates them, making it an essential tool for autonomous underwater vehicles(AUVs) to navigate and interact with their underwater environment effectively.AUVs use image segmentation to analyze images captured by their cameras tonavigate underwater environments. In this paper, we use instance segmentationto calculate the area of individual objects within an image, we use YOLOV7 inRoboflow to generate a set of bounding boxes for each object in the image witha class label and a confidence score for every detection. A segmentation maskis then created for each object by applying a binary mask to the object'sbounding box. The masks are generated by applying a binary threshold to theoutput of a convolutional neural network trained to segment objects from thebackground. Finally, refining the segmentation mask for each object is done byapplying post-processing techniques such as morphological operations andcontour detection, to improve the accuracy and quality of the mask. The processof estimating the area of instance segmentation involves calculating the areaof each segmented instance separately and then summing up the areas of allinstances to obtain the total area. The calculation is carried out usingstandard formulas based on the shape of the object, such as rectangles andcircles. In cases where the object is complex, the Monte Carlo method is usedto estimate the area. This method provides a higher degree of accuracy thantraditional methods, especially when using a large number of samples.</description><author>Mohana Sri S, Swethaa S, Aouthithiye Barathwaj SR Y, Sai Ganesh CS</author><pubDate>Tue, 19 Sep 2023 14:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10617v1</guid></item><item><title>Near-Optimal $Φ$-Regret Learning in Extensive-Form Games</title><link>http://arxiv.org/abs/2208.09747v3</link><description>In this paper, we establish efficient and uncoupled learning dynamics sothat, when employed by all players in multiplayer perfect-recallimperfect-information extensive-form games, the trigger regret of each playergrows as $O(\log T)$ after $T$ repetitions of play. This improves exponentiallyover the prior best known trigger-regret bound of $O(T^{1/4})$, and settles arecent open question by Bai et al. (2022). As an immediate consequence, weguarantee convergence to the set of extensive-form correlated equilibria andcoarse correlated equilibria at a near-optimal rate of $\frac{\log T}{T}$. Building on prior work, at the heart of our construction lies a more generalresult regarding fixed points deriving from rational functions with polynomialdegree, a property that we establish for the fixed points of (coarse) triggerdeviation functions. Moreover, our construction leverages a refined regretcircuit for the convex hull, which -- unlike prior guarantees -- preserves theRVU property introduced by Syrgkanis et al. (NIPS, 2015); this observation hasan independent interest in establishing near-optimal regret under learningdynamics based on a CFR-type decomposition of the regret.</description><author>Ioannis Anagnostides, Gabriele Farina, Tuomas Sandholm</author><pubDate>Tue, 19 Sep 2023 14:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.09747v3</guid></item><item><title>Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network</title><link>http://arxiv.org/abs/2309.03694v2</link><description>Short-term load forecasting is of paramount importance in the efficientoperation and planning of power systems, given its inherent non-linear anddynamic nature. Recent strides in deep learning have shown promise inaddressing this challenge. However, these methods often grapple withhyperparameter sensitivity, opaqueness in interpretability, and highcomputational overhead for real-time deployment. In this paper, I propose anovel solution that surmounts these obstacles. Our approach harnesses the powerof the Particle-Swarm Optimization algorithm to autonomously explore andoptimize hyperparameters, a Multi-Head Attention mechanism to discern thesalient features crucial for accurate forecasting, and a streamlined frameworkfor computational efficiency. Our method undergoes rigorous evaluation using agenuine electricity demand dataset. The results underscore its superiority interms of accuracy, robustness, and computational efficiency. Notably, our MeanAbsolute Percentage Error of 1.9376 marks a significant advancement overexisting state-of-the-art approaches, heralding a new era in short-term loadforecasting.</description><author>Paapa Kwesi Quansah, Edwin Kwesi Ansah Tenkorang</author><pubDate>Tue, 19 Sep 2023 14:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03694v2</guid></item></channel></rss>