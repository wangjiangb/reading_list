<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 06 Aug 2024 01:00:15 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs</title><link>http://arxiv.org/abs/2408.01355v2</link><description>Multi-modal Large Language Models (MLLMs) have demonstrated remarkableperformance on various visual-language understanding and generation tasks.However, MLLMs occasionally generate content inconsistent with the givenimages, which is known as "hallucination". Prior works primarily center onevaluating hallucination using standard, unperturbed benchmarks, which overlookthe prevalent occurrence of perturbed inputs in real-world scenarios-such asimage cropping or blurring-that are critical for a comprehensive assessment ofMLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,the first benchmark designed to evaluate Hallucination in MLLMs withinPerturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,containing 1,260 perturbed images from 11 object types. Each image isaccompanied by detailed annotations, which include fine-grained hallucinationtypes, such as existence, attribute, and relation. We equip these annotationswith a rich set of questions, making Hallu-PI suitable for both discriminativeand generative tasks. Extensive experiments on 12 mainstream MLLMs, such asGPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significanthallucinations on Hallu-PI, which is not observed in unperturbed scenarios.Furthermore, our research reveals a severe bias in MLLMs' ability to handledifferent types of hallucinations. We also design two baselines specificallyfor perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hopethat our study will bring researchers' attention to the limitations of MLLMswhen dealing with perturbed inputs, and spur further investigations to addressthis issue. Our code and datasets are publicly available athttps://github.com/NJUNLP/Hallu-PI.</description><author>Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen, Jiajun Chen, Shujian Huang</author><pubDate>Mon, 05 Aug 2024 02:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01355v2</guid></item><item><title>Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM Auto-Prompting</title><link>http://arxiv.org/abs/2408.01423v1</link><description>Large Language Models (LLMs) exhibit remarkable proficiency in addressing adiverse array of tasks within the Natural Language Processing (NLP) domain,with various prompt design strategies significantly augmenting theircapabilities. However, these prompts, while beneficial, each possess inherentlimitations. The primary prompt design methodologies are twofold: The first,exemplified by the Chain of Thought (CoT), involves manually crafting promptsspecific to individual datasets, hence termed Expert-Designed Prompts (EDPs).Once these prompts are established, they are unalterable, and theireffectiveness is capped by the expertise of the human designers. When appliedto LLMs, the static nature of EDPs results in a uniform approach to both simpleand complex problems within the same dataset, leading to the inefficient use oftokens for straightforward issues. The second method involves promptsautonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), whichprovide tailored solutions to specific problems, mitigating the limitations ofEDPs. However, LDPs may encounter a decline in performance when tacklingcomplex problems due to the potential for error accumulation during thesolution planning process. To address these challenges, we have conceived anovel Prompt Recursive Search (PRS) framework that leverages the LLM togenerate solutions specific to the problem, thereby conserving tokens. Theframework incorporates an assessment of problem complexity and an adjustablestructure, ensuring a reduction in the likelihood of errors. We havesubstantiated the efficacy of PRS framework through extensive experiments usingLLMs with different numbers of parameters across a spectrum of datasets invarious domains. Compared to the CoT method, the PRS method has increased theaccuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22%improvement.</description><author>Xiangyu Zhao, Chengqian Ma</author><pubDate>Fri, 02 Aug 2024 17:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01423v1</guid></item><item><title>A Survey on Data Selection for Language Models</title><link>http://arxiv.org/abs/2402.16827v3</link><description>A major factor in the recent success of large language models is the use ofenormous and ever-growing text datasets for unsupervised pre-training. However,naively training a model on all available data may not be optimal (orfeasible), as the quality of available text data can vary. Filtering out datacan also decrease the carbon footprint and financial costs of training modelsby reducing the amount of training required. Data selection methods aim todetermine which candidate data points to include in the training dataset andhow to appropriately sample from the selected data points. The promise ofimproved data selection methods has caused the volume of research in the areato rapidly expand. However, because deep learning is mostly driven by empiricalevidence and experimentation on large-scale data is expensive, feworganizations have the resources for extensive data selection research.Consequently, knowledge of effective data selection practices has becomeconcentrated within a few organizations, many of which do not openly sharetheir findings and methodologies. To narrow this gap in knowledge, we present acomprehensive review of existing literature on data selection methods andrelated research areas, providing a taxonomy of existing approaches. Bydescribing the current landscape of research, this work aims to accelerateprogress in data selection by establishing an entry point for new andestablished researchers. Additionally, throughout this review we draw attentionto noticeable holes in the literature and conclude the paper by proposingpromising avenues for future research.</description><author>Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang</author><pubDate>Fri, 02 Aug 2024 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16827v3</guid></item><item><title>Mission Impossible: A Statistical Perspective on Jailbreaking LLMs</title><link>http://arxiv.org/abs/2408.01420v1</link><description>Large language models (LLMs) are trained on a deluge of text data withlimited quality control. As a result, LLMs can exhibit unintended or evenharmful behaviours, such as leaking information, fake news or hate speech.Countermeasures, commonly referred to as preference alignment, includefine-tuning the pretrained LLMs with carefully crafted text examples of desiredbehaviour. Even then, empirical evidence shows preference aligned LLMs can beenticed to harmful behaviour. This so called jailbreaking of LLMs is typicallyachieved by adversarially modifying the input prompt to the LLM. Our paperprovides theoretical insights into the phenomenon of preference alignment andjailbreaking from a statistical perspective. Under our framework, we first showthat pretrained LLMs will mimic harmful behaviour if present in the trainingcorpus. Under that same framework, we then introduce a statistical notion ofalignment, and lower-bound the jailbreaking probability, showing that it isunpreventable under reasonable assumptions. Based on our insights, we proposean alteration to the currently prevalent alignment strategy RLHF. Specifically,we introduce a simple modification to the RLHF objective, we call E-RLHF, thataims to increase the likelihood of safe responses. E-RLHF brings no additionaltraining cost, and is compatible with other methods. Empirically, wedemonstrate that E-RLHF outperforms RLHF on all alignment problems put forwardby the AdvBench and HarmBench project without sacrificing model performance asmeasured by the MT-Bench project.</description><author>Jingtong Su, Julia Kempe, Karen Ullrich</author><pubDate>Fri, 02 Aug 2024 17:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01420v1</guid></item><item><title>DebateQA: Evaluating Question Answering on Debatable Knowledge</title><link>http://arxiv.org/abs/2408.01419v1</link><description>The rise of large language models (LLMs) has enabled us to seek answers toinherently debatable questions on LLM chatbots, necessitating a reliable way toevaluate their ability. However, traditional QA benchmarks assume fixed answersare inadequate for this purpose. To address this, we introduce DebateQA, adataset of 2,941 debatable questions, each accompanied by multiplehuman-annotated partial answers that capture a variety of perspectives. Wedevelop two metrics: Perspective Diversity, which evaluates thecomprehensiveness of perspectives, and Dispute Awareness, which assesses if theLLM acknowledges the question's debatable nature. Experiments demonstrate thatboth metrics align with human preferences and are stable across differentunderlying models. Using DebateQA with two metrics, we assess 12 popular LLMsand retrieval-augmented generation methods. Our findings reveal that while LLMsgenerally excel at recognizing debatable issues, their ability to providecomprehensive answers encompassing diverse perspectives varies considerably.</description><author>Rongwu Xu, Xuan Qi, Zehan Qi, Wei Xu, Zhijiang Guo</author><pubDate>Fri, 02 Aug 2024 17:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01419v1</guid></item><item><title>Gemma 2: Improving Open Language Models at a Practical Size</title><link>http://arxiv.org/abs/2408.00118v2</link><description>In this work, we introduce Gemma 2, a new addition to the Gemma family oflightweight, state-of-the-art open models, ranging in scale from 2 billion to27 billion parameters. In this new version, we apply several known technicalmodifications to the Transformer architecture, such as interleavinglocal-global attentions (Beltagy et al., 2020a) and group-query attention(Ainslie et al., 2023). We also train the 2B and 9B models with knowledgedistillation (Hinton et al., 2015) instead of next token prediction. Theresulting models deliver the best performance for their size, and even offercompetitive alternatives to models that are 2-3 times bigger. We release allour models to the community.</description><author>Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinb</author><pubDate>Fri, 02 Aug 2024 17:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00118v2</guid></item><item><title>Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs</title><link>http://arxiv.org/abs/2408.01417v1</link><description>Humans spontaneously use increasingly efficient language as interactionsprogress, by adapting and forming ad-hoc conventions. This phenomenon has beenstudied extensively using reference games, showing properties of human languagethat go beyond relaying intents. It remains unexplored whether multimodal largelanguage models (MLLMs) similarly increase communication efficiency duringinteractions, and what mechanisms they may adopt for this purpose. We introduceICCA, an automated framework to evaluate such conversational adaptation as anin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, andobserve that while they may understand the increasingly efficient language oftheir interlocutor, they do not spontaneously make their own language moreefficient over time. This latter ability can only be elicited in some models(e.g., GPT-4) with heavy-handed prompting. This shows that this property oflinguistic interaction does not arise from current training regimes, eventhough it is a common hallmark of human language. ICCA is available athttps://github.com/lil-lab/ICCA.</description><author>Yilun Hua, Yoav Artzi</author><pubDate>Fri, 02 Aug 2024 17:51:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01417v1</guid></item><item><title>The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability</title><link>http://arxiv.org/abs/2408.01416v1</link><description>Interpretability provides a toolset for understanding how and why neuralnetworks behave in certain ways. However, there is little unity in the field:most studies employ ad-hoc evaluations and do not share theoreticalfoundations, making it difficult to measure progress and compare the pros andcons of different techniques. Furthermore, while mechanistic understanding isfrequently discussed, the basic causal units underlying these mechanisms areoften not explicitly defined. In this paper, we propose a perspective oninterpretability research grounded in causal mediation analysis. Specifically,we describe the history and current state of interpretability taxonomizedaccording to the types of causal units (mediators) employed, as well as methodsused to search over mediators. We discuss the pros and cons of each mediator,providing insights as to when particular kinds of mediators and search methodsare most appropriate depending on the goals of a given study. We argue thatthis framing yields a more cohesive narrative of the field, as well asactionable insights for future work. Specifically, we recommend a focus ondiscovering new mediators with better trade-offs between human-interpretabilityand compute-efficiency, and which can uncover more sophisticated abstractionsfrom neural networks than the primarily linear mediators employed in currentwork. We also argue for more standardized evaluations that enable principledcomparisons across mediator types, such that we can better understand whenparticular causal units are better suited to particular use cases.</description><author>Aaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, Yonatan Belinkov</author><pubDate>Fri, 02 Aug 2024 17:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01416v1</guid></item><item><title>Conditional LoRA Parameter Generation</title><link>http://arxiv.org/abs/2408.01415v1</link><description>Generative models have achieved remarkable success in image, video, and textdomains. Inspired by this, researchers have explored utilizing generativemodels to generate neural network parameters. However, these efforts have beenlimited by the parameter size and the practicality of generatinghigh-performance parameters. In this paper, we propose COND P-DIFF, a novelapproach that demonstrates the feasibility of controllable high-performanceparameter generation, particularly for LoRA (Low-Rank Adaptation) weights,during the fine-tuning process. Specifically, we employ an autoencoder toextract efficient latent representations for parameters. We then train aconditional latent diffusion model to synthesize high-performing modelparameters from random noise based on specific task conditions. Experimentalresults in both computer vision and natural language processing domainsconsistently demonstrate that COND P-DIFF can generate high-performanceparameters conditioned on the given task. Moreover, we observe that theparameter distribution generated by COND P-DIFF exhibits differences comparedto the distribution obtained through normal optimization methods, indicating acertain level of generalization capability. Our work paves the way for furtherexploration of condition-driven parameter generation, offering a promisingdirection for task-specific adaptation of neural networks.</description><author>Xiaolong Jin, Kai Wang, Dongwen Tang, Wangbo Zhao, Yukun Zhou, Junshu Tang, Yang You</author><pubDate>Fri, 02 Aug 2024 17:43:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01415v1</guid></item><item><title>A Comprehensive Evaluation on Event Reasoning of Large Language Models</title><link>http://arxiv.org/abs/2404.17513v2</link><description>Event reasoning is a fundamental ability that underlies many applications. Itrequires event schema knowledge to perform global reasoning and needs to dealwith the diversity of the inter-event relations and the reasoning paradigms.How well LLMs accomplish event reasoning on various relations and reasoningparadigms remains unknown. To mitigate this disparity, we comprehensivelyevaluate the abilities of event reasoning of LLMs. We introduce a novelbenchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels ofevaluation of schema and instance and is comprehensive in relations andreasoning paradigms. We conduct extensive experiments on EV2. We find that LLMshave abilities to accomplish event reasoning but their performances are farfrom satisfactory. We also notice the imbalance of event reasoning abilities inLLMs. Besides, LLMs have event schema knowledge, however, they're not alignedwith humans on how to utilize the knowledge. Based on these findings, we guidethe LLMs in utilizing the event schema knowledge as memory leading toimprovements on event reasoning.</description><author>Zhengwei Tao, Zhi Jin, Yifan Zhang, Xiancai Chen, Haiyan Zhao, Jia Li, Bing Liang, Chongyang Tao, Qun Liu, Kam-Fai Wong</author><pubDate>Fri, 02 Aug 2024 17:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17513v2</guid></item><item><title>Derivation of Back-propagation for Graph Convolutional Networks using Matrix Calculus and its Application to Explainable Artificial Intelligence</title><link>http://arxiv.org/abs/2408.01408v1</link><description>This paper provides a comprehensive and detailed derivation of thebackpropagation algorithm for graph convolutional neural networks using matrixcalculus. The derivation is extended to include arbitrary element-wiseactivation functions and an arbitrary number of layers. The study addresses twofundamental problems, namely node classification and link prediction. Tovalidate our method, we compare it with reverse-mode automatic differentiation.The experimental results demonstrate that the median sum of squared errors ofthe updated weight matrices, when comparing our method to the approach usingreverse-mode automatic differentiation, falls within the range of $10^{-18}$ to$10^{-14}$. These outcomes are obtained from conducting experiments on afive-layer graph convolutional network, applied to a node classificationproblem on Zachary's karate club social network and a link prediction problemon a drug-drug interaction network. Finally, we show how the derivedclosed-form solution can facilitate the development of explainable AI andsensitivity analysis.</description><author>Yen-Che Hsiao, Rongting Yue, Abhishek Dutta</author><pubDate>Fri, 02 Aug 2024 17:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01408v1</guid></item><item><title>Doubly Robust Interval Estimation for Optimal Policy Evaluation in Online Learning</title><link>http://arxiv.org/abs/2110.15501v4</link><description>Evaluating the performance of an ongoing policy plays a vital role in manyareas such as medicine and economics, to provide crucial instructions on theearly-stop of the online experiment and timely feedback from the environment.Policy evaluation in online learning thus attracts increasing attention byinferring the mean outcome of the optimal policy (i.e., the value) inreal-time. Yet, such a problem is particularly challenging due to the dependentdata generated in the online environment, the unknown optimal policy, and thecomplex exploration and exploitation trade-off in the adaptive experiment. Inthis paper, we aim to overcome these difficulties in policy evaluation foronline learning. We explicitly derive the probability of exploration thatquantifies the probability of exploring non-optimal actions under commonly usedbandit algorithms. We use this probability to conduct valid inference on theonline conditional mean estimator under each action and develop the doublyrobust interval estimation (DREAM) method to infer the value under theestimated optimal policy in online learning. The proposed value estimatorprovides double protection for consistency and is asymptotically normal with aWald-type confidence interval provided. Extensive simulation studies and realdata applications are conducted to demonstrate the empirical validity of theproposed DREAM method.</description><author>Ye Shen, Hengrui Cai, Rui Song</author><pubDate>Fri, 02 Aug 2024 17:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.15501v4</guid></item><item><title>Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer</title><link>http://arxiv.org/abs/2408.01402v1</link><description>Decision Transformer (DT) has emerged as a promising class of algorithms inoffline reinforcement learning (RL) tasks, leveraging pre-collected datasetsand Transformer's capability to model long sequences. Recent works havedemonstrated that using parts of trajectories from training tasks as prompts inDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.However, collecting data from specific environments can be both costly andunsafe in many scenarios, leading to suboptimal performance and limitedfew-shot prompt abilities due to the data-hungry nature of Transformer-basedmodels. Additionally, the limited datasets used in pre-training make itchallenging for Prompt-DT type of methods to distinguish between various RLtasks through prompts alone. To address these challenges, we introduce theLanguage model-initialized Prompt Decision Transformer (LPDT), which leveragespre-trained language models for meta-RL tasks and fine-tunes the model usingLow-rank Adaptation (LoRA). We further incorporate prompt regularization toeffectively differentiate between tasks based on prompt featurerepresentations. Our approach integrates pre-trained language model and RLtasks seamlessly. Extensive empirical studies demonstrate that initializingwith a pre-trained language model significantly enhances the performance ofPrompt-DT on unseen tasks compared to baseline methods.</description><author>Yu Yang, Pan Xu</author><pubDate>Fri, 02 Aug 2024 17:25:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01402v1</guid></item><item><title>"A Good Bot Always Knows Its Limitations": Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence</title><link>http://arxiv.org/abs/2407.19631v2</link><description>How can intelligent machines assess their competencies in completing tasks?This question has come into focus for autonomous systems that algorithmicallyreason and make decisions under uncertainty. It is argued here that machineself-confidence - a form of meta-reasoning based on self-assessments of anagent's knowledge about the state of the world and itself, as well as itsability to reason about and execute tasks - leads to many eminently computableand useful competency indicators for such agents. This paper presents aculmination of work on this concept in the form of a computational frameworkcalled Factorized Machine Self-confidence (FaMSeC), which provides a holisticengineering-focused description of factors driving an algorithmicdecision-making process, including: outcome assessment, solver quality, modelquality, alignment quality, and past experience. In FaMSeC, self confidenceindicators are derived from hierarchical `problem-solving statistics' embeddedwithin broad classes of probabilistic decision-making algorithms such as Markovdecision processes. The problem-solving statistics are obtained by evaluatingand grading probabilistic exceedance margins with respect to given competencystandards, which are specified for each of the various decision-makingcompetency factors by the informee (e.g. a non-expert user or an expert systemdesigner). This approach allows `algorithmic goodness of fit' evaluations to beeasily incorporated into the design of many kinds of autonomous agents in theform of human-interpretable competency self-assessment reports. Detaileddescriptions and application examples for a Markov decision process agent showhow two of the FaMSeC factors (outcome assessment and solver quality) can becomputed and reported for a range of possible tasking contexts through noveluse of meta-utility functions, behavior simulations, and surrogate predictionmodels.</description><author>Brett Israelsen, Nisar R. Ahmed, Matthew Aitken, Eric W. Frew, Dale A. Lawrence, Brian M. Argrow</author><pubDate>Fri, 02 Aug 2024 17:10:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19631v2</guid></item><item><title>Improving Multilingual Neural Machine Translation by Utilizing Semantic and Linguistic Features</title><link>http://arxiv.org/abs/2408.01394v1</link><description>The many-to-many multilingual neural machine translation can be regarded asthe process of integrating semantic features from the source sentences andlinguistic features from the target sentences. To enhance zero-shottranslation, models need to share knowledge across languages, which can beachieved through auxiliary tasks for learning a universal representation orcross-lingual mapping. To this end, we propose to exploit both semantic andlinguistic features between multiple languages to enhance multilingualtranslation. On the encoder side, we introduce a disentangling learning taskthat aligns encoder representations by disentangling semantic and linguisticfeatures, thus facilitating knowledge transfer while preserving completeinformation. On the decoder side, we leverage a linguistic encoder to integratelow-level linguistic features to assist in the target language generation.Experimental results on multilingual datasets demonstrate significantimprovement in zero-shot translation compared to the baseline system, whilemaintaining performance in supervised translation. Further analysis validatesthe effectiveness of our method in leveraging both semantic and linguisticfeatures. The code is available at https://github.com/ictnlp/SemLing-MNMT.</description><author>Mengyu Bu, Shuhao Gu, Yang Feng</author><pubDate>Fri, 02 Aug 2024 17:10:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01394v1</guid></item><item><title>FT K-Means: A High-Performance K-Means on GPU with Fault Tolerance</title><link>http://arxiv.org/abs/2408.01391v1</link><description>K-Means is a widely used algorithm in clustering, however, its efficiency isprimarily constrained by the computational cost of distance computing. Existingimplementations suffer from suboptimal utilization of computational units andlack resilience against soft errors. To address these challenges, we introduceFT K-Means, a high-performance GPU-accelerated implementation of K-Means withonline fault tolerance. We first present a stepwise optimization strategy thatachieves competitive performance compared to NVIDIA's cuML library. We furtherimprove FT K-Means with a template-based code generation framework thatsupports different data types and adapts to different input shapes. A novelwarp-level tensor-core error correction scheme is proposed to address thefailure of existing fault tolerance methods due to memory asynchronizationduring copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100GPU demonstrate that FT K-Means without fault tolerance outperforms cuML'sK-Means implementation, showing a performance increase of 10\%-300\% inscenarios involving irregular data shapes. Moreover, the fault tolerancefeature of FT K-Means introduces only an overhead of 11\%, maintaining robustperformance even with tens of errors injected per second.</description><author>Shixun Wu, Yitong Ding, Yujia Zhai, Jinyang Liu, Jiajun Huang, Zizhe Jian, Huangliang Dai, Sheng Di, Bryan M. Wong, Zizhong Chen, Franck Cappello</author><pubDate>Fri, 02 Aug 2024 17:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01391v1</guid></item><item><title>Parallel Strategies for Best-First Generalized Planning</title><link>http://arxiv.org/abs/2407.21485v2</link><description>In recent years, there has been renewed interest in closing the performancegap between state-of-the-art planning solvers and generalized planning (GP), aresearch area of AI that studies the automated synthesis of algorithmic-likesolutions capable of solving multiple classical planning instances. One of thecurrent advancements has been the introduction of Best-First GeneralizedPlanning (BFGP), a GP algorithm based on a novel solution space that can beexplored with heuristic search, one of the foundations of modern planners. Thispaper evaluates the application of parallel search techniques to BFGP, anothercritical component in closing the performance gap. We first discuss why BFGP iswell suited for parallelization and some of its differentiating characteristicsfrom classical planners. Then, we propose two simple shared-memory parallelstrategies with good scaling with the number of cores.</description><author>Alejandro Fernández-Alburquerque, Javier Segovia-Aguas</author><pubDate>Fri, 02 Aug 2024 16:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21485v2</guid></item><item><title>NeuralBeta: Estimating Beta Using Deep Learning</title><link>http://arxiv.org/abs/2408.01387v1</link><description>Traditional approaches to estimating beta in finance often involve rigidassumptions and fail to adequately capture beta dynamics, limiting theireffectiveness in use cases like hedging. To address these limitations, we havedeveloped a novel method using neural networks called NeuralBeta, which iscapable of handling both univariate and multivariate scenarios and tracking thedynamic behavior of beta. To address the issue of interpretability, weintroduce a new output layer inspired by regularized weighted linearregression, which provides transparency into the model's decision-makingprocess. We conducted extensive experiments on both synthetic and market data,demonstrating NeuralBeta's superior performance compared to benchmark methodsacross various scenarios, especially instances where beta is highlytime-varying, e.g., during regime shifts in the market. This model not onlyrepresents an advancement in the field of beta estimation, but also showspotential for applications in other financial contexts that assume linearrelationships.</description><author>Yuxin Liu, Jimin Lin, Achintya Gopal</author><pubDate>Fri, 02 Aug 2024 16:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01387v1</guid></item><item><title>Learning Visual Quadrupedal Loco-Manipulation from Demonstrations</title><link>http://arxiv.org/abs/2403.20328v2</link><description>Quadruped robots are progressively being integrated into human environments.Despite the growing locomotion capabilities of quadrupedal robots, theirinteraction with objects in realistic scenes is still limited. While additionalrobotic arms on quadrupedal robots enable manipulating objects, they aresometimes redundant given that a quadruped robot is essentially a mobile unitequipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,we aim to empower a quadruped robot to execute real-world manipulation tasksusing only its legs. We decompose the loco-manipulation process into alow-level reinforcement learning (RL)-based controller and a high-levelBehavior Cloning (BC)-based planner. By parameterizing the manipulationtrajectory, we synchronize the efforts of the upper and lower layers, therebyleveraging the advantages of both RL and BC. Our approach is validated throughsimulations and real-world experiments, demonstrating the robot's ability toperform tasks that demand mobility and high precision, such as lifting a basketfrom the ground while moving, closing a dishwasher, pressing a button, andpushing a door. Project website: https://zhengmaohe.github.io/leg-manip</description><author>Zhengmao He, Kun Lei, Yanjie Ze, Koushil Sreenath, Zhongyu Li, Huazhe Xu</author><pubDate>Fri, 02 Aug 2024 16:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20328v2</guid></item><item><title>NOLO: Navigate Only Look Once</title><link>http://arxiv.org/abs/2408.01384v1</link><description>The in-context learning ability of Transformer models has brought newpossibilities to visual navigation. In this paper, we focus on the videonavigation setting, where an in-context navigation policy needs to be learnedpurely from videos in an offline manner, without access to the actualenvironment. For this setting, we propose Navigate Only Look Once (NOLO), amethod for learning a navigation policy that possesses the in-context abilityand adapts to new scenes by taking corresponding context videos as inputwithout finetuning or re-training. To enable learning from videos, we firstpropose a pseudo action labeling procedure using optical flow to recover theaction label from egocentric videos. Then, offline reinforcement learning isapplied to learn the navigation policy. Through extensive experiments ondifferent scenes, we show that our algorithm outperforms baselines by a largemargin, which demonstrates the in-context learning ability of the learnedpolicy.</description><author>Bohan Zhou, Jiangxing Wang, Zongqing Lu</author><pubDate>Fri, 02 Aug 2024 16:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01384v1</guid></item><item><title>Explaining a probabilistic prediction on the simplex with Shapley compositions</title><link>http://arxiv.org/abs/2408.01382v1</link><description>Originating in game theory, Shapley values are widely used for explaining amachine learning model's prediction by quantifying the contribution of eachfeature's value to the prediction. This requires a scalar prediction as inbinary classification, whereas a multiclass probabilistic prediction is adiscrete probability distribution, living on a multidimensional simplex. Insuch a multiclass setting the Shapley values are typically computed separatelyon each class in a one-vs-rest manner, ignoring the compositional nature of theoutput distribution. In this paper, we introduce Shapley compositions as awell-founded way to properly explain a multiclass probabilistic prediction,using the Aitchison geometry from compositional data analysis. We prove thatthe Shapley composition is the unique quantity satisfying linearity, symmetryand efficiency on the Aitchison simplex, extending the corresponding axiomaticproperties of the standard Shapley value. We demonstrate this proper multiclasstreatment in a range of scenarios.</description><author>Paul-Gauthier Noé, Miquel Perelló-Nieto, Jean-François Bonastre, Peter Flach</author><pubDate>Fri, 02 Aug 2024 16:40:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01382v1</guid></item><item><title>Coalitions of Large Language Models Increase the Robustness of AI Agents</title><link>http://arxiv.org/abs/2408.01380v1</link><description>The emergence of Large Language Models (LLMs) have fundamentally altered theway we interact with digital systems and have led to the pursuit of LLM poweredAI agents to assist in daily workflows. LLMs, whilst powerful and capable ofdemonstrating some emergent properties, are not logical reasoners and oftenstruggle to perform well at all sub-tasks carried out by an AI agent to planand execute a workflow. While existing studies tackle this lack of proficiencyby generalised pretraining at a huge scale or by specialised fine-tuning fortool use, we assess if a system comprising of a coalition of pretrained LLMs,each exhibiting specialised performance at individual sub-tasks, can match theperformance of single model agents. The coalition of models approach showcasesits potential for building robustness and reducing the operational costs ofthese AI agents by leveraging traits exhibited by specific models. Our findingsdemonstrate that fine-tuning can be mitigated by considering a coalition ofpretrained models and believe that this approach can be applied to othernon-agentic systems which utilise LLMs.</description><author>Prattyush Mangal, Carol Mak, Theo Kanakis, Timothy Donovan, Dave Braines, Edward Pyzer-Knapp</author><pubDate>Fri, 02 Aug 2024 16:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01380v1</guid></item><item><title>Resampling and averaging coordinates on data</title><link>http://arxiv.org/abs/2408.01379v1</link><description>We introduce algorithms for robustly computing intrinsic coordinates on pointclouds. Our approach relies on generating many candidate coordinates bysubsampling the data and varying hyperparameters of the embedding algorithm(e.g., manifold learning). We then identify a subset of representativeembeddings by clustering the collection of candidate coordinates and usingshape descriptors from topological data analysis. The final output is theembedding obtained as an average of the representative embeddings usinggeneralized Procrustes analysis. We validate our algorithm on both syntheticdata and experimental measurements from genomics, demonstrating robustness tonoise and outliers.</description><author>Andrew J. Blumberg, Mathieu Carriere, Jun Hou Fung, Michael A. Mandell</author><pubDate>Fri, 02 Aug 2024 16:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01379v1</guid></item><item><title>Adaptive Recruitment Resource Allocation to Improve Cohort Representativeness in Participatory Biomedical Datasets</title><link>http://arxiv.org/abs/2408.01375v1</link><description>Large participatory biomedical studies, studies that recruit individuals tojoin a dataset, are gaining popularity and investment, especially for analysisby modern AI methods. Because they purposively recruit participants, thesestudies are uniquely able to address a lack of historical representation, anissue that has affected many biomedical datasets. In this work, we definerepresentativeness as the similarity to a target population distribution of aset of attributes and our goal is to mirror the U.S. population acrossdistributions of age, gender, race, and ethnicity. Many participatory studiesrecruit at several institutions, so we introduce a computational approach toadaptively allocate recruitment resources among sites to improverepresentativeness. In simulated recruitment of 10,000-participant cohorts frommedical centers in the STAR Clinical Research Network, we show that ourapproach yields a more representative cohort than existing baselines. Thus, wehighlight the value of computational modeling in guiding recruitment efforts.</description><author>Victor Borza, Andrew Estornell, Ellen Wright Clayton, Chien-Ju Ho, Russell Rothman, Yevgeniy Vorobeychik, Bradley Malin</author><pubDate>Fri, 02 Aug 2024 16:32:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01375v1</guid></item><item><title>CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging</title><link>http://arxiv.org/abs/2407.11652v3</link><description>Federated Learning (FL) offers a privacy-preserving approach to train modelson decentralized data. Its potential in healthcare is significant, butchallenges arise due to cross-client variations in medical image data,exacerbated by limited annotations. This paper introduces Cross-ClientVariations Adaptive Federated Learning (CCVA-FL) to address these issues.CCVA-FL aims to minimize cross-client variations by transforming images into acommon feature space. It involves expert annotation of a subset of images fromeach client, followed by the selection of a client with the least datacomplexity as the target. Synthetic medical images are then generated usingScalable Diffusion Models with Transformers (DiT) based on the target client'sannotated images. These synthetic images, capturing diversity and representingthe original data, are shared with other clients. Each client then translatesits local images into the target image space using image-to-image translation.The translated images are subsequently used in a federated learning setting todevelop a server model. Our results demonstrate that CCVA-FL outperformsVanilla Federated Averaging by effectively addressing data distributiondifferences across clients without compromising privacy.</description><author>Sunny Gupta, Amit Sethi</author><pubDate>Fri, 02 Aug 2024 16:30:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11652v3</guid></item><item><title>Hybrid Coordinate Descent for Efficient Neural Network Learning Using Line Search and Gradient Descent</title><link>http://arxiv.org/abs/2408.01374v1</link><description>This paper presents a novel coordinate descent algorithm leveraging acombination of one-directional line search and gradient information forparameter updates for a squared error loss function. Each parameter undergoesupdates determined by either the line search or gradient method, contingentupon whether the modulus of the gradient of the loss with respect to thatparameter surpasses a predefined threshold. Notably, a larger threshold valueenhances algorithmic efficiency. Despite the potentially slower nature of theline search method relative to gradient descent, its parallelizabilityfacilitates computational time reduction. Experimental validation conducted ona 2-layer Rectified Linear Unit network with synthetic data elucidates theimpact of hyperparameters on convergence rates and computational efficiency.</description><author>Yen-Che Hsiao, Abhishek Dutta</author><pubDate>Fri, 02 Aug 2024 16:29:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01374v1</guid></item><item><title>Spatial-Spectral Morphological Mamba for Hyperspectral Image Classification</title><link>http://arxiv.org/abs/2408.01372v1</link><description>In recent years, Transformers have garnered significant attention forHyperspectral Image Classification (HSIC) due to their self-attentionmechanism, which provides strong classification performance. However, thesemodels face major challenges in computational efficiency, as their complexityincreases quadratically with the sequence length. The Mamba architecture,leveraging a State Space Model, offers a more efficient alternative toTransformers. This paper introduces the Spatial-Spectral Morphological Mamba(MorpMamba) model. In the MorpMamba model, a token generation module firstconverts the Hyperspectral Image (HSI) patch into spatial-spectral tokens.These tokens are then processed by a morphology block, which computesstructural and shape information using depthwise separable convolutionaloperations. The extracted information is enhanced in a feature enhancementmodule that adjusts the spatial and spectral tokens based on the center regionof the HSI sample, allowing for effective information fusion within each block.Subsequently, the tokens are refined in a multi-head self-attention block tofurther improve the feature space. Finally, the combined information is fedinto the state space block for classification and the creation of the groundtruth map. Experiments on widely used Hyperspectral (HS) datasets demonstratethat the MorpMamba model outperforms (parametric efficiency) both CNN andTransformer models.</description><author>Muhammad Ahmad, Muhammad Hassaan Farooq Butt, Muhammad Usama, Adil Mehmood Khan, Manual Mazzara, Salvatore Distenano</author><pubDate>Fri, 02 Aug 2024 16:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01372v1</guid></item><item><title>EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using Windowed Nonlinear Optimization</title><link>http://arxiv.org/abs/2408.01370v1</link><description>Event cameras are an interesting visual exteroceptive sensor that reacts tobrightness changes rather than integrating absolute image intensities. Owing tothis design, the sensor exhibits strong performance in situations ofchallenging dynamics and illumination conditions. While event-basedsimultaneous tracking and mapping remains a challenging problem, a number ofrecent works have pointed out the sensor's suitability for prior map-basedtracking. By making use of cross-modal registration paradigms, the camera'sego-motion can be tracked across a large spectrum of illumination and dynamicsconditions on top of accurate maps that have been created a priori by moretraditional sensors. The present paper follows up on a recently introducedevent-based geometric semi-dense tracking paradigm, and proposes the additionof inertial signals in order to robustify the estimation. More specifically,the added signals provide strong cues for pose initialization as well asregularization during windowed, multi-frame tracking. As a result, the proposedframework achieves increased performance under challenging illuminationconditions as well as a reduction of the rate at which intermediate eventrepresentations need to be registered in order to maintain stable trackingacross highly dynamic sequences. Our evaluation focuses on a diverse set ofreal world sequences and comprises a comparison of our proposed method againsta purely event-based alternative running at different rates.</description><author>Runze Yuan, Tao Liu, Zijia Dai, Yi-Fan Zuo, Laurent Kneip</author><pubDate>Fri, 02 Aug 2024 16:24:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01370v1</guid></item><item><title>Transformers are Universal In-context Learners</title><link>http://arxiv.org/abs/2408.01367v1</link><description>Transformers are deep architectures that define "in-context mappings" whichenable predicting new tokens based on a given set of tokens (such as a promptin NLP applications or a set of patches for vision transformers). This workstudies in particular the ability of these architectures to handle anarbitrarily large number of context tokens. To mathematically and uniformlyaddress the expressivity of these architectures, we consider the case that themappings are conditioned on a context represented by a probability distributionof tokens (discrete for a finite number of tokens). The related notion ofsmoothness corresponds to continuity in terms of the Wasserstein distancebetween these contexts. We demonstrate that deep transformers are universal andcan approximate continuous in-context mappings to arbitrary precision,uniformly over compact token domains. A key aspect of our results, compared toexisting findings, is that for a fixed precision, a single transformer canoperate on an arbitrary (even infinite) number of tokens. Additionally, itoperates with a fixed embedding dimension of tokens (this dimension does notincrease with precision) and a fixed number of heads (proportional to thedimension). The use of MLP layers between multi-head attention layers is alsoexplicitly controlled.</description><author>Takashi Furuya, Maarten V. de Hoop, Gabriel Peyré</author><pubDate>Fri, 02 Aug 2024 16:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01367v1</guid></item><item><title>Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation</title><link>http://arxiv.org/abs/2408.01366v1</link><description>Humans possess a remarkable talent for flexibly alternating to differentsenses when interacting with the environment. Picture a chef skillfully gaugingthe timing of ingredient additions and controlling the heat according to thecolors, sounds, and aromas, seamlessly navigating through every stage of thecomplex cooking process. This ability is founded upon a thorough comprehensionof task stages, as achieving the sub-goal within each stage can necessitate theutilization of different senses. In order to endow robots with similar ability,we incorporate the task stages divided by sub-goals into the imitation learningprocess to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, astage-guided dynamic multi-sensory fusion method with coarse-to-fine stageunderstanding, which dynamically adjusts the priority of modalities based onthe fine-grained state within the predicted current stage. We train a robotsystem equipped with visual, auditory, and tactile sensors to accomplishchallenging robotic manipulation tasks: pouring and peg insertion with keyway.Experimental results indicate that our approach enables more effective andexplainable dynamic fusion, aligning more closely with the human fusion processthan existing methods.</description><author>Ruoxuan Feng, Di Hu, Wenke Ma, Xuelong Li</author><pubDate>Fri, 02 Aug 2024 16:20:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01366v1</guid></item><item><title>Data Debugging is NP-hard for Classifiers Trained with SGD</title><link>http://arxiv.org/abs/2408.01365v1</link><description>Data debugging is to find a subset of the training data such that the modelobtained by retraining on the subset has a better accuracy. A bunch ofheuristic approaches are proposed, however, none of them are guaranteed tosolve this problem effectively. This leaves an open issue whether there existsan efficient algorithm to find the subset such that the model obtained byretraining on it has a better accuracy. To answer this open question andprovide theoretical basis for further study on developing better algorithms fordata debugging, we investigate the computational complexity of the problemnamed Debuggable. Given a machine learning model $\mathcal{M}$ obtained bytraining on dataset $D$ and a test instance$(\mathbf{x}_\text{test},y_\text{test})$ where$\mathcal{M}(\mathbf{x}_\text{test})\neq y_\text{test}$, Debuggable is todetermine whether there exists a subset $D^\prime$ of $D$ such that the model$\mathcal{M}^\prime$ obtained by retraining on $D^\prime$ satisfies$\mathcal{M}^\prime(\mathbf{x}_\text{test})=y_\text{test}$. To cover a widerange of commonly used models, we take SGD-trained linear classifier as themodel and derive the following main results. (1) If the loss function and thedimension of the model are not fixed, Debuggable is NP-complete regardless ofthe training order in which all the training samples are processed during SGD.(2) For hinge-like loss functions, a comprehensive analysis on thecomputational complexity of Debuggable is provided; (3) If the loss function isa linear function, Debuggable can be solved in linear time, that is, datadebugging can be solved easily in this case. These results not only highlightthe limitations of current approaches but also offer new insights into datadebugging.</description><author>Zizheng Guo, Pengyu Chen, Yanzhang Fu, Dongjing Miao</author><pubDate>Fri, 02 Aug 2024 16:17:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01365v1</guid></item><item><title>The Importance of Downstream Networks in Digital Pathology Foundation Models</title><link>http://arxiv.org/abs/2311.17804v3</link><description>Digital pathology has significantly advanced disease detection andpathologist efficiency through the analysis of gigapixel whole-slide images(WSI). In this process, WSIs are first divided into patches, for which afeature extractor model is applied to obtain feature vectors, which aresubsequently processed by an aggregation model to predict the respective WSIlabel. With the rapid evolution of representation learning, numerous newfeature extractor models, often termed foundational models, have emerged.Traditional evaluation methods rely on a static downstream aggregation modelsetup, encompassing a fixed architecture and hyperparameters, a practice weidentify as potentially biasing the results. Our study uncovers a sensitivityof feature extractor models towards aggregation model configurations,indicating that performance comparability can be skewed based on the chosenconfigurations. By accounting for this sensitivity, we find that theperformance of many current feature extractor models is notably similar. Wesupport this insight by evaluating seven feature extractor models across threedifferent datasets with 162 different aggregation model configurations. Thiscomprehensive approach provides a more nuanced understanding of the featureextractors' sensitivity to various aggregation model configurations, leading toa fairer and more accurate assessment of new foundation models in digitalpathology.</description><author>Gustav Bredell, Marcel Fischer, Przemyslaw Szostak, Samaneh Abbasi-Sureshjani, Alvaro Gomariz</author><pubDate>Fri, 02 Aug 2024 16:17:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17804v3</guid></item><item><title>Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation</title><link>http://arxiv.org/abs/2408.01363v1</link><description>Vision--Language Models (VLMs) have demonstrated success across diverseapplications, yet their potential to assist in relevance judgments remainsuncertain. This paper assesses the relevance estimation capabilities of VLMs,including CLIP, LLaVA, and GPT-4V, within a large-scale \textit{ad hoc}retrieval task tailored for multimedia content creation in a zero-shot fashion.Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,encompassing open-source and closed-source visual-instruction-tuned LargeLanguage Models (LLMs), achieve notable Kendall's $\tau \sim 0.4$ when comparedto human relevance judgments, surpassing the CLIPScore metric. (2) WhileCLIPScore is strongly preferred, LLMs are less biased towards CLIP-basedretrieval systems. (3) GPT-4V's score distribution aligns more closely withhuman judgments than other models, achieving a Cohen's $\kappa$ value of around0.08, which outperforms CLIPScore at approximately -0.096. These findingsunderscore the potential of LLM-powered VLMs in enhancing relevance judgments.</description><author>Jheng-Hong Yang, Jimmy Lin</author><pubDate>Fri, 02 Aug 2024 16:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01363v1</guid></item><item><title>Motion-aware Latent Diffusion Models for Video Frame Interpolation</title><link>http://arxiv.org/abs/2404.13534v3</link><description>With the advancement of AIGC, video frame interpolation (VFI) has become acrucial component in existing video generation frameworks, attractingwidespread research interest. For the VFI task, the motion estimation betweenneighboring frames plays a crucial role in avoiding motion ambiguity. However,existing VFI methods always struggle to accurately predict the motioninformation between consecutive frames, and this imprecise estimation leads toblurred and visually incoherent interpolated frames. In this paper, we proposea novel diffusion framework, motion-aware latent diffusion models (MADiff),which is specifically designed for the VFI task. By incorporating motion priorsbetween the conditional neighboring frames with the target interpolated framepredicted throughout the diffusion sampling procedure, MADiff progressivelyrefines the intermediate outcomes, culminating in generating both visuallysmooth and realistic results. Extensive experiments conducted on benchmarkdatasets demonstrate that our method achieves state-of-the-art performancesignificantly outperforming existing approaches, especially under challengingscenarios involving dynamic textures with complex motion.</description><author>Zhilin Huang, Yijie Yu, Ling Yang, Chujun Qin, Bing Zheng, Xiawu Zheng, Zikun Zhou, Yaowei Wang, Wenming Yang</author><pubDate>Fri, 02 Aug 2024 16:14:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13534v3</guid></item><item><title>Autoencoders in Function Space</title><link>http://arxiv.org/abs/2408.01362v1</link><description>Autoencoders have found widespread application, in both their originaldeterministic form and in their variational formulation (VAEs). In scientificapplications it is often of interest to consider data that are comprised offunctions; the same perspective is useful in image processing. In practice,discretisation (of differential equations arising in the sciences) orpixellation (of images) renders problems finite dimensional, but conceivingfirst of algorithms that operate on functions, and only then discretising orpixellating, leads to better algorithms that smoothly operate between differentlevels of discretisation or pixellation. In this paper function-space versionsof the autoencoder (FAE) and variational autoencoder (FVAE) are introduced,analysed, and deployed. Well-definedness of the objective function governingVAEs is a subtle issue, even in finite dimension, and more so on functionspace. The FVAE objective is well defined whenever the data distribution iscompatible with the chosen generative model; this happens, for example, whenthe data arise from a stochastic differential equation. The FAE objective isvalid much more broadly, and can be straightforwardly applied to data governedby differential equations. Pairing these objectives with neural operatorarchitectures, which can thus be evaluated on any mesh, enables newapplications of autoencoders to inpainting, superresolution, and generativemodelling of scientific data.</description><author>Justin Bunker, Mark Girolami, Hefin Lambley, Andrew M. Stuart, T. J. Sullivan</author><pubDate>Fri, 02 Aug 2024 16:13:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01362v1</guid></item><item><title>MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection</title><link>http://arxiv.org/abs/2404.18849v2</link><description>In real-world scenarios, using multiple modalities like visible (RGB) andinfrared (IR) can greatly improve the performance of a predictive task such asobject detection (OD). Multimodal learning is a common way to leverage thesemodalities, where multiple modality-specific encoders and a fusion module areused to improve performance. In this paper, we tackle a different way to employRGB and IR modalities, where only one modality or the other is observed by asingle shared vision encoder. This realistic setting requires a lower memoryfootprint and is more suitable for applications such as autonomous driving andsurveillance, which commonly rely on RGB and IR data. However, when learning asingle encoder on multiple modalities, one modality can dominate the other,producing uneven recognition results. This work investigates how to efficientlyleverage RGB and IR modalities to train a common transformer-based OD visionencoder, while countering the effects of modality imbalance. For this, weintroduce a novel training technique to Mix Patches (MiPa) from the twomodalities, in conjunction with a patch-wise modality agnostic module, forlearning a common representation of both modalities. Our experiments show thatMiPa can learn a representation to reach competitive results on traditionalRGB/IR benchmarks while only requiring a single modality during inference. Ourcode is available at: https://github.com/heitorrapela/MiPa.</description><author>Heitor R. Medeiros, David Latortue, Eric Granger, Marco Pedersoli</author><pubDate>Fri, 02 Aug 2024 16:13:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18849v2</guid></item><item><title>Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance</title><link>http://arxiv.org/abs/2406.04551v2</link><description>With the growing popularity of text-to-image generative models, there hasbeen increasing focus on understanding their risks and biases. Recent work hasfound that state-of-the-art models struggle to depict everyday objects with thetrue diversity of the real world and have notable gaps between geographicregions. In this work, we aim to increase the diversity of generated images ofcommon objects such that per-region variations are representative of the realworld. We introduce an inference time intervention, contextualized Vendi ScoreGuidance (c-VSG), that guides the backwards steps of latent diffusion models toincrease the diversity of a sample as compared to a "memory bank" of previouslygenerated images while constraining the amount of variation within that of anexemplar set of real-world contextualizing images. We evaluate c-VSG with twogeographically representative datasets and find that it substantially increasesthe diversity of generated images, both for the worst performing regions and onaverage, while simultaneously maintaining or improving image quality andconsistency. Additionally, qualitative analyses reveal that diversity ofgenerated images is significantly improved, including along the lines ofreductive region portrayals present in the original model. We hope that thiswork is a step towards text-to-image generative models that reflect the truegeographic diversity of the world.</description><author>Reyhane Askari Hemmat, Melissa Hall, Alicia Sun, Candace Ross, Michal Drozdzal, Adriana Romero-Soriano</author><pubDate>Fri, 02 Aug 2024 16:09:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04551v2</guid></item><item><title>A Reparameterized Discrete Diffusion Model for Text Generation</title><link>http://arxiv.org/abs/2302.05737v3</link><description>This work studies discrete diffusion probabilistic models with applicationsto natural language generation. We derive an alternative yet equivalentformulation of the sampling from discrete diffusion processes and leverage thisinsight to develop a family of reparameterized discrete diffusion models. Thederived generic framework is highly flexible, offers a fresh perspective of thegeneration process in discrete diffusion models, and features more effectivetraining and decoding techniques. We conduct extensive experiments to evaluatethe text generation capability of our model, demonstrating significantimprovements over existing diffusion models.</description><author>Lin Zheng, Jianbo Yuan, Lei Yu, Lingpeng Kong</author><pubDate>Fri, 02 Aug 2024 16:09:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05737v3</guid></item><item><title>Balanced Residual Distillation Learning for 3D Point Cloud Class-Incremental Semantic Segmentation</title><link>http://arxiv.org/abs/2408.01356v1</link><description>Class-incremental learning (CIL) thrives due to its success in processing theinflux of information by learning from continuously added new classes whilepreventing catastrophic forgetting about the old ones. It is essential for theperformance breakthrough of CIL to effectively refine past knowledge from thebase model and balance it with new learning. However, such an issue has not yetbeen considered in current research. In this work, we explore the potential ofCIL from these perspectives and propose a novel balanced residual distillationframework (BRD-CIL) to push the performance bar of CIL to a new higher level.Specifically, BRD-CIL designs a residual distillation learning strategy, whichcan dynamically expand the network structure to capture the residuals betweenthe base and target models, effectively refining the past knowledge.Furthermore, BRD-CIL designs a balanced pseudo-label learning strategy bygenerating a guidance mask to reduce the preference for old classes, ensuringbalanced learning from new and old classes. We apply the proposed BRD-CIL to achallenging 3D point cloud semantic segmentation task where the data areunordered and unstructured. Extensive experimental results demonstrate thatBRD-CIL sets a new benchmark with an outstanding balance capability inclass-biased scenarios.</description><author>Yuanzhi Su, Siyuan Chen, Yuan-Gen Wang</author><pubDate>Fri, 02 Aug 2024 16:09:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01356v1</guid></item><item><title>Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs</title><link>http://arxiv.org/abs/2408.01355v1</link><description>Multi-modal Large Language Models (MLLMs) have demonstrated remarkableperformance on various visual-language understanding and generation tasks.However, MLLMs occasionally generate content inconsistent with the givenimages, which is known as "hallucination". Prior works primarily center onevaluating hallucination using standard, unperturbed benchmarks, which overlookthe prevalent occurrence of perturbed inputs in real-world scenarios-such asimage cropping or blurring-that are critical for a comprehensive assessment ofMLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,the first benchmark designed to evaluate Hallucination in MLLMs withinPerturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,containing 1,260 perturbed images from 11 object types. Each image isaccompanied by detailed annotations, which include fine-grained hallucinationtypes, such as existence, attribute, and relation. We equip these annotationswith a rich set of questions, making Hallu-PI suitable for both discriminativeand generative tasks. Extensive experiments on 12 mainstream MLLMs, such asGPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significanthallucinations on Hallu-PI, which is not observed in unperturbed scenarios.Furthermore, our research reveals a severe bias in MLLMs' ability to handledifferent types of hallucinations. We also design two baselines specificallyfor perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hopethat our study will bring researchers' attention to the limitations of MLLMswhen dealing with perturbed inputs, and spur further investigations to addressthis issue. Our code and datasets are publicly available athttps://github.com/NJUNLP/Hallu-PI.</description><author>Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen, Jiajun Chen, Shujian Huang</author><pubDate>Fri, 02 Aug 2024 16:07:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01355v1</guid></item><item><title>PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy Correspondence Learning in Cross-Modal Retrieval</title><link>http://arxiv.org/abs/2408.01349v1</link><description>In the realm of cross-modal retrieval, seamlessly integrating diversemodalities within multimedia remains a formidable challenge, especially giventhe complexities introduced by noisy correspondence learning (NCL). Such noiseoften stems from mismatched data pairs, which is a significant obstacledistinct from traditional noisy labels. This paper introducesPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to addressthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes anauxiliary "pseudo-classification" task that interprets captions as categoricallabels, steering the model to learn image-text semantic similarity through anon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,capitalizing on PC$^2$'s pseudo-classification capability, we generatepseudo-captions to provide more informative and tangible supervision for eachmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowedto assistant the correction of correspondence. In addition to technicalcontributions, we develop a realistic NCL dataset called Noise of Web (NoW),which could be a new powerful NCL benchmark where noise exists naturally.Empirical evaluations of PC$^2$ showcase marked improvements over existingstate-of-the-art robust cross-modal retrieval techniques on both simulated andrealistic datasets with various NCL settings. The contributed dataset andsource code are released at https://github.com/alipay/PC2-NoiseofWeb.</description><author>Yue Duan, Zhangxuan Gu, Zhenzhe Ying, Lei Qi, Changhua Meng, Yinghuan Shi</author><pubDate>Fri, 02 Aug 2024 15:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01349v1</guid></item><item><title>A multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers</title><link>http://arxiv.org/abs/2403.13940v2</link><description>Counterfactuals are widely used to explain ML model predictions by providingalternative scenarios for obtaining the more desired predictions. They can begenerated by a variety of methods that optimize different, sometimesconflicting, quality measures and produce quite different solutions. However,choosing the most appropriate explanation method and one of the generatedcounterfactuals is not an easy task. Instead of forcing the user to test manydifferent explanation methods and analysing conflicting solutions, in thispaper, we propose to use a multi-stage ensemble approach that will selectsingle counterfactual based on the multiple-criteria analysis. It offers acompromise solution that scores well on several popular quality measures. Thisapproach exploits the dominance relation and the ideal point decision aidmethod, which selects one counterfactual from the Pareto front. The conductedexperiments demonstrated that the proposed approach generates fully actionablecounterfactuals with attractive compromise values of the considered qualitymeasures.</description><author>Ignacy Stępka, Mateusz Lango, Jerzy Stefanowski</author><pubDate>Fri, 02 Aug 2024 15:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13940v2</guid></item><item><title>Prompt Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social Science Tasks</title><link>http://arxiv.org/abs/2408.01346v1</link><description>Large Language Models are expressive tools that enable complex tasks of textunderstanding within Computational Social Science. Their versatility, whilebeneficial, poses a barrier for establishing standardized best practices withinthe field. To bring clarity on the values of different strategies, we presentan overview of the performance of modern LLM-based classification methods on abenchmark of 23 social knowledge tasks. Our results point to three bestpractices: select models with larger vocabulary and pre-training corpora; avoidsimple zero-shot in favor of AI-enhanced prompting; fine-tune on task-specificdata, and consider more complex forms instruction-tuning on multiple datasetsonly when only training data is more abundant.</description><author>Anders Giovanni Møller, Luca Maria Aiello</author><pubDate>Fri, 02 Aug 2024 15:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01346v1</guid></item><item><title>Accurate and Efficient Event-based Semantic Segmentation Using Adaptive Spiking Encoder-Decoder Network</title><link>http://arxiv.org/abs/2304.11857v3</link><description>Spiking neural networks (SNNs), known for their low-power, event-drivencomputation and intrinsic temporal dynamics, are emerging as promisingsolutions for processing dynamic, asynchronous signals from event-basedsensors. Despite their potential, SNNs face challenges in training andarchitectural design, resulting in limited performance in challengingevent-based dense prediction tasks compared to artificial neural networks(ANNs). In this work, we develop an efficient spiking encoder-decoder network(SpikingEDN) for large-scale event-based semantic segmentation tasks. Toenhance the learning efficiency from dynamic event streams, we harness theadaptive threshold which improves network accuracy, sparsity and robustness instreaming inference. Moreover, we develop a dual-path SpikingSpatially-Adaptive Modulation module, which is specifically tailored to enhancethe representation of sparse events and multi-modal inputs, therebyconsiderably improving network performance. Our SpikingEDN attains a meanintersection over union (MIoU) of 72.57\% on the DDD17 dataset and 58.32\% onthe larger DSEC-Semantic dataset, showing competitive results to thestate-of-the-art ANNs while requiring substantially fewer computationalresources. Our results shed light on the untapped potential of SNNs inevent-based vision applications. The source code will be made publiclyavailable.</description><author>Rui Zhang, Luziwei Leng, Kaiwei Che, Hu Zhang, Jie Cheng, Qinghai Guo, Jiangxing Liao, Ran Cheng</author><pubDate>Fri, 02 Aug 2024 15:43:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11857v3</guid></item><item><title>StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation</title><link>http://arxiv.org/abs/2408.01343v1</link><description>Multimodal semantic segmentation shows significant potential for enhancingsegmentation accuracy in complex scenes. However, current methods oftenincorporate specialized feature fusion modules tailored to specific modalities,thereby restricting input flexibility and increasing the number of trainingparameters. To address these challenges, we propose StitchFusion, astraightforward yet effective modal fusion framework that integrateslarge-scale pre-trained models directly as encoders and feature fusers. Thisapproach facilitates comprehensive multi-modal and multi-scale feature fusion,accommodating any visual modal inputs. Specifically, Our framework achievesmodal integration during encoding by sharing multi-modal visual information. Toenhance information exchange across modalities, we introduce amulti-directional adapter module (MultiAdapter) to enable cross-modalinformation transfer during encoding. By leveraging MultiAdapter to propagatemulti-scale information across pre-trained encoders during the encodingprocess, StitchFusion achieves multi-modal visual information integrationduring encoding. Extensive comparative experiments demonstrate that our modelachieves state-of-the-art performance on four multi-modal segmentation datasetswith minimal additional parameters. Furthermore, the experimental integrationof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights theircomplementary nature. Our code is available at StitchFusion_repo.</description><author>Bingyu Li, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li</author><pubDate>Fri, 02 Aug 2024 15:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01343v1</guid></item><item><title>Leveraging Knowledge Graph Embedding for Effective Conversational Recommendation</title><link>http://arxiv.org/abs/2408.01342v1</link><description>Conversational recommender system (CRS), which combines the techniques ofdialogue system and recommender system, has obtained increasing interestrecently. In contrast to traditional recommender system, it learns the userpreference better through interactions (i.e. conversations), and then furtherboosts the recommendation performance. However, existing studies on CRS ignoreto address the relationship among attributes, users, and items effectively,which might lead to inappropriate questions and inaccurate recommendations. Inthis view, we propose a knowledge graph based conversational recommender system(referred as KG-CRS). Specifically, we first integrate the user-item graph anditem-attribute graph into a dynamic graph, i.e., dynamically changing duringthe dialogue process by removing negative items or attributes. We then learninformative embedding of users, items, and attributes by also consideringpropagation through neighbors on the graph. Extensive experiments on three realdatasets validate the superiority of our method over the state-of-the-artapproaches in terms of both the recommendation and conversation tasks.</description><author>Yunwen Xia, Hui Fang, Jie Zhang, Chong Long</author><pubDate>Fri, 02 Aug 2024 15:38:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01342v1</guid></item><item><title>A Hyperparameter Study for Quantum Kernel Methods</title><link>http://arxiv.org/abs/2310.11891v3</link><description>Quantum kernel methods are a promising method in quantum machine learningthanks to the guarantees connected to them. Their accessibility for analyticconsiderations also opens up the possibility of prescreening datasets based ontheir potential for a quantum advantage. To do so, earlier works developed thegeometric difference, which can be understood as a closeness measure betweentwo kernel-based machine learning approaches, most importantly between aquantum kernel and a classical kernel. This metric links the quantum andclassical model complexities, and it was developed to bound generalizationerror. Therefore, it raises the question of how this metric behaves in anempirical setting. In this work, we investigate the effects of hyperparameterchoice on the model performance and the generalization gap between classicaland quantum kernels. The importance of hyperparameters is well known also forclassical machine learning. Of special interest are hyperparameters associatedwith the quantum Hamiltonian evolution feature map, as well as the number ofqubits to trace out before computing a projected quantum kernel. We conduct athorough investigation of the hyperparameters across 11 datasets and weidentify certain aspects that can be exploited. Analyzing the effects ofcertain hyperparameter settings on the empirical performance, as measured bycross validation accuracy, and generalization ability, as measured by geometricdifference described above, brings us one step closer to understanding thepotential of quantum kernel methods on classical datasets.</description><author>Sebastian Egginger, Alona Sakhnenko, Jeanette Miriam Lorenz</author><pubDate>Fri, 02 Aug 2024 15:38:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11891v3</guid></item><item><title>Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services</title><link>http://arxiv.org/abs/2407.00110v2</link><description>The widespread adoption of large language models (LLMs) has created apressing need for an efficient, secure and private serving infrastructure,which allows researchers to run open source or custom fine-tuned LLMs andensures users that their data remains private and is not stored without theirconsent. While high-performance computing (HPC) systems equipped withstate-of-the-art GPUs are well-suited for training LLMs, their batch schedulingparadigm is not designed to support real-time serving of AI applications. Cloudsystems, on the other hand, are well suited for web services but commonly lackaccess to the computational power of HPC clusters, especially expensive andscarce high-end GPUs, which are required for optimal inference speed. Wepropose an architecture with an implementation consisting of a web service thatruns on a cloud VM with secure access to a scalable backend running a multitudeof LLM models on HPC systems. By offering a web service using our HPCinfrastructure to host LLMs, we leverage the trusted environment of localuniversities and research centers to offer a private and secure alternative tocommercial LLM services. Our solution natively integrates with the HPC batchscheduler Slurm, enabling seamless deployment on HPC clusters, and is able torun side by side with regular Slurm workloads, while utilizing gaps in theschedule created by Slurm. In order to ensure the security of the HPC system,we use the SSH ForceCommand directive to construct a robust circuit breaker,which prevents successful attacks on the web-facing server from affecting thecluster. We have successfully deployed our system as a production service, andmade the source code available at \url{https://github.com/gwdg/chat-ai}</description><author>Ali Doosthosseini, Jonathan Decker, Hendrik Nolte, Julian M. Kunkel</author><pubDate>Fri, 02 Aug 2024 15:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00110v2</guid></item><item><title>MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models</title><link>http://arxiv.org/abs/2408.01337v1</link><description>Multimodal models that jointly process audio and language hold great promisein audio understanding and are increasingly being adopted in the music domain.By allowing users to query via text and obtain information about a given audioinput, these models have the potential to enable a variety of musicunderstanding tasks via language-based interfaces. However, their evaluationposes considerable challenges, and it remains unclear how to effectively assesstheir ability to correctly interpret music-related inputs with current methods.Motivated by this, we introduce MuChoMusic, a benchmark for evaluating musicunderstanding in multimodal language models focused on audio. MuChoMusiccomprises 1,187 multiple-choice questions, all validated by human annotators,on 644 music tracks sourced from two publicly available music datasets, andcovering a wide variety of genres. Questions in the benchmark are crafted toassess knowledge and reasoning abilities across several dimensions that coverfundamental musical concepts and their relation to cultural and functionalcontexts. Through the holistic analysis afforded by the benchmark, we evaluatefive open-source models and identify several pitfalls, including anover-reliance on the language modality, pointing to a need for bettermultimodal integration. Data and code are open-sourced.</description><author>Benno Weck, Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas, Dmitry Bogdanov</author><pubDate>Fri, 02 Aug 2024 15:34:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01337v1</guid></item><item><title>Sparse Linear Regression when Noises and Covariates are Heavy-Tailed and Contaminated by Outliers</title><link>http://arxiv.org/abs/2408.01336v1</link><description>We investigate a problem estimating coefficients of linear regression undersparsity assumption when covariates and noises are sampled from heavy taileddistributions. Additionally, we consider the situation where not onlycovariates and noises are sampled from heavy tailed distributions but alsocontaminated by outliers. Our estimators can be computed efficiently, andexhibit sharp error bounds.</description><author>Takeyuki Sasai, Hironori Fujisawa</author><pubDate>Fri, 02 Aug 2024 15:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01336v1</guid></item><item><title>A Backbone for Long-Horizon Robot Task Understanding</title><link>http://arxiv.org/abs/2408.01334v1</link><description>End-to-end robot learning, particularly for long-horizon tasks, often resultsin unpredictable outcomes and poor generalization. To address these challenges,we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robottask understanding and transferability. This framework uses therbligs (basicaction elements) as the backbone to decompose high-level robot tasks intoelemental robot configurations, which are then integrated with currentfoundation models to improve task understanding. The approach consists of twostages: offline training and online testing. During the offline training stage,we developed the Meta-RGate SynerFusion (MGSF) network for accurate therbligsegmentation across various tasks. In the online testing stage, after aone-shot demonstration of a new task is collected, our MGSF network extractshigh-level knowledge, which is then encoded into the image using ActionRegistration (ActionREG). Additionally, the Large Language Model(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensureprecise action execution, facilitating trajectory transfer in novel robotscenarios. Experimental results validate these methods, achieving 94.37% recallin therblig segmentation and success rates of 94.4% and 80% in real-worldonline robot testing for simple and complex scenarios, respectively.Supplementary material is available at:https://sites.google.com/view/therbligsbasedbackbone/home</description><author>Xiaoshuai Chen, Wei Chen, Dongmyoung Lee, Yukun Ge, Nicolas Rojas, Petar Kormushev</author><pubDate>Fri, 02 Aug 2024 15:32:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01334v1</guid></item><item><title>Deep Reinforcement Learning for Traveling Purchaser Problems</title><link>http://arxiv.org/abs/2404.02476v3</link><description>The traveling purchaser problem (TPP) is an important combinatorialoptimization problem with broad applications. Due to the coupling betweenrouting and purchasing, existing works on TPPs commonly address routeconstruction and purchase planning simultaneously, which, however, leads toexact methods with high computational cost and heuristics with sophisticateddesign but limited performance. In sharp contrast, we propose a novel approachbased on deep reinforcement learning (DRL), which addresses route constructionand purchase planning separately, while evaluating and optimizing the solutionfrom a global perspective. The key components of our approach include abipartite graph representation for TPPs to capture the market-productrelations, and a policy network that extracts information from the bipartitegraph and uses it to sequentially construct the route. One significant benefitof our framework is that we can efficiently construct the route using thepolicy network, and once the route is determined, the associated purchasingplan can be easily derived through linear programming, while, leveraging DRL,we can train the policy network to optimize the global solution objective.Furthermore, by introducing a meta-learning strategy, the policy network can betrained stably on large-sized TPP instances, and generalize well acrossinstances of varying sizes and distributions, even to much larger instancesthat are never seen during training. Experiments on various synthetic TPPinstances and the TPPLIB benchmark demonstrate that our DRL-based approach cansignificantly outperform well-established TPP heuristics, reducing theoptimality gap by 40%-90%, and also showing an advantage in runtime, especiallyon large-sized instances.</description><author>Haofeng Yuan, Rongping Zhu, Wanlu Yang, Shiji Song, Keyou You, Yuli Zhang, C. L. Philip Chen</author><pubDate>Fri, 02 Aug 2024 15:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02476v3</guid></item><item><title>HMDN: Hierarchical Multi-Distribution Network for Click-Through Rate Prediction</title><link>http://arxiv.org/abs/2408.01332v1</link><description>As the recommendation service needs to address increasingly diversedistributions, such as multi-population, multi-scenario, multitarget, andmulti-interest, more and more recent works have focused on multi-distributionmodeling and achieved great progress. However, most of them only considermodeling in a single multi-distribution manner, ignoring that mixedmulti-distributions often coexist and form hierarchical relationships. Toaddress these challenges, we propose a flexible modeling paradigm, namedHierarchical Multi-Distribution Network (HMDN), which efficiently models thesehierarchical relationships and can seamlessly integrate with existingmulti-distribution methods, such as Mixture of-Experts (MoE) and Dynamic-Weight(DW) models. Specifically, we first design a hierarchical multi-distributionrepresentation refinement module, employing a multi-level residual quantizationto obtain fine-grained hierarchical representation. Then, the refinedhierarchical representation is integrated into the existing singlemulti-distribution models, seamlessly expanding them into mixedmulti-distribution models. Experimental results on both public and industrialdatasets validate the effectiveness and flexibility of HMDN.</description><author>Xingyu Lou, Yu Yang, Kuiyao Dong, Heyuan Huang, Wenyi Yu, Ping Wang, Xiu Li, Jun Wang</author><pubDate>Fri, 02 Aug 2024 15:29:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01332v1</guid></item><item><title>UnifiedNN: Efficient Neural Network Training on the Cloud</title><link>http://arxiv.org/abs/2408.01331v1</link><description>Nowadays, cloud-based services are widely favored over the traditionalapproach of locally training a Neural Network (NN) model. Oftentimes, a cloudservice processes multiple requests from users--thus training multiple NNmodels concurrently. However, training NN models concurrently is a challengingprocess, which typically requires significant amounts of available computingresources and takes a long time to complete. In this paper, we presentUnifiedNN to effectively train multiple NN models concurrently on the cloud.UnifiedNN effectively "combines" multiple NN models and features several memoryand time conservation mechanisms to train multiple NN models simultaneouslywithout impacting the accuracy of the training process. Specifically, UnifiedNNmerges multiple NN models and creates a large singular unified model in orderto efficiently train all models at once. We have implemented a prototype ofUnifiedNN in PyTorch and we have compared its performance with relevantstate-of-the-art frameworks. Our experimental results demonstrate thatUnifiedNN can reduce memory consumption by up to 53% and training time by up to81% when compared with vanilla PyTorch without impacting the model training andtesting accuracy. Finally, our results indicate that UnifiedNN can reducememory consumption by up to 52% and training time by up to 41% when compared tostate-of-the-art frameworks when training multiple models concurrently.</description><author>Sifat Ut Taki, Spyridon Mastorakis, Arthi Padmanabhan</author><pubDate>Fri, 02 Aug 2024 15:29:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01331v1</guid></item><item><title>SPIdepth: Strengthened Pose Information for Self-supervised Monocular Depth Estimation</title><link>http://arxiv.org/abs/2404.12501v2</link><description>Self-supervised monocular depth estimation has garnered considerableattention for its applications in autonomous driving and robotics. While recentmethods have made strides in leveraging techniques like the Self Query Layer(SQL) to infer depth from motion, they often overlook the potential ofstrengthening pose information. In this paper, we introduce SPIdepth, a novelapproach that prioritizes enhancing the pose network for improved depthestimation. Building upon the foundation laid by SQL, SPIdepth emphasizes theimportance of pose information in capturing fine-grained scene structures. Byenhancing the pose network's capabilities, SPIdepth achieves remarkableadvancements in scene understanding and depth estimation. Experimental resultson benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth'sstate-of-the-art performance, surpassing previous methods by significantmargins. Specifically, SPIdepth tops the self-supervised KITTI benchmark.Additionally, SPIdepth achieves the lowest AbsRel (0.029), SqRel (0.069), andRMSE (1.394) on KITTI, establishing new state-of-the-art results. OnCityscapes, SPIdepth shows improvements over SQLdepth of 21.7% in AbsRel, 36.8%in SqRel, and 16.5% in RMSE, even without using motion masks. On Make3D,SPIdepth in zero-shot outperforms all other models. Remarkably, SPIdepthachieves these results using only a single image for inference, surpassing evenmethods that utilize video sequences for inference, thus demonstrating itsefficacy and efficiency in real-world applications. Our approach represents asignificant leap forward in self-supervised monocular depth estimation,underscoring the importance of strengthening pose information for advancingscene understanding in real-world applications.</description><author>Mykola Lavreniuk</author><pubDate>Fri, 02 Aug 2024 15:28:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12501v2</guid></item><item><title>Mixed moving average field guided learning for spatio-temporal data</title><link>http://arxiv.org/abs/2301.00736v4</link><description>Influenced mixed moving average fields are a versatile modeling class forspatio-temporal data. However, their predictive distribution is not generallyknown. Under this modeling assumption, we define a novel spatio-temporalembedding and a theory-guided machine learning approach that employs ageneralized Bayesian algorithm to make ensemble forecasts. We use Lipschitzpredictors and determine fixed-time and any-time PAC Bayesian bounds in thebatch learning setting. Performing causal forecast is a highlight of ourmethodology as its potential application to data with spatial and temporalshort and long-range dependence. We then test the performance of our learningmethodology by using linear predictors and data sets simulated from aspatio-temporal Ornstein-Uhlenbeck process.</description><author>Imma Valentina Curato, Orkun Furat, Lorenzo Proietti, Bennet Stroeh</author><pubDate>Fri, 02 Aug 2024 15:26:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00736v4</guid></item><item><title>FANNO: Augmenting High-Quality Instruction Data with Open-Sourced LLMs Only</title><link>http://arxiv.org/abs/2408.01323v1</link><description>Instruction fine-tuning stands as a crucial advancement in leveraging largelanguage models (LLMs) for enhanced task performance. However, the annotationof instruction datasets has traditionally been expensive and laborious, oftenrelying on manual annotations or costly API calls of proprietary LLMs. Toaddress these challenges, we introduce FANNO, a fully autonomous, open-sourcedframework that revolutionizes the annotation process without the need forpre-existing annotated data. Utilizing a Mistral-7b-instruct model, FANNOefficiently produces diverse and high-quality datasets through a structuredprocess involving document pre-screening, instruction generation, and responsegeneration. Experiments on Open LLM Leaderboard and AlpacaEval benchmark showthat the FANNO can generate high-quality data with diversity and complexity forfree, comparable to human-annotated or cleaned datasets likeAlpaca-GPT4-Cleaned.</description><author>He Zhu, Junyou Su, Tianle Lun, Yicheng Tao, Wenjia Zhang, Zipei Fan, Guanhua Chen</author><pubDate>Fri, 02 Aug 2024 15:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01323v1</guid></item><item><title>A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty and Semantic Object Cues for Gaze Guidance in Dynamic Scenes</title><link>http://arxiv.org/abs/2408.01322v1</link><description>How we perceive objects around us depends on what we actively attend to, yetour eye movements depend on the perceived objects. Still, object segmentationand gaze behavior are typically treated as two independent processes. Drawingon an information processing pattern from robotics, we present a mechanisticmodel that simulates these processes for dynamic real-world scenes. Ourimage-computable model uses the current scene segmentation for object-basedsaccadic decision-making while using the foveated object to refine its scenesegmentation recursively. To model this refinement, we use a Bayesian filter,which also provides an uncertainty estimate for the segmentation that we use toguide active scene exploration. We demonstrate that this model closelyresembles observers' free viewing behavior, measured by scanpath statistics,including foveation duration and saccade amplitude distributions used forparameter fitting and higher-level statistics not used for fitting. Theseinclude how object detections, inspections, and returns are balanced and adelay of returning saccades without an explicit implementation of such temporalinhibition of return. Extensive simulations and ablation studies show thatuncertainty promotes balanced exploration and that semantic object cues arecrucial to form the perceptual units used in object-based attention. Moreover,we show how our model's modular design allows for extensions, such asincorporating saccadic momentum or pre-saccadic attention, to further align itsoutput with human scanpaths.</description><author>Vito Mengers, Nicolas Roth, Oliver Brock, Klaus Obermayer, Martin Rolfs</author><pubDate>Fri, 02 Aug 2024 15:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01322v1</guid></item><item><title>A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks</title><link>http://arxiv.org/abs/2408.01319v1</link><description>In an era defined by the explosive growth of data and rapid technologicaladvancements, Multimodal Large Language Models (MLLMs) stand at the forefrontof artificial intelligence (AI) systems. Designed to seamlessly integratediverse data types-including text, images, videos, audio, and physiologicalsequences-MLLMs address the complexities of real-world applications far beyondthe capabilities of single-modality systems. In this paper, we systematicallysort out the applications of MLLM in multimodal tasks such as natural language,vision, and audio. We also provide a comparative analysis of the focus ofdifferent MLLMs in the tasks, and provide insights into the shortcomings ofcurrent MLLMs, and suggest potential directions for future research. Throughthese discussions, this paper hopes to provide valuable insights for thefurther development and application of MLLM.</description><author>Jiaqi Wang, Hanqi Jiang, Yiheng Liu, Chong Ma, Xu Zhang, Yi Pan, Mengyuan Liu, Peiran Gu, Sichen Xia, Wenjun Li, Yutong Zhang, Zihao Wu, Zhengliang Liu, Tianyang Zhong, Bao Ge, Tuo Zhang, Ning Qiang, Xintao Hu, Xi Jiang, Xin Zhang, Wei Zhang, Dinggang Shen, Tianming Liu, Shu Zhang</author><pubDate>Fri, 02 Aug 2024 15:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01319v1</guid></item><item><title>LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison</title><link>http://arxiv.org/abs/2407.02659v2</link><description>In light of recent legal allegations brought by publishers, newspapers, andother creators of copyrighted corpora against large language model developerswho use their copyrighted materials for training or fine-tuning purposes, wepropose a novel system, a variant of a plagiarism detection system, thatassesses whether a knowledge source has been used in the training orfine-tuning of a large language model. Unlike current methods, we utilize anapproach that uses Resource Description Framework (RDF) triples to createknowledge graphs from both a source document and an LLM continuation of thatdocument. These graphs are then analyzed with respect to content using cosinesimilarity and with respect to structure using a normalized version of graphedit distance that shows the degree of isomorphism. Unlike traditionalplagiarism systems that focus on content matching and keyword identificationbetween a source and a target corpus, our approach enables a broader and moreaccurate evaluation of similarity between a source document and LLMcontinuation by focusing on relationships between ideas and their organizationwith regards to others. Additionally, our approach does not require access toLLM metrics like perplexity that may be unavailable in closed large languagemodel "black-box" systems, as well as the training corpus. We thus assesswhether an LLM has "plagiarized" a corpus in its continuation throughsimilarity measures. A prototype of our system will be found on a hyperlinkedGitHub repository.</description><author>Devam Mondal, Carlo Lipizzi</author><pubDate>Fri, 02 Aug 2024 15:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02659v2</guid></item><item><title>Point Prediction for Streaming Data</title><link>http://arxiv.org/abs/2408.01318v1</link><description>We present two new approaches for point prediction with streaming data. Oneis based on the Count-Min sketch (CMS) and the other is based on Gaussianprocess priors with a random bias. These methods are intended for the mostgeneral predictive problems where no true model can be usefully formulated forthe data stream. In statistical contexts, this is often called the$\mathcal{M}$-open problem class. Under the assumption that the data consistsof i.i.d samples from a fixed distribution function $F$, we show that theCMS-based estimates of the distribution function are consistent. We compare our new methods with two established predictors in terms ofcumulative $L^1$ error. One is based on the Shtarkov solution (often called thenormalized maximum likelihood) in the normal experts setting and the other isbased on Dirichlet process priors. These comparisons are for two cases. Thefirst is one-pass meaning that the updating of the predictors is done using thefact that the CMS is a sketch. For predictors that are not one-pass, we usestreaming $K$-means to give a representative subset of fixed size that can beupdated as data accumulate. Preliminary computational work suggests that the one-pass median version ofthe CMS method is rarely outperformed by the other methods for sufficientlycomplex data. We also find that predictors based on Gaussian process priorswith random biases perform well. The Shtarkov predictors we use here did notperform as well probably because we were only using the simplest example. Theother predictors seemed to perform well mainly when the data did not look likethey came from an M-open data generator.</description><author>Aleena Chanda, N. V. Vinodchandran, Bertrand Clarke</author><pubDate>Fri, 02 Aug 2024 15:12:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01318v1</guid></item><item><title>Synergistic pathways of modulation enable robust task packing within neural dynamics</title><link>http://arxiv.org/abs/2408.01316v1</link><description>Understanding how brain networks learn and manage multiple taskssimultaneously is of interest in both neuroscience and artificial intelligence.In this regard, a recent research thread in theoretical neuroscience hasfocused on how recurrent neural network models and their internal dynamicsenact multi-task learning. To manage different tasks requires a mechanism toconvey information about task identity or context into the model, which from abiological perspective may involve mechanisms of neuromodulation. In thisstudy, we use recurrent network models to probe the distinctions between twoforms of contextual modulation of neural dynamics, at the level of neuronalexcitability and at the level of synaptic strength. We characterize thesemechanisms in terms of their functional outcomes, focusing on their robustnessto context ambiguity and, relatedly, their efficiency with respect to packingmultiple tasks into finite size networks. We also demonstrate distinctionbetween these mechanisms at the level of the neuronal dynamics they induce.Together, these characterizations indicate complementarity and synergy in howthese mechanisms act, potentially over multiple time-scales, toward enhancingrobustness of multi-task learning.</description><author>Giacomo Vedovati, ShiNung Ching</author><pubDate>Fri, 02 Aug 2024 15:12:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01316v1</guid></item><item><title>Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation</title><link>http://arxiv.org/abs/2406.07867v2</link><description>In this paper, we introduce a novel Face-to-Face spoken dialogue model. Itprocesses audio-visual speech from user input and generates audio-visual speechas the response, marking the initial step towards creating an avatar chatbotsystem without relying on intermediate text. To this end, we newly introduceMultiDialog, the first large-scale multimodal (i.e., audio and visual) spokendialogue corpus containing 340 hours of approximately 9,000 dialogues, recordedbased on the open domain dialogue dataset, TopicalChat. The MultiDialogcontains parallel audio-visual recordings of conversation partners actingaccording to the given script with emotion annotations, which we expect to openup research opportunities in multimodal synthesis. Our Face-to-Face spokendialogue model incorporates a textually pretrained large language model andadapts it into the audio-visual spoken dialogue domain by incorporatingspeech-text joint pretraining. Through extensive experiments, we validate theeffectiveness of our model in facilitating a face-to-face conversation. Demoand data are available at https://multidialog.github.io andhttps://huggingface.co/datasets/IVLLab/MultiDialog, respectively.</description><author>Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeong Hun Yeo, Yong Man Ro</author><pubDate>Fri, 02 Aug 2024 15:05:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07867v2</guid></item><item><title>TopoNAS: Boosting Search Efficiency of Gradient-based NAS via Topological Simplification</title><link>http://arxiv.org/abs/2408.01311v1</link><description>Improving search efficiency serves as one of the crucial objectives of NeuralArchitecture Search (NAS). However, many current approaches ignore theuniversality of the search strategy and fail to reduce the computationalredundancy during the search process, especially in one-shot NAS architectures.Besides, current NAS methods show invalid reparameterization in non-linearsearch space, leading to poor efficiency in common search spaces like DARTS. Inthis paper, we propose TopoNAS, a model-agnostic approach for gradient-basedone-shot NAS that significantly reduces searching time and memory usage bytopological simplification of searchable paths. Firstly, we model thenon-linearity in search spaces to reveal the parameterization difficulties. Toimprove the search efficiency, we present a topological simplification methodand iteratively apply module-sharing strategies to simplify the topologicalstructure of searchable paths. In addition, a kernel normalization technique isalso proposed to preserve the search accuracy. Experimental results on theNASBench201 benchmark with various search spaces demonstrate the effectivenessof our method. It proves the proposed TopoNAS enhances the performance ofvarious architectures in terms of search efficiency while maintaining a highlevel of accuracy. The project page is available athttps://xdedss.github.io/topo_simplification.</description><author>Danpei Zhao, Zhuoran Liu, Bo Yuan</author><pubDate>Fri, 02 Aug 2024 15:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01311v1</guid></item><item><title>Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models</title><link>http://arxiv.org/abs/2408.01308v1</link><description>Learning token embeddings based on token co-occurrence statistics has proveneffective for both pre-training and fine-tuning in natural language processing.However, recent studies have pointed out the distribution of learned embeddingsdegenerates into anisotropy, and even pre-trained language models (PLMs) sufferfrom a loss of semantics-related information in embeddings for low-frequencytokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large,and demonstrates its robustness against degeneration. On the basis of thisfinding, we propose DefinitionEMB, a method that utilizes definitions toconstruct isotropically distributed and semantics-related token embeddings forPLMs while maintaining original robustness during fine-tuning. Our experimentsdemonstrate the effectiveness of leveraging definitions from Wiktionary toconstruct such embeddings for RoBERTa-base and BART-large. Furthermore, theconstructed embeddings for low-frequency tokens improve the performance ofthese models across various GLUE and four text summarization datasets.</description><author>Ying Zhang, Dongyuan Li, Manabu Okumura</author><pubDate>Fri, 02 Aug 2024 15:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01308v1</guid></item><item><title>Decentralized Smoothing ADMM for Quantile Regression with Non-Convex Sparse Penalties</title><link>http://arxiv.org/abs/2408.01307v1</link><description>In the rapidly evolving internet-of-things (IoT) ecosystem, effective dataanalysis techniques are crucial for handling distributed data generated bysensors. Addressing the limitations of existing methods, such as thesub-gradient approach, which fails to distinguish between active and non-activecoefficients effectively, this paper introduces the decentralized smoothingalternating direction method of multipliers (DSAD) for penalized quantileregression. Our method leverages non-convex sparse penalties like the minimaxconcave penalty (MCP) and smoothly clipped absolute deviation (SCAD), improvingthe identification and retention of significant predictors. DSAD incorporates atotal variation norm within a smoothing ADMM framework, achieving consensusamong distributed nodes and ensuring uniform model performance across disparatedata sources. This approach overcomes traditional convergence challengesassociated with non-convex penalties in decentralized settings. We presenttheoretical proofs and extensive simulation results to validate theeffectiveness of the DSAD, demonstrating its superiority in achieving reliableconvergence and enhancing estimation accuracy compared with prior methods.</description><author>Reza Mirzaeifard, Diyako Ghaderyan, Stefan Werner</author><pubDate>Fri, 02 Aug 2024 15:00:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01307v1</guid></item><item><title>Bond Graphs for multi-physics informed Neural Networks for multi-variate time series</title><link>http://arxiv.org/abs/2405.13586v2</link><description>In the trend of hybrid Artificial Intelligence techniques, Physical-InformedMachine Learning has seen a growing interest. It operates mainly by imposingdata, learning, or architecture bias with simulation data, Partial DifferentialEquations, or equivariance and invariance properties. While it has shown greatsuccess on tasks involving one physical domain, such as fluid dynamics,existing methods are not adapted to tasks with complex multi-physical andmulti-domain phenomena. In addition, it is mainly formulated as an end-to-endlearning scheme. To address these challenges, we propose to leverage BondGraphs, a multi-physics modeling approach, together with Message Passing GraphNeural Networks. We propose a Neural Bond graph Encoder (NBgE) producingmulti-physics-informed representations that can be fed into any task-specificmodel. It provides a unified way to integrate both data and architecture biasesin deep learning. Our experiments on two challenging multi-domain physicalsystems - a Direct Current Motor and the Respiratory System - demonstrate theeffectiveness of our approach on a multivariate time-series forecasting task.</description><author>Alexis-Raja Brachet, Pierre-Yves Richard, Céline Hudelot</author><pubDate>Fri, 02 Aug 2024 14:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13586v2</guid></item><item><title>CP-Prompt: Composition-Based Cross-modal Prompting for Domain-Incremental Continual Learning</title><link>http://arxiv.org/abs/2407.21043v2</link><description>The key challenge of cross-modal domain-incremental learning (DIL) is toenable the learning model to continuously learn from novel data with differentfeature distributions under the same task without forgetting old ones. However,existing top-performing methods still cause high forgetting rates, by lackingintra-domain knowledge extraction and inter-domain common prompting strategy.In this paper, we propose a simple yet effective framework, CP-Prompt, bytraining limited parameters to instruct a pre-trained model to learn newdomains and avoid forgetting existing feature distributions. CP-Prompt capturesintra-domain knowledge by compositionally inserting personalized prompts onmulti-head self-attention layers and then learns the inter-domain knowledgewith a common prompting strategy. CP-Prompt shows superiority compared withstate-of-the-art baselines among three widely evaluated DIL tasks. The sourcecode is available at https://github.com/dannis97500/CP_Prompt.</description><author>Yu Feng, Zhen Tian, Yifan Zhu, Zongfu Han, Haoran Luo, Guangwei Zhang, Meina Song</author><pubDate>Fri, 02 Aug 2024 14:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21043v2</guid></item><item><title>Routoo: Learning to Route to Large Language Models Effectively</title><link>http://arxiv.org/abs/2401.13979v2</link><description>Developing foundational large language models (LLMs) is becoming increasinglycostly and inefficient. Also, closed-source and larger open-source modelsgenerally offer better response quality but come with higher inference coststhan smaller models. In this paper, we introduce Routoo, an architecturedesigned to optimize the selection of LLMs for specific prompts based onperformance, cost, and efficiency. Routoo consists of two key components: aperformance predictor and a cost-aware decoding. The performance predictor is alightweight LLM that estimates the performance of various underlying LLMswithout needing to execute and evaluate them. The cost-aware decoding thenselects the most suitable model based on these predictions and otherconstraints like cost and latency. We evaluated Routoo using the MMLU benchmarkacross 57 domains employing open-source models. Our results show that Routoomatches the performance of the Mixtral 8x7b model while reducing inferencecosts by one-third. Additionally, by allowing increased costs, Routoo surpassesMixtral's accuracy by over 5% at equivalent costs, achieving an accuracy of75.9%. When integrating GPT4 into our model pool, Routoo nearly matches GPT4'sperformance at half the cost and exceeds it with a 25% cost reduction. Theseoutcomes highlight Routoo's potential to create new SOTA in a cost-effectivemanner by leveraging the collective knowledge of multiple LLMs.</description><author>Alireza Mohammadshahi, Arshad Rafiq Shaikh, Majid Yazdani</author><pubDate>Fri, 02 Aug 2024 14:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13979v2</guid></item><item><title>A Decision-driven Methodology for Designing Uncertainty-aware AI Self-Assessment</title><link>http://arxiv.org/abs/2408.01301v1</link><description>Artificial intelligence (AI) has revolutionized decision-making processes andsystems throughout society and, in particular, has emerged as a significanttechnology in high-impact scenarios of national interest. Yet, despite AI'simpressive predictive capabilities in controlled settings, it still suffersfrom a range of practical setbacks preventing its widespread use in variouscritical scenarios. In particular, it is generally unclear if a given AIsystem's predictions can be trusted by decision-makers in downstreamapplications. To address the need for more transparent, robust, and trustworthyAI systems, a suite of tools has been developed to quantify the uncertainty ofAI predictions and, more generally, enable AI to "self-assess" the reliabilityof its predictions. In this manuscript, we categorize methods for AIself-assessment along several key dimensions and provide guidelines forselecting and designing the appropriate method for a practitioner's needs. Inparticular, we focus on uncertainty estimation techniques that consider theimpact of self-assessment on the choices made by downstream decision-makers andon the resulting costs and benefits of decision outcomes. To demonstrate theutility of our methodology for self-assessment design, we illustrate its usefor two realistic national-interest scenarios. This manuscript is a practicalguide for machine learning engineers and AI system users to select the idealself-assessment techniques for each problem.</description><author>Gregory Canal, Vladimir Leung, Philip Sage, Eric Heim, I-Jeng Wang</author><pubDate>Fri, 02 Aug 2024 14:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01301v1</guid></item><item><title>Assessing Robustness of Machine Learning Models using Covariate Perturbations</title><link>http://arxiv.org/abs/2408.01300v1</link><description>As machine learning models become increasingly prevalent in criticaldecision-making models and systems in fields like finance, healthcare, etc.,ensuring their robustness against adversarial attacks and changes in the inputdata is paramount, especially in cases where models potentially overfit. Thispaper proposes a comprehensive framework for assessing the robustness ofmachine learning models through covariate perturbation techniques. We explorevarious perturbation strategies to assess robustness and examine their impacton model predictions, including separate strategies for numeric and non-numericvariables, summaries of perturbations to assess and compare model robustnessacross different scenarios, and local robustness diagnosis to identify anyregions in the data where a model is particularly unstable. Through empiricalstudies on real world dataset, we demonstrate the effectiveness of our approachin comparing robustness across models, identifying the instabilities in themodel, and enhancing model robustness.</description><author>Arun Prakash R, Anwesha Bhattacharyya, Joel Vaughan, Vijayan N. Nair</author><pubDate>Fri, 02 Aug 2024 14:41:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01300v1</guid></item><item><title>Reinforcement Learning applied to Insurance Portfolio Pursuit</title><link>http://arxiv.org/abs/2408.00713v2</link><description>When faced with a new customer, many factors contribute to an insurancefirm's decision of what offer to make to that customer. In addition to theexpected cost of providing the insurance, the firm must consider the otheroffers likely to be made to the customer, and how sensitive the customer is todifferences in price. Moreover, firms often target a specific portfolio ofcustomers that could depend on, e.g., age, location, and occupation. Given sucha target portfolio, firms may choose to modulate an individual customer's offerbased on whether the firm desires the customer within their portfolio. We termthe problem of modulating offers to achieve a desired target portfolio theportfolio pursuit problem. Having formulated the portfolio pursuit problem as asequential decision making problem, we devise a novel reinforcement learningalgorithm for its solution. We test our method on a complex synthetic marketenvironment, and demonstrate that it outperforms a baseline method which mimicscurrent industry approaches to portfolio pursuit.</description><author>Edward James Young, Alistair Rogers, Elliott Tong, James Jordon</author><pubDate>Fri, 02 Aug 2024 14:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00713v2</guid></item><item><title>Optimal Mixed Integer Linear Optimization Trained Multivariate Classification Trees</title><link>http://arxiv.org/abs/2408.01297v1</link><description>Multivariate decision trees are powerful machine learning tools forclassification and regression that attract many researchers and industryprofessionals. An optimal binary tree has two types of vertices, (i) branchingvertices which have exactly two children and where datapoints are assessed on aset of discrete features and (ii) leaf vertices at which datapoints are given aprediction, and can be obtained by solving a biobjective optimization problemthat seeks to (i) maximize the number of correctly classified datapoints and(ii) minimize the number of branching vertices. Branching vertices are linearcombinations of training features and therefore can be thought of ashyperplanes. In this paper, we propose two cut-based mixed integer linearoptimization (MILO) formulations for designing optimal binary classificationtrees (leaf vertices assign discrete classes). Our models leverage on-the-flyidentification of minimal infeasible subsystems (MISs) from which we derivecutting planes that hold the form of packing constraints. We show theoreticalimprovements on the strongest flow-based MILO formulation currently in theliterature and conduct experiments on publicly available datasets to show ourmodels' ability to scale, strength against traditional branch and boundapproaches, and robustness in out-of-sample test performance. Our code and dataare available on GitHub.</description><author>Brandon Alston, Illya V. Hicks</author><pubDate>Fri, 02 Aug 2024 14:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01297v1</guid></item><item><title>Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning</title><link>http://arxiv.org/abs/2408.00690v2</link><description>While Large Language Models show remarkable performance in natural languageunderstanding, their resource-intensive nature makes them less accessible. Incontrast, smaller language models such as MiniCPM offer more sustainablescalability, but often underperform without specialized optimization. In thispaper, we explore the enhancement of smaller language models through theimprovement of their text embeddings. We select three language models, MiniCPM,Phi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Ourresults demonstrate that this fine-tuning method enhances the quality of textembeddings for all three models across various benchmarks, with MiniCPM showingthe most significant improvements of an average 56.33% performance gain. Thecontrastive fine-tuning code is publicly available athttps://github.com/trapoom555/Language-Model-STS-CFT.</description><author>Trapoom Ukarapol, Zhicheng Lee, Amy Xin</author><pubDate>Fri, 02 Aug 2024 14:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00690v2</guid></item><item><title>Don't Waste Your Time: Early Stopping Cross-Validation</title><link>http://arxiv.org/abs/2405.03389v2</link><description>State-of-the-art automated machine learning systems for tabular data oftenemploy cross-validation; ensuring that measured performances generalize tounseen data, or that subsequent ensembling does not overfit. However, usingk-fold cross-validation instead of holdout validation drastically increases thecomputational cost of validating a single configuration. While ensuring bettergeneralization and, by extension, better performance, the additional cost isoften prohibitive for effective model selection within a time budget. We aim tomake model selection with cross-validation more effective. Therefore, we studyearly stopping the process of cross-validation during model selection. Weinvestigate the impact of early stopping on random search for two algorithms,MLP and random forest, across 36 classification datasets. We further analyzethe impact of the number of folds by considering 3-, 5-, and 10-folds. Inaddition, we investigate the impact of early stopping with Bayesianoptimization instead of random search and also repeated cross-validation. Ourexploratory study shows that even a simple-to-understand and easy-to-implementmethod consistently allows model selection to converge faster; in ~94% of alldatasets, on average by ~214%. Moreover, stopping cross-validation enablesmodel selection to explore the search space more exhaustively by considering+167% configurations on average within one hour, while also obtaining betteroverall performance.</description><author>Edward Bergman, Lennart Purucker, Frank Hutter</author><pubDate>Fri, 02 Aug 2024 14:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03389v2</guid></item><item><title>Feature Clock: High-Dimensional Effects in Two-Dimensional Plots</title><link>http://arxiv.org/abs/2408.01294v1</link><description>Humans struggle to perceive and interpret high-dimensional data. Therefore,high-dimensional data are often projected into two dimensions forvisualization. Many applications benefit from complex nonlinear dimensionalityreduction techniques, but the effects of individual high-dimensional featuresare hard to explain in the two-dimensional space. Most visualization solutionsuse multiple two-dimensional plots, each showing the effect of onehigh-dimensional feature in two dimensions; this approach creates a need for avisual inspection of k plots for a k-dimensional input space. Our solution,Feature Clock, provides a novel approach that eliminates the need to inspectthese k plots to grasp the influence of original features on the data structuredepicted in two dimensions. Feature Clock enhances the explainability andcompactness of visualizations of embedded data and is available in anopen-source Python library.</description><author>Olga Ovcharenko, Rita Sevastjanova, Valentina Boeva</author><pubDate>Fri, 02 Aug 2024 14:31:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01294v1</guid></item><item><title>Underwater Object Detection Enhancement via Channel Stabilization</title><link>http://arxiv.org/abs/2408.01293v1</link><description>The complex marine environment exacerbates the challenges of object detectionmanifold. Marine trash endangers the aquatic ecosystem, presenting a persistentchallenge. Accurate detection of marine deposits is crucial for mitigating thisharm. Our work addresses underwater object detection by enhancing image qualityand evaluating detection methods. We use Detectron2's backbone with variousbase models and configurations for this task. We propose a novel channel stabilization technique alongside a simplifiedimage enhancement model to reduce haze and color cast in training images,improving multi-scale object detection. Following image processing, we testdifferent Detectron2 backbones for optimal detection accuracy. Additionally, weapply a sharpening filter with augmentation techniques to highlight objectprofiles for easier recognition. Results are demonstrated on the TrashCan Dataset, both instance and materialversions. The best-performing backbone method incorporates our channelstabilization and augmentation techniques. We also compare our Detectron2detection results with the Deformable Transformer. In the instance version ofTrashCan 1.0, our method achieves a 9.53% absolute increase in averageprecision for small objects and a 7% absolute gain in bounding box detectioncompared to the baseline. The code will be available on Code:https://github.com/aliman80/Underwater-Object-Detection-via-Channel-Stablization</description><author>Muhammad Ali, Salman Khan</author><pubDate>Fri, 02 Aug 2024 14:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01293v1</guid></item><item><title>3DPX: Progressive 2D-to-3D Oral Image Reconstruction with Hybrid MLP-CNN Networks</title><link>http://arxiv.org/abs/2408.01292v1</link><description>Panoramic X-ray (PX) is a prevalent modality in dental practice for its wideavailability and low cost. However, as a 2D projection image, PX does notcontain 3D anatomical information, and therefore has limited use in dentalapplications that can benefit from 3D information, e.g., tooth angularmisa-lignment detection and classification. Reconstructing 3D structuresdirectly from 2D PX has recently been explored to address limitations withexisting methods primarily reliant on Convolutional Neural Networks (CNNs) fordirect 2D-to-3D mapping. These methods, however, are unable to correctly inferdepth-axis spatial information. In addition, they are limited by the in-trinsiclocality of convolution operations, as the convolution kernels only capture theinformation of immediate neighborhood pixels. In this study, we propose aprogressive hybrid Multilayer Perceptron (MLP)-CNN pyra-mid network (3DPX) for2D-to-3D oral PX reconstruction. We introduce a progressive reconstructionstrategy, where 3D images are progressively re-constructed in the 3DPX withguidance imposed on the intermediate recon-struction result at each pyramidlevel. Further, motivated by the recent ad-vancement of MLPs that show promisein capturing fine-grained long-range dependency, our 3DPX integrates MLPs andCNNs to improve the semantic understanding during reconstruction. Extensiveexperiments on two large datasets involving 464 studies demonstrate that our3DPX outperforms state-of-the-art 2D-to-3D oral reconstruction methods,including standalone MLP and transformers, in reconstruction quality, and alsoim-proves the performance of downstream angular misalignment classificationtasks.</description><author>Xiaoshuang Li, Mingyuan Meng, Zimo Huang, Lei Bi, Eduardo Delamare, Dagan Feng, Bin Sheng, Jinman Kim</author><pubDate>Fri, 02 Aug 2024 14:28:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01292v1</guid></item><item><title>A Family of Pretrained Transformer Language Models for Russian</title><link>http://arxiv.org/abs/2309.10931v4</link><description>Transformer language models (LMs) are fundamental to NLP researchmethodologies and applications in various languages. However, developing suchmodels specifically for the Russian language has received little attention.This paper introduces a collection of 13 Russian Transformer LMs, which spansencoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder(ruT5, FRED-T5) architectures. We provide a report on the model architecturedesign and pretraining, and the results of evaluating their generalizationabilities on Russian language understanding and generation datasets andbenchmarks. By pretraining and releasing these specialized Transformer LMs, weaim to broaden the scope of the NLP research directions and enable thedevelopment of industrial solutions for the Russian language.</description><author>Dmitry Zmitrovich, Alexander Abramov, Andrey Kalmykov, Maria Tikhonova, Ekaterina Taktasheva, Danil Astafurov, Mark Baushenko, Artem Snegirev, Vitalii Kadulin, Sergey Markov, Tatiana Shavrina, Vladislav Mikhailov, Alena Fenogenova</author><pubDate>Fri, 02 Aug 2024 14:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10931v4</guid></item><item><title>Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives</title><link>http://arxiv.org/abs/2407.14962v3</link><description>The emergence of Generative Artificial Intelligence (AI) and Large LanguageModels (LLMs) has marked a new era of Natural Language Processing (NLP),introducing unprecedented capabilities that are revolutionizing variousdomains. This paper explores the current state of these cutting-edgetechnologies, demonstrating their remarkable advancements and wide-rangingapplications. Our paper contributes to providing a holistic perspective on thetechnical foundations, practical applications, and emerging challenges withinthe evolving landscape of Generative AI and LLMs. We believe that understandingthe generative capabilities of AI systems and the specific context of LLMs iscrucial for researchers, practitioners, and policymakers to collaborativelyshape the responsible and ethical integration of these technologies intovarious domains. Furthermore, we identify and address main research gaps,providing valuable insights to guide future research endeavors within the AIresearch community.</description><author>Desta Haileselassie Hagos, Rick Battle, Danda B. Rawat</author><pubDate>Fri, 02 Aug 2024 14:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14962v3</guid></item><item><title>Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition</title><link>http://arxiv.org/abs/2308.11635v2</link><description>Electroencephalography (EEG) is an objective tool for emotion recognitionwith promising applications. However, the scarcity of labeled data remains amajor challenge in this field, limiting the widespread use of EEG-based emotionrecognition. In this paper, a semi-supervised Dual-stream Self-AttentiveAdversarial Graph Contrastive learning framework (termed as DS-AGC) is proposedto tackle the challenge of limited labeled data in cross-subject EEG-basedemotion recognition. The DS-AGC framework includes two parallel streams forextracting non-structural and structural EEG features. The non-structuralstream incorporates a semi-supervised multi-domain adaptation method toalleviate distribution discrepancy among labeled source domain, unlabeledsource domain, and unknown target domain. The structural stream develops agraph contrastive learning method to extract effective graph-based featurerepresentation from multiple EEG channels in a semi-supervised manner. Further,a self-attentive fusion module is developed for feature fusion, sampleselection, and emotion recognition, which highlights EEG features more relevantto emotions and data samples in the labeled source domain that are closer tothe target domain. Extensive experiments conducted on two benchmark databases(SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-outcross-validation evaluation scheme show that the proposed model outperformsexisting methods under different incomplete label conditions (with an averageimprovement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating itseffectiveness in addressing the label scarcity problem in cross-subjectEEG-based emotion recognition.</description><author>Weishan Ye, Zhiguo Zhang, Fei Teng, Min Zhang, Jianhong Wang, Dong Ni, Fali Li, Peng Xu, Zhen Liang</author><pubDate>Fri, 02 Aug 2024 14:25:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11635v2</guid></item><item><title>TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling</title><link>http://arxiv.org/abs/2408.01291v1</link><description>Given a 3D mesh, we aim to synthesize 3D textures that correspond toarbitrary textual descriptions. Current methods for generating and assemblingtextures from sampled views often result in prominent seams or excessivesmoothing. To tackle these issues, we present TexGen, a novel multi-viewsampling and resampling framework for texture generation leveraging apre-trained text-to-image diffusion model. For view consistent sampling, firstof all we maintain a texture map in RGB space that is parameterized by thedenoising step and updated after each sampling step of the diffusion model toprogressively reduce the view discrepancy. An attention-guided multi-viewsampling strategy is exploited to broadcast the appearance information acrossviews. To preserve texture details, we develop a noise resampling techniquethat aids in the estimation of noise, generating inputs for subsequentdenoising steps, as directed by the text prompt and current texture map.Through an extensive amount of qualitative and quantitative evaluations, wedemonstrate that our proposed method produces significantly better texturequality for diverse 3D objects with a high degree of view consistency and richappearance details, outperforming current state-of-the-art methods.Furthermore, our proposed texture generation technique can also be applied totexture editing while preserving the original identity. More experimentalresults are available at https://dong-huo.github.io/TexGen/</description><author>Dong Huo, Zixin Guo, Xinxin Zuo, Zhihao Shi, Juwei Lu, Peng Dai, Songcen Xu, Li Cheng, Yee-Hong Yang</author><pubDate>Fri, 02 Aug 2024 14:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01291v1</guid></item><item><title>Weakly Supervised Text-to-SQL Parsing through Question Decomposition</title><link>http://arxiv.org/abs/2112.06311v4</link><description>Text-to-SQL parsers are crucial in enabling non-experts to effortlessly queryrelational data. Training such parsers, by contrast, generally requiresexpertise in annotating natural language (NL) utterances with corresponding SQLqueries. In this work, we propose a weak supervision approach for trainingtext-to-SQL parsers. We take advantage of the recently proposed questionmeaning representation called QDMR, an intermediate between NL and formal querylanguages. Given questions, their QDMR structures (annotated by non-experts orautomatically predicted), and the answers, we are able to automaticallysynthesize SQL queries that are used to train text-to-SQL models. We test ourapproach by experimenting on five benchmark datasets. Our results show that theweakly supervised models perform competitively with those trained on annotatedNL-SQL data. Overall, we effectively train text-to-SQL parsers, while usingzero SQL annotations.</description><author>Tomer Wolfson, Daniel Deutch, Jonathan Berant</author><pubDate>Fri, 02 Aug 2024 14:21:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.06311v4</guid></item><item><title>Deep Learning based Visually Rich Document Content Understanding: A Survey</title><link>http://arxiv.org/abs/2408.01287v1</link><description>Visually Rich Documents (VRDs) are essential in academia, finance, medicalfields, and marketing due to their multimodal information content. Traditionalmethods for extracting information from VRDs depend on expert knowledge andmanual labor, making them costly and inefficient. The advent of deep learninghas revolutionized this process, introducing models that leverage multimodalinformation vision, text, and layout along with pretraining tasks to developcomprehensive document representations. These models have achievedstate-of-the-art performance across various downstream tasks, significantlyenhancing the efficiency and accuracy of information extraction from VRDs. Inresponse to the growing demands and rapid developments in Visually RichDocument Understanding (VRDU), this paper provides a comprehensive review ofdeep learning-based VRDU frameworks. We systematically survey and analyzeexisting methods and benchmark datasets, categorizing them based on adoptedstrategies and downstream tasks. Furthermore, we compare different techniquesused in VRDU models, focusing on feature representation and fusion, modelarchitecture, and pretraining methods, while highlighting their strengths,limitations, and appropriate scenarios. Finally, we identify emerging trendsand challenges in VRDU, offering insights into future research directions andpractical applications. This survey aims to provide a thorough understanding ofVRDU advancements, benefiting both academic and industrial sectors.</description><author>Yihao Ding, Jean Lee, Soyeon Caren Han</author><pubDate>Fri, 02 Aug 2024 14:19:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01287v1</guid></item><item><title>Answering Questions by Meta-Reasoning over Multiple Chains of Thought</title><link>http://arxiv.org/abs/2304.13007v4</link><description>Modern systems for multi-hop question answering (QA) typically breakquestions into a sequence of reasoning steps, termed chain-of-thought (CoT),before arriving at a final answer. Often, multiple chains are sampled andaggregated through a voting mechanism over the final answers, but theintermediate steps themselves are discarded. While such approaches improveperformance, they do not consider the relations between intermediate stepsacross chains and do not provide a unified explanation for the predictedanswer. We introduce Multi-Chain Reasoning (MCR), an approach which promptslarge language models to meta-reason over multiple chains of thought, ratherthan aggregating their answers. MCR examines different reasoning chains, mixesinformation between them and selects the most relevant facts in generating anexplanation and predicting the answer. MCR outperforms strong baselines on 7multi-hop QA datasets. Moreover, our analysis reveals that MCR explanationsexhibit high quality, enabling humans to verify its answers.</description><author>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, Jonathan Berant</author><pubDate>Fri, 02 Aug 2024 14:18:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13007v4</guid></item><item><title>The Mismeasure of Man and Models: Evaluating Allocational Harms in Large Language Models</title><link>http://arxiv.org/abs/2408.01285v1</link><description>Large language models (LLMs) are now being considered and even deployed forapplications that support high-stakes decision-making, such as recruitment andclinical decisions. While several methods have been proposed for measuringbias, there remains a gap between predictions, which are what the proposedmethods consider, and how they are used to make decisions. In this work, weintroduce Rank-Allocational-Based Bias Index (RABBI), a model-agnostic biasmeasure that assesses potential allocational harms arising from biases in LLMpredictions. We compare RABBI and current bias metrics on two allocationdecision tasks. We evaluate their predictive validity across ten LLMs andutility for model selection. Our results reveal that commonly-used bias metricsbased on average performance gap and distribution distance fail to reliablycapture group disparities in allocation outcomes, whereas RABBI exhibits astrong correlation with allocation disparities. Our work highlights the need toaccount for how models are used in contexts with limited resource constraints.</description><author>Hannah Chen, Yangfeng Ji, David Evans</author><pubDate>Fri, 02 Aug 2024 14:13:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01285v1</guid></item><item><title>Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot Learning: A General Framework</title><link>http://arxiv.org/abs/2408.01284v1</link><description>Generalized Zero-Shot Learning (GZSL) is a challenging task requiringaccurate classification of both seen and unseen classes. Within this domain,Audio-visual GZSL emerges as an extremely exciting yet difficult task, giventhe inclusion of both visual and acoustic features as multi-modal inputs.Existing efforts in this field mostly utilize either embedding-based orgenerative-based methods. However, generative training is difficult andunstable, while embedding-based methods often encounter domain shift problem.Thus, we find it promising to integrate both methods into a unified frameworkto leverage their advantages while mitigating their respective disadvantages.Our study introduces a general framework employing out-of-distribution (OOD)detection, aiming to harness the strengths of both approaches. We first employgenerative adversarial networks to synthesize unseen features, enabling thetraining of an OOD detector alongside classifiers for seen and unseen classes.This detector determines whether a test feature belongs to seen or unseenclasses, followed by classification utilizing separate classifiers for eachfeature type. We test our framework on three popular audio-visual datasets andobserve a significant improvement comparing to existing state-of-the-art works.Codes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.</description><author>Liuyuan Wen</author><pubDate>Fri, 02 Aug 2024 14:10:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01284v1</guid></item><item><title>A Tiny Supervised ODL Core with Auto Data Pruning for Human Activity Recognition</title><link>http://arxiv.org/abs/2408.01283v1</link><description>In this paper, we introduce a low-cost and low-power tiny supervisedon-device learning (ODL) core that can address the distributional shift ofinput data for human activity recognition. Although ODL for resource-limitededge devices has been studied recently, how exactly to provide the traininglabels to these devices at runtime remains an open-issue. To address thisproblem, we propose to combine an automatic data pruning with supervised ODL toreduce the number queries needed to acquire predicted labels from a nearbyteacher device and thus save power consumption during model retraining. Thedata pruning threshold is automatically tuned, eliminating a manual thresholdtuning. As a tinyML solution at a few mW for the human activity recognition, wedesign a supervised ODL core that supports our automatic data pruning using a45nm CMOS process technology. We show that the required memory size for thecore is smaller than the same-shaped multilayer perceptron (MLP) and the powerconsumption is only 3.39mW. Experiments using a human activity recognitiondataset show that the proposed automatic data pruning reduces the communicationvolume by 55.7% and power consumption accordingly with only 0.9% accuracy loss.</description><author>Hiroki Matsutani, Radu Marculescu</author><pubDate>Fri, 02 Aug 2024 14:09:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01283v1</guid></item><item><title>DragD3D: Realistic Mesh Editing with Rigidity Control Driven by 2D Diffusion Priors</title><link>http://arxiv.org/abs/2310.04561v2</link><description>Direct mesh editing and deformation are key components in the geometricmodeling and animation pipeline. Mesh editing methods are typically framed asoptimization problems combining user-specified vertex constraints with aregularizer that determines the position of the rest of the vertices. Thechoice of the regularizer is key to the realism and authenticity of the finalresult. Physics and geometry-based regularizers are not aware of the globalcontext and semantics of the object, and the more recent deep learning priorsare limited to a specific class of 3D object deformations. Our maincontribution is a vertex-based mesh editing method called DragD3D based on (1)a novel optimization formulation that decouples the rotation and stretchcomponents of the deformation and combines a 3D geometric regularizer with (2)the recently introduced DDS loss which scores the faithfulness of the rendered2D image to one from a diffusion model. Thus, our deformation method achievesglobally realistic shape deformation which is not restricted to any class ofobjects. Our new formulation optimizes directly the transformation of theneural Jacobian field explicitly separating the rotational and stretchingcomponents. The objective function of the optimization combines the approximategradients of DDS and the gradients from the geometric loss to satisfy thevertex constraints. Additional user control over desired global shapedeformation is made possible by allowing explicit per-triangle deformationcontrol as well as explicit separation of rotational and stretching componentsof the deformation. We show that our deformations can be controlled to yieldrealistic shape deformations that are aware of the global context of theobjects, and provide better results than just using geometric regularizers.</description><author>Tianhao Xie, Eugene Belilovsky, Sudhir Mudur, Tiberiu Popa</author><pubDate>Fri, 02 Aug 2024 14:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04561v2</guid></item><item><title>Wave-Mamba: Wavelet State Space Model for Ultra-High-Definition Low-Light Image Enhancement</title><link>http://arxiv.org/abs/2408.01276v1</link><description>Ultra-high-definition (UHD) technology has attracted widespread attention dueto its exceptional visual quality, but it also poses new challenges forlow-light image enhancement (LLIE) techniques. UHD images inherently possesshigh computational complexity, leading existing UHD LLIE methods to employhigh-magnification downsampling to reduce computational costs, which in turnresults in information loss. The wavelet transform not only allows downsamplingwithout loss of information, but also separates the image content from thenoise. It enables state space models (SSMs) to avoid being affected by noisewhen modeling long sequences, thus making full use of the long-sequencemodeling capability of SSMs. On this basis, we propose Wave-Mamba, a novelapproach based on two pivotal insights derived from the wavelet domain: 1) mostof the content information of an image exists in the low-frequency component,less in the high-frequency component. 2) The high-frequency component exerts aminimal influence on the outcomes of low-light enhancement. Specifically, toefficiently model global content information on UHD images, we proposed alow-frequency state space block (LFSSBlock) by improving SSMs to focus onrestoring the information of low-frequency sub-bands. Moreover, we propose ahigh-frequency enhance block (HFEBlock) for high-frequency sub-bandinformation, which uses the enhanced low-frequency information to correct thehigh-frequency information and effectively restore the correct high-frequencydetails. Through comprehensive evaluation, our method has demonstrated superiorperformance, significantly outshining current leading techniques whilemaintaining a more streamlined architecture. The code is available athttps://github.com/AlexZou14/Wave-Mamba.</description><author>Wenbin Zou, Hongxia Gao, Weipeng Yang, Tongtong Liu</author><pubDate>Fri, 02 Aug 2024 14:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01276v1</guid></item><item><title>Comprehensive Library of Variational LSE Solvers</title><link>http://arxiv.org/abs/2404.09916v2</link><description>Linear systems of equations can be found in various mathematical domains, aswell as in the field of machine learning. By employing noisy intermediate-scalequantum devices, variational solvers promise to accelerate finding solutionsfor large systems. Although there is a wealth of theoretical research on thesealgorithms, only fragmentary implementations exist. To fill this gap, we havedeveloped the variational-lse-solver framework, which realizes existingapproaches in literature, and introduces several enhancements. Theuser-friendly interface is designed for researchers that work at theabstraction level of identifying and developing end-to-end applications.</description><author>Nico Meyer, Martin Röhn, Jakob Murauer, Axel Plinge, Christopher Mutschler, Daniel D. Scherer</author><pubDate>Fri, 02 Aug 2024 13:59:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09916v2</guid></item><item><title>Certified Robust Invariant Polytope Training in Neural Controlled ODEs</title><link>http://arxiv.org/abs/2408.01273v1</link><description>We consider a nonlinear control system modeled as an ordinary differentialequation subject to disturbance, with a state feedback controller parameterizedas a feedforward neural network. We propose a framework for trainingcontrollers with certified robust forward invariant polytopes, where anytrajectory initialized inside the polytope remains within the polytope,regardless of the disturbance. First, we parameterize a family of liftedcontrol systems in a higher dimensional space, where the original neuralcontrolled system evolves on an invariant subspace of each lifted system. Weuse interval analysis and neural network verifiers to further construct afamily of lifted embedding systems, carefully capturing the knowledge of thisinvariant subspace. If the vector field of any lifted embedding systemsatisfies a sign constraint at a single point, then a certain convex polytopeof the original system is robustly forward invariant. Treating the neuralnetwork controller and the lifted system parameters as variables, we propose analgorithm to train controllers with certified forward invariant polytopes inthe closed-loop control system. Through two examples, we demonstrate how thesimplicity of the sign constraint allows our approach to scale with systemdimension to over $50$ states, and outperform state-of-the-art Lyapunov-basedsampling approaches in runtime.</description><author>Akash Harapanahalli, Samuel Coogan</author><pubDate>Fri, 02 Aug 2024 13:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01273v1</guid></item><item><title>LlamaTouch: A Faithful and Scalable Testbed for Mobile UI Task Automation</title><link>http://arxiv.org/abs/2404.16054v2</link><description>The emergent large language/multimodal models facilitate the evolution ofmobile agents, especially in mobile UI task automation. However, existingevaluation approaches, which rely on human validation or established datasetsto compare agent-predicted actions with predefined action sequences, areunscalable and unfaithful. To overcome these limitations, this paper presentsLlamaTouch, a testbed for on-device mobile UI task execution and faithful,scalable task evaluation. By observing that the task execution process onlytransfers UI states, LlamaTouch employs a novel evaluation approach that onlyassesses whether an agent traverses all manually annotated, essentialapplication/system states. LlamaTouch comprises three key techniques: (1)On-device task execution that enables mobile agents to interact with realisticmobile environments for task execution. (2) Fine-grained UI componentannotation that merges pixel-level screenshots and textual screen hierarchiesto explicitly identify and precisely annotate essential UI components with arich set of designed annotation primitives. (3) A multi-level application statematching algorithm that utilizes exact and fuzzy matching to accurately detectcritical information in each screen, even with unpredictable UI layout/contentdynamics. LlamaTouch currently incorporates four mobile agents and 496 tasks,encompassing both tasks in the widely-used datasets and our self-constructedones to cover more diverse mobile applications. Evaluation results demonstrateLlamaTouch's high faithfulness of evaluation in real-world mobile environmentsand its better scalability than human validation. LlamaTouch also enables easytask annotation and integration of new mobile agents. Code and dataset arepublicly available at https://github.com/LlamaTouch/LlamaTouch.</description><author>Li Zhang, Shihe Wang, Xianqing Jia, Zhihan Zheng, Yunhe Yan, Longxi Gao, Yuanchun Li, Mengwei Xu</author><pubDate>Fri, 02 Aug 2024 13:49:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16054v2</guid></item><item><title>A General Framework to Boost 3D GS Initialization for Text-to-3D Generation by Lexical Richness</title><link>http://arxiv.org/abs/2408.01269v1</link><description>Text-to-3D content creation has recently received much attention, especiallywith the prevalence of 3D Gaussians Splatting. In general, GS-based methodscomprise two key stages: initialization and rendering optimization. To achieveinitialization, existing works directly apply random sphere initialization or3D diffusion models, e.g., Point-E, to derive the initial shapes. However, suchstrategies suffer from two critical yet challenging problems: 1) the finalshapes are still similar to the initial ones even after training; 2) shapes canbe produced only from simple texts, e.g., "a dog", not for lexically richertexts, e.g., "a dog is sitting on the top of the airplane". To address theseproblems, this paper proposes a novel general framework to boost the 3D GSInitialization for text-to-3D generation upon the lexical richness. Our keyidea is to aggregate 3D Gaussians into spatially uniform voxels to representcomplex shapes while enabling the spatial interaction among the 3D Gaussiansand semantic interaction between Gaussians and texts. Specifically, we firstconstruct a voxelized representation, where each voxel holds a 3D Gaussian withits position, scale, and rotation fixed while setting opacity as the solefactor to determine a position's occupancy. We then design an initializationnetwork mainly consisting of two novel components: 1) Global InformationPerception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a designenables each 3D Gaussian to assimilate the spatial information from other areasand semantic information from texts. Extensive experiments show the superiorityof our framework of high-quality 3D GS initialization against the existingmethods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.Also, our framework can be seamlessly plugged into SoTA training frameworks,e.g., LucidDreamer, for semantically consistent text-to-3D generation.</description><author>Lutao Jiang, Hangyu Li, Lin Wang</author><pubDate>Fri, 02 Aug 2024 13:46:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01269v1</guid></item><item><title>D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions</title><link>http://arxiv.org/abs/2407.02604v2</link><description>Large vision language models (VLMs) have progressed incredibly from researchto applicability for general-purpose use cases. LLaVA-Med, a pioneering largelanguage and vision assistant for biomedicine, can perform multi-modalbiomedical image and data analysis to provide a natural language interface forradiologists. While it is highly generalizable and works with multi-modal data,it is currently limited by well-known challenges that exist in the largelanguage model space. Hallucinations and imprecision in responses can lead tomisdiagnosis which currently hinder the clinical adaptability of VLMs. Tocreate precise, user-friendly models in healthcare, we propose D-Rax -- adomain-specific, conversational, radiologic assistance tool that can be used togain insights about a particular radiologic image. In this study, we enhancethe conversational analysis of chest X-ray (CXR) images to support radiologicalreporting, offering comprehensive insights from medical imaging and aiding inthe formulation of accurate diagnosis. D-Rax is achieved by fine-tuning theLLaVA-Med architecture on our curated enhanced instruction-following data,comprising of images, instructions, as well as disease diagnosis anddemographic predictions derived from MIMIC-CXR imaging data, CXR-related visualquestion answer (VQA) pairs, and predictive outcomes from multiple expert AImodels. We observe statistically significant improvement in responses whenevaluated for both open and close-ended conversations. Leveraging the power ofstate-of-the-art diagnostic models combined with VLMs, D-Rax empowersclinicians to interact with medical images using natural language, which couldpotentially streamline their decision-making process, enhance diagnosticaccuracy, and conserve their time.</description><author>Hareem Nisar, Syed Muhammad Anwar, Zhifan Jiang, Abhijeet Parida, Ramon Sanchez-Jacob, Vishwesh Nath, Holger R. Roth, Marius George Linguraru</author><pubDate>Fri, 02 Aug 2024 13:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02604v2</guid></item><item><title>The virtual CAT: A tool for algorithmic thinking assessment in Swiss compulsory education</title><link>http://arxiv.org/abs/2408.01263v1</link><description>In today's digital era, holding algorithmic thinking (AT) skills is crucial,not only in computer science-related fields. These abilities enable individualsto break down complex problems into more manageable steps and create a sequenceof actions to solve them. To address the increasing demand for AT assessmentsin educational settings and the limitations of current methods, this paperintroduces the virtual Cross Array Task (CAT), a digital adaptation of anunplugged assessment activity designed to evaluate algorithmic skills in Swisscompulsory education. This tool offers scalable and automated assessment,reducing human involvement and mitigating potential data collection errors. Theplatform features gesture-based and visual block-based programming interfaces,ensuring its usability for diverse learners, further supported by multilingualcapabilities. To evaluate the virtual CAT platform, we conducted a pilotevaluation in Switzerland involving a heterogeneous group of students. Thefindings show the platform's usability, proficiency and suitability forassessing AT skills among students of diverse ages, development stages, andeducational backgrounds, as well as the feasibility of large-scale datacollection.</description><author>Giorgia Adorni, Alberto Piatti</author><pubDate>Fri, 02 Aug 2024 13:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01263v1</guid></item><item><title>RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework</title><link>http://arxiv.org/abs/2408.01262v1</link><description>Retrieval-Augmented Generation (RAG) systems have demonstrated theiradvantages in alleviating the hallucination of Large Language Models (LLMs).Existing RAG benchmarks mainly focus on evaluating whether LLMs can correctlyanswer the general knowledge. However, they are unable to evaluate theeffectiveness of the RAG system in dealing with the data from differentvertical domains. This paper introduces RAGEval, a framework for automaticallygenerating evaluation datasets to evaluate the knowledge usage ability ofdifferent LLMs in different scenarios. Specifically, RAGEval summarizes aschema from seed documents, applies the configurations to generate diversedocuments, and constructs question-answering pairs according to both articlesand configurations. We propose three novel metrics, Completeness,Hallucination, and Irrelevance, to carefully evaluate the responses generatedby LLMs. By benchmarking RAG models in vertical domains, RAGEval has theability to better evaluate the knowledge usage ability of LLMs, which avoidsthe confusion regarding the source of knowledge in answering question inexisting QA datasets--whether it comes from parameterized memory or retrieval.</description><author>Kunlun Zhu, Yifan Luo, Dingling Xu, Ruobing Wang, Shi Yu, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, Maosong Sun</author><pubDate>Fri, 02 Aug 2024 13:35:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01262v1</guid></item><item><title>Detection and Characterization of Coordinated Online Behavior: A Survey</title><link>http://arxiv.org/abs/2408.01257v1</link><description>Coordination is a fundamental aspect of life. The advent of social media hasmade it integral also to online human interactions, such as those thatcharacterize thriving online communities and social movements. At the sametime, coordination is also core to effective disinformation, manipulation, andhate campaigns. This survey collects, categorizes, and critically discusses thebody of work produced as a result of the growing interest on coordinated onlinebehavior. We reconcile industry and academic definitions, propose acomprehensive framework to study coordinated online behavior, and review andcritically discuss the existing detection and characterization methods. Ouranalysis identifies open challenges and promising directions of research,serving as a guide for scholars, practitioners, and policymakers inunderstanding and addressing the complexities inherent to online coordination.</description><author>Lorenzo Mannocci, Michele Mazza, Anna Monreale, Maurizio Tesconi, Stefano Cresci</author><pubDate>Fri, 02 Aug 2024 13:27:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01257v1</guid></item><item><title>Canonical Decision Diagrams Modulo Theories</title><link>http://arxiv.org/abs/2404.16455v3</link><description>Decision diagrams (DDs) are powerful tools to represent effectivelypropositional formulas, which are largely used in many domains, in particularin formal verification and in knowledge compilation. Some forms of DDs (e.g.,OBDDs, SDDs) are canonical, that is, (under given conditions on the atom list)they univocally represent equivalence classes of formulas. Given the limitedexpressiveness of propositional logic, a few attempts to leverage DDs to SMTlevel have been presented in the literature. Unfortunately, these techniquesstill suffer from some limitations: most procedures are theory-specific; someproduce theory DDs (T-DDs) which do not univocally represent T-valid formulasor T-inconsistent formulas; none of these techniques provably producestheory-canonical T-DDs, which (under given conditions on the T-atom list)univocally represent T-equivalence classes of formulas. Also, these proceduresare not easy to implement, and very few implementations are actually available.In this paper, we present a novel very-general technique to leverage DDs to SMTlevel, which has several advantages: it is very easy to implement on top of anAllSMT solver and a DD package, which are used as blackboxes; it works forevery form of DDs and every theory, or combination thereof, supported by theAllSMT solver; it produces theory-canonical T-DDs if the propositional DD iscanonical. We have implemented a prototype tool for both T-OBDDs and T-SDDs ontop of OBDD and SDD packages and the MathSAT SMT solver. Some preliminaryempirical evaluation supports the effectiveness of the approach.</description><author>Massimo Michelutti, Gabriele Masina, Giuseppe Spallitta, Roberto Sebastiani</author><pubDate>Fri, 02 Aug 2024 13:27:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16455v3</guid></item><item><title>Incremental Object-Based Novelty Detection with Feedback Loop</title><link>http://arxiv.org/abs/2311.09004v2</link><description>Object-based Novelty Detection (ND) aims to identify unknown objects that donot belong to classes seen during training by an object detection model. Thetask is particularly crucial in real-world applications, as it allows to avoidpotentially harmful behaviours, e.g. as in the case of object detection modelsadopted in a self-driving car or in an autonomous robot. Traditional approachesto ND focus on one time offline post processing of the pretrained objectdetection output, leaving no possibility to improve the model robustness aftertraining and discarding the abundant amount of out-of-distribution dataencountered during deployment. In this work, we propose a novel framework forobject-based ND, assuming that human feedback can be requested on the predictedoutput and later incorporated to refine the ND model without negativelyaffecting the main object detection performance. This refinement operation isrepeated whenever new feedback is available. To tackle this new formulation ofthe problem for object detection, we propose a lightweight ND module attachedon top of a pre-trained object detection model, which is incrementally updatedthrough a feedback loop. We also propose a new benchmark to evaluate methods onthis new setting and test extensively our ND approach against baselines,showing increased robustness and a successful incorporation of the receivedfeedback.</description><author>Simone Caldarella, Elisa Ricci, Rahaf Aljundi</author><pubDate>Fri, 02 Aug 2024 13:27:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09004v2</guid></item></channel></rss>