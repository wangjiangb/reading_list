<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 12 Sep 2024 13:00:41 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered Stereo Pairs</title><link>http://arxiv.org/abs/2409.07456v1</link><description>3D Gaussian Splatting (GS) significantly struggles to accurately representthe underlying 3D scene geometry, resulting in inaccuracies and floatingartifacts when rendering depth maps. In this paper, we address this limitation,undertaking a comprehensive analysis of the integration of depth priorsthroughout the optimization process of Gaussian primitives, and present a novelstrategy for this purpose. This latter dynamically exploits depth cues from areadily available stereo network, processing virtual stereo pairs rendered bythe GS model itself during training and achieving consistent self-improvementof the scene representation. Experimental results on three popular datasets,breaking ground as the first to assess depth accuracy for these models,validate our findings.</description><author>Sadra Safadoust, Fabio Tosi, Fatma GÃ¼ney, Matteo Poggi</author><pubDate>Wed, 11 Sep 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07456v1</guid></item><item><title>DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for Text-to-3D Generation</title><link>http://arxiv.org/abs/2409.07454v1</link><description>Learning radiance fields (NeRF) with powerful 2D diffusion models hasgarnered popularity for text-to-3D generation. Nevertheless, the implicit 3Drepresentations of NeRF lack explicit modeling of meshes and textures oversurfaces, and such surface-undefined way may suffer from the issues, e.g.,noisy surfaces with ambiguous texture details or cross-view inconsistency. Toalleviate this, we present DreamMesh, a novel text-to-3D architecture thatpivots on well-defined surfaces (triangle meshes) to generate high-fidelityexplicit 3D model. Technically, DreamMesh capitalizes on a distinctivecoarse-to-fine scheme. In the coarse stage, the mesh is first deformed bytext-guided Jacobians and then DreamMesh textures the mesh with an interlaceduse of 2D diffusion models in a tuning free manner from multiple viewpoints. Inthe fine stage, DreamMesh jointly manipulates the mesh and refines the texturemap, leading to high-quality triangle meshes with high-fidelity texturedmaterials. Extensive experiments demonstrate that DreamMesh significantlyoutperforms state-of-the-art text-to-3D methods in faithfully generating 3Dcontent with richer textual details and enhanced geometry. Our project page isavailable at https://dreammesh.github.io.</description><author>Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Zuxuan Wu, Yu-Gang Jiang, Tao Mei</author><pubDate>Wed, 11 Sep 2024 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07454v1</guid></item><item><title>"My Grade is Wrong!": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays</title><link>http://arxiv.org/abs/2409.07453v1</link><description>Interactive feedback, where feedback flows in both directions between teacherand student, is more effective than traditional one-way feedback. However, itis often too time-consuming for widespread use in educational practice. WhileLarge Language Models (LLMs) have potential for automating feedback, theystruggle with reasoning and interaction in an interactive setting. This paperintroduces CAELF, a Contestable AI Empowered LLM Framework for automatinginteractive feedback. CAELF allows students to query, challenge, and clarifytheir feedback by integrating a multi-agent system with computationalargumentation. Essays are first assessed by multiple Teaching-Assistant Agents(TA Agents), and then a Teacher Agent aggregates the evaluations through formalreasoning to generate feedback and grades. Students can further engage with thefeedback to refine their understanding. A case study on 500 critical thinkingessays with user studies demonstrates that CAELF significantly improvesinteractive feedback, enhancing the reasoning and interaction capabilities ofLLMs. This approach offers a promising solution to overcoming the time andresource barriers that have limited the adoption of interactive feedback ineducational settings.</description><author>Shengxin Hong, Chang Cai, Sixuan Du, Haiyue Feng, Siyuan Liu, Xiuyi Fan</author><pubDate>Wed, 11 Sep 2024 17:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07453v1</guid></item><item><title>Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models</title><link>http://arxiv.org/abs/2409.07452v1</link><description>Despite having tremendous progress in image-to-3D generation, existingmethods still struggle to produce multi-view consistent images withhigh-resolution textures in detail, especially in the paradigm of 2D diffusionthat lacks 3D awareness. In this work, we present High-resolution Image-to-3Dmodel (Hi3D), a new video diffusion based paradigm that redefines a singleimage to multi-view images as 3D-aware sequential image generation (i.e.,orbital video generation). This methodology delves into the underlying temporalconsistency knowledge in video diffusion model that generalizes well togeometry consistency across multiple views in 3D generation. Technically, Hi3Dfirst empowers the pre-trained video diffusion model with 3D-aware prior(camera pose condition), yielding multi-view images with low-resolution texturedetails. A 3D-aware video-to-video refiner is learnt to further scale up themulti-view images with high-resolution texture details. Such high-resolutionmulti-view images are further augmented with novel views through 3D GaussianSplatting, which are finally leveraged to obtain high-fidelity meshes via 3Dreconstruction. Extensive experiments on both novel view synthesis and singleview reconstruction demonstrate that our Hi3D manages to produce superiormulti-view consistency images with highly-detailed textures. Source code anddata are available at \url{https://github.com/yanghb22-fdu/Hi3D-Official}.</description><author>Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Chong-Wah Ngo, Tao Mei</author><pubDate>Wed, 11 Sep 2024 17:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07452v1</guid></item><item><title>FreeEnhance: Tuning-Free Image Enhancement via Content-Consistent Noising-and-Denoising Process</title><link>http://arxiv.org/abs/2409.07451v1</link><description>The emergence of text-to-image generation models has led to the recognitionthat image enhancement, performed as post-processing, would significantlyimprove the visual quality of the generated images. Exploring diffusion modelsto enhance the generated images nevertheless is not trivial and necessitates todelicately enrich plentiful details while preserving the visual appearance ofkey content in the original image. In this paper, we propose a novel framework,namely FreeEnhance, for content-consistent image enhancement using theoff-the-shelf image diffusion models. Technically, FreeEnhance is a two-stageprocess that firstly adds random noise to the input image and then capitalizeson a pre-trained image diffusion model (i.e., Latent Diffusion Models) todenoise and enhance the image details. In the noising stage, FreeEnhance isdevised to add lighter noise to the region with higher frequency to preservethe high-frequent patterns (e.g., edge, corner) in the original image. In thedenoising stage, we present three target properties as constraints toregularize the predicted noise, enhancing images with high acutance and highvisual quality. Extensive experiments conducted on the HPDv2 datasetdemonstrate that our FreeEnhance outperforms the state-of-the-art imageenhancement models in terms of quantitative metrics and human preference. Moreremarkably, FreeEnhance also shows higher human preference compared to thecommercial image enhancement solution of Magnific AI.</description><author>Yang Luo, Yiheng Zhang, Zhaofan Qiu, Ting Yao, Zhineng Chen, Yu-Gang Jiang, Tao Mei</author><pubDate>Wed, 11 Sep 2024 17:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07451v1</guid></item><item><title>VMAS: Video-to-Music Generation via Semantic Alignment in Web Music Videos</title><link>http://arxiv.org/abs/2409.07450v1</link><description>We present a framework for learning to generate background music from videoinputs. Unlike existing works that rely on symbolic musical annotations, whichare limited in quantity and diversity, our method leverages large-scale webvideos accompanied by background music. This enables our model to learn togenerate realistic and diverse music. To accomplish this goal, we develop agenerative video-music Transformer with a novel semantic video-music alignmentscheme. Our model uses a joint autoregressive and contrastive learningobjective, which encourages the generation of music aligned with high-levelvideo content. We also introduce a novel video-beat alignment scheme to matchthe generated music beats with the low-level motions in the video. Lastly, tocapture fine-grained visual cues in a video needed for realistic backgroundmusic generation, we introduce a new temporal video encoder architecture,allowing us to efficiently process videos consisting of many densely sampledframes. We train our framework on our newly curated DISCO-MV dataset,consisting of 2.2M video-music samples, which is orders of magnitude largerthan any prior datasets used for video music generation. Our method outperformsexisting approaches on the DISCO-MV and MusicCaps datasets according to variousmusic generation evaluation metrics, including human evaluation. Results areavailable at https://genjib.github.io/project_page/VMAs/index.html</description><author>Yan-Bo Lin, Yu Tian, Linjie Yang, Gedas Bertasius, Heng Wang</author><pubDate>Wed, 11 Sep 2024 17:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07450v1</guid></item><item><title>Introducing Perturb-ability Score (PS) to Enhance Robustness Against Evasion Adversarial Attacks on ML-NIDS</title><link>http://arxiv.org/abs/2409.07448v1</link><description>This paper proposes a novel Perturb-ability Score (PS) that can be used toidentify Network Intrusion Detection Systems (NIDS) features that can be easilymanipulated by attackers in the problem-space. We demonstrate that using PS toselect only non-perturb-able features for ML-based NIDS maintains detectionperformance while enhancing robustness against adversarial attacks.</description><author>Mohamed elShehaby, Ashraf Matrawy</author><pubDate>Wed, 11 Sep 2024 17:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07448v1</guid></item><item><title>StereoCrafter: Diffusion-based Generation of Long and High-fidelity Stereoscopic 3D from Monocular Videos</title><link>http://arxiv.org/abs/2409.07447v1</link><description>This paper presents a novel framework for converting 2D videos to immersivestereoscopic 3D, addressing the growing demand for 3D content in immersiveexperience. Leveraging foundation models as priors, our approach overcomes thelimitations of traditional methods and boosts the performance to ensure thehigh-fidelity generation required by the display devices. The proposed systemconsists of two main steps: depth-based video splatting for warping andextracting occlusion mask, and stereo video inpainting. We utilize pre-trainedstable video diffusion as the backbone and introduce a fine-tuning protocol forthe stereo video inpainting task. To handle input video with varying lengthsand resolutions, we explore auto-regressive strategies and tiled processing.Finally, a sophisticated data processing pipeline has been developed toreconstruct a large-scale and high-quality dataset to support our training. Ourframework demonstrates significant improvements in 2D-to-3D video conversion,offering a practical solution for creating immersive content for 3D deviceslike Apple Vision Pro and 3D displays. In summary, this work contributes to thefield by presenting an effective method for generating high-qualitystereoscopic videos from monocular input, potentially transforming how weexperience digital media.</description><author>Sijie Zhao, Wenbo Hu, Xiaodong Cun, Yong Zhang, Xiaoyu Li, Zhe Kong, Xiangjun Gao, Muyao Niu, Ying Shan</author><pubDate>Wed, 11 Sep 2024 17:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07447v1</guid></item><item><title>Adaptive Adapter Routing for Long-Tailed Class-Incremental Learning</title><link>http://arxiv.org/abs/2409.07446v1</link><description>In our ever-evolving world, new data exhibits a long-tailed distribution,such as e-commerce platform reviews. This necessitates continuous modellearning imbalanced data without forgetting, addressing the challenge oflong-tailed class-incremental learning (LTCIL). Existing methods often rely onretraining linear classifiers with former data, which is impractical inreal-world settings. In this paper, we harness the potent representationcapabilities of pre-trained models and introduce AdaPtive Adapter RouTing(APART) as an exemplar-free solution for LTCIL. To counteract forgetting, wetrain inserted adapters with frozen pre-trained weights for deeper adaptationand maintain a pool of adapters for selection during sequential model updates.Additionally, we present an auxiliary adapter pool designed for effectivegeneralization, especially on minority classes. Adaptive instance routingacross these pools captures crucial correlations, facilitating a comprehensiverepresentation of all classes. Consequently, APART tackles the imbalanceproblem as well as catastrophic forgetting in a unified framework. Extensivebenchmark experiments validate the effectiveness of APART. Code is availableat: https://github.com/vita-qzh/APART</description><author>Zhi-Hong Qi, Da-Wei Zhou, Yiran Yao, Han-Jia Ye, De-Chuan Zhan</author><pubDate>Wed, 11 Sep 2024 17:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07446v1</guid></item><item><title>RainMamba: Enhanced Locality Learning with State Space Models for Video Deraining</title><link>http://arxiv.org/abs/2407.21773v2</link><description>The outdoor vision systems are frequently contaminated by rain streaks andraindrops, which significantly degenerate the performance of visual tasks andmultimedia applications. The nature of videos exhibits redundant temporal cuesfor rain removal with higher stability. Traditional video deraining methodsheavily rely on optical flow estimation and kernel-based manners, which have alimited receptive field. Yet, transformer architectures, while enablinglong-term dependencies, bring about a significant increase in computationalcomplexity. Recently, the linear-complexity operator of the state space models(SSMs) has contrarily facilitated efficient long-term temporal modeling, whichis crucial for rain streaks and raindrops removal in videos. Unexpectedly, itsuni-dimensional sequential process on videos destroys the local correlationsacross the spatio-temporal dimension by distancing adjacent pixels. To addressthis, we present an improved SSMs-based video deraining network (RainMamba)with a novel Hilbert scanning mechanism to better capture sequence-level localinformation. We also introduce a difference-guided dynamic contrastive localitylearning strategy to enhance the patch-level self-similarity learning abilityof the proposed network. Extensive experiments on four synthesized videoderaining datasets and real-world rainy videos demonstrate the effectivenessand efficiency of our network in the removal of rain streaks and raindrops. Ourcode and results are available at https://github.com/TonyHongtaoWu/RainMamba.</description><author>Hongtao Wu, Yijun Yang, Huihui Xu, Weiming Wang, Jinni Zhou, Lei Zhu</author><pubDate>Wed, 11 Sep 2024 17:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21773v2</guid></item><item><title>SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories</title><link>http://arxiv.org/abs/2409.07440v1</link><description>Given that Large Language Models (LLMs) have made significant progress inwriting code, can they now be used to autonomously reproduce results fromresearch repositories? Such a capability would be a boon to the researchcommunity, helping researchers validate, understand, and extend prior work. Toadvance towards this goal, we introduce SUPER, the first benchmark designed toevaluate the capability of LLMs in setting up and executing tasks from researchrepositories. SUPERaims to capture the realistic challenges faced byresearchers working with Machine Learning (ML) and Natural Language Processing(NLP) research repositories. Our benchmark comprises three distinct problemsets: 45 end-to-end problems with annotated expert solutions, 152 sub problemsderived from the expert set that focus on specific challenges (e.g.,configuring a trainer), and 602 automatically generated problems forlarger-scale development. We introduce various evaluation measures to assessboth task success and progress, utilizing gold solutions when available orapproximations otherwise. We show that state-of-the-art approaches struggle tosolve these problems with the best model (GPT-4o) solving only 16.3% of theend-to-end set, and 46.1% of the scenarios. This illustrates the challenge ofthis task, and suggests that SUPER can serve as a valuable resource for thecommunity to make and measure progress.</description><author>Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot</author><pubDate>Wed, 11 Sep 2024 17:37:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07440v1</guid></item><item><title>A Suite for Acoustic Language Model Evaluation</title><link>http://arxiv.org/abs/2409.07437v1</link><description>Speech language models have recently demonstrated great potential asuniversal speech processing systems. Such models have the ability to model therich acoustic information existing in audio signals, beyond spoken content,such as emotion, background noise, etc. Despite this, evaluation benchmarkswhich evaluate awareness to a wide range of acoustic aspects, are lacking. Tohelp bridge this gap, we introduce SALMon, a novel evaluation suiteencompassing background noise, emotion, speaker identity and room impulseresponse. The proposed benchmarks both evaluate the consistency of theinspected element and how much it matches the spoken text. We follow amodelling based approach, measuring whether a model gives correct sampleshigher scores than incorrect ones. This approach makes the benchmark fast tocompute even for large models. We evaluated several speech language models onSALMon, thus highlighting the strengths and weaknesses of each evaluatedmethod. Code and data are publicly available athttps://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .</description><author>Gallil Maimon, Amit Roth, Yossi Adi</author><pubDate>Wed, 11 Sep 2024 17:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07437v1</guid></item><item><title>Asymptotics of Stochastic Gradient Descent with Dropout Regularization in Linear Models</title><link>http://arxiv.org/abs/2409.07434v1</link><description>This paper proposes an asymptotic theory for online inference of thestochastic gradient descent (SGD) iterates with dropout regularization inlinear regression. Specifically, we establish the geometric-moment contraction(GMC) for constant step-size SGD dropout iterates to show the existence of aunique stationary distribution of the dropout recursive function. By the GMCproperty, we provide quenched central limit theorems (CLT) for the differencebetween dropout and $\ell^2$-regularized iterates, regardless ofinitialization. The CLT for the difference between the Ruppert-Polyak averagedSGD (ASGD) with dropout and $\ell^2$-regularized iterates is also presented.Based on these asymptotic normality results, we further introduce an onlineestimator for the long-run covariance matrix of ASGD dropout to facilitateinference in a recursive manner with efficiency in computational time andmemory. The numerical experiments demonstrate that for sufficiently largesamples, the proposed confidence intervals for ASGD with dropout nearly achievethe nominal coverage probability.</description><author>Jiaqi Li, Johannes Schmidt-Hieber, Wei Biao Wu</author><pubDate>Wed, 11 Sep 2024 17:28:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07434v1</guid></item><item><title>Physically Feasible Semantic Segmentation</title><link>http://arxiv.org/abs/2408.14672v2</link><description>State-of-the-art semantic segmentation models are typically optimized in adata-driven fashion, minimizing solely per-pixel classification objectives ontheir training data. This purely data-driven paradigm often leads to absurdsegmentations, especially when the domain of input images is shifted from theone encountered during training. For instance, state-of-the-art models mayassign the label ``road'' to a segment which is located above a segment that isrespectively labeled as ``sky'', although our knowledge of the physical worlddictates that such a configuration is not feasible for images captured byforward-facing upright cameras. Our method, Physically Feasible SemanticSegmentation (PhyFea), extracts explicit physical constraints that governspatial class relations from the training sets of semantic segmentationdatasets and enforces a differentiable loss function that penalizes violationsof these constraints to promote prediction feasibility. PhyFea yieldssignificant performance improvements in mIoU over each state-of-the-art networkwe use as baseline across ADE20K, Cityscapes and ACDC, notably a $1.5\%$improvement on ADE20K and a $2.1\%$ improvement on ACDC.</description><author>Shamik Basu, Luc Van Gool, Christos Sakaridis</author><pubDate>Wed, 11 Sep 2024 17:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14672v2</guid></item><item><title>Synthetic continued pretraining</title><link>http://arxiv.org/abs/2409.07431v1</link><description>Pretraining on large-scale, unstructured internet text has enabled languagemodels to acquire a significant amount of world knowledge. However, thisknowledge acquisition is data-inefficient -- to learn a given fact, models mustbe trained on hundreds to thousands of diverse representations of it. Thisposes a challenge when adapting a pretrained model to a small corpus ofdomain-specific documents, where each fact may appear rarely or only once. Wepropose to bridge this gap with synthetic continued pretraining: using thesmall domain-specific corpus to synthesize a large corpus more amenable tolearning, and then performing continued pretraining on the synthesized corpus.We instantiate this proposal with EntiGraph, a synthetic data augmentationalgorithm that extracts salient entities from the source documents and thengenerates diverse text by drawing connections between the sampled entities.Synthetic continued pretraining using EntiGraph enables a language model toanswer questions and follow generic instructions related to the sourcedocuments without access to them. If instead, the source documents areavailable at inference time, we show that the knowledge acquired through ourapproach compounds with retrieval-augmented generation. To better understandthese results, we build a simple mathematical model of EntiGraph, and show howsynthetic data augmentation can "rearrange" knowledge to enable moredata-efficient learning.</description><author>Zitong Yang, Neil Band, Shuangping Li, Emmanuel CandÃ¨s, Tatsunori Hashimoto</author><pubDate>Wed, 11 Sep 2024 17:21:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07431v1</guid></item><item><title>Agent Workflow Memory</title><link>http://arxiv.org/abs/2409.07429v1</link><description>Despite the potential of language model-based agents to solve real-worldtasks such as web navigation, current methods still struggle with long-horizontasks with complex action trajectories. In contrast, humans can flexibly solvecomplex tasks by learning reusable task workflows from past experiences andusing them to guide future actions. To build agents that can similarly benefitfrom this process, we introduce Agent Workflow Memory (AWM), a method forinducing commonly reused routines, i.e., workflows, and selectively providingworkflows to the agent to guide subsequent generations. AWM flexibly applies toboth offline and online scenarios, where agents induce workflows from trainingexamples beforehand or from test queries on the fly. We experiment on two majorweb navigation benchmarks -- Mind2Web and WebArena -- that collectively cover1000+ tasks from 200+ domains across travel, shopping, and social media, amongothers. AWM substantially improves the baseline results by 24.6% and 51.1%relative success rate on Mind2Web and WebArena while reducing the number ofsteps taken to solve WebArena tasks successfully. Furthermore, online AWMrobustly generalizes in cross-task, website, and domain evaluations, surpassingbaselines from 8.9 to 14.0 absolute points as train-test task distribution gapswiden.</description><author>Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, Graham Neubig</author><pubDate>Wed, 11 Sep 2024 17:21:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07429v1</guid></item><item><title>Latent-EnSF: A Latent Ensemble Score Filter for High-Dimensional Data Assimilation with Sparse Observation Data</title><link>http://arxiv.org/abs/2409.00127v3</link><description>Accurate modeling and prediction of complex physical systems often rely ondata assimilation techniques to correct errors inherent in model simulations.Traditional methods like the Ensemble Kalman Filter (EnKF) and its variants aswell as the recently developed Ensemble Score Filters (EnSF) face significantchallenges when dealing with high-dimensional and nonlinear Bayesian filteringproblems with sparse observations, which are ubiquitous in real-worldapplications. In this paper, we propose a novel data assimilation method,Latent-EnSF, which leverages EnSF with efficient and consistent latentrepresentations of the full states and sparse observations to address the jointchallenges of high dimensionlity in states and high sparsity in observationsfor nonlinear Bayesian filtering. We introduce a coupled VariationalAutoencoder (VAE) with two encoders to encode the full states and sparseobservations in a consistent way guaranteed by a latent distribution matchingand regularization as well as a consistent state reconstruction. Withcomparison to several methods, we demonstrate the higher accuracy, fasterconvergence, and higher efficiency of Latent-EnSF for two challengingapplications with complex models in shallow water wave propagation andmedium-range weather forecasting, for highly sparse observations in both spaceand time.</description><author>Phillip Si, Peng Chen</author><pubDate>Wed, 11 Sep 2024 17:18:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00127v3</guid></item><item><title>Deep Neural Network-Based Sign Language Recognition: A Comprehensive Approach Using Transfer Learning with Explainability</title><link>http://arxiv.org/abs/2409.07426v1</link><description>To promote inclusion and ensuring effective communication for those who relyon sign language as their main form of communication, sign language recognition(SLR) is crucial. Sign language recognition (SLR) seamlessly incorporates withdiverse technology, enhancing accessibility for the deaf community byfacilitating their use of digital platforms, video calls, and communicationdevices. To effectively solve this problem, we suggest a novel solution thatuses a deep neural network to fully automate sign language recognition. Thismethodology integrates sophisticated preprocessing methodologies to optimisethe overall performance. The architectures resnet, inception, xception, and vggare utilised to selectively categorise images of sign language. We prepared aDNN architecture and merged it with the pre-processing architectures. In thepost-processing phase, we utilised the SHAP deep explainer, which is based oncooperative game theory, to quantify the influence of specific features on theoutput of a machine learning model. Bhutanese-Sign-Language (BSL) dataset wasused for training and testing the suggested technique. While training onBhutanese-Sign-Language (BSL) dataset, overall ResNet50 with the DNN modelperformed better accuracy which is 98.90%. Our model's ability to provideinformational clarity was assessed using the SHAP (SHapley AdditiveexPlanations) method. In part to its considerable robustness and reliability,the proposed methodological approach can be used to develop a fully automatedsystem for sign language recognition.</description><author>A. E. M Ridwan, Mushfiqul Islam Chowdhury, Mekhala Mariam Mary, Md Tahmid Chowdhury Abir</author><pubDate>Wed, 11 Sep 2024 17:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07426v1</guid></item><item><title>VLMEvalKit: An Open-Source Toolkit for Evaluating Large Multi-Modality Models</title><link>http://arxiv.org/abs/2407.11691v2</link><description>We present VLMEvalKit: an open-source toolkit for evaluating largemulti-modality models based on PyTorch. The toolkit aims to provide auser-friendly and comprehensive framework for researchers and developers toevaluate existing multi-modality models and publish reproducible evaluationresults. In VLMEvalKit, we implement over 70 different large multi-modalitymodels, including both proprietary APIs and open-source models, as well as morethan 20 different multi-modal benchmarks. By implementing a single interface,new models can be easily added to the toolkit, while the toolkit automaticallyhandles the remaining workloads, including data preparation, distributedinference, prediction post-processing, and metric calculation. Although thetoolkit is currently mainly used for evaluating large vision-language models,its design is compatible with future updates that incorporate additionalmodalities, such as audio and video. Based on the evaluation results obtainedwith the toolkit, we host OpenVLM Leaderboard, a comprehensive leaderboard totrack the progress of multi-modality learning research. The toolkit is releasedat https://github.com/open-compass/VLMEvalKit and is actively maintained.</description><author>Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Amit Agarwal, Zhe Chen, Mo Li, Yubo Ma, Hailong Sun, Xiangyu Zhao, Junbo Cui, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, Kai Chen</author><pubDate>Wed, 11 Sep 2024 17:10:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11691v2</guid></item><item><title>Towards Fairer Health Recommendations: finding informative unbiased samples via Word Sense Disambiguation</title><link>http://arxiv.org/abs/2409.07424v1</link><description>There have been growing concerns around high-stake applications that rely onmodels trained with biased data, which consequently produce biased predictions,often harming the most vulnerable. In particular, biased medical data couldcause health-related applications and recommender systems to create outputsthat jeopardize patient care and widen disparities in health outcomes. A recentframework titled Fairness via AI posits that, instead of attempting to correctmodel biases, researchers must focus on their root causes by using AI to debiasdata. Inspired by this framework, we tackle bias detection in medical curriculausing NLP models, including LLMs, and evaluate them on a gold standard datasetcontaining 4,105 excerpts annotated by medical experts for bias from a largecorpus. We build on previous work by coauthors which augments the set ofnegative samples with non-annotated text containing social identifier terms.However, some of these terms, especially those related to race and ethnicity,can carry different meanings (e.g., "white matter of spinal cord"). To addressthis issue, we propose the use of Word Sense Disambiguation models to refinedataset quality by removing irrelevant sentences. We then evaluate fine-tunedvariations of BERT models as well as GPT models with zero- and few-shotprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable forbias detection, while fine-tuned BERT models generally perform well across allevaluated metrics.</description><author>Gavin Butts, Pegah Emdad, Jethro Lee, Shannon Song, Chiman Salavati, Willmar Sosa Diaz, Shiri Dori-Hacohen, Fabricio Murai</author><pubDate>Wed, 11 Sep 2024 17:10:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07424v1</guid></item><item><title>Enhancing adversarial robustness in Natural Language Inference using explanations</title><link>http://arxiv.org/abs/2409.07423v1</link><description>The surge of state-of-the-art Transformer-based models has undoubtedly pushedthe limits of NLP model performance, excelling in a variety of tasks. We castthe spotlight on the underexplored task of Natural Language Inference (NLI),since models trained on popular well-suited datasets are susceptible toadversarial attacks, allowing subtle input interventions to mislead the model.In this work, we validate the usage of natural language explanation as amodel-agnostic defence strategy through extensive experimentation: only byfine-tuning a classifier on the explanation rather than premise-hypothesisinputs, robustness under various adversarial attacks is achieved in comparisonto explanation-free baselines. Moreover, since there is no standard strategy oftesting the semantic validity of the generated explanations, we research thecorrelation of widely used language generation metrics with human perception,in order for them to serve as a proxy towards robust NLI models. Our approachis resource-efficient and reproducible without significant computationallimitations.</description><author>Alexandros Koulakos, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou</author><pubDate>Wed, 11 Sep 2024 17:09:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07423v1</guid></item><item><title>Engineering software 2.0 by Interpolating Neural Networks: Unifying Training, Solving, and Calibration</title><link>http://arxiv.org/abs/2404.10296v3</link><description>The evolution of artificial intelligence (AI) and neural network theories hasrevolutionized the way software is programmed, shifting from a hard-codedseries of codes, Software 1.0, to a vast neural network, Software 2.0. However,this transition in engineering software has faced challenges such as datascarcity, multi-modality of data, low model accuracy, and slow inference. Here,we propose a new network based on interpolation theories and tensordecomposition, the interpolating neural network (INN) to open the new era ofEngineering Software 2.0 that unifies training, solving, and calibration.Instead of interpolating training data, a common notion in computer science,INN interpolates grid points in the physical space whose coordinates and valuesare trainable. INN features orders of magnitude fewer trainable parameters (ordegrees of freedom for solving), faster training/solving, less inference cost,smaller memory footprint, and higher model accuracy compared to multi-layerperceptron (MLP) or physics-informed neural networks (PINN). Various numericalexperiments that cover computer science and engineering domains demonstratethat INN can solve over Zetta scale (10^{21}) partial differential equationsand train/calibrate a dataset with extraordinary accuracy but fewer parametersusing only a single graphics processing unit (GPU).</description><author>Chanwook Park, Sourav Saha, Jiachen Guo, Hantao Zhang, Xiaoyu Xie, Miguel A. Bessa, Dong Qian, Wei Chen, Gregory J. Wagner, Jian Cao, Wing Kam Liu</author><pubDate>Wed, 11 Sep 2024 17:08:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10296v3</guid></item><item><title>Controllable retinal image synthesis using conditional StyleGAN and latent space manipulation for improved diagnosis and grading of diabetic retinopathy</title><link>http://arxiv.org/abs/2409.07422v1</link><description>Diabetic retinopathy (DR) is a consequence of diabetes mellitus characterizedby vascular damage within the retinal tissue. Timely detection is paramount tomitigate the risk of vision loss. However, training robust grading models ishindered by a shortage of annotated data, particularly for severe cases. Thispaper proposes a framework for controllably generating high-fidelity anddiverse DR fundus images, thereby improving classifier performance in DRgrading and detection. We achieve comprehensive control over DR severity andvisual features (optic disc, vessel structure, lesion areas) within generatedimages solely through a conditional StyleGAN, eliminating the need for featuremasks or auxiliary networks. Specifically, leveraging the SeFa algorithm toidentify meaningful semantics within the latent space, we manipulate the DRimages generated conditionally on grades, further enhancing the datasetdiversity. Additionally, we propose a novel, effective SeFa-based dataaugmentation strategy, helping the classifier focus on discriminative regionswhile ignoring redundant features. Using this approach, a ResNet50 modeltrained for DR detection achieves 98.09% accuracy, 99.44% specificity, 99.45%precision, and an F1-score of 98.09%. Moreover, incorporating synthetic imagesgenerated by conditional StyleGAN into ResNet50 training for DR grading yields83.33% accuracy, a quadratic kappa score of 87.64%, 95.67% specificity, and72.24% precision. Extensive experiments conducted on the APTOS 2019 datasetdemonstrate the exceptional realism of the generated images and the superiorperformance of our classifier compared to recent studies.</description><author>Somayeh Pakdelmoez, Saba Omidikia, Seyyed Ali Seyyedsalehi, Seyyede Zohreh Seyyedsalehi</author><pubDate>Wed, 11 Sep 2024 17:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07422v1</guid></item><item><title>Approximation and generalization properties of the random projection classification method</title><link>http://arxiv.org/abs/2108.06339v4</link><description>The generalization gap of a classifier is related to the complexity of theset of functions among which the classifier is chosen. We study a family oflow-complexity classifiers consisting of thresholding a random one-dimensionalfeature. The feature is obtained by projecting the data on a random line afterembedding it into a higher-dimensional space parametrized by monomials of orderup to k. More specifically, the extended data is projected n-times and the bestclassifier among those n, based on its performance on training data, is chosen.We show that this type of classifier is extremely flexible as, given fullknowledge of the class conditional densities, under mild conditions, the errorof these classifiers would converge to the optimal (Bayes) error as k and n goto infinity. We also bound the generalization gap of the random classifiers. Ingeneral, these bounds are better than those for any classifier with VCdimension greater than O(ln n). In particular, the bounds imply that, unlessthe number of projections n is extremely large, the generalization gap of therandom projection approach is significantly smaller than that of a linearclassifier in the extended space. Thus, for certain classification problems(e.g., those with a large Rashomon ratio), there is a potntially large gain ingeneralization properties by selecting parameters at random, rather thanselecting the best one amongst the class.</description><author>Mireille Boutin, Evzenie Coupkova</author><pubDate>Wed, 11 Sep 2024 17:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.06339v4</guid></item><item><title>Efficient One-Step Diffusion Refinement for Snapshot Compressive Imaging</title><link>http://arxiv.org/abs/2409.07417v1</link><description>Coded Aperture Snapshot Spectral Imaging (CASSI) is a crucial technique forcapturing three-dimensional multispectral images (MSIs) through the complexinverse task of reconstructing these images from coded two-dimensionalmeasurements. Current state-of-the-art methods, predominantly end-to-end, facelimitations in reconstructing high-frequency details and often rely onconstrained datasets like KAIST and CAVE, resulting in models with poorgeneralizability. In response to these challenges, this paper introduces anovel one-step Diffusion Probabilistic Model within a self-supervisedadaptation framework for Snapshot Compressive Imaging (SCI). Our approachleverages a pretrained SCI reconstruction network to generate initialpredictions from two-dimensional measurements. Subsequently, a one-stepdiffusion model produces high-frequency residuals to enhance these initialpredictions. Additionally, acknowledging the high costs associated withcollecting MSIs, we develop a self-supervised paradigm based on the EquivariantImaging (EI) framework. Experimental results validate the superiority of ourmodel compared to previous methods, showcasing its simplicity and adaptabilityto various end-to-end or unfolding techniques.</description><author>Yunzhen Wang, Haijin Zeng, Shaoguang Huang, Hongyu Chen, Hongyan Zhang</author><pubDate>Wed, 11 Sep 2024 17:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07417v1</guid></item><item><title>Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise Recommendation</title><link>http://arxiv.org/abs/2409.07416v1</link><description>Modern listwise recommendation systems need to consider both long-term userperceptions and short-term interest shifts. Reinforcement learning can beapplied on recommendation to study such a problem but is also subject to largesearch space, sparse user feedback and long interactive latency. Motivated byrecent progress in hierarchical reinforcement learning, we propose a novelframework called mccHRL to provide different levels of temporal abstraction onlistwise recommendation. Within the hierarchical framework, the high-levelagent studies the evolution of user perception, while the low-level agentproduces the item selection policy by modeling the process as a sequentialdecision-making problem. We argue that such framework has a well-defineddecomposition of the outra-session context and the intra-session context, whichare encoded by the high-level and low-level agents, respectively. To verifythis argument, we implement both a simulator-based environment and anindustrial dataset-based experiment. Results observe significant performanceimprovement by our method, compared with several well-known baselines. Data andcodes have been made public.</description><author>Luo Ji, Gao Liu, Mingyang Yin, Hongxia Yang, Jingren Zhou</author><pubDate>Wed, 11 Sep 2024 17:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07416v1</guid></item><item><title>EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis</title><link>http://arxiv.org/abs/2409.06644v2</link><description>Early detection of eye diseases like glaucoma, macular degeneration, anddiabetic retinopathy is crucial for preventing vision loss. While artificialintelligence (AI) foundation models hold significant promise for addressingthese challenges, existing ophthalmic foundation models primarily focus on asingle modality, whereas diagnosing eye diseases requires multiple modalities.A critical yet often overlooked aspect is harnessing the multi-view informationacross various modalities for the same patient. Additionally, due to thelong-tail nature of ophthalmic diseases, standard fully supervised orunsupervised learning approaches often struggle. Therefore, it is essential tointegrate clinical text to capture a broader spectrum of diseases. We proposeEyeCLIP, a visual-language foundation model developed using over 2.77 millionmulti-modal ophthalmology images with partial text data. To fully leverage thelarge multi-modal unlabeled and labeled data, we introduced a pretrainingstrategy that combines self-supervised reconstructions, multi-modal imagecontrastive learning, and image-text contrastive learning to learn a sharedrepresentation of multiple modalities. Through evaluation using 14 benchmarkdatasets, EyeCLIP can be transferred to a wide range of downstream tasksinvolving ocular and systemic diseases, achieving state-of-the-art performancein disease classification, visual question answering, and cross-modalretrieval. EyeCLIP represents a significant advancement over previous methods,especially showcasing few-shot, even zero-shot capabilities in real-worldlong-tail scenarios.</description><author>Danli Shi, Weiyi Zhang, Jiancheng Yang, Siyu Huang, Xiaolan Chen, Mayinuer Yusufu, Kai Jin, Shan Lin, Shunming Liu, Qing Zhang, Mingguang He</author><pubDate>Wed, 11 Sep 2024 17:00:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06644v2</guid></item><item><title>SoK: Security and Privacy Risks of Medical AI</title><link>http://arxiv.org/abs/2409.07415v1</link><description>The integration of technology and healthcare has ushered in a new era wheresoftware systems, powered by artificial intelligence and machine learning, havebecome essential components of medical products and services. While theseadvancements hold great promise for enhancing patient care and healthcaredelivery efficiency, they also expose sensitive medical data and systemintegrity to potential cyberattacks. This paper explores the security andprivacy threats posed by AI/ML applications in healthcare. Through a thoroughexamination of existing research across a range of medical domains, we haveidentified significant gaps in understanding the adversarial attacks targetingmedical AI systems. By outlining specific adversarial threat models for medicalsettings and identifying vulnerable application domains, we lay the groundworkfor future research that investigates the security and resilience of AI-drivenmedical systems. Through our analysis of different threat models andfeasibility studies on adversarial attacks in different medical domains, weprovide compelling insights into the pressing need for cybersecurity researchin the rapidly evolving field of AI healthcare technology.</description><author>Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang</author><pubDate>Wed, 11 Sep 2024 16:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07415v1</guid></item><item><title>NVRC: Neural Video Representation Compression</title><link>http://arxiv.org/abs/2409.07414v1</link><description>Recent advances in implicit neural representation (INR)-based video codinghave demonstrated its potential to compete with both conventional and otherlearning-based approaches. With INR methods, a neural network is trained tooverfit a video sequence, with its parameters compressed to obtain a compactrepresentation of the video content. However, although promising results havebeen achieved, the best INR-based methods are still out-performed by the lateststandard codecs, such as VVC VTM, partially due to the simple model compressiontechniques employed. In this paper, rather than focusing on representationarchitectures as in many existing works, we propose a novel INR-based videocompression framework, Neural Video Representation Compression (NVRC),targeting compression of the representation. Based on the novel entropy codingand quantization models proposed, NVRC, for the first time, is able to optimizean INR-based video codec in a fully end-to-end manner. To further minimize theadditional bitrate overhead introduced by the entropy models, we have alsoproposed a new model compression framework for coding all the network,quantization and entropy model parameters hierarchically. Our experiments showthat NVRC outperforms many conventional and learning-based benchmark codecs,with a 24% average coding gain over VVC VTM (Random Access) on the UVG dataset,measured in PSNR. As far as we are aware, this is the first time an INR-basedvideo codec achieving such performance. The implementation of NVRC will bereleased at www.github.com.</description><author>Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David Bull</author><pubDate>Wed, 11 Sep 2024 16:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07414v1</guid></item><item><title>Manifold Learning via Foliations and Knowledge Transfer</title><link>http://arxiv.org/abs/2409.07412v1</link><description>Understanding how real data is distributed in high dimensional spaces is thekey to many tasks in machine learning. We want to provide a natural geometricstructure on the space of data employing a deep ReLU neural network trained asa classifier. Through the data information matrix (DIM), a variation of theFisher information matrix, the model will discern a singular foliationstructure on the space of data. We show that the singular points of suchfoliation are contained in a measure zero set, and that a local regularfoliation exists almost everywhere. Experiments show that the data iscorrelated with leaves of such foliation. Moreover we show the potential of ourapproach for knowledge transfer by analyzing the spectrum of the DIM to measuredistances between datasets.</description><author>E. Tron, E. Fioresi</author><pubDate>Wed, 11 Sep 2024 16:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07412v1</guid></item><item><title>Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries</title><link>http://arxiv.org/abs/2311.12573v3</link><description>The AI development community is increasingly making use of hostingintermediaries such as Hugging Face provide easy access to user-uploaded modelsand training data. These model marketplaces lower technical deployment barriersfor hundreds of thousands of users, yet can be used in numerous potentiallyharmful and illegal ways. In this article, we explain ways in which AI systems,which can both `contain' content and be open-ended tools, present one of thetrickiest platform governance challenges seen to date. We provide case studiesof several incidents across three illustrative platforms -- Hugging Face,GitHub and Civitai -- to examine how model marketplaces moderate models.Building on this analysis, we outline important (and yet nevertheless limited)practices that industry has been developing to respond to moderation demands:licensing, access and use restrictions, automated content moderation, and openpolicy development. While the policy challenge at hand is a considerable one,we conclude with some ideas as to how platforms could better mobilize resourcesto act as a careful, fair, and proportionate regulatory access point.</description><author>Robert Gorwa, Michael Veale</author><pubDate>Wed, 11 Sep 2024 16:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12573v3</guid></item><item><title>Robust Robot Walker: Learning Agile Locomotion over Tiny Traps</title><link>http://arxiv.org/abs/2409.07409v1</link><description>Quadruped robots must exhibit robust walking capabilities in practicalapplications. In this work, we propose a novel approach that enables quadrupedrobots to pass various small obstacles, or "tiny traps". Existing methods oftenrely on exteroceptive sensors, which can be unreliable for detecting such tinytraps. To overcome this limitation, our approach focuses solely onproprioceptive inputs. We introduce a two-stage training frameworkincorporating a contact encoder and a classification head to learn implicitrepresentations of different traps. Additionally, we design a set of tailoredreward functions to improve both the stability of training and the ease ofdeployment for goal-tracking tasks. To benefit further research, we design anew benchmark for tiny trap task. Extensive experiments in both simulation andreal-world settings demonstrate the effectiveness and robustness of our method.Project Page: https://robust-robot-walker.github.io/</description><author>Shaoting Zhu, Runhan Huang, Linzhan Mou, Hang Zhao</author><pubDate>Wed, 11 Sep 2024 16:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07409v1</guid></item><item><title>CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification</title><link>http://arxiv.org/abs/2409.07407v1</link><description>Large Language Models (LLMs) have shown great promise in vulnerabilityidentification. As C/C++ comprises half of the Open-Source Software (OSS)vulnerabilities over the past decade and updates in OSS mainly occur throughcommits, enhancing LLMs' ability to identify C/C++ Vulnerability-ContributingCommits (VCCs) is essential. However, current studies primarily focus onfurther pre-training LLMs on massive code datasets, which is resource-intensiveand poses efficiency challenges. In this paper, we enhance the ability ofBERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We proposeCodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++programs and LLMs. Based on commits, CLNX efficiently converts the source codeinto a more natural representation while preserving key details. Specifically,CLNX first applies structure-level naturalization to decompose complexprograms, followed by token-level naturalization to interpret complex symbols.We evaluate CLNX on public datasets of 25,872 C/C++ functions with theircommits. The results show that CLNX significantly enhances the performance ofLLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves newstate-of-the-art and identifies 38 OSS vulnerabilities in the real world.</description><author>Zeqing Qin, Yiwei Wu, Lansheng Han</author><pubDate>Wed, 11 Sep 2024 16:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07407v1</guid></item><item><title>What to align in multimodal contrastive learning?</title><link>http://arxiv.org/abs/2409.07402v1</link><description>Humans perceive the world through multisensory integration, blending theinformation of different modalities to adapt their behavior. Contrastivelearning offers an appealing solution for multimodal self-supervised learning.Indeed, by considering each modality as a different view of the same entity, itlearns to align features of different modalities in a shared representationspace. However, this approach is intrinsically limited as it only learns sharedor redundant information between modalities, while multimodal interactions canarise in other ways. In this work, we introduce CoMM, a Contrastive MultiModallearning strategy that enables the communication between modalities in a singlemultimodal space. Instead of imposing cross- or intra- modality constraints, wepropose to align multimodal representations by maximizing the mutualinformation between augmented versions of these multimodal features. Ourtheoretical analysis shows that shared, synergistic and unique terms ofinformation naturally emerge from this formulation, allowing us to estimatemultimodal interactions beyond redundancy. We test CoMM both in a controlledand in a series of real-world settings: in the former, we demonstrate that CoMMeffectively captures redundant, unique and synergistic information betweenmodalities. In the latter, CoMM learns complex multimodal interactions andachieves state-of-the-art results on the six multimodal benchmarks.</description><author>Benoit Dufumier, Javiera Castillo-Navarro, Devis Tuia, Jean-Philippe Thiran</author><pubDate>Wed, 11 Sep 2024 16:42:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07402v1</guid></item><item><title>Using ResNet to Utilize 4-class T2-FLAIR Slice Classification Based on the Cholinergic Pathways Hyperintensities Scale for Pathological Aging</title><link>http://arxiv.org/abs/2311.05477v2</link><description>The Cholinergic Pathways Hyperintensities Scale (CHIPS) is a visual ratingscale used to assess the extent of cholinergic white matter hyperintensities inT2-FLAIR images, serving as an indicator of dementia severity. However, themanual selection of four specific slices for rating throughout the entire brainis a time-consuming process. Our goal was to develop a deep learning-basedmodel capable of automatically identifying the four slices relevant to CHIPS.To achieve this, we trained a 4-class slice classification model (BSCA) usingthe ADNI T2-FLAIR dataset (N=150) with the assistance of ResNet. Subsequently,we tested the model's performance on a local dataset (N=30). The resultsdemonstrated the efficacy of our model, with an accuracy of 99.82% and anF1-score of 99.83%. This achievement highlights the potential impact of BSCA asan automatic screening tool, streamlining the selection of four specificT2-FLAIR slices that encompass white matter landmarks along the cholinergicpathways. Clinicians can leverage this tool to assess the risk of clinicaldementia development efficiently.</description><author>Wei-Chun Kevin Tsai, Yi-Chien Liu, Ming-Chun Yu, Chia-Ju Chou, Sui-Hing Yan, Yang-Teng Fan, Yan-Hsiang Huang, Yen-Ling Chiu, Yi-Fang Chuang, Ran-Zan Wang, Yao-Chia Shih</author><pubDate>Wed, 11 Sep 2024 16:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05477v2</guid></item><item><title>MCTR: Multi Camera Tracking Transformer</title><link>http://arxiv.org/abs/2408.13243v2</link><description>Multi-camera tracking plays a pivotal role in various real-worldapplications. While end-to-end methods have gained significant interest insingle-camera tracking, multi-camera tracking remains predominantly reliant onheuristic techniques. In response to this gap, this paper introducesMulti-Camera Tracking tRansformer (MCTR), a novel end-to-end approach tailoredfor multi-object detection and tracking across multiple cameras withoverlapping fields of view. MCTR leverages end-to-end detectors like DEtectorTRansformer (DETR) to produce detections and detection embeddings independentlyfor each camera view. The framework maintains set of track embeddings thatencaplusate global information about the tracked objects, and updates them atevery frame by integrating the local information from the view-specificdetection embeddings. The track embeddings are probabilistically associatedwith detections in every camera view and frame to generate consistent objecttracks. The soft probabilistic association facilitates the design ofdifferentiable losses that enable end-to-end training of the entire system. Tovalidate our approach, we conduct experiments on MMPTrack and AI CityChallenge, two recently introduced large-scale multi-camera multi-objecttracking datasets.</description><author>Alexandru Niculescu-Mizil, Deep Patel, Iain Melvin</author><pubDate>Wed, 11 Sep 2024 16:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13243v2</guid></item><item><title>Convergence of continuous-time stochastic gradient descent with applications to linear deep neural networks</title><link>http://arxiv.org/abs/2409.07401v1</link><description>We study a continuous-time approximation of the stochastic gradient descentprocess for minimizing the expected loss in learning problems. The main resultsestablish general sufficient conditions for the convergence, extending theresults of Chatterjee (2022) established for (nonstochastic) gradient descent.We show how the main result can be applied to the case of overparametrizedlinear neural network training.</description><author>Gabor Lugosi, Eulalia Nualart</author><pubDate>Wed, 11 Sep 2024 16:40:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07401v1</guid></item><item><title>Revisiting Static Feature-Based Android Malware Detection</title><link>http://arxiv.org/abs/2409.07397v1</link><description>The increasing reliance on machine learning (ML) in computer security,particularly for malware classification, has driven significant advancements.However, the replicability and reproducibility of these results are oftenoverlooked, leading to challenges in verifying research findings. This paperhighlights critical pitfalls that undermine the validity of ML research inAndroid malware detection, focusing on dataset and methodological issues. Wecomprehensively analyze Android malware detection using two datasets and assessoffline and continual learning settings with six widely used ML models. Ourstudy reveals that when properly tuned, simpler baseline methods can oftenoutperform more complex models. To address reproducibility challenges, wepropose solutions for improving datasets and methodological practices, enablingfairer model comparisons. Additionally, we open-source our code to facilitatemalware analysis, making it extensible for new models and datasets. Our paperaims to support future research in Android malware detection and other securitydomains, enhancing the reliability and reproducibility of published results.</description><author>Md Tanvirul Alam, Dipkamal Bhusal, Nidhi Rastogi</author><pubDate>Wed, 11 Sep 2024 16:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07397v1</guid></item><item><title>AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge</title><link>http://arxiv.org/abs/2409.07394v1</link><description>Knowledge conflict arises from discrepancies between information in thecontext of a large language model (LLM) and the knowledge stored in itsparameters. This can hurt performance when using standard decoding techniques,which tend to ignore the context. Existing test-time contrastive methods seekto address this by comparing the LLM's output distribution with and without thecontext and adjust the model according to the contrast between them. However,we find that these methods frequently misjudge the degree of conflict andstruggle to handle instances that vary in their amount of conflict, with staticmethods over-adjusting when conflict is absent. We propose a fine-grained,instance-level approach called AdaCAD, which dynamically infers the weight ofadjustment based on the degree of conflict, as measured by the Jensen-Shannondivergence between distributions representing contextual and parametricknowledge. Our experiments across four models on six diverse question-answering(QA) datasets and three summarization tasks demonstrate that our training-freeadaptive method consistently outperforms other decoding methods on QA, withaverage accuracy gains of 14.21% (absolute) over a static contrastive baseline,and improves the factuality of summaries by 5.59 (AlignScore). Furthermore, ouranalysis shows that while decoding with contrastive baselines hurts performancewhen conflict is absent, AdaCAD mitigates these losses, making it moreapplicable to real-world datasets in which some examples have conflict andothers do not.</description><author>Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Wed, 11 Sep 2024 16:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07394v1</guid></item><item><title>LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs</title><link>http://arxiv.org/abs/2409.02076v3</link><description>The abilities of long-context language models (LMs) are often evaluated usingthe "Needle-in-a-Haystack" (NIAH) test, which comprises tasks designed toassess a model's ability to identify specific information ("needle") withinlarge text sequences ("haystack"). While these benchmarks measure how wellmodels understand long-context input sequences, they do not effectively gaugethe quality of long-form text generation--a critical aspect for applicationssuch as design proposals and creative writing. To address this gap, we haveintroduced a new long-form text evaluation benchmark, LongGenbench, which testsmodels' ability to identify specific events within generated long textsequences. In this benchmark, we prompt long-context LMs to create long-formtext that must include particular events or constraints and evaluate theirability to incorporate these elements. We evaluated ten long-context LMs acrossfour distinct scenarios, three types of prompt instructions, and two differentgeneration-length settings (16K and 32K). Although these models perform well onNIAH benchmarks, none demonstrated satisfactory performance on theLongGenbench, raising concerns about their ability to generate coherentlong-form text that follows instructions. Additionally, as the length of thegenerated text increases, all models exhibit a significant drop in performance.</description><author>Yuhao Wu, Ming Shan Hee, Zhiqing Hu, Roy Ka-Wei Lee</author><pubDate>Wed, 11 Sep 2024 16:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02076v3</guid></item><item><title>A Scalable Algorithm for Active Learning</title><link>http://arxiv.org/abs/2409.07392v1</link><description>FIRAL is a recently proposed deterministic active learning algorithm formulticlass classification using logistic regression. It was shown to outperformthe state-of-the-art in terms of accuracy and robustness and comes withtheoretical performance guarantees. However, its scalability suffers whendealing with datasets featuring a large number of points $n$, dimensions $d$,and classes $c$, due to its $\mathcal{O}(c^2d^2+nc^2d)$ storage and$\mathcal{O}(c^3(nd^2 + bd^3 + bn))$ computational complexity where $b$ is thenumber of points to select in active learning. To address these challenges, wepropose an approximate algorithm with storage requirements reduced to$\mathcal{O}(n(d+c) + cd^2)$ and a computational complexity of$\mathcal{O}(bncd^2)$. Additionally, we present a parallel implementation onGPUs. We demonstrate the accuracy and scalability of our approach using MNIST,CIFAR-10, Caltech101, and ImageNet. The accuracy tests reveal no deteriorationin accuracy compared to FIRAL. We report strong and weak scaling tests on up to12 GPUs, for three million point synthetic dataset.</description><author>Youguang Chen, Zheyu Wen, George Biros</author><pubDate>Wed, 11 Sep 2024 16:34:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07392v1</guid></item><item><title>Explaining Text Classifiers with Counterfactual Representations</title><link>http://arxiv.org/abs/2402.00711v3</link><description>One well motivated explanation method for classifiers leveragescounterfactuals which are hypothetical events identical to real observations inall aspects except for one feature. Constructing such counterfactual posesspecific challenges for texts, however, as some attribute values may notnecessarily align with plausible real-world events. In this paper we propose asimple method for generating counterfactuals by intervening in the space oftext representations which bypasses this limitation. We argue that ourinterventions are minimally disruptive and that they are theoretically sound asthey align with counterfactuals as defined in Pearl's causal inferenceframework. To validate our method, we conducted experiments first on asynthetic dataset and then on a realistic dataset of counterfactuals. Thisallows for a direct comparison between classifier predictions based on groundtruth counterfactuals - obtained through explicit text interventions - and ourcounterfactuals, derived through interventions in the representation space.Eventually, we study a real world scenario where our counterfactuals can beleveraged both for explaining a classifier and for bias mitigation.</description><author>Pirmin Lemberger, Antoine Saillenfest</author><pubDate>Wed, 11 Sep 2024 16:32:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00711v3</guid></item><item><title>Alignist: CAD-Informed Orientation Distribution Estimation by Fusing Shape and Correspondences</title><link>http://arxiv.org/abs/2409.06683v2</link><description>Object pose distribution estimation is crucial in robotics for better pathplanning and handling of symmetric objects. Recent distribution estimationapproaches employ contrastive learning-based approaches by maximizing thelikelihood of a single pose estimate in the absence of a CAD model. We proposea pose distribution estimation method leveraging symmetry respectingcorrespondence distributions and shape information obtained using a CAD model.Contrastive learning-based approaches require an exhaustive amount of trainingimages from different viewpoints to learn the distribution properly, which isnot possible in realistic scenarios. Instead, we propose a pipeline that canleverage correspondence distributions and shape information from the CAD model,which are later used to learn pose distributions. Besides, having access topose distribution based on correspondences before learning pose distributionsconditioned on images, can help formulate the loss between distributions. Theprior knowledge of distribution also helps the network to focus on gettingsharper modes instead. With the CAD prior, our approach converges much fasterand learns distribution better by focusing on learning sharper distributionnear all the valid modes, unlike contrastive approaches, which focus on asingle mode at a time. We achieve benchmark results on SYMSOL-I and T-Lessdatasets.</description><author>Shishir Reddy Vutukur, Rasmus Laurvig Haugaard, Junwen Huang, Benjamin Busam, Tolga Birdal</author><pubDate>Wed, 11 Sep 2024 16:29:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06683v2</guid></item><item><title>Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation</title><link>http://arxiv.org/abs/2402.18334v3</link><description>We introduce Bonito, an open-source model for conditional task generationthat converts unannotated text into task-specific training datasets forinstruction tuning. We aim to enable zero-shot task adaptation of largelanguage models on users' specialized, private data. We train Bonito byfine-tuning a pretrained large language model on a new large-scale dataset with1.65M examples created by remixing existing instruction tuning datasets intometa-templates. The meta-templates for a dataset produce training exampleswhere the input is the unannotated text and the task attribute and the outputconsists of the instruction and the response. We use Bonito to generatesynthetic tasks for seven datasets from specialized domains with unannotatedtext across three task types -- yes-no question answering, extractive questionanswering, and natural language inference -- and adapt language models. We showthat Bonito significantly improves the average performance of pretrained andinstruction tuned models over the de facto self supervised baseline. Forexample, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistraland Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1points whereas the next word prediction objective undoes some of the benefitsof instruction tuning and reduces the average performance by 0.8 F1 points. Weconduct additional experiments with Bonito to understand the effects of thedomain, the size of the training set, and the choice of alternative synthetictask generators. Overall, we show that learning with synthetic instructiontuning datasets is an effective way to adapt language models to new domains.The model, dataset, and code are available athttps://github.com/BatsResearch/bonito.</description><author>Nihal V. Nayak, Yiyang Nan, Avi Trost, Stephen H. Bach</author><pubDate>Wed, 11 Sep 2024 16:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18334v3</guid></item><item><title>DreamMapping: High-Fidelity Text-to-3D Generation via Variational Distribution Mapping</title><link>http://arxiv.org/abs/2409.05099v2</link><description>Score Distillation Sampling (SDS) has emerged as a prevalent technique fortext-to-3D generation, enabling 3D content creation by distillingview-dependent information from text-to-2D guidance. However, they frequentlyexhibit shortcomings such as over-saturated color and excess smoothness. Inthis paper, we conduct a thorough analysis of SDS and refine its formulation,finding that the core design is to model the distribution of rendered images.Following this insight, we introduce a novel strategy called VariationalDistribution Mapping (VDM), which expedites the distribution modeling processby regarding the rendered images as instances of degradation fromdiffusion-based generation. This special design enables the efficient trainingof variational distribution by skipping the calculations of the Jacobians inthe diffusion U-Net. We also introduce timestep-dependent DistributionCoefficient Annealing (DCA) to further improve distilling precision. LeveragingVDM and DCA, we use Gaussian Splatting as the 3D representation and build atext-to-3D generation framework. Extensive experiments and evaluationsdemonstrate the capability of VDM and DCA to generate high-fidelity andrealistic assets with optimization efficiency.</description><author>Zeyu Cai, Duotun Wang, Yixun Liang, Zhijing Shao, Ying-Cong Chen, Xiaohang Zhan, Zeyu Wang</author><pubDate>Wed, 11 Sep 2024 16:27:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05099v2</guid></item><item><title>D-CAPTCHA++: A Study of Resilience of Deepfake CAPTCHA under Transferable Imperceptible Adversarial Attack</title><link>http://arxiv.org/abs/2409.07390v1</link><description>The advancements in generative AI have enabled the improvement of audiosynthesis models, including text-to-speech and voice conversion. This raisesconcerns about its potential misuse in social manipulation and politicalinterference, as synthetic speech has become indistinguishable from naturalhuman speech. Several speech-generation programs are utilized for maliciouspurposes, especially impersonating individuals through phone calls. Therefore,detecting fake audio is crucial to maintain social security and safeguard theintegrity of information. Recent research has proposed a D-CAPTCHA system basedon the challenge-response protocol to differentiate fake phone calls from realones. In this work, we study the resilience of this system and introduce a morerobust version, D-CAPTCHA++, to defend against fake calls. Specifically, wefirst expose the vulnerability of the D-CAPTCHA system under transferableimperceptible adversarial attack. Secondly, we mitigate such vulnerability byimproving the robustness of the system by using adversarial training inD-CAPTCHA deepfake detectors and task classifiers.</description><author>Hong-Hanh Nguyen-Le, Van-Tuan Tran, Dinh-Thuc Nguyen, Nhien-An Le-Khac</author><pubDate>Wed, 11 Sep 2024 16:25:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07390v1</guid></item><item><title>Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective</title><link>http://arxiv.org/abs/2409.07388v1</link><description>Multimodal affective computing (MAC) has garnered increasing attention due toits broad applications in analyzing human behaviors and intentions, especiallyin text-dominated multimodal affective computing field. This survey presentsthe recent trends of multimodal affective computing from NLP perspectivethrough four hot tasks: multimodal sentiment analysis, multimodal emotionrecognition in conversation, multimodal aspect-based sentiment analysis andmultimodal multi-label emotion recognition. The goal of this survey is toexplore the current landscape of multimodal affective research, identifydevelopment trends, and highlight the similarities and differences acrossvarious tasks, offering a comprehensive report on the recent progress inmultimodal affective computing from an NLP perspective. This survey covers theformalization of tasks, provides an overview of relevant works, describesbenchmark datasets, and details the evaluation metrics for each task.Additionally, it briefly discusses research in multimodal affective computinginvolving facial expressions, acoustic signals, physiological signals, andemotion causes. Additionally, we discuss the technical approaches, challenges,and future directions in multimodal affective computing. To support furtherresearch, we released a repository that compiles related works in multimodalaffective computing, providing detailed resources and references for thecommunity.</description><author>Guimin Hu, Yi Xin, Weimin Lyu, Haojian Huang, Chang Sun, Zhihong Zhu, Lin Gui, Ruichu Cai</author><pubDate>Wed, 11 Sep 2024 16:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07388v1</guid></item><item><title>A Contrastive Symmetric Forward-Forward Algorithm (SFFA) for Continual Learning Tasks</title><link>http://arxiv.org/abs/2409.07387v1</link><description>The so-called Forward-Forward Algorithm (FFA) has recently gained momentum asan alternative to the conventional back-propagation algorithm for neuralnetwork learning, yielding competitive performance across various modelingtasks. By replacing the backward pass of gradient back-propagation with twocontrastive forward passes, the FFA avoids several shortcomings undergone byits predecessor (e.g., vanishing/exploding gradient) by enabling layer-wisetraining heuristics. In classification tasks, this contrastive method has beenproven to effectively create a latent sparse representation of the input data,ultimately favoring discriminability. However, FFA exhibits an inherentasymmetric gradient behavior due to an imbalanced loss function betweenpositive and negative data, adversely impacting on the model's generalizationcapabilities and leading to an accuracy degradation. To address this issue,this work proposes the Symmetric Forward-Forward Algorithm (SFFA), a novelmodification of the original FFA which partitions each layer into positive andnegative neurons. This allows the local fitness function to be defined as theratio between the activation of positive neurons and the overall layeractivity, resulting in a symmetric loss landscape during the training phase. Toevaluate the enhanced convergence of our method, we conduct several experimentsusing multiple image classification benchmarks, comparing the accuracy ofmodels trained with SFFA to those trained with its FFA counterpart. As abyproduct of this reformulation, we explore the advantages of using alayer-wise training algorithm for Continual Learning (CL) tasks. Thespecialization of neurons and the sparsity of their activations induced bylayer-wise training algorithms enable efficient CL strategies that incorporatenew knowledge (classes) into the neural network, while preventing catastrophicforgetting of previously...</description><author>Erik B. Terres-Escudero, Javier Del Ser, Pablo Garcia Bringas</author><pubDate>Wed, 11 Sep 2024 16:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07387v1</guid></item><item><title>On the Improvement of Generalization and Stability of Forward-Only Learning via Neural Polarization</title><link>http://arxiv.org/abs/2408.09210v2</link><description>Forward-only learning algorithms have recently gained attention asalternatives to gradient backpropagation, replacing the backward step of thislatter solver with an additional contrastive forward pass. Among theseapproaches, the so-called Forward-Forward Algorithm (FFA) has been shown toachieve competitive levels of performance in terms of generalization andcomplexity. Networks trained using FFA learn to contrastively maximize alayer-wise defined goodness score when presented with real data (denoted aspositive samples) and to minimize it when processing synthetic data (corr.negative samples). However, this algorithm still faces weaknesses thatnegatively affect the model accuracy and training stability, primarily due to agradient imbalance between positive and negative samples. To overcome thisissue, in this work we propose a novel implementation of the FFA algorithm,denoted as Polar-FFA, which extends the original formulation by introducing aneural division (\emph{polarization}) between positive and negative instances.Neurons in each of these groups aim to maximize their goodness when presentedwith their respective data type, thereby creating a symmetric gradientbehavior. To empirically gauge the improved learning capabilities of ourproposed Polar-FFA, we perform several systematic experiments using differentactivation and goodness functions over image classification datasets. Ourresults demonstrate that Polar-FFA outperforms FFA in terms of accuracy andconvergence speed. Furthermore, its lower reliance on hyperparameters reducesthe need for hyperparameter tuning to guarantee optimal generalizationcapabilities, thereby allowing for a broader range of neural networkconfigurations.</description><author>Erik B. Terres-Escudero, Javier Del Ser, Pablo Garcia-Bringas</author><pubDate>Wed, 11 Sep 2024 16:13:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09210v2</guid></item><item><title>FIRAL: An Active Learning Algorithm for Multinomial Logistic Regression</title><link>http://arxiv.org/abs/2409.07379v1</link><description>We investigate theory and algorithms for pool-based active learning formulticlass classification using multinomial logistic regression. Using finitesample analysis, we prove that the Fisher Information Ratio (FIR) lower andupper bounds the excess risk. Based on our theoretical analysis, we propose anactive learning algorithm that employs regret minimization to minimize the FIR.To verify our derived excess risk bounds, we conduct experiments on syntheticdatasets. Furthermore, we compare FIRAL with five other methods and found thatour scheme outperforms them: it consistently produces the smallestclassification error in the multiclass logistic regression setting, asdemonstrated through experiments on MNIST, CIFAR-10, and 50-class ImageNet.</description><author>Youguang Chen, George Biros</author><pubDate>Wed, 11 Sep 2024 16:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07379v1</guid></item><item><title>Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring System via Language Model Coordination</title><link>http://arxiv.org/abs/2409.07372v1</link><description>The vast pre-existing slides serve as rich and important materials to carrylecture knowledge. However, effectively leveraging lecture slides to servestudents is difficult due to the multi-modal nature of slide content and theheterogeneous teaching actions. We study the problem of discovering effectivedesigns that convert a slide into an interactive lecture. We developSlide2Lecture, a tuning-free and knowledge-regulated intelligent tutoringsystem that can (1) effectively convert an input lecture slide into astructured teaching agenda consisting of a set of heterogeneous teachingactions; (2) create and manage an interactive lecture that generates responsiveinteractions catering to student learning demands while regulating theinteractions to follow teaching actions. Slide2Lecture contains a completepipeline for learners to obtain an interactive classroom experience to learnthe slide. For teachers and developers, Slide2Lecture enables customization tocater to personalized demands. The evaluation rated by annotators and studentsshows that Slide2Lecture is effective in outperforming the remainingimplementation. Slide2Lecture's online deployment has made more than 200Kinteraction with students in the 3K lecture sessions. We open sourceSlide2Lecture's implementation inhttps://anonymous.4open.science/r/slide2lecture-4210/.</description><author>Daniel Zhang-Li, Zheyuan Zhang, Jifan Yu, Joy Lim Jia Yin, Shangqing Tu, Linlu Gong, Haohua Wang, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li</author><pubDate>Wed, 11 Sep 2024 16:03:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07372v1</guid></item><item><title>The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing</title><link>http://arxiv.org/abs/2407.07786v2</link><description>Rapid progress in general-purpose AI has sparked significant interest in "redteaming," a practice of adversarial testing originating in military andcybersecurity applications. AI red teaming raises many questions about thehuman factor, such as how red teamers are selected, biases and blindspots inhow tests are conducted, and harmful content's psychological effects on redteamers. A growing body of HCI and CSCW literature examines relatedpractices-including data labeling, content moderation, and algorithmicauditing. However, few, if any have investigated red teaming itself. Futurestudies may explore topics ranging from fairness to mental health and otherareas of potential harm. We aim to facilitate a community of researchers andpractitioners who can begin to meet these challenges with creativity,innovation, and thoughtful reflection.</description><author>Alice Qian Zhang, Ryland Shaw, Jacy Reese Anthis, Ashlee Milton, Emily Tseng, Jina Suh, Lama Ahmad, Ram Shankar Siva Kumar, Julian Posada, Benjamin Shestakofsky, Sarah T. Roberts, Mary L. Gray</author><pubDate>Wed, 11 Sep 2024 16:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07786v2</guid></item><item><title>Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code</title><link>http://arxiv.org/abs/2409.07368v1</link><description>This paper introduces SGCode, a flexible prompt-optimizing system to generatesecure code with large language models (LLMs). SGCode integrates recentprompt-optimization approaches with LLMs in a unified system accessible throughfront-end and back-end APIs, enabling users to 1) generate secure code, whichis free of vulnerabilities, 2) review and share security analysis, and 3)easily switch from one prompt optimization approach to another, while providinginsights on model and system performance. We populated SGCode on an AWS serverwith PromSec, an approach that optimizes prompts by combining an LLM andsecurity tools with a lightweight generative adversarial graph neural networkto detect and fix security vulnerabilities in the generated code. Extensiveexperiments show that SGCode is practical as a public tool to gain insightsinto the trade-offs between model utility, secure code generation, and systemcost. SGCode has only a marginal cost compared with prompting LLMs. SGCode isavailable at: http://3.131.141.63:8501/.</description><author>Khiem Ton, Nhi Nguyen, Mahmoud Nazzal, Abdallah Khreishah, Cristian Borcea, NhatHai Phan, Ruoming Jin, Issa Khalil, Yelong Shen</author><pubDate>Wed, 11 Sep 2024 15:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07368v1</guid></item><item><title>Event-based Mosaicing Bundle Adjustment</title><link>http://arxiv.org/abs/2409.07365v1</link><description>We tackle the problem of mosaicing bundle adjustment (i.e., simultaneousrefinement of camera orientations and scene map) for a purely rotating eventcamera. We formulate the problem as a regularized non-linear least squaresoptimization. The objective function is defined using the linearized eventgeneration model in the camera orientations and the panoramic gradient map ofthe scene. We show that this BA optimization has an exploitable block-diagonalsparsity structure, so that the problem can be solved efficiently. To the bestof our knowledge, this is the first work to leverage such sparsity to speed upthe optimization in the context of event-based cameras, without the need toconvert events into image-like representations. We evaluate our method, calledEMBA, on both synthetic and real-world datasets to show its effectiveness (50%photometric error decrease), yielding results of unprecedented quality. Inaddition, we demonstrate EMBA using high spatial resolution event cameras,yielding delicate panoramas in the wild, even without an initial map. Projectpage: https://github.com/tub-rip/emba</description><author>Shuang Guo, Guillermo Gallego</author><pubDate>Wed, 11 Sep 2024 15:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07365v1</guid></item><item><title>Error-Driven Uncertainty Aware Training</title><link>http://arxiv.org/abs/2405.01205v2</link><description>Neural networks are often overconfident about their predictions, whichundermines their reliability and trustworthiness. In this work, we present anovel technique, named Error-Driven Uncertainty Aware Training (EUAT), whichaims to enhance the ability of neural classifiers to estimate their uncertaintycorrectly, namely to be highly uncertain when they output inaccuratepredictions and low uncertain when their output is accurate. The EUAT approachoperates during the model's training phase by selectively employing two lossfunctions depending on whether the training examples are correctly orincorrectly predicted by the model. This allows for pursuing the twofold goalof i) minimizing model uncertainty for correctly predicted inputs and ii)maximizing uncertainty for mispredicted inputs, while preserving the model'smisprediction rate. We evaluate EUAT using diverse neural models and datasetsin the image recognition domains considering both non-adversarial andadversarial settings. The results show that EUAT outperforms existingapproaches for uncertainty estimation (including other uncertainty-awaretraining techniques, calibration, ensembles, and DEUP) by providing uncertaintyestimates that not only have higher quality when evaluated via statisticalmetrics (e.g., correlation with residuals) but also when employed to buildbinary classifiers that decide whether the model's output can be trusted or notand under distributional data shifts.</description><author>Pedro Mendes, Paolo Romano, David Garlan</author><pubDate>Wed, 11 Sep 2024 15:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01205v2</guid></item><item><title>A Machine Learning Based Approach for Statistical Analysis of Detonation Cells from Soot Foils</title><link>http://arxiv.org/abs/2409.06466v2</link><description>This study presents a novel algorithm based on machine learning (ML) for theprecise segmentation and measurement of detonation cells from soot foil images,addressing the limitations of manual and primitive edge detection methodsprevalent in the field. Using advances in cellular biology segmentation models,the proposed algorithm is designed to accurately extract cellular patternswithout a training procedure or dataset, which is a significant challenge indetonation research. The algorithm's performance was validated using a seriesof test cases that mimic experimental and numerical detonation studies. Theresults demonstrated consistent accuracy, with errors remaining within 10%,even in complex cases. The algorithm effectively captured key cell metrics suchas cell area and span, revealing trends across different soot foil samples withuniform to highly irregular cellular structures. Although the model provedrobust, challenges remain in segmenting and analyzing highly complex orirregular cellular patterns. This work highlights the broad applicability andpotential of the algorithm to advance the understanding of detonation wavedynamics.</description><author>Vansh Sharma, Michael Ullman, Venkat Raman</author><pubDate>Wed, 11 Sep 2024 15:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06466v2</guid></item><item><title>Explicit Mutual Information Maximization for Self-Supervised Learning</title><link>http://arxiv.org/abs/2409.04747v2</link><description>Recently, self-supervised learning (SSL) has been extensively studied.Theoretically, mutual information maximization (MIM) is an optimal criterionfor SSL, with a strong theoretical foundation in information theory. However,it is difficult to directly apply MIM in SSL since the data distribution is notanalytically available in applications. In practice, many existing methods canbe viewed as approximate implementations of the MIM criterion. This work showsthat, based on the invariance property of MI, explicit MI maximization can beapplied to SSL under a generic distribution assumption, i.e., a relaxedcondition of the data distribution. We further illustrate this by analyzing thegeneralized Gaussian distribution. Based on this result, we derive a lossfunction based on the MIM criterion using only second-order statistics. Weimplement the new loss for SSL and demonstrate its effectiveness via extensiveexperiments.</description><author>Lele Chang, Peilin Liu, Qinghai Guo, Fei Wen</author><pubDate>Wed, 11 Sep 2024 15:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04747v2</guid></item><item><title>Quantifying Knee Cartilage Shape and Lesion: From Image to Metrics</title><link>http://arxiv.org/abs/2409.07361v1</link><description>Imaging features of knee articular cartilage have been shown to be potentialimaging biomarkers for knee osteoarthritis. Despite recent methodologicaladvancements in image analysis techniques like image segmentation,registration, and domain-specific image computing algorithms, only a few worksfocus on building fully automated pipelines for imaging feature extraction. Inthis study, we developed a deep-learning-based medical image analysisapplication for knee cartilage morphometrics, CartiMorph Toolbox (CMT). Weproposed a 2-stage joint template learning and registration network, CMT-reg.We trained the model using the OAI-ZIB dataset and assessed its performance intemplate-to-image registration. The CMT-reg demonstrated competitive resultscompared to other state-of-the-art models. We integrated the proposed modelinto an automated pipeline for the quantification of cartilage shape and lesion(full-thickness cartilage loss, specifically). The toolbox provides acomprehensive, user-friendly solution for medical image analysis and datavisualization. The software and models are available athttps://github.com/YongchengYAO/CMT-AMAI24paper .</description><author>Yongcheng Yao, Weitian Chen</author><pubDate>Wed, 11 Sep 2024 15:48:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07361v1</guid></item><item><title>CriticEval: Evaluating Large Language Model as Critic</title><link>http://arxiv.org/abs/2402.13764v4</link><description>Critique ability, i.e., the capability of Large Language Models (LLMs) toidentify and rectify flaws in responses, is crucial for their applications inself-improvement and scalable oversight. While numerous studies have beenproposed to evaluate critique ability of LLMs, their comprehensiveness andreliability are still limited. To overcome this problem, we introduceCriticEval, a novel benchmark designed to comprehensively and reliably evaluatecritique ability of LLMs. Specifically, to ensure the comprehensiveness,CriticEval evaluates critique ability from four dimensions across nine diversetask scenarios. It evaluates both scalar-valued and textual critiques,targeting responses of varying quality. To ensure the reliability, a largenumber of critiques are annotated to serve as references, enabling GPT-4 toevaluate textual critiques reliably. Extensive evaluations of open-source andclosed-source LLMs first validate the reliability of evaluation in CriticEval.Then, experimental results demonstrate the promising potential of open-sourceLLMs, the effectiveness of critique datasets and several intriguingrelationships between the critique ability and some critical factors, includingtask types, response qualities and critique dimensions. Datasets and evaluationtoolkit for CriticEval will be publicly released.</description><author>Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, Xian-ling Mao</author><pubDate>Wed, 11 Sep 2024 15:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13764v4</guid></item><item><title>Training-Free Guidance for Discrete Diffusion Models for Molecular Generation</title><link>http://arxiv.org/abs/2409.07359v1</link><description>Training-free guidance methods for continuous data have seen an explosion ofinterest due to the fact that they enable foundation diffusion models to bepaired with interchangable guidance models. Currently, equivalent guidancemethods for discrete diffusion models are unknown. We present a framework forapplying training-free guidance to discrete data and demonstrate its utility onmolecular graph generation tasks using the discrete diffusion modelarchitecture of DiGress. We pair this model with guidance functions that returnthe proportion of heavy atoms that are a specific atom type and the molecularweight of the heavy atoms and demonstrate our method's ability to guide thedata generation.</description><author>Thomas J. Kerby, Kevin R. Moon</author><pubDate>Wed, 11 Sep 2024 15:43:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07359v1</guid></item><item><title>Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation</title><link>http://arxiv.org/abs/2409.07355v1</link><description>This study introduces \textbf{InteractEval}, a framework that integrateshuman expertise and Large Language Models (LLMs) using the Think-Aloud (TA)method to generate attributes for checklist-based text evaluation. By combininghuman flexibility and reasoning with LLM consistency, InteractEval outperformstraditional non-LLM-based and LLM-based baselines across four distinctdimensions, consisting of Coherence, Fluency, Consistency, and Relevance. Theexperiment also investigates the effectiveness of the TA method, showing thatit promotes divergent thinking in both humans and LLMs, leading to thegeneration of a wider range of relevant attributes and enhance text evaluationperformance. Comparative analysis reveals that humans excel at identifyingattributes related to internal quality (Coherence and Fluency), but LLMsperform better at those attributes related to external alignment (Consistencyand Relevance). Consequently, leveraging both humans and LLMs together producesthe best evaluation outcomes. In other words, this study emphasizes thenecessity of effectively combining humans and LLMs in an automatedchecklist-based text evaluation framework. The code is available at\textbf{\url{https://github.com/BBeeChu/InteractEval.git}}.</description><author>SeongYeub Chu, JongWoo Kim, MunYong Yi</author><pubDate>Wed, 11 Sep 2024 15:40:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07355v1</guid></item><item><title>Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks</title><link>http://arxiv.org/abs/2409.07353v1</link><description>Large Vision-Language Models (LVLMs), trained on multimodal big datasets,have significantly advanced AI by excelling in vision-language tasks. However,these models remain vulnerable to adversarial attacks, particularly jailbreakattacks, which bypass safety protocols and cause the model to generatemisleading or harmful responses. This vulnerability stems from both theinherent susceptibilities of LLMs and the expanded attack surface introduced bythe visual modality. We propose Sim-CLIP+, a novel defense mechanism thatadversarially fine-tunes the CLIP vision encoder by leveraging a Siamesearchitecture. This approach maximizes cosine similarity between perturbed andclean samples, facilitating resilience against adversarial manipulations.Sim-CLIP+ offers a plug-and-play solution, allowing seamless integration intoexisting LVLM architectures as a robust vision encoder. Unlike previousdefenses, our method requires no structural modifications to the LVLM andincurs minimal computational overhead. Sim-CLIP+ demonstrates effectivenessagainst both gradient-based adversarial attacks and various jailbreaktechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attackstrategies and perform clean evaluations using standard downstream datasets,including COCO for image captioning and OKVQA for visual question answering.Extensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracywhile substantially improving robustness against both gradient-basedadversarial attacks and jailbreak techniques. Our code and robust visionencoders are available athttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.</description><author>Md Zarif Hossain, Ahmed Imteaj</author><pubDate>Wed, 11 Sep 2024 15:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07353v1</guid></item><item><title>Federated Impression for Learning with Distributed Heterogeneous Data</title><link>http://arxiv.org/abs/2409.07351v1</link><description>Standard deep learning-based classification approaches may not always bepractical in real-world clinical applications, as they require a centralizedcollection of all samples. Federated learning (FL) provides a paradigm that canlearn from distributed datasets across clients without requiring them to sharedata, which can help mitigate privacy and data ownership issues. In FL,sub-optimal convergence caused by data heterogeneity is common among data fromdifferent health centers due to the variety in data collection protocols andpatient demographics across centers. Through experimentation in this study, weshow that data heterogeneity leads to the phenomenon of catastrophic forgettingduring local training. We propose FedImpres which alleviates catastrophicforgetting by restoring synthetic data that represents the global informationas federated impression. To achieve this, we distill the global model resultingfrom each communication round. Subsequently, we use the synthetic dataalongside the local data to enhance the generalization of local training.Extensive experiments show that the proposed method achieves state-of-the-artperformance on both the BloodMNIST and Retina datasets, which contain labelimbalance and domain shift, with an improvement in classification accuracy ofup to 20%.</description><author>Sana Ayromlou, Atrin Arya, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li</author><pubDate>Wed, 11 Sep 2024 15:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07351v1</guid></item><item><title>Label Alignment Regularization for Distribution Shift</title><link>http://arxiv.org/abs/2211.14960v5</link><description>Recent work has highlighted the label alignment property (LAP) in supervisedlearning, where the vector of all labels in the dataset is mostly in the spanof the top few singular vectors of the data matrix. Drawing inspiration fromthis observation, we propose a regularization method for unsupervised domainadaptation that encourages alignment between the predictions in the targetdomain and its top singular vectors. Unlike conventional domain adaptationapproaches that focus on regularizing representations, we instead regularizethe classifier to align with the unsupervised target data, guided by the LAP inboth the source and target domains. Theoretical analysis demonstrates that,under certain assumptions, our solution resides within the span of the topright singular vectors of the target domain data and aligns with the optimalsolution. By removing the reliance on the commonly used optimal joint riskassumption found in classic domain adaptation theory, we showcase theeffectiveness of our method on addressing problems where traditional domainadaptation methods often fall short due to high joint error. Additionally, wereport improved performance over domain adaptation baselines in well-knowntasks such as MNIST-USPS domain adaptation and cross-lingual sentimentanalysis.</description><author>Ehsan Imani, Guojun Zhang, Runjia Li, Jun Luo, Pascal Poupart, Philip H. S. Torr, Yangchen Pan</author><pubDate>Wed, 11 Sep 2024 15:34:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14960v5</guid></item><item><title>The Role of Explainable AI in Revolutionizing Human Health Monitoring</title><link>http://arxiv.org/abs/2409.07347v1</link><description>The complex nature of disease mechanisms and the variability of patientsymptoms present significant obstacles in developing effective diagnostictools. Although machine learning has made considerable advances in medicaldiagnosis, its decision-making processes frequently lack transparency, whichcan jeopardize patient outcomes. This underscores the critical need forExplainable AI (XAI), which not only offers greater clarity but also has thepotential to significantly improve patient care. In this literature review, weconduct a detailed analysis of analyzing XAI methods identified throughsearches across various databases, focusing on chronic conditions such asParkinson's, stroke, depression, cancer, heart disease, and Alzheimer'sdisease. The literature search revealed the application of 9 trending XAIalgorithms in the field of healthcare and highlighted the pros and cons of eachof them. Thus, the article is concluded with a critical appraisal of thechallenges and future research opportunities for XAI in human healthmonitoring.</description><author>Abdullah Alharthi, Ahmed Alqurashi, Turki Alharbi, Mohammed Alammar, Nasser Aldosari, Houssem Bouchekara, Yusuf Shaaban, Mohammad Shoaib Shahriar, Abdulrahman Al Ayidh</author><pubDate>Wed, 11 Sep 2024 15:31:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07347v1</guid></item><item><title>BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event Stream</title><link>http://arxiv.org/abs/2407.02174v3</link><description>Neural implicit representation of visual scenes has attracted a lot ofattention in recent research of computer vision and graphics. Most priormethods focus on how to reconstruct 3D scene representation from a set ofimages. In this work, we demonstrate the possibility to recover the neuralradiance fields (NeRF) from a single blurry image and its corresponding eventstream. We model the camera motion with a cubic B-Spline in SE(3) space. Boththe blurry image and the brightness change within a time interval, can then besynthesized from the 3D scene representation given the 6-DoF poses interpolatedfrom the cubic B-Spline. Our method can jointly learn both the implicit neuralscene representation and recover the camera motion by minimizing thedifferences between the synthesized data and the real measurements withoutpre-computed camera poses from COLMAP. We evaluate the proposed method withboth synthetic and real datasets. The experimental results demonstrate that weare able to render view-consistent latent sharp images from the learned NeRFand bring a blurry image alive in high quality. Code and data are available athttps://github.com/wu-cvgl/BeNeRF.</description><author>Wenpu Li, Pian Wan, Peng Wang, Jinghang Li, Yi Zhou, Peidong Liu</author><pubDate>Wed, 11 Sep 2024 15:25:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02174v3</guid></item><item><title>Online Decision MetaMorphFormer: A Casual Transformer-Based Reinforcement Learning Framework of Universal Embodied Intelligence</title><link>http://arxiv.org/abs/2409.07341v1</link><description>Interactive artificial intelligence in the motion control field is aninteresting topic, especially when universal knowledge is adaptive to multipletasks and universal environments. Despite there being increasing efforts in thefield of Reinforcement Learning (RL) with the aid of transformers, most of themmight be limited by the offline training pipeline, which prohibits explorationand generalization abilities. To address this limitation, we propose theframework of Online Decision MetaMorphFormer (ODM) which aims to achieveself-awareness, environment recognition, and action planning through a unifiedmodel architecture. Motivated by cognitive and behavioral psychology, an ODMagent is able to learn from others, recognize the world, and practice itselfbased on its own experience. ODM can also be applied to any arbitrary agentwith a multi-joint body, located in different environments, and trained withdifferent types of tasks using large-scale pre-trained datasets. Through theuse of pre-trained datasets, ODM can quickly warm up and learn the necessaryknowledge to perform the desired task, while the target environment continuesto reinforce the universal policy. Extensive online experiments as well asfew-shot and zero-shot environmental tests are used to verify ODM's performanceand generalization ability. The results of our study contribute to the study ofgeneral artificial intelligence in embodied and cognitive fields. Code,results, and video examples can be found on the website\url{https://rlodm.github.io/odm/}.</description><author>Luo Ji, Runji Lin</author><pubDate>Wed, 11 Sep 2024 15:22:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07341v1</guid></item><item><title>A Framework for Predicting the Impact of Game Balance Changes through Meta Discovery</title><link>http://arxiv.org/abs/2409.07340v1</link><description>A metagame is a collection of knowledge that goes beyond the rules of a game.In competitive, team-based games like Pok\'emon or League of Legends, it refersto the set of current dominant characters and/or strategies within the playerbase. Developer changes to the balance of the game can have drastic andunforeseen consequences on these sets of meta characters. A framework forpredicting the impact of balance changes could aid developers in making moreinformed balance decisions. In this paper we present such a Meta Discoveryframework, leveraging Reinforcement Learning for automated testing of balancechanges. Our results demonstrate the ability to predict the outcome of balancechanges in Pok\'emon Showdown, a collection of competitive Pok\'emon tiers,with high accuracy.</description><author>Akash Saravanan, Matthew Guzdial</author><pubDate>Wed, 11 Sep 2024 15:20:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07340v1</guid></item><item><title>Benchmarking 2D Egocentric Hand Pose Datasets</title><link>http://arxiv.org/abs/2409.07337v1</link><description>Hand pose estimation from egocentric video has broad implications acrossvarious domains, including human-computer interaction, assistive technologies,activity recognition, and robotics, making it a topic of significant researchinterest. The efficacy of modern machine learning models depends on the qualityof data used for their training. Thus, this work is devoted to the analysis ofstate-of-the-art egocentric datasets suitable for 2D hand pose estimation. Wepropose a novel protocol for dataset evaluation, which encompasses not only theanalysis of stated dataset characteristics and assessment of data quality, butalso the identification of dataset shortcomings through the evaluation ofstate-of-the-art hand pose estimation models. Our study reveals that despitethe availability of numerous egocentric databases intended for 2D hand poseestimation, the majority are tailored for specific use cases. There is no idealbenchmark dataset yet; however, H2O and GANerated Hands datasets emerge as themost promising real and synthetic datasets, respectively.</description><author>Olga Taran, Damian M. Manzone, Jose Zariffa</author><pubDate>Wed, 11 Sep 2024 15:18:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07337v1</guid></item><item><title>Explanation, Debate, Align: A Weak-to-Strong Framework for Language Model Generalization</title><link>http://arxiv.org/abs/2409.07335v1</link><description>The rapid advancement of artificial intelligence systems has brought thechallenge of AI alignment to the forefront of research, particularly in complexdecision-making and task execution. As these systems surpass human-levelperformance in sophisticated problems, ensuring their alignment with humanvalues, intentions, and ethical guidelines becomes crucial. Building onprevious work in explanation generation for human-agent alignment, we addressthe more complex dynamics of multi-agent systems and human-AI teams. This paperintroduces a novel approach to model alignment through weak-to-stronggeneralization in the context of language models. We present a framework wherea strong model facilitates the improvement of a weaker model, bridging the gapbetween explanation generation and model alignment. Our method, formalized as afacilitation function, allows for the transfer of capabilities from advancedmodels to less capable ones without direct access to extensive training data.Our results suggest that this facilitation-based approach not only enhancesmodel performance but also provides insights into the nature of model alignmentand the potential for scalable oversight of AI systems.</description><author>Mehrdad Zakershahrak, Samira Ghodratnama</author><pubDate>Wed, 11 Sep 2024 15:16:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07335v1</guid></item><item><title>Learning to Compress Contexts for Efficient Knowledge-based Visual Question Answering</title><link>http://arxiv.org/abs/2409.07331v1</link><description>Multimodal Large Language Models (MLLMs) have demonstrated great zero-shotperformance on visual question answering (VQA). However, when it comes toknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specializeddomain knowledge to answer such questions and require obtaining necessaryinformation from external knowledge sources. Previous works likeRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much inputinformation, such as image-based textual descriptions and retrieved knowledge,as possible to improve performance, but they all overlook the issue that withthe number of input tokens increasing, inference efficiency significantlydecreases, which contradicts the demands of practical applications. To addressthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts(RACC). RACC learns to compress and aggregate retrieved contexts, from which itgenerates a compact modulation in the form of Key-Value (KV) cache. Thismodulation is then used to adapt the downstream frozen MLLM, thereby achievingeffective and efficient inference. RACC achieves a state-of-the-art (SOTA)performance of 62.9% on OK-VQA. Moreover, it significantly reduces inferencelatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experimentsshow RACC's broad applicability. It is compatible with various off-the-shelfMLLMs and can also handle different knowledge sources including textual andmultimodal documents.</description><author>Weixi Weng, Jieming Zhu, Hao Zhang, Xiaojun Meng, Rui Zhang, Chun Yuan</author><pubDate>Wed, 11 Sep 2024 15:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07331v1</guid></item><item><title>Current Symmetry Group Equivariant Convolution Frameworks for Representation Learning</title><link>http://arxiv.org/abs/2409.07327v1</link><description>Euclidean deep learning is often inadequate for addressing real-world signalswhere the representation space is irregular and curved with complex topologies.Interpreting the geometric properties of such feature spaces has becomeparamount in obtaining robust and compact feature representations that remainunaffected by nontrivial geometric transformations, which vanilla CNNs cannoteffectively handle. Recognizing rotation, translation, permutation, or scalesymmetries can lead to equivariance properties in the learned representations.This has led to notable advancements in computer vision and machine learningtasks under the framework of geometric deep learning, as compared to theirinvariant counterparts. In this report, we emphasize the importance of symmetrygroup equivariant deep learning models and their realization ofconvolution-like operations on graphs, 3D shapes, and non-Euclidean spaces byleveraging group theory and symmetry. We categorize them as regular, steerable,and PDE-based convolutions and thoroughly examine the inherent symmetries oftheir input spaces and ensuing representations. We also outline themathematical link between group convolutions or message aggregation operationsand the concept of equivariance. The report also highlights various datasets,their application scopes, limitations, and insightful observations on futuredirections to serve as a valuable reference and stimulate further research inthis emerging discipline.</description><author>Ramzan Basheer, Deepak Mishra</author><pubDate>Wed, 11 Sep 2024 15:07:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07327v1</guid></item><item><title>ART: Artifact Removal Transformer for Reconstructing Noise-Free Multichannel Electroencephalographic Signals</title><link>http://arxiv.org/abs/2409.07326v1</link><description>Artifact removal in electroencephalography (EEG) is a longstanding challengethat significantly impacts neuroscientific analysis and brain-computerinterface (BCI) performance. Tackling this problem demands advanced algorithms,extensive noisy-clean training data, and thorough evaluation strategies. Thisstudy presents the Artifact Removal Transformer (ART), an innovative EEGdenoising model employing transformer architecture to adeptly capture thetransient millisecond-scale dynamics characteristic of EEG signals. Ourapproach offers a holistic, end-to-end denoising solution for diverse artifacttypes in multichannel EEG data. We enhanced the generation of noisy-clean EEGdata pairs using an independent component analysis, thus fortifying thetraining scenarios critical for effective supervised learning. We performedcomprehensive validations using a wide range of open datasets from various BCIapplications, employing metrics like mean squared error and signal-to-noiseratio, as well as sophisticated techniques such as source localization and EEGcomponent classification. Our evaluations confirm that ART surpasses otherdeep-learning-based artifact removal methods, setting a new benchmark in EEGsignal processing. This advancement not only boosts the accuracy andreliability of artifact removal but also promises to catalyze furtherinnovations in the field, facilitating the study of brain dynamics innaturalistic environments.</description><author>Chun-Hsiang Chuang, Kong-Yi Chang, Chih-Sheng Huang, Anne-Mei Bessas</author><pubDate>Wed, 11 Sep 2024 15:05:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07326v1</guid></item><item><title>Statistically Valid Information Bottleneck via Multiple Hypothesis Testing</title><link>http://arxiv.org/abs/2409.07325v1</link><description>The information bottleneck (IB) problem is a widely studied framework inmachine learning for extracting compressed features that are informative fordownstream tasks. However, current approaches to solving the IB problem rely ona heuristic tuning of hyperparameters, offering no guarantees that the learnedfeatures satisfy information-theoretic constraints. In this work, we introducea statistically valid solution to this problem, referred to as IB via multiplehypothesis testing (IB-MHT), which ensures that the learned features meet theIB constraints with high probability, regardless of the size of the availabledataset. The proposed methodology builds on Pareto testing and learn-then-test(LTT), and it wraps around existing IB solvers to provide statisticalguarantees on the IB constraints. We demonstrate the performance of IB-MHT onclassical and deterministic IB formulations, validating the effectiveness ofIB-MHT in outperforming conventional methods in terms of statistical robustnessand reliability.</description><author>Amirmohammad Farzaneh, Osvaldo Simeone</author><pubDate>Wed, 11 Sep 2024 15:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07325v1</guid></item><item><title>Efficient and Unbiased Sampling of Boltzmann Distributions via Consistency Models</title><link>http://arxiv.org/abs/2409.07323v1</link><description>Diffusion models have shown promising potential for advancing BoltzmannGenerators. However, two critical challenges persist: (1) inherent errors insamples due to model imperfections, and (2) the requirement of hundreds offunctional evaluations (NFEs) to achieve high-quality samples. While existingsolutions like importance sampling and distillation address these issuesseparately, they are often incompatible, as most distillation models lack thenecessary density information for importance sampling. This paper introduces anovel sampling method that effectively combines Consistency Models (CMs) withimportance sampling. We evaluate our approach on both synthetic energyfunctions and equivariant n-body particle systems. Our method produces unbiasedsamples using only 6-25 NFEs while achieving a comparable Effective Sample Size(ESS) to Denoising Diffusion Probabilistic Models (DDPMs) that requireapproximately 100 NFEs.</description><author>Fengzhe Zhang, Jiajun He, Laurence I. Midgley, Javier AntorÃ¡n, JosÃ© Miguel HernÃ¡ndez-Lobato</author><pubDate>Wed, 11 Sep 2024 15:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07323v1</guid></item><item><title>Three-Dimensional, Multimodal Synchrotron Data for Machine Learning Applications</title><link>http://arxiv.org/abs/2409.07322v1</link><description>Machine learning techniques are being increasingly applied in medical andphysical sciences across a variety of imaging modalities; however, an importantissue when developing these tools is the availability of good quality trainingdata. Here we present a unique, multimodal synchrotron dataset of a bespokezinc-doped Zeolite 13X sample that can be used to develop advanced deeplearning and data fusion pipelines. Multi-resolution micro X-ray computedtomography was performed on a zinc-doped Zeolite 13X fragment to characteriseits pores and features, before spatially resolved X-ray diffraction computedtomography was carried out to characterise the homogeneous distribution ofsodium and zinc phases. Zinc absorption was controlled to create a simple,spatially isolated, two-phase material. Both raw and processed data isavailable as a series of Zenodo entries. Altogether we present a spatiallyresolved, three-dimensional, multimodal, multi-resolution dataset that can beused for the development of machine learning techniques. Such techniquesinclude development of super-resolution, multimodal data fusion, and 3Dreconstruction algorithm development.</description><author>Calum Green, Sharif Ahmed, Shashidhara Marathe, Liam Perera, Alberto Leonardi, Killian Gmyrek, Daniele Dini, James Le Houx</author><pubDate>Wed, 11 Sep 2024 15:00:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07322v1</guid></item><item><title>Module-wise Adaptive Adversarial Training for End-to-end Autonomous Driving</title><link>http://arxiv.org/abs/2409.07321v1</link><description>Recent advances in deep learning have markedly improved autonomous driving(AD) models, particularly end-to-end systems that integrate perception,prediction, and planning stages, achieving state-of-the-art performance.However, these models remain vulnerable to adversarial attacks, wherehuman-imperceptible perturbations can disrupt decision-making processes. Whileadversarial training is an effective method for enhancing model robustnessagainst such attacks, no prior studies have focused on its application toend-to-end AD models. In this paper, we take the first step in adversarialtraining for end-to-end AD models and present a novel Module-wise AdaptiveAdversarial Training (MA2T). However, extending conventional adversarialtraining to this context is highly non-trivial, as different stages within themodel have distinct objectives and are strongly interconnected. To addressthese challenges, MA2T first introduces Module-wise Noise Injection, whichinjects noise before the input of different modules, targeting training modelswith the guidance of overall objectives rather than each independent moduleloss. Additionally, we introduce Dynamic Weight Accumulation Adaptation, whichincorporates accumulated weight changes to adaptively learn and adjust the lossweights of each module based on their contributions (accumulated reductionrates) for better balance and robust training. To demonstrate the efficacy ofour defense, we conduct extensive experiments on the widely-used nuScenesdataset across several end-to-end AD models under both white-box and black-boxattacks, where our method outperforms other baselines by large margins(+5-10%). Moreover, we validate the robustness of our defense throughclosed-loop evaluation in the CARLA simulation environment, showing improvedresilience even against natural corruption.</description><author>Tianyuan Zhang, Lu Wang, Jiaqi Kang, Xinwei Zhang, Siyuan Liang, Yuwei Chen, Aishan Liu, Xianglong Liu</author><pubDate>Wed, 11 Sep 2024 15:00:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07321v1</guid></item><item><title>Explainable Identification of Hate Speech towards Islam using Graph Neural Networks</title><link>http://arxiv.org/abs/2311.04916v3</link><description>Islamophobic language on online platforms fosters intolerance, makingdetection and elimination crucial for promoting harmony. Traditional hatespeech detection models rely on NLP techniques like tokenization,part-of-speech tagging, and encoder-decoder models. However, Graph NeuralNetworks (GNNs), with their ability to utilize relationships between datapoints, offer more effective detection and greater explainability. In thiswork, we represent speeches as nodes and connect them with edges based on theircontext and similarity to develop the graph. This study introduces a novelparadigm using GNNs to identify and explain hate speech towards Islam. Ourmodel leverages GNNs to understand the context and patterns of hate speech byconnecting texts via pretrained NLP-generated word embeddings, achievingstate-of-the-art performance and enhancing detection accuracy while providingvaluable explanations. This highlights the potential of GNNs in combatingonline hate speech and fostering a safer, more inclusive online environment.</description><author>Azmine Toushik Wasi</author><pubDate>Wed, 11 Sep 2024 14:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04916v3</guid></item><item><title>MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications</title><link>http://arxiv.org/abs/2409.07314v1</link><description>The rapid development of Large Language Models (LLMs) for healthcareapplications has spurred calls for holistic evaluation beyond frequently-citedbenchmarks like USMLE, to better reflect real-world performance. Whilereal-world assessments are valuable indicators of utility, they often lagbehind the pace of LLM evolution, likely rendering findings obsolete upondeployment. This temporal disconnect necessitates a comprehensive upfrontevaluation that can guide model selection for specific clinical applications.We introduce MEDIC, a framework assessing LLMs across five critical dimensionsof clinical competence: medical reasoning, ethics and bias, data and languageunderstanding, in-context learning, and clinical safety. MEDIC features a novelcross-examination framework quantifying LLM performance across areas likecoverage and hallucination detection, without requiring reference outputs. Weapply MEDIC to evaluate LLMs on medical question-answering, safety,summarization, note generation, and other tasks. Our results show performancedisparities across model sizes, baseline vs medically finetuned models, andhave implications on model selection for applications requiring specific modelstrengths, such as low hallucination or lower cost of inference. MEDIC'smultifaceted evaluation reveals these performance trade-offs, bridging the gapbetween theoretical capabilities and practical implementation in healthcaresettings, ensuring that the most promising models are identified and adaptedfor diverse healthcare applications.</description><author>Praveen K Kanithi, ClÃ©ment Christophe, Marco AF Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan</author><pubDate>Wed, 11 Sep 2024 14:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07314v1</guid></item><item><title>Bayesian Quantile Regression with Subset Selection: A Posterior Summarization Perspective</title><link>http://arxiv.org/abs/2311.02043v3</link><description>Quantile regression is a powerful tool in epidemiological studies whereinterest lies in inferring how different exposures affect specific percentilesof the distribution of a health or life outcome. Existing methods eitherestimate conditional quantiles separately for each quantile of interest orestimate the entire conditional distribution using semi- or non-parametricmodels. The former often produce inadequate models for real data and do notshare information across quantiles, while the latter are characterized bycomplex and constrained models that can be difficult to interpret andcomputationally inefficient. Further, neither approach is well-suited forquantile-specific subset selection. Instead, we pose the fundamental problemsof linear quantile estimation, uncertainty quantification, and subset selectionfrom a Bayesian decision analysis perspective. For any Bayesian regressionmodel, we derive optimal and interpretable linear estimates and uncertaintyquantification for each model-based conditional quantile. Our approachintroduces a quantile-focused squared error loss, which enables efficient,closed-form computing and maintains a close relationship with Wasserstein-baseddensity estimation. In an extensive simulation study, our methods demonstratesubstantial gains in quantile estimation accuracy, variable selection, andinference over frequentist and Bayesian competitors. We use these tools toidentify and quantify the heterogeneous impacts of multiple social stressorsand environmental exposures on educational outcomes across the full spectrum oflow-, medium-, and high-achieving students in North Carolina.</description><author>Joseph Feldman, Daniel Kowal</author><pubDate>Wed, 11 Sep 2024 14:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02043v3</guid></item><item><title>AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts</title><link>http://arxiv.org/abs/2404.05993v2</link><description>As Large Language Models (LLMs) and generative AI become more widespread, thecontent safety risks associated with their use also increase. We find a notabledeficiency in high-quality content safety datasets and benchmarks thatcomprehensively cover a wide range of critical safety areas. To address this,we define a broad content safety risk taxonomy, comprising 13 critical risk and9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a newdataset of approximately 26, 000 human-LLM interaction instances, complete withhuman annotations adhering to the taxonomy. We plan to release this dataset tothe community to further research and to help benchmark LLM models for safety.To demonstrate the effectiveness of the dataset, we instruction-tune multipleLLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS),not only surpass or perform competitively with the state-of-the-art LLM-basedsafety models and general purpose LLMs, but also exhibit robustness acrossmultiple jail-break attack categories. We also show how usingAEGISSAFETYDATASET during the LLM alignment phase does not negatively impactthe performance of the aligned models on MT Bench scores. Furthermore, wepropose AEGIS, a novel application of a no-regret online adaptation frameworkwith strong theoretical guarantees, to perform content moderation with anensemble of LLM content safety experts in deployment</description><author>Shaona Ghosh, Prasoon Varshney, Erick Galinkin, Christopher Parisien</author><pubDate>Wed, 11 Sep 2024 14:42:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05993v2</guid></item><item><title>Medical diffusion on a budget: Textual Inversion for medical image generation</title><link>http://arxiv.org/abs/2303.13430v2</link><description>Diffusion models for text-to-image generation, known for their efficiency,accessibility, and quality, have gained popularity. While inference with thesesystems on consumer-grade GPUs is increasingly feasible, training from scratchrequires large captioned datasets and significant computational resources. Inmedical image generation, the limited availability of large, publiclyaccessible datasets with text reports poses challenges due to legal and ethicalconcerns. This work shows that adapting pre-trained Stable Diffusion models tomedical imaging modalities is achievable by training text embeddings usingTextual Inversion. In this study, we experimented with small medical datasets(100 samples each from three modalities) and trained within hours to generatediagnostically accurate images, as judged by an expert radiologist. Experimentswith Textual Inversion training and inference parameters reveal the necessityof larger embeddings and more examples in the medical domain. Classificationexperiments show an increase in diagnostic accuracy (AUC) for detectingprostate cancer on MRI, from 0.78 to 0.80. Further experiments demonstrateembedding flexibility through disease interpolation, combining pathologies, andinpainting for precise disease appearance control. The trained embeddings arecompact (less than 1 MB), enabling easy data sharing with reduced privacyconcerns.</description><author>Bram de Wilde, Anindo Saha, Maarten de Rooij, Henkjan Huisman, Geert Litjens</author><pubDate>Wed, 11 Sep 2024 14:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13430v2</guid></item><item><title>Optimizing Neural Network Performance and Interpretability with Diophantine Equation Encoding</title><link>http://arxiv.org/abs/2409.07310v1</link><description>This paper explores the integration of Diophantine equations into neuralnetwork (NN) architectures to improve model interpretability, stability, andefficiency. By encoding and decoding neural network parameters as integersolutions to Diophantine equations, we introduce a novel approach that enhancesboth the precision and robustness of deep learning models. Our methodintegrates a custom loss function that enforces Diophantine constraints duringtraining, leading to better generalization, reduced error bounds, and enhancedresilience against adversarial attacks. We demonstrate the efficacy of thisapproach through several tasks, including image classification and naturallanguage processing, where improvements in accuracy, convergence, androbustness are observed. This study offers a new perspective on combiningmathematical theory and machine learning to create more interpretable andefficient models.</description><author>Ronald Katende</author><pubDate>Wed, 11 Sep 2024 14:38:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07310v1</guid></item><item><title>Non-Invasive Glucose Prediction System Enhanced by Mixed Linear Models and Meta-Forests for Domain Generalization</title><link>http://arxiv.org/abs/2409.07308v1</link><description>In this study, we present a non-invasive glucose prediction system thatintegrates Near-Infrared (NIR) spectroscopy and millimeter-wave (mm-wave)sensing. We employ a Mixed Linear Model (MixedLM) to analyze the associationbetween mm-wave frequency S_21 parameters and blood glucose levels within aheterogeneous dataset. The MixedLM method considers inter-subject variabilityand integrates multiple predictors, offering a more comprehensive analysis thantraditional correlation analysis. Additionally, we incorporate a DomainGeneralization (DG) model, Meta-forests, to effectively handle domain variancein the dataset, enhancing the model's adaptability to individual differences.Our results demonstrate promising accuracy in glucose prediction for unseensubjects, with a mean absolute error (MAE) of 17.47 mg/dL, a root mean squareerror (RMSE) of 31.83 mg/dL, and a mean absolute percentage error (MAPE) of10.88%, highlighting its potential for clinical application. This study marks asignificant step towards developing accurate, personalized, and non-invasiveglucose monitoring systems, contributing to improved diabetes management.</description><author>Yuyang Sun, Panagiotis Kosmas</author><pubDate>Wed, 11 Sep 2024 14:36:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07308v1</guid></item><item><title>Data Augmentation via Latent Diffusion for Saliency Prediction</title><link>http://arxiv.org/abs/2409.07307v1</link><description>Saliency prediction models are constrained by the limited diversity andquantity of labeled data. Standard data augmentation techniques such asrotating and cropping alter scene composition, affecting saliency. We propose anovel data augmentation method for deep saliency prediction that edits naturalimages while preserving the complexity and variability of real-world scenes.Since saliency depends on high-level and low-level features, our approachinvolves learning both by incorporating photometric and semantic attributessuch as color, contrast, brightness, and class. To that end, we introduce asaliency-guided cross-attention mechanism that enables targeted edits on thephotometric properties, thereby enhancing saliency within specific imageregions. Experimental results show that our data augmentation methodconsistently improves the performance of various saliency models. Moreover,leveraging the augmentation features for saliency prediction yields superiorperformance on publicly available saliency benchmarks. Our predictions alignclosely with human visual attention patterns in the edited images, as validatedby a user study.</description><author>Bahar Aydemir, Deblina Bhattacharjee, Tong Zhang, Mathieu Salzmann, Sabine SÃ¼sstrunk</author><pubDate>Wed, 11 Sep 2024 14:36:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07307v1</guid></item><item><title>LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Medical Image Segmentation</title><link>http://arxiv.org/abs/2404.05102v2</link><description>The rise of Transformer architectures has revolutionized medical imagesegmentation, leading to hybrid models that combine Convolutional NeuralNetworks (CNNs) and Transformers for enhanced accuracy. However, these modelsoften suffer from increased complexity and overlook the interplay betweenspatial and channel features, which is vital for segmentation precision. Weintroduce LHU-Net, a streamlined Hybrid U-Net for volumetric medical imagesegmentation, designed to first analyze spatial and then channel features foreffective feature extraction. Tested on five benchmark datasets (Synapse, LA,Pancreas, ACDC, BRaTS 2018), LHU-Net demonstrated superior efficiency andaccuracy, notably achieving a 92.66 Dice score on ACDC with 85\% fewerparameters and a quarter of the computational demand compared to leadingmodels. This performance, achieved without pre-training, extra data, or modelensembles, sets new benchmarks for computational efficiency and accuracy insegmentation, using under 11 million parameters. This achievement highlightsthat balancing computational efficiency with high accuracy in medical imagesegmentation is feasible. Our implementation of LHU-Net is freely accessible tothe research community on GitHub (https://github.com/xmindflow/LHUNet).</description><author>Yousef Sadegheih, Afshin Bozorgpour, Pratibha Kumari, Reza Azad, Dorit Merhof</author><pubDate>Wed, 11 Sep 2024 14:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05102v2</guid></item><item><title>Lossless Image Compression Using Multi-level Dictionaries: Binary Images</title><link>http://arxiv.org/abs/2406.03087v3</link><description>Lossless image compression is required in various applications to reducestorage or transmission costs of images, while requiring the reconstructedimages to have zero information loss compared to the original. Existinglossless image compression methods either have simple design but poorcompression performance, or complex design, better performance, but with noperformance guarantees. In our endeavor to develop a lossless image compressionmethod with low complexity and guaranteed performance, we argue thatcompressibility of a color image is essentially derived from the patterns inits spatial structure, intensity variations, and color variations. Thus, wedivide the overall design of a lossless image compression scheme into threeparts that exploit corresponding redundancies. We further argue that thebinarized version of an image captures its fundamental spatial structure. Inthis first part of our work, we propose a scheme for lossless compression ofbinary images. The proposed scheme first learns dictionaries of $16\times16$, $8\times8$,$4\times4$, and $2\times 2$ square pixel patterns from various datasets ofbinary images. It then uses these dictionaries to encode binary images. Thesedictionaries have various interesting properties that are further exploited toconstruct an efficient and scalable scheme. Our preliminary results show thatthe proposed scheme consistently outperforms existing conventional and learningbased lossless compression approaches, and provides, on average, as much as$1.5\times$ better performance than a common general purpose losslesscompression scheme (WebP), more than $3\times$ better performance than a stateof the art learning based scheme, and better performance than a specializedscheme for binary image compression (JBIG2).</description><author>Samar Agnihotri, Renu Rameshan, Ritwik Ghosal</author><pubDate>Wed, 11 Sep 2024 14:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03087v3</guid></item><item><title>BLS-GAN: A Deep Layer Separation Framework for Eliminating Bone Overlap in Conventional Radiographs</title><link>http://arxiv.org/abs/2409.07304v1</link><description>Conventional radiography is the widely used imaging technology in diagnosing,monitoring, and prognosticating musculoskeletal (MSK) diseases because of itseasy availability, versatility, and cost-effectiveness. In conventionalradiographs, bone overlaps are prevalent, and can impede the accurateassessment of bone characteristics by radiologists or algorithms, posingsignificant challenges to conventional and computer-aided diagnoses. This workinitiated the study of a challenging scenario - bone layer separation inconventional radiographs, in which separate overlapped bone regions enable theindependent assessment of the bone characteristics of each bone layer and laythe groundwork for MSK disease diagnosis and its automation. This work proposeda Bone Layer Separation GAN (BLS-GAN) framework that can produce high-qualitybone layer images with reasonable bone characteristics and texture. Thisframework introduced a reconstructor based on conventional radiography imagingprinciples, which achieved efficient reconstruction and mitigates the recurrentcalculations and training instability issues caused by soft tissue in theoverlapped regions. Additionally, pre-training with synthetic images wasimplemented to enhance the stability of both the training process and theresults. The generated images passed the visual Turing test, and improvedperformance in downstream tasks. This work affirms the feasibility ofextracting bone layer images from conventional radiographs, which holds promisefor leveraging bone layer separation technology to facilitate morecomprehensive analytical research in MSK diagnosis, monitoring, and prognosis.Code and dataset will be made available.</description><author>Haolin Wang, Yafei Ou, Prasoon Ambalathankandy, Gen Ota, Pengyu Dai, Masayuki Ikebe, Kenji Suzuki, Tamotsu Kamishima</author><pubDate>Wed, 11 Sep 2024 14:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07304v1</guid></item><item><title>GMT: Guided Mask Transformer for Leaf Instance Segmentation</title><link>http://arxiv.org/abs/2406.17109v2</link><description>Leaf instance segmentation is a challenging multi-instance segmentation task,aiming to separate and delineate each leaf in an image of a plant. Accuratesegmentation of each leaf is crucial for plant-related applications such as thefine-grained monitoring of plant growth and crop yield estimation. This task ischallenging because of the high similarity (in shape and colour), great sizevariation, and heavy occlusions among leaf instances. Furthermore, thetypically small size of annotated leaf datasets makes it more difficult tolearn the distinctive features needed for precise segmentation. We hypothesisethat the key to overcoming the these challenges lies in the specific spatialpatterns of leaf distribution. In this paper, we propose the Guided MaskTransformer (GMT), which leverages and integrates leaf spatial distributionpriors into a Transformer-based segmentor. These spatial priors are embedded ina set of guide functions that map leaves at different positions into a moreseparable embedding space. Our GMT consistently outperforms thestate-of-the-art on three public plant datasets.</description><author>Feng Chen, Sotirios A. Tsaftaris, Mario Valerio Giuffrida</author><pubDate>Wed, 11 Sep 2024 14:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17109v2</guid></item><item><title>EmoVOCA: Speech-Driven Emotional 3D Talking Heads</title><link>http://arxiv.org/abs/2403.12886v2</link><description>The domain of 3D talking head generation has witnessed significant progressin recent years. A notable challenge in this field consists in blendingspeech-related motions with expression dynamics, which is primarily caused bythe lack of comprehensive 3D datasets that combine diversity in spokensentences with a variety of facial expressions. Whereas literature worksattempted to exploit 2D video data and parametric 3D models as a workaround,these still show limitations when jointly modeling the two motions. In thiswork, we address this problem from a different perspective, and propose aninnovative data-driven technique that we used for creating a synthetic dataset,called EmoVOCA, obtained by combining a collection of inexpressive 3D talkingheads and a set of 3D expressive sequences. To demonstrate the advantages ofthis approach, and the quality of the dataset, we then designed and trained anemotional 3D talking head generator that accepts a 3D face, an audio file, anemotion label, and an intensity value as inputs, and learns to animate theaudio-synchronized lip movements with expressive traits of the face.Comprehensive experiments, both quantitative and qualitative, using our dataand generator evidence superior ability in synthesizing convincing animations,when compared with the best performing methods in the literature. Our code andpre-trained model will be made available.</description><author>Federico Nocentini, Claudio Ferrari, Stefano Berretti</author><pubDate>Wed, 11 Sep 2024 14:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12886v2</guid></item><item><title>Exclusive Style Removal for Cross Domain Novel Class Discovery</title><link>http://arxiv.org/abs/2406.18140v2</link><description>As a promising field in open-world learning, \textit{Novel Class Discovery}(NCD) is usually a task to cluster unseen novel classes in an unlabeled setbased on the prior knowledge of labeled data within the same domain. However,the performance of existing NCD methods could be severely compromised whennovel classes are sampled from a different distribution with the labeled ones.In this paper, we explore and establish the solvability of NCD in cross domainsetting with the necessary condition that style information must be removed.Based on the theoretical analysis, we introduce an exclusive style removalmodule for extracting style information that is distinctive from the baselinefeatures, thereby facilitating inference. Moreover, this module is easy tointegrate with other NCD methods, acting as a plug-in to improve performance onnovel classes with different distributions compared to the seen labeled set.Additionally, recognizing the non-negligible influence of different backbonesand pre-training strategies on the performance of the NCD methods, we build afair benchmark for future NCD research. Extensive experiments on three commondatasets demonstrate the effectiveness of our proposed module.</description><author>Yicheng Wang, Feng Liu, Junmin Liu, Zhen Fang, Kai Sun</author><pubDate>Wed, 11 Sep 2024 14:27:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18140v2</guid></item><item><title>PaveSAM Segment Anything for Pavement Distress</title><link>http://arxiv.org/abs/2409.07295v1</link><description>Automated pavement monitoring using computer vision can analyze pavementconditions more efficiently and accurately than manual methods. Accuratesegmentation is essential for quantifying the severity and extent of pavementdefects and consequently, the overall condition index used for prioritizingrehabilitation and maintenance activities. Deep learning-based segmentationmodels are however, often supervised and require pixel-level annotations, whichcan be costly and time-consuming. While the recent evolution of zero-shotsegmentation models can generate pixel-wise labels for unseen classes withoutany training data, they struggle with irregularities of cracks and texturedpavement backgrounds. This research proposes a zero-shot segmentation model,PaveSAM, that can segment pavement distresses using bounding box prompts. Byretraining SAM's mask decoder with just 180 images, pavement distresssegmentation is revolutionized, enabling efficient distress segmentation usingbounding box prompts, a capability not found in current segmentation models.This not only drastically reduces labeling efforts and costs but also showcasesour model's high performance with minimal input, establishing the pioneeringuse of SAM in pavement distress segmentation. Furthermore, researchers can useexisting open-source pavement distress images annotated with bounding boxes tocreate segmentation masks, which increases the availability and diversity ofsegmentation pavement distress datasets.</description><author>Neema Jakisa Owor, Yaw Adu-Gyamfi, Armstrong Aboah, Mark Amo-Boateng</author><pubDate>Wed, 11 Sep 2024 14:24:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07295v1</guid></item><item><title>$\textit{sweet}$- An Open Source Modular Platform for Contactless Hand Vascular Biometric Experiments</title><link>http://arxiv.org/abs/2404.09376v2</link><description>Current finger-vein or palm-vein recognition systems usually require directcontact of the subject with the apparatus. This can be problematic inenvironments where hygiene is of primary importance. In this work we present acontactless vascular biometrics sensor platform named \sweet which can be usedfor hand vascular biometrics studies (wrist, palm, and finger-vein) and surfacefeatures such as palmprint. It supports several acquisition modalities such asmulti-spectral Near-Infrared (NIR), RGB-color, Stereo Vision (SV) andPhotometric Stereo (PS). Using this platform we collect a dataset consisting ofthe fingers, palm and wrist vascular data of 120 subjects and develop apowerful 3D pipeline for the pre-processing of this data. We then presentbiometric experimental results, focusing on Finger-Vein Recognition (FVR).Finally, we discuss fusion of multiple modalities, such palm-vein combined withpalm-print biometrics. The acquisition software, parts of the hardware design,the new FV dataset, as well as source-code for our experiments are publiclyavailable for research purposes.</description><author>David GeissbÃ¼hler, Sushil Bhattacharjee, Ketan Kotwal, Guillaume Clivaz, SÃ©bastien Marcel</author><pubDate>Wed, 11 Sep 2024 14:22:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09376v2</guid></item><item><title>A Unified Contrastive Loss for Self-Training</title><link>http://arxiv.org/abs/2409.07292v1</link><description>Self-training methods have proven to be effective in exploiting abundantunlabeled data in semi-supervised learning, particularly when labeled data isscarce. While many of these approaches rely on a cross-entropy loss function(CE), recent advances have shown that the supervised contrastive loss function(SupCon) can be more effective. Additionally, unsupervised contrastive learningapproaches have also been shown to capture high quality data representations inthe unsupervised setting. To benefit from these advantages in a semi-supervisedsetting, we propose a general framework to enhance self-training methods, whichreplaces all instances of CE losses with a unique contrastive loss. By usingclass prototypes, which are a set of class-wise trainable parameters, werecover the probability distributions of the CE setting and show a theoreticalequivalence with it. Our framework, when applied to popular self-trainingmethods, results in significant performance improvements across three differentdatasets with a limited number of labeled data. Additionally, we demonstratefurther improvements in convergence speed, transfer ability, and hyperparameterstability. The code is available at\url{https://github.com/AurelienGauffre/semisupcon/}.</description><author>Aurelien Gauffre, Julien Horvat, Massih-Reza Amini</author><pubDate>Wed, 11 Sep 2024 14:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07292v1</guid></item><item><title>Real-Time Human Action Recognition on Embedded Platforms</title><link>http://arxiv.org/abs/2409.05662v2</link><description>With advancements in computer vision and deep learning, video-based humanaction recognition (HAR) has become practical. However, due to the complexityof the computation pipeline, running HAR on live video streams incurs excessivedelays on embedded platforms. This work tackles the real-time performancechallenges of HAR with four contributions: 1) an experimental study identifyinga standard Optical Flow (OF) extraction technique as the latency bottleneck ina state-of-the-art HAR pipeline, 2) an exploration of the latency-accuracytradeoff between the standard and deep learning approaches to OF extraction,which highlights the need for a novel, efficient motion feature extractor, 3)the design of Integrated Motion Feature Extractor (IMFE), a novel single-shotneural network architecture for motion feature extraction with drasticimprovement in latency, 4) the development of RT-HARE, a real-time HAR systemtailored for embedded platforms. Experimental results on an Nvidia JetsonXavier NX platform demonstrated that RT-HARE realizes real-time HAR at a videoframe rate of 30 frames per second while delivering high levels of recognitionaccuracy.</description><author>Ruiqi Wang, Zichen Wang, Peiqi Gao, Mingzhen Li, Jaehwan Jeong, Yihang Xu, Yejin Lee, Carolyn M. Baum, Lisa Tabor Connor, Chenyang Lu</author><pubDate>Wed, 11 Sep 2024 14:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05662v2</guid></item><item><title>Contrastive Learning and Abstract Concepts: The Case of Natural Numbers</title><link>http://arxiv.org/abs/2408.02247v5</link><description>Contrastive Learning (CL) has been successfully applied to classification andother downstream tasks related to concrete concepts, such as objects containedin the ImageNet dataset. No attempts seem to have been made so far in applyingthis promising scheme to more abstract entities. A prominent example of thesecould be the concept of (discrete) Quantity. CL can be frequently interpretedas a self-supervised scheme guided by some profound and ubiquitous conservationprinciple (e.g. conservation of identity in object classification tasks). Inthis introductory work we apply a suitable conservation principle to thesemi-abstract concept of natural numbers by which discrete quantities can beestimated or predicted. We experimentally show, by means of a toy problem, thatcontrastive learning can be trained to count at a glance with high accuracyboth at human as well as at super-human ranges.. We compare this with theresults of a trained-to-count at a glance supervised learning (SL) neuralnetwork scheme of similar architecture. We show that both schemes exhibitsimilar good performance on baseline experiments, where the distributions ofthe training and testing stages are equal. Importantly, we demonstrate that insome generalization scenarios, where training and testing distributions differ,CL boasts more robust and much better error performance.</description><author>Daniel N. Nissani</author><pubDate>Wed, 11 Sep 2024 14:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02247v5</guid></item><item><title>Exploring User-level Gradient Inversion with a Diffusion Prior</title><link>http://arxiv.org/abs/2409.07291v1</link><description>We explore user-level gradient inversion as a new attack surface indistributed learning. We first investigate existing attacks on their ability tomake inferences about private information beyond training data reconstruction.Motivated by the low reconstruction quality of existing methods, we propose anovel gradient inversion attack that applies a denoising diffusion model as astrong image prior in order to enhance recovery in the large batch setting.Unlike traditional attacks, which aim to reconstruct individual samples andsuffer at large batch and image sizes, our approach instead aims to recover arepresentative image that captures the sensitive shared semantic informationcorresponding to the underlying user. Our experiments with face imagesdemonstrate the ability of our methods to recover realistic facial images alongwith private user attributes.</description><author>Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Bradley Malin, Kieran Parsons, Ye Wang</author><pubDate>Wed, 11 Sep 2024 14:20:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07291v1</guid></item><item><title>Using Generative Agents to Create Tip Sheets for Investigative Data Reporting</title><link>http://arxiv.org/abs/2409.07286v1</link><description>This paper introduces a system using generative AI agents to create tipsheets for investigative data reporting. Our system employs three specializedagents--an analyst, a reporter, and an editor--to collaboratively generate andrefine tips from datasets. We validate this approach using real-worldinvestigative stories, demonstrating that our agent-based system generallygenerates more newsworthy and accurate insights compared to a baseline modelwithout agents, although some variability was noted between different stories.Our findings highlight the potential of generative AI to provide leads forinvestigative data reporting.</description><author>Joris Veerbeek, Nicholas Diakopoulos</author><pubDate>Wed, 11 Sep 2024 14:14:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07286v1</guid></item><item><title>TLD-READY: Traffic Light Detection -- Relevance Estimation and Deployment Analysis</title><link>http://arxiv.org/abs/2409.07284v1</link><description>Effective traffic light detection is a critical component of the perceptionstack in autonomous vehicles. This work introduces a novel deep-learningdetection system while addressing the challenges of previous work. Utilizing acomprehensive dataset amalgamation, including the Bosch Small Traffic LightsDataset, LISA, the DriveU Traffic Light Dataset, and a proprietary dataset fromKarlsruhe, we ensure a robust evaluation across varied scenarios. Furthermore,we propose a relevance estimation system that innovatively uses directionalarrow markings on the road, eliminating the need for prior map creation. On theDriveU dataset, this approach results in 96% accuracy in relevance estimation.Finally, a real-world evaluation is performed to evaluate the deployment andgeneralizing abilities of these models. For reproducibility and to facilitatefurther research, we provide the model weights and code:https://github.com/KASTEL-MobilityLab/traffic-light-detection.</description><author>Nikolai Polley, Svetlana Pavlitska, Yacin Boualili, Patrick Rohrbeck, Paul Stiller, Ashok Kumar Bangaru, J. Marius ZÃ¶llner</author><pubDate>Wed, 11 Sep 2024 14:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07284v1</guid></item><item><title>MA-CDMR: An Intelligent Cross-domain Multicast Routing Method based on Multiagent Deep Reinforcement Learning in Multi-domain SDWN</title><link>http://arxiv.org/abs/2409.05888v2</link><description>The cross-domain multicast routing problem in a software-defined wirelessnetwork with multiple controllers is a classic NP-hard optimization problem. Asthe network size increases, designing and implementing cross-domain multicastrouting paths in the network requires not only designing efficient solutionalgorithms to obtain the optimal cross-domain multicast tree but also ensuringthe timely and flexible acquisition and maintenance of global network stateinformation. However, existing solutions have a limited ability to sense thenetwork traffic state, affecting the quality of service of multicast services.In addition, these methods have difficulty adapting to the highly dynamicallychanging network states and have slow convergence speeds. To this end, thispaper aims to design and implement a multiagent deep reinforcement learningbased cross-domain multicast routing method for SDWN with multicontrollerdomains. First, a multicontroller communication mechanism and a multicast groupmanagement module are designed to transfer and synchronize network informationbetween different control domains of the SDWN, thus effectively managing thejoining and classification of members in the cross-domain multicast group.Second, a theoretical analysis and proof show that the optimal cross-domainmulticast tree includes an interdomain multicast tree and an intradomainmulticast tree. An agent is established for each controller, and a cooperationmechanism between multiple agents is designed to effectively optimizecross-domain multicast routing and ensure consistency and validity in therepresentation of network state information for cross-domain multicast routingdecisions. Third, a multiagent reinforcement learning-based method thatcombines online and offline training is designed to reduce the dependence onthe real-time environment and increase the convergence speed of multipleagents.</description><author>Miao Ye, Hongwen Hu, Xiaoli Wang, Yuping Wang, Yong Wang, Wen Peng, Jihao Zheng</author><pubDate>Wed, 11 Sep 2024 13:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05888v2</guid></item></channel></rss>