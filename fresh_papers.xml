<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 22 Oct 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Putting the Object Back into Video Object Segmentation</title><link>http://arxiv.org/abs/2310.12982v1</link><description>We present Cutie, a video object segmentation (VOS) network with object-levelmemory reading, which puts the object representation from memory back into thevideo object segmentation result. Recent works on VOS employ bottom-uppixel-level memory reading which struggles due to matching noise, especially inthe presence of distractors, resulting in lower performance in more challengingdata. In contrast, Cutie performs top-down object-level memory reading byadapting a small set of object queries for restructuring and interacting withthe bottom-up pixel features iteratively with a query-based object transformer(qt, hence Cutie). The object queries act as a high-level summary of the targetobject, while high-resolution feature maps are retained for accuratesegmentation. Together with foreground-background masked attention, Cutiecleanly separates the semantics of the foreground object from the background.On the challenging MOSE dataset, Cutie improves by 8.7 J&amp;F over XMem with asimilar running time and improves by 4.2 J&amp;F over DeAOT while running threetimes as fast. Code is available at: https://hkchengrex.github.io/Cutie</description><author>Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, Alexander Schwing</author><pubDate>Thu, 19 Oct 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12982v1</guid></item><item><title>HumanTOMATO: Text-aligned Whole-body Motion Generation</title><link>http://arxiv.org/abs/2310.12978v1</link><description>This work targets a novel text-driven whole-body motion generation task,which takes a given textual description as input and aims at generatinghigh-quality, diverse, and coherent facial expressions, hand gestures, and bodymotions simultaneously. Previous works on text-driven motion generation tasksmainly have two limitations: they ignore the key role of fine-grained hand andface controlling in vivid whole-body motion generation, and lack a goodalignment between text and motion. To address such limitations, we propose aText-aligned whOle-body Motion generATiOn framework, named HumanTOMATO, whichis the first attempt to our knowledge towards applicable holistic motiongeneration in this research area. To tackle this challenging task, our solutionincludes two key designs: (1) a Holistic Hierarchical VQ-VAE (aka H$^2$VQ) anda Hierarchical-GPT for fine-grained body and hand motion reconstruction andgeneration with two structured codebooks; and (2) a pre-trainedtext-motion-alignment model to help generated motion align with the inputtextual description explicitly. Comprehensive experiments verify that our modelhas significant advantages in both the quality of generated motions and theiralignment with text.</description><author>Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, Heung-Yeung Shum</author><pubDate>Thu, 19 Oct 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12978v1</guid></item><item><title>Training Dynamics of Deep Network Linear Regions</title><link>http://arxiv.org/abs/2310.12977v1</link><description>The study of Deep Network (DN) training dynamics has largely focused on theevolution of the loss function, evaluated on or around train and test set datapoints. In fact, many DN phenomenon were first introduced in literature withthat respect, e.g., double descent, grokking. In this study, we look at thetraining dynamics of the input space partition or linear regions formed bycontinuous piecewise affine DNs, e.g., networks with (leaky)ReLUnonlinearities. First, we present a novel statistic that encompasses the localcomplexity (LC) of the DN based on the concentration of linear regions insidearbitrary dimensional neighborhoods around data points. We observe that duringtraining, the LC around data points undergoes a number of phases, starting witha decreasing trend after initialization, followed by an ascent and ending witha final descending trend. Using exact visualization methods, we come across theperplexing observation that during the final LC descent phase of training,linear regions migrate away from training and test samples towards the decisionboundary, making the DN input-output nearly linear everywhere else. We alsoobserve that the different LC phases are closely related to the memorizationand generalization performance of the DN, especially during grokking.</description><author>Ahmed Imtiaz Humayun, Randall Balestriero, Richard Baraniuk</author><pubDate>Thu, 19 Oct 2023 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12977v1</guid></item><item><title>NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models</title><link>http://arxiv.org/abs/2305.16986v3</link><description>Trained with an unprecedented scale of data, large language models (LLMs)like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilitiesfrom model scaling. Such a trend underscored the potential of training LLMswith unlimited language data, advancing the development of a universal embodiedagent. In this work, we introduce the NavGPT, a purely LLM-basedinstruction-following navigation agent, to reveal the reasoning capability ofGPT models in complex embodied scenes by performing zero-shot sequential actionprediction for vision-and-language navigation (VLN). At each step, NavGPT takesthe textual descriptions of visual observations, navigation history, and futureexplorable directions as inputs to reason the agent's current status, and makesthe decision to approach the target. Through comprehensive experiments, wedemonstrate NavGPT can explicitly perform high-level planning for navigation,including decomposing instruction into sub-goal, integrating commonsenseknowledge relevant to navigation task resolution, identifying landmarks fromobserved scenes, tracking navigation progress, and adapting to exceptions withplan adjustment. Furthermore, we show that LLMs is capable of generatinghigh-quality navigational instructions from observations and actions along apath, as well as drawing accurate top-down metric trajectory given the agent'snavigation history. Despite the performance of using NavGPT to zero-shot R2Rtasks still falling short of trained models, we suggest adapting multi-modalityinputs for LLMs to use as visual navigation agents and applying the explicitreasoning of LLMs to benefit learning-based models.</description><author>Gengze Zhou, Yicong Hong, Qi Wu</author><pubDate>Thu, 19 Oct 2023 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16986v3</guid></item><item><title>On the Hidden Waves of Image</title><link>http://arxiv.org/abs/2310.12976v1</link><description>In this paper, we introduce an intriguing phenomenon-the successfulreconstruction of images using a set of one-way wave equations with hidden andlearnable speeds. Each individual image corresponds to a solution with a uniqueinitial condition, which can be computed from the original image using a visualencoder (e.g., a convolutional neural network). Furthermore, the solution foreach image exhibits two noteworthy mathematical properties: (a) it can bedecomposed into a collection of special solutions of the same one-way waveequations that are first-order autoregressive, with shared coefficient matricesfor autoregression, and (b) the product of these coefficient matrices forms adiagonal matrix with the speeds of the wave equations as its diagonal elements.We term this phenomenon hidden waves, as it reveals that, although the speedsof the set of wave equations and autoregressive coefficient matrices arelatent, they are both learnable and shared across images. This represents amathematical invariance across images, providing a new mathematical perspectiveto understand images.</description><author>Yinpeng Chen, Dongdong Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, Zicheng Liu, Youzuo Lin</author><pubDate>Thu, 19 Oct 2023 18:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12976v1</guid></item><item><title>Variational Inference for SDEs Driven by Fractional Noise</title><link>http://arxiv.org/abs/2310.12975v1</link><description>We present a novel variational framework for performing inference in (neural)stochastic differential equations (SDEs) driven by Markov-approximatefractional Brownian motion (fBM). SDEs offer a versatile tool for modelingreal-world continuous-time dynamic systems with inherent noise and randomness.Combining SDEs with the powerful inference capabilities of variational methods,enables the learning of representative function distributions throughstochastic gradient descent. However, conventional SDEs typically assume theunderlying noise to follow a Brownian motion (BM), which hinders their abilityto capture long-term dependencies. In contrast, fractional Brownian motion(fBM) extends BM to encompass non-Markovian dynamics, but existing methods forinferring fBM parameters are either computationally demanding or statisticallyinefficient. In this paper, building upon the Markov approximation of fBM, wederive the evidence lower bound essential for efficient variational inferenceof posterior path measures, drawing from the well-established field ofstochastic analysis. Additionally, we provide a closed-form expression todetermine optimal approximation coefficients. Furthermore, we propose the useof neural networks to learn the drift, diffusion and control terms within ourvariational posterior, leading to the variational training of neural-SDEs. Inthis framework, we also optimize the Hurst index, governing the nature of ourfractional noise. Beyond validation on synthetic data, we contribute a novelarchitecture for variational latent video prediction,-an approach that, to thebest of our knowledge, enables the first variational neural-SDE application tovideo perception.</description><author>Rembert Daems, Manfred Opper, Guillaume Crevecoeur, Tolga Birdal</author><pubDate>Thu, 19 Oct 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12975v1</guid></item><item><title>FSD: Fast Self-Supervised Single RGB-D to Categorical 3D Objects</title><link>http://arxiv.org/abs/2310.12974v1</link><description>In this work, we address the challenging task of 3D object recognitionwithout the reliance on real-world 3D labeled data. Our goal is to predict the3D shape, size, and 6D pose of objects within a single RGB-D image, operatingat the category level and eliminating the need for CAD models during inference.While existing self-supervised methods have made strides in this field, theyoften suffer from inefficiencies arising from non-end-to-end processing,reliance on separate models for different object categories, and slow surfaceextraction during the training of implicit reconstruction models; thushindering both the speed and real-world applicability of the 3D recognitionprocess. Our proposed method leverages a multi-stage training pipeline,designed to efficiently transfer synthetic performance to the real-worlddomain. This approach is achieved through a combination of 2D and 3D supervisedlosses during the synthetic domain training, followed by the incorporation of2D supervised and 3D self-supervised losses on real-world data in twoadditional learning stages. By adopting this comprehensive strategy, our methodsuccessfully overcomes the aforementioned limitations and outperforms existingself-supervised 6D pose and size estimation baselines on the NOCS test-set witha 16.4% absolute improvement in mAP for 6D pose estimation while running innear real-time at 5 Hz.</description><author>Mayank Lunayach, Sergey Zakharov, Dian Chen, Rares Ambrus, Zsolt Kira, Muhammad Zubair Irshad</author><pubDate>Thu, 19 Oct 2023 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12974v1</guid></item><item><title>Frozen Transformers in Language Models Are Effective Visual Encoder Layers</title><link>http://arxiv.org/abs/2310.12973v1</link><description>This paper reveals that large language models (LLMs), despite being trainedsolely on textual data, are surprisingly strong encoders for purely visualtasks in the absence of language. Even more intriguingly, this can be achievedby a simple yet previously overlooked strategy -- employing a frozentransformer block from pre-trained LLMs as a constituent encoder layer todirectly process visual tokens. Our work pushes the boundaries of leveragingLLMs for computer vision tasks, significantly departing from conventionalpractices that typically necessitate a multi-modal vision-language setup withassociated language prompts, inputs, or outputs. We demonstrate that ourapproach consistently enhances performance across a diverse range of tasks,encompassing pure 2D and 3D visual recognition tasks (e.g., image and pointcloud classification), temporal modeling tasks (e.g., action recognition),non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g.,2D/3D visual question answering and image-text retrieval). Such improvementsare a general phenomenon, applicable to various types of LLMs (e.g., LLaMA andOPT) and different LLM transformer blocks. We additionally propose theinformation filtering hypothesis to explain the effectiveness of pre-trainedLLMs in visual encoding -- the pre-trained LLM transformer blocks discerninformative visual tokens and further amplify their effect. This hypothesis isempirically supported by the observation that the feature activation, aftertraining with LLM transformer blocks, exhibits a stronger focus on relevantregions. We hope that our work inspires new perspectives on utilizing LLMs anddeepening our understanding of their underlying mechanisms. Code is availableat https://github.com/ziqipang/LM4VisualEncoding.</description><author>Ziqi Pang, Ziyang Xie, Yunze Man, Yu-Xiong Wang</author><pubDate>Thu, 19 Oct 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12973v1</guid></item><item><title>CLAIR: Evaluating Image Captions with Large Language Models</title><link>http://arxiv.org/abs/2310.12971v1</link><description>The evaluation of machine-generated image captions poses an interesting yetpersistent challenge. Effective evaluation measures must consider numerousdimensions of similarity, including semantic relevance, visual structure,object interactions, caption diversity, and specificity. Existinghighly-engineered measures attempt to capture specific aspects, but fall shortin providing a holistic score that aligns closely with human judgments. Here,we propose CLAIR, a novel method that leverages the zero-shot language modelingcapabilities of large language models (LLMs) to evaluate candidate captions. Inour evaluations, CLAIR demonstrates a stronger correlation with human judgmentsof caption quality compared to existing measures. Notably, on Flickr8K-Expert,CLAIR achieves relative correlation improvements over SPICE of 39.6% and overimage-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR providesnoisily interpretable results by allowing the language model to identify theunderlying reasoning behind its assigned score. Code is available athttps://davidmchan.github.io/clair/</description><author>David Chan, Suzanne Petryk, Joseph E. Gonzalez, Trevor Darrell, John Canny</author><pubDate>Thu, 19 Oct 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12971v1</guid></item><item><title>Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding</title><link>http://arxiv.org/abs/2310.12970v1</link><description>The real-world deployment of an autonomous driving system requires itscomponents to run on-board and in real-time, including the motion predictionmodule that predicts the future trajectories of surrounding trafficparticipants. Existing agent-centric methods have demonstrated outstandingperformance on public benchmarks. However, they suffer from high computationaloverhead and poor scalability as the number of agents to be predictedincreases. To address this problem, we introduce the K-nearest neighborattention with relative pose encoding (KNARPE), a novel attention mechanismallowing the pairwise-relative representation to be used by Transformers. Then,based on KNARPE we present the Heterogeneous Polyline Transformer with Relativepose encoding (HPTR), a hierarchical framework enabling asynchronous tokenupdate during the online inference. By sharing contexts among agents andreusing the unchanged contexts, our approach is as efficient as scene-centricmethods, while performing on par with state-of-the-art agent-centric methods.Experiments on Waymo and Argoverse-2 datasets show that HPTR achieves superiorperformance among end-to-end methods that do not apply expensivepost-processing or model ensembling. The code is available athttps://github.com/zhejz/HPTR.</description><author>Zhejun Zhang, Alexander Liniger, Christos Sakaridis, Fisher Yu, Luc Van Gool</author><pubDate>Thu, 19 Oct 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12970v1</guid></item><item><title>Demystifying the Myths and Legends of Nonconvex Convergence of SGD</title><link>http://arxiv.org/abs/2310.12969v1</link><description>Stochastic gradient descent (SGD) and its variants are the main workhorsesfor solving large-scale optimization problems with nonconvex objectivefunctions. Although the convergence of SGDs in the (strongly) convex case iswell-understood, their convergence for nonconvex functions stands on weakmathematical foundations. Most existing studies on the nonconvex convergence ofSGD show the complexity results based on either the minimum of the expectedgradient norm or the functional sub-optimality gap (for functions with extrastructural property) by searching the entire range of iterates. Hence the lastiterations of SGDs do not necessarily maintain the same complexity guarantee.This paper shows that an $\epsilon$-stationary point exists in the finaliterates of SGDs, given a large enough total iteration budget, $T$, not justanywhere in the entire range of iterates -- a much stronger result than theexisting one. Additionally, our analyses allow us to measure the density of the$\epsilon$-stationary points in the final iterates of SGD, and we recover theclassical $O(\frac{1}{\sqrt{T}})$ asymptotic rate under various existingassumptions on the objective function and the bounds on the stochasticgradient. As a result of our analyses, we addressed certain myths and legendsrelated to the nonconvex convergence of SGD and posed some thought-provokingquestions that could set new directions for research.</description><author>Aritra Dutta, El Houcine Bergou, Soumia Boucherouite, Nicklas Werge, Melih Kandemir, Xin Li</author><pubDate>Thu, 19 Oct 2023 18:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12969v1</guid></item><item><title>Does Your Model Think Like an Engineer? Explainable AI for Bearing Fault Detection with Deep Learning</title><link>http://arxiv.org/abs/2310.12967v1</link><description>Deep Learning has already been successfully applied to analyze industrialsensor data in a variety of relevant use cases. However, the opaque nature ofmany well-performing methods poses a major obstacle for real-world deployment.Explainable AI (XAI) and especially feature attribution techniques promise toenable insights about how such models form their decision. But the plainapplication of such methods often fails to provide truly informative andproblem-specific insights to domain experts. In this work, we focus on thespecific task of detecting faults in rolling element bearings from vibrationsignals. We propose a novel and domain-specific feature attribution frameworkthat allows us to evaluate how well the underlying logic of a model correspondswith expert reasoning. Utilizing the framework we are able to validate thetrustworthiness and to successfully anticipate the generalization ability ofdifferent well-performing deep learning models. Our methodology demonstrateshow signal processing tools can effectively be used to enhance Explainable AItechniques and acts as a template for similar problems.</description><author>Thomas Decker, Michael Lebacher, Volker Tresp</author><pubDate>Thu, 19 Oct 2023 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12967v1</guid></item><item><title>IC3: Image Captioning by Committee Consensus</title><link>http://arxiv.org/abs/2302.01328v3</link><description>If you ask a human to describe an image, they might do so in a thousanddifferent ways. Traditionally, image captioning models are trained to generatea single "best" (most like a reference) image caption. Unfortunately, doing soencourages captions that are "informationally impoverished," and focus on onlya subset of the possible details, while ignoring other potentially usefulinformation in the scene. In this work, we introduce a simple, yet novel,method: "Image Captioning by Committee Consensus" (IC3), designed to generate asingle caption that captures high-level details from several annotatorviewpoints. Humans rate captions produced by IC3 at least as helpful asbaseline SOTA models more than two thirds of the time, and IC3 can improve theperformance of SOTA automated recall systems by up to 84%, outperforming singlehuman-generated reference captions, and indicating significant improvementsover SOTA approaches for visual description. Code is available athttps://davidmchan.github.io/caption-by-committee/</description><author>David M. Chan, Austin Myers, Sudheendra Vijayanarasimhan, David A. Ross, John Canny</author><pubDate>Thu, 19 Oct 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01328v3</guid></item><item><title>ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations</title><link>http://arxiv.org/abs/2306.08141v2</link><description>As generative AI becomes more prevalent, it is important to study how humanusers interact with such models. In this work, we investigate how people usetext-to-image models to generate desired target images. To study thisinteraction, we created ArtWhisperer, an online game where users are given atarget image and are tasked with iteratively finding a prompt that creates asimilar-looking image as the target. Through this game, we recorded over 50,000human-AI interactions; each interaction corresponds to one text prompt createdby a user and the corresponding generated image. The majority of these arerepeated interactions where a user iterates to find the best prompt for theirtarget image, making this a unique sequential dataset for studying human-AIcollaborations. In an initial analysis of this dataset, we identify severalcharacteristics of prompt interactions and user strategies. People submitdiverse prompts and are able to discover a variety of text descriptions thatgenerate similar images. Interestingly, prompt diversity does not decrease asusers find better prompts. We further propose a new metric to quantify thesteerability of AI using our dataset. We define steerability as the expectednumber of interactions required to adequately complete a task. We estimate thisvalue by fitting a Markov chain for each target task and calculating theexpected time to reach an adequate score in the Markov chain. We quantify andcompare AI steerability across different types of target images and twodifferent models, finding that images of cities and natural world images aremore steerable than artistic and fantasy images. These findings provideinsights into human-AI interaction behavior, present a concrete method ofassessing AI steerability, and demonstrate the general utility of theArtWhisperer dataset.</description><author>Kailas Vodrahalli, James Zou</author><pubDate>Thu, 19 Oct 2023 18:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08141v2</guid></item><item><title>PAC Prediction Sets Under Label Shift</title><link>http://arxiv.org/abs/2310.12964v1</link><description>Prediction sets capture uncertainty by predicting sets of labels rather thanindividual labels, enabling downstream decisions to conservatively account forall plausible outcomes. Conformal inference algorithms construct predictionsets guaranteed to contain the true label with high probability. Theseguarantees fail to hold in the face of distribution shift, which is preciselywhen reliable uncertainty quantification can be most useful. We propose a novelalgorithm for constructing prediction sets with PAC guarantees in the labelshift setting. This method estimates the predicted probabilities of the classesin a target domain, as well as the confusion matrix, then propagatesuncertainty in these estimates through a Gaussian elimination algorithm tocompute confidence intervals for importance weights. Finally, it uses theseintervals to construct prediction sets. We evaluate our approach on fivedatasets: the CIFAR-10, ChestX-Ray and Entity-13 image datasets, the tabularCDC Heart dataset, and the AGNews text dataset. Our algorithm satisfies the PACguarantee while producing smaller, more informative, prediction sets comparedto several baselines.</description><author>Wenwen Si, Sangdon Park, Insup Lee, Edgar Dobriban, Osbert Bastani</author><pubDate>Thu, 19 Oct 2023 18:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12964v1</guid></item><item><title>AutoMix: Automatically Mixing Language Models</title><link>http://arxiv.org/abs/2310.12963v1</link><description>Large language models (LLMs) are now available in various sizes andconfigurations from cloud API providers. While this diversity offers a broadspectrum of choices, effectively leveraging the options to optimizecomputational cost and performance remains challenging. In this work, wepresent AutoMix, an approach that strategically routes queries to larger LMs,based on the approximate correctness of outputs from a smaller LM. Central toAutoMix is a few-shot self-verification mechanism, which estimates thereliability of its own outputs without requiring training. Given thatverifications can be noisy, we employ a meta verifier in AutoMix to refine theaccuracy of these assessments. Our experiments using LLAMA2-13/70B, on fivecontext-grounded reasoning datasets demonstrate that AutoMix surpassesestablished baselines, improving the incremental benefit per cost by up to 89%.Our code and data are available at https://github.com/automix-llm/automix.</description><author>Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, Manaal Faruqui</author><pubDate>Thu, 19 Oct 2023 18:57:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12963v1</guid></item><item><title>An Emulator for Fine-Tuning Large Language Models using Small Language Models</title><link>http://arxiv.org/abs/2310.12962v1</link><description>Widely used language models (LMs) are typically built by scaling up atwo-stage training pipeline: a pre-training stage that uses a very large,diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage thatuses targeted examples or other specifications of desired behaviors. While ithas been hypothesized that knowledge and skills come from pre-training, andfine-tuning mostly filters this knowledge and skillset, this intuition has notbeen extensively tested. To aid in doing so, we introduce a novel technique fordecoupling the knowledge and skills gained in these two stages, enabling adirect answer to the question, "What would happen if we combined the knowledgelearned by a large model during pre-training with the knowledge learned by asmall model during fine-tuning (or vice versa)?" Using an RL-based frameworkderived from recent developments in learning from human preferences, weintroduce emulated fine-tuning (EFT), a principled and practical method forsampling from a distribution that approximates (or 'emulates') the result ofpre-training and fine-tuning at different scales. Our experiments with EFT showthat scaling up fine-tuning tends to improve helpfulness, while scaling uppre-training tends to improve factuality. Beyond decoupling scale, we show thatEFT enables test-time adjustment of competing behavioral traits likehelpfulness and harmlessness without additional training. Finally, a specialcase of emulated fine-tuning, which we call LM up-scaling, avoidsresource-intensive fine-tuning of large pre-trained models by ensembling themwith small fine-tuned models, essentially emulating the result of fine-tuningthe large pre-trained model. Up-scaling consistently improves helpfulness andfactuality of instruction-following models in the Llama, Llama-2, and Falconfamilies, without additional hyperparameters or training.</description><author>Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, Christopher D. Manning</author><pubDate>Thu, 19 Oct 2023 18:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12962v1</guid></item><item><title>SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving</title><link>http://arxiv.org/abs/2310.12960v1</link><description>Large Language Models (LLMs) have driven substantial progress in artificialintelligence in recent years, exhibiting impressive capabilities across a widerange of tasks, including mathematical problem-solving. Inspired by the successof subgoal-based methods, we propose a novel framework called\textbf{SE}quential sub\textbf{G}oal \textbf{O}ptimization (SEGO) to enhanceLLMs' ability to solve mathematical problems. By establishing a connectionbetween the subgoal breakdown process and the probability of solving problems,SEGO aims to identify better subgoals with theoretical guarantees. Addressingthe challenge of identifying suitable subgoals in a large solution space, ourframework generates problem-specific subgoals and adjusts them according tocarefully designed criteria. Incorporating these optimized subgoals into thepolicy model training leads to significant improvements in problem-solvingperformance. We validate SEGO's efficacy through experiments on two benchmarks,GSM8K and MATH, where our approach outperforms existing methods, highlightingthe potential of SEGO in AI-driven mathematical problem-solving. Data and code associated with this paper will be available athttps://github.com/zhaoxlpku/SEGO</description><author>Xueliang Zhao, Xinting Huang, Wei Bi, Lingpeng Kong</author><pubDate>Thu, 19 Oct 2023 18:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12960v1</guid></item><item><title>Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems</title><link>http://arxiv.org/abs/2310.12956v1</link><description>In this work, we study rapid, step-wise improvements of the loss intransformers when being confronted with multi-step decision tasks. We foundthat transformers struggle to learn the intermediate tasks, whereas CNNs haveno such issue on the tasks we studied. When transformers learn the intermediatetask, they do this rapidly and unexpectedly after both training and validationloss saturated for hundreds of epochs. We call these rapid improvementsEureka-moments, since the transformer appears to suddenly learn a previouslyincomprehensible task. Similar leaps in performance have become known asGrokking. In contrast to Grokking, for Eureka-moments, both the validation andthe training loss saturate before rapidly improving. We trace the problem backto the Softmax function in the self-attention block of transformers and showways to alleviate the problem. These fixes improve training speed. The improvedmodels reach 95% of the baseline model in just 20% of training steps whilehaving a much higher likelihood to learn the intermediate task, lead to higherfinal accuracy and are more robust to hyper-parameters.</description><author>David T. Hoffmann, Simon Schrodi, Nadine Behrmann, Volker Fischer, Thomas Brox</author><pubDate>Thu, 19 Oct 2023 18:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12956v1</guid></item><item><title>Towards Robust Offline Reinforcement Learning under Diverse Data Corruption</title><link>http://arxiv.org/abs/2310.12955v1</link><description>Offline reinforcement learning (RL) presents a promising approach forlearning reinforced policies from offline datasets without the need for costlyor unsafe interactions with the environment. However, datasets collected byhumans in real-world environments are often noisy and may even be maliciouslycorrupted, which can significantly degrade the performance of offline RL. Inthis work, we first investigate the performance of current offline RLalgorithms under comprehensive data corruption, including states, actions,rewards, and dynamics. Our extensive experiments reveal that implicitQ-learning (IQL) demonstrates remarkable resilience to data corruption amongvarious offline RL algorithms. Furthermore, we conduct both empirical andtheoretical analyses to understand IQL's robust performance, identifying itssupervised policy learning scheme as the key factor. Despite its relativerobustness, IQL still suffers from heavy-tail targets of Q functions underdynamics corruption. To tackle this challenge, we draw inspiration from robuststatistics to employ the Huber loss to handle the heavy-tailedness and utilizequantile estimators to balance penalization for corrupted data and learningstability. By incorporating these simple yet effective modifications into IQL,we propose a more robust offline RL approach named Robust IQL (RIQL). Extensiveexperiments demonstrate that RIQL exhibits highly robust performance whensubjected to diverse data corruption scenarios.</description><author>Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han, Tong Zhang</author><pubDate>Thu, 19 Oct 2023 18:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12955v1</guid></item><item><title>Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation</title><link>http://arxiv.org/abs/2310.12953v1</link><description>Thanks to their generative capabilities, large language models (LLMs) havebecome an invaluable tool for creative processes. These models have thecapacity to produce hundreds and thousands of visual and textual outputs,offering abundant inspiration for creative endeavors. But are we harnessingtheir full potential? We argue that current interaction paradigms fall short,guiding users towards rapid convergence on a limited set of ideas, rather thanempowering them to explore the vast latent design space in generative models.To address this limitation, we propose a framework that facilitates thestructured generation of design space in which users can seamlessly explore,evaluate, and synthesize a multitude of responses. We demonstrate thefeasibility and usefulness of this framework through the design and developmentof an interactive system, Luminate, and a user study with 8 professionalwriters. Our work advances how we interact with LLMs for creative tasks,introducing a way to harness the creative potential of LLMs.</description><author>Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, Haijun Xia</author><pubDate>Thu, 19 Oct 2023 18:53:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12953v1</guid></item><item><title>Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning</title><link>http://arxiv.org/abs/2310.12952v1</link><description>Measuring diversity accurately is important for many scientific fields,including machine learning (ML), ecology, and chemistry. The Vendi Score wasintroduced as a generic similarity-based diversity metric that extends the Hillnumber of order q=1 by leveraging ideas from quantum statistical mechanics.Contrary to many diversity metrics in ecology, the Vendi Score accounts forsimilarity and does not require knowledge of the prevalence of the categoriesin the collection to be evaluated for diversity. However, the Vendi Scoretreats each item in a given collection with a level of sensitivity proportionalto the item's prevalence. This is undesirable in settings where there is asignificant imbalance in item prevalence. In this paper, we extend the otherHill numbers using similarity to provide flexibility in allocating sensitivityto rare or common items. This leads to a family of diversity metrics -- Vendiscores with different levels of sensitivity -- that can be used in a variety ofapplications. We study the properties of the scores in a synthetic controlledsetting where the ground truth diversity is known. We then test their utilityin improving molecular simulations via Vendi Sampling. Finally, we use theVendi scores to better understand the behavior of image generative models interms of memorization, duplication, diversity, and sample quality.</description><author>Amey Pasarkar, Adji Bousso Dieng</author><pubDate>Thu, 19 Oct 2023 18:52:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12952v1</guid></item><item><title>Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining</title><link>http://arxiv.org/abs/2305.14281v2</link><description>Recent work in vision-and-language pretraining has investigated supervisedsignals from object detection data to learn better, fine-grained multimodalrepresentations. In this work, we take a step further and explore how we cantap into supervision from small-scale visual relation data. In particular, wepropose two pretraining approaches to contextualise visual entities in amultimodal setup. With verbalised scene graphs, we transform visual relationtriplets into structured captions, and treat them as additional imagedescriptions. With masked relation prediction, we further encourage relatingentities from image regions with visually masked contexts. When applied tostrong baselines pretrained on large amounts of Web data, zero-shot evaluationson both coarse-grained and fine-grained tasks show the efficacy of our methodsin learning multimodal representations from weakly-supervised relations data.</description><author>Emanuele Bugliarello, Aida Nematzadeh, Lisa Anne Hendricks</author><pubDate>Thu, 19 Oct 2023 18:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14281v2</guid></item><item><title>3D-GPT: Procedural 3D Modeling with Large Language Models</title><link>http://arxiv.org/abs/2310.12945v1</link><description>In the pursuit of efficient automated content creation, proceduralgeneration, leveraging modifiable parameters and rule-based systems, emerges asa promising approach. Nonetheless, it could be a demanding endeavor, given itsintricate nature necessitating a deep understanding of rules, algorithms, andparameters. To reduce workload, we introduce 3D-GPT, a framework utilizinglarge language models~(LLMs) for instruction-driven 3D modeling. 3D-GPTpositions LLMs as proficient problem solvers, dissecting the procedural 3Dmodeling tasks into accessible segments and appointing the apt agent for eachtask. 3D-GPT integrates three core agents: the task dispatch agent, theconceptualization agent, and the modeling agent. They collaboratively achievetwo objectives. First, it enhances concise initial scene descriptions, evolvingthem into detailed forms while dynamically adapting the text based onsubsequent instructions. Second, it integrates procedural generation,extracting parameter values from enriched text to effortlessly interface with3D software for asset creation. Our empirical investigations confirm that3D-GPT not only interprets and executes instructions, delivering reliableresults but also collaborates effectively with human designers. Furthermore, itseamlessly integrates with Blender, unlocking expanded manipulationpossibilities. Our work highlights the potential of LLMs in 3D modeling,offering a basic framework for future advancements in scene generation andanimation.</description><author>Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, Stephen Gould</author><pubDate>Thu, 19 Oct 2023 18:41:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12945v1</guid></item><item><title>On the Representational Capacity of Recurrent Neural Language Models</title><link>http://arxiv.org/abs/2310.12942v1</link><description>This work investigates the computational expressivity of language models(LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992)famously showed that RNNs with rational weights and hidden states and unboundedcomputation time are Turing complete. However, LMs define weightings overstrings in addition to just (unweighted) language membership and the analysisof the computational power of RNN LMs (RLMs) should reflect this. We extend theTuring completeness result to the probabilistic case, showing how a rationallyweighted RLM with unbounded computation time can simulate any probabilisticTuring machine (PTM). Since, in practice, RLMs work in real-time, processing asymbol at every time step, we treat the above result as an upper bound on theexpressivity of RLMs. We also provide a lower bound by showing that under therestriction to real-time computation, such models can simulate deterministicreal-time rational PTMs.</description><author>Franz Nowak, Anej Svete, Li Du, Ryan Cotterell</author><pubDate>Thu, 19 Oct 2023 18:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12942v1</guid></item><item><title>The Foundation Model Transparency Index</title><link>http://arxiv.org/abs/2310.12941v1</link><description>Foundation models have rapidly permeated society, catalyzing a wave ofgenerative AI applications spanning enterprise and consumer-facing contexts.While the societal impact of foundation models is growing, transparency is onthe decline, mirroring the opacity that has plagued past digital technologies(e.g. social media). Reversing this trend is essential: transparency is a vitalprecondition for public accountability, scientific innovation, and effectivegovernance. To assess the transparency of the foundation model ecosystem andhelp improve transparency over time, we introduce the Foundation ModelTransparency Index. The Foundation Model Transparency Index specifies 100fine-grained indicators that comprehensively codify transparency for foundationmodels, spanning the upstream resources used to build a foundation model (e.gdata, labor, compute), details about the model itself (e.g. size, capabilities,risks), and the downstream use (e.g. distribution channels, usage policies,affected geographies). We score 10 major foundation model developers (e.g.OpenAI, Google, Meta) against the 100 indicators to assess their transparency.To facilitate and standardize assessment, we score developers in relation totheir practices for their flagship foundation model (e.g. GPT-4 for OpenAI,PaLM 2 for Google, Llama 2 for Meta). We present 10 top-level findings aboutthe foundation model ecosystem: for example, no developer currently disclosessignificant information about the downstream impact of its flagship model, suchas the number of users, affected market sectors, or how users can seek redressfor harm. Overall, the Foundation Model Transparency Index establishes thelevel of transparency today to drive progress on foundation model governancevia industry standards and regulatory intervention.</description><author>Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, Percy Liang</author><pubDate>Thu, 19 Oct 2023 18:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12941v1</guid></item><item><title>Learning to translate by learning to communicate</title><link>http://arxiv.org/abs/2207.07025v2</link><description>We formulate and test a technique to use Emergent Communication (EC) with apre-trained multilingual model to improve on modern Unsupervised NMT systems,especially for low-resource languages. It has been argued that the currentdominant paradigm in NLP of pre-training on text-only corpora will not yieldrobust natural language understanding systems, and the need for grounded,goal-oriented, and interactive language learning has been high lighted. In ourapproach, we embed a multilingual model (mBART, Liu et al., 2020) into an ECimage-reference game, in which the model is incentivized to use multilingualgenerations to accomplish a vision-grounded task. The hypothesis is that thiswill align multiple languages to a shared task space. We present two variantsof EC Fine-Tuning (Steinert-Threlkeld et al., 2022), one of which outperforms abacktranslation-only baseline in all four languages investigated, including thelow-resource language Nepali.</description><author>C. M. Downey, Xuhui Zhou, Leo Z. Liu, Shane Steinert-Threlkeld</author><pubDate>Thu, 19 Oct 2023 18:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.07025v2</guid></item><item><title>A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models</title><link>http://arxiv.org/abs/2310.12936v1</link><description>Various types of social biases have been reported with pretrained MaskedLanguage Models (MLMs) in prior work. However, multiple underlying factors areassociated with an MLM such as its model size, size of the training data,training objectives, the domain from which pretraining data is sampled,tokenization, and languages present in the pretrained corpora, to name a few.It remains unclear as to which of those factors influence social biases thatare learned by MLMs. To study the relationship between model factors and thesocial biases learned by an MLM, as well as the downstream task performance ofthe model, we conduct a comprehensive study over 39 pretrained MLMs coveringdifferent model sizes, training objectives, tokenization methods, training datadomains and languages. Our results shed light on important factors oftenneglected in prior literature, such as tokenization or model objectives.</description><author>Yi Zhou, Jose Camacho-Collados, Danushka Bollegala</author><pubDate>Thu, 19 Oct 2023 18:33:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12936v1</guid></item><item><title>Generative Flow Networks as Entropy-Regularized RL</title><link>http://arxiv.org/abs/2310.12934v1</link><description>The recently proposed generative flow networks (GFlowNets) are a method oftraining a policy to sample compositional discrete objects with probabilitiesproportional to a given reward via a sequence of actions. GFlowNets exploit thesequential nature of the problem, drawing parallels with reinforcement learning(RL). Our work extends the connection between RL and GFlowNets to a generalcase. We demonstrate how the task of learning a generative flow network can beefficiently redefined as an entropy-regularized RL problem with a specificreward and regularizer structure. Furthermore, we illustrate the practicalefficiency of this reformulation by applying standard soft RL algorithms toGFlowNet training across several probabilistic modeling tasks. Contrary topreviously reported results, we show that entropic RL approaches can becompetitive against established GFlowNet training methods. This perspectiveopens a direct path for integrating reinforcement learning principles into therealm of generative flow networks.</description><author>Daniil Tiapkin, Nikita Morozov, Alexey Naumov, Dmitry Vetrov</author><pubDate>Thu, 19 Oct 2023 18:31:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12934v1</guid></item><item><title>CAW-coref: Conjunction-Aware Word-level Coreference Resolution</title><link>http://arxiv.org/abs/2310.06165v2</link><description>State-of-the-art coreference resolutions systems depend on multiple LLM callsper document and are thus prohibitively expensive for many use cases (e.g.,information extraction with large corpora). The leading word-level coreferencesystem (WL-coref) attains 96.6% of these SOTA systems' performance while beingmuch more efficient. In this work, we identify a routine yet important failurecase of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. Weoffer a simple yet effective solution that improves the performance on theOntoNotes test set by 0.9% F1, shrinking the gap between efficient word-levelcoreference resolution and expensive SOTA approaches by 34.6%. OurConjunction-Aware Word-level coreference model (CAW-coref) and code isavailable at https://github.com/KarelDO/wl-coref.</description><author>Karel D'Oosterlinck, Semere Kiros Bitew, Brandon Papineau, Christopher Potts, Thomas Demeester, Chris Develder</author><pubDate>Thu, 19 Oct 2023 18:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06165v2</guid></item><item><title>Eureka: Human-Level Reward Design via Coding Large Language Models</title><link>http://arxiv.org/abs/2310.12931v1</link><description>Large Language Models (LLMs) have excelled as high-level semantic plannersfor sequential decision-making tasks. However, harnessing them to learn complexlow-level manipulation tasks, such as dexterous pen spinning, remains an openproblem. We bridge this fundamental gap and present Eureka, a human-levelreward design algorithm powered by LLMs. Eureka exploits the remarkablezero-shot generation, code-writing, and in-context improvement capabilities ofstate-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization overreward code. The resulting rewards can then be used to acquire complex skillsvia reinforcement learning. Without any task-specific prompting or pre-definedreward templates, Eureka generates reward functions that outperform experthuman-engineered rewards. In a diverse suite of 29 open-source RL environmentsthat include 10 distinct robot morphologies, Eureka outperforms human expertson 83% of the tasks, leading to an average normalized improvement of 52%. Thegenerality of Eureka also enables a new gradient-free in-context learningapproach to reinforcement learning from human feedback (RLHF), readilyincorporating human inputs to improve the quality and the safety of thegenerated rewards without model updating. Finally, using Eureka rewards in acurriculum learning setting, we demonstrate for the first time, a simulatedShadow Hand capable of performing pen spinning tricks, adeptly manipulating apen in circles at rapid speed.</description><author>Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, Anima Anandkumar</author><pubDate>Thu, 19 Oct 2023 18:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12931v1</guid></item><item><title>Probabilistic Modeling of Human Teams to Infer False Beliefs</title><link>http://arxiv.org/abs/2310.12929v1</link><description>We develop a probabilistic graphical model (PGM) for artificially intelligent(AI) agents to infer human beliefs during a simulated urban search and rescue(USAR) scenario executed in a Minecraft environment with a team of threeplayers. The PGM approach makes observable states and actions explicit, as wellas beliefs and intentions grounded by evidence about what players see and doover time. This approach also supports inferring the effect of interventions,which are vital if AI agents are to assist human teams. The experimentincorporates manipulations of players' knowledge, and the virtualMinecraft-based testbed provides access to several streams of information,including the objects in the players' field of view. The participants areequipped with a set of marker blocks that can be placed near room entrances tosignal the presence or absence of victims in the rooms to their teammates. Ineach team, one of the members is given a different legend for the markers thanthe other two, which may mislead them about the state of the rooms; that is,they will hold a false belief. We extend previous works in this field byintroducing ToMCAT, an AI agent that can reason about individual and sharedmental states. We find that the players' behaviors are affected by what theysee in their in-game field of view, their beliefs about the meaning of themarkers, and their beliefs about which meaning the team decided to adopt. Inaddition, we show that ToMCAT's beliefs are consistent with the players'actions and that it can infer false beliefs with accuracy significantly betterthan chance and comparable to inferences made by human observers.</description><author>Paulo Soares, Adarsh Pyarelal, Kobus Barnard</author><pubDate>Thu, 19 Oct 2023 18:28:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12929v1</guid></item><item><title>Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous Core Networks</title><link>http://arxiv.org/abs/2310.12924v1</link><description>Existing distributed denial of service attack (DDoS) solutions cannot handlehighly aggregated data rates; thus, they are unsuitable for Internet serviceprovider (ISP) core networks. This article proposes a digital twin-enabledintelligent DDoS detection mechanism using an online learning method forautonomous systems. Our contributions are three-fold: we first design a DDoSdetection architecture based on the digital twin for ISP core networks. Weimplemented a Yet Another Next Generation (YANG) model and an automated featureselection (AutoFS) module to handle core network data. We used an onlinelearning approach to update the model instantly and efficiently, improve thelearning model quickly, and ensure accurate predictions. Finally, we revealthat our proposed solution successfully detects DDoS attacks and updates thefeature selection method and learning model with a true classification rate ofninety-seven percent. Our proposed solution can estimate the attack withinapproximately fifteen minutes after the DDoS attack starts.</description><author>Yagmur Yigit, Bahadir Bal, Aytac Karameseoglu, Trung Q. Duong, Berk Canberk</author><pubDate>Thu, 19 Oct 2023 18:19:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12924v1</guid></item><item><title>Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning</title><link>http://arxiv.org/abs/2310.12921v1</link><description>Reinforcement learning (RL) requires either manually specifying a rewardfunction, which is often infeasible, or learning a reward model from a largeamount of human feedback, which is often very expensive. We study a moresample-efficient alternative: using pretrained vision-language models (VLMs) aszero-shot reward models (RMs) to specify tasks via natural language. We proposea natural and general approach to using VLMs as reward models, which we callVLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learncomplex tasks without a manually specified reward function, such as kneeling,doing the splits, and sitting in a lotus position. For each of these tasks, weonly provide a single sentence text prompt describing the desired task withminimal prompt engineering. We provide videos of the trained agents at:https://sites.google.com/view/vlm-rm. We can improve performance by providing asecond ``baseline'' prompt and projecting out parts of the CLIP embedding spaceirrelevant to distinguish between goal and baseline. Further, we find a strongscaling effect for VLM-RMs: larger VLMs trained with more compute and data arebetter reward models. The failure modes of VLM-RMs we encountered are allrelated to known capability limitations of current VLMs, such as limitedspatial reasoning ability or visually unrealistic environments that are faroff-distribution for the VLM. We find that VLM-RMs are remarkably robust aslong as the VLM is large enough. This suggests that future VLMs will becomemore and more useful reward models for a wide range of RL applications.</description><author>Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David Lindner</author><pubDate>Thu, 19 Oct 2023 18:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12921v1</guid></item><item><title>Generative Marginalization Models</title><link>http://arxiv.org/abs/2310.12920v1</link><description>We introduce marginalization models (MaMs), a new family of generative modelsfor high-dimensional discrete data. They offer scalable and flexible generativemodeling with tractable likelihoods by explicitly modeling all induced marginaldistributions. Marginalization models enable fast evaluation of arbitrarymarginal probabilities with a single forward pass of the neural network, whichovercomes a major limitation of methods with exact marginal inference, such asautoregressive models (ARMs). We propose scalable methods for learning themarginals, grounded in the concept of "marginalization self-consistency".Unlike previous methods, MaMs support scalable training of any-order generativemodels for high-dimensional problems under the setting of energy-basedtraining, where the goal is to match the learned distribution to a givendesired probability (specified by an unnormalized (log) probability functionsuch as energy function or reward function). We demonstrate the effectivenessof the proposed model on a variety of discrete data distributions, includingbinary images, language, physical systems, and molecules, for maximumlikelihood and energy-based training settings. MaMs achieve orders of magnitudespeedup in evaluating the marginal probabilities on both settings. Forenergy-based training tasks, MaMs enable any-order generative modeling ofhigh-dimensional problems beyond the capability of previous methods. Code is athttps://github.com/PrincetonLIPS/MaM.</description><author>Sulin Liu, Peter J. Ramadge, Ryan P. Adams</author><pubDate>Thu, 19 Oct 2023 18:14:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12920v1</guid></item><item><title>Network-Aware AutoML Framework for Software-Defined Sensor Networks</title><link>http://arxiv.org/abs/2310.12914v1</link><description>As the current detection solutions of distributed denial of service attacks(DDoS) need additional infrastructures to handle high aggregate data rates,they are not suitable for sensor networks or the Internet of Things. Besides,the security architecture of software-defined sensor networks needs to payattention to the vulnerabilities of both software-defined networks and sensornetworks. In this paper, we propose a network-aware automated machine learning(AutoML) framework which detects DDoS attacks in software-defined sensornetworks. Our framework selects an ideal machine learning algorithm to detectDDoS attacks in network-constrained environments, using metrics such asvariable traffic load, heterogeneous traffic rate, and detection time whilepreventing over-fitting. Our contributions are two-fold: (i) we firstinvestigate the trade-off between the efficiency of ML algorithms andnetwork/traffic state in the scope of DDoS detection. (ii) we design andimplement a software architecture containing open-source network tools, withthe deployment of multiple ML algorithms. Lastly, we show that under the denialof service attacks, our framework ensures the traffic packets are stilldelivered within the network with additional delays.</description><author>Emre Horsanali, Yagmur Yigit, Gokhan Secinti, Aytac Karameseoglu, Berk Canberk</author><pubDate>Thu, 19 Oct 2023 18:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12914v1</guid></item><item><title>Unsupervised Object Localization in the Era of Self-Supervised ViTs: A Survey</title><link>http://arxiv.org/abs/2310.12904v1</link><description>The recent enthusiasm for open-world vision systems show the high interest ofthe community to perform perception tasks outside of the closed-vocabularybenchmark setups which have been so popular until now. Being able to discoverobjects in images/videos without knowing in advance what objects populate thedataset is an exciting prospect. But how to find objects without knowinganything about them? Recent works show that it is possible to performclass-agnostic unsupervised object localization by exploiting self-supervisedpre-trained features. We propose here a survey of unsupervised objectlocalization methods that discover objects in images without requiring anymanual annotation in the era of self-supervised ViTs. We gather links ofdiscussed methods in the repositoryhttps://github.com/valeoai/Awesome-Unsupervised-Object-Localization.</description><author>Oriane Siméoni, Éloi Zablocki, Spyros Gidaris, Gilles Puy, Patrick Pérez</author><pubDate>Thu, 19 Oct 2023 17:57:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12904v1</guid></item><item><title>Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling</title><link>http://arxiv.org/abs/2310.12902v1</link><description>The paper proposes a framework that combines behavioral and computationalexperiments employing fictional prompts as a novel tool for investigatingcultural artifacts and social biases in storytelling both by humans andgenerative AI. The study analyzes 250 stories authored by crowdworkers in June2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by mergingmethods from narratology and inferential statistics. Both crowdworkers andlarge language models responded to identical prompts about creating and fallingin love with an artificial human. The proposed experimental paradigm allows adirect comparison between human and LLM-generated storytelling. Responses tothe Pygmalionesque prompts confirm the pervasive presence of the Pygmalion mythin the collective imaginary of both humans and large language models. Allsolicited narratives present a scientific or technological pursuit. Theanalysis reveals that narratives from GPT-3.5 and particularly GPT-4 are moremore progressive in terms of gender roles and sexuality than those written byhumans. While AI narratives can occasionally provide innovative plot twists,they offer less imaginative scenarios and rhetoric than human-authored texts.The proposed framework argues that fiction can be used as a window into humanand AI-based collective imaginary and social dimensions.</description><author>Nina Begus</author><pubDate>Thu, 19 Oct 2023 17:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12902v1</guid></item><item><title>Personalized human mobility prediction for HuMob challenge</title><link>http://arxiv.org/abs/2310.12900v1</link><description>We explain the methodology used to create the data submitted to HuMobChallenge, a data analysis competition for human mobility prediction. Weadopted a personalized model to predict the individual's movement trajectoryfrom their data, instead of predicting from the overall movement, based on thehypothesis that human movement is unique to each person. We devised thefeatures such as the date and time, activity time, days of the week, time ofday, and frequency of visits to POI (Point of Interest). As additionalfeatures, we incorporated the movement of other individuals with similarbehavior patterns through the employment of clustering. The machine learningmodel we adopted was the Support Vector Regression (SVR). We performed accuracythrough offline assessment and carried out feature selection and parametertuning. Although overall dataset provided consists of 100,000 users trajectory,our method use only 20,000 target users data, and do not need to use other80,000 data. Despite the personalized model's traditional feature engineeringapproach, this model yields reasonably good accuracy with lower computationalcost.</description><author>Masahiro Suzuki, Shomu Furuta, Yusuke Fukazawa</author><pubDate>Thu, 19 Oct 2023 17:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12900v1</guid></item><item><title>Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System</title><link>http://arxiv.org/abs/2306.09821v2</link><description>Dialogue systems and large language models (LLMs) have gained considerableattention. However, the direct utilization of LLMs as task-oriented dialogue(TOD) models has been found to underperform compared to smaller task-specificmodels. Nonetheless, it is crucial to acknowledge the significant potential ofLLMs and explore improved approaches for leveraging their impressive abilities.Motivated by the goal of leveraging LLMs, we propose an alternative approachcalled User-Guided Response Optimization (UGRO) to combine it with a smallerTOD model. This approach uses LLM as annotation-free user simulator to assessdialogue responses, combining them with smaller fine-tuned end-to-end TODmodels. By utilizing the satisfaction feedback generated by LLMs, UGRO furtheroptimizes the supervised fine-tuned TOD model. Specifically, the TOD modeltakes the dialogue history as input and, with the assistance of the usersimulator's feedback, generates high-satisfaction responses that meet theuser's requirements. Through empirical experiments on two TOD benchmarks, wevalidate the effectiveness of our method. The results demonstrate that ourapproach outperforms previous state-of-the-art (SOTA) results.</description><author>Zhiyuan Hu, Yue Feng, Anh Tuan Luu, Bryan Hooi, Aldo Lipani</author><pubDate>Thu, 19 Oct 2023 17:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09821v2</guid></item><item><title>Monocular 2D Camera-based Proximity Monitoring for Human-Machine Collision Warning on Construction Sites</title><link>http://arxiv.org/abs/2305.17931v2</link><description>Accident of struck-by machines is one of the leading causes of casualties onconstruction sites. Monitoring workers' proximities to avoid human-machinecollisions has aroused great concern in construction safety management.Existing methods are either too laborious and costly to apply extensively, orlacking spatial perception for accurate monitoring. Therefore, this studyproposes a novel framework for proximity monitoring using only an ordinary 2Dcamera to realize real-time human-machine collision warning, which is designedto integrate a monocular 3D object detection model to perceive spatialinformation from 2D images and a post-processing classification module toidentify the proximity as four predefined categories: Dangerous, PotentiallyDangerous, Concerned, and Safe. A virtual dataset containing 22000 images with3D annotations is constructed and publicly released to facilitate the systemdevelopment and evaluation. Experimental results show that the trained 3Dobject detection model achieves 75% loose AP within 20 meters. Besides, theimplemented system is real-time and camera carrier-independent, achieving an F1of roughly 0.8 within 50 meters under specified settings for machines ofdifferent sizes. This study preliminarily reveals the potential and feasibilityof proximity monitoring using only a 2D camera, providing a new promising andeconomical way for early warning of human-machine collisions.</description><author>Yuexiong Ding, Xiaowei Luo</author><pubDate>Thu, 19 Oct 2023 17:48:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17931v2</guid></item><item><title>Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines</title><link>http://arxiv.org/abs/2310.07940v2</link><description>Researchers have long touted a vision of the future enabled by aproliferation of internet-of-things devices, including smart sensors, homes,and cities. Increasingly, embedding intelligence in such devices involves theuse of deep neural networks. However, their storage and processing requirementsmake them prohibitive for cheap, off-the-shelf platforms. Overcoming thoserequirements is necessary for enabling widely-applicable smart devices. Whilemany ways of making models smaller and more efficient have been developed,there is a lack of understanding of which ones are best suited for particularscenarios. More importantly for edge platforms, those choices cannot beanalyzed in isolation from cost and user experience. In this work, weholistically explore how quantization, model scaling, and multi-modalityinteract with system components such as memory, sensors, and processors. Weperform this hardware/software co-design from the cost, latency, anduser-experience perspective, and develop a set of guidelines for optimal systemdesign and model deployment for the most cost-constrained platforms. Wedemonstrate our approach using an end-to-end, on-device, biometric userauthentication system using a $20 ESP-EYE board.</description><author>Ravit Sharma, Wojciech Romaszkan, Feiqian Zhu, Puneet Gupta, Ankur Mehta</author><pubDate>Thu, 19 Oct 2023 17:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07940v2</guid></item><item><title>Blind quantum machine learning with quantum bipartite correlator</title><link>http://arxiv.org/abs/2310.12893v1</link><description>Distributed quantum computing is a promising computational paradigm forperforming computations that are beyond the reach of individual quantumdevices. Privacy in distributed quantum computing is critical for maintainingconfidentiality and protecting the data in the presence of untrusted computingnodes. In this work, we introduce novel blind quantum machine learningprotocols based on the quantum bipartite correlator algorithm. Our protocolshave reduced communication overhead while preserving the privacy of data fromuntrusted parties. We introduce robust algorithm-specific privacy-preservingmechanisms with low computational overhead that do not require complexcryptographic techniques. We then validate the effectiveness of the proposedprotocols through complexity and privacy analysis. Our findings pave the wayfor advancements in distributed quantum computing, opening up new possibilitiesfor privacy-aware machine learning applications in the era of quantumtechnologies.</description><author>Changhao Li, Boning Li, Omar Amer, Ruslan Shaydulin, Shouvanik Chakrabarti, Guoqing Wang, Haowei Xu, Hao Tang, Isidor Schoch, Niraj Kumar, Charles Lim, Ju Li, Paola Cappellaro, Marco Pistoia</author><pubDate>Thu, 19 Oct 2023 17:42:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12893v1</guid></item><item><title>A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems</title><link>http://arxiv.org/abs/2310.12892v1</link><description>Achieving robust language technologies that can perform well across theworld's many languages is a central goal of multilingual NLP. In this work, wetake stock of and empirically analyse task performance disparities that existbetween multilingual task-oriented dialogue (ToD) systems. We first define newquantitative measures of absolute and relative equivalence in systemperformance, capturing disparities across languages and within individuallanguages. Through a series of controlled experiments, we demonstrate thatperformance disparities depend on a number of factors: the nature of the ToDtask at hand, the underlying pretrained language model, the target language,and the amount of ToD annotated data. We empirically prove the existence of theadaptation and intrinsic biases in current ToD systems: e.g., ToD systemstrained for Arabic or Turkish using annotated ToD data fully parallel toEnglish ToD data still exhibit diminished ToD task performance. Beyondproviding a series of insights into the performance disparities of ToD systemsin different languages, our analyses offer practical tips on how to approachToD data collection and system development for new languages.</description><author>Songbo Hu, Han Zhou, Moy Yuan, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Anna Korhonen, Ivan Vulić</author><pubDate>Thu, 19 Oct 2023 17:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12892v1</guid></item><item><title>Microscaling Data Formats for Deep Learning</title><link>http://arxiv.org/abs/2310.10537v3</link><description>Narrow bit-width data formats are key to reducing the computational andstorage costs of modern deep learning applications. This paper evaluatesMicroscaling (MX) data formats that combine a per-block scaling factor withnarrow floating-point and integer types for individual elements. MX formatsbalance the competing needs of hardware efficiency, model accuracy, and userfriction. Empirical results on over two dozen benchmarks demonstratepracticality of MX data formats as a drop-in replacement for baseline FP32 forAI inference and training with low user friction. We also show the firstinstance of training generative language models at sub-8-bit weights,activations, and gradients with minimal accuracy loss and no modifications tothe training recipe.</description><author>Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, Stosic Dusan, Venmugil Elango, Maximilian Golub, Alexander Heinecke, Phil James-Roxby, Dharmesh Jani, Gaurav Kolhe, Martin Langhammer, Ada Li, Levi Melnick, Maral Mesmakhosroshahi, Andres Rodriguez, Michael Schulte, Rasoul Shafipour, Lei Shao, Michael Siu, Pradeep Dubey, Paulius Micikevicius, Maxim Naumov, Colin Verrilli, Ralph Wittig, Doug Burger, Eric Chung</author><pubDate>Thu, 19 Oct 2023 17:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10537v3</guid></item><item><title>Sequential Gibbs Posteriors with Applications to Principal Component Analysis</title><link>http://arxiv.org/abs/2310.12882v1</link><description>Gibbs posteriors are proportional to a prior distribution multiplied by anexponentiated loss function, with a key tuning parameter weighting informationin the loss relative to the prior and providing a control of posterioruncertainty. Gibbs posteriors provide a principled framework forlikelihood-free Bayesian inference, but in many situations, including a singletuning parameter inevitably leads to poor uncertainty quantification. Inparticular, regardless of the value of the parameter, credible regions have farfrom the nominal frequentist coverage even in large samples. We propose asequential extension to Gibbs posteriors to address this problem. We prove theproposed sequential posterior exhibits concentration and a Bernstein-von Misestheorem, which holds under easy to verify conditions in Euclidean space and onmanifolds. As a byproduct, we obtain the first Bernstein-von Mises theorem fortraditional likelihood-based Bayesian posteriors on manifolds. All methods areillustrated with an application to principal component analysis.</description><author>Steven Winter, Omar Melikechi, David B. Dunson</author><pubDate>Thu, 19 Oct 2023 17:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12882v1</guid></item><item><title>TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports</title><link>http://arxiv.org/abs/2310.12880v1</link><description>The idea of next-generation ports has become more apparent in the last tenyears in response to the challenge posed by the rising demand for efficiencyand the ever-increasing volume of goods. In this new era of intelligentinfrastructure and facilities, it is evident that cyber-security has recentlyreceived the most significant attention from the seaport and maritimeauthorities, and it is a primary concern on the agenda of most ports.Traditional security solutions can be applied to safeguard IoT andCyber-Physical Systems (CPS) from harmful entities. Nevertheless, securityresearchers can only watch, examine, and learn about the behaviors of attackersif these solutions operate more transparently. Herein, honeypots are potentialsolutions since they offer valuable information about the attackers. It can bevirtual or physical. Virtual honeypots must be more realistic to enticeattackers, necessitating better high-fidelity. To this end, Digital Twin (DT)technology can be employed to increase the complexity and simulation fidelityof the honeypots. Seaports can be attacked from both their existing devices andexternal devices at the same time. Existing mechanisms are insufficient todetect external attacks; therefore, the current systems cannot handle attacksat the desired level. DT and honeypot technologies can be used together totackle them. Consequently, we suggest a DT-assisted honeypot, called TwinPot,for external attacks in smart seaports. Moreover, we propose an intelligentattack detection mechanism to handle different attack types using DT forinternal attacks. Finally, we build an extensive smart seaport dataset forinternal and external attacks using the MANSIM tool and two existing datasetsto test the performance of our system. We show that under simultaneous internaland external attacks on the system, our solution successfully detects internaland external attacks.</description><author>Yagmur Yigit, Omer Kemal Kinaci, Trung Q. Duong, Berk Canberk</author><pubDate>Thu, 19 Oct 2023 17:35:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12880v1</guid></item><item><title>Perceptual Assessment and Optimization of High Dynamic Range Image Rendering</title><link>http://arxiv.org/abs/2310.12877v1</link><description>High dynamic range (HDR) imaging has gained increasing popularity for itsability to faithfully reproduce the luminance levels in natural scenes.Accordingly, HDR image quality assessment (IQA) is crucial but has beensuperficially treated. The majority of existing IQA models are developed forand calibrated against low dynamic range (LDR) images, which have been shown tobe poorly correlated with human perception of HDR image quality. In this work,we propose a family of HDR IQA models by transferring the recent advances inLDR IQA. The key step in our approach is to specify a simple inverse displaymodel that decomposes an HDR image to a set of LDR images with differentexposures, which will be assessed by existing LDR quality models. The localquality scores of each exposure are then aggregated with the help of a simplewell-exposedness measure into a global quality score for each exposure, whichwill be further weighted across exposures to obtain the overall quality score.When assessing LDR images, the proposed HDR quality models reduce gracefully tothe original LDR ones with the same performance. Experiments on fourhuman-rated HDR image datasets demonstrate that our HDR quality models areconsistently better than existing IQA methods, including the HDR-VDP family.Moreover, we demonstrate their strengths in perceptual optimization of HDRnovel view synthesis.</description><author>Peibei Cao, Rafal K. Mantiuk, Kede Ma</author><pubDate>Thu, 19 Oct 2023 17:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12877v1</guid></item><item><title>StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding</title><link>http://arxiv.org/abs/2310.12874v1</link><description>Analogy-making between narratives is one of the most critical abilities innatural language understanding. In this paper, we evaluate the ability toidentify and generate analogy by building a first-of-its-kind large-scalestory-level analogy corpus, StoryAnalogy, which contains 24K story pairs fromdiverse domains with human annotations on two similarities from the extendedStructure-Mapping Theory. We design a set of tests on StoryAnalogy, presentingthe first evaluation of story-level analogy identification and generation.Interestingly, we find that the analogy identification tasks are extremelychallenging not only for the sentence embedding models but also for the recentlarge language models (LLMs) such as ChatGPT and LLaMa, where ChatGPT onlyachieved around 30% accuracy in multiple-choice questions (&gt; 85% accuracy forhumans). Finally, we find that data in StoryAnalogy can improve LLMs analogygeneration quality, where a fine-tuned FlanT5-xxl model yields comparableperformance to zero-shot ChatGPT.</description><author>Cheng Jiayang, Lin Qiu, Tsz Ho Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang, Zheng Zhang</author><pubDate>Thu, 19 Oct 2023 17:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12874v1</guid></item><item><title>Voyager: An Open-Ended Embodied Agent with Large Language Models</title><link>http://arxiv.org/abs/2305.16291v2</link><description>We introduce Voyager, the first LLM-powered embodied lifelong learning agentin Minecraft that continuously explores the world, acquires diverse skills, andmakes novel discoveries without human intervention. Voyager consists of threekey components: 1) an automatic curriculum that maximizes exploration, 2) anever-growing skill library of executable code for storing and retrievingcomplex behaviors, and 3) a new iterative prompting mechanism that incorporatesenvironment feedback, execution errors, and self-verification for programimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypassesthe need for model parameter fine-tuning. The skills developed by Voyager aretemporally extended, interpretable, and compositional, which compounds theagent's abilities rapidly and alleviates catastrophic forgetting. Empirically,Voyager shows strong in-context lifelong learning capability and exhibitsexceptional proficiency in playing Minecraft. It obtains 3.3x more uniqueitems, travels 2.3x longer distances, and unlocks key tech tree milestones upto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skilllibrary in a new Minecraft world to solve novel tasks from scratch, while othertechniques struggle to generalize. We open-source our full codebase and promptsat https://voyager.minedojo.org/.</description><author>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar</author><pubDate>Thu, 19 Oct 2023 17:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16291v2</guid></item><item><title>zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning</title><link>http://arxiv.org/abs/2310.02554v3</link><description>Federated Learning (FL) is a machine learning paradigm, which enablesmultiple and decentralized clients to collaboratively train a model under theorchestration of a central aggregator. Traditional FL solutions rely on thetrust assumption of the centralized aggregator, which forms cohorts of clientsin a fair and honest manner. However, a malicious aggregator, in reality, couldabandon and replace the client's training models, or launch Sybil attacks toinsert fake clients. Such malicious behaviors give the aggregator more power tocontrol clients in the FL setting and determine the final training results. Inthis work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) totackle the issue of a malicious aggregator during the training modelaggregation process. To guarantee the correct aggregation results, theaggregator needs to provide a proof per round. The proof can demonstrate to theclients that the aggregator executes the intended behavior faithfully. Tofurther reduce the verification cost of clients, we employ a blockchain tohandle the proof in a zero-knowledge way, where miners (i.e., the nodesvalidating and maintaining the blockchain data) can verify the proof withoutknowing the clients' local and aggregated models. The theoretical analysis andempirical results show that zkFL can achieve better security and privacy thantraditional FL, without modifying the underlying FL network structure orheavily compromising the training speed.</description><author>Zhipeng Wang, Nanqing Dong, Jiahao Sun, William Knottenbelt</author><pubDate>Thu, 19 Oct 2023 17:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02554v3</guid></item><item><title>Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair</title><link>http://arxiv.org/abs/2309.00608v2</link><description>During Automated Program Repair (APR), it can be challenging to synthesizecorrect patches for real-world systems in general-purpose programminglanguages. Recent Large Language Models (LLMs) have been shown to be helpful"copilots" in assisting developers with various coding tasks, and have alsobeen directly applied for patch synthesis. However, most LLMs treat programs assequences of tokens, meaning that they are ignorant of the underlying semanticsconstraints of the target programming language. This results in plenty ofstatically invalid generated patches, impeding the practicality of thetechnique. Therefore, we propose Repilot, a framework to further copilot the AI"copilots" (i.e., LLMs) by synthesizing more valid patches during the repairprocess. Our key insight is that many LLMs produce outputs autoregressively(i.e., token by token), resembling human writing programs, which can besignificantly boosted and guided through a Completion Engine. Repilotsynergistically synthesizes a candidate patch through the interaction betweenan LLM and a Completion Engine, which 1) prunes away infeasible tokenssuggested by the LLM and 2) proactively completes the token based on thesuggestions provided by the Completion Engine. Our evaluation on a subset ofthe widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and50 bugs, respectively, surpassing the best-performing baseline by 14 and 16bugs fixed. More importantly, Repilot is capable of producing more valid andcorrect patches than the base LLM when given the same generation budget.</description><author>Yuxiang Wei, Chunqiu Steven Xia, Lingming Zhang</author><pubDate>Thu, 19 Oct 2023 17:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00608v2</guid></item><item><title>The Adaptive $τ$-Lasso: Robustness and Oracle Properties</title><link>http://arxiv.org/abs/2304.09310v2</link><description>This paper introduces a new regularized version of the robust$\tau$-regression estimator for analyzing high-dimensional datasets subject togross contamination in the response variables and covariates (explanatoryvariables). The resulting estimator, termed adaptive $\tau$-Lasso, is robust tooutliers and high-leverage points. It also incorporates an adaptive$\ell_1$-norm penalty term, which enables the selection of relevant variablesand reduces the bias associated with large true regression coefficients. Morespecifically, this adaptive $\ell_1$-norm penalty term assigns a weight to eachregression coefficient. For a fixed number of predictors $p$, we show that theadaptive $\tau$-Lasso has the oracle property, ensuring both variable-selectionconsistency and asymptotic normality. Asymptotic normality applies only to theentries of the regression vector corresponding to the true support, assumingknowledge of the true regression vector support. We characterize its robustnessvia the finite-sample breakdown point and the influence function. We carry outextensive simulations and observe that the class of $\tau$-Lasso estimatorsexhibits robustness and reliable performance in both contaminated anduncontaminated data settings. We also validate our theoretical findings onrobustness properties through simulation experiments. In the face of outliersand high-leverage points, the adaptive $\tau$-Lasso and $\tau$-Lasso estimatorsachieve the best performance or close-to-best performance in terms ofprediction and variable selection accuracy compared to other competingregularized estimators for all scenarios considered in this study. Therefore,the adaptive $\tau$-Lasso and $\tau$-Lasso estimators can be effectivelyemployed for a variety of sparse linear regression problems, particularly inhigh-dimensional settings and when the data is contaminated by outliers andhigh-leverage points.</description><author>Emadaldin Mozafari-Majd, Visa Koivunen</author><pubDate>Thu, 19 Oct 2023 17:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09310v2</guid></item><item><title>EMIT-Diff: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model</title><link>http://arxiv.org/abs/2310.12868v1</link><description>Large-scale, big-variant, and high-quality data are crucial for developingrobust and successful deep-learning models for medical applications since theypotentially enable better generalization performance and avoid overfitting.However, the scarcity of high-quality labeled data always presents significantchallenges. This paper proposes a novel approach to address this challenge bydeveloping controllable diffusion models for medical image synthesis, calledEMIT-Diff. We leverage recent diffusion probabilistic models to generaterealistic and diverse synthetic medical image data that preserve the essentialcharacteristics of the original medical images by incorporating edgeinformation of objects to guide the synthesis process. In our approach, weensure that the synthesized samples adhere to medically relevant constraintsand preserve the underlying structure of imaging data. Due to the randomsampling process by the diffusion model, we can generate an arbitrary number ofsynthetic images with diverse appearances. To validate the effectiveness of ourproposed method, we conduct an extensive set of medical image segmentationexperiments on multiple datasets, including Ultrasound breast (+13.87%), CTspleen (+0.38%), and MRI prostate (+7.78%), achieving significant improvementsover the baseline segmentation methods. For the first time, to our bestknowledge, the promising results demonstrate the effectiveness of our EMIT-Difffor medical image segmentation tasks and show the feasibility of introducing afirst-ever text-guided diffusion model for general medical image segmentationtasks. With carefully designed ablation experiments, we investigate theinfluence of various data augmentation ratios, hyper-parameter settings, patchsize for generating random merging mask settings, and combined influence withdifferent network architectures.</description><author>Zheyuan Zhang, Lanhong Yao, Bin Wang, Debesh Jha, Elif Keles, Alpay Medetalibeyoglu, Ulas Bagci</author><pubDate>Thu, 19 Oct 2023 17:18:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12868v1</guid></item><item><title>Predicting Ovarian Cancer Treatment Response in Histopathology using Hierarchical Vision Transformers and Multiple Instance Learning</title><link>http://arxiv.org/abs/2310.12866v1</link><description>For many patients, current ovarian cancer treatments offer limited clinicalbenefit. For some therapies, it is not possible to predict patients' responses,potentially exposing them to the adverse effects of treatment without anytherapeutic benefit. As part of the automated prediction of treatmenteffectiveness in ovarian cancer using histopathological images (ATEC23)challenge, we evaluated the effectiveness of deep learning to predict whether acourse of treatment including the antiangiogenic drug bevacizumab couldcontribute to remission or prevent disease progression for at least 6 months ina set of 282 histopathology whole slide images (WSIs) from 78 ovarian cancerpatients. Our approach used a pretrained Hierarchical Image Pyramid Transformer(HIPT) to extract region-level features and an attention-based multipleinstance learning (ABMIL) model to aggregate features and classify wholeslides. The optimal HIPT-ABMIL model had an internal balanced accuracy of 60.2%+- 2.9% and an AUC of 0.646 +- 0.033. Histopathology-specific model pretrainingwas found to be beneficial to classification performance, though hierarchicaltransformers were not, with a ResNet feature extractor achieving similarperformance. Due to the dataset being small and highly heterogeneous,performance was variable across 5-fold cross-validation folds, and there weresome extreme differences between validation and test set performance withinfolds. The model did not generalise well to tissue microarrays, with accuracyworse than random chance. It is not yet clear whether ovarian cancer WSIscontain information that can be used to accurately predict treatment response,with further validation using larger, higher-quality datasets required.</description><author>Jack Breen, Katie Allen, Kieran Zucker, Geoff Hall, Nishant Ravikumar, Nicolas M. Orsi</author><pubDate>Thu, 19 Oct 2023 17:16:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12866v1</guid></item><item><title>The Locality and Symmetry of Positional Encodings</title><link>http://arxiv.org/abs/2310.12864v1</link><description>Positional Encodings (PEs) are used to inject word-order information intotransformer-based language models. While they can significantly enhance thequality of sentence representations, their specific contribution to languagemodels is not fully understood, especially given recent findings that variouspositional encodings are insensitive to word order. In this work, we conduct asystematic study of positional encodings in \textbf{Bidirectional MaskedLanguage Models} (BERT-style) , which complements existing work in threeaspects: (1) We uncover the core function of PEs by identifying two commonproperties, Locality and Symmetry; (2) We show that the two properties areclosely correlated with the performances of downstream tasks; (3) We quantifythe weakness of current PEs by introducing two new probing tasks, on whichcurrent PEs perform poorly. We believe that these results are the basis fordeveloping better PEs for transformer-based language models. The code isavailable at \faGithub~ \url{https://github.com/tigerchen52/locality\_symmetry}</description><author>Lihu Chen, Gaël Varoquaux, Fabian M. Suchanek</author><pubDate>Thu, 19 Oct 2023 17:15:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12864v1</guid></item><item><title>Fine-Tuning Generative Models as an Inference Method for Robotic Tasks</title><link>http://arxiv.org/abs/2310.12862v1</link><description>Adaptable models could greatly benefit robotic agents operating in the realworld, allowing them to deal with novel and varying conditions. Whileapproaches such as Bayesian inference are well-studied frameworks for adaptingmodels to evidence, we build on recent advances in deep generative models whichhave greatly affected many areas of robotics. Harnessing modern GPUacceleration, we investigate how to quickly adapt the sample generation ofneural network models to observations in robotic tasks. We propose a simple andgeneral method that is applicable to various deep generative models and roboticenvironments. The key idea is to quickly fine-tune the model by fitting it togenerated samples matching the observed evidence, using the cross-entropymethod. We show that our method can be applied to both autoregressive modelsand variational autoencoders, and demonstrate its usability in object shapeinference from grasping, inverse kinematics calculation, and point cloudcompletion.</description><author>Orr Krupnik, Elisei Shafer, Tom Jurgenson, Aviv Tamar</author><pubDate>Thu, 19 Oct 2023 17:11:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12862v1</guid></item><item><title>Probing LLMs for hate speech detection: strengths and vulnerabilities</title><link>http://arxiv.org/abs/2310.12860v1</link><description>Recently efforts have been made by social media platforms as well asresearchers to detect hateful or toxic language using large language models.However, none of these works aim to use explanation, additional context andvictim community information in the detection process. We utilise differentprompt variation, input information and evaluate large language models in zeroshot setting (without adding any in-context examples). We select three largelanguage models (GPT-3.5, text-davinci and Flan-T5) and three datasets -HateXplain, implicit hate and ToxicSpans. We find that on average including thetarget information in the pipeline improves the model performance substantially(~20-30%) over the baseline across the datasets. There is also a considerableeffect of adding the rationales/explanations into the pipeline (~10-20%) overthe baseline across the datasets. In addition, we further provide a typology ofthe error cases where these large language models fail to (i) classify and (ii)explain the reason for the decisions they take. Such vulnerable pointsautomatically constitute 'jailbreak' prompts for these models and industryscale safeguard techniques need to be developed to make the models robustagainst such prompts.</description><author>Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha</author><pubDate>Thu, 19 Oct 2023 17:11:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12860v1</guid></item><item><title>Audio Editing with Non-Rigid Text Prompts</title><link>http://arxiv.org/abs/2310.12858v1</link><description>In this paper, we explore audio-editing with non-rigid text edits. We showthat the proposed editing pipeline is able to create audio edits that remainfaithful to the input audio. We explore text prompts that perform addition,style transfer, and in-painting. We quantitatively and qualitatively show thatthe edits are able to obtain results which outperform Audio-LDM, a recentlyreleased text-prompted audio generation model. Qualitative inspection of theresults points out that the edits given by our approach remain more faithful tothe input audio in terms of keeping the original onsets and offsets of theaudio events.</description><author>Francesco Paissan, Zhepei Wang, Mirco Ravanelli, Paris Smaragdis, Cem Subakan</author><pubDate>Thu, 19 Oct 2023 17:09:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12858v1</guid></item><item><title>ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing</title><link>http://arxiv.org/abs/2310.08099v2</link><description>Climate change's impact on human health poses unprecedented and diversechallenges. Unless proactive measures based on solid evidence are implemented,these threats will likely escalate and continue to endanger human well-being.The escalating advancements in information and communication technologies havefacilitated the widespread availability and utilization of social mediaplatforms. Individuals utilize platforms such as Twitter and Facebook toexpress their opinions, thoughts, and critiques on diverse subjects,encompassing the pressing issue of climate change. The proliferation of climatechange-related content on social media necessitates comprehensive analysis toglean meaningful insights. This paper employs natural language processing (NLP)techniques to analyze climate change discourse and quantify the sentiment ofclimate change-related tweets. We use ClimateBERT, a pretrained modelfine-tuned specifically for the climate change domain. The objective is todiscern the sentiment individuals express and uncover patterns in publicopinion concerning climate change. Analyzing tweet sentiments allows a deepercomprehension of public perceptions, concerns, and emotions about this criticalglobal challenge. The findings from this experiment unearth valuable insightsinto public sentiment and the entities associated with climate changediscourse. Policymakers, researchers, and organizations can leverage suchanalyses to understand public perceptions, identify influential actors, anddevise informed strategies to address climate change challenges.</description><author>Ajay Krishnan, V. S. Anoop</author><pubDate>Thu, 19 Oct 2023 17:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08099v2</guid></item><item><title>Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning</title><link>http://arxiv.org/abs/2306.00477v4</link><description>Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)has emerged as a highly successful approach, with training only a small numberof parameters without sacrificing performance and becoming the de-factolearning paradigm with the increasing size of PLMs. However, existing PEFTmethods are not memory-efficient, because they still require caching most ofthe intermediate activations for the gradient calculation, akin to fine-tuning.One effective way to reduce the activation memory is to apply a reversiblemodel, so the intermediate activations are not necessary to be cached and canbe recomputed. Nevertheless, modifying a PLM to its reversible variant is notstraightforward, since the reversible model has a distinct architecture fromthe currently released PLMs. In this paper, we first investigate what is a keyfactor for the success of existing PEFT methods, and realize that it'sessential to preserve the PLM's starting point when initializing a PEFT method.With this finding, we propose memory-efficient fine-tuning (MEFT) that insertsadapters into a PLM, preserving the PLM's starting point and making itreversible without additional pre-training. We evaluate MEFT on the GLUEbenchmark and five question-answering tasks with various backbones, BERT,RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to84% of full fine-tuning with a negligible amount of trainable parameters.Moreover, MEFT achieves the same score on GLUE and a comparable score on thequestion-answering tasks as full fine-tuning. A similar finding is alsoobserved for the image classification task.</description><author>Baohao Liao, Shaomu Tan, Christof Monz</author><pubDate>Thu, 19 Oct 2023 17:03:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00477v4</guid></item><item><title>EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks</title><link>http://arxiv.org/abs/2310.12851v1</link><description>In the era of advanced artificial intelligence and human-computerinteraction, identifying emotions in spoken language is paramount. Thisresearch explores the integration of deep learning techniques in speech emotionrecognition, offering a comprehensive solution to the challenges associatedwith speaker diarization and emotion identification. It introduces a frameworkthat combines a pre-existing speaker diarization pipeline and an emotionidentification model built on a Convolutional Neural Network (CNN) to achievehigher precision. The proposed model was trained on data from five speechemotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, outof which the latter is a speech emotion dataset created specifically for thisresearch. The features extracted from each sample include Mel FrequencyCepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS),and various data augmentation algorithms like pitch, noise, stretch, and shift.This feature extraction approach aims to enhance prediction accuracy whilereducing computational complexity. The proposed model yields an unweightedaccuracy of 63%, demonstrating remarkable efficiency in accurately identifyingemotional states within speech signals.</description><author>Hanan Hamza, Fiza Gafoor, Fathima Sithara, Gayathri Anil, V. S. Anoop</author><pubDate>Thu, 19 Oct 2023 17:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12851v1</guid></item><item><title>Neural Degradation Representation Learning for All-In-One Image Restoration</title><link>http://arxiv.org/abs/2310.12848v1</link><description>Existing methods have demonstrated effective performance on a singledegradation type. In practical applications, however, the degradation is oftenunknown, and the mismatch between the model and the degradation will result ina severe performance drop. In this paper, we propose an all-in-one imagerestoration network that tackles multiple degradations. Due to theheterogeneous nature of different types of degradations, it is difficult toprocess multiple degradations in a single network. To this end, we propose tolearn a neural degradation representation (NDR) that captures the underlyingcharacteristics of various degradations. The learned NDR decomposes differenttypes of degradations adaptively, similar to a neural dictionary thatrepresents basic degradation components. Subsequently, we develop a degradationquery module and a degradation injection module to effectively recognize andutilize the specific degradation based on NDR, enabling the all-in-onerestoration ability for multiple degradations. Moreover, we propose abidirectional optimization strategy to effectively drive NDR to learn thedegradation representation by optimizing the degradation and restorationprocesses alternately. Comprehensive experiments on representative types ofdegradations (including noise, haze, rain, and downsampling) demonstrate theeffectiveness and generalization capability of our method.</description><author>Mingde Yao, Ruikang Xu, Yuanshen Guan, Jie Huang, Zhiwei Xiong</author><pubDate>Thu, 19 Oct 2023 16:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12848v1</guid></item><item><title>Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration</title><link>http://arxiv.org/abs/2310.05069v2</link><description>Pretrained multilingual encoder models can directly perform zero-shotmultilingual tasks or linguistic probing by reformulating the input examplesinto cloze-style prompts. This is accomplished by predicting the probabilitiesof the label words at the masked token position, without requiring any updatesto the model parameters. However, the performance of this method is limited bythe model's bias toward predicting label words which frequently occurred duringthe pretraining. These words typically receive high probabilities. To addressthis issue, we combine the models with calibration techniques which modify theprobabilities of label words predicted by the models. We first validate theeffectiveness of a proposed simple calibration method together with otherexisting techniques on monolingual encoders in both zero- and few-shotscenarios. We subsequently employ these calibration techniques on multilingualencoders, resulting in substantial performance improvements across a wide rangeof tasks.</description><author>Ercong Nie, Helmut Schmid, Hinrich Schütze</author><pubDate>Thu, 19 Oct 2023 16:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05069v2</guid></item><item><title>Physical Information Neural Networks for Solving High-index Differential-algebraic Equation Systems Based on Radau Methods</title><link>http://arxiv.org/abs/2310.12846v1</link><description>As is well known, differential algebraic equations (DAEs), which are able todescribe dynamic changes and underlying constraints, have been widely appliedin engineering fields such as fluid dynamics, multi-body dynamics, mechanicalsystems and control theory. In practical physical modeling within thesedomains, the systems often generate high-index DAEs. Classical implicitnumerical methods typically result in varying order reduction of numericalaccuracy when solving high-index systems.~Recently, the physics-informed neuralnetwork (PINN) has gained attention for solving DAE systems. However, it faceschallenges like the inability to directly solve high-index systems, lowerpredictive accuracy, and weaker generalization capabilities. In this paper, wepropose a PINN computational framework, combined Radau IIA numerical methodwith a neural network structure via the attention mechanisms, to directly solvehigh-index DAEs. Furthermore, we employ a domain decomposition strategy toenhance solution accuracy. We conduct numerical experiments with two classicalhigh-index systems as illustrative examples, investigating how different ordersof the Radau IIA method affect the accuracy of neural network solutions. Theexperimental results demonstrate that the PINN based on a 5th-order Radau IIAmethod achieves the highest level of system accuracy. Specifically, theabsolute errors for all differential variables remains as low as $10^{-6}$, andthe absolute errors for algebraic variables is maintained at $10^{-5}$,surpassing the results found in existing literature. Therefore, our methodexhibits excellent computational accuracy and strong generalizationcapabilities, providing a feasible approach for the high-precision solution oflarger-scale DAEs with higher indices or challenging high-dimensional partialdifferential algebraic equation systems.</description><author>Jiasheng Chen, Juan Tang, Ming Yan, Shuai Lai, Kun Liang, Jianguang Lu, Wenqiang Yang</author><pubDate>Thu, 19 Oct 2023 16:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12846v1</guid></item><item><title>An Introduction to Transformers</title><link>http://arxiv.org/abs/2304.10557v4</link><description>The transformer is a neural network component that can be used to learnuseful representations of sequences or sets of data-points. The transformer hasdriven recent advances in natural language processing, computer vision, andspatio-temporal modelling. There are many introductions to transformers, butmost do not contain precise mathematical descriptions of the architecture andthe intuitions behind the design choices are often also missing. Moreover, asresearch takes a winding path, the explanations for the components of thetransformer can be idiosyncratic. In this note we aim for a mathematicallyprecise, intuitive, and clean description of the transformer architecture. Wewill not discuss training as this is rather standard. We assume that the readeris familiar with fundamental topics in machine learning including multi-layerperceptrons, linear transformations, softmax functions and basic probability.</description><author>Richard E. Turner</author><pubDate>Thu, 19 Oct 2023 16:52:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10557v4</guid></item><item><title>Model-agnostic variable importance for predictive uncertainty: an entropy-based approach</title><link>http://arxiv.org/abs/2310.12842v1</link><description>In order to trust the predictions of a machine learning algorithm, it isnecessary to understand the factors that contribute to those predictions. Inthe case of probabilistic and uncertainty-aware models, it is necessary tounderstand not only the reasons for the predictions themselves, but also themodel's level of confidence in those predictions. In this paper, we show howexisting methods in explainability can be extended to uncertainty-aware modelsand how such extensions can be used to understand the sources of uncertainty ina model's predictive distribution. In particular, by adapting permutationfeature importance, partial dependence plots, and individual conditionalexpectation plots, we demonstrate that novel insights into model behaviour maybe obtained and that these methods can be used to measure the impact offeatures on both the entropy of the predictive distribution and thelog-likelihood of the ground truth labels under that distribution. Withexperiments using both synthetic and real-world data, we demonstrate theutility of these approaches in understanding both the sources of uncertaintyand their impact on model performance.</description><author>Danny Wood, Theodore Papamarkou, Matt Benatan, Richard Allmendinger</author><pubDate>Thu, 19 Oct 2023 16:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12842v1</guid></item><item><title>The Kernel Density Integral Transformation</title><link>http://arxiv.org/abs/2309.10194v2</link><description>Feature preprocessing continues to play a critical role when applying machinelearning and statistical methods to tabular data. In this paper, we propose theuse of the kernel density integral transformation as a feature preprocessingstep. Our approach subsumes the two leading feature preprocessing methods aslimiting cases: linear min-max scaling and quantile transformation. Wedemonstrate that, without hyperparameter tuning, the kernel density integraltransformation can be used as a simple drop-in replacement for either method,offering protection from the weaknesses of each. Alternatively, with tuning ofa single continuous hyperparameter, we frequently outperform both of thesemethods. Finally, we show that the kernel density transformation can beprofitably applied to statistical data analysis, particularly in correlationanalysis and univariate clustering.</description><author>Calvin McCarter</author><pubDate>Thu, 19 Oct 2023 16:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10194v2</guid></item><item><title>PK-ICR: Persona-Knowledge Interactive Context Retrieval for Grounded Dialogue</title><link>http://arxiv.org/abs/2302.06674v3</link><description>Identifying relevant persona or knowledge for conversational systems iscritical to grounded dialogue response generation. However, each grounding hasbeen mostly researched in isolation with more practical multi-context dialoguetasks introduced in recent works. We define Persona and Knowledge Dual ContextIdentification as the task to identify persona and knowledge jointly for agiven dialogue, which could be of elevated importance in complex multi-contextdialogue settings. We develop a novel grounding retrieval method that utilizesall contexts of dialogue simultaneously. Our method requires less computationalpower via utilizing neural QA retrieval models. We further introduce our novelnull-positive rank test which measures ranking performance on semanticallydissimilar samples (i.e. hard negatives) in relation to data augmentation.</description><author>Minsik Oh, Joosung Lee, Jiwei Li, Guoyin Wang</author><pubDate>Thu, 19 Oct 2023 16:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06674v3</guid></item><item><title>Knowledge-Augmented Language Model Verification</title><link>http://arxiv.org/abs/2310.12836v1</link><description>Recent Language Models (LMs) have shown impressive capabilities in generatingtexts with the knowledge internalized in parameters. Yet, LMs often generatethe factually incorrect responses to the given queries, since their knowledgemay be inaccurate, incomplete, and outdated. To address this problem, previousworks propose to augment LMs with the knowledge retrieved from an externalknowledge source. However, such approaches often show suboptimal textgeneration performance due to two reasons: 1) the model may fail to retrievethe knowledge relevant to the given query, or 2) the model may not faithfullyreflect the retrieved knowledge in the generated text. To overcome these, wepropose to verify the output and the knowledge of the knowledge-augmented LMswith a separate verifier, which is a small LM that is trained to detect thosetwo types of errors through instruction-finetuning. Then, when the verifierrecognizes an error, we can rectify it by either retrieving new knowledge orgenerating new text. Further, we use an ensemble of the outputs from differentinstructions with a single verifier to enhance the reliability of theverification processes. We validate the effectiveness of the proposedverification steps on multiple question answering benchmarks, whose resultsshow that the proposed verifier effectively identifies retrieval and generationerrors, allowing LMs to provide more factually correct outputs. Our code isavailable at https://github.com/JinheonBaek/KALMV.</description><author>Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, Sung Ju Hwang</author><pubDate>Thu, 19 Oct 2023 16:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12836v1</guid></item><item><title>VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights</title><link>http://arxiv.org/abs/2310.11368v2</link><description>Recognizing vulnerability is crucial for understanding and implementingtargeted support to empower individuals in need. This is especially importantat the European Court of Human Rights (ECtHR), where the court adaptsConvention standards to meet actual individual needs and thus ensures effectivehuman rights protection. However, the concept of vulnerability remains elusiveat the ECtHR and no prior NLP research has dealt with it. To enable futureresearch in this area, we present VECHR, a novel expert-annotated multi-labeldataset comprising of vulnerability type classification and explanationrationale. We benchmark the performance of state-of-the-art models on VECHRfrom both prediction and explainability perspectives. Our results demonstratethe challenging nature of the task with lower prediction performance andlimited agreement between models and experts. Further, we analyze therobustness of these models in dealing with out-of-domain (OOD) data and observeoverall limited performance. Our dataset poses unique challenges offeringsignificant room for improvement regarding performance, explainability, androbustness.</description><author>Shanshan Xu, Leon Staufer, Santosh T. Y. S. S, Oana Ichim, Corina Heri, Matthias Grabmair</author><pubDate>Thu, 19 Oct 2023 16:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11368v2</guid></item><item><title>URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates</title><link>http://arxiv.org/abs/2307.03810v2</link><description>Representation learning has significantly driven the field to developpretrained models that can act as a valuable starting point when transferringto new datasets. With the rising demand for reliable machine learning anduncertainty quantification, there is a need for pretrained models that not onlyprovide embeddings but also transferable uncertainty estimates. To guide thedevelopment of such models, we propose the Uncertainty-aware RepresentationLearning (URL) benchmark. Besides the transferability of the representations,it also measures the zero-shot transferability of the uncertainty estimateusing a novel metric. We apply URL to evaluate eleven uncertainty quantifiersthat are pretrained on ImageNet and transferred to eight downstream datasets.We find that approaches that focus on the uncertainty of the representationitself or estimate the prediction risk directly outperform those that are basedon the probabilities of upstream classes. Yet, achieving transferableuncertainty quantification remains an open challenge. Our findings indicatethat it is not necessarily in conflict with traditional representation learninggoals. Code is provided under https://github.com/mkirchhof/url .</description><author>Michael Kirchhof, Bálint Mucsányi, Seong Joon Oh, Enkelejda Kasneci</author><pubDate>Thu, 19 Oct 2023 16:34:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03810v2</guid></item><item><title>From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification</title><link>http://arxiv.org/abs/2310.11878v2</link><description>In legal NLP, Case Outcome Classification (COC) must not only be accurate butalso trustworthy and explainable. Existing work in explainable COC has beenlimited to annotations by a single expert. However, it is well-known thatlawyers may disagree in their assessment of case facts. We hence collect anovel dataset RAVE: Rationale Variation in ECHR1, which is obtained from twoexperts in the domain of international human rights law, for whom we observeweak agreement. We study their disagreements and build a two-leveltask-independent taxonomy, supplemented with COC-specific subcategories. To ourknowledge, this is the first work in the legal NLP that focuses on human labelvariation. We quantitatively assess different taxonomy categories and find thatdisagreements mainly stem from underspecification of the legal context, whichposes challenges given the typically limited granularity and noise in COCmetadata. We further assess the explainablility of SOTA COC models on RAVE andobserve limited agreement between models and experts. Overall, our case studyreveals hitherto underappreciated complexities in creating benchmark datasetsin legal NLP that revolve around identifying aspects of a case's factssupposedly relevant to its outcome.</description><author>Shanshan Xu, Santosh T. Y. S. S, Oana Ichim, Isabella Risini, Barbara Plank, Matthias Grabmair</author><pubDate>Thu, 19 Oct 2023 16:34:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11878v2</guid></item><item><title>INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold</title><link>http://arxiv.org/abs/2204.07439v3</link><description>Binary Neural Networks (BNNs) have emerged as a promising solution forreducing the memory footprint and compute costs of deep neural networks, butthey suffer from quality degradation due to the lack of freedom as activationsand weights are constrained to the binary values. To compensate for theaccuracy drop, we propose a novel BNN design called Binary Neural Network withINSTAnce-aware threshold (INSTA-BNN), which controls the quantization thresholddynamically in an input-dependent or instance-aware manner. According to ourobservation, higher-order statistics can be a representative metric to estimatethe characteristics of the input distribution. INSTA-BNN is designed to adjustthe threshold dynamically considering various information, includinghigher-order statistics, but it is also optimized judiciously to realizeminimal overhead on a real device. Our extensive study shows that INSTA-BNNoutperforms the baseline by 3.0% and 2.8% on the ImageNet classification taskwith comparable computing cost, achieving 68.5% and 72.2% top-1 accuracy onResNet-18 and MobileNetV1 based models, respectively.</description><author>Changhun Lee, Hyungjun Kim, Eunhyeok Park, Jae-Joon Kim</author><pubDate>Thu, 19 Oct 2023 16:26:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.07439v3</guid></item><item><title>OceanGPT: A Large Language Model for Ocean Science Tasks</title><link>http://arxiv.org/abs/2310.02031v3</link><description>Ocean science, which delves into the oceans that are reservoirs of life andbiodiversity, is of great significance given that oceans cover over 70% of ourplanet's surface. Recently, advances in Large Language Models (LLMs) havetransformed the paradigm in science. Despite the success in other domains,current LLMs often fall short in catering to the needs of domain experts likeoceanographers, and the potential of LLMs for ocean science is under-explored.The intrinsic reason may be the immense and intricate nature of ocean data aswell as the necessity for higher granularity and richness in knowledge. Toalleviate these issues, we introduce OceanGPT, the first-ever LLM in the oceandomain, which is expert in various ocean science tasks. We propose DoInstruct,a novel framework to automatically obtain a large volume of ocean domaininstruction data, which generates instructions based on multi-agentcollaboration. Additionally, we construct the first oceanography benchmark,OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Thoughcomprehensive experiments, OceanGPT not only shows a higher level of knowledgeexpertise for oceans science tasks but also gains preliminary embodiedintelligence capabilities in ocean technology. Codes, data and checkpoints willsoon be available at https://github.com/zjunlp/KnowLM.</description><author>Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, Huajun Chen</author><pubDate>Thu, 19 Oct 2023 16:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02031v3</guid></item><item><title>Fairness in Streaming Submodular Maximization over a Matroid Constraint</title><link>http://arxiv.org/abs/2305.15118v2</link><description>Streaming submodular maximization is a natural model for the task ofselecting a representative subset from a large-scale dataset. If datapointshave sensitive attributes such as gender or race, it becomes important toenforce fairness to avoid bias and discrimination. This has spurred significantinterest in developing fair machine learning algorithms. Recently, suchalgorithms have been developed for monotone submodular maximization under acardinality constraint. In this paper, we study the natural generalization of this problem to amatroid constraint. We give streaming algorithms as well as impossibilityresults that provide trade-offs between efficiency, quality and fairness. Wevalidate our findings empirically on a range of well-known real-worldapplications: exemplar-based clustering, movie recommendation, and maximumcoverage in social networks.</description><author>Marwa El Halabi, Federico Fusco, Ashkan Norouzi-Fard, Jakab Tardos, Jakub Tarnawski</author><pubDate>Thu, 19 Oct 2023 16:22:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15118v2</guid></item><item><title>Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter</title><link>http://arxiv.org/abs/2303.14090v2</link><description>Physics-informed neural networks have emerged as a coherent framework forbuilding predictive models that combine statistical patterns with domainknowledge. The underlying notion is to enrich the optimization loss functionwith known relationships to constrain the space of possible solutions.Hydrodynamic simulations are a core constituent of modern cosmology, while therequired computations are both expensive and time-consuming. At the same time,the comparatively fast simulation of dark matter requires fewer resources,which has led to the emergence of machine learning algorithms for baryoninpainting as an active area of research; here, recreating the scatter found inhydrodynamic simulations is an ongoing challenge. This paper presents the firstapplication of physics-informed neural networks to baryon inpainting bycombining advances in neural network architectures with physical constraints,injecting theory on baryon conversion efficiency into the model loss function.We also introduce a punitive prediction comparison based on theKullback-Leibler divergence, which enforces scatter reproduction. Bysimultaneously extracting the complete set of baryonic properties for the Simbasuite of cosmological simulations, our results demonstrate improved accuracy ofbaryonic predictions based on dark matter halo properties, successful recoveryof the fundamental metallicity relation, and retrieve scatter that traces thetarget simulation's distribution.</description><author>Zhenyu Dai, Ben Moews, Ricardo Vilalta, Romeel Dave</author><pubDate>Thu, 19 Oct 2023 16:20:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14090v2</guid></item><item><title>AgentTuning: Enabling Generalized Agent Abilities for LLMs</title><link>http://arxiv.org/abs/2310.12823v1</link><description>Open large language models (LLMs) with great performance in various taskshave significantly advanced the development of LLMs. However, they are farinferior to commercial models such as ChatGPT and GPT-4 when acting as agentsto tackle complex tasks in the real world. These agent tasks employ LLMs as thecentral controller responsible for planning, memorization, and toolutilization, necessitating both fine-grained prompting methods and robust LLMsto achieve satisfactory performance. Though many prompting methods have beenproposed to complete particular agent tasks, there is lack of research focusingon improving the agent capabilities of LLMs themselves without compromisingtheir general abilities. In this work, we present AgentTuning, a simple andgeneral method to enhance the agent abilities of LLMs while maintaining theirgeneral LLM capabilities. We construct AgentInstruct, a lightweightinstruction-tuning dataset containing high-quality interaction trajectories. Weemploy a hybrid instruction-tuning strategy by combining AgentInstruct withopen-source instructions from general domains. AgentTuning is used toinstruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations showthat AgentTuning enables LLMs' agent capabilities without compromising generalabilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agenttasks, demonstrating generalized agent capabilities. We open source theAgentInstruct and AgentLM-7B, 13B, and 70B models athttps://github.com/THUDM/AgentTuning , serving open and powerful alternativesto commercial LLMs for agent tasks.</description><author>Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang</author><pubDate>Thu, 19 Oct 2023 16:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12823v1</guid></item><item><title>The Power of Populations in Decentralized Learning Dynamics</title><link>http://arxiv.org/abs/2306.08670v2</link><description>We study a distributed multi-armed bandit setting among a population of $n$memory-constrained nodes in the gossip model: at each round, every node locallyadopts one of $m$ arms, observes a reward drawn from the arm's (adversariallychosen) distribution, and then communicates with a randomly sampled neighbor,exchanging information to determine its policy in the next round. We introduceand analyze several families of dynamics for this task that are decentralized:each node's decision is entirely local and depends only on its most recentlyobtained reward and that of the neighbor it sampled. We show a connectionbetween the global evolution of these decentralized dynamics with a certainclass of "zero-sum" multiplicative weights update algorithms, and we develop ageneral framework for analyzing the population-level regret of these naturalprotocols. Using this framework, we derive sublinear regret bounds under a widerange of parameter regimes (i.e., the size of the population and number ofarms) for both the stationary reward setting (where the mean of each arm'sdistribution is fixed over time) and the adversarial reward setting (wheremeans can vary over time). Further, we show that these protocols canapproximately optimize convex functions over the simplex when the rewarddistributions are generated from a stochastic gradient oracle.</description><author>John Lazarsfeld, Dan Alistarh</author><pubDate>Thu, 19 Oct 2023 16:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08670v2</guid></item><item><title>Generating collective counterfactual explanations in score-based classification via mathematical optimization</title><link>http://arxiv.org/abs/2310.12822v1</link><description>Due to the increasing use of Machine Learning models in high stakes decisionmaking settings, it has become increasingly important to have tools tounderstand how models arrive at decisions. Assuming a trained SupervisedClassification model, explanations can be obtained via counterfactual analysis:a counterfactual explanation of an instance indicates how this instance shouldbe minimally modified so that the perturbed instance is classified in thedesired class by the Machine Learning classification model. Most of theCounterfactual Analysis literature focuses on the single-instancesingle-counterfactual setting, in which the analysis is done for one singleinstance to provide one single explanation. Taking a stakeholder's perspective,in this paper we introduce the so-called collective counterfactualexplanations. By means of novel Mathematical Optimization models, we provide acounterfactual explanation for each instance in a group of interest, so thatthe total cost of the perturbations is minimized under some linkingconstraints. Making the process of constructing counterfactuals collectiveinstead of individual enables us to detect the features that are critical tothe entire dataset to have the individuals classified in the desired class. Ourmethodology allows for some instances to be treated individually, performingthe collective counterfactual analysis for a fraction of records of the groupof interest. This way, outliers are identified and handled appropriately. Undersome assumptions on the classifier and the space in which counterfactuals aresought, finding collective counterfactuals is reduced to solving a convexquadratic linearly constrained mixed integer optimization problem, which, fordatasets of moderate size, can be solved to optimality using existing solvers.The performance of our approach is illustrated on real-world datasets,demonstrating its usefulness.</description><author>Emilio Carrizosa, Jasone Ramírez-Ayerbe, Dolores Romero Morales</author><pubDate>Thu, 19 Oct 2023 16:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12822v1</guid></item><item><title>GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents</title><link>http://arxiv.org/abs/2310.12821v1</link><description>Current gesture recognition systems primarily focus on identifying gestureswithin a predefined set, leaving a gap in connecting these gestures tointeractive GUI elements or system functions (e.g., linking a 'thumb-up'gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gestureunderstanding and grounding framework leveraging large language models (LLMs).Gesture descriptions are formulated based on hand landmark coordinates fromgesture videos and fed into our dual-agent dialogue system. A gesture agentdeciphers these descriptions and queries about the interaction context (e.g.,interface, history, gaze data), which a context agent organizes and provides.Following iterative exchanges, the gesture agent discerns user intent,grounding it to an interactive function. We validated the gesture descriptionmodule using public first-view and third-view gesture datasets and tested thewhole system in two real-world settings: video streaming and smart home IoTcontrol. The highest zero-shot Top-5 grounding accuracies are 80.11% for videostreaming and 90.78% for smart home tasks, showing potential of the new gestureunderstanding paradigm.</description><author>Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, Yiqiang Chen</author><pubDate>Thu, 19 Oct 2023 16:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12821v1</guid></item><item><title>Hybrid Search for Efficient Planning with Completeness Guarantees</title><link>http://arxiv.org/abs/2310.12819v1</link><description>Solving complex planning problems has been a long-standing challenge incomputer science. Learning-based subgoal search methods have shown promise intackling these problems, but they often suffer from a lack of completenessguarantees, meaning that they may fail to find a solution even if one exists.In this paper, we propose an efficient approach to augment a subgoal searchmethod to achieve completeness in discrete action spaces. Specifically, weaugment the high-level search with low-level actions to execute a multi-level(hybrid) search, which we call complete subgoal search. This solution achievesthe best of both worlds: the practical efficiency of high-level search and thecompleteness of low-level search. We apply the proposed search method to arecently proposed subgoal search algorithm and evaluate the algorithm trainedon offline data on complex planning problems. We demonstrate that our completesubgoal search not only guarantees completeness but can even improveperformance in terms of search expansions for instances that the high-levelcould solve without low-level augmentations. Our approach makes it possible toapply subgoal-level planning for systems where completeness is a criticalrequirement.</description><author>Kalle Kujanpää, Joni Pajarinen, Alexander Ilin</author><pubDate>Thu, 19 Oct 2023 16:16:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12819v1</guid></item><item><title>Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models</title><link>http://arxiv.org/abs/2310.12818v1</link><description>Parameter-shared pre-trained language models (PLMs) have emerged as asuccessful approach in resource-constrained environments, enabling substantialreductions in model storage and memory costs without significant performancecompromise. However, it is important to note that parameter sharing does notalleviate computational burdens associated with inference, thus impeding itspracticality in situations characterized by limited stringent latencyrequirements or computational resources. Building upon neural ordinarydifferential equations (ODEs), we introduce a straightforward technique toenhance the inference efficiency of parameter-shared PLMs. Additionally, wepropose a simple pre-training technique that leads to fully or partially sharedmodels capable of achieving even greater inference acceleration. Theexperimental results demonstrate the effectiveness of our methods on bothautoregressive and autoencoding PLMs, providing novel insights into moreefficient utilization of parameter-shared models in resource-constrainedsettings.</description><author>Weize Chen, Xiaoyue Xu, Xu Han, Yankai Lin, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou</author><pubDate>Thu, 19 Oct 2023 16:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12818v1</guid></item><item><title>2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision</title><link>http://arxiv.org/abs/2310.12817v1</link><description>We present a Multimodal Interlaced Transformer (MIT) that jointly considers2D and 3D data for weakly supervised point cloud segmentation. Research studieshave shown that 2D and 3D features are complementary for point cloudsegmentation. However, existing methods require extra 2D annotations to achieve2D-3D information fusion. Considering the high annotation cost of point clouds,effective 2D and 3D feature fusion based on weakly supervised learning is ingreat demand. To this end, we propose a transformer model with two encoders andone decoder for weakly supervised point cloud segmentation using onlyscene-level class tags. Specifically, the two encoders compute theself-attended features for 3D point clouds and 2D multi-view images,respectively. The decoder implements interlaced 2D-3D cross-attention andcarries out implicit 2D and 3D feature fusion. We alternately switch the rolesof queries and key-value pairs in the decoder layers. It turns out that the 2Dand 3D features are iteratively enriched by each other. Experiments show thatit performs favorably against existing weakly supervised point cloudsegmentation methods by a large margin on the S3DIS and ScanNet benchmarks. Theproject page will be available at https://jimmy15923.github.io/mit_web/.</description><author>Cheng-Kun Yang, Min-Hung Chen, Yung-Yu Chuang, Yen-Yu Lin</author><pubDate>Thu, 19 Oct 2023 16:12:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12817v1</guid></item><item><title>Prompt Injection Attacks and Defenses in LLM-Integrated Applications</title><link>http://arxiv.org/abs/2310.12815v1</link><description>Large Language Models (LLMs) are increasingly deployed as the backend for avariety of real-world applications called LLM-Integrated Applications. Multiplerecent works showed that LLM-Integrated Applications are vulnerable to promptinjection attacks, in which an attacker injects malicious instruction/data intothe input of those applications such that they produce results as the attackerdesires. However, existing works are limited to case studies. As a result, theliterature lacks a systematic understanding of prompt injection attacks andtheir defenses. We aim to bridge the gap in this work. In particular, wepropose a general framework to formalize prompt injection attacks. Existingattacks, which are discussed in research papers and blog posts, are specialcases in our framework. Our framework enables us to design a new attack bycombining existing attacks. Moreover, we also propose a framework tosystematize defenses against prompt injection attacks. Using our frameworks, weconduct a systematic evaluation on prompt injection attacks and their defenseswith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research inthis field. Our code is available athttps://github.com/liu00222/Open-Prompt-Injection.</description><author>Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong</author><pubDate>Thu, 19 Oct 2023 16:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12815v1</guid></item><item><title>PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques</title><link>http://arxiv.org/abs/2304.12410v2</link><description>Recent parameter-efficient finetuning (PEFT) techniques aim to improve overthe considerable cost of fully finetuning large pretrained language models(PLM). As different PEFT techniques proliferate, it is becoming difficult tocompare them, in particular in terms of (i) the structure and functionalitythey add to the PLM, (ii) the different types and degrees of efficiencyimprovements achieved, (iii) performance at different downstream tasks, and(iv) how differences in structure and functionality relate to efficiency andtask performance. To facilitate such comparisons, this paper presents areference architecture which standardises aspects shared by different PEFTtechniques, while isolating differences to specific locations and interactionswith the standard components. Through this process of standardising andisolating differences, a modular view of PEFT techniques emerges, supportingnot only direct comparison of different techniques and their efficiency andtask performance, but also systematic exploration of reusability andcomposability of the different types of finetuned modules. We demonstrate howthe reference architecture can be applied to understand properties and relativeadvantages of PEFT techniques, hence to inform selection of techniques forspecific tasks, and design choices for new PEFT techniques.</description><author>Mohammed Sabry, Anya Belz</author><pubDate>Thu, 19 Oct 2023 16:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12410v2</guid></item><item><title>Hierarchical Forecasting at Scale</title><link>http://arxiv.org/abs/2310.12809v1</link><description>Existing hierarchical forecasting techniques scale poorly when the number oftime series increases. We propose to learn a coherent forecast for millions oftime series with a single bottom-level forecast model by using a sparse lossfunction that directly optimizes the hierarchical product and/or temporalstructure. The benefit of our sparse hierarchical loss function is that itprovides practitioners a method of producing bottom-level forecasts that arecoherent to any chosen cross-sectional or temporal hierarchy. In addition,removing the need for a post-processing step as required in traditionalhierarchical forecasting techniques reduces the computational cost of theprediction phase in the forecasting pipeline. On the public M5 dataset, oursparse hierarchical loss function performs up to 10% (RMSE) better compared tothe baseline loss function. We implement our sparse hierarchical loss functionwithin an existing forecasting model at bol, a large European e-commerceplatform, resulting in an improved forecasting performance of 2% at the productlevel. Finally, we found an increase in forecasting performance of about 5-10%when evaluating the forecasting performance across the cross-sectionalhierarchies that we defined. These results demonstrate the usefulness of oursparse hierarchical loss applied to a production forecasting system at a majore-commerce platform.</description><author>Olivier Sprangers, Wander Wadman, Sebastian Schelter, Maarten de Rijke</author><pubDate>Thu, 19 Oct 2023 16:06:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12809v1</guid></item><item><title>Multi-label Node Classification On Graph-Structured Data</title><link>http://arxiv.org/abs/2304.10398v3</link><description>Graph Neural Networks (GNNs) have shown state-of-the-art improvements in nodeclassification tasks on graphs. While these improvements have been largelydemonstrated in a multi-class classification scenario, a more general andrealistic scenario in which each node could have multiple labels has so farreceived little attention. The first challenge in conducting focused studies onmulti-label node classification is the limited number of publicly availablemulti-label graph datasets. Therefore, as our first contribution, we collectand release three real-world biological datasets and develop a multi-labelgraph generator to generate datasets with tunable properties. While high labelsimilarity (high homophily) is usually attributed to the success of GNNs, weargue that a multi-label scenario does not follow the usual semantics ofhomophily and heterophily so far defined for a multi-class scenario. As oursecond contribution, we define homophily and Cross-Class NeighborhoodSimilarity for the multi-label scenario and provide a thorough analyses of thecollected $9$ multi-label datasets. Finally, we perform a large-scalecomparative study with $8$ methods and $9$ datasets and analyse theperformances of the methods to assess the progress made by current state of theart in the multi-label node classification scenario. We release our benchmarkat https://github.com/Tianqi-py/MLGNC.</description><author>Tianqi Zhao, Ngan Thi Dong, Alan Hanjalic, Megha Khosla</author><pubDate>Thu, 19 Oct 2023 16:03:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10398v3</guid></item><item><title>Model Merging by Uncertainty-Based Gradient Matching</title><link>http://arxiv.org/abs/2310.12808v1</link><description>Models trained on different datasets can be merged by a weighted-averaging oftheir parameters, but why does it work and when can it fail? Here, we connectthe inaccuracy of weighted-averaging to mismatches in the gradients and proposea new uncertainty-based scheme to improve the performance by reducing themismatch. The connection also reveals implicit assumptions in other schemessuch as averaging, task arithmetic, and Fisher-weighted averaging. Our newmethod gives consistent improvements for large language models and visiontransformers, both in terms of performance and robustness to hyperparameters.</description><author>Nico Daheim, Thomas Möllenhoff, Edoardo Maria Ponti, Iryna Gurevych, Mohammad Emtiyaz Khan</author><pubDate>Thu, 19 Oct 2023 16:02:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12808v1</guid></item><item><title>DCSI -- An improved measure of cluster separability based on separation and connectedness</title><link>http://arxiv.org/abs/2310.12806v1</link><description>Whether class labels in a given data set correspond to meaningful clusters iscrucial for the evaluation of clustering algorithms using real-world data sets.This property can be quantified by separability measures. A review of theexisting literature shows that neither classification-based complexity measuresnor cluster validity indices (CVIs) adequately incorporate the central aspectsof separability for density-based clustering: between-class separation andwithin-class connectedness. A newly developed measure (density clusterseparability index, DCSI) aims to quantify these two characteristics and canalso be used as a CVI. Extensive experiments on synthetic data indicate thatDCSI correlates strongly with the performance of DBSCAN measured via theadjusted rand index (ARI) but lacks robustness when it comes to multi-classdata sets with overlapping classes that are ill-suited for density-based hardclustering. Detailed evaluation on frequently used real-world data sets showsthat DCSI can correctly identify touching or overlapping classes that do notform meaningful clusters.</description><author>Jana Gauss, Fabian Scheipl, Moritz Herrmann</author><pubDate>Thu, 19 Oct 2023 16:01:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12806v1</guid></item><item><title>Detection and Evaluation of bias-inducing Features in Machine learning</title><link>http://arxiv.org/abs/2310.12805v1</link><description>The cause-to-effect analysis can help us decompose all the likely causes of aproblem, such as an undesirable business situation or unintended harm to theindividual(s). This implies that we can identify how the problems areinherited, rank the causes to help prioritize fixes, simplify a complex problemand visualize them. In the context of machine learning (ML), one can usecause-to-effect analysis to understand the reason for the biased behavior ofthe system. For example, we can examine the root causes of biases by checkingeach feature for a potential cause of bias in the model. To approach this, onecan apply small changes to a given feature or a pair of features in the data,following some guidelines and observing how it impacts the decision made by themodel (i.e., model prediction). Therefore, we can use cause-to-effect analysisto identify the potential bias-inducing features, even when these features areoriginally are unknown. This is important since most current methods require apre-identification of sensitive features for bias assessment and can actuallymiss other relevant bias-inducing features, which is why systematicidentification of such features is necessary. Moreover, it often occurs that toachieve an equitable outcome, one has to take into account sensitive featuresin the model decision. Therefore, it should be up to the domain experts todecide based on their knowledge of the context of a decision whether biasinduced by specific features is acceptable or not. In this study, we propose anapproach for systematically identifying all bias-inducing features of a modelto help support the decision-making of domain experts. We evaluated ourtechnique using four well-known datasets to showcase how our contribution canhelp spearhead the standard procedure when developing, testing, maintaining,and deploying fair/equitable machine learning systems.</description><author>Moses Openja, Gabriel Laberge, Foutse Khomh</author><pubDate>Thu, 19 Oct 2023 16:01:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12805v1</guid></item><item><title>Differentiable Vertex Fitting for Jet Flavour Tagging</title><link>http://arxiv.org/abs/2310.12804v1</link><description>We propose a differentiable vertex fitting algorithm that can be used forsecondary vertex fitting, and that can be seamlessly integrated into neuralnetworks for jet flavour tagging. Vertex fitting is formulated as anoptimization problem where gradients of the optimized solution vertex aredefined through implicit differentiation and can be passed to upstream ordownstream neural network components for network training. More broadly, thisis an application of differentiable programming to integrate physics knowledgeinto neural network models in high energy physics. We demonstrate howdifferentiable secondary vertex fitting can be integrated into largertransformer-based models for flavour tagging and improve heavy flavour jetclassification.</description><author>Rachel E. C. Smith, Inês Ochoa, Rúben Inácio, Jonathan Shoemaker, Michael Kagan</author><pubDate>Thu, 19 Oct 2023 16:01:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12804v1</guid></item><item><title>Causal-structure Driven Augmentations for Text OOD Generalization</title><link>http://arxiv.org/abs/2310.12803v1</link><description>The reliance of text classifiers on spurious correlations can lead to poorgeneralization at deployment, raising concerns about their use insafety-critical domains such as healthcare. In this work, we propose to usecounterfactual data augmentation, guided by knowledge of the causal structureof the data, to simulate interventions on spurious features and to learn morerobust text classifiers. We show that this strategy is appropriate inprediction problems where the label is spuriously correlated with an attribute.Under the assumptions of such problems, we discuss the favorable samplecomplexity of counterfactual data augmentation, compared to importancere-weighting. Pragmatically, we match examples using auxiliary data, based ondiff-in-diff methodology, and use a large language model (LLM) to represent aconditional probability of text. Through extensive experimentation on learningcaregiver-invariant predictors of clinical diagnoses from medical narrativesand on semi-synthetic data, we demonstrate that our method for simulatinginterventions improves out-of-distribution (OOD) accuracy compared to baselineinvariant learning algorithms.</description><author>Amir Feder, Yoav Wald, Claudia Shi, Suchi Saria, David Blei</author><pubDate>Thu, 19 Oct 2023 15:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12803v1</guid></item><item><title>Bayesian tomography using polynomial chaos expansion and deep generative networks</title><link>http://arxiv.org/abs/2307.04228v4</link><description>Implementations of Markov chain Monte Carlo (MCMC) methods need to confronttwo fundamental challenges: accurate representation of prior information andefficient evaluation of likelihoods. Principal component analysis (PCA) andrelated techniques can in some cases facilitate the definition and sampling ofthe prior distribution, as well as the training of accurate surrogate models,using for instance, polynomial chaos expansion (PCE). However, complexgeological priors with sharp contrasts necessitate more complexdimensionality-reduction techniques, such as, deep generative models (DGMs). Bysampling a low-dimensional prior probability distribution defined in thelow-dimensional latent space of such a model, it becomes possible toefficiently sample the physical domain at the price of a generator that istypically highly non-linear. Training a surrogate that is capable of capturingintricate non-linear relationships between latent parameters and outputs offorward modeling presents a notable challenge. Indeed, while PCE models providehigh accuracy when the input-output relationship can be effectivelyapproximated by relatively low-degree multivariate polynomials, this conditionis typically not met when employing latent variables derived from DGMs. In thiscontribution, we present a strategy combining the excellent reconstructionperformances of a variational autoencoder (VAE) with the accuracy of PCA-PCEsurrogate modeling in the context of Bayesian ground penetrating radar (GPR)traveltime tomography. Within the MCMC process, the parametrization of the VAEis leveraged for prior exploration and sample proposals. Concurrently,surrogate modeling is conducted using PCE, which operates on either globally orlocally defined principal components of the VAE samples under examination.</description><author>Giovanni Angelo Meles, Macarena Amaya, Shiran Levy, Stefano Marelli, Niklas Linde</author><pubDate>Thu, 19 Oct 2023 15:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04228v4</guid></item><item><title>An effective theory of collective deep learning</title><link>http://arxiv.org/abs/2310.12802v1</link><description>Unraveling the emergence of collective learning in systems of coupledartificial neural networks is an endeavor with broader implications forphysics, machine learning, neuroscience and society. Here we introduce aminimal model that condenses several recent decentralized algorithms byconsidering a competition between two terms: the local learning dynamics in theparameters of each neural network unit, and a diffusive coupling among unitsthat tends to homogenize the parameters of the ensemble. We derive thecoarse-grained behavior of our model via an effective theory for linearnetworks that we show is analogous to a deformed Ginzburg-Landau model withquenched disorder. This framework predicts (depth-dependent)disorder-order-disorder phase transitions in the parameters' solutions thatreveal the onset of a collective learning phase, along with a depth-induceddelay of the critical point and a robust shape of the microscopic learningpath. We validate our theory in realistic ensembles of coupled nonlinearnetworks trained in the MNIST dataset under privacy constraints. Interestingly,experiments confirm that individual networks -- trained only with private data-- can fully generalize to unseen data classes when the collective learningphase emerges. Our work elucidates the physics of collective learning andcontributes to the mechanistic interpretability of deep learning indecentralized settings.</description><author>Lluís Arola-Fernández, Lucas Lacasa</author><pubDate>Thu, 19 Oct 2023 15:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12802v1</guid></item><item><title>Exploring Graph Neural Networks for Indian Legal Judgment Prediction</title><link>http://arxiv.org/abs/2310.12800v1</link><description>The burdensome impact of a skewed judges-to-cases ratio on the judicialsystem manifests in an overwhelming backlog of pending cases alongside anongoing influx of new ones. To tackle this issue and expedite the judicialprocess, the proposition of an automated system capable of suggesting caseoutcomes based on factual evidence and precedent from past cases gainssignificance. This research paper centres on developing a graph neuralnetwork-based model to address the Legal Judgment Prediction (LJP) problem,recognizing the intrinsic graph structure of judicial cases and making it abinary node classification problem. We explored various embeddings as modelfeatures, while nodes such as time nodes and judicial acts were added andpruned to evaluate the model's performance. The study is done while consideringthe ethical dimension of fairness in these predictions, considering gender andname biases. A link prediction task is also conducted to assess the model'sproficiency in anticipating connections between two specified nodes. Byharnessing the capabilities of graph neural networks and incorporating fairnessanalyses, this research aims to contribute insights towards streamlining theadjudication process, enhancing judicial efficiency, and fostering a moreequitable legal landscape, ultimately alleviating the strain imposed bymounting case backlogs. Our best-performing model with XLNet pre-trainedembeddings as its features gives the macro F1 score of 75% for the LJP task.For link prediction, the same set of features is the best performing giving ROCof more than 80%</description><author>Mann Khatri, Mirza Yusuf, Yaman Kumar, Rajiv Ratn Shah, Ponnurangam Kumaraguru</author><pubDate>Thu, 19 Oct 2023 15:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12800v1</guid></item><item><title>MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter</title><link>http://arxiv.org/abs/2310.12798v1</link><description>Language Models (LMs) have demonstrated impressive molecule understandingability on various 1D text-related tasks. However, they inherently lack 2Dgraph perception - a critical ability of human professionals in comprehendingmolecules' topological structures. To bridge this gap, we propose MolCA:Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-ModalAdapter. MolCA enables an LM (e.g., Galactica) to understand both text- andgraph-based molecular contents via the cross-modal projector. Specifically, thecross-modal projector is implemented as a Q-Former to connect a graph encoder'srepresentation space and an LM's text space. Further, MolCA employs a uni-modaladapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks.Unlike previous studies that couple an LM with a graph encoder via cross-modalcontrastive learning, MolCA retains the LM's ability of open-ended textgeneration and augments it with 2D graph information. To showcase itseffectiveness, we extensively benchmark MolCA on tasks of molecule captioning,IUPAC name prediction, and molecule-text retrieval, on which MolCAsignificantly outperforms the baselines. Our codes and checkpoints can be foundat https://github.com/acharkq/MolCA.</description><author>Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua</author><pubDate>Thu, 19 Oct 2023 15:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12798v1</guid></item><item><title>Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization</title><link>http://arxiv.org/abs/2310.12794v1</link><description>Large language models (LLMs) have exhibited considerable cross-lingualgeneralization abilities, whereby they implicitly transfer knowledge acrosslanguages. However, the transfer is not equally successful for all languages,especially for low-resource ones, which poses an ongoing challenge. It isunclear whether we have reached the limits of implicit cross-lingualgeneralization and if explicit knowledge transfer is viable. In this paper, weinvestigate the potential for explicitly aligning conceptual correspondencebetween languages to enhance cross-lingual generalization. Using the syntacticaspect of language as a testbed, our analyses of 43 languages reveal a highdegree of alignability among the spaces of structural concepts within eachlanguage for both encoder-only and decoder-only LLMs. We then propose ameta-learning-based method to learn to align conceptual spaces of differentlanguages, which facilitates zero-shot and few-shot generalization in conceptclassification and also offers insights into the cross-lingual in-contextlearning phenomenon. Experiments on syntactic analysis tasks show that ourapproach achieves competitive results with state-of-the-art methods and narrowsthe performance gap between languages, particularly benefiting those withlimited resources.</description><author>Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang</author><pubDate>Thu, 19 Oct 2023 15:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12794v1</guid></item><item><title>OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift</title><link>http://arxiv.org/abs/2310.12793v1</link><description>Existing works have made great progress in improving adversarial robustness,but typically test their method only on data from the same distribution as thetraining data, i.e. in-distribution (ID) testing. As a result, it is unclearhow such robustness generalizes under input distribution shifts, i.e.out-of-distribution (OOD) testing. This is a concerning omission as suchdistribution shifts are unavoidable when methods are deployed in the wild. Toaddress this issue we propose a benchmark named OODRobustBench tocomprehensively assess OOD adversarial robustness using 23 dataset-wise shifts(i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts(i.e., unforeseen adversarial threat models). OODRobustBench is used to assess706 robust models using 60.7K adversarial evaluations. This large-scaleanalysis shows that: 1) adversarial robustness suffers from a severe OODgeneralization issue; 2) ID robustness correlates strongly with OOD robustness,in a positive linear way, under many distribution shifts. The latter enablesthe prediction of OOD robustness from ID robustness. Based on this, we are ableto predict the upper limit of OOD robustness for existing robust trainingschemes. The results suggest that achieving OOD robustness requires designingnovel methods beyond the conventional ones. Last, we discover that extra data,data augmentation, advanced model architectures and particular regularizationapproaches can improve OOD robustness. Noticeably, the discovered trainingschemes, compared to the baseline, exhibit dramatically higher robustness underthreat shift while keeping high ID robustness, demonstrating new promisingsolutions for robustness against both multi-attack and unforeseen attacks.</description><author>Lin Li, Yifei Wang, Chawin Sitawarin, Michael Spratling</author><pubDate>Thu, 19 Oct 2023 15:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12793v1</guid></item><item><title>Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection</title><link>http://arxiv.org/abs/2310.12790v1</link><description>Open-set supervised anomaly detection (OSAD) - a recently emerging anomalydetection area - aims at utilizing a few samples of anomaly classes seen duringtraining to detect unseen anomalies (i.e., samples from open-set anomalyclasses), while effectively identifying the seen anomalies. Benefiting from theprior knowledge illustrated by the seen anomalies, current OSAD methods canoften largely reduce false positive errors. However, these methods treat theanomaly examples as from a homogeneous distribution, rendering them lesseffective in generalizing to unseen anomalies that can be drawn from anydistribution. In this paper, we propose to learn heterogeneous anomalydistributions using the limited anomaly examples to address this issue. To thisend, we introduce a novel approach, namely Anomaly Heterogeneity Learning(AHL), that simulates a diverse set of heterogeneous (seen and unseen) anomalydistributions and then utilizes them to learn a unified heterogeneousabnormality model. Further, AHL is a generic framework that existing OSADmodels can plug and play for enhancing their abnormality modeling. Extensiveexperiments on nine real-world anomaly detection datasets show that AHL can 1)substantially enhance different state-of-the-art (SOTA) OSAD models indetecting both seen and unseen anomalies, achieving new SOTA performance on alarge set of datasets, and 2) effectively generalize to unseen anomalies in newtarget domains.</description><author>Jiawen Zhu, Choubo Ding, Yu Tian, Guansong Pang</author><pubDate>Thu, 19 Oct 2023 15:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12790v1</guid></item></channel></rss>