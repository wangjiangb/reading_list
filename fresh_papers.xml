<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 19 Sep 2023 06:00:17 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>General In-Hand Object Rotation with Vision and Touch</title><link>http://arxiv.org/abs/2309.09979v1</link><description>We introduce RotateIt, a system that enables fingertip-based object rotationalong multiple axes by leveraging multimodal sensory inputs. Our system istrained in simulation, where it has access to ground-truth object shapes andphysical properties. Then we distill it to operate on realistic yet noisysimulated visuotactile and proprioceptive sensory inputs. These multimodalinputs are fused via a visuotactile transformer, enabling online inference ofobject shapes and physical properties during deployment. We show significantperformance improvements over prior methods and the importance of visual andtactile sensing.</description><author>Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, Jitendra Malik</author><pubDate>Mon, 18 Sep 2023 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09979v1</guid></item><item><title>Image Hijacks: Adversarial Images can Control Generative Models at Runtime</title><link>http://arxiv.org/abs/2309.00236v2</link><description>Are foundation models secure from malicious actors? In this work, we focus onthe image input to a vision-language model (VLM). We discover image hijacks,adversarial images that control generative models at runtime. We introduceBehaviour Matching, a general method for creating image hijacks, and we use itto explore three types of attacks. Specific string attacks generate arbitraryoutput of the adversary's choice. Leak context attacks leak information fromthe context window into the output. Jailbreak attacks circumvent a model'ssafety training. We study these attacks against LLaVA, a state-of-the-art VLMbased on CLIP and LLaMA-2, and find that all our attack types have above a 90%success rate. Moreover, our attacks are automated and require only small imageperturbations. These findings raise serious concerns about the security offoundation models. If image hijacks are as difficult to defend against asadversarial examples in CIFAR-10, then it might be many years before a solutionis found -- if it even exists.</description><author>Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons</author><pubDate>Mon, 18 Sep 2023 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00236v2</guid></item><item><title>A Multi-Token Coordinate Descent Method for Semi-Decentralized Vertical Federated Learning</title><link>http://arxiv.org/abs/2309.09977v1</link><description>Communication efficiency is a major challenge in federated learning (FL). Inclient-server schemes, the server constitutes a bottleneck, and whiledecentralized setups spread communications, they do not necessarily reduce themdue to slower convergence. We propose Multi-Token Coordinate Descent (MTCD), acommunication-efficient algorithm for semi-decentralized vertical federatedlearning, exploiting both client-server and client-client communications wheneach client holds a small subset of features. Our multi-token method can beseen as a parallel Markov chain (block) coordinate descent algorithm and itsubsumes the client-server and decentralized setups as special cases. We obtaina convergence rate of $\mathcal{O}(1/T)$ for nonconvex objectives when tokensroam over disjoint subsets of clients and for convex objectives when they roamover possibly overlapping subsets. Numerical results show that MTCD improvesthe state-of-the-art communication efficiency and allows for a tunable amountof parallel communications.</description><author>Pedro Valdeira, Yuejie Chi, Cláudia Soares, João Xavier</author><pubDate>Mon, 18 Sep 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09977v1</guid></item><item><title>$Des$-$q$: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification</title><link>http://arxiv.org/abs/2309.09976v1</link><description>Decision trees are widely used in machine learning due to their simplicity inconstruction and interpretability. However, as data sizes grow, traditionalmethods for construction and retraining decision trees become increasinglyslow, scaling polynomially with the number of training examples. In this work,we introduce a novel quantum algorithm, named $Des$-$q$, for constructing andretraining decision trees in regression and binary classification tasks.Assuming the data stream produces small increments of new training examples, wedemonstrate that our $Des$-$q$ algorithm significantly reduces the timerequired for tree retraining, achieving a poly-logarithmic time complexity inthe number of training examples, even accounting for the time needed to loadthe new examples into quantum-accessible memory. Our approach involves buildinga decision tree algorithm to perform k-piecewise linear tree splits at eachinternal node. These splits simultaneously generate multiple hyperplanes,dividing the feature space into k distinct regions. To determine the k suitableanchor points for these splits, we develop an efficient quantum-supervisedclustering method, building upon the q-means algorithm of Kerenidis $et$ $al$.$Des$-$q$ first efficiently estimates each feature weight using a novel quantumtechnique to estimate the Pearson correlation. Subsequently, we employ weighteddistance estimation to cluster the training examples in k disjoint regions andthen proceed to expand the tree using the same procedure. We benchmark theperformance of the simulated version of our algorithm against thestate-of-the-art classical decision tree for regression and binaryclassification on multiple data sets with numerical features. Further, weshowcase that the proposed algorithm exhibits similar performance to thestate-of-the-art decision tree while significantly speeding up the periodictree retraining.</description><author>Niraj Kumar, Romina Yalovetzky, Changhao Li, Pierre Minnsen, Marco Pistoia</author><pubDate>Mon, 18 Sep 2023 18:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09976v1</guid></item><item><title>GEDepth: Ground Embedding for Monocular Depth Estimation</title><link>http://arxiv.org/abs/2309.09975v1</link><description>Monocular depth estimation is an ill-posed problem as the same 2D image canbe projected from infinite 3D scenes. Although the leading algorithms in thisfield have reported significant improvement, they are essentially geared to theparticular compound of pictorial observations and camera parameters (i.e.,intrinsics and extrinsics), strongly limiting their generalizability inreal-world scenarios. To cope with this challenge, this paper proposes a novelground embedding module to decouple camera parameters from pictorial cues, thuspromoting the generalization capability. Given camera parameters, the proposedmodule generates the ground depth, which is stacked with the input image andreferenced in the final depth prediction. A ground attention is designed in themodule to optimally combine ground depth with residual depth. Our groundembedding is highly flexible and lightweight, leading to a plug-in module thatis amenable to be integrated into various depth estimation networks.Experiments reveal that our approach achieves the state-of-the-art results onpopular benchmarks, and more importantly, renders significant generalizationimprovement on a wide range of cross-domain tests.</description><author>Xiaodong Yang, Zhuang Ma, Zhiyu Ji, Zhe Ren</author><pubDate>Mon, 18 Sep 2023 18:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09975v1</guid></item><item><title>MindAgent: Emergent Gaming Interaction</title><link>http://arxiv.org/abs/2309.09971v1</link><description>Large Language Models (LLMs) have the capacity of performing complexscheduling in a multi-agent system and can coordinate these agents intocompleting sophisticated tasks that require extensive collaboration. However,despite the introduction of numerous gaming frameworks, the community hasinsufficient benchmarks towards building general multi-agents collaborationinfrastructure that encompass both LLM and human-NPCs collaborations. In thiswork, we propose a novel infrastructure - MindAgent - to evaluate planning andcoordination emergent capabilities for gaming interaction. In particular, ourinfrastructure leverages existing gaming framework, to i) require understandingof the coordinator for a multi-agent system, ii) collaborate with human playersvia un-finetuned proper instructions, and iii) establish an in-context learningon few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a newgaming scenario and related benchmark that dispatch a multi-agent collaborationefficiency and supervise multiple agents playing the game simultaneously. Weconduct comprehensive evaluations with new auto-metric CoS for calculating thecollaboration efficiency. Finally, our infrastructure can be deployed intoreal-world gaming scenarios in a customized VR version of CUISINEWORLD andadapted in existing broader Minecraft gaming domain. We hope our findings onLLMs and the new infrastructure for general-purpose scheduling and coordinationcan help shed light on how such skills can be obtained by learning from largelanguage corpora.</description><author>Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao</author><pubDate>Mon, 18 Sep 2023 18:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09971v1</guid></item><item><title>Empirical Study of Mix-based Data Augmentation Methods in Physiological Time Series Data</title><link>http://arxiv.org/abs/2309.09970v1</link><description>Data augmentation is a common practice to help generalization in theprocedure of deep model training. In the context of physiological time seriesclassification, previous research has primarily focused on label-invariant dataaugmentation methods. However, another class of augmentation techniques(\textit{i.e., Mixup}) that emerged in the computer vision field has yet to befully explored in the time series domain. In this study, we systematicallyreview the mix-based augmentations, including mixup, cutmix, and manifoldmixup, on six physiological datasets, evaluating their performance acrossdifferent sensory data and classification tasks. Our results demonstrate thatthe three mix-based augmentations can consistently improve the performance onthe six datasets. More importantly, the improvement does not rely on expertknowledge or extensive parameter tuning. Lastly, we provide an overview of theunique properties of the mix-based augmentation methods and highlight thepotential benefits of using the mix-based augmentation in physiological timeseries data.</description><author>Peikun Guo, Huiyuan Yang, Akane Sano</author><pubDate>Mon, 18 Sep 2023 18:51:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09970v1</guid></item><item><title>Learning Human-Human Interactions in Images from Weak Textual Supervision</title><link>http://arxiv.org/abs/2304.14104v4</link><description>Interactions between humans are diverse and context-dependent, but previousworks have treated them as categorical, disregarding the heavy tail of possibleinteractions. We propose a new paradigm of learning human-human interactions asfree text from a single still image, allowing for flexibility in modeling theunlimited space of situations and relationships between people. To overcome theabsence of data labelled specifically for this task, we use knowledgedistillation applied to synthetic caption data produced by a large languagemodel without explicit supervision. We show that the pseudo-labels produced bythis procedure can be used to train a captioning model to effectivelyunderstand human-human interactions in images, as measured by a variety ofmetrics that measure textual and semantic faithfulness and factual groundednessof our predictions. We further show that our approach outperforms SOTA imagecaptioning and situation recognition models on this task. We will release ourcode and pseudo-labels along with Waldo and Wenda, a manually-curated test setfor still image human-human interaction understanding.</description><author>Morris Alper, Hadar Averbuch-Elor</author><pubDate>Mon, 18 Sep 2023 18:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14104v4</guid></item><item><title>Fast Feedforward Networks</title><link>http://arxiv.org/abs/2308.14711v2</link><description>We break the linear link between the layer size and its inference cost byintroducing the fast feedforward (FFF) architecture, a log-time alternative tofeedforward networks. We demonstrate that FFFs are up to 220x faster thanfeedforward networks, up to 6x faster than mixture-of-experts networks, andexhibit better training properties than mixtures of experts thanks to noiselessconditional execution. Pushing FFFs to the limit, we show that they can use aslittle as 1% of layer neurons for inference in vision transformers whilepreserving 94.2% of predictive performance.</description><author>Peter Belcak, Roger Wattenhofer</author><pubDate>Mon, 18 Sep 2023 18:50:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14711v2</guid></item><item><title>Prompt a Robot to Walk with Large Language Models</title><link>http://arxiv.org/abs/2309.09969v1</link><description>Large language models (LLMs) pre-trained on vast internet-scale data haveshowcased remarkable capabilities across diverse domains. Recently, there hasbeen escalating interest in deploying LLMs for robotics, aiming to harness thepower of foundation models in real-world settings. However, this approach facessignificant challenges, particularly in grounding these models in the physicalworld and in generating dynamic robot motions. To address these issues, weintroduce a novel paradigm in which we use few-shot prompts collected from thephysical environment, enabling the LLM to autoregressively generate low-levelcontrol commands for robots without task-specific fine-tuning. Experimentsacross various robots and environments validate that our method can effectivelyprompt a robot to walk. We thus illustrate how LLMs can proficiently functionas low-level feedback controllers for dynamic motion control even inhigh-dimensional robotic systems. The project website and source code can befound at: https://prompt2walk.github.io/ .</description><author>Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath</author><pubDate>Mon, 18 Sep 2023 18:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09969v1</guid></item><item><title>Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning</title><link>http://arxiv.org/abs/2309.07383v2</link><description>This paper studies convergence rates for some value function approximationsthat arise in a collection of reproducing kernel Hilbert spaces (RKHS)$H(\Omega)$. By casting an optimal control problem in a specific class ofnative spaces, strong rates of convergence are derived for the operatorequation that enables offline approximations that appear in policy iteration.Explicit upper bounds on error in value function approximations are derived interms of power function $\Pwr_{H,N}$ for the space of finite dimensionalapproximants $H_N$ in the native space $H(\Omega)$. These bounds are geometricin nature and refine some well-known, now classical results concerningconvergence of approximations of value functions.</description><author>Ali Bouland, Shengyuan Niu, Sai Tej Paruchuri, Andrew Kurdila, John Burns, Eugenio Schuster</author><pubDate>Mon, 18 Sep 2023 18:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07383v2</guid></item><item><title>Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees</title><link>http://arxiv.org/abs/2309.09968v1</link><description>Tabular data is hard to acquire and is subject to missing values. This paperproposes a novel approach to generate and impute mixed-type (continuous andcategorical) tabular data using score-based diffusion and conditional flowmatching. Contrary to previous work that relies on neural networks as functionapproximators, we instead utilize XGBoost, a popular Gradient-Boosted Tree(GBT) method. In addition to being elegant, we empirically show on variousdatasets that our method i) generates highly realistic synthetic data when thetraining dataset is either clean or tainted by missing data and ii) generatesdiverse plausible data imputations. Our method often outperforms deep-learninggeneration methods and can trained in parallel using CPUs without the need fora GPU. To make it easily accessible, we release our code through a Pythonlibrary on PyPI and an R package on CRAN.</description><author>Alexia Jolicoeur-Martineau, Kilian Fatras, Tal Kachman</author><pubDate>Mon, 18 Sep 2023 18:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09968v1</guid></item><item><title>Tumoral Angiogenic Optimizer: A new bio-inspired based metaheuristic</title><link>http://arxiv.org/abs/2309.05947v2</link><description>In this article, we propose a new metaheuristic inspired by the morphogeneticcellular movements of endothelial cells (ECs) that occur during the tumorangiogenesis process. This algorithm starts with a random initial population.In each iteration, the best candidate selected as the tumor, while the otherindividuals in the population are treated as ECs migrating toward the tumor'sdirection following a coordinated dynamics through a spatial relationshipbetween tip and follower ECs. This algorithm has an advantage compared to othersimilar optimization metaheuristics: the model parameters are alreadyconfigured according to the tumor angiogenesis phenomenon modeling, preventingresearchers from initializing them with arbitrary values. Subsequently, thealgorithm is compared against well-known benchmark functions, and the resultsare validated through a comparative study with Particle Swarm Optimization(PSO). The results demonstrate that the algorithm is capable of providinghighly competitive outcomes. Furthermore, the proposed algorithm is applied toreal-world problems (cantilever beam design, pressure vessel design,tension/compression spring and sustainable explotation renewable resource). Theresults showed that the proposed algorithm worked effectively in solvingconstrained optimization problems. The results obtained were compared withseveral known algorithms.</description><author>Hernández Rodríguez, Matías Ezequiel</author><pubDate>Mon, 18 Sep 2023 18:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05947v2</guid></item><item><title>An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models</title><link>http://arxiv.org/abs/2309.09958v1</link><description>Visual instruction tuning has recently shown encouraging progress withopen-source large multimodal models (LMM) such as LLaVA and MiniGPT-4. However,most existing studies of open-source LMM are performed using models with 13Bparameters or smaller. In this paper we present an empirical study of scalingLLaVA up to 33B and 65B/70B, and share our findings from our explorations inimage resolution, data mixing and parameter-efficient training methods such asLoRA/QLoRA. These are evaluated by their impact on the multi-modal and languagecapabilities when completing real-world tasks in the wild. We find that scaling LMM consistently enhances model performance and improveslanguage capabilities, and performance of LoRA/QLoRA tuning of LMM arecomparable to the performance of full-model fine-tuning. Additionally, thestudy highlights the importance of higher image resolutions and mixingmultimodal-language data to improve LMM performance, and visual instructiontuning can sometimes improve LMM's pure language capability. We hope that thisstudy makes state-of-the-art LMM research at a larger scale more accessible,thus helping establish stronger baselines for future research. Code andcheckpoints will be made public.</description><author>Yadong Lu, Chunyuan Li, Haotian Liu, Jianwei Yang, Jianfeng Gao, Yelong Shen</author><pubDate>Mon, 18 Sep 2023 18:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09958v1</guid></item><item><title>RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color Editing of 3D Scenes</title><link>http://arxiv.org/abs/2301.07958v3</link><description>Radiance fields have gradually become a main representation of media.Although its appearance editing has been studied, how to achieveview-consistent recoloring in an efficient manner is still under explored. Wepresent RecolorNeRF, a novel user-friendly color editing approach for theneural radiance fields. Our key idea is to decompose the scene into a set ofpure-colored layers, forming a palette. By this means, color manipulation canbe conducted by altering the color components of the palette directly. Tosupport efficient palette-based editing, the color of each layer needs to be asrepresentative as possible. In the end, the problem is formulated as anoptimization problem, where the layers and their blending weights are jointlyoptimized with the NeRF itself. Extensive experiments show that ourjointly-optimized layer decomposition can be used against multiple backbonesand produce photo-realistic recolored novel-view renderings. We demonstratethat RecolorNeRF outperforms baseline methods both quantitatively andqualitatively for color editing even in complex real-world scenes.</description><author>Bingchen Gong, Yuehao Wang, Xiaoguang Han, Qi Dou</author><pubDate>Mon, 18 Sep 2023 18:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07958v3</guid></item><item><title>vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems</title><link>http://arxiv.org/abs/2309.09954v1</link><description>Medical Imaging (MI) tasks, such as accelerated Parallel Magnetic ResonanceImaging (MRI), often involve reconstructing an image from noisy or incompletemeasurements. This amounts to solving ill-posed inverse problems, where asatisfactory closed-form analytical solution is not available. Traditionalmethods such as Compressed Sensing (CS) in MRI reconstruction can betime-consuming or prone to obtaining low-fidelity images. Recently, a plethoraof supervised and self-supervised Deep Learning (DL) approaches havedemonstrated superior performance in inverse-problem solving, surpassingconventional methods. In this study, we propose vSHARP (variable SplittingHalf-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novelDL-based method for solving ill-posed inverse problems arising in MI. vSHARPutilizes the Half-Quadratic Variable Splitting method and employs theAlternating Direction Method of Multipliers (ADMM) to unroll the optimizationprocess. For data consistency, vSHARP unrolls a differentiable gradient descentprocess in the image domain, while a DL-based denoiser, such as a U-Netarchitecture, is applied to enhance image quality. vSHARP also employs adilated-convolution DL-based model to predict the Lagrange multipliers for theADMM initialization. We evaluate the proposed model by applying it to the taskof accelerated Parallel MRI Reconstruction on two distinct datasets. We presenta comparative analysis of our experimental results with state-of-the-artapproaches, highlighting the superior performance of vSHARP.</description><author>George Yiasemis, Nikita Moriakov, Jan-Jakob Sonke, Jonas Teuwen</author><pubDate>Mon, 18 Sep 2023 18:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09954v1</guid></item><item><title>How to Generate Popular Post Headlines on Social Media?</title><link>http://arxiv.org/abs/2309.09949v1</link><description>Posts, as important containers of user-generated-content pieces on socialmedia, are of tremendous social influence and commercial value. As an integralcomponents of a post, the headline has a decisive contribution to the post'spopularity. However, current mainstream method for headline generation is stillmanually writing, which is unstable and requires extensive human effort. Thisdrives us to explore a novel research question: Can we automate the generationof popular headlines on social media? We collect more than 1 million posts of42,447 celebrities from public data of Xiaohongshu, which is a well-knownsocial media platform in China. We then conduct careful observations on theheadlines of these posts. Observation results demonstrate that trends andpersonal styles are widespread in headlines on social medias and havesignificant contribution to posts's popularity. Motivated by these insights, wepresent MEBART, which combines Multiple preference-Extractors withBidirectional and Auto-Regressive Transformers (BART), capturing trends andpersonal styles to generate popular headlines on social medias. We performextensive experiments on real-world datasets and achieve state-of-the-artperformance compared with several advanced baselines. In addition, ablation andcase studies demonstrate that MEBART advances in capturing trends and personalstyles.</description><author>Zhouxiang Fang, Min Yu, Zhendong Fu, Boning Zhang, Xuanwen Huang, Xiaoqi Tang, Yang Yang</author><pubDate>Mon, 18 Sep 2023 18:12:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09949v1</guid></item><item><title>End-to-End Learned Event- and Image-based Visual Odometry</title><link>http://arxiv.org/abs/2309.09947v1</link><description>Visual Odometry (VO) is crucial for autonomous robotic navigation, especiallyin GPS-denied environments like planetary terrains. While standard RGB camerasstruggle in low-light or high-speed motion, event-based cameras offer highdynamic range and low latency. However, seamlessly integrating asynchronousevent data with synchronous frames remains challenging. We introduce RAMP-VO,the first end-to-end learned event- and image-based VO system. It leveragesnovel Recurrent, Asynchronous, and Massively Parallel (RAMP) encoders that are8x faster and 20% more accurate than existing asynchronous encoders. RAMP-VOfurther employs a novel pose forecasting technique to predict future poses forinitialization. Despite being trained only in simulation, RAMP-VO outperformsimage- and event-based methods by 52% and 20%, respectively, on traditional,real-world benchmarks as well as newly introduced Apollo and Malapert landingsequences, paving the way for robust and asynchronous VO in space.</description><author>Roberto Pellerito, Marco Cannici, Daniel Gehrig, Joris Belhadj, Olivier Dubois-Matra, Massimo Casasco, Davide Scaramuzza</author><pubDate>Mon, 18 Sep 2023 18:12:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09947v1</guid></item><item><title>What is a Fair Diffusion Model? Designing Generative Text-To-Image Models to Incorporate Various Worldviews</title><link>http://arxiv.org/abs/2309.09944v1</link><description>Generative text-to-image (GTI) models produce high-quality images from shorttextual descriptions and are widely used in academic and creative domains.However, GTI models frequently amplify biases from their training data, oftenproducing prejudiced or stereotypical images. Yet, current bias mitigationstrategies are limited and primarily focus on enforcing gender parity acrossoccupations. To enhance GTI bias mitigation, we introduce DiffusionWorldViewer,a tool to analyze and manipulate GTI models' attitudes, values, stories, andexpectations of the world that impact its generated images. Through aninteractive interface deployed as a web-based GUI and Jupyter Notebook plugin,DiffusionWorldViewer categorizes existing demographics of GTI-generated imagesand provides interactive methods to align image demographics with userworldviews. In a study with 13 GTI users, we find that DiffusionWorldViewerallows users to represent their varied viewpoints about what GTI outputs arefair and, in doing so, challenges current notions of fairness that assume auniversal worldview.</description><author>Zoe De Simone, Angie Boggust, Arvind Satyanarayan, Ashia Wilson</author><pubDate>Mon, 18 Sep 2023 18:04:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09944v1</guid></item><item><title>Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails</title><link>http://arxiv.org/abs/2309.06415v2</link><description>This paper conducts a robustness audit of the safety feedback of PaLM 2through a novel toxicity rabbit hole framework introduced here. Starting with astereotype, the framework instructs PaLM 2 to generate more toxic content thanthe stereotype. Every subsequent iteration it continues instructing PaLM 2 togenerate more toxic content than the previous iteration until PaLM 2 safetyguardrails throw a safety violation. Our experiments uncover highly disturbingantisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few)generated content that PaLM 2 safety guardrails do not evaluate as highlyunsafe.</description><author>Adel Khorramrouz, Sujan Dutta, Arka Dutta, Ashiqur R. KhudaBukhsh</author><pubDate>Mon, 18 Sep 2023 17:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06415v2</guid></item><item><title>Generative Knowledge Graph Construction: A Review</title><link>http://arxiv.org/abs/2210.12714v3</link><description>Generative Knowledge Graph Construction (KGC) refers to those methods thatleverage the sequence-to-sequence framework for building knowledge graphs,which is flexible and can be adapted to widespread tasks. In this study, wesummarize the recent compelling progress in generative knowledge graphconstruction. We present the advantages and weaknesses of each paradigm interms of different generation targets and provide theoretical insight andempirical analysis. Based on the review, we suggest promising researchdirections for the future. Our contributions are threefold: (1) We present adetailed, complete taxonomy for the generative KGC methods; (2) We provide atheoretical and empirical analysis of the generative KGC methods; (3) Wepropose several research directions that can be developed in the future.</description><author>Hongbin Ye, Ningyu Zhang, Hui Chen, Huajun Chen</author><pubDate>Mon, 18 Sep 2023 17:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12714v3</guid></item><item><title>Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction</title><link>http://arxiv.org/abs/2210.10709v5</link><description>With the development of pre-trained language models, many prompt-basedapproaches to data-efficient knowledge graph construction have been proposedand achieved impressive performance. However, existing prompt-based learningmethods for knowledge graph construction are still susceptible to severalpotential limitations: (i) semantic gap between natural language and outputstructured knowledge with pre-defined schema, which means model cannot fullyexploit semantic knowledge with the constrained templates; (ii) representationlearning with locally individual instances limits the performance given theinsufficient features, which are unable to unleash the potential analogicalcapability of pre-trained language models. Motivated by these observations, wepropose a retrieval-augmented approach, which retrieves schema-aware ReferenceAs Prompt (RAP), for data-efficient knowledge graph construction. It candynamically leverage schema and knowledge inherited from human-annotated andweak-supervised data as a prompt for each sample, which is model-agnostic andcan be plugged into widespread existing approaches. Experimental resultsdemonstrate that previous methods integrated with RAP can achieve impressiveperformance gains in low-resource settings on five datasets of relationaltriple extraction and event extraction for knowledge graph construction. Codeis available in https://github.com/zjunlp/RAP.</description><author>Yunzhi Yao, Shengyu Mao, Ningyu Zhang, Xiang Chen, Shumin Deng, Xi Chen, Huajun Chen</author><pubDate>Mon, 18 Sep 2023 17:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10709v5</guid></item><item><title>Hierarchical Attention and Graph Neural Networks: Toward Drift-Free Pose Estimation</title><link>http://arxiv.org/abs/2309.09934v1</link><description>The most commonly used method for addressing 3D geometric registration is theiterative closet-point algorithm, this approach is incremental and prone todrift over multiple consecutive frames. The Common strategy to address thedrift is the pose graph optimization subsequent to frame-to-frame registration,incorporating a loop closure process that identifies previously visited places.In this paper, we explore a framework that replaces traditional geometricregistration and pose graph optimization with a learned model utilizinghierarchical attention mechanisms and graph neural networks. We propose astrategy to condense the data flow, preserving essential information requiredfor the precise estimation of rigid poses. Our results, derived from tests onthe KITTI Odometry dataset, demonstrate a significant improvement in poseestimation accuracy. This improvement is especially notable in determiningrotational components when compared with results obtained through conventionalmulti-way registration via pose graph optimization. The code will be madeavailable upon completion of the review process.</description><author>Kathia Melbouci, Fawzi Nashashibi</author><pubDate>Mon, 18 Sep 2023 17:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09934v1</guid></item><item><title>One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER</title><link>http://arxiv.org/abs/2301.10410v5</link><description>Cross-domain NER is a challenging task to address the low-resource problem inpractical scenarios. Previous typical solutions mainly obtain a NER model bypre-trained language models (PLMs) with data from a rich-resource domain andadapt it to the target domain. Owing to the mismatch issue among entity typesin different domains, previous approaches normally tune all parameters of PLMs,ending up with an entirely new NER model for each domain. Moreover, currentmodels only focus on leveraging knowledge in one general source domain whilefailing to successfully transfer knowledge from multiple sources to the target.To address these issues, we introduce Collaborative Domain-Prefix Tuning forcross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically,we present text-to-text generation grounding domain-related instructors totransfer knowledge to new domain NER tasks without structural modifications. Weutilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulatethe potential of PLMs to handle NER tasks across various domains. Experimentalresults on the Cross-NER benchmark show that the proposed approach has flexibletransfer ability and performs better on both one-source and multiple-sourcecross-domain NER tasks. Codes are available inhttps://github.com/zjunlp/DeepKE/tree/main/example/ner/cross.</description><author>Xiang Chen, Lei Li, Shuofei Qiao, Ningyu Zhang, Chuanqi Tan, Yong Jiang, Fei Huang, Huajun Chen</author><pubDate>Mon, 18 Sep 2023 17:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10410v5</guid></item><item><title>Evaluating Adversarial Robustness with Expected Viable Performance</title><link>http://arxiv.org/abs/2309.09928v1</link><description>We introduce a metric for evaluating the robustness of a classifier, withparticular attention to adversarial perturbations, in terms of expectedfunctionality with respect to possible adversarial perturbations. A classifieris assumed to be non-functional (that is, has a functionality of zero) withrespect to a perturbation bound if a conventional measure of performance, suchas classification accuracy, is less than a minimally viable threshold when theclassifier is tested on examples from that perturbation bound. Definingrobustness in terms of an expected value is motivated by a domain generalapproach to robustness quantification.</description><author>Ryan McCoppin, Colin Dawson, Sean M. Kennedy, Leslie M. Blaha</author><pubDate>Mon, 18 Sep 2023 17:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09928v1</guid></item><item><title>KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction</title><link>http://arxiv.org/abs/2104.07650v7</link><description>Recently, prompt-tuning has achieved promising results for specific few-shotclassification tasks. The core idea of prompt-tuning is to insert text pieces(i.e., templates) into the input and transform a classification task into amasked language modeling problem. However, for relation extraction, determiningan appropriate prompt template requires domain expertise, and it is cumbersomeand time-consuming to obtain a suitable label word. Furthermore, there existsabundant semantic and prior knowledge among the relation labels that cannot beignored. To this end, we focus on incorporating knowledge among relation labelsinto prompt-tuning for relation extraction and propose a Knowledge-awarePrompt-tuning approach with synergistic optimization (KnowPrompt).Specifically, we inject latent knowledge contained in relation labels intoprompt construction with learnable virtual type words and answer words. Then,we synergistically optimize their representation with structured constraints.Extensive experimental results on five datasets with standard and low-resourcesettings demonstrate the effectiveness of our approach. Our code and datasetsare available in https://github.com/zjunlp/KnowPrompt for reproducibility.</description><author>Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen</author><pubDate>Mon, 18 Sep 2023 17:46:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.07650v7</guid></item><item><title>DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population</title><link>http://arxiv.org/abs/2201.03335v6</link><description>We present an open-source and extensible knowledge extraction toolkit DeepKE,supporting complicated low-resource, document-level and multimodal scenarios inthe knowledge base population. DeepKE implements various information extractiontasks, including named entity recognition, relation extraction and attributeextraction. With a unified framework, DeepKE allows developers and researchersto customize datasets and models to extract information from unstructured dataaccording to their requirements. Specifically, DeepKE not only provides variousfunctional modules and model implementation for different tasks and scenariosbut also organizes all components by consistent frameworks to maintainsufficient modularity and extensibility. We release the source code at GitHubin https://github.com/zjunlp/DeepKE with Google Colab tutorials andcomprehensive documents for beginners. Besides, we present an online system inhttp://deepke.openkg.cn/EN/re_doc_show.html for real-time extraction of varioustasks, and a demo video.</description><author>Ningyu Zhang, Xin Xu, Liankuan Tao, Haiyang Yu, Hongbin Ye, Shuofei Qiao, Xin Xie, Xiang Chen, Zhoubo Li, Lei Li, Xiaozhuan Liang, Yunzhi Yao, Shumin Deng, Peng Wang, Wen Zhang, Zhenru Zhang, Chuanqi Tan, Qiang Chen, Feiyu Xiong, Fei Huang, Guozhou Zheng, Huajun Chen</author><pubDate>Mon, 18 Sep 2023 17:42:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.03335v6</guid></item><item><title>Graph topological property recovery with heat and wave dynamics-based features on graphsD</title><link>http://arxiv.org/abs/2309.09924v1</link><description>In this paper, we propose Graph Differential Equation Network (GDeNet), anapproach that harnesses the expressive power of solutions to PDEs on a graph toobtain continuous node- and graph-level representations for various downstreamtasks. We derive theoretical results connecting the dynamics of heat and waveequations to the spectral properties of the graph and to the behavior ofcontinuous-time random walks on graphs. We demonstrate experimentally thatthese dynamics are able to capture salient aspects of graph geometry andtopology by recovering generating parameters of random graphs, Ricci curvature,and persistent homology. Furthermore, we demonstrate the superior performanceof GDeNet on real-world datasets including citation graphs, drug-likemolecules, and proteins.</description><author>Dhananjay Bhaskar, Yanlei Zhang, Charles Xu, Xingzhi Sun, Oluwadamilola Fasina, Guy Wolf, Maximilian Nickel, Michael Perlmutter, Smita Krishnaswamy</author><pubDate>Mon, 18 Sep 2023 17:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09924v1</guid></item><item><title>Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion</title><link>http://arxiv.org/abs/2205.02357v5</link><description>Multimodal Knowledge Graphs (MKGs), which organize visual-text factualknowledge, have recently been successfully applied to tasks such as informationretrieval, question answering, and recommendation system. Since most MKGs arefar from complete, extensive knowledge graph completion studies have beenproposed focusing on the multimodal entity, relation extraction and linkprediction. However, different tasks and modalities require changes to themodel architecture, and not all images/objects are relevant to text input,which hinders the applicability to diverse real-world scenarios. In this paper,we propose a hybrid transformer with multi-level fusion to address thoseissues. Specifically, we leverage a hybrid transformer architecture withunified input-output for diverse multimodal knowledge graph completion tasks.Moreover, we propose multi-level fusion, which integrates visual and textrepresentation via coarse-grained prefix-guided interaction and fine-grainedcorrelation-aware fusion modules. We conduct extensive experiments to validatethat our MKGformer can obtain SOTA performance on four datasets of multimodallink prediction, multimodal RE, and multimodal NER. Code is available inhttps://github.com/zjunlp/MKGformer.</description><author>Xiang Chen, Ningyu Zhang, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu, Fei Huang, Luo Si, Huajun Chen</author><pubDate>Mon, 18 Sep 2023 17:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.02357v5</guid></item><item><title>Interpretable and Fair Boolean Rule Sets via Column Generation</title><link>http://arxiv.org/abs/2111.08466v2</link><description>This paper considers the learning of Boolean rules in disjunctive normal form(DNF, OR-of-ANDs, equivalent to decision rule sets) as an interpretable modelfor classification. An integer program is formulated to optimally tradeclassification accuracy for rule simplicity. We also consider the fairnesssetting and extend the formulation to include explicit constraints on twodifferent measures of classification parity: equality of opportunity andequalized odds. Column generation (CG) is used to efficiently search over anexponential number of candidate rules without the need for heuristic rulemining. To handle large data sets, we propose an approximate CG algorithm usingrandomization. Compared to three recently proposed alternatives, the CGalgorithm dominates the accuracy-simplicity trade-off in 8 out of 16 data sets.When maximized for accuracy, CG is competitive with rule learners designed forthis purpose, sometimes finding significantly simpler solutions that are noless accurate. Compared to other fair and interpretable classifiers, our methodis able to find rule sets that meet stricter notions of fairness with a modesttrade-off in accuracy.</description><author>Connor Lawless, Sanjeeb Dash, Oktay Gunluk, Dennis Wei</author><pubDate>Mon, 18 Sep 2023 17:36:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.08466v2</guid></item><item><title>A Heterogeneous Graph-Based Multi-Task Learning for Fault Event Diagnosis in Smart Grid</title><link>http://arxiv.org/abs/2309.09921v1</link><description>Precise and timely fault diagnosis is a prerequisite for a distributionsystem to ensure minimum downtime and maintain reliable operation. Thisnecessitates access to a comprehensive procedure that can provide the gridoperators with insightful information in the case of a fault event. In thispaper, we propose a heterogeneous multi-task learning graph neural network(MTL-GNN) capable of detecting, locating and classifying faults in addition toproviding an estimate of the fault resistance and current. Using a graph neuralnetwork (GNN) allows for learning the topological representation of thedistribution system as well as feature learning through a message-passingscheme. We investigate the robustness of our proposed model using the IEEE-123test feeder system. This work also proposes a novel GNN-based explainabilitymethod to identify key nodes in the distribution system which then facilitatesinformed sparse measurements. Numerical tests validate the performance of themodel across all tasks.</description><author>Dibaloke Chanda, Nasim Yahya Soltani</author><pubDate>Mon, 18 Sep 2023 17:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09921v1</guid></item><item><title>Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation</title><link>http://arxiv.org/abs/2309.09920v1</link><description>Much research effort is being applied to the task of compressing theknowledge of self-supervised models, which are powerful, yet large and memoryconsuming. In this work, we show that the original method of knowledgedistillation (and its more recently proposed extension, decoupled knowledgedistillation) can be applied to the task of distilling HuBERT. In contrast tomethods that focus on distilling internal features, this allows for morefreedom in the network architecture of the compressed model. We thus propose todistill HuBERT's Transformer layers into an LSTM-based distilled model thatreduces the number of parameters even below DistilHuBERT and at the same timeshows improved performance in automatic speech recognition.</description><author>Danilo de Oliveira, Timo Gerkmann</author><pubDate>Mon, 18 Sep 2023 17:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09920v1</guid></item><item><title>Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents</title><link>http://arxiv.org/abs/2309.09919v1</link><description>Recent advancements in large language models (LLMs) have enabled a newresearch domain, LLM agents, for solving robotics and planning tasks byleveraging the world knowledge and general reasoning abilities of LLMs obtainedduring pretraining. However, while considerable effort has been made to teachthe robot the "dos," the "don'ts" received relatively less attention. We arguethat, for any practical usage, it is as crucial to teach the robot the"don'ts": conveying explicit instructions about prohibited actions, assessingthe robot's comprehension of these restrictions, and, most importantly,ensuring compliance. Moreover, verifiable safe operation is essential fordeployments that satisfy worldwide standards such as ISO 61508, which definesstandards for safely deploying robots in industrial factory environmentsworldwide. Aiming at deploying the LLM agents in a collaborative environment,we propose a queryable safety constraint module based on linear temporal logic(LTL) that simultaneously enables natural language (NL) to temporal constraintsencoding, safety violation reasoning and explaining, and unsafe action pruning.To demonstrate the effectiveness of our system, we conducted experiments inVirtualHome environment and on a real robot. The experimental results show thatour system strictly adheres to the safety constraints and scales well withcomplex safety constraints, highlighting its potential for practical utility.</description><author>Ziyi Yang, Shreyas S. Raman, Ankit Shah, Stefanie Tellex</author><pubDate>Mon, 18 Sep 2023 17:33:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09919v1</guid></item><item><title>Evaluation of Human-Understandability of Global Model Explanations using Decision Tree</title><link>http://arxiv.org/abs/2309.09917v1</link><description>In explainable artificial intelligence (XAI) research, the predominant focushas been on interpreting models for experts and practitioners. Model agnosticand local explanation approaches are deemed interpretable and sufficient inmany applications. However, in domains like healthcare, where end users arepatients without AI or domain expertise, there is an urgent need for modelexplanations that are more comprehensible and instil trust in the model'soperations. We hypothesise that generating model explanations that arenarrative, patient-specific and global(holistic of the model) would enablebetter understandability and enable decision-making. We test this using adecision tree model to generate both local and global explanations for patientsidentified as having a high risk of coronary heart disease. These explanationsare presented to non-expert users. We find a strong individual preference for aspecific type of explanation. The majority of participants prefer globalexplanations, while a smaller group prefers local explanations. A task basedevaluation of mental models of these participants provide valuable feedback toenhance narrative global explanations. This, in turn, guides the design ofhealth informatics systems that are both trustworthy and actionable.</description><author>Adarsa Sivaprasad, Ehud Reiter, Nava Tintarev, Nir Oren</author><pubDate>Mon, 18 Sep 2023 17:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09917v1</guid></item><item><title>Learning Nonparametric High-Dimensional Generative Models: The Empirical-Beta-Copula Autoencoder</title><link>http://arxiv.org/abs/2309.09916v1</link><description>By sampling from the latent space of an autoencoder and decoding the latentspace samples to the original data space, any autoencoder can simply be turnedinto a generative model. For this to work, it is necessary to model theautoencoder's latent space with a distribution from which samples can beobtained. Several simple possibilities (kernel density estimates, Gaussiandistribution) and more sophisticated ones (Gaussian mixture models, copulamodels, normalization flows) can be thought of and have been tried recently.This study aims to discuss, assess, and compare various techniques that can beused to capture the latent space so that an autoencoder can become a generativemodel while striving for simplicity. Among them, a new copula-based method, theEmpirical Beta Copula Autoencoder, is considered. Furthermore, we provideinsights into further aspects of these methods, such as targeted sampling orsynthesizing new data with specific features.</description><author>Maximilian Coblenz, Oliver Grothe, Fabian Kächele</author><pubDate>Mon, 18 Sep 2023 17:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09916v1</guid></item><item><title>Efficient Benchmarking (of Language Models)</title><link>http://arxiv.org/abs/2308.11696v3</link><description>The increasing versatility of language models LMs has given rise to a newclass of benchmarks that comprehensively assess a broad range of capabilities.Such benchmarks are associated with massive computational costs reachingthousands of GPU hours per model. However the efficiency aspect of theseevaluation efforts had raised little discussion in the literature. In this workwe present the problem of Efficient Benchmarking namely intelligently reducingthe computation costs of LM evaluation without compromising reliability. Usingthe HELM benchmark as a test case we investigate how different benchmark designchoices affect the computation-reliability tradeoff. We propose to evaluate thereliability of such decisions by using a new measure Decision Impact onReliability DIoR for short. We find for example that the current leader on HELMmay change by merely removing a low-ranked model from the benchmark and observethat a handful of examples suffice to obtain the correct benchmark ranking.Conversely a slightly different choice of HELM scenarios varies ranking widely.Based on our findings we outline a set of concrete recommendations for moreefficient benchmark design and utilization practices leading to dramatic costsavings with minimal loss of benchmark reliability often reducing computationby x100 or more.</description><author>Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</author><pubDate>Mon, 18 Sep 2023 17:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11696v3</guid></item><item><title>Two-Dimensional Quantum Material Identification via Self-Attention and Soft-labeling in Deep Learning</title><link>http://arxiv.org/abs/2205.15948v2</link><description>In quantum machine field, detecting two-dimensional (2D) materials in Siliconchips is one of the most critical problems. Instance segmentation can beconsidered as a potential approach to solve this problem. However, similar toother deep learning methods, the instance segmentation requires a large scaletraining dataset and high quality annotation in order to achieve a considerableperformance. In practice, preparing the training dataset is a challenge sinceannotators have to deal with a large image, e.g 2K resolution, and extremelydense objects in this problem. In this work, we present a novel method totackle the problem of missing annotation in instance segmentation in 2D quantummaterial identification. We propose a new mechanism for automatically detectingfalse negative objects and an attention based loss strategy to reduce thenegative impact of these objects contributing to the overall loss function. Weexperiment on the 2D material detection datasets, and the experiments show ourmethod outperforms previous works.</description><author>Xuan Bac Nguyen, Apoorva Bisht, Ben Thompson, Hugh Churchill, Khoa Luu, Samee U. Khan</author><pubDate>Mon, 18 Sep 2023 17:24:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.15948v2</guid></item><item><title>Wait, That Feels Familiar: Learning to Extrapolate Human Preferences for Preference Aligned Path Planning</title><link>http://arxiv.org/abs/2309.09912v1</link><description>Autonomous mobility tasks such as lastmile delivery require reasoning aboutoperator indicated preferences over terrains on which the robot should navigateto ensure both robot safety and mission success. However, coping with out ofdistribution data from novel terrains or appearance changes due to lightingvariations remains a fundamental problem in visual terrain adaptive navigation.Existing solutions either require labor intensive manual data recollection andlabeling or use handcoded reward functions that may not align with operatorpreferences. In this work, we posit that operator preferences for visuallynovel terrains, which the robot should adhere to, can often be extrapolatedfrom established terrain references within the inertial, proprioceptive, andtactile domain. Leveraging this insight, we introduce Preference extrApolationfor Terrain awarE Robot Navigation, PATERN, a novel framework for extrapolatingoperator terrain preferences for visual navigation. PATERN learns to mapinertial, proprioceptive, tactile measurements from the robots observations toa representation space and performs nearest neighbor search in this space toestimate operator preferences over novel terrains. Through physical robotexperiments in outdoor environments, we assess PATERNs capability toextrapolate preferences and generalize to novel terrains and challenginglighting conditions. Compared to baseline approaches, our findings indicatethat PATERN robustly generalizes to diverse terrains and varied lightingconditions, while navigating in a preference aligned manner.</description><author>Haresh Karnan, Elvin Yang, Garrett Warnell, Joydeep Biswas, Peter Stone</author><pubDate>Mon, 18 Sep 2023 17:24:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09912v1</guid></item><item><title>Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity</title><link>http://arxiv.org/abs/2303.13634v3</link><description>Regular physics-informed neural networks (PINNs) predict the solution ofpartial differential equations using sparse labeled data but only over a singledomain. On the other hand, fully supervised learning models are first trainedusually over a few thousand domains with known solutions (i.e., labeled data)and then predict the solution over a few hundred unseen domains.Physics-informed PointNet (PIPN) is primarily designed to fill this gap betweenPINNs (as weakly supervised learning models) and fully supervised learningmodels. In this article, we demonstrate that PIPN predicts the solution ofdesired partial differential equations over a few hundred domainssimultaneously, while it only uses sparse labeled data. This framework benefitsfast geometric designs in the industry when only sparse labeled data areavailable. Particularly, we show that PIPN predicts the solution of a planestress problem over more than 500 domains with different geometries,simultaneously. Moreover, we pioneer implementing the concept of remarkablebatch size (i.e., the number of geometries fed into PIPN at each sub-epoch)into PIPN. Specifically, we try batch sizes of 7, 14, 19, 38, 76, and 133.Additionally, the effect of the PIPN size, symmetric function in the PIPNarchitecture, and static and dynamic weights for the component of the sparselabeled data in the loss function are investigated.</description><author>Ali Kashefi, Leonidas J. Guibas, Tapan Mukerji</author><pubDate>Mon, 18 Sep 2023 17:22:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13634v3</guid></item><item><title>Finite Expression Methods for Discovering Physical Laws from Data</title><link>http://arxiv.org/abs/2305.08342v2</link><description>Nonlinear dynamics is a pervasive phenomenon observed in scientific andengineering disciplines. However, the task of deriving analytical expressionsto describe nonlinear dynamics from limited data remains challenging. In thispaper, we shall present a novel deep symbolic learning method called the"finite expression method" (FEX) to discover governing equations within afunction space containing a finite set of analytic expressions, based onobserved dynamic data. The key concept is to employ FEX to generate analyticalexpressions of the governing equations by learning the derivatives of partialdifferential equation (PDE) solutions through convolutions. Our numericalresults demonstrate that our FEX surpasses other existing methods (such asPDE-Net, SINDy, GP, and SPL) in terms of numerical performance across a rangeof problems, including time-dependent PDE problems and nonlinear dynamicalsystems with time-varying coefficients. Moreover, the results highlight FEX'sflexibility and expressive power in accurately approximating symbolic governingequations.</description><author>Zhongyi Jiang, Chunmei Wang, Haizhao Yang</author><pubDate>Mon, 18 Sep 2023 17:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08342v2</guid></item><item><title>Quantum Vision Clustering</title><link>http://arxiv.org/abs/2309.09907v1</link><description>Unsupervised visual clustering has recently received considerable attention.It aims to explain distributions of unlabeled visual images by clustering themvia a parameterized appearance model. From a different perspective, theclustering algorithms can be treated as assignment problems, often NP-hard.They can be solved precisely for small instances on current hardware. Adiabaticquantum computing (AQC) offers a solution, as it can soon provide aconsiderable speedup on a range of NP-hard optimization problems. However,current clustering formulations are unsuitable for quantum computing due totheir scaling properties. Consequently, in this work, we propose the firstclustering formulation designed to be solved with AQC. We employ an Ising modelrepresenting the quantum mechanical system implemented on the AQC. Our approachis competitive compared to state-of-the-art optimization-based approaches, evenusing of-the-shelf integer programming solvers. Finally, we demonstrate thatour clustering problem is already solvable on the current generation of realquantum computers for small examples and analyze the properties of the measuredsolutions.</description><author>Xuan Bac Nguyen, Benjamin Thompson, Hugh Churchill, Khoa Luu, Samee U. Khan</author><pubDate>Mon, 18 Sep 2023 17:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09907v1</guid></item><item><title>Learning to Generate Lumped Hydrological Models</title><link>http://arxiv.org/abs/2309.09904v1</link><description>In a lumped hydrological model structure, the hydrological function of acatchment is characterized by only a few parameters. Given a set of parametervalues, a numerical function useful for hydrological prediction is generated.Thus, this study assumes that the hydrological function of a catchment can besufficiently well characterized by a small number of latent variables. Byspecifying the variable values, a numerical function resembling thehydrological function of a real-world catchment can be generated using agenerative model. In this study, a deep learning method is used to learn boththe generative model and the latent variable values of different catchmentsdirectly from their climate forcing and runoff data, without using catchmentattributes. The generative models can be used similarly to a lumped modelstructure, i.e., by estimating the optimal parameter or latent variable valuesusing a generic model calibration algorithm, an optimal numerical model can bederived. In this study, generative models using eight latent variables werelearned from data from over 3,000 catchments worldwide, and the learnedgenerative models were applied to model over 700 different catchments using ageneric calibration algorithm. The quality of the resulting optimal models wasgenerally comparable to or better than that obtained using 36 different typesof lump model structures or using non-generative deep learning methods. Insummary, this study presents a data-driven approach for representing thehydrological function of a catchment in low-dimensional space and a method forreconstructing specific hydrological functions from the representations.</description><author>Yang Yang, Ting Fong May Chui</author><pubDate>Mon, 18 Sep 2023 17:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09904v1</guid></item><item><title>Speaker attribution in German parliamentary debates with QLoRA-adapted large language models</title><link>http://arxiv.org/abs/2309.09902v1</link><description>The growing body of political texts opens up new opportunities for richinsights into political dynamics and ideologies but also increases the workloadfor manual analysis. Automated speaker attribution, which detects who said whatto whom in a speech event and is closely related to semantic role labeling, isan important processing step for computational text analysis. We study thepotential of the large language model family Llama 2 to automate speakerattribution in German parliamentary debates from 2017-2021. We fine-tune Llama2 with QLoRA, an efficient training strategy, and observe our approach toachieve competitive performance in the GermEval 2023 Shared Task On SpeakerAttribution in German News Articles and Parliamentary Debates. Our results shedlight on the capabilities of large language models in automating speakerattribution, revealing a promising avenue for computational analysis ofpolitical discourse and the development of semantic role labeling systems.</description><author>Tobias Bornheim, Niklas Grieger, Patrick Gustav Blaneck, Stephan Bialonski</author><pubDate>Mon, 18 Sep 2023 17:06:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09902v1</guid></item><item><title>The role of causality in explainable artificial intelligence</title><link>http://arxiv.org/abs/2309.09901v1</link><description>Causality and eXplainable Artificial Intelligence (XAI) have developed asseparate fields in computer science, even though the underlying concepts ofcausation and explanation share common ancient roots. This is further enforcedby the lack of review works jointly covering these two fields. In this paper,we investigate the literature to try to understand how and to what extentcausality and XAI are intertwined. More precisely, we seek to uncover whatkinds of relationships exist between the two concepts and how one can benefitfrom them, for instance, in building trust in AI systems. As a result, threemain perspectives are identified. In the first one, the lack of causality isseen as one of the major limitations of current AI and XAI approaches, and the"optimal" form of explanations is investigated. The second is a pragmaticperspective and considers XAI as a tool to foster scientific exploration forcausal inquiry, via the identification of pursue-worthy experimentalmanipulations. Finally, the third perspective supports the idea that causalityis propaedeutic to XAI in three possible manners: exploiting concepts borrowedfrom causality to support or improve XAI, utilizing counterfactuals forexplainability, and considering accessing a causal model as explaining itself.To complement our analysis, we also provide relevant software solutions used toautomate causal tasks. We believe our work provides a unified view of the twofields of causality and XAI by highlighting potential domain bridges anduncovering possible limitations.</description><author>Gianluca Carloni, Andrea Berti, Sara Colantonio</author><pubDate>Mon, 18 Sep 2023 17:05:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09901v1</guid></item><item><title>Towards Ontology Construction with Language Models</title><link>http://arxiv.org/abs/2309.09898v1</link><description>We present a method for automatically constructing a concept hierarchy for agiven domain by querying a large language model. We apply this method tovarious domains using OpenAI's GPT 3.5. Our experiments indicate that LLMs canbe of considerable help for constructing concept hierarchies.</description><author>Maurice Funk, Simon Hosemann, Jean Christoph Jung, Carsten Lutz</author><pubDate>Mon, 18 Sep 2023 17:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09898v1</guid></item><item><title>Walking fingerprinting</title><link>http://arxiv.org/abs/2309.09897v1</link><description>We consider the problem of predicting an individual's identity fromaccelerometry data collected during walking. In a previous paper we introducedan approach that transforms the accelerometry time series into an image byconstructing its complete empirical autocorrelation distribution. Predictorsderived by partitioning this image into grid cells were used in logisticregression to predict individuals. Here we: (1) implement machine learningmethods for prediction using the grid cell-derived predictors; (2) deriveinferential methods to screen for the most predictive grid cells; and (3)develop a novel multivariate functional regression model that avoidspartitioning of the predictor space into cells. Prediction methods are comparedon two open source data sets: (1) accelerometry data collected from $32$individuals walking on a $1.06$ kilometer path; and (2) accelerometry datacollected from six repetitions of walking on a $20$ meter path on two separateoccasions at least one week apart for $153$ study participants. In the$32$-individual study, all methods achieve at least $95$% rank-1 accuracy,while in the $153$-individual study, accuracy varies from $41$% to $98$%,depending on the method and prediction task. Methods provide insights into whysome individuals are easier to predict than others.</description><author>Lily Koffman, Ciprian Crainiceanu, Andrew Leroux</author><pubDate>Mon, 18 Sep 2023 17:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09897v1</guid></item><item><title>Context $\approx$ Environment</title><link>http://arxiv.org/abs/2309.09888v1</link><description>Two lines of work are taking the central stage in AI research. On the onehand, the community is making increasing efforts to build models that discardspurious correlations and generalize better in novel test environments.Unfortunately, the bitter lesson so far is that no proposal convincinglyoutperforms a simple empirical risk minimization baseline. On the other hand,large language models (LLMs) have erupted as algorithms able to learnin-context, generalizing on-the-fly to the eclectic contextual circumstancesthat users enforce by means of prompting. In this paper, we argue that context$\approx$ environment, and posit that in-context learning holds the key tobetter domain generalization. Via extensive theory and experiments, we showthat paying attention to context$\unicode{x2013}\unicode{x2013}$unlabeledexamples as they arrive$\unicode{x2013}\unicode{x2013}$allows our proposedIn-Context Risk Minimization (ICRM) algorithm to zoom-in on the testenvironment risk minimizer, leading to significant out-of-distributionperformance improvements. From all of this, two messages are worth taking home.Researchers in domain generalization should consider environment as context,and harness the adaptive power of in-context learning. Researchers in LLMsshould consider context as environment to better structure data towardsgeneralization.</description><author>Sharut Gupta, Stefanie Jegelka, David Lopez-Paz, Kartik Ahuja</author><pubDate>Mon, 18 Sep 2023 16:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09888v1</guid></item><item><title>On Model Explanations with Transferable Neural Pathways</title><link>http://arxiv.org/abs/2309.09887v1</link><description>Neural pathways as model explanations consist of a sparse set of neurons thatprovide the same level of prediction performance as the whole model. Existingmethods primarily focus on accuracy and sparsity but the generated pathways mayoffer limited interpretability thus fall short in explaining the modelbehavior. In this paper, we suggest two interpretability criteria of neuralpathways: (i) same-class neural pathways should primarily consist ofclass-relevant neurons; (ii) each instance's neural pathway sparsity should beoptimally determined. To this end, we propose a Generative Class-relevantNeural Pathway (GEN-CNP) model that learns to predict the neural pathways fromthe target model's feature maps. We propose to learn class-relevant informationfrom features of deep and shallow layers such that same-class neural pathwaysexhibit high similarity. We further impose a faithfulness criterion for GEN-CNPto generate pathways with instance-specific sparsity. We propose to transferthe class-relevant neural pathways to explain samples of the same class andshow experimentally and qualitatively their faithfulness and interpretability.</description><author>Xinmiao Lin, Wentao Bao, Qi Yu, Yu Kong</author><pubDate>Mon, 18 Sep 2023 16:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09887v1</guid></item><item><title>Deep Reinforcement Learning for the Joint Control of Traffic Light Signaling and Vehicle Speed Advice</title><link>http://arxiv.org/abs/2309.09881v1</link><description>Traffic congestion in dense urban centers presents an economical andenvironmental burden. In recent years, the availability of vehicle-to-anythingcommunication allows for the transmission of detailed vehicle states to theinfrastructure that can be used for intelligent traffic light control. Theother way around, the infrastructure can provide vehicles with advice ondriving behavior, such as appropriate velocities, which can improve theefficacy of the traffic system. Several research works applied deepreinforcement learning to either traffic light control or vehicle speed advice.In this work, we propose a first attempt to jointly learn the control of both.We show this to improve the efficacy of traffic systems. In our experiments,the joint control approach reduces average vehicle trip delays, w.r.t.controlling only traffic lights, in eight out of eleven benchmark scenarios.Analyzing the qualitative behavior of the vehicle speed advice policy, weobserve that this is achieved by smoothing out the velocity profile of vehiclesnearby a traffic light. Learning joint control of traffic signaling and speedadvice in the real world could help to reduce congestion and mitigate theeconomical and environmental repercussions of today's traffic systems.</description><author>Johannes V. S. Busch, Robert Voelckner, Peter Sossalla, Christian L. Vielhaus, Roberto Calandra, Frank H. P. Fitzek</author><pubDate>Mon, 18 Sep 2023 16:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09881v1</guid></item><item><title>Error Reduction from Stacked Regressions</title><link>http://arxiv.org/abs/2309.09880v1</link><description>Stacking regressions is an ensemble technique that forms linear combinationsof different regression estimators to enhance predictive accuracy. Theconventional approach uses cross-validation data to generate predictions fromthe constituent estimators, and least-squares with nonnegativity constraints tolearn the combination weights. In this paper, we learn these weightsanalogously by minimizing an estimate of the population risk subject to anonnegativity constraint. When the constituent estimators are linearleast-squares projections onto nested subspaces separated by at least threedimensions, we show that thanks to a shrinkage effect, the resulting stackedestimator has strictly smaller population risk than best single estimator amongthem. Here ``best'' refers to a model that minimizes a selection criterion suchas AIC or BIC. In other words, in this setting, the best single estimator isinadmissible. Because the optimization problem can be reformulated as isotonicregression, the stacked estimator requires the same order of computation as thebest single estimator, making it an attractive alternative in terms of bothperformance and implementation.</description><author>Xin Chen, Jason M. Klusowski, Yan Shuo Tan</author><pubDate>Mon, 18 Sep 2023 16:42:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09880v1</guid></item><item><title>Not Enough Labeled Data? Just Add Semantics: A Data-Efficient Method for Inferring Online Health Texts</title><link>http://arxiv.org/abs/2309.09877v1</link><description>User-generated texts available on the web and social platforms are often longand semantically challenging, making them difficult to annotate. Obtaininghuman annotation becomes increasingly difficult as problem domains become morespecialized. For example, many health NLP problems require domain experts to bea part of the annotation pipeline. Thus, it is crucial that we developlow-resource NLP solutions able to work with this set of limited-data problems.In this study, we employ Abstract Meaning Representation (AMR) graphs as ameans to model low-resource Health NLP tasks sourced from various online healthresources and communities. AMRs are well suited to model online health texts asthey can represent multi-sentence inputs, abstract away from complexterminology, and model long-distance relationships between co-referring tokens.AMRs thus improve the ability of pre-trained language models to reason abouthigh-complexity texts. Our experiments show that we can improve performance on6 low-resource health NLP tasks by augmenting text embeddings with semanticgraph embeddings. Our approach is task agnostic and easy to merge into anystandard text classification pipeline. We experimentally validate that AMRs areuseful in the modeling of complex texts by analyzing performance through thelens of two textual complexity measures: the Flesch Kincaid Reading Level andSyntactic Complexity. Our error analysis shows that AMR-infused language modelsperform better on complex texts and generally show less predictive variance inthe presence of changing complexity.</description><author>Joseph Gatto, Sarah M. Preum</author><pubDate>Mon, 18 Sep 2023 16:37:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09877v1</guid></item><item><title>RaLF: Flow-based Global and Metric Radar Localization in LiDAR Maps</title><link>http://arxiv.org/abs/2309.09875v1</link><description>Localization is paramount for autonomous robots. While camera and LiDAR-basedapproaches have been extensively investigated, they are affected by adverseillumination and weather conditions. Therefore, radar sensors have recentlygained attention due to their intrinsic robustness to such conditions. In thispaper, we propose RaLF, a novel deep neural network-based approach forlocalizing radar scans in a LiDAR map of the environment, by jointly learningto address both place recognition and metric localization. RaLF is composed ofradar and LiDAR feature encoders, a place recognition head that generatesglobal descriptors, and a metric localization head that predicts the 3-DoFtransformation between the radar scan and the map. We tackle the placerecognition task by learning a shared embedding space between the twomodalities via cross-modal metric learning. Additionally, we perform metriclocalization by predicting pixel-level flow vectors that align the query radarscan with the LiDAR map. We extensively evaluate our approach on multiplereal-world driving datasets and show that RaLF achieves state-of-the-artperformance for both place recognition and metric localization. Moreover, wedemonstrate that our approach can effectively generalize to different citiesand sensor setups than the ones used during training. We make the code andtrained models publicly available at http://ralf.cs.uni-freiburg.de.</description><author>Abhijeet Nayak, Daniele Cattaneo, Abhinav Valada</author><pubDate>Mon, 18 Sep 2023 16:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09875v1</guid></item><item><title>MuMUR : Multilingual Multimodal Universal Retrieval</title><link>http://arxiv.org/abs/2208.11553v6</link><description>Multi-modal retrieval has seen tremendous progress with the development ofvision-language models. However, further improving these models requireadditional labelled data which is a huge manual effort. In this paper, wepropose a framework MuMUR, that utilizes knowledge transfer from a multilingualmodel to boost the performance of multi-modal (image and video) retrieval. Wefirst use state-of-the-art machine translation models to construct pseudoground-truth multilingual visual-text pairs. We then use this data to learn ajoint vision-text representation where English and non-English text queries arerepresented in a common embedding space based on pretrained multilingualmodels. We evaluate our proposed approach on a diverse set of retrievaldatasets: five video retrieval datasets such as MSRVTT, MSVD, DiDeMo, Charadesand MSRVTT multilingual, two image retrieval datasets such as Flickr30k andMulti30k . Experimental results demonstrate that our approach achievesstate-of-the-art results on all video retrieval datasets outperforming previousmodels. Additionally, our framework MuMUR significantly beats othermultilingual video retrieval dataset. We also observe that MuMUR exhibitsstrong performance on image retrieval. This demonstrates the universal abilityof MuMUR to perform retrieval across all visual inputs (image and video) andtext inputs (monolingual and multilingual).</description><author>Avinash Madasu, Estelle Aflalo, Gabriela Ben Melech Stan, Shachar Rosenman, Shao-Yen Tseng, Gedas Bertasius, Vasudev Lal</author><pubDate>Mon, 18 Sep 2023 16:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.11553v6</guid></item><item><title>EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning</title><link>http://arxiv.org/abs/2309.09867v1</link><description>When translating UI design prototypes to code in industry, automaticallygenerating code from design prototypes can expedite the development ofapplications and GUI iterations. However, in design prototypes without strictdesign specifications, UI components may be composed of fragmented elements.Grouping these fragmented elements can greatly improve the readability andmaintainability of the generated code. Current methods employ a two-stagestrategy that introduces hand-crafted rules to group fragmented elements.Unfortunately, the performance of these methods is not satisfying due tovisually overlapped and tiny UI elements. In this study, we propose EGFE, anovel method for automatically End-to-end Grouping Fragmented Elements via UIsequence prediction. To facilitate the UI understanding, we innovativelyconstruct a Transformer encoder to model the relationship between the UIelements with multi-modal representation learning. The evaluation on a datasetof 4606 UI prototypes collected from professional UI designers shows that ourmethod outperforms the state-of-the-art baselines in the precision (by29.75\%), recall (by 31.07\%), and F1-score (by 30.39\%) at edit distancethreshold of 4. In addition, we conduct an empirical study to assess theimprovement of the generated front-end code. The results demonstrate theeffectiveness of our method on a real software engineering application. Ourend-to-end fragmented elements grouping method creates opportunities forimproving UI-related software engineering tasks.</description><author>Liuqing Chen, Yunnong Chen, Shuhong Xiao, Yaxuan Song, Lingyun Sun, Yankun Zhen, Tingting Zhou, Yanfang Chang</author><pubDate>Mon, 18 Sep 2023 16:28:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09867v1</guid></item><item><title>Domain Generalization with Fourier Transform and Soft Thresholding</title><link>http://arxiv.org/abs/2309.09866v1</link><description>Domain generalization aims to train models on multiple source domains so thatthey can generalize well to unseen target domains. Among many domaingeneralization methods, Fourier-transform-based domain generalization methodshave gained popularity primarily because they exploit the power of Fouriertransformation to capture essential patterns and regularities in the data,making the model more robust to domain shifts. The mainstreamFourier-transform-based domain generalization swaps the Fourier amplitudespectrum while preserving the phase spectrum between the source and the targetimages. However, it neglects background interference in the amplitude spectrum.To overcome this limitation, we introduce a soft-thresholding function in theFourier domain. We apply this newly designed algorithm to retinal fundus imagesegmentation, which is important for diagnosing ocular diseases but the neuralnetwork's performance can degrade across different sources due to domainshifts. The proposed technique basically enhances fundus image augmentation byeliminating small values in the Fourier domain and providing bettergeneralization. The innovative nature of the soft thresholding fused withFourier-transform-based domain generalization improves neural network models'performance by reducing the target images' background interferencesignificantly. Experiments on public data validate our approach's effectivenessover conventional and state-of-the-art methods with superior segmentationmetrics.</description><author>Hongyi Pan, Bin Wang, Zheyuan Zhan, Xin Zhu, Debesh Jha, Ahmet Enis Cetin, Concetto Spampinato, Ulas Bagci</author><pubDate>Mon, 18 Sep 2023 16:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09866v1</guid></item><item><title>Neural Operator: Is data all you need to model the world? An insight into the impact of Physics Informed Machine Learning</title><link>http://arxiv.org/abs/2301.13331v2</link><description>Numerical approximations of partial differential equations (PDEs) areroutinely employed to formulate the solution of physics, engineering andmathematical problems involving functions of several variables, such as thepropagation of heat or sound, fluid flow, elasticity, electrostatics,electrodynamics, and more. While this has led to solving many complexphenomena, there are some limitations. Conventional approaches such as FiniteElement Methods (FEMs) and Finite Differential Methods (FDMs) requireconsiderable time and are computationally expensive. In contrast, data drivenmachine learning-based methods such as neural networks provide a faster, fairlyaccurate alternative, and have certain advantages such as discretizationinvariance and resolution invariance. This article aims to provide acomprehensive insight into how data-driven approaches can complementconventional techniques to solve engineering and physics problems, while alsonoting some of the major pitfalls of machine learning-based approaches.Furthermore, we highlight, a novel and fast machine learning-based approach(~1000x) to learning the solution operator of a PDE operator learning. We willnote how these new computational approaches can bring immense advantages intackling many problems in fundamental and applied physics.</description><author>Hrishikesh Viswanath, Md Ashiqur Rahman, Abhijeet Vyas, Andrey Shor, Beatriz Medeiros, Stephanie Hernandez, Suhas Eswarappa Prameela, Aniket Bera</author><pubDate>Mon, 18 Sep 2023 16:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13331v2</guid></item><item><title>Contrastive Learning for Enhancing Robust Scene Transfer in Vision-based Agile Flight</title><link>http://arxiv.org/abs/2309.09865v1</link><description>Scene transfer for vision-based mobile robotics applications is a highlyrelevant and challenging problem. The utility of a robot greatly depends on itsability to perform a task in the real world, outside of a well-controlled labenvironment. Existing scene transfer end-to-end policy learning approachesoften suffer from poor sample efficiency or limited generalizationcapabilities, making them unsuitable for mobile robotics applications. Thiswork proposes an adaptive multi-pair contrastive learning strategy for visualrepresentation learning that enables zero-shot scene transfer and real-worlddeployment. Control policies relying on the embedding are able to operate inunseen environments without the need for finetuning in the deploymentenvironment. We demonstrate the performance of our approach on the task ofagile, vision-based quadrotor flight. Extensive simulation and real-worldexperiments demonstrate that our approach successfully generalizes beyond thetraining domain and outperforms all baselines.</description><author>Jiaxu Xing, Leonard Bauersfeld, Yunlong Song, Chunwei Xing, Davide Scaramuzza</author><pubDate>Mon, 18 Sep 2023 16:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09865v1</guid></item><item><title>Learning Spatial and Temporal Hierarchies: Hierarchical Active Inference for navigation in Multi-Room Maze Environments</title><link>http://arxiv.org/abs/2309.09864v1</link><description>Cognitive maps play a crucial role in facilitating flexible behaviour byrepresenting spatial and conceptual relationships within an environment. Theability to learn and infer the underlying structure of the environment iscrucial for effective exploration and navigation. This paper introduces ahierarchical active inference model addressing the challenge of inferringstructure in the world from pixel-based observations. We propose a three-layerhierarchical model consisting of a cognitive map, an allocentric, and anegocentric world model, combining curiosity-driven exploration withgoal-oriented behaviour at the different levels of reasoning from context toplace to motion. This allows for efficient exploration and goal-directed searchin room-structured mini-grid environments.</description><author>Daria de Tinguy, Toon Van de Maele, Tim Verbelen, Bart Dhoedt</author><pubDate>Mon, 18 Sep 2023 16:24:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09864v1</guid></item><item><title>FedDCT: Federated Learning of Large Convolutional Neural Networks on Resource Constrained Devices using Divide and Collaborative Training</title><link>http://arxiv.org/abs/2211.10948v2</link><description>We introduce FedDCT, a novel distributed learning paradigm that enables theusage of large, high-performance CNNs on resource-limited edge devices. Asopposed to traditional FL approaches, which require each client to train thefull-size neural network independently during each training round, the proposedFedDCT allows a cluster of several clients to collaboratively train a largedeep learning model by dividing it into an ensemble of several small sub-modelsand train them on multiple devices in parallel while maintaining privacy. Inthis collaborative training process, clients from the same cluster can alsolearn from each other, further improving their ensemble performance. In theaggregation stage, the server takes a weighted average of all the ensemblemodels trained by all the clusters. FedDCT reduces the memory requirements andallows low-end devices to participate in FL. We empirically conduct extensiveexperiments on standardized datasets, including CIFAR-10, CIFAR-100, and tworeal-world medical datasets HAM10000 and VAIPE. Experimental results show thatFedDCT outperforms a set of current SOTA FL methods with interestingconvergence behaviors. Furthermore, compared to other existing approaches,FedDCT achieves higher accuracy and substantially reduces the number ofcommunication rounds (with $4-8$ times fewer memory requirements) to achievethe desired accuracy on the testing dataset without incurring any extratraining cost on the server side.</description><author>Quan Nguyen, Hieu H. Pham, Kok-Seng Wong, Phi Le Nguyen, Truong Thao Nguyen, Minh N. Do</author><pubDate>Mon, 18 Sep 2023 16:21:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10948v2</guid></item><item><title>Unsupervised Open-Vocabulary Object Localization in Videos</title><link>http://arxiv.org/abs/2309.09858v1</link><description>In this paper, we show that recent advances in video representation learningand pre-trained vision-language models allow for substantial improvements inself-supervised video object localization. We propose a method that firstlocalizes objects in videos via a slot attention approach and then assigns textto the obtained slots. The latter is achieved by an unsupervised way to readlocalized semantic information from the pre-trained CLIP model. The resultingvideo object localization is entirely unsupervised apart from the implicitannotation contained in CLIP, and it is effectively the first unsupervisedapproach that yields good results on regular video benchmarks.</description><author>Ke Fan, Zechen Bai, Tianjun Xiao, Dominik Zietlow, Max Horn, Zixu Zhao, Carl-Johann Simon-Gabriel, Mike Zheng Shou, Francesco Locatello, Bernt Schiele, Thomas Brox, Zheng Zhang, Yanwei Fu, Tong He</author><pubDate>Mon, 18 Sep 2023 16:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09858v1</guid></item><item><title>Investigation of Compressor Cascade Flow Using Physics- Informed Neural Networks with Adaptive Learning Strategy</title><link>http://arxiv.org/abs/2308.04501v2</link><description>In this study, we utilize the emerging Physics Informed Neural Networks(PINNs) approach for the first time to predict the flow field of a compressorcascade. Different from conventional training methods, a new adaptive learningstrategy that mitigates gradient imbalance through incorporating adaptiveweights in conjunction with dynamically adjusting learning rate is used duringthe training process to improve the convergence of PINNs. The performance ofPINNs is assessed here by solving both the forward and inverse problems. In theforward problem, by encapsulating the physical relations among relevantvariables, PINNs demonstrate their effectiveness in accurately forecasting thecompressor's flow field. PINNs also show obvious advantages over thetraditional CFD approaches, particularly in scenarios lacking complete boundaryconditions, as is often the case in inverse engineering problems. PINNssuccessfully reconstruct the flow field of the compressor cascade solely basedon partial velocity vectors and near-wall pressure information. Furthermore,PINNs show robust performance in the environment of various levels of aleatoryuncertainties stemming from labeled data. This research provides evidence thatPINNs can offer turbomachinery designers an additional and promising optionalongside the current dominant CFD methods.</description><author>Zhihui Li, Francesco Montomoli, Sanjiv Sharma</author><pubDate>Mon, 18 Sep 2023 16:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04501v2</guid></item><item><title>Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI</title><link>http://arxiv.org/abs/2308.12915v2</link><description>In this paper, we present "1001 Nights", an AI-native game that allowsplayers lead in-game reality through co-created storytelling with the characterdriven by large language model. The concept is inspired by Wittgenstein's ideaof the limits of one's world being determined by the bounds of their language.Using advanced AI tools like GPT-4 and Stable Diffusion, the second iterationof the game enables the protagonist, Shahrzad, to realize words and stories inher world. The player can steer the conversation with the AI King towardsspecific keywords, which then become battle equipment in the game. This blendof interactive narrative and text-to-image transformation challenges theconventional border between the game world and reality through a dualperspective. We focus on Shahrzad, who seeks to alter her fate compared to theoriginal folklore, and the player, who collaborates with AI to craft narrativesand shape the game world. We explore the technical and design elements ofimplementing such a game with an objective to enhance the narrative game genrewith AI-generated content and to delve into AI-native gameplay possibilities.</description><author>Yuqian Sun, Zhouyi Li, Ke Fang, Chang Hee Lee, Ali Asadipour</author><pubDate>Mon, 18 Sep 2023 16:16:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12915v2</guid></item><item><title>PseudoCal: Towards Initialisation-Free Deep Learning-Based Camera-LiDAR Self-Calibration</title><link>http://arxiv.org/abs/2309.09855v1</link><description>Camera-LiDAR extrinsic calibration is a critical task for multi-sensor fusionin autonomous systems, such as self-driving vehicles and mobile robots.Traditional techniques often require manual intervention or specificenvironments, making them labour-intensive and error-prone. Existing deeplearning-based self-calibration methods focus on small realignments and stillrely on initial estimates, limiting their practicality. In this paper, wepresent PseudoCal, a novel self-calibration method that overcomes theselimitations by leveraging the pseudo-LiDAR concept and working directly in the3D space instead of limiting itself to the camera field of view. In typicalautonomous vehicle and robotics contexts and conventions, PseudoCal is able toperform one-shot calibration quasi-independently of initial parameterestimates, addressing extreme cases that remain unsolved by existingapproaches.</description><author>Mathieu Cocheteux, Julien Moreau, Franck Davoine</author><pubDate>Mon, 18 Sep 2023 16:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09855v1</guid></item><item><title>CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs</title><link>http://arxiv.org/abs/2309.09844v1</link><description>Corner case scenarios are an essential tool for testing and validating thesafety of autonomous vehicles (AVs). As these scenarios are ofteninsufficiently present in naturalistic driving datasets, augmenting the datawith synthetic corner cases greatly enhances the safe operation of AVs inunique situations. However, the generation of synthetic, yet realistic, cornercases poses a significant challenge. In this work, we introduce a novelapproach based on Heterogeneous Graph Neural Networks (HGNNs) to transformregular driving scenarios into corner cases. To achieve this, we first generateconcise representations of regular driving scenes as scene graphs, minimallymanipulating their structure and properties. Our model then learns to perturbthose graphs to generate corner cases using attention and triple embeddings.The input and perturbed graphs are then imported back into the simulation togenerate corner case scenarios. Our model successfully learned to producecorner cases from input scene graphs, achieving 89.9% prediction accuracy onour testing dataset. We further validate the generated scenarios on baselineautonomous driving methods, demonstrating our model's ability to effectivelycreate critical situations for the baselines.</description><author>George Drayson, Efimia Panagiotaki, Daniel Omeiza, Lars Kunze</author><pubDate>Mon, 18 Sep 2023 15:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09844v1</guid></item><item><title>Instruction-Following Speech Recognition</title><link>http://arxiv.org/abs/2309.09843v1</link><description>Conventional end-to-end Automatic Speech Recognition (ASR) models primarilyfocus on exact transcription tasks, lacking flexibility for nuanced userinteractions. With the advent of Large Language Models (LLMs) in speechprocessing, more organic, text-prompt-based interactions have become possible.However, the mechanisms behind these models' speech understanding and"reasoning" capabilities remain underexplored. To study this question from thedata perspective, we introduce instruction-following speech recognition,training a Listen-Attend-Spell model to understand and execute a diverse set offree-form text instructions. This enables a multitude of speech recognitiontasks -- ranging from transcript manipulation to summarization -- withoutrelying on predefined command sets. Remarkably, our model, trained from scratchon Librispeech, interprets and executes simple instructions without requiringLLMs or pre-trained speech modules. It also offers selective transcriptionoptions based on instructions like "transcribe first half and then turn offlistening," providing an additional layer of privacy and safety compared toexisting LLMs. Our findings highlight the significant potential ofinstruction-following training to advance speech foundation models.</description><author>Cheng-I Jeff Lai, Zhiyun Lu, Liangliang Cao, Ruoming Pang</author><pubDate>Mon, 18 Sep 2023 15:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09843v1</guid></item><item><title>Hypr: A comprehensive study for ASR hypothesis revising with a reference corpus</title><link>http://arxiv.org/abs/2309.09838v1</link><description>With the development of deep learning, automatic speech recognition (ASR) hasmade significant progress. To further enhance the performance, revisingrecognition results is one of the lightweight but efficient manners. Variousmethods can be roughly classified into N-best reranking methods and errorcorrection models. The former aims to select the hypothesis with the lowesterror rate from a set of candidates generated by ASR for a given input speech.The latter focuses on detecting recognition errors in a given hypothesis andcorrecting these errors to obtain an enhanced result. However, we observe thatthese studies are hardly comparable to each other as they are usually evaluatedon different corpora, paired with different ASR models, and even use differentdatasets to train the models. Accordingly, we first concentrate on releasing anASR hypothesis revising (HypR) dataset in this study. HypR contains severalcommonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provides 50recognition hypotheses for each speech utterance. The checkpoint models of theASR are also published. In addition, we implement and compare several classicand representative methods, showing the recent research progress in revisingspeech recognition results. We hope the publicly available HypR dataset canbecome a reference benchmark for subsequent research and promote the school ofresearch to an advanced level.</description><author>Yi-Wei Wang, Ke-Han Lu, Kuan-Yu Chen</author><pubDate>Mon, 18 Sep 2023 15:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09838v1</guid></item><item><title>RECAP: Retrieval-Augmented Audio Captioning</title><link>http://arxiv.org/abs/2309.09836v1</link><description>We present RECAP (REtrieval-Augmented Audio CAPtioning), a novel andeffective audio captioning system that generates captions conditioned on aninput audio and other captions similar to the audio retrieved from a datastore.Additionally, our proposed method can transfer to any domain without the needfor any additional fine-tuning. To generate a caption for an audio sample, weleverage an audio-text model CLAP to retrieve captions similar to it from areplaceable datastore, which are then used to construct a prompt. Next, we feedthis prompt to a GPT-2 decoder and introduce cross-attention layers between theCLAP encoder and GPT-2 to condition the audio for caption generation.Experiments on two benchmark datasets, Clotho and AudioCaps, show that RECAPachieves competitive performance in in-domain settings and significantimprovements in out-of-domain settings. Additionally, due to its capability toexploit a large text-captions-only datastore in a \textit{training-free}fashion, RECAP shows unique capabilities of captioning novel audio events neverseen during training and compositional audios with multiple events. To promoteresearch in this space, we also release 150,000+ new weakly labeled captionsfor AudioSet, AudioCaps, and Clotho.</description><author>Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy Evuru, Ramani Duraiswami, Dinesh Manocha</author><pubDate>Mon, 18 Sep 2023 15:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09836v1</guid></item><item><title>Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits</title><link>http://arxiv.org/abs/2309.09832v1</link><description>Multi-task learning (MTL) aims to improve the performance of a primary taskby jointly learning with related auxiliary tasks. Traditional MTL methodsselect tasks randomly during training. However, both previous studies and ourresults suggest that such the random selection of tasks may not be helpful, andcan even be harmful to performance. Therefore, new strategies for taskselection and assignment in MTL need to be explored. This paper studies themulti-modal, multi-task dialogue act classification task, and proposes a methodfor selecting and assigning tasks based on non-stationary multi-armed bandits(MAB) with discounted Thompson Sampling (TS) using Gaussian priors. Ourexperimental results show that in different training stages, different taskshave different utility. Our proposed method can effectively identify the taskutility, actively avoid useless or harmful tasks, and realise the taskassignment during training. Our proposed method is significantly superior interms of UAR and F1 to the single-task and multi-task baselines with p-values &lt;0.05. Further analysis of experiments indicates that for the dataset with thedata imbalance problem, our proposed method has significantly higher stabilityand can obtain consistent and decent performance for minority classes. Ourproposed method is superior to the current state-of-the-art model.</description><author>Xiangheng He, Junjie Chen, Björn W. Schuller</author><pubDate>Mon, 18 Sep 2023 15:51:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09832v1</guid></item><item><title>AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities</title><link>http://arxiv.org/abs/2308.04992v2</link><description>Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., textand image) for a comprehensive understanding of entities. Despite the recentprogress of large-scale MMKGs, existing MMKGs neglect the multi-aspect natureof entities, limiting the ability to comprehend entities from variousperspectives. In this paper, we construct AspectMMKG, the first MMKG withaspect-related images by matching images to different entity aspects.Specifically, we collect aspect-related images from a knowledge base, andfurther extract aspect-related sentences from the knowledge base as queries toretrieve a large number of aspect-related images via an online image searchengine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and645,383 aspect-related images. We demonstrate the usability of AspectMMKG inentity aspect linking (EAL) downstream task and show that previous EAL modelsachieve a new state-of-the-art performance with the help of AspectMMKG. Tofacilitate the research on aspect-related MMKG, we further propose anaspect-related image retrieval (AIR) model, that aims to correct and expandaspect-related images in AspectMMKG. We train an AIR model to learn therelationship between entity image and entity aspect-related images byincorporating entity image, aspect, and aspect image information. Experimentalresults indicate that the AIR model could retrieve suitable images for a givenentity w.r.t different aspects.</description><author>Jingdan Zhang, Jiaan Wang, Xiaodan Wang, Zhixu Li, Yanghua Xiao</author><pubDate>Mon, 18 Sep 2023 15:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04992v2</guid></item><item><title>Pivotal Estimation of Linear Discriminant Analysis in High Dimensions</title><link>http://arxiv.org/abs/2309.09831v1</link><description>We consider the linear discriminant analysis problem in the high-dimensionalsettings. In this work, we propose PANDA(PivotAl liNear Discriminant Analysis),a tuning-insensitive method in the sense that it requires very little effort totune the parameters. Moreover, we prove that PANDA achieves the optimalconvergence rate in terms of both the estimation error and misclassificationrate. Our theoretical results are backed up by thorough numerical studies usingboth simulated and real datasets. In comparison with the existing methods, weobserve that our proposed PANDA yields equal or better performance, andrequires substantially less effort in parameter tuning.</description><author>Ethan X. Fang, Yajun Mei, Yuyang Shi, Qunzhi Xu, Tuo Zhao</author><pubDate>Mon, 18 Sep 2023 15:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09831v1</guid></item><item><title>Clustering of Urban Traffic Patterns by K-Means and Dynamic Time Warping: Case Study</title><link>http://arxiv.org/abs/2309.09830v1</link><description>Clustering of urban traffic patterns is an essential task in many differentareas of traffic management and planning. In this paper, two significantapplications in the clustering of urban traffic patterns are described. Thefirst application estimates the missing speed values using the speed of roadsegments with similar traffic patterns to colorify map tiles. The second one isthe estimation of essential road segments for generating addresses for a localpoint on the map, using the similarity patterns of different road segments. Thespeed time series extracts the traffic pattern in different road segments. Inthis paper, we proposed the time series clustering algorithm based on K-Meansand Dynamic Time Warping. The case study of our proposed algorithm is based onthe Snapp application's driver speed time series data. The results of the twoapplications illustrate that the proposed method can extract similar urbantraffic patterns.</description><author>Sadegh Etemad, Raziyeh Mosayebi, Tadeh Alexani Khodavirdian, Elahe Dastan, Amir Salari Telmadarreh, Mohammadreza Jafari, Sepehr Rafiei</author><pubDate>Mon, 18 Sep 2023 15:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09830v1</guid></item><item><title>Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding</title><link>http://arxiv.org/abs/2309.09826v1</link><description>Auto-completing code enables developers to speed up coding significantly.Recent advances in transformer-based large language model (LLM) technologieshave been applied to code synthesis. However, studies show that many of suchsynthesized codes contain vulnerabilities. We propose a novelvulnerability-constrained decoding approach to reduce the amount of vulnerablecode generated by such models. Using a small dataset of labeled vulnerablelines of code, we fine-tune an LLM to include vulnerability labels whengenerating code, acting as an embedded classifier. Then, during decoding, wedeny the model to generate these labels to avoid generating vulnerable code. Toevaluate the method, we chose to automatically complete Ethereum Blockchainsmart contracts (SCs) as the case study due to the strict requirements of SCsecurity. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuningtook more than one week using ten GPUs. The results showed that our fine-tunedmodel could synthesize SCs with an average BLEU (BiLingual EvaluationUnderstudy) score of 0.557. However, many codes in the auto-completed SCs werevulnerable. Using the code before the vulnerable line of 176 SCs containingdifferent types of vulnerabilities to auto-complete the code, we found thatmore than 70% of the auto-completed codes were insecure. Thus, we furtherfine-tuned the model on other 941 vulnerable SCs containing the same types ofvulnerabilities and applied vulnerability-constrained decoding. The fine-tuningtook only one hour with four GPUs. We then auto-completed the 176 SCs again andfound that our approach could identify 62% of the code to be generated asvulnerable and avoid generating 67% of them, indicating the approach couldefficiently and effectively avoid vulnerabilities in the auto-completed code.</description><author>André Storhaug, Jingyue Li, Tianyuan Hu</author><pubDate>Mon, 18 Sep 2023 15:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09826v1</guid></item><item><title>Bias of AI-Generated Content: An Examination of News Produced by Large Language Models</title><link>http://arxiv.org/abs/2309.09825v1</link><description>Large language models (LLMs) have the potential to transform our lives andwork through the content they generate, known as AI-Generated Content (AIGC).To harness this transformation, we need to understand the limitations of LLMs.Here, we investigate the bias of AIGC produced by seven representative LLMs,including ChatGPT and LLaMA. We collect news articles from The New York Timesand Reuters, both known for delivering relatively unbiased news. We then applyeach examined LLM to generate news content with headlines of these newsarticles as prompts, and evaluate the gender and racial biases of the AIGCproduced by the LLM by comparing the AIGC and the original news articles. Wefurther analyze the gender bias of each LLM under biased prompts by addinggender-biased messages to prompts constructed from these news headlines. Ourstudy reveals that the AIGC produced by each examined LLM demonstratessubstantial gender and racial biases. Moreover, the AIGC generated by each LLMexhibits notable discrimination against females and individuals of the Blackrace. Among the LLMs, the AIGC generated by ChatGPT demonstrates the lowestlevel of bias, and ChatGPT is the sole model capable of declining contentgeneration when provided with biased prompts.</description><author>Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, Xiaohang Zhao</author><pubDate>Mon, 18 Sep 2023 15:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09825v1</guid></item><item><title>NOVIS: A Case for End-to-End Near-Online Video Instance Segmentation</title><link>http://arxiv.org/abs/2308.15266v2</link><description>Until recently, the Video Instance Segmentation (VIS) community operatedunder the common belief that offline methods are generally superior to a frameby frame online processing. However, the recent success of online methodsquestions this belief, in particular, for challenging and long video sequences.We understand this work as a rebuttal of those recent observations and anappeal to the community to focus on dedicated near-online VIS approaches. Tosupport our argument, we present a detailed analysis on different processingparadigms and the new end-to-end trainable NOVIS (Near-Online Video InstanceSegmentation) method. Our transformer-based model directly predictsspatio-temporal mask volumes for clips of frames and performs instance trackingbetween clips via overlap embeddings. NOVIS represents the first near-onlineVIS approach which avoids any handcrafted tracking heuristics. We outperformall existing VIS methods by large margins and provide new state-of-the-artresults on both YouTube-VIS (2019/2021) and the OVIS benchmarks.</description><author>Tim Meinhardt, Matt Feiszli, Yuchen Fan, Laura Leal-Taixe, Rakesh Ranjan</author><pubDate>Mon, 18 Sep 2023 15:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15266v2</guid></item><item><title>Over-The-Air Federated Learning Over Scalable Cell-free Massive MIMO</title><link>http://arxiv.org/abs/2212.06482v3</link><description>Cell-free massive MIMO is emerging as a promising technology for futurewireless communication systems, which is expected to offer uniform coverage andhigh spectral efficiency compared to classical cellular systems. We study inthis paper how cell-free massive MIMO can support federated edge learning.Taking advantage of the additive nature of the wireless multiple accesschannel, over-the-air computation is exploited, where the clients send theirlocal updates simultaneously over the same communication resource. Thisapproach, known as over-the-air federated learning (OTA-FL), is proven toalleviate the communication overhead of federated learning over wirelessnetworks. Considering channel correlation and only imperfect channel stateinformation available at the central server, we propose a practicalimplementation of OTA-FL over cell-free massive MIMO. The convergence of theproposed implementation is studied analytically and experimentally, confirmingthe benefits of cell-free massive MIMO for OTA-FL.</description><author>Houssem Sifaou, Geoffrey Ye Li</author><pubDate>Mon, 18 Sep 2023 15:43:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.06482v3</guid></item><item><title>Grasp-Anything: Large-scale Grasp Dataset from Foundation Models</title><link>http://arxiv.org/abs/2309.09818v1</link><description>Foundation models such as ChatGPT have made significant strides in robotictasks due to their universal representation of real-world domains. In thispaper, we leverage foundation models to tackle grasp detection, a persistentchallenge in robotics with broad industrial applications. Despite numerousgrasp datasets, their object diversity remains limited compared to real-worldfigures. Fortunately, foundation models possess an extensive repository ofreal-world knowledge, including objects we encounter in our daily lives. As aconsequence, a promising solution to the limited representation in previousgrasp datasets is to harness the universal knowledge embedded in thesefoundation models. We present Grasp-Anything, a new large-scale grasp datasetsynthesized from foundation models to implement this solution. Grasp-Anythingexcels in diversity and magnitude, boasting 1M samples with text descriptionsand more than 3M objects, surpassing prior datasets. Empirically, we show thatGrasp-Anything successfully facilitates zero-shot grasp detection onvision-based tasks and real-world robotic experiments. Our dataset and code areavailable at https://grasp-anything-2023.github.io.</description><author>An Dinh Vuong, Minh Nhat Vu, Hieu Le, Baoru Huang, Binh Huynh, Thieu Vo, Andreas Kugi, Anh Nguyen</author><pubDate>Mon, 18 Sep 2023 15:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09818v1</guid></item><item><title>Fairness in Visual Clustering: A Novel Transformer Clustering Approach</title><link>http://arxiv.org/abs/2304.07408v2</link><description>Promoting fairness for deep clustering models in unsupervised clusteringsettings to reduce demographic bias is a challenging goal. This is because ofthe limitation of large-scale balanced data with well-annotated labels forsensitive or protected attributes. In this paper, we first evaluate demographicbias in deep clustering models from the perspective of cluster purity, which ismeasured by the ratio of positive samples within a cluster to their correlationdegree. This measurement is adopted as an indication of demographic bias. Then,a novel loss function is introduced to encourage a purity consistency for allclusters to maintain the fairness aspect of the learned clustering model.Moreover, we present a novel attention mechanism, Cross-attention, to measurecorrelations between multiple clusters, strengthening faraway positive samplesand improving the purity of clusters during the learning process. Experimentalresults on a large-scale dataset with numerous attribute settings havedemonstrated the effectiveness of the proposed approach on both clusteringaccuracy and fairness enhancement on several sensitive attributes.</description><author>Xuan-Bac Nguyen, Chi Nhan Duong, Marios Savvides, Kaushik Roy, Hugh Churchill, Khoa Luu</author><pubDate>Mon, 18 Sep 2023 15:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07408v2</guid></item><item><title>When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm</title><link>http://arxiv.org/abs/2306.02552v2</link><description>User behavior analysis is crucial in human-centered AI applications. In thisfield, the collection of sufficient and high-quality user behavior data hasalways been a fundamental yet challenging problem. An intuitive idea to addressthis problem is automatically simulating the user behaviors. However, due tothe subjective and complex nature of human cognitive processes, reliablysimulating the user behavior is difficult. Recently, large language models(LLM) have obtained remarkable successes, showing great potential to achievehuman-like intelligence. We argue that these models present significantopportunities for reliable user simulation, and have the potential torevolutionize traditional study paradigms in user behavior analysis. In thispaper, we take recommender system as an example to explore the potential ofusing LLM for user simulation. Specifically, we regard each user as anLLM-based autonomous agent, and let different agents freely communicate, behaveand evolve in a virtual simulator called RecAgent. For comprehensivelysimulation, we not only consider the behaviors within the recommender system(\emph{e.g.}, item browsing and clicking), but also accounts for externalinfluential factors, such as, friend chatting and social advertisement. Oursimulator contains at most 1000 agents, and each agent is composed of aprofiling module, a memory module and an action module, enabling it to behaveconsistently, reasonably and reliably. In addition, to more flexibly operateour simulator, we also design two global functions including real-human playingand system intervention. To evaluate the effectiveness of our simulator, weconduct extensive experiments from both agent and system perspectives. In orderto advance this direction, we have released our project at{https://github.com/RUC-GSAI/YuLan-Rec}.</description><author>Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, Ji-Rong Wen</author><pubDate>Mon, 18 Sep 2023 15:36:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02552v2</guid></item><item><title>Convolutional Deep Kernel Machines</title><link>http://arxiv.org/abs/2309.09814v1</link><description>Deep kernel machines (DKMs) are a recently introduced kernel method with theflexibility of other deep models including deep NNs and deep Gaussianprocesses. DKMs work purely with kernels, never with features, and aretherefore different from other methods ranging from NNs to deep kernel learningand even deep Gaussian processes, which all use features as a fundamentalcomponent. Here, we introduce convolutional DKMs, along with an efficientinter-domain inducing point approximation scheme. Further, we develop andexperimentally assess a number of model variants, including 9 different typesof normalisation designed for the convolutional DKMs, two likelihoods, and twodifferent types of top-layer. The resulting models achieve around 99% testaccuracy on MNIST, 92% on CIFAR-10 and 71% on CIFAR-100, despite training inonly around 28 GPU hours, 1-2 orders of magnitude faster than full NNGP / NTK /Myrtle kernels, whilst achieving comparable performance.</description><author>Edward Milsom, Ben Anson, Laurence Aitchison</author><pubDate>Mon, 18 Sep 2023 15:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09814v1</guid></item><item><title>R2GenGPT: Radiology Report Generation with Frozen LLMs</title><link>http://arxiv.org/abs/2309.09812v1</link><description>Large Language Models (LLMs) have consistently showcased remarkablegeneralization capabilities when applied to various language tasks.Nonetheless, harnessing the full potential of LLMs for Radiology ReportGeneration (R2Gen) still presents a challenge, stemming from the inherentdisparity in modality between LLMs and the R2Gen task. To bridge this gapeffectively, we propose R2GenGPT, which is a novel solution that aligns visualfeatures with the word embedding space of LLMs using an efficient visualalignment module. This innovative approach empowers the previously static LLMto seamlessly integrate and process image information, marking a step forwardin optimizing R2Gen performance. R2GenGPT offers the following benefits. First,it attains state-of-the-art (SOTA) performance by training only the lightweightvisual alignment module while freezing all the parameters of LLM. Second, itexhibits high training efficiency, as it requires the training of anexceptionally minimal number of parameters while achieving rapid convergence.By employing delta tuning, our model only trains 5M parameters (whichconstitute just 0.07\% of the total parameter count) to achieve performanceclose to the SOTA levels. Our code is available athttps://github.com/wang-zhanyu/R2GenGPT.</description><author>Zhanyu Wang, Lingqiao Liu, Lei Wang, Luping Zhou</author><pubDate>Mon, 18 Sep 2023 15:35:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09812v1</guid></item><item><title>Stop overkilling simple tasks with black-box models and use transparent models instead</title><link>http://arxiv.org/abs/2302.02804v3</link><description>In recent years, the employment of deep learning methods has led to severalsignificant breakthroughs in artificial intelligence. Different fromtraditional machine learning models, deep learning-based approaches are able toextract features autonomously from raw data. This allows for bypassing thefeature engineering process, which is generally considered to be botherror-prone and tedious. Moreover, deep learning strategies often outperformtraditional models in terms of accuracy.</description><author>Matteo Rizzo, Matteo Marcuzzo, Alessandro Zangari, Andrea Gasparetto, Andrea Albarelli</author><pubDate>Mon, 18 Sep 2023 15:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02804v3</guid></item><item><title>VisualProg Distiller: Learning to Fine-tune Non-differentiable Visual Programming Frameworks</title><link>http://arxiv.org/abs/2309.09809v1</link><description>As an interpretable and universal neuro-symbolic paradigm based on LargeLanguage Models, visual programming (VisualProg) can execute compositionalvisual tasks without training, but its performance is markedly inferiorcompared to task-specific supervised learning models. To increase itspracticality, the performance of VisualProg on specific tasks needs to beimproved. However, the non-differentiability of VisualProg limits thepossibility of employing the fine-tuning strategy on specific tasks to achievefurther improvements. In our analysis, we discovered that significantperformance issues in VisualProg's execution originated from errors made by thesub-modules at corresponding visual sub-task steps. To address this, we propose``VisualProg Distiller", a method of supplementing and distilling processknowledge to optimize the performance of each VisualProg sub-module ondecoupled visual sub-tasks, thus enhancing the overall task performance.Specifically, we choose an end-to-end model that is well-performed on the giventask as the teacher and further distill the knowledge of the teacher into theinvoked visual sub-modules step-by-step based on the execution flow of theVisualProg-generated programs. In this way, our method is capable offacilitating the fine-tuning of the non-differentiable VisualProg frameworkseffectively. Extensive and comprehensive experimental evaluations demonstratethat our method can achieve a substantial performance improvement ofVisualProg, and outperforms all the compared state-of-the-art methods by largemargins. Furthermore, to provide valuable process supervision for the GQA task,we construct a large-scale dataset by utilizing the distillation process of ourmethod.</description><author>Wentao Wan, Zeqing Wang, Nan Kang, Keze Wang, Zhiyu Shen, Liang Lin</author><pubDate>Mon, 18 Sep 2023 15:28:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09809v1</guid></item><item><title>Efficient Concept Drift Handling for Batch Android Malware Detection Models</title><link>http://arxiv.org/abs/2309.09807v1</link><description>The rapidly evolving nature of Android apps poses a significant challenge tostatic batch machine learning algorithms employed in malware detection systems,as they quickly become obsolete. Despite this challenge, the existingliterature pays limited attention to addressing this issue, with many advancedAndroid malware detection approaches, such as Drebin, DroidDet and MaMaDroid,relying on static models. In this work, we show how retraining techniques areable to maintain detector capabilities over time. Particularly, we analyze theeffect of two aspects in the efficiency and performance of the detectors: 1)the frequency with which the models are retrained, and 2) the data used forretraining. In the first experiment, we compare periodic retraining with a moreadvanced concept drift detection method that triggers retraining only whennecessary. In the second experiment, we analyze sampling methods to reduce theamount of data used to retrain models. Specifically, we compare fixed sizedwindows of recent data and state-of-the-art active learning methods that selectthose apps that help keep the training dataset small but diverse. Ourexperiments show that concept drift detection and sample selection mechanismsresult in very efficient retraining strategies which can be successfully usedto maintain the performance of the static Android malware state-of-the-artdetectors in changing environments.</description><author>Molina-Coronado B., Mori U., Mendiburu A., Miguel-Alonso J</author><pubDate>Mon, 18 Sep 2023 15:28:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09807v1</guid></item><item><title>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models</title><link>http://arxiv.org/abs/2304.06364v2</link><description>Evaluating the general abilities of foundation models to tackle human-leveltasks is a vital aspect of their development and application in the pursuit ofArtificial General Intelligence (AGI). Traditional benchmarks, which rely onartificial datasets, may not accurately represent human-level capabilities. Inthis paper, we introduce AGIEval, a novel benchmark specifically designed toassess foundation model in the context of human-centric standardized exams,such as college entrance exams, law school admission tests, math competitions,and lawyer qualification tests. We evaluate several state-of-the-art foundationmodels, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and mathcompetitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%accuracy on the English test of the Chinese national college entrance exam.This demonstrates the extraordinary performance of contemporary foundationmodels. In contrast, we also find that GPT-4 is less proficient in tasks thatrequire complex reasoning or specific domain knowledge. Our comprehensiveanalyses of model capabilities (understanding, knowledge, reasoning, andcalculation) reveal these models' strengths and limitations, providing valuableinsights into future directions for enhancing their general capabilities. Byconcentrating on tasks pertinent to human cognition and decision-making, ourbenchmark delivers a more meaningful and robust evaluation of foundationmodels' performance in real-world scenarios. The data, code, and all modeloutputs are released in https://github.com/ruixiangcui/AGIEval.</description><author>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan</author><pubDate>Mon, 18 Sep 2023 15:23:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06364v2</guid></item><item><title>Learning Optimal Contracts: How to Exploit Small Action Spaces</title><link>http://arxiv.org/abs/2309.09801v1</link><description>We study principal-agent problems in which a principal commits to anoutcome-dependent payment scheme -- called contract -- in order to induce anagent to take a costly, unobservable action leading to favorable outcomes. Weconsider a generalization of the classical (single-round) version of theproblem in which the principal interacts with the agent by committing tocontracts over multiple rounds. The principal has no information about theagent, and they have to learn an optimal contract by only observing the outcomerealized at each round. We focus on settings in which the size of the agent'saction space is small. We design an algorithm that learns anapproximately-optimal contract with high probability in a number of roundspolynomial in the size of the outcome space, when the number of actions isconstant. Our algorithm solves an open problem by Zhu et al.[2022]. Moreover,it can also be employed to provide a $\tilde{\mathcal{O}}(T^{4/5})$ regretbound in the related online learning setting in which the principal aims atmaximizing their cumulative utility, thus considerably improvingpreviously-known regret bounds.</description><author>Francesco Bacchiocchi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti</author><pubDate>Mon, 18 Sep 2023 15:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09801v1</guid></item><item><title>AMuRD: Annotated Multilingual Receipts Dataset for Cross-lingual Key Information Extraction and Classification</title><link>http://arxiv.org/abs/2309.09800v1</link><description>Key information extraction involves recognizing and extracting text fromscanned receipts, enabling retrieval of essential content, and organizing itinto structured documents. This paper presents a novel multilingual dataset forreceipt extraction, addressing key challenges in information extraction anditem classification. The dataset comprises $47,720$ samples, includingannotations for item names, attributes like (price, brand, etc.), andclassification into $44$ product categories. We introduce the InstructLLaMAapproach, achieving an F1 score of $0.76$ and an accuracy of $0.68$ for keyinformation extraction and item classification. We provide code, datasets, andcheckpoints.\footnote{\url{https://github.com/Update-For-Integrated-Business-AI/AMuRD}}.</description><author>Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt</author><pubDate>Mon, 18 Sep 2023 15:18:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09800v1</guid></item><item><title>Watch the Speakers: A Hybrid Continuous Attribution Network for Emotion Recognition in Conversation With Emotion Disentanglement</title><link>http://arxiv.org/abs/2309.09799v1</link><description>Emotion Recognition in Conversation (ERC) has attracted widespread attentionin the natural language processing field due to its enormous potential forpractical applications. Existing ERC methods face challenges in achievinggeneralization to diverse scenarios due to insufficient modeling of context,ambiguous capture of dialogue relationships and overfitting in speakermodeling. In this work, we present a Hybrid Continuous Attributive Network(HCAN) to address these issues in the perspective of emotional continuation andemotional attribution. Specifically, HCAN adopts a hybrid recurrent andattention-based module to model global emotion continuity. Then a novelEmotional Attribution Encoding (EAE) is proposed to model intra- andinter-emotional attribution for each utterance. Moreover, aiming to enhance therobustness of the model in speaker modeling and improve its performance indifferent scenarios, A comprehensive loss function emotional cognitive loss$\mathcal{L}_{\rm EC}$ is proposed to alleviate emotional drift and overcomethe overfitting of the model to speaker modeling. Our model achievesstate-of-the-art performance on three datasets, demonstrating the superiorityof our work. Another extensive comparative experiments and ablation studies onthree benchmarks are conducted to provided evidence to support the efficacy ofeach module. Further exploration of generalization ability experiments showsthe plug-and-play nature of the EAE module in our method.</description><author>Shanglin Lei, Xiaoping Wang, Guanting Dong, Jiang Li, Yingjian Liu</author><pubDate>Mon, 18 Sep 2023 15:18:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09799v1</guid></item><item><title>Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks</title><link>http://arxiv.org/abs/2309.07937v2</link><description>We propose a decoder-only language model, \textit{VoxtLM}, that can performfour tasks: speech recognition, speech synthesis, text generation, and speechcontinuation. VoxtLM integrates text vocabulary with discrete speech tokensfrom self-supervised speech features and uses special tokens to enablemultitask learning. Compared to a single-task model, VoxtLM exhibits asignificant improvement in speech synthesis, with improvements in both speechintelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90.VoxtLM also improves speech generation and speech recognition performance overthe single-task counterpart. VoxtLM is trained with publicly available data andtraining recipes and model checkpoints will be open-sourced to make fullyreproducible work.</description><author>Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe</author><pubDate>Mon, 18 Sep 2023 15:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07937v2</guid></item><item><title>CRYPTO-MINE: Cryptanalysis via Mutual Information Neural Estimation</title><link>http://arxiv.org/abs/2309.08019v2</link><description>The use of Mutual Information (MI) as a measure to evaluate the efficiency ofcryptosystems has an extensive history. However, estimating MI between unknownrandom variables in a high-dimensional space is challenging. Recent advances inmachine learning have enabled progress in estimating MI using neural networks.This work presents a novel application of MI estimation in the field ofcryptography. We propose applying this methodology directly to estimate the MIbetween plaintext and ciphertext in a chosen plaintext attack. The leakedinformation, if any, from the encryption could potentially be exploited byadversaries to compromise the computational security of the cryptosystem. Weevaluate the efficiency of our approach by empirically analyzing multipleencryption schemes and baseline approaches. Furthermore, we extend the analysisto novel network coding-based cryptosystems that provide individual secrecy andstudy the relationship between information leakage and input distribution.</description><author>Benjamin D. Kim, Vipindev Adat Vasudevan, Jongchan Woo, Alejandro Cohen, Rafael G. L. D'Oliveira, Thomas Stahlbuhk, Muriel Médard</author><pubDate>Mon, 18 Sep 2023 15:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08019v2</guid></item><item><title>Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction</title><link>http://arxiv.org/abs/2309.06584v3</link><description>Alzheimer's disease and related dementias (ADRD) ranks as the sixth leadingcause of death in the US, underlining the importance of accurate ADRD riskprediction. While recent advancement in ADRD risk prediction have primarilyrelied on imaging analysis, yet not all patients undergo medical imaging beforean ADRD diagnosis. Merging machine learning with claims data can revealadditional risk factors and uncover interconnections among diverse medicalcodes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data forADRD risk prediction. Addressing the lack of human-interpretable reasons behindthese predictions, we introduce an innovative method to evaluate relationshipimportance and its influence on ADRD risk prediction, ensuring comprehensiveinterpretation. We employed Variationally Regularized Encoder-decoder Graph Neural Network(VGNN) for estimating ADRD likelihood. We created three scenarios to assess themodel's efficiency, using Random Forest and Light Gradient Boost Machine asbaselines. We further used our relation importance method to clarify the keyrelationships for ADRD risk prediction. VGNN surpassed other baseline models by10% in the area under the receiver operating characteristic. The integration ofthe GNN model and relation importance interpretation could potentially play anessential role in providing valuable insight into factors that may contributeto or delay ADRD progression. Employing a GNN approach with claims data enhances ADRD risk prediction andprovides insights into the impact of interconnected medical code relationships.This methodology not only enables ADRD risk modeling but also shows potentialfor other image analysis predictions using claims data.</description><author>Xinyue Hu, Zenan Sun, Yi Nian, Yifang Dang, Fang Li, Jingna Feng, Evan Yu, Cui Tao</author><pubDate>Mon, 18 Sep 2023 15:10:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06584v3</guid></item><item><title>Harnessing Collective Intelligence Under a Lack of Cultural Consensus</title><link>http://arxiv.org/abs/2309.09787v1</link><description>Harnessing collective intelligence to drive effective decision-making andcollaboration benefits from the ability to detect and characterizeheterogeneity in consensus beliefs. This is particularly true in domains suchas technology acceptance or leadership perception, where a consensus defines anintersubjective truth, leading to the possibility of multiple "ground truths"when subsets of respondents sustain mutually incompatible consensuses. CulturalConsensus Theory (CCT) provides a statistical framework for detecting andcharacterizing these divergent consensus beliefs. However, it is unworkable inmodern applications because it lacks the ability to generalize across evenhighly similar beliefs, is ineffective with sparse data, and can leverageneither external knowledge bases nor learned machine representations. Here, weovercome these limitations through Infinite Deep Latent Construct CulturalConsensus Theory (iDLC-CCT), a nonparametric Bayesian model that extends CCTwith a latent construct that maps between pretrained deep neural networkembeddings of entities and the consensus beliefs regarding those entities amongone or more subsets of respondents. We validate the method across domainsincluding perceptions of risk sources, food healthiness, leadership, firstimpressions, and humor. We find that iDLC-CCT better predicts the degree ofconsensus, generalizes well to out-of-sample entities, and is effective evenwith sparse data. To improve scalability, we introduce an efficienthard-clustering variant of the iDLC-CCT using an algorithm derived from asmall-variance asymptotic analysis of the model. The iDLC-CCT, therefore,provides a workable computational foundation for harnessing collectiveintelligence under a lack of cultural consensus and may potentially form thebasis of consensus-aware information technologies.</description><author>Necdet Gürkan, Jordan W. Suchow</author><pubDate>Mon, 18 Sep 2023 15:05:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09787v1</guid></item><item><title>Fixed Integral Neural Networks</title><link>http://arxiv.org/abs/2307.14439v2</link><description>It is often useful to perform integration over learned functions representedby neural networks. However, this integration is usually performed numerically,as analytical integration over learned functions (especially neural networks)is generally viewed as intractable. In this work, we present a method forrepresenting the analytical integral of a learned function $f$. This allows theexact integral of a neural network to be computed, and enables constrainedneural networks to be parametrised by applying constraints directly to theintegral. Crucially, we also introduce a method to constrain $f$ to bepositive, a necessary condition for many applications (e.g. probabilitydistributions, distance metrics, etc). Finally, we introduce severalapplications where our fixed-integral neural network (FINN) can be utilised.</description><author>Ryan Kortvelesy</author><pubDate>Mon, 18 Sep 2023 15:03:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14439v2</guid></item><item><title>Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II)</title><link>http://arxiv.org/abs/2112.08581v6</link><description>The non-dominated sorting genetic algorithm II (NSGA-II) is the mostintensively used multi-objective evolutionary algorithm (MOEA) in real-worldapplications. However, in contrast to several simple MOEAs analyzed also viamathematical means, no such study exists for the NSGA-II so far. In this work,we show that mathematical runtime analyses are feasible also for the NSGA-II.As particular results, we prove that with a population size four times largerthan the size of the Pareto front, the NSGA-II with two classic mutationoperators and four different ways to select the parents satisfies the sameasymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basicOneMinMax and LeadingOnesTrailingZeros benchmarks. However, if the populationsize is only equal to the size of the Pareto front, then the NSGA-II cannotefficiently compute the full Pareto front: for an exponential number ofiterations, the population will always miss a constant fraction of the Paretofront. Our experiments confirm the above findings.</description><author>Weijie Zheng, Benjamin Doerr</author><pubDate>Mon, 18 Sep 2023 15:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.08581v6</guid></item><item><title>The ParlaSent multilingual training dataset for sentiment identification in parliamentary proceedings</title><link>http://arxiv.org/abs/2309.09783v1</link><description>Sentiments inherently drive politics. How we receive and process informationplays an essential role in political decision-making, shaping our judgment withstrategic consequences both on the level of legislators and the masses. Ifsentiment plays such an important role in politics, how can we study andmeasure it systematically? The paper presents a new dataset ofsentiment-annotated sentences, which are used in a series of experimentsfocused on training a robust sentiment classifier for parliamentaryproceedings. The paper also introduces the first domain-specific LLM forpolitical science applications additionally pre-trained on 1.72 billiondomain-specific words from proceedings of 27 European parliaments. We presentexperiments demonstrating how the additional pre-training of LLM onparliamentary data can significantly improve the model downstream performanceon the domain-specific tasks, in our case, sentiment detection in parliamentaryproceedings. We further show that multilingual models perform very well onunseen languages and that additional data from other languages significantlyimproves the target parliament's results. The paper makes an importantcontribution to multiple domains of social sciences and bridges them withcomputer science and computational linguistics. Lastly, it sets up a morerobust approach to sentiment analysis of political texts in general, whichallows scholars to study political sentiment from a comparative perspectiveusing standardized tools and techniques.</description><author>Michal Mochtak, Peter Rupnik, Nikola Ljubešić</author><pubDate>Mon, 18 Sep 2023 15:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09783v1</guid></item><item><title>DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving</title><link>http://arxiv.org/abs/2309.09777v1</link><description>World models, especially in autonomous driving, are trending and drawingextensive attention due to their capacity for comprehending drivingenvironments. The established world model holds immense potential for thegeneration of high-quality driving videos, and driving policies for safemaneuvering. However, a critical limitation in relevant research lies in itspredominant focus on gaming environments or simulated settings, thereby lackingthe representation of real-world driving scenarios. Therefore, we introduceDriveDreamer, a pioneering world model entirely derived from real-world drivingscenarios. Regarding that modeling the world in intricate driving scenesentails an overwhelming search space, we propose harnessing the powerfuldiffusion model to construct a comprehensive representation of the complexenvironment. Furthermore, we introduce a two-stage training pipeline. In theinitial phase, DriveDreamer acquires a deep understanding of structured trafficconstraints, while the subsequent stage equips it with the ability toanticipate future states. The proposed DriveDreamer is the first world modelestablished from real-world driving scenarios. We instantiate DriveDreamer onthe challenging nuScenes benchmark, and extensive experiments verify thatDriveDreamer empowers precise, controllable video generation that faithfullycaptures the structural constraints of real-world traffic scenarios.Additionally, DriveDreamer enables the generation of realistic and reasonabledriving policies, opening avenues for interaction and practical applications.</description><author>Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiwen Lu</author><pubDate>Mon, 18 Sep 2023 14:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09777v1</guid></item><item><title>Towards Self-Adaptive Pseudo-Label Filtering for Semi-Supervised Learning</title><link>http://arxiv.org/abs/2309.09774v1</link><description>Recent semi-supervised learning (SSL) methods typically include a filteringstrategy to improve the quality of pseudo labels. However, these filteringstrategies are usually hand-crafted and do not change as the model is updated,resulting in a lot of correct pseudo labels being discarded and incorrectpseudo labels being selected during the training process. In this work, weobserve that the distribution gap between the confidence values of correct andincorrect pseudo labels emerges at the very beginning of the training, whichcan be utilized to filter pseudo labels. Based on this observation, we proposea Self-Adaptive Pseudo-Label Filter (SPF), which automatically filters noise inpseudo labels in accordance with model evolvement by modeling the confidencedistribution throughout the training process. Specifically, with an onlinemixture model, we weight each pseudo-labeled sample by the posterior of itbeing correct, which takes into consideration the confidence distribution atthat time. Unlike previous handcrafted filters, our SPF evolves together withthe deep neural network without manual tuning. Extensive experimentsdemonstrate that incorporating SPF into the existing SSL methods can helpimprove the performance of SSL, especially when the labeled data is extremelyscarce.</description><author>Lei Zhu, Zhanghan Ke, Rynson Lau</author><pubDate>Mon, 18 Sep 2023 14:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09774v1</guid></item><item><title>Difficult Lessons on Social Prediction from Wisconsin Public Schools</title><link>http://arxiv.org/abs/2304.06205v2</link><description>Early warning systems (EWS) are predictive tools at the center of recentefforts to improve graduation rates in public schools across the United States.These systems assist in targeting interventions to individual students bypredicting which students are at risk of dropping out. Despite significantinvestments in their widespread adoption, there remain large gaps in ourunderstanding of the efficacy of EWS, and the role of statistical risk scoresin education. In this work, we draw on nearly a decade's worth of data from a system usedthroughout Wisconsin to provide the first large-scale evaluation of thelong-term impact of EWS on graduation outcomes. We present empirical evidencethat the prediction system accurately sorts students by their dropout risk. Wealso find that it may have caused a single-digit percentage increase ingraduation rates, though our empirical analyses cannot reliably rule out thatthere has been no positive treatment effect. Going beyond a retrospective evaluation of DEWS, we draw attention to acentral question at the heart of the use of EWS: Are individual risk scoresnecessary for effectively targeting interventions? We propose a simplemechanism that only uses information about students' environments -- such astheir schools, and districts -- and argue that this mechanism can targetinterventions just as efficiently as the individual risk score-based mechanism.Our argument holds even if individual predictions are highly accurate andeffective interventions exist. In addition to motivating this simple targetingmechanism, our work provides a novel empirical backbone for the robustqualitative understanding among education researchers that dropout isstructurally determined. Combined, our insights call into question the marginalvalue of individual predictions in settings where outcomes are driven by highlevels of inequality.</description><author>Juan C. Perdomo, Tolani Britton, Moritz Hardt, Rediet Abebe</author><pubDate>Mon, 18 Sep 2023 14:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06205v2</guid></item><item><title>Semantically Redundant Training Data Removal and Deep Model Classification Performance: A Study with Chest X-rays</title><link>http://arxiv.org/abs/2309.09773v1</link><description>Deep learning (DL) has demonstrated its innate capacity to independentlylearn hierarchical features from complex and multi-dimensional data. A commonunderstanding is that its performance scales up with the amount of trainingdata. Another data attribute is the inherent variety. It follows, therefore,that semantic redundancy, which is the presence of similar or repetitiveinformation, would tend to lower performance and limit generalizability tounseen data. In medical imaging data, semantic redundancy can occur due to thepresence of multiple images that have highly similar presentations for thedisease of interest. Further, the common use of augmentation methods togenerate variety in DL training may be limiting performance when applied tosemantically redundant data. We propose an entropy-based sample scoringapproach to identify and remove semantically redundant training data. Wedemonstrate using the publicly available NIH chest X-ray dataset that the modeltrained on the resulting informative subset of training data significantlyoutperforms the model trained on the full training set, during both internal(recall: 0.7164 vs 0.6597, p&lt;0.05) and external testing (recall: 0.3185 vs0.2589, p&lt;0.05). Our findings emphasize the importance of information-orientedtraining sample selection as opposed to the conventional practice of using allavailable training data.</description><author>Sivaramakrishnan Rajaraman, Ghada Zamzmi, Feng Yang, Zhaohui Liang, Zhiyun Xue, Sameer Antani</author><pubDate>Mon, 18 Sep 2023 14:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09773v1</guid></item><item><title>How to Data in Datathons</title><link>http://arxiv.org/abs/2309.09770v1</link><description>The rise of datathons, also known as data or data science hackathons, hasprovided a platform to collaborate, learn, and innovate in a short timeframe.Despite their significant potential benefits, organizations often struggle toeffectively work with data due to a lack of clear guidelines and best practicesfor potential issues that might arise. Drawing on our own experiences andinsights from organizing &gt;80 datathon challenges with &gt;60 partnershiporganizations since 2016, we provide guidelines and recommendations that serveas a resource for organizers to navigate the data-related complexities ofdatathons. We apply our proposed framework to 10 case studies.</description><author>Carlos Mougan, Richard Plant, Clare Teng, Marya Bazzi, Alvaro Cabregas Ejea, Ryan Sze-Yin Chan, David Salvador Jasin, Martin Stoffel, Kirstie Jane Whitaker, Jules Manser</author><pubDate>Mon, 18 Sep 2023 14:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09770v1</guid></item><item><title>Localization-Guided Track: A Deep Association Multi-Object Tracking Framework Based on Localization Confidence of Detections</title><link>http://arxiv.org/abs/2309.09765v1</link><description>In currently available literature, no tracking-by-detection (TBD)paradigm-based tracking method has considered the localization confidence ofdetection boxes. In most TBD-based methods, it is considered that objects oflow detection confidence are highly occluded and thus it is a normal practiceto directly disregard such objects or to reduce their priority in matching. Inaddition, appearance similarity is not a factor to consider for matching theseobjects. However, in terms of the detection confidence fusing classificationand localization, objects of low detection confidence may have inaccuratelocalization but clear appearance; similarly, objects of high detectionconfidence may have inaccurate localization or unclear appearance; yet theseobjects are not further classified. In view of these issues, we proposeLocalization-Guided Track (LG-Track). Firstly, localization confidence isapplied in MOT for the first time, with appearance clarity and localizationaccuracy of detection boxes taken into account, and an effective deepassociation mechanism is designed; secondly, based on the classificationconfidence and localization confidence, a more appropriate cost matrix can beselected and used; finally, extensive experiments have been conducted on MOT17and MOT20 datasets. The results show that our proposed method outperforms thecompared state-of-art tracking methods. For the benefit of the community, ourcode has been made publicly at https://github.com/mengting2023/LG-Track.</description><author>Ting Meng, Chunyun Fu, Mingguang Huang, Xiyang Wang, Jiawei He, Tao Huang, Wankai Shi</author><pubDate>Mon, 18 Sep 2023 14:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09765v1</guid></item></channel></rss>