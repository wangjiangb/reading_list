<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 03 Jul 2024 06:00:14 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</title><link>http://arxiv.org/abs/2407.02490v1</link><description>The computational challenges of Large Language Model (LLM) inference remain asignificant barrier to their widespread deployment, especially as promptlengths continue to increase. Due to the quadratic complexity of the attentioncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens(i.e., the pre-filling stage) on a single A100 GPU. Existing methods forspeeding up prefilling often fail to maintain acceptable accuracy or efficiencywhen applied to long-context LLMs. To address this gap, we introduce MInference(Milliontokens Inference), a sparse calculation method designed to acceleratepre-filling of long-sequence processing. Specifically, we identify three uniquepatterns in long-context attention matrices-the A-shape, Vertical-Slash, andBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. Wedetermine the optimal pattern for each attention head offline and dynamicallybuild sparse indices based on the assigned pattern during inference. With thepattern and sparse indices, we perform efficient sparse attention calculationsvia our optimized GPU kernels to significantly reduce the latency in thepre-filling stage of long-context LLMs. Our proposed technique can be directlyapplied to existing LLMs without any modifications to the pre-training setup oradditional fine-tuning. By evaluating on a wide range of downstream tasks,including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and modelsincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, wedemonstrate that MInference effectively reduces inference latency by up to 10xfor pre-filling on an A100, while maintaining accuracy. Our code is availableat https://aka.ms/MInference.</description><author>Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu</author><pubDate>Tue, 02 Jul 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02490v1</guid></item><item><title>Magic Insert: Style-Aware Drag-and-Drop</title><link>http://arxiv.org/abs/2407.02489v1</link><description>We present Magic Insert, a method for dragging-and-dropping subjects from auser-provided image into a target image of a different style in a physicallyplausible manner while matching the style of the target image. This workformalizes the problem of style-aware drag-and-drop and presents a method fortackling it by addressing two sub-problems: style-aware personalization andrealistic object insertion in stylized images. For style-aware personalization,our method first fine-tunes a pretrained text-to-image diffusion model usingLoRA and learned text tokens on the subject image, and then infuses it with aCLIP representation of the target style. For object insertion, we useBootstrapped Domain Adaption to adapt a domain-specific photorealistic objectinsertion model to the domain of diverse artistic styles. Overall, the methodsignificantly outperforms traditional approaches such as inpainting. Finally,we present a dataset, SubjectPlop, to facilitate evaluation and future progressin this area. Project page: https://magicinsert.github.io/</description><author>Nataniel Ruiz, Yuanzhen Li, Neal Wadhwa, Yael Pritch, Michael Rubinstein, David E. Jacobs, Shlomi Fruchter</author><pubDate>Tue, 02 Jul 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02489v1</guid></item><item><title>Neurocache: Efficient Vector Retrieval for Long-range Language Modeling</title><link>http://arxiv.org/abs/2407.02486v1</link><description>This paper introduces Neurocache, an approach to extend the effective contextsize of large language models (LLMs) using an external vector cache to storeits past states. Like recent vector retrieval approaches, Neurocache uses anefficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past statesand incorporate them into the attention process. Neurocache improves uponprevious methods by (1) storing compressed states, which reduces cache size;(2) performing a single retrieval operation per token which increases inferencespeed; and (3) extending the retrieval window to neighboring states, whichimproves both language modeling and downstream task accuracy. Our experimentsshow the effectiveness of Neurocache both for models trained from scratch andfor pre-trained models such as Llama2-7B and Mistral-7B when enhanced with thecache mechanism. We also compare Neurocache with text retrieval methods andshow improvements in single-document question-answering and few-shot learningtasks. We made the source code available under:https://github.com/alisafaya/neurocache</description><author>Ali Safaya, Deniz Yuret</author><pubDate>Tue, 02 Jul 2024 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02486v1</guid></item><item><title>RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs</title><link>http://arxiv.org/abs/2407.02485v1</link><description>Large language models (LLMs) typically utilize the top-k contexts from aretriever in retrieval-augmented generation (RAG). In this work, we propose anovel instruction fine-tuning framework RankRAG, which instruction-tunes asingle LLM for the dual purpose of context ranking and answer generation inRAG. In particular, the instruction-tuned LLMs work surprisingly well by addinga small fraction of ranking data into the training blend, and outperformexisting expert ranking models, including the same LLM exclusively fine-tunedon a large amount of ranking data. For generation, we compare our model withmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, andChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAGbenchmarks. Specifically, our Llama3-RankRAG significantly outperformsLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. Inaddition, it also performs comparably to GPT-4 on five RAG benchmarks in thebiomedical domain without instruction fine-tuning on biomedical data,demonstrating its superb capability for generalization to new domains.</description><author>Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, Bryan Catanzaro</author><pubDate>Tue, 02 Jul 2024 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02485v1</guid></item><item><title>Characterizing the Interpretability of Attention Maps in Digital Pathology</title><link>http://arxiv.org/abs/2407.02484v1</link><description>Interpreting machine learning model decisions is crucial for high-riskapplications like healthcare. In digital pathology, large whole slide images(WSIs) are decomposed into smaller tiles and tile-derived features areprocessed by attention-based multiple instance learning (ABMIL) models topredict WSI-level labels. These networks generate tile-specific attentionweights, which can be visualized as attention maps for interpretability.However, a standardized evaluation framework for these maps is lacking,questioning their reliability and ability to detect spurious correlations thatcan mislead models. We herein propose a framework to assess the ability ofattention networks to attend to relevant features in digital pathology bycreating artificial model confounders and using dedicated interpretabilitymetrics. Models are trained and evaluated on data with tile modificationscorrelated with WSI labels, enabling the analysis of model sensitivity toartificial confounders and the accuracy of attention maps in highlighting them.Confounders are introduced either through synthetic tile modifications orthrough tile ablations based on their specific image-based features, with thelatter being used to assess more clinically relevant scenarios. We also analyzethe impact of varying confounder quantities at both the tile and WSI levels.Our results show that ABMIL models perform as desired within our framework.While attention maps generally highlight relevant regions, their robustness isaffected by the type and number of confounders. Our versatile framework has thepotential to be used in the evaluation of various methods and the explorationof image-based features driving model predictions, which could aid in biomarkerdiscovery.</description><author>Tomé Albuquerque, Anil Yüce, Markus D. Herrmann, Alvaro Gomariz</author><pubDate>Tue, 02 Jul 2024 18:58:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02484v1</guid></item><item><title>MMedAgent: Learning to Use Medical Tools with Multi-modal Agent</title><link>http://arxiv.org/abs/2407.02483v1</link><description>Multi-Modal Large Language Models (MLLMs), despite being successful, exhibitlimited generality and often fall short when compared to specialized models.Recently, LLM-based agents have been developed to address these challenges byselecting appropriate specialized models as tools based on user inputs.However, such advancements have not been extensively explored within themedical domain. To bridge this gap, this paper introduces the first agentexplicitly designed for the medical field, named \textbf{M}ulti-modal\textbf{Med}ical \textbf{Agent} (MMedAgent). We curate an instruction-tuningdataset comprising six medical tools solving seven tasks, enabling the agent tochoose the most suitable tools for a given task. Comprehensive experimentsdemonstrate that MMedAgent achieves superior performance across a variety ofmedical tasks compared to state-of-the-art open-source methods and even theclosed-source model, GPT-4o. Furthermore, MMedAgent exhibits efficiency inupdating and integrating new medical tools.</description><author>Binxu Li, Tiankai Yan, Yuanting Pan, Zhe Xu, Jie Luo, Ruiyang Ji, Shilong Liu, Haoyu Dong, Zihao Lin, Yixin Wang</author><pubDate>Tue, 02 Jul 2024 18:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02483v1</guid></item><item><title>Boosting Consistency in Story Visualization with Rich-Contextual Conditional Diffusion Models</title><link>http://arxiv.org/abs/2407.02482v1</link><description>Recent research showcases the considerable potential of conditional diffusionmodels for generating consistent stories. However, current methods, whichpredominantly generate stories in an autoregressive and excessivelycaption-dependent manner, often underrate the contextual consistency andrelevance of frames during sequential generation. To address this, we propose anovel Rich-contextual Conditional Diffusion Models (RCDMs), a two-stageapproach designed to enhance story generation's semantic consistency andtemporal consistency. Specifically, in the first stage, the frame-priortransformer diffusion model is presented to predict the frame semanticembedding of the unknown clip by aligning the semantic correlations between thecaptions and frames of the known clip. The second stage establishes a robustmodel with rich contextual conditions, including reference images of the knownclip, the predicted frame semantic embedding of the unknown clip, and textembeddings of all captions. By jointly injecting these rich contextualconditions at the image and feature levels, RCDMs can generate semantic andtemporal consistency stories. Moreover, RCDMs can generate consistent storieswith a single forward inference compared to autoregressive models. Ourqualitative and quantitative results demonstrate that our proposed RCDMsoutperform in challenging scenarios. The code and model will be available athttps://github.com/muzishen/RCDMs.</description><author>Fei Shen, Hu Ye, Sibo Liu, Jun Zhang, Cong Wang, Xiao Han, Wei Yang</author><pubDate>Tue, 02 Jul 2024 18:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02482v1</guid></item><item><title>Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents</title><link>http://arxiv.org/abs/2402.12327v2</link><description>Large Language Models (LLMs) have increasingly been utilized in socialsimulations, where they are often guided by carefully crafted instructions tostably exhibit human-like behaviors during simulations. Nevertheless, we doubtthe necessity of shaping agents' behaviors for accurate social simulations.Instead, this paper emphasizes the importance of spontaneous phenomena, whereinagents deeply engage in contexts and make adaptive decisions without explicitdirections. We explored spontaneous cooperation across three competitivescenarios and successfully simulated the gradual emergence of cooperation,findings that align closely with human behavioral data. This approach not onlyaids the computational social science community in bridging the gap betweensimulations and real-world dynamics but also offers the AI community a novelmethod to assess LLMs' capability of deliberate reasoning.</description><author>Zengqing Wu, Run Peng, Shuyuan Zheng, Qianying Liu, Xu Han, Brian Inhyuk Kwon, Makoto Onizuka, Shaojie Tang, Chuan Xiao</author><pubDate>Tue, 02 Jul 2024 18:56:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12327v2</guid></item><item><title>Understanding Alignment in Multimodal LLMs: A Comprehensive Study</title><link>http://arxiv.org/abs/2407.02477v1</link><description>Preference alignment has become a crucial component in enhancing theperformance of Large Language Models (LLMs), yet its impact in Multimodal LargeLanguage Models (MLLMs) remains comparatively underexplored. Similar tolanguage models, MLLMs for image understanding tasks encounter challenges likehallucination. In MLLMs, hallucination can occur not only by stating incorrectfacts but also by producing responses that are inconsistent with the imagecontent. A primary objective of alignment for MLLMs is to encourage thesemodels to align responses more closely with image information. Recently,multiple works have introduced preference datasets for MLLMs and examineddifferent alignment methods, including Direct Preference Optimization (DPO) andProximal Policy Optimization (PPO). However, due to variations in datasets,base model types, and alignment methods, it remains unclear which specificelements contribute most significantly to the reported improvements in theseworks. In this paper, we independently analyze each aspect of preferencealignment in MLLMs. We start by categorizing the alignment algorithms into twogroups, offline (such as DPO), and online (such as online-DPO), and show thatcombining offline and online methods can improve the performance of the modelin certain scenarios. We review a variety of published multimodal preferencedatasets and discuss how the details of their construction impact modelperformance. Based on these insights, we introduce a novel way of creatingmultimodal preference data called Bias-Driven Hallucination Sampling (BDHS)that needs neither additional annotation nor external models, and show that itcan achieve competitive performance to previously published alignment work formultimodal models across a range of benchmarks.</description><author>Elmira Amirloo, Jean-Philippe Fauconnier, Christoph Roesmann, Christian Kerl, Rinu Boney, Yusu Qian, Zirui Wang, Afshin Dehghan, Yinfei Yang, Zhe Gan, Peter Grasch</author><pubDate>Tue, 02 Jul 2024 18:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02477v1</guid></item><item><title>Scalable Multi-Output Gaussian Processes with Stochastic Variational Inference</title><link>http://arxiv.org/abs/2407.02476v1</link><description>The Multi-Output Gaussian Process is is a popular tool for modelling datafrom multiple sources. A typical choice to build a covariance function for aMOGP is the Linear Model of Coregionalization (LMC) which parametrically modelsthe covariance between outputs. The Latent Variable MOGP (LV-MOGP) generalisesthis idea by modelling the covariance between outputs using a kernel applied tolatent variables, one per output, leading to a flexible MOGP model that allowsefficient generalization to new outputs with few data points. Computationalcomplexity in LV-MOGP grows linearly with the number of outputs, which makes itunsuitable for problems with a large number of outputs. In this paper, wepropose a stochastic variational inference approach for the LV-MOGP that allowsmini-batches for both inputs and outputs, making computational complexity pertraining iteration independent of the number of outputs.</description><author>Xiaoyu Jiang, Sokratia Georgaka, Magnus Rattray, Mauricio A. Alvarez</author><pubDate>Tue, 02 Jul 2024 18:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02476v1</guid></item><item><title>ImageFlowNet: Forecasting Multiscale Trajectories of Disease Progression with Irregularly-Sampled Longitudinal Medical Images</title><link>http://arxiv.org/abs/2406.14794v2</link><description>The forecasting of disease progression from images is a holy grail forclinical decision making. However, this task is complicated by the inherenthigh dimensionality, temporal sparsity and sampling irregularity inlongitudinal image acquisitions. Existing methods often rely on extractinghand-crafted features and performing time-series analysis in this vector space,leading to a loss of rich spatial information within the images. To overcomethese challenges, we introduce ImageFlowNet, a novel framework that learnslatent-space flow fields that evolve multiscale representations in jointembedding spaces using neural ODEs and SDEs to model disease progression in theimage domain. Notably, ImageFlowNet learns multiscale joint representationspaces by combining cohorts of patients together so that information can betransferred between the patient samples. The dynamics then provide plausibletrajectories of progression, with the SDE providing alternative trajectoriesfrom the same starting point. We provide theoretical insights that support ourformulation of ODEs, and motivate our regularizations involving high-levelvisual features, latent space organization, and trajectory smoothness. We thendemonstrate ImageFlowNet's effectiveness through empirical evaluations on threelongitudinal medical image datasets depicting progression in retinal geographicatrophy, multiple sclerosis, and glioblastoma.</description><author>Chen Liu, Ke Xu, Liangbo L. Shen, Guillaume Huguet, Zilong Wang, Alexander Tong, Danilo Bzdok, Jay Stewart, Jay C. Wang, Lucian V. Del Priore, Smita Krishnaswamy</author><pubDate>Tue, 02 Jul 2024 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14794v2</guid></item><item><title>Free Energy in a Circumplex Model of Emotion</title><link>http://arxiv.org/abs/2407.02474v1</link><description>Previous active inference accounts of emotion translate fluctuations in freeenergy to a sense of emotion, mainly focusing on valence. However, in affectivescience, emotions are often represented as multi-dimensional. In this paper, wepropose to adopt a Circumplex Model of emotion by mapping emotions into atwo-dimensional spectrum of valence and arousal. We show how one can derive avalence and arousal signal from an agent's expected free energy, relatingarousal to the entropy of posterior beliefs and valence to utility lessexpected utility. Under this formulation, we simulate artificial agents engagedin a search task. We show that the manipulation of priors and object presenceresults in commonsense variability in emotional states.</description><author>Candice Pattisapu, Tim Verbelen, Riddhi J. Pitliya, Alex B. Kiefer, Mahault Albarracin</author><pubDate>Tue, 02 Jul 2024 18:52:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02474v1</guid></item><item><title>Benchmarking bias: Expanding clinical AI model card to incorporate bias reporting of social and non-social factors</title><link>http://arxiv.org/abs/2311.12560v2</link><description>Clinical AI model reporting cards should be expanded to incorporate a broadbias reporting of both social and non-social factors. Non-social factorsconsider the role of other factors, such as disease dependent, anatomic, orinstrument factors on AI model bias, which are essential to ensure safedeployment.</description><author>Carolina A. M. Heming, Mohamed Abdalla, Shahram Mohanna, Monish Ahluwalia, Linglin Zhang, Hari Trivedi, MinJae Woo, Benjamin Fine, Judy Wawira Gichoya, Leo Anthony Celi, Laleh Seyyed-Kalantari</author><pubDate>Tue, 02 Jul 2024 18:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12560v2</guid></item><item><title>ValueScope: Unveiling Implicit Norms and Values via Return Potential Model of Social Interactions</title><link>http://arxiv.org/abs/2407.02472v1</link><description>This study introduces ValueScope, a framework leveraging language models toquantify social norms and values within online communities, grounded in socialscience perspectives on normative structures. We employ ValueScope to dissectand analyze linguistic and stylistic expressions across 13 Reddit communitiescategorized under gender, politics, science, and finance. Our analysis providesa quantitative foundation showing that even closely related communities exhibitremarkably diverse norms. This diversity supports existing theories and adds anew dimension--community preference--to understanding community interactions.ValueScope not only delineates differing social norms among communities butalso effectively traces their evolution and the influence of significantexternal events like the U.S. presidential elections and the emergence of newsub-communities. The framework thus highlights the pivotal role of social normsin shaping online interactions, presenting a substantial advance in both thetheory and application of social norm studies in digital spaces.</description><author>Chan Young Park, Shuyue Stella Li, Hayoung Jung, Svitlana Volkova, Tanushree Mitra, David Jurgens, Yulia Tsvetkov</author><pubDate>Tue, 02 Jul 2024 18:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02472v1</guid></item><item><title>Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling</title><link>http://arxiv.org/abs/2402.17019v4</link><description>Making legal knowledge accessible to non-experts is crucial for enhancinggeneral legal literacy and encouraging civic participation in democracy.However, legal documents are often challenging to understand for people withoutlegal backgrounds. In this paper, we present a novel application of largelanguage models (LLMs) in legal education to help non-experts learn intricatelegal concepts through storytelling, an effective pedagogical tool in conveyingcomplex and abstract concepts. We also introduce a new dataset LegalStories,which consists of 294 complex legal doctrines, each accompanied by a story anda set of multiple-choice questions generated by LLMs. To construct the dataset,we experiment with various LLMs to generate legal stories explaining theseconcepts. Furthermore, we use an expert-in-the-loop approach to iterativelydesign multiple-choice questions. Then, we evaluate the effectiveness ofstorytelling with LLMs through randomized controlled trials (RCTs) with legalnovices on 10 samples from the dataset. We find that LLM-generated storiesenhance comprehension of legal concepts and interest in law among non-nativespeakers compared to only definitions. Moreover, stories consistently helpparticipants relate legal concepts to their lives. Finally, we find thatlearning with stories shows a higher retention rate for non-native speakers inthe follow-up assessment. Our work has strong implications for using LLMs inpromoting teaching and learning in the legal field and beyond.</description><author>Hang Jiang, Xiajie Zhang, Robert Mahari, Daniel Kessler, Eric Ma, Tal August, Irene Li, Alex 'Sandy' Pentland, Yoon Kim, Deb Roy, Jad Kabbara</author><pubDate>Tue, 02 Jul 2024 18:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17019v4</guid></item><item><title>PWM: Policy Learning with Large World Models</title><link>http://arxiv.org/abs/2407.02466v1</link><description>Reinforcement Learning (RL) has achieved impressive results on complex tasksbut struggles in multi-task settings with different embodiments. World modelsoffer scalability by learning a simulation of the environment, yet they oftenrely on inefficient gradient-free optimization methods. We introduce Policylearning with large World Models (PWM), a novel model-based RL algorithm thatlearns continuous control policies from large multi-task world models. Bypre-training the world model on offline data and using it for first-ordergradient policy learning, PWM effectively solves tasks with up to 152 actiondimensions and outperforms methods using ground-truth dynamics. Additionally,PWM scales to an 80-task setting, achieving up to 27% higher rewards thanexisting baselines without the need for expensive online planning.Visualizations and code available at https://policy-world-model.github.io</description><author>Ignat Georgiev, Varun Giridhar, Nicklas Hansen, Animesh Garg</author><pubDate>Tue, 02 Jul 2024 18:47:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02466v1</guid></item><item><title>Belief sharing: a blessing or a curse</title><link>http://arxiv.org/abs/2407.02465v1</link><description>When collaborating with multiple parties, communicating relevant informationis of utmost importance to efficiently completing the tasks at hand. Underactive inference, communication can be cast as sharing beliefs betweenfree-energy minimizing agents, where one agent's beliefs get transformed intoan observation modality for the other. However, the best approach fortransforming beliefs into observations remains an open question. In this paper,we demonstrate that naively sharing posterior beliefs can give rise to thenegative social dynamics of echo chambers and self-doubt. We propose analternate belief sharing strategy which mitigates these issues.</description><author>Ozan Catal, Toon Van de Maele, Riddhi J. Pitliya, Mahault Albarracin, Candice Pattisapu, Tim Verbelen</author><pubDate>Tue, 02 Jul 2024 18:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02465v1</guid></item><item><title>Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I</title><link>http://arxiv.org/abs/2407.02464v1</link><description>The traditional evaluation of information retrieval (IR) systems is generallyvery costly as it requires manual relevance annotation from human experts.Recent advancements in generative artificial intelligence -- specifically largelanguage models (LLMs) -- can generate relevance annotations at an enormousscale with relatively small computational costs. Potentially, this couldalleviate the costs traditionally associated with IR evaluation and make itapplicable to numerous low-resource applications. However, generated relevanceannotations are not immune to (systematic) errors, and as a result, directlyusing them for evaluation produces unreliable results. In this work, we propose two methods based on prediction-powered inferenceand conformal risk control that utilize computer-generated relevanceannotations to place reliable confidence intervals (CIs) around IR evaluationmetrics. Our proposed methods require a small number of reliable annotationsfrom which the methods can statistically analyze the errors in the generatedannotations. Using this information, we can place CIs around evaluation metricswith strong theoretical guarantees. Unlike existing approaches, our conformalrisk control method is specifically designed for ranking metrics and can varyits CIs per query and document. Our experimental results show that our CIsaccurately capture both the variance and bias in evaluation based on LLMannotations, better than the typical empirical bootstrapping estimates. We hopeour contributions bring reliable evaluation to the many IR applications wherethis was traditionally infeasible.</description><author>Harrie Oosterhuis, Rolf Jagerman, Zhen Qin, Xuanhui Wang, Michael Bendersky</author><pubDate>Tue, 02 Jul 2024 18:44:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02464v1</guid></item><item><title>Decentralized Intelligence Network (DIN)</title><link>http://arxiv.org/abs/2407.02461v1</link><description>Decentralized Intelligence Network (DIN) addresses the significant challengesof data sovereignty and AI utilization caused by the fragmentation and siloingof data across providers and institutions. This comprehensive frameworkovercomes access barriers to scalable data sources previously hindered by silosby leveraging: 1) personal data stores as a prerequisite for data sovereignty;2) a scalable federated learning protocol implemented on a public blockchainfor decentralized AI training, where data remains with participants and onlymodel parameter updates are shared; and 3) a scalable, trustless rewardsmechanism to incentivize participation and ensure fair reward distribution.This framework ensures that no entity can prevent or control access to trainingon data offered by participants or determine financial benefits, as theseprocesses operate on a public blockchain with an immutable record and without athird party. It supports effective AI training, allowing participants tomaintain control over their data, benefit financially, and contribute to adecentralized, scalable ecosystem that leverages collective AI to developbeneficial algorithms.</description><author>Abraham Nash</author><pubDate>Tue, 02 Jul 2024 18:40:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02461v1</guid></item><item><title>LtU-ILI: An All-in-One Framework for Implicit Inference in Astrophysics and Cosmology</title><link>http://arxiv.org/abs/2402.05137v2</link><description>This paper presents the Learning the Universe Implicit Likelihood Inference(LtU-ILI) pipeline, a codebase for rapid, user-friendly, and cutting-edgemachine learning (ML) inference in astrophysics and cosmology. The pipelineincludes software for implementing various neural architectures, trainingschemata, priors, and density estimators in a manner easily adaptable to anyresearch workflow. It includes comprehensive validation metrics to assessposterior estimate coverage, enhancing the reliability of inferred results.Additionally, the pipeline is easily parallelizable and is designed forefficient exploration of modeling hyperparameters. To demonstrate itscapabilities, we present real applications across a range of astrophysics andcosmology problems, such as: estimating galaxy cluster masses from X-rayphotometry; inferring cosmology from matter power spectra and halo pointclouds; characterizing progenitors in gravitational wave signals; capturingphysical dust parameters from galaxy colors and luminosities; and establishingproperties of semi-analytic models of galaxy formation. We also includeexhaustive benchmarking and comparisons of all implemented methods as well asdiscussions about the challenges and pitfalls of ML inference in astronomicalsciences. All code and examples are made publicly available athttps://github.com/maho3/ltu-ili.</description><author>Matthew Ho, Deaglan J. Bartlett, Nicolas Chartier, Carolina Cuesta-Lazaro, Simon Ding, Axel Lapel, Pablo Lemos, Christopher C. Lovell, T. Lucas Makinen, Chirag Modi, Viraj Pandya, Shivam Pandey, Lucia A. Perez, Benjamin Wandelt, Greg L. Bryan</author><pubDate>Tue, 02 Jul 2024 18:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05137v2</guid></item><item><title>Statistical Advantages of Oblique Randomized Decision Trees and Forests</title><link>http://arxiv.org/abs/2407.02458v1</link><description>This work studies the statistical advantages of using features comprised ofgeneral linear combinations of covariates to partition the data in randomizeddecision tree and forest regression algorithms. Using random tessellationtheory in stochastic geometry, we provide a theoretical analysis of a class ofefficiently generated random tree and forest estimators that allow for obliquesplits along such features. We call these estimators oblique Mondrian trees andforests, as the trees are generated by first selecting a set of features fromlinear combinations of the covariates and then running a Mondrian process thathierarchically partitions the data along these features. Generalization errorbounds and convergence rates are obtained for the flexible dimension reductionmodel class of ridge functions (also known as multi-index models), where theoutput is assumed to depend on a low dimensional relevant feature subspace ofthe input domain. The results highlight how the risk of these estimatorsdepends on the choice of features and quantify how robust the risk is withrespect to error in the estimation of relevant features. The asymptoticanalysis also provides conditions on the selected features along which the datais split for these estimators to obtain minimax optimal rates of convergencewith respect to the dimension of the relevant feature subspace. Additionally, alower bound on the risk of axis-aligned Mondrian trees (where features arerestricted to the set of covariates) is obtained proving that these estimatorsare suboptimal for these linear dimension reduction models in general, nomatter how the distribution over the covariates used to divide the data at eachtree node is weighted.</description><author>Eliza O'Reilly</author><pubDate>Tue, 02 Jul 2024 18:35:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02458v1</guid></item><item><title>SUPER: Seated Upper Body Pose Estimation using mmWave Radars</title><link>http://arxiv.org/abs/2407.02455v1</link><description>In industrial countries, adults spend a considerable amount of time sedentaryeach day at work, driving and during activities of daily living. Characterizingthe seated upper body human poses using mmWave radars is an important, yetunder-studied topic with many applications in human-machine interaction,transportation and road safety. In this work, we devise SUPER, a framework forseated upper body human pose estimation that utilizes dual-mmWave radars inclose proximity. A novel masking algorithm is proposed to coherently fuse datafrom the radars to generate intensity and Doppler point clouds withcomplementary information for high-motion but small radar cross section areas(e.g., upper extremities) and low-motion but large RCS areas (e.g. torso). Alightweight neural network extracts both global and local features of upperbody and output pose parameters for the Skinned Multi-Person Linear (SMPL)model. Extensive leave-one-subject-out experiments on various motion sequencesfrom multiple subjects show that SUPER outperforms a state-of-the-art baselinemethod by 30 -- 184%. We also demonstrate its utility in a simple downstreamtask for hand-object interaction.</description><author>Bo Zhang, Zimeng Zhou, Boyu Jiang, Rong Zheng</author><pubDate>Tue, 02 Jul 2024 18:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02455v1</guid></item><item><title>Ensemble of pre-trained language models and data augmentation for hate speech detection from Arabic tweets</title><link>http://arxiv.org/abs/2407.02448v1</link><description>Today, hate speech classification from Arabic tweets has drawn the attentionof several researchers. Many systems and techniques have been developed toresolve this classification task. Nevertheless, two of the major challengesfaced in this context are the limited performance and the problem of imbalanceddata. In this study, we propose a novel approach that leverages ensemblelearning and semi-supervised learning based on previously manually labeled. Weconducted experiments on a benchmark dataset by classifying Arabic tweets into5 distinct classes: non-hate, general hate, racial, religious, or sexism.Experimental results show that: (1) ensemble learning based on pre-trainedlanguage models outperforms existing related works; (2) Our proposed dataaugmentation improves the accuracy results of hate speech detection from Arabictweets and outperforms existing related works. Our main contribution is theachievement of encouraging results in Arabic hate speech detection.</description><author>Kheir Eddine Daouadi, Yaakoub Boualleg, Kheir Eddine Haouaouchi</author><pubDate>Tue, 02 Jul 2024 18:26:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02448v1</guid></item><item><title>Sparse Variational Contaminated Noise Gaussian Process Regression with Applications in Geomagnetic Perturbations Forecasting</title><link>http://arxiv.org/abs/2402.17570v3</link><description>Gaussian Processes (GP) have become popular machine-learning methods forkernel-based learning on datasets with complicated covariance structures. Inthis paper, we present a novel extension to the GP framework using acontaminated normal likelihood function to better account for heteroscedasticvariance and outlier noise. We propose a scalable inference algorithm based onthe Sparse Variational Gaussian Process (SVGP) method for fitting sparseGaussian process regression models with contaminated normal noise on largedatasets. We examine an application to geomagnetic ground perturbations, wherethe state-of-the-art prediction model is based on neural networks. We show thatour approach yields shorter prediction intervals for similar coverage andaccuracy when compared to an artificial dense neural network baseline.</description><author>Daniel Iong, Matthew McAnear, Yuezhou Qu, Shasha Zou, Gabor Toth, Yang Chen</author><pubDate>Tue, 02 Jul 2024 18:25:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17570v3</guid></item><item><title>PLeaS -- Merging Models with Permutations and Least Squares</title><link>http://arxiv.org/abs/2407.02447v1</link><description>The democratization of machine learning systems has made the process offine-tuning accessible to a large number of practitioners, leading to a widerange of open-source models fine-tuned on specialized tasks and datasets.Recent work has proposed to merge such models to combine their functionalities.However, prior approaches are restricted to models that are fine-tuned from thesame base model. Furthermore, the final merged model is typically restricted tobe of the same size as the original models. In this work, we propose a newtwo-step algorithm to merge models-termed PLeaS-which relaxes theseconstraints. First, leveraging the Permutation symmetries inherent in the twomodels, PLeaS partially matches nodes in each layer by maximizing alignment.Next, PLeaS computes the weights of the merged model as a layer-wise LeastSquares solution to minimize the approximation error between the features ofthe merged model and the permuted features of the original models. into asingle model of a desired size, even when the two original models arefine-tuned from different base models. We also present a variant of our methodwhich can merge models without using data from the fine-tuning domains. Wedemonstrate our method to merge ResNet models trained with shared and differentlabel spaces, and show that we can perform better than the state-of-the-artmerging methods by 8 to 15 percentage points for the same target compute whilemerging models trained on DomainNet and on fine-grained classification tasks.</description><author>Anshul Nasery, Jonathan Hayase, Pang Wei Koh, Sewoong Oh</author><pubDate>Tue, 02 Jul 2024 18:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02447v1</guid></item><item><title>Towards Robust Speech Representation Learning for Thousands of Languages</title><link>http://arxiv.org/abs/2407.00837v2</link><description>Self-supervised learning (SSL) has helped extend speech technologies to morelanguages by reducing the need for labeled data. However, models are still farfrom supporting the world's 7000+ languages. We propose XEUS, a Cross-lingualEncoder for Universal Speech, trained on over 1 million hours of data across4057 languages, extending the language coverage of SSL models 4-fold. Wecombine 1 million hours of speech from existing publicly accessible corporawith a newly created corpus of 7400+ hours from 4057 languages, which will bepublicly released. To handle the diverse conditions of multilingual speechdata, we augment the typical SSL masked prediction approach with a noveldereverberation objective, increasing robustness. We evaluate XEUS on severalbenchmarks, and show that it consistently outperforms or achieves comparableresults to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUSsets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters orpre-training data. Checkpoints, code, and data are found inhttps://www.wavlab.org/activities/2024/xeus/.</description><author>William Chen, Wangyou Zhang, Yifan Peng, Xinjian Li, Jinchuan Tian, Jiatong Shi, Xuankai Chang, Soumi Maiti, Karen Livescu, Shinji Watanabe</author><pubDate>Tue, 02 Jul 2024 18:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00837v2</guid></item><item><title>Cradle: Empowering Foundation Agents Towards General Computer Control</title><link>http://arxiv.org/abs/2403.03186v3</link><description>Despite the success in specific scenarios, existing foundation agents stillstruggle to generalize across various virtual scenarios, mainly due to thedramatically different encapsulations of environments with manually designedobservation and action spaces. To handle this issue, we propose the GeneralComputer Control (GCC) setting to restrict foundation agents to interact withsoftware through the most unified and standardized interface, i.e., usingscreenshots as input and keyboard and mouse actions as output. We introduceCradle, a modular and flexible LMM-powered framework, as a preliminary attempttowards GCC. Enhanced by six key modules, Cradle can understand inputscreenshots and output executable code for low-level keyboard and mouse controlafter high-level planning, so that Cradle can interact with any software andcomplete long-horizon complex tasks without relying on any built-in APIs.Experimental results show that Cradle exhibits remarkable generalizability andimpressive performance across four previously unexplored commercial videogames, five software applications, and a comprehensive benchmark, OSWorld.Cradle is the first to enable foundation agents to follow the main storylineand complete 40-minute-long real missions in the complex AAA game Red DeadRedemption 2 (RDR2). Cradle can also create a city of a thousand people inCities: Skylines, farm and harvest parsnips in Stardew Valley, and trade andbargain with a maximal weekly total profit of 87% in Dealer's Life 2. Cradlecan not only operate daily software, like Chrome, Outlook, and Feishu, but alsoedit images and videos using Meitu and CapCut. Cradle greatly extends the reachof foundation agents by enabling the easy conversion of any software,especially complex games, into benchmarks to evaluate agents' various abilitiesand facilitate further data collection, thus paving the way for generalistagents.</description><author>Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, Ruyi An, Molei Qin, Chuqiao Zong, Longtao Zheng, Yujie Wu, Xiaoqiang Chai, Yifei Bi, Tianbao Xie, Pengjie Gu, Xiyun Li, Ceyao Zhang, Long Tian, Chaojie Wang, Xinrun Wang, Börje F. Karlsson, Bo An, Shuicheng Yan, Zongqing Lu</author><pubDate>Tue, 02 Jul 2024 18:23:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03186v3</guid></item><item><title>Predicting vs. Acting: A Trade-off Between World Modeling &amp; Agent Modeling</title><link>http://arxiv.org/abs/2407.02446v1</link><description>RLHF-aligned LMs have shown unprecedented ability on both benchmarks andlong-form text generation, yet they struggle with one foundational task:next-token prediction. As RLHF models become agent models aimed at interactingwith humans, they seem to lose their world modeling -- the ability to predictwhat comes next in arbitrary documents, which is the foundational trainingobjective of the Base LMs that RLHF adapts. Besides empirically demonstrating this trade-off, we propose a potentialexplanation: to perform coherent long-form generation, RLHF models restrictrandomness via implicit blueprints. In particular, RLHF models concentrateprobability on sets of anchor spans that co-occur across multiple generationsfor the same prompt, serving as textual scaffolding but also limiting a model'sability to generate documents that do not include these spans. We study thistrade-off on the most effective current agent models, those aligned with RLHF,while exploring why this may remain a fundamental trade-off between models thatact and those that predict, even as alignment techniques improve.</description><author>Margaret Li, Weijia Shi, Artidoro Pagnoni, Peter West, Ari Holtzman</author><pubDate>Tue, 02 Jul 2024 18:22:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02446v1</guid></item><item><title>Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials</title><link>http://arxiv.org/abs/2407.02445v1</link><description>We present Meta 3D AssetGen (AssetGen), a significant advancement intext-to-3D generation which produces faithful, high-quality meshes with textureand material control. Compared to works that bake shading in the 3D object'sappearance, AssetGen outputs physically-based rendering (PBR) materials,supporting realistic relighting. AssetGen generates first several views of theobject with factored shaded and albedo appearance channels, and thenreconstructs colours, metalness and roughness in 3D, using a deferred shadingloss for efficient supervision. It also uses a sign-distance function torepresent 3D shape more reliably and introduces a corresponding loss for directshape supervision. This is implemented using fused kernels for high memoryefficiency. After mesh extraction, a texture refinement transformer operatingin UV space significantly improves sharpness and details. AssetGen achieves 17%improvement in Chamfer Distance and 40% in LPIPS over the best concurrent workfor few-view reconstruction, and a human preference of 72% over the bestindustry competitors of comparable speed, including those that support PBR.Project page with generated assets: https://assetgen.github.io</description><author>Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, David Novotny</author><pubDate>Tue, 02 Jul 2024 18:21:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02445v1</guid></item><item><title>Predicting Visual Attention in Graphic Design Documents</title><link>http://arxiv.org/abs/2407.02439v1</link><description>We present a model for predicting visual attention during the free viewing ofgraphic design documents. While existing works on this topic have aimed atpredicting static saliency of graphic designs, our work is the first attempt topredict both spatial attention and dynamic temporal order in which the documentregions are fixated by gaze using a deep learning based model. We propose atwo-stage model for predicting dynamic attention on such documents, withwebpages being our primary choice of document design for demonstration. In thefirst stage, we predict the saliency maps for each of the document components(e.g. logos, banners, texts, etc. for webpages) conditioned on the type ofdocument layout. These component saliency maps are then jointly used to predictthe overall document saliency. In the second stage, we use theselayout-specific component saliency maps as the state representation for aninverse reinforcement learning model of fixation scanpath prediction duringdocument viewing. To test our model, we collected a new dataset consisting ofeye movements from 41 people freely viewing 450 webpages (the largest datasetof its kind). Experimental results show that our model outperforms existingmodels in both saliency and scanpath prediction for webpages, and alsogeneralizes very well to other graphic design documents such as comics,posters, mobile UIs, etc. and natural images.</description><author>Souradeep Chakraborty, Zijun Wei, Conor Kelton, Seoyoung Ahn, Aruna Balasubramanian, Gregory J. Zelinsky, Dimitris Samaras</author><pubDate>Tue, 02 Jul 2024 18:15:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02439v1</guid></item><item><title>Parameter Matching Attack: Enhancing Practical Applicability of Availability Attacks</title><link>http://arxiv.org/abs/2407.02437v1</link><description>The widespread use of personal data for training machine learning modelsraises significant privacy concerns, as individuals have limited control overhow their public data is subsequently utilized. Availability attacks haveemerged as a means for data owners to safeguard their data by desningimperceptible perturbations that degrade model performance when incorporatedinto training datasets. However, existing availability attacks exhibitlimitations in practical applicability, particularly when only a portion of thedata can be perturbed. To address this challenge, we propose a novelavailability attack approach termed Parameter Matching Attack (PMA). PMA is thefirst availability attack that works when only a portion of data can beperturbed. PMA optimizes perturbations so that when the model is trained on amixture of clean and perturbed data, the resulting model will approach a modeldesigned to perform poorly. Experimental results across four datasetsdemonstrate that PMA outperforms existing methods, achieving significant modelperformance degradation when a part of the training data is perturbed. Our codeis available in the supplementary.</description><author>Yu Zhe, Jun Sakuma</author><pubDate>Tue, 02 Jul 2024 18:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02437v1</guid></item><item><title>Evaluating the Robustness of Adverse Drug Event Classification Models Using Templates</title><link>http://arxiv.org/abs/2407.02432v1</link><description>An adverse drug effect (ADE) is any harmful event resulting from medical drugtreatment. Despite their importance, ADEs are often under-reported in officialchannels. Some research has therefore turned to detecting discussions of ADEsin social media. Impressive results have been achieved in various attempts todetect ADEs. In a high-stakes domain such as medicine, however, an in-depthevaluation of a model's abilities is crucial. We address the issue of thoroughperformance evaluation in English-language ADE detection with hand-craftedtemplates for four capabilities: Temporal order, negation, sentiment, andbeneficial effect. We find that models with similar performance on held-outtest sets have varying results on these capabilities.</description><author>Dorothea MacPhail, David Harbecke, Lisa Raithel, Sebastian Möller</author><pubDate>Tue, 02 Jul 2024 18:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02432v1</guid></item><item><title>On the Robustness of Graph Reduction Against GNN Backdoor</title><link>http://arxiv.org/abs/2407.02431v1</link><description>Graph Neural Networks (GNNs) are gaining popularity across various domainsdue to their effectiveness in learning graph-structured data. Nevertheless,they have been shown to be susceptible to backdoor poisoning attacks, whichpose serious threats to real-world applications. Meanwhile, graph reductiontechniques, including coarsening and sparsification, which have long beenemployed to improve the scalability of large graph computational tasks, haverecently emerged as effective methods for accelerating GNN training onlarge-scale graphs. However, the current development and deployment of graphreduction techniques for large graphs overlook the potential risks of datapoisoning attacks against GNNs. It is not yet clear how graph reductioninteracts with existing backdoor attacks. This paper conducts a thoroughexamination of the robustness of graph reduction methods in scalable GNNtraining in the presence of state-of-the-art backdoor attacks. We performed acomprehensive robustness analysis across six coarsening methods and sixsparsification methods for graph reduction, under three GNN backdoor attacksagainst three GNN architectures. Our findings indicate that the effectivenessof graph reduction methods in mitigating attack success rates variessignificantly, with some methods even exacerbating the attacks. Throughdetailed analyses of triggers and poisoned nodes, we interpret our findings andenhance our understanding of how graph reduction interacts with backdoorattacks. These results highlight the critical need for incorporating robustnessconsiderations in graph reduction for GNN training, ensuring that enhancementsin computational efficiency do not compromise the security of GNN systems.</description><author>Yuxuan Zhu, Michael Mandulak, Kerui Wu, George Slota, Yuseok Jeon, Ka-Ho Chow, Lei Yu</author><pubDate>Tue, 02 Jul 2024 18:08:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02431v1</guid></item><item><title>Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects</title><link>http://arxiv.org/abs/2407.02430v1</link><description>The recent availability and adaptability of text-to-image models has sparkeda new era in many related domains that benefit from the learned text priors aswell as high-quality and fast generation capabilities, one of which is texturegeneration for 3D objects. Although recent texture generation methods achieveimpressive results by using text-to-image networks, the combination of globalconsistency, quality, and speed, which is crucial for advancing texturegeneration to real-world applications, remains elusive. To that end, weintroduce Meta 3D TextureGen: a new feedforward method comprised of twosequential networks aimed at generating high-quality and globally consistenttextures for arbitrary geometries of any complexity degree in less than 20seconds. Our method achieves state-of-the-art results in quality and speed byconditioning a text-to-image model on 3D semantics in 2D space and fusing theminto a complete and high-resolution UV texture map, as demonstrated byextensive qualitative and quantitative evaluations. In addition, we introduce atexture enhancement network that is capable of up-scaling any texture by anarbitrary ratio, producing 4k pixel resolution textures.</description><author>Raphael Bensadoun, Yanir Kleiman, Idan Azuri, Omri Harosh, Andrea Vedaldi, Natalia Neverova, Oran Gafni</author><pubDate>Tue, 02 Jul 2024 18:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02430v1</guid></item><item><title>Analytics of Longitudinal System Monitoring Data for Performance Prediction</title><link>http://arxiv.org/abs/2007.03451v2</link><description>In recent years, several HPC facilities have started continuous monitoring oftheir systems and jobs to collect performance-related data for understandingperformance and operational efficiency. Such data can be used to optimize theperformance of individual jobs and the overall system by creating data-drivenmodels that can predict the performance of jobs waiting in the scheduler queue.In this paper, we model the performance of representative control jobs usinglongitudinal system-wide monitoring data and machine learning to explore thecauses of performance variability. We analyze these prediction models in greatdetail to identify the features that are dominant predictors of performance. Wedemonstrate that such models can be application-agnostic and can be used forpredicting performance of applications that are not included in training.</description><author>Ian J. Costello, Abhinav Bhatele</author><pubDate>Tue, 02 Jul 2024 18:02:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.03451v2</guid></item><item><title>Naturalistic Music Decoding from EEG Data via Latent Diffusion Models</title><link>http://arxiv.org/abs/2405.09062v3</link><description>In this article, we explore the potential of using latent diffusion models, afamily of powerful generative models, for the task of reconstructingnaturalistic music from electroencephalogram (EEG) recordings. Unlike simplermusic with limited timbres, such as MIDI-generated tunes or monophonic pieces,the focus here is on intricate music featuring a diverse array of instruments,voices, and effects, rich in harmonics and timbre. This study represents aninitial foray into achieving general music reconstruction of high-quality usingnon-invasive EEG data, employing an end-to-end training approach directly onraw data without the need for manual pre-processing and channel selection. Wetrain our models on the public NMED-T dataset and perform quantitativeevaluation proposing neural embedding-based metrics. We additionally performsong classification based on the generated tracks. Our work contributes to theongoing research in neural decoding and brain-computer interfaces, offeringinsights into the feasibility of using EEG data for complex auditoryinformation reconstruction.</description><author>Emilian Postolache, Natalia Polouliakh, Hiroaki Kitano, Akima Connelly, Emanuele Rodolà, Luca Cosmo, Taketo Akama</author><pubDate>Tue, 02 Jul 2024 18:00:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09062v3</guid></item><item><title>Comparative Evaluation of Learning Models for Bionic Robots: Non-Linear Transfer Function Identifications</title><link>http://arxiv.org/abs/2407.02428v1</link><description>The control and modeling of bionic robot dynamics have increasingly adoptedmodel-free control strategies using machine learning methods. Given thenon-linear elastic nature of bionic robotic systems, learning-based methodsprovide reliable alternatives by utilizing numerical data to establish a directmapping from actuation inputs to robot trajectories without complex kinematicsmodels. However, for developers, the method of identifying an appropriatelearning model for their specific bionic robots and further constructing thetransfer function has not been thoroughly discussed. Thus, this research trainsfour types of models, including ensemble learning models, regularization-basedmodels, kernel-based models, and neural network models, suitable formulti-input multi-output (MIMO) data and non-linear transfer functionidentification, in order to evaluate their (1) accuracy, (2) computationcomplexity, and (3) performance of capturing biological movements. Thisresearch encompasses data collection methods for control inputs and actionoutputs, selection of machine learning models, comparative analysis of trainingresults, and transfer function identifications. The main objective is toprovide a comprehensive evaluation strategy and framework for the applicationof model-free control.</description><author>Po-Yu Hsieh, June-Hao Hou</author><pubDate>Tue, 02 Jul 2024 18:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02428v1</guid></item><item><title>Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction</title><link>http://arxiv.org/abs/2310.10958v2</link><description>Deep neural networks (DNN) have achieved remarkable success in variousfields, including computer vision and natural language processing. However,training an effective DNN model still poses challenges. This paper aims topropose a method to optimize the training effectiveness of DNN, with the goalof improving model performance. Firstly, based on the observation that the DNNparameters change in certain laws during training process, the potential ofparameter prediction for improving model training efficiency and performance isdiscovered. Secondly, considering the magnitude of DNN model parameters,hardware limitations and characteristics of Stochastic Gradient Descent (SGD)for noise tolerance, a Parameter Linear Prediction (PLP) method is exploit toperform DNN parameter prediction. Finally, validations are carried out on somerepresentative backbones. Experiment results show that compare to the normaltraining ways, under the same training conditions and epochs, by employingproposed PLP method, the optimal model is able to obtain average about 1%accuracy improvement and 0.01 top-1/top-5 error reduction for Vgg16, Resnet18and GoogLeNet based on CIFAR-100 dataset, which shown the effectiveness of theproposed method on different DNN structures, and validated its capacity inenhancing DNN training efficiency and performance.</description><author>Hejie Ying, Mengmeng Song, Yaohong Tang, Shungen Xiao, Zimin Xiao</author><pubDate>Tue, 02 Jul 2024 17:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10958v2</guid></item><item><title>Reinforcement Learning and Machine ethics:a systematic review</title><link>http://arxiv.org/abs/2407.02425v1</link><description>Machine ethics is the field that studies how ethical behaviour can beaccomplished by autonomous systems. While there exist some systematic reviewsaiming to consolidate the state of the art in machine ethics prior to 2020,these tend to not include work that uses reinforcement learning agents asentities whose ethical behaviour is to be achieved. The reason for this is thatonly in the last years we have witnessed an increase in machine ethics studieswithin reinforcement learning. We present here a systematic review ofreinforcement learning for machine ethics and machine ethics withinreinforcement learning. Additionally, we highlight trends in terms of ethicsspecifications, components and frameworks of reinforcement learning, andenvironments used to result in ethical behaviour. Our systematic review aims toconsolidate the work in machine ethics and reinforcement learning thuscompleting the gap in the state of the art machine ethics landscape</description><author>Ajay Vishwanath, Louise A. Dennis, Marija Slavkovik</author><pubDate>Tue, 02 Jul 2024 17:54:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02425v1</guid></item><item><title>Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement</title><link>http://arxiv.org/abs/2403.06659v3</link><description>Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial fordetecting cardiac arrhythmic diseases in clinical practice. While ECGSelf-supervised Learning (eSSL) methods show promise in representation learningfrom unannotated ECG data, they often overlook the clinical knowledge that canbe found in reports. This oversight and the requirement for annotated samplesfor downstream tasks limit eSSL's versatility. In this work, we address theseissues with the Multimodal ECG Representation Learning (MERL}) framework.Through multimodal learning on ECG records and associated reports, MERL iscapable of performing zero-shot ECG classification with text prompts,eliminating the need for training data in downstream tasks. At test time, wepropose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach,which uses Large Language Models (LLMs) to exploit external expert-verifiedclinical knowledge databases, generating more descriptive prompts and reducinghallucinations in LLM-generated content to boost zero-shot classification.Based on MERL, we perform the first benchmark across six public ECG datasets,showing the superior performance of MERL compared against eSSL methods.Notably, MERL achieves an average AUC score of 75.2% in zero-shotclassification (without training data), 3.2% higher than linear probed eSSLmethods with 10\% annotated training data, averaged across all six datasets.Code and models are available at https://github.com/cheliu-computation/MERL</description><author>Che Liu, Zhongwei Wan, Cheng Ouyang, Anand Shah, Wenjia Bai, Rossella Arcucci</author><pubDate>Tue, 02 Jul 2024 17:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06659v3</guid></item><item><title>A Pattern Language for Machine Learning Tasks</title><link>http://arxiv.org/abs/2407.02424v1</link><description>Idealised as universal approximators, learners such as neural networks can beviewed as "variable functions" that may become one of a range of concretefunctions after training. In the same way that equations constrain the possiblevalues of variables in algebra, we may view objective functions as constraintson the behaviour of learners. We extract the equivalences perfectly optimisedobjective functions impose, calling them "tasks". For these tasks, we develop aformal graphical language that allows us to: (1) separate the core tasks of abehaviour from its implementation details; (2) reason about and designbehaviours model-agnostically; and (3) simply describe and unify approaches inmachine learning across domains. As proof-of-concept, we design a novel task that enables convertingclassifiers into generative models we call "manipulators", which we implementby directly translating task specifications into code. The resulting modelsexhibit capabilities such as style transfer and interpretable latent-spaceediting, without the need for custom architectures, adversarial training orrandom sampling. We formally relate the behaviour of manipulators to GANs, andempirically demonstrate their competitive performance with VAEs. We report onexperiments across vision and language domains aiming to characterisemanipulators as approximate Bayesian inversions of discriminative classifiers.</description><author>Benjamin Rodatz, Ian Fan, Tuomas Laakkonen, Neil John Ortega, Thomas Hoffman, Vincent Wang-Mascianica</author><pubDate>Tue, 02 Jul 2024 17:50:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02424v1</guid></item><item><title>On the Anatomy of Attention</title><link>http://arxiv.org/abs/2407.02423v1</link><description>We introduce a category-theoretic diagrammatic formalism in order tosystematically relate and reason about machine learning models. Our diagramspresent architectures intuitively but without loss of essential detail, wherenatural relationships between models are captured by graphical transformations,and important differences and similarities can be identified at a glance. Inthis paper, we focus on attention mechanisms: translating folklore intomathematical derivations, and constructing a taxonomy of attention variants inthe literature. As a first example of an empirical investigation underpinned byour formalism, we identify recurring anatomical components of attention, whichwe exhaustively recombine to explore a space of variations on the attentionmechanism.</description><author>Nikhil Khatri, Tuomas Laakkonen, Jonathon Liu, Vincent Wang-Maścianica</author><pubDate>Tue, 02 Jul 2024 17:50:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02423v1</guid></item><item><title>Close, But Not There: Boosting Geographic Distance Sensitivity in Visual Place Recognition</title><link>http://arxiv.org/abs/2407.02422v1</link><description>Visual Place Recognition (VPR) plays a critical role in many localization andmapping pipelines. It consists of retrieving the closest sample to a queryimage, in a certain embedding space, from a database of geotagged references.The image embedding is learned to effectively describe a place despitevariations in visual appearance, viewpoint, and geometric changes. In thiswork, we formulate how limitations in the Geographic Distance Sensitivity ofcurrent VPR embeddings result in a high probability of incorrectly sorting thetop-k retrievals, negatively impacting the recall. In order to address thisissue in single-stage VPR, we propose a novel mining strategy, CliqueMining,that selects positive and negative examples by sampling cliques from a graph ofvisually similar images. Our approach boosts the sensitivity of VPR embeddingsat small distance ranges, significantly improving the state of the art onrelevant benchmarks. In particular, we raise recall@1 from 75% to 82% in MSLSChallenge, and from 76% to 90% in Nordland. Models and code are available athttps://github.com/serizba/cliquemining.</description><author>Sergio Izquierdo, Javier Civera</author><pubDate>Tue, 02 Jul 2024 17:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02422v1</guid></item><item><title>Proceedings of the 2nd International Workshop on Adaptive Cyber Defense</title><link>http://arxiv.org/abs/2308.09520v5</link><description>The 2nd International Workshop on Adaptive Cyber Defense was held at theFlorida Institute of Technology, Florida. This workshop was organized to shareresearch that explores unique applications of Artificial Intelligence (AI) andMachine Learning (ML) as foundational capabilities for the pursuit of adaptivecyber defense. The cyber domain cannot currently be reliably and effectivelydefended without extensive reliance on human experts. Skilled cyber defendersare in short supply and often cannot respond fast enough to cyber threats. Building on recent advances in AI and ML the Cyber defense research communityhas been motivated to develop new dynamic and sustainable defenses through theadoption of AI and ML techniques to cyber settings. Bridging critical gapsbetween AI and Cyber researchers and practitioners can accelerate efforts tocreate semi-autonomous cyber defenses that can learn to recognize and respondto cyber attacks or discover and mitigate weaknesses in cooperation with othercyber operation systems and human experts. Furthermore, these defenses areexpected to be adaptive and able to evolve over time to thwart changes inattacker behavior, changes in the system health and readiness, and naturalshifts in user behavior over time. The workshop was comprised of invited keynote talks, technical presentationsand a panel discussion about how AI/ML can enable autonomous mitigation ofcurrent and future cyber attacks. Workshop submissions were peer reviewed by apanel of domain experts with a proceedings consisting of six technical articlesexploring challenging problems of critical importance to national and globalsecurity. Participation in this workshop offered new opportunities to stimulateresearch and innovation in the emerging domain of adaptive and autonomous cyberdefense.</description><author>Marco Carvalho, Damian Marriott, Mark Bilinski, Ahmad Ridley</author><pubDate>Tue, 02 Jul 2024 17:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09520v5</guid></item><item><title>Quantum Curriculum Learning</title><link>http://arxiv.org/abs/2407.02419v1</link><description>Quantum machine learning (QML) requires significant quantum resources toachieve quantum advantage. Research should prioritize both the efficient designof quantum architectures and the development of learning strategies to optimizeresource usage. We propose a framework called quantum curriculum learning(Q-CurL) for quantum data, where the curriculum introduces simpler tasks ordata to the learning model before progressing to more challenging ones. Wedefine the curriculum criteria based on the data density ratio between tasks todetermine the curriculum order. We also implement a dynamic learning scheduleto emphasize the significance of quantum data in optimizing the loss function.Empirical evidence shows that Q-CurL enhances the training convergence and thegeneralization for unitary learning tasks and improves the robustness ofquantum phase recognition tasks. Our framework provides a general learningstrategy, bringing QML closer to realizing practical advantages.</description><author>Quoc Hoan Tran, Yasuhiro Endo, Hirotaka Oshima</author><pubDate>Tue, 02 Jul 2024 17:44:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02419v1</guid></item><item><title>AXIAL: Attention-based eXplainability for Interpretable Alzheimer's Localized Diagnosis using 2D CNNs on 3D MRI brain scans</title><link>http://arxiv.org/abs/2407.02418v1</link><description>This study presents an innovative method for Alzheimer's disease diagnosisusing 3D MRI designed to enhance the explainability of model decisions. Ourapproach adopts a soft attention mechanism, enabling 2D CNNs to extractvolumetric representations. At the same time, the importance of each slice indecision-making is learned, allowing the generation of a voxel-level attentionmap to produces an explainable MRI. To test our method and ensure thereproducibility of our results, we chose a standardized collection of MRI datafrom the Alzheimer's Disease Neuroimaging Initiative (ADNI). On this dataset,our method significantly outperforms state-of-the-art methods in (i)distinguishing AD from cognitive normal (CN) with an accuracy of 0.856 andMatthew's correlation coefficient (MCC) of 0.712, representing improvements of2.4\% and 5.3\% respectively over the second-best, and (ii) in the prognostictask of discerning stable from progressive mild cognitive impairment (MCI) withan accuracy of 0.725 and MCC of 0.443, showing improvements of 10.2\% and20.5\% respectively over the second-best. We achieved this prognostic result byadopting a double transfer learning strategy, which enhanced sensitivity tomorphological changes and facilitated early-stage AD detection. Withvoxel-level precision, our method identified which specific areas are beingpaid attention to, identifying these predominant brain regions: the\emph{hippocampus}, the \emph{amygdala}, the \emph{parahippocampal}, and the\emph{inferior lateral ventricles}. All these areas are clinically associatedwith AD development. Furthermore, our approach consistently found the sameAD-related areas across different cross-validation folds, proving itsrobustness and precision in highlighting areas that align closely with knownpathological markers of the disease.</description><author>Gabriele Lozupone, Alessandro Bria, Francesco Fontanella, Claudio De Stefano</author><pubDate>Tue, 02 Jul 2024 17:44:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02418v1</guid></item><item><title>Matching domain experts by training from scratch on domain knowledge</title><link>http://arxiv.org/abs/2405.09395v2</link><description>Recently, large language models (LLMs) have outperformed human experts inpredicting the results of neuroscience experiments (Luo et al., 2024). What isthe basis for this performance? One possibility is that statistical patterns inthat specific scientific literature, as opposed to emergent reasoning abilitiesarising from broader training, underlie LLMs' performance. To evaluate thispossibility, we trained (next word prediction) a relatively small124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge.Despite being orders of magnitude smaller than larger LLMs trained on trillionsof tokens, small models achieved expert-level performance in predictingneuroscience results. Small models trained on the neuroscience literaturesucceeded when they were trained from scratch using a tokenizer specificallytrained on neuroscience text or when the neuroscience literature was used tofinetune a pretrained GPT-2. Our results indicate that expert-level performancemay be attained by even small LLMs through domain-specific, auto-regressivetraining approaches.</description><author>Xiaoliang Luo, Guangzhi Sun, Bradley C. Love</author><pubDate>Tue, 02 Jul 2024 17:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09395v2</guid></item><item><title>ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image and Text</title><link>http://arxiv.org/abs/2401.01456v2</link><description>Diffusion models have recently demonstrated their effectiveness in generatingextremely high-quality images and are now utilized in a wide range ofapplications, including automatic sketch colorization. Although many methodshave been developed for guided sketch colorization, there has been limitedexploration of the potential conflicts between image prompts and sketch inputs,which can lead to severe deterioration in the results. Therefore, this paperexhaustively investigates reference-based sketch colorization models that aimto colorize sketch images using reference color images. We specificallyinvestigate two critical aspects of reference-based diffusion models: the"distribution problem", which is a major shortcoming compared to text-basedcounterparts, and the capability in zero-shot sequential text-basedmanipulation. We introduce two variations of an image-guided latent diffusionmodel utilizing different image tokens from the pre-trained CLIP image encoderand propose corresponding manipulation methods to adjust their resultssequentially using weighted text inputs. We conduct comprehensive evaluationsof our models through qualitative and quantitative experiments as well as auser study.</description><author>Dingkun Yan, Liang Yuan, Erwin Wu, Yuma Nishioka, Issei Fujishiro, Suguru Saito</author><pubDate>Tue, 02 Jul 2024 17:35:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01456v2</guid></item><item><title>Video Watermarking: Safeguarding Your Video from (Unauthorized) Annotations by Video-based LLMs</title><link>http://arxiv.org/abs/2407.02411v1</link><description>The advent of video-based Large Language Models (LLMs) has significantlyenhanced video understanding. However, it has also raised some safety concernsregarding data protection, as videos can be more easily annotated, even withoutauthorization. This paper introduces Video Watermarking, a novel technique toprotect videos from unauthorized annotations by such video-based LLMs,especially concerning the video content and description, in response tospecific queries. By imperceptibly embedding watermarks into key video frameswith multi-modal flow-based losses, our method preserves the viewing experiencewhile preventing misuse by video-based LLMs. Extensive experiments show thatVideo Watermarking significantly reduces the comprehensibility of videos withvarious video-based LLMs, demonstrating both stealth and robustness. Inessence, our method provides a solution for securing video content, ensuringits integrity and confidentiality in the face of evolving video-based LLMstechnologies.</description><author>Jinmin Li, Kuofeng Gao, Yang Bai, Jingyun Zhang, Shu-Tao Xia</author><pubDate>Tue, 02 Jul 2024 17:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02411v1</guid></item><item><title>On the consistency of hyper-parameter selection in value-based deep reinforcement learning</title><link>http://arxiv.org/abs/2406.17523v2</link><description>Deep reinforcement learning (deep RL) has achieved tremendous success onvarious domains through a combination of algorithmic design and carefulselection of hyper-parameters. Algorithmic improvements are often the result ofiterative enhancements built upon prior approaches, while hyper-parameterchoices are typically inherited from previous methods or fine-tunedspecifically for the proposed technique. Despite their crucial impact onperformance, hyper-parameter choices are frequently overshadowed by algorithmicadvancements. This paper conducts an extensive empirical study focusing on thereliability of hyper-parameter selection for value-based deep reinforcementlearning agents, including the introduction of a new score to quantify theconsistency and reliability of various hyper-parameters. Our findings not onlyhelp establish which hyper-parameters are most critical to tune, but also helpclarify which tunings remain consistent across different training regimes.</description><author>Johan Obando-Ceron, João G. M. Araújo, Aaron Courville, Pablo Samuel Castro</author><pubDate>Tue, 02 Jul 2024 17:33:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17523v2</guid></item><item><title>CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models</title><link>http://arxiv.org/abs/2407.02408v1</link><description>As Large Language Models (LLMs) are increasingly deployed to handle variousnatural language processing (NLP) tasks, concerns regarding the potentialnegative societal impacts of LLM-generated content have also arisen. Toevaluate the biases exhibited by LLMs, researchers have recently proposed avariety of datasets. However, existing bias evaluation efforts often focus ononly a particular type of bias and employ inconsistent evaluation metrics,leading to difficulties in comparison across different datasets and LLMs. Toaddress these limitations, we collect a variety of datasets designed for thebias evaluation of LLMs, and further propose CEB, a Compositional EvaluationBenchmark that covers different types of bias across different social groupsand tasks. The curation of CEB is based on our newly proposed compositionaltaxonomy, which characterizes each dataset from three dimensions: bias types,social groups, and tasks. By combining the three dimensions, we develop acomprehensive evaluation strategy for the bias in LLMs. Our experimentsdemonstrate that the levels of bias vary across these dimensions, therebyproviding guidance for the development of specific bias mitigation methods.</description><author>Song Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, Jundong Li</author><pubDate>Tue, 02 Jul 2024 17:31:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02408v1</guid></item><item><title>Tiny-PULP-Dronets: Squeezing Neural Networks for Faster and Lighter Inference on Multi-Tasking Autonomous Nano-Drones</title><link>http://arxiv.org/abs/2407.02405v1</link><description>Pocket-sized autonomous nano-drones can revolutionize many robotic use cases,such as visual inspection in narrow, constrained spaces, and ensure saferhuman-robot interaction due to their tiny form factor and weight -- i.e., tensof grams. This compelling vision is challenged by the high level ofintelligence needed aboard, which clashes against the limited computational andstorage resources available on PULP (parallel-ultra-low-power) MCU classnavigation and mission controllers that can be hosted aboard. This work movesfrom PULP-Dronet, a State-of-the-Art convolutional neural network forautonomous navigation on nano-drones. We introduce Tiny-PULP-Dronet: a novelmethodology to squeeze by more than one order of magnitude model size (50xfewer parameters), and number of operations (27x less multiply-and-accumulate)required to run inference with similar flight performance as PULP-Dronet. Thismassive reduction paves the way towards affordable multi-tasking onnano-drones, a fundamental requirement for achieving high-level intelligence.</description><author>Lorenzo Lamberti, Vlad Niculescu, Michał Barcis, Lorenzo Bellone, Enrico Natalizio, Luca Benini, Daniele Palossi</author><pubDate>Tue, 02 Jul 2024 17:24:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02405v1</guid></item><item><title>Face Reconstruction Transfer Attack as Out-of-Distribution Generalization</title><link>http://arxiv.org/abs/2407.02403v1</link><description>Understanding the vulnerability of face recognition systems to maliciousattacks is of critical importance. Previous works have focused onreconstructing face images that can penetrate a targeted verification system.Even in the white-box scenario, however, naively reconstructed imagesmisrepresent the identity information, hence the attacks are easily neutralizedonce the face system is updated or changed. In this paper, we aim toreconstruct face images which are capable of transferring face attacks onunseen encoders. We term this problem as Face Reconstruction Transfer Attack(FRTA) and show that it can be formulated as an out-of-distribution (OOD)generalization problem. Inspired by its OOD nature, we propose to solve FRTA byAveraged Latent Search and Unsupervised Validation with pseudo target (ALSUV).To strengthen the reconstruction attack on OOD unseen encoders, ALSUVreconstructs the face by searching the latent of amortized generator StyleGAN2through multiple latent optimization, latent optimization trajectory averaging,and unsupervised validation with a pseudo target. We demonstrate the efficacyand generalization of our method on widely used face datasets, accompanying itwith extensive ablation studies and visually, qualitatively, and quantitativelyanalyses. The source code will be released.</description><author>Yoon Gyo Jung, Jaewoo Park, Xingbo Dong, Hojin Park, Andrew Beng Jin Teoh, Octavia Camps</author><pubDate>Tue, 02 Jul 2024 17:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02403v1</guid></item><item><title>Assessing the Code Clone Detection Capability of Large Language Models</title><link>http://arxiv.org/abs/2407.02402v1</link><description>This study aims to assess the performance of two advanced Large LanguageModels (LLMs), GPT-3.5 and GPT-4, in the task of code clone detection. Theevaluation involves testing the models on a variety of code pairs of differentclone types and levels of similarity, sourced from two datasets: BigCloneBench(human-made) and GPTCloneBench (LLM-generated). Findings from the studyindicate that GPT-4 consistently surpasses GPT-3.5 across all clone types. Acorrelation was observed between the GPTs' accuracy at identifying code clonesand code similarity, with both GPT models exhibiting low effectiveness indetecting the most complex Type-4 code clones. Additionally, GPT modelsdemonstrate a higher performance identifying code clones in LLM-generated codecompared to humans-generated code. However, they do not reach impressiveaccuracy. These results emphasize the imperative for ongoing enhancements inLLM capabilities, particularly in the recognition of code clones and inmitigating their predisposition towards self-generated code clones--which islikely to become an issue as software engineers are more numerous to leverageLLM-enabled code generation and code refactoring tools.</description><author>Zixian Zhang, Takfarinas Saber</author><pubDate>Tue, 02 Jul 2024 17:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02402v1</guid></item><item><title>Consistency Flow Matching: Defining Straight Flows with Velocity Consistency</title><link>http://arxiv.org/abs/2407.02398v1</link><description>Flow matching (FM) is a general framework for defining probability paths viaOrdinary Differential Equations (ODEs) to transform between noise and datasamples. Recent approaches attempt to straighten these flow trajectories togenerate high-quality samples with fewer function evaluations, typicallythrough iterative rectification methods or optimal transport solutions. In thispaper, we introduce Consistency Flow Matching (Consistency-FM), a novel FMmethod that explicitly enforces self-consistency in the velocity field.Consistency-FM directly defines straight flows starting from different times tothe same endpoint, imposing constraints on their velocity values. Additionally,we propose a multi-segment training approach for Consistency-FM to enhanceexpressiveness, achieving a better trade-off between sampling quality andspeed. Preliminary experiments demonstrate that our Consistency-FMsignificantly improves training efficiency by converging 4.4x faster thanconsistency models and 1.7x faster than rectified flow models while achievingbetter generation quality. Our code is available at:https://github.com/YangLing0818/consistency_flow_matching</description><author>Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, Bin Cui</author><pubDate>Tue, 02 Jul 2024 17:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02398v1</guid></item><item><title>Learning to Refine with Fine-Grained Natural Language Feedback</title><link>http://arxiv.org/abs/2407.02397v1</link><description>Recent work has explored the capability of large language models (LLMs) toidentify and correct errors in LLM-generated responses. These refinementapproaches frequently evaluate what sizes of models are able to do refinementfor what problems, but less attention is paid to what effective feedback forrefinement looks like. In this work, we propose looking at refinement withfeedback as a composition of three distinct LLM competencies: (1)identification of bad generations; (2) fine-grained natural language feedbackgeneration; (3) refining with fine-grained feedback. The first step can beimplemented with a high-performing discriminative model and steps 2 and 3 canbe implemented either via prompted or fine-tuned LLMs. A key property of thisapproach is that the step 2 critique model can give fine-grained feedback abouterrors, made possible by offloading the discrimination to a separate model instep 1. We show that models of different capabilities benefit from refiningwith this approach on the task of improving factual consistency of documentgrounded summaries. Overall, our proposed method consistently outperformsexisting end-to-end refinement approaches and current trained models notfine-tuned for factuality critiquing.</description><author>Manya Wadhwa, Xinyu Zhao, Junyi Jessy Li, Greg Durrett</author><pubDate>Tue, 02 Jul 2024 17:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02397v1</guid></item><item><title>Is Your AI-Generated Code Really Secure? Evaluating Large Language Models on Secure Code Generation with CodeSecEval</title><link>http://arxiv.org/abs/2407.02395v1</link><description>Large language models (LLMs) have brought significant advancements to codegeneration and code repair, benefiting both novice and experienced developers.However, their training using unsanitized data from open-source repositories,like GitHub, raises the risk of inadvertently propagating securityvulnerabilities. Despite numerous studies investigating the safety of codeLLMs, there remains a gap in comprehensively addressing their securityfeatures. In this work, we aim to present a comprehensive study aimed atprecisely evaluating and enhancing the security aspects of code LLMs. Tosupport our research, we introduce CodeSecEval, a meticulously curated datasetdesigned to address 44 critical vulnerability types with 180 distinct samples.CodeSecEval serves as the foundation for the automatic evaluation of codemodels in two crucial tasks: code generation and code repair, with a strongemphasis on security. Our experimental results reveal that current modelsfrequently overlook security issues during both code generation and repairprocesses, resulting in the creation of vulnerable code. In response, wepropose different strategies that leverage vulnerability-aware information andinsecure code explanations to mitigate these security vulnerabilities.Furthermore, our findings highlight that certain vulnerability typesparticularly challenge model performance, influencing their effectiveness inreal-world applications. Based on these findings, we believe our study willhave a positive impact on the software engineering community, inspiring thedevelopment of improved methods for training and utilizing LLMs, therebyleading to safer and more trustworthy model deployment.</description><author>Jiexin Wang, Xitong Luo, Liuwen Cao, Hongkui He, Hailin Huang, Jiayuan Xie, Adam Jatowt, Yi Cai</author><pubDate>Tue, 02 Jul 2024 17:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02395v1</guid></item><item><title>Efficient Evolutionary Search Over Chemical Space with Large Language Models</title><link>http://arxiv.org/abs/2406.16976v2</link><description>Molecular discovery, when formulated as an optimization problem, presentssignificant computational challenges because optimization objectives can benon-differentiable. Evolutionary Algorithms (EAs), often used to optimizeblack-box objectives in molecular discovery, traverse chemical space byperforming random mutations and crossovers, leading to a large number ofexpensive objective evaluations. In this work, we ameliorate this shortcomingby incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely,we redesign crossover and mutation operations in EAs using LLMs trained onlarge corpora of chemical information. We perform extensive empirical studieson both commercial and open-source models on multiple tasks involving propertyoptimization, molecular rediscovery, and structure-based drug design,demonstrating that the joint usage of LLMs with EAs yields superior performanceover all baseline models across single- and multi-objective settings. Wedemonstrate that our algorithm improves both the quality of the final solutionand convergence speed, thereby reducing the number of required objectiveevaluations. Our code is available at http://github.com/zoom-wang112358/MOLLEO</description><author>Haorui Wang, Marta Skreta, Cher-Tian Ser, Wenhao Gao, Lingkai Kong, Felix Strieth-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, Alán Aspuru-Guzik, Kirill Neklyudov, Chao Zhang</author><pubDate>Tue, 02 Jul 2024 17:12:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16976v2</guid></item><item><title>Similarity Distance-Based Label Assignment for Tiny Object Detection</title><link>http://arxiv.org/abs/2407.02394v1</link><description>Tiny object detection is becoming one of the most challenging tasks incomputer vision because of the limited object size and lack of information. Thelabel assignment strategy is a key factor affecting the accuracy of objectdetection. Although there are some effective label assignment strategies fortiny objects, most of them focus on reducing the sensitivity to the boundingboxes to increase the number of positive samples and have some fixedhyperparameters need to set. However, more positive samples may not necessarilylead to better detection results, in fact, excessive positive samples may leadto more false positives. In this paper, we introduce a simple but effectivestrategy named the Similarity Distance (SimD) to evaluate the similaritybetween bounding boxes. This proposed strategy not only considers both locationand shape similarity but also learns hyperparameters adaptively, ensuring thatit can adapt to different datasets and various object sizes in a dataset. Ourapproach can be simply applied in common anchor-based detectors in place of theIoU for label assignment and Non Maximum Suppression (NMS). Extensiveexperiments on four mainstream tiny object detection datasets demonstratesuperior performance of our method, especially, 1.8 AP points and 4.1 AP pointsof very tiny higher than the state-of-the-art competitors on AI-TOD. Code isavailable at: \url{https://github.com/cszzshi/SimD}.</description><author>Shuohao Shi, Qiang Fang, Tong Zhao, Xin Xu</author><pubDate>Tue, 02 Jul 2024 17:12:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02394v1</guid></item><item><title>TokenPacker: Efficient Visual Projector for Multimodal LLM</title><link>http://arxiv.org/abs/2407.02392v1</link><description>The visual projector serves as an essential bridge between the visual encoderand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMsadopt a simple MLP to preserve all visual contexts via one-to-onetransformation. However, the visual tokens are redundant and can beconsiderably increased when dealing with high-resolution images, impairing theefficiency of MLLMs significantly. Some recent works have introduced resampleror abstractor to reduce the number of resulting visual tokens. Unfortunately,they fail to capture finer details and undermine the visual reasoningcapabilities of MLLMs. In this work, we propose a novel visual projector, whichadopts a coarse-to-fine scheme to inject the enriched characteristics togenerate the condensed visual tokens. In specific, we first interpolate thevisual features as a low-resolution point query, providing the overall visualrepresentation as the foundation. Then, we introduce a region-to-pointinjection module that utilizes high-resolution, multi-level region-based cuesas fine-grained reference keys and values, allowing them to be fully absorbedwithin the corresponding local context region. This step effectively updatesthe coarse point query, transforming it into an enriched one for the subsequentLLM reasoning. Extensive experiments demonstrate that our approach compressesthe visual tokens by 75%~89%, while achieves comparable or even betterperformance across diverse benchmarks with significantly higher efficiency. Thesource codes can be found at https://github.com/CircleRadon/TokenPacker.</description><author>Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jianke Zhu, Lei Zhang</author><pubDate>Tue, 02 Jul 2024 17:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02392v1</guid></item><item><title>Forward Learning for Gradient-based Black-box Saliency Map Generation</title><link>http://arxiv.org/abs/2403.15603v2</link><description>Gradient-based saliency maps are widely used to explain deep neural networkdecisions. However, as models become deeper and more black-box, such as inclosed-source APIs like ChatGPT, computing gradients become challenging,hindering conventional explanation methods. In this work, we introduce a novelunified framework for estimating gradients in black-box settings and generatingsaliency maps to interpret model decisions. We employ the likelihood ratiomethod to estimate output-to-input gradients and utilize them for saliency mapgeneration. Additionally, we propose blockwise computation techniques toenhance estimation accuracy. Extensive experiments in black-box settingsvalidate the effectiveness of our method, demonstrating accurate gradientestimation and explainability of generated saliency maps. Furthermore, weshowcase the scalability of our approach by applying it to explain GPT-Vision,revealing the continued relevance of gradient-based explanation methods in theera of large, closed-source, and black-box models.</description><author>Zeliang Zhang, Mingqian Feng, Jinyang Jiang, Rongyi Zhu, Yijie Peng, Chenliang Xu</author><pubDate>Tue, 02 Jul 2024 17:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15603v2</guid></item><item><title>Uncertainty-Aware Decarbonization for Datacenters</title><link>http://arxiv.org/abs/2407.02390v1</link><description>This paper represents the first effort to quantify uncertainty in carbonintensity forecasting for datacenter decarbonization. We identify and analyzetwo types of uncertainty -- temporal and spatial -- and discuss their systemimplications. To address the temporal dynamics in quantifying uncertainty forcarbon intensity forecasting, we introduce a conformal prediction-basedframework. Evaluation results show that our technique robustly achieves targetcoverages in uncertainty quantification across various significance levels. Weconduct two case studies using production power traces, focusing on temporaland spatial load shifting respectively. The results show that incorporatinguncertainty into scheduling decisions can prevent a 5% and 14% increase incarbon emissions, respectively. These percentages translate to an absolutereduction of 2.1 and 10.4 tons of carbon emissions in a 20 MW datacentercluster.</description><author>Amy Li, Sihang Liu, Yi Ding</author><pubDate>Tue, 02 Jul 2024 17:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02390v1</guid></item><item><title>SINCERE: Supervised Information Noise-Contrastive Estimation REvisited</title><link>http://arxiv.org/abs/2309.14277v3</link><description>The information noise-contrastive estimation (InfoNCE) loss function providesthe basis of many self-supervised deep learning methods due to its strongempirical results and theoretic motivation. Previous work suggests a supervisedcontrastive (SupCon) loss to extend InfoNCE to learn from available classlabels. This SupCon loss has been widely-used due to reports of good empiricalperformance. However, in this work we find that the prior SupCon lossformulation has questionable justification because it can encourage some imagesfrom the same class to repel one another in the learned embedding space. Thisproblematic intra-class repulsion gets worse as the number of images sharingone class label increases. We propose the Supervised InfoNCE REvisited(SINCERE) loss as a theoretically-justified supervised extension of InfoNCEthat eliminates intra-class repulsion. Experiments show that SINCERE leads tobetter separation of embeddings from different classes and improves transferlearning classification accuracy. We additionally utilize probabilisticmodeling to derive an information-theoretic bound that relates SINCERE loss tothe symmeterized KL divergence between data-generating distributions for atarget class and all other classes.</description><author>Patrick Feeney, Michael C. Hughes</author><pubDate>Tue, 02 Jul 2024 17:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14277v3</guid></item><item><title>SafaRi:Adaptive Sequence Transformer for Weakly Supervised Referring Expression Segmentation</title><link>http://arxiv.org/abs/2407.02389v1</link><description>Referring Expression Segmentation (RES) aims to provide a segmentation maskof the target object in an image referred to by the text (i.e., referringexpression). Existing methods require large-scale mask annotations. Moreover,such approaches do not generalize well to unseen/zero-shot scenarios. Toaddress the aforementioned issues, we propose a weakly-supervised bootstrappingarchitecture for RES with several new algorithmic innovations. To the best ofour knowledge, ours is the first approach that considers only a fraction ofboth mask and box annotations (shown in Figure 1 and Table 1) for training. Toenable principled training of models in such low-annotation settings, improveimage-text region-level alignment, and further enhance spatial localization ofthe target object in the image, we propose Cross-modal Fusion with AttentionConsistency module. For automatic pseudo-labeling of unlabeled samples, weintroduce a novel Mask Validity Filtering routine based on a spatially awarezero-shot proposal scoring approach. Extensive experiments show that with just30% annotations, our model SafaRi achieves 59.31 and 48.26 mIoUs as compared to58.93 and 48.19 mIoUs obtained by the fully-supervised SOTA method SeqTRrespectively on RefCOCO+@testA and RefCOCO+testB datasets. SafaRi alsooutperforms SeqTR by 11.7% (on RefCOCO+testA) and 19.6% (on RefCOCO+testB) in afully-supervised setting and demonstrates strong generalization capabilities inunseen/zero-shot tasks.</description><author>Sayan Nag, Koustava Goswami, Srikrishna Karanam</author><pubDate>Tue, 02 Jul 2024 17:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02389v1</guid></item><item><title>Real HSI-MSI-PAN image dataset for the hyperspectral/multi-spectral/panchromatic image fusion and super-resolution fields</title><link>http://arxiv.org/abs/2407.02387v1</link><description>Nowadays, most of the hyperspectral image (HSI) fusion experiments are basedon simulated datasets to compare different fusion methods. However, most of thespectral response functions and spatial downsampling functions used to createthe simulated datasets are not entirely accurate, resulting in deviations inspatial and spectral features between the generated images for fusion and thereal images for fusion. This reduces the credibility of the fusion algorithm,causing unfairness in the comparison between different algorithms and hinderingthe development of the field of hyperspectral image fusion. Therefore, werelease a real HSI/MSI/PAN image dataset to promote the development of thefield of hyperspectral image fusion. These three images are spatiallyregistered, meaning fusion can be performed between HSI and MSI, HSI and PANimage, MSI and PAN image, as well as among HSI, MSI, and PAN image. This realdataset could be available at https://aistudio.baidu.com/datasetdetail/281612.The related code to process the data could be available athttps://github.com/rs-lsl/CSSNet.</description><author>Shuangliang Li</author><pubDate>Tue, 02 Jul 2024 17:01:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02387v1</guid></item><item><title>OpenSlot: Mixed Open-set Recognition with Object-centric Learning</title><link>http://arxiv.org/abs/2407.02386v1</link><description>Existing open-set recognition (OSR) studies typically assume that each imagecontains only one class label, and the unknown test set (negative) has adisjoint label space from the known test set (positive), a scenario termedfull-label shift. This paper introduces the mixed OSR problem, where testimages contain multiple class semantics, with known and unknown classesco-occurring in negatives, leading to a more challenging super-label shift.Addressing the mixed OSR requires classification models to accuratelydistinguish different class semantics within images and measure their"knowness". In this study, we propose the OpenSlot framework, built uponobject-centric learning. OpenSlot utilizes slot features to represent diverseclass semantics and produce class predictions. Through our proposedanti-noise-slot (ANS) technique, we mitigate the impact of noise (invalid andbackground) slots during classification training, effectively addressing thesemantic misalignment between class predictions and the ground truth. Weconduct extensive experiments with OpenSlot on mixed &amp; conventional OSRbenchmarks. Without elaborate designs, OpenSlot not only exceeds existing OSRstudies in detecting super-label shifts across single &amp; multi-label mixed OSRtasks but also achieves state-of-the-art performance on conventionalbenchmarks. Remarkably, our method can localize class objects without usingbounding boxes during training. The competitive performance in open-set objectdetection demonstrates OpenSlot's ability to explicitly explain label shiftsand benefits in computational efficiency and generalization.</description><author>Xu Yin, Fei Pan, Guoyuan An, Yuchi Huo, Zixuan Xie, Sung-Eui Yoon</author><pubDate>Tue, 02 Jul 2024 17:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02386v1</guid></item><item><title>StructLDM: Structured Latent Diffusion for 3D Human Generation</title><link>http://arxiv.org/abs/2404.01241v3</link><description>Recent 3D human generative models have achieved remarkable progress bylearning 3D-aware GANs from 2D images. However, existing 3D human generativemethods model humans in a compact 1D latent space, ignoring the articulatedstructure and semantics of human body topology. In this paper, we explore moreexpressive and higher-dimensional latent space for 3D human modeling andpropose StructLDM, a diffusion-based unconditional 3D human generative model,which is learned from 2D images. StructLDM solves the challenges imposed due tothe high-dimensional growth of latent space with three key designs: 1) Asemantic structured latent space defined on the dense surface manifold of astatistical human body template. 2) A structured 3D-aware auto-decoder thatfactorizes the global latent space into several semantic body partsparameterized by a set of conditional structured local NeRFs anchored to thebody template, which embeds the properties learned from the 2D training dataand can be decoded to render view-consistent humans under different poses andclothing styles. 3) A structured latent diffusion model for generative humanappearance sampling. Extensive experiments validate StructLDM'sstate-of-the-art generation performance and illustrate the expressiveness ofthe structured latent space over the well-adopted 1D latent space. Notably,StructLDM enables different levels of controllable 3D human generation andediting, including pose/view/shape control, and high-level tasks includingcompositional generations, part-aware clothing editing, 3D virtual try-on, etc.Our project page is at: https://taohuumd.github.io/projects/StructLDM/.</description><author>Tao Hu, Fangzhou Hong, Ziwei Liu</author><pubDate>Tue, 02 Jul 2024 16:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01241v3</guid></item><item><title>Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs</title><link>http://arxiv.org/abs/2404.06025v2</link><description>Morphing attacks are an emerging threat to state-of-the-art Face Recognition(FR) systems, which aim to create a single image that contains the biometricinformation of multiple identities. Diffusion Morphs (DiM) are a recentlyproposed morphing attack that has achieved state-of-the-art performance forrepresentation-based morphing attacks. However, none of the existing researchon DiMs have leveraged the iterative nature of DiMs and left the DiM model as ablack box, treating it no differently than one would a Generative AdversarialNetwork (GAN) or Varational AutoEncoder (VAE). We propose a greedy strategy onthe iterative sampling process of DiM models which searches for an optimal stepguided by an identity-based heuristic function. We compare our proposedalgorithm against ten other state-of-the-art morphing algorithms using theopen-source SYN-MAD 2022 competition dataset. We find that our proposedalgorithm is unreasonably effective, fooling all of the tested FR systems withan MMPMR of 100%, outperforming all other morphing algorithms compared.</description><author>Zander W. Blasingame, Chen Liu</author><pubDate>Tue, 02 Jul 2024 16:48:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06025v2</guid></item><item><title>SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks</title><link>http://arxiv.org/abs/2401.15741v7</link><description>Improving the efficiency of state-of-the-art methods in semantic segmentationrequires overcoming the increasing computational cost as well as issues such asfusing semantic information from global and local contexts. Based on the recentsuccess and problems that convolutional neural networks (CNNs) encounter insemantic segmentation, this research proposes an encoder-decoder architecturewith a unique efficient residual network, Efficient-ResNet. Attention-boostinggates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming tofuse the equivariant and feature-based semantic information with the equivalentsizes of the output of global context of the efficient residual network in theencoder. Respectively, the decoder network is developed with the additionalattention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improvethe efficiency in the one-to-one conversion of the semantic information bydeploying additional convolution layers in the decoder part. Our network istested on the challenging CamVid and Cityscapes datasets, and the proposedmethods reveal significant improvements on the residual networks. To the bestof our knowledge, the developed network, SERNet-Former, achievesstate-of-the-art results (84.62 % mean IoU) on CamVid dataset and challengingresults (87.35 % mean IoU) on Cityscapes validation dataset.</description><author>Serdar Erisen</author><pubDate>Tue, 02 Jul 2024 16:48:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15741v7</guid></item><item><title>Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions</title><link>http://arxiv.org/abs/2207.12067v3</link><description>How can agents learn internal models that veridically represent interactionswith the real world is a largely open question. As machine learning is movingtowards representations containing not just observational but alsointerventional knowledge, we study this problem using tools from representationlearning and group theory. We propose methods enabling an agent acting upon theworld to learn internal representations of sensory information that areconsistent with actions that modify it. We use an autoencoder equipped with agroup representation acting on its latent space, trained using anequivariance-derived loss in order to enforce a suitable homomorphism propertyon the group representation. In contrast to existing work, our approach doesnot require prior knowledge of the group and does not restrict the set ofactions the agent can perform. We motivate our method theoretically, and showempirically that it can learn a group representation of the actions, therebycapturing the structure of the set of transformations applied to theenvironment. We further show that this allows agents to predict the effect ofsequences of future actions with improved accuracy.</description><author>Hamza Keurti, Hsiao-Ru Pan, Michel Besserve, Benjamin F. Grewe, Bernhard Schölkopf</author><pubDate>Tue, 02 Jul 2024 16:46:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.12067v3</guid></item><item><title>xLSTM-UNet can be an Effective 2D &amp; 3D Medical Image Segmentation Backbone with Vision-LSTM (ViL) better than its Mamba Counterpart</title><link>http://arxiv.org/abs/2407.01530v2</link><description>Convolutional Neural Networks (CNNs) and Vision Transformers (ViT) have beenpivotal in biomedical image segmentation, yet their ability to managelong-range dependencies remains constrained by inherent locality andcomputational overhead. To overcome these challenges, in this technical report,we first propose xLSTM-UNet, a UNet structured deep learning neural networkthat leverages Vision-LSTM (xLSTM) as its backbone for medical imagesegmentation. xLSTM is a recently proposed as the successor of Long Short-TermMemory (LSTM) networks and have demonstrated superior performance compared toTransformers and State Space Models (SSMs) like Mamba in Neural LanguageProcessing (NLP) and image classification (as demonstrated in Vision-LSTM, orViL implementation). Here, xLSTM-UNet we designed extend the success inbiomedical image segmentation domain. By integrating the local featureextraction strengths of convolutional layers with the long-range dependencycapturing abilities of xLSTM, xLSTM-UNet offers a robust solution forcomprehensive image analysis. We validate the efficacy of xLSTM-UNet throughexperiments. Our findings demonstrate that xLSTM-UNet consistently surpassesthe performance of leading CNN-based, Transformer-based, and Mamba-basedsegmentation networks in multiple datasets in biomedical segmentation includingorgans in abdomen MRI, instruments in endoscopic images, and cells inmicroscopic images. With comprehensive experiments performed, this technicalreport highlights the potential of xLSTM-based architectures in advancingbiomedical image analysis in both 2D and 3D. The code, models, and datasets arepublicly available at http://tianrun-chen.github.io/xLSTM-UNet/</description><author>Tianrun Chen, Chaotao Ding, Lanyun Zhu, Tao Xu, Deyi Ji, Yan Wang, Ying Zang, Zejian Li</author><pubDate>Tue, 02 Jul 2024 16:45:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01530v2</guid></item><item><title>Steerable Pyramid Transform Enables Robust Left Ventricle Quantification</title><link>http://arxiv.org/abs/2201.08388v2</link><description>Predicting cardiac indices has long been a focal point in the medical imagingcommunity. While various deep learning models have demonstrated success inquantifying cardiac indices, they remain susceptible to mild inputperturbations, e.g., spatial transformations, image distortions, andadversarial attacks. This vulnerability undermines confidence in usinglearning-based automated systems for diagnosing cardiovascular diseases. Inthis work, we describe a simple yet effective method to learn robust models forleft ventricle (LV) quantification, encompassing cavity and myocardium areas,directional dimensions, and regional wall thicknesses. Our success hinges onemploying the biologically inspired steerable pyramid transform (SPT) for fixedfront-end processing, which offers three main benefits. First, the basisfunctions of SPT align with the anatomical structure of LV and the geometricfeatures of the measured indices. Second, SPT facilitates weight sharing acrossdifferent orientations as a form of parameter regularization and naturallycaptures the scale variations of LV. Third, the residual highpass subband canbe conveniently discarded, promoting robust feature learning. Extensiveexperiments on the Cardiac-Dig benchmark show that our SPT-augmented model notonly achieves reasonable prediction accuracy compared to state-of-the-artmethods, but also exhibits significantly improved robustness against inputperturbations.</description><author>Xiangyang Zhu, Kede Ma, Wufeng Xue</author><pubDate>Tue, 02 Jul 2024 16:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.08388v2</guid></item><item><title>OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation</title><link>http://arxiv.org/abs/2407.02371v1</link><description>Text-to-video (T2V) generation has recently garnered significant attentionthanks to the large multi-modality model Sora. However, T2V generation stillfaces two important challenges: 1) Lacking a precise open sourced high-qualitydataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,are either with low quality or too large for most research institutions.Therefore, it is challenging but crucial to collect a precise high-qualitytext-video pairs for T2V generation. 2) Ignoring to fully utilize textualinformation. Recent T2V methods have focused on vision transformers, using asimple cross attention module for video generation, which falls short ofthoroughly extracting semantic information from text prompt. To address theseissues, we introduce OpenVid-1M, a precise high-quality dataset with expressivecaptions. This open-scenario dataset contains over 1 million text-video pairs,facilitating research on T2V generation. Furthermore, we curate 433K 1080pvideos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definitionvideo generation. Additionally, we propose a novel Multi-modal Video DiffusionTransformer (MVDiT) capable of mining both structure information from visualtokens and semantic information from text tokens. Extensive experiments andablation studies verify the superiority of OpenVid-1M over previous datasetsand the effectiveness of our MVDiT.</description><author>Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, Ying Tai</author><pubDate>Tue, 02 Jul 2024 16:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02371v1</guid></item><item><title>Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion</title><link>http://arxiv.org/abs/2407.01392v2</link><description>This paper presents Diffusion Forcing, a new training paradigm where adiffusion model is trained to denoise a set of tokens with independentper-token noise levels. We apply Diffusion Forcing to sequence generativemodeling by training a causal next-token prediction model to generate one orseveral future tokens without fully diffusing past ones. Our approach is shownto combine the strengths of next-token prediction models, such asvariable-length generation, with the strengths of full-sequence diffusionmodels, such as the ability to guide sampling to desirable trajectories. Ourmethod offers a range of additional capabilities, such as (1) rolling-outsequences of continuous tokens, such as video, with lengths past the traininghorizon, where baselines diverge and (2) new sampling and guiding schemes thatuniquely profit from Diffusion Forcing's variable-horizon and causalarchitecture, and which lead to marked performance gains in decision-making andplanning tasks. In addition to its empirical success, our method is proven tooptimize a variational lower bound on the likelihoods of all subsequences oftokens drawn from the true joint distribution. Project website:https://boyuan.space/diffusion-forcing/</description><author>Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, Vincent Sitzmann</author><pubDate>Tue, 02 Jul 2024 16:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01392v2</guid></item><item><title>Investigating Event-Based Cameras for Video Frame Interpolation in Sports</title><link>http://arxiv.org/abs/2407.02370v1</link><description>Slow-motion replays provide a thrilling perspective on pivotal moments withinsports games, offering a fresh and captivating visual experience. However,capturing slow-motion footage typically demands high-tech, expensive camerasand infrastructures. Deep learning Video Frame Interpolation (VFI) techniqueshave emerged as a promising avenue, capable of generating high-speed footagefrom regular camera feeds. Moreover, the utilization of event-based cameras hasrecently gathered attention as they provide valuable motion information betweenframes, further enhancing the VFI performances. In this work, we present afirst investigation of event-based VFI models for generating sports slow-motionvideos. Particularly, we design and implement a bi-camera recording setup,including an RGB and an event-based camera to capture sports videos, totemporally align and spatially register both cameras. Our experimentalvalidation demonstrates that TimeLens, an off-the-shelf event-based VFI model,can effectively generate slow-motion footage for sports videos. This firstinvestigation underscores the practical utility of event-based cameras inproducing sports slow-motion content and lays the groundwork for futureresearch endeavors in this domain.</description><author>Antoine Deckyvere, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck</author><pubDate>Tue, 02 Jul 2024 16:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02370v1</guid></item><item><title>Two-Step Q-Learning</title><link>http://arxiv.org/abs/2407.02369v1</link><description>Q-learning is a stochastic approximation version of the classic valueiteration. The literature has established that Q-learning suffers from bothmaximization bias and slower convergence. Recently, multi-step algorithms haveshown practical advantages over existing methods. This paper proposes a noveloff-policy two-step Q-learning algorithms, without importance sampling. Withsuitable assumption it was shown that, iterates in the proposed two-stepQ-learning is bounded and converges almost surely to the optimal Q-values. Thisstudy also address the convergence analysis of the smooth version of two-stepQ-learning, i.e., by replacing max function with the log-sum-exp function. Theproposed algorithms are robust and easy to implement. Finally, we test theproposed algorithms on benchmark problems such as the roulette problem,maximization bias problem, and randomly generated Markov decision processes andcompare it with the existing methods available in literature. Numericalexperiments demonstrate the superior performance of both the two-stepQ-learning and its smooth variants.</description><author>Antony Vijesh, Shreyas S R</author><pubDate>Tue, 02 Jul 2024 16:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02369v1</guid></item><item><title>Let Guidelines Guide You: A Prescriptive Guideline-Centered Data Annotation Methodology</title><link>http://arxiv.org/abs/2406.14099v2</link><description>We introduce the Guideline-Centered annotation process, a novel dataannotation methodology focused on reporting the annotation guidelinesassociated with each data sample. We identify three main limitations of thestandard prescriptive annotation process and describe how theGuideline-Centered methodology overcomes them by reducing the loss ofinformation in the annotation process and ensuring adherence to guidelines.Additionally, we discuss how the Guideline-Centered enables the reuse ofannotated data across multiple tasks at the cost of a single human-annotationprocess.</description><author>Federico Ruggeri, Eleonora Misino, Arianna Muti, Katerina Korre, Paolo Torroni, Alberto Barrón-Cedeño</author><pubDate>Tue, 02 Jul 2024 16:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14099v2</guid></item><item><title>Reproducibility in Machine Learning-based Research: Overview, Barriers and Drivers</title><link>http://arxiv.org/abs/2406.14325v2</link><description>Research in various fields is currently experiencing challenges regarding thereproducibility of results. This problem is also prevalent in machine learning(ML) research. The issue arises, for example, due to unpublished data and/orsource code and the sensitivity of ML training conditions. Although differentsolutions have been proposed to address this issue, such as using ML platforms,the level of reproducibility in ML-driven research remains unsatisfactory.Therefore, in this article, we discuss the reproducibility of ML-drivenresearch with three main aims: (i) identifying the barriers to reproducibilitywhen applying ML in research as well as categorize the barriers to differenttypes of reproducibility (description, code, data, and experimentreproducibility), (ii) discussing potential drivers such as tools, practices,and interventions that support ML reproducibility, as well as distinguishbetween technology-driven drivers, procedural drivers, and drivers related toawareness and education, and (iii) mapping the drivers to the barriers. Withthis work, we hope to provide insights and to contribute to the decision-makingprocess regarding the adoption of different solutions to support MLreproducibility.</description><author>Harald Semmelrock, Tony Ross-Hellauer, Simone Kopeinik, Dieter Theiler, Armin Haberl, Stefan Thalmann, Dominik Kowald</author><pubDate>Tue, 02 Jul 2024 16:36:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14325v2</guid></item><item><title>Ents: An Efficient Three-party Training Framework for Decision Trees by Communication Optimization</title><link>http://arxiv.org/abs/2406.07948v4</link><description>Multi-party training frameworks for decision trees based on securemulti-party computation enable multiple parties to train high-performancemodels on distributed private data with privacy preservation. The trainingprocess essentially involves frequent dataset splitting according to thesplitting criterion (e.g. Gini impurity). However, existing multi-partytraining frameworks for decision trees demonstrate communication inefficiencydue to the following issues: (1) They suffer from huge communication overheadin securely splitting a dataset with continuous attributes. (2) They sufferfrom huge communication overhead due to performing almost all the computationson a large ring to accommodate the secure computations for the splittingcriterion. In this paper, we are motivated to present an efficient three-party trainingframework, namely Ents, for decision trees by communication optimization. Forthe first issue, we present a series of training protocols based on the secureradix sort protocols to efficiently and securely split a dataset withcontinuous attributes. For the second issue, we propose an efficient shareconversion protocol to convert shares between a small ring and a large ring toreduce the communication overhead incurred by performing almost all thecomputations on a large ring. Experimental results from eight widely useddatasets show that Ents outperforms state-of-the-art frameworks by $5.5\times\sim 9.3\times$ in communication sizes and $3.9\times \sim 5.3\times$ incommunication rounds. In terms of training time, Ents yields an improvement of$3.5\times \sim 6.7\times$. To demonstrate its practicality, Ents requires lessthan three hours to securely train a decision tree on a widely used real-worlddataset (Skin Segmentation) with more than 245,000 samples in the WAN setting.</description><author>Guopeng Lin, Weili Han, Wenqiang Ruan, Ruisheng Zhou, Lushan Song, Bingshuai Li, Yunfeng Shao</author><pubDate>Tue, 02 Jul 2024 16:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07948v4</guid></item><item><title>ROS-LLM: A ROS framework for embodied AI with task feedback and structured reasoning</title><link>http://arxiv.org/abs/2406.19741v2</link><description>We present a framework for intuitive robot programming by non-experts,leveraging natural language prompts and contextual information from the RobotOperating System (ROS). Our system integrates large language models (LLMs),enabling non-experts to articulate task requirements to the system through achat interface. Key features of the framework include: integration of ROS withan AI agent connected to a plethora of open-source and commercial LLMs,automatic extraction of a behavior from the LLM output and execution of ROSactions/services, support for three behavior modes (sequence, behavior tree,state machine), imitation learning for adding new robot actions to the libraryof possible actions, and LLM reflection via human and environment feedback.Extensive experiments validate the framework, showcasing robustness,scalability, and versatility in diverse scenarios, including long-horizontasks, tabletop rearrangements, and remote supervisory control. To facilitatethe adoption of our framework and support the reproduction of our results, wehave made our code open-source. You can access it at:https://github.com/huawei-noah/HEBO/tree/master/ROSLLM.</description><author>Christopher E. Mower, Yuhui Wan, Hongzhan Yu, Antoine Grosnit, Jonas Gonzalez-Billandon, Matthieu Zimmer, Jinlong Wang, Xinyu Zhang, Yao Zhao, Anbang Zhai, Puze Liu, Daniel Palenicek, Davide Tateo, Cesar Cadena, Marco Hutter, Jan Peters, Guangjian Tian, Yuzheng Zhuang, Kun Shao, Xingyue Quan, Jianye Hao, Jun Wang, Haitham Bou-Ammar</author><pubDate>Tue, 02 Jul 2024 16:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19741v2</guid></item><item><title>ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos</title><link>http://arxiv.org/abs/2406.19392v2</link><description>We introduce ReXTime, a benchmark designed to rigorously test AI models'ability to perform temporal reasoning within video events. Specifically,ReXTime focuses on reasoning across time, i.e. human-like understanding whenthe question and its corresponding answer occur in different video segments.This form of reasoning, requiring advanced understanding of cause-and-effectrelationships across video segments, poses significant challenges to even thefrontier multimodal large language models. To facilitate this evaluation, wedevelop an automated pipeline for generating temporal reasoning question-answerpairs, significantly reducing the need for labor-intensive manual annotations.Our benchmark includes 921 carefully vetted validation samples and 2,143 testsamples, each manually curated for accuracy and relevance. Evaluation resultsshow that while frontier large language models outperform academic models, theystill lag behind human performance by a significant 14.3% accuracy gap.Additionally, our pipeline creates a training dataset of 9,695 machinegenerated samples without manual effort, which empirical studies suggest canenhance the across-time reasoning via fine-tuning.</description><author>Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, Yen-Chun Chen, Yu-Chiang Frank Wang</author><pubDate>Tue, 02 Jul 2024 16:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19392v2</guid></item><item><title>Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA</title><link>http://arxiv.org/abs/2407.02362v1</link><description>Modern Neural Network (NN) architectures heavily rely on vast numbers ofmultiply-accumulate arithmetic operations, constituting the predominantcomputational cost. Therefore, this paper proposes a high-throughput, scalableand energy efficient non-element-wise matrix multiplication unit on FPGAs as abasic component of the NNs. We firstly streamline inter-layer and intra-layerredundancies of MADDNESS algorithm, a LUT-based approximate matrixmultiplication, to design a fast, efficient scalable approximate matrixmultiplication module termed "Approximate Multiplication Unit (AMU)". The AMUoptimizes LUT-based matrix multiplications further through dedicated memorymanagement and access design, decoupling computational overhead from inputresolution and boosting FPGA-based NN accelerator efficiency significantly. Theexperimental results show that using our AMU achieves up to 9x higherthroughput and 112x higher energy efficiency over the state-of-the-artsolutions for the FPGA-based Quantised Neural Network (QNN) accelerators.</description><author>Xuqi Zhu, Huaizhi Zhang, JunKyu Lee, Jiacheng Zhu, Chandrajit Pal, Sangeet Saha, Klaus D. McDonald-Maier, Xiaojun Zhai</author><pubDate>Tue, 02 Jul 2024 16:28:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02362v1</guid></item><item><title>GCF: Graph Convolutional Networks for Facial Expression Recognition</title><link>http://arxiv.org/abs/2407.02361v1</link><description>Facial Expression Recognition (FER) is vital for understanding interpersonalcommunication. However, existing classification methods often face challengessuch as vulnerability to noise, imbalanced datasets, overfitting, andgeneralization issues. In this paper, we propose GCF, a novel approach thatutilizes Graph Convolutional Networks for FER. GCF integrates ConvolutionalNeural Networks (CNNs) for feature extraction, using either customarchitectures or pretrained models. The extracted visual features are thenrepresented on a graph, enhancing local CNN features with global features via aGraph Convolutional Neural Network layer. We evaluate GCF on benchmark datasetsincluding CK+, JAFFE, and FERG. The results show that GCF significantlyimproves performance over state-of-the-art methods. For example, GCF enhancesthe accuracy of ResNet18 from 92% to 98% on CK+, from 66% to 89% on JAFFE, andfrom 94% to 100% on FERG. Similarly, GCF improves the accuracy of VGG16 from89% to 97% on CK+, from 72% to 92% on JAFFE, and from 96% to 99.49% on FERG. Weprovide a comprehensive analysis of our approach, demonstrating itseffectiveness in capturing nuanced facial expressions. By integrating graphconvolutions with CNNs, GCF significantly advances FER, offering improvedaccuracy and robustness in real-world applications.</description><author>Hozaifa Kassab, Mohamed Bahaa, Ali Hamdi</author><pubDate>Tue, 02 Jul 2024 16:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02361v1</guid></item><item><title>SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration</title><link>http://arxiv.org/abs/2312.07541v3</link><description>Recent techniques for real-time view synthesis have rapidly advanced infidelity and speed, and modern methods are capable of renderingnear-photorealistic scenes at interactive frame rates. At the same time, atension has arisen between explicit scene representations amenable torasterization and neural fields built on ray marching, with state-of-the-artinstances of the latter surpassing the former in quality while beingprohibitively expensive for real-time applications. In this work, we introduceSMERF, a view synthesis approach that achieves state-of-the-art accuracy amongreal-time methods on large scenes with footprints up to 300 m$^2$ at avolumetric resolution of 3.5 mm$^3$. Our method is built upon two primarycontributions: a hierarchical model partitioning scheme, which increases modelcapacity while constraining compute and memory consumption, and a distillationtraining strategy that simultaneously yields high fidelity and internalconsistency. Our approach enables full six degrees of freedom (6DOF) navigationwithin a web browser and renders in real-time on commodity smartphones andlaptops. Extensive experiments show that our method exceeds the currentstate-of-the-art in real-time novel view synthesis by 0.78 dB on standardbenchmarks and 1.78 dB on large scenes, renders frames three orders ofmagnitude faster than state-of-the-art radiance field models, and achievesreal-time performance across a wide variety of commodity devices, includingsmartphones. We encourage readers to explore these models interactively at ourproject website: https://smerf-3d.github.io.</description><author>Daniel Duckworth, Peter Hedman, Christian Reiser, Peter Zhizhin, Jean-François Thibert, Mario Lučić, Richard Szeliski, Jonathan T. Barron</author><pubDate>Tue, 02 Jul 2024 16:26:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07541v3</guid></item><item><title>Contrastive independent component analysis</title><link>http://arxiv.org/abs/2407.02357v1</link><description>Visualizing data and finding patterns in data are ubiquitous problems in thesciences. Increasingly, applications seek signal and structure in a contrastivesetting: a foreground dataset relative to a background dataset. For thispurpose, we propose contrastive independent component analysis (cICA). Thisgeneralizes independent component analysis to independent latent variablesacross a foreground and background. We propose a hierarchical tensordecomposition algorithm for cICA. We study the identifiability of cICA anddemonstrate its performance visualizing data and finding patterns in data,using synthetic and real-world datasets, comparing the approach to existingcontrastive methods.</description><author>Kexin Wang, Aida Maraj, Anna Seigal</author><pubDate>Tue, 02 Jul 2024 16:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02357v1</guid></item><item><title>Enable the Right to be Forgotten with Federated Client Unlearning in Medical Imaging</title><link>http://arxiv.org/abs/2407.02356v1</link><description>The right to be forgotten, as stated in most data regulations, poses anunderexplored challenge in federated learning (FL), leading to the developmentof federated unlearning (FU). However, current FU approaches often facetrade-offs between efficiency, model performance, forgetting efficacy, andprivacy preservation. In this paper, we delve into the paradigm of FederatedClient Unlearning (FCU) to guarantee a client the right to erase thecontribution or the influence, introducing the first FU framework in medicalimaging. In the unlearning process of a client, the proposed model-contrastiveunlearning marks a pioneering step towards feature-level unlearning, andfrequency-guided memory preservation ensures smooth forgetting of localknowledge while maintaining the generalizability of the trained global model,thus avoiding performance compromises and guaranteeing rapid post-training. Weevaluated our FCU framework on two public medical image datasets, includingIntracranial hemorrhage diagnosis and skin lesion diagnosis, demonstrating thatour framework outperformed other state-of-the-art FU frameworks, with anexpected speed-up of 10-15 times compared with retraining from scratch. Thecode and the organized datasets can be found at:https://github.com/dzp2095/FCU.</description><author>Zhipeng Deng, Luyang Luo, Hao Chen</author><pubDate>Tue, 02 Jul 2024 16:21:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02356v1</guid></item><item><title>Talking to Machines: do you read me?</title><link>http://arxiv.org/abs/2407.02354v1</link><description>In this dissertation I would like to guide the reader to the research ondialogue but more precisely the research I have conducted during my careersince my PhD thesis. Starting from modular architectures with machinelearning/deep learning and reinforcement learning to end-to-end deep neuralnetworks. Besides my work as research associate, I also present the work I havesupervised in the last years. I review briefly the state of the art and highlight the open researchproblems on conversational agents. Afterwards, I present my contribution toTask-Oriented Dialogues (TOD), both as research associate and as the industrialsupervisor of CIFRE theses. I discuss conversational QA. Particularly, Ipresent the work of two PhD candidates Thibault Cordier and Sebastien Montella;as well as the work of the young researcher Quentin Brabant. Finally, I presentthe scientific project, where I discuss about Large Language Models (LLMs) forTask-Oriented Dialogue and Multimodal Task-Oriented Dialogue.</description><author>Lina M. Rojas-Barahona</author><pubDate>Tue, 02 Jul 2024 16:19:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02354v1</guid></item><item><title>Contractual Reinforcement Learning: Pulling Arms with Invisible Hands</title><link>http://arxiv.org/abs/2407.01458v2</link><description>The agency problem emerges in today's large scale machine learning tasks,where the learners are unable to direct content creation or enforce datacollection. In this work, we propose a theoretical framework for aligningeconomic interests of different stakeholders in the online learning problemsthrough contract design. The problem, termed \emph{contractual reinforcementlearning}, naturally arises from the classic model of Markov decisionprocesses, where a learning principal seeks to optimally influence the agent'saction policy for their common interests through a set of payment rulescontingent on the realization of next state. For the planning problem, wedesign an efficient dynamic programming algorithm to determine the optimalcontracts against the far-sighted agent. For the learning problem, we introducea generic design of no-regret learning algorithms to untangle the challengesfrom robust design of contracts to the balance of exploration and exploitation,reducing the complexity analysis to the construction of efficient searchalgorithms. For several natural classes of problems, we design tailored searchalgorithms that provably achieve $\tilde{O}(\sqrt{T})$ regret. We also presentan algorithm with $\tilde{O}(T^{2/3})$ for the general problem that improvesthe existing analysis in online contract design with mild technicalassumptions.</description><author>Jibang Wu, Siyu Chen, Mengdi Wang, Huazheng Wang, Haifeng Xu</author><pubDate>Tue, 02 Jul 2024 16:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01458v2</guid></item><item><title>Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification</title><link>http://arxiv.org/abs/2407.02352v1</link><description>Large Visual Language Models (LVLMs) struggle with hallucinations in visualinstruction following task(s), limiting their trustworthiness and real-worldapplicability. We propose Pelican -- a novel framework designed to detect andmitigate hallucinations through claim verification. Pelican first decomposesthe visual claim into a chain of sub-claims based on first-order predicates.These sub-claims consist of (predicate, question) pairs and can beconceptualized as nodes of a computational graph. We then useProgram-of-Thought prompting to generate Python code for answering thesequestions through flexible composition of external tools. Pelican improves overprior work by introducing (1) intermediate variables for precise grounding ofobject instances, and (2) shared computation for answering the sub-question toenable adaptive corrections and inconsistency identification. We finally usereasoning abilities of LLM to verify the correctness of the the claim byconsidering the consistency and confidence of the (question, answer) pairs fromeach sub-claim. Our experiments reveal a drop in hallucination rate by$\sim$8%-32% across various baseline LVLMs and a 27% drop compared toapproaches proposed for hallucination mitigation on MMHal-Bench. Results on twoother benchmarks further corroborate our results.</description><author>Pritish Sahu, Karan Sikka, Ajay Divakaran</author><pubDate>Tue, 02 Jul 2024 16:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02352v1</guid></item><item><title>Generative Large Language Models in Automated Fact-Checking: A Survey</title><link>http://arxiv.org/abs/2407.02351v1</link><description>The dissemination of false information across online platforms poses aserious societal challenge, necessitating robust measures for informationverification. While manual fact-checking efforts are still instrumental, thegrowing volume of false information requires automated methods. Large languagemodels (LLMs) offer promising opportunities to assist fact-checkers, leveragingLLM's extensive knowledge and robust reasoning capabilities. In this surveypaper, we investigate the utilization of generative LLMs in the realm offact-checking, illustrating various approaches that have been employed andtechniques for prompting or fine-tuning LLMs. By providing an overview ofexisting approaches, this survey aims to improve the understanding of utilizingLLMs in fact-checking and to facilitate further progress in LLMs' involvementin this process.</description><author>Ivan Vykopal, Matúš Pikuliak, Simon Ostermann, Marián Šimko</author><pubDate>Tue, 02 Jul 2024 16:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02351v1</guid></item><item><title>Conceptual Codebook Learning for Vision-Language Models</title><link>http://arxiv.org/abs/2407.02350v1</link><description>In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novelfine-tuning method for vision-language models (VLMs) to address the challengeof improving the generalization capability of VLMs while fine-tuning them ondownstream tasks in a few-shot setting. We recognize that visual concepts, suchas textures, shapes, and colors are naturally transferable across domains andplay a crucial role in generalization tasks. Motivated by this interestingfinding, we learn a conceptual codebook consisting of visual concepts as keysand conceptual prompts as values, which serves as a link between the imageencoder's outputs and the text encoder's inputs. Specifically, for a givenimage, we leverage the codebook to identify the most relevant conceptualprompts associated with the class embeddings to perform the classification.Additionally, we incorporate a handcrafted concept cache as a regularization toalleviate the overfitting issues in low-shot scenarios. We observe that thisconceptual codebook learning method is able to achieve enhanced alignmentbetween visual and linguistic modalities. Extensive experimental resultsdemonstrate that our CoCoLe method remarkably outperforms the existingstate-of-the-art methods across various evaluation settings, includingbase-to-new generalization, cross-dataset evaluation, and domain generalizationtasks. Detailed ablation studies further confirm the efficacy of each componentin CoCoLe.</description><author>Yi Zhang, Ke Yu, Siqi Wu, Zhihai He</author><pubDate>Tue, 02 Jul 2024 16:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02350v1</guid></item><item><title>Dynamic Q&amp;A of Clinical Documents with Large Language Models</title><link>http://arxiv.org/abs/2401.10733v2</link><description>Electronic health records (EHRs) house crucial patient data in clinicalnotes. As these notes grow in volume and complexity, manual extraction becomeschallenging. This work introduces a natural language interface using largelanguage models (LLMs) for dynamic question-answering on clinical notes. Ourchatbot, powered by Langchain and transformer-based LLMs, allows users to queryin natural language, receiving relevant answers from clinical notes.Experiments, utilizing various embedding models and advanced LLMs, show WizardVicuna's superior accuracy, albeit with high compute demands. Modeloptimization, including weight quantization, improves latency by approximately48 times. Promising results indicate potential, yet challenges such as modelhallucinations and limited diverse medical case evaluations remain. Addressingthese gaps is crucial for unlocking the value in clinical notes and advancingAI-driven clinical decision-making.</description><author>Ran Elgedawy, Ioana Danciu, Maria Mahbub, Sudarshan Srinivasan</author><pubDate>Tue, 02 Jul 2024 16:14:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10733v2</guid></item><item><title>Revisiting Cascaded Ensembles for Efficient Inference</title><link>http://arxiv.org/abs/2407.02348v1</link><description>A common approach to make machine learning inference more efficient is to useexample-specific adaptive schemes, which route or select models for eachexample at inference time. In this work we study a simple scheme for adaptiveinference. We build a cascade of ensembles (CoE), beginning withresource-efficient models and growing to larger, more expressive models, whereensemble agreement serves as a data-dependent routing criterion. This scheme iseasy to incorporate into existing inference pipelines, requires no additionaltraining, and can be used to place models across multiple resource tiers--forinstance, serving efficient models at the edge and invoking larger models inthe cloud only when necessary. In cases where parallel inference is feasible,we show that CoE can improve accuracy relative to the single best model whilereducing the average cost of inference by up to 7x, and providesPareto-dominate solutions in accuracy and efficiency relative to existingadaptive inference baselines. These savings translate to an over 3x-reductionin total monetary cost when performing inference using a heterogeneous clusterof GPUs. Finally, for edge inference scenarios where portions of the cascadereside at the edge vs. in the cloud, CoE can provide a 14x reduction incommunication cost and inference latency without sacrificing accuracy.</description><author>Steven Kolawole, Don Dennis, Ameet Talwalkar, Virginia Smith</author><pubDate>Tue, 02 Jul 2024 16:14:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02348v1</guid></item><item><title>MORPHEUS: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space</title><link>http://arxiv.org/abs/2407.02345v1</link><description>Personalized Dialogue Generation (PDG) aims to create coherent responsesaccording to roles or personas. Traditional PDG relies on external role data,which can be scarce and raise privacy concerns. Approaches address these issuesby extracting role information from dialogue history, which often fail togenerically model roles in continuous space. To overcome these limitations, weintroduce a novel framework \textbf{MO}dels \textbf{R}oles from\textbf{P}ersonalized Dialogue \textbf{H}istory by \textbf{E}xploring and\textbf{U}tilizing Latent \textbf{S}pace (MORPHEUS) through a three-stagetraining process. Specifically, we create a persona codebook to represent rolesin latent space compactly, and this codebook is used to construct a posteriordistribution of role information. This method enables the model to generalizeacross roles, allowing the generation of personalized dialogues even for unseenroles. Experiments on both Chinese and English datasets demonstrate thatMORPHEUS enhances the extraction of role information, and improves responsegeneration without external role data. Additionally, MORPHEUS can be consideredan efficient fine-tuning for large language models.</description><author>Yihong Tang, Bo Wang, Dongming Zhao, Xiaojia Jin, Jijun Zhang, Ruifang He, Yuexian Hou</author><pubDate>Tue, 02 Jul 2024 16:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02345v1</guid></item><item><title>SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark</title><link>http://arxiv.org/abs/2310.20436v3</link><description>We present SignAvatars, the first large-scale, multi-prompt 3D sign language(SL) motion dataset designed to bridge the communication gap for Deaf andhard-of-hearing individuals. While there has been an exponentially growingnumber of research regarding digital communication, the majority of existingcommunication technologies primarily cater to spoken or written languages,instead of SL, the essential communication method for Deaf and hard-of-hearingcommunities. Existing SL datasets, dictionaries, and sign language production(SLP) methods are typically limited to 2D as annotating 3D models and avatarsfor SL is usually an entirely manual and labor-intensive process conducted bySL experts, often resulting in unnatural avatars. In response to thesechallenges, we compile and curate the SignAvatars dataset, which comprises70,000 videos from 153 signers, totaling 8.34 million frames, covering bothisolated signs and continuous, co-articulated signs, with multiple promptsincluding HamNoSys, spoken language, and words. To yield 3D holisticannotations, including meshes and biomechanically-valid poses of body, hands,and face, as well as 2D and 3D keypoints, we introduce an automated annotationpipeline operating on our large corpus of SL videos. SignAvatars facilitatesvarious tasks such as 3D sign language recognition (SLR) and the novel 3D SLproduction (SLP) from diverse inputs like text scripts, individual words, andHamNoSys notation. Hence, to evaluate the potential of SignAvatars, we furtherpropose a unified benchmark of 3D SL holistic motion production. We believethat this work is a significant step forward towards bringing the digital worldto the Deaf and hard-of-hearing communities as well as people interacting withthem.</description><author>Zhengdi Yu, Shaoli Huang, Yongkang Cheng, Tolga Birdal</author><pubDate>Tue, 02 Jul 2024 16:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20436v3</guid></item><item><title>RVISA: Reasoning and Verification for Implicit Sentiment Analysis</title><link>http://arxiv.org/abs/2407.02340v1</link><description>With an increasing social demand for fine-grained sentiment analysis (SA),implicit sentiment analysis (ISA) poses a significant challenge with theabsence of salient cue words in expressions. It necessitates reliable reasoningto understand how the sentiment is aroused and thus determine implicitsentiments. In the era of Large Language Models (LLMs), Encoder-Decoder (ED)LLMs have gained popularity to serve as backbone models for SA applications,considering impressive text comprehension and reasoning ability among diversetasks. On the other hand, Decoder-only (DO) LLMs exhibit superior naturallanguage generation and in-context learning capabilities. However, theirresponses may contain misleading or inaccurate information. To identifyimplicit sentiment with reliable reasoning, this study proposes RVISA, atwo-stage reasoning framework that harnesses the generation ability of DO LLMsand the reasoning ability of ED LLMs to train an enhanced reasoner.Specifically, we adopt three-hop reasoning prompting to explicitly furnishsentiment elements as cues. The generated rationales are utilized to fine-tunean ED LLM into a skilled reasoner. Additionally, we develop a straightforwardyet effective verification mechanism to ensure the reliability of the reasoninglearning. We evaluated the proposed method on two benchmark datasets andachieved state-of-the-art results in ISA performance.</description><author>Wenna Lai, Haoran Xie, Guandong Xu, Qing Li</author><pubDate>Tue, 02 Jul 2024 16:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02340v1</guid></item><item><title>Open foundation models for Azerbaijani language</title><link>http://arxiv.org/abs/2407.02337v1</link><description>The emergence of multilingual large language models has enabled thedevelopment of language understanding and generation systems in Azerbaijani.However, most of the production-grade systems rely on cloud solutions, such asGPT-4. While there have been several attempts to develop open foundation modelsfor Azerbaijani, these works have not found their way into common use due to alack of systemic benchmarking. This paper encompasses several lines of workthat promote open-source foundation models for Azerbaijani. We introduce (1) alarge text corpus for Azerbaijani, (2) a family of encoder-only language modelstrained on this dataset, (3) labeled datasets for evaluating these models, and(4) extensive evaluation that covers all major open-source models withAzerbaijani support.</description><author>Jafar Isbarov, Kavsar Huseynova, Elvin Mammadov, Mammad Hajili</author><pubDate>Tue, 02 Jul 2024 16:05:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02337v1</guid></item><item><title>CALICO: Confident Active Learning with Integrated Calibration</title><link>http://arxiv.org/abs/2407.02335v1</link><description>The growing use of deep learning in safety-critical applications, such asmedical imaging, has raised concerns about limited labeled data, where thisdemand is amplified as model complexity increases, posing hurdles for domainexperts to annotate data. In response to this, active learning (AL) is used toefficiently train models with limited annotation costs. In the context of deepneural networks (DNNs), AL often uses confidence or probability outputs as ascore for selecting the most informative samples. However, modern DNNs exhibitunreliable confidence outputs, making calibration essential. We propose an ALframework that self-calibrates the confidence used for sample selection duringthe training process, referred to as Confident Active Learning with IntegratedCalibratiOn (CALICO). CALICO incorporates the joint training of a classifierand an energy-based model, instead of the standard softmax-based classifier.This approach allows for simultaneous estimation of the input data distributionand the class probabilities during training, improving calibration withoutneeding an additional labeled dataset. Experimental results showcase improvedclassification performance compared to a softmax-based classifier with fewerlabeled samples. Furthermore, the calibration stability of the model isobserved to depend on the prior class distribution of the data.</description><author>Lorenzo S. Querol, Hajime Nagahara, Hideaki Hayashi</author><pubDate>Tue, 02 Jul 2024 16:05:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02335v1</guid></item><item><title>Why do LLaVA Vision-Language Models Reply to Images in English?</title><link>http://arxiv.org/abs/2407.02333v1</link><description>We uncover a surprising multilingual bias occurring in a popular class ofmultimodal vision-language models (VLMs). Including an image in the query to aLLaVA-style VLM significantly increases the likelihood of the model returningan English response, regardless of the language of the query. This paperinvestigates the causes of this loss with a two-pronged approach that combinesextensive ablation of the design space with a mechanistic analysis of themodels' internal representations of image and text inputs. Both approachesindicate that the issue stems in the language modelling component of the LLaVAmodel. Statistically, we find that switching the language backbone for abilingual language model has the strongest effect on reducing this error.Mechanistically, we provide compelling evidence that visual inputs are notmapped to a similar space as text ones, and that intervening on intermediaryattention layers can reduce this bias. Our findings provide important insightsto researchers and engineers seeking to understand the crossover betweenmultimodal and multilingual spaces, and contribute to the goal of developingcapable and inclusive VLMs for non-English contexts.</description><author>Musashi Hinck, Carolin Holtermann, Matthew Lyle Olson, Florian Schneider, Sungduk Yu, Anahita Bhiwandiwalla, Anne Lauscher, Shaoyen Tseng, Vasudev Lal</author><pubDate>Tue, 02 Jul 2024 16:01:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02333v1</guid></item><item><title>MIGC++: Advanced Multi-Instance Generation Controller for Image Synthesis</title><link>http://arxiv.org/abs/2407.02329v1</link><description>We introduce the Multi-Instance Generation (MIG) task, which focuses ongenerating multiple instances within a single image, each accurately placed atpredefined positions with attributes such as category, color, and shape,strictly following user specifications. MIG faces three main challenges:avoiding attribute leakage between instances, supporting diverse instancedescriptions, and maintaining consistency in iterative generation. To addressattribute leakage, we propose the Multi-Instance Generation Controller (MIGC).MIGC generates multiple instances through a divide-and-conquer strategy,breaking down multi-instance shading into single-instance tasks with singularattributes, later integrated. To provide more types of instance descriptions,we developed MIGC++. MIGC++ allows attribute control through text \&amp; images andposition control through boxes \&amp; masks. Lastly, we introduced theConsistent-MIG algorithm to enhance the iterative MIG ability of MIGC andMIGC++. This algorithm ensures consistency in unmodified regions during theaddition, deletion, or modification of instances, and preserves the identity ofinstances when their attributes are changed. We introduce the COCO-MIG andMultimodal-MIG benchmarks to evaluate these methods. Extensive experiments onthese benchmarks, along with the COCO-Position benchmark and DrawBench,demonstrate that our methods substantially outperform existing techniques,maintaining precise control over aspects including position, attribute, andquantity. Project page: https://github.com/limuloo/MIGC.</description><author>Dewei Zhou, You Li, Fan Ma, Zongxin Yang, Yi Yang</author><pubDate>Tue, 02 Jul 2024 15:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02329v1</guid></item></channel></rss>