<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 07 Aug 2024 01:00:49 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Segment anything model 2: an application to 2D and 3D medical images</title><link>http://arxiv.org/abs/2408.00756v2</link><description>Segment Anything Model (SAM) has gained significant attention because of itsability to segment varous objects in images given a prompt. The recentlydeveloped SAM 2 has extended this ability to video inputs. This opens anopportunity to apply SAM to 3D images, one of the fundamental tasks in themedical imaging field. In this paper, we extensively evaluate SAM 2's abilityto segment both 2D and 3D medical images by first collecting 18 medical imagingdatasets, including common 3D modalities such as computed tomography (CT),magnetic resonance imaging (MRI), and positron emission tomography (PET) aswell as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines ofSAM 2 are considered: (1) multi-frame 3D segmentation, where prompts areprovided to one or multiple slice(s) selected from the volume, and (2)single-frame 2D segmentation, where prompts are provided to each slice. Theformer is only applicable to 3D modalities, while the latter applies to both 2Dand 3D modalities. Our results show that SAM 2 exhibits similar performance asSAM under single-frame 2D segmentation, and has variable performance undermulti-frame 3D segmentation depending on the choices of slices to annotate, thedirection of the propagation, the predictions utilized during the propagation,etc.</description><author>Haoyu Dong, Hanxue Gu, Yaqian Chen, Jichen Yang, Maciej A. Mazurowski</author><pubDate>Tue, 06 Aug 2024 17:40:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00756v2</guid></item><item><title>GenAI Arena: An Open Evaluation Platform for Generative Models</title><link>http://arxiv.org/abs/2406.04485v3</link><description>Generative AI has made remarkable strides to revolutionize fields such asimage and video generation. These advancements are driven by innovativealgorithms, architecture, and data. However, the rapid proliferation ofgenerative models has highlighted a critical gap: the absence of trustworthyevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etcoften fail to capture the nuanced quality and user satisfaction associated withgenerative outputs. This paper proposes an open platform GenAI-Arena toevaluate different image and video generative models, where users can activelyparticipate in evaluating these models. By leveraging collective user feedbackand votes, GenAI-Arena aims to provide a more democratic and accurate measureof model performance. It covers three arenas for text-to-image generation,text-to-video generation, and image editing respectively. Currently, we cover atotal of 27 open-source generative models. GenAI-Arena has been operating forfour months, amassing over 6000 votes from the community. We describe ourplatform, analyze the data, and explain the statistical methods for ranking themodels. To further promote the research in building model-based evaluationmetrics, we release a cleaned version of our preference data for the threetasks, namely GenAI-Bench. We prompt the existing multi-modal models likeGemini, GPT-4o to mimic human voting. We compute the correlation between modelvoting with human voting to understand their judging abilities. Our resultsshow existing multimodal models are still lagging in assessing the generatedvisual content, even the best model GPT-4o only achieves a Pearson correlationof 0.22 in the quality subscore, and behaves like random guessing in others.</description><author>Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, Wenhu Chen</author><pubDate>Tue, 06 Aug 2024 16:35:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04485v3</guid></item><item><title>Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry Reconstruction in Field Conditions</title><link>http://arxiv.org/abs/2402.10344v3</link><description>We evaluate different Neural Radiance Fields (NeRFs) techniques for the 3Dreconstruction of plants in varied environments, from indoor settings tooutdoor fields. Traditional methods usually fail to capture the complexgeometric details of plants, which is crucial for phenotyping and breedingstudies. We evaluate the reconstruction fidelity of NeRFs in three scenarioswith increasing complexity and compare the results with the point cloudobtained using LiDAR as ground truth. In the most realistic field scenario, theNeRF models achieve a 74.6% F1 score after 30 minutes of training on the GPU,highlighting the efficacy of NeRFs for 3D reconstruction in challengingenvironments. Additionally, we propose an early stopping technique for NeRFtraining that almost halves the training time while achieving only a reductionof 7.4% in the average F1 score. This optimization process significantlyenhances the speed and efficiency of 3D reconstruction using NeRFs. Ourfindings demonstrate the potential of NeRFs in detailed and realistic 3D plantreconstruction and suggest practical approaches for enhancing the speed andefficiency of NeRFs in the 3D reconstruction process.</description><author>Muhammad Arbab Arshad, Talukder Jubery, James Afful, Anushrut Jignasu, Aditya Balu, Baskar Ganapathysubramanian, Soumik Sarkar, Adarsh Krishnamurthy</author><pubDate>Tue, 06 Aug 2024 15:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10344v3</guid></item><item><title>SentenceVAE: Enable Next-sentence Prediction for Large Language Models with Faster Speed, Higher Accuracy and Longer Context</title><link>http://arxiv.org/abs/2408.00655v3</link><description>Current large language models (LLMs) primarily utilize next-token predictionmethod for inference, which significantly impedes their processing speed. Inthis paper, we introduce a novel inference methodology termed next-sentenceprediction, aimed at enhancing the inference efficiency of LLMs. We presentSentence Variational Autoencoder (SentenceVAE), a tiny model consisting of aSentence Encoder and a Sentence Decoder. The Sentence Encoder can effectivelycondense the information within a sentence into a singular token, while theSentence Decoder can reconstruct this compressed token back into sentence. Byintegrating SentenceVAE into the input and output layers of LLMs, we developSentence-level LLMs (SLLMs) that employ a sentence-by-sentence inferencemethod. In addition, the SentenceVAE module of SLLMs can maintain the integrityof the original semantic content by segmenting the context into sentences,thereby improving accuracy while boosting inference speed. Moreover, comparedto previous LLMs, SLLMs process fewer tokens over equivalent context length,significantly reducing memory demands for self-attention computation andfacilitating the handling of longer context. Extensive experiments on Wanjuandataset have reveal that the proposed method can accelerate inference speed by204~365%, reduce perplexity (PPL) to 46~75% of its original metric, anddecrease memory overhead by 86~91% for the equivalent context length, comparedto the token-by-token method.</description><author>Hongjun An, Yifan Chen, Xiaozhen Qiao, Zhe Sun, Xuelong Li</author><pubDate>Tue, 06 Aug 2024 13:38:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00655v3</guid></item><item><title>When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX</title><link>http://arxiv.org/abs/2405.01661v3</link><description>Explanations for Convolutional Neural Networks (CNNs) based on relevance ofinput pixels might be too unspecific to evaluate which and how input featuresimpact model decisions. Especially in complex real-world domains like biology,the presence of specific concepts and of relations between concepts might bediscriminating between classes. Pixel relevance is not expressive enough toconvey this type of information. In consequence, model evaluation is limitedand relevant aspects present in the data and influencing the model decisionsmight be overlooked. This work presents a novel method to explain and evaluateCNN models, which uses a concept- and relation-based explainer (CoReX). Itexplains the predictive behavior of a model on a set of images by masking(ir-)relevant concepts from the decision-making process and by constrainingrelations in a learned interpretable surrogate model. We test our approach withseveral image data sets and CNN architectures. Results show that CoReXexplanations are faithful to the CNN model in terms of predictive outcomes. Wefurther demonstrate through a human evaluation that CoReX is a suitable toolfor generating combined explanations that help assessing the classificationquality of CNNs. We further show that CoReX supports the identification andre-classification of incorrect or ambiguous classifications.</description><author>Bettina Finzel, Patrick Hilme, Johannes Rabold, Ute Schmid</author><pubDate>Tue, 06 Aug 2024 12:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01661v3</guid></item><item><title>Convergence Analysis of Natural Gradient Descent for Over-parameterized Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2408.00573v2</link><description>First-order methods, such as gradient descent (GD) and stochastic gradientdescent (SGD), have been proven effective in training neural networks. In thecontext of over-parameterization, there is a line of work demonstrating thatrandomly initialized (stochastic) gradient descent converges to a globallyoptimal solution at a linear convergence rate for the quadratic loss function.However, the learning rate of GD for training two-layer neural networksexhibits poor dependence on the sample size and the Gram matrix, leading to aslow training process. In this paper, we show that for the $L^2$ regressionproblems, the learning rate can be improved from $\mathcal{O}(\lambda_0/n^2)$to $\mathcal{O}(1/\|\bm{H}^{\infty}\|_2)$, which implies that GD actuallyenjoys a faster convergence rate. Furthermore, we generalize the method to GDin training two-layer Physics-Informed Neural Networks (PINNs), showing asimilar improvement for the learning rate. Although the improved learning ratehas a mild dependence on the Gram matrix, we still need to set it small enoughin practice due to the unknown eigenvalues of the Gram matrix. Moreimportantly, the convergence rate is tied to the least eigenvalue of the Grammatrix, which can lead to slow convergence. In this work, we provide theconvergence analysis of natural gradient descent (NGD) in training two-layerPINNs, demonstrating that the learning rate can be $\mathcal{O}(1)$, and atthis rate, the convergence rate is independent of the Gram matrix.</description><author>Xianliang Xu, Ting Du, Wang Kong, Ye Li, Zhongyi Huang</author><pubDate>Tue, 06 Aug 2024 12:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00573v2</guid></item><item><title>eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs</title><link>http://arxiv.org/abs/2407.21483v3</link><description>Over the past few years, we have seen the emergence of large knowledge graphscombining information from multiple sources. Sometimes, this information isprovided in the form of assertions about other assertions, defining contextswhere assertions are valid. A recent extension to RDF which admits statementsover statements, called RDF-star, is in revision to become a W3C standard.However, there is no proposal for a semantics of these RDF-star statements nora built-in facility to operate over them. In this paper, we propose a querylanguage for epistemic RDF-star metadata based on a four-valued logic, calledeSPARQL. Our proposed query language extends SPARQL-star, the query languagefor RDF-star, with a new type of FROM clause to facilitate operating withmultiple and sometimes conflicting beliefs. We show that the proposed querylanguage can express four use case queries, including the following features:(i) querying the belief of an individual, (ii) the aggregating of beliefs,(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs(i.e., nesting of beliefs).</description><author>Xinyi Pan, Daniel Hernández, Philipp Seifer, Ralf Lämmel, Steffen Staab</author><pubDate>Tue, 06 Aug 2024 12:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21483v3</guid></item><item><title>Feature Clock: High-Dimensional Effects in Two-Dimensional Plots</title><link>http://arxiv.org/abs/2408.01294v2</link><description>Humans struggle to perceive and interpret high-dimensional data. Therefore,high-dimensional data are often projected into two dimensions forvisualization. Many applications benefit from complex nonlinear dimensionalityreduction techniques, but the effects of individual high-dimensional featuresare hard to explain in the two-dimensional space. Most visualization solutionsuse multiple two-dimensional plots, each showing the effect of onehigh-dimensional feature in two dimensions; this approach creates a need for avisual inspection of k plots for a k-dimensional input space. Our solution,Feature Clock, provides a novel approach that eliminates the need to inspectthese k plots to grasp the influence of original features on the data structuredepicted in two dimensions. Feature Clock enhances the explainability andcompactness of visualizations of embedded data and is available in anopen-source Python library.</description><author>Olga Ovcharenko, Rita Sevastjanova, Valentina Boeva</author><pubDate>Tue, 06 Aug 2024 12:24:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01294v2</guid></item><item><title>FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation</title><link>http://arxiv.org/abs/2408.00998v2</link><description>Large-scale text-to-image diffusion models have been a revolutionarymilestone in the evolution of generative AI and multimodal technology, allowingwonderful image generation with natural-language text prompt. However, theissue of lacking controllability of such models restricts their practicalapplicability for real-life content creation. Thus, attention has been focusedon leveraging a reference image to control text-to-image synthesis, which isalso regarded as manipulating (or editing) a reference image as per a textprompt, namely, text-driven image-to-image translation. This paper contributesa novel, concise, and efficient approach that adapts pre-trained large-scaletext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in aplug-and-play manner, realizing high-quality and versatile text-driven I2Itranslation without any model training, model fine-tuning, or onlineoptimization process. To guide T2I generation with a reference image, wepropose to decompose diverse guiding factors with different frequency bands ofdiffusion features in the DCT spectral space, and accordingly devise a novelfrequency band substitution layer which realizes dynamic control of thereference image to the T2I generation result in a plug-and-play manner. Wedemonstrate that our method allows flexible control over both guiding factorand guiding intensity of the reference image simply by tuning the type andbandwidth of the substituted frequency band, respectively. Extensivequalitative and quantitative experiments verify superiority of our approachover related methods in I2I translation visual quality, versatility, andcontrollability. The code is publicly available at:https://github.com/XiangGao1102/FBSDiff.</description><author>Xiang Gao, Jiaying Liu</author><pubDate>Tue, 06 Aug 2024 12:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00998v2</guid></item><item><title>Source-Free Domain-Invariant Performance Prediction</title><link>http://arxiv.org/abs/2408.02209v2</link><description>Accurately estimating model performance poses a significant challenge,particularly in scenarios where the source and target domains follow differentdata distributions. Most existing performance prediction methods heavily relyon the source data in their estimation process, limiting their applicability ina more realistic setting where only the trained model is accessible. The fewmethods that do not require source data exhibit considerably inferiorperformance. In this work, we propose a source-free approach centred onuncertainty-based estimation, using a generative model for calibration in theabsence of source data. We establish connections between our approach forunsupervised calibration and temperature scaling. We then employ agradient-based strategy to evaluate the correctness of the calibratedpredictions. Our experiments on benchmark object recognition datasets revealthat existing source-based methods fall short with limited source sampleavailability. Furthermore, our approach significantly outperforms the currentstate-of-the-art source-free and source-based methods, affirming itseffectiveness in domain-invariant performance estimation.</description><author>Ekaterina Khramtsova, Mahsa Baktashmotlagh, Guido Zuccon, Xi Wang, Mathieu Salzmann</author><pubDate>Tue, 06 Aug 2024 11:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02209v2</guid></item><item><title>Enhancing AI-based Generation of Software Exploits with Contextual Information</title><link>http://arxiv.org/abs/2408.02402v2</link><description>This practical experience report explores Neural Machine Translation (NMT)models' capability to generate offensive security code from natural language(NL) descriptions, highlighting the significance of contextual understandingand its impact on model performance. Our study employs a dataset comprisingreal shellcodes to evaluate the models across various scenarios, includingmissing information, necessary context, and unnecessary context. Theexperiments are designed to assess the models' resilience against incompletedescriptions, their proficiency in leveraging context for enhanced accuracy,and their ability to discern irrelevant information. The findings reveal thatthe introduction of contextual data significantly improves performance.However, the benefits of additional context diminish beyond a certain point,indicating an optimal level of contextual information for model training.Moreover, the models demonstrate an ability to filter out unnecessary context,maintaining high levels of accuracy in the generation of offensive securitycode. This study paves the way for future research on optimizing context use inAI-driven code generation, particularly for applications requiring a highdegree of technical precision such as the generation of offensive code.</description><author>Pietro Liguori, Cristina Improta, Roberto Natella, Bojan Cukic, Domenico Cotroneo</author><pubDate>Tue, 06 Aug 2024 10:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02402v2</guid></item><item><title>RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential Recommenders</title><link>http://arxiv.org/abs/2408.02354v2</link><description>Scalability is a major challenge in modern recommender systems. In sequentialrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-artrecommendation quality but consumes excessive GPU memory with large itemcatalogs, limiting its practicality. Using a GPU-efficient locality-sensitivehashing-like algorithm for approximating large tensor of logits, this paperintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantlyreduces memory consumption while allowing one to enjoy the state-of-the-artperformance of full CE loss. Experimental results on various datasets show thatRECE cuts training peak memory usage by up to 12 times compared to existingmethods while retaining or exceeding performance metrics of CE loss. Theapproach also opens up new possibilities for large-scale applications in otherdomains.</description><author>Danil Gusak, Gleb Mezentsev, Ivan Oseledets, Evgeny Frolov</author><pubDate>Tue, 06 Aug 2024 10:11:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02354v2</guid></item><item><title>Automated Explanation Selection for Scientific Discovery</title><link>http://arxiv.org/abs/2407.17454v3</link><description>Automated reasoning is a key technology in the young but rapidly growingfield of Explainable Artificial Intelligence (XAI). Explanability helps buildtrust in artificial intelligence systems beyond their mere predictive accuracyand robustness. In this paper, we propose a cycle of scientific discovery thatcombines machine learning with automated reasoning for the generation and theselection of explanations. We present a taxonomy of explanation selectionproblems that draws on insights from sociology and cognitive science. Theseselection criteria subsume existing notions and extend them with newproperties.</description><author>Markus Iser</author><pubDate>Tue, 06 Aug 2024 08:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17454v3</guid></item><item><title>Conditioning of Banach Space Valued Gaussian Random Variables: An Approximation Approach Based on Martingales</title><link>http://arxiv.org/abs/2404.03453v3</link><description>In this paper we investigate the conditional distributions of two Banachspace valued, jointly Gaussian random variables. We show that these conditionaldistributions are again Gaussian and that their means and covariances aredetermined by a general finite dimensional approximation scheme based upon amartingale approach. In particular, it turns out that the covariance operatorsoccurring in this scheme converge with respect to the nuclear norm and that theconditional probabilities converge weakly. Moreover, we discuss in detail, howour approximation scheme can be implemented in several classes of importantBanach spaces such as (reproducing kernel) Hilbert spaces and spaces ofcontinuous functions. As an example, we then apply our general results to thecase of Gaussian processes with continuous paths conditioned to partial butinfinite observations of their paths. Here we show that conditioning onsufficiently rich, increasing sets of finitely many observations leads toconsistent approximations, that is, both the mean and covariance functionsconverge uniformly and the conditional probabilities converge weakly. Moreover,we discuss how these results improve our understanding of the popular Gaussianprocesses for machine learning.</description><author>Ingo Steinwart</author><pubDate>Tue, 06 Aug 2024 07:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03453v3</guid></item><item><title>Explaining Reinforcement Learning: A Counterfactual Shapley Values Approach</title><link>http://arxiv.org/abs/2408.02529v2</link><description>This paper introduces a novel approach Counterfactual Shapley Values (CSV),which enhances explainability in reinforcement learning (RL) by integratingcounterfactual analysis with Shapley Values. The approach aims to quantify andcompare the contributions of different state dimensions to various actionchoices. To more accurately analyze these impacts, we introduce newcharacteristic value functions, the ``Counterfactual Difference CharacteristicValue" and the ``Average Counterfactual Difference Characteristic Value." Thesefunctions help calculate the Shapley values to evaluate the differences incontributions between optimal and non-optimal actions. Experiments acrossseveral RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate theeffectiveness of the CSV method. The results show that this method not onlyimproves transparency in complex RL systems but also quantifies the differencesacross various decisions.</description><author>Yiwei Shi, Qi Zhang, Kevin McAreavey, Weiru Liu</author><pubDate>Tue, 06 Aug 2024 07:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02529v2</guid></item><item><title>Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval</title><link>http://arxiv.org/abs/2407.10805v3</link><description>Retrieval-augmented generation (RAG) has significantly advanced largelanguage models (LLMs) by enabling dynamic information retrieval to mitigateknowledge gaps and hallucinations in generated content. However, these systemsoften falter with complex reasoning and consistency across diverse queries. Inthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that alignsquestions with the knowledge graph and uses it as a navigational tool, whichdeepens and refines the RAG paradigm for information collection andintegration. The KG-guided navigation fosters deep and long-range associationsto uphold logical consistency and optimize the scope of retrieval for precisionand interoperability. In conjunction, factual consistency can be better ensuredthrough semantic similarity guided by precise directives. ToG${2.0}$ not onlyimproves the accuracy and reliability of LLMs' responses but also demonstratesthe potential of hybrid structured knowledge systems to significantly advanceLLM reasoning, aligning it closer to human-like performance. We conductedextensive experiments on four public datasets to demonstrate the advantages ofour method compared to the baseline.</description><author>Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo</author><pubDate>Tue, 06 Aug 2024 06:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10805v3</guid></item><item><title>Neural Network Emulator for Atmospheric Chemical ODE</title><link>http://arxiv.org/abs/2408.01829v2</link><description>Modeling atmospheric chemistry is complex and computationally intense. Giventhe recent success of Deep neural networks in digital signal processing, wepropose a Neural Network Emulator for fast chemical concentration modeling. Weconsider atmospheric chemistry as a time-dependent Ordinary DifferentialEquation. To extract the hidden correlations between initial states and futuretime evolution, we propose ChemNNE, an Attention based Neural Network Emulator(NNE) that can model the atmospheric chemistry as a neural ODE process. Toefficiently simulate the chemical changes, we propose the sinusoidal timeembedding to estimate the oscillating tendency over time. More importantly, weuse the Fourier neural operator to model the ODE process for efficientcomputation. We also propose three physical-informed losses to supervise thetraining optimization. To evaluate our model, we propose a large-scale chemicaldataset that can be used for neural network training and evaluation. Theextensive experiments show that our approach achieves state-of-the-artperformance in modeling accuracy and computational speed.</description><author>Zhi-Song Liu, Petri Clusius, Michael Boy</author><pubDate>Tue, 06 Aug 2024 06:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01829v2</guid></item><item><title>CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging</title><link>http://arxiv.org/abs/2407.11652v4</link><description>Federated Learning (FL) offers a privacy-preserving approach to train modelson decentralized data. Its potential in healthcare is significant, butchallenges arise due to cross-client variations in medical image data,exacerbated by limited annotations. This paper introduces Cross-ClientVariations Adaptive Federated Learning (CCVA-FL) to address these issues.CCVA-FL aims to minimize cross-client variations by transforming images into acommon feature space. It involves expert annotation of a subset of images fromeach client, followed by the selection of a client with the least datacomplexity as the target. Synthetic medical images are then generated usingScalable Diffusion Models with Transformers (DiT) based on the target client'sannotated images. These synthetic images, capturing diversity and representingthe original data, are shared with other clients. Each client then translatesits local images into the target image space using image-to-image translation.The translated images are subsequently used in a federated learning setting todevelop a server model. Our results demonstrate that CCVA-FL outperformsVanilla Federated Averaging by effectively addressing data distributiondifferences across clients without compromising privacy.</description><author>Sunny Gupta, Amit Sethi</author><pubDate>Tue, 06 Aug 2024 05:10:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11652v4</guid></item><item><title>DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models</title><link>http://arxiv.org/abs/2408.01933v2</link><description>Large language models (LLMs) have recently showcased remarkable capabilities,spanning a wide range of tasks and applications, including those in the medicaldomain. Models like GPT-4 excel in medical question answering but may facechallenges in the lack of interpretability when handling complex tasks in realclinical settings. We thus introduce the diagnostic reasoning dataset forclinical notes (DiReCT), aiming at evaluating the reasoning ability andinterpretability of LLMs compared to human doctors. It contains 511 clinicalnotes, each meticulously annotated by physicians, detailing the diagnosticreasoning process from observations in a clinical note to the final diagnosis.Additionally, a diagnostic knowledge graph is provided to offer essentialknowledge for reasoning, which may not be covered in the training data ofexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significantgap between their reasoning ability and that of human doctors, highlighting thecritical need for models that can reason effectively in real-world clinicalscenarios.</description><author>Bowen Wang, Jiuyang Chang, Yiming Qian, Guoxin Chen, Junhao Chen, Zhouqiang Jiang, Jiahao Zhang, Yuta Nakashima, Hajime Nagahara</author><pubDate>Tue, 06 Aug 2024 04:28:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01933v2</guid></item><item><title>Infusing Environmental Captions for Long-Form Video Language Grounding</title><link>http://arxiv.org/abs/2408.02336v2</link><description>In this work, we tackle the problem of long-form video-language grounding(VLG). Given a long-form video and a natural language query, a model shouldtemporally localize the precise moment that answers the query. Humans caneasily solve VLG tasks, even with arbitrarily long videos, by discardingirrelevant moments using extensive and robust knowledge gained from experience.Unlike humans, existing VLG methods are prone to fall into superficial cueslearned from small-scale datasets, even when they are within irrelevant frames.To overcome this challenge, we propose EI-VLG, a VLG method that leveragesricher textual information provided by a Multi-modal Large Language Model(MLLM) as a proxy for human experiences, helping to effectively excludeirrelevant frames. We validate the effectiveness of the proposed method viaextensive experiments on a challenging EgoNLQ benchmark.</description><author>Hyogun Lee, Soyeon Hong, Mujeen Sung, Jinwoo Choi</author><pubDate>Tue, 06 Aug 2024 04:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02336v2</guid></item><item><title>Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models</title><link>http://arxiv.org/abs/2408.02085v2</link><description>Instruction tuning plays a critical role in aligning large language models(LLMs) with human preference. Despite the vast amount of open instructiondatasets, naively training a LLM on all existing instructions may not beoptimal and practical. To pinpoint the most beneficial datapoints, dataassessment and selection methods have been proposed in the fields of naturallanguage processing (NLP) and deep learning. However, under the context ofinstruction tuning, there still exists a gap in knowledge on what kind of dataevaluation metrics can be employed and how they can be integrated into theselection mechanism. To bridge this gap, we present a comprehensive review onexisting literature of data assessment and selection especially for instructiontuning of LLMs. We systematically categorize all applicable methods intoquality-based, diversity-based, and importance-based ones where a unified,fine-grained taxonomy is structured. For each category, representative methodsare elaborated to describe the landscape of relevant research. In addition,comparison between latest methods is conducted on their officially reportedresults to provide in-depth discussions on their limitations. Finally, wesummarize the open challenges and propose the promosing avenues for futurestudies. All related contents are available athttps://github.com/yuleiqin/fantastic-data-engineering.</description><author>Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, Xing Sun</author><pubDate>Tue, 06 Aug 2024 03:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02085v2</guid></item><item><title>Reinforcement Learning with Generalizable Gaussian Splatting</title><link>http://arxiv.org/abs/2404.07950v3</link><description>An excellent representation is crucial for reinforcement learning (RL)performance, especially in vision-based reinforcement learning tasks. Thequality of the environment representation directly influences the achievementof the learning task. Previous vision-based RL typically uses explicit orimplicit ways to represent environments, such as images, points, voxels, andneural radiance fields. However, these representations contain severaldrawbacks. They cannot either describe complex local geometries or generalizewell to unseen scenes, or require precise foreground masks. Moreover, theseimplicit neural representations are akin to a ``black box", significantlyhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicitscene representation and differentiable rendering nature, is considered arevolutionary change for reconstruction and representation methods. In thispaper, we propose a novel Generalizable Gaussian Splatting framework to be therepresentation of RL tasks, called GSRL. Through validation in the RoboMimicenvironment, our method achieves better results than other baselines inmultiple tasks, improving the performance by 10%, 44%, and 15% compared withbaselines on the hardest task. This work is the first attempt to leveragegeneralizable 3DGS as a representation for RL.</description><author>Jiaxu Wang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Gang Han, Wen Zhao, Weining Zhang, Yecheng Shao, Yijie Guo, Renjing Xu</author><pubDate>Tue, 06 Aug 2024 02:42:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07950v3</guid></item><item><title>A Survey and Evaluation of Adversarial Attacks for Object Detection</title><link>http://arxiv.org/abs/2408.01934v2</link><description>Deep learning models excel in various computer vision tasks but aresusceptible to adversarial examples-subtle perturbations in input data thatlead to incorrect predictions. This vulnerability poses significant risks insafety-critical applications such as autonomous vehicles, securitysurveillance, and aircraft health monitoring. While numerous surveys focus onadversarial attacks in image classification, the literature on such attacks inobject detection is limited. This paper offers a comprehensive taxonomy ofadversarial attacks specific to object detection, reviews existing adversarialrobustness evaluation metrics, and systematically assesses open-source attackmethods and model robustness. Key observations are provided to enhance theunderstanding of attack effectiveness and corresponding countermeasures.Additionally, we identify crucial research challenges to guide future effortsin securing automated object detection systems.</description><author>Khoi Nguyen Tiet Nguyen, Wenyu Zhang, Kangkang Lu, Yuhuan Wu, Xingjian Zheng, Hui Li Tan, Liangli Zhen</author><pubDate>Tue, 06 Aug 2024 02:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01934v2</guid></item><item><title>Analyzing Polysemy Evolution Using Semantic Cells</title><link>http://arxiv.org/abs/2407.16110v3</link><description>The senses of words evolve. The sense of the same word may change from todayto tomorrow, and multiple senses of the same word may be the result of theevolution of each other, that is, they may be parents and children. If we viewJuba as an evolving ecosystem, the paradigm of learning the correct answer,which does not move with the sense of a word, is no longer valid. This paper isa case study that shows that word polysemy is an evolutionary consequence ofthe modification of Semantic Cells, which has al-ready been presented by theauthor, by introducing a small amount of diversity in its initial state as anexample of analyzing the current set of short sentences. In particular, theanalysis of a sentence sequence of 1000 sentences in some order for each of thefour senses of the word Spring, collected using Chat GPT, shows that the wordacquires the most polysemy monotonically in the analysis when the senses arearranged in the order in which they have evolved. In other words, we present amethod for analyzing the dynamism of a word's acquiring polysemy with evolutionand, at the same time, a methodology for viewing polysemy from an evolutionaryframework rather than a learning-based one.</description><author>Yukio Ohsawa, Dingming Xue, Kaira Sekiguchi</author><pubDate>Tue, 06 Aug 2024 02:37:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16110v3</guid></item><item><title>SIGMA: Similarity-based Efficient Global Aggregation for Heterophilous Graph Neural Networks</title><link>http://arxiv.org/abs/2305.09958v3</link><description>Graph neural networks (GNNs) realize great success in graph learning butsuffer from performance loss when meeting heterophily, i.e. neighboring nodesare dissimilar, due to their local and uniform aggregation. Existing attemptsof heterophilous GNNs incorporate long-range or global aggregations todistinguish nodes in the graph. However, these aggregations usually requireiteratively maintaining and updating full-graph information, which limits theirefficiency when applying to large-scale graphs. In this paper, we proposeSIGMA, an efficient global heterophilous GNN aggregation integrating thestructural similarity measurement SimRank. Our theoretical analysis illustratesthat SIGMA inherently captures distant global similarity even underheterophily, that conventional approaches can only achieve after iterativeaggregations. Furthermore, it enjoys efficient one-time computation with acomplexity only linear to the node set size $\mathcal{O}(n)$. Comprehensiveevaluation demonstrates that SIGMA achieves state-of-the-art performance withsuperior aggregation and overall efficiency. Notably, it obtains 5$\times$acceleration on the large-scale heterophily dataset \emph{pokec} with over 30million edges compared to the best baseline aggregation.</description><author>Haoyu Liu, Ningyi Liao, Siqiang Luo</author><pubDate>Tue, 06 Aug 2024 02:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09958v3</guid></item><item><title>Enhanced Local Explainability and Trust Scores with Random Forest Proximities</title><link>http://arxiv.org/abs/2310.12428v3</link><description>We initiate a novel approach to explain the predictions and out of sampleperformance of random forest (RF) regression and classification models byexploiting the fact that any RF can be mathematically formulated as an adaptiveweighted K nearest-neighbors model. Specifically, we employ a recent resultthat, for both regression and classification tasks, any RF prediction can berewritten exactly as a weighted sum of the training targets, where the weightsare RF proximities between the corresponding pairs of data points. We show thatthis linearity facilitates a local notion of explainability of RF predictionsthat generates attributions for any model prediction across observations in thetraining set, and thereby complements established feature-based methods likeSHAP, which generate attributions for a model prediction across input features.We show how this proximity-based approach to explainability can be used inconjunction with SHAP to explain not just the model predictions, but alsoout-of-sample performance, in the sense that proximities furnish a novel meansof assessing when a given model prediction is more or less likely to becorrect. We demonstrate this approach in the modeling of US corporate bondprices and returns in both regression and classification cases.</description><author>Joshua Rosaler, Dhruv Desai, Bhaskarjit Sarmah, Dimitrios Vamvourellis, Deran Onay, Dhagash Mehta, Stefano Pasquali</author><pubDate>Tue, 06 Aug 2024 01:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12428v3</guid></item><item><title>UnifiedNN: Efficient Neural Network Training on the Cloud</title><link>http://arxiv.org/abs/2408.01331v2</link><description>Nowadays, cloud-based services are widely favored over the traditionalapproach of locally training a Neural Network (NN) model. Oftentimes, a cloudservice processes multiple requests from users--thus training multiple NNmodels concurrently. However, training NN models concurrently is a challengingprocess, which typically requires significant amounts of available computingresources and takes a long time to complete. In this paper, we presentUnifiedNN to effectively train multiple NN models concurrently on the cloud.UnifiedNN effectively "combines" multiple NN models and features several memoryand time conservation mechanisms to train multiple NN models simultaneouslywithout impacting the accuracy of the training process. Specifically, UnifiedNNmerges multiple NN models and creates a large singular unified model in orderto efficiently train all models at once. We have implemented a prototype ofUnifiedNN in PyTorch and we have compared its performance with relevantstate-of-the-art frameworks. Our experimental results demonstrate thatUnifiedNN can reduce memory consumption by up to 53% and training time by up to81% when compared with vanilla PyTorch without impacting the model training andtesting accuracy. Finally, our results indicate that UnifiedNN can reducememory consumption by up to 52% and training time by up to 41% when compared tostate-of-the-art frameworks when training multiple models concurrently.</description><author>Sifat Ut Taki, Arthi Padmanabhan, Spyridon Mastorakis</author><pubDate>Tue, 06 Aug 2024 01:10:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01331v2</guid></item><item><title>Integrating ESG and AI: A Comprehensive Responsible AI Assessment Framework</title><link>http://arxiv.org/abs/2408.00965v2</link><description>Artificial Intelligence (AI) is a widely developed and adopted technologyacross entire industry sectors. Integrating environmental, social, andgovernance (ESG) considerations with AI investments is crucial for ensuringethical and sustainable technological advancement. Particularly from aninvestor perspective, this integration not only mitigates risks but alsoenhances long-term value creation by aligning AI initiatives with broadersocietal goals. Yet, this area has been less explored in both academia andindustry. To bridge the gap, we introduce a novel ESG-AI framework, which isdeveloped based on insights from engagements with 28 companies and comprisesthree key components. The framework provides a structured approach to thisintegration, developed in collaboration with industry practitioners. The ESG-AIframework provides an overview of the environmental and social impacts of AIapplications, helping users such as investors assess the materiality of AI use.Moreover, it enables investors to evaluate a company's commitment toresponsible AI through structured engagements and thorough assessment ofspecific risk areas. We have publicly released the framework and toolkit inApril 2024, which has received significant attention and positive feedback fromthe investment community. This paper details each component of the framework,demonstrating its applicability in real-world contexts and its potential toguide ethical AI investments.</description><author>Sung Une Lee, Harsha Perera, Yue Liu, Boming Xia, Qinghua Lu, Liming Zhu, Jessica Cairns, Moana Nottage</author><pubDate>Tue, 06 Aug 2024 00:12:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00965v2</guid></item><item><title>Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics</title><link>http://arxiv.org/abs/2408.02672v1</link><description>Implicit Neural Networks (INRs) have emerged as powerful representations toencode all forms of data, including images, videos, audios, and scenes. Withvideo, many INRs for video have been proposed for the compression task, andrecent methods feature significant improvements with respect to encoding time,storage, and reconstruction quality. However, these encoded representationslack semantic meaning, so they cannot be used for any downstream tasks thatrequire such properties, such as retrieval. This can act as a barrier foradoption of video INRs over traditional codecs as they do not offer anysignificant edge apart from compression. To alleviate this, we propose aflexible framework that decouples the spatial and temporal aspects of the videoINR. We accomplish this with a dictionary of per-frame latents that are learnedjointly with a set of video specific hypernetworks, such that given a latent,these hypernetworks can predict the INR weights to reconstruct the given frame.This framework not only retains the compression efficiency, but the learnedlatents can be aligned with features from large vision models, which grantsthem discriminative properties. We align these latents with CLIP and show goodperformance for both compression and video retrieval tasks. By aligning withVideoLlama, we are able to perform open-ended chat with our learned latents asthe visual inputs. Additionally, the learned latents serve as a proxy for theunderlying weights, allowing us perform tasks like video interpolation. Thesesemantic properties and applications, existing simultaneously with ability toperform compression, interpolation, and superresolution properties, are a firstin this field of work.</description><author>Shishira R Maiya, Anubhav Gupta, Matthew Gwilliam, Max Ehrlich, Abhinav Shrivastava</author><pubDate>Mon, 05 Aug 2024 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02672v1</guid></item><item><title>Self-Taught Evaluators</title><link>http://arxiv.org/abs/2408.02666v1</link><description>Model-based evaluation is at the heart of successful model development -- asa reward model for training, and as a replacement for human evaluation. Totrain such evaluators, the standard approach is to collect a large amount ofhuman preference judgments over model responses, which is costly and the databecomes stale as models improve. In this work, we present an approach that aimsto im-prove evaluators without human annotations, using synthetic training dataonly. Starting from unlabeled instructions, our iterative self-improvementscheme generates contrasting model outputs and trains an LLM-as-a-Judge toproduce reasoning traces and final judgments, repeating this training at eachnew iteration using the improved predictions. Without any labeled preferencedata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperformscommonly used LLM judges such as GPT-4 and matches the performance of thetop-performing reward models trained with labeled examples.</description><author>Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li</author><pubDate>Mon, 05 Aug 2024 17:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02666v1</guid></item><item><title>FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic Segmentation of Diverse Landscapes</title><link>http://arxiv.org/abs/2405.04634v3</link><description>Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as anew tool to monitor territory and support public policies. Processing ALS dataat scale requires efficient point classification methods that perform well overhighly diverse territories. To evaluate them, researchers need large annotatedLidar datasets, however, current Lidar benchmark datasets have restricted scopeand often cover a single urban area. To bridge this data gap, we present theFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: anultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds withhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL isbuilt upon France's nationwide open Lidar data. It achieves spatial andsemantic diversity via a sampling scheme that explicitly concentrates rareclasses and challenging landscapes from five French regions. It should supportthe development of 3D deep learning approaches for large-scale land monitoring.We describe the nature of the source data, the sampling workflow, the contentof the resulting dataset, and provide an initial evaluation of segmentationperformance using a performant 3D neural architecture.</description><author>Charles Gaydon, Michel Daab, Floryne Roche</author><pubDate>Mon, 05 Aug 2024 17:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04634v3</guid></item><item><title>Quantised Global Autoencoder: A Holistic Approach to Representing Visual Data</title><link>http://arxiv.org/abs/2407.11913v2</link><description>In quantised autoencoders, images are usually split into local patches, eachencoded by one token. This representation is redundant in the sense that thesame number of tokens is spend per region, regardless of the visual informationcontent in that region. Adaptive discretisation schemes like quadtrees areapplied to allocate tokens for patches with varying sizes, but this just variesthe region of influence for a token which nevertheless remains a localdescriptor. Modern architectures add an attention mechanism to the autoencoderwhich infuses some degree of global information into the local tokens. Despitethe global context, tokens are still associated with a local image region. Incontrast, our method is inspired by spectral decompositions which transform aninput signal into a superposition of global frequencies. Taking the data-drivenperspective, we learn custom basis functions corresponding to the codebookentries in our VQ-VAE setup. Furthermore, a decoder combines these basisfunctions in a non-linear fashion, going beyond the simple linear superpositionof spectral decompositions. We can achieve this global description with anefficient transpose operation between features and channels and demonstrate ourperformance on compression.</description><author>Tim Elsner, Paula Usinger, Victor Czech, Gregor Kobsik, Yanjiang He, Isaak Lim, Leif Kobbelt</author><pubDate>Mon, 05 Aug 2024 17:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11913v2</guid></item><item><title>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining</title><link>http://arxiv.org/abs/2408.02657v1</link><description>We present Lumina-mGPT, a family of multimodal autoregressive models capableof various vision and language tasks, particularly excelling in generatingflexible photorealistic images from text descriptions. Unlike existingautoregressive image generation approaches, Lumina-mGPT employs a pretraineddecoder-only transformer as a unified framework for modeling multimodal tokensequences. Our key insight is that a simple decoder-only transformer withmultimodal Generative PreTraining (mGPT), utilizing the next-token predictionobjective on massive interleaved text-image sequences, can learn broad andgeneral multimodal capabilities, thereby illuminating photorealistictext-to-image generation. Building on these pretrained models, we proposeFlexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-textpairs to fully unlock their potential for high-aesthetic image synthesis at anyresolution while maintaining their general multimodal capabilities.Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),transforming Lumina-mGPT into a foundation model that seamlessly achievesomnipotent task unification. The resulting model demonstrates versatilemultimodal capabilities, including visual generation tasks like flexibletext-to-image generation and controllable generation, visual recognition taskslike segmentation and depth estimation, and vision-language tasks likemultiturn visual question answering. Additionally, we analyze the differencesand similarities between diffusion-based and autoregressive methods in a directcomparison.</description><author>Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, Peng Gao</author><pubDate>Mon, 05 Aug 2024 17:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02657v1</guid></item><item><title>BSRBF-KAN: A combination of B-splines and Radial Basic Functions in Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2406.11173v3</link><description>In this paper, we introduce BSRBF-KAN, a Kolmogorov Arnold Network (KAN) thatcombines Bsplines and radial basis functions (RBFs) to fit input vectors indata training. We perform experiments with BSRBF-KAN, MLP, and other popularKANs, including EfficientKAN, FastKAN, FasterKAN, and GottliebKAN over theMNIST and Fashion-MNIST datasets. BSRBF-KAN shows stability in 5 trainingsessions with a competitive average accuracy of 97.55% on MNIST and 89.33% onFashionMNIST and obtains convergence better than other networks. We expectBSRBF-KAN to open many combinations of mathematical functions to design KANs.Our repo is publicly available at: https://github.com/hoangthangta/BSRBF-KAN.</description><author>Hoang-Thang Ta</author><pubDate>Mon, 05 Aug 2024 17:39:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11173v3</guid></item><item><title>On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization</title><link>http://arxiv.org/abs/2408.02654v1</link><description>The effectiveness of training neural networks directly impacts computationalcosts, resource allocation, and model development timelines in machine learningapplications. An optimizer's ability to train the model adequately (in terms oftrained model performance) depends on the model's initial weights. Model weightinitialization schemes use pseudorandom number generators (PRNGs) as a sourceof randomness. We investigate whether substituting PRNGs for low-discrepancy quasirandomnumber generators (QRNGs) -- namely Sobol' sequences -- as a source ofrandomness for initializers can improve model performance. We examineMulti-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), LongShort-Term Memory (LSTM), and Transformer architectures trained on MNIST,CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis usesten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);Orthogonal, Random Normal, Truncated Normal, and Random Uniform. Models withweights set using PRNG- and QRNG-based initializers are compared pairwise foreach combination of dataset, architecture, optimizer, and initializationscheme. Our findings indicate that QRNG-based neural network initializers eitherreach a higher accuracy or achieve the same accuracy more quickly thanPRNG-based initializers in 60% of the 120 experiments conducted. Thus, usingQRNG-based initializers instead of PRNG-based initializers can speed up andimprove model training.</description><author>Andriy Miranskyy, Adam Sorrenti, Viral Thakar</author><pubDate>Mon, 05 Aug 2024 17:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02654v1</guid></item><item><title>Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?</title><link>http://arxiv.org/abs/2408.02651v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities innatural language tasks, but their safety and morality remain contentious due totheir training on internet text corpora. To address these concerns, alignmenttechniques have been developed to improve the public usability and safety ofLLMs. Yet, the potential for generating harmful content through these modelsseems to persist. This paper explores the concept of jailbreakingLLMs-reversing their alignment through adversarial triggers. Previous methods,such as soft embedding prompts, manually crafted prompts, and gradient-basedautomatic prompts, have had limited success on black-box models due to theirrequirements for model access and for producing a low variety of manuallycrafted prompts, making them susceptible to being blocked. This paperintroduces a novel approach using reinforcement learning to optimizeadversarial triggers, requiring only inference API access to the target modeland a small surrogate model. Our method, which leverages a BERTScore-basedreward function, enhances the transferability and effectiveness of adversarialtriggers on new black-box models. We demonstrate that this approach improvesthe performance of adversarial triggers on a previously untested languagemodel.</description><author>Mohammad Bahrami Karkevandi, Nishant Vishwamitra, Peyman Najafirad</author><pubDate>Mon, 05 Aug 2024 17:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02651v1</guid></item><item><title>Detection of Compromised Functions in a Serverless Cloud Environment</title><link>http://arxiv.org/abs/2408.02641v1</link><description>Serverless computing is an emerging cloud paradigm with serverless functionsat its core. While serverless environments enable software developers to focuson developing applications without the need to actively manage the underlyingruntime infrastructure, they open the door to a wide variety of securitythreats that can be challenging to mitigate with existing methods. Existingsecurity solutions do not apply to all serverless architectures, since theyrequire significant modifications to the serverless infrastructure or rely onthird-party services for the collection of more detailed data. In this paper,we present an extendable serverless security threat detection model thatleverages cloud providers' native monitoring tools to detect anomalous behaviorin serverless applications. Our model aims to detect compromised serverlessfunctions by identifying post-exploitation abnormal behavior related todifferent types of attacks on serverless functions, and therefore, it is a lastline of defense. Our approach is not tied to any specific serverlessapplication, is agnostic to the type of threats, and is adaptable through modeladjustments. To evaluate our model's performance, we developed a serverlesscybersecurity testbed in an AWS cloud environment, which includes two differentserverless applications and simulates a variety of attack scenarios that coverthe main security threats faced by serverless functions. Our evaluationdemonstrates our model's ability to detect all implemented attacks whilemaintaining a negligible false alarm rate.</description><author>Danielle Lavi, Oleg Brodt, Dudu Mimran, Yuval Elovici, Asaf Shabtai</author><pubDate>Mon, 05 Aug 2024 17:14:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02641v1</guid></item><item><title>Command-line Obfuscation Detection using Small Language Models</title><link>http://arxiv.org/abs/2408.02637v1</link><description>To avoid detection, adversaries often use command-line obfuscation. There arenumerous techniques of the command-line obfuscation, all designed to alter thecommand-line syntax without affecting its original functionality. Thisvariability forces most security solutions to create an exhaustive enumerationof signatures for even a single pattern. In contrast to using signatures, wehave implemented a scalable NLP-based detection method that leverages acustom-trained, small transformer language model that can be applied to anysource of execution logs. The evaluation on top of real-world telemetrydemonstrates that our approach yields high-precision detections even onhigh-volume telemetry from a diverse set of environments spanning fromuniversities and businesses to healthcare or finance. The practical value isdemonstrated in a case study of real-world samples detected by our model. Weshow the model's superiority to signatures on established malware known toemploy obfuscation and showcase previously unseen obfuscated samples detectedby our model.</description><author>Vojtech Outrata, Michael Adam Polak, Martin Kopp</author><pubDate>Mon, 05 Aug 2024 17:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02637v1</guid></item><item><title>Partial End-to-end Reinforcement Learning for Robustness Against Modelling Error in Autonomous Racing</title><link>http://arxiv.org/abs/2312.06406v2</link><description>In this paper, we address the issue of increasing the performance ofreinforcement learning (RL) solutions for autonomous racing cars whennavigating under conditions where practical vehicle modelling errors (commonlyknown as \emph{model mismatches}) are present. To address this challenge, wepropose a partial end-to-end algorithm that decouples the planning and controltasks. Within this framework, an RL agent generates a trajectory comprising apath and velocity, which is subsequently tracked using a pure pursuit steeringcontroller and a proportional velocity controller, respectively. In contrast,many current learning-based (i.e., reinforcement and imitation learning)algorithms utilise an end-to-end approach whereby a deep neural networkdirectly maps from sensor data to control commands. By leveraging therobustness of a classical controller, our partial end-to-end driving algorithmexhibits better robustness towards model mismatches than standard end-to-endalgorithms.</description><author>Andrew Murdoch, Johannes Cornelius Schoeman, Hendrik Willem Jordaan</author><pubDate>Mon, 05 Aug 2024 17:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06406v2</guid></item><item><title>Interactive 3D Medical Image Segmentation with SAM 2</title><link>http://arxiv.org/abs/2408.02635v1</link><description>Interactive medical image segmentation (IMIS) has shown significant potentialin enhancing segmentation accuracy by integrating iterative feedback frommedical professionals. However, the limited availability of enough 3D medicaldata restricts the generalization and robustness of most IMIS methods. TheSegment Anything Model (SAM), though effective for 2D images, requiresexpensive semi-auto slice-by-slice annotations for 3D medical images. In thispaper, we explore the zero-shot capabilities of SAM 2, the next-generation MetaSAM model trained on videos, for 3D medical image segmentation. By treatingsequential 2D slices of 3D images as video frames, SAM 2 can fullyautomatically propagate annotations from a single frame to the entire 3Dvolume. We propose a practical pipeline for using SAM 2 in 3D medical imagesegmentation and present key findings highlighting its efficiency and potentialfor further optimization. Concretely, numerical experiments on the BraTS2020and the medical segmentation decathlon datasets demonstrate that SAM 2 stillhas a gap with supervised methods but can narrow the gap in specific settingsand organ types, significantly reducing the annotation burden on medicalprofessionals. Our code will be open-sourced and available athttps://github.com/Chuyun-Shen/SAM_2_Medical_3D.</description><author>Chuyun Shen, Wenhao Li, Yuhang Shi, Xiangfeng Wang</author><pubDate>Mon, 05 Aug 2024 16:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02635v1</guid></item><item><title>SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models</title><link>http://arxiv.org/abs/2408.02632v1</link><description>As large language models (LLMs) continue to advance in capability andinfluence, ensuring their security and preventing harmful outputs has becomecrucial. A promising approach to address these concerns involves trainingmodels to automatically generate adversarial prompts for red teaming. However,the evolving subtlety of vulnerabilities in LLMs challenges the effectivenessof current adversarial methods, which struggle to specifically target andexplore the weaknesses of these models. To tackle these challenges, weintroduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving}\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$optimization framework, which enhances security by leveraging data generated bythe model itself. SEAS operates through three iterative stages: Initialization,Attack, and Adversarial Optimization, refining both the Red Team and Targetmodels to improve robustness and safety. This framework reduces reliance onmanual testing and significantly enhances the security capabilities of LLMs.Our contributions include a novel adversarial framework, a comprehensive safetydataset, and after three iterations, the Target model achieves a security levelcomparable to GPT-4, while the Red Team model shows a marked increase in attacksuccess rate (ASR) against advanced models.</description><author>Muxi Diao, Rumei Li, Shiyang Liu, Guogang Liao, Jingang Wang, Xunliang Cai, Weiran Xu</author><pubDate>Mon, 05 Aug 2024 16:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02632v1</guid></item><item><title>VidGen-1M: A Large-Scale Dataset for Text-to-video Generation</title><link>http://arxiv.org/abs/2408.02629v1</link><description>The quality of video-text pairs fundamentally determines the upper bound oftext-to-video models. Currently, the datasets used for training these modelssuffer from significant shortcomings, including low temporal consistency,poor-quality captions, substandard video quality, and imbalanced datadistribution. The prevailing video curation process, which depends on imagemodels for tagging and manual rule-based curation, leads to a highcomputational load and leaves behind unclean data. As a result, there is a lackof appropriate training datasets for text-to-video models. To address thisproblem, we present VidGen-1M, a superior training dataset for text-to-videomodels. Produced through a coarse-to-fine curation strategy, this datasetguarantees high-quality videos and detailed captions with excellent temporalconsistency. When used to train the video generation model, this dataset hasled to experimental results that surpass those obtained with other models.</description><author>Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, Hao Li</author><pubDate>Mon, 05 Aug 2024 16:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02629v1</guid></item><item><title>Unsupervised Change Detection for Space Habitats Using 3D Point Clouds</title><link>http://arxiv.org/abs/2312.02396v3</link><description>This work presents an algorithm for scene change detection from point cloudsto enable autonomous robotic caretaking in future space habitats. Autonomousrobotic systems will help maintain future deep-space habitats, such as theGateway space station, which will be uncrewed for extended periods. Existingscene analysis software used on the International Space Station (ISS) relies onmanually-labeled images for detecting changes. In contrast, the algorithmpresented in this work uses raw, unlabeled point clouds as inputs. Thealgorithm first applies modified Expectation-Maximization Gaussian MixtureModel (GMM) clustering to two input point clouds. It then performs changedetection by comparing the GMMs using the Earth Mover's Distance. The algorithmis validated quantitatively and qualitatively using a test dataset collected byan Astrobee robot in the NASA Ames Granite Lab comprising single frame depthimages taken directly by Astrobee and full-scene reconstructed maps built withRGB-D and pose data from Astrobee. The runtimes of the approach are alsoanalyzed in depth. The source code is publicly released to promote furtherdevelopment.</description><author>Jamie Santos, Holly Dinkel, Julia Di, Paulo V. K. Borges, Marina Moreira, Oleg Alexandrov, Brian Coltin, Trey Smith</author><pubDate>Mon, 05 Aug 2024 16:49:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02396v3</guid></item><item><title>YOWOv3: An Efficient and Generalized Framework for Human Action Detection and Recognition</title><link>http://arxiv.org/abs/2408.02623v1</link><description>In this paper, we propose a new framework called YOWOv3, which is an improvedversion of YOWOv2, designed specifically for the task of Human Action Detectionand Recognition. This framework is designed to facilitate extensiveexperimentation with different configurations and supports easy customizationof various components within the model, reducing efforts required forunderstanding and modifying the code. YOWOv3 demonstrates its superiorperformance compared to YOWOv2 on two widely used datasets for Human ActionDetection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessormodel YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2,respectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model -YOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33%and 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate thatYOWOv3 significantly reduces the number of parameters and GFLOPs while stillachieving comparable performance.</description><author>Duc Manh Nguyen Dang, Viet Hang Duong, Jia Ching Wang, Nhan Bui Duc</author><pubDate>Mon, 05 Aug 2024 16:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02623v1</guid></item><item><title>Language Model Can Listen While Speaking</title><link>http://arxiv.org/abs/2408.02622v1</link><description>Dialogue serves as the most natural manner of human-computer interaction(HCI). Recent advancements in speech language models (SLM) have significantlyenhanced speech-based conversational AI. However, these models are limited toturn-based conversation, lacking the ability to interact with humans inreal-time spoken scenarios, for example, being interrupted when the generatedcontent is not satisfactory. To address these limitations, we explore fullduplex modeling (FDM) in interactive speech language models (iSLM), focusing onenhancing real-time interaction and, more explicitly, exploring thequintessential ability of interruption. We introduce a novel model design,namely listening-while-speaking language model (LSLM), an end-to-end systemequipped with both listening and speaking channels. Our LSLM employs atoken-based decoder-only TTS for speech generation and a streamingself-supervised learning (SSL) encoder for real-time audio input. LSLM fusesboth channels for autoregressive generation and detects turn-taking in realtime. Three fusion strategies -- early fusion, middle fusion, and late fusion-- are explored, with middle fusion achieving an optimal balance between speechgeneration and real-time interaction. Two experimental settings, command-basedFDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivityto diverse instructions. Our results highlight LSLM's capability to achieveduplex communication with minimal impact on existing systems. This study aimsto advance the development of interactive speech dialogue systems, enhancingtheir applicability in real-world contexts.</description><author>Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen</author><pubDate>Mon, 05 Aug 2024 16:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02622v1</guid></item><item><title>LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba</title><link>http://arxiv.org/abs/2408.02615v1</link><description>Recent Transformer-based diffusion models have shown remarkable performance,largely attributed to the ability of the self-attention mechanism to accuratelycapture both global and local contexts by computing all-pair interactions amonginput tokens. However, their quadratic complexity poses significantcomputational challenges for long-sequence inputs. Conversely, a recent statespace model called Mamba offers linear complexity by compressing a filteredglobal context into a hidden state. Despite its efficiency, compressioninevitably leads to information loss of fine-grained local dependencies amongtokens, which are crucial for effective visual generative modeling. Motivatedby these observations, we introduce Local Attentional Mamba (LaMamba) blocksthat combine the strengths of self-attention and Mamba, capturing both globalcontexts and local details with linear complexity. Leveraging the efficientU-Net architecture, our model exhibits exceptional scalability and surpassesthe performance of DiT across various model scales on ImageNet at 256x256resolution, all while utilizing substantially fewer GFLOPs and a comparablenumber of parameters. Compared to state-of-the-art diffusion models on ImageNet256x256 and 512x512, our largest model presents notable advantages, such as areduction of up to 62\% GFLOPs compared to DiT-XL/2, while achieving superiorperformance with comparable or fewer parameters.</description><author>Yunxiang Fu, Chaoqi Chen, Yizhou Yu</author><pubDate>Mon, 05 Aug 2024 16:39:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02615v1</guid></item><item><title>SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications</title><link>http://arxiv.org/abs/2403.11515v2</link><description>Monocular depth estimation (MDE) has advanced significantly, primarilythrough the integration of convolutional neural networks (CNNs) and morerecently, Transformers. However, concerns about their susceptibility toadversarial attacks have emerged, especially in safety-critical domains likeautonomous driving and robotic navigation. Existing approaches for assessingCNN-based depth prediction methods have fallen short in inducing comprehensivedisruptions to the vision system, often limited to specific local areas. Inthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novelapproach designed to comprehensively disrupt monocular depth estimation (MDE)in autonomous navigation applications. Our patch is crafted to selectivelyundermine MDE in two distinct ways: by distorting estimated distances or bycreating the illusion of an object disappearing from the system's perspective.Notably, our patch is shape-sensitive, meaning it considers the specific shapeand scale of the target object, thereby extending its influence beyondimmediate proximity. Furthermore, our patch is trained to effectively addressdifferent scales and distances from the camera. Experimental resultsdemonstrate that our approach induces a mean depth estimation error surpassing0.5, impacting up to 99% of the targeted region for CNN-based MDE models.Additionally, we investigate the vulnerability of Transformer-based MDE modelsto patch-based attacks, revealing that SSAP yields a significant error of 0.59and exerts substantial influence over 99% of the target region on these models.</description><author>Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Bassem Ouni, Muhammad Shafique</author><pubDate>Mon, 05 Aug 2024 16:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11515v2</guid></item><item><title>APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation</title><link>http://arxiv.org/abs/2303.01351v3</link><description>In recent times, monocular depth estimation (MDE) has experienced significantadvancements in performance, largely attributed to the integration ofinnovative architectures, i.e., convolutional neural networks (CNNs) andTransformers. Nevertheless, the susceptibility of these models to adversarialattacks has emerged as a noteworthy concern, especially in domains where safetyand security are paramount. This concern holds particular weight for MDE due toits critical role in applications like autonomous driving and roboticnavigation, where accurate scene understanding is pivotal. To assess thevulnerability of CNN-based depth prediction methods, recent work tries todesign adversarial patches against MDE. However, the existing approaches fallshort of inducing a comprehensive and substantially disruptive impact on thevision system. Instead, their influence is partial and confined to specificlocal areas. These methods lead to erroneous depth predictions only within theoverlapping region with the input image, without considering thecharacteristics of the target object, such as its size, shape, and position. Inthis paper, we introduce a novel adversarial patch named APARATE. This patchpossesses the ability to selectively undermine MDE in two distinct ways: bydistorting the estimated distances or by creating the illusion of an objectdisappearing from the perspective of the autonomous system. Notably, APARATE isdesigned to be sensitive to the shape and scale of the target object, and itsinfluence extends beyond immediate proximity. APARATE, results in a mean depthestimation error surpassing $0.5$, significantly impacting as much as $99\%$ ofthe targeted region when applied to CNN-based MDE models. Furthermore, ityields a significant error of $0.34$ and exerts substantial influence over$94\%$ of the target region in the context of Transformer-based MDE.</description><author>Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique</author><pubDate>Mon, 05 Aug 2024 16:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01351v3</guid></item><item><title>Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need</title><link>http://arxiv.org/abs/2303.07338v2</link><description>Class-incremental learning (CIL) aims to adapt to emerging new classeswithout forgetting old ones. Traditional CIL models are trained from scratch tocontinually acquire knowledge as data evolves. Recently, pre-training hasachieved substantial progress, making vast pre-trained models (PTMs) accessiblefor CIL. Contrary to traditional methods, PTMs possess generalizableembeddings, which can be easily transferred for CIL. In this work, we revisitCIL with PTMs and argue that the core factors in CIL are adaptivity for modelupdating and generalizability for knowledge transferring. 1) We first revealthat frozen PTM can already provide generalizable embeddings for CIL.Surprisingly, a simple baseline (SimpleCIL) which continually sets theclassifiers of PTM to prototype features can beat state-of-the-art even withouttraining on the downstream task. 2) Due to the distribution gap betweenpre-trained and downstream datasets, PTM can be further cultivated withadaptivity via model adaptation. We propose AdaPt and mERge (APER), whichaggregates the embeddings of PTM and adapted models for classifierconstruction. APER is a general framework that can be orthogonally combinedwith any parameter-efficient tuning method, which holds the advantages of PTM'sgeneralizability and adapted model's adaptivity. 3) Additionally, consideringprevious ImageNet-based benchmarks are unsuitable in the era of PTM due to dataoverlapping, we propose four new benchmarks for assessment, namely ImageNet-A,ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate theeffectiveness of APER with a unified and concise framework. Code is availableat https://github.com/zhoudw-zdw/RevisitingCIL</description><author>Da-Wei Zhou, Zi-Wen Cai, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu</author><pubDate>Mon, 05 Aug 2024 16:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07338v2</guid></item><item><title>Backward explanations via redefinition of predicates</title><link>http://arxiv.org/abs/2408.02606v1</link><description>History eXplanation based on Predicates (HXP), studies the behavior of aReinforcement Learning (RL) agent in a sequence of agent's interactions withthe environment (a history), through the prism of an arbitrary predicate. Tothis end, an action importance score is computed for each action in thehistory. The explanation consists in displaying the most important actions tothe user. As the calculation of an action's importance is #W[1]-hard, it isnecessary for long histories to approximate the scores, at the expense of theirquality. We therefore propose a new HXP method, called Backward-HXP, to provideexplanations for these histories without having to approximate scores.Experiments show the ability of B-HXP to summarise long histories.</description><author>Léo Saulières, Martin C. Cooper, Florence Dupin de Saint Cyr</author><pubDate>Mon, 05 Aug 2024 16:31:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02606v1</guid></item><item><title>Learning rheological parameters of non-Newtonian fluids from velocimetry data</title><link>http://arxiv.org/abs/2408.02604v1</link><description>We solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilatesvelocimetry data in order to jointly reconstruct the flow field and learn theunknown N-S parameters. By incorporating a Carreau shear-thinning viscositymodel into the N-S problem, we devise an algorithm that learns the most likelyCarreau parameters of a shear-thinning fluid, and estimates theiruncertainties, from velocimetry data alone. We then conduct a flow-MRIexperiment to obtain velocimetry data of an axisymmetric laminar jet through anidealised medical device (FDA nozzle) for a blood analogue fluid. We show thatthe algorithm can successfully reconstruct the flow field by learning the mostlikely Carreau parameters, and that the learned parameters are in very goodagreement with rheometry measurements. The algorithm accepts any algebraiceffective viscosity model, as long as the model is differentiable, and it canbe extended to more complicated non-Newtonian fluids (e.g. Oldroyd-B fluid) ifa viscoelastic model is incorporated into the N-S problem.</description><author>Alexandros Kontogiannis, Richard Hodgkinson, Emily L. Manchester</author><pubDate>Mon, 05 Aug 2024 16:27:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02604v1</guid></item><item><title>BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba</title><link>http://arxiv.org/abs/2408.02600v1</link><description>The advancement of natural language processing (NLP) in biology hinges onmodels' ability to interpret intricate biomedical literature. Traditionalmodels often struggle with the complex and domain-specific language in thisfield. In this paper, we present BioMamba, a pre-trained model specificallydesigned for biomedical text mining. BioMamba builds upon the Mambaarchitecture and is pre-trained on an extensive corpus of biomedicalliterature. Our empirical studies demonstrate that BioMamba significantlyoutperforms models like BioBERT and general-domain Mamba across variousbiomedical tasks. For instance, BioMamba achieves a 100 times reduction inperplexity and a 4 times reduction in cross-entropy loss on the BioASQ testset. We provide an overview of the model architecture, pre-training process,and fine-tuning techniques. Additionally, we release the code and trained modelto facilitate further research.</description><author>Ling Yue, Sixue Xing, Yingzhou Lu, Tianfan Fu</author><pubDate>Mon, 05 Aug 2024 16:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02600v1</guid></item><item><title>Progressively Selective Label Enhancement for Language Model Alignment</title><link>http://arxiv.org/abs/2408.02599v1</link><description>Large Language Models have demonstrated impressive capabilities in variouslanguage tasks but may produce content that misaligns with human expectations,raising ethical and legal concerns. Therefore, it is important to explore thelimitations and implement restrictions on the models to ensure safety andcompliance, with Reinforcement Learning from Human Feedback (RLHF) being theprimary method. Due to challenges in stability and scalability with the RLHFstages, researchers are exploring alternative methods to achieve effectscomparable to those of RLHF. However, these methods often depend on largehigh-quality datasets and inefficiently utilize generated data. To deal withthis problem, we propose PSLE, i.e., Progressively Selective Label Enhancementfor Language Model Alignment, a framework that fully utilizes all generateddata by guiding the model with principles to align outputs with humanexpectations. Using a dynamically updated threshold, our approach ensuresefficient data utilization by incorporating all generated responses andweighting them based on their corresponding reward scores. Experimental resultson multiple datasets demonstrate the effectiveness of PSLE compared to existinglanguage model alignment methods.</description><author>Biao Liu, Ning Xu, Xin Geng</author><pubDate>Mon, 05 Aug 2024 16:21:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02599v1</guid></item><item><title>Predicting and Understanding Human Action Decisions: Insights from Large Language Models and Cognitive Instance-Based Learning</title><link>http://arxiv.org/abs/2407.09281v2</link><description>Large Language Models (LLMs) have demonstrated their capabilities acrossvarious tasks, from language translation to complex reasoning. Understandingand predicting human behavior and biases are crucial for artificialintelligence (AI) assisted systems to provide useful assistance, yet it remainsan open question whether these models can achieve this. This paper addressesthis gap by leveraging the reasoning and generative capabilities of the LLMs topredict human behavior in two sequential decision-making tasks. These tasksinvolve balancing between exploitative and exploratory actions and handlingdelayed feedback, both essential for simulating real-life decision processes.We compare the performance of LLMs with a cognitive instance-based learning(IBL) model, which imitates human experiential decision-making. Our findingsindicate that LLMs excel at rapidly incorporating feedback to enhanceprediction accuracy. In contrast, the cognitive IBL model better accounts forhuman exploratory behaviors and effectively captures loss aversion bias, i.e.,the tendency to choose a sub-optimal goal with fewer step-cost penalties ratherthan exploring to find the optimal choice, even with limited experience. Theresults highlight the benefits of integrating LLMs with cognitivearchitectures, suggesting that this synergy could enhance the modeling andunderstanding of complex human decision-making patterns.</description><author>Thuy Ngoc Nguyen, Kasturi Jamale, Cleotilde Gonzalez</author><pubDate>Mon, 05 Aug 2024 16:16:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09281v2</guid></item><item><title>AI-Driven Strategies for Reducing Student Withdrawal -- A Study of EMU Student Stopout</title><link>http://arxiv.org/abs/2408.02598v1</link><description>Not everyone who enrolls in college will leave with a certificate or degree,but the number of people who drop out or take a break is much higher thanexperts previously believed. In December 2013, there were 29 million peoplewith some college education but no degree. That number jumped to 36 million byDecember of 2018, according to a new report from the National StudentClearinghouse Research Center[1]. It is imperative to understand the underlyingfactors contributing to student withdrawal and to assist decision-makers toidentify effective strategies to prevent it. By analyzing the characteristicsand educational pathways of the stopout student population, our aim is toprovide actionable insights that can benefit institutions facing similarchallenges. Eastern Michigan University (EMU) faces significant challenges instudent retention, with approximately 55% of its undergraduate students notcompleting their degrees within six years. As an institution committed tostudent success, EMU conducted a comprehensive study of student withdrawals tounderstand the influencing factors. And the paper revealed a high correlationbetween certain factors and withdrawals, even in the early stages of universityattendance. Based on these findings, we developed a predictive model thatemploys artificial intelligence techniques to assess the potential risk thatstudents abandon their studies. These models enable universities to implementearly intervention strategies, support at-risk students, and improve overallhigher education success.</description><author>Yan Zhao, Amy Otteson</author><pubDate>Mon, 05 Aug 2024 16:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02598v1</guid></item><item><title>Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection</title><link>http://arxiv.org/abs/2408.02595v1</link><description>Sarcasm is a type of irony, characterized by an inherent mismatch between theliteral interpretation and the intended connotation. Though sarcasm detectionin text has been extensively studied, there are situations in which textualinput alone might be insufficient to perceive sarcasm. The inclusion ofadditional contextual cues, such as images, is essential to recognize sarcasmin social media data effectively. This study presents a novel framework formultimodal sarcasm detection that can process input triplets. Two components ofthese triplets comprise the input text and its associated image, as provided inthe datasets. Additionally, a supplementary modality is introduced in the formof descriptive image captions. The motivation behind incorporating this visualsemantic representation is to more accurately capture the discrepancies betweenthe textual and visual content, which are fundamental to the sarcasm detectiontask. The primary contributions of this study are: (1) a robust textual featureextraction branch that utilizes a cross-lingual language model; (2) a visualfeature extraction branch that incorporates a self-regulated residual ConvNetintegrated with a lightweight spatially aware attention module; (3) anadditional modality in the form of image captions generated using anencoder-decoder architecture capable of reading text embedded in images; (4)distinct attention modules to effectively identify the incongruities betweenthe text and two levels of image representations; (5) multi-level cross-domainsemantic incongruity representation achieved through feature fusion. Comparedwith cutting-edge baselines, the proposed model achieves the best accuracy of92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm andMultiBully datasets.</description><author>Sajal Aggarwal, Ananya Pandey, Dinesh Kumar Vishwakarma</author><pubDate>Mon, 05 Aug 2024 16:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02595v1</guid></item><item><title>Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning</title><link>http://arxiv.org/abs/2402.16517v2</link><description>Finite element-based high-order solvers of conservation laws offer largeaccuracy but face challenges near discontinuities due to the Gibbs phenomenon.Artificial viscosity is a popular and effective solution to this problem basedon physical insight. In this work, we present a physics-informed machinelearning algorithm to automate the discovery of artificial viscosity models ina non-supervised paradigm. The algorithm is inspired by reinforcement learningand trains a neural network acting cell-by-cell (the viscosity model) byminimizing a loss defined as the difference with respect to a referencesolution thanks to automatic differentiation. This enables a dataset-freetraining procedure. We prove that the algorithm is effective by integrating itinto a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcaseseveral numerical tests on scalar and vectorial problems, such as Burgers' andEuler's equations in one and two dimensions. Results demonstrate that theproposed approach trains a model that is able to outperform classical viscositymodels. Moreover, we show that the learnt artificial viscosity model is able togeneralize across different problems and parameters.</description><author>Matteo Caldana, Paola F. Antonietti, Luca Dede'</author><pubDate>Mon, 05 Aug 2024 16:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16517v2</guid></item><item><title>Intent Detection and Entity Extraction from BioMedical Literature</title><link>http://arxiv.org/abs/2404.03598v2</link><description>Biomedical queries have become increasingly prevalent in web searches,reflecting the growing interest in accessing biomedical literature. Despiterecent research on large-language models (LLMs) motivated by endeavours toattain generalized intelligence, their efficacy in replacing task anddomain-specific natural language understanding approaches remains questionable.In this paper, we address this question by conducting a comprehensive empiricalevaluation of intent detection and named entity recognition (NER) tasks frombiomedical text. We show that Supervised Fine Tuned approaches are stillrelevant and more effective than general-purpose LLMs. Biomedical transformermodels such as PubMedBERT can surpass ChatGPT on NER task with only 5supervised examples.</description><author>Ankan Mullick, Mukur Gupta, Pawan Goyal</author><pubDate>Mon, 05 Aug 2024 16:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03598v2</guid></item><item><title>Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization</title><link>http://arxiv.org/abs/2408.02584v1</link><description>The ever-increasing volume of digital information necessitates efficientmethods for users to extract key insights from lengthy documents. Aspect-basedsummarization offers a targeted approach, generating summaries focused onspecific aspects within a document. Despite advancements in aspect-basedsummarization research, there is a continuous quest for improved modelperformance. Given that large language models (LLMs) have demonstrated thepotential to revolutionize diverse tasks within natural language processing,particularly in the problem of summarization, this paper explores the potentialof fine-tuning LLMs for the aspect-based summarization task. We evaluate theimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,Gemma and Aya, on a publicly available domain-specific aspect based summarydataset. We hypothesize that this approach will enable these models toeffectively identify and extract aspect-related information, leading tosuperior quality aspect-based summaries compared to the state-of-the-art. Weestablish a comprehensive evaluation framework to compare the performance offine-tuned LLMs against competing aspect-based summarization methods andvanilla counterparts of the fine-tuned LLMs. Our work contributes to the fieldof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMsfor generating high-quality aspect-based summaries. Furthermore, it opens doorsfor further exploration of using LLMs for targeted information extraction tasksacross various NLP domains.</description><author>Ankan Mullick, Sombit Bose, Rounak Saha, Ayan Kumar Bhowmick, Aditya Vempaty, Pawan Goyal, Niloy Ganguly, Prasenjit Dey, Ravi Kokku</author><pubDate>Mon, 05 Aug 2024 16:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02584v1</guid></item><item><title>Clustering and Mining Accented Speech for Inclusive and Fair Speech Recognition</title><link>http://arxiv.org/abs/2408.02582v1</link><description>Modern automatic speech recognition (ASR) systems are typically trained onmore than tens of thousands hours of speech data, which is one of the mainfactors for their great success. However, the distribution of such data istypically biased towards common accents or typical speech patterns. As aresult, those systems often poorly perform on atypical accented speech. In thispaper, we present accent clustering and mining schemes for fair speechrecognition systems which can perform equally well on under-representedaccented speech. For accent recognition, we applied three schemes to overcomelimited size of supervised accent data: supervised or unsupervisedpre-training, distributionally robust optimization (DRO) and unsupervisedclustering. Three schemes can significantly improve the accent recognitionmodel especially for unbalanced and small accented speech. Fine-tuning ASR onthe mined Indian accent speech using the proposed supervised or unsupervisedclustering schemes showed 10.0% and 5.3% relative improvements compared tofine-tuning on the randomly sampled speech, respectively.</description><author>Jaeyoung Kim, Han Lu, Soheil Khorram, Anshuman Tripathi, Qian Zhang, Hasim Sak</author><pubDate>Mon, 05 Aug 2024 16:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02582v1</guid></item><item><title>Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles</title><link>http://arxiv.org/abs/2407.18932v2</link><description>Human mobility is inextricably linked to social issues such as trafficcongestion, energy consumption, and public health; however, privacy concernsrestrict access to mobility data. Recently, research have utilized LargeLanguage Models (LLMs) for human mobility generation, in which the challenge ishow LLMs can understand individuals' mobility behavioral differences togenerate realistic trajectories conforming to real world contexts. This studyhandles this problem by presenting an LLM agent-based framework (MobAgent)composing two phases: understanding-based mobility pattern extraction andreasoning-based trajectory generation, which enables generate more real traveldiaries at urban scale, considering different individual profiles. MobAgentextracts reasons behind specific mobility trendiness and attribute influencesto provide reliable patterns; infers the relationships between contextualfactors and underlying motivations of mobility; and based on the patterns andthe recursive reasoning process, MobAgent finally generates more authentic andpersonalized mobilities that reflect both individual differences and real-worldconstraints. We validate our framework with 0.2 million travel survey data,demonstrating its effectiveness in producing personalized and accurate traveldiaries. This study highlights the capacity of LLMs to provide detailed andsophisticated understanding of human mobility through the real-world mobilitydata.</description><author>Xuchuan Li, Fei Huang, Jianrong Lv, Zhixiong Xiao, Guolong Li, Yang Yue</author><pubDate>Mon, 05 Aug 2024 15:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18932v2</guid></item><item><title>Operational range bounding of spectroscopy models with anomaly detection</title><link>http://arxiv.org/abs/2408.02581v1</link><description>Safe operation of machine learning models requires architectures thatexplicitly delimit their operational ranges. We evaluate the ability of anomalydetection algorithms to provide indicators correlated with degraded modelperformance. By placing acceptance thresholds over such indicators, hardboundaries are formed that define the model's coverage. As a use case, weconsider the extraction of exoplanetary spectra from transit light curves,specifically within the context of ESA's upcoming Ariel mission. IsolationForests are shown to effectively identify contexts where prediction models arelikely to fail. Coverage/error trade-offs are evaluated under conditions ofdata and concept drift. The best performance is seen when Isolation Forestsmodel projections of the prediction model's explainability SHAP values.</description><author>Luís F. Simões, Pierluigi Casale, Marília Felismino, Kai Hou Yip, Ingo P. Waldmann, Giovanna Tinetti, Theresa Lueftinger</author><pubDate>Mon, 05 Aug 2024 15:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02581v1</guid></item><item><title>Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection</title><link>http://arxiv.org/abs/2405.06093v2</link><description>Large Language Models (LLMs) have demonstrated their efficacy across a broadspectrum of tasks in healthcare applications. However, often LLMs need to befine-tuned on task-specific expert annotated data to achieve optimalperformance, which can be expensive and time consuming. In this study, wefine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labelsobtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)tables, which specify care plan in clinical trial protocols. We introduce afiltering mechanism to select high-confidence labels for this tableclassification task, thereby reducing the noise in the auto-generated labels.We show that fine-tuned PaLM-2 with those labels achieves performance thatexceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance isclose to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Ourresults show that leveraging LLM-generated labels through powerful models likegemini-pro can potentially serve as a viable strategy for improving LLMperformance through fine-tuning in specialized tasks, particularly in domainswhere expert annotations are scarce, expensive, or time-consuming to obtain.</description><author>Bhawesh Kumar, Jonathan Amar, Eric Yang, Nan Li, Yugang Jia</author><pubDate>Mon, 05 Aug 2024 15:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06093v2</guid></item><item><title>Bridging Smoothness and Approximation: Theoretical Insights into Over-Smoothing in Graph Neural Networks</title><link>http://arxiv.org/abs/2407.01281v2</link><description>In this paper, we explore the approximation theory of functions defined ongraphs. Our study builds upon the approximation results derived from the$K$-functional. We establish a theoretical framework to assess the lower boundsof approximation for target functions using Graph Convolutional Networks (GCNs)and examine the over-smoothing phenomenon commonly observed in these networks.Initially, we introduce the concept of a $K$-functional on graphs, establishingits equivalence to the modulus of smoothness. We then analyze a typical type ofGCN to demonstrate how the high-frequency energy of the output decays, anindicator of over-smoothing. This analysis provides theoretical insights intothe nature of over-smoothing within GCNs. Furthermore, we establish a lowerbound for the approximation of target functions by GCNs, which is governed bythe modulus of smoothness of these functions. This finding offers a newperspective on the approximation capabilities of GCNs. In our numericalexperiments, we analyze several widely applied GCNs and observe the phenomenonof energy decay. These observations corroborate our theoretical results onexponential decay order.</description><author>Guangrui Yang, Jianfei Li, Ming Li, Han Feng, Ding-Xuan Zhou</author><pubDate>Mon, 05 Aug 2024 15:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01281v2</guid></item><item><title>Artificial Intelligence for Public Health Surveillance in Africa: Applications and Opportunities</title><link>http://arxiv.org/abs/2408.02575v1</link><description>Artificial Intelligence (AI) is revolutionizing various fields, includingpublic health surveillance. In Africa, where health systems frequentlyencounter challenges such as limited resources, inadequate infrastructure,failed health information systems and a shortage of skilled healthprofessionals, AI offers a transformative opportunity. This paper investigatesthe applications of AI in public health surveillance across the continent,presenting successful case studies and examining the benefits, opportunities,and challenges of implementing AI technologies in African healthcare settings.Our paper highlights AI's potential to enhance disease monitoring and healthoutcomes, and support effective public health interventions. The findingspresented in the paper demonstrate that AI can significantly improve theaccuracy and timeliness of disease detection and prediction, optimize resourceallocation, and facilitate targeted public health strategies. Additionally, ourpaper identified key barriers to the widespread adoption of AI in Africanpublic health systems and proposed actionable recommendations to overcome thesechallenges.</description><author>Jean Marie Tshimula, Mitterrand Kalengayi, Dieumerci Makenga, Dorcas Lilonge, Marius Asumani, Déborah Madiya, Élie Nkuba Kalonji, Hugues Kanda, René Manassé Galekwa, Josias Kumbu, Hardy Mikese, Grace Tshimula, Jean Tshibangu Muabila, Christian N. Mayemba, D'Jeff K. Nkashama, Kalonji Kalala, Steve Ataky, Tighana Wenge Basele, Mbuyi Mukendi Didier, Selain K. Kasereka, Maximilien V. Dialufuma, Godwill Ilunga Wa Kumwita, Lionel Muyuku, Jean-Paul Kimpesa, Dominique Muteba, Aaron Aruna Abedi, Lambert Mukendi Ntobo, Gloria M. Bundutidi, Désiré Kulimba Mashinda, Emmanuel Kabengele Mpinga, Nathanaël M. Kasoro</author><pubDate>Mon, 05 Aug 2024 15:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02575v1</guid></item><item><title>Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs</title><link>http://arxiv.org/abs/2408.02571v1</link><description>The emoticons are symbolic representations that generally accompany thetextual content to visually enhance or summarize the true intention of awritten message. Although widely utilized in the realm of social media, thecore semantics of these emoticons have not been extensively explored based onmultiple modalities. Incorporating textual and visual information within asingle message develops an advanced way of conveying information. Hence, thisresearch aims to analyze the relationship among sentences, visuals, andemoticons. For an orderly exposition, this paper initially provides a detailedexamination of the various techniques for extracting multimodal features,emphasizing the pros and cons of each method. Through conducting acomprehensive examination of several multimodal algorithms, with specificemphasis on the fusion approaches, we have proposed a novel contrastivelearning based multimodal architecture. The proposed model employs the jointtraining of dual-branch encoder along with the contrastive learning toaccurately map text and images into a common latent space. Our key finding isthat by integrating the principle of contrastive learning with that of theother two branches yields superior results. The experimental resultsdemonstrate that our suggested methodology surpasses existing multimodalapproaches in terms of accuracy and robustness. The proposed model attained anaccuracy of 91% and an MCC-score of 90% while assessing emoticons using theMultimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidencethat deep features acquired by contrastive learning are more efficient,suggesting that the proposed fusion technique also possesses stronggeneralisation capabilities for recognising emoticons across several modes.</description><author>Ananya Pandey, Dinesh Kumar Vishwakarma</author><pubDate>Mon, 05 Aug 2024 15:45:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02571v1</guid></item><item><title>Cross-Modality Clustering-based Self-Labeling for Multimodal Data Classification</title><link>http://arxiv.org/abs/2408.02568v1</link><description>Technological advances facilitate the ability to acquire multimodal data,posing a challenge for recognition systems while also providing an opportunityto use the heterogeneous nature of the information to increase thegeneralization capability of models. An often overlooked issue is the cost ofthe labeling process, which is typically high due to the need for a significantinvestment in time and money associated with human experts. Existingsemi-supervised learning methods often focus on operating in the feature spacecreated by the fusion of available modalities, neglecting the potential forcross-utilizing complementary information available in each modality. Toaddress this problem, we propose Cross-Modality Clustering-based Self-Labeling(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instancesbelonging to each modality in the deep feature space and then propagates knownlabels within the resulting clusters. Next, information about the instances'class membership in each modality is exchanged based on the Euclidean distanceto ensure more accurate labeling. Experimental evaluation conducted on 20datasets derived from the MM-IMDb dataset indicates that cross-propagation oflabels between modalities -- especially when the number of pre-labeledinstances is small -- can allow for more reliable labeling and thus increasethe classification performance in each modality.</description><author>Paweł Zyblewski, Leandro L. Minku</author><pubDate>Mon, 05 Aug 2024 15:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02568v1</guid></item><item><title>Vision Learners Meet Web Image-Text Pairs</title><link>http://arxiv.org/abs/2301.07088v3</link><description>Many self-supervised learning methods are pre-trained on the well-curatedImageNet-1K dataset. In this work, given the excellent scalability of web data,we consider self-supervised pre-training on noisy web sourced image-text paireddata. First, we conduct a benchmark study of representative self-supervisedpre-training methods on large-scale web data in a like-for-like setting. Wecompare a range of methods, including single-modal ones that use maskedtraining objectives and multi-modal ones that use image-text constrastivetraining. We observe that existing multi-modal methods do not outperform theirsingle-modal counterparts on vision transfer learning tasks. We derive aninformation-theoretical view to explain these benchmark results, which providesinsight into how to design a novel vision learner. Inspired by this insight, wepresent a new visual representation pre-training method, MUlti-modalGenerator~(MUG), that learns from scalable web sourced image-text data. MUGachieves state-of-the-art transfer performance on a variety of tasks anddemonstrates promising scaling properties. Pre-trained models and code will bemade public upon acceptance.</description><author>Bingchen Zhao, Quan Cui, Hao Wu, Osamu Yoshie, Cheng Yang, Oisin Mac Aodha</author><pubDate>Mon, 05 Aug 2024 15:38:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07088v3</guid></item><item><title>HQOD: Harmonious Quantization for Object Detection</title><link>http://arxiv.org/abs/2408.02561v1</link><description>Task inharmony problem commonly occurs in modern object detectors, leading toinconsistent qualities between classification and regression tasks. Thepredicted boxes with high classification scores but poor localization positionsor low classification scores but accurate localization positions will worsenthe performance of detectors after Non-Maximum Suppression. Furthermore, whenobject detectors collaborate with Quantization-Aware Training (QAT), we observethat the task inharmony problem will be further exacerbated, which isconsidered one of the main causes of the performance degradation of quantizeddetectors. To tackle this issue, we propose the Harmonious Quantization forObject Detection (HQOD) framework, which consists of two components. Firstly,we propose a task-correlated loss to encourage detectors to focus on improvingsamples with lower task harmony quality during QAT. Secondly, a harmoniousIntersection over Union (IoU) loss is incorporated to balance the optimizationof the regression branch across different IoU levels. The proposed HQOD can beeasily integrated into different QAT algorithms and detectors. Remarkably, onthe MS COCO dataset, our 4-bit ATSS with ResNet-50 backbone achieves astate-of-the-art mAP of 39.6%, even surpassing the full-precision one.</description><author>Long Huang, Zhiwei Dong, Song-Lu Chen, Ruiyao Zhang, Shutong Ti, Feng Chen, Xu-Cheng Yin</author><pubDate>Mon, 05 Aug 2024 15:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02561v1</guid></item><item><title>Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information</title><link>http://arxiv.org/abs/2408.02559v1</link><description>Large language models (LLMs) have shown success in handling simple games withimperfect information and enabling multi-agent coordination, but their abilityto facilitate practical collaboration against other agents in complex,imperfect information environments, especially in a non-English environment,still needs to be explored. This study investigates the applicability ofknowledge acquired by open-source and API-based LLMs to sophisticatedtext-based games requiring agent collaboration under imperfect information,comparing their performance to established baselines using other types ofagents. We propose a Theory of Mind (ToM) planning technique that allows LLMagents to adapt their strategy against various adversaries using only gamerules, current state, and historical context as input. An external tool wasincorporated to mitigate the challenge of dynamic and extensive action spacesin this card game. Our results show that although a performance gap existsbetween current LLMs and state-of-the-art reinforcement learning (RL) models,LLMs demonstrate ToM capabilities in this game setting. It consistentlyimproves their performance against opposing agents, suggesting their ability tounderstand the actions of allies and adversaries and establish collaborationwith allies. To encourage further research and understanding, we have made ourcodebase openly accessible.</description><author>Yauwai Yim, Chunkit Chan, Tianyu Shi, Zheye Deng, Wei Fan, Tianshi Zheng, Yangqiu Song</author><pubDate>Mon, 05 Aug 2024 15:36:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02559v1</guid></item><item><title>Peer-induced Fairness: A Causal Approach to Reveal Algorithmic Unfairness in Credit Approval</title><link>http://arxiv.org/abs/2408.02558v1</link><description>This paper introduces a novel framework, "peer-induced fairness", toscientifically audit algorithmic fairness. It addresses a critical but oftenoverlooked issue: distinguishing between adverse outcomes due to algorithmicdiscrimination and those resulting from individuals' insufficient capabilities.By utilizing counterfactual fairness and advanced causal inference techniques,such as the Single World Intervention Graph, this model-agnostic approachevaluates fairness at the individual level through peer comparisons andhypothesis testing. It also tackles challenges like data scarcity andimbalance, offering a flexible, plug-and-play self-audit tool for stakeholdersand an external audit tool for regulators, while providing explainable feedbackfor those affected by unfavorable decisions.</description><author>Shiqi Fang, Zexun Chen, Jake Ansell</author><pubDate>Mon, 05 Aug 2024 15:35:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02558v1</guid></item><item><title>MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization</title><link>http://arxiv.org/abs/2408.02555v1</link><description>We introduce MeshAnything V2, an autoregressive transformer that generatesArtist-Created Meshes (AM) aligned to given shapes. It can be integrated withvarious 3D asset production pipelines to achieve high-quality, highlycontrollable AM generation. MeshAnything V2 surpasses previous methods in bothefficiency and performance using models of the same size. These improvementsare due to our newly proposed mesh tokenization method: Adjacent MeshTokenization (AMT). Different from previous methods that represent each facewith three vertices, AMT uses a single vertex whenever possible. Compared toprevious methods, AMT requires about half the token sequence length torepresent the same mesh in average. Furthermore, the token sequences from AMTare more compact and well-structured, fundamentally benefiting AM generation.Our extensive experiments show that AMT significantly improves the efficiencyand performance of AM generation. Project Page:https://buaacyw.github.io/meshanything-v2/</description><author>Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, Guosheng Lin</author><pubDate>Mon, 05 Aug 2024 15:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02555v1</guid></item><item><title>Process-constrained batch Bayesian approaches for yield optimization in multi-reactor systems</title><link>http://arxiv.org/abs/2408.02551v1</link><description>The optimization of yields in multi-reactor systems, which are advanced toolsin heterogeneous catalysis research, presents a significant challenge due tohierarchical technical constraints. To this respect, this work introduces anovel approach called process-constrained batch Bayesian optimization viaThompson sampling (pc-BO-TS) and its generalized hierarchical extension(hpc-BO-TS). This method, tailored for the efficiency demands in multi-reactorsystems, integrates experimental constraints and balances between explorationand exploitation in a sequential batch optimization strategy. It offers animprovement over other Bayesian optimization methods. The performance ofpc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in arealistic scenario based on data obtained from high-throughput experiments doneon a multi-reactor system available in the REALCAT platform. The proposedmethods often outperform other sequential Bayesian optimizations and existingprocess-constrained batch Bayesian optimization methods. This work proposes anovel approach to optimize the yield of a reaction in a multi-reactor system,marking a significant step forward in digital catalysis and generally inoptimization methods for chemical engineering.</description><author>Markus Grimm, Sébastien Paul, Pierre Chainais</author><pubDate>Mon, 05 Aug 2024 15:26:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02551v1</guid></item><item><title>The Role of Functional Muscle Networks in Improving Hand Gesture Perception for Human-Machine Interfaces</title><link>http://arxiv.org/abs/2408.02547v1</link><description>Developing accurate hand gesture perception models is critical for variousrobotic applications, enabling effective communication between humans andmachines and directly impacting neurorobotics and interactive robots. Recently,surface electromyography (sEMG) has been explored for its rich informationalcontext and accessibility when combined with advanced machine learningapproaches and wearable systems. The literature presents numerous approaches toboost performance while ensuring robustness for neurorobots using sEMG, oftenresulting in models requiring high processing power, large datasets, and lessscalable solutions. This paper addresses this challenge by proposing thedecoding of muscle synchronization rather than individual muscle activation. Westudy coherence-based functional muscle networks as the core of our perceptionmodel, proposing that functional synchronization between muscles and thegraph-based network of muscle connectivity encode contextual information aboutintended hand gestures. This can be decoded using shallow machine learningapproaches without the need for deep temporal networks. Our technique couldimpact myoelectric control of neurorobots by reducing computational burdens andenhancing efficiency. The approach is benchmarked on the Ninapro database,which contains 12 EMG signals from 40 subjects performing 17 hand gestures. Itachieves an accuracy of 85.1%, demonstrating improved performance compared toexisting methods while requiring much less computational power. The resultssupport the hypothesis that a coherence-based functional muscle network encodescritical information related to gesture execution, significantly enhancing handgesture perception with potential applications for neurorobotic systems andinteractive machines.</description><author>Costanza Armanini, Tuka Alhanai, Farah E. Shamout, S. Farokh Atashzar</author><pubDate>Mon, 05 Aug 2024 15:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02547v1</guid></item><item><title>RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2408.02545v1</link><description>Implementing Retrieval-Augmented Generation (RAG) systems is inherentlycomplex, requiring deep understanding of data, use cases, and intricate designdecisions. Additionally, evaluating these systems presents significantchallenges, necessitating assessment of both retrieval accuracy and generativequality through a multi-faceted approach. We introduce RAG Foundry, anopen-source framework for augmenting large language models for RAG use cases.RAG Foundry integrates data creation, training, inference and evaluation into asingle workflow, facilitating the creation of data-augmented datasets fortraining and evaluating large language models in RAG settings. This integrationenables rapid prototyping and experimentation with various RAG techniques,allowing users to easily generate datasets and train RAG models using internalor specialized knowledge sources. We demonstrate the framework effectiveness byaugmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAGconfigurations, showcasing consistent improvements across threeknowledge-intensive datasets. Code is released as open-source inhttps://github.com/IntelLabs/RAGFoundry.</description><author>Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat, Peter Izsak</author><pubDate>Mon, 05 Aug 2024 15:16:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02545v1</guid></item><item><title>Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions</title><link>http://arxiv.org/abs/2408.02544v1</link><description>This paper investigates the faithfulness of multimodal large language model(MLLM) agents in the graphical user interface (GUI) environment, aiming toaddress the research question of whether multimodal GUI agents can bedistracted by environmental context. A general setting is proposed where boththe user and the agent are benign, and the environment, while not malicious,contains unrelated content. A wide range of MLLMs are evaluated as GUI agentsusing our simulated dataset, following three working patterns with differentlevels of perception. Experimental results reveal that even the most powerfulmodels, whether generalist agents or specialist GUI agents, are susceptible todistractions. While recent studies predominantly focus on the helpfulness(i.e., action accuracy) of multimodal agents, our findings indicate that theseagents are prone to environmental distractions, resulting in unfaithfulbehaviors. Furthermore, we switch to the adversarial perspective and implementenvironment injection, demonstrating that such unfaithfulness can be exploited,leading to unexpected risks.</description><author>Xinbei Ma, Yiting Wang, Yao Yao, Tongxin Yuan, Aston Zhang, Zhuosheng Zhang, Hai Zhao</author><pubDate>Mon, 05 Aug 2024 15:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02544v1</guid></item><item><title>Transformer Layers as Painters</title><link>http://arxiv.org/abs/2407.09298v2</link><description>Despite their nearly universal adoption for large language models, theinternal workings of transformers are not well understood. We aim to betterunderstand the impact of removing or reorganizing information throughout thelayers of a pretrained transformer. Such an understanding could both yieldbetter usage of existing models as well as to make architectural improvementsto produce new variants. We present a series of empirical studies on frozenmodels that show that the lower and final layers of pretrained transformersdiffer from middle layers, but that middle layers have a surprising amount ofuniformity. We further show that some classes of problems have robustness toskipping layers, running the layers in an order different from how they weretrained, or running the layers in parallel. Our observations suggest that evenfrozen pretrained models may gracefully trade accuracy for latency by skippinglayers or running layers in parallel.</description><author>Qi Sun, Marc Pickett, Aakash Kumar Nain, Llion Jones</author><pubDate>Mon, 05 Aug 2024 15:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09298v2</guid></item><item><title>What Do Language Models Learn in Context? The Structured Task Hypothesis</title><link>http://arxiv.org/abs/2406.04216v3</link><description>Large language models (LLMs) exhibit an intriguing ability to learn a noveltask from in-context examples presented in a demonstration, termed in-contextlearning (ICL). Understandably, a swath of research has been dedicated touncovering the theories underpinning ICL. One popular hypothesis explains ICLby task selection. LLMs identify the task based on the demonstration andgeneralize it to the prompt. Another popular hypothesis is that ICL is a formof meta-learning, i.e., the models learn a learning algorithm at pre-trainingtime and apply it to the demonstration. Finally, a third hypothesis argues thatLLMs use the demonstration to select a composition of tasks learned duringpre-training to perform ICL. In this paper, we empirically explore these threehypotheses that explain LLMs' ability to learn in context with a suite ofexperiments derived from common text classification tasks. We invalidate thefirst two hypotheses with counterexamples and provide evidence in support ofthe last hypothesis. Our results suggest an LLM could learn a novel task incontext via composing tasks learned during pre-training.</description><author>Jiaoda Li, Yifan Hou, Mrinmaya Sachan, Ryan Cotterell</author><pubDate>Mon, 05 Aug 2024 15:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04216v3</guid></item><item><title>Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms</title><link>http://arxiv.org/abs/2310.02812v2</link><description>Manufacturing is gathering extensive amounts of diverse data, thanks to thegrowing number of sensors and rapid advances in sensing technologies. Among thevarious data types available in SMS settings, time-series data plays a pivotalrole. Hence, TSC emerges is crucial in this domain. The objective of this studyis to fill this gap by providing a rigorous experimental evaluation of the SoTAML and DL algorithms for TSC tasks in manufacturing and industrial settings. Wefirst explored and compiled a comprehensive list of more than 92 SoTAalgorithms from both TSC and manufacturing literature. Following, we selectedthe 36 most representative algorithms from this list. To evaluate theirperformance across various manufacturing classification tasks, we curated a setof 22 manufacturing datasets, representative of different characteristics thatcover diverse manufacturing problems. Subsequently, we implemented andevaluated the algorithms on the manufacturing benchmark datasets, and analyzedthe results for each dataset. Based on the results, ResNet, DrCIF,InceptionTime, and ARSENAL are the top-performing algorithms, boasting anaverage accuracy of over 96.6% across all 22 manufacturing TSC datasets. Thesefindings underscore the robustness, efficiency, scalability, and effectivenessof convolutional kernels in capturing temporal features in time-series data, asthree out of the top four performing algorithms leverage these kernels forfeature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserverecognition for their effectiveness in capturing features within time-seriesdata using RNN-based structures.</description><author>Mojtaba A. Farahani, M. R. McCormick, Ramy Harik, Thorsten Wuest</author><pubDate>Mon, 05 Aug 2024 15:06:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02812v2</guid></item><item><title>LMEMs for post-hoc analysis of HPO Benchmarking</title><link>http://arxiv.org/abs/2408.02533v1</link><description>The importance of tuning hyperparameters in Machine Learning (ML) and DeepLearning (DL) is established through empirical research and applications,evident from the increase in new hyperparameter optimization (HPO) algorithmsand benchmarks steadily added by the community. However, current benchmarkingpractices using averaged performance across many datasets may obscure keydifferences between HPO methods, especially for pairwise comparisons. In thiswork, we apply Linear Mixed-Effect Models-based (LMEMs) significance testingfor post-hoc analysis of HPO benchmarking runs. LMEMs allow flexible andexpressive modeling on the entire experiment data, including information suchas benchmark meta-features, offering deeper insights than current analysispractices. We demonstrate this through a case study on the PriorBand paper'sexperiment data to find insights not reported in the original work.</description><author>Anton Geburek, Neeratyoy Mallik, Danny Stoll, Xavier Bouthillier, Frank Hutter</author><pubDate>Mon, 05 Aug 2024 15:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02533v1</guid></item><item><title>High-arity PAC learning via exchangeability</title><link>http://arxiv.org/abs/2402.14294v2</link><description>We develop a theory of high-arity PAC learning, which is statistical learningin the presence of "structured correlation". In this theory, hypotheses areeither graphs, hypergraphs or, more generally, structures in finite relationallanguages, and i.i.d. sampling is replaced by sampling an induced substructure,producing an exchangeable distribution. Our main theorems establish ahigh-arity (agnostic) version of the fundamental theorem of statisticallearning.</description><author>Leonardo N. Coregliano, Maryanthe Malliaris</author><pubDate>Mon, 05 Aug 2024 14:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14294v2</guid></item><item><title>Counterfactual Shapley Values for Explaining Reinforcement Learning</title><link>http://arxiv.org/abs/2408.02529v1</link><description>This paper introduces a novel approach Counterfactual Shapley Values (CSV),which enhances explainability in reinforcement learning (RL) by integratingcounterfactual analysis with Shapley Values. The approach aims to quantify andcompare the contributions of different state dimensions to various actionchoices. To more accurately analyze these impacts, we introduce newcharacteristic value functions, the ``Counterfactual Difference CharacteristicValue" and the ``Average Counterfactual Difference Characteristic Value." Thesefunctions help calculate the Shapley values to evaluate the differences incontributions between optimal and non-optimal actions. Experiments acrossseveral RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate theeffectiveness of the CSV method. The results show that this method not onlyimproves transparency in complex RL systems but also quantifies the differencesacross various decisions.</description><author>Yiwei Shi, Qi Zhang, Kevin McAreavey, Weiru Liu</author><pubDate>Mon, 05 Aug 2024 14:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02529v1</guid></item><item><title>Doubly-Dynamic ISAC Precoding for Vehicular Networks: A Constrained Deep Reinforcement Learning (CDRL) Approach</title><link>http://arxiv.org/abs/2405.14347v2</link><description>Integrated sensing and communication (ISAC) technology is essential forenabling the vehicular networks. However, the communication channel in thisscenario exhibits time-varying characteristics, and the potential targets maymove rapidly, creating a doubly-dynamic phenomenon. This nature poses achallenge for real-time precoder design. While optimization-based solutions arewidely researched, they are complex and heavily rely on perfect priorinformation, which is impractical in double dynamics. To address thischallenge, we propose using constrained deep reinforcement learning (CDRL) tofacilitate dynamic updates to the ISAC precoder design. Additionally, theprimal dual-deep deterministic policy gradient (PD-DDPG) and Wolpertingerarchitecture are tailored to efficiently train the algorithm under complexconstraints and variable numbers of users. The proposed scheme not only adaptsto the dynamics based on observations but also leverages environmentalinformation to enhance performance and reduce complexity. Its superiority overexisting candidates has been validated through experiments.</description><author>Zonghui Yang, Shijian Gao, Xiang Cheng</author><pubDate>Mon, 05 Aug 2024 14:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14347v2</guid></item><item><title>Single-tap Latency Reduction with Single- or Double- tap Prediction</title><link>http://arxiv.org/abs/2408.02525v1</link><description>Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops(touchpad), and single and double taps are the most basic and common operationson them. The detection of single or double taps causes the single-tap latencyproblem, which creates a bottleneck in terms of the sensitivity of touchinputs. To reduce the single-tap latency, we propose a novelmachine-learning-based tap prediction method called PredicTaps. Our methodpredicts whether a detected tap is a single tap or the first contact of adouble tap without having to wait for the hundreds of millisecondsconventionally required. We present three evaluations and one user evaluationthat demonstrate its broad applicability and usability for various tapsituations on two form factors (touchpad and smartphone). The results showedPredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptopsand to 17.6 ms on smartphones without reducing usability.</description><author>Naoto Nishida, Kaori Ikematsu, Junichi Sato, Shota Yamanaka, Kota Tsubouchi</author><pubDate>Mon, 05 Aug 2024 14:46:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02525v1</guid></item><item><title>OneLove beyond the field -- A few-shot pipeline for topic and sentiment analysis during the FIFA World Cup in Qatar</title><link>http://arxiv.org/abs/2408.02520v1</link><description>The FIFA World Cup in Qatar was discussed extensively in the news and onsocial media. Due to news reports with allegations of human rights violations,there were calls to boycott it. Wearing a OneLove armband was part of a plannedprotest activity. Controversy around the armband arose when FIFA threatened tosanction captains who wear it. To understand what topics Twitter users Tweetedabout and what the opinion of German Twitter users was towards the OneLovearmband, we performed an analysis of German Tweets published during the WorldCup using in-context learning with LLMs. We validated the labels on humanannotations. We found that Twitter users initially discussed the armband'simpact, LGBT rights, and politics; after the ban, the conversation shiftedtowards politics in sports in general, accompanied by a subtle shift insentiment towards neutrality. Our evaluation serves as a framework for futureresearch to explore the impact of sports activism and evolving publicsentiment. This is especially useful in settings where labeling datasets forspecific opinions is unfeasible, such as when events are unfolding.</description><author>Christoph Rauchegger, Sonja Mei Wang, Pieter Delobelle</author><pubDate>Mon, 05 Aug 2024 14:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02520v1</guid></item><item><title>Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem Compatibility Estimation</title><link>http://arxiv.org/abs/2408.02514v1</link><description>This paper explores the automated process of determining stem compatibilityby identifying audio recordings of single instruments that blend well with agiven musical context. To tackle this challenge, we present Stem-JEPA, a novelJoint-Embedding Predictive Architecture (JEPA) trained on a multi-track datasetusing a self-supervised learning approach. Our model comprises two networks: an encoder and a predictor, which arejointly trained to predict the embeddings of compatible stems from theembeddings of a given context, typically a mix of several instruments. Traininga model in this manner allows its use in estimating stem compatibility -retrieving, aligning, or generating a stem to match a given mix - or fordownstream tasks such as genre or key estimation, as the training paradigmrequires the model to learn information related to timbre, harmony, and rhythm. We evaluate our model's performance on a retrieval task on the MUSDB18dataset, testing its ability to find the missing stem from a mix and through asubjective user study. We also show that the learned embeddings capturetemporal alignment information and, finally, evaluate the representationslearned by our model on several downstream tasks, highlighting that theyeffectively capture meaningful musical features.</description><author>Alain Riou, Stefan Lattner, Gaëtan Hadjeres, Michael Anslow, Geoffroy Peeters</author><pubDate>Mon, 05 Aug 2024 14:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02514v1</guid></item><item><title>Practical Attacks against Black-box Code Completion Engines</title><link>http://arxiv.org/abs/2408.02509v1</link><description>Modern code completion engines, powered by large language models, havedemonstrated impressive capabilities to generate functionally correct codebased on surrounding context. As these tools are extensively used by millionsof developers, it is crucial to investigate their security implications. Inthis work, we present INSEC, a novel attack that directs code completionengines towards generating vulnerable code. In line with most commercialcompletion engines, such as GitHub Copilot, INSEC assumes only black-box queryaccess to the targeted engine, without requiring any knowledge of the engine'sinternals. Our attack works by inserting a malicious attack string as a shortcomment in the completion input. To derive the attack string, we design aseries of specialized initialization schemes and an optimization procedure forfurther refinement. We demonstrate the strength of INSEC not only onstate-of-the-art open-source models but also on black-box commercial servicessuch as the OpenAI API and GitHub Copilot. On a comprehensive set ofsecurity-critical test cases covering 16 CWEs across 5 programming languages,INSEC significantly increases the likelihood of the considered completionengines in generating unsafe code by &gt;50% in absolute, while maintaining theability in producing functionally correct code. At the same time, our attackhas low resource requirements, and can be developed for a cost of well underten USD on commodity hardware.</description><author>Slobodan Jenko, Jingxuan He, Niels Mündler, Mark Vero, Martin Vechev</author><pubDate>Mon, 05 Aug 2024 14:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02509v1</guid></item><item><title>Estimating Pore Location of PBF-LB/M Processes with Segmentation Models</title><link>http://arxiv.org/abs/2408.02507v1</link><description>Reliably manufacturing defect free products is still an open challenge forLaser Powder Bed Fusion processes. Particularly, pores that occur frequentlyhave a negative impact on mechanical properties like fatigue performance.Therefore, an accurate localisation of pores is mandatory for qualityassurance, but requires time-consuming post-processing steps like computertomography scans. Although existing solutions using in-situ monitoring data candetect pore occurrence within a layer, they are limited in their localisationprecision. Therefore, we propose a pore localisation approach that estimatestheir position within a single layer using a Gaussian kernel densityestimation. This allows segmentation models to learn the correlation betweenin-situ monitoring data and the derived probability distribution of poreoccurrence. Within our experiments, we compare the prediction performance ofdifferent segmentation models depending on machine parameter configuration andgeometry features. From our results, we conclude that our approach allows aprecise localisation of pores that requires minimal data preprocessing. Ourresearch extends the literature by providing a foundation for more precise poredetection systems.</description><author>Hans Aoyang Zhou, Jan Theunissen, Marco Kemmerling, Anas Abdelrazeq, Johannes Henrich Schleifenbaum, Robert H. Schmitt</author><pubDate>Mon, 05 Aug 2024 14:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02507v1</guid></item><item><title>UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model</title><link>http://arxiv.org/abs/2408.02503v1</link><description>Significant advancements has recently been achieved in the field ofmulti-modal large language models (MLLMs), demonstrating their remarkablecapabilities in understanding and reasoning across diverse tasks. However,these models are often trained for specific tasks and rely on task-specificinput-output formats, limiting their applicability to a broader range of tasks.This raises a fundamental question: Can we develop a unified approach torepresent and handle different multi-modal tasks to maximize thegeneralizability of MLLMs? In this paper, we propose UnifiedMLLM, acomprehensive model designed to represent various tasks using a unifiedrepresentation. Our model exhibits strong capabilities in comprehending theimplicit intent of user instructions and preforming reasoning. In addition togenerating textual responses, our model also outputs task tokens and groundingtokens, serving as indicators of task types and task granularity. These outputsare subsequently routed through the task router and directed to specific expertmodels for task completion. To train our model, we construct a task-specificdataset and an 100k multi-task dataset encompassing complex scenarios.Employing a three-stage training strategy, we equip our model with robustreasoning and task processing capabilities while preserving its generalizationcapacity and knowledge reservoir. Extensive experiments showcase the impressiveperformance of our unified representation approach across various tasks,surpassing existing methodologies. Furthermore, our approach exhibitsexceptional scalability and generality. Our code, model, and dataset will beavailable at \url{https://github.com/lzw-lzw/UnifiedMLLM}.</description><author>Zhaowei Li, Wei Wang, YiQing Cai, Xu Qi, Pengyu Wang, Dong Zhang, Hang Song, Botian Jiang, Zhida Huang, Tao Wang</author><pubDate>Mon, 05 Aug 2024 14:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02503v1</guid></item><item><title>Automatic rating of incomplete hippocampal inversions evaluated across multiple cohorts</title><link>http://arxiv.org/abs/2408.02496v1</link><description>Incomplete Hippocampal Inversion (IHI), sometimes called hippocampalmalrotation, is an atypical anatomical pattern of the hippocampus found inabout 20% of the general population. IHI can be visually assessed on coronalslices of T1 weighted MR images, using a composite score that combines fouranatomical criteria. IHI has been associated with several brain disorders(epilepsy, schizophrenia). However, these studies were based on small samples.Furthermore, the factors (genetic or environmental) that contribute to thegenesis of IHI are largely unknown. Large-scale studies are thus needed tofurther understand IHI and their potential relationships to neurological andpsychiatric disorders. However, visual evaluation is long and tedious,justifying the need for an automatic method. In this paper, we propose, for thefirst time, to automatically rate IHI. We proceed by predicting four anatomicalcriteria, which are then summed up to form the IHI score, providing theadvantage of an interpretable score. We provided an extensive experimentalinvestigation of different machine learning methods and training strategies. Weperformed automatic rating using a variety of deep learning models (conv5-FC3,ResNet and SECNN) as well as a ridge regression. We studied the generalizationof our models using different cohorts and performed multi-cohort learning. Werelied on a large population of 2,008 participants from the IMAGEN study, 993and 403 participants from the QTIM/QTAB studies as well as 985 subjects fromthe UKBiobank. We showed that deep learning models outperformed a ridgeregression. We demonstrated that the performances of the conv5-FC3 network wereat least as good as more complex networks while maintaining a low complexityand computation time. We showed that training on a single cohort may lack invariability while training on several cohorts improves generalization.</description><author>Lisa Hemforth, Baptiste Couvy-Duchesne, Kevin De Matos, Camille Brianceau, Matthieu Joulot, Tobias Banaschewski, Arun L. W. Bokde, Sylvane Desrivières, Herta Flor, Antoine Grigis, Hugh Garavan, Penny Gowland, Andreas Heinz, Rüdiger Brühl, Jean-Luc Martinot, Marie-Laure Paillère Martinot, Eric Artiges, Dimitri Papadopoulos, Herve Lemaitre, Tomas Paus, Luise Poustka, Sarah Hohmann, Nathalie Holz, Juliane H. Fröhner, Michael N. Smolka, Nilakshi Vaidya, Henrik Walter, Robert Whelan, Gunter Schumann, Christian Büchel, JB Poline, Bernd Itterman, Vincent Frouin, Alexandre Martin, IMAGEN study group, Claire Cury, Olivier Colliot</author><pubDate>Mon, 05 Aug 2024 14:19:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02496v1</guid></item><item><title>HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions</title><link>http://arxiv.org/abs/2408.02494v1</link><description>Traditional deep learning models rely on methods such as softmaxcross-entropy and ArcFace loss for tasks like classification and facerecognition. These methods mainly explore angular features in a hypersphericalspace, often resulting in entangled inter-class features due to dense angulardata across many classes. In this paper, a new field of feature exploration isproposed known as HyperSpaceX which enhances class discrimination by exploringboth angular and radial dimensions in multi-hyperspherical spaces, facilitatedby a novel DistArc loss. The proposed DistArc loss encompasses three featurearrangement components: two angular and one radial, enforcing intra-classbinding and inter-class separation in multi-radial arrangement, improvingfeature discriminability. Evaluation of HyperSpaceX framework for the novelrepresentation utilizes a proposed predictive measure that accounts for bothangular and radial elements, providing a more comprehensive assessment of modelaccuracy beyond standard metrics. Experiments across seven objectclassification and six face recognition datasets demonstrate state-of-the-art(SoTA) results obtained from HyperSpaceX, achieving up to a 20% performanceimprovement on large-scale object datasets in lower dimensions and up to 6%gain in higher dimensions.</description><author>Chiranjeev Chiranjeev, Muskan Dosi, Kartik Thakral, Mayank Vatsa, Richa Singh</author><pubDate>Mon, 05 Aug 2024 14:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02494v1</guid></item><item><title>Full error analysis of policy gradient learning algorithms for exploratory linear quadratic mean-field control problem in continuous time with common noise</title><link>http://arxiv.org/abs/2408.02489v1</link><description>We consider reinforcement learning (RL) methods for finding optimal policiesin linear quadratic (LQ) mean field control (MFC) problems over an infinitehorizon in continuous time, with common noise and entropy regularization. Westudy policy gradient (PG) learning and first demonstrate convergence in amodel-based setting by establishing a suitable gradient dominationcondition.Next, our main contribution is a comprehensive error analysis, wherewe prove the global linear convergence and sample complexity of the PGalgorithm with two-point gradient estimates in a model-free setting withunknown parameters. In this setting, the parameterized optimal policies arelearned from samples of the states and population distribution.Finally, weprovide numerical evidence supporting the convergence of our implementedalgorithms.</description><author>Noufel Frikha, Huyên Pham, Xuanye Song</author><pubDate>Mon, 05 Aug 2024 14:11:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02489v1</guid></item><item><title>A First Look at License Compliance Capability of LLMs in Code Generation</title><link>http://arxiv.org/abs/2408.02487v1</link><description>Recent advances in Large Language Models (LLMs) have revolutionized codegeneration, leading to widespread adoption of AI coding tools by developers.However, LLMs can generate license-protected code without providing thenecessary license information, leading to potential intellectual propertyviolations during software production. This paper addresses the critical, yetunderexplored, issue of license compliance in LLM-generated code byestablishing a benchmark to evaluate the ability of LLMs to provide accuratelicense information for their generated code. To establish this benchmark, weconduct an empirical study to identify a reasonable standard for "strikingsimilarity" that excludes the possibility of independent creation, indicating acopy relationship between the LLM output and certain open-source code. Based onthis standard, we propose an evaluation benchmark LiCoEval, to evaluate thelicense compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popularLLMs, finding that even top-performing LLMs produce a non-negligible proportion(0.88% to 2.01%) of code strikingly similar to existing open-sourceimplementations. Notably, most LLMs fail to provide accurate licenseinformation, particularly for code under copyleft licenses. These findingsunderscore the urgent need to enhance LLM compliance capabilities in codegeneration tasks. Our study provides a foundation for future research anddevelopment to improve license compliance in AI-assisted software development,contributing to both the protection of open-source software copyrights and themitigation of legal risks for LLM users.</description><author>Weiwei Xu, Kai Gao, Hao He, Minghui Zhou</author><pubDate>Mon, 05 Aug 2024 14:09:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02487v1</guid></item><item><title>Annotation Cost-Efficient Active Learning for Deep Metric Learning Driven Remote Sensing Image Retrieval</title><link>http://arxiv.org/abs/2406.10107v2</link><description>Deep metric learning (DML) has shown to be effective for content-based imageretrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on ahigh number of annotated images to accurately learn model parameters of deepneural networks (DNNs). However, gathering such data is time-consuming andcostly. To address this, we propose an annotation cost-efficient activelearning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims tocreate a small but informative training set made up of similar and dissimilarimage pairs to be utilized for accurately learning a metric space. Theinformativeness of image pairs is evaluated by combining uncertainty anddiversity criteria. To assess the uncertainty of image pairs, we introduce twoalgorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binaryclassifier guided uncertainty estimation (BCGUE). MGUE algorithm automaticallyestimates a threshold value that acts as a boundary between similar anddissimilar image pairs based on the distances in the metric space. The closerthe similarity between image pairs is to the estimated threshold value thehigher their uncertainty. BCGUE algorithm estimates the uncertainty of theimage pairs based on the confidence of the classifier in assigning correctsimilarity labels. The diversity criterion is assessed through aclustering-based strategy. ANNEAL combines either MGUE or BCGUE algorithm withthe clustering-based strategy to select the most informative image pairs, whichare then labelled by expert annotators as similar or dissimilar. This way ofannotating images significantly reduces the annotation cost compared toannotating images with land-use land-cover class labels. Experimental resultson two RS benchmark datasets demonstrate the effectiveness of our method. Thecode of this work is publicly available at\url{https://git.tu-berlin.de/rsim/anneal_tgrs}.</description><author>Genc Hoxha, Gencer Sumbul, Julia Henkel, Lars Möllenbrok, Begüm Demir</author><pubDate>Mon, 05 Aug 2024 14:06:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10107v2</guid></item><item><title>Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection</title><link>http://arxiv.org/abs/2408.02484v1</link><description>Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontiertopic due to its capability to detect HOIs beyond a predefined set ofcategories. This task entails not only identifying the interactiveness ofhuman-object pairs and localizing them but also recognizing both seen andunseen interaction categories. In this paper, we introduce a novel frameworkfor zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP.This approach enhances the generalization of large foundation models, such asCLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learningmethods, we propose learning decoupled vision and language prompts forinteractiveness-aware visual feature extraction and generalizable interactionclassification, respectively. Specifically, we integrate prior knowledge ofdifferent granularity into conditional vision prompts, including aninput-conditioned instance prior and a global spatial pattern prior. The formerencourages the image encoder to treat instances belonging to seen orpotentially unseen HOI concepts equally while the latter providesrepresentative plausible spatial configuration of the human and object underinteraction. Besides, we employ language-aware prompt learning with aconsistency constraint to preserve the knowledge of the large foundation modelto enable better generalization in the text branch. Extensive experimentsdemonstrate the efficacy of our detector with conditional multi-modal prompts,outperforming previous state-of-the-art on unseen classes of various zero-shotsettings. The code and models are available at\url{https://github.com/ltttpku/CMMP}.</description><author>Ting Lei, Shaofeng Yin, Yuxin Peng, Yang Liu</author><pubDate>Mon, 05 Aug 2024 14:05:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02484v1</guid></item><item><title>On the influence of dependent features in classification problems: a game-theoretic perspective</title><link>http://arxiv.org/abs/2408.02481v1</link><description>This paper deals with a new measure of the influence of each feature on theresponse variable in classification problems, accounting for potentialdependencies among certain feature subsets. Within this framework, we considera sample of individuals characterized by specific features, each featureencompassing a finite range of values, and classified based on a binaryresponse variable. This measure turns out to be an influence measure exploredin existing literature and related to cooperative game theory. We provide anaxiomatic characterization of our proposed influence measure by tailoringproperties from the cooperative game theory to our specific context.Furthermore, we demonstrate that our influence measure becomes a generalcharacterization of the well-known Banzhaf-Owen value for games with a prioriunions, from the perspective of classification problems. The definitions andresults presented herein are illustrated through numerical examples and variousapplications, offering practical insights into our methodologies.</description><author>Laura Davila-Pena, Alejandro Saavedra-Nieves, Balbina Casas-Méndez</author><pubDate>Mon, 05 Aug 2024 14:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02481v1</guid></item><item><title>Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples</title><link>http://arxiv.org/abs/2403.02875v2</link><description>Current multimodal models leveraging contrastive learning often facelimitations in developing fine-grained conceptual understanding. This is due torandom negative samples during pretraining, causing almost exclusively verydissimilar concepts to be compared in the loss function. Consequently, themodels struggle with fine-grained semantic differences. To address thisproblem, we introduce a novel pretraining method incorporating synthetic hardnegative text examples. The hard negatives permute terms corresponding tovisual concepts, leading to a more fine-grained visual and textual conceptalignment. Further, we introduce InpaintCOCO, a new challenging dataset forassessing the fine-grained alignment of colors, objects, and sizes invision-language models. We created the dataset using generative inpainting fromCOCO images by changing the visual concepts so that the images no longer matchtheir original captions. Our results show significant improvements infine-grained concept understanding across a wide range of vision-languagedatasets, including our InpaintCOCO dataset.</description><author>Philipp J. Rösch, Norbert Oswald, Michaela Geierhos, Jindřich Libovický</author><pubDate>Mon, 05 Aug 2024 14:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02875v2</guid></item><item><title>From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future</title><link>http://arxiv.org/abs/2408.02479v1</link><description>With the rise of large language models (LLMs), researchers are increasinglyexploring their applications in var ious vertical domains, such as softwareengineering. LLMs have achieved remarkable success in areas including codegeneration and vulnerability detection. However, they also exhibit numerouslimitations and shortcomings. LLM-based agents, a novel tech nology with thepotential for Artificial General Intelligence (AGI), combine LLMs as the corefor decision-making and action-taking, addressing some of the inherentlimitations of LLMs such as lack of autonomy and self-improvement. Despitenumerous studies and surveys exploring the possibility of using LLMs insoftware engineering, it lacks a clear distinction between LLMs and LLM basedagents. It is still in its early stage for a unified standard and benchmarkingto qualify an LLM solution as an LLM-based agent in its domain. In this survey,we broadly investigate the current practice and solutions for LLMs andLLM-based agents for software engineering. In particular we summarise six keytopics: requirement engineering, code generation, autonomous decision-making,software design, test generation, and software maintenance. We review anddifferentiate the work of LLMs and LLM-based agents from these six topics,examining their differences and similarities in tasks, benchmarks, andevaluation metrics. Finally, we discuss the models and benchmarks used,providing a comprehensive analysis of their applications and effectiveness insoftware engineering. We anticipate this work will shed some lights on pushingthe boundaries of LLM-based agents in software engineering for future research.</description><author>Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen</author><pubDate>Mon, 05 Aug 2024 14:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02479v1</guid></item><item><title>UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with Ultrasound Reflection Direction Parameterization</title><link>http://arxiv.org/abs/2408.00860v2</link><description>Three-dimensional ultrasound imaging is a critical technology widely used inmedical diagnostics. However, traditional 3D ultrasound imaging methods havelimitations such as fixed resolution, low storage efficiency, and insufficientcontextual connectivity, leading to poor performance in handling complexartifacts and reflection characteristics. Recently, techniques based on NeRF(Neural Radiance Fields) have made significant progress in view synthesis and3D reconstruction, but there remains a research gap in high-quality ultrasoundimaging. To address these issues, we propose a new model, UlRe-NeRF, whichcombines implicit neural networks and explicit ultrasound volume rendering intoan ultrasound neural rendering architecture. This model incorporates reflectiondirection parameterization and harmonic encoding, using a directional MLPmodule to generate view-dependent high-frequency reflection intensityestimates, and a spatial MLP module to produce the medium's physical propertyparameters. These parameters are used in the volume rendering process toaccurately reproduce the propagation and reflection behavior of ultrasoundwaves in the medium. Experimental results demonstrate that the UlRe-NeRF modelsignificantly enhances the realism and accuracy of high-fidelity ultrasoundimage reconstruction, especially in handling complex medium structures.</description><author>Ziwen Guo, Zi Fang, Zhuang Fu</author><pubDate>Mon, 05 Aug 2024 14:00:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00860v2</guid></item><item><title>Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture and Automated Deployment Flow</title><link>http://arxiv.org/abs/2408.02473v1</link><description>One of the challenges for Tiny Machine Learning (tinyML) is keeping up withthe evolution of Machine Learning models from Convolutional Neural Networks toTransformers. We address this by leveraging a heterogeneous architecturaltemplate coupling RISC-V processors with hardwired accelerators supported by anautomated deployment flow. We demonstrate an Attention-based model in a tinyMLpower envelope with an octa-core cluster coupled with an accelerator forquantized Attention. Our deployment flow enables an end-to-end 8-bitMobileBERT, achieving leading-edge energy efficiency and throughput of 2960GOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOItechnology).</description><author>Philip Wiese, Gamze İslamoğlu, Moritz Scherer, Luka Macan, Victor J. B. Jung, Alessio Burrello, Francesco Conti, Luca Benini</author><pubDate>Mon, 05 Aug 2024 13:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02473v1</guid></item></channel></rss>