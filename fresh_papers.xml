<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 21 Aug 2024 01:00:14 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>$p$SVM: Soft-margin SVMs with $p$-norm Hinge Loss</title><link>http://arxiv.org/abs/2408.09908v2</link><description>Support Vector Machines (SVMs) based on hinge loss have been extensivelydiscussed and applied to various binary classification tasks. These SVMsachieve a balance between margin maximization and the minimization of slack dueto outliers. Although many efforts have been dedicated to enhancing theperformance of SVMs with hinge loss, studies on $p$SVMs, soft-margin SVMs with$p$-norm hinge loss, remain relatively scarce. In this paper, we explore theproperties, performance, and training algorithms of $p$SVMs. We first derivethe generalization bound of $p$SVMs, then formulate the dual optimizationproblem, comparing it with the traditional approach. Furthermore, we discuss ageneralized version of the Sequential Minimal Optimization (SMO) algorithm,$p$SMO, to train our $p$SVM model. Comparative experiments on various datasets,including binary and multi-class classification tasks, demonstrate theeffectiveness and advantages of our $p$SVM model and the $p$SMO method. Code isavailable at https://github.com/CoderBak/pSVM.</description><author>Haoxiang Sun</author><pubDate>Tue, 20 Aug 2024 12:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09908v2</guid></item><item><title>Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts</title><link>http://arxiv.org/abs/2406.10868v3</link><description>Large Language Models (LLMs) possess vast amounts of knowledge within theirparameters, prompting research into methods for locating and editing thisknowledge. Previous work has largely focused on locating entity-related (oftensingle-token) facts in smaller models. However, several key questions remainunanswered: (1) How can we effectively locate query-relevant neurons incontemporary autoregressive LLMs, such as Llama and Mistral? (2) How can weaddress the challenge of long-form text generation? (3) Are there localizedknowledge regions in LLMs? In this study, we introduce Query-Relevant NeuronCluster Attribution (QRNCA), a novel architecture-agnostic framework capable ofidentifying query-relevant neurons in LLMs. QRNCA allows for the examination oflong-form answers beyond triplet facts by employing the proxy task ofmulti-choice question answering. To evaluate the effectiveness of our detectedneurons, we build two multi-choice QA datasets spanning diverse domains andlanguages. Empirical evaluations demonstrate that our method outperformsbaseline methods significantly. Further, analysis of neuron distributionsreveals the presence of visible localized regions, particularly withindifferent domains. Finally, we show potential applications of our detectedneurons in knowledge editing and neuron-based prediction.</description><author>Lihu Chen, Adam Dejl, Francesca Toni</author><pubDate>Tue, 20 Aug 2024 09:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10868v3</guid></item><item><title>"Image, Tell me your story!" Predicting the original meta-context of visual misinformation</title><link>http://arxiv.org/abs/2408.09939v2</link><description>To assist human fact-checkers, researchers have developed automatedapproaches for visual misinformation detection. These methods assign veracityscores by identifying inconsistencies between the image and its caption, or bydetecting forgeries in the image. However, they neglect a crucial point of thehuman fact-checking process: identifying the original meta-context of theimage. By explaining what is actually true about the image, fact-checkers canbetter detect misinformation, focus their efforts on check-worthy visualcontent, engage in counter-messaging before misinformation spreads widely, andmake their explanation more convincing. Here, we fill this gap by introducingthe task of automated image contextualization. We create 5Pils, a dataset of1,676 fact-checked images with question-answer pairs about their originalmeta-context. Annotations are based on the 5 Pillars fact-checking framework.We implement a first baseline that grounds the image in its originalmeta-context using the content of the image and textual evidence retrieved fromthe open web. Our experiments show promising results while highlighting severalopen challenges in retrieval and reasoning. We make our code and data publiclyavailable.</description><author>Jonathan Tonglet, Marie-Francine Moens, Iryna Gurevych</author><pubDate>Tue, 20 Aug 2024 08:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09939v2</guid></item><item><title>PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities</title><link>http://arxiv.org/abs/2408.10111v2</link><description>Financial time series modeling is crucial for understanding and predictingmarket behaviors but faces challenges such as non-linearity, non-stationarity,and high noise levels. Traditional models struggle to capture complex patternsdue to these issues, compounded by limitations in computational resources andmodel capacity. Inspired by the success of large language models in NLP, weintroduce $\textbf{PLUTUS}$, a $\textbf{P}$re-trained $\textbf{L}$arge$\textbf{U}$nified $\textbf{T}$ransformer-based model that $\textbf{U}$nveilsregularities in financial time $\textbf{S}$eries. PLUTUS uses an invertibleembedding module with contrastive learning and autoencoder techniques to createan approximate one-to-one mapping between raw data and patch embeddings.TimeFormer, an attention based architecture, forms the core of PLUTUS,effectively modeling high-noise time series. We incorporate a novel attentionmechanisms to capture features across both variable and temporal dimensions.PLUTUS is pre-trained on an unprecedented dataset of 100 billion observations,designed to thrive in noisy financial environments. To our knowledge, PLUTUS isthe first open-source, large-scale, pre-trained financial time series modelwith over one billion parameters. It achieves state-of-the-art performance invarious tasks, demonstrating strong transferability and establishing a robustfoundational model for finance. Our research provides technical guidance forpre-training financial time series data, setting a new standard in the field.</description><author>Yuanjian Xu, Anxian Liu, Jianing Hao, Zhenzhuo Li, Shichang Meng, Guang Zhang</author><pubDate>Tue, 20 Aug 2024 02:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10111v2</guid></item><item><title>LoopSplat: Loop Closure by Registering 3D Gaussian Splats</title><link>http://arxiv.org/abs/2408.10154v2</link><description>Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats(3DGS) has recently shown promise towards more accurate, dense 3D scene maps.However, existing 3DGS-based methods fail to address the global consistency ofthe scene via loop closure and/or global bundle adjustment. To this end, wepropose LoopSplat, which takes RGB-D images as input and performs dense mappingwith 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closureonline and computes relative loop edge constraints between submaps directly via3DGS registration, leading to improvements in efficiency and accuracy overtraditional global-to-local point cloud registration. It uses a robust posegraph optimization formulation and rigidly aligns the submaps to achieve globalconsistency. Evaluation on the synthetic Replica and real-world TUM-RGBD,ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking,mapping, and rendering compared to existing methods for dense RGB-D SLAM. Codeis available at loopsplat.github.io.</description><author>Liyuan Zhu, Yue Li, Erik Sandstr√∂m, Shengyu Huang, Konrad Schindler, Iro Armeni</author><pubDate>Tue, 20 Aug 2024 02:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10154v2</guid></item><item><title>Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge</title><link>http://arxiv.org/abs/2408.08808v3</link><description>Large Language Models (LLMs) have revolutionized the landscape of machinelearning, yet current benchmarks often fall short in capturing the diversebehavior of these models in real-world applications. A benchmark's usefulnessis determined by its ability to clearly differentiate between models of varyingcapabilities (separability) and closely align with human preferences. Existingframeworks like Alpaca-Eval 2.0 LC\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\cite{li2024crowdsourced} are limited by their focus on general-purpose queriesand lack of diversity across domains such as law, medicine, and multilingualcontexts. In this paper, we address these limitations by introducing a noveldata pipeline that curates diverse, domain-specific evaluation sets tailoredfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manualcuration, semi-supervised learning to generate clusters, and stratifiedsampling to ensure balanced representation across a wide range of domains andlanguages. The resulting evaluation set, which includes 1573 samples across 14categories, demonstrates high separability (84\%) across ten top-ranked models,and agreement (84\%) with Chatbot Arena and (0.915) Spearman correlation. Theagreement values are 9\% better than Arena Hard and 20\% better than AlpacaEval2.0 LC, while the Spearman coefficient is 0.7 more than the next bestbenchmark, showcasing a significant improvement in the usefulness of thebenchmark. We further provide an open-source evaluation tool that enablesfine-grained analysis of model performance across user-defined categories,offering valuable insights for practitioners. This work contributes to theongoing effort to enhance the transparency, diversity, and effectiveness of LLMevaluation methodologies.</description><author>Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, Urmish Thakker</author><pubDate>Tue, 20 Aug 2024 02:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08808v3</guid></item><item><title>KAN 2.0: Kolmogorov-Arnold Networks Meet Science</title><link>http://arxiv.org/abs/2408.10205v1</link><description>A major challenge of AI + Science lies in their inherent incompatibility:today's AI is primarily based on connectionism, while science depends onsymbolism. To bridge the two worlds, we propose a framework to seamlesslysynergize Kolmogorov-Arnold Networks (KANs) and science. The frameworkhighlights KANs' usage for three aspects of scientific discovery: identifyingrelevant features, revealing modular structures, and discovering symbolicformulas. The synergy is bidirectional: science to KAN (incorporatingscientific knowledge into KANs), and KAN to science (extracting scientificinsights from KANs). We highlight major new functionalities in the pykanpackage: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KANcompiler that compiles symbolic formulas into KANs. (3) tree converter: convertKANs (or any neural networks) to tree graphs. Based on these tools, wedemonstrate KANs' capability to discover various types of physical laws,including conserved quantities, Lagrangians, symmetries, and constitutive laws.</description><author>Ziming Liu, Pingchuan Ma, Yixuan Wang, Wojciech Matusik, Max Tegmark</author><pubDate>Mon, 19 Aug 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10205v1</guid></item><item><title>Towards Quantum Federated Learning</title><link>http://arxiv.org/abs/2306.09912v4</link><description>Quantum Federated Learning (QFL) is an emerging interdisciplinary field thatmerges the principles of Quantum Computing (QC) and Federated Learning (FL),with the goal of leveraging quantum technologies to enhance privacy, security,and efficiency in the learning process. Currently, there is no comprehensivesurvey for this interdisciplinary field. This review offers a thorough,holistic examination of QFL. We aim to provide a comprehensive understanding ofthe principles, techniques, and emerging applications of QFL. We discuss thecurrent state of research in this rapidly evolving field, identify challengesand opportunities associated with integrating these technologies, and outlinefuture directions and open research questions. We propose a unique taxonomy ofQFL techniques, categorized according to their characteristics and the quantumtechniques employed. As the field of QFL continues to progress, we cananticipate further breakthroughs and applications across various industries,driving innovation and addressing challenges related to data privacy, security,and resource optimization. This review serves as a first-of-its-kindcomprehensive guide for researchers and practitioners interested inunderstanding and advancing the field of QFL.</description><author>Chao Ren, Rudai Yan, Huihui Zhu, Han Yu, Minrui Xu, Yuan Shen, Yan Xu, Ming Xiao, Zhao Yang Dong, Mikael Skoglund, Dusit Niyato, Leong Chuan Kwek</author><pubDate>Mon, 19 Aug 2024 17:58:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09912v4</guid></item><item><title>Criticality Leveraged Adversarial Training (CLAT) for Boosted Performance via Parameter Efficiency</title><link>http://arxiv.org/abs/2408.10204v1</link><description>Adversarial training enhances neural network robustness but suffers from atendency to overfit and increased generalization errors on clean data. Thiswork introduces CLAT, an innovative approach that mitigates adversarialoverfitting by introducing parameter efficiency into the adversarial trainingprocess, improving both clean accuracy and adversarial robustness. Instead oftuning the entire model, CLAT identifies and fine-tunes robustness-criticallayers - those predominantly learning non-robust features - while freezing theremaining model to enhance robustness. It employs dynamic critical layerselection to adapt to changes in layer criticality throughout the fine-tuningprocess. Empirically, CLAT can be applied on top of existing adversarialtraining methods, significantly reduces the number of trainable parameters byapproximately 95%, and achieves more than a 2% improvement in adversarialrobustness compared to baseline methods.</description><author>Bhavna Gopal, Huanrui Yang, Jingyang Zhang, Mark Horton, Yiran Chen</author><pubDate>Mon, 19 Aug 2024 17:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10204v1</guid></item><item><title>SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP</title><link>http://arxiv.org/abs/2408.10202v1</link><description>Large-scale vision-language models, such as CLIP, are known to containharmful societal bias regarding protected attributes (e.g., gender and age). Inthis paper, we aim to address the problems of societal bias in CLIP. Althoughprevious studies have proposed to debias societal bias through adversariallearning or test-time projecting, our comprehensive study of these worksidentifies two critical limitations: 1) loss of attribute information when itis explicitly disclosed in the input and 2) use of the attribute annotationsduring debiasing process. To mitigate societal bias in CLIP and overcome theselimitations simultaneously, we introduce a simple-yet-effective debiasingmethod called SANER (societal attribute neutralizer) that eliminates attributeinformation from CLIP text features only of attribute-neutral descriptions.Experimental results show that SANER, which does not require attributeannotations and preserves original information for attribute-specificdescriptions, demonstrates superior debiasing ability than the existingmethods.</description><author>Yusuke Hirota, Min-Hung Chen, Chien-Yi Wang, Yuta Nakashima, Yu-Chiang Frank Wang, Ryo Hachiuma</author><pubDate>Mon, 19 Aug 2024 17:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10202v1</guid></item><item><title>MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model</title><link>http://arxiv.org/abs/2408.10198v1</link><description>Open-world 3D reconstruction models have recently garnered significantattention. However, without sufficient 3D inductive bias, existing methodstypically entail expensive training costs and struggle to extract high-quality3D meshes. In this work, we introduce MeshFormer, a sparse-view reconstructionmodel that explicitly leverages 3D native structure, input guidance, andtraining supervision. Specifically, instead of using a triplane representation,we store features in 3D sparse voxels and combine transformers with 3Dconvolutions to leverage an explicit 3D structure and projective bias. Inaddition to sparse-view RGB input, we require the network to take input andgenerate corresponding normal maps. The input normal maps can be predicted by2D diffusion models, significantly aiding in the guidance and refinement of thegeometry's learning. Moreover, by combining Signed Distance Function (SDF)supervision with surface rendering, we directly learn to generate high-qualitymeshes without the need for complex multi-stage training processes. Byincorporating these explicit 3D biases, MeshFormer can be trained efficientlyand deliver high-quality textured meshes with fine-grained geometric details.It can also be integrated with 2D diffusion models to enable fastsingle-image-to-3D and text-to-3D tasks. Project page:https://meshformer3d.github.io</description><author>Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, Hongzhi Wu, Hao Su</author><pubDate>Mon, 19 Aug 2024 17:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10198v1</guid></item><item><title>Demystifying the Communication Characteristics for Distributed Transformer Models</title><link>http://arxiv.org/abs/2408.10197v1</link><description>Deep learning (DL) models based on the transformer architecture haverevolutionized many DL applications such as large language models (LLMs),vision transformers, audio generation, and time series prediction. Much of thisprogress has been fueled by distributed training, yet distributed communicationremains a substantial bottleneck to training progress. This paper examines thecommunication behavior of transformer models - that is, how differentparallelism schemes used in multi-node/multi-GPU DL Training communicate datain the context of transformers. We use GPT-based language models as a casestudy of the transformer architecture due to their ubiquity. We validate theempirical results obtained from our communication logs using analytical models.At a high level, our analysis reveals a need to optimize small messagepoint-to-point communication further, correlations between sequence length,per-GPU throughput, model size, and optimizations used, and where topotentially guide further optimizations in framework and HPC middleware designand optimization.</description><author>Quentin Anthony, Benjamin Michalowicz, Jacob Hatef, Lang Xu, Mustafa Abduljabbar, Aamir Shafi, Hari Subramoni, Dhabaleswar Panda</author><pubDate>Mon, 19 Aug 2024 17:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10197v1</guid></item><item><title>SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views</title><link>http://arxiv.org/abs/2408.10195v1</link><description>Open-world 3D generation has recently attracted considerable attention. Whilemany single-image-to-3D methods have yielded visually appealing outcomes, theyoften lack sufficient controllability and tend to produce hallucinated regionsthat may not align with users' expectations. In this paper, we explore animportant scenario in which the input consists of one or a few unposed 2Dimages of a single object, with little or no overlap. We propose a novelmethod, SpaRP, to reconstruct a 3D textured mesh and estimate the relativecamera poses for these sparse-view images. SpaRP distills knowledge from 2Ddiffusion models and finetunes them to implicitly deduce the 3D spatialrelationships between the sparse views. The diffusion model is trained tojointly predict surrogate representations for camera poses and multi-viewimages of the object under known poses, integrating all information from theinput sparse views. These predictions are then leveraged to accomplish 3Dreconstruction and pose estimation, and the reconstructed 3D model can be usedto further refine the camera poses of input views. Through extensiveexperiments on three datasets, we demonstrate that our method not onlysignificantly outperforms baseline methods in terms of 3D reconstructionquality and pose prediction accuracy but also exhibits strong efficiency. Itrequires only about 20 seconds to produce a textured mesh and camera poses forthe input views. Project page: https://chaoxu.xyz/sparp.</description><author>Chao Xu, Ang Li, Linghao Chen, Yulin Liu, Ruoxi Shi, Hao Su, Minghua Liu</author><pubDate>Mon, 19 Aug 2024 17:53:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10195v1</guid></item><item><title>Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification</title><link>http://arxiv.org/abs/2408.10193v1</link><description>Evaluation Metrics is an important question for model evaluation and modelselection in binary classification tasks. This study investigates howconsistent metrics are at evaluating different models under different datascenarios. Analyzing over 150 data scenarios and 18 model evaluation metricsusing statistical simulation, I find that for binary classification tasks,evaluation metrics that are less influenced by prevalence offer more consistentranking of a set of different models. In particular, Area Under the ROC Curve(AUC) has smallest variance in ranking of different models. Matthew'scorrelation coefficient as a more strict measure of model performance has thesecond smallest variance. These patterns holds across a rich set of datascenarios and five commonly used machine learning models as well as a naiverandom guess model. The results have significant implications for modelevaluation and model selection in binary classification tasks.</description><author>Jing Li</author><pubDate>Mon, 19 Aug 2024 17:52:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10193v1</guid></item><item><title>Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models</title><link>http://arxiv.org/abs/2408.10189v1</link><description>Transformer architectures have become a dominant paradigm for domains likelanguage modeling but suffer in many inference settings due to theirquadratic-time self-attention. Recently proposed subquadratic architectures,such as Mamba, have shown promise, but have been pretrained with substantiallyless computational resources than the strongest Transformer models. In thiswork, we present a method that is able to distill a pretrained Transformerarchitecture into alternative architectures such as state space models (SSMs).The key idea to our approach is that we can view both Transformers and SSMs asapplying different forms of mixing matrices over the token sequences. We canthus progressively distill the Transformer architecture by matching differentdegrees of granularity in the SSM: first matching the mixing matricesthemselves, then the hidden units at each block, and finally the end-to-endpredictions. Our method, called MOHAWK, is able to distill a Mamba-2 variantbased on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybridversion (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of thetraining data typically used to train models from scratch, Phi-Mamba boastssubstantially stronger performance compared to all past open-sourcenon-Transformer models. MOHAWK allows models like SSMs to leveragecomputational resources invested in training Transformer-based architectures,highlighting a new avenue for building such models.</description><author>Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter, Albert Gu</author><pubDate>Mon, 19 Aug 2024 17:48:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10189v1</guid></item><item><title>LongVILA: Scaling Long-Context Visual Language Models for Long Videos</title><link>http://arxiv.org/abs/2408.10188v1</link><description>Long-context capability is critical for multi-modal foundation models. Weintroduce LongVILA, a full-stack solution for long-context vision-languagemodels, including system, model training, and dataset development. On thesystem side, we introduce the first Multi-Modal Sequence Parallelism (MM-SP)system that enables long-context training and inference, enabling 2M contextlength training on 256 GPUs. MM-SP is also efficient, being 2.1x - 5.7x fasterthan Ring-Style Sequence Parallelism and 1.1x - 1.4x faster than Megatron-LM intext-only settings. Moreover, it seamlessly integrates with Hugging FaceTransformers. For model training, we propose a five-stage pipeline comprisingalignment, pre-training, context extension, and long-short joint supervisedfine-tuning. Regarding datasets, we meticulously construct large-scale visuallanguage pre-training datasets and long video instruction-following datasets tosupport our multi-stage training process. The full-stack solution extends thefeasible frame number of VILA by a factor of 128 (from 8 to 1024 frames) andimproves long video captioning score from 2.00 to 3.26 (1.6x), achieving 99.5%accuracy in 1400-frames video (274k context length) needle in a haystack.LongVILA-8B also demonstrates a consistent improvement in performance on longvideos within the VideoMME benchmark as the video frames increase.</description><author>Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han</author><pubDate>Mon, 19 Aug 2024 17:48:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10188v1</guid></item><item><title>A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning</title><link>http://arxiv.org/abs/2311.00212v2</link><description>Symmetry is present throughout nature and continues to play an increasinglycentral role in physics and machine learning. Fundamental symmetries, such asPoincar\'{e} invariance, allow physical laws discovered in laboratories onEarth to be extrapolated to the farthest reaches of the universe. Symmetry isessential to achieving this extrapolatory power in machine learningapplications. For example, translation invariance in image classificationallows models with fewer parameters, such as convolutional neural networks, tobe trained on smaller data sets and achieve state-of-the-art performance. Inthis paper, we provide a unifying theoretical and methodological framework forincorporating symmetry into machine learning models in three ways: 1. enforcingknown symmetry when training a model; 2. discovering unknown symmetries of agiven model or data set; and 3. promoting symmetry during training by learninga model that breaks symmetries within a user-specified group of candidates whenthere is sufficient evidence in the data. We show that these tasks can be castwithin a common mathematical framework whose central object is the Liederivative associated with fiber-linear Lie group actions on vector bundles. Weextend and unify several existing results by showing that enforcing anddiscovering symmetry are linear-algebraic tasks that are dual with respect tothe bilinear structure of the Lie derivative. We also propose a novel way topromote symmetry by introducing a class of convex regularization functionsbased on the Lie derivative and nuclear norm relaxation to penalize symmetrybreaking during training of machine learning models. We explain how these ideascan be applied to a wide range of machine learning models including basisfunction regression, dynamical systems discovery, neural networks, and neuraloperators acting on fields.</description><author>Samuel E. Otto, Nicholas Zolman, J. Nathan Kutz, Steven L. Brunton</author><pubDate>Mon, 19 Aug 2024 17:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00212v2</guid></item><item><title>Assessment of Spectral based Solutions for the Detection of Floating Marine Debris</title><link>http://arxiv.org/abs/2408.10187v1</link><description>Typically, the detection of marine debris relies on in-situ campaigns thatare characterized by huge human effort and limited spatial coverage. Followingthe need of a rapid solution for the detection of floating plastic, methodsbased on remote sensing data have been proposed recently. Their main limitationis represented by the lack of a general reference for evaluating performance.Recently, the Marine Debris Archive (MARIDA) has been released as a standarddataset to develop and evaluate Machine Learning (ML) algorithms for detectionof Marine Plastic Debris. The MARIDA dataset has been created for simplifyingthe comparison between detection solutions with the aim of stimulating theresearch in the field of marine environment preservation. In this work, anassessment of spectral based solutions is proposed by evaluating performance onMARIDA dataset. The outcome highlights the need of precise reference for fairevaluation.</description><author>Muhammad Al√¨, Francesca Razzano, Sergio Vitale, Giampaolo Ferraioli, Vito Pascazio, Gilda Schirinzi, Silvia Ullo</author><pubDate>Mon, 19 Aug 2024 17:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10187v1</guid></item><item><title>Imbalance-Aware Culvert-Sewer Defect Segmentation Using an Enhanced Feature Pyramid Network</title><link>http://arxiv.org/abs/2408.10181v1</link><description>Imbalanced datasets are a significant challenge in real-world scenarios. Theylead to models that underperform on underrepresented classes, which is acritical issue in infrastructure inspection. This paper introduces the EnhancedFeature Pyramid Network (E-FPN), a deep learning model for the semanticsegmentation of culverts and sewer pipes within imbalanced datasets. The E-FPNincorporates architectural innovations like sparsely connected blocks anddepth-wise separable convolutions to improve feature extraction and handleobject variations. To address dataset imbalance, the model employs strategieslike class decomposition and data augmentation. Experimental results on theculvert-sewer defects dataset and a benchmark aerial semantic segmentationdrone dataset show that the E-FPN outperforms state-of-the-art methods,achieving an average Intersection over Union (IoU) improvement of 13.8% and27.2%, respectively. Additionally, class decomposition and data augmentationtogether boost the model's performance by approximately 6.9% IoU. The proposedE-FPN presents a promising solution for enhancing object segmentation inchallenging, multi-class real-world datasets, with potential applicationsextending beyond culvert-sewer defect detection.</description><author>Rasha Alshawi, Md Meftahul Ferdaus, Mahdi Abdelguerfi, Kendall Niles, Ken Pathak, Steve Sloan</author><pubDate>Mon, 19 Aug 2024 17:40:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10181v1</guid></item><item><title>NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction</title><link>http://arxiv.org/abs/2408.10178v1</link><description>Signed Distance Function (SDF)-based volume rendering has demonstratedsignificant capabilities in surface reconstruction. Although promising,SDF-based methods often fail to capture detailed geometric structures,resulting in visible defects. By comparing SDF-based volume rendering todensity-based volume rendering, we identify two main factors within theSDF-based approach that degrade surface quality: SDF-to-density representationand geometric regularization. These factors introduce challenges that hinderthe optimization of the SDF field. To address these issues, we introduceNeuRodin, a novel two-stage neural surface reconstruction framework that notonly achieves high-fidelity surface reconstruction but also retains theflexible optimization characteristics of density-based methods. NeuRodinincorporates innovative strategies that facilitate transformation of arbitrarytopologies and reduce artifacts associated with density bias. Extensiveevaluations on the Tanks and Temples and ScanNet++ datasets demonstrate thesuperiority of NeuRodin, showing strong reconstruction capabilities for bothindoor and outdoor environments using solely posed RGB captures. Projectwebsite: https://open3dvlab.github.io/NeuRodin/</description><author>Yifan Wang, Di Huang, Weicai Ye, Guofeng Zhang, Wanli Ouyang, Tong He</author><pubDate>Mon, 19 Aug 2024 17:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10178v1</guid></item><item><title>Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic Bias in Facial Recognition</title><link>http://arxiv.org/abs/2408.10175v1</link><description>This study investigates the effects of occlusions on the fairness of facerecognition systems, particularly focusing on demographic biases. Using theRacial Faces in the Wild (RFW) dataset and synthetically added realisticocclusions, we evaluate their effect on the performance of face recognitionmodels trained on the BUPT-Balanced and BUPT-GlobalFace datasets. We noteincreases in the dispersion of FMR, FNMR, and accuracy alongside decreases infairness according to Equilized Odds, Demographic Parity, STD of Accuracy, andFairness Discrepancy Rate. Additionally, we utilize a pixel attribution methodto understand the importance of occlusions in model predictions, proposing anew metric, Face Occlusion Impact Ratio (FOIR), that quantifies the extent towhich occlusions affect model performance across different demographic groups.Our results indicate that occlusions exacerbate existing demographic biases,with models placing higher importance on occlusions in an unequal fashion,particularly affecting African individuals more severely.</description><author>Rafael M. Mamede, Pedro C. Neto, Ana F. Sequeira</author><pubDate>Mon, 19 Aug 2024 17:34:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10175v1</guid></item><item><title>SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models</title><link>http://arxiv.org/abs/2408.10174v1</link><description>Deep model training on extensive datasets is increasingly becomingcost-prohibitive, prompting the widespread adoption of deep model fusiontechniques to leverage knowledge from pre-existing models. From simple weightaveraging to more sophisticated methods like AdaMerging, model fusioneffectively improves model performance and accelerates the development of newmodels. However, potential interference between parameters of individual modelsand the lack of interpretability in the fusion progress remain significantchallenges. Existing methods often try to resolve the parameter interferenceissue by evaluating attributes of parameters, such as their magnitude or sign,or by parameter pruning. In this study, we begin by examining the fine-tuningof linear layers through the lens of subspace analysis and explicitly defineparameter interference as an optimization problem to shed light on thissubject. Subsequently, we introduce an innovative approach to model fusioncalled zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, whichallows for the upscaling of source models into an MoE model without extra dataor further training. Our approach relies on the observation that fine-tuningmostly keeps the important parts from the pre-training, but it uses lesssignificant or unused areas to adapt to new tasks. Also, the issue of parameterinterference, which is intrinsically intractable in the original parameterspace, can be managed by expanding the dimensions. We conduct extensiveexperiments across diverse scenarios, such as image classification and textgeneralization tasks, using full fine-tuning and LoRA fine-tuning, and we applyour method to large language models (CLIP models, Flan-T5 models, andMistral-7B models), highlighting the adaptability and scalability of SMILE.Code is available at https://github.com/tanganke/fusion_bench</description><author>Anke Tang, Li Shen, Yong Luo, Shuai Xie, Han Hu, Lefei Zhang, Bo Du, Dacheng Tao</author><pubDate>Mon, 19 Aug 2024 17:32:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10174v1</guid></item><item><title>Classical Machine Learning: Seventy Years of Algorithmic Learning Evolution</title><link>http://arxiv.org/abs/2408.01747v2</link><description>Machine learning (ML) has transformed numerous fields, but understanding itsfoundational research is crucial for its continued progress. This paperpresents an overview of the significant classical ML algorithms and examinesthe state-of-the-art publications spanning twelve decades through an extensivebibliometric analysis study. We analyzed a dataset of highly cited papers fromprominent ML conferences and journals, employing citation and keyword analysesto uncover critical insights. The study further identifies the most influentialpapers and authors, reveals the evolving collaborative networks within the MLcommunity, and pinpoints prevailing research themes and emerging focus areas.Additionally, we examine the geographic distribution of highly citedpublications, highlighting the leading countries in ML research. This studyprovides a comprehensive overview of the evolution of traditional learningalgorithms and their impacts. It discusses challenges and opportunities forfuture development, focusing on the Global South. The findings from this paperoffer valuable insights for both ML experts and the broader research community,enhancing understanding of the field's trajectory and its significant influenceon recent advances in learning algorithms.</description><author>Absalom E. Ezugwu, Yuh-Shan Ho, Ojonukpe S. Egwuche, Olufisayo S. Ekundayo, Annette Van Der Merwe, Apu K. Saha, Jayanta Pal</author><pubDate>Mon, 19 Aug 2024 17:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01747v2</guid></item><item><title>Mechanistic Design and Scaling of Hybrid Architectures</title><link>http://arxiv.org/abs/2403.17844v2</link><description>The development of deep learning architectures is a resource-demandingprocess, due to a vast design space, long prototyping times, and high computecosts associated with at-scale model training and evaluation. We set out tosimplify this process by grounding it in an end-to-end mechanistic architecturedesign (MAD) pipeline, encompassing small-scale capability unit testspredictive of scaling laws. Through a suite of synthetic token manipulationtasks such as compression and recall, designed to probe capabilities, weidentify and test new hybrid architectures constructed from a variety ofcomputational primitives. We experimentally validate the resultingarchitectures via an extensive compute-optimal and a new state-optimal scalinglaw analysis, training over 500 language models between 70M to 7B parameters.Surprisingly, we find MAD synthetics to correlate with compute-optimalperplexity, enabling accurate evaluation of new architectures via isolatedproxy tasks. The new architectures found via MAD, based on simple ideas such ashybridization and sparsity, outperform state-of-the-art Transformer,convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) inscaling, both at compute-optimal budgets and in overtrained regimes. Overall,these results provide evidence that performance on curated synthetic tasks canbe predictive of scaling laws, and that an optimal architecture should leveragespecialized layers via a hybrid topology.</description><author>Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj√∂rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R√©, Ce Zhang, Stefano Massaroli</author><pubDate>Mon, 19 Aug 2024 17:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17844v2</guid></item><item><title>HGRN2: Gated Linear RNNs with State Expansion</title><link>http://arxiv.org/abs/2404.07904v2</link><description>Hierarchically gated linear RNN (HGRN, \citealt{HGRN}) has demonstratedcompetitive training speed and performance in language modeling while offeringefficient inference. However, the recurrent state size of HGRN remainsrelatively small, limiting its expressiveness. To address this issue, weintroduce a simple outer product-based state expansion mechanism, whichsignificantly enlarges the recurrent state size without introducing anyadditional parameters. This enhancement also provides a linear attentioninterpretation for HGRN2, enabling hardware-efficient training. Our extensiveexperiments verify the advantage of HGRN2 over HGRN consistently acrossdifferent settings and competitive with other recurrent models.</description><author>Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong</author><pubDate>Mon, 19 Aug 2024 17:16:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07904v2</guid></item><item><title>Physics-Aware Combinatorial Assembly Planning using Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2408.10162v1</link><description>Combinatorial assembly uses standardized unit primitives to build objectsthat satisfy user specifications. Lego is a widely used platform forcombinatorial assembly, in which people use unit primitives (ie Lego bricks) tobuild highly customizable 3D objects. This paper studies sequence planning forphysical combinatorial assembly using Lego. Given the shape of the desiredobject, we want to find a sequence of actions for placing Lego bricks to buildthe target object. In particular, we aim to ensure the planned assemblysequence is physically executable. However, assembly sequence planning (ASP)for combinatorial assembly is particularly challenging due to its combinatorialnature, ie the vast number of possible combinations and complex constraints. Toaddress the challenges, we employ deep reinforcement learning to learn aconstruction policy for placing unit primitives sequentially to build thedesired object. Specifically, we design an online physics-aware action maskthat efficiently filters out invalid actions and guides policy learning. In theend, we demonstrate that the proposed method successfully plans physicallyvalid assembly sequences for constructing different Lego structures. Thegenerated construction plan can be executed in real.</description><author>Ruixuan Liu, Alan Chen, Weiye Zhao, Changliu Liu</author><pubDate>Mon, 19 Aug 2024 17:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10162v1</guid></item><item><title>Topic-Based Watermarks for LLM-Generated Text</title><link>http://arxiv.org/abs/2404.02138v3</link><description>The indistinguishability of text generated by large language models (LLMs)from human-generated text poses significant challenges. Watermarking algorithmsare potential solutions by embedding detectable signatures within LLM-generatedoutputs. However, current watermarking schemes lack robustness to a range ofattacks such as text substitution or manipulation, undermining theirreliability. This paper proposes a novel topic-based watermarking algorithm forLLMs, designed to enhance the robustness of watermarking in LLMs. Our approachleverages the topics extracted from input prompts or outputs of non-watermarkedLLMs in the generation process of watermarked text. We dynamically utilizetoken lists on identified topics and adjust token sampling weights accordingly.By using these topic-specific token biases, we embed a topic-sensitivewatermarking into the generated text. We outline the theoretical framework ofour topic-based watermarking algorithm and discuss its potential advantages invarious scenarios. Additionally, we explore a comprehensive range of attacksagainst watermarking algorithms, including discrete alterations, paraphrasing,and tokenizations. We demonstrate that our proposed watermarking schemeclassifies various watermarked text topics with 99.99% confidence andoutperforms existing algorithms in terms of z-score robustness and thefeasibility of modeling text degradation by potential attackers, whileconsidering the trade-offs between the benefits and losses of watermarkingLLM-generated text.</description><author>Alexander Nemecek, Yuzhou Jiang, Erman Ayday</author><pubDate>Mon, 19 Aug 2024 17:16:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02138v3</guid></item><item><title>Structure Learning with Continuous Optimization: A Sober Look and Beyond</title><link>http://arxiv.org/abs/2304.02146v2</link><description>This paper investigates in which cases continuous optimization for directedacyclic graph (DAG) structure learning can and cannot perform well and why thishappens, and suggests possible directions to make the search procedure morereliable. Reisach et al. (2021) suggested that the remarkable performance ofseveral continuous structure learning approaches is primarily driven by a highagreement between the order of increasing marginal variances and thetopological order, and demonstrated that these approaches do not perform wellafter data standardization. We analyze this phenomenon for continuousapproaches assuming equal and non-equal noise variances, and show that thestatement may not hold in either case by providing counterexamples,justifications, and possible alternative explanations. We further demonstratethat nonconvexity may be a main concern especially for the non-equal noisevariances formulation, while recent advances in continuous structure learningfail to achieve improvement in this case. Our findings suggest that futureworks should take into account the non-equal noise variances formulation tohandle more general settings and for a more comprehensive empirical evaluation.Lastly, we provide insights into other aspects of the search procedure,including thresholding and sparsity, and show that they play an important rolein the final solutions.</description><author>Ignavier Ng, Biwei Huang, Kun Zhang</author><pubDate>Mon, 19 Aug 2024 17:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02146v2</guid></item><item><title>NeuFlow v2: High-Efficiency Optical Flow Estimation on Edge Devices</title><link>http://arxiv.org/abs/2408.10161v1</link><description>Real-time high-accuracy optical flow estimation is crucial for variousreal-world applications. While recent learning-based optical flow methods haveachieved high accuracy, they often come with significant computational costs.In this paper, we propose a highly efficient optical flow method that balanceshigh accuracy with reduced computational demands. Building upon NeuFlow v1, weintroduce new components including a much more light-weight backbone and a fastrefinement module. Both these modules help in keeping the computational demandslight while providing close to state of the art accuracy. Compares to otherstate of the art methods, our model achieves a 10x-70x speedup whilemaintaining comparable performance on both synthetic and real-world data. It iscapable of running at over 20 FPS on 512x384 resolution images on a Jetson OrinNano. The full training and evaluation code is available athttps://github.com/neufieldrobotics/NeuFlow_v2.</description><author>Zhiyong Zhang, Aniket Gupta, Huaizu Jiang, Hanumant Singh</author><pubDate>Mon, 19 Aug 2024 17:13:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10161v1</guid></item><item><title>Customizing Language Models with Instance-wise LoRA for Sequential Recommendation</title><link>http://arxiv.org/abs/2408.10159v1</link><description>Sequential recommendation systems predict a user's next item of interest byanalyzing past interactions, aligning recommendations with individualpreferences. Leveraging the strengths of Large Language Models (LLMs) inknowledge comprehension and reasoning, recent approaches have applied LLMs tosequential recommendation through language generation paradigms. These methodsconvert user behavior sequences into prompts for LLM fine-tuning, utilizingLow-Rank Adaptation (LoRA) modules to refine recommendations. However, theuniform application of LoRA across diverse user behaviors sometimes fails tocapture individual variability, leading to suboptimal performance and negativetransfer between disparate sequences. To address these challenges, we proposeInstance-wise LoRA (iLoRA), integrating LoRA with the Mixture of Experts (MoE)framework. iLoRA creates a diverse array of experts, each capturing specificaspects of user preferences, and introduces a sequence representation guidedgate function. This gate function processes historical interaction sequences togenerate enriched representations, guiding the gating network to outputcustomized expert participation weights. This tailored approach mitigatesnegative transfer and dynamically adjusts to diverse behavior patterns.Extensive experiments on three benchmark datasets demonstrate the effectivenessof iLoRA, highlighting its superior performance compared to existing methods incapturing user-specific preferences and improving recommendation accuracy.</description><author>Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, Xiangnan He</author><pubDate>Mon, 19 Aug 2024 17:09:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10159v1</guid></item><item><title>LoopSplat: Loop Closure by Registering 3D Gaussian Splats</title><link>http://arxiv.org/abs/2408.10154v1</link><description>Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats(3DGS) has recently shown promise towards more accurate, dense 3D scene maps.However, existing 3DGS-based methods fail to address the global consistency ofthe scene via loop closure and/or global bundle adjustment. To this end, wepropose LoopSplat, which takes RGB-D images as input and performs dense mappingwith 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closureonline and computes relative loop edge constraints between submaps directly via3DGS registration, leading to improvements in efficiency and accuracy overtraditional global-to-local point cloud registration. It uses a robust posegraph optimization formulation and rigidly aligns the submaps to achieve globalconsistency. Evaluation on the synthetic Replica and real-world TUM-RGBD,ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking,mapping, and rendering compared to existing methods for dense RGB-D SLAM. Codeis available at \href{https://loopsplat.github.io/}{loopsplat.github.io}.</description><author>Liyuan Zhu, Yue Li, Erik Sandstr√∂m, Konrad Schindler, Iro Armeni</author><pubDate>Mon, 19 Aug 2024 17:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10154v1</guid></item><item><title>Structure-preserving Image Translation for Depth Estimation in Colonoscopy Video</title><link>http://arxiv.org/abs/2408.10153v1</link><description>Monocular depth estimation in colonoscopy video aims to overcome the unusuallighting properties of the colonoscopic environment. One of the majorchallenges in this area is the domain gap between annotated but unrealisticsynthetic data and unannotated but realistic clinical data. Previous attemptsto bridge this domain gap directly target the depth estimation task itself. Wepropose a general pipeline of structure-preserving synthetic-to-real (sim2real)image translation (producing a modified version of the input image) to retaindepth geometry through the translation process. This allows us to generatelarge quantities of realistic-looking synthetic images for supervised depthestimation with improved generalization to the clinical domain. We also proposea dataset of hand-picked sequences from clinical colonoscopies to improve theimage translation process. We demonstrate the simultaneous realism of thetranslated images and preservation of depth maps via the performance ofdownstream depth estimation on various datasets.</description><author>Shuxian Wang, Akshay Paruchuri, Zhaoxi Zhang, Sarah McGill, Roni Sengupta</author><pubDate>Mon, 19 Aug 2024 17:02:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10153v1</guid></item><item><title>Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models</title><link>http://arxiv.org/abs/2408.10151v1</link><description>While recent large language models (LLMs) demonstrate remarkable abilities inresponding to queries in diverse languages, their ability to handle longmultilingual contexts is unexplored. As such, a systematic evaluation of thelong-context capabilities of LLMs in multilingual settings is crucial,specifically in the context of information retrieval. To address this gap, weintroduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed toassess a model's ability to retrieve relevant information (the needle) from acollection of multilingual distractor texts (the haystack). This test serves asan extension of the multilingual question-answering task, encompassing bothmonolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMson MLNeedle. Our findings reveal that model performance can vary significantlywith language and needle position. Specifically, we observe that modelperformance is the lowest when the needle is (i) in a language outside theEnglish language family and (ii) located in the middle of the input context.Furthermore, although some models claim a context size of $8k$ tokens orgreater, none demonstrate satisfactory cross-lingual retrieval performance asthe context length increases. Our analysis provides key insights into thelong-context behavior of LLMs in multilingual settings to guide futureevaluation protocols. To our knowledge, this is the first study to investigatethe multilingual long-context behavior of LLMs.</description><author>Amey Hengle, Prasoon Bajpai, Soham Dan, Tanmoy Chakraborty</author><pubDate>Mon, 19 Aug 2024 17:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10151v1</guid></item><item><title>CoMusion: Towards Consistent Stochastic Human Motion Prediction via Motion Diffusion</title><link>http://arxiv.org/abs/2305.12554v3</link><description>Stochastic Human Motion Prediction (HMP) aims to predict multiple possiblefuture human pose sequences from observed ones. Most prior works learn motiondistributions through encoding-decoding in the latent space, which does notpreserve motion's spatial-temporal structure. While effective, these methodsoften require complex, multi-stage training and yield predictions that areinconsistent with the provided history and can be physically unrealistic. Toaddress these issues, we propose CoMusion, a single-stage, end-to-enddiffusion-based stochastic HMP framework. CoMusion is inspired from the insightthat a smooth future pose initialization improves prediction performance, astrategy not previously utilized in stochastic models but evidenced indeterministic works. To generate such initialization, CoMusion's motionpredictor starts with a Transformer-based network for initial reconstruction ofcorrupted motion. Then, a graph convolutional network (GCN) is employed torefine the prediction considering past observations in the discrete cosinetransformation (DCT) space. Our method, facilitated by the Transformer-GCNmodule design and a proposed variance scheduler, excels in predicting accurate,realistic, and consistent motions, while maintaining appropriate diversity.Experimental results on benchmark datasets demonstrate that CoMusion surpassesprior methods across metrics, while demonstrating superior generation quality.Our Code is released at https://github.com/jsun57/CoMusion/ .</description><author>Jiarui Sun, Girish Chowdhary</author><pubDate>Mon, 19 Aug 2024 16:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12554v3</guid></item><item><title>In-Context Learning with Representations: Contextual Generalization of Trained Transformers</title><link>http://arxiv.org/abs/2408.10147v1</link><description>In-context learning (ICL) refers to a remarkable capability of pretrainedlarge language models, which can learn a new task given a few examples duringinference. However, theoretical understanding of ICL is largely under-explored,particularly whether transformers can be trained to generalize to unseenexamples in a prompt, which will require the model to acquire contextualknowledge of the prompt for generalization. This paper investigates thetraining dynamics of transformers by gradient descent through the lens ofnon-linear regression tasks. The contextual generalization here can be attainedvia learning the template function for each task in-context, where all templatefunctions lie in a linear space with $m$ basis functions. We analyze thetraining dynamics of one-layer multi-head transformers to in-contextly predictunlabeled inputs given partially labeled prompts, where the labels containGaussian noise and the number of examples in each prompt are not sufficient todetermine the template. Under mild assumptions, we show that the training lossfor a one-layer multi-head transformer converges linearly to a global minimum.Moreover, the transformer effectively learns to perform ridge regression overthe basis functions. To our knowledge, this study is the first provabledemonstration that transformers can learn contextual (i.e., template)information to generalize to both unseen examples and tasks when promptscontain only a small number of query-answer pairs.</description><author>Tong Yang, Yu Huang, Yingbin Liang, Yuejie Chi</author><pubDate>Mon, 19 Aug 2024 16:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10147v1</guid></item><item><title>Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games</title><link>http://arxiv.org/abs/2402.18781v4</link><description>Asymmetric information stochastic games (AISGs) arise in many complexsocio-technical systems, such as cyber-physical systems and IT infrastructures.Existing computational methods for AISGs are primarily offline and can notadapt to equilibrium deviations. Further, current methods are limited toparticular information structures to avoid belief hierarchies. Consideringthese limitations, we propose conjectural online learning (COL), an onlinelearning method under generic information structures in AISGs. COL uses aforecaster-actor-critic (FAC) architecture, where subjective forecasts are usedto conjecture the opponents' strategies within a lookahead horizon, andBayesian learning is used to calibrate the conjectures. To adapt strategies tononstationary environments based on information feedback, COL uses onlinerollout with cost function approximation (actor-critic). We prove that theconjectures produced by COL are asymptotically consistent with the informationfeedback in the sense of a relaxed Bayesian consistency. We also prove that theempirical strategy profile induced by COL converges to the Berk-Nashequilibrium, a solution concept characterizing rationality under subjectivity.Experimental results from an intrusion response use case demonstrate COL's{faster convergence} over state-of-the-art reinforcement learning methodsagainst nonstationary attacks.</description><author>Tao Li, Kim Hammar, Rolf Stadler, Quanyan Zhu</author><pubDate>Mon, 19 Aug 2024 16:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18781v4</guid></item><item><title>Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge</title><link>http://arxiv.org/abs/2408.08808v2</link><description>Large Language Models (LLMs) have revolutionized the landscape of machinelearning, yet current benchmarks often fall short in capturing the diversebehavior of these models in real-world applications. A benchmark's usefulnessis determined by its ability to clearly differentiate between models of varyingcapabilities (separability) and closely align with human preferences. Existingframeworks like Alpaca-Eval 2.0 LC\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\cite{li2024crowdsourced} are limited by their focus on general-purpose queriesand lack of diversity across domains such as law, medicine, and multilingualcontexts. In this paper, we address these limitations by introducing a noveldata pipeline that curates diverse, domain-specific evaluation sets tailoredfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manualcuration, semi-supervised learning to generate clusters, and stratifiedsampling to ensure balanced representation across a wide range of domains andlanguages. The resulting evaluation set, which includes 1573 samples across 14categories, demonstrates high separability (84\%) across ten top-ranked models,and agreement (84\%) with Chatbot Arena and (0.915) Spearman correlation. Theagreement values are 9\% better than Arena Hard and 20\% better than AlpacaEval2.0 LC, while the Spearman coefficient is 0.7 more than the next bestbenchmark, showcasing a significant improvement in the usefulness of thebenchmark. We further provide an open-source evaluation tool that enablesfine-grained analysis of model performance across user-defined categories,offering valuable insights for practitioners. This work contributes to theongoing effort to enhance the transparency, diversity, and effectiveness of LLMevaluation methodologies.</description><author>Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, Urmish Thakkar</author><pubDate>Mon, 19 Aug 2024 16:44:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08808v2</guid></item><item><title>Multi-Scale Representation Learning for Image Restoration with State-Space Model</title><link>http://arxiv.org/abs/2408.10145v1</link><description>Image restoration endeavors to reconstruct a high-quality, detail-rich imagefrom a degraded counterpart, which is a pivotal process in photography andvarious computer vision systems. In real-world scenarios, different types ofdegradation can cause the loss of image details at various scales and degradeimage contrast. Existing methods predominantly rely on CNN and Transformer tocapture multi-scale representations. However, these methods are often limitedby the high computational complexity of Transformers and the constrainedreceptive field of CNN, which hinder them from achieving superior performanceand efficiency in image restoration. To address these challenges, we propose anovel Multi-Scale State-Space Model-based (MS-Mamba) for efficient imagerestoration that enhances the capacity for multi-scale representation learningthrough our proposed global and regional SSM modules. Additionally, an AdaptiveGradient Block (AGB) and a Residual Fourier Block (RFB) are proposed to improvethe network's detail extraction capabilities by capturing gradients in variousdirections and facilitating learning details in the frequency domain. Extensiveexperiments on nine public benchmarks across four classic image restorationtasks, image deraining, dehazing, denoising, and low-light enhancement,demonstrate that our proposed method achieves new state-of-the-art performancewhile maintaining low computational complexity. The source code will bepublicly available.</description><author>Yuhong He, Long Peng, Qiaosi Yi, Chen Wu, Lu Wang</author><pubDate>Mon, 19 Aug 2024 16:42:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10145v1</guid></item><item><title>Instruction Finetuning for Leaderboard Generation from Empirical AI Research</title><link>http://arxiv.org/abs/2408.10141v1</link><description>This study demonstrates the application of instruction finetuning ofpretrained Large Language Models (LLMs) to automate the generation of AIresearch leaderboards, extracting (Task, Dataset, Metric, Score) quadruplesfrom articles. It aims to streamline the dissemination of advancements in AIresearch by transitioning from traditional, manual community curation, orotherwise taxonomy-constrained natural language inference (NLI) models, to anautomated, generative LLM-based approach. Utilizing the FLAN-T5 model, thisresearch enhances LLMs' adaptability and reliability in information extraction,offering a novel method for structured knowledge representation.</description><author>Salomon Kabongo, Jennifer D'Souza</author><pubDate>Mon, 19 Aug 2024 16:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10141v1</guid></item><item><title>Tensor network compressibility of convolutional models</title><link>http://arxiv.org/abs/2403.14379v2</link><description>Convolutional neural networks (CNNs) are one of the most widely used neuralnetwork architectures, showcasing state-of-the-art performance in computervision tasks. Although larger CNNs generally exhibit higher accuracy, theirsize can be effectively reduced by ``tensorization'' while maintainingaccuracy, namely, replacing the convolution kernels with compact decompositionssuch as Tucker, Canonical Polyadic decompositions, or quantum-inspireddecompositions such as matrix product states, and directly training the factorsin the decompositions to bias the learning towards low-rank decompositions. Butwhy doesn't tensorization seem to impact the accuracy adversely? We explorethis by assessing how \textit{truncating} the convolution kernels of\textit{dense} (untensorized) CNNs impact their accuracy. Specifically, wetruncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50pre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. Wefound that kernels (especially those inside deeper layers) could often betruncated along several cuts resulting in significant loss in kernel norm butnot in classification accuracy. This suggests that such ``correlationcompression'' (underlying tensorization) is an intrinsic feature of howinformation is encoded in dense CNNs. We also found that aggressively truncatedmodels could often recover the pre-truncation accuracy after only a few epochsof re-training, suggesting that compressing the internal correlations ofconvolution layers does not often transport the model to a worse minimum. Ourresults can be applied to tensorize and compress CNN models more effectively.</description><author>Sukhbinder Singh, Saeed S. Jahromi, Roman Orus</author><pubDate>Mon, 19 Aug 2024 16:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14379v2</guid></item><item><title>Robust spectral clustering with rank statistics</title><link>http://arxiv.org/abs/2408.10136v1</link><description>This paper analyzes the statistical performance of a robust spectralclustering method for latent structure recovery in noisy data matrices. Weconsider eigenvector-based clustering applied to a matrix of nonparametric rankstatistics that is derived entrywise from the raw, original data matrix. Thisapproach is robust in the sense that, unlike traditional spectral clusteringprocedures, it can provably recover population-level latent block structureeven when the observed data matrix includes heavy-tailed entries and has aheterogeneous variance profile. Our main theoretical contributions are threefold and hold under flexible datagenerating conditions. First, we establish that robust spectral clustering withrank statistics can consistently recover latent block structure, viewed ascommunities of nodes in a graph, in the sense that unobserved communitymemberships for all but a vanishing fraction of nodes are correctly recoveredwith high probability when the data matrix is large. Second, we refine theformer result and further establish that, under certain conditions, thecommunity membership of any individual, specified node of interest can beasymptotically exactly recovered with probability tending to one in thelarge-data limit. Third, we establish asymptotic normality results associatedwith the truncated eigenstructure of matrices whose entries are rankstatistics, made possible by synthesizing contemporary entrywise matrixperturbation analysis with the classical nonparametric theory of so-calledsimple linear rank statistics. Collectively, these results demonstrate thestatistical utility of rank-based data transformations when paired withspectral techniques for dimensionality reduction. Additionally, for a datasetof human connectomes, our approach yields parsimonious dimensionality reductionand improved recovery of ground-truth neuroanatomical cluster structure.</description><author>Joshua Cape, Xianshi Yu, Jonquil Z. Liao</author><pubDate>Mon, 19 Aug 2024 16:33:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10136v1</guid></item><item><title>$R^2$-Mesh: Reinforcement Learning Powered Mesh Reconstruction via Geometry and Appearance Refinement</title><link>http://arxiv.org/abs/2408.10135v1</link><description>Mesh reconstruction based on Neural Radiance Fields (NeRF) is popular in avariety of applications such as computer graphics, virtual reality, and medicalimaging due to its efficiency in handling complex geometric structures andfacilitating real-time rendering. However, existing works often fail to capturefine geometric details accurately and struggle with optimizing renderingquality. To address these challenges, we propose a novel algorithm thatprogressively generates and optimizes meshes from multi-view images. Ourapproach initiates with the training of a NeRF model to establish an initialSigned Distance Field (SDF) and a view-dependent appearance field.Subsequently, we iteratively refine the SDF through a differentiable meshextraction method, continuously updating both the vertex positions and theirconnectivity based on the loss from mesh differentiable rasterization, whilealso optimizing the appearance representation. To further leveragehigh-fidelity and detail-rich representations from NeRF, we propose anonline-learning strategy based on Upper Confidence Bound (UCB) to enhanceviewpoints by adaptively incorporating images rendered by the initial NeRFmodel into the training dataset. Through extensive experiments, we demonstratethat our method delivers highly competitive and robust performance in both meshrendering quality and geometric quality.</description><author>Haoyang Wang, Liming Liu, Quanlu Jia, Jiangkai Wu, Haodan Zhang, Peiheng Wang, Xinggong Zhang</author><pubDate>Mon, 19 Aug 2024 16:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10135v1</guid></item><item><title>Perceptual Depth Quality Assessment of Stereoscopic Omnidirectional Images</title><link>http://arxiv.org/abs/2408.10134v1</link><description>Depth perception plays an essential role in the viewer experience forimmersive virtual reality (VR) visual environments. However, previous researchinvestigations in the depth quality of 3D/stereoscopic images are ratherlimited, and in particular, are largely lacking for 3D viewing of 360-degreeomnidirectional content. In this work, we make one of the first attempts todevelop an objective quality assessment model named depth quality index (DQI)for efficient no-reference (NR) depth quality assessment of stereoscopicomnidirectional images. Motivated by the perceptual characteristics of thehuman visual system (HVS), the proposed DQI is built upon multi-color-channel,adaptive viewport selection, and interocular discrepancy features. Experimentalresults demonstrate that the proposed method outperforms state-of-the-art imagequality assessment (IQA) and depth quality assessment (DQA) approaches inpredicting the perceptual depth quality when tested using both single-viewportand omnidirectional stereoscopic image databases. Furthermore, we demonstratethat combining the proposed depth quality model with existing IQA methodssignificantly boosts the performance in predicting the overall quality of 3Domnidirectional images.</description><author>Wei Zhou, Zhou Wang</author><pubDate>Mon, 19 Aug 2024 16:28:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10134v1</guid></item><item><title>Rhyme-aware Chinese lyric generator based on GPT</title><link>http://arxiv.org/abs/2408.10130v1</link><description>Neural language representation models such as GPT, pre-trained on large-scalecorpora, can effectively capture rich semantic patterns from plain text and befine-tuned to consistently improve natural language generation performance.However, existing pre-trained language models used to generate lyrics rarelyconsider rhyme information, which is crucial in lyrics. Using a pre-trainedmodel directly results in poor performance. To enhance the rhyming quality ofgenerated lyrics, we incorporate integrated rhyme information into our model,thereby improving lyric generation performance.</description><author>Yixiao Yuan, Yangchen Huang, Yu Ma, Xinjin Li, Zhenglin Li, Yiming Shi, Huapeng Zhou</author><pubDate>Mon, 19 Aug 2024 16:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10130v1</guid></item><item><title>UNINEXT-Cutie: The 1st Solution for LSVOS Challenge RVOS Track</title><link>http://arxiv.org/abs/2408.10129v1</link><description>Referring video object segmentation (RVOS) relies on natural languageexpressions to segment target objects in video. In this year, LSVOS ChallengeRVOS Track replaced the origin YouTube-RVOS benchmark with MeViS. MeViS focuseson referring the target object in a video through its motion descriptionsinstead of static attributes, posing a greater challenge to RVOS task. In thiswork, we integrate strengths of that leading RVOS and VOS models to build up asimple and effective pipeline for RVOS. Firstly, We finetune thestate-of-the-art RVOS model to obtain mask sequences that are correlated withlanguage descriptions. Secondly, based on a reliable and high-quality keyframes, we leverage VOS model to enhance the quality and temporal consistencyof the mask results. Finally, we further improve the performance of the RVOSmodel using semi-supervised learning. Our solution achieved 62.57 J&amp;F on theMeViS test set and ranked 1st place for 6th LSVOS Challenge RVOS Track.</description><author>Hao Fang, Feiyu Pan, Xiankai Lu, Wei Zhang, Runmin Cong</author><pubDate>Mon, 19 Aug 2024 16:15:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10129v1</guid></item><item><title>Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a Low-Resource Language</title><link>http://arxiv.org/abs/2408.10128v1</link><description>Voice cloning is a prominent feature in personalized speech interfaces. Aneural vocal cloning system can mimic someone's voice using just a few audiosamples. Both speaker encoding and speaker adaptation are topics of research inthe field of voice cloning. Speaker adaptation relies on fine-tuning amulti-speaker generative model, which involves training a separate model toinfer a new speaker embedding used for speaker encoding. Both methods canachieve excellent performance, even with a small number of cloning audios, interms of the speech's naturalness and similarity to the original speaker.Speaker encoding approaches are more appropriate for low-resource deploymentsince they require significantly less memory and have a faster cloning timethan speaker adaption, which can offer slightly greater naturalness andsimilarity. The main goal is to create a vocal cloning system that producesaudio output with a Nepali accent or that sounds like Nepali. For the furtheradvancement of TTS, the idea of transfer learning was effectively used toaddress several issues that were encountered in the development of this system,including the poor audio quality and the lack of available data.</description><author>Manjil Karki, Pratik Shakya, Sandesh Acharya, Ravi Pandit, Dinesh Gothe</author><pubDate>Mon, 19 Aug 2024 16:15:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10128v1</guid></item><item><title>Learning Brave Assumption-Based Argumentation Frameworks via ASP</title><link>http://arxiv.org/abs/2408.10126v1</link><description>Assumption-based Argumentation (ABA) is advocated as a unifying formalism forvarious forms of non-monotonic reasoning, including logic programming. Itallows capturing defeasible knowledge, subject to argumentative debate. While,in much existing work, ABA frameworks are given up-front, in this paper wefocus on the problem of automating their learning from background knowledge andpositive/negative examples. Unlike prior work, we newly frame the problem interms of brave reasoning under stable extensions for ABA. We present a novelalgorithm based on transformation rules (such as Rote Learning, Folding,Assumption Introduction and Fact Subsumption) and an implementation thereofthat makes use of Answer Set Programming. Finally, we compare our technique tostate-of-the-art ILP systems that learn defeasible knowledge.</description><author>Emanuele De Angelis, Maurizio Proietti, Francesca Toni</author><pubDate>Mon, 19 Aug 2024 16:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10126v1</guid></item><item><title>Video Object Segmentation via SAM 2: The 4th Solution for LSVOS Challenge VOS Track</title><link>http://arxiv.org/abs/2408.10125v1</link><description>Video Object Segmentation (VOS) task aims to segmenting a particular objectinstance throughout the entire video sequence given only the object mask of thefirst frame. Recently, Segment Anything Model 2 (SAM 2) is proposed, which is afoundation model towards solving promptable visual segmentation in images andvideos. SAM 2 builds a data engine, which improves model and data via userinteraction, to collect the largest video segmentation dataset to date. SAM 2is a simple transformer architecture with streaming memory for real-time videoprocessing, which trained on the date provides strong performance across a widerange of tasks. In this work, we evaluate the zero-shot performance of SAM 2 onthe more challenging VOS datasets MOSE and LVOS. Without fine-tuning on thetraining set, SAM 2 achieved 75.79 J&amp;F on the test set and ranked 4th place for6th LSVOS Challenge VOS Track.</description><author>Feiyu Pan, Hao Fang, Runmin Cong, Wei Zhang, Xiankai Lu</author><pubDate>Mon, 19 Aug 2024 16:13:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10125v1</guid></item><item><title>Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models</title><link>http://arxiv.org/abs/2408.10124v1</link><description>Molecular property prediction is a crucial foundation for drug discovery. Inrecent years, pre-trained deep learning models have been widely applied to thistask. Some approaches that incorporate prior biological domain knowledge intothe pre-training framework have achieved impressive results. However, thesemethods heavily rely on biochemical experts, and retrieving and summarizingvast amounts of domain knowledge literature is both time-consuming andexpensive. Large Language Models (LLMs) have demonstrated remarkableperformance in understanding and efficiently providing general knowledge.Nevertheless, they occasionally exhibit hallucinations and lack precision ingenerating domain-specific knowledge. Conversely, Domain-specific Small Models(DSMs) possess rich domain knowledge and can accurately calculate moleculardomain-related metrics. However, due to their limited model size and singularfunctionality, they lack the breadth of knowledge necessary for comprehensiverepresentation learning. To leverage the advantages of both approaches inmolecular property prediction, we propose a novel Molecular Graphrepresentation learning framework that integrates Large language models andDomain-specific small models (MolGraph-LarDo). Technically, we design atwo-stage prompt strategy where DSMs are introduced to calibrate the knowledgeprovided by LLMs, enhancing the accuracy of domain-specific information andthus enabling LLMs to generate more precise textual descriptions for molecularsamples. Subsequently, we employ a multi-modal alignment method to coordinatevarious modalities, including molecular graphs and their correspondingdescriptive texts, to guide the pre-training of molecular representations.Extensive experiments demonstrate the effectiveness of the proposed method.</description><author>Tianyu Zhang, Yuxiang Ren, Chengbin Hou, Hairong Lv, Xuegong Zhang</author><pubDate>Mon, 19 Aug 2024 16:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10124v1</guid></item><item><title>Learning Precise Affordances from Egocentric Videos for Robotic Manipulation</title><link>http://arxiv.org/abs/2408.10123v1</link><description>Affordance, defined as the potential actions that an object offers, iscrucial for robotic manipulation tasks. A deep understanding of affordance canlead to more intelligent AI systems. For example, such knowledge directs anagent to grasp a knife by the handle for cutting and by the blade when passingit to someone. In this paper, we present a streamlined affordance learningsystem that encompasses data collection, effective model training, and robotdeployment. First, we collect training data from egocentric videos in anautomatic manner. Different from previous methods that focus only on the objectgraspable affordance and represent it as coarse heatmaps, we cover bothgraspable (e.g., object handles) and functional affordances (e.g., knifeblades, hammer heads) and extract data with precise segmentation masks. We thenpropose an effective model, termed Geometry-guided Affordance Transformer(GKT), to train on the collected data. GKT integrates an innovative DepthFeature Injector (DFI) to incorporate 3D shape and geometric priors, enhancingthe model's understanding of affordances. To enable affordance-orientedmanipulation, we further introduce Aff-Grasp, a framework that combines GKTwith a grasp generation model. For comprehensive evaluation, we create anaffordance evaluation dataset with pixel-wise annotations, and designreal-world tasks for robot experiments. The results show that GKT surpasses thestate-of-the-art by 15.9% in mIoU, and Aff-Grasp achieves high success rates of95.5% in affordance prediction and 77.1% in successful grasping among 179trials, including evaluations with seen, unseen objects, and cluttered scenes.</description><author>Gen Li, Nikolaos Tsagkas, Jifei Song, Ruaridh Mon-Williams, Sethu Vijayakumar, Kun Shao, Laura Sevilla-Lara</author><pubDate>Mon, 19 Aug 2024 16:11:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10123v1</guid></item><item><title>Geometry Informed Tokenization of Molecules for Language Model Generation</title><link>http://arxiv.org/abs/2408.10120v1</link><description>We consider molecule generation in 3D space using language models (LMs),which requires discrete tokenization of 3D molecular geometries. Althoughtokenization of molecular graphs exists, that for 3D geometries is largelyunexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, whichconverts molecular geometries into $SE(3)$-invariant 1D discrete sequences.Geo2Seq consists of canonical labeling and invariant spherical representationsteps, which together maintain geometric and atomic fidelity in a formatconducive to LMs. Our experiments show that, when coupled with Geo2Seq, variousLMs excel in molecular geometry generation, especially in controlled generationtasks.</description><author>Xiner Li, Limei Wang, Youzhi Luo, Carl Edwards, Shurui Gui, Yuchao Lin, Heng Ji, Shuiwang Ji</author><pubDate>Mon, 19 Aug 2024 16:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10120v1</guid></item><item><title>Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism</title><link>http://arxiv.org/abs/2407.21611v2</link><description>The task of partially spoofed audio localization aims to accurately determineaudio authenticity at a frame level. Although some works have achievedencouraging results, utilizing boundary information within a single modelremains an unexplored research topic. In this work, we propose a novel methodcalled Boundary-aware Attention Mechanism (BAM). Specifically, it consists oftwo core modules: Boundary Enhancement and Boundary Frame-wise Attention. Theformer assembles the intra-frame and inter-frame information to extractdiscriminative boundary features that are subsequently used for boundaryposition detection and authenticity decision, while the latter leveragesboundary prediction results to explicitly control the feature interactionbetween frames, which achieves effective discrimination between real and fakeframes. Experimental results on PartialSpoof database demonstrate our proposedmethod achieves the best performance. The code is available athttps://github.com/media-sec-lab/BAM.</description><author>Jiafeng Zhong, Bin Li, Jiangyan Yi</author><pubDate>Mon, 19 Aug 2024 16:09:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21611v2</guid></item><item><title>Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data</title><link>http://arxiv.org/abs/2408.10119v1</link><description>Text-to-video (T2V) generation has gained significant attention due to itswide applications to video generation, editing, enhancement and translation,\etc. However, high-quality (HQ) video synthesis is extremely challengingbecause of the diverse and complex motions existed in real world. Most existingworks struggle to address this problem by collecting large-scale HQ videos,which are inaccessible to the community. In this work, we show that publiclyavailable limited and low-quality (LQ) data are sufficient to train a HQ videogenerator without recaptioning or finetuning. We factorize the whole T2Vgeneration process into two steps: generating an image conditioned on a highlydescriptive caption, and synthesizing the video conditioned on the generatedimage and a concise caption of motion details. Specifically, we present\emph{Factorized-Dreamer}, a factorized spatiotemporal framework with severalcritical designs for T2V generation, including an adapter to combine text andimage embeddings, a pixel-aware cross attention module to capture pixel-levelimage information, a T5 text encoder to better understand motion description,and a PredictNet to supervise optical flows. We further present a noiseschedule, which plays a key role in ensuring the quality and stability of videogeneration. Our model lowers the requirements in detailed captions and HQvideos, and can be directly trained on limited LQ datasets with noisy and briefcaptions such as WebVid-10M, largely alleviating the cost to collectlarge-scale HQ video-text pairs. Extensive experiments in a variety of T2V andimage-to-video generation tasks demonstrate the effectiveness of our proposedFactorized-Dreamer. Our source codes are available at\url{https://github.com/yangxy/Factorized-Dreamer/}.</description><author>Tao Yang, Yangming Shi, Yunwen Huang, Feng Chen, Yin Zheng, Lei Zhang</author><pubDate>Mon, 19 Aug 2024 16:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10119v1</guid></item><item><title>Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset</title><link>http://arxiv.org/abs/2403.17632v2</link><description>The escalating challenges of traffic congestion and environmental degradationunderscore the critical importance of embracing E-Mobility solutions in urbanspaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,play a pivotal role in this transition, offering sustainable alternatives forurban commuters. However, the energy consumption patterns for these tools are acritical aspect that impacts their effectiveness in real-world scenarios and isessential for trip planning and boosting user confidence in using these. Tothis effect, recent studies have utilised physical models customised forspecific mobility tools and conditions, but these models struggle withgeneralization and effectiveness in real-world scenarios due to a notableabsence of open datasets for thorough model evaluation and verification. Tofill this gap, our work presents an open dataset, collected in Dublin, Ireland,specifically designed for energy modelling research related to E-Scooters andE-Bikes. Furthermore, we provide a comprehensive analysis of energy consumptionmodelling based on the dataset using a set of representative machine learningalgorithms and compare their performance against the contemporary mathematicalmodels as a baseline. Our results demonstrate a notable advantage fordata-driven models in comparison to the corresponding mathematical models forestimating energy consumption. Specifically, data-driven models outperformphysical models in accuracy by up to 83.83% for E-Bikes and 82.16% forE-Scooters based on an in-depth analysis of the dataset under certainassumptions.</description><author>Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li, Mingming Liu</author><pubDate>Mon, 19 Aug 2024 16:07:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17632v2</guid></item><item><title>GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization</title><link>http://arxiv.org/abs/2408.10115v1</link><description>Pre-trained language models are increasingly being used in multi-documentsummarization tasks. However, these models need large-scale corpora forpre-training and are domain-dependent. Other non-neural unsupervisedsummarization approaches mostly rely on key sentence extraction, which can leadto information loss. To address these challenges, we propose a lightweight yeteffective unsupervised approach called GLIMMER: a Graph and LexIcal featuresbased unsupervised Multi-docuMEnt summaRization approach. It first constructs asentence graph from the source documents, then automatically identifiessemantic clusters by mining low-level features from raw texts, therebyimproving intra-cluster correlation and the fluency of generated sentences.Finally, it summarizes clusters into natural sentences. Experiments conductedon Multi-News, Multi-XScience and DUC-2004 demonstrate that our approachoutperforms existing unsupervised approaches. Furthermore, it surpassesstate-of-the-art pre-trained multi-document summarization models (e.g. PEGASUSand PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally,human evaluations indicate that summaries generated by GLIMMER achieve highreadability and informativeness scores. Our code is available athttps://github.com/Oswald1997/GLIMMER.</description><author>Ran Liu, Ming Liu, Min Yu, Jianguo Jiang, Gang Li, Dan Zhang, Jingyuan Li, Xiang Meng, Weiqing Huang</author><pubDate>Mon, 19 Aug 2024 16:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10115v1</guid></item><item><title>CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving</title><link>http://arxiv.org/abs/2310.07794v2</link><description>Benchmarking is a common method for evaluating trajectory prediction modelsfor autonomous driving. Existing benchmarks rely on datasets, which are biasedtowards more common scenarios, such as cruising, and distance-based metricsthat are computed by averaging over all scenarios. Following such a regimentprovides a little insight into the properties of the models both in terms ofhow well they can handle different scenarios and how admissible and diversetheir outputs are. There exist a number of complementary metrics designed tomeasure the admissibility and diversity of trajectories, however, they sufferfrom biases, such as length of trajectories. In this paper, we propose a new benChmarking paRadIgm for evaluaTingtrajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) amethod for extracting driving scenarios at varying levels of specificityaccording to the structure of the roads, models' performance, and dataproperties for fine-grained ranking of prediction models; 2) A set of newbias-free metrics for measuring diversity, by incorporating the characteristicsof a given scenario, and admissibility, by considering the structure of roadsand kinematic compliancy, motivated by real-world driving constraints. 3) Usingthe proposed benchmark, we conduct extensive experimentation on arepresentative set of the prediction models using the large scale Argoversedataset. We show that the proposed benchmark can produce a more accurateranking of the models and serve as a means of characterizing their behavior. Wefurther present ablation studies to highlight contributions of differentelements that are used to compute the proposed metrics.</description><author>Changhe Chen, Mozhgan Pourkeshavarz, Amir Rasouli</author><pubDate>Mon, 19 Aug 2024 16:01:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07794v2</guid></item><item><title>Enhancing Reinforcement Learning Through Guided Search</title><link>http://arxiv.org/abs/2408.10113v1</link><description>With the aim of improving performance in Markov Decision Problem in anOff-Policy setting, we suggest taking inspiration from what is done in OfflineReinforcement Learning (RL). In Offline RL, it is a common practice duringpolicy learning to maintain proximity to a reference policy to mitigateuncertainty, reduce potential policy errors, and help improve performance. Wefind ourselves in a different setting, yet it raises questions about whether asimilar concept can be applied to enhance performance ie, whether it ispossible to find a guiding policy capable of contributing to performanceimprovement, and how to incorporate it into our RL agent. Our attention isparticularly focused on algorithms based on Monte Carlo Tree Search (MCTS) as aguide.MCTS renowned for its state-of-the-art capabilities across variousdomains, catches our interest due to its ability to converge to equilibrium insingle-player and two-player contexts. By harnessing the power of MCTS as aguide for our RL agent, we observed a significant performance improvement,surpassing the outcomes achieved by utilizing each method in isolation. Ourexperiments were carried out on the Atari 100k benchmark.</description><author>J√©r√¥me Arjonilla, Abdallah Saffidine, Tristan Cazenave</author><pubDate>Mon, 19 Aug 2024 16:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10113v1</guid></item><item><title>PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities</title><link>http://arxiv.org/abs/2408.10111v1</link><description>Financial time series modeling is crucial for understanding and predictingmarket behaviors but faces challenges such as non-linearity, non-stationarity,and high noise levels. Traditional models struggle to capture complex patternsdue to these issues, compounded by limitations in computational resources andmodel capacity. Inspired by the success of large language models in NLP, weintroduce \textbf{PLUTUS}, a \textbf{P}re-trained \textbf{L}arge\textbf{U}nified \textbf{T}ransformer-based model that \textbf{U}nveilsregularities in financial time \textbf{S}eries. PLUTUS uses an invertibleembedding module with contrastive learning and autoencoder techniques to createan approximate one-to-one mapping between raw data and patch embeddings.TimeFormer, an attention based architecture, forms the core of PLUTUS,effectively modeling high-noise time series. We incorporate a novel attentionmechanisms to capture features across both variable and temporal dimensions.PLUTUS is pre-trained on an unprecedented dataset of 100 billion observations,designed to thrive in noisy financial environments. To our knowledge, PLUTUS isthe first open-source, large-scale, pre-trained financial time series modelwith over one billion parameters. It achieves state-of-the-art performance invarious tasks, demonstrating strong transferability and establishing a robustfoundational model for finance. Our research provides technical guidance forpre-training financial time series data, setting a new standard in the field.</description><author>Yuanjian Xu, Anxian Liu, Jianing Hao, Zhenzhuo Li, Shichang Meng, Guang Zhang</author><pubDate>Mon, 19 Aug 2024 15:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10111v1</guid></item><item><title>Semantic Prototypes: Enhancing Transparency Without Black Boxes</title><link>http://arxiv.org/abs/2407.15871v3</link><description>As machine learning (ML) models and datasets increase in complexity, thedemand for methods that enhance explainability and interpretability becomesparamount. Prototypes, by encapsulating essential characteristics within data,offer insights that enable tactical decision-making and enhance transparency.Traditional prototype methods often rely on sub-symbolic raw data and opaquelatent spaces, reducing explainability and increasing the risk ofmisinterpretations. This paper presents a novel framework that utilizessemantic descriptions to define prototypes and provide clear explanations,effectively addressing the shortcomings of conventional methods. Our approachleverages concept-based descriptions to cluster data on the semantic level,ensuring that prototypes not only represent underlying properties intuitivelybut are also straightforward to interpret. Our method simplifies theinterpretative process and effectively bridges the gap between complex datastructures and human cognitive processes, thereby enhancing transparency andfostering trust. Our approach outperforms existing widely-used prototypemethods in facilitating human understanding and informativeness, as validatedthrough a user survey.</description><author>Orfeas Menis-Mastromichalakis, Giorgos Filandrianos, Jason Liartis, Edmund Dervakos, Giorgos Stamou</author><pubDate>Mon, 19 Aug 2024 15:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15871v3</guid></item><item><title>Envisioning Possibilities and Challenges of AI for Personalized Cancer Care</title><link>http://arxiv.org/abs/2408.10108v1</link><description>The use of Artificial Intelligence (AI) in healthcare, including in caringfor cancer survivors, has gained significant interest. However, gaps remain inour understanding of how such AI systems can provide care, especially forethnic and racial minority groups who continue to face care disparities.Through interviews with six cancer survivors, we identify critical gaps incurrent healthcare systems such as a lack of personalized care and insufficientcultural and linguistic accommodation. AI, when applied to care, was seen as away to address these issues by enabling real-time, culturally aligned, andlinguistically appropriate interactions. We also uncovered concerns about theimplications of AI-driven personalization, such as data privacy, loss of humantouch in caregiving, and the risk of echo chambers that limit exposure todiverse information. We conclude by discussing the trade-offs betweenAI-enhanced personalization and the need for structural changes in healthcarethat go beyond technological solutions, leading us to argue that we shouldbegin by asking, ``Why personalization?''</description><author>Elaine Kong, Kuo-Ting, Huang, Aakash Gautam</author><pubDate>Mon, 19 Aug 2024 15:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10108v1</guid></item><item><title>Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments</title><link>http://arxiv.org/abs/2408.10107v1</link><description>Accessing machine learning models through remote APIs has been gainingprevalence following the recent trend of scaling up model parameters forincreased performance. Even though these models exhibit remarkable ability,detecting out-of-distribution (OOD) samples remains a crucial safety concernfor end users as these samples may induce unreliable outputs from the model. Inthis work, we propose an OOD detection framework, MixDiff, that is applicableeven when the model's parameters or its activations are not accessible to theend user. To bypass the access restriction, MixDiff applies an identicalinput-level perturbation to a given target sample and a similar in-distribution(ID) sample, then compares the relative difference in the model outputs ofthese two samples. MixDiff is model-agnostic and compatible with existingoutput-based OOD detection methods. We provide theoretical analysis toillustrate MixDiff's effectiveness in discerning OOD samples that induceoverconfident outputs from the model and empirically demonstrate that MixDiffconsistently enhances the OOD detection performance on various datasets invision and text domains.</description><author>Heeyoung Lee, Hoyoon Byun, Changdae Oh, JinYeong Bak, Kyungwoo Song</author><pubDate>Mon, 19 Aug 2024 15:51:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10107v1</guid></item><item><title>Learning Using Generated Privileged Information by Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2309.15238v2</link><description>Learning Using Privileged Information is a particular type of knowledgedistillation where the teacher model benefits from an additional datarepresentation during training, called privileged information, improving thestudent model, which does not see the extra representation. However, privilegedinformation is rarely available in practice. To this end, we propose a textclassification framework that harnesses text-to-image diffusion models togenerate artificial privileged information. The generated images and theoriginal text samples are further used to train multimodal teacher models basedon state-of-the-art transformer-based architectures. Finally, the knowledgefrom multimodal teachers is distilled into a text-based (unimodal) student.Hence, by employing a generative model to produce synthetic data as privilegedinformation, we guide the training of the student model. Our framework, calledLearning Using Generated Privileged Information (LUGPI), yields noticeableperformance gains on four text classification data sets, demonstrating itspotential in text classification without any additional cost during inference.</description><author>Rafael-Edy Menadil, Mariana-Iuliana Georgescu, Radu Tudor Ionescu</author><pubDate>Mon, 19 Aug 2024 15:39:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15238v2</guid></item><item><title>Convert and Speak: Zero-shot Accent Conversion with Minimum Supervision</title><link>http://arxiv.org/abs/2408.10096v1</link><description>Low resource of parallel data is the key challenge of accent conversion(AC)problem in which both the pronunciation units and prosody pattern need to beconverted. We propose a two-stage generative framework "convert-and-speak" inwhich the conversion is only operated on the semantic token level and thespeech is synthesized conditioned on the converted semantic token with a speechgenerative model in target accent domain. The decoupling design enables the"speaking" module to use massive amount of target accent speech and relievesthe parallel data required for the "conversion" module. Conversion with thebridge of semantic token also relieves the requirement for the data with texttranscriptions and unlocks the usage of language pre-training technology tofurther efficiently reduce the need of parallel accent speech data. To reducethe complexity and latency of "speaking", a single-stage AR generative model isdesigned to achieve good quality as well as lower computation cost. Experimentson Indian-English to general American-English conversion show that the proposedframework achieves state-of-the-art performance in accent similarity, speechquality, and speaker maintenance with only 15 minutes of weakly parallel datawhich is not constrained to the same speaker. Extensive experimentation withdiverse accent types suggests that this framework possesses a high degree ofadaptability, making it readily scalable to accommodate other accents withlow-resource data. Audio samples are available athttps://www.microsoft.com/en-us/research/project/convert-and-speak-zero-shot-accent-conversion-with-minimumsupervision/.</description><author>Zhijun Jia, Huaying Xue, Xiulian Peng, Yan Lu</author><pubDate>Mon, 19 Aug 2024 15:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10096v1</guid></item><item><title>Federated Frank-Wolfe Algorithm</title><link>http://arxiv.org/abs/2408.10090v1</link><description>Federated learning (FL) has gained a lot of attention in recent years forbuilding privacy-preserving collaborative learning systems. However, FLalgorithms for constrained machine learning problems are still limited,particularly when the projection step is costly. To this end, we propose aFederated Frank-Wolfe Algorithm (FedFW). FedFW features data privacy, lowper-iteration cost, and communication of sparse signals. In the deterministicsetting, FedFW achieves an $\varepsilon$-suboptimal solution within$O(\varepsilon^{-2})$ iterations for smooth and convex objectives, and$O(\varepsilon^{-3})$ iterations for smooth but non-convex objectives.Furthermore, we present a stochastic variant of FedFW and show that it finds asolution within $O(\varepsilon^{-3})$ iterations in the convex setting. Wedemonstrate the empirical performance of FedFW on several machine learningtasks.</description><author>Ali Dadras, Sourasekhar Banerjee, Karthik Prakhya, Alp Yurtsever</author><pubDate>Mon, 19 Aug 2024 15:31:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10090v1</guid></item><item><title>Adaptive Draft-Verification for Efficient Large Language Model Decoding</title><link>http://arxiv.org/abs/2407.12021v2</link><description>Large language model (LLM) decoding involves generating a sequence of tokensbased on a given context, where each token is predicted one at a time using themodel's learned probabilities. The typical autoregressive decoding methodrequires a separate forward pass through the model for each token generated,which is computationally inefficient and poses challenges for deploying LLMs inlatency-sensitive scenarios. The main limitations of current decoding methodsstem from their inefficiencies and resource demands. Existing approaches eithernecessitate fine-tuning smaller models, which is resource-intensive, or rely onfixed retrieval schemes to construct drafts for the next tokens, which lackadaptability and fail to generalize across different models and contexts. Toaddress these issues, we introduce a novel methodology called ADED, whichaccelerates LLM decoding without requiring fine-tuning. Our approach involvesan adaptive draft-verification process that evolves over time to improveefficiency. We utilize a tri-gram matrix-based LLM representation todynamically approximate the output distribution of the LLM, allowing the modelto adjust to changing token probabilities during the decoding process.Additionally, we implement a draft construction mechanism that effectivelybalances exploration and exploitation, ensuring that the drafts generated areboth diverse and close to the true output distribution of the LLM. Theimportance of this design lies in its ability to optimize the draftdistribution adaptively, leading to faster and more accurate decoding. Throughextensive experiments on various benchmark datasets and LLM architectures, wedemonstrate that ADED significantly accelerates the decoding process whilemaintaining high accuracy, making it suitable for deployment in a wide range ofpractical applications.</description><author>Xukun Liu, Bowen Lei, Ruqi Zhang, Dongkuan Xu</author><pubDate>Mon, 19 Aug 2024 15:28:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12021v2</guid></item><item><title>ARMADA: Attribute-Based Multimodal Data Augmentation</title><link>http://arxiv.org/abs/2408.10086v1</link><description>In Multimodal Language Models (MLMs), the cost of manually annotatinghigh-quality image-text pair data for fine-tuning and alignment is extremelyhigh. While existing multimodal data augmentation frameworks propose ways toaugment image-text pairs, they either suffer from semantic inconsistencybetween texts and images, or generate unrealistic images, causing knowledge gapwith real world examples. To address these issues, we propose Attribute-basedMultimodal Data Augmentation (ARMADA), a novel multimodal data augmentationmethod via knowledge-guided manipulation of visual attributes of the mentionedentities. Specifically, we extract entities and their visual attributes fromthe original text data, then search for alternative values for the visualattributes under the guidance of knowledge bases (KBs) and large languagemodels (LLMs). We then utilize an image-editing model to edit the images withthe extracted attributes. ARMADA is a novel multimodal data generationframework that: (i) extracts knowledge-grounded attributes from symbolic KBsfor semantically consistent yet distinctive image-text pair generation, (ii)generates visually similar images of disparate categories using neighboringentities in the KB hierarchy, and (iii) uses the commonsense knowledge of LLMsto modulate auxiliary visual attributes such as backgrounds for more robustrepresentation of original entities. Our empirical results over four downstreamtasks demonstrate the efficacy of our framework to produce high-quality dataand enhance the model performance. This also highlights the need to leverageexternal knowledge proxies for enhanced interpretability and real-worldgrounding.</description><author>Xiaomeng Jin, Jeonghwan Kim, Yu Zhou, Kuan-Hao Huang, Te-Lin Wu, Nanyun Peng, Heng Ji</author><pubDate>Mon, 19 Aug 2024 15:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10086v1</guid></item><item><title>MASALA: Model-Agnostic Surrogate Explanations by Locality Adaptation</title><link>http://arxiv.org/abs/2408.10085v1</link><description>Existing local Explainable AI (XAI) methods, such as LIME, select a region ofthe input space in the vicinity of a given input instance, for which theyapproximate the behaviour of a model using a simpler and more interpretablesurrogate model. The size of this region is often controlled by a user-definedlocality hyperparameter. In this paper, we demonstrate the difficultiesassociated with defining a suitable locality size to capture impactful modelbehaviour, as well as the inadequacy of using a single locality size to explainall predictions. We propose a novel method, MASALA, for generatingexplanations, which automatically determines the appropriate local region ofimpactful model behaviour for each individual instance being explained. MASALAapproximates the local behaviour used by a complex model to make a predictionby fitting a linear surrogate model to a set of points which experience similarmodel behaviour. These points are found by clustering the input space intoregions of linear behavioural trends exhibited by the model. We compare thefidelity and consistency of explanations generated by our method with existinglocal XAI methods, namely LIME and CHILLI. Experiments on the PHM08 and MIDASdatasets show that our method produces more faithful and consistentexplanations than existing methods, without the need to define any sensitivelocality hyperparameters.</description><author>Saif Anwar, Nathan Griffiths, Abhir Bhalerao, Thomas Popham</author><pubDate>Mon, 19 Aug 2024 15:26:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10085v1</guid></item><item><title>TANGO: Clustering with Typicality-Aware Nonlocal Mode-Seeking and Graph-Cut Optimization</title><link>http://arxiv.org/abs/2408.10084v1</link><description>Density-based clustering methods by mode-seeking usually achieve clusteringby using local density estimation to mine structural information, such as localdependencies from lower density points to higher neighbors. However, they oftenrely too heavily on \emph{local} structures and neglect \emph{global}characteristics, which can lead to significant errors in peak selection anddependency establishment. Although introducing more hyperparameters that revisedependencies can help mitigate this issue, tuning them is challenging and evenimpossible on real-world datasets. In this paper, we propose a new algorithm(TANGO) to establish local dependencies by exploiting a global-view\emph{typicality} of points, which is obtained by mining further the densitydistributions and initial dependencies. TANGO then obtains sub-clusters withthe help of the adjusted dependencies, and characterizes the similarity betweensub-clusters by incorporating path-based connectivity. It achieves finalclustering by employing graph-cut on sub-clusters, thus avoiding thechallenging selection of cluster centers. Moreover, this paper providestheoretical analysis and an efficient method for the calculation of typicality.Experimental results on several synthetic and $16$ real-world datasetsdemonstrate the effectiveness and superiority of TANGO.</description><author>Haowen Ma, Zhiguo Long, Hua Meng</author><pubDate>Mon, 19 Aug 2024 15:26:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10084v1</guid></item><item><title>No Screening is More Efficient with Multiple Objects</title><link>http://arxiv.org/abs/2408.10077v1</link><description>We study efficient mechanism design for allocating multiple heterogeneousobjects. We aim to maximize the residual surplus, the total value generatedfrom an allocation minus the costs for screening agents' values. We discover arobust trend indicating that no-screening mechanisms such as serialdictatorship with exogenous priority order tend to perform better as thevariety of goods increases. We analyze the underlying reasons by characterizingefficient mechanisms in a stylized environment. We also apply an automatedmechanism design approach to numerically derive efficient mechanisms andvalidate the trend in general environments. Building on this implication, wepropose the register-invite-book system (RIB) as an efficient system forscheduling vaccination against pandemic diseases.</description><author>Shunya Noda, Genta Okada</author><pubDate>Mon, 19 Aug 2024 15:20:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10077v1</guid></item><item><title>ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability</title><link>http://arxiv.org/abs/2404.14712v5</link><description>Earth system predictability is challenged by the complexity of environmentaldynamics and the multitude of variables involved. Current AI foundation models,although advanced by leveraging large and heterogeneous data, are oftenconstrained by their size and data integration, limiting their effectiveness inaddressing the full range of Earth system prediction challenges. To overcomethese limitations, we introduce the Oak Ridge Base Foundation Model for EarthSystem Predictability (ORBIT), an advanced vision transformer model that scalesup to 113 billion parameters using a novel hybrid tensor-data orthogonalparallelism technique. As the largest model of its kind, ORBIT surpasses thecurrent climate AI foundation model size by a thousandfold. Performance scalingtests conducted on the Frontier supercomputer have demonstrated that ORBITachieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scalingefficiency maintained at 41% to 85% across 49,152 AMD GPUs. These breakthroughsestablish new advances in AI-driven climate modeling and demonstrate promise tosignificantly improve the Earth system predictability.</description><author>Xiao Wang, Siyan Liu, Aristeidis Tsaris, Jong-Youl Choi, Ashwin Aji, Ming Fan, Wei Zhang, Junqi Yin, Moetasim Ashfaq, Dan Lu, Prasanna Balaprakash</author><pubDate>Mon, 19 Aug 2024 15:20:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14712v5</guid></item><item><title>Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning</title><link>http://arxiv.org/abs/2408.10075v1</link><description>Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm foraligning foundation models to human values and preferences. However, currentRLHF techniques cannot account for the naturally occurring differences inindividual human preferences across a diverse population. When thesedifferences arise, traditional RLHF frameworks simply average over them,leading to inaccurate rewards and poor performance for individual subgroups. Toaddress the need for pluralistic alignment, we develop a class of multimodalRLHF methods. Our proposed techniques are based on a latent variableformulation - inferring a novel user-specific latent and learning reward modelsand policies conditioned on this latent without additional user-specific data.While conceptually simple, we show that in practice, this reward modelingrequires careful algorithmic considerations around model architecture andreward scaling. To empirically validate our proposed technique, we first showthat it can provide a way to combat underspecification in simulated controlproblems, inferring and optimizing user-specific reward functions. Next, weconduct experiments on pluralistic language datasets representing diverse userpreferences and demonstrate improved reward function accuracy. We additionallyshow the benefits of this probabilistic framework in terms of measuringuncertainty, and actively learning user preferences. This work enables learningfrom diverse populations of users with divergent preferences, an importantchallenge that naturally occurs in problems from robot learning to foundationmodel alignment.</description><author>Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, Natasha Jaques</author><pubDate>Mon, 19 Aug 2024 15:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10075v1</guid></item><item><title>Synthesis of Reward Machines for Multi-Agent Equilibrium Design (Full Version)</title><link>http://arxiv.org/abs/2408.10074v1</link><description>Mechanism design is a well-established game-theoretic paradigm for designinggames to achieve desired outcomes. This paper addresses a closely related butdistinct concept, equilibrium design. Unlike mechanism design, the designer'sauthority in equilibrium design is more constrained; she can only modify theincentive structures in a given game to achieve certain outcomes without theability to create the game from scratch. We study the problem of equilibriumdesign using dynamic incentive structures, known as reward machines. We useweighted concurrent game structures for the game model, with goals (for theplayers and the designer) defined as mean-payoff objectives. We show how rewardmachines can be used to represent dynamic incentives that allocate rewards in amanner that optimises the designer's goal. We also introduce the main decisionproblem within our framework, the payoff improvement problem. This problemessentially asks whether there exists a dynamic incentive (represented by somereward machine) that can improve the designer's payoff by more than a giventhreshold value. We present two variants of the problem: strong and weak. Wedemonstrate that both can be solved in polynomial time using a Turing machineequipped with an NP oracle. Furthermore, we also establish that these variantsare either NP-hard or coNP-hard. Finally, we show how to synthesise thecorresponding reward machine if it exists.</description><author>Muhammad Najib, Giuseppe Perelli</author><pubDate>Mon, 19 Aug 2024 15:17:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10074v1</guid></item><item><title>Modelling the Distribution of Human Motion for Sign Language Assessment</title><link>http://arxiv.org/abs/2408.10073v1</link><description>Sign Language Assessment (SLA) tools are useful to aid in language learningand are underdeveloped. Previous work has focused on isolated signs orcomparison against a single reference video to assess Sign Languages (SL). Thispaper introduces a novel SLA tool designed to evaluate the comprehensibility ofSL by modelling the natural distribution of human motion. We train our pipelineon data from native signers and evaluate it using SL learners. We compare ourresults to ratings from a human raters study and find strong correlationbetween human ratings and our tool. We visually demonstrate our tools abilityto detect anomalous results spatio-temporally, providing actionable feedback toaid in SL learning and assessment.</description><author>Oliver Cory, Ozge Mercanoglu Sincan, Matthew Vowels, Alessia Battisti, Franz Holzknecht, Katja Tissi, Sandra Sidler-Miserez, Tobias Haug, Sarah Ebling, Richard Bowden</author><pubDate>Mon, 19 Aug 2024 15:16:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10073v1</guid></item><item><title>FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant</title><link>http://arxiv.org/abs/2408.10072v1</link><description>The rapid advancement of deepfake technologies has sparked widespread publicconcern, particularly as face forgery poses a serious threat to publicinformation security. However, the unknown and diverse forgery techniques,varied facial features and complex environmental factors pose significantchallenges for face forgery analysis. Existing datasets lack descriptions ofthese aspects, making it difficult for models to distinguish between real andforged faces using only visual information amid various confounding factors. Inaddition, existing methods do not yield user-friendly and explainable results,complicating the understanding of the model's decision-making process. Toaddress these challenges, we introduce a novel Open-World Face Forgery AnalysisVQA (OW-FFA-VQA) task and the corresponding benchmark. To tackle this task, wefirst establish a dataset featuring a diverse collection of real and forgedface images with essential descriptions and reliable forgery reasoning. Base onthis dataset, we introduce FFAA: Face Forgery Analysis Assistant, consisting ofa fine-tuned Multimodal Large Language Model (MLLM) and Multi-answerIntelligent Decision System (MIDS). By integrating hypothetical prompts withMIDS, the impact of fuzzy classification boundaries is effectively mitigated,enhancing the model's robustness. Extensive experiments demonstrate that ourmethod not only provides user-friendly explainable results but alsosignificantly boosts accuracy and robustness compared to previous methods.</description><author>Zhengchao Huang, Bin Xia, Zicheng Lin, Zhun Mou, Wenming Yang</author><pubDate>Mon, 19 Aug 2024 15:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10072v1</guid></item><item><title>LNQ 2023 challenge: Benchmark of weakly-supervised techniques for mediastinal lymph node quantification</title><link>http://arxiv.org/abs/2408.10069v1</link><description>Accurate assessment of lymph node size in 3D CT scans is crucial for cancerstaging, therapeutic management, and monitoring treatment response. Existingstate-of-the-art segmentation frameworks in medical imaging often rely on fullyannotated datasets. However, for lymph node segmentation, these datasets aretypically small due to the extensive time and expertise required to annotatethe numerous lymph nodes in 3D CT scans. Weakly-supervised learning, whichleverages incomplete or noisy annotations, has recently gained interest in themedical imaging community as a potential solution. Despite the variety ofweakly-supervised techniques proposed, most have been validated only on privatedatasets or small publicly available datasets. To address this limitation, theMediastinal Lymph Node Quantification (LNQ) challenge was organized inconjunction with the 26th International Conference on Medical Image Computingand Computer Assisted Intervention (MICCAI 2023). This challenge aimed toadvance weakly-supervised segmentation methods by providing a new, partiallyannotated dataset and a robust evaluation framework. A total of 16 teams from 5countries submitted predictions to the validation leaderboard, and 6 teams from3 countries participated in the evaluation phase. The results highlighted boththe potential and the current limitations of weakly-supervised approaches. Onone hand, weakly-supervised approaches obtained relatively good performancewith a median Dice score of $61.0\%$. On the other hand, top-ranked teams, witha median Dice score exceeding $70\%$, boosted their performance by leveragingsmaller but fully annotated datasets to combine weak supervision and fullsupervision. This highlights both the promise of weakly-supervised methods andthe ongoing need for high-quality, fully annotated data to achieve highersegmentation performance.</description><author>Reuben Dorent, Roya Khajavi, Tagwa Idris, Erik Ziegler, Bhanusupriya Somarouthu, Heather Jacene, Ann LaCasce, Jonathan Deissler, Jan Ehrhardt, Sofija Engelson, Stefan M. Fischer, Yun Gu, Heinz Handels, Satoshi Kasai, Satoshi Kondo, Klaus Maier-Hein, Julia A. Schnabel, Guotai Wang, Litingyu Wang, Tassilo Wald, Guang-Zhong Yang, Hanxiao Zhang, Minghui Zhang, Steve Pieper, Gordon Harris, Ron Kikinis, Tina Kapur</author><pubDate>Mon, 19 Aug 2024 15:11:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10069v1</guid></item><item><title>Towards a Benchmark for Colorectal Cancer Segmentation in Endorectal Ultrasound Videos: Dataset and Model Development</title><link>http://arxiv.org/abs/2408.10067v1</link><description>Endorectal ultrasound (ERUS) is an important imaging modality that provideshigh reliability for diagnosing the depth and boundary of invasion incolorectal cancer. However, the lack of a large-scale ERUS dataset withhigh-quality annotations hinders the development of automatic ultrasounddiagnostics. In this paper, we collected and annotated the first benchmarkdataset that covers diverse ERUS scenarios, i.e. colorectal cancersegmentation, detection, and infiltration depth staging. Our ERUS-10K datasetcomprises 77 videos and 10,000 high-resolution annotated frames. Based on thisdataset, we further introduce a benchmark model for colorectal cancersegmentation, named the Adaptive Sparse-context TRansformer (ASTR). ASTR isdesigned based on three considerations: scanning mode discrepancy, temporalinformation, and low computational complexity. For generalizing to differentscanning modes, the adaptive scanning-mode augmentation is proposed to convertbetween raw sector images and linear scan ones. For mining temporalinformation, the sparse-context transformer is incorporated to integrateinter-frame local and global features. For reducing computational complexity,the sparse-context block is introduced to extract contextual features fromauxiliary frames. Finally, on the benchmark dataset, the proposed ASTR modelachieves a 77.6% Dice score in rectal cancer segmentation, largelyoutperforming previous state-of-the-art methods.</description><author>Yuncheng Jiang, Yiwen Hu, Zixun Zhang, Jun Wei, Chun-Mei Feng, Xuemei Tang, Xiang Wan, Yong Liu, Shuguang Cui, Zhen Li</author><pubDate>Mon, 19 Aug 2024 15:04:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10067v1</guid></item><item><title>Spatial-Frequency Dual Progressive Attention Network For Medical Image Segmentation</title><link>http://arxiv.org/abs/2406.07952v2</link><description>In medical images, various types of lesions often manifest significantdifferences in their shape and texture. Accurate medical image segmentationdemands deep learning models with robust capabilities in multi-scale andboundary feature learning. However, previous networks still have limitations inaddressing the above issues. Firstly, previous networks simultaneously fusemulti-level features or employ deep supervision to enhance multi-scalelearning. However, this may lead to feature redundancy and excessivecomputational overhead, which is not conducive to network training and clinicaldeployment. Secondly, the majority of medical image segmentation networksexclusively learn features in the spatial domain, disregarding the abundantglobal information in the frequency domain. This results in a bias towardslow-frequency components, neglecting crucial high-frequency information. Toaddress these problems, we introduce SF-UNet, a spatial-frequency dual-domainattention network. It comprises two main components: the Multi-scaleProgressive Channel Attention (MPCA) block, which progressively extractmulti-scale features across adjacent encoder layers, and the lightweightFrequency-Spatial Attention (FSA) block, with only 0.05M parameters, enablingconcurrent learning of texture and boundary features from both spatial andfrequency domains. We validate the effectiveness of the proposed SF-UNet onthree public datasets. Experimental results show that compared to previousstate-of-the-art (SOTA) medical image segmentation networks, SF-UNet achievesthe best performance, and achieves up to 9.4\% and 10.78\% improvement in DSCand IOU. Codes will be released at https://github.com/nkicsl/SF-UNet.</description><author>Zhenhuan Zhou, Along He, Yanlin Wu, Rui Yao, Xueshuo Xie, Tao Li</author><pubDate>Mon, 19 Aug 2024 14:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07952v2</guid></item><item><title>Facial Wrinkle Segmentation for Cosmetic Dermatology: Pretraining with Texture Map-Based Weak Supervision</title><link>http://arxiv.org/abs/2408.10060v1</link><description>Facial wrinkle detection plays a crucial role in cosmetic dermatology.Precise manual segmentation of facial wrinkles is challenging andtime-consuming, with inherent subjectivity leading to inconsistent resultsamong graders. To address this issue, we propose two solutions. First, we buildand release the first public facial wrinkle dataset, `FFHQ-Wrinkle', anextension of the NVIDIA FFHQ dataset. This dataset includes 1,000 images withhuman labels and 50,000 images with automatically generated weak labels. Thisdataset can foster the research community to develop advanced wrinkle detectionalgorithms. Second, we introduce a training strategy for U-Net-likeencoder-decoder models to detect wrinkles across the face automatically. Ourmethod employs a two-stage training strategy: texture map pretraining andfinetuning on human-labeled data. Initially, we pretrain models on a largedataset with weak labels (N=50k) or masked texture maps generated throughcomputer vision techniques, without human intervention. Subsequently, wefinetune the models using human-labeled data (N=1k), which consists of manuallylabeled wrinkle masks. During finetuning, the network inputs a combination ofRGB and masked texture maps, comprising four channels. We effectively combinelabels from multiple annotators to minimize subjectivity in manual labeling.Our strategies demonstrate improved segmentation performance in facial wrinklesegmentation both quantitatively and visually compared to existing pretrainingmethods.</description><author>Junho Moon, Haejun Chung, Ikbeom Jang</author><pubDate>Mon, 19 Aug 2024 14:54:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10060v1</guid></item><item><title>Efficient Exploration in Deep Reinforcement Learning: A Novel Bayesian Actor-Critic Algorithm</title><link>http://arxiv.org/abs/2408.10055v1</link><description>Reinforcement learning (RL) and Deep Reinforcement Learning (DRL), inparticular, have the potential to disrupt and are already changing the way weinteract with the world. One of the key indicators of their applicability istheir ability to scale and work in real-world scenarios, that is in large-scaleproblems. This scale can be achieved via a combination of factors, thealgorithm's ability to make use of large amounts of data and computationalresources and the efficient exploration of the environment for viable solutions(i.e. policies). In this work, we investigate and motivate some theoretical foundations fordeep reinforcement learning. We start with exact dynamic programming and workour way up to stochastic approximations and stochastic approximations for amodel-free scenario, which forms the theoretical basis of modern reinforcementlearning. We present an overview of this highly varied and rapidly changingfield from the perspective of Approximate Dynamic Programming. We then focusour study on the short-comings with respect to exploration of the cornerstoneapproaches (i.e. DQN, DDQN, A2C) in deep reinforcement learning. On the theoryside, our main contribution is the proposal of a novel Bayesian actor-criticalgorithm. On the empirical side, we evaluate Bayesian exploration as well asactor-critic algorithms on standard benchmarks as well as state-of-the-artevaluation suites and show the benefits of both of these approaches overcurrent state-of-the-art deep RL methods. We release all the implementationsand provide a full python library that is easy to install and hopefully willserve the reinforcement learning community in a meaningful way, and provide astrong foundation for future work.</description><author>Nikolai Rozanov</author><pubDate>Mon, 19 Aug 2024 14:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10055v1</guid></item><item><title>Privacy Checklist: Privacy Violation Detection Grounding on Contextual Integrity Theory</title><link>http://arxiv.org/abs/2408.10053v1</link><description>Privacy research has attracted wide attention as individuals worry that theirprivate data can be easily leaked during interactions with smart devices,social platforms, and AI applications. Computer science researchers, on theother hand, commonly study privacy issues through privacy attacks and defenseson segmented fields. Privacy research is conducted on various sub-fields,including Computer Vision (CV), Natural Language Processing (NLP), and ComputerNetworks. Within each field, privacy has its own formulation. Though pioneeringworks on attacks and defenses reveal sensitive privacy issues, they arenarrowly trapped and cannot fully cover people's actual privacy concerns.Consequently, the research on general and human-centric privacy researchremains rather unexplored. In this paper, we formulate the privacy issue as areasoning problem rather than simple pattern matching. We ground on theContextual Integrity (CI) theory which posits that people's perceptions ofprivacy are highly correlated with the corresponding social context. Based onsuch an assumption, we develop the first comprehensive checklist that coverssocial identities, private attributes, and existing privacy regulations. Unlikeprior works on CI that either cover limited expert annotated norms or modelincomplete social context, our proposed privacy checklist uses the whole HealthInsurance Portability and Accountability Act of 1996 (HIPAA) as an example, toshow that we can resort to large language models (LLMs) to completely cover theHIPAA's regulations. Additionally, our checklist also gathers expertannotations across multiple ontologies to determine private informationincluding but not limited to personally identifiable information (PII). We useour preliminary results on the HIPAA to shed light on future context-centricprivacy research to cover more privacy regulations, social norms and standards.</description><author>Haoran Li, Wei Fan, Yulin Chen, Jiayang Cheng, Tianshu Chu, Xuebing Zhou, Peizhao Hu, Yangqiu Song</author><pubDate>Mon, 19 Aug 2024 14:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10053v1</guid></item><item><title>UniMem: Towards a Unified View of Long-Context Large Language Models</title><link>http://arxiv.org/abs/2402.03009v2</link><description>Long-context processing is a critical ability that constrains theapplicability of large language models (LLMs). Although there exist variousmethods devoted to enhancing the long-context processing ability of LLMs, theyare developed in an isolated manner and lack systematic analysis andintegration of their strengths, hindering further developments. In this paper,we introduce UniMem, a Unified framework that reformulates existinglong-context methods from the view of Memory augmentation of LLMs.Distinguished by its four core dimensions-Memory Management, Memory Writing,Memory Reading, and Memory Injection, UniMem empowers researchers to conductsystematic exploration of long-context methods. We re-formulate 16 existingmethods based on UniMem and analyze four representative methods:Transformer-XL, Memorizing Transformer, RMT, and Longformer into equivalentUniMem forms to reveal their design principles and strengths. Based on theseanalyses, we propose UniMix, an innovative approach that integrates thestrengths of these algorithms. Experimental results show that UniMix achievessuperior performance in handling long contexts with significantly lowerperplexity than baselines.</description><author>Junjie Fang, Likai Tang, Hongzhe Bi, Yujia Qin, Si Sun, Zhenyu Li, Haolun Li, Yongjian Li, Xin Cong, Yankai Lin, Yukun Yan, Xiaodong Shi, Sen Song, Zhiyuan Liu, Maosong Sun</author><pubDate>Mon, 19 Aug 2024 14:47:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03009v2</guid></item><item><title>Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online Coverage Path Planning</title><link>http://arxiv.org/abs/2406.04920v2</link><description>Sim-to-real transfer presents a difficult challenge, where models trained insimulation are to be deployed in the real world. The distribution shift betweenthe two settings leads to biased representations of the dynamics, and thus tosuboptimal predictions in the real-world environment. In this work, we tacklethe challenge of sim-to-real transfer of reinforcement learning (RL) agents forcoverage path planning (CPP). In CPP, the task is for a robot to find a paththat covers every point of a confined area. Specifically, we consider the casewhere the environment is unknown, and the agent needs to plan the path onlinewhile mapping the environment. We bridge the sim-to-real gap through asemi-virtual environment, including a real robot and real-time aspects, whileutilizing a simulated sensor and obstacles to enable environment randomizationand automated episode resetting. We investigate what level of fine-tuning isneeded for adapting to a realistic setting, comparing to an agent trainedsolely in simulation. We find that a high inference frequency allowsfirst-order Markovian policies to transfer directly from simulation, whilehigher-order policies can be fine-tuned to further reduce the sim-to-real gap.Moreover, they can operate at a lower frequency, thus reducing computationalrequirements. In both cases, our approaches transfer state-of-the-art resultsfrom simulation to the real domain, where direct learning would take in theorder of weeks with manual interaction, that is, it would be completelyinfeasible.</description><author>Arvi Jonnarth, Ola Johansson, Michael Felsberg</author><pubDate>Mon, 19 Aug 2024 14:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04920v2</guid></item><item><title>MagicFace: Training-free Universal-Style Human Image Customized Synthesis</title><link>http://arxiv.org/abs/2408.07433v3</link><description>Current state-of-the-art methods for human image customized synthesistypically require tedious training on large-scale datasets. In such cases, theyare prone to overfitting and struggle to personalize individuals of unseenstyles. Moreover, these methods extensively focus on single-concept human imagesynthesis and lack the flexibility needed for customizing individuals withmultiple given concepts, thereby impeding their broader practical application.To this end, we propose MagicFace, a novel training-free method foruniversal-style human image personalized synthesis, enabling multi-conceptcustomization by accurately integrating reference concept features into theirlatent generated region at the pixel level. Specifically, MagicFace introducesa coarse-to-fine generation pipeline, involving two sequential stages: semanticlayout construction and concept feature injection. This is achieved by ourReference-aware Self-Attention (RSA) and Region-grouped Blend Attention (RBA)mechanisms. In the first stage, RSA enables the latent image to query featuresfrom all reference concepts simultaneously, extracting the overall semanticunderstanding to facilitate the initial semantic layout establishment. In thesecond stage, we employ an attention-based semantic segmentation method topinpoint the latent generated regions of all concepts at each step. Followingthis, RBA divides the pixels of the latent image into semantic groups, witheach group querying fine-grained features from the corresponding referenceconcept, which ensures precise attribute alignment and feature injection.Throughout the generation process, a weighted mask strategy is employed toensure the model focuses more on the reference concepts. Extensive experimentsdemonstrate the superiority of MagicFace in both human-centric subject-to-imagesynthesis and multi-concept human image customization.</description><author>Yibin Wang, Weizhong Zhang, Cheng Jin</author><pubDate>Mon, 19 Aug 2024 14:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07433v3</guid></item><item><title>A fuzzy loss for ontology classification</title><link>http://arxiv.org/abs/2405.02083v2</link><description>Deep learning models are often unaware of the inherent constraints of thetask they are applied to. However, many downstream tasks require logicalconsistency. For ontology classification tasks, such constraints includesubsumption and disjointness relations between classes. In order to increase the consistency of deep learning models, we propose afuzzy loss that combines label-based loss with terms penalising subsumption- ordisjointness-violations. Our evaluation on the ChEBI ontology shows that thefuzzy loss is able to decrease the number of consistency violations by severalorders of magnitude without decreasing the classification performance. Inaddition, we use the fuzzy loss for unsupervised learning. We show that thiscan further improve consistency on data from a</description><author>Simon Fl√ºgel, Martin Glauer, Till Mossakowski, Fabian Neuhaus</author><pubDate>Mon, 19 Aug 2024 14:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02083v2</guid></item><item><title>Revisiting Day-ahead Electricity Price: Simple Model Save Millions</title><link>http://arxiv.org/abs/2405.14893v2</link><description>Accurate day-ahead electricity price forecasting is essential for residentialwelfare, yet current methods often fall short in forecast accuracy. We observethat commonly used time series models struggle to utilize the prior correlationbetween price and demand-supply, which, we found, can contribute a lot to areliable electricity price forecaster. Leveraging this prior, we propose asimple piecewise linear model that significantly enhances forecast accuracy bydirectly deriving prices from readily forecastable demand-supply values.Experiments in the day-ahead electricity markets of Shanxi province and ISO NewEngland reveal that such forecasts could potentially save residents millions ofdollars a year compared to existing methods. Our findings underscore the valueof suitably integrating time series modeling with economic prior for enhancedelectricity price forecasting accuracy.</description><author>Linian Wang, Jianghong Liu, Huibin Zhang, Leye Wang</author><pubDate>Mon, 19 Aug 2024 14:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14893v2</guid></item><item><title>Exploiting Fine-Grained Prototype Distribution for Boosting Unsupervised Class Incremental Learning</title><link>http://arxiv.org/abs/2408.10046v1</link><description>The dynamic nature of open-world scenarios has attracted more attention toclass incremental learning (CIL). However, existing CIL methods typicallypresume the availability of complete ground-truth labels throughout thetraining process, an assumption rarely met in practical applications.Consequently, this paper explores a more challenging problem of unsupervisedclass incremental learning (UCIL). The essence of addressing this problem liesin effectively capturing comprehensive feature representations and discoveringunknown novel classes. To achieve this, we first model the knowledge of classdistribution by exploiting fine-grained prototypes. Subsequently, a granularityalignment technique is introduced to enhance the unsupervised class discovery.Additionally, we proposed a strategy to minimize overlap between novel andexisting classes, thereby preserving historical knowledge and mitigating thephenomenon of catastrophic forgetting. Extensive experiments on the fivedatasets demonstrate that our approach significantly outperforms currentstate-of-the-art methods, indicating the effectiveness of the proposed method.</description><author>Jiaming Liu, Hongyuan Liu, Zhili Qin, Wei Han, Yulu Fan, Qinli Yang, Junming Shao</author><pubDate>Mon, 19 Aug 2024 14:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10046v1</guid></item><item><title>GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent</title><link>http://arxiv.org/abs/2305.03515v7</link><description>Decision Trees (DTs) are commonly used for many machine learning tasks due totheir high degree of interpretability. However, learning a DT from data is adifficult optimization problem, as it is non-convex and non-differentiable.Therefore, common approaches learn DTs using a greedy growth algorithm thatminimizes the impurity locally at each internal node. Unfortunately, thisgreedy procedure can lead to inaccurate trees. In this paper, we present anovel approach for learning hard, axis-aligned DTs with gradient descent. Theproposed method uses backpropagation with a straight-through operator on adense DT representation, to jointly optimize all tree parameters. Our approachoutperforms existing methods on binary classification benchmarks and achievescompetitive results for multi-class tasks. The method is available under:https://github.com/s-marton/GradTree</description><author>Sascha Marton, Stefan L√ºdtke, Christian Bartelt, Heiner Stuckenschmidt</author><pubDate>Mon, 19 Aug 2024 14:34:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03515v7</guid></item><item><title>Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane Representation</title><link>http://arxiv.org/abs/2408.10041v1</link><description>Recent advancements in photo-realistic novel view synthesis have beensignificantly driven by Gaussian Splatting (3DGS). Nevertheless, the explicitnature of 3DGS data entails considerable storage requirements, highlighting apressing need for more efficient data representations. To address this, wepresent Implicit Gaussian Splatting (IGS), an innovative hybrid model thatintegrates explicit point clouds with implicit feature embeddings through amulti-level tri-plane architecture. This architecture features 2D feature gridsat various resolutions across different levels, facilitating continuous spatialdomain representation and enhancing spatial correlations among Gaussianprimitives. Building upon this foundation, we introduce a level-basedprogressive training scheme, which incorporates explicit spatialregularization. This method capitalizes on spatial correlations to enhance boththe rendering quality and the compactness of the IGS representation.Furthermore, we propose a novel compression pipeline tailored for both pointclouds and 2D feature grids, considering the entropy variations acrossdifferent levels. Extensive experimental evaluations demonstrate that ouralgorithm can deliver high-quality rendering using only a few MBs, effectivelybalancing storage efficiency and rendering fidelity, and yielding results thatare competitive with the state-of-the-art.</description><author>Minye Wu, Tinne Tuytelaars</author><pubDate>Mon, 19 Aug 2024 14:34:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10041v1</guid></item><item><title>TED: Accelerate Model Training by Internal Generalization</title><link>http://arxiv.org/abs/2405.03228v2</link><description>Large language models have demonstrated strong performance in recent years,but the high cost of training drives the need for efficient methods to compressdataset sizes. We propose TED pruning, a method that addresses the challenge ofoverfitting under high pruning ratios by quantifying the model's ability toimprove performance on pruned data while fitting retained data, known asInternal Generalization (IG). TED uses an optimization objective based onInternal Generalization Distance (IGD), measuring changes in IG before andafter pruning to align with true generalization performance and achieveimplicit regularization. The IGD optimization objective was verified to allowthe model to achieve the smallest upper bound on generalization error. Theimpact of small mask fluctuations on IG is studied through masks and Taylorapproximation, and fast estimation of IGD is enabled. In analyzing continuoustraining dynamics, the prior effect of IGD is validated, and a progressivepruning strategy is proposed. Experiments on image classification, naturallanguage understanding, and large language model fine-tuning show TED achieveslossless performance with 60-70\% of the data. Upon acceptance, our code willbe made publicly available.</description><author>Jinying Xiao, Ping Li, Jie Nie</author><pubDate>Mon, 19 Aug 2024 14:33:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03228v2</guid></item><item><title>The Practimum-Optimum Algorithm for Manufacturing Scheduling: A Paradigm Shift Leading to Breakthroughs in Scale and Performance</title><link>http://arxiv.org/abs/2408.10040v1</link><description>The Practimum-Optimum (P-O) algorithm represents a paradigm shift indeveloping automatic optimization products for complex real-life businessproblems such as large-scale manufacturing scheduling. It leverages deepbusiness domain expertise to create a group of virtual human expert (VHE)agents with different "schools of thought" on how to create high-qualityschedules. By computerizing them into algorithms, P-O generates many validschedules at far higher speeds than human schedulers are capable of. Initially,these schedules can also be local optimum peaks far away from high-qualityschedules. By submitting these schedules to a reinforced machine learningalgorithm (RL), P-O learns the weaknesses and strengths of each VHE schedule,and accordingly derives reward and punishment changes in the Demand Set thatwill modify the relative priorities for time and resource allocation that jobsreceived in the prior iteration that led to the current state of the schedule.These cause the core logic of the VHE algorithms to explore, in the subsequentiteration, substantially different parts of the schedules universe andpotentially find higher-quality schedules. Using the hill climbing analogy,this may be viewed as a big jump, shifting from a given local peak to a farawaypromising start point equipped with knowledge embedded in the demand set forfuture iterations. This is a fundamental difference from most contemporaryalgorithms, which spend considerable time on local micro-steps restricted tothe neighbourhoods of local peaks they visit. This difference enables abreakthrough in scale and performance for fully automatic manufacturingscheduling in complex organizations. The P-O algorithm is at the heart ofPlataine Scheduler that, in one click, routinely schedules 30,000-50,000 tasksfor real-life complex manufacturing operations.</description><author>Moshe BenBassat</author><pubDate>Mon, 19 Aug 2024 14:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10040v1</guid></item><item><title>MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis</title><link>http://arxiv.org/abs/2408.10039v1</link><description>Clinical diagnosis is critical in medical practice, typically requiring acontinuous and evolving process that includes primary diagnosis, differentialdiagnosis, and final diagnosis. However, most existing clinical diagnostictasks are single-step processes, which does not align with the complexmulti-step diagnostic procedures found in real-world clinical settings. In thispaper, we propose a multi-step diagnostic task and annotate a clinicaldiagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,differential diagnosis, and final diagnosis questions. Additionally, we proposea novel and effective framework. This framework combines forward inference,backward inference, reflection, and refinement, enabling the LLM toself-evaluate and adjust its diagnostic results. To assess the effectiveness ofour proposed method, we design and conduct extensive experiments. Theexperimental results demonstrate the effectiveness of the proposed method. Wealso provide a comprehensive experimental analysis and suggest future researchdirections for this task.</description><author>Ruihui Hou, Shencheng Chen, Yongqi Fan, Lifeng Zhu, Jing Sun, Jingping Liu, Tong Ruan</author><pubDate>Mon, 19 Aug 2024 14:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10039v1</guid></item><item><title>SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition</title><link>http://arxiv.org/abs/2408.10037v1</link><description>Hand pose represents key information for action recognition in the egocentricperspective, where the user is interacting with objects. We propose to improveegocentric 3D hand pose estimation based on RGB frames only by usingpseudo-depth images. Incorporating state-of-the-art single RGB image depthestimation techniques, we generate pseudo-depth representations of the framesand use distance knowledge to segment irrelevant parts of the scene. Theresulting depth maps are then used as segmentation masks for the RGB frames.Experimental results on H2O Dataset confirm the high accuracy of the estimatedpose with our method in an action recognition task. The 3D hand pose, togetherwith information from object detection, is processed by a transformer-basedaction recognition network, resulting in an accuracy of 91.73%, outperformingall state-of-the-art methods. Estimations of 3D hand pose result in competitiveperformance with existing methods with a mean pose error of 28.66 mm. Thismethod opens up new possibilities for employing distance information inegocentric 3D hand pose estimation without relying on depth sensors.</description><author>Wiktor Mucha, Michael Wray, Martin Kampel</author><pubDate>Mon, 19 Aug 2024 14:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10037v1</guid></item><item><title>Dynamic Label Injection for Imbalanced Industrial Defect Segmentation</title><link>http://arxiv.org/abs/2408.10031v1</link><description>In this work, we propose a simple yet effective method to tackle the problemof imbalanced multi-class semantic segmentation in deep learning systems. Oneof the key properties for a good training set is the balancing among theclasses. When the input distribution is heavily imbalanced in the number ofinstances, the learning process could be hindered or difficult to carry on. Tothis end, we propose a Dynamic Label Injection (DLI) algorithm to impose auniform distribution in the input batch. Our algorithm computes the currentbatch defect distribution and re-balances it by transferring defects using acombination of Poisson-based seamless image cloning and cut-paste techniques. Athorough experimental section on the Magnetic Tiles dataset shows betterresults of DLI compared to other balancing loss approaches also in thechallenging weakly-supervised setup. The code is available athttps://github.com/covisionlab/dynamic-label-injection.git</description><author>Emanuele Caruso, Francesco Pelosin, Alessandro Simoni, Marco Boschetti</author><pubDate>Mon, 19 Aug 2024 14:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10031v1</guid></item><item><title>Collaborative Multi-source Domain Adaptation Through Optimal Transport</title><link>http://arxiv.org/abs/2404.06599v3</link><description>Multi-source Domain Adaptation (MDA) seeks to adapt models trained on datafrom multiple labeled source domains to perform effectively on an unlabeledtarget domain data, assuming access to sources data. To address the challengesof model adaptation and data privacy, we introduce Collaborative MDA ThroughOptimal Transport (CMDA-OT), a novel framework consisting of two key phases. Inthe first phase, each source domain is independently adapted to the targetdomain using optimal transport methods. In the second phase, a centralizedcollaborative learning architecture is employed, which aggregates the N modelsfrom the N sources without accessing their data, thereby safeguarding privacy.During this process, the server leverages a small set of pseudo-labeled samplesfrom the target domain, known as the target validation subset, to refine andguide the adaptation. This dual-phase approach not only improves modelperformance on the target domain but also addresses vital privacy challengesinherent in domain adaptation.</description><author>Omar Ghannou, Youn√®s Bennani</author><pubDate>Mon, 19 Aug 2024 14:24:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06599v3</guid></item><item><title>Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations</title><link>http://arxiv.org/abs/2403.06009v3</link><description>Large language models (LLMs) are susceptible to a variety of risks, fromnon-faithful output to biased and toxic generations. Due to several limitingfactors surrounding LLMs (training cost, API access, data availability, etc.),it may not always be feasible to impose direct safety constraints on a deployedmodel. Therefore, an efficient and reliable alternative is required. To thisend, we present our ongoing efforts to create and deploy a library ofdetectors: compact and easy-to-build classification models that provide labelsfor various harms. In addition to the detectors themselves, we discuss a widerange of uses for these detector models - from acting as guardrails to enablingeffective AI governance. We also deep dive into inherent challenges in theirdevelopment and discuss future work aimed at making the detectors more reliableand broadening their scope.</description><author>Swapnaja Achintalwar, Adriana Alvarado Garcia, Ateret Anaby-Tavor, Ioana Baldini, Sara E. Berger, Bishwaranjan Bhattacharjee, Djallel Bouneffouf, Subhajit Chaudhury, Pin-Yu Chen, Lamogha Chiazor, Elizabeth M. Daly, Kirushikesh DB, Rog√©rio Abreu de Paula, Pierre Dognin, Eitan Farchi, Soumya Ghosh, Michael Hind, Raya Horesh, George Kour, Ja Young Lee, Nishtha Madaan, Sameep Mehta, Erik Miehling, Keerthiram Murugesan, Manish Nagireddy, Inkit Padhi, David Piorkowski, Ambrish Rawat, Orna Raz, Prasanna Sattigeri, Hendrik Strobelt, Sarathkrishna Swaminathan, Christoph Tillmann, Aashka Trivedi, Kush R. Varshney, Dennis Wei, Shalisha Witherspooon, Marcel Zalmanovici</author><pubDate>Mon, 19 Aug 2024 14:24:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06009v3</guid></item><item><title>In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model</title><link>http://arxiv.org/abs/2403.06126v2</link><description>Current pre-trained vision-language models, such as CLIP, have demonstratedremarkable zero-shot generalization capabilities across various downstreamtasks. However, their performance significantly degrades when test inputsexhibit different distributions. In this paper, we explore the concept oftest-time prompt tuning (TTPT), which facilitates the adaptation of the CLIPmodel to novel downstream tasks through a one-step unsupervised optimizationthat involves only test samples. Inspired by in-context learning in naturallanguage processing (NLP), we propose In-Context Prompt Learning (InCPL) fortest-time visual recognition tasks, which empowers a pre-trainedvision-language model with labeled examples as context information ondownstream task. Specifically, InCPL associates a new test sample with very fewlabeled examples (sometimes just one) as context information, enabling reliablelabel estimation for the test sample and facilitating model adaptation. Toachieve this, InCPL employs an efficient language-to-vision translator toexplore the textual prior information for visual prompt learning. Further, weintroduce a context-aware unsupervised loss to optimize visual prompts tailoredto test samples. Finally, we design a cyclic learning strategy for visual andtextual prompts to ensure mutual synergy across different modalities. Thisenables a pre-trained, frozen CLIP model to adapt to any task using its learnedadaptive prompt. Our method demonstrates superior performance and achievesstate-of-the-art results across various downstream datasets.</description><author>Junhui Yin, Xinyu Zhang, Lin Wu, Xiaojie Wang</author><pubDate>Mon, 19 Aug 2024 14:22:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06126v2</guid></item><item><title>Towards Robust Federated Image Classification: An Empirical Study of Weight Selection Strategies in Manufacturing</title><link>http://arxiv.org/abs/2408.10024v1</link><description>In the realm of Federated Learning (FL), particularly within themanufacturing sector, the strategy for selecting client weights for serveraggregation is pivotal for model performance. This study investigates thecomparative effectiveness of two weight selection strategies: Final EpochWeight Selection (FEWS) and Optimal Epoch Weight Selection (OEWS). Designed formanufacturing contexts where collaboration typically involves a limited numberof partners (two to four clients), our research focuses on federated imageclassification tasks. We employ various neural network architectures, includingEfficientNet, ResNet, and VGG, to assess the impact of these weight selectionstrategies on model convergence and robustness. Our research aims to determine whether FEWS or OEWS enhances the global FLmodel's performance across communication rounds (CRs). Through empiricalanalysis and rigorous experimentation, we seek to provide valuable insights foroptimizing FL implementations in manufacturing, ensuring that collaborativeefforts yield the most effective and reliable models with a limited number ofparticipating clients. The findings from this study are expected to refine FLpractices significantly in manufacturing, thereby enhancing the efficiency andperformance of collaborative machine learning endeavors in this vital sector.</description><author>Vinit Hegiste, Tatjana Legler, Martin Ruskowski</author><pubDate>Mon, 19 Aug 2024 14:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10024v1</guid></item><item><title>Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks</title><link>http://arxiv.org/abs/2408.00359v2</link><description>Fine-tuning large pre-trained models is a common practice in machine learningapplications, yet its mathematical analysis remains largely unexplored. In thispaper, we study fine-tuning through the lens of memorization capacity. Our newmeasure, the Fine-Tuning Capacity (FTC), is defined as the maximum number ofsamples a neural network can fine-tune, or equivalently, as the minimum numberof neurons ($m$) needed to arbitrarily change $N$ labels among $K$ samplesconsidered in the fine-tuning process. In essence, FTC extends the memorizationcapacity concept to the fine-tuning scenario. We analyze FTC for the additivefine-tuning scenario where the fine-tuned network is defined as the summationof the frozen pre-trained network $f$ and a neural network $g$ (with $m$neurons) designed for fine-tuning. When $g$ is a ReLU network with either 2 or3 layers, we obtain tight upper and lower bounds on FTC; we show that $N$samples can be fine-tuned with $m=\Theta(N)$ neurons for 2-layer networks, andwith $m=\Theta(\sqrt{N})$ neurons for 3-layer networks, no matter how large $K$is. Our results recover the known memorization capacity results when $N = K$ asa special case.</description><author>Jy-yong Sohn, Dohyun Kwon, Seoyeon An, Kangwook Lee</author><pubDate>Mon, 19 Aug 2024 14:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00359v2</guid></item><item><title>Detecting Adversarial Attacks in Semantic Segmentation via Uncertainty Estimation: A Deep Analysis</title><link>http://arxiv.org/abs/2408.10021v1</link><description>Deep neural networks have demonstrated remarkable effectiveness across a widerange of tasks such as semantic segmentation. Nevertheless, these networks arevulnerable to adversarial attacks that add imperceptible perturbations to theinput image, leading to false predictions. This vulnerability is particularlydangerous in safety-critical applications like automated driving. Whileadversarial examples and defense strategies are well-researched in the contextof image classification, there is comparatively less research focused onsemantic segmentation. Recently, we have proposed an uncertainty-based methodfor detecting adversarial attacks on neural networks for semantic segmentation.We observed that uncertainty, as measured by the entropy of the outputdistribution, behaves differently on clean versus adversely perturbed images,and we utilize this property to differentiate between the two. In this extendedversion of our work, we conduct a detailed analysis of uncertainty-baseddetection of adversarial attacks including a diverse set of adversarial attacksand various state-of-the-art neural networks. Our numerical experiments showthe effectiveness of the proposed uncertainty-based detection method, which islightweight and operates as a post-processing step, i.e., no modelmodifications or knowledge of the adversarial example generation process arerequired.</description><author>Kira Maag, Roman Resner, Asja Fischer</author><pubDate>Mon, 19 Aug 2024 14:13:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10021v1</guid></item><item><title>Deterministic Policy Gradient Primal-Dual Methods for Continuous-Space Constrained MDPs</title><link>http://arxiv.org/abs/2408.10015v1</link><description>We study the problem of computing deterministic optimal policies forconstrained Markov decision processes (MDPs) with continuous state and actionspaces, which are widely encountered in constrained dynamical systems.Designing deterministic policy gradient methods in continuous state and actionspaces is particularly challenging due to the lack of enumerable state-actionpairs and the adoption of deterministic policies, hindering the application ofexisting policy gradient methods for constrained MDPs. To this end, we developa deterministic policy gradient primal-dual method to find an optimaldeterministic policy with non-asymptotic convergence. Specifically, we leverageregularization of the Lagrangian of the constrained MDP to propose adeterministic policy gradient primal-dual (D-PGPD) algorithm that updates thedeterministic policy via a quadratic-regularized gradient ascent step and thedual variable via a quadratic-regularized gradient descent step. We prove thatthe primal-dual iterates of D-PGPD converge at a sub-linear rate to an optimalregularized primal-dual pair. We instantiate D-PGPD with function approximationand prove that the primal-dual iterates of D-PGPD converge at a sub-linear rateto an optimal regularized primal-dual pair, up to a function approximationerror. Furthermore, we demonstrate the effectiveness of our method in twocontinuous control problems: robot navigation and fluid control. To the best ofour knowledge, this appears to be the first work that proposes a deterministicpolicy search method for continuous-space constrained MDPs.</description><author>Sergio Rozada, Dongsheng Ding, Antonio G. Marques, Alejandro Ribeiro</author><pubDate>Mon, 19 Aug 2024 14:11:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10015v1</guid></item></channel></rss>