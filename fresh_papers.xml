<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 13 Mar 2024 14:00:17 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</title><link>http://arxiv.org/abs/2402.13254v2</link><description>We propose CounterCurate, a framework to comprehensively improve thevisio-linguistic compositional reasoning capability for both contrastive andgenerative multimodal models. In particular, we identify two criticalunder-explored problems: the neglect of the physically grounded reasoning(counting and position understanding) and the potential of using highly capabletext and image generation models for semantic counterfactual fine-tuning. Ourwork pioneers an approach that addresses these gaps. We first spotlight thenear-chance performance of multimodal models like CLIP and LLaVA in physicallygrounded compositional reasoning. We then apply simple data augmentation usinggrounded image generation model GLIGEN to generate fine-tuning data, resultingin significant performance improvements: +33% and +37% for CLIP and LLaVA,respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, weexploit the capabilities of high-performing text generation and imagegeneration models, specifically GPT-4V and DALLE-3, to curate challengingsemantic counterfactuals, thereby further enhancing compositional reasoningcapabilities on benchmarks such as SugarCrepe, where CounterCurate outperformsGPT-4V.</description><author>Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee</author><pubDate>Tue, 12 Mar 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13254v2</guid></item><item><title>Beyond Text: Frozen Large Language Models in Visual Signal Comprehension</title><link>http://arxiv.org/abs/2403.07874v1</link><description>In this work, we investigate the potential of a large language model (LLM) todirectly comprehend visual signals without the necessity of fine-tuning onmulti-modal datasets. The foundational concept of our method views an image asa linguistic entity, and translates it to a set of discrete words derived fromthe LLM's vocabulary. To achieve this, we present the Vision-to-LanguageTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a``foreign language'' with the combined aid of an encoder-decoder, the LLMvocabulary, and a CLIP model. With this innovative image encoding, the LLMgains the ability not only for visual comprehension but also for imagedenoising and restoration in an auto-regressive fashion-crucially, without anyfine-tuning. We undertake rigorous experiments to validate our method,encompassing understanding tasks like image recognition, image captioning, andvisual question answering, as well as image denoising tasks like inpainting,outpainting, deblurring, and shift restoration. Code and models are availableat https://github.com/zh460045050/V2L-Tokenizer.</description><author>Lei Zhu, Fangyun Wei, Yanye Lu</author><pubDate>Tue, 12 Mar 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07874v1</guid></item><item><title>Rethinking Generative Large Language Model Evaluation for Semantic Comprehension</title><link>http://arxiv.org/abs/2403.07872v1</link><description>Despite their sophisticated capabilities, large language models (LLMs)encounter a major hurdle in effective assessment. This paper first revisits theprevalent evaluation method-multiple choice question answering (MCQA), whichallows for straightforward accuracy measurement. Through a comprehensiveevaluation of 24 models across 11 benchmarks, we highlight several potentialdrawbacks of MCQA, for instance, the inconsistency between the MCQA evaluationand the generation of open-ended responses in practical scenarios. In response,we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, withGPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. Thissystem is designed to mirror real-world usage, and for this purpose, we havecompiled a new benchmark called ``Real-world questions'' (RWQ), comprising20,772 authentic user inquiries. Additionally, we thoroughly analyze thecharacteristics of our system and compare it with prior leaderboards likeAlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elosystem, the feasibility of registering new models, and its potential to reshapeLLM leaderboards.</description><author>Fangyun Wei, Xi Chen, Lin Luo</author><pubDate>Tue, 12 Mar 2024 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07872v1</guid></item><item><title>TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation</title><link>http://arxiv.org/abs/2403.07869v1</link><description>A critical bottleneck limiting imitation learning in robotics is the lack ofdata. This problem is more severe in mobile manipulation, where collectingdemonstrations is harder than in stationary manipulation due to the lack ofavailable and easy-to-use teleoperation interfaces. In this work, wedemonstrate TeleMoMa, a general and modular interface for whole-bodyteleoperation of mobile manipulators. TeleMoMa unifies multiple humaninterfaces including RGB and depth cameras, virtual reality controllers,keyboard, joysticks, etc., and any combination thereof. In its more accessibleversion, TeleMoMa works using simply vision (e.g., an RGB-D camera), loweringthe entry bar for humans to provide mobile manipulation demonstrations. Wedemonstrate the versatility of TeleMoMa by teleoperating several existingmobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation andthe real world. We demonstrate the quality of the demonstrations collected withTeleMoMa by training imitation learning policies for mobile manipulation tasksinvolving synchronized whole-body motion. Finally, we also show that TeleMoMa'steleoperation channel enables teleoperation on site, looking at the robot, orremote, sending commands and observations through a computer network, andperform user studies to evaluate how easy it is for novice users to learn tocollect demonstrations with different combinations of human interfaces enabledby our system. We hope TeleMoMa becomes a helpful tool for the communityenabling researchers to collect whole-body mobile manipulation demonstrations.For more information and video results,https://robin-lab.cs.utexas.edu/telemoma-web.</description><author>Shivin Dass, Wensi Ai, Yuqian Jiang, Samik Singh, Jiaheng Hu, Ruohan Zhang, Peter Stone, Ben Abbatematteo, Roberto Martin-Martin</author><pubDate>Tue, 12 Mar 2024 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07869v1</guid></item><item><title>Exploring Safety Generalization Challenges of Large Language Models via Code</title><link>http://arxiv.org/abs/2403.07865v1</link><description>The rapid advancement of Large Language Models (LLMs) has brought aboutremarkable capabilities in natural language processing but also raised concernsabout their potential misuse. While strategies like supervised fine-tuning andreinforcement learning from human feedback have enhanced their safety, thesemethods primarily focus on natural languages, which may not generalize to otherdomains. This paper introduces CodeAttack, a framework that transforms naturallanguage inputs into code inputs, presenting a novel environment for testingthe safety generalization of LLMs. Our comprehensive studies onstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal acommon safety vulnerability of these models against code input: CodeAttackconsistently bypasses the safety guardrails of all models more than 80\% of thetime. Furthermore, we find that a larger distribution gap between CodeAttackand natural language leads to weaker safety generalization, such as encodingnatural language input with data structures or using less popular programminglanguages. These findings highlight new safety risks in the code domain and theneed for more robust safety alignment algorithms to match the code capabilitiesof LLMs.</description><author>Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma</author><pubDate>Tue, 12 Mar 2024 18:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07865v1</guid></item><item><title>Low coordinate degree algorithms I: Universality of computational thresholds for hypothesis testing</title><link>http://arxiv.org/abs/2403.07862v1</link><description>We study when low coordinate degree functions (LCDF) -- linear combinationsof functions depending on small subsets of entries of a vector -- canhypothesis test between high-dimensional probability measures. These functionsare a generalization, proposed in Hopkins' 2018 thesis but seldom studiedsince, of low degree polynomials (LDP), a class widely used in recentliterature as a proxy for all efficient algorithms for tasks in statistics andoptimization. Instead of the orthogonal polynomial decompositions used in LDPcalculations, our analysis of LCDF is based on the Efron-Stein or ANOVAdecomposition, making it much more broadly applicable. By way of illustration,we prove channel universality for the success of LCDF in testing for thepresence of sufficiently "dilute" random signals through noisy channels: theefficacy of LCDF depends on the channel only through the scalar Fisherinformation for a class of channels including nearly arbitrary additive i.i.d.noise and nearly arbitrary exponential families. As applications, we extendlower bounds against LDP for spiked matrix and tensor models under additiveGaussian noise to lower bounds against LCDF under general noisy channels. Wealso give a simple and unified treatment of the effect of censoring models byerasing observations at random and of quantizing models by taking the sign ofthe observations. These results are the first computational lower boundsagainst any large class of algorithms for all of these models when the channelis not one of a few special cases, and thereby give the first substantialevidence for the universality of several statistical-to-computational gaps.</description><author>Dmitriy Kunisky</author><pubDate>Tue, 12 Mar 2024 18:52:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07862v1</guid></item><item><title>Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation</title><link>http://arxiv.org/abs/2403.07860v1</link><description>Text-to-image generation has made significant advancements with theintroduction of text-to-image diffusion models. These models typically consistof a language model that interprets user prompts and a vision model thatgenerates corresponding images. As language and vision models continue toprogress in their respective domains, there is a great potential in exploringthe replacement of components in text-to-image diffusion models with moreadvanced counterparts. A broader research objective would therefore be toinvestigate the integration of any two unrelated language and generative visionmodels for text-to-image generation. In this paper, we explore this objectiveand propose LaVi-Bridge, a pipeline that enables the integration of diversepre-trained language models and generative vision models for text-to-imagegeneration. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible andplug-and-play approach without requiring modifications to the original weightsof the language and vision models. Our pipeline is compatible with variouslanguage models and generative vision models, accommodating differentstructures. Within this framework, we demonstrate that incorporating superiormodules, such as more advanced language models or generative vision models,results in notable improvements in capabilities like text alignment or imagequality. Extensive evaluations have been conducted to verify the effectivenessof LaVi-Bridge. Code is available athttps://github.com/ShihaoZhaoZSH/LaVi-Bridge.</description><author>Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</author><pubDate>Tue, 12 Mar 2024 18:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07860v1</guid></item><item><title>Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias</title><link>http://arxiv.org/abs/2403.07857v1</link><description>Model-induced distribution shifts (MIDS) occur as previous model outputspollute new model training sets over generations of models. This is known asmodel collapse in the case of generative models, and performative prediction orunfairness feedback loops for supervised models. When a model induces adistribution shift, it also encodes its mistakes, biases, and unfairnesses intothe ground truth of its data ecosystem. We introduce a framework that allows usto track multiple MIDS over many generations, finding that they can lead toloss in performance, fairness, and minoritized group representation, even ininitially unbiased datasets. Despite these negative consequences, we identifyhow models might be used for positive, intentional, interventions in their dataecosystems, providing redress for historical discrimination through a frameworkcalled algorithmic reparation (AR). We simulate AR interventions by curatingrepresentative training batches for stochastic gradient descent to demonstratehow AR can improve upon the unfairnesses of models and data ecosystems subjectto other MIDS. Our work takes an important step towards identifying,mitigating, and taking accountability for the unfair feedback loops enabled bythe idea that ML systems are inherently neutral and objective.</description><author>Sierra Wyllie, Ilia Shumailov, Nicolas Papernot</author><pubDate>Tue, 12 Mar 2024 18:48:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07857v1</guid></item><item><title>RudolfV: A Foundation Model by Pathologists for Pathologists</title><link>http://arxiv.org/abs/2401.04079v3</link><description>Histopathology plays a central role in clinical medicine and biomedicalresearch. While artificial intelligence shows promising results on manypathological tasks, generalization and dealing with rare diseases, wheretraining data is scarce, remains a challenge. Distilling knowledge fromunlabelled data into a foundation model before learning from, potentiallylimited, labelled data provides a viable path to address these challenges. Inthis work, we extend the state of the art of foundation models for digitalpathology whole slide images by semi-automated data curation and incorporatingpathologist domain knowledge. Specifically, we combine computational andpathologist domain knowledge (1) to curate a diverse dataset of 133k slidescorresponding to 1.2 billion image patches covering data from differentfixation, staining, and scanning protocols as well as data from differentindications and labs across the EU and US, (2) for grouping semanticallysimilar slides and tissue patches, and (3) to augment the input images duringtraining. We evaluate the resulting model on a set of public and internalbenchmarks and show that although our foundation model is trained with an orderof magnitude less slides, it performs on par or better than competing models.We expect that scaling our approach to more data and larger models will furtherincrease its performance and capacity to deal with increasingly complex realworld tasks in diagnostics and biomedical research.</description><author>Jonas Dippel, Barbara Feulner, Tobias Winterhoff, Simon Schallenberg, Gabriel Dernbach, Andreas Kunft, Stephan Tietz, Timo Milbich, Simon Heinke, Marie-Lisa Eich, Julika Ribbat-Idel, Rosemarie Krupar, Philipp Jurmeister, David Horst, Lukas Ruff, Klaus-Robert Müller, Frederick Klauschen, Maximilian Alber</author><pubDate>Tue, 12 Mar 2024 18:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04079v3</guid></item><item><title>Quantum Support Vector Machine for Prostate Cancer Detection: A Performance Analysis</title><link>http://arxiv.org/abs/2403.07856v1</link><description>This study addresses the urgent need for improved prostate cancer detectionmethods by harnessing the power of advanced technological solutions. Weintroduce the application of Quantum Support Vector Machine (QSVM) to thiscritical healthcare challenge, showcasing an enhancement in diagnosticperformance over the classical Support Vector Machine (SVM) approach. Our studynot only outlines the remarkable improvements in diagnostic performance made byQSVM over the classic SVM technique, but it delves into the advancementsbrought about by the quantum feature map architecture, which has been carefullyidentified and evaluated, ensuring it aligns seamlessly with the uniquecharacteristics of our prostate cancer dataset. This architecture succeded increating a distinct feature space, enabling the detection of complex,non-linear patterns in the data. The findings reveal not only a comparableaccuracy with classical SVM ($92\%$) but also a $7.14\%$ increase insensitivity and a notably high F1-Score ($93.33\%$). This study's importantcombination of quantum computing in medical diagnostics marks a pivotal stepforward in cancer detection, offering promising implications for the future ofhealthcare technology.</description><author>Walid El Maouaki, Taoufik Said, Mohamed Bennai</author><pubDate>Tue, 12 Mar 2024 18:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07856v1</guid></item><item><title>Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation</title><link>http://arxiv.org/abs/2308.08079v3</link><description>Subsurface datasets inherently possess big data characteristics such as vastvolume, diverse features, and high sampling speeds, further compounded by thecurse of dimensionality from various physical, engineering, and geologicalinputs. Among the existing dimensionality reduction (DR) methods, nonlineardimensionality reduction (NDR) methods, especially Metric-multidimensionalscaling (MDS), are preferred for subsurface datasets due to their inherentcomplexity. While MDS retains intrinsic data structure and quantifiesuncertainty, its limitations include unstabilized unique solutions invariant toEuclidean transformations and an absence of out-of-sample points (OOSP)extension. To enhance subsurface inferential and machine learning workflows,datasets must be transformed into stable, reduced-dimension representationsthat accommodate OOSP. Our solution employs rigid transformations for a stabilized Euclideaninvariant representation for LDS. By computing an MDS input dissimilaritymatrix, and applying rigid transformations on multiple realizations, we ensuretransformation invariance and integrate OOSP. This process leverages a convexhull algorithm and incorporates loss function and normalized stress fordistortion quantification. We validate our approach with synthetic data,varying distance metrics, and real-world wells from the Duvernay Formation.Results confirm our method's efficacy in achieving consistent LDSrepresentations. Furthermore, our proposed "stress ratio" (SR) metric providesinsight into uncertainty, beneficial for model adjustments and inferentialanalysis. Consequently, our workflow promises enhanced repeatability andcomparability in NDR for subsurface energy resource engineering and associatedbig data workflows.</description><author>Ademide O. Mabadeje, Michael J. Pyrcz</author><pubDate>Tue, 12 Mar 2024 18:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08079v3</guid></item><item><title>Unsupervised Multi-Person 3D Human Pose Estimation From 2D Poses Alone</title><link>http://arxiv.org/abs/2309.14865v3</link><description>Current unsupervised 2D-3D human pose estimation (HPE) methods do not work inmulti-person scenarios due to perspective ambiguity in monocular images.Therefore, we present one of the first studies investigating the feasibility ofunsupervised multi-person 2D-3D HPE from just 2D poses alone, focusing onreconstructing human interactions. To address the issue of perspectiveambiguity, we expand upon prior work by predicting the cameras' elevation anglerelative to the subjects' pelvis. This allows us to rotate the predicted posesto be level with the ground plane, while obtaining an estimate for the verticaloffset in 3D between individuals. Our method involves independently liftingeach subject's 2D pose to 3D, before combining them in a shared 3D coordinatesystem. The poses are then rotated and offset by the predicted elevation anglebefore being scaled. This by itself enables us to retrieve an accurate 3Dreconstruction of their poses. We present our results on the CHI3D dataset,introducing its use for unsupervised 2D-3D pose estimation with three newquantitative metrics, and establishing a benchmark for future research.</description><author>Peter Hardy, Hansung Kim</author><pubDate>Tue, 12 Mar 2024 18:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14865v3</guid></item><item><title>Distilling the Knowledge in Data Pruning</title><link>http://arxiv.org/abs/2403.07854v1</link><description>With the increasing size of datasets used for training neural networks, datapruning becomes an attractive field of research. However, most current datapruning algorithms are limited in their ability to preserve accuracy comparedto models trained on the full data, especially in high pruning regimes. In thispaper we explore the application of data pruning while incorporating knowledgedistillation (KD) when training on a pruned subset. That is, rather thanrelying solely on ground-truth labels, we also use the soft predictions from ateacher network pre-trained on the complete data. By integrating KD intotraining, we demonstrate significant improvement across datasets, pruningmethods, and on all pruning fractions. We first establish a theoreticalmotivation for employing self-distillation to improve training on pruned data.Then, we empirically make a compelling and highly practical observation: usingKD, simple random pruning is comparable or superior to sophisticated pruningmethods across all pruning regimes. On ImageNet for example, we achievesuperior accuracy despite training on a random subset of only 50% of the data.Additionally, we demonstrate a crucial connection between the pruning factorand the optimal knowledge distillation weight. This helps mitigate the impactof samples with noisy labels and low-quality images retained by typical pruningalgorithms. Finally, we make an intriguing observation: when using lowerpruning fractions, larger teachers lead to accuracy degradation, whilesurprisingly, employing teachers with a smaller capacity than the student's mayimprove results. Our code will be made available.</description><author>Emanuel Ben-Baruch, Adam Botach, Igor Kviatkovsky, Manoj Aggarwal, Gérard Medioni</author><pubDate>Tue, 12 Mar 2024 18:44:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07854v1</guid></item><item><title>12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning</title><link>http://arxiv.org/abs/2403.07851v1</link><description>Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systemsto expand their inference capabilities to new classes using only a few labeledexamples, without forgetting the previously learned classes. Classicalbackpropagation-based learning and its variants are often unsuitable forbattery-powered, memory-constrained systems at the extreme edge. In this work,we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on alightweight model consisting of a pretrained and metalearned feature extractorand an expandable explicit memory storing the class prototypes. Thearchitecture is pretrained with a novel feature orthogonality regularizationand metalearned with a multi-margin loss. For learning a new class, ourapproach extends the explicit memory with novel class prototypes, while theremaining architecture is kept frozen. This allows learning previously unseenclasses based on only a few examples with one single pass (hence online).O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,achieving state-of-the-art results. Tailored for ultra-low-power platforms, weimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating onlinelearning capabilities within just 12 mJ per new class.</description><author>Yoga Esa Wibowo, Cristian Cioflan, Thorir Mar Ingolfsson, Michael Hersche, Leo Zhao, Abbas Rahimi, Luca Benini</author><pubDate>Tue, 12 Mar 2024 18:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07851v1</guid></item><item><title>Geometry of the Visual Cortex with Applications to Image Inpainting and Enhancement</title><link>http://arxiv.org/abs/2308.07652v2</link><description>Equipping the rototranslation group $SE(2)$ with a sub-Riemannian structureinspired by the visual cortex V1, we propose algorithms for image inpaintingand enhancement based on hypoelliptic diffusion. We innovate on previousimplementations of the methods by Citti, Sarti, and Boscain et al., byproposing an alternative that prevents fading and is capable of producingsharper results in a procedure that we call WaxOn-WaxOff. We also exploit thesub-Riemannian structure to define a completely new unsharp filter using$SE(2)$, analogous to the classical unsharp filter for 2D image processing. Wedemonstrate our method on blood vessels enhancement in retinal scans.</description><author>Francesco Ballerin, Erlend Grong</author><pubDate>Tue, 12 Mar 2024 18:41:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07652v2</guid></item><item><title>Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations</title><link>http://arxiv.org/abs/2403.07849v1</link><description>We formulate an XAI-based model improvement approach for Graph NeuralNetworks (GNNs) for node classification, called Explanation Enhanced GraphLearning (EEGL). The goal is to improve predictive performance of GNN usingexplanations. EEGL is an iterative self-improving algorithm, which starts witha learned "vanilla" GNN, and repeatedly uses frequent subgraph mining to findrelevant patterns in explanation subgraphs. These patterns are then filteredfurther to obtain application-dependent features corresponding to the presenceof certain subgraphs in the node neighborhoods. Giving an application-dependentalgorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL)algorithm has previously been posed as an open problem. We present experimentalevidence, with synthetic and real-world data, which show that EEGL outperformsrelated approaches in predictive performance and that it has anode-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL'straining dynamics.</description><author>Harish G. Naik, Jan Polster, Raj Shekhar, Tamás Horváth, György Turán</author><pubDate>Tue, 12 Mar 2024 18:41:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07849v1</guid></item><item><title>Misgendering and Assuming Gender in Machine Translation when Working with Low-Resource Languages</title><link>http://arxiv.org/abs/2401.13165v3</link><description>This chapter focuses on gender-related errors in machine translation (MT) inthe context of low-resource languages. We begin by explaining what low-resourcelanguages are, examining the inseparable social and computational factors thatcreate such linguistic hierarchies. We demonstrate through a case study of ourmother tongue Bengali, a global language spoken by almost 300 million peoplebut still classified as low-resource, how gender is assumed and inferred intranslations to and from the high(est)-resource English when no suchinformation is provided in source texts. We discuss the postcolonial andsocietal impacts of such errors leading to linguistic erasure andrepresentational harms, and conclude by discussing potential solutions towardsuplifting languages by providing them more agency in MT conversations.</description><author>Sourojit Ghosh, Srishti Chatterjee</author><pubDate>Tue, 12 Mar 2024 18:41:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13165v3</guid></item><item><title>Group Decision-Making among Privacy-Aware Agents</title><link>http://arxiv.org/abs/2402.08156v3</link><description>How can individuals exchange information to learn from each other despitetheir privacy needs and security concerns? For example, consider individualsdeliberating a contentious topic and being concerned about divulging theirprivate experiences. Preserving individual privacy and enabling efficientsocial learning are both important desiderata but seem fundamentally at oddswith each other and very hard to reconcile. We do so by controlling informationleakage using rigorous statistical guarantees that are based on differentialprivacy (DP). Our agents use log-linear rules to update their beliefs aftercommunicating with their neighbors. Adding DP randomization noise to beliefsprovides communicating agents with plausible deniability with regard to theirprivate information and their network neighborhoods. We consider two learningenvironments one for distributed maximum-likelihood estimation given a finitenumber of private signals and another for online learning from an infinite,intermittent signal stream. Noisy information aggregation in the finite caseleads to interesting tradeoffs between rejecting low-quality states and makingsure all high-quality states are accepted in the algorithm output. Our resultsflesh out the nature of the trade-offs in both cases between the quality of thegroup decision outcomes, learning accuracy, communication cost, and the levelof privacy protections that the agents are afforded.</description><author>Marios Papachristou, M. Amin Rahimian</author><pubDate>Tue, 12 Mar 2024 18:37:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08156v3</guid></item><item><title>CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming</title><link>http://arxiv.org/abs/2403.06025v2</link><description>We introduce a new approach using computer vision to predict the land surfacedisplacement from subsurface geometry images for Carbon Capture andSequestration (CCS). CCS has been proved to be a key component for a carbonneutral society. However, scientists see there are challenges along the wayincluding the high computational cost due to the large model scale andlimitations to generalize a pre-trained model with complex physics. We tacklethose challenges by training models directly from the subsurface geometryimages. The goal is to understand the respons of land surface displacement dueto carbon injection and utilize our trained models to inform decision making inCCS projects. We implement multiple models (CNN, ResNet, and ResNetUNet) for staticmechanics problem, which is a image prediction problem. Next, we use the LSTMand transformer for transient mechanics scenario, which is a video predictionproblem. It shows ResNetUNet outperforms the others thanks to its architecturein static mechanics problem, and LSTM shows comparable performance totransformer in transient problem. This report proceeds by outlining our datasetin detail followed by model descriptions in method section. Result anddiscussion state the key learning, observations, and conclusion with futurework rounds out the paper.</description><author>Wei Chen, Yunan Li, Yuan Tian</author><pubDate>Tue, 12 Mar 2024 18:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06025v2</guid></item><item><title>Novelty Detection on Radio Astronomy Data using Signatures</title><link>http://arxiv.org/abs/2402.14892v2</link><description>We introduce SigNova, a new semi-supervised framework for detecting anomaliesin streamed data. While our initial examples focus on detecting radio-frequencyinterference (RFI) in digitized signals within the field of radio astronomy, itis important to note that SigNova's applicability extends to any type ofstreamed data. The framework comprises three primary components. Firstly, weuse the signature transform to extract a canonical collection of summarystatistics from observational sequences. This allows us to representvariable-length visibility samples as finite-dimensional feature vectors.Secondly, each feature vector is assigned a novelty score, calculated as theMahalanobis distance to its nearest neighbor in an RFI-free training set. Bythresholding these scores we identify observation ranges that deviate from theexpected behavior of RFI-free visibility samples without relying on stringentdistributional assumptions. Thirdly, we integrate this anomaly detector withPysegments, a segmentation algorithm, to localize consecutive observationscontaminated with RFI, if any. This approach provides a compelling alternativeto classical windowing techniques commonly used for RFI detection. Importantly,the complexity of our algorithm depends on the RFI pattern rather than on thesize of the observation window. We demonstrate how SigNova improves thedetection of various types of RFI (e.g., broadband and narrowband) intime-frequency visibility data. We validate our framework on the MurchisonWidefield Array (MWA) telescope and simulated data and the Hydrogen Epoch ofReionization Array (HERA).</description><author>Paola Arrubarrena, Maud Lemercier, Bojan Nikolic, Terry Lyons, Thomas Cass</author><pubDate>Tue, 12 Mar 2024 18:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14892v2</guid></item><item><title>A Machine learning and Empirical Bayesian Approach for Predictive Buying in B2B E-commerce</title><link>http://arxiv.org/abs/2403.07843v1</link><description>In the context of developing nations like India, traditional business tobusiness (B2B) commerce heavily relies on the establishment of robustrelationships, trust, and credit arrangements between buyers and sellers.Consequently, ecommerce enterprises frequently. Established in 2016 with avision to revolutionize trade in India through technology, Udaan is thecountrys largest business to business ecommerce platform. Udaan operates acrossdiverse product categories, including lifestyle, electronics, home and employtelecallers to cultivate buyer relationships, streamline order placementprocedures, and promote special promotions. The accurate anticipation of buyerorder placement behavior emerges as a pivotal factor for attaining sustainablegrowth, heightening competitiveness, and optimizing the efficiency of thesetelecallers. To address this challenge, we have employed an ensemble approachcomprising XGBoost and a modified version of Poisson Gamma model to predictcustomer order patterns with precision. This paper provides an in-depthexploration of the strategic fusion of machine learning and an empiricalBayesian approach, bolstered by the judicious selection of pertinent features.This innovative approach has yielded a remarkable 3 times increase in customerorder rates, show casing its potential for transformative impact in theecommerce industry.</description><author>Tuhin Subhra De, Pranjal Singh, Alok Patel</author><pubDate>Tue, 12 Mar 2024 18:32:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07843v1</guid></item><item><title>Quantifying and Mitigating Privacy Risks for Tabular Generative Models</title><link>http://arxiv.org/abs/2403.07842v1</link><description>Synthetic data from generative models emerges as the privacy-preservingdata-sharing solution. Such a synthetic data set shall resemble the originaldata without revealing identifiable private information. The backbonetechnology of tabular synthesizers is rooted in image generative models,ranging from Generative Adversarial Networks (GANs) to recent diffusion models.Recent prior work sheds light on the utility-privacy tradeoff on tabular data,revealing and quantifying privacy risks on synthetic data. We first conduct anexhaustive empirical analysis, highlighting the utility-privacy tradeoff offive state-of-the-art tabular synthesizers, against eight privacy attacks, witha special focus on membership inference attacks. Motivated by the observationof high data quality but also high privacy risk in tabular diffusion, wepropose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, whichis composed of an autoencoder network to encode the tabular data and a latentdiffusion model to synthesize the latent tables. Following the emerging f-DPframework, we apply DP-SGD to train the auto-encoder in combination with batchclipping and use the separation value as the privacy metric to better capturethe privacy gain from DP algorithms. Our empirical evaluation demonstrates thatDP-TLDM is capable of achieving a meaningful theoretical privacy guaranteewhile also significantly enhancing the utility of synthetic data. Specifically,compared to other DP-protected tabular generative models, DP-TLDM improves thesynthetic quality by an average of 35% in data resemblance, 15% in the utilityfor downstream tasks, and 50% in data discriminability, all while preserving acomparable level of privacy risk.</description><author>Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. Pérez, Marten van Dijk, Lydia Y. Chen</author><pubDate>Tue, 12 Mar 2024 18:27:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07842v1</guid></item><item><title>Data Interpreter: An LLM Agent For Data Science</title><link>http://arxiv.org/abs/2402.18679v3</link><description>Large Language Model (LLM)-based agents have demonstrated remarkableeffectiveness. However, their performance can be compromised in data sciencescenarios that require real-time data adjustment, expertise in optimization dueto complex dependencies among various tasks, and the ability to identifylogical errors for precise reasoning. In this study, we introduce the DataInterpreter, a solution designed to solve with code that emphasizes threepivotal techniques to augment problem-solving in data science: 1) dynamicplanning with hierarchical graph structures for real-time data adaptability;2)tool integration dynamically to enhance code proficiency during execution,enriching the requisite expertise;3) logical inconsistency identification infeedback, and efficiency enhancement through experience recording. We evaluatethe Data Interpreter on various data science and real-world tasks. Compared toopen-source baselines, it demonstrated superior performance, exhibitingsignificant improvements in machine learning tasks, increasing from 0.86 to0.95. Additionally, it showed a 26% increase in the MATH dataset and aremarkable 112% improvement in open-ended tasks. The solution will be releasedat https://github.com/geekan/MetaGPT.</description><author>Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu</author><pubDate>Tue, 12 Mar 2024 18:26:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18679v3</guid></item><item><title>A Two-Stage Feature Selection Approach for Robust Evaluation of Treatment Effects in High-Dimensional Observational Data</title><link>http://arxiv.org/abs/2111.13800v2</link><description>A Randomized Control Trial (RCT) is considered as the gold standard forevaluating the effect of any intervention or treatment. However, itsfeasibility is often hindered by ethical, economical, and legal considerations,making observational data a valuable alternative for drawing causalconclusions. Nevertheless, healthcare observational data presents a difficultchallenge due to its high dimensionality, requiring careful consideration toensure unbiased, reliable, and robust causal inferences. To overcome thischallenge, in this study, we propose a novel two-stage feature selectiontechnique called, Outcome Adaptive Elastic Net (OAENet), explicitly designedfor making robust causal inference decisions using matching techniques. OAENetoffers several key advantages over existing methods: superior performance oncorrelated and high-dimensional data compared to the existing methods and theability to select specific sets of variables (including confounders andvariables associated only with the outcome). This ensures robustness andfacilitates an unbiased estimate of the causal effect. Numerical experiments onsimulated data demonstrate that OAENet significantly outperformsstate-of-the-art methods by either producing a higher-quality estimate or acomparable estimate in significantly less time. To illustrate the applicabilityof OAENet, we employ large-scale US healthcare data to estimate the effect ofOpioid Use Disorder (OUD) on suicidal behavior. When compared to competingmethods, OAENet closely aligns with existing literature on the relationshipbetween OUD and suicidal behavior. Performance on both simulated and real-worlddata highlights that OAENet notably enhances the accuracy of estimatingtreatment effects or evaluating policy decision-making with causal inference.</description><author>Md Saiful Islam, Sahil Shikalgar, Md. Noor-E-Alam</author><pubDate>Tue, 12 Mar 2024 18:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.13800v2</guid></item><item><title>MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric</title><link>http://arxiv.org/abs/2403.07839v1</link><description>Vision-language pre-trained models have achieved impressive performance onvarious downstream tasks. However, their large model sizes hinder theirutilization on platforms with limited computational resources. We find thatdirectly using smaller pre-trained models and applying magnitude-based pruningon CLIP models leads to inflexibility and inferior performance. Recent effortsfor VLP compression either adopt uni-modal compression metrics resulting inlimited performance or involve costly mask-search processes with learnablemasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)metric, accurately assessing CLIP module importance by performance decline oncross-modal tasks. Using the MoPE metric, we introduce a unified pruningframework applicable to both pre-training and task-specific fine-tuningcompression stages. For pre-training, MoPE-CLIP effectively leverages knowledgefrom the teacher model, significantly reducing pre-training costs whilemaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruningfrom width to depth yields highly competitive task-specific models. Extensiveexperiments in two stages demonstrate the effectiveness of the MoPE metric, andMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.</description><author>Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, Zhenan Sun</author><pubDate>Tue, 12 Mar 2024 18:24:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07839v1</guid></item><item><title>LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery</title><link>http://arxiv.org/abs/2311.02058v3</link><description>We introduce LOTUS, a continual imitation learning algorithm that empowers aphysical robot to continuously and efficiently learn to solve new manipulationtasks throughout its lifespan. The core idea behind LOTUS is constructing anever-growing skill library from a sequence of new tasks with a small number ofhuman demonstrations. LOTUS starts with a continual skill discovery processusing an open-vocabulary vision model, which extracts skills as recurringpatterns presented in unsegmented demonstrations. Continual skill discoveryupdates existing skills to avoid catastrophic forgetting of previous tasks andadds new skills to solve novel tasks. LOTUS trains a meta-controller thatflexibly composes various skills to tackle vision-based manipulation tasks inthe lifelong learning process. Our comprehensive experiments show that LOTUSoutperforms state-of-the-art baselines by over 11% in success rate, showing itssuperior knowledge transfer ability compared to prior methods. More results andvideos can be found on the project website:https://ut-austin-rpl.github.io/Lotus/.</description><author>Weikang Wan, Yifeng Zhu, Rutav Shah, Yuke Zhu</author><pubDate>Tue, 12 Mar 2024 18:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02058v3</guid></item><item><title>When Eye-Tracking Meets Machine Learning: A Systematic Review on Applications in Medical Image Analysis</title><link>http://arxiv.org/abs/2403.07834v1</link><description>Eye-gaze tracking research offers significant promise in enhancing varioushealthcare-related tasks, above all in medical image analysis andinterpretation. Eye tracking, a technology that monitors and records themovement of the eyes, provides valuable insights into human visual attentionpatterns. This technology can transform how healthcare professionals andmedical specialists engage with and analyze diagnostic images, offering a moreinsightful and efficient approach to medical diagnostics. Hence, extractingmeaningful features and insights from medical images by leveraging eye-gazedata improves our understanding of how radiologists and other medical expertsmonitor, interpret, and understand images for diagnostic purposes. Eye-trackingdata, with intricate human visual attention patterns embedded, provides abridge to integrating artificial intelligence (AI) development and humancognition. This integration allows novel methods to incorporate domainknowledge into machine learning (ML) and deep learning (DL) approaches toenhance their alignment with human-like perception and decision-making.Moreover, extensive collections of eye-tracking data have also enabled novelML/DL methods to analyze human visual patterns, paving the way to a betterunderstanding of human vision, attention, and cognition. This systematic reviewinvestigates eye-gaze tracking applications and methodologies for enhancingML/DL algorithms for medical image analysis in depth.</description><author>Sahar Moradizeyveh, Mehnaz Tabassum, Sidong Liu, Robert Ahadizad Newport, Amin Beheshti, Antonio Di Ieva</author><pubDate>Tue, 12 Mar 2024 18:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07834v1</guid></item><item><title>On Solving Close Enough Orienteering Problem with Overlapped Neighborhoods</title><link>http://arxiv.org/abs/2310.04257v2</link><description>Close Enough Traveling Salesman Problem (CETSP) is a well-known variant ofTSP whereby the agent may complete its mission at any point within a targetneighborhood. Heuristics based on overlapped neighborhoods, known as SteinerZones (SZ), have gained attention in addressing CETSP. While SZs offereffective approximations to the original graph, their inherent overlap imposesconstraints on search space, potentially conflicting with global optimizationobjectives. Here we show how such limitations can be converted into advantagesin a Close Enough Orienteering Problem (CEOP) by aggregating prizes acrossoverlapped neighborhoods. We further extend classic CEOP with Non-uniformNeighborhoods (CEOP-N) by introducing non-uniform costs for prize collection.To tackle CEOP and CEOP-N, we develop a new approach featuring a RandomizedSteiner Zone Discretization (RSZD) scheme coupled with a hybrid algorithm basedon Particle Swarm Optimization (PSO) and Ant Colony System (ACS), CRaSZe-AntS.The RSZD scheme identifies sub-regions for PSO exploration, and ACS determinesthe discrete visiting sequence. We evaluate the RSZD's discretizationperformance on CEOP instances derived from established CETSP instances andcompare CRaSZe-AntS against the most relevant state-of-the-art heuristicfocused on single-neighborhood optimization for CEOP instances. We also comparethe performance of the interior search within SZs and the boundary search onindividual neighborhoods in the context of CEOP-N. Our experimental resultsshow that CRaSZe-AntS can yield comparable solution quality with significantlyreduced computation time compared to the single neighborhood strategy, where weobserve an average 140.44% increase in prize collection and a 55.18% reductionin algorithm execution time. CRaSZe-AntS is thus highly effective in solvingemerging CEOP-N, examples of which include truck-and-drone delivery scenarios.</description><author>Qiuchen Qian, Yanran Wang, David Boyle</author><pubDate>Tue, 12 Mar 2024 18:16:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04257v2</guid></item><item><title>The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing</title><link>http://arxiv.org/abs/2403.07825v1</link><description>Large Language Models have revolutionized numerous tasks with theirremarkable efficacy.However, the editing of these models, crucial forrectifying outdated or erroneous information, often leads to a complex issueknown as the ripple effect in the hidden space. This effect, while difficult todetect, can significantly impede the efficacy of model editing tasks anddeteriorate model performance.This paper addresses this scientific challenge byproposing a novel evaluation methodology, Graphical Outlier Relation basedAssessment(GORA), which quantitatively evaluates the adaptations of the modeland the subsequent impact of editing. Furthermore, we introduce the SelectiveOutlier Re-Editing Approach(SORA), a model editing method designed to mitigatethis ripple effect. Our comprehensive evaluations reveal that the ripple effectin the hidden space is a significant issue in all current model editingmethods. However, our proposed methods, GORA and SORA, effectively identify andalleviate this issue, respectively, contributing to the advancement of LLMediting techniques.</description><author>Jianchen Wang, Zhouhong Gu, Zhuozhi Xiong, Hongwei Feng, Yanghua Xiao</author><pubDate>Tue, 12 Mar 2024 18:04:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07825v1</guid></item><item><title>Fusing Climate Data Products using a Spatially Varying Autoencoder</title><link>http://arxiv.org/abs/2403.07822v1</link><description>Autoencoders are powerful machine learning models used to compressinformation from multiple data sources. However, autoencoders, like allartificial neural networks, are often unidentifiable and uninterpretable. Thisresearch focuses on creating an identifiable and interpretable autoencoder thatcan be used to meld and combine climate data products. The proposed autoencoderutilizes a Bayesian statistical framework, allowing for probabilisticinterpretations while also varying spatially to capture useful spatial patternsacross the various data products. Constraints are placed on the autoencoder asit learns patterns in the data, creating an interpretable consensus thatincludes the important features from each input. We demonstrate the utility ofthe autoencoder by combining information from multiple precipitation productsin High Mountain Asia.</description><author>Jacob A. Johnson, Matthew J. Heaton, William F. Christensen, Lynsie R. Warr, Summer B. Rupper</author><pubDate>Tue, 12 Mar 2024 18:03:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07822v1</guid></item><item><title>Unveiling the Pitfalls of Knowledge Editing for Large Language Models</title><link>http://arxiv.org/abs/2310.02129v3</link><description>As the cost associated with fine-tuning Large Language Models (LLMs)continues to rise, recent research efforts have pivoted towards developingmethodologies to edit implicit knowledge embedded within LLMs. Yet, there'sstill a dark cloud lingering overhead -- will knowledge editing triggerbutterfly effect? since it is still unclear whether knowledge editing mightintroduce side effects that pose potential risks or not. This paper pioneersthe investigation into the potential pitfalls associated with knowledge editingfor LLMs. To achieve this, we introduce new benchmark datasets and proposeinnovative evaluation metrics. Our results underline two pivotal concerns: (1)Knowledge Conflict: Editing groups of facts that logically clash can magnifythe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)Knowledge Distortion: Altering parameters with the aim of editing factualknowledge can irrevocably warp the innate knowledge structure of LLMs.Experimental results vividly demonstrate that knowledge editing mightinadvertently cast a shadow of unintended consequences on LLMs, which warrantattention and efforts for future works. Code and data are available athttps://github.com/zjunlp/PitfallsKnowledgeEditing.</description><author>Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen</author><pubDate>Tue, 12 Mar 2024 17:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02129v3</guid></item><item><title>Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling</title><link>http://arxiv.org/abs/2403.07818v1</link><description>Echocardiography (echo) is the first imaging modality used when assessingcardiac function. The measurement of functional biomarkers from echo reliesupon the segmentation of cardiac structures and deep learning models have beenproposed to automate the segmentation process. However, in order to translatethese tools to widespread clinical use it is important that the segmentationmodels are robust to a wide variety of images (e.g. acquired from differentscanners, by operators with different levels of expertise etc.). To achievethis level of robustness it is necessary that the models are trained withmultiple diverse datasets. A significant challenge faced when training withmultiple diverse datasets is the variation in label presence, i.e. the combineddata are often partially-labelled. Adaptations of the cross entropy lossfunction have been proposed to deal with partially labelled data. In this paperwe show that training naively with such a loss function and multiple diversedatasets can lead to a form of shortcut learning, where the model associateslabel presence with domain characteristics, leading to a drop in performance.To address this problem, we propose a novel label dropout scheme to break thelink between domain characteristics and the presence or absence of labels. Wedemonstrate that label dropout improves echo segmentation Dice score by 62% and25% on two cardiac structures when training using multiple diverse partiallylabelled datasets.</description><author>Iman Islam, Esther Puyol-Antón, Bram Ruijsink, Andrew J. Reader, Andrew P. King</author><pubDate>Tue, 12 Mar 2024 17:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07818v1</guid></item><item><title>Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM</title><link>http://arxiv.org/abs/2403.07816v1</link><description>We investigate efficient methods for training Large Language Models (LLMs) topossess capabilities in multiple specialized domains, such as coding, mathreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), startsfrom a seed model, which is branched to train experts in embarrassinglyparallel fashion with high throughput and reduced communication cost. Afterindividual experts are asynchronously trained, BTX brings together theirfeedforward parameters as experts in Mixture-of-Expert (MoE) layers andaverages the remaining parameters, followed by an MoE-finetuning stage to learntoken-level routing. BTX generalizes two special cases, the Branch-Train-Mergemethod, which does not have the MoE finetuning stage to learn routing, andsparse upcycling, which omits the stage of training experts asynchronously.Compared to alternative approaches, BTX achieves the best accuracy-efficiencytradeoff.</description><author>Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria Lin, Baptiste Rozière, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston, Xian Li</author><pubDate>Tue, 12 Mar 2024 17:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07816v1</guid></item><item><title>BizBench: A Quantitative Reasoning Benchmark for Business and Finance</title><link>http://arxiv.org/abs/2311.06602v2</link><description>Answering questions within business and finance requires reasoning,precision, and a wide-breadth of technical knowledge. Together, theserequirements make this domain difficult for large language models (LLMs). Weintroduce BizBench, a benchmark for evaluating models' ability to reason aboutrealistic financial problems. BizBench comprises eight quantitative reasoningtasks, focusing on question-answering (QA) over financial data via programsynthesis. We include three financially-themed code-generation tasks from newlycollected and augmented QA data. Additionally, we isolate the reasoningcapabilities required for financial QA: reading comprehension of financial textand tables for extracting intermediate values, and understanding financialconcepts and formulas needed to calculate complex solutions. Collectively,these tasks evaluate a model's financial background knowledge, ability to parsefinancial documents, and capacity to solve problems with code. We conduct anin-depth evaluation of open-source and commercial LLMs, comparing andcontrasting the behavior of code-focused and language-focused models. Wedemonstrate that the current bottleneck in performance is due to LLMs' limitedbusiness and financial understanding, highlighting the value of a challengingbenchmark for quantitative reasoning within this domain.</description><author>Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, Chris Tanner</author><pubDate>Tue, 12 Mar 2024 17:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06602v2</guid></item><item><title>Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond</title><link>http://arxiv.org/abs/2403.06279v2</link><description>This paper aims to develop and provide a rigorous treatment to the problem ofentropy regularized fine-tuning in the context of continuous-time diffusionmodels, which was recently proposed by Uehara et al. (arXiv:2402.15194, 2024).The idea is to use stochastic control for sample generation, where the entropyregularizer is introduced to mitigate reward collapse. We also show how theanalysis can be extended to fine-tuning involving a general $f$-divergenceregularizer.</description><author>Wenpin Tang</author><pubDate>Tue, 12 Mar 2024 17:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06279v2</guid></item><item><title>Chronos: Learning the Language of Time Series</title><link>http://arxiv.org/abs/2403.07815v1</link><description>We introduce Chronos, a simple yet effective framework for pretrainedprobabilistic time series models. Chronos tokenizes time series values usingscaling and quantization into a fixed vocabulary and trains existingtransformer-based language model architectures on these tokenized time seriesvia the cross-entropy loss. We pretrained Chronos models based on the T5 family(ranging from 20M to 710M parameters) on a large collection of publiclyavailable datasets, complemented by a synthetic dataset that we generated viaGaussian processes to improve generalization. In a comprehensive benchmarkconsisting of 42 datasets, and comprising both classical local models and deeplearning methods, we show that Chronos models: (a) significantly outperformother methods on datasets that were part of the training corpus; and (b) havecomparable and occasionally superior zero-shot performance on new datasets,relative to methods that were trained specifically on them. Our resultsdemonstrate that Chronos models can leverage time series data from diversedomains to improve zero-shot accuracy on unseen forecasting tasks, positioningpretrained models as a viable tool to greatly simplify forecasting pipelines.</description><author>Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, Yuyang Wang</author><pubDate>Tue, 12 Mar 2024 17:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07815v1</guid></item><item><title>The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition</title><link>http://arxiv.org/abs/2402.19344v3</link><description>This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW)Competition, which is part of the respective Workshop held in conjunction withIEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges inunderstanding human emotions and behaviors, crucial for the development ofhuman-centered technologies. In more detail, the Competition focuses on affectrelated benchmarking tasks and comprises of five sub-challenges: i)Valence-Arousal Estimation (the target is to estimate two continuous affectdimensions, valence and arousal), ii) Expression Recognition (the target is torecognise between the mutually exclusive classes of the 7 basic expressions and'other'), iii) Action Unit Detection (the target is to detect 12 action units),iv) Compound Expression Recognition (the target is to recognise between the 7mutually exclusive compound expression classes), and v) Emotional MimicryIntensity Estimation (the target is to estimate six continuous emotiondimensions). In the paper, we present these Challenges, describe theirrespective datasets and challenge protocols (we outline the evaluation metrics)and present the baseline systems as well as their obtained performance. Moreinformation for the Competition can be found in:https://affective-behavior-analysis-in-the-wild.github.io/6th.</description><author>Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Stefanos Zafeiriou, Irene Kotsia, Alice Baird, Chris Gagne, Chunchang Shao, Guanyu Hu</author><pubDate>Tue, 12 Mar 2024 17:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19344v3</guid></item><item><title>pyvene: A Library for Understanding and Improving PyTorch Models via Interventions</title><link>http://arxiv.org/abs/2403.07809v1</link><description>Interventions on model-internal states are fundamental operations in manyareas of AI, including model editing, steering, robustness, andinterpretability. To facilitate such research, we introduce $\textbf{pyvene}$,an open-source Python library that supports customizable interventions on arange of different PyTorch modules. $\textbf{pyvene}$ supports complexintervention schemes with an intuitive configuration format, and itsinterventions can be static or include trainable parameters. We show how$\textbf{pyvene}$ provides a unified and extensible framework for performinginterventions on neural models and sharing the intervened upon models withothers. We illustrate the power of the library via interpretability analysesusing causal abstraction and knowledge localization. We publish our librarythrough Python Package Index (PyPI) and provide code, documentation, andtutorials at https://github.com/stanfordnlp/pyvene.</description><author>Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, Christopher Potts</author><pubDate>Tue, 12 Mar 2024 17:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07809v1</guid></item><item><title>StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting</title><link>http://arxiv.org/abs/2403.07807v1</link><description>We introduce StyleGaussian, a novel 3D style transfer technique that allowsinstant transfer of any image's style to a 3D scene at 10 frames per second(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves styletransfer without compromising its real-time rendering ability and multi-viewconsistency. It achieves instant style transfer with three steps: embedding,transfer, and decoding. Initially, 2D VGG scene features are embedded intoreconstructed 3D Gaussians. Next, the embedded features are transformedaccording to a reference style image. Finally, the transformed features aredecoded into the stylized RGB. StyleGaussian has two novel designs. The firstis an efficient feature rendering strategy that first renders low-dimensionalfeatures and then maps them into high-dimensional features while embedding VGGfeatures. It cuts the memory consumption significantly and enables 3DGS torender the high-dimensional memory-intensive features. The second is aK-nearest-neighbor-based 3D CNN. Working as the decoder for the stylizedfeatures, it eliminates the 2D CNN operations that compromise strict multi-viewconsistency. Extensive experiments show that StyleGaussian achieves instant 3Dstylization with superior stylization quality while preserving real-timerendering and strict multi-view consistency. Project page:https://kunhao-liu.github.io/StyleGaussian/</description><author>Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu</author><pubDate>Tue, 12 Mar 2024 17:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07807v1</guid></item><item><title>Beyond Memorization: The Challenge of Random Memory Access in Language Models</title><link>http://arxiv.org/abs/2403.07805v1</link><description>Recent developments in Language Models (LMs) have shown their effectivenessin NLP tasks, particularly in knowledge-intensive tasks. However, themechanisms underlying knowledge storage and memory access within theirparameters remain elusive. In this paper, we investigate whether a generativeLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Throughcarefully-designed synthetic tasks, covering the scenarios of full recitation,selective recitation and grounded question answering, we reveal that LMs manageto sequentially access their memory while encountering challenges in randomlyaccessing memorized content. We find that techniques including recitation andpermutation improve the random memory access capability of LMs. Furthermore, byapplying this intervention to realistic scenarios of open-domain questionanswering, we validate that enhancing random access by recitation leads tonotable improvements in question answering. The code to reproduce ourexperiments can be found at https://github.com/sail-sg/lm-random-memory-access.</description><author>Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min Lin</author><pubDate>Tue, 12 Mar 2024 17:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07805v1</guid></item><item><title>Boosting keyword spotting through on-device learnable user speech characteristics</title><link>http://arxiv.org/abs/2403.07802v1</link><description>Keyword spotting systems for always-on TinyML-constrained applicationsrequire on-site tuning to boost the accuracy of offline trained classifierswhen deployed in unseen inference conditions. Adapting to the speechpeculiarities of target users requires many in-domain samples, oftenunavailable in real-world scenarios. Furthermore, current on-device learningtechniques rely on computationally intensive and memory-hungry backbone updateschemes, unfit for always-on, battery-powered devices. In this work, we proposea novel on-device learning architecture, composed of a pretrained backbone anda user-aware embedding learning the user's speech characteristics. Theso-generated features are fused and used to classify the input utterance. Fordomain shifts generated by unseen speakers, we measure error rate reductions ofup to 19% from 30.1% to 24.3% based on the 35-class problem of the GoogleSpeech Commands dataset, through the inexpensive update of the userprojections. We moreover demonstrate the few-shot learning capabilities of ourproposed architecture in sample- and class-scarce learning conditions. With23.7 kparameters and 1 MFLOP per epoch required for on-device training, oursystem is feasible for TinyML applications aimed at battery-poweredmicrocontrollers.</description><author>Cristian Cioflan, Lukas Cavigelli, Luca Benini</author><pubDate>Tue, 12 Mar 2024 17:41:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07802v1</guid></item><item><title>Rotation-Agnostic Image Representation Learning for Digital Pathology</title><link>http://arxiv.org/abs/2311.08359v2</link><description>This paper addresses complex challenges in histopathological image analysisthrough three key contributions. Firstly, it introduces a fast patch selectionmethod, FPS, for whole-slide image (WSI) analysis, significantly reducingcomputational cost while maintaining accuracy. Secondly, it presents PathDino,a lightweight histopathology feature extractor with a minimal configuration offive Transformer blocks and only 9 million parameters, markedly fewer thanalternatives. Thirdly, it introduces a rotation-agnostic representationlearning paradigm using self-supervised learning, effectively mitigatingoverfitting. We also show that our compact model outperforms existingstate-of-the-art histopathology-specific vision transformers on 12 diversedatasets, including both internal datasets spanning four sites (breast, liver,skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS,DigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a trainingdataset of 6 million histopathology patches from The Cancer Genome Atlas(TCGA), our approach demonstrates an average 8.5% improvement in patch-levelmajority vote performance. These contributions provide a robust framework forenhancing image analysis in digital pathology, rigorously validated throughextensive evaluation. Project Page:https://kimialabmayo.github.io/PathDino-Page/</description><author>Saghir Alfasly, Abubakr Shafique, Peyman Nejat, Jibran Khan, Areej Alsaafin, Ghazal Alabtah, H. R. Tizhoosh</author><pubDate>Tue, 12 Mar 2024 17:40:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08359v2</guid></item><item><title>BraSyn 2023 challenge: Missing MRI synthesis and the effect of different learning objectives</title><link>http://arxiv.org/abs/2403.07800v1</link><description>This work is addressing the Brain Magnetic Resonance Image Synthesis forTumor Segmentation (BraSyn) challenge which was hosted as part of the BrainTumor Segmentation challenge (BraTS) 2023. In this challenge researchers areinvited to work on synthesizing a missing magnetic resonance image sequencegiven other available sequences to facilitate tumor segmentation pipelinestrained on complete sets of image sequences. This problem can be addressedusing deep learning in the framework of paired images-to-image translation. Inthis work, we proposed to investigate the effectiveness of a commonly-used deeplearning framework such as Pix2Pix trained under supervision of differentimage-quality loss functions. Our results indicate that using different lossfunctions significantly affects the synthesis quality. We systematically studythe impact of different loss functions in the multi-sequence MR image synthesissetting of the BraSyn challenge. Furthermore, we show how image synthesisperformance can be optimized by beneficially combining different learningobjectives.</description><author>Ivo M. Baltruschat, Parvaneh Janbakhshi, Matthias Lenga</author><pubDate>Tue, 12 Mar 2024 17:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07800v1</guid></item><item><title>Adversarial Distortion Learning for Medical Image Denoising</title><link>http://arxiv.org/abs/2204.14100v2</link><description>We present a novel adversarial distortion learning (ADL) for denoising two-and three-dimensional (2D/3D) biomedical image data. The proposed ADL consistsof two auto-encoders: a denoiser and a discriminator. The denoiser removesnoise from input data and the discriminator compares the denoised result to itsnoise-free counterpart. This process is repeated until the discriminator cannotdifferentiate the denoised data from the reference. Both the denoiser and thediscriminator are built upon a proposed auto-encoder called Efficient-Unet.Efficient-Unet has a light architecture that uses the residual blocks and anovel pyramidal approach in the backbone to efficiently extract and re-usefeature maps. During training, the textural information and contrast arecontrolled by two novel loss functions. The architecture of Efficient-Unetallows generalizing the proposed method to any sort of biomedical data. The 2Dversion of our network was trained on ImageNet and tested on biomedicaldatasets whose distribution is completely different from ImageNet; so, there isno need for re-training. Experimental results carried out on magnetic resonanceimaging (MRI), dermatoscopy, electron microscopy and X-ray datasets show thatthe proposed method achieved the best on each benchmark. Our implementation andpre-trained models are available at https://github.com/mogvision/ADL.</description><author>Morteza Ghahremani, Mohammad Khateri, Alejandra Sierra, Jussi Tohka</author><pubDate>Tue, 12 Mar 2024 17:36:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.14100v2</guid></item><item><title>A Fourier Transform Framework for Domain Adaptation</title><link>http://arxiv.org/abs/2403.07798v1</link><description>By using unsupervised domain adaptation (UDA), knowledge can be transferredfrom a label-rich source domain to a target domain that contains relevantinformation but lacks labels. Many existing UDA algorithms suffer from directlyusing raw images as input, resulting in models that overly focus on redundantinformation and exhibit poor generalization capability. To address this issue,we attempt to improve the performance of unsupervised domain adaptation byemploying the Fourier method (FTF).Specifically, FTF is inspired by theamplitude of Fourier spectra, which primarily preserves low-level statisticalinformation. In FTF, we effectively incorporate low-level information from thetarget domain into the source domain by fusing the amplitudes of both domainsin the Fourier domain. Additionally, we observe that extracting features frombatches of images can eliminate redundant information while retainingclass-specific features relevant to the task. Building upon this observation,we apply the Fourier Transform at the data stream level for the first time. Tofurther align multiple sources of data, we introduce the concept of correlationalignment. To evaluate the effectiveness of our FTF method, we conductedevaluations on four benchmark datasets for domain adaptation, includingOffice-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our resultsdemonstrate superior performance.</description><author>Le Luo, Bingrong Xu, Qingyong Zhang, Cheng Lian, Jie Luo</author><pubDate>Tue, 12 Mar 2024 17:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07798v1</guid></item><item><title>MMSR: Symbolic Regression is a Multimodal Task</title><link>http://arxiv.org/abs/2402.18603v3</link><description>Mathematical formulas are the crystallization of human wisdom in exploringthe laws of nature for thousands of years. Describing the complex laws ofnature with a concise mathematical formula is a constant pursuit of scientistsand a great challenge for artificial intelligence. This field is calledsymbolic regression. Symbolic regression was originally formulated as acombinatorial optimization problem, and GP and reinforcement learningalgorithms were used to solve it. However, GP is sensitive to hyperparameters,and these two types of algorithms are inefficient. To solve this problem,researchers treat the mapping from data to expressions as a translationproblem. And the corresponding large-scale pre-trained model is introduced.However, the data and expression skeletons do not have very clear wordcorrespondences as the two languages do. Instead, they are more like twomodalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.The SR problem is solved as a pure multimodal problem, and contrastive learningis also introduced in the training process for modal alignment to facilitatelater modal feature fusion. It is worth noting that in order to better promotethe modal feature fusion, we adopt the strategy of training contrastivelearning loss and other losses at the same time, which only needs one-steptraining, instead of training contrastive learning loss first and then trainingother losses. Because our experiments prove training together can make thefeature extraction module and feature fusion module running-in better.Experimental results show that compared with multiple large-scale pre-trainingbaselines, MMSR achieves the most advanced results on multiple mainstreamdatasets including SRBench.</description><author>Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan Hao, Su Wei, Yusong Deng</author><pubDate>Tue, 12 Mar 2024 17:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18603v3</guid></item><item><title>Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data</title><link>http://arxiv.org/abs/2403.07797v1</link><description>Mechanisms for generating differentially private synthetic data based onmarginals and graphical models have been successful in a wide range ofsettings. However, one limitation of these methods is their inability toincorporate public data. Initializing a data generating model by pre-trainingon public data has shown to improve the quality of synthetic data, but thistechnique is not applicable when model structure is not determined a priori. Wedevelop the mechanism jam-pgm, which expands the adaptive measurementsframework to jointly select between measuring public data and private data.This technique allows for public data to be included in a graphical-model-basedmechanism. We show that jam-pgm is able to outperform both publicly assistedand non publicly assisted synthetic data generation mechanisms even when thepublic data distribution is biased.</description><author>Miguel Fuentes, Brett Mullins, Ryan McKenna, Gerome Miklau, Daniel Sheldon</author><pubDate>Tue, 12 Mar 2024 17:34:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07797v1</guid></item><item><title>Fine-tuning Large Language Models with Sequential Instructions</title><link>http://arxiv.org/abs/2403.07794v1</link><description>Large language models (LLMs) struggle to follow a sequence of instructions ina single query as they may ignore or misinterpret part of it. This impairstheir performance in complex problems whose solution requires multipleintermediate steps, such as multilingual (translate then answer) and multimodal(caption then answer) tasks. We empirically verify this with open-source LLMsas large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequentialinstructions in present-day data, we propose sequential instruction tuning, asimple yet effective strategy to automatically augment instruction tuning dataand equip LLMs with the ability to execute multiple sequential instructions.After exploring interleaving instructions in existing datasets, such as Alpaca,with a wide range of intermediate tasks, we find that sequentialinstruction-tuned models consistently outperform the conventionalinstruction-tuned baselines in downstream tasks involving reasoning,multilingual, and multimodal abilities. To shed further light on our technique,we analyse how adversarial intermediate texts, unseen tasks, promptverbalization, number of tasks, and prompt length affect SIT. We hope that thismethod will open new research avenues on instruction tuning for complex tasks.</description><author>Hanxu Hu, Pinzhen Chen, Edoardo M. Ponti</author><pubDate>Tue, 12 Mar 2024 17:33:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07794v1</guid></item><item><title>Computational limits to the legibility of the imaged human brain</title><link>http://arxiv.org/abs/2309.07096v3</link><description>Our knowledge of the organisation of the human brain at the population-levelis yet to translate into power to predict functional differences at theindividual-level, limiting clinical applications, and casting doubt on thegeneralisability of inferred mechanisms. It remains unknown whether thedifficulty arises from the absence of individuating biological patterns withinthe brain, or from limited power to access them with the models and compute atour disposal. Here we comprehensively investigate the resolvability of suchpatterns with data and compute at unprecedented scale. Across 23 810 uniqueparticipants from UK Biobank, we systematically evaluate the predictability of25 individual biological characteristics, from all available combinations ofstructural and functional neuroimaging data. Over 4526 GPU hours ofcomputation, we train, optimize, and evaluate out-of-sample 700 individualpredictive models, including fully-connected feed-forward neural networks ofdemographic, psychological, serological, chronic disease, and functionalconnectivity characteristics, and both uni- and multi-modal 3D convolutionalneural network models of macro- and micro-structural brain imaging. We find amarked discrepancy between the high predictability of sex (balanced accuracy99.7%), age (mean absolute error 2.048 years, R2 0.859), and weight (meanabsolute error 2.609Kg, R2 0.625), for which we set new state-of-the-artperformance, and the surprisingly low predictability of other characteristics.Neither structural nor functional imaging predicted psychology better than thecoincidence of chronic disease (p&lt;0.05). Serology predicted chronic disease(p&lt;0.05) and was best predicted by it (p&lt;0.001), followed by structuralneuroimaging (p&lt;0.05). Our findings suggest either more informative imaging ormore powerful models are needed to decipher individual level characteristicsfrom the human brain.</description><author>James K Ruffle, Robert J Gray, Samia Mohinta, Guilherme Pombo, Chaitanya Kaul, Harpreet Hyare, Geraint Rees, Parashkev Nachev</author><pubDate>Tue, 12 Mar 2024 17:30:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07096v3</guid></item><item><title>Recent Advances in Embedding Methods for Multi-Object Tracking: A Survey</title><link>http://arxiv.org/abs/2205.10766v2</link><description>Multi-object tracking (MOT) aims to associate target objects across videoframes in order to obtain entire moving trajectories. With the advancement ofdeep neural networks and the increasing demand for intelligent video analysis,MOT has gained significantly increased interest in the computer visioncommunity. Embedding methods play an essential role in object locationestimation and temporal identity association in MOT. Unlike other computervision tasks, such as image classification, object detection,re-identification, and segmentation, embedding methods in MOT have largevariations, and they have never been systematically analyzed and summarized. Inthis survey, we first conduct a comprehensive overview with in-depth analysisfor embedding methods in MOT from seven different perspectives, includingpatch-level embedding, single-frame embedding, cross-frame joint embedding,correlation embedding, sequential embedding, tracklet embedding, andcross-track relational embedding. We further summarize the existing widely usedMOT datasets and analyze the advantages of existing state-of-the-art methodsaccording to their embedding strategies. Finally, some critical yetunder-investigated areas and future research directions are discussed.</description><author>Gaoang Wang, Mingli Song, Jenq-Neng Hwang</author><pubDate>Tue, 12 Mar 2024 17:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.10766v2</guid></item><item><title>LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content</title><link>http://arxiv.org/abs/2403.05854v2</link><description>Long-tail recognition is challenging because it requires the model to learngood representations from tail categories and address imbalances across allcategories. In this paper, we propose a novel generative and fine-tuningframework, LTGC, to handle long-tail recognition via leveraging generatedcontent. Firstly, inspired by the rich implicit knowledge in large-scale models(e.g., large language models, LLMs), LTGC leverages the power of these modelsto parse and reason over the original tail data to produce diverse tail-classcontent. We then propose several novel designs for LTGC to ensure the qualityof the generated data and to efficiently fine-tune the model using both thegenerated and original data. The visualization demonstrates the effectivenessof the generation module in LTGC, which produces accurate and diverse taildata. Additionally, the experimental results demonstrate that our LTGCoutperforms existing state-of-the-art methods on popular long-tailedbenchmarks.</description><author>Qihao Zhao, Yalun Dai, Hao Li, Wei Hu, Fan Zhang, Jun Liu</author><pubDate>Tue, 12 Mar 2024 17:26:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05854v2</guid></item><item><title>DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</title><link>http://arxiv.org/abs/2403.07788v1</link><description>Imitation learning from human hand motion data presents a promising avenuefor imbuing robots with human-like dexterity in real-world manipulation tasks.Despite this potential, substantial challenges persist, particularly with theportability of existing hand motion capture (mocap) systems and the difficultyof translating mocap data into effective control policies. To tackle theseissues, we introduce DexCap, a portable hand motion capture system, alongsideDexIL, a novel imitation algorithm for training dexterous robot skills directlyfrom human hand mocap data. DexCap offers precise, occlusion-resistant trackingof wrist and finger motions based on SLAM and electromagnetic field togetherwith 3D observations of the environment. Utilizing this rich dataset, DexILemploys inverse kinematics and point cloud-based imitation learning toreplicate human actions with robot hands. Beyond learning from human motion,DexCap also offers an optional human-in-the-loop correction mechanism to refineand further improve robot performance. Through extensive evaluation across sixdexterous manipulation tasks, our approach not only demonstrates superiorperformance but also showcases the system's capability to effectively learnfrom in-the-wild mocap data, paving the way for future data collection methodsfor dexterous manipulation. More details can be found athttps://dex-cap.github.io</description><author>Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C. Karen Liu</author><pubDate>Tue, 12 Mar 2024 17:23:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07788v1</guid></item><item><title>Generative deep learning-enabled ultra-large field-of-view lens-free imaging</title><link>http://arxiv.org/abs/2403.07786v1</link><description>Advancements in high-throughput biomedical applications necessitatereal-time, large field-of-view (FOV) imaging capabilities. Conventionallens-free imaging (LFI) systems, while addressing the limitations of physicallenses, have been constrained by dynamic, hard-to-model optical fields,resulting in a limited one-shot FOV of approximately 20 $mm^2$. Thisrestriction has been a major bottleneck in applications like live-cell imagingand automation of microfluidic systems for biomedical research. Here, wepresent a deep-learning(DL)-based imaging framework -- GenLFI -- leveraginggenerative artificial intelligence (AI) for holographic image reconstruction.We demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$,surpassing the current LFI system by more than 20-fold, and even larger thanthe world's largest confocal microscope by 1.76 times. The resolution is at thesub-pixel level of 5.52 $\mu m$, without the need for a shifting light source.The unsupervised learning-based reconstruction does not require optical fieldmodeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidicsand 3D cell models) in complex optical fields possible. This GenLFI frameworkunlocks the potential of LFI systems, offering a robust tool to tackle newfrontiers in high-throughput biomedical applications such as drug discovery.</description><author>Ronald B. Liu, Zhe Liu, Max G. A. Wolf, Krishna P. Purohit, Gregor Fritz, Yi Feng, Carsten G. Hansen, Pierre O. Bagnaninchi, Xavier Casadevall i Solvas, Yunjie Yang</author><pubDate>Tue, 12 Mar 2024 17:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07786v1</guid></item><item><title>Disjoint Contrastive Regression Learning for Multi-Sourced Annotations</title><link>http://arxiv.org/abs/2112.15411v2</link><description>Large-scale datasets are important for the development of deep learningmodels. Such datasets usually require a heavy workload of annotations, whichare extremely time-consuming and expensive. To accelerate the annotationprocedure, multiple annotators may be employed to label different subsets ofthe data. However, the inconsistency and bias among different annotators areharmful to the model training, especially for qualitative and subjectivetasks.To address this challenge, in this paper, we propose a novel contrastiveregression framework to address the disjoint annotations problem, where eachsample is labeled by only one annotator and multiple annotators work ondisjoint subsets of the data. To take account of both the intra-annotatorconsistency and inter-annotator inconsistency, two strategies areemployed.Firstly, a contrastive-based loss is applied to learn the relativeranking among different samples of the same annotator, with the assumption thatthe ranking of samples from the same annotator is unanimous. Secondly, we applythe gradient reversal layer to learn robust representations that are invariantto different annotators. Experiments on the facial expression prediction task,as well as the image quality assessment task, verify the effectiveness of ourproposed framework.</description><author>Xiaoqian Ruan, Gaoang Wang</author><pubDate>Tue, 12 Mar 2024 17:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.15411v2</guid></item><item><title>CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images</title><link>http://arxiv.org/abs/2307.03293v3</link><description>The development of successful artificial intelligence models for chest X-rayanalysis relies on large, diverse datasets with high-quality annotations. Whileseveral databases of chest X-ray images have been released, most includedisease diagnosis labels but lack detailed pixel-level anatomical segmentationlabels. To address this gap, we introduce an extensive chest X-ray multi-centersegmentation dataset with uniform and fine-grain anatomical annotations forimages coming from five well-known publicly available databases: ChestX-ray8,Chexpert, MIMIC-CXR-JPG, Padchest, and VinDr-CXR, resulting in 657,566segmentation masks. Our methodology utilizes the HybridGNet model to ensureconsistent and high-quality segmentations across all datasets. Rigorousvalidation, including expert physician evaluation and automatic qualitycontrol, was conducted to validate the resulting masks. Additionally, weprovide individualized quality indices per mask and an overall qualityestimation per dataset. This dataset serves as a valuable resource for thebroader scientific community, streamlining the development and assessment ofinnovative methodologies in chest X-ray analysis. The CheXmask dataset ispublicly available at:https://physionet.org/content/chexmask-cxr-segmentation-data/</description><author>Nicolás Gaggion, Candelaria Mosquera, Lucas Mansilla, Julia Mariel Saidman, Martina Aineseder, Diego H. Milone, Enzo Ferrante</author><pubDate>Tue, 12 Mar 2024 17:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03293v3</guid></item><item><title>Knowledge Distillation of Large Language Models</title><link>http://arxiv.org/abs/2306.08543v3</link><description>Knowledge Distillation (KD) is a promising technique for reducing the highcomputational demand of large language models (LLMs). However, previous KDmethods are primarily applied to white-box classification models or trainingsmall models to imitate black-box model APIs like ChatGPT. How to effectivelydistill the knowledge of white-box LLMs into small models is stillunder-explored, which becomes more important with the prosperity of open-sourceLLMs. In this work, we propose a KD approach that distills LLMs into smallerlanguage models. We first replace the forward Kullback-Leibler divergence (KLD)objective in the standard KD approaches with reverse KLD, which is moresuitable for KD on generative language models, to prevent the student modelfrom overestimating the low-probability regions of the teacher distribution.Then, we derive an effective optimization approach to learn this objective. Thestudent models are named MiniLLM. Extensive experiments in theinstruction-following setting show that MiniLLM generates more preciseresponses with higher overall quality, lower exposure bias, better calibration,and higher long-text generation performance than the baselines. Our method isscalable for different model families with 120M to 13B parameters. Our code,data, and model checkpoints can be found inhttps://github.com/microsoft/LMOps/tree/main/minillm.</description><author>Yuxian Gu, Li Dong, Furu Wei, Minlie Huang</author><pubDate>Tue, 12 Mar 2024 17:15:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08543v3</guid></item><item><title>A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles</title><link>http://arxiv.org/abs/2311.02099v3</link><description>This work introduces a preference learning method that ensures adherence togiven specifications, with an application to autonomous vehicles. Our approachincorporates the priority ordering of Signal Temporal Logic (STL) formulasdescribing traffic rules into a learning framework. By leveraging ParametricWeighted Signal Temporal Logic (PWSTL), we formulate the problem ofsafety-guaranteed preference learning based on pairwise comparisons and proposean approach to solve this learning problem. Our approach finds a feasiblevaluation for the weights of the given PWSTL formula such that, with theseweights, preferred signals have weighted quantitative satisfaction measuresgreater than their non-preferred counterparts. The feasible valuation ofweights given by our approach leads to a weighted STL formula that can be usedin correct-and-custom-by-construction controller synthesis. We demonstrate theperformance of our method with a pilot human subject study in two differentsimulated driving scenarios involving a stop sign and a pedestrian crossing.Our approach yields competitive results compared to existing preferencelearning methods in terms of capturing preferences and notably outperforms themwhen safety is considered.</description><author>Ruya Karagulle, Nikos Arechiga, Andrew Best, Jonathan DeCastro, Necmiye Ozay</author><pubDate>Tue, 12 Mar 2024 17:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02099v3</guid></item><item><title>FairRR: Pre-Processing for Group Fairness through Randomized Response</title><link>http://arxiv.org/abs/2403.07780v1</link><description>The increasing usage of machine learning models in consequentialdecision-making processes has spurred research into the fairness of thesesystems. While significant work has been done to study group fairness in thein-processing and post-processing setting, there has been little thattheoretically connects these results to the pre-processing domain. This paperproposes that achieving group fairness in downstream models can be formulatedas finding the optimal design matrix in which to modify a response variable ina Randomized Response framework. We show that measures of group fairness can bedirectly controlled for with optimal model utility, proposing a pre-processingalgorithm called FairRR that yields excellent downstream model utility andfairness.</description><author>Xianli Zeng, Joshua Ward, Guang Cheng</author><pubDate>Tue, 12 Mar 2024 17:08:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07780v1</guid></item><item><title>The Principles of Data-Centric AI (DCAI)</title><link>http://arxiv.org/abs/2211.14611v2</link><description>Data is a crucial infrastructure to how artificial intelligence (AI) systemslearn. However, these systems to date have been largely model-centric, puttinga premium on the model at the expense of the data quality. Data quality issuesbeset the performance of AI systems, particularly in downstream deployments andin real-world applications. Data-centric AI (DCAI) as an emerging conceptbrings data, its quality and its dynamism to the forefront in considerations ofAI systems through an iterative and systematic approach. As one of the firstoverviews, this article brings together data-centric perspectives and conceptsto outline the foundations of DCAI. It specifically formulates six guidingprinciples for researchers and practitioners and gives direction for futureadvancement of DCAI.</description><author>Mohammad Hossein Jarrahi, Ali Memariani, Shion Guha</author><pubDate>Tue, 12 Mar 2024 17:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14611v2</guid></item><item><title>Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning</title><link>http://arxiv.org/abs/2402.12177v4</link><description>Retrieval Augmented Generation (RAG) has emerged as an effective solution formitigating hallucinations in Large Language Models (LLMs). The retrieval stagein RAG typically involves a pre-trained embedding model, which converts queriesand passages into vectors to capture their semantics. However, a standardpre-trained embedding model may exhibit sub-optimal performance when applied tospecific domain knowledge, necessitating fine-tuning. This paper addressesscenarios where the embeddings are only available from a black-box model. Weintroduce Model augmented fine-tuning (Mafin) -- a novel approach forfine-tuning a black-box embedding model by augmenting it with a trainableembedding model. Our results demonstrate that Mafin significantly enhances theperformance of the black-box embeddings by only requiring the training of asmall augmented model. We validate the effectiveness of our method on bothlabeled and unlabeled datasets, illustrating its broad applicability andefficiency.</description><author>Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber</author><pubDate>Tue, 12 Mar 2024 17:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12177v4</guid></item><item><title>Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume</title><link>http://arxiv.org/abs/2403.03229v2</link><description>The right ventricular (RV) function deterioration strongly predicts clinicaloutcomes in numerous circumstances. To boost the clinical deployment ofensemble regression methods that quantify RV volumes using tabular data fromthe widely available two-dimensional echocardiography (2DE), we propose tocomplement the volume predictions with uncertainty scores. To this end, weemploy an instance-based method which uses the learned tree structure toidentify the nearest training samples to a target instance and then uses anumber of distribution types to more flexibly model the output. Theprobabilistic and point-prediction performances of the proposed framework areevaluated on a relatively small-scale dataset, comprising 100 end-diastolic andend-systolic RV volumes. The reference values for point performance wereobtained from MRI. The results demonstrate that our flexible approach yieldsimproved probabilistic and point performances over other state-of-the-artmethods. The appropriateness of the proposed framework is showcased byproviding exemplar cases. The estimated uncertainty embodies both aleatoric andepistemic types. This work aligns with trustworthy artificial intelligencesince it can be used to enhance the decision-making process and reduce risks.The feature importance scores of our framework can be exploited to reduce thenumber of required 2DE views which could enhance the proposed pipeline'sclinical application.</description><author>Tuan A. Bohoran, Polydoros N. Kampaktsis, Laura McLaughlin, Jay Leb, Gerry P. McCann, Archontis Giannakidis</author><pubDate>Tue, 12 Mar 2024 17:03:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03229v2</guid></item><item><title>Switching the Loss Reduces the Cost in Batch Reinforcement Learning</title><link>http://arxiv.org/abs/2403.05385v3</link><description>We propose training fitted Q-iteration with log-loss (FQI-LOG) for batchreinforcement learning (RL). We show that the number of samples needed to learna near-optimal policy with FQI-LOG scales with the accumulated cost of theoptimal policy, which is zero in problems where acting optimally achieves thegoal and incurs no cost. In doing so, we provide a general framework forproving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimalachievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG usesfewer samples than FQI trained with squared loss on problems where the optimalpolicy reliably achieves the goal.</description><author>Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James McInerney, Dawen Liang, Nathan Kallus, Csaba Szepesvári</author><pubDate>Tue, 12 Mar 2024 17:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05385v3</guid></item><item><title>SemCity: Semantic Scene Generation with Triplane Diffusion</title><link>http://arxiv.org/abs/2403.07773v1</link><description>We present "SemCity," a 3D diffusion model for semantic scene generation inreal-world outdoor environments. Most 3D diffusion models focus on generating asingle object, synthetic indoor scenes, or synthetic outdoor scenes, while thegeneration of real-world outdoor scenes is rarely addressed. In this paper, weconcentrate on generating a real-outdoor scene through learning a diffusionmodel on a real-world outdoor dataset. In contrast to synthetic data,real-outdoor datasets often contain more empty spaces due to sensorlimitations, causing challenges in learning real-outdoor distributions. Toaddress this issue, we exploit a triplane representation as a proxy form ofscene distributions to be learned by our diffusion model. Furthermore, wepropose a triplane manipulation that integrates seamlessly with our triplanediffusion model. The manipulation improves our diffusion model's applicabilityin a variety of downstream tasks related to outdoor scene generation such asscene inpainting, scene outpainting, and semantic scene completion refinements.In experimental results, we demonstrate that our triplane diffusion model showsmeaningful generation results compared with existing work in a real-outdoordataset, SemanticKITTI. We also show our triplane manipulation facilitatesseamlessly adding, removing, or modifying objects within a scene. Further, italso enables the expansion of scenes toward a city-level scale. Finally, weevaluate our method on semantic scene completion refinements where ourdiffusion model enhances predictions of semantic scene completion networks bylearning scene distribution. Our code is available athttps://github.com/zoomin-lee/SemCity.</description><author>Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, Sung-Eui Yoon</author><pubDate>Tue, 12 Mar 2024 16:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07773v1</guid></item><item><title>Enhancing Group Fairness in Online Settings Using Oblique Decision Forests</title><link>http://arxiv.org/abs/2310.11401v3</link><description>Fairness, especially group fairness, is an important consideration in thecontext of machine learning systems. The most commonly adopted groupfairness-enhancing techniques are in-processing methods that rely on a mixtureof a fairness objective (e.g., demographic parity) and a task-specificobjective (e.g., cross-entropy) during the training process. However, when dataarrives in an online fashion -- one instance at a time -- optimizing suchfairness objectives poses several challenges. In particular, group fairnessobjectives are defined using expectations of predictions across differentdemographic groups. In the online setting, where the algorithm has access to asingle instance at a time, estimating the group fairness objective requiresadditional storage and significantly more computation (e.g., forward/backwardpasses) than the task-specific objective at every time step. In this paper, wepropose Aranyani, an ensemble of oblique decision trees, to make fair decisionsin online settings. The hierarchical tree structure of Aranyani enablesparameter isolation and allows us to efficiently compute the fairness gradientsusing aggregate statistics of previous decisions, eliminating the need foradditional storage and forward/backward passes. We also present an efficientframework to train Aranyani and theoretically analyze several of itsproperties. We conduct empirical evaluations on 5 publicly available benchmarks(including vision and language datasets) to show that Aranyani achieves abetter accuracy-fairness trade-off compared to baseline approaches.</description><author>Somnath Basu Roy Chowdhury, Nicholas Monath, Ahmad Beirami, Rahul Kidambi, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi</author><pubDate>Tue, 12 Mar 2024 16:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11401v3</guid></item><item><title>Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations</title><link>http://arxiv.org/abs/2403.07769v1</link><description>This article explores the dynamic influence of computational entities basedon multi-agent systems theory (SMA) combined with large language models (LLM),which are characterized by their ability to simulate complex humaninteractions, as a possibility to revolutionize human user interaction from theuse of specialized artificial agents to support everything from operationalorganizational processes to strategic decision making based on appliedknowledge and human orchestration. Previous investigations reveal that thereare limitations, particularly in the autonomous approach of artificial agents,especially when dealing with new challenges and pragmatic tasks such asinducing logical reasoning and problem solving. It is also considered thattraditional techniques, such as the stimulation of chains of thoughts, requireexplicit human guidance. In our approach we employ agents developed from largelanguage models (LLM), each with distinct prototyping that considers behavioralelements, driven by strategies that stimulate the generation of knowledge basedon the use case proposed in the scenario (role-play) business, using adiscussion approach between agents (guided conversation). We demonstrate thepotential of developing agents useful for organizational strategies, based onmulti-agent system theories (SMA) and innovative uses based on large languagemodels (LLM based), offering a differentiated and adaptable experiment todifferent applications, complexities, domains, and capabilities from LLM.</description><author>Carlos Jose Xavier Cruz</author><pubDate>Tue, 12 Mar 2024 16:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07769v1</guid></item><item><title>Cascading Reinforcement Learning</title><link>http://arxiv.org/abs/2401.08961v3</link><description>Cascading bandits have gained popularity in recent years due to theirapplicability to recommendation systems and online advertising. In thecascading bandit model, at each timestep, an agent recommends an ordered subsetof items (called an item list) from a pool of items, each associated with anunknown attraction probability. Then, the user examines the list, and clicksthe first attractive item (if any), and after that, the agent receives areward. The goal of the agent is to maximize the expected cumulative reward.However, the prior literature on cascading bandits ignores the influences ofuser states (e.g., historical behaviors) on recommendations and the change ofstates as the session proceeds. Motivated by this fact, we propose ageneralized cascading RL framework, which considers the impact of user statesand state transition into decisions. In cascading RL, we need to select itemsnot only with large attraction probabilities but also leading to good successorstates. This imposes a huge computational challenge due to the combinatorialaction space. To tackle this challenge, we delve into the properties of valuefunctions, and design an oracle BestPerm to efficiently find the optimal itemlist. Equipped with BestPerm, we develop two algorithms CascadingVI andCascadingBPI, which are both computationally-efficient and sample-efficient,and provide near-optimal regret and sample complexity guarantees. Furthermore,we present experiments to show the improved computational and sampleefficiencies of our algorithms compared to straightforward adaptations ofexisting RL algorithms in practice.</description><author>Yihan Du, R. Srikant, Wei Chen</author><pubDate>Tue, 12 Mar 2024 16:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.08961v3</guid></item><item><title>Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets</title><link>http://arxiv.org/abs/2403.07767v1</link><description>Paralinguistic traits like cognitive load and emotion are increasinglyrecognized as pivotal areas in speech recognition research, often examinedthrough specialized datasets like CLSE and IEMOCAP. However, the integrity ofthese datasets is seldom scrutinized for text-dependency. This paper criticallyevaluates the prevalent assumption that machine learning models trained on suchdatasets genuinely learn to identify paralinguistic traits, rather than merelycapturing lexical features. By examining the lexical overlap in these datasetsand testing the performance of machine learning models, we expose significanttext-dependency in trait-labeling. Our results suggest that some machinelearning models, especially large pre-trained models like HuBERT, mightinadvertently focus on lexical characteristics rather than the intendedparalinguistic features. The study serves as a call to action for the researchcommunity to reevaluate the reliability of existing datasets and methodologies,ensuring that machine learning models genuinely learn what they are designed torecognize.</description><author>Jan Pešán, Santosh Kesiraju, Lukáš Burget, Jan ''Honza'' Černocký</author><pubDate>Tue, 12 Mar 2024 16:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07767v1</guid></item><item><title>Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model</title><link>http://arxiv.org/abs/2403.07764v1</link><description>Current makeup transfer methods are limited to simple makeup styles, makingthem difficult to apply in real-world scenarios. In this paper, we introduceStable-Makeup, a novel diffusion-based makeup transfer method capable ofrobustly transferring a wide range of real-world makeup, onto user-providedfaces. Stable-Makeup is based on a pre-trained diffusion model and utilizes aDetail-Preserving (D-P) makeup encoder to encode makeup details. It alsoemploys content and structural control modules to preserve the content andstructural information of the source image. With the aid of our newly addedmakeup cross-attention layers in U-Net, we can accurately transfer the detailedmakeup to the corresponding position in the source image. Aftercontent-structure decoupling training, Stable-Makeup can maintain content andthe facial structure of the source image. Moreover, our method has demonstratedstrong robustness and generalizability, making it applicable to varioustaskssuch as cross-domain makeup transfer, makeup-guided text-to-image generationand so on. Extensive experiments have demonstrated that our approach deliversstate-of-the-art (SOTA) results among existing makeup transfer methods andexhibits a highly promising with broad potential applications in variousrelated fields.</description><author>Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, Haibo Zhao</author><pubDate>Tue, 12 Mar 2024 16:53:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07764v1</guid></item><item><title>MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback</title><link>http://arxiv.org/abs/2309.10691v3</link><description>To solve complex tasks, large language models (LLMs) often require multiplerounds of interactions with the user, sometimes assisted by external tools.However, current evaluation protocols often emphasize benchmark performancewith single-turn exchanges, neglecting the nuanced interactions among the user,LLMs, and external tools, while also underestimating the importance of naturallanguage feedback from users. These oversights contribute to discrepanciesbetween research benchmark evaluations and real-world use cases. We introduceMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turninteractions by (1) using tools and (2) leveraging natural language feedback.To ensure reproducibility, we provide an evaluation framework where LLMs canaccess tools by executing Python code and receive users' natural languagefeedback simulated by GPT-4. We repurpose a diverse set of establishedevaluation datasets focusing on reasoning, coding, and decision-making andcarefully curate them into a compact subset for efficient evaluation. Ouranalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)LLMs generally benefit from tools and language feedback, with performance gains(absolute, same below) of 1-8% for each turn of tool use and 2-17% with naturallanguage feedback. (b) Better single-turn performance does not guarantee bettermulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervisedinstruction-finetuning (SIFT) and reinforcement learning from human feedback(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measureprogress and incentivize research in improving LLMs' capabilities in multi-turninteractions, especially for open-source communities where multi-turn humanevaluation can be less accessible compared to commercial LLMs with a largeruser base.</description><author>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji</author><pubDate>Tue, 12 Mar 2024 16:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10691v3</guid></item><item><title>MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning</title><link>http://arxiv.org/abs/2403.06914v2</link><description>Large Language models (LLMs) have demonstrated impressive in-context learning(ICL) capabilities, where a LLM makes predictions for a given test inputtogether with a few input-output pairs (demonstrations). Nevertheless, theinclusion of demonstrations leads to a quadratic increase in the computationaloverhead of the self-attention mechanism. Existing solutions attempt to distilllengthy demonstrations into compact vectors. However, they often requiretask-specific retraining or compromise LLM's in-context learning performance.To mitigate these challenges, we present Meta dEmonstratioN Distillation(MEND), where a language model learns to distill any lengthy demonstrationsinto vectors without retraining for a new downstream task. We exploit theknowledge distillation to enhance alignment between MEND and LLM, achievingboth efficiency and effectiveness simultaneously. MEND is endowed with themeta-knowledge of distilling demonstrations through a two-stage trainingprocess, which includes meta-distillation pretraining and fine-tuning.Comprehensive evaluations across seven diverse ICL task partitions usingdecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It notonly matches but often outperforms the Vanilla ICL as well as otherstate-of-the-art distillation models, while significantly reducing thecomputational demands. This innovation promises enhanced scalability andefficiency for the practical deployment of large language models</description><author>Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo</author><pubDate>Tue, 12 Mar 2024 16:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06914v2</guid></item><item><title>Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language Conversion for Language Models</title><link>http://arxiv.org/abs/2401.11725v2</link><description>Symbols (or more broadly, non-natural language textual representations) suchas numerical sequences, molecular formulas, and table delimiters widely exist,playing important roles in various tasks such as abstract reasoning, chemicalproperty prediction, and table question answering. Despite the impressivenatural language comprehension capabilities of large language models (LLMs),their reasoning abilities for symbols remain inadequate, which could attributedto the difference between symbol representations and general natural languages.We propose symbol-to-language (S2L), a tuning-free method that enables largelanguage models to solve symbol-related problems with information expressed innatural language. Specifically, S2L first converts the symbols involved tolanguage-based representations, which can be implemented by prompting LLMs orleveraging external tools, then these language-based representations areintegrated into the original problem via direct substitution or concatenation,serving as useful input information for LLMs. We evaluate the S2L method usingboth API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eightsymbol-related tasks, ranging from symbol-only abstract reasoning to sentimentanalysis in social media. Experimental results show that S2L consistently leadsto superior performance. For example, by employing S2L for GPT-4, there can beaverage significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC andDyck language, respectively. Codes and data are available athttps://github.com/THUNLP-MT/symbol2language.</description><author>Yile Wang, Sijie Cheng, Zixin Sun, Peng Li, Yang Liu</author><pubDate>Tue, 12 Mar 2024 16:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11725v2</guid></item><item><title>Evaluating a Methodology for Increasing AI Transparency: A Case Study</title><link>http://arxiv.org/abs/2201.13224v2</link><description>In reaction to growing concerns about the potential harms of artificialintelligence (AI), societies have begun to demand more transparency about howAI models and systems are created and used. To address these concerns, severalefforts have proposed documentation templates containing questions to beanswered by model developers. These templates provide a useful starting point,but no single template can cover the needs of diverse documentation consumers.It is possible in principle, however, to create a repeatable methodology togenerate truly useful documentation. Richards et al. [25] proposed such amethodology for identifying specific documentation needs and creating templatesto address those needs. Although this is a promising proposal, it has not beenevaluated. This paper presents the first evaluation of this user-centered methodology inpractice, reporting on the experiences of a team in the domain of AI forhealthcare that adopted it to increase transparency for several AI models. Themethodology was found to be usable by developers not trained in user-centeredtechniques, guiding them to creating a documentation template that addressedthe specific needs of their consumers while still being reusable acrossdifferent models and use cases. Analysis of the benefits and costs of thismethodology are reviewed and suggestions for further improvement in both themethodology and supporting tools are summarized.</description><author>David Piorkowski, John Richards, Michael Hind</author><pubDate>Tue, 12 Mar 2024 16:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.13224v2</guid></item><item><title>DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis</title><link>http://arxiv.org/abs/2303.14207v2</link><description>We present DiffuScene for indoor 3D scene synthesis based on a novel sceneconfiguration denoising diffusion model. It generates 3D instance propertiesstored in an unordered object set and retrieves the most similar geometry foreach object configuration, which is characterized as a concatenation ofdifferent attributes, including location, size, orientation, semantics, andgeometry features. We introduce a diffusion network to synthesize a collectionof 3D indoor objects by denoising a set of unordered object attributes.Unordered parametrization simplifies and eases the joint distributionapproximation. The shape feature diffusion facilitates natural objectplacements, including symmetries. Our method enables many downstreamapplications, including scene completion, scene arrangement, andtext-conditioned scene synthesis. Experiments on the 3D-FRONT dataset show thatour method can synthesize more physically plausible and diverse indoor scenesthan state-of-the-art methods. Extensive ablation studies verify theeffectiveness of our design choice in scene diffusion models.</description><author>Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, Matthias Nießner</author><pubDate>Tue, 12 Mar 2024 16:40:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14207v2</guid></item><item><title>Vision-based Vehicle Re-identification in Bridge Scenario using Flock Similarity</title><link>http://arxiv.org/abs/2403.07752v1</link><description>Due to the needs of road traffic flow monitoring and public safetymanagement, video surveillance cameras are widely distributed in urban roads.However, the information captured directly by each camera is siloed, making itdifficult to use it effectively. Vehicle re-identification refers to finding avehicle that appears under one camera in another camera, which can correlatethe information captured by multiple cameras. While license plate recognitionplays an important role in some applications, there are some scenarios wherere-identification method based on vehicle appearance are more suitable. Themain challenge is that the data of vehicle appearance has the characteristicsof high inter-class similarity and large intra-class differences. Therefore, itis difficult to accurately distinguish between different vehicles by relyingonly on vehicle appearance information. At this time, it is often necessary tointroduce some extra information, such as spatio-temporal information.Nevertheless, the relative position of the vehicles rarely changes when passingthrough two adjacent cameras in the bridge scenario. In this paper, we presenta vehicle re-identification method based on flock similarity, which improvesthe accuracy of vehicle re-identification by utilizing vehicle informationadjacent to the target vehicle. When the relative position of the vehiclesremains unchanged and flock size is appropriate, we obtain an average relativeimprovement of 204% on VeRi dataset in our experiments. Then, the effect of themagnitude of the relative position change of the vehicles as they pass throughtwo cameras is discussed. We present two metrics that can be used to quantifythe difference and establish a connection between them. Although thisassumption is based on the bridge scenario, it is often true in other scenariosdue to driving safety and camera location.</description><author>Chunfeng Zhang, Ping Wang</author><pubDate>Tue, 12 Mar 2024 16:39:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07752v1</guid></item><item><title>Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings</title><link>http://arxiv.org/abs/2403.07750v1</link><description>The creation of high-quality human-labeled image-caption datasets presents asignificant bottleneck in the development of Visual-Language Models (VLMs). Wepropose a novel approach that leverages the strengths of Large Language Models(LLMs) and image generation models to create synthetic image-text pairs forefficient and effective VLM training. Our method employs pretraining atext-to-image model to synthesize image embeddings starting from captionsgenerated by an LLM. These synthetic pairs are then used to train a VLM.Extensive experiments demonstrate that the VLM trained with synthetic dataexhibits comparable performance on image captioning, while requiring a fractionof the data used by models trained solely on human-annotated data. Inparticular, we outperform the baseline by 17% through augmentation with asynthetic dataset. Furthermore, we show that synthesizing in the imageembedding space is 25% faster than in the pixel space. This research introducesa promising technique for generating large-scale, customizable image datasets,leading to enhanced VLM performance and wider applicability across variousdomains, all with improved data efficiency and resource utilization.</description><author>Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, Dharshan Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, Andrea Banino</author><pubDate>Tue, 12 Mar 2024 16:36:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07750v1</guid></item><item><title>Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics</title><link>http://arxiv.org/abs/2310.07990v2</link><description>Background: Missing data is a common challenge in mass spectrometry-basedmetabolomics, which can lead to biased and incomplete analyses. The integrationof whole-genome sequencing (WGS) data with metabolomics data has emerged as apromising approach to enhance the accuracy of data imputation in metabolomicsstudies. Method: In this study, we propose a novel method that leverages theinformation from WGS data and reference metabolites to impute unknownmetabolites. Our approach utilizes a multi-view variational autoencoder tojointly model the burden score, polygenetic risk score (PGS), and linkagedisequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for featureextraction and missing metabolomics data imputation. By learning the latentrepresentations of both omics data, our method can effectively impute missingmetabolomics values based on genomic information. Results: We evaluate theperformance of our method on empirical metabolomics datasets with missingvalues and demonstrate its superiority compared to conventional imputationtechniques. Using 35 template metabolites derived burden scores, PGS andLD-pruned SNPs, the proposed methods achieved R^2-scores &gt; 0.01 for 71.55% ofmetabolites. Conclusion: The integration of WGS data in metabolomics imputationnot only improves data completeness but also enhances downstream analyses,paving the way for more comprehensive and accurate investigations of metabolicpathways and disease associations. Our findings offer valuable insights intothe potential benefits of utilizing WGS data for metabolomics data imputationand underscore the importance of leveraging multi-modal data integration inprecision medicine research.</description><author>Chen Zhao, Kuan-Jui Su, Chong Wu, Xuewei Cao, Qiuying Sha, Wu Li, Zhe Luo, Tian Qin, Chuan Qiu, Lan Juan Zhao, Anqi Liu, Lindong Jiang, Xiao Zhang, Hui Shen, Weihua Zhou, Hong-Wen Deng</author><pubDate>Tue, 12 Mar 2024 16:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07990v2</guid></item><item><title>Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents in an Unknown Graph</title><link>http://arxiv.org/abs/2403.07748v1</link><description>We investigate two fundamental problems in mobile computing: exploration andrendezvous, with two distinct mobile agents in an unknown graph. The agents canread and write information on whiteboards that are located at all nodes. Theyboth move along one adjacent edge at every time-step. In the explorationproblem, both agents start from the same node of the graph and must traverseall of its edges. We show that a simple variant of depth-first search achievescollective exploration in $m$ synchronous time-steps, where $m$ is the numberof edges of the graph. This improves the competitive ratio of collective graphexploration. In the rendezvous problem, the agents start from different nodesof the graph and must meet as fast as possible. We introduce an algorithmguaranteeing rendezvous in at most $\frac{3}{2}m$ time-steps. This improvesover the so-called `wait for Mommy' algorithm which requires $2m$ time-steps.All our guarantees are derived from a more general asynchronous setting inwhich the speeds of the agents are controlled by an adversary at all times. Ourguarantees also generalize to weighted graphs, if the number of edges $m$ isreplaced by the sum of all edge lengths.</description><author>Romain Cosson</author><pubDate>Tue, 12 Mar 2024 16:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07748v1</guid></item><item><title>FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models</title><link>http://arxiv.org/abs/2403.07747v1</link><description>To thoroughly assess the mathematical reasoning abilities of Large LanguageModels (LLMs), we need to carefully curate evaluation datasets covering diversemathematical concepts and mathematical problems at different difficulty levels.In pursuit of this objective, we propose FineMath in this paper, a fine-grainedmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMathis created to cover the major key mathematical concepts taught in elementaryschool math, which are further divided into 17 categories of math wordproblems, enabling in-depth analysis of mathematical reasoning abilities ofLLMs. All the 17 categories of math word problems are manually annotated withtheir difficulty levels according to the number of reasoning steps required tosolve these problems. We conduct extensive experiments on a wide range of LLMson FineMath and find that there is still considerable room for improvements interms of mathematical reasoning capability of Chinese LLMs. We also carry outan in-depth analysis on the evaluation process and methods that have beenoverlooked previously. These two factors significantly influence the modelresults and our understanding of their mathematical reasoning capabilities. Thedataset will be publicly available soon.</description><author>Yan Liu, Renren Jin, Lin Shi, Zheng Yao, Deyi Xiong</author><pubDate>Tue, 12 Mar 2024 16:32:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07747v1</guid></item><item><title>StoRM: A Diffusion-based Stochastic Regeneration Model for Speech Enhancement and Dereverberation</title><link>http://arxiv.org/abs/2212.11851v2</link><description>Diffusion models have shown a great ability at bridging the performance gapbetween predictive and generative approaches for speech enhancement. We haveshown that they may even outperform their predictive counterparts fornon-additive corruption types or when they are evaluated on mismatchedconditions. However, diffusion models suffer from a high computational burden,mainly as they require to run a neural network for each reverse diffusion step,whereas predictive approaches only require one pass. As diffusion models aregenerative approaches they may also produce vocalizing and breathing artifactsin adverse conditions. In comparison, in such difficult scenarios, predictivemodels typically do not produce such artifacts but tend to distort the targetspeech instead, thereby degrading the speech quality. In this work, we presenta stochastic regeneration approach where an estimate given by a predictivemodel is provided as a guide for further diffusion. We show that the proposedapproach uses the predictive model to remove the vocalizing and breathingartifacts while producing very high quality samples thanks to the diffusionmodel, even in adverse conditions. We further show that this approach enablesto use lighter sampling schemes with fewer diffusion steps without sacrificingquality, thus lifting the computational burden by an order of magnitude. Sourcecode and audio examples are available online (https://uhh.de/inf-sp-storm).</description><author>Jean-Marie Lemercier, Julius Richter, Simon Welker, Timo Gerkmann</author><pubDate>Tue, 12 Mar 2024 16:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.11851v2</guid></item><item><title>Towards Saner Deep Image Registration</title><link>http://arxiv.org/abs/2307.09696v3</link><description>With recent advances in computing hardware and surges of deep-learningarchitectures, learning-based deep image registration methods have surpassedtheir traditional counterparts, in terms of metric performance and inferencetime. However, these methods focus on improving performance measurements suchas Dice, resulting in less attention given to model behaviors that are equallydesirable for registrations, especially for medical imaging. This paperinvestigates these behaviors for popular learning-based deep registrationsunder a sanity-checking microscope. We find that most existing registrationssuffer from low inverse consistency and nondiscrimination of identical pairsdue to overly optimized image similarities. To rectify these behaviors, wepropose a novel regularization-based sanity-enforcer method that imposes twosanity checks on the deep model to reduce its inverse consistency errors andincrease its discriminative power simultaneously. Moreover, we derive a set oftheoretical guarantees for our sanity-checked image registration method, withexperimental results supporting our theoretical findings and theireffectiveness in increasing the sanity of models without sacrificing anyperformance. Our code and models are available athttps://github.com/tuffr5/Saner-deep-registration.</description><author>Bin Duan, Ming Zhong, Yan Yan</author><pubDate>Tue, 12 Mar 2024 16:29:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09696v3</guid></item><item><title>Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified 3D Perception</title><link>http://arxiv.org/abs/2403.07746v1</link><description>Low-cost, vision-centric 3D perception systems for autonomous driving havemade significant progress in recent years, narrowing the gap to expensiveLiDAR-based methods. The primary challenge in becoming a fully reliablealternative lies in robust depth prediction capabilities, as camera-basedsystems struggle with long detection ranges and adverse lighting and weatherconditions. In this work, we introduce HyDRa, a novel camera-radar fusionarchitecture for diverse 3D perception tasks. Building upon the principles ofdense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybridfusion approach to combine the strengths of complementary camera and radarfeatures in two distinct representation spaces. Our Height AssociationTransformer module leverages radar features already in the perspective view toproduce more robust and accurate depth predictions. In the BEV, we refine theinitial sparse representation by a Radar-weighted Depth Consistency. HyDRaachieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our newsemantically rich and spatially accurate BEV features can be directly convertedinto a powerful occupancy representation, beating all previous camera-basedmethods on the Occ3D benchmark by an impressive 3.7 mIoU.</description><author>Philipp Wolters, Johannes Gilg, Torben Teepe, Fabian Herzog, Anouar Laouichi, Martin Hofmann, Gerhard Rigoll</author><pubDate>Tue, 12 Mar 2024 16:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07746v1</guid></item><item><title>Probabilistic Easy Variational Causal Effect</title><link>http://arxiv.org/abs/2403.07745v1</link><description>Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the onehand, for the case that $X$ and $Z$ are continuous, by using the ideas from thetotal variation and the flux of $g$, we develop a point of view in causalinference capable of dealing with a broad domain of causal problems. Indeed, wefocus on a function, called Probabilistic Easy Variational Causal Effect(PEACE), which can measure the direct causal effect of $X$ on $Y$ with respectto continuously and interventionally changing the values of $X$ while keepingthe value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degreemanaging the strengths of probability density values $f(x|z)$. On the otherhand, we generalize the above idea for the discrete case and show itscompatibility with the continuous case. Further, we investigate some propertiesof PEACE using measure theoretical concepts. Furthermore, we provide someidentifiability criteria and several examples showing the generic capability ofPEACE. We note that PEACE can deal with the causal problems for whichmicro-level or just macro-level changes in the value of the input variables areimportant. Finally, PEACE is stable under small changes in $\partialg_{in}/\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ isobtained from $g$ by removing all functional relationships defining $X$ and$Z$.</description><author>Usef Faghihi, Amir Saki</author><pubDate>Tue, 12 Mar 2024 16:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07745v1</guid></item><item><title>VideoMamba: State Space Model for Efficient Video Understanding</title><link>http://arxiv.org/abs/2403.06977v2</link><description>Addressing the dual challenges of local redundancy and global dependencies invideo understanding, this work innovatively adapts the Mamba to the videodomain. The proposed VideoMamba overcomes the limitations of existing 3Dconvolution neural networks and video transformers. Its linear-complexityoperator enables efficient long-term modeling, which is crucial forhigh-resolution long video understanding. Extensive evaluations revealVideoMamba's four core abilities: (1) Scalability in the visual domain withoutextensive dataset pretraining, thanks to a novel self-distillation technique;(2) Sensitivity for recognizing short-term actions even with fine-grainedmotion differences; (3) Superiority in long-term video understanding,showcasing significant advancements over traditional feature-based models; and(4) Compatibility with other modalities, demonstrating robustness inmulti-modal contexts. Through these distinct advantages, VideoMamba sets a newbenchmark for video understanding, offering a scalable and efficient solutionfor comprehensive video understanding. All the code and models are available athttps://github.com/OpenGVLab/VideoMamba.</description><author>Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao</author><pubDate>Tue, 12 Mar 2024 16:22:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06977v2</guid></item><item><title>Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs</title><link>http://arxiv.org/abs/2403.07743v1</link><description>Histopathology is a gold standard for cancer diagnosis under a microscopicexamination. However, histological tissue processing procedures result inartifacts, which are ultimately transferred to the digitized version of glassslides, known as whole slide images (WSIs). Artifacts are diagnosticallyirrelevant areas and may result in wrong deep learning (DL) algorithmspredictions. Therefore, detecting and excluding artifacts in the computationalpathology (CPATH) system is essential for reliable automated diagnosis. In thispaper, we propose a mixture of experts (MoE) scheme for detecting five notableartifacts, including damaged tissue, blur, folded tissue, air bubbles, andhistologically irrelevant blood from WSIs. First, we train independent binaryDL models as experts to capture particular artifact morphology. Then, weensemble their predictions using a fusion mechanism. We apply probabilisticthresholding over the final probability distribution to improve the sensitivityof the MoE. We developed DL pipelines using two MoEs and two multiclass modelsof state-of-the-art deep convolutional neural networks (DCNNs) and visiontransformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformedsimpler multiclass models and were tested on datasets from different hospitalsand cancer types, where MoE using DCNNs yielded the best results. The proposedMoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retainingless computational cost for inference than MoE using ViTs. This bestperformance of MoEs comes with relatively higher computational trade-offs thanmulticlass models. The proposed artifact detection pipeline will not onlyensure reliable CPATH predictions but may also provide quality control.</description><author>Neel Kanwal, Farbod Khoraminia, Umay Kiraz, Andres Mosquera-Zamudio, Carlos Monteagudo, Emiel A. M. Janssen, Tahlita C. M. Zuiverloon, Chunmig Rong, Kjersti Engan</author><pubDate>Tue, 12 Mar 2024 16:22:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07743v1</guid></item><item><title>Toward a Plug-and-Play Vision-Based Grasping Module for Robotics</title><link>http://arxiv.org/abs/2310.04349v2</link><description>Despite recent advancements in AI for robotics, grasping remains a partiallysolved challenge, hindered by the lack of benchmarks and reproducibilityconstraints. This paper introduces a vision-based grasping framework that caneasily be transferred across multiple manipulators. LeveragingQuality-Diversity (QD) algorithms, the framework generates diverse repertoiresof open-loop grasping trajectories, enhancing adaptability while maintaining adiversity of grasps. This framework addresses two main issues: the lack of anoff-the-shelf vision module for detecting object pose and the generalization ofQD trajectories to the whole robot operational space. The proposed solutioncombines multiple vision modules for 6DoF object detection and tracking whilerigidly transforming QD-generated trajectories into the object frame.Experiments on a Franka Research 3 arm and a UR5 arm with a SIH Schunk handdemonstrate comparable performance when the real scene aligns with thesimulation used for grasp generation. This work represents a significant stridetoward building a reliable vision-based grasping module transferable to newplatforms, while being adaptable to diverse scenarios without further trainingiterations.</description><author>François Hélénon, Johann Huber, Faïz Ben Amar, Stéphane Doncieux</author><pubDate>Tue, 12 Mar 2024 16:22:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04349v2</guid></item><item><title>Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation</title><link>http://arxiv.org/abs/2403.07741v1</link><description>The estimation of 6D object poses is a fundamental task in many computervision applications. Particularly, in high risk scenarios such as human-robotinteraction, industrial inspection, and automation, reliable pose estimates arecrucial. In the last years, increasingly accurate and robustdeep-learning-based approaches for 6D object pose estimation have beenproposed. Many top-performing methods are not end-to-end trainable but consistof multiple stages. In the context of deep uncertainty quantification, deepensembles are considered as state of the art since they have been proven toproduce well-calibrated and robust uncertainty estimates. However, deepensembles can only be applied to methods that can be trained end-to-end. Inthis work, we propose a method to quantify the uncertainty of multi-stage 6Dobject pose estimation approaches with deep ensembles. For the implementation,we choose SurfEmb as representative, since it is one of the top-performing 6Dobject pose estimation approaches in the BOP Challenge 2022. We applyestablished metrics and concepts for deep uncertainty quantification toevaluate the results. Furthermore, we propose a novel uncertainty calibrationscore for regression tasks to quantify the quality of the estimateduncertainty.</description><author>Kira Wursthorn, Markus Hillemann, Markus Ulrich</author><pubDate>Tue, 12 Mar 2024 16:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07741v1</guid></item><item><title>The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels</title><link>http://arxiv.org/abs/2403.07735v1</link><description>Kernel techniques are among the most influential approaches in data scienceand statistics. Under mild conditions, the reproducing kernel Hilbert spaceassociated to a kernel is capable of encoding the independence of $M\ge 2$random variables. Probably the most widespread independence measure relying onkernels is the so-called Hilbert-Schmidt independence criterion (HSIC; alsoreferred to as distance covariance in the statistics literature). Despitevarious existing HSIC estimators designed since its introduction close to twodecades ago, the fundamental question of the rate at which HSIC can beestimated is still open. In this work, we prove that the minimax optimal rateof HSIC estimation on $\mathbb R^d$ for Borel measures containing the Gaussianswith continuous bounded translation-invariant characteristic kernels is$\mathcal O\!\left(n^{-1/2}\right)$. Specifically, our result implies theoptimality in the minimax sense of many of the most-frequently used estimators(including the U-statistic, the V-statistic, and the Nystr\"om-based one) on$\mathbb R^d$.</description><author>Florian Kalinke, Zoltan Szabo</author><pubDate>Tue, 12 Mar 2024 16:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07735v1</guid></item><item><title>DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven Segmentation</title><link>http://arxiv.org/abs/2403.07733v1</link><description>Explainable Artificial Intelligence is critical in unraveling decision-makingprocesses in complex machine learning models. LIME (Local InterpretableModel-agnostic Explanations) is a well-known XAI framework for image analysis.It utilizes image segmentation to create features to identify relevant areasfor classification. Consequently, poor segmentation can compromise theconsistency of the explanation and undermine the importance of the segments,affecting the overall interpretability. Addressing these challenges, weintroduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) adata-driven segmentation for human-recognized feature generation, and ii) ahierarchical segmentation procedure through composition. We benchmark DSEG-LIMEon pre-trained models with images from the ImageNet dataset - scenarios withoutdomain-specific knowledge. The analysis includes a quantitative evaluationusing established XAI metrics, complemented by a qualitative assessment througha user study. Our findings demonstrate that DSEG outperforms in most of the XAImetrics and enhances the alignment of explanations with human-recognizedconcepts, significantly improving interpretability. The code is availableunder: https://github. com/patrick-knab/DSEG-LIME</description><author>Patrick Knab, Sascha Marton, Christian Bartelt</author><pubDate>Tue, 12 Mar 2024 16:13:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07733v1</guid></item><item><title>WaveNets: Wavelet Channel Attention Networks</title><link>http://arxiv.org/abs/2211.02695v2</link><description>Channel Attention reigns supreme as an effective technique in the field ofcomputer vision. However, the proposed channel attention by SENet suffers frominformation loss in feature learning caused by the use of Global AveragePooling (GAP) to represent channels as scalars. Thus, designing effectivechannel attention mechanisms requires finding a solution to enhance featurespreservation in modeling channel inter-dependencies. In this work, we utilizeWavelet transform compression as a solution to the channel representationproblem. We first test wavelet transform as an Auto-Encoder model equipped withconventional channel attention module. Next, we test wavelet transform as astandalone channel compression method. We prove that global average pooling isequivalent to the recursive approximate Haar wavelet transform. With thisproof, we generalize channel attention using Wavelet compression and name itWaveNet. Implementation of our method can be embedded within existing channelattention methods with a couple of lines of code. We test our proposed methodusing ImageNet dataset for image classification task. Our method outperformsthe baseline SENet, and achieves the state-of-the-art results. Our codeimplementation is publicly available at https://github.com/hady1011/WaveNet-C.</description><author>Hadi Salman, Caleb Parks, Shi Yin Hong, Justin Zhan</author><pubDate>Tue, 12 Mar 2024 16:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.02695v2</guid></item><item><title>CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control</title><link>http://arxiv.org/abs/2403.07728v1</link><description>We study the problem of post-selection predictive inference in an onlinefashion. To avoid devoting resources to unimportant units, a preliminaryselection of the current individual before reporting its prediction interval iscommon and meaningful in online predictive tasks. Since the online selectioncauses a temporal multiplicity in the selected prediction intervals, it isimportant to control the real-time false coverage-statement rate (FCR) tomeasure the averaged miscoverage error. We develop a general framework namedCAS (Calibration after Adaptive Selection) that can wrap around any predictionmodel and online selection rule to output post-selection prediction intervals.If the current individual is selected, we first perform an adaptive selectionon historical data to construct a calibration set, then output a conformalprediction interval for the unobserved label. We provide tractableconstructions for the calibration set for popular online selection rules. Weproved that CAS can achieve an exact selection-conditional coverage guaranteein the finite-sample and distribution-free regimes. For the decision-drivenselection rule, including most online multiple-testing procedures, CAS canexactly control the real-time FCR below the target level without anydistributional assumptions. For the online selection with symmetric thresholds,we establish the error bound for the control gap of FCR under milddistributional assumptions. To account for the distribution shift in onlinedata, we also embed CAS into some recent dynamic conformal prediction methodsand examine the long-run FCR control. Numerical results on both synthetic andreal data corroborate that CAS can effectively control FCR around the targetlevel and yield more narrowed prediction intervals over existing baselinesacross various settings.</description><author>Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou</author><pubDate>Tue, 12 Mar 2024 16:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07728v1</guid></item><item><title>A local approach to parameter space reduction for regression and classification tasks</title><link>http://arxiv.org/abs/2107.10867v3</link><description>Parameter space reduction has been proved to be a crucial tool to speed-upthe execution of many numerical tasks such as optimization, inverse problems,sensitivity analysis, and surrogate models' design, especially when in presenceof high-dimensional parametrized systems. In this work we propose a new methodcalled local active subspaces (LAS), which explores the synergies of activesubspaces with supervised clustering techniques in order to carry out a moreefficient dimension reduction in the parameter space. The clustering isperformed without losing the input-output relations by introducing a distancemetric induced by the global active subspace. We present two possibleclustering algorithms: K-medoids and a hierarchical top-down approach, which isable to impose a variety of subdivision criteria specifically tailored forparameter space reduction tasks. This method is particularly useful for thecommunity working on surrogate modelling. Frequently, the parameter spacepresents subdomains where the objective function of interest varies less onaverage along different directions. So, it could be approximated moreaccurately if restricted to those subdomains and studied separately. We testedthe new method over several numerical experiments of increasing complexity, weshow how to deal with vectorial outputs, and how to classify the differentregions with respect to the local active subspace dimension. Employing thisclassification technique as a preprocessing step in the parameter space, oroutput space in case of vectorial outputs, brings remarkable results for thepurpose of surrogate modelling.</description><author>Francesco Romor, Marco Tezzele, Gianluigi Rozza</author><pubDate>Tue, 12 Mar 2024 16:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.10867v3</guid></item><item><title>SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes</title><link>http://arxiv.org/abs/2403.07726v1</link><description>This paper presents the results of the SHROOM, a shared task focused ondetecting hallucinations: outputs from natural language generation (NLG)systems that are fluent, yet inaccurate. Such cases of overgeneration put injeopardy many NLG applications, where correctness is often mission-critical.The shared task was conducted with a newly constructed dataset of 4000 modeloutputs labeled by 5 annotators each, spanning 3 NLP tasks: machinetranslation, paraphrase generation and definition modeling. The shared task was tackled by a total of 58 different users grouped in 42teams, out of which 27 elected to write a system description paper;collectively, they submitted over 300 prediction sets on both tracks of theshared task. We observe a number of key trends in how this approach was tackled-- many participants rely on a handful of model, and often rely either onsynthetic data for fine-tuning or zero-shot prompting strategies. While amajority of the teams did outperform our proposed baseline system, theperformances of top-scoring systems are still consistent with a random handlingof the more challenging items.</description><author>Timothee Mickus, Elaine Zosa, Raúl Vázquez, Teemu Vahtola, Jörg Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki</author><pubDate>Tue, 12 Mar 2024 16:06:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07726v1</guid></item><item><title>Robustifying Point Cloud Networks by Refocusing</title><link>http://arxiv.org/abs/2308.05525v3</link><description>The ability to cope with out-of-distribution (OOD) corruptions andadversarial attacks is crucial in real-world safety-demanding applications. Inthis study, we develop a general mechanism to increase neural networkrobustness based on focus analysis. Recent studies have revealed the phenomenon of \textit{Overfocusing}, whichleads to a performance drop. When the network is primarily influenced by smallinput regions, it becomes less robust and prone to misclassify under noise andcorruptions. However, quantifying overfocusing is still vague and lacks clear definitions.Here, we provide a mathematical definition of \textbf{focus},\textbf{overfocusing} and \textbf{underfocusing}. The notions are general, butin this study, we specifically investigate the case of 3D point clouds. We observe that corrupted sets result in a biased focus distribution comparedto the clean training set. We show that as focus distribution deviates from the one learned in thetraining phase - classification performance deteriorates. We thus propose a parameter-free \textbf{refocusing} algorithm that aims tounify all corruptions under the same distribution. We validate our findings on a 3D zero-shot classification task, achievingSOTA in robust 3D classification on ModelNet-C dataset, and in adversarialdefense against Shape-Invariant attack. Code is available in:https://github.com/yossilevii100/refocusing.</description><author>Meir Yossef Levi, Guy Gilboa</author><pubDate>Tue, 12 Mar 2024 16:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05525v3</guid></item><item><title>Balancing Fairness and Accuracy in Data-Restricted Binary Classification</title><link>http://arxiv.org/abs/2403.07724v1</link><description>Applications that deal with sensitive information may have restrictionsplaced on the data available to a machine learning (ML) classifier. Forexample, in some applications, a classifier may not have direct access tosensitive attributes, affecting its ability to produce accurate and fairdecisions. This paper proposes a framework that models the trade-off betweenaccuracy and fairness under four practical scenarios that dictate the type ofdata available for analysis. Prior works examine this trade-off by analyzingthe outputs of a scoring function that has been trained to implicitly learn theunderlying distribution of the feature vector, class label, and sensitiveattribute of a dataset. In contrast, our framework directly analyzes thebehavior of the optimal Bayesian classifier on this underlying distribution byconstructing a discrete approximation it from the dataset itself. This approachenables us to formulate multiple convex optimization problems, which allow usto answer the question: How is the accuracy of a Bayesian classifier affectedin different data restricting scenarios when constrained to be fair? Analysisis performed on a set of fairness definitions that include group and individualfairness. Experiments on three datasets demonstrate the utility of the proposedframework as a tool for quantifying the trade-offs among different fairnessnotions and their distributional dependencies.</description><author>Zachary McBride Lazri, Danial Dervovic, Antigoni Polychroniadou, Ivan Brugere, Dana Dachman-Soled, Min Wu</author><pubDate>Tue, 12 Mar 2024 16:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07724v1</guid></item><item><title>On the Last-Iterate Convergence of Shuffling Gradient Methods</title><link>http://arxiv.org/abs/2403.07723v1</link><description>Shuffling gradient methods, which are also known as stochastic gradientdescent (SGD) without replacement, are widely implemented in practice,particularly including three popular algorithms: Random Reshuffle (RR), ShuffleOnce (SO), and Incremental Gradient (IG). Compared to the empirical success,the theoretical guarantee of shuffling gradient methods was notwell-understanding for a long time. Until recently, the convergence rates hadjust been established for the average iterate for convex functions and the lastiterate for strongly convex problems (using squared distance as the metric).However, when using the function value gap as the convergence criterion,existing theories cannot interpret the good performance of the last iterate indifferent settings (e.g., constrained optimization). To bridge this gap betweenpractice and theory, we prove last-iterate convergence rates for shufflinggradient methods with respect to the objective value even without strongconvexity. Our new results either (nearly) match the existing last-iteratelower bounds or are as fast as the previous best upper bounds for the averageiterate.</description><author>Zijian Liu, Zhengyuan Zhou</author><pubDate>Tue, 12 Mar 2024 16:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07723v1</guid></item><item><title>Multi-modal Auto-regressive Modeling via Visual Words</title><link>http://arxiv.org/abs/2403.07720v1</link><description>Large Language Models (LLMs), benefiting from the auto-regressive modellingapproach performed on massive unannotated texts corpora, demonstrates powerfulperceptual and reasoning capabilities. However, as for extendingauto-regressive modelling to multi-modal scenarios to build Large Multi-modalModels (LMMs), there lies a great difficulty that the image information isprocessed in the LMM as continuous visual embeddings, which cannot obtaindiscrete supervised labels for classification. In this paper, we successfullyperform multi-modal auto-regressive modeling with a unified objective for thefirst time. Specifically, we propose the concept of visual words, which mapsthe visual features to probability distributions over LLM's vocabulary,providing supervision information for visual modelling. We further explore thedistribution of visual features in the semantic space within LMM and thepossibility of using text embeddings to represent visual information.Experimental results and ablation studies on 5 VQA tasks and 4 benchmarktoolkits validate the powerful performance of our proposed approach.</description><author>Tianshuo Peng, Zuchao Li, Lefei Zhang, Hai Zhao, Ping Wang, Bo Du</author><pubDate>Tue, 12 Mar 2024 15:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07720v1</guid></item><item><title>Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis</title><link>http://arxiv.org/abs/2403.07719v1</link><description>Histopathological whole slide images (WSIs) classification has become afoundation task in medical microscopic imaging processing. Prevailingapproaches involve learning WSIs as instance-bag representations, emphasizingsignificant instances but struggling to capture the interactions betweeninstances. Additionally, conventional graph representation methods utilizeexplicit spatial positions to construct topological structures but restrict theflexible interaction capabilities between instances at arbitrary locations,particularly when spatially distant. In response, we propose a novel dynamicgraph representation algorithm that conceptualizes WSIs as a form of theknowledge graph structure. Specifically, we dynamically construct neighbors anddirected edge embeddings based on the head and tail relationships betweeninstances. Then, we devise a knowledge-aware attention mechanism that canupdate the head node features by learning the joint attention score of eachneighbor and edge. Finally, we obtain a graph-level embedding through theglobal pooling process of the updated head, serving as an implicitrepresentation for the WSI classification. Our end-to-end graph representationlearning approach has outperformed the state-of-the-art WSI analysis methods onthree TCGA benchmark datasets and in-house test sets. Our code is available athttps://github.com/WonderLandxD/WiKG.</description><author>Jiawen Li, Yuxuan Chen, Hongbo Chu, Qiehe Sun, Tian Guan, Anjia Han, Yonghong He</author><pubDate>Tue, 12 Mar 2024 15:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07719v1</guid></item><item><title>WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?</title><link>http://arxiv.org/abs/2403.07718v1</link><description>We study the use of large language model-based agents for interacting withsoftware via web browsers. Unlike prior work, we focus on measuring the agents'ability to perform tasks that span the typical daily work of knowledge workersutilizing enterprise software systems. To this end, we propose WorkArena, aremote-hosted benchmark of 29 tasks based on the widely-used ServiceNowplatform. We also introduce BrowserGym, an environment for the design andevaluation of such agents, offering a rich set of actions as well as multimodalobservations. Our empirical evaluation reveals that while current agents showpromise on WorkArena, there remains a considerable gap towards achieving fulltask automation. Notably, our analysis uncovers a significant performancedisparity between open and closed-source LLMs, highlighting a critical area forfuture exploration and development in the field.</description><author>Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, Alexandre Lacoste</author><pubDate>Tue, 12 Mar 2024 15:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07718v1</guid></item><item><title>Integrating Uncertainty Awareness into Conformalized Quantile Regression</title><link>http://arxiv.org/abs/2306.08693v2</link><description>Conformalized Quantile Regression (CQR) is a recently proposed method forconstructing prediction intervals for a response $Y$ given covariates $X$,without making distributional assumptions. However, existing constructions ofCQR can be ineffective for problems where the quantile regressors performbetter in certain parts of the feature space than others. The reason is thatthe prediction intervals of CQR do not distinguish between two forms ofuncertainty: first, the variability of the conditional distribution of $Y$given $X$ (i.e., aleatoric uncertainty), and second, our uncertainty inestimating this conditional distribution (i.e., epistemic uncertainty). Thiscan lead to intervals that are overly narrow in regions where epistemicuncertainty is high. To address this, we propose a new variant of the CQRmethodology, Uncertainty-Aware CQR (UACQR), that explicitly separates these twosources of uncertainty to adjust quantile regressors differentially across thefeature space. Compared to CQR, our methods enjoy the same distribution-freetheoretical coverage guarantees, while demonstrating in our experimentsstronger conditional coverage properties in simulated settings and real-worlddata sets alike.</description><author>Raphael Rossellini, Rina Foygel Barber, Rebecca Willett</author><pubDate>Tue, 12 Mar 2024 15:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08693v2</guid></item><item><title>Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound</title><link>http://arxiv.org/abs/2403.07715v1</link><description>Self-supervised learning (SSL) is one strategy for addressing the paucity oflabelled data in medical imaging by learning representations from unlabelledimages. Contrastive and non-contrastive SSL methods produce learnedrepresentations that are similar for pairs of related images. Such pairs arecommonly constructed by randomly distorting the same image twice. Thevideographic nature of ultrasound offers flexibility for defining thesimilarity relationship between pairs of images. In this study, we investigatedthe effect of utilizing proximal, distinct images from the same B-modeultrasound video as pairs for SSL. Additionally, we introduced a sampleweighting scheme that increases the weight of closer image pairs anddemonstrated how it can be integrated into SSL objectives. Named Intra-VideoPositive Pairs (IVPP), the method surpassed previous ultrasound-specificcontrastive learning methods' average test accuracy on COVID-19 classificationwith the POCUS dataset by $\ge 1.3\%$. Detailed investigations of IVPP'shyperparameters revealed that some combinations of IVPP hyperparameters canlead to improved or worsened performance, depending on the downstream task.Guidelines for practitioners were synthesized based on the results, such as themerit of IVPP with task-specific hyperparameters, and the improved performanceof contrastive methods for ultrasound compared to non-contrastive counterparts.</description><author>Blake VanBerlo, Alexander Wong, Jesse Hoey, Robert Arntfield</author><pubDate>Tue, 12 Mar 2024 15:57:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07715v1</guid></item></channel></rss>