<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 20 Jul 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering</title><link>http://arxiv.org/abs/2307.10173v1</link><description>Realistic human-centric rendering plays a key role in both computer visionand computer graphics. Rapid progress has been made in the algorithm aspectover the years, yet existing human-centric rendering datasets and benchmarksare rather impoverished in terms of diversity, which are crucial for renderingeffect. Researchers are usually constrained to explore and evaluate a small setof rendering problems on current datasets, while real-world applicationsrequire methods to be robust across different scenarios. In this work, wepresent DNA-Rendering, a large-scale, high-fidelity repository of humanperformance data for neural actor rendering. DNA-Rendering presents severalalluring attributes. First, our dataset contains over 1500 human subjects, 5000motion sequences, and 67.5M frames' data volume. Second, we provide rich assetsfor each subject -- 2D/3D human body keypoints, foreground masks, SMPLX models,cloth/accessory materials, multi-view images, and videos. These assets boostthe current method's accuracy on downstream rendering tasks. Third, weconstruct a professional multi-view system to capture data, which contains 60synchronous cameras with max 4096 x 3000 resolution, 15 fps speed, and sterncamera calibration steps, ensuring high-quality resources for task training andevaluation. Along with the dataset, we provide a large-scale and quantitativebenchmark in full-scale, with multiple tasks to evaluate the existing progressof novel view synthesis, novel pose animation synthesis, and novel identityrendering methods. In this manuscript, we describe our DNA-Rendering effort asa revealing of new observations, challenges, and future directions tohuman-centric rendering. The dataset, code, and benchmarks will be publiclyavailable at https://dna-rendering.github.io/</description><author>Wei Cheng, Ruixiang Chen, Wanqi Yin, Siming Fan, Keyu Chen, Honglin He, Huiwen Luo, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, Daxuan Ren, Lei Yang, Ziwei Liu, Chen Change Loy, Chen Qian, Wayne Wu, Dahua Lin, Bo Dai, Kwan-Yee Lin</author><pubDate>Wed, 19 Jul 2023 18:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10173v1</guid></item><item><title>DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI</title><link>http://arxiv.org/abs/2307.10172v1</link><description>Despite advancements in conversational AI, language models encounterchallenges to handle diverse conversational tasks, and existing dialoguedataset collections often lack diversity and comprehensiveness. To tackle theseissues, we introduce DialogStudio: the largest and most diverse collection ofdialogue datasets, unified under a consistent format while preserving theiroriginal information. Our collection encompasses data from open-domaindialogues, task-oriented dialogues, natural language understanding,conversational recommendation, dialogue summarization, and knowledge-groundeddialogues, making it an incredibly rich and diverse resource for dialogueresearch and model training. To further enhance the utility of DialogStudio, weidentify the licenses for each dataset and design domain-aware prompts forselected dialogues to facilitate instruction-aware fine-tuning. Furthermore, wedevelop conversational AI models using the dataset collection, and ourexperiments in both zero-shot and few-shot learning scenarios demonstrate thesuperiority of DialogStudio. To improve transparency and support dataset andtask-based research, as well as language model pre-training, all datasets,licenses, codes, and models associated with DialogStudio are made publiclyaccessible at https://github.com/salesforce/DialogStudio</description><author>Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke, Rui Meng, Ye Liu, Zhou Yu, Silvio Savarese, Caiming Xiong</author><pubDate>Wed, 19 Jul 2023 18:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10172v1</guid></item><item><title>LightPath: Lightweight and Scalable Path Representation Learning</title><link>http://arxiv.org/abs/2307.10171v1</link><description>Movement paths are used widely in intelligent transportation and smart cityapplications. To serve such applications, path representation learning aims toprovide compact representations of paths that enable efficient and accurateoperations when used for different downstream tasks such as path ranking andtravel cost estimation. In many cases, it is attractive that the pathrepresentation learning is lightweight and scalable; in resource-limitedenvironments and under green computing limitations, it is essential. Yet,existing path representation learning studies focus on accuracy and pay at mostsecondary attention to resource consumption and scalability. We propose a lightweight and scalable path representation learning framework,termed LightPath, that aims to reduce resource consumption and achievescalability without affecting accuracy, thus enabling broader applicability.More specifically, we first propose a sparse auto-encoder that ensures that theframework achieves good scalability with respect to path length. Next, wepropose a relational reasoning framework to enable faster training of morerobust sparse path encoders. We also propose global-local knowledgedistillation to further reduce the size and improve the performance of sparsepath encoders. Finally, we report extensive experiments on two real-worlddatasets to offer insight into the efficiency, scalability, and effectivenessof the proposed framework.</description><author>Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen</author><pubDate>Wed, 19 Jul 2023 18:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10171v1</guid></item><item><title>Sequential Kernelized Independence Testing</title><link>http://arxiv.org/abs/2212.07383v3</link><description>Independence testing is a classical statistical problem that has beenextensively studied in the batch setting when one fixes the sample size beforecollecting data. However, practitioners often prefer procedures that adapt tothe complexity of a problem at hand instead of setting sample size in advance.Ideally, such procedures should (a) stop earlier on easy tasks (and later onharder tasks), hence making better use of available resources, and (b)continuously monitor the data and efficiently incorporate statistical evidenceafter collecting new data, while controlling the false alarm rate. Classicalbatch tests are not tailored for streaming data: valid inference after datapeeking requires correcting for multiple testing which results in low power.Following the principle of testing by betting, we design sequential kernelizedindependence tests that overcome such shortcomings. We exemplify our broadframework using bets inspired by kernelized dependence measures, e.g., theHilbert-Schmidt independence criterion. Our test is also valid undernon-i.i.d., time-varying settings. We demonstrate the power of our approacheson both simulated and real data.</description><author>Aleksandr Podkopaev, Patrick Bl√∂baum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas</author><pubDate>Wed, 19 Jul 2023 18:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07383v3</guid></item><item><title>Challenges and Applications of Large Language Models</title><link>http://arxiv.org/abs/2307.10169v1</link><description>Large Language Models (LLMs) went from non-existent to ubiquitous in themachine learning discourse within a few years. Due to the fast pace of thefield, it is difficult to identify the remaining challenges and alreadyfruitful application areas. In this paper, we aim to establish a systematic setof open problems and application successes so that ML researchers cancomprehend the field's current state more quickly and become productive.</description><author>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy</author><pubDate>Wed, 19 Jul 2023 18:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10169v1</guid></item><item><title>LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs</title><link>http://arxiv.org/abs/2307.10168v1</link><description>LLMs have shown promise in replicating human-like behavior in crowdsourcingtasks that were previously thought to be exclusive to human abilities. However,current efforts focus mainly on simple atomic tasks. We explore whether LLMscan replicate more complex crowdsourcing pipelines. We find that modern LLMscan simulate some of crowdworkers' abilities in these "human computationalgorithms," but the level of success is variable and influenced by requesters'understanding of LLM capabilities, the specific skills required for sub-tasks,and the optimal interaction modality for performing these sub-tasks. We reflecton human and LLMs' different sensitivities to instructions, stress theimportance of enabling human-facing safeguards for LLMs, and discuss thepotential of training humans and LLMs with complementary skill sets. Crucially,we show that replicating crowdsourcing pipelines offers a valuable platform toinvestigate (1) the relative strengths of LLMs on different tasks (bycross-comparing their performances on sub-tasks) and (2) LLMs' potential incomplex tasks, where they can complete part of the tasks while leaving othersto humans.</description><author>Tongshuang Wu, Haiyi Zhu, Maya Albayrak, Alexis Axon, Amanda Bertsch, Wenxing Deng, Ziqi Ding, Bill Guo, Sireesh Gururaja, Tzu-Sheng Kuo, Jenny T. Liang, Ryan Liu, Ihita Mandal, Jeremiah Milbauer, Xiaolin Ni, Namrata Padmanabhan, Subhashini Ramkumar, Alexis Sudjianto, Jordan Taylor, Ying-Jui Tseng, Patricia Vaidos, Zhijin Wu, Wei Wu, Chenyang Yang</author><pubDate>Wed, 19 Jul 2023 18:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10168v1</guid></item><item><title>VITS : Variational Inference Thomson Sampling for contextual bandits</title><link>http://arxiv.org/abs/2307.10167v1</link><description>In this paper, we introduce and analyze a variant of the Thompson sampling(TS) algorithm for contextual bandits. At each round, traditional TS requiressamples from the current posterior distribution, which is usually intractable.To circumvent this issue, approximate inference techniques can be used andprovide samples with distribution close to the posteriors. However, currentapproximate techniques yield to either poor estimation (Laplace approximation)or can be computationally expensive (MCMC methods, Ensemble sampling...). Inthis paper, we propose a new algorithm, Varational Inference Thompson samplingVITS, based on Gaussian Variational Inference. This scheme provides powerfulposterior approximations which are easy to sample from, and is computationallyefficient, making it an ideal choice for TS. In addition, we show that VITSachieves a sub-linear regret bound of the same order in the dimension andnumber of round as traditional TS for linear contextual bandit. Finally, wedemonstrate experimentally the effectiveness of VITS on both synthetic and realworld datasets.</description><author>Pierre Clavier, Tom Huix, Alain Durmus</author><pubDate>Wed, 19 Jul 2023 18:53:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10167v1</guid></item><item><title>Persistent Animal Identification Leveraging Non-Visual Markers</title><link>http://arxiv.org/abs/2112.06809v8</link><description>Our objective is to locate and provide a unique identifier for each mouse ina cluttered home-cage environment through time, as a precursor to automatedbehaviour recognition for biological research. This is a very challengingproblem due to (i) the lack of distinguishing visual features for each mouse,and (ii) the close confines of the scene with constant occlusion, makingstandard visual tracking approaches unusable. However, a coarse estimate ofeach mouse's location is available from a unique RFID implant, so there is thepotential to optimally combine information from (weak) tracking with coarseinformation on identity. To achieve our objective, we make the following keycontributions: (a) the formulation of the object identification problem as anassignment problem (solved using Integer Linear Programming), and (b) a novelprobabilistic model of the affinity between tracklets and RFID data. The latteris a crucial part of the model, as it provides a principled probabilistictreatment of object detections given coarse localisation. Our approach achieves77% accuracy on this animal identification problem, and is able to rejectspurious detections when the animals are hidden.</description><author>Michael P. J. Camilleri, Li Zhang, Rasneer S. Bains, Andrew Zisserman, Christopher K. I. Williams</author><pubDate>Wed, 19 Jul 2023 18:50:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.06809v8</guid></item><item><title>Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis</title><link>http://arxiv.org/abs/2307.10166v1</link><description>Generative Engineering Design approaches driven by Deep Generative Models(DGM) have been proposed to facilitate industrial engineering processes. Insuch processes, designs often come in the form of images, such as blueprints,engineering drawings, and CAD models depending on the level of detail. DGMshave been successfully employed for synthesis of natural images, e.g.,displaying animals, human faces and landscapes. However, industrial designimages are fundamentally different from natural scenes in that they containrich structural patterns and long-range dependencies, which are challenging forconvolution-based DGMs to generate. Moreover, DGM-driven generation process istypically triggered based on random noisy inputs, which outputs unpredictablesamples and thus cannot perform an efficient industrial design exploration. Wetackle these challenges by proposing a novel model Self-Attention AdversarialLatent Autoencoder (SA-ALAE), which allows generating feasible design images ofcomplex engineering parts. With SA-ALAE, users can not only explore novelvariants of an existing design, but also control the generation process byoperating in latent space. The potential of SA-ALAE is shown by generatingengineering blueprints in a real automotive design task.</description><author>Jiajie Fan, Laure Vuaille, Hao Wang, Thomas B√§ck</author><pubDate>Wed, 19 Jul 2023 18:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10166v1</guid></item><item><title>Drone navigation and license place detection for vehicle location in indoor spaces</title><link>http://arxiv.org/abs/2307.10165v1</link><description>Millions of vehicles are transported every year, tightly parked in vessels orboats. To reduce the risks of associated safety issues like fires, knowing thelocation of vehicles is essential, since different vehicles may need differentmitigation measures, e.g. electric cars. This work is aimed at creating asolution based on a nano-drone that navigates across rows of parked vehiclesand detects their license plates. We do so via a wall-following algorithm, anda CNN trained to detect license plates. All computations are done in real-timeon the drone, which just sends position and detected images that allow thecreation of a 2D map with the position of the plates. Our solution is capableof reading all plates across eight test cases (with several rows of plates,different drone speeds, or low light) by aggregation of measurements acrossseveral drone journeys.</description><author>Moa Arvidsson, Sithichot Sawirot, Cristofer Englund, Fernando Alonso-Fernandez, Martin Torstensson, Boris Duran</author><pubDate>Wed, 19 Jul 2023 18:46:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10165v1</guid></item><item><title>Rethinking Backdoor Attacks</title><link>http://arxiv.org/abs/2307.10163v1</link><description>In a backdoor attack, an adversary inserts maliciously constructed backdoorexamples into a training set to make the resulting model vulnerable tomanipulation. Defending against such attacks typically involves viewing theseinserted examples as outliers in the training set and using techniques fromrobust statistics to detect and remove them. In this work, we present a different approach to the backdoor attack problem.Specifically, we show that without structural information about the trainingdata distribution, backdoor attacks are indistinguishable fromnaturally-occurring features in the data--and thus impossible to "detect" in ageneral sense. Then, guided by this observation, we revisit existing defensesagainst backdoor attacks and characterize the (often latent) assumptions theymake and on which they depend. Finally, we explore an alternative perspectiveon backdoor attacks: one that assumes these attacks correspond to the strongestfeature in the training data. Under this assumption (which we make formal) wedevelop a new primitive for detecting backdoor attacks. Our primitive naturallygives rise to a detection algorithm that comes with theoretical guarantees andis effective in practice.</description><author>Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, Aleksander Madry</author><pubDate>Wed, 19 Jul 2023 18:44:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10163v1</guid></item><item><title>Robust Driving Policy Learning with Guided Meta Reinforcement Learning</title><link>http://arxiv.org/abs/2307.10160v1</link><description>Although deep reinforcement learning (DRL) has shown promising results forautonomous navigation in interactive traffic scenarios, existing work typicallyadopts a fixed behavior policy to control social vehicles in the trainingenvironment. This may cause the learned driving policy to overfit theenvironment, making it difficult to interact well with vehicles with different,unseen behaviors. In this work, we introduce an efficient method to traindiverse driving policies for social vehicles as a single meta-policy. Byrandomizing the interaction-based reward functions of social vehicles, we cangenerate diverse objectives and efficiently train the meta-policy throughguiding policies that achieve specific objectives. We further propose atraining strategy to enhance the robustness of the ego vehicle's driving policyusing the environment where social vehicles are controlled by the learnedmeta-policy. Our method successfully learns an ego driving policy thatgeneralizes well to unseen situations with out-of-distribution (OOD) socialagents' behaviors in a challenging uncontrolled T-intersection scenario.</description><author>Kanghoon Lee, Jiachen Li, David Isele, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer</author><pubDate>Wed, 19 Jul 2023 18:42:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10160v1</guid></item><item><title>FABRIC: Personalizing Diffusion Models with Iterative Feedback</title><link>http://arxiv.org/abs/2307.10159v1</link><description>In an era where visual content generation is increasingly driven by machinelearning, the integration of human feedback into generative models presentssignificant opportunities for enhancing user experience and output quality.This study explores strategies for incorporating iterative human feedback intothe generative process of diffusion-based text-to-image models. We proposeFABRIC, a training-free approach applicable to a wide range of populardiffusion models, which exploits the self-attention layer present in the mostwidely used architectures to condition the diffusion process on a set offeedback images. To ensure a rigorous assessment of our approach, we introducea comprehensive evaluation methodology, offering a robust mechanism to quantifythe performance of generative visual models that integrate human feedback. Weshow that generation results improve over multiple rounds of iterative feedbackthrough exhaustive analysis, implicitly optimizing arbitrary user preferences.The potential applications of these findings extend to fields such aspersonalized content creation and customization.</description><author>Dimitri von R√ºtte, Elisabetta Fedele, Jonathan Thomm, Lukas Wolf</author><pubDate>Wed, 19 Jul 2023 18:39:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10159v1</guid></item><item><title>Pattern Recovery in Penalized and Thresholded Estimation and its Geometry</title><link>http://arxiv.org/abs/2307.10158v1</link><description>We consider the framework of penalized estimation where the penalty term isgiven by a real-valued polyhedral gauge, which encompasses methods such asLASSO (and many variants thereof such as the generalized LASSO), SLOPE, OSCAR,PACS and others. Each of these estimators can uncover a different structure or``pattern'' of the unknown parameter vector. We define a general notion ofpatterns based on subdifferentials and formalize an approach to measure theircomplexity. For pattern recovery, we provide a minimal condition for aparticular pattern to be detected by the procedure with positive probability,the so-called accessibility condition. Using our approach, we also introducethe stronger noiseless recovery condition. For the LASSO, it is well known thatthe irrepresentability condition is necessary for pattern recovery withprobability larger than $1/2$ and we show that the noiseless recovery playsexactly the same role, thereby extending and unifying the irrepresentabilitycondition of the LASSO to a broad class of penalized estimators. We show thatthe noiseless recovery condition can be relaxed when turning to thresholdedpenalized estimators, extending the idea of the thresholded LASSO: we provethat the accessibility condition is already sufficient (and necessary) for surepattern recovery by thresholded penalized estimation provided that the signalof the pattern is large enough. Throughout the article, we demonstrate how ourfindings can be interpreted through a geometrical lens.</description><author>Piotr Graczyk, Ulrike Schneider, Tomasz Skalski, Patrick Tardivel</author><pubDate>Wed, 19 Jul 2023 18:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10158v1</guid></item><item><title>Leveraging Visemes for Better Visual Speech Representation and Lip Reading</title><link>http://arxiv.org/abs/2307.10157v1</link><description>Lip reading is a challenging task that has many potential applications inspeech recognition, human-computer interaction, and security systems. However,existing lip reading systems often suffer from low accuracy due to thelimitations of video features. In this paper, we propose a novel approach thatleverages visemes, which are groups of phonetically similar lip shapes, toextract more discriminative and robust video features for lip reading. Weevaluate our approach on various tasks, including word-level and sentence-levellip reading, and audiovisual speech recognition using the Arman-AV dataset, alargescale Persian corpus. Our experimental results show that our viseme basedapproach consistently outperforms the state-of-theart methods in all thesetasks. The proposed method reduces the lip-reading word error rate (WER) by9.1% relative to the best previous method.</description><author>Javad Peymanfard, Vahid Saeedi, Mohammad Reza Mohammadi, Hossein Zeinali, Nasser Mozayani</author><pubDate>Wed, 19 Jul 2023 18:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10157v1</guid></item><item><title>Exploring Transformer Extrapolation</title><link>http://arxiv.org/abs/2307.10156v1</link><description>Length extrapolation has attracted considerable attention recently since itallows transformers to be tested on longer sequences than those used intraining. Previous research has shown that this property can be attained byusing carefully designed Relative Positional Encodings (RPEs). While thesemethods perform well on a variety of corpora, the conditions for lengthextrapolation have yet to be investigated. This paper attempts to determinewhat types of RPEs allow for length extrapolation through a thoroughmathematical and empirical analysis. We discover that a transformer is certainto possess this property as long as the series that corresponds to the RPE'sexponential converges. Two practices are derived from the conditions andexamined in language modeling tasks on a variety of corpora. As a bonus fromthe conditions, we derive a new Theoretical Receptive Field (TRF) to measurethe receptive field of RPEs without taking any training steps. Extensiveexperiments are conducted on the Wikitext-103, Books, Github, and WikiBookdatasets to demonstrate the viability of our discovered conditions. We alsocompare TRF to Empirical Receptive Field (ERF) across different models, showingconsistently matched trends on the aforementioned datasets. The code isavailable at https://github.com/OpenNLPLab/Rpe.</description><author>Zhen Qin, Yiran Zhong, Hui Deng</author><pubDate>Wed, 19 Jul 2023 18:37:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10156v1</guid></item><item><title>Curvature-based Clustering on Graphs</title><link>http://arxiv.org/abs/2307.10155v1</link><description>Unsupervised node clustering (or community detection) is a classical graphlearning task. In this paper, we study algorithms, which exploit the geometryof the graph to identify densely connected substructures, which form clustersor communities. Our method implements discrete Ricci curvatures and theirassociated geometric flows, under which the edge weights of the graph evolve toreveal its community structure. We consider several discrete curvature notionsand analyze the utility of the resulting algorithms. In contrast to priorliterature, we study not only single-membership community detection, where eachnode belongs to exactly one community, but also mixed-membership communitydetection, where communities may overlap. For the latter, we argue that it isbeneficial to perform community detection on the line graph, i.e., the graph'sdual. We provide both theoretical and empirical evidence for the utility of ourcurvature-based clustering algorithms. In addition, we give several results onthe relationship between the curvature of a graph and that of its dual, whichenable the efficient implementation of our proposed mixed-membership communitydetection approach and which may be of independent interest for curvature-basednetwork analysis.</description><author>Yu Tian, Zachary Lubberts, Melanie Weber</author><pubDate>Wed, 19 Jul 2023 18:35:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10155v1</guid></item><item><title>API-Miner: an API-to-API Specification Recommendation Engine</title><link>http://arxiv.org/abs/2212.07253v2</link><description>When designing a new API for a large project, developers need to make smartdesign choices so that their code base can grow sustainably. To ensure that newAPI components are well designed, developers can learn from existing APIcomponents. However, the lack of standardized methods for comparing API designsmakes this learning process time-consuming and difficult. To address this gapwe developed API-Miner, to the best of our knowledge, one of the firstAPI-to-API specification recommendation engines. API-Miner retrieves relevantspecification components written in OpenAPI (a widely adopted language used todescribe web APIs). API-miner presents several significant contributions,including: (1) novel methods of processing and extracting key information fromOpenAPI specifications, (2) innovative feature extraction techniques that areoptimized for the highly technical API specification domain, and (3) a novellog-linear probabilistic model that combines multiple signals to retrieverelevant and high quality OpenAPI specification components given a queryspecification. We evaluate API-Miner in both quantitative and qualitative tasksand achieve an overall of 91.7% recall@1 and 56.2% F1, which surpasses baselineperformance by 15.4% in recall@1 and 3.2% in F1. Overall, API-Miner will allowdevelopers to retrieve relevant OpenAPI specification components from a publicor internal database in the early stages of the API development cycle, so thatthey can learn from existing established examples and potentially identifyredundancies in their work. It provides the guidance developers need toaccelerate development process and contribute thoughtfully designed APIs thatpromote code maintainability and quality. Code is available on GitHub athttps://github.com/jpmorganchase/api-miner.</description><author>Sae Young Moon, Gregor Kerr, Fran Silavong, Sean Moran</author><pubDate>Wed, 19 Jul 2023 18:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07253v2</guid></item><item><title>Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks</title><link>http://arxiv.org/abs/2307.04228v2</link><description>Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamentalchallenges: the accurate characterization of the prior distribution and theefficient evaluation of the likelihood. In the context of Bayesian studies ontomography, principal component analysis (PCA) can in some cases facilitate thestraightforward definition of the prior distribution, while simultaneouslyenabling the implementation of accurate surrogate models based on polynomialchaos expansion (PCE) to replace computationally intensive full-physics forwardsolvers. When faced with scenarios where PCA does not offer a direct means ofeasily defining the prior distribution alternative methods like deep generativemodels (e.g., variational autoencoders (VAEs)), can be employed as viableoptions. However, accurately producing a surrogate capable of capturing theintricate non-linear relationship between the latent parameters of a VAE andthe outputs of forward modeling presents a notable challenge. Indeed, while PCEmodels provide high accuracy when the input-output relationship can beeffectively approximated by relatively low-degree multivariate polynomials,this condition is typically unmet when utilizing latent variables derived fromdeep generative models. In this contribution, we present a strategy thatcombines the excellent reconstruction performances of VAE in terms of priorepresentation with the accuracy of PCA-PCE surrogate modeling in the contextof Bayesian ground penetrating radar (GPR) travel-time tomography. Within theMCMC process, the parametrization of the VAE is leveraged for prior explorationand sample proposal. Concurrently, modeling is conducted using PCE, whichoperates on either globally or locally defined principal components of the VAEsamples under examination.</description><author>Giovanni Angelo Meles, Macarena Amaya, Shiran Levy, Stefano Marelli, Niklas Linde</author><pubDate>Wed, 19 Jul 2023 18:24:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04228v2</guid></item><item><title>Benchmarking Potential Based Rewards for Learning Humanoid Locomotion</title><link>http://arxiv.org/abs/2307.10142v1</link><description>The main challenge in developing effective reinforcement learning (RL)pipelines is often the design and tuning the reward functions. Well-designedshaping reward can lead to significantly faster learning. Naively formulatedrewards, however, can conflict with the desired behavior and result inoverfitting or even erratic performance if not properly tuned. In theory, thebroad class of potential based reward shaping (PBRS) can help guide thelearning process without affecting the optimal policy. Although several studieshave explored the use of potential based reward shaping to accelerate learningconvergence, most have been limited to grid-worlds and low-dimensional systems,and RL in robotics has predominantly relied on standard forms of rewardshaping. In this paper, we benchmark standard forms of shaping with PBRS for ahumanoid robot. We find that in this high-dimensional system, PBRS has onlymarginal benefits in convergence speed. However, the PBRS reward terms aresignificantly more robust to scaling than typical reward shaping approaches,and thus easier to tune.</description><author>Se Hwan Jeon, Steve Heim, Charles Khazoom, Sangbae Kim</author><pubDate>Wed, 19 Jul 2023 18:12:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10142v1</guid></item><item><title>Llama 2: Open Foundation and Fine-Tuned Chat Models</title><link>http://arxiv.org/abs/2307.09288v2</link><description>In this work, we develop and release Llama 2, a collection of pretrained andfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized fordialogue use cases. Our models outperform open-source chat models on mostbenchmarks we tested, and based on our human evaluations for helpfulness andsafety, may be a suitable substitute for closed-source models. We provide adetailed description of our approach to fine-tuning and safety improvements ofLlama 2-Chat in order to enable the community to build on our work andcontribute to the responsible development of LLMs.</description><author>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing </author><pubDate>Wed, 19 Jul 2023 18:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09288v2</guid></item><item><title>An Improved NeuMIP with Better Accuracy</title><link>http://arxiv.org/abs/2307.10135v1</link><description>Neural reflectance models are capable of accurately reproducing thespatially-varying appearance of many real-world materials at different scales.However, existing methods have difficulties handling highly glossy materials.To address this problem, we introduce a new neural reflectance model which,compared with existing methods, better preserves not only specular highlightsbut also fine-grained details. To this end, we enhance the neural networkperformance by encoding input data to frequency space, inspired by NeRF, tobetter preserve the details. Furthermore, we introduce a gradient-based lossand employ it in multiple stages, adaptive to the progress of the learningphase. Lastly, we utilize an optional extension to the decoder network usingthe Inception module for more accurate yet costly performance. We demonstratethe effectiveness of our method using a variety of synthetic and real examples.</description><author>Bowen Xue, Shuang Zhao, Henrik Wann Jensen, Zahra Montazeri</author><pubDate>Wed, 19 Jul 2023 18:00:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10135v1</guid></item><item><title>General vs. Long-Tailed Age Estimation: An Approach to Kill Two Birds with One Stone</title><link>http://arxiv.org/abs/2307.10129v1</link><description>Facial age estimation has received a lot of attention for its diverseapplication scenarios. Most existing studies treat each sample equally and aimto reduce the average estimation error for the entire dataset, which can besummarized as General Age Estimation. However, due to the long-taileddistribution prevalent in the dataset, treating all samples equally willinevitably bias the model toward the head classes (usually the adult with amajority of samples). Driven by this, some works suggest that each class shouldbe treated equally to improve performance in tail classes (with a minority ofsamples), which can be summarized as Long-tailed Age Estimation. However,Long-tailed Age Estimation usually faces a performance trade-off, i.e.,achieving improvement in tail classes by sacrificing the head classes. In thispaper, our goal is to design a unified framework to perform well on both tasks,killing two birds with one stone. To this end, we propose a simple, effective,and flexible training paradigm named GLAE, which is two-fold. Our GLAE providesa surprising improvement on Morph II, reaching the lowest MAE and CMAE of 1.14and 1.27 years, respectively. Compared to the previous best method, MAE droppedby up to 34%, which is an unprecedented improvement, and for the first time,MAE is close to 1 year old. Extensive experiments on other age benchmarkdatasets, including CACD, MIVIA, and Chalearn LAP 2015, also indicate that GLAEoutperforms the state-of-the-art approaches significantly.</description><author>Zenghao Bao, Zichang Tan, Jun Li, Jun Wan, Xibo Ma, Zhen Lei</author><pubDate>Wed, 19 Jul 2023 17:51:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10129v1</guid></item><item><title>Value Summation: A Novel Scoring Function for MPC-based Model-based Reinforcement Learning</title><link>http://arxiv.org/abs/2209.08169v2</link><description>This paper proposes a novel scoring function for the planning module ofMPC-based reinforcement learning methods to address the inherent bias of usingthe reward function to score trajectories. The proposed method enhances thelearning efficiency of existing MPC-based MBRL methods using the discounted sumof values. The method utilizes optimal trajectories to guide policy learningand updates its state-action value function based on real-world and augmentedonboard data. The learning efficiency of the proposed method is evaluated inselected MuJoCo Gym environments as well as in learning locomotion skills for asimulated model of the Cassie robot. The results demonstrate that the proposedmethod outperforms the current state-of-the-art algorithms in terms of learningefficiency and average reward return.</description><author>Mehran Raisi, Amirhossein Noohian, Luc Mccutcheon, Saber Fallah</author><pubDate>Wed, 19 Jul 2023 17:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08169v2</guid></item><item><title>Two Approaches to Supervised Image Segmentation</title><link>http://arxiv.org/abs/2307.10123v1</link><description>Though performed almost effortlessly by humans, segmenting 2D gray-scale orcolor images in terms of their constituent regions of interest(e.g.~background, objects or portions of objects) constitutes one of thegreatest challenges in science and technology as a consequence of the involveddimensionality reduction(3D to 2D), noise, reflections, shades, and occlusions,among many other possible effects. While a large number of interestingapproaches have been respectively suggested along the last decades, it wasmainly with the more recent development of deep learning that more effectiveand general solutions have been obtained, currently constituting the basiccomparison reference for this type of operation. Also developed recently, amultiset-based methodology has been described that is capable of encouragingperformance that combines spatial accuracy, stability, and robustness whilerequiring minimal computational resources (hardware and/or training andrecognition time). The interesting features of the latter methodology mostlyfollow from the enhanced selectivity and sensitivity, as well as goodrobustness to data perturbations and outliers, allowed by the coincidencesimilarity index on which the multiset approach to supervised imagesegmentation is based. After describing the deep learning and multisetapproaches, the present work develops two comparison experiments between themwhich are primarily aimed at illustrating their respective main interestingfeatures when applied to the adopted specific type of data and parameterconfigurations. While the deep learning approach confirmed its potential forperforming image segmentation, the alternative multiset methodology allowed forencouraging accuracy while requiring little computational resources.</description><author>Alexandre Benatti, Luciano da F. Costa</author><pubDate>Wed, 19 Jul 2023 17:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10123v1</guid></item><item><title>A benchmark of categorical encoders for binary classification</title><link>http://arxiv.org/abs/2307.09191v2</link><description>Categorical encoders transform categorical features into numericalrepresentations that are indispensable for a wide range of machine learningmodels. Existing encoder benchmark studies lack generalizability because oftheir limited choice of (1) encoders, (2) experimental factors, and (3)datasets. Additionally, inconsistencies arise from the adoption of varyingaggregation strategies. This paper is the most comprehensive benchmark ofcategorical encoders to date, including an extensive evaluation of 32configurations of encoders from diverse families, with 36 combinations ofexperimental factors, and on 50 datasets. The study shows the profoundinfluence of dataset selection, experimental factors, and aggregationstrategies on the benchmark's conclusions -- aspects disregarded in previousencoder benchmarks.</description><author>Federico Matteucci, Vadim Arzamasov, Klemens Boehm</author><pubDate>Wed, 19 Jul 2023 17:24:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09191v2</guid></item><item><title>Evaluation of Complexity Measures for Deep Learning Generalization in Medical Image Analysis</title><link>http://arxiv.org/abs/2103.03328v3</link><description>The generalization performance of deep learning models for medical imageanalysis often decreases on images collected with different devices for dataacquisition, device settings, or patient population. A better understanding ofthe generalization capacity on new images is crucial for clinicians'trustworthiness in deep learning. Although significant research efforts havebeen recently directed toward establishing generalization bounds and complexitymeasures, still, there is often a significant discrepancy between the predictedand actual generalization performance. As well, related large empirical studieshave been primarily based on validation with general-purpose image datasets.This paper presents an empirical study that investigates the correlationbetween 25 complexity measures and the generalization abilities of superviseddeep learning classifiers for breast ultrasound images. The results indicatethat PAC-Bayes flatness-based and path norm-based measures produce the mostconsistent explanation for the combination of models and data. We alsoinvestigate the use of multi-task classification and segmentation approach forbreast images, and report that such learning approach acts as an implicitregularizer and is conducive toward improved generalization.</description><author>Aleksandar Vakanski, Min Xian</author><pubDate>Wed, 19 Jul 2023 17:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.03328v3</guid></item><item><title>Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?</title><link>http://arxiv.org/abs/2306.13197v2</link><description>Gradient based attribution methods for neural networks working as classifiersuse gradients of network scores. Here we discuss the practical differencesbetween using gradients of pre-softmax scores versus post-softmax scores, andtheir respective advantages and disadvantages.</description><author>Miguel Lerma, Mirtha Lucas</author><pubDate>Wed, 19 Jul 2023 17:19:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13197v2</guid></item><item><title>SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems</title><link>http://arxiv.org/abs/2210.12547v2</link><description>Optimization problems with nonlinear cost functions and combinatorialconstraints appear in many real-world applications but remain challenging tosolve efficiently compared to their linear counterparts. To bridge this gap, wepropose $\textbf{SurCo}$ that learns linear $\underline{\text{Sur}}$rogatecosts which can be used in existing $\underline{\text{Co}}$mbinatorial solversto output good solutions to the original nonlinear combinatorial optimizationproblem. The surrogate costs are learned end-to-end with nonlinear loss bydifferentiating through the linear surrogate solver, combining the flexibilityof gradient-based methods with the structure of linear combinatorialoptimization. We propose three $\texttt{SurCo}$ variants:$\texttt{SurCo}-\texttt{zero}$ for individual nonlinear problems,$\texttt{SurCo}-\texttt{prior}$ for problem distributions, and$\texttt{SurCo}-\texttt{hybrid}$ to combine both distribution andproblem-specific information. We give theoretical intuition motivating$\texttt{SurCo}$, and evaluate it empirically. Experiments show that$\texttt{SurCo}$ finds better solutions faster than state-of-the-art and domainexpert approaches in real-world optimization problems such as embedding tablesharding, inverse photonic design, and nonlinear route planning.</description><author>Aaron Ferber, Taoan Huang, Daochen Zha, Martin Schubert, Benoit Steiner, Bistra Dilkina, Yuandong Tian</author><pubDate>Wed, 19 Jul 2023 17:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.12547v2</guid></item><item><title>ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction</title><link>http://arxiv.org/abs/2302.05783v4</link><description>Deep neural networks (DNN) have shown great capacity of modeling a dynamicalsystem; nevertheless, they usually do not obey physics constraints such asconservation laws. This paper proposes a new learning framework named ConCerNetto improve the trustworthiness of the DNN based dynamics modeling to endow theinvariant properties. ConCerNet consists of two steps: (i) a contrastivelearning method to automatically capture the system invariants (i.e.conservation properties) along the trajectory observations; (ii) a neuralprojection layer to guarantee that the learned dynamics models preserve thelearned invariants. We theoretically prove the functional relationship betweenthe learned latent representation and the unknown system invariant function.Experiments show that our method consistently outperforms the baseline neuralnetworks in both coordinate error and conservation metrics by a large margin.With neural network based parameterization and no dependence on priorknowledge, our method can be extended to complex and large-scale dynamics byleveraging an autoencoder.</description><author>Wang Zhang, Tsui-Wei Weng, Subhro Das, Alexandre Megretski, Luca Daniel, Lam M. Nguyen</author><pubDate>Wed, 19 Jul 2023 17:14:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05783v4</guid></item><item><title>Memory Efficient And Minimax Distribution Estimation Under Wasserstein Distance Using Bayesian Histograms</title><link>http://arxiv.org/abs/2307.10099v1</link><description>We study Bayesian histograms for distribution estimation on $[0,1]^d$ underthe Wasserstein $W_v, 1 \leq v &lt; \infty$ distance in the i.i.d sampling regime.We newly show that when $d &lt; 2v$, histograms possess a special \textit{memoryefficiency} property, whereby in reference to the sample size $n$, order$n^{d/2v}$ bins are needed to obtain minimax rate optimality. This result holdsfor the posterior mean histogram and with respect to posterior contraction:under the class of Borel probability measures and some classes of smoothdensities. The attained memory footprint overcomes existing minimax optimalprocedures by a polynomial factor in $n$; for example an $n^{1 - d/2v}$ factorreduction in the footprint when compared to the empirical measure, a minimaxestimator in the Borel probability measure class. Additionally constructingboth the posterior mean histogram and the posterior itself can be donesuper--linearly in $n$. Due to the popularity of the $W_1,W_2$ metrics and thecoverage provided by the $d &lt; 2v$ case, our results are of most practicalinterest in the $(d=1,v =1,2), (d=2,v=2), (d=3,v=2)$ settings and we providesimulations demonstrating the theory in several of these instances.</description><author>Peter Matthew Jacobs, Lekha Patel, Anirban Bhattacharya, Debdeep Pati</author><pubDate>Wed, 19 Jul 2023 17:13:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10099v1</guid></item><item><title>Gradient Sparsification For Masked Fine-Tuning of Transformers</title><link>http://arxiv.org/abs/2307.10098v1</link><description>Fine-tuning pretrained self-supervised language models is widely adopted fortransfer learning to downstream tasks. Fine-tuning can be achieved by freezinggradients of the pretrained network and only updating gradients of a newlyadded classification layer, or by performing gradient updates on allparameters. Gradual unfreezing makes a trade-off between the two by graduallyunfreezing gradients of whole layers during training. This has been aneffective strategy to trade-off between storage and training speed withgeneralization performance. However, it is not clear whether graduallyunfreezing layers throughout training is optimal, compared to sparse variantsof gradual unfreezing which may improve fine-tuning performance. In this paper,we propose to stochastically mask gradients to regularize pretrained languagemodels for improving overall fine-tuned performance. We introduce GradDrop andvariants thereof, a class of gradient sparsification methods that maskgradients during the backward pass, acting as gradient noise. GradDrop issparse and stochastic unlike gradual freezing. Extensive experiments on themultilingual XGLUE benchmark with XLMR-Large show that GradDrop is competitiveagainst methods that use additional translated data for intermediatepretraining and outperforms standard fine-tuning and gradual unfreezing. Apost-analysis shows how GradDrop improves performance with languages it was nottrained on, such as under-resourced languages.</description><author>James O' Neill, Sourav Dutta</author><pubDate>Wed, 19 Jul 2023 17:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10098v1</guid></item><item><title>Boundary-Refined Prototype Generation: A General End-to-End Paradigm for Semi-Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2307.10097v1</link><description>Prototype-based classification is a classical method in machine learning, andrecently it has achieved remarkable success in semi-supervised semanticsegmentation. However, the current approach isolates the prototypeinitialization process from the main training framework, which appears to beunnecessary. Furthermore, while the direct use of K-Means algorithm forprototype generation has considered rich intra-class variance, it may not bethe optimal solution for the classification task. To tackle these problems, wepropose a novel boundary-refined prototype generation (BRPG) method, which isincorporated into the whole training framework. Specifically, our approachsamples and clusters high- and low-confidence features separately based on aconfidence threshold, aiming to generate prototypes closer to the classboundaries. Moreover, an adaptive prototype optimization strategy is introducedto make prototype augmentation for categories with scattered featuredistributions. Extensive experiments on the PASCAL VOC 2012 and Cityscapesdatasets demonstrate the superiority and scalability of the proposed method,outperforming the current state-of-the-art approaches. The code is available atxxxxxxxxxxxxxx.</description><author>Junhao Dong, Zhu Meng, Delong Liu, Zhicheng Zhao, Fei Su</author><pubDate>Wed, 19 Jul 2023 17:12:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10097v1</guid></item><item><title>IST-Net: Prior-free Category-level Pose Estimation with Implicit Space Transformation</title><link>http://arxiv.org/abs/2303.13479v2</link><description>Category-level 6D pose estimation aims to predict the poses and sizes ofunseen objects from a specific category. Thanks to prior deformation, whichexplicitly adapts a category-specific 3D prior (i.e., a 3D template) to a givenobject instance, prior-based methods attained great success and have become amajor research stream. However, obtaining category-specific priors requirescollecting a large amount of 3D models, which is labor-consuming and often notaccessible in practice. This motivates us to investigate whether priors arenecessary to make prior-based methods effective. Our empirical study shows thatthe 3D prior itself is not the credit to the high performance. The keypointactually is the explicit deformation process, which aligns camera and worldcoordinates supervised by world-space 3D models (also called canonical space).Inspired by these observations, we introduce a simple prior-free implicit spacetransformation network, namely IST-Net, to transform camera-space features toworld-space counterparts and build correspondence between them in an implicitmanner without relying on 3D priors. Besides, we design camera- and world-spaceenhancers to enrich the features with pose-sensitive information andgeometrical constraints, respectively. Albeit simple, IST-Net achievesstate-of-the-art performance based-on prior-free design, with top inferencespeed on the REAL275 benchmark. Our code and models are available athttps://github.com/CVMI-Lab/IST-Net.</description><author>Jianhui Liu, Yukang Chen, Xiaoqing Ye, Xiaojuan Qi</author><pubDate>Wed, 19 Jul 2023 17:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13479v2</guid></item><item><title>Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis</title><link>http://arxiv.org/abs/2307.10094v1</link><description>Cross-modality medical image synthesis is a critical topic and has thepotential to facilitate numerous applications in the medical imaging field.Despite recent successes in deep-learning-based generative models, most currentmedical image synthesis methods rely on generative adversarial networks andsuffer from notorious mode collapse and unstable training. Moreover, the 2Dbackbone-driven approaches would easily result in volumetric inconsistency,while 3D backbones are challenging and impractical due to the tremendous memorycost and training difficulty. In this paper, we introduce a new paradigm forvolumetric medical data synthesis by leveraging 2D backbones and present adiffusion-based framework, Make-A-Volume, for cross-modality 3D medical imagesynthesis. To learn the cross-modality slice-wise mapping, we employ a latentdiffusion model and learn a low-dimensional latent space, resulting in highcomputational efficiency. To enable the 3D image synthesis and mitigatevolumetric inconsistency, we further insert a series of volumetric layers inthe 2D slice-mapping model and fine-tune them with paired 3D data. Thisparadigm extends the 2D image diffusion model to a volumetric version with aslightly increasing number of parameters and computation, offering a principledsolution for generic cross-modality 3D medical image synthesis. We showcase theeffectiveness of our Make-A-Volume framework on an in-house SWI-MRA brain MRIdataset and a public T1-T2 brain MRI dataset. Experimental results demonstratethat our framework achieves superior synthesis results with volumetricconsistency.</description><author>Lingting Zhu, Zeyue Xue, Zhenchao Jin, Xian Liu, Jingzhen He, Ziwei Liu, Lequan Yu</author><pubDate>Wed, 19 Jul 2023 17:01:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10094v1</guid></item><item><title>Revisiting invariances and introducing priors in Gromov-Wasserstein distances</title><link>http://arxiv.org/abs/2307.10093v1</link><description>Gromov-Wasserstein distance has found many applications in machine learningdue to its ability to compare measures across metric spaces and its invarianceto isometric transformations. However, in certain applications, this invarianceproperty can be too flexible, thus undesirable. Moreover, theGromov-Wasserstein distance solely considers pairwise sample similarities ininput datasets, disregarding the raw feature representations. We propose a newoptimal transport-based distance, called Augmented Gromov-Wasserstein, thatallows for some control over the level of rigidity to transformations. It alsoincorporates feature alignments, enabling us to better leverage prior knowledgeon the input data for improved performance. We present theoretical insightsinto the proposed metric. We then demonstrate its usefulness for single-cellmulti-omic alignment tasks and a transfer learning scenario in machinelearning.</description><author>Pinar Demetci, Quang Huy Tran, Ievgen Redko, Ritambhara Singh</author><pubDate>Wed, 19 Jul 2023 17:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10093v1</guid></item><item><title>Self-Supervised Learning for Videos: A Survey</title><link>http://arxiv.org/abs/2207.00419v3</link><description>The remarkable success of deep learning in various domains relies on theavailability of large-scale annotated datasets. However, obtaining annotationsis expensive and requires great effort, which is especially challenging forvideos. Moreover, the use of human-generated annotations leads to models withbiased learning and poor domain generalization and robustness. As analternative, self-supervised learning provides a way for representationlearning which does not require annotations and has shown promise in both imageand video domains. Different from the image domain, learning videorepresentations are more challenging due to the temporal dimension, bringing inmotion and other environmental dynamics. This also provides opportunities forvideo-exclusive ideas that advance self-supervised learning in the video andmultimodal domain. In this survey, we provide a review of existing approacheson self-supervised learning focusing on the video domain. We summarize thesemethods into four different categories based on their learning objectives: 1)pretext tasks, 2) generative learning, 3) contrastive learning, and 4)cross-modal agreement. We further introduce the commonly used datasets,downstream evaluation tasks, insights into the limitations of existing works,and the potential future directions in this area.</description><author>Madeline C. Schiappa, Yogesh S. Rawat, Mubarak Shah</author><pubDate>Wed, 19 Jul 2023 17:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.00419v3</guid></item><item><title>CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction</title><link>http://arxiv.org/abs/2307.04838v2</link><description>In this paper, we explore the potential of Vision-Language Models (VLMs),specifically CLIP, in predicting visual object relationships, which involvesinterpreting visual features from images into language-based relations. Currentstate-of-the-art methods use complex graphical models that utilize languagecues and visual features to address this challenge. We hypothesize that thestrong language priors in CLIP embeddings can simplify these graphical modelspaving for a simpler approach. We adopt the UVTransE relation predictionframework, which learns the relation as a translational embedding with subject,object, and union box embeddings from a scene. We systematically explore thedesign of CLIP-based subject, object, and union-box representations within theUVTransE framework and propose CREPE (CLIP Representation Enhanced PredicateEstimation). CREPE utilizes text-based representations for all three boundingboxes and introduces a novel contrastive training strategy to automaticallyinfer the text prompt for union-box. Our approach achieves state-of-the-artperformance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the VisualGenome benchmark, achieving a 15.3\% gain in performance over recentstate-of-the-art at mR@20. This work demonstrates CLIP's effectiveness inobject relation prediction and encourages further research on VLMs in thischallenging domain.</description><author>Rakshith Subramanyam, T. S. Jayram, Rushil Anirudh, Jayaraman J. Thiagarajan</author><pubDate>Wed, 19 Jul 2023 16:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04838v2</guid></item><item><title>Android in the Wild: A Large-Scale Dataset for Android Device Control</title><link>http://arxiv.org/abs/2307.10088v1</link><description>There is a growing interest in device-control systems that can interprethuman natural language instructions and execute them on a digital device bydirectly controlling its user interface. We present a dataset fordevice-control research, Android in the Wild (AITW), which is orders ofmagnitude larger than current datasets. The dataset contains humandemonstrations of device interactions, including the screens and actions, andcorresponding natural language instructions. It consists of 715k episodesspanning 30k unique instructions, four versions of Android (v10-13),and eightdevice types (Pixel 2 XL to Pixel 6) with varying screen resolutions. Itcontains multi-step tasks that require semantic understanding of language andvisual context. This dataset poses a new challenge: actions available throughthe user interface must be inferred from their visual appearance. And, insteadof simple UI element-based actions, the action space consists of precisegestures (e.g., horizontal scrolls to operate carousel widgets). We organizeour dataset to encourage robustness analysis of device-control systems, i.e.,how well a system performs in the presence of new task descriptions, newapplications, or new platform versions. We develop two agents and reportperformance across the dataset. The dataset is available athttps://github.com/google-research/google-research/tree/master/android_in_the_wild.</description><author>Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, Timothy Lillicrap</author><pubDate>Wed, 19 Jul 2023 16:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10088v1</guid></item><item><title>iSLAM: Imperative SLAM</title><link>http://arxiv.org/abs/2306.07894v3</link><description>Simultaneous localization and mapping (SLAM) stands as one of the criticalchallenges in robot navigation. Recent advancements suggest that methods basedon supervised learning deliver impressive performance in front-end odometry,while traditional optimization-based methods still play a vital role in theback-end for minimizing estimation drift. In this paper, we found that suchdecoupled paradigm can lead to only sub-optimal performance, consequentlycurtailing system capabilities and generalization potential. To solve thisproblem, we proposed a novel self-supervised learning framework, imperativeSLAM (iSLAM), which fosters reciprocal correction between the front-end andback-end, thus enhancing performance without necessitating any externalsupervision. Specifically, we formulate a SLAM system as a bi-leveloptimization problem so that the two components are bidirectionally connected.As a result, the front-end model is able to learn global geometric knowledgeobtained through pose graph optimization by back-propagating the residuals fromthe back-end. This significantly improves the generalization ability of theentire system and thus achieves the accuracy improvement up to 45%. To the bestof our knowledge, iSLAM is the first SLAM system showing that the front-end andback-end can learn jointly and mutually contribute to each other in aself-supervised manner.</description><author>Taimeng Fu, Shaoshu Su, Chen Wang</author><pubDate>Wed, 19 Jul 2023 16:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07894v3</guid></item><item><title>A decision making framework for recommended maintenance of road segments</title><link>http://arxiv.org/abs/2307.10085v1</link><description>With the rapid development of global road transportation, countries worldwidehave completed the construction of road networks. However, the ensuingchallenge lies in the maintenance of existing roads. It is well-known thatcountries allocate limited budgets to road maintenance projects, and roadmanagement departments face difficulties in making scientifically informedmaintenance decisions. Therefore, integrating various artificial intelligencedecision-making techniques to thoroughly explore historical maintenance dataand adapt them to the context of road maintenance scientific decision-makinghas become an urgent issue. This integration aims to provide road managementdepartments with more scientific tools and evidence for decision-making. Theframework proposed in this paper primarily addresses the following four issues:1) predicting the pavement performance of various routes, 2) determining theprioritization of maintenance routes, 3) making maintenance decisions based onthe evaluation of the effects of past maintenance, and consideringcomprehensive technical and management indicators, and 4) determining theprioritization of maintenance sections based on the maintenance effectivenessand recommended maintenance effectiveness. By tackling these four problems, theframework enables intelligent decision-making for the optimal maintenance planand maintenance sections, taking into account limited funding and historicalmaintenance management experience.</description><author>Haoyu Sun, Yan Yan</author><pubDate>Wed, 19 Jul 2023 16:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10085v1</guid></item><item><title>A Dual Formulation for Probabilistic Principal Component Analysis</title><link>http://arxiv.org/abs/2307.10078v1</link><description>In this paper, we characterize Probabilistic Principal Component Analysis inHilbert spaces and demonstrate how the optimal solution admits a representationin dual space. This allows us to develop a generative framework for kernelmethods. Furthermore, we show how it englobes Kernel Principal ComponentAnalysis and illustrate its working on a toy and a real dataset.</description><author>Henri De Plaen, Johan A. K. Suykens</author><pubDate>Wed, 19 Jul 2023 16:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10078v1</guid></item><item><title>An exponentially-growing family of universal quantum circuits</title><link>http://arxiv.org/abs/2212.00736v2</link><description>Quantum machine learning has become an area of growing interest but hascertain theoretical and hardware-specific limitations. Notably, the problem ofvanishing gradients, or barren plateaus, renders the training impossible forcircuits with high qubit counts, imposing a limit on the number of qubits thatdata scientists can use for solving problems. Independently, angle-embeddedsupervised quantum neural networks were shown to produce truncated Fourierseries with a degree directly dependent on two factors: the depth of theencoding and the number of parallel qubits the encoding applied to. The degreeof the Fourier series limits the model expressivity. This work introduces twonew architectures whose Fourier degrees grow exponentially: the sequential andparallel exponential quantum machine learning architectures. This is done byefficiently using the available Hilbert space when encoding, increasing theexpressivity of the quantum encoding. Therefore, the exponential growth allowsstaying at the low-qubit limit to create highly expressive circuits avoidingbarren plateaus. Practically, parallel exponential architecture was shown tooutperform the existing linear architectures by reducing their final meansquare error value by up to 44.7% in a one-dimensional test problem.Furthermore, the feasibility of this technique was also shown on a trapped ionquantum processing unit.</description><author>Mo Kordzanganeh, Pavel Sekatski, Markus Pflitsch, Alexey Melnikov</author><pubDate>Wed, 19 Jul 2023 16:43:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00736v2</guid></item><item><title>Entropy regularization in probabilistic clustering</title><link>http://arxiv.org/abs/2307.10065v1</link><description>Bayesian nonparametric mixture models are widely used to clusterobservations. However, one major drawback of the approach is that the estimatedpartition often presents unbalanced clusters' frequencies with only a fewdominating clusters and a large number of sparsely-populated ones. This featuretranslates into results that are often uninterpretable unless we accept toignore a relevant number of observations and clusters. Interpreting theposterior distribution as penalized likelihood, we show how the unbalance canbe explained as a direct consequence of the cost functions involved inestimating the partition. In light of our findings, we propose a novel Bayesianestimator of the clustering configuration. The proposed estimator is equivalentto a post-processing procedure that reduces the number of sparsely-populatedclusters and enhances interpretability. The procedure takes the form ofentropy-regularization of the Bayesian estimate. While being computationallyconvenient with respect to alternative strategies, it is also theoreticallyjustified as a correction to the Bayesian loss function used for pointestimation and, as such, can be applied to any posterior distribution ofclusters, regardless of the specific model used.</description><author>Beatrice Franzolini, Giovanni Rebaudo</author><pubDate>Wed, 19 Jul 2023 16:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10065v1</guid></item><item><title>Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples</title><link>http://arxiv.org/abs/2307.10062v1</link><description>Deploying deep visual models can lead to performance drops due to thediscrepancies between source and target distributions. Several approachesleverage labeled source data to estimate target domain accuracy, but accessinglabeled source data is often prohibitively difficult due to dataconfidentiality or resource limitations on serving devices. Our work proposes anew framework to estimate model accuracy on unlabeled target data withoutaccess to source data. We investigate the feasibility of using pseudo-labelsfor accuracy estimation and evolve this idea into adopting recent advances insource-free domain adaptation algorithms. Our approach measures thedisagreement rate between the source hypothesis and the target pseudo-labelingfunction, adapted from the source hypothesis. We mitigate the impact oferroneous pseudo-labels that may arise due to a high ideal joint hypothesisrisk by employing adaptive adversarial perturbation on the input of the targetmodel. Our proposed source-free framework effectively addresses the challengingdistribution shift scenarios and outperforms existing methods requiring sourcedata and labels for training.</description><author>JoonHo Lee, Jae Oh Woo, Hankyu Moon, Kwonho Lee</author><pubDate>Wed, 19 Jul 2023 16:33:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10062v1</guid></item><item><title>Accurate deep learning sub-grid scale models for large eddy simulations</title><link>http://arxiv.org/abs/2307.10060v1</link><description>We present two families of sub-grid scale (SGS) turbulence models developedfor large-eddy simulation (LES) purposes. Their development required theformulation of physics-informed robust and efficient Deep Learning (DL)algorithms which, unlike state-of-the-art analytical modeling techniques canproduce high-order complex non-linear relations between inputs and outputs.Explicit filtering of data from direct simulations of the canonical channelflow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 providedaccurate data for training and testing. The two sets of models use differentnetwork architectures. One of the architectures uses tensor basis neuralnetworks (TBNN) and embeds the simplified analytical model form of the generaleffective-viscosity hypothesis, thus incorporating the Galilean, rotational andreflectional invariances. The other architecture is that of a relatively simplenetwork, that is able to incorporate the Galilean invariance only. However,this simpler architecture has better feature extraction capacity owing to itsability to establish relations between and extract information fromcross-components of the integrity basis tensors and the SGS stresses. Both setsof models are used to predict the SGS stresses for feature datasets generatedwith different filter widths, and at different Reynolds numbers. It is shownthat due to the simpler model's better feature learning capabilities, itoutperforms the invariance embedded model in statistical performance metrics.In a priori tests, both sets of models provide similar levels of dissipationand backscatter. Based on the test results, both sets of models should beusable in a posteriori actual LESs.</description><author>Rikhi Bose, Arunabha M. Roy</author><pubDate>Wed, 19 Jul 2023 16:30:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10060v1</guid></item><item><title>Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization</title><link>http://arxiv.org/abs/2307.10053v1</link><description>In this paper, we investigate the convergence properties of the stochasticgradient descent (SGD) method and its variants, especially in training neuralnetworks built from nonsmooth activation functions. We develop a novelframework that assigns different timescales to stepsizes for updating themomentum terms and variables, respectively. Under mild conditions, we prove theglobal convergence of our proposed framework in both single-timescale andtwo-timescale cases. We show that our proposed framework encompasses a widerange of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion,normalized SGD and clipped SGD. Furthermore, when the objective function adoptsa finite-sum formulation, we prove the convergence properties for theseSGD-type methods based on our proposed framework. In particular, we prove thatthese SGD-type methods find the Clarke stationary points of the objectivefunction with randomly chosen stepsizes and initial points under mildassumptions. Preliminary numerical experiments demonstrate the high efficiencyof our analyzed SGD-type methods.</description><author>Nachuan Xiao, Xiaoyin Hu, Kim-Chuan Toh</author><pubDate>Wed, 19 Jul 2023 16:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10053v1</guid></item><item><title>ThoughtSource: A central hub for large language model reasoning data</title><link>http://arxiv.org/abs/2301.11596v3</link><description>Large language models (LLMs) such as GPT-4 have recently demonstratedimpressive results across a wide range of tasks. LLMs are still limited,however, in that they frequently fail at complex reasoning, their reasoningprocesses are opaque, they are prone to 'hallucinate' facts, and there areconcerns about their underlying biases. Letting models verbalize reasoningsteps as natural language, a technique known as chain-of-thought prompting, hasrecently been proposed as a way to address some of these issues. Here wepresent ThoughtSource, a meta-dataset and software library for chain-of-thought(CoT) reasoning. The goal of ThoughtSource is to improve future artificialintelligence systems by facilitating qualitative understanding of CoTs,enabling empirical evaluations, and providing training data. This first releaseof ThoughtSource integrates six scientific/medical, three general-domain andfive math word question answering datasets.</description><author>Simon Ott, Konstantin Hebenstreit, Valentin Li√©vin, Christoffer Egeberg Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole Winther, Matthias Samwald</author><pubDate>Wed, 19 Jul 2023 16:25:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11596v3</guid></item><item><title>Divert More Attention to Vision-Language Object Tracking</title><link>http://arxiv.org/abs/2307.10046v1</link><description>Multimodal vision-language (VL) learning has noticeably pushed the tendencytoward generic intelligence owing to emerging large foundation models. However,tracking, as a fundamental vision problem, surprisingly enjoys less bonus fromrecent flourishing VL learning. We argue that the reasons are two-fold: thelack of large-scale vision-language annotated videos and ineffectivevision-language interaction learning of current works. These nuisances motivateus to design more effective vision-language representation for tracking,meanwhile constructing a large database with language annotation for modellearning. Particularly, in this paper, we first propose a general attributeannotation strategy to decorate videos in six popular tracking benchmarks,which contributes a large-scale vision-language tracking database with morethan 23,000 videos. We then introduce a novel framework to improve tracking bylearning a unified-adaptive VL representation, where the cores are the proposedasymmetric architecture search and modality mixer (ModaMixer). To furtherimprove VL representation, we introduce a contrastive loss to align differentmodalities. To thoroughly evidence the effectiveness of our method, weintegrate the proposed framework on three tracking methods with differentdesigns, i.e., the CNN-based SiamCAR, the Transformer-based OSTrack, and thehybrid structure TransT. The experiments demonstrate that our framework cansignificantly improve all baselines on six benchmarks. Besides empiricalresults, we theoretically analyze our approach to show its rationality. Byrevealing the potential of VL representation, we expect the community to divertmore attention to VL tracking and hope to open more possibilities for futuretracking with diversified multimodal messages.</description><author>Mingzhe Guo, Zhipeng Zhang, Liping Jing, Haibin Ling, Heng Fan</author><pubDate>Wed, 19 Jul 2023 16:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10046v1</guid></item><item><title>Class Attention to Regions of Lesion for Imbalanced Medical Image Recognition</title><link>http://arxiv.org/abs/2307.10036v1</link><description>Automated medical image classification is the key component in intelligentdiagnosis systems. However, most medical image datasets contain plenty ofsamples of common diseases and just a handful of rare ones, leading to majorclass imbalances. Currently, it is an open problem in intelligent diagnosis toeffectively learn from imbalanced training data. In this paper, we propose asimple yet effective framework, named \textbf{C}lass \textbf{A}ttention to\textbf{RE}gions of the lesion (CARE), to handle data imbalance issues byembedding attention into the training process of \textbf{C}onvolutional\textbf{N}eural \textbf{N}etworks (CNNs). The proposed attention module helpsCNNs attend to lesion regions of rare diseases, therefore helping CNNs to learntheir characteristics more effectively. In addition, this attention moduleworks only during the training phase and does not change the architecture ofthe original network, so it can be directly combined with any existing CNNarchitecture. The CARE framework needs bounding boxes to represent the lesionregions of rare diseases. To alleviate the need for manual annotation, wefurther developed variants of CARE by leveraging the traditional saliencymethods or a pretrained segmentation model for bounding box generation. Resultsshow that the CARE variants with automated bounding box generation arecomparable to the original CARE framework with \textit{manual} bounding boxannotations. A series of experiments on an imbalanced skin image dataset and apneumonia dataset indicates that our method can effectively help the networkfocus on the lesion regions of rare diseases and remarkably improves theclassification performance of rare diseases.</description><author>Jia-Xin Zhuang, Jiabin Cai, Jianguo Zhang, Wei-shi Zheng, Ruixuan Wang</author><pubDate>Wed, 19 Jul 2023 16:19:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10036v1</guid></item><item><title>Uncovering Bias in Personal Informatics</title><link>http://arxiv.org/abs/2303.15592v2</link><description>Personal informatics (PI) systems, powered by smartphones and wearables,enable people to lead healthier lifestyles by providing meaningful andactionable insights that break down barriers between users and their healthinformation. Today, such systems are used by billions of users for monitoringnot only physical activity and sleep but also vital signs and women's and hearthealth, among others. Despite their widespread usage, the processing ofsensitive PI data may suffer from biases, which may entail practical andethical implications. In this work, we present the first comprehensiveempirical and analytical study of bias in PI systems, including biases in rawdata and in the entire machine learning life cycle. We use the most detailedframework to date for exploring the different sources of bias and find thatbiases exist both in the data generation and the model learning andimplementation streams. According to our results, the most affected minoritygroups are users with health issues, such as diabetes, joint issues, andhypertension, and female users, whose data biases are propagated or evenamplified by learning models, while intersectional biases can also be observed.</description><author>Sofia Yfantidou, Pavlos Sermpezis, Athena Vakali, Ricardo Baeza-Yates</author><pubDate>Wed, 19 Jul 2023 16:16:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15592v2</guid></item><item><title>Automatic Conversion of MiniZinc Programs to QUBO</title><link>http://arxiv.org/abs/2307.10032v1</link><description>Obtaining Quadratic Unconstrained Binary Optimisation models for variousoptimisation problems, in order to solve those on physical quantum computers(such as the the DWave annealers) is nowadays a lengthy and tedious processthat requires one to remodel all problem variables as binary variables andsqueeze the target function and the constraints into a single quadraticpolynomial into these new variables. We report here on the basis of our automatic converter from MiniZinc to QUBO,which is able to process a large set of constraint optimisation and constraintsatisfaction problems and turn them into equivalent QUBOs, effectivelyoptimising the whole process.</description><author>Armin Wolf, Cristian Grozea</author><pubDate>Wed, 19 Jul 2023 16:16:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10032v1</guid></item><item><title>Contextual Reliability: When Different Features Matter in Different Contexts</title><link>http://arxiv.org/abs/2307.10026v1</link><description>Deep neural networks often fail catastrophically by relying on spuriouscorrelations. Most prior work assumes a clear dichotomy into spurious andreliable features; however, this is often unrealistic. For example, most of thetime we do not want an autonomous car to simply copy the speed of surroundingcars -- we don't want our car to run a red light if a neighboring car does so.However, we cannot simply enforce invariance to next-lane speed, since it couldprovide valuable information about an unobservable pedestrian at a crosswalk.Thus, universally ignoring features that are sometimes (but not always)reliable can lead to non-robust performance. We formalize a new setting calledcontextual reliability which accounts for the fact that the "right" features touse may vary depending on the context. We propose and analyze a two-stageframework called Explicit Non-spurious feature Prediction (ENP) which firstidentifies the relevant features to use for a given context, then trains amodel to rely exclusively on these features. Our work theoretically andempirically demonstrates the advantages of ENP over existing methods andprovides new benchmarks for contextual reliability.</description><author>Gaurav Ghosal, Amrith Setlur, Daniel S. Brown, Anca D. Dragan, Aditi Raghunathan</author><pubDate>Wed, 19 Jul 2023 16:11:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10026v1</guid></item><item><title>An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods</title><link>http://arxiv.org/abs/2307.10025v1</link><description>Fertility issues are closely related to population security, in 60 yearsChina's population for the first time in a negative growth trend, the change offertility policy is of great concern to the community. 2023 ``two sessions"proposal ``suggests that the country in the form of legislation, the birth ofthe registration of the cancellation of the marriage restriction" This topicwas once a hot topic on the Internet, and ``unbundling" the relationshipbetween birth registration and marriage has become the focus of social debate.In this paper, we adopt co-occurrence semantic analysis, topic analysis andsentiment analysis to conduct multi-granularity semantic analysis of microblogcomments. It is found that the discussion on the proposal of ``removingmarriage restrictions from birth registration" involves the individual, societyand the state at three dimensions, and is detailed into social issues such aspersonal behaviour, social ethics and law, and national policy, with people'ssentiment inclined to be negative in most of the topics. Based on this, eightproposals were made to provide a reference for governmental decision making andto form a reference method for researching public opinion on political issues.</description><author>Yulin Zhou</author><pubDate>Wed, 19 Jul 2023 16:09:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10025v1</guid></item><item><title>Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK</title><link>http://arxiv.org/abs/2307.10022v1</link><description>I propose an open dataset of country-level historical opinion polling datafor the European Union and the UK. The dataset aims to fill a gap in availableopinion polling data for the European Union. Some existing datasets arerestricted to the past five years, limiting research opportunities. At the sametime, some larger proprietary datasets exist but are available only in a visualpreprocessed time series format. Finally, while other large datasets forindividual countries might exist, these could be inaccessible due to languagebarriers. The data was gathered from Wikipedia, and preprocessed using thepandas library. Both the raw and the preprocessed data are in the .csv format.I hope that given the recent advances in LLMs and deep learning in general,this large dataset will enable researchers to uncover complex interactionsbetween multimodal data (news articles, economic indicators, social media) andvoting behavior. The raw data, the preprocessed data, and the preprocessingscripts are available on GitHub.</description><author>Konstantinos Pitas</author><pubDate>Wed, 19 Jul 2023 16:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10022v1</guid></item><item><title>Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing</title><link>http://arxiv.org/abs/2303.15585v2</link><description>The field of mobile and wearable computing is undergoing a revolutionaryintegration of machine learning. Devices can now diagnose diseases, predictheart irregularities, and unlock the full potential of human cognition.However, the underlying algorithms powering these predictions are not immune tobiases with respect to sensitive attributes (e.g., gender, race), leading todiscriminatory outcomes. The goal of this work is to explore the extent towhich the mobile and wearable computing community has adopted ways of reportinginformation about datasets and models to surface and, eventually, counterbiases. Our systematic review of papers published in the Proceedings of the ACMInteractive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal from2018-2022 indicates that, while there has been progress made on algorithmicfairness, there is still ample room for growth. Our findings show that only asmall portion (5%) of published papers adheres to modern fairness reporting,while the overwhelming majority thereof focuses on accuracy or error metrics.To generalize these results across venues of similar scope, we analyzed recentproceedings of ACM MobiCom, MobiSys, and SenSys, IEEE Pervasive, and IEEETransactions on Mobile Computing Computing, and found no deviation from ourprimary result. In light of these findings, our work provides practicalguidelines for the design and development of mobile and wearable technologiesthat not only strive for accuracy but also fairness.</description><author>Sofia Yfantidou, Marios Constantinides, Dimitris Spathis, Athena Vakali, Daniele Quercia, Fahim Kawsar</author><pubDate>Wed, 19 Jul 2023 16:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15585v2</guid></item><item><title>Rob√¥CIn Small Size League Extended Team Description Paper for RoboCup 2023</title><link>http://arxiv.org/abs/2307.10018v1</link><description>Rob\^oCIn has participated in RoboCup Small Size League since 2019, won itsfirst world title in 2022 (Division B), and is currently a three-timesLatin-American champion. This paper presents our improvements to defend theSmall Size League (SSL) division B title in RoboCup 2023 in Bordeaux, France.This paper aims to share some of the academic research that our team developedover the past year. Our team has successfully published 2 articles related toSSL at two high-impact conferences: the 25th RoboCup International Symposiumand the 19th IEEE Latin American Robotics Symposium (LARS 2022). Over the lastyear, we have been continuously migrating from our past codebase toUnification. We will describe the new architecture implemented and some pointsof software and AI refactoring. In addition, we discuss the process ofintegrating machined components into the mechanical system, our development forparticipating in the vision blackout challenge last year and what we arepreparing for this year.</description><author>Aline Lima de Oliveira, Cau√™ Addae da Silva Gomes, Cec√≠lia Virginia Santos da Silva, Charles Matheus de Sousa Alves, Danilo Andrade Martins de Souza, Driele Pires Ferreira Ara√∫jo Xavier, Edgleyson Pereira da Silva, Felipe Bezerra Martins, Lucas Henrique Cavalcanti Santos, Lucas Dias Maciel, Matheus Paix√£o Gumercindo dos Santos, Matheus Lafayette Vasconcelos, Matheus Vin√≠cius Teotonio do Nascimento Andrade, Jo√£o Guilherme Oliveira Carvalho de Melo, Jo√£o Pedro Souza Pereira de Moura, Jos√© Ronald da Silva, Jos√© Victor Silva Cruz, Pedro Henrique Santana de Morais, Pedro Paulo Salman de Oliveira, Riei Joaquim Matos Rodrigues, Roberto Costa Fernandes, Ryan Vinicius Santos Morais, Tamara Mayara Ramos Teobaldo, Washington Igor dos Santos Silva, Edna Natividade Silva Barros</author><pubDate>Wed, 19 Jul 2023 15:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10018v1</guid></item><item><title>Finite-Time Analysis of Natural Actor-Critic for POMDPs</title><link>http://arxiv.org/abs/2202.09753v3</link><description>We consider the reinforcement learning problem for partially observed Markovdecision processes (POMDPs) with large or even countably infinite state spaces,where the controller has access to only noisy observations of the underlyingcontrolled Markov chain. We consider a natural actor-critic method that employsa finite internal memory for policy parameterization, and a multi-step temporaldifference learning algorithm for policy evaluation. We establish, to the bestof our knowledge, the first non-asymptotic global convergence of actor-criticmethods for partially observed systems under function approximation. Inparticular, in addition to the function approximation and statistical errorsthat also arise in MDPs, we explicitly characterize the error due to the use offinite-state controllers. This additional error is stated in terms of the totalvariation distance between the traditional belief state in POMDPs and theposterior distribution of the hidden state when using a finite-statecontroller. Further, we show that this error can be made small in the case ofsliding-block controllers by using larger block sizes.</description><author>Semih Cayci, Niao He, R. Srikant</author><pubDate>Wed, 19 Jul 2023 15:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.09753v3</guid></item><item><title>Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization</title><link>http://arxiv.org/abs/2307.06385v2</link><description>Audio-Visual Event Localization (AVEL) is the task of temporally localizingand classifying \emph{audio-visual events}, i.e., events simultaneously visibleand audible in a video. In this paper, we solve AVEL in a weakly-supervisedsetting, where only video-level event labels (their presence/absence, but nottheir locations in time) are available as supervision for training. Our idea isto use a base model to estimate labels on the training data at a finer temporalresolution than at the video level and re-train the model with these labels.I.e., we determine the subset of labels for each \emph{slice} of frames in atraining video by (i) replacing the frames outside the slice with those from asecond video having no overlap in video-level labels, and (ii) feeding thissynthetic video into the base model to extract labels for just the slice inquestion. To handle the out-of-distribution nature of our synthetic videos, wepropose an auxiliary objective for the base model that induces more reliablepredictions of the localized event labels as desired. Our three-stage pipelineoutperforms several existing AVEL methods with no architectural changes andimproves performance on a related weakly-supervised task as well.</description><author>Kalyan Ramakrishnan</author><pubDate>Wed, 19 Jul 2023 15:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06385v2</guid></item><item><title>Towards Fair Face Verification: An In-depth Analysis of Demographic Biases</title><link>http://arxiv.org/abs/2307.10011v1</link><description>Deep learning-based person identification and verification systems haveremarkably improved in terms of accuracy in recent years; however, suchsystems, including widely popular cloud-based solutions, have been found toexhibit significant biases related to race, age, and gender, a problem thatrequires in-depth exploration and solutions. This paper presents an in-depthanalysis, with a particular emphasis on the intersectionality of thesedemographic factors. Intersectional bias refers to the performancediscrepancies w.r.t. the different combinations of race, age, and gendergroups, an area relatively unexplored in current literature. Furthermore, thereliance of most state-of-the-art approaches on accuracy as the principalevaluation metric often masks significant demographic disparities inperformance. To counter this crucial limitation, we incorporate five additionalmetrics in our quantitative analysis, including disparate impact andmistreatment metrics, which are typically ignored by the relevantfairness-aware approaches. Results on the Racial Faces in-the-Wild (RFW)benchmark indicate pervasive biases in face recognition systems, extendingbeyond race, with different demographic factors yielding significantlydisparate outcomes. In particular, Africans demonstrate an 11.25% lower TruePositive Rate (TPR) compared to Caucasians, while only a 3.51% accuracy drop isobserved. Even more concerning, the intersections of multiple protected groups,such as African females over 60 years old, demonstrate a +39.89% disparatemistreatment rate compared to the highest Caucasians rate. By shedding light onthese biases and their implications, this paper aims to stimulate furtherresearch towards developing fairer, more equitable face recognition andverification systems.</description><author>Ioannis Sarridis, Christos Koutlis, Symeon Papadopoulos, Christos Diou</author><pubDate>Wed, 19 Jul 2023 15:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10011v1</guid></item><item><title>Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting</title><link>http://arxiv.org/abs/2106.07677v4</link><description>Restless and collapsing bandits are often used to model budget-constrainedresource allocation in settings where arms have action-dependent transitionprobabilities, such as the allocation of health interventions among patients.However, state-of-the-art Whittle-index-based approaches to this planningproblem either do not consider fairness among arms, or incentivize fairnesswithout guaranteeing it. We thus introduce ProbFair, a probabilistically fairpolicy that maximizes total expected reward and satisfies the budget constraintwhile ensuring a strictly positive lower bound on the probability of beingpulled at each timestep. We evaluate our algorithm on a real-world application,where interventions support continuous positive airway pressure (CPAP) therapyadherence among patients, as well as on a broader class of synthetic transitionmatrices. We find that ProbFair preserves utility while providing fairnessguarantees.</description><author>Christine Herlihy, Aviva Prins, Aravind Srinivasan, John P. Dickerson</author><pubDate>Wed, 19 Jul 2023 15:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.07677v4</guid></item><item><title>MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions</title><link>http://arxiv.org/abs/2307.10008v1</link><description>Audio-driven portrait animation aims to synthesize portrait videos that areconditioned by given audio. Animating high-fidelity and multimodal videoportraits has a variety of applications. Previous methods have attempted tocapture different motion modes and generate high-fidelity portrait videos bytraining different models or sampling signals from given videos. However,lacking correlation learning between lip-sync and other movements (e.g., headpose/eye blinking) usually leads to unnatural results. In this paper, wepropose a unified system for multi-person, diverse, and high-fidelity talkingportrait generation. Our method contains three stages, i.e., 1) Mapping-Oncenetwork with Dual Attentions (MODA) generates talking representation from givenaudio. In MODA, we design a dual-attention module to encode accurate mouthmovements and diverse modalities. 2) Facial composer network generates denseand detailed face landmarks, and 3) temporal-guided renderer syntheses stablevideos. Extensive evaluations demonstrate that the proposed system producesmore natural and realistic video portraits compared to previous methods.</description><author>Yunfei Liu, Lijian Lin, Fei Yu, Changyin Zhou, Yu Li</author><pubDate>Wed, 19 Jul 2023 15:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10008v1</guid></item><item><title>Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling</title><link>http://arxiv.org/abs/2303.11103v2</link><description>Sionna is a GPU-accelerated open-source library for link-level simulationsbased on TensorFlow. Since release v0.14 it integrates a differentiable raytracer (RT) for the simulation of radio wave propagation. This unique featureallows for the computation of gradients of the channel impulse response andother related quantities with respect to many system and environmentparameters, such as material properties, antenna patterns, array geometries, aswell as transmitter and receiver orientations and positions. In this paper, weoutline the key components of Sionna RT and showcase example applications suchas learning radio materials and optimizing transmitter orientations by gradientdescent. While classic ray tracing is a crucial tool for 6G research topicslike reconfigurable intelligent surfaces, integrated sensing andcommunications, as well as user localization, differentiable ray tracing is akey enabler for many novel and exciting research directions, for example,digital twins.</description><author>Jakob Hoydis, Fay√ßal A√Øt Aoudia, Sebastian Cammerer, Merlin Nimier-David, Nikolaus Binder, Guillermo Marcus, Alexander Keller</author><pubDate>Wed, 19 Jul 2023 15:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11103v2</guid></item><item><title>Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success</title><link>http://arxiv.org/abs/2208.07734v6</link><description>Self-supervised learning (SSL) has emerged as a promising alternative tocreate supervisory signals to real-world problems, avoiding the extensive costof manual labeling. SSL is particularly attractive for unsupervised tasks suchas anomaly detection (AD), where labeled anomalies are rare or oftennonexistent. A large catalog of augmentation functions has been used forSSL-based AD (SSAD) on image data, and recent works have reported that the typeof augmentation has a significant impact on accuracy. Motivated by those, thiswork sets out to put image-based SSAD under a larger lens and investigate therole of data augmentation in SSAD. Through extensive experiments on 3 differentdetector models and across 420 AD tasks, we provide comprehensive numerical andvisual evidences that the alignment between data augmentation andanomaly-generating mechanism is the key to the success of SSAD, and in the lackthereof, SSL may even impair accuracy. To the best of our knowledge, this isthe first meta-analysis on the role of data augmentation in SSAD.</description><author>Jaemin Yoo, Tiancheng Zhao, Leman Akoglu</author><pubDate>Wed, 19 Jul 2023 15:39:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07734v6</guid></item><item><title>6G Network Business Support System</title><link>http://arxiv.org/abs/2307.10004v1</link><description>6G is the next-generation intelligent and integrated digital informationinfrastructure, characterized by ubiquitous interconnection, nativeintelligence, multi-dimensional perception, global coverage, green andlow-carbon, native network security, etc. 6G will realize the transition fromserving people and people-things communication to supporting the efficientconnection of intelligent agents, and comprehensively leading the digital,intelligent and green transformation of the economy and the society. As thecore support system for mobile communication network, 6 6G BSS need tointegrate with new business models brought about by the development of thenext-generation Internet and IT, upgrade from "network-centric" to "businessand service centric" and "customer-centric". 6G OSS and BSS systems need tostrengthen their integration to improve the operational efficiency and benefitsof customers by connecting the digital intelligence support capabilities onboth sides of supply and demand. This paper provides a detailed introduction tothe overall vision, potential key technologies, and functional architecture of6G BSS systems. It also presents an evolutionary roadmap and technologicalprospects for the BSS systems from 5G to 6G.</description><author>Ye Ouyang, Yaqin Zhang, Peng Wang, Yunxin Liu, Wen Qiao, Jun Zhu, Yang Liu, Feng Zhang, Shuling Wang, Xidong Wang</author><pubDate>Wed, 19 Jul 2023 15:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10004v1</guid></item><item><title>A comparative analysis of SRGAN models</title><link>http://arxiv.org/abs/2307.09456v2</link><description>In this study, we evaluate the performance of multiple state-of-the-art SRGAN(Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGANand EDSR, on a benchmark dataset of real-world images which undergo degradationusing a pipeline. Our results show that some models seem to significantlyincrease the resolution of the input images while preserving their visualquality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASEmodel from huggingface outperforms the remaining candidate models in terms ofboth quantitative metrics and subjective visual quality assessments with leastcompute overhead. Specifically, EDSR generates images with higher peaksignal-to-noise ratio (PSNR) and structural similarity index (SSIM) values andare seen to return high quality OCR results with Tesseract OCR engine. Thesefindings suggest that EDSR is a robust and effective approach for single-imagesuper-resolution and may be particularly well-suited for applications wherehigh-quality visual fidelity is critical and optimized compute.</description><author>Fatemeh Rezapoor Nikroo, Ajinkya Deshmukh, Anantha Sharma, Adrian Tam, Kaarthik Kumar, Cleo Norris, Aditya Dangi</author><pubDate>Wed, 19 Jul 2023 15:27:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09456v2</guid></item><item><title>TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction</title><link>http://arxiv.org/abs/2307.10003v1</link><description>The field of Explainable Artificial Intelligence (XAI) aims to improve theinterpretability of black-box machine learning models. Building a heatmap basedon the importance value of input features is a popular method for explainingthe underlying functions of such models in producing their predictions.Heatmaps are almost understandable to humans, yet they are not without flaws.Non-expert users, for example, may not fully understand the logic of heatmaps(the logic in which relevant pixels to the model's prediction are highlightedwith different intensities or colors). Additionally, objects and regions of theinput image that are relevant to the model prediction are frequently notentirely differentiated by heatmaps. In this paper, we propose a frameworkcalled TbExplain that employs XAI techniques and a pre-trained object detectorto present text-based explanations of scene classification models. Moreover,TbExplain incorporates a novel method to correct predictions and textuallyexplain them based on the statistics of objects in the input image when theinitial prediction is unreliable. To assess the trustworthiness and validity ofthe text-based explanations, we conducted a qualitative experiment, and thefindings indicated that these explanations are sufficiently reliable.Furthermore, our quantitative and qualitative experiments on TbExplain withscene classification datasets reveal an improvement in classification accuracyover ResNet variants.</description><author>Amirhossein Aminimehr, Pouya Khani, Amirali Molaei, Amirmohammad Kazemeini, Erik Cambria</author><pubDate>Wed, 19 Jul 2023 15:23:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10003v1</guid></item><item><title>Tackling Provably Hard Representative Selection via Graph Neural Networks</title><link>http://arxiv.org/abs/2205.10403v2</link><description>Representative Selection (RS) is the problem of finding a small subset ofexemplars from a dataset that is representative of the dataset. In this paper,we study RS for attributed graphs, and focus on finding representative nodesthat optimize the accuracy of a model trained on the selected representatives.Theoretically, we establish a new hardness result forRS (in the absence of agraph structure) by proving that a particular, highly practical variant of it(RS for Learning) is hard to approximate in polynomial time within anyreasonable factor, which implies a significant potential gap between theoptimum solution of widely-used surrogate functions and the actual accuracy ofthe model. We then study the setting where a (homophilous) graph structure isavailable, or can be constructed, between the data points.We show that with anappropriate modeling approach, the presence of such a structure can turn a hardRS (for learning) problem into one that can be effectively solved. To this end,we develop RS-GNN, a representation learning-based RS model based on GraphNeural Networks. Empirically, we demonstrate the effectiveness of RS-GNN onproblems with predefined graph structures as well as problems with graphsinduced from node feature similarities, by showing that RS-GNN achievessignificant improvements over established baselines on a suite of eightbenchmarks.</description><author>Mehran Kazemi, Anton Tsitsulin, Hossein Esfandiari, MohammadHossein Bateni, Deepak Ramachandran, Bryan Perozzi, Vahab Mirrokni</author><pubDate>Wed, 19 Jul 2023 15:23:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.10403v2</guid></item><item><title>As large as it gets: Learning infinitely large Filters via Neural Implicit Functions in the Fourier Domain</title><link>http://arxiv.org/abs/2307.10001v1</link><description>Motivated by the recent trend towards the usage of larger receptive fieldsfor more context-aware neural networks in vision applications, we aim toinvestigate how large these receptive fields really need to be. To facilitatesuch study, several challenges need to be addressed, most importantly: (i) Weneed to provide an effective way for models to learn large filters (potentiallyas large as the input data) without increasing their memory consumption duringtraining or inference, (ii) the study of filter sizes has to be decoupled fromother effects such as the network width or number of learnable parameters, and(iii) the employed convolution operation should be a plug-and-play module thatcan replace any conventional convolution in a Convolutional Neural Network(CNN) and allow for an efficient implementation in current frameworks. Tofacilitate such models, we propose to learn not spatial but frequencyrepresentations of filter weights as neural implicit functions, such that eveninfinitely large filters can be parameterized by only a few learnable weights.The resulting neural implicit frequency CNNs are the first models to achieveresults on par with the state-of-the-art on large image classificationbenchmarks while executing convolutions solely in the frequency domain and canbe employed within any CNN architecture. They allow us to provide an extensiveanalysis of the learned receptive fields. Interestingly, our analysis showsthat, although the proposed networks could learn very large convolutionkernels, the learned filters practically translate into well-localized andrelatively small convolution kernels in the spatial domain.</description><author>Julia Grabinski, Janis Keuper, Margret Keuper</author><pubDate>Wed, 19 Jul 2023 15:21:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10001v1</guid></item><item><title>Towards the Sparseness of Projection Head in Self-Supervised Learning</title><link>http://arxiv.org/abs/2307.08913v2</link><description>In recent years, self-supervised learning (SSL) has emerged as a promisingapproach for extracting valuable representations from unlabeled data. Onesuccessful SSL method is contrastive learning, which aims to bring positiveexamples closer while pushing negative examples apart. Many current contrastivelearning approaches utilize a parameterized projection head. Through acombination of empirical analysis and theoretical investigation, we provideinsights into the internal mechanisms of the projection head and itsrelationship with the phenomenon of dimensional collapse. Our findingsdemonstrate that the projection head enhances the quality of representations byperforming contrastive loss in a projected subspace. Therefore, we propose anassumption that only a subset of features is necessary when minimizing thecontrastive loss of a mini-batch of data. Theoretical analysis further suggeststhat a sparse projection head can enhance generalization, leading us tointroduce SparseHead - a regularization term that effectively constrains thesparsity of the projection head, and can be seamlessly integrated with anyself-supervised learning (SSL) approaches. Our experimental results validatethe effectiveness of SparseHead, demonstrating its ability to improve theperformance of existing contrastive methods.</description><author>Zeen Song, Xingzhe Su, Jingyao Wang, Wenwen Qiang, Changwen Zheng, Fuchun Sun</author><pubDate>Wed, 19 Jul 2023 15:18:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08913v2</guid></item><item><title>On sampling determinantal and Pfaffian point processes on a quantum computer</title><link>http://arxiv.org/abs/2305.15851v2</link><description>DPPs were introduced by Macchi as a model in quantum optics the 1970s. Sincethen, they have been widely used as models and subsampling tools in statisticsand computer science. Most applications require sampling from a DPP, and giventheir quantum origin, it is natural to wonder whether sampling a DPP on aquantum computer is easier than on a classical one. We focus here on DPPs overa finite state space, which are distributions over the subsets of$\{1,\dots,N\}$ parametrized by an $N\times N$ Hermitian kernel matrix. Vanillasampling consists in two steps, of respective costs $\mathcal{O}(N^3)$ and$\mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rankof the kernel matrix. A large first part of the current paper consists inexplaining why the state-of-the-art in quantum simulation of fermionic systemsalready yields quantum DPP sampling algorithms. We then modify existing quantumcircuits, and discuss their insertion in a full DPP sampling pipeline thatstarts from practical kernel specifications. The bottom line is that, with $P$(classical) parallel processors, we can divide the preprocessing cost by $P$and build a quantum circuit with $\mathcal{O}(Nr)$ gates that sample a givenDPP, with depth varying from $\mathcal{O}(N)$ to $\mathcal{O}(r\log N)$depending on qubit-communication constraints on the target machine. We alsoconnect existing work on the simulation of superconductors to Pfaffian pointprocesses, which generalize DPPs and would be a natural addition to the machinelearner's toolbox. Finally, the circuits are empirically validated on aclassical simulator and on 5-qubit machines.</description><author>R√©mi Bardenet, Micha√´l Fanuel, Alexandre Feller</author><pubDate>Wed, 19 Jul 2023 15:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15851v2</guid></item><item><title>Generating Mathematical Derivations with Large Language Models</title><link>http://arxiv.org/abs/2307.09998v1</link><description>The derivation of mathematical results in specialised fields using LargeLanguage Models (LLMs) is an emerging research direction that can help identifymodels' limitations, and potentially support mathematical discovery. In thispaper, we leverage a symbolic engine to generate derivations of equations atscale, and investigate the capabilities of LLMs when deriving goal equationsfrom premises. Specifically, we employ in-context learning for GPT andfine-tune a range of T5 models to compare the robustness and generalisation ofpre-training strategies to specialised models. Empirical results show thatfine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static andout-of-distribution test sets in terms of absolute performance. However, anin-depth analysis reveals that the fine-tuned models are more sensitive toperturbations involving unseen symbols and (to a lesser extent) changes toequation structure. In addition, we analyse 1.7K equations and over 200derivations to highlight common reasoning errors such as the inclusion ofincorrect, irrelevant, and redundant equations, along with the tendency to skipderivation steps. Finally, we explore the suitability of existing metrics forevaluating mathematical derivations finding evidence that, while they capturegeneral properties such as sensitivity to perturbations, they fail to highlightfine-grained reasoning errors and essential differences between models.Overall, this work demonstrates that training models on synthetic data canimprove their mathematical capabilities beyond larger architectures.</description><author>Jordan Meadows, Marco Valentino, Andre Freitas</author><pubDate>Wed, 19 Jul 2023 15:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09998v1</guid></item><item><title>ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks</title><link>http://arxiv.org/abs/2303.15056v2</link><description>Many NLP applications require manual data annotations for a variety of tasks,notably to train classifiers or evaluate the performance of unsupervisedmodels. Depending on the size and degree of complexity, the tasks may beconducted by crowd-workers on platforms such as MTurk as well as trainedannotators, such as research assistants. Using a sample of 2,382 tweets, wedemonstrate that ChatGPT outperforms crowd-workers for several annotationtasks, including relevance, stance, topics, and frames detection. Specifically,the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out offive tasks, while ChatGPT's intercoder agreement exceeds that of bothcrowd-workers and trained annotators for all tasks. Moreover, theper-annotation cost of ChatGPT is less than $0.003 -- about twenty timescheaper than MTurk. These results show the potential of large language modelsto drastically increase the efficiency of text classification.</description><author>Fabrizio Gilardi, Meysam Alizadeh, Ma√´l Kubli</author><pubDate>Wed, 19 Jul 2023 15:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15056v2</guid></item><item><title>TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical Phase Recognition</title><link>http://arxiv.org/abs/2307.09997v1</link><description>To enable context-aware computer assistance in the operating room of thefuture, cognitive systems need to understand automatically which surgical phaseis being performed by the medical team. The primary source of information forsurgical phase recognition is typically video, which presents two challenges:extracting meaningful features from the video stream and effectively modelingtemporal information in the sequence of visual features. For temporal modeling,attention mechanisms have gained popularity due to their ability to capturelong-range dependencies. In this paper, we explore design choices for attentionin existing temporal models for surgical phase recognition and propose a novelapproach that does not resort to local attention or regularization of attentionweights: TUNeS is an efficient and simple temporal model that incorporatesself-attention at the coarsest stage of a U-Net-like structure. In addition, wepropose to train the feature extractor, a standard CNN, together with an LSTMon preferably long video segments, i.e., with long temporal context. In ourexperiments, all temporal models performed better on top of feature extractorsthat were trained with longer temporal context. On top of these contextualizedfeatures, TUNeS achieves state-of-the-art results on Cholec80.</description><author>Isabel Funke, Dominik Rivoir, Stefanie Krell, Stefanie Speidel</author><pubDate>Wed, 19 Jul 2023 15:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09997v1</guid></item><item><title>Impact of Disentanglement on Pruning Neural Networks</title><link>http://arxiv.org/abs/2307.09994v1</link><description>Deploying deep learning neural networks on edge devices, to accomplish taskspecific objectives in the real-world, requires a reduction in their memoryfootprint, power consumption, and latency. This can be realized via efficientmodel compression. Disentangled latent representations produced by variationalautoencoder (VAE) networks are a promising approach for achieving modelcompression because they mainly retain task-specific information, discardinguseless information for the task at hand. We make use of the Beta-VAE frameworkcombined with a standard criterion for pruning to investigate the impact offorcing the network to learn disentangled representations on the pruningprocess for the task of classification. In particular, we perform experimentson MNIST and CIFAR10 datasets, examine disentanglement challenges, and proposea path forward for future works.</description><author>Carl Shneider, Peyman Rostami, Anis Kacem, Nilotpal Sinha, Abd El Rahman Shabayek, Djamila Aouada</author><pubDate>Wed, 19 Jul 2023 14:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09994v1</guid></item><item><title>M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization</title><link>http://arxiv.org/abs/2307.08347v2</link><description>Medical vision-language models enable co-learning and integrating featuresfrom medical imaging and clinical text. However, these models are not easy totrain and the latent representation space can be complex. Here we propose anovel way for pre-training and regularising medical vision-language models. Theproposed method, named Medical vision-language pre-training with Frozenlanguage models and Latent spAce Geometry optimization (M-FLAG), leverages afrozen language model for training stability and efficiency and introduces anovel orthogonality loss to harmonize the latent space geometry. We demonstratethe potential of the pre-trained model on three downstream tasks: medical imageclassification, segmentation, and object detection. Extensive experimentsacross five public datasets demonstrate that M-FLAG significantly outperformsexisting medical vision-language pre-training approaches and reduces the numberof parameters by 78\%. Notably, M-FLAG achieves outstanding performance on thesegmentation task while using only 1\% of the RSNA dataset, even outperformingImageNet pre-trained models that have been fine-tuned using 100\% of the data.</description><author>Che Liu, Sibo Cheng, Chen Chen, Mengyun Qiao, Weitong Zhang, Anand Shah, Wenjia Bai, Rossella Arcucci</author><pubDate>Wed, 19 Jul 2023 14:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08347v2</guid></item><item><title>UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing</title><link>http://arxiv.org/abs/2307.09989v1</link><description>When doing private domain marketing with cloud services, the merchantsusually have to purchase different machine learning models for the multiplemarketing purposes, leading to a very high cost. We present a unified user-itemmatching framework to simultaneously conduct item recommendation and usertargeting with just one model. We empirically demonstrate that the aboveconcurrent modeling is viable via modeling the user-item interaction matrixwith the multinomial distribution, and propose a bidirectional bias-correctedNCE loss for the implementation. The proposed loss function guides the model tolearn the user-item joint probability $p(u,i)$ instead of the conditionalprobability $p(i|u)$ or $p(u|i)$ through correcting both the users and items'biases caused by the in-batch negative sampling. In addition, our framework ismodel-agnostic enabling a flexible adaptation of different model architectures.Extensive experiments demonstrate that our framework results in significantperformance gains in comparison with the state-of-the-art methods, with greatlyreduced cost on computing resources and daily maintenance.</description><author>Qifang Zhao, Tianyu Li, Meng Du, Yu Jiang, Qinghui Sun, Zhongyao Wang, Hong Liu, Huan Xu</author><pubDate>Wed, 19 Jul 2023 14:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09989v1</guid></item><item><title>TinyTrain: Deep Neural Network Training at the Extreme Edge</title><link>http://arxiv.org/abs/2307.09988v1</link><description>On-device training is essential for user personalisation and privacy. Withthe pervasiveness of IoT devices and microcontroller units (MCU), this taskbecomes more challenging due to the constrained memory and compute resources,and the limited availability of labelled user data. Nonetheless, prior worksneglect the data scarcity issue, require excessively long training time (e.g. afew hours), or induce substantial accuracy loss ($\geq$10\%). We proposeTinyTrain, an on-device training approach that drastically reduces trainingtime by selectively updating parts of the model and explicitly coping with datascarcity. TinyTrain introduces a task-adaptive sparse-update method thatdynamically selects the layer/channel based on a multi-objective criterion thatjointly captures user data, the memory, and the compute capabilities of thetarget device, leading to high accuracy on unseen tasks with reducedcomputation and memory footprint. TinyTrain outperforms vanilla fine-tuning ofthe entire network by 3.6-5.0\% in accuracy, while reducing the backward-passmemory and computation cost by up to 2,286$\times$ and 7.68$\times$,respectively. Targeting broadly used real-world edge devices, TinyTrainachieves 9.5$\times$ faster and 3.5$\times$ more energy-efficient training overstatus-quo approaches, and 2.8$\times$ smaller memory footprint than SOTAapproaches, while remaining within the 1 MB memory envelope of MCU-gradeplatforms.</description><author>Young D. Kwon, Rui Li, Stylianos I. Venieris, Jagmohan Chauhan, Nicholas D. Lane, Cecilia Mascolo</author><pubDate>Wed, 19 Jul 2023 14:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09988v1</guid></item><item><title>Generalization Error Bounds for Noisy, Iterative Algorithms via Maximal Leakage</title><link>http://arxiv.org/abs/2302.14518v2</link><description>We adopt an information-theoretic framework to analyze the generalizationbehavior of the class of iterative, noisy learning algorithms. This class isparticularly suitable for study under information-theoretic metrics as thealgorithms are inherently randomized, and it includes commonly used algorithmssuch as Stochastic Gradient Langevin Dynamics (SGLD). Herein, we use themaximal leakage (equivalently, the Sibson mutual information of order infinity)metric, as it is simple to analyze, and it implies both bounds on theprobability of having a large generalization error and on its expected value.We show that, if the update function (e.g., gradient) is bounded in $L_2$-normand the additive noise is isotropic Gaussian noise, then one can obtain anupper-bound on maximal leakage in semi-closed form. Furthermore, we demonstratehow the assumptions on the update function affect the optimal (in the sense ofminimizing the induced maximal leakage) choice of the noise. Finally, wecompute explicit tight upper bounds on the induced maximal leakage for otherscenarios of interest.</description><author>Ibrahim Issa, Amedeo Roberto Esposito, Michael Gastpar</author><pubDate>Wed, 19 Jul 2023 14:48:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14518v2</guid></item><item><title>Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?</title><link>http://arxiv.org/abs/2307.09985v1</link><description>A typical benchmark dataset for recommender system (RecSys) evaluationconsists of user-item interactions generated on a platform within a timeperiod. The interaction generation mechanism partially explains why a userinteracts with (e.g.,like, purchase, rate) an item, and the context of when aparticular interaction happened. In this study, we conduct a meticulousanalysis on the MovieLens dataset and explain the potential impact on using thedataset for evaluating recommendation algorithms. We make a few main findingsfrom our analysis. First, there are significant differences in userinteractions at the different stages when a user interacts with the MovieLensplatform. The early interactions largely define the user portrait which affectthe subsequent interactions. Second, user interactions are highly affected bythe candidate movies that are recommended by the platform's internalrecommendation algorithm(s). Removal of interactions that happen nearer to thelast few interactions of a user leads to increasing difficulty in learning userpreference, thus deteriorating recommendation accuracy. Third, changing theorder of user interactions makes it more difficult for sequential algorithms tocapture the progressive interaction process. Based on these findings, wefurther discuss the discrepancy between the interaction generation mechanismthat is employed by the MovieLens system and that of typical real worldrecommendation scenarios. In summary, models that achieve excellentrecommendation accuracy on the MovieLens dataset may not demonstrate superiorperformance in practice for at least two kinds of differences: (i) thedifferences in the contexts of user-item interaction generation, and (ii) thedifferences in user knowledge about the item collections.</description><author>Yu-chen Fan, Yitong Ji, Jie Zhang, Aixin Sun</author><pubDate>Wed, 19 Jul 2023 14:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09985v1</guid></item><item><title>Revisiting Softmax for Uncertainty Approximation in Text Classification</title><link>http://arxiv.org/abs/2210.14037v2</link><description>Uncertainty approximation in text classification is an important area withapplications in domain adaptation and interpretability. One of the most widelyused uncertainty approximation methods is Monte Carlo (MC) Dropout, which iscomputationally expensive as it requires multiple forward passes through themodel. A cheaper alternative is to simply use the softmax based on a singleforward pass without dropout to estimate model uncertainty. However, prior workhas indicated that these predictions tend to be overconfident. In this paper,we perform a thorough empirical analysis of these methods on five datasets withtwo base neural architectures in order to identify the trade-offs between thetwo. We compare both softmax and an efficient version of MC Dropout on theiruncertainty approximations and downstream text classification performance,while weighing their runtime (cost) against performance (benefit). We findthat, while MC dropout produces the best uncertainty approximations, using asimple softmax leads to competitive and in some cases better uncertaintyestimation for text classification at a much lower computational cost,suggesting that softmax can in fact be a sufficient uncertainty estimate whencomputational resources are a concern.</description><author>Andreas Nugaard Holm, Dustin Wright, Isabelle Augenstein</author><pubDate>Wed, 19 Jul 2023 14:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.14037v2</guid></item><item><title>Lazy Visual Localization via Motion Averaging</title><link>http://arxiv.org/abs/2307.09981v1</link><description>Visual (re)localization is critical for various applications in computervision and robotics. Its goal is to estimate the 6 degrees of freedom (DoF)camera pose for each query image, based on a set of posed database images.Currently, all leading solutions are structure-based that either explicitlyconstruct 3D metric maps from the database with structure-from-motion, orimplicitly encode the 3D information with scene coordinate regression models.On the contrary, visual localization without reconstructing the scene in 3Doffers clear benefits. It makes deployment more convenient by reducing databasepre-processing time, releasing storage requirements, and remaining unaffectedby imperfect reconstruction, etc. In this technical report, we demonstrate thatit is possible to achieve high localization accuracy without reconstructing thescene from the database. The key to achieving this owes to a tailored motionaveraging over database-query pairs. Experiments show that our visuallocalization proposal, LazyLoc, achieves comparable performance againststate-of-the-art structure-based methods. Furthermore, we showcase theversatility of LazyLoc, which can be easily extended to handle complexconfigurations such as multi-query co-localization and camera rigs.</description><author>Siyan Dong, Shaohui Liu, Hengkai Guo, Baoquan Chen, Marc Pollefeys</author><pubDate>Wed, 19 Jul 2023 14:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09981v1</guid></item><item><title>Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks</title><link>http://arxiv.org/abs/2307.09977v1</link><description>The paradigm of federated learning (FL) to address data privacy concerns bylocally training parameters on resource-constrained clients in a distributedmanner has garnered significant attention. Nonetheless, FL is not applicablewhen not all clients within the coverage of the FL server are registered withthe FL network. To bridge this gap, this paper proposes joint learner referralaided federated client selection (LRef-FedCS), along with communications andcomputing resource scheduling, and local model accuracy optimization (LMAO)methods. These methods are designed to minimize the cost incurred by theworst-case participant and ensure the long-term fairness of FL in hierarchicalInternet of Things (HieIoT) networks. Utilizing the Lyapunov optimizationtechnique, we reformulate the original problem into a stepwise jointoptimization problem (JOP). Subsequently, to tackle the mixed-integernon-convex JOP, we separatively and iteratively address LRef-FedCS and LMAOthrough the centralized method and self-adaptive global best harmony search(SGHS) algorithm, respectively. To enhance scalability, we further propose adistributed LRef-FedCS approach based on a matching game to replace thecentralized method described above. Numerical simulations and experimentalresults on the MNIST/CIFAR-10 datasets demonstrate that our proposed LRef-FedCSapproach could achieve a good balance between pursuing high global accuracy andreducing cost.</description><author>Yulan Gao, Ziqiang Ye, Yue Xiao, Wei Xiang</author><pubDate>Wed, 19 Jul 2023 14:33:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09977v1</guid></item><item><title>BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits</title><link>http://arxiv.org/abs/2307.03587v2</link><description>We propose a novel Bayesian-Optimistic Frequentist Upper Confidence Bound(BOF-UCB) algorithm for stochastic contextual linear bandits in non-stationaryenvironments. This unique combination of Bayesian and frequentist principlesenhances adaptability and performance in dynamic settings. The BOF-UCBalgorithm utilizes sequential Bayesian updates to infer the posteriordistribution of the unknown regression parameter, and subsequently employs afrequentist approach to compute the Upper Confidence Bound (UCB) by maximizingthe expected reward over the posterior distribution. We provide theoreticalguarantees of BOF-UCB's performance and demonstrate its effectiveness inbalancing exploration and exploitation on synthetic datasets and classicalcontrol tasks in a reinforcement learning setting. Our results show thatBOF-UCB outperforms existing methods, making it a promising solution forsequential decision-making in non-stationary environments.</description><author>Nicklas Werge, Abdullah Akg√ºl, Melih Kandemir</author><pubDate>Wed, 19 Jul 2023 14:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03587v2</guid></item><item><title>Disentangle then Parse:Night-time Semantic Segmentation with Illumination Disentanglement</title><link>http://arxiv.org/abs/2307.09362v2</link><description>Most prior semantic segmentation methods have been developed for day-timescenes, while typically underperforming in night-time scenes due toinsufficient and complicated lighting conditions. In this work, we tackle thischallenge by proposing a novel night-time semantic segmentation paradigm, i.e.,disentangle then parse (DTP). DTP explicitly disentangles night-time imagesinto light-invariant reflectance and light-specific illumination components andthen recognizes semantics based on their adaptive fusion. Concretely, theproposed DTP comprises two key components: 1) Instead of processinglighting-entangled features as in prior works, our Semantic-OrientedDisentanglement (SOD) framework enables the extraction of reflectance componentwithout being impeded by lighting, allowing the network to consistentlyrecognize the semantics under cover of varying and complicated lightingconditions. 2) Based on the observation that the illumination component canserve as a cue for some semantically confused regions, we further introduce anIllumination-Aware Parser (IAParser) to explicitly learn the correlationbetween semantics and lighting, and aggregate the illumination features toyield more precise predictions. Extensive experiments on the night-timesegmentation task with various settings demonstrate that DTP significantlyoutperforms state-of-the-art methods. Furthermore, with negligible additionalparameters, DTP can be directly used to benefit existing day-time methods fornight-time segmentation.</description><author>Zhixiang Wei, Lin Chen, Tao Tu, Huaian Chen, Pengyang Ling, Yi Jin</author><pubDate>Wed, 19 Jul 2023 14:21:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09362v2</guid></item><item><title>AdaMSS: Adaptive Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images</title><link>http://arxiv.org/abs/2305.09946v2</link><description>Survival prediction is a major concern for cancer management. Deep survivalmodels based on deep learning have been widely adopted to perform end-to-endsurvival prediction from medical images. Recent deep survival models achievedpromising performance by jointly performing tumor segmentation with survivalprediction, where the models were guided to extract tumor-related informationthrough Multi-Task Learning (MTL). However, these deep survival models havedifficulties in exploring out-of-tumor prognostic information. In addition,existing deep survival models are unable to effectively leverage multi-modalityimages. Empirically-designed fusion strategies were commonly adopted to fusemulti-modality information via task-specific manually-designed networks, thuslimiting the adaptability to different scenarios. In this study, we propose anAdaptive Multi-modality Segmentation-to-Survival model (AdaMSS) for survivalprediction from PET/CT images. Instead of adopting MTL, we propose a novelSegmentation-to-Survival Learning (SSL) strategy, where our AdaMSS is trainedfor tumor segmentation and survival prediction sequentially in two stages. Thisstrategy enables the AdaMSS to focus on tumor regions in the first stage andgradually expand its focus to include other prognosis-related regions in thesecond stage. We also propose a data-driven strategy to fuse multi-modalityinformation, which realizes adaptive optimization of fusion strategies based ontraining data during training. With the SSL and data-driven fusion strategies,our AdaMSS is designed as an adaptive model that can self-adapt its focusregions and fusion strategy for different training stages. Extensiveexperiments with two large clinical datasets show that our AdaMSS outperformsstate-of-the-art survival prediction methods.</description><author>Mingyuan Meng, Bingxin Gu, Michael Fulham, Shaoli Song, Dagan Feng, Lei Bi, Jinman Kim</author><pubDate>Wed, 19 Jul 2023 14:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09946v2</guid></item><item><title>Towards green AI-based software systems: an architecture-centric approach (GAISSA)</title><link>http://arxiv.org/abs/2307.09964v1</link><description>Nowadays, AI-based systems have achieved outstanding results and haveoutperformed humans in different domains. However, the processes of training AImodels and inferring from them require high computational resources, which posea significant challenge in the current energy efficiency societal demand. Tocope with this challenge, this research project paper describes the mainvision, goals, and expected outcomes of the GAISSA project. The GAISSA projectaims at providing data scientists and software engineers tool-supported,architecture-centric methods for the modelling and development of greenAI-based systems. Although the project is in an initial stage, we describe thecurrent research results, which illustrate the potential to achieve GAISSAobjectives.</description><author>Silverio Mart√≠nez-Fern√°ndez, Xavier Franch, Francisco Dur√°n</author><pubDate>Wed, 19 Jul 2023 14:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09964v1</guid></item><item><title>Mining Negative Temporal Contexts For False Positive Suppression In Real-Time Ultrasound Lesion Detection</title><link>http://arxiv.org/abs/2305.18060v2</link><description>During ultrasonic scanning processes, real-time lesion detection can assistradiologists in accurate cancer diagnosis. However, this essential task remainschallenging and underexplored. General-purpose real-time object detectionmodels can mistakenly report obvious false positives (FPs) when applied toultrasound videos, potentially misleading junior radiologists. One key issue istheir failure to utilize negative symptoms in previous frames, denoted asnegative temporal contexts (NTC). To address this issue, we propose to extractcontexts from previous frames, including NTC, with the guidance of inverseoptical flow. By aggregating extracted contexts, we endow the model with theability to suppress FPs by leveraging NTC. We call the resulting modelUltraDet. The proposed UltraDet demonstrates significant improvement overprevious state-of-the-arts and achieves real-time inference speed. We releasethe code, checkpoints, and high-quality labels of the CVA-BUS dataset inhttps://github.com/HaojunYu1998/UltraDet.</description><author>Haojun Yu, Youcheng Li, QuanLin Wu, Ziwei Zhao, Dengbo Chen, Dong Wang, Liwei Wang</author><pubDate>Wed, 19 Jul 2023 14:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18060v2</guid></item><item><title>Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics</title><link>http://arxiv.org/abs/2210.06226v2</link><description>Several algorithms involving the Variational R\'enyi (VR) bound have beenproposed to minimize an alpha-divergence between a target posteriordistribution and a variational distribution. Despite promising empiricalresults, those algorithms resort to biased stochastic gradient descentprocedures and thus lack theoretical guarantees. In this paper, we formalizeand study the VR-IWAE bound, a generalization of the Importance WeightedAuto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys severaldesirable properties and notably leads to the same stochastic gradient descentprocedure as the VR bound in the reparameterized case, but this time by relyingon unbiased gradient estimators. We then provide two complementary theoreticalanalyses of the VR-IWAE bound and thus of the standard IWAE bound. Thoseanalyses shed light on the benefits or lack thereof of these bounds. Lastly, weillustrate our theoretical claims over toy and real-data examples.</description><author>Kam√©lia Daudel, Joe Benton, Yuyang Shi, Arnaud Doucet</author><pubDate>Wed, 19 Jul 2023 14:08:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06226v2</guid></item><item><title>GUIDO: A Hybrid Approach to Guideline Discovery &amp; Ordering from Natural Language Texts</title><link>http://arxiv.org/abs/2307.09959v1</link><description>Extracting workflow nets from textual descriptions can be used to simplifyguidelines or formalize textual descriptions of formal processes like businessprocesses and algorithms. The task of manually extracting processes, however,requires domain expertise and effort. While automatic process model extractionis desirable, annotating texts with formalized process models is expensive.Therefore, there are only a few machine-learning-based extraction approaches.Rule-based approaches, in turn, require domain specificity to work well and canrarely distinguish relevant and irrelevant information in textual descriptions.In this paper, we present GUIDO, a hybrid approach to the process modelextraction task that first, classifies sentences regarding their relevance tothe process model, using a BERT-based sentence classifier, and second, extractsa process model from the sentences classified as relevant, using dependencyparsing. The presented approach achieves significantly better results than apure rule-based approach. GUIDO achieves an average behavioral similarity scoreof $0.93$. Still, in comparison to purely machine-learning-based approaches,the annotation costs stay low.</description><author>Nils Freyer, Dustin Thewes, Matthias Meinecke</author><pubDate>Wed, 19 Jul 2023 14:01:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09959v1</guid></item><item><title>MixPath: A Unified Approach for One-shot Neural Architecture Search</title><link>http://arxiv.org/abs/2001.05887v4</link><description>Blending multiple convolutional kernels is proved advantageous in neuralarchitecture design. However, current two-stage neural architecture searchmethods are mainly limited to single-path search spaces. How to efficientlysearch models of multi-path structures remains a difficult problem. In thispaper, we are motivated to train a one-shot multi-path supernet to accuratelyevaluate the candidate architectures. Specifically, we discover that in thestudied search spaces, feature vectors summed from multiple paths are nearlymultiples of those from a single path. Such disparity perturbs the supernettraining and its ranking ability. Therefore, we propose a novel mechanismcalled Shadow Batch Normalization (SBN) to regularize the disparate featurestatistics. Extensive experiments prove that SBNs are capable of stabilizingthe optimization and improving ranking performance. We call our unifiedmulti-path one-shot approach as MixPath, which generates a series of modelsthat achieve state-of-the-art results on ImageNet.</description><author>Xiangxiang Chu, Shun Lu, Xudong Li, Bo Zhang</author><pubDate>Wed, 19 Jul 2023 13:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2001.05887v4</guid></item><item><title>XSkill: Cross Embodiment Skill Discovery</title><link>http://arxiv.org/abs/2307.09955v1</link><description>Human demonstration videos are a widely available data source for robotlearning and an intuitive user interface for expressing desired behavior.However, directly extracting reusable robot manipulation skills fromunstructured human videos is challenging due to the big embodiment differenceand unobserved action parameters. To bridge this embodiment gap, this paperintroduces XSkill, an imitation learning framework that 1) discovers across-embodiment representation called skill prototypes purely from unlabeledhuman and robot manipulation videos, 2) transfers the skill representation torobot actions using conditional diffusion policy, and finally, 3) composes thelearned skill to accomplish unseen tasks specified by a human prompt video. Ourexperiments in simulation and real-world environments show that the discoveredskill prototypes facilitate both skill transfer and composition for unseentasks, resulting in a more general and scalable imitation learning framework.The performance of XSkill is best understood from the anonymous website:https://xskillcorl.github.io.</description><author>Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, Shuran Song</author><pubDate>Wed, 19 Jul 2023 13:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09955v1</guid></item><item><title>U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation</title><link>http://arxiv.org/abs/2307.09947v1</link><description>Deep neural networks have shown exceptional performance in various tasks, buttheir lack of robustness, reliability, and tendency to be overconfident posechallenges for their deployment in safety-critical applications like autonomousdriving. In this regard, quantifying the uncertainty inherent to a model'sprediction is a promising endeavour to address these shortcomings. In thiswork, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) thatincorporates dynamic predictive uncertainties into the training process bypixel-wise weighting of the well-known cross-entropy loss (CE). Throughextensive experimentation, we demonstrate the superiority of U-CE over regularCE training on two benchmark datasets, Cityscapes and ACDC, using two commonbackbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to trainmodels that not only improve their segmentation performance but also providemeaningful uncertainties after training. Consequently, we contribute to thedevelopment of more robust and reliable segmentation models, ultimatelyadvancing the state-of-the-art in safety-critical applications and beyond.</description><author>Steven Landgraf, Markus Hillemann, Kira Wursthorn, Markus Ulrich</author><pubDate>Wed, 19 Jul 2023 13:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09947v1</guid></item><item><title>ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method</title><link>http://arxiv.org/abs/2307.09944v1</link><description>Capsule Networks have emerged as a powerful class of deep learningarchitectures, known for robust performance with relatively few parameterscompared to Convolutional Neural Networks (CNNs). However, their inherentefficiency is often overshadowed by their slow, iterative routing mechanismswhich establish connections between Capsule layers, posing computationalchallenges resulting in an inability to scale. In this paper, we introduce anovel, non-iterative routing mechanism, inspired by trainable prototypeclustering. This innovative approach aims to mitigate computational complexity,while retaining, if not enhancing, performance efficacy. Furthermore, weharness a shared Capsule subspace, negating the need to project eachlower-level Capsule to each higher-level Capsule, thereby significantlyreducing memory requisites during training. Our approach demonstrates superiorresults compared to the current best non-iterative Capsule Network and tests onthe Imagewoof dataset, which is too computationally demanding to handleefficiently by iterative approaches. Our findings underscore the potential ofour proposed methodology in enhancing the operational efficiency andperformance of Capsule Networks, paving the way for their application inincreasingly complex computational scenarios.</description><author>Miles Everett, Mingjun Zhong, Georgios Leontidis</author><pubDate>Wed, 19 Jul 2023 13:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09944v1</guid></item><item><title>Impatient Bandits: Optimizing for the Long-Term Without Delay</title><link>http://arxiv.org/abs/2307.09943v1</link><description>Recommender systems are a ubiquitous feature of online platforms.Increasingly, they are explicitly tasked with increasing users' long-termsatisfaction. In this context, we study a content exploration task, which weformalize as a multi-armed bandit problem with delayed rewards. We observe thatthere is an apparent trade-off in choosing the learning signal: Waiting for thefull reward to become available might take several weeks, hurting the rate atwhich learning happens, whereas measuring short-term proxy rewards reflects theactual long-term goal only imperfectly. We address this challenge in two steps.First, we develop a predictive model of delayed rewards that incorporates allinformation obtained to date. Full observations as well as partial (short ormedium-term) outcomes are combined through a Bayesian filter to obtain aprobabilistic belief. Second, we devise a bandit algorithm that takes advantageof this new predictive model. The algorithm quickly learns to identify contentaligned with long-term success by carefully balancing exploration andexploitation. We apply our approach to a podcast recommendation problem, wherewe seek to identify shows that users engage with repeatedly over two months. Weempirically validate that our approach results in substantially betterperformance compared to approaches that either optimize for short-term proxies,or wait for the long-term outcome to be fully realized.</description><author>Thomas McDonald, Lucas Maystre, Mounia Lalmas, Daniel Russo, Kamil Ciosek</author><pubDate>Wed, 19 Jul 2023 13:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09943v1</guid></item><item><title>TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network</title><link>http://arxiv.org/abs/2307.09942v1</link><description>Clinical trials are critical for drug development but often suffer fromexpensive and inefficient patient recruitment. In recent years, machinelearning models have been proposed for speeding up patient recruitment viaautomatically matching patients with clinical trials based on longitudinalpatient electronic health records (EHR) data and eligibility criteria ofclinical trials. However, they either depend on trial-specific expert rulesthat cannot expand to other trials or perform matching at a very general levelwith a black-box model where the lack of interpretability makes the modelresults difficult to be adopted. To provide accurate and interpretable patient trial matching, we introduce apersonalized dynamic tree-based memory network model named TREEMENT. Itutilizes hierarchical clinical ontologies to expand the personalized patientrepresentation learned from sequential EHR data, and then uses an attentionalbeam-search query learned from eligibility criteria embedding to offer agranular level of alignment for improved performance and interpretability. Weevaluated TREEMENT against existing models on real-world datasets anddemonstrated that TREEMENT outperforms the best baseline by 7% in terms oferror reduction in criteria-level matching and achieves state-of-the-artresults in its trial-level matching ability. Furthermore, we also show TREEMENTcan offer good interpretability to make the model results easier for adoption.</description><author>Brandon Theodorou, Cao Xiao, Jimeng Sun</author><pubDate>Wed, 19 Jul 2023 13:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09942v1</guid></item><item><title>LongNet: Scaling Transformers to 1,000,000,000 Tokens</title><link>http://arxiv.org/abs/2307.02486v2</link><description>Scaling sequence length has become a critical demand in the era of largelanguage models. However, existing methods struggle with either computationalcomplexity or model expressivity, rendering the maximum sequence lengthrestricted. To address this issue, we introduce LongNet, a Transformer variantthat can scale sequence length to more than 1 billion tokens, withoutsacrificing the performance on shorter sequences. Specifically, we proposedilated attention, which expands the attentive field exponentially as thedistance grows. LongNet has significant advantages: 1) it has a linearcomputation complexity and a logarithm dependency between any two tokens in asequence; 2) it can be served as a distributed trainer for extremely longsequences; 3) its dilated attention is a drop-in replacement for standardattention, which can be seamlessly integrated with the existingTransformer-based optimization. Experiments results demonstrate that LongNetyields strong performance on both long-sequence modeling and general languagetasks. Our work opens up new possibilities for modeling very long sequences,e.g., treating a whole corpus or even the entire Internet as a sequence.</description><author>Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei</author><pubDate>Wed, 19 Jul 2023 13:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02486v2</guid></item><item><title>AGAR: Attention Graph-RNN for Adaptative Motion Prediction of Point Clouds of Deformable Objects</title><link>http://arxiv.org/abs/2307.09936v1</link><description>This paper focuses on motion prediction for point cloud sequences in thechallenging case of deformable 3D objects, such as human body motion. First, weinvestigate the challenges caused by deformable shapes and complex motionspresent in this type of representation, with the ultimate goal of understandingthe technical limitations of state-of-the-art models. From this understanding,we propose an improved architecture for point cloud prediction of deformable 3Dobjects. Specifically, to handle deformable shapes, we propose a graph-basedapproach that learns and exploits the spatial structure of point clouds toextract more representative features. Then we propose a module able to combinethe learned features in an adaptative manner according to the point cloudmovements. The proposed adaptative module controls the composition of local andglobal motions for each point, enabling the network to model complex motions indeformable 3D objects more effectively. We tested the proposed method on thefollowing datasets: MNIST moving digits, the Mixamo human bodies motions, JPEGand CWIPC-SXR real-world dynamic bodies. Simulation results demonstrate thatour method outperforms the current baseline methods given its improved abilityto model complex movements as well as preserve point cloud shape. Furthermore,we demonstrate the generalizability of the proposed framework for dynamicfeature learning, by testing the framework for action recognition on theMSRAction3D dataset and achieving results on-par with state-of-the-art methods</description><author>Pedro Gomes, Silvia Rossi, Laura Toni</author><pubDate>Wed, 19 Jul 2023 13:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09936v1</guid></item><item><title>Planning with Dynamically Estimated Action Costs</title><link>http://arxiv.org/abs/2206.04166v3</link><description>Information about action costs is critical for real-world AI planningapplications. Rather than rely solely on declarative action models, recentapproaches also use black-box external action cost estimators, often learnedfrom data, that are applied during the planning phase. These, however, can becomputationally expensive, and produce uncertain values. In this paper wesuggest a generalization of deterministic planning with action costs thatallows selecting between multiple estimators for action cost, to balancecomputation time against bounded estimation uncertainty. This enables a muchricher -- and correspondingly more realistic -- problem representation.Importantly, it allows planners to bound plan accuracy, thereby increasingreliability, while reducing unnecessary computational burden, which is criticalfor scaling to large problems. We introduce a search algorithm, generalizing$A^*$, that solves such planning problems, and additional algorithmicextensions. In addition to theoretical guarantees, extensive experiments showconsiderable savings in runtime compared to alternatives.</description><author>Eyal Weiss, Gal A. Kaminka</author><pubDate>Wed, 19 Jul 2023 13:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.04166v3</guid></item><item><title>The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus</title><link>http://arxiv.org/abs/2302.07265v2</link><description>One of the unsolved challenges in the field of Explainable AI (XAI) isdetermining how to most reliably estimate the quality of an explanation methodin the absence of ground truth explanation labels. Resolving this issue is ofutmost importance as the evaluation outcomes generated by competing evaluationmethods (or ''quality estimators''), which aim at measuring the same propertyof an explanation method, frequently present conflicting rankings. Suchdisagreements can be challenging for practitioners to interpret, therebycomplicating their ability to select the best-performing explanation method. Weaddress this problem through a meta-evaluation of different quality estimatorsin XAI, which we define as ''the process of evaluating the evaluation method''.Our novel framework, MetaQuantus, analyses two complementary performancecharacteristics of a quality estimator: its resilience to noise and reactivityto randomness, thus circumventing the need for ground truth labels. Wedemonstrate the effectiveness of our framework through a series of experiments,targeting various open questions in XAI such as the selection andhyperparameter optimisation of quality estimators. Our work is released underan open-source license (https://github.com/annahedstroem/MetaQuantus) to serveas a development tool for XAI- and Machine Learning (ML) practitioners toverify and benchmark newly constructed quality estimators in a givenexplainability context. With this work, we provide the community with clear andtheoretically-grounded guidance for identifying reliable evaluation methods,thus facilitating reproducibility in the field.</description><author>Anna Hedstr√∂m, Philine Bommer, Kristoffer K. Wickstr√∏m, Wojciech Samek, Sebastian Lapuschkin, Marina M. -C. H√∂hne</author><pubDate>Wed, 19 Jul 2023 13:18:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07265v2</guid></item></channel></rss>