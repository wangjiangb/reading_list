<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 14 Jul 2023 13:58:31 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models</title><link>http://arxiv.org/abs/2307.06949v1</link><description>Personalization has emerged as a prominent aspect within the field ofgenerative AI, enabling the synthesis of individuals in diverse contexts andstyles, while retaining high-fidelity to their identities. However, the processof personalization presents inherent challenges in terms of time and memoryrequirements. Fine-tuning each personalized model needs considerable GPU timeinvestment, and storing a personalized model per subject can be demanding interms of storage capacity. To overcome these challenges, we proposeHyperDreamBooth-a hypernetwork capable of efficiently generating a small set ofpersonalized weights from a single image of a person. By composing theseweights into the diffusion model, coupled with fast finetuning, HyperDreamBoothcan generate a person's face in various contexts and styles, with high subjectdetails while also preserving the model's crucial knowledge of diverse stylesand semantic modifications. Our method achieves personalization on faces inroughly 20 seconds, 25x faster than DreamBooth and 125x faster than TextualInversion, using as few as one reference image, with the same quality and stylediversity as DreamBooth. Also our method yields a model that is 10000x smallerthan a normal DreamBooth model. Project page: https://hyperdreambooth.github.io</description><author>Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, Kfir Aberman</author><pubDate>Thu, 13 Jul 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06949v1</guid></item><item><title>Self-regulating Prompts: Foundational Model Adaptation without Forgetting</title><link>http://arxiv.org/abs/2307.06948v1</link><description>Prompt learning has emerged as an efficient alternative for fine-tuningfoundational models, such as CLIP, for various downstream tasks. Conventionallytrained using the task-specific objective, i.e., cross-entropy loss, promptstend to overfit downstream data distributions and find it challenging tocapture task-agnostic general features from the frozen CLIP. This leads to theloss of the model's original generalization capability. To address this issue,our work introduces a self-regularization framework for prompting calledPromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides theprompts to optimize for both task-specific and task-agnostic generalrepresentations using a three-pronged approach by: (a) regulating {prompted}representations via mutual agreement maximization with the frozen model, (b)regulating with self-ensemble of prompts over the training trajectory to encodetheir complementary strengths, and (c) regulating with textual diversity tomitigate sample diversity imbalance with the visual branch. To the best of ourknowledge, this is the first regularization framework for prompt learning thatavoids overfitting by jointly attending to pre-trained model features, thetraining trajectory during prompting, and the textual diversity. PromptSRCexplicitly steers the prompts to learn a representation space that maximizesperformance on downstream tasks without compromising CLIP generalization. Weperform extensive experiments on 4 benchmarks where PromptSRC overall performsfavorably well compared to the existing methods. Our code and pre-trainedmodels are publicly available at: https://github.com/muzairkhattak/PromptSRC.</description><author>Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan</author><pubDate>Thu, 13 Jul 2023 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06948v1</guid></item><item><title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</title><link>http://arxiv.org/abs/2307.06947v1</link><description>Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on three large-scale datasets(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Ourcode/models are released at https://github.com/TalalWasim/Video-FocalNets.</description><author>Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</author><pubDate>Thu, 13 Jul 2023 18:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06947v1</guid></item><item><title>In-context Autoencoder for Context Compression in a Large Language Model</title><link>http://arxiv.org/abs/2307.06945v1</link><description>We propose the In-context Autoencoder (ICAE) for context compression in alarge language model (LLM). The ICAE has two modules: a learnable encoderadapted with LoRA from an LLM for compressing a long context into a limitednumber of memory slots, and a fixed decoder which is the target LLM that cancondition on the memory slots for various purposes. We first pretrain the ICAEusing both autoencoding and language modeling objectives on massive text data,enabling it to generate memory slots that accurately and comprehensivelyrepresent the original context. Then, we fine-tune the pretrained ICAE on asmall amount of instruct data to enhance its interaction with various promptsfor producing desirable responses. Our experimental results demonstrate thatthe ICAE learned with our proposed pretraining and fine-tuning paradigm caneffectively produce memory slots with $4\times$ context compression, which canbe well conditioned on by the target LLM to respond to various prompts. Thepromising results demonstrate significant implications of the ICAE for itsnovel approach to the long context problem and its potential to reducecomputation and memory overheads for LLM inference in practice, suggestingfurther research effort in context management for an LLM. Our code and datawill be released shortly.</description><author>Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu Wei</author><pubDate>Thu, 13 Jul 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06945v1</guid></item><item><title>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</title><link>http://arxiv.org/abs/2307.06942v1</link><description>This paper introduces InternVid, a large-scale video-centric multimodaldataset that enables learning powerful and transferable video-textrepresentations for multimodal understanding and generation. The InternViddataset contains over 7 million videos lasting nearly 760K hours, yielding 234Mvideo clips accompanied by detailed descriptions of total 4.1B words. Our corecontribution is to develop a scalable approach to autonomously build ahigh-quality video-text dataset with large language models (LLM), therebyshowcasing its efficacy in learning video-language representation at scale.Specifically, we utilize a multi-scale approach to generate video-relateddescriptions. Furthermore, we introduce ViCLIP, a video-text representationlearning model based on ViT-L. Learned on InternVid via contrastive learning,this model demonstrates leading zero-shot action recognition and competitivevideo retrieval performance. Beyond basic video understanding tasks likerecognition and retrieval, our dataset and model have broad applications. Theyare particularly beneficial for generating interleaved video-text data forlearning a video-centric dialogue system, advancing video-to-text andtext-to-video generation research. These proposed resources provide a tool forresearchers and practitioners interested in multimodal video understanding andgeneration.</description><author>Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao</author><pubDate>Thu, 13 Jul 2023 18:58:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06942v1</guid></item><item><title>On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations</title><link>http://arxiv.org/abs/2307.06941v1</link><description>Explainable Artificial Intelligence (XAI) has received widespread interest inrecent years, and two of the most popular types of explanations are featureattributions, and counterfactual explanations. These classes of approaches havebeen largely studied independently and the few attempts at reconciling themhave been primarily empirical. This work establishes a clear theoreticalconnection between game-theoretic feature attributions, focusing on but notlimited to SHAP, and counterfactuals explanations. After motivating operativechanges to Shapley values based feature attributions and counterfactualexplanations, we prove that, under conditions, they are in fact equivalent. Wethen extend the equivalency result to game-theoretic solution concepts beyondShapley values. Moreover, through the analysis of the conditions of suchequivalence, we shed light on the limitations of naively using counterfactualexplanations to provide feature importances. Experiments on three datasetsquantitatively show the difference in explanations at every stage of theconnection between the two approaches and corroborate the theoretical findings.</description><author>Emanuele Albini, Shubham Sharma, Saumitra Mishra, Danial Dervovic, Daniele Magazzeni</author><pubDate>Thu, 13 Jul 2023 18:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06941v1</guid></item><item><title>Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation</title><link>http://arxiv.org/abs/2307.06940v1</link><description>Generating videos for visual storytelling can be a tedious and complexprocess that typically requires either live-action filming or graphicsanimation rendering. To bypass these challenges, our key idea is to utilize theabundance of existing video clips and synthesize a coherent storytelling videoby customizing their appearances. We achieve this by developing a frameworkcomprised of two functional modules: (i) Motion Structure Retrieval, whichprovides video candidates with desired scene or motion context described byquery texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generatesplot-aligned videos under the guidance of motion structure and text prompts.For the first module, we leverage an off-the-shelf video retrieval system andextract video depths as motion structure. For the second module, we propose acontrollable video generation model that offers flexible controls overstructure and characters. The videos are synthesized by following thestructural guidance and appearance instruction. To ensure visual consistencyacross clips, we propose an effective concept personalization approach, whichallows the specification of the desired character identities through textprompts. Extensive experiments demonstrate that our approach exhibitssignificant advantages over various existing baselines.</description><author>Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, Qifeng Chen</author><pubDate>Thu, 13 Jul 2023 18:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06940v1</guid></item><item><title>Declarative Mechanism Design</title><link>http://arxiv.org/abs/1912.13122v4</link><description>Regulation of Multi-Agent Systems (MAS) and Declarative ElectronicInstitutions (DEIs) was a multidisciplinary research topic of the past decadeinvolving (Physical and Software) Agents and Law since the beginning, butrecently evolved towards News-claimed Robot Lawyer since 2016. One of thesefirst proposals of restricting the behaviour of Software Agentswas ElectronicInstitutions.However, with the recent reformulation of Artificial NeuralNetworks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legalissues regarding the use of DL has raised concerns in the ArtificialIntelligence (AI) Community. Now that the Regulation of MAS is almost correctlyaddressed, we propose the Regulation of Artificial Neural Networks asAgent-based Training of a special type of regulated Artificial Neural Networkthat we call Institutional Neural Network (INN).The main purpose of this paperis to bring attention to Artificial Teaching (AT) and to give a tentativeanswer showing a proof-of-concept implementation of Regulated Deep Learning(RDL). This paper introduces the former concept and provide sI, a languagepreviously used to model declaratively and extend Electronic Institutions, as ameans to regulate the execution of Artificial Neural Networks and theirinteractions with Artificial Teachers (ATs)</description><author>Andrés García-Camino</author><pubDate>Thu, 13 Jul 2023 18:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1912.13122v4</guid></item><item><title>mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs</title><link>http://arxiv.org/abs/2307.06930v1</link><description>Modular vision-language models (Vision-LLMs) align pretrained image encoderswith (pretrained) large language models (LLMs), representing a computationallymuch more efficient alternative to end-to-end training of large vision-languagemodels from scratch, which is prohibitively expensive for most. Vision-LLMsinstead post-hoc condition LLMs to `understand' the output of an image encoder.With the abundance of readily available high-quality English image-text data aswell as monolingual English LLMs, the research focus has been on English-onlyVision-LLMs. Multilingual vision-language models are still predominantlyobtained via expensive end-to-end pretraining, resulting in comparativelysmaller models, trained on limited multilingual image data supplemented withtext-only multilingual corpora. In this work, we present mBLIP, the firstmultilingual Vision-LLM, which we obtain in a computationally efficient manner-- on consumer hardware using only a few million training examples -- byleveraging a pretrained multilingual LLM. To this end, we \textit{re-align} animage encoder previously tuned to an English LLM to a new, multilingual LLM --for this, we leverage multilingual data from a mix of vision-and-languagetasks, which we obtain by machine-translating high-quality English data to 95languages. On the IGLUE benchmark, mBLIP yields results competitive withstate-of-the-art models. Moreover, in image captioning on XM3600, mBLIP(zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared tothese very large multilingual vision-language models trained from scratch, weobtain mBLIP by training orders of magnitude fewer parameters on magnitudesless data. We release our model and code at\url{https://github.com/gregor-ge/mBLIP}.</description><author>Gregor Geigle, Abhay Jain, Radu Timofte, Goran Glavaš</author><pubDate>Thu, 13 Jul 2023 18:51:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06930v1</guid></item><item><title>Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models</title><link>http://arxiv.org/abs/2307.06925v1</link><description>Text-to-image (T2I) personalization allows users to guide the creative imagegeneration process by combining their own visual concepts in natural languageprompts. Recently, encoder-based techniques have emerged as a new effectiveapproach for T2I personalization, reducing the need for multiple images andlong training times. However, most existing encoders are limited to asingle-class domain, which hinders their ability to handle diverse concepts. Inthis work, we propose a domain-agnostic method that does not require anyspecialized dataset or prior information about the personalized concepts. Weintroduce a novel contrastive-based regularization technique to maintain highfidelity to the target concept characteristics while keeping the predictedembeddings close to editable regions of the latent space, by pushing thepredicted tokens toward their nearest existing CLIP tokens. Our experimentalresults demonstrate the effectiveness of our approach and show how the learnedtokens are more semantic than tokens predicted by unregularized models. Thisleads to a better representation that achieves state-of-the-art performancewhile being more flexible than previous methods.</description><author>Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, Amit H. Bermano</author><pubDate>Thu, 13 Jul 2023 18:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06925v1</guid></item><item><title>DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding</title><link>http://arxiv.org/abs/2307.06924v1</link><description>Persons with visual impairments (PwVI) have difficulties understanding andnavigating spaces around them. Current wayfinding technologies either focussolely on navigation or provide limited communication about the environment.Motivated by recent advances in visual-language grounding and semanticnavigation, we propose DRAGON, a guiding robot powered by a dialogue system andthe ability to associate the environment with natural language. Byunderstanding the commands from the user, DRAGON is able to guide the user tothe desired landmarks on the map, describe the environment, and answerquestions from visual observations. Through effective utilization of dialogue,the robot can ground the user's free-form descriptions to landmarks in theenvironment, and give the user semantic information through spoken language. Weconduct a user study with blindfolded participants in an everyday indoorenvironment. Our results demonstrate that DRAGON is able to communicate withthe user smoothly, provide a good guiding experience, and connect users withtheir surrounding environment in an intuitive manner.</description><author>Shuijing Liu, Aamir Hasan, Kaiwen Hong, Runxuan Wang, Peixin Chang, Zachary Mizrachi, Justin Lin, D. Livingston McPherson, Wendy A. Rogers, Katherine Driggs-Campbell</author><pubDate>Thu, 13 Jul 2023 18:46:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06924v1</guid></item><item><title>LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT</title><link>http://arxiv.org/abs/2307.06917v1</link><description>Knowledge Graphs (KG) provide us with a structured, flexible, transparent,cross-system, and collaborative way of organizing our knowledge and data acrossvarious domains in society and industrial as well as scientific disciplines.KGs surpass any other form of representation in terms of effectiveness.However, Knowledge Graph Engineering (KGE) requires in-depth experiences ofgraph structures, web technologies, existing models and vocabularies, rulesets, logic, as well as best practices. It also demands a significant amount ofwork. Considering the advancements in large language models (LLMs) and theirinterfaces and applications in recent years, we have conducted comprehensiveexperiments with ChatGPT to explore its potential in supporting KGE. In thispaper, we present a selection of these experiments and their results todemonstrate how ChatGPT can assist us in the development and management of KGs.</description><author>Lars-Peter Meyer, Claus Stadler, Johannes Frey, Norman Radtke, Kurt Junghanns, Roy Meissner, Gordian Dziwis, Kirill Bulert, Michael Martin</author><pubDate>Thu, 13 Jul 2023 18:31:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06917v1</guid></item><item><title>Large Language Models for Supply Chain Optimization</title><link>http://arxiv.org/abs/2307.03875v2</link><description>Supply chain operations traditionally involve a variety of complex decisionmaking problems. Over the last few decades, supply chains greatly benefitedfrom advances in computation, which allowed the transition from manualprocessing to automation and cost-effective optimization. Nonetheless, businessoperators still need to spend substantial efforts in explaining andinterpreting the optimization outcomes to stakeholders. Motivated by the recentadvances in Large Language Models (LLMs), we study how this disruptivetechnology can help bridge the gap between supply chain automation and humancomprehension and trust thereof. We design OptiGuide -- a framework thataccepts as input queries in plain text, and outputs insights about theunderlying optimization outcomes. Our framework does not forgo thestate-of-the-art combinatorial optimization technology, but rather leverages itto quantitatively answer what-if scenarios (e.g., how would the cost change ifwe used supplier B instead of supplier A for a given demand?). Importantly, ourdesign does not require sending proprietary data over to LLMs, which can be aprivacy concern in some circumstances. We demonstrate the effectiveness of ourframework on a real server placement scenario within Microsoft's cloud supplychain. Along the way, we develop a general evaluation benchmark, which can beused to evaluate the accuracy of the LLM output in other scenarios.</description><author>Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, Ishai Menache</author><pubDate>Thu, 13 Jul 2023 18:29:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03875v2</guid></item><item><title>Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality</title><link>http://arxiv.org/abs/2307.06915v1</link><description>Stochastic Gradient Descent (SGD) is one of the simplest and most popularalgorithms in modern statistical and machine learning due to its computationaland memory efficiency. Various averaging schemes have been proposed toaccelerate the convergence of SGD in different settings. In this paper, weexplore a general averaging scheme for SGD. Specifically, we establish theasymptotic normality of a broad range of weighted averaged SGD solutions andprovide asymptotically valid online inference approaches. Furthermore, wepropose an adaptive averaging scheme that exhibits both optimal statisticalrate and favorable non-asymptotic convergence, drawing insights from theoptimal weight for the linear model in terms of non-asymptotic mean squarederror (MSE).</description><author>Ziyang Wei, Wanrong Zhu, Wei Biao Wu</author><pubDate>Thu, 13 Jul 2023 18:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06915v1</guid></item><item><title>Uncovering Unique Concept Vectors through Latent Space Decomposition</title><link>http://arxiv.org/abs/2307.06913v1</link><description>Interpreting the inner workings of deep learning models is crucial forestablishing trust and ensuring model safety. Concept-based explanations haveemerged as a superior approach that is more interpretable than featureattribution estimates such as pixel saliency. However, defining the conceptsfor the interpretability analysis biases the explanations by the user'sexpectations on the concepts. To address this, we propose a novel post-hocunsupervised method that automatically uncovers the concepts learned by deepmodels during training. By decomposing the latent space of a layer in singularvectors and refining them by unsupervised clustering, we uncover conceptvectors aligned with directions of high variance that are relevant to the modelprediction, and that point to semantically distinct concepts. Our extensiveexperiments reveal that the majority of our concepts are readily understandableto humans, exhibit coherency, and bear relevance to the task at hand. Moreover,we showcase the practical utility of our method in dataset exploration, whereour concept vectors successfully identify outlier training samples affected byvarious confounding factors. This novel exploration technique has remarkableversatility to data types and model architectures and it will facilitate theidentification of biases and the discovery of sources of error within trainingdata.</description><author>Mara Graziani, Laura O' Mahony, An-Phi Nguyen, Henning Müller, Vincent Andrearczyk</author><pubDate>Thu, 13 Jul 2023 18:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06913v1</guid></item><item><title>Generating Benchmarks for Factuality Evaluation of Language Models</title><link>http://arxiv.org/abs/2307.06908v1</link><description>Before deploying a language model (LM) within a given domain, it is importantto measure its tendency to generate factually incorrect information in thatdomain. Existing factual generation evaluation methods focus on facts sampledfrom the LM itself, and thus do not control the set of evaluated facts andmight under-represent rare and unlikely facts. We propose FACTOR: FactualAssessment via Corpus TransfORmation, a scalable approach for evaluating LMfactuality. FACTOR automatically transforms a factual corpus of interest into abenchmark evaluating an LM's propensity to generate true facts from the corpusvs. similar but incorrect statements. We use our framework to create twobenchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scoresincrease with model size and improve when the LM is augmented with retrieval;(ii) benchmark score correlates with perplexity, but the two metrics do notalways agree on model ranking; and (iii) when perplexity and benchmark scoredisagree, the latter better reflects factuality in open-ended generation, asmeasured by human annotators. We make our data and code publicly available inhttps://github.com/AI21Labs/factor.</description><author>Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, Yoav Shoham</author><pubDate>Thu, 13 Jul 2023 18:14:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06908v1</guid></item><item><title>Autonomous Navigation of Underactuated Bipedal Robots in Height-Constrained Environments</title><link>http://arxiv.org/abs/2109.05714v4</link><description>Navigating a large-scaled robot in unknown and cluttered height-constrainedenvironments is challenging. Not only is a fast and reliable planning algorithmrequired to go around obstacles, the robot should also be able to change itsintrinsic dimension by crouching in order to travel underneathheight-constrained regions. There are few mobile robots that are capable ofhandling such a challenge, and bipedal robots provide a solution. However, asbipedal robots have nonlinear and hybrid dynamics, trajectory planning whileensuring dynamic feasibility and safety on these robots is challenging. Thispaper presents an end-to-end autonomous navigation framework which leveragesthree layers of planners and a variable walking height controller to enablebipedal robots to safely explore height-constrained environments. Avertically-actuated Spring-Loaded Inverted Pendulum (vSLIP) model is introducedto capture the robot's coupled dynamics of planar walking and vertical walkingheight. This reduced-order model is utilized to optimize for long-term andshort-term safe trajectory plans. A variable walking height controller isleveraged to enable the bipedal robot to maintain stable periodic walking gaitswhile following the planned trajectory. The entire framework is tested andexperimentally validated using a bipedal robot Cassie. This demonstratesreliable autonomy to drive the robot to safely avoid obstacles while walking tothe goal location in various kinds of height-constrained clutteredenvironments.</description><author>Zhongyu Li, Jun Zeng, Shuxiao Chen, Koushil Sreenath</author><pubDate>Thu, 13 Jul 2023 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.05714v4</guid></item><item><title>Applying a Color Palette with Local Control using Diffusion Models</title><link>http://arxiv.org/abs/2307.02698v2</link><description>We demonstrate two novel editing procedures in the context of fantasy cardart. Palette transfer applies a specified reference palette to a given card.For fantasy art, the desired change in palette can be very large, leading tohuge changes in the "look" of the art. We demonstrate that a pipeline of vectorquantization; matching; and "vector dequantization" (using a diffusion model)produces successful extreme palette transfers. Segment control allows an artistto move one or more image segments, and to optionally specify the desired colorof the result. The combination of these two types of edit yields valuableworkflows, including: move a segment, then recolor; recolor, then force somesegments to take a prescribed color. We demonstrate our methods on thechallenging Yu-Gi-Oh card art dataset.</description><author>Vaibhav Vavilala, David Forsyth</author><pubDate>Thu, 13 Jul 2023 17:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02698v2</guid></item><item><title>Words are not Wind -- How Joint Commitment and Reputation Solve Social Dilemmas, without Repeated Interactions or Enforcement by Third Parties</title><link>http://arxiv.org/abs/2307.06898v1</link><description>Joint commitment was argued to "make our social world" (Gilbert, 2014) and toseparate us from other primates. 'Joint' entails that neither of us promisesanything, unless the other promises as well. When we need to coordinate for thebest mutual outcome, any commitment is beneficial. However, when we are temptedto free-ride (i.e. in social dilemmas), commitment serves no obvious purpose.We show that a reputation system, which judges action in social dilemmas onlyafter joint commitment, can prevent free-riding. Keeping commitments buildstrust. We can selectively enter joint commitments with trustworthy individualsto ensure their cooperation (since they will now be judged). We simply do notcommit to cooperate with those we do not trust, and hence can freely defectwithout losing the trust of others. This principle might be the reason forpointedly public joint commitments, such as marriage. It is especially relevantto our evolutionary past, in which no mechanisms existed to enforce commitmentsreliably and impartially (e.g. via a powerful and accountable government). Muchresearch from anthropology, philosophy and psychology made the assumption thatpast collaborations were mutually beneficial and had little possibilities tofree-ride, for which there is little support. Our evolutionary game theoryapproach proves that this assumption is not necessary, because free-ridingcould have been dealt with joint commitments and reputation.</description><author>Marcus Krellner, The Anh Han</author><pubDate>Thu, 13 Jul 2023 17:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06898v1</guid></item><item><title>Blocks2World: Controlling Realistic Scenes with Editable Primitives</title><link>http://arxiv.org/abs/2307.03847v2</link><description>We present Blocks2World, a novel method for 3D scene rendering and editingthat leverages a two-step process: convex decomposition of images andconditioned synthesis. Our technique begins by extracting 3D parallelepipedsfrom various objects in a given scene using convex decomposition, thusobtaining a primitive representation of the scene. These primitives are thenutilized to generate paired data through simple ray-traced depth maps. The nextstage involves training a conditioned model that learns to generate images fromthe 2D-rendered convex primitives. This step establishes a direct mappingbetween the 3D model and its 2D representation, effectively learning thetransition from a 3D model to an image. Once the model is fully trained, itoffers remarkable control over the synthesis of novel and edited scenes. Thisis achieved by manipulating the primitives at test time, including translatingor adding them, thereby enabling a highly customizable scene rendering process.Our method provides a fresh perspective on 3D scene rendering and editing,offering control and flexibility. It opens up new avenues for research andapplications in the field, including authoring and data augmentation.</description><author>Vaibhav Vavilala, Seemandhar Jain, Rahul Vasanth, Anand Bhattad, David Forsyth</author><pubDate>Thu, 13 Jul 2023 17:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03847v2</guid></item><item><title>Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks</title><link>http://arxiv.org/abs/2307.06887v1</link><description>Feature learning, i.e. extracting meaningful representations of data, isquintessential to the practical success of neural networks trained withgradient descent, yet it is notoriously difficult to explain how and why itoccurs. Recent theoretical studies have shown that shallow neural networksoptimized on a single task with gradient-based methods can learn meaningfulfeatures, extending our understanding beyond the neural tangent kernel orrandom feature regime in which negligible feature learning occurs. But inpractice, neural networks are increasingly often trained on {\em many} taskssimultaneously with differing loss functions, and these prior analyses do notgeneralize to such settings. In the multi-task learning setting, a variety ofstudies have shown effective feature learning by simple linear models. However,multi-task learning via {\em nonlinear} models, arguably the most commonlearning paradigm in practice, remains largely mysterious. In this work, wepresent the first results proving feature learning occurs in a multi-tasksetting with a nonlinear model. We show that when the tasks are binaryclassification problems with labels depending on only $r$ directions within theambient $d\gg r$-dimensional input space, executing a simple gradient-basedmultitask learning algorithm on a two-layer ReLU neural network learns theground-truth $r$ directions. In particular, any downstream task on the $r$ground-truth coordinates can be solved by learning a linear classifier withsample and neuron complexity independent of the ambient dimension $d$, while arandom feature model requires exponential complexity in $d$ for such aguarantee.</description><author>Liam Collins, Hamed Hassani, Mahdi Soltanolkotabi, Aryan Mokhtari, Sanjay Shakkottai</author><pubDate>Thu, 13 Jul 2023 17:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06887v1</guid></item><item><title>Min-Max Optimization under Delays</title><link>http://arxiv.org/abs/2307.06886v1</link><description>Delays and asynchrony are inevitable in large-scale machine-learning problemswhere communication plays a key role. As such, several works have extensivelyanalyzed stochastic optimization with delayed gradients. However, as far as weare aware, no analogous theory is available for min-max optimization, a topicthat has gained recent popularity due to applications in adversarialrobustness, game theory, and reinforcement learning. Motivated by this gap, weexamine the performance of standard min-max optimization algorithms withdelayed gradient updates. First, we show (empirically) that even small delayscan cause prominent algorithms like Extra-gradient (\texttt{EG}) to diverge onsimple instances for which \texttt{EG} guarantees convergence in the absence ofdelays. Our empirical study thus suggests the need for a careful analysis ofdelayed versions of min-max optimization algorithms. Accordingly, undersuitable technical assumptions, we prove that Gradient Descent-Ascent(\texttt{GDA}) and \texttt{EG} with delayed updates continue to guaranteeconvergence to saddle points for convex-concave and strongly convex-stronglyconcave settings. Our complexity bounds reveal, in a transparent manner, theslow-down in convergence caused by delays.</description><author>Arman Adibi, Aritra Mitra, Hamed Hassani</author><pubDate>Thu, 13 Jul 2023 17:39:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06886v1</guid></item><item><title>The complexity of non-stationary reinforcement learning</title><link>http://arxiv.org/abs/2307.06877v1</link><description>The problem of continual learning in the domain of reinforcement learning,often called non-stationary reinforcement learning, has been identified as animportant challenge to the application of reinforcement learning. We prove aworst-case complexity result, which we believe captures this challenge:Modifying the probabilities or the reward of a single state-action pair in areinforcement learning problem requires an amount of time almost as large asthe number of states in order to keep the value function up to date, unless thestrong exponential time hypothesis (SETH) is false; SETH is a widely acceptedstrengthening of the P $\neq$ NP conjecture. Recall that the number of statesin current applications of reinforcement learning is typically astronomical. Incontrast, we show that just $\textit{adding}$ a new state-action pair isconsiderably easier to implement.</description><author>Christos Papadimitriou, Binghui Peng</author><pubDate>Thu, 13 Jul 2023 17:25:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06877v1</guid></item><item><title>Identifying Early Help Referrals For Local Authorities With Machine Learning And Bias Analysis</title><link>http://arxiv.org/abs/2307.06871v1</link><description>Local authorities in England, such as Leicestershire County Council (LCC),provide Early Help services that can be offered at any point in a youngperson's life when they experience difficulties that cannot be supported byuniversal services alone, such as schools. This paper investigates theutilisation of machine learning (ML) to assist experts in identifying familiesthat may need to be referred for Early Help assessment and support. LCCprovided an anonymised dataset comprising 14360 records of young people underthe age of 18. The dataset was pre-processed, machine learning models werebuild, and experiments were conducted to validate and test the performance ofthe models. Bias mitigation techniques were applied to improve the fairness ofthese models. During testing, while the models demonstrated the capability toidentify young people requiring intervention or early help, they also produceda significant number of false positives, especially when constructed withimbalanced data, incorrectly identifying individuals who most likely did notneed an Early Help referral. This paper empirically explores the suitability ofdata-driven ML models for identifying young people who may require Early Helpservices and discusses their appropriateness and limitations for this task.</description><author>Eufrásio de A. Lima Neto, Jonathan Bailiss, Axel Finke, Jo Miller, Georgina Cosma</author><pubDate>Thu, 13 Jul 2023 17:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06871v1</guid></item><item><title>Embodied Lifelong Learning for Task and Motion Planning</title><link>http://arxiv.org/abs/2307.06870v1</link><description>A robot deployed in a home over long stretches of time faces a true lifelonglearning problem. As it seeks to provide assistance to its users, the robotshould leverage any accumulated experience to improve its own knowledge tobecome a more proficient assistant. We formalize this setting with a novellifelong learning problem formulation in the context of learning for task andmotion planning (TAMP). Exploiting the modularity of TAMP systems, we develop agenerative mixture model that produces candidate continuous parameters for aplanner. Whereas most existing lifelong learning approaches determine a priorihow data is shared across task models, our approach learns shared andnon-shared models and determines which to use online during planning based onauxiliary tasks that serve as a proxy for each model's understanding of astate. Our method exhibits substantial improvements in planning success onsimulated 2D domains and on several problems from the BEHAVIOR benchmark.</description><author>Jorge A. Mendez, Leslie Pack Kaelbling, Tomás Lozano-Pérez</author><pubDate>Thu, 13 Jul 2023 17:18:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06870v1</guid></item><item><title>DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering</title><link>http://arxiv.org/abs/2307.06869v1</link><description>Existing evaluation metrics for natural language generation (NLG) tasks facethe challenges on generalization ability and interpretability. Specifically,most of the well-performed metrics are required to train on evaluation datasetsof specific NLG tasks and evaluation dimensions, which may cause over-fittingto task-specific datasets. Furthermore, existing metrics only provide anevaluation score for each dimension without revealing the evidence to interprethow this score is obtained. To deal with these challenges, we propose a simpleyet effective metric called DecompEval. This metric formulates NLG evaluationas an instruction-style question answering task and utilizes instruction-tunedpre-trained language models (PLMs) without training on evaluation datasets,aiming to enhance the generalization ability. To make the evaluation processmore interpretable, we decompose our devised instruction-style question aboutthe quality of generated texts into the subquestions that measure the qualityof each sentence. The subquestions with their answers generated by PLMs arethen recomposed as evidence to obtain the evaluation result. Experimentalresults show that DecompEval achieves state-of-the-art performance in untrainedmetrics for evaluating text summarization and dialogue generation, which alsoexhibits strong dimension-level / task-level generalization ability andinterpretability.</description><author>Pei Ke, Fei Huang, Fei Mi, Yasheng Wang, Qun Liu, Xiaoyan Zhu, Minlie Huang</author><pubDate>Thu, 13 Jul 2023 17:16:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06869v1</guid></item><item><title>Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success</title><link>http://arxiv.org/abs/2307.06865v1</link><description>The generations of large language models are commonly controlled throughprompting techniques, where a user's query to the model is prefixed with aprompt that aims to guide the model's behaviour on the query. The prompts usedby companies to guide their models are often treated as secrets, to be hiddenfrom the user making the query. They have even been treated as commodities tobe bought and sold. However, there has been anecdotal evidence showing that theprompts can be extracted by a user even when they are kept secret. In thispaper, we present a framework for systematically measuring the success ofprompt extraction attacks. In experiments with multiple sources of prompts andmultiple underlying language models, we find that simple text-based attacks canin fact reveal prompts with high probability.</description><author>Yiming Zhang, Daphne Ippolito</author><pubDate>Thu, 13 Jul 2023 17:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06865v1</guid></item><item><title>A kernel Stein test of goodness of fit for sequential models</title><link>http://arxiv.org/abs/2210.10741v3</link><description>We propose a goodness-of-fit measure for probability densities modelingobservations with varying dimensionality, such as text documents of differinglengths or variable-length sequences. The proposed measure is an instance ofthe kernel Stein discrepancy (KSD), which has been used to constructgoodness-of-fit tests for unnormalized densities. The KSD is defined by itsStein operator: current operators used in testing apply to fixed-dimensionalspaces. As our main contribution, we extend the KSD to the variable-dimensionsetting by identifying appropriate Stein operators, and propose a novel KSDgoodness-of-fit test. As with the previous variants, the proposed KSD does notrequire the density to be normalized, allowing the evaluation of a large classof models. Our test is shown to perform well in practice on discrete sequentialdata benchmarks.</description><author>Jerome Baum, Heishiro Kanagawa, Arthur Gretton</author><pubDate>Thu, 13 Jul 2023 17:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10741v3</guid></item><item><title>LVLane: Deep Learning for Lane Detection and Classification in Challenging Conditions</title><link>http://arxiv.org/abs/2307.06853v1</link><description>Lane detection plays a pivotal role in the field of autonomous vehicles andadvanced driving assistant systems (ADAS). Over the years, numerous algorithmshave emerged, spanning from rudimentary image processing techniques tosophisticated deep neural networks. The performance of deep learning-basedmodels is highly dependent on the quality of their training data. Consequently,these models often experience a decline in performance when confronted withchallenging scenarios such as extreme lighting conditions, partially visiblelane markings, and sparse lane markings like Botts' dots. To address this, wepresent an end-to-end lane detection and classification system based on deeplearning methodologies. In our study, we introduce a unique datasetmeticulously curated to encompass scenarios that pose significant challengesfor state-of-the-art (SOTA) models. Through fine-tuning selected models, we aimto achieve enhanced localization accuracy. Moreover, we propose a CNN-basedclassification branch, seamlessly integrated with the detector, facilitatingthe identification of distinct lane types. This architecture enables informedlane-changing decisions and empowers more resilient ADAS capabilities. We alsoinvestigate the effect of using mixed precision training and testing ondifferent models and batch sizes. Experimental evaluations conducted on thewidely-used TuSimple dataset, Caltech lane dataset, and our LVLane datasetdemonstrate the effectiveness of our model in accurately detecting andclassifying lanes amidst challenging scenarios. Our method achievesstate-of-the-art classification results on the TuSimple dataset. The code ofthe work will be published upon the acceptance of the paper.</description><author>Zillur Rahman, Brendan Tran Morris</author><pubDate>Thu, 13 Jul 2023 17:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06853v1</guid></item><item><title>Self-Supervised Learning for Interactive Perception of Surgical Thread for Autonomous Suture Tail-Shortening</title><link>http://arxiv.org/abs/2307.06845v1</link><description>Accurate 3D sensing of suturing thread is a challenging problem in automatedsurgical suturing because of the high state-space complexity, thinness anddeformability of the thread, and possibility of occlusion by the grippers andtissue. In this work we present a method for tracking surgical thread in 3Dwhich is robust to occlusions and complex thread configurations, and apply itto autonomously perform the surgical suture "tail-shortening" task: pullingthread through tissue until a desired "tail" length remains exposed. The methodutilizes a learned 2D surgical thread detection network to segment suturingthread in RGB images. It then identifies the thread path in 2D and reconstructsthe thread in 3D as a NURBS spline by triangulating the detections from twostereo cameras. Once a 3D thread model is initialized, the method tracks thethread across subsequent frames. Experiments suggest the method achieves a 1.33pixel average reprojection error on challenging single-frame 3D threadreconstructions, and an 0.84 pixel average reprojection error on two trackingsequences. On the tail-shortening task, it accomplishes a 90% success rateacross 20 trials. Supplemental materials are available athttps://sites.google.com/berkeley.edu/autolab-surgical-thread/ .</description><author>Vincent Schorp, Will Panitch, Kaushik Shivakumar, Vainavi Viswanath, Justin Kerr, Yahav Avigal, Danyal M Fer, Lionel Ott, Ken Goldberg</author><pubDate>Thu, 13 Jul 2023 17:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06845v1</guid></item><item><title>Human Biophysics as Network Weights: Conditional Generative Models for Dynamic Simulation</title><link>http://arxiv.org/abs/2211.01856v3</link><description>Simulations of biophysical systems are fundamental for studying physiologicalmechanisms and developing human machine interfaces. Whilst advanced numericalmethods, such as finite element models, can excel in this task, they areextremely computationally expensive to use when generating a large number ofsimulations or simulating dynamic events with continuously changing structuralparameters. We propose an architecture that uses a conditional generative modelto interpolate between the numerical model states, dramatically lowering themodeling time while maintaining a high generation accuracy. As a demonstrationof this concept, we present BioMime, a hybrid-structured generative model thatenables an accurate, ultra-fast, and arbitrarily high temporal-resolutionsimulation of a specific biophysical system during dynamic changes. Thismethodology has wide applications in physiological and clinical research aswell as in supporting data augmentation strategies for signal analysis,representing a computationally efficient and highly accurate model forbiophysical simulations.</description><author>Shihan Ma, Alexander Kenneth Clarke, Kostiantyn Maksymenko, Samuel Deslauriers-Gauthier, Xinjun Sheng, Xiangyang Zhu, Dario Farina</author><pubDate>Thu, 13 Jul 2023 17:07:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01856v3</guid></item><item><title>Adapting to Mixing Time in Stochastic Optimization with Markovian Data</title><link>http://arxiv.org/abs/2202.04428v3</link><description>We consider stochastic optimization problems where data is drawn from aMarkov chain. Existing methods for this setting crucially rely on knowing themixing time of the chain, which in real-world applications is usually unknown.We propose the first optimization method that does not require the knowledge ofthe mixing time, yet obtains the optimal asymptotic convergence rate whenapplied to convex problems. We further show that our approach can be extendedto: (i) finding stationary points in non-convex optimization with Markoviandata, and (ii) obtaining better dependence on the mixing time in temporaldifference (TD) learning; in both cases, our method is completely oblivious tothe mixing time. Our method relies on a novel combination of multi-level MonteCarlo (MLMC) gradient estimation together with an adaptive learning method.</description><author>Ron Dorfman, Kfir Y. Levy</author><pubDate>Thu, 13 Jul 2023 17:05:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.04428v3</guid></item><item><title>Convex Decomposition of Indoor Scenes</title><link>http://arxiv.org/abs/2307.04246v2</link><description>We describe a method to parse a complex, cluttered indoor scene intoprimitives which offer a parsimonious abstraction of scene structure. Ourprimitives are simple convexes. Our method uses a learned regression procedureto parse a scene into a fixed number of convexes from RGBD input, and canoptionally accept segmentations to improve the decomposition. The result isthen polished with a descent method which adjusts the convexes to produce avery good fit, and greedily removes superfluous primitives. Because the entirescene is parsed, we can evaluate using traditional depth, normal, andsegmentation error metrics. Our evaluation procedure demonstrates that theerror from our primitive representation is comparable to that of predictingdepth from a single image.</description><author>Vaibhav Vavilala, David Forsyth</author><pubDate>Thu, 13 Jul 2023 16:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04246v2</guid></item><item><title>PC-Droid: Faster diffusion and improved quality for particle cloud generation</title><link>http://arxiv.org/abs/2307.06836v1</link><description>Building on the success of PC-JeDi we introduce PC-Droid, a substantiallyimproved diffusion model for the generation of jet particle clouds. Byleveraging a new diffusion formulation, studying more recent integrationsolvers, and training on all jet types simultaneously, we are able to achievestate-of-the-art performance for all types of jets across all evaluationmetrics. We study the trade-off between generation speed and quality bycomparing two attention based architectures, as well as the potential ofconsistency distillation to reduce the number of diffusion steps. Both thefaster architecture and consistency models demonstrate performance surpassingmany competing models, with generation time up to two orders of magnitudefaster than PC-JeDi.</description><author>Matthew Leigh, Debajyoti Sengupta, John Andrew Raine, Guillaume Quétant, Tobias Golling</author><pubDate>Thu, 13 Jul 2023 16:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06836v1</guid></item><item><title>Personalization for BERT-based Discriminative Speech Recognition Rescoring</title><link>http://arxiv.org/abs/2307.06832v1</link><description>Recognition of personalized content remains a challenge in end-to-end speechrecognition. We explore three novel approaches that use personalized content ina neural rescoring step to improve recognition: gazetteers, prompting, and across-attention based encoder-decoder model. We use internal de-identifieden-US data from interactions with a virtual voice assistant supplemented withpersonalized named entities to compare these approaches. On a test set withpersonalized named entities, we show that each of these approaches improvesword error rate by over 10%, against a neural rescoring baseline. We also showthat on this test set, natural language prompts can improve word error rate by7% without any training and with a marginal loss in generalization. Overall,gazetteers were found to perform the best with a 10% improvement in word errorrate (WER), while also improving WER on a general test set by 1%.</description><author>Jari Kolehmainen, Yile Gu, Aditya Gourav, Prashanth Gurunath Shivakumar, Ankur Gandhe, Ariya Rastrow, Ivan Bulyko</author><pubDate>Thu, 13 Jul 2023 16:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06832v1</guid></item><item><title>A Novel Bayes' Theorem for Upper Probabilities</title><link>http://arxiv.org/abs/2307.06831v1</link><description>In their seminal 1990 paper, Wasserman and Kadane establish an upper boundfor the Bayes' posterior probability of a measurable set $A$, when the priorlies in a class of probability measures $\mathcal{P}$ and the likelihood isprecise. They also give a sufficient condition for such upper bound to holdwith equality. In this paper, we introduce a generalization of their result byadditionally addressing uncertainty related to the likelihood. We give an upperbound for the posterior probability when both the prior and the likelihoodbelong to a set of probabilities. Furthermore, we give a sufficient conditionfor this upper bound to become an equality. This result is interesting on itsown, and has the potential of being applied to various fields of engineering(e.g. model predictive control), machine learning, and artificial intelligence.</description><author>Michele Caprio, Yusuf Sale, Eyke Hüllermeier, Insup Lee</author><pubDate>Thu, 13 Jul 2023 16:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06831v1</guid></item><item><title>A Causal Framework to Unify Common Domain Generalization Approaches</title><link>http://arxiv.org/abs/2307.06825v1</link><description>Domain generalization (DG) is about learning models that generalize well tonew domains that are related to, but different from, the training domain(s). Itis a fundamental problem in machine learning and has attracted much attentionin recent years. A large number of approaches have been proposed. Differentapproaches are motivated from different perspectives, making it difficult togain an overall understanding of the area. In this paper, we propose a causalframework for domain generalization and present an understanding of common DGapproaches in the framework. Our work sheds new lights on the followingquestions: (1) What are the key ideas behind each DG method? (2) Why is itexpected to improve generalization to new domains theoretically? (3) How aredifferent DG methods related to each other and what are relative advantages andlimitations? By providing a unified perspective on DG, we hope to helpresearchers better understand the underlying principles and develop moreeffective approaches for this critical problem in machine learning.</description><author>Nevin L. Zhang, Kaican Li, Han Gao, Weiyan Xie, Zhi Lin, Zhenguo Li, Luning Wang, Yongxiang Huang</author><pubDate>Thu, 13 Jul 2023 16:40:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06825v1</guid></item><item><title>TinyMetaFed: Efficient Federated Meta-Learning for TinyML</title><link>http://arxiv.org/abs/2307.06822v1</link><description>The field of Tiny Machine Learning (TinyML) has made substantial advancementsin democratizing machine learning on low-footprint devices, such asmicrocontrollers. The prevalence of these miniature devices raises the questionof whether aggregating their knowledge can benefit TinyML applications.Federated meta-learning is a promising answer to this question, as it addressesthe scarcity of labeled data and heterogeneous data distribution across devicesin the real world. However, deploying TinyML hardware faces unique resourceconstraints, making existing methods impractical due to energy, privacy, andcommunication limitations. We introduce TinyMetaFed, a model-agnosticmeta-learning framework suitable for TinyML. TinyMetaFed facilitatescollaborative training of a neural network initialization that can be quicklyfine-tuned on new devices. It offers communication savings and privacyprotection through partial local reconstruction and Top-P% selectivecommunication, computational efficiency via online learning, and robustness toclient heterogeneity through few-shot learning. The evaluations on three TinyMLuse cases demonstrate that TinyMetaFed can significantly reduce energyconsumption and communication overhead, accelerate convergence, and stabilizethe training process.</description><author>Haoyu Ren, Xue Li, Darko Anicic, Thomas A. Runkler</author><pubDate>Thu, 13 Jul 2023 16:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06822v1</guid></item><item><title>Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting</title><link>http://arxiv.org/abs/2307.05766v2</link><description>Radiology reporting is a crucial part of the communication betweenradiologists and other medical professionals, but it can be time-consuming anderror-prone. One approach to alleviate this is structured reporting, whichsaves time and enables a more accurate evaluation than free-text reports.However, there is limited research on automating structured reporting, and nopublic benchmark is available for evaluating and comparing different methods.To close this gap, we introduce Rad-ReStruct, a new benchmark dataset thatprovides fine-grained, hierarchically ordered annotations in the form ofstructured reports for X-Ray images. We model the structured reporting task ashierarchical visual question answering (VQA) and propose hi-VQA, a novel methodthat considers prior context in the form of previously asked questions andanswers for populating a structured radiology report. Our experiments show thathi-VQA achieves competitive performance to the state-of-the-art on the medicalVQA benchmark VQARad while performing best among methods withoutdomain-specific vision-language pretraining and provides a strong baseline onRad-ReStruct. Our work represents a significant step towards the automatedpopulation of structured radiology reports and provides a valuable firstbenchmark for future research in this area. We will make all annotations andour code for annotation generation, model evaluation, and training publiclyavailable upon acceptance. Our dataset and code is available athttps://github.com/ChantalMP/Rad-ReStruct.</description><author>Chantal Pellegrini, Matthias Keicher, Ege Özsoy, Nassir Navab</author><pubDate>Thu, 13 Jul 2023 16:28:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05766v2</guid></item><item><title>Robust online active learning</title><link>http://arxiv.org/abs/2302.00422v5</link><description>In many industrial applications, obtaining labeled observations is notstraightforward as it often requires the intervention of human experts or theuse of expensive testing equipment. In these circumstances, active learning canbe highly beneficial in suggesting the most informative data points to be usedwhen fitting a model. Reducing the number of observations needed for modeldevelopment alleviates both the computational burden required for training andthe operational expenses related to labeling. Online active learning, inparticular, is useful in high-volume production processes where the decisionabout the acquisition of the label for a data point needs to be taken within anextremely short time frame. However, despite the recent efforts to developonline active learning strategies, the behavior of these methods in thepresence of outliers has not been thoroughly examined. In this work, weinvestigate the performance of online active linear regression in contaminateddata streams. Our study shows that the currently available query strategies areprone to sample outliers, whose inclusion in the training set eventuallydegrades the predictive performance of the models. To address this issue, wepropose a solution that bounds the search area of a conditional D-optimalalgorithm and uses a robust estimator. Our approach strikes a balance betweenexploring unseen regions of the input space and protecting against outliers.Through numerical simulations, we show that the proposed method is effective inimproving the performance of online active learning in the presence ofoutliers, thus expanding the potential applications of this powerful tool.</description><author>Davide Cacciarelli, Murat Kulahci, John Sølve Tyssedal</author><pubDate>Thu, 13 Jul 2023 16:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00422v5</guid></item><item><title>Fast and Functional Structured Data Generators Rooted in Out-of-Equilibrium Physics</title><link>http://arxiv.org/abs/2307.06797v1</link><description>In this study, we address the challenge of using energy-based models toproduce high-quality, label-specific data in complex structured datasets, suchas population genetics, RNA or protein sequences data. Traditional trainingmethods encounter difficulties due to inefficient Markov chain Monte Carlomixing, which affects the diversity of synthetic data and increases generationtimes. To address these issues, we use a novel training algorithm that exploitsnon-equilibrium effects. This approach, applied on the Restricted BoltzmannMachine, improves the model's ability to correctly classify samples andgenerate high-quality synthetic data in only a few sampling steps. Theeffectiveness of this method is demonstrated by its successful application tofour different types of data: handwritten digits, mutations of human genomesclassified by continental origin, functionally characterized sequences of anenzyme protein family, and homologous RNA sequences from specific taxonomies.</description><author>Alessandra Carbone, Aurélien Decelle, Lorenzo Rosset, Beatriz Seoane</author><pubDate>Thu, 13 Jul 2023 16:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06797v1</guid></item><item><title>Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics</title><link>http://arxiv.org/abs/2212.00679v3</link><description>Automated synthesis of provably correct controllers for cyber-physicalsystems is crucial for deployment in safety-critical scenarios. However, hybridfeatures and stochastic or unknown behaviours make this problem challenging. Wepropose a method for synthesising controllers for Markov jump linear systems(MJLSs), a class of discrete-time models for cyber-physical systems, so thatthey certifiably satisfy probabilistic computation tree logic (PCTL) formulae.An MJLS consists of a finite set of stochastic linear dynamics and discretejumps between these dynamics that are governed by a Markov decision process(MDP). We consider the cases where the transition probabilities of this MDP areeither known up to an interval or completely unknown. Our approach is based ona finite-state abstraction that captures both the discrete (mode-jumping) andcontinuous (stochastic linear) behaviour of the MJLS. We formalise thisabstraction as an interval MDP (iMDP) for which we compute intervals oftransition probabilities using sampling techniques from the so-called 'scenarioapproach', resulting in a probabilistically sound approximation. We apply ourmethod to multiple realistic benchmark problems, in particular, a temperaturecontrol and an aerial vehicle delivery problem.</description><author>Luke Rickard, Thom Badings, Licio Romao, Alessandro Abate</author><pubDate>Thu, 13 Jul 2023 16:07:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00679v3</guid></item><item><title>Learning Graph ARMA Processes from Time-Vertex Spectra</title><link>http://arxiv.org/abs/2302.06887v2</link><description>The modeling of time-varying graph signals as stationary time-vertexstochastic processes permits the inference of missing signal values byefficiently employing the correlation patterns of the process across differentgraph nodes and time instants. In this study, we propose an algorithm forcomputing graph autoregressive moving average (graph ARMA) processes based onlearning the joint time-vertex power spectral density of the process from itsincomplete realizations for the task of signal interpolation. Our solutionrelies on first roughly estimating the joint spectrum of the process frompartially observed realizations and then refining this estimate by projectingit onto the spectrum manifold of the graph ARMA process through convexrelaxations. The initially missing signal values are then estimated based onthe learnt model. Experimental results show that the proposed approach achieveshigh accuracy in time-vertex signal estimation problems.</description><author>Eylem Tugce Guneyi, Berkay Yaldiz, Abdullah Canbolat, Elif Vural</author><pubDate>Thu, 13 Jul 2023 16:06:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06887v2</guid></item><item><title>Leveraging Vision-Language Foundation Models for Fine-Grained Downstream Tasks</title><link>http://arxiv.org/abs/2307.06795v1</link><description>Vision-language foundation models such as CLIP have shown impressivezero-shot performance on many tasks and datasets, especially thanks to theirfree-text inputs. However, they struggle to handle some downstream tasks, suchas fine-grained attribute detection and localization. In this paper, we proposea multitask fine-tuning strategy based on a positive/negative promptformulation to further leverage the capacities of the vision-languagefoundation models. Using the CLIP architecture as baseline, we show strongimprovements on bird fine-grained attribute detection and localization tasks,while also increasing the classification performance on the CUB200-2011dataset. We provide source code for reproducibility purposes: it is availableat https://github.com/FactoDeepLearning/MultitaskVLFM.</description><author>Denis Coquenet, Clément Rambour, Emanuele Dalsasso, Nicolas Thome</author><pubDate>Thu, 13 Jul 2023 16:05:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06795v1</guid></item><item><title>Negated Complementary Commonsense using Large Language Models</title><link>http://arxiv.org/abs/2307.06794v1</link><description>Larger language models, such as GPT-3, have shown to be excellent in manytasks. However, we demonstrate that out-of-ordinary questions can throw themodel off guard. This work focuses on finding answers to negated complementaryquestions in commonsense scenarios. We illustrate how such questions adverselyaffect the model responses. We propose a model-agnostic methodology to improvethe performance in negated complementary scenarios. Our method outperformsfew-shot generation from GPT-3 (by more than 11 points) and, more importantly,highlights the significance of studying the response of large language modelsin negated complementary questions. The code, data, and experiments areavailable under: https://github.com/navidre/negated_complementary_commonsense.</description><author>Navid Rezaei, Marek Z. Reformat</author><pubDate>Thu, 13 Jul 2023 16:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06794v1</guid></item><item><title>Reading Radiology Imaging Like The Radiologist</title><link>http://arxiv.org/abs/2307.05921v2</link><description>Automated radiology report generation aims to generate radiology reports thatcontain rich, fine-grained descriptions of radiology imaging. Compared withimage captioning in the natural image domain, medical images are very similarto each other, with only minor differences in the occurrence of diseases. Giventhe importance of these minor differences in the radiology report, it iscrucial to encourage the model to focus more on the subtle regions of diseaseoccurrence. Secondly, the problem of visual and textual data biases is serious.Not only do normal cases make up the majority of the dataset, but sentencesdescribing areas with pathological changes also constitute only a small part ofthe paragraph. Lastly, generating medical image reports involves the challengeof long text generation, which requires more expertise and empirical trainingin medical knowledge. As a result, the difficulty of generating such reports isincreased. To address these challenges, we propose a disease-oriented retrievalframework that utilizes similar reports as prior knowledge references. Wedesign a factual consistency captioning generator to generate more accurate andfactually consistent disease descriptions. Our framework can find most similarreports for a given disease from the CXR database by retrieving adisease-oriented mask consisting of the position and morphologicalcharacteristics. By referencing the disease-oriented similar report and thevisual features, the factual consistency model can generate a more accurateradiology report.</description><author>Yuhao Wang</author><pubDate>Thu, 13 Jul 2023 15:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05921v2</guid></item><item><title>Rebalanced Zero-shot Learning</title><link>http://arxiv.org/abs/2210.07031v2</link><description>Zero-shot learning (ZSL) aims to identify unseen classes with zero samplesduring training. Broadly speaking, present ZSL methods usually adoptclass-level semantic labels and compare them with instance-level semanticpredictions to infer unseen classes. However, we find that such existing modelsmostly produce imbalanced semantic predictions, i.e. these models could performprecisely for some semantics, but may not for others. To address the drawback,we aim to introduce an imbalanced learning framework into ZSL. However, we findthat imbalanced ZSL has two unique challenges: (1) Its imbalanced predictionsare highly correlated with the value of semantic labels rather than the numberof samples as typically considered in the traditional imbalanced learning; (2)Different semantics follow quite different error distributions between classes.To mitigate these issues, we first formalize ZSL as an imbalanced regressionproblem which offers empirical evidences to interpret how semantic labels leadto imbalanced semantic predictions. We then propose a re-weighted loss termedRe-balanced Mean-Squared Error (ReMSE), which tracks the mean and variance oferror distributions, thus ensuring rebalanced learning across classes. As amajor contribution, we conduct a series of analyses showing that ReMSE istheoretically well established. Extensive experiments demonstrate that theproposed method effectively alleviates the imbalance in semantic prediction andoutperforms many state-of-the-art ZSL methods. Our code is available athttps://github.com/FouriYe/ReZSL-TIP23.</description><author>Zihan Ye, Guanyu Yang, Xiaobo Jin, Youfa Liu, Kaizhu Huang</author><pubDate>Thu, 13 Jul 2023 15:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07031v2</guid></item><item><title>Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation</title><link>http://arxiv.org/abs/2307.06784v1</link><description>This paper presents a novel algorithm for crack localisation and detectionbased on visual and tactile analysis via fibre-optics. A finger-shaped sensorbased on fibre-optics is employed for the data acquisition to collect data forthe analysis and the experiments. To detect the possible locations of cracks acamera is used to scan an environment while running an object detectionalgorithm. Once the crack is detected, a fully-connected graph is created froma skeletonised version of the crack. A minimum spanning tree is then employedfor calculating the shortest path to explore the crack which is then used todevelop the motion planner for the robotic manipulator. The motion plannerdivides the crack into multiple nodes which are then explored individually.Then, the manipulator starts the exploration and performs the tactile dataclassification to confirm if there is indeed a crack in that location or just afalse positive from the vision algorithm. If a crack is detected, also thelength, width, orientation and number of branches are calculated. This isrepeated until all the nodes of the crack are explored. In order to validate the complete algorithm, various experiments areperformed: comparison of exploration of cracks through full scan and motionplanning algorithm, implementation of frequency-based features for crackclassification and geometry analysis using a combination of vision and tactiledata. From the results of the experiments, it is shown that the proposedalgorithm is able to detect cracks and improve the results obtained from visionto correctly classify cracks and their geometry with minimal cost thanks to themotion planning algorithm.</description><author>Francesca Palermo, Bukeikhan Omarali, Changae Oh, Kaspar Althoefer, Ildar Farkhatdinov</author><pubDate>Thu, 13 Jul 2023 15:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06784v1</guid></item><item><title>A Novel Driver Distraction Behavior Detection Method Based on Self-supervised Learning with Masked Image Modeling</title><link>http://arxiv.org/abs/2306.00543v4</link><description>Driver distraction causes a significant number of traffic accidents everyyear, resulting in economic losses and casualties. Currently, the level ofautomation in commercial vehicles is far from completely unmanned, and driversstill play an important role in operating and controlling the vehicle.Therefore, driver distraction behavior detection is crucial for road safety. Atpresent, driver distraction detection primarily relies on traditionalconvolutional neural networks (CNN) and supervised learning methods. However,there are still challenges such as the high cost of labeled datasets, limitedability to capture high-level semantic information, and weak generalizationperformance. In order to solve these problems, this paper proposes a newself-supervised learning method based on masked image modeling for driverdistraction behavior detection. Firstly, a self-supervised learning frameworkfor masked image modeling (MIM) is introduced to solve the serious human andmaterial consumption issues caused by dataset labeling. Secondly, the SwinTransformer is employed as an encoder. Performance is enhanced by reconfiguringthe Swin Transformer block and adjusting the distribution of the number ofwindow multi-head self-attention (W-MSA) and shifted window multi-headself-attention (SW-MSA) detection heads across all stages, which leads to modelmore lightening. Finally, various data augmentation strategies are used alongwith the best random masking strategy to strengthen the model's recognition andgeneralization ability. Test results on a large-scale driver distractionbehavior dataset show that the self-supervised learning method proposed in thispaper achieves an accuracy of 99.60%, approximating the excellent performanceof advanced supervised learning methods. Our code is publicly available atgithub.com/Rocky1salady-killer/SL-DDBD.</description><author>Yingzhi Zhang, Taiguo Li, Chao Li, Xinghong Zhou</author><pubDate>Thu, 13 Jul 2023 15:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00543v4</guid></item><item><title>Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach</title><link>http://arxiv.org/abs/2307.01316v2</link><description>The dynamic nature of driving environments and the presence of diverse roadusers pose significant challenges for decision-making in autonomous driving.Deep reinforcement learning (DRL) has emerged as a popular approach to tacklethis problem. However, the application of existing DRL solutions is mainlyconfined to simulated environments due to safety concerns, impeding theirdeployment in real-world. To overcome this limitation, this paper introduces anovel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics(DRLSL) that combines the strengths of DRL (learning from experience) andsymbolic first-order logics (knowledge-driven reasoning) to enable safelearning in real-time interactions of autonomous driving within realenvironments. This innovative approach provides a means to learn autonomousdriving policies by actively engaging with the physical environment whileensuring safety. We have implemented the DRLSL framework in autonomous drivingusing the highD dataset and demonstrated that our method successfully avoidsunsafe actions during both the training and testing phases. Furthermore, ourresults indicate that DRLSL achieves faster convergence during training andexhibits better generalizability to new driving scenarios compared totraditional DRL methods.</description><author>Iman Sharifi, Mustafa Yildirim, Saber Fallah</author><pubDate>Thu, 13 Jul 2023 15:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01316v2</guid></item><item><title>Provably Faster Gradient Descent via Long Steps</title><link>http://arxiv.org/abs/2307.06324v2</link><description>This work establishes provably faster convergence rates for gradient descentvia a computer-assisted analysis technique. Our theory allows nonconstantstepsize policies with frequent long steps potentially violating descent byanalyzing the overall effect of many iterations at once rather than the typicalone-iteration inductions used in most first-order method analyses. We show thatlong steps, which may increase the objective value in the short term, lead toprovably faster convergence in the long term. A conjecture towards proving afaster $O(1/T\log T)$ rate for gradient descent is also motivated along withsimple numerical validation.</description><author>Benjamin Grimmer</author><pubDate>Thu, 13 Jul 2023 15:28:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06324v2</guid></item><item><title>Generalizing Supervised Deep Learning MRI Reconstruction to Multiple and Unseen Contrasts using Meta-Learning Hypernetworks</title><link>http://arxiv.org/abs/2307.06771v1</link><description>Meta-learning has recently been an emerging data-efficient learning techniquefor various medical imaging operations and has helped advance contemporary deeplearning models. Furthermore, meta-learning enhances the knowledgegeneralization of the imaging tasks by learning both shared and discriminativeweights for various configurations of imaging tasks. However, existingmeta-learning models attempt to learn a single set of weight initializations ofa neural network that might be restrictive for multimodal data. This work aimsto develop a multimodal meta-learning model for image reconstruction, whichaugments meta-learning with evolutionary capabilities to encompass diverseacquisition settings of multimodal data. Our proposed model called KM-MAML(Kernel Modulation-based Multimodal Meta-Learning), has hypernetworks thatevolve to generate mode-specific weights. These weights provide themode-specific inductive bias for multiple modes by re-calibrating each kernelof the base network for image reconstruction via a low-rank kernel modulationoperation. We incorporate gradient-based meta-learning (GBML) in the contextualspace to update the weights of the hypernetworks for different modes. Thehypernetworks and the reconstruction network in the GBML setting providediscriminative mode-specific features and low-level image features,respectively. Experiments on multi-contrast MRI reconstruction show that ourmodel, (i) exhibits superior reconstruction performance over joint training,other meta-learning methods, and context-specific MRI reconstruction methods,and (ii) better adaptation capabilities with improvement margins of 0.5 dB inPSNR and 0.01 in SSIM. Besides, a representation analysis with U-Net shows thatkernel modulation infuses 80% of mode-specific representation changes in thehigh-resolution layers. Our source code is available athttps://github.com/sriprabhar/KM-MAML/.</description><author>Sriprabha Ramanarayanan, Arun Palla, Keerthi Ram, Mohanasankar Sivaprakasam</author><pubDate>Thu, 13 Jul 2023 15:22:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06771v1</guid></item><item><title>Visually Adversarial Attacks and Defenses in the Physical World: A Survey</title><link>http://arxiv.org/abs/2211.01671v5</link><description>Although Deep Neural Networks (DNNs) have been widely applied in variousreal-world scenarios, they are vulnerable to adversarial examples. The currentadversarial attacks in computer vision can be divided into digital attacks andphysical attacks according to their different attack forms. Compared withdigital attacks, which generate perturbations in the digital pixels, physicalattacks are more practical in the real world. Owing to the serious securityproblem caused by physically adversarial examples, many works have beenproposed to evaluate the physically adversarial robustness of DNNs in the pastyears. In this paper, we summarize a survey versus the current physicallyadversarial attacks and physically adversarial defenses in computer vision. Toestablish a taxonomy, we organize the current physical attacks from attacktasks, attack forms, and attack methods, respectively. Thus, readers can have asystematic knowledge of this topic from different aspects. For the physicaldefenses, we establish the taxonomy from pre-processing, in-processing, andpost-processing for the DNN models to achieve full coverage of the adversarialdefenses. Based on the above survey, we finally discuss the challenges of thisresearch field and further outlook on the future direction.</description><author>Xingxing Wei, Bangzheng Pu, Jiefan Lu, Baoyuan Wu</author><pubDate>Thu, 13 Jul 2023 15:18:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01671v5</guid></item><item><title>Classification and Generation of real-world data with an Associative Memory Model</title><link>http://arxiv.org/abs/2207.04827v4</link><description>Drawing from memory the face of a friend you have not seen in years is adifficult task. However, if you happen to cross paths, you would easilyrecognize each other. The biological memory is equipped with an impressivecompression algorithm that can store the essential, and then infer the detailsto match perception. The Willshaw Memory is a simple abstract model forcortical computations which implements mechanisms of biological memories. Usingour recently proposed sparse coding prescription for visual patterns, thismodel can store and retrieve an impressive amount of real-world data in afault-tolerant manner. In this paper, we extend the capabilities of the basicAssociative Memory Model by using a Multiple-Modality framework. In thissetting, the memory stores several modalities (e.g., visual, or textual) ofeach pattern simultaneously. After training, the memory can be used to infermissing modalities when just a subset is perceived. Using a simpleencoder-memory-decoder architecture, and a newly proposed iterative retrievalalgorithm for the Willshaw Model, we perform experiments on the MNIST dataset.By storing both the images and labels as modalities, a single Memory can beused not only to retrieve and complete patterns but also to classify andgenerate new ones. We further discuss how this model could be used for otherlearning tasks, thus serving as a biologically-inspired framework for learning.</description><author>Rodrigo Simas, Luis Sa-Couto, Andreas Wichert</author><pubDate>Thu, 13 Jul 2023 15:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.04827v4</guid></item><item><title>A Deep Learning Method for Comparing Bayesian Hierarchical Models</title><link>http://arxiv.org/abs/2301.11873v3</link><description>Bayesian model comparison (BMC) offers a principled approach for assessingthe relative merits of competing computational models and propagatinguncertainty into model selection decisions. However, BMC is often intractablefor the popular class of hierarchical models due to their high-dimensionalnested parameter structure. To address this intractability, we propose a deeplearning method for performing BMC on any set of hierarchical models which canbe instantiated as probabilistic programs. Since our method enables amortizedinference, it allows efficient re-estimation of posterior model probabilitiesand fast performance validation prior to any real-data application. In a seriesof extensive validation studies, we benchmark the performance of our methodagainst the state-of-the-art bridge sampling method and demonstrate excellentamortized inference across all BMC settings. We then showcase our method bycomparing four hierarchical evidence accumulation models that have previouslybeen deemed intractable for BMC due to partly implicit likelihoods. In thisapplication, we corroborate evidence for the recently proposed L\'evy flightmodel of decision-making and show how transfer learning can be leveraged toenhance training efficiency. We provide reproducible code for all analyses andan open-source implementation of our method.</description><author>Lasse Elsemüller, Martin Schnuerch, Paul-Christian Bürkner, Stefan T. Radev</author><pubDate>Thu, 13 Jul 2023 15:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11873v3</guid></item><item><title>Privacy-Utility Trade-offs in Neural Networks for Medical Population Graphs: Insights from Differential Privacy and Graph Structure</title><link>http://arxiv.org/abs/2307.06760v1</link><description>We initiate an empirical investigation into differentially private graphneural networks on population graphs from the medical domain by examiningprivacy-utility trade-offs at different privacy levels on both real-world andsynthetic datasets and performing auditing through membership inferenceattacks. Our findings highlight the potential and the challenges of thisspecific DP application area. Moreover, we find evidence that the underlyinggraph structure constitutes a potential factor for larger performance gaps byshowing a correlation between the degree of graph homophily and the accuracy ofthe trained model.</description><author>Tamara T. Mueller, Maulik Chevli, Ameya Daigavane, Daniel Rueckert, Georgios Kaissis</author><pubDate>Thu, 13 Jul 2023 14:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06760v1</guid></item><item><title>Layered controller synthesis for dynamic multi-agent systems</title><link>http://arxiv.org/abs/2307.06758v1</link><description>In this paper we present a layered approach for multi-agent control problem,decomposed into three stages, each building upon the results of the previousone. First, a high-level plan for a coarse abstraction of the system iscomputed, relying on parametric timed automata augmented with stopwatches asthey allow to efficiently model simplified dynamics of such systems. In thesecond stage, the high-level plan, based on SMT-formulation, mainly handles thecombinatorial aspects of the problem, provides a more dynamically accuratesolution. These stages are collectively referred to as the SWA-SMT solver. Theyare correct by construction but lack a crucial feature: they cannot be executedin real time. To overcome this, we use SWA-SMT solutions as the initialtraining dataset for our last stage, which aims at obtaining a neural networkcontrol policy. We use reinforcement learning to train the policy, and showthat the initial dataset is crucial for the overall success of the method.</description><author>Emily Clement, Nicolas Perrin-Gilbert, Philipp Schlehuber-Caissier</author><pubDate>Thu, 13 Jul 2023 14:56:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06758v1</guid></item><item><title>Revisiting Discrete Soft Actor-Critic</title><link>http://arxiv.org/abs/2209.10081v3</link><description>We study the adaption of soft actor-critic (SAC) from continuous action spaceto discrete action space. We revisit vanilla SAC and provide an in-depthunderstanding of its Q value underestimation and performance instability issueswhen applied to discrete settings. We thereby propose entropy-penalty anddouble average Q-learning with Q-clip to address these issues. Extensiveexperiments on typical benchmarks with discrete action space, including Atarigames and a large-scale MOBA game, show the efficacy of our proposed method.Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.</description><author>Haibin Zhou, Zichuan Lin, Junyou Li, Qiang Fu, Wei Yang, Deheng Ye</author><pubDate>Thu, 13 Jul 2023 14:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.10081v3</guid></item><item><title>PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation</title><link>http://arxiv.org/abs/2307.00470v3</link><description>Large language models(LLMS) have shown excellent text generationcapabilities,capable of generating fluent responses for many downstream tasks.However,applying large language models to real-world critical tasks remainschallenging due to their susceptibility to hallucinations and inability todirectly use external knowledge. To address the above challenges,this paperproposes PatternGPT, a pattern-driven text generation framework for largelanguage models. First,the framework utilizes the extraction capabilities oflarge language models to generate rich and diverse patterns and later draws onthe idea of federated learning. Using multiple agents to achieve sharing toobtain more diverse patterns. Finally, it searches for high-quality patternsusing judgment criteria and optimization algorithms and uses the searchedpatterns to guide the model for generation. This framework has the advantagesof generating diversified patterns, protecting data privacy,combining externalknowledge, and improving the quality of generation, which provides an effectivemethod to optimize the text generation capability of large language models,andmake it better applied to the field of intelligent dialogue and contentgeneration.</description><author>Le Xiao, Xin Shan</author><pubDate>Thu, 13 Jul 2023 14:46:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00470v3</guid></item><item><title>Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent</title><link>http://arxiv.org/abs/2307.06753v1</link><description>The learning of Gaussian Mixture Models (also referred to simply as GMMs)plays an important role in machine learning. Known for their expressiveness andinterpretability, Gaussian mixture models have a wide range of applications,from statistics, computer vision to distributional reinforcement learning.However, as of today, few known algorithms can fit or learn these models, someof which include Expectation-Maximization algorithms and Sliced WassersteinDistance. Even fewer algorithms are compatible with gradient descent, thecommon learning process for neural networks. In this paper, we derive a closed formula of two GMMs in the univariate,one-dimensional case, then propose a distance function called Sliced Cram\'er2-distance for learning general multivariate GMMs. Our approach has severaladvantages over many previous methods. First, it has a closed-form expressionfor the univariate case and is easy to compute and implement using commonmachine learning libraries (e.g., PyTorch and TensorFlow). Second, it iscompatible with gradient descent, which enables us to integrate GMMs withneural networks seamlessly. Third, it can fit a GMM not only to a set of datapoints, but also to another GMM directly, without sampling from the targetmodel. And fourth, it has some theoretical guarantees like global gradientboundedness and unbiased sampling gradient. These features are especiallyuseful for distributional reinforcement learning and Deep Q Networks, where thegoal is to learn a distribution over future rewards. We will also construct aGaussian Mixture Distributional Deep Q Network as a toy example to demonstrateits effectiveness. Compared with previous models, this model is parameterefficient in terms of representing a distribution and possesses betterinterpretability.</description><author>Ruichong Zhang</author><pubDate>Thu, 13 Jul 2023 14:43:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06753v1</guid></item><item><title>Watch Your Pose: Unsupervised Domain Adaption with Pose based Triplet Selection for Gait Recognition</title><link>http://arxiv.org/abs/2307.06751v1</link><description>Gait Recognition is a computer vision task aiming to identify people by theirwalking patterns. Existing methods show impressive results on individualdatasets but lack the ability to generalize to unseen scenarios. UnsupervisedDomain Adaptation (UDA) tries to adapt a model, pre-trained in a supervisedmanner on a source domain, to an unlabelled target domain. UDA for GaitRecognition is still in its infancy and existing works proposed solutions tolimited scenarios. In this paper, we reveal a fundamental phenomenon inadaptation of gait recognition models, in which the target domain is biased topose-based features rather than identity features, causing a significantperformance drop in the identification task. We suggest Gait Orientation-basedmethod for Unsupervised Domain Adaptation (GOUDA) to reduce this bias. To thisend, we present a novel Triplet Selection algorithm with a curriculum learningframework, aiming to adapt the embedding space by pushing away samples ofsimilar poses and bringing closer samples of different poses. We provideextensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP,GREW, and Gait3D, and on three backbones, GaitSet, GaitPart, and GaitGL,showing the superiority of our proposed method over prior works.</description><author>Gavriel Habib, Noa Barzilay, Or Shimshi, Rami Ben-Ari, Nir Darshan</author><pubDate>Thu, 13 Jul 2023 14:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06751v1</guid></item><item><title>Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling Services: A Multi-Agent Hierarchical Reinforcement Learning Approach</title><link>http://arxiv.org/abs/2307.06742v1</link><description>The integrated development of city clusters has given rise to an increasingdemand for intercity travel. Intercity ride-pooling service exhibitsconsiderable potential in upgrading traditional intercity bus services byimplementing demand-responsive enhancements. Nevertheless, its onlineoperations suffer the inherent complexities due to the coupling of vehicleresource allocation among cities and pooled-ride vehicle routing. To tacklethese challenges, this study proposes a two-level framework designed tofacilitate online fleet management. Specifically, a novel multi-agent feudalreinforcement learning model is proposed at the upper level of the framework tocooperatively assign idle vehicles to different intercity lines, while thelower level updates the routes of vehicles using an adaptive large neighborhoodsearch heuristic. Numerical studies based on the realistic dataset of Xiamenand its surrounding cities in China show that the proposed frameworkeffectively mitigates the supply and demand imbalances, and achievessignificant improvement in both the average daily system profit and orderfulfillment ratio.</description><author>Jinhua Si, Fang He, Xi Lin, Xindi Tang</author><pubDate>Thu, 13 Jul 2023 14:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06742v1</guid></item><item><title>Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events</title><link>http://arxiv.org/abs/2005.04490v6</link><description>Along with the development of modern smart cities, human-centric videoanalysis has been encountering the challenge of analyzing diverse and complexevents in real scenes. A complex event relates to dense crowds, anomalousindividuals, or collective behaviors. However, limited by the scale andcoverage of existing video datasets, few human analysis approaches havereported their performances on such complex events. To this end, we present anew large-scale dataset with comprehensive annotations, named Human-in-Eventsor HiEve (Human-centric video analysis in complex Events), for theunderstanding of human motions, poses, and actions in a variety of realisticevents, especially in crowd &amp; complex events. It contains a record number ofposes (&gt;1M), the largest number of action instances (&gt;56k) under complexevents, as well as one of the largest numbers of trajectories lasting forlonger time (with an average trajectory length of &gt;480 frames). Based on itsdiverse annotation, we present two simple baselines for action recognition andpose estimation, respectively. They leverage cross-label information duringtraining to enhance the feature learning in corresponding visual tasks.Experiments show that they could boost the performance of existing actionrecognition and pose estimation pipelines. More importantly, they prove thewidely ranged annotations in HiEve can improve various video tasks.Furthermore, we conduct extensive experiments to benchmark recent videoanalysis approaches together with our baseline methods, demonstrating HiEve isa challenging dataset for human-centric video analysis. We expect that thedataset will advance the development of cutting-edge techniques inhuman-centric analysis and the understanding of complex events. The dataset isavailable at http://humaninevents.org</description><author>Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Rui Qian, Tao Wang, Ning Xu, Hongkai Xiong, Guo-Jun Qi, Nicu Sebe</author><pubDate>Thu, 13 Jul 2023 14:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.04490v6</guid></item><item><title>PIGEON: Predicting Image Geolocations</title><link>http://arxiv.org/abs/2307.05845v2</link><description>We introduce PIGEON, a multi-task end-to-end system for planet-scale imagegeolocalization that achieves state-of-the-art performance on both externalbenchmarks and in human evaluation. Our work incorporates semantic geocellcreation with label smoothing, conducts pretraining of a vision transformer onimages with geographic information, and refines location predictions withProtoNets across a candidate set of geocells. The contributions of PIGEON arethree-fold: first, we design a semantic geocells creation and splittingalgorithm based on open-source data which can be adapted to any geospatialdataset. Second, we show the effectiveness of intra-geocell refinement and theapplicability of unsupervised clustering and ProtNets to the task. Finally, wemake our pre-trained CLIP transformer model, StreetCLIP, publicly available foruse in adjacent domains with applications to fighting climate change and urbanand rural scene understanding.</description><author>Lukas Haas, Michal Skreta, Silas Alberti</author><pubDate>Thu, 13 Jul 2023 14:22:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05845v2</guid></item><item><title>Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data</title><link>http://arxiv.org/abs/2307.06737v1</link><description>Human Pose Estimation is a thoroughly researched problem; however, mostdatasets focus on the side and front-view scenarios. We address the limitationby proposing a novel approach that tackles the challenges posed by extremeviewpoints and poses. We introduce a new method for synthetic data generation -RePoGen, RarE POses GENerator - with comprehensive control over pose and viewto augment the COCO dataset. Experiments on a new dataset of real images showthat adding RePoGen data to the COCO surpasses previous attempts to top-viewpose estimation and significantly improves performance on the bottom-viewdataset. Through an extensive ablation study on both the top and bottom viewdata, we elucidate the contributions of methodological choices and demonstrateimproved performance. The code and the datasets are available on the projectwebsite.</description><author>Miroslav Purkrábek, Jiří Matas</author><pubDate>Thu, 13 Jul 2023 14:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06737v1</guid></item><item><title>MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting</title><link>http://arxiv.org/abs/2307.06736v1</link><description>Time series forecasting has received wide interest from existing research dueto its broad applications and inherent challenging. The research challenge liesin identifying effective patterns in historical series and applying them tofuture forecasting. Advanced models based on point-wise connected MLP andTransformer architectures have strong fitting power, but their secondarycomputational complexity limits practicality. Additionally, those structuresinherently disrupt the temporal order, reducing the information utilization andmaking the forecasting process uninterpretable. To solve these problems, thispaper proposes a forecasting model, MPR-Net. It first adaptively decomposesmulti-scale historical series patterns using convolution operation, thenconstructs a pattern extension forecasting method based on the prior knowledgeof pattern reproduction, and finally reconstructs future patterns into futureseries using deconvolution operation. By leveraging the temporal dependenciespresent in the time series, MPR-Net not only achieves linear time complexity,but also makes the forecasting process interpretable. By carrying outsufficient experiments on more than ten real data sets of both short and longterm forecasting tasks, MPR-Net achieves the state of the art forecastingperformance, as well as good generalization and robustness performance.</description><author>Tianlong Zhao, Xiang Ma, Xuemei Li, Caiming Zhang</author><pubDate>Thu, 13 Jul 2023 14:16:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06736v1</guid></item><item><title>Learning fixed points of recurrent neural networks by reparameterizing the network model</title><link>http://arxiv.org/abs/2307.06732v1</link><description>In computational neuroscience, fixed points of recurrent neural networkmodels are commonly used to model neural responses to static or slowly changingstimuli. These applications raise the question of how to train the weights in arecurrent neural network to minimize a loss function evaluated on fixed points.A natural approach is to use gradient descent on the Euclidean space ofsynaptic weights. We show that this approach can lead to poor learningperformance due, in part, to singularities that arise in the loss surface. Weuse a re-parameterization of the recurrent network model to derive twoalternative learning rules that produces more robust learning dynamics. We showthat these learning rules can be interpreted as steepest descent and gradientdescent, respectively, under a non-Euclidean metric on the space of recurrentweights. Our results question the common, implicit assumption that learning inthe brain should necessarily follow the negative Euclidean gradient of synapticweights.</description><author>Vicky Zhu, Robert Rosenbaum</author><pubDate>Thu, 13 Jul 2023 14:09:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06732v1</guid></item><item><title>On the Validity of Conformal Prediction for Network Data Under Non-Uniform Sampling</title><link>http://arxiv.org/abs/2306.07252v4</link><description>We study the properties of conformal prediction for network data undervarious sampling mechanisms that commonly arise in practice but often result ina non-representative sample of nodes. We interpret these sampling mechanisms asselection rules applied to a superpopulation and study the validity ofconformal prediction conditional on an appropriate selection event. We showthat the sampled subarray is exchangeable conditional on the selection event ifthe selection rule satisfies a permutation invariance property and a jointexchangeability condition holds for the superpopulation. Our result implies thefinite-sample validity of conformal prediction for certain selection eventsrelated to ego networks and snowball sampling. We also show that when data aresampled via a random walk on a graph, a variant of weighted conformalprediction yields asymptotically valid prediction sets for an independentlyselected node from the population.</description><author>Robert Lunde</author><pubDate>Thu, 13 Jul 2023 13:42:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07252v4</guid></item><item><title>Local Intrinsic Dimensionality Measures for Graphs, with Applications to Graph Embeddings</title><link>http://arxiv.org/abs/2208.11986v2</link><description>The notion of local intrinsic dimensionality (LID) is an importantadvancement in data dimensionality analysis, with applications in data mining,machine learning and similarity search problems. Existing distance-based LIDestimators were designed for tabular datasets encompassing data pointsrepresented as vectors in a Euclidean space. After discussing their limitationsfor graph-structured data considering graph embeddings and graph distances, wepropose NC-LID, a novel LID-related measure for quantifying the discriminatorypower of the shortest-path distance with respect to natural communities ofnodes as their intrinsic localities. It is shown how this measure can be usedto design LID-aware graph embedding algorithms by formulating two LID-elasticvariants of node2vec with personalized hyperparameters that are adjustedaccording to NC-LID values. Our empirical analysis of NC-LID on a large numberof real-world graphs shows that this measure is able to point to nodes withhigh link reconstruction errors in node2vec embeddings better than nodecentrality metrics. The experimental evaluation also shows that the proposedLID-elastic node2vec extensions improve node2vec by better preserving graphstructure in generated embeddings.</description><author>Miloš Savić, Vladimir Kurbalija, Miloš Radovanović</author><pubDate>Thu, 13 Jul 2023 13:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.11986v2</guid></item><item><title>Multimodal Object Detection in Remote Sensing</title><link>http://arxiv.org/abs/2307.06724v1</link><description>Object detection in remote sensing is a crucial computer vision task that hasseen significant advancements with deep learning techniques. However, mostexisting works in this area focus on the use of generic object detection and donot leverage the potential of multimodal data fusion. In this paper, we presenta comparison of methods for multimodal object detection in remote sensing,survey available multimodal datasets suitable for evaluation, and discussfuture directions.</description><author>Abdelbadie Belmouhcine, Jean-Christophe Burnel, Luc Courtrai, Minh-Tan Pham, Sébastien Lefèvre</author><pubDate>Thu, 13 Jul 2023 13:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06724v1</guid></item><item><title>Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications</title><link>http://arxiv.org/abs/2304.12330v3</link><description>The coupling of deep reinforcement learning to numerical flow controlproblems has recently received a considerable attention, leading togroundbreaking results and opening new perspectives for the domain. Due to theusually high computational cost of fluid dynamics solvers, the use of parallelenvironments during the learning process represents an essential ingredient toattain efficient control in a reasonable time. Yet, most of the deepreinforcement learning literature for flow control relies on on-policyalgorithms, for which the massively parallel transition collection may breaktheoretical assumptions and lead to suboptimal control models. To overcome thisissue, we propose a parallelism pattern relying on partial-trajectory buffersterminated by a return bootstrapping step, allowing a flexible use of parallelenvironments while preserving the on-policiness of the updates. This approachis illustrated on a CPU-intensive continuous flow control problem from theliterature.</description><author>J. Viquerat, E. Hachem</author><pubDate>Thu, 13 Jul 2023 13:33:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12330v3</guid></item><item><title>A Survey on Evaluation of Large Language Models</title><link>http://arxiv.org/abs/2307.03109v4</link><description>Large language models (LLMs) are gaining increasing popularity in bothacademia and industry, owing to their unprecedented performance in variousapplications. As LLMs continue to play a vital role in both research and dailyuse, their evaluation becomes increasingly critical, not only at the tasklevel, but also at the society level for better understanding of theirpotential risks. Over the past years, significant efforts have been made toexamine LLMs from various perspectives. This paper presents a comprehensivereview of these evaluation methods for LLMs, focusing on three key dimensions:what to evaluate, where to evaluate, and how to evaluate. Firstly, we providean overview from the perspective of evaluation tasks, encompassing generalnatural language processing tasks, reasoning, medical usage, ethics,educations, natural and social sciences, agent applications, and other areas.Secondly, we answer the `where' and `how' questions by diving into theevaluation methods and benchmarks, which serve as crucial components inassessing performance of LLMs. Then, we summarize the success and failure casesof LLMs in different tasks. Finally, we shed light on several future challengesthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights toresearchers in the realm of LLMs evaluation, thereby aiding the development ofmore proficient LLMs. Our key point is that evaluation should be treated as anessential discipline to better assist the development of LLMs. We consistentlymaintain the related open-source materials at:https://github.com/MLGroupJLU/LLM-eval-survey.</description><author>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie</author><pubDate>Thu, 13 Jul 2023 13:33:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03109v4</guid></item><item><title>Breaking 3-Factor Approximation for Correlation Clustering in Polylogarithmic Rounds</title><link>http://arxiv.org/abs/2307.06723v1</link><description>In this paper, we study parallel algorithms for the correlation clusteringproblem, where every pair of two different entities is labeled with similar ordissimilar. The goal is to partition the entities into clusters to minimize thenumber of disagreements with the labels. Currently, all efficient parallelalgorithms have an approximation ratio of at least 3. In comparison with the$1.994+\epsilon$ ratio achieved by polynomial-time sequential algorithms[CLN22], a significant gap exists. We propose the first poly-logarithmic depth parallel algorithm that achievesa better approximation ratio than 3. Specifically, our algorithm computes a$(2.4+\epsilon)$-approximate solution and uses $\tilde{O}(m^{1.5})$ work.Additionally, it can be translated into a $\tilde{O}(m^{1.5})$-time sequentialalgorithm and a poly-logarithmic rounds sublinear-memory MPC algorithm with$\tilde{O}(m^{1.5})$ total memory. Our approach is inspired by Awerbuch, Khandekar, and Rao's [AKR12]length-constrained multi-commodity flow algorithm, where we develop anefficient parallel algorithm to solve a truncated correlation clustering linearprogram of Charikar, Guruswami, and Wirth [CGW05]. Then we show the solution ofthe truncated linear program can be rounded with a factor of at most 2.4 lossby using the framework of [CMSY15]. Such a rounding framework can then beimplemented using parallel pivot-based approaches.</description><author>Nairen Cao, Shang-En Huang, Hsin-Hao Su</author><pubDate>Thu, 13 Jul 2023 13:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06723v1</guid></item><item><title>Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative</title><link>http://arxiv.org/abs/2307.06721v1</link><description>Dialog policies, which determine a system's action based on the current stateat each dialog turn, are crucial to the success of the dialog. In recent years,reinforcement learning (RL) has emerged as a promising option for dialog policylearning (DPL). In RL-based DPL, dialog policies are updated according torewards. The manual construction of fine-grained rewards, such asstate-action-based ones, to effectively guide the dialog policy is challengingin multi-domain task-oriented dialog scenarios with numerous state-action paircombinations. One way to estimate rewards from collected data is to train thereward estimator and dialog policy simultaneously using adversarial learning(AL). Although this method has demonstrated superior performanceexperimentally, it is fraught with the inherent problems of AL, such as modecollapse. This paper first identifies the role of AL in DPL through detailedanalyses of the objective functions of dialog policy and reward estimator.Next, based on these analyses, we propose a method that eliminates AL fromreward estimation and DPL while retaining its advantages. We evaluate ourmethod using MultiWOZ, a multi-domain task-oriented dialog corpus.</description><author>Sho Shimoyama, Tetsuro Morimura, Kenshi Abe, Toda Takamichi, Yuta Tomomatsu, Masakazu Sugiyama, Asahi Hentona, Yuuki Azuma, Hirotaka Ninomiya</author><pubDate>Thu, 13 Jul 2023 13:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06721v1</guid></item><item><title>Weakly supervised marine animal detection from remote sensing images using vector-quantized variational autoencoder</title><link>http://arxiv.org/abs/2307.06720v1</link><description>This paper studies a reconstruction-based approach for weakly-supervisedanimal detection from aerial images in marine environments. Such an approachleverages an anomaly detection framework that computes metrics directly on theinput space, enhancing interpretability and anomaly localization compared tofeature embedding methods. Building upon the success of Vector-QuantizedVariational Autoencoders in anomaly detection on computer vision datasets, weadapt them to the marine animal detection domain and address the challenge ofhandling noisy data. To evaluate our approach, we compare it with existingmethods in the context of marine animal detection from aerial image data.Experiments conducted on two dedicated datasets demonstrate the superiorperformance of the proposed method over recent studies in the literature. Ourframework offers improved interpretability and localization of anomalies,providing valuable insights for monitoring marine ecosystems and mitigating theimpact of human activities on marine animals.</description><author>Minh-Tan Pham, Hugo Gangloff, Sébastien Lefèvre</author><pubDate>Thu, 13 Jul 2023 13:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06720v1</guid></item><item><title>Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models</title><link>http://arxiv.org/abs/2307.06713v1</link><description>A wide variety of natural language tasks are currently being addressed withlarge-scale language models (LLMs). These models are usually trained with avery large amount of unsupervised text data and adapted to perform a downstreamnatural language task using methods like fine-tuning, calibration or in-contextlearning. In this work, we propose an approach to adapt the prior classdistribution to perform text classification tasks without the need for labelledsamples and only few in-domain sample queries. The proposed approach treats theLLM as a black box, adding a stage where the model posteriors are calibrated tothe task. Results show that these methods outperform the un-adapted model fordifferent number of training shots in the prompt and a previous approach werecalibration is performed without using any adaptation data.</description><author>Lautaro Estienne</author><pubDate>Thu, 13 Jul 2023 13:11:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06713v1</guid></item><item><title>GRAN is superior to GraphRNN: node orderings, kernel- and graph embeddings-based metrics for graph generators</title><link>http://arxiv.org/abs/2307.06709v1</link><description>A wide variety of generative models for graphs have been proposed. They areused in drug discovery, road networks, neural architecture search, and programsynthesis. Generating graphs has theoretical challenges, such as isomorphicrepresentations -- evaluating how well a generative model performs isdifficult. Which model to choose depending on the application domain? We extensively study kernel-based metrics on distributions of graphinvariants and manifold-based and kernel-based metrics in graph embeddingspace. Manifold-based metrics outperform kernel-based metrics in embeddingspace. We use these metrics to compare GraphRNN and GRAN, two well-knowngenerative models for graphs, and unveil the influence of node orderings. Itshows the superiority of GRAN over GraphRNN - further, our proposed adaptationof GraphRNN with a depth-first search ordering is effective for small-sizedgraphs. A guideline on good practices regarding dataset selection and node featureinitialization is provided. Our work is accompanied by open-source code andreproducible experiments.</description><author>Ousmane Touat, Julian Stier, Pierre-Edouard Portier, Michael Granitzer</author><pubDate>Thu, 13 Jul 2023 13:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06709v1</guid></item><item><title>To share or not to share: What risks would laypeople accept to give sensitive data to differentially-private NLP systems?</title><link>http://arxiv.org/abs/2307.06708v1</link><description>Although the NLP community has adopted central differential privacy as ago-to framework for privacy-preserving model training or data sharing, thechoice and interpretation of the key parameter, privacy budget $\varepsilon$that governs the strength of privacy protection, remains largely arbitrary. Weargue that determining the $\varepsilon$ value should not be solely in thehands of researchers or system developers, but must also take into account theactual people who share their potentially sensitive data. In other words: Wouldyou share your instant messages for $\varepsilon$ of 10? We address thisresearch gap by designing, implementing, and conducting a behavioral experiment(311 lay participants) to study the behavior of people in uncertaindecision-making situations with respect to privacy-threatening situations.Framing the risk perception in terms of two realistic NLP scenarios and using avignette behavioral study help us determine what $\varepsilon$ thresholds wouldlead lay people to be willing to share sensitive textual data - to ourknowledge, the first study of its kind.</description><author>Christopher Weiss, Frauke Kreuter, Ivan Habernal</author><pubDate>Thu, 13 Jul 2023 13:06:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06708v1</guid></item><item><title>Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues</title><link>http://arxiv.org/abs/2307.06703v1</link><description>Answer selection in open-domain dialogues aims to select an accurate answerfrom candidates. Recent success of answer selection models hinges on trainingwith large amounts of labeled data. However, collecting large-scale labeleddata is labor-intensive and time-consuming. In this paper, we introduce thepredicted intent labels to calibrate answer labels in a self-training paradigm.Specifically, we propose the intent-calibrated self-training (ICAST) to improvethe quality of pseudo answer labels through the intent-calibrated answerselection paradigm, in which we employ pseudo intent labels to help improvepseudo answer labels. We carry out extensive experiments on two benchmarkdatasets with open-domain dialogues. The experimental results show that ICASToutperforms baselines consistently with 1%, 5% and 10% labeled data.Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets,compared with the strongest baseline with only 5% labeled data.</description><author>Wentao Deng, Jiahuan Pei, Zhaochun Ren, Zhumin Chen, Pengjie Ren</author><pubDate>Thu, 13 Jul 2023 13:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06703v1</guid></item><item><title>S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction</title><link>http://arxiv.org/abs/2307.06701v1</link><description>We address the video prediction task by putting forth a novel model thatcombines (i) our recently proposed hierarchical residual vector quantizedvariational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN(ST-PixelCNN). We refer to this approach as a sequential hierarchical residuallearning vector quantized variational autoencoder (S-HR-VQVAE). By leveragingthe intrinsic capabilities of HR-VQVAE at modeling still images with aparsimonious representation, combined with the ST-PixelCNN's ability athandling spatiotemporal information, S-HR-VQVAE can better deal with chiefchallenges in video prediction. These include learning spatiotemporalinformation, handling high dimensional data, combating blurry prediction, andimplicit modeling of physical characteristics. Extensive experimental resultson the KTH Human Action and Moving-MNIST tasks demonstrate that our modelcompares favorably against top video prediction techniques both in quantitativeand qualitative evaluations despite a much smaller model size. Finally, weboost S-HR-VQVAE by proposing a novel training method to jointly estimate theHR-VQVAE and ST-PixelCNN parameters.</description><author>Mohammad Adiban, Kalin Stefanov, Sabato Marco Siniscalchi, Giampiero Salvi</author><pubDate>Thu, 13 Jul 2023 12:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06701v1</guid></item><item><title>Parmesan: mathematical concept extraction for education</title><link>http://arxiv.org/abs/2307.06699v1</link><description>Mathematics is a highly specialized domain with its own unique set ofchallenges that has seen limited study in natural language processing. However,mathematics is used in a wide variety of fields and multidisciplinary researchin many different domains often relies on an understanding of mathematicalconcepts. To aid researchers coming from other fields, we develop a prototypesystem for searching for and defining mathematical concepts in context,focusing on the field of category theory. This system, Parmesan, depends onnatural language processing components including concept extraction, relationextraction, definition extraction, and entity linking. In developing thissystem, we show that existing techniques cannot be applied directly to thecategory theory domain, and suggest hybrid techniques that do perform well,though we expect the system to evolve over time. We also provide two cleanedmathematical corpora that power the prototype system, which are based onjournal articles and wiki pages, respectively. The corpora have been annotatedwith dependency trees, lemmas, and part-of-speech tags.</description><author>Jacob Collard, Valeria de Paiva, Eswaran Subrahmanian</author><pubDate>Thu, 13 Jul 2023 12:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06699v1</guid></item><item><title>IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation</title><link>http://arxiv.org/abs/2307.06698v1</link><description>Knowledge Graph Embedding (KGE) models are used to learn continuousrepresentations of entities and relations. A key task in the literature ispredicting missing links between entities. However, Knowledge Graphs are notjust sets of links but also have semantics underlying their structure.Semantics is crucial in several downstream tasks, such as query answering orreasoning. We introduce the subgraph inference task, where a model has togenerate likely and semantically valid subgraphs. We propose IntelliGraphs, aset of five new Knowledge Graph datasets. The IntelliGraphs datasets containsubgraphs with semantics expressed in logical rules for evaluating subgraphinference. We also present the dataset generator that produced the syntheticdatasets. We designed four novel baseline models, which include three modelsbased on traditional KGEs. We evaluate their expressiveness and show that thesemodels cannot capture the semantics. We believe this benchmark will encouragethe development of machine learning models that emphasize semanticunderstanding.</description><author>Thiviyan Thanapalasingam, Emile van Krieken, Peter Bloem, Paul Groth</author><pubDate>Thu, 13 Jul 2023 12:54:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06698v1</guid></item><item><title>Ageing Analysis of Embedded SRAM on a Large-Scale Testbed Using Machine Learning</title><link>http://arxiv.org/abs/2307.06693v1</link><description>Ageing detection and failure prediction are essential in many Internet ofThings (IoT) deployments, which operate huge quantities of embedded devicesunattended in the field for years. In this paper, we present a large-scaleempirical analysis of natural SRAM wear-out using 154 boards from ageneral-purpose testbed. Starting from SRAM initialization bias, which eachnode can easily collect at startup, we apply various metrics for featureextraction and experiment with common machine learning methods to predict theage of operation for this node. Our findings indicate that even though ageingimpacts are subtle, our indicators can well estimate usage times with an $R^2$score of 0.77 and a mean error of 24% using regressors, and with an F1 scoreabove 0.6 for classifiers applying a six-months resolution.</description><author>Leandro Lanzieri, Peter Kietzmann, Goerschwin Fey, Holger Schlarb, Thomas C. Schmidt</author><pubDate>Thu, 13 Jul 2023 12:37:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06693v1</guid></item><item><title>YOLIC: An Efficient Method for Object Localization and Classification on Edge Devices</title><link>http://arxiv.org/abs/2307.06689v1</link><description>In the realm of Tiny AI, we introduce "You Only Look at Interested Cells"(YOLIC), an efficient method for object localization and classification on edgedevices. Seamlessly blending the strengths of semantic segmentation and objectdetection, YOLIC offers superior computational efficiency and precision. Byadopting Cells of Interest for classification instead of individual pixels,YOLIC encapsulates relevant information, reduces computational load, andenables rough object shape inference. Importantly, the need for bounding boxregression is obviated, as YOLIC capitalizes on the predetermined cellconfiguration that provides information about potential object location, size,and shape. To tackle the issue of single-label classification limitations, amulti-label classification approach is applied to each cell, effectivelyrecognizing overlapping or closely situated objects. This paper presentsextensive experiments on multiple datasets, demonstrating that YOLIC achievesdetection performance comparable to the state-of-the-art YOLO algorithms whilesurpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU. All resourcesrelated to this study, including datasets, cell designer, image annotationtool, and source code, have been made publicly available on our project websiteat https://kai3316.github.io/yolic.github.io</description><author>Kai Su, Qiangfu Zhao, Yoichi Tomioka, Yong Liu</author><pubDate>Thu, 13 Jul 2023 12:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06689v1</guid></item><item><title>Aeolus Ocean -- A simulation environment for the autonomous COLREG-compliant navigation of Unmanned Surface Vehicles using Deep Reinforcement Learning and Maritime Object Detection</title><link>http://arxiv.org/abs/2307.06688v1</link><description>Heading towards navigational autonomy in unmanned surface vehicles (USVs) inthe maritime sector can fundamentally lead towards safer waters as well asreduced operating costs, while also providing a range of exciting newcapabilities for oceanic research, exploration and monitoring. However,achieving such a goal is challenging. USV control systems must, safely andreliably, be able to adhere to the international regulations for preventingcollisions at sea (COLREGs) in encounters with other vessels as they navigateto a given waypoint while being affected by realistic weather conditions,either during the day or at night. To deal with the multitude of possiblescenarios, it is critical to have a virtual environment that is able toreplicate the realistic operating conditions USVs will encounter, before theycan be implemented in the real world. Such "digital twins" form the foundationsupon which Deep Reinforcement Learning (DRL) and Computer Vision (CV)algorithms can be used to develop and guide USV control systems. In this paperwe describe the novel development of a COLREG-compliant DRL-based collisionavoidant navigational system with CV-based awareness in a realistic oceansimulation environment. The performance of the trained autonomous Agentsresulting from this approach is evaluated in several successful navigations toset waypoints in both open sea and coastal encounters with other vessels. Abinary executable version of the simulator with trained agents is available athttps://github.com/aavek/Aeolus-Ocean</description><author>Andrew Alexander Vekinis, Stavros Perantonis</author><pubDate>Thu, 13 Jul 2023 12:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06688v1</guid></item><item><title>Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities</title><link>http://arxiv.org/abs/2307.06687v1</link><description>In recent years, ubiquitous semantic Metaverse has been studied torevolutionize immersive cyber-virtual experiences for augmented reality (AR)and virtual reality (VR) users, which leverages advanced semantic understandingand representation to enable seamless, context-aware interactions withinmixed-reality environments. This survey focuses on the intelligence andspatio-temporal characteristics of four fundamental system components inubiquitous semantic Metaverse, i.e., artificial intelligence (AI),spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),and semantic-enhanced digital twin (SDT). We thoroughly survey therepresentative techniques of the four fundamental system components that enableintelligent, personalized, and context-aware interactions with typical usecases of the ubiquitous semantic Metaverse, such as remote education, work andcollaboration, entertainment and socialization, healthcare, and e-commercemarketing. Furthermore, we outline the opportunities for constructing thefuture ubiquitous semantic Metaverse, including scalability andinteroperability, privacy and security, performance measurement andstandardization, as well as ethical considerations and responsible AI.Addressing those challenges is important for creating a robust, secure, andethically sound system environment that offers engaging immersive experiencesfor the users and AR/VR applications.</description><author>Kai Li, Billy Lau, Xin Yuan, Wei Ni, Mohsen Guizani, Chau Yuen</author><pubDate>Thu, 13 Jul 2023 12:14:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06687v1</guid></item><item><title>Convergence of Message Passing Graph Neural Networks with Generic Aggregation On Large Random Graphs</title><link>http://arxiv.org/abs/2304.11140v2</link><description>We study the convergence of message passing graph neural networks on randomgraph models to their continuous counterpart as the number of nodes tends toinfinity. Until now, this convergence was only known for architectures withaggregation functions in the form of normalized means, or, equivalently, of anapplication of classical operators like the adjacency matrix or the graphLaplacian. We extend such results to a large class of aggregation functions,that encompasses all classically used message passing graph neural networks,such as attention-based message passing, max convolutional message passing or(degree-normalized) convolutional message passing. Under mild assumptions, wegive non-asymptotic bounds with high probability to quantify this convergence.Our main result is based on the McDiarmid inequality. Interestingly, thisresult does not apply to the case where the aggregation is a coordinate-wisemaximum. We treat this case separately and obtain a different convergence rate.</description><author>Matthieu Cordonnier, Nicolas Keriven, Nicolas Tremblay, Samuel Vaiter</author><pubDate>Thu, 13 Jul 2023 12:14:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11140v2</guid></item><item><title>Explainable Artificial Intelligence driven mask design for self-supervised seismic denoising</title><link>http://arxiv.org/abs/2307.06682v1</link><description>The presence of coherent noise in seismic data leads to errors anduncertainties, and as such it is paramount to suppress noise as early andefficiently as possible. Self-supervised denoising circumvents the commonrequirement of deep learning procedures of having noisy-clean training pairs.However, self-supervised coherent noise suppression methods require extensiveknowledge of the noise statistics. We propose the use of explainable artificialintelligence approaches to see inside the black box that is the denoisingnetwork and use the gained knowledge to replace the need for any priorknowledge of the noise itself. This is achieved in practice by leveragingbias-free networks and the direct linear link between input and output providedby the associated Jacobian matrix; we show that a simple averaging of theJacobian contributions over a number of randomly selected input pixels,provides an indication of the most effective mask to suppress noise present inthe data. The proposed method therefore becomes a fully automated denoisingprocedure requiring no clean training labels or prior knowledge. Realisticsynthetic examples with noise signals of varying complexities, ranging fromsimple time-correlated noise to complex pseudo rig noise propagating at thevelocity of the ocean, are used to validate the proposed approach. Itsautomated nature is highlighted further by an application to two fielddatasets. Without any substantial pre-processing or any knowledge of theacquisition environment, the automatically identified blind-masks are shown toperform well in suppressing both trace-wise noise in common shot gathers fromthe Volve marine dataset and colored noise in post stack seismic images from aland seismic survey.</description><author>Claire Birnie, Matteo Ravasi</author><pubDate>Thu, 13 Jul 2023 12:02:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06682v1</guid></item><item><title>The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework</title><link>http://arxiv.org/abs/2307.05201v2</link><description>In the context of label-efficient learning on video data, the distillationmethod and the structural design of the teacher-student architecture have asignificant impact on knowledge distillation. However, the relationship betweenthese factors has been overlooked in previous research. To address this gap, wepropose a new weakly supervised learning framework for knowledge distillationin video classification that is designed to improve the efficiency and accuracyof the student model. Our approach leverages the concept of substage-basedlearning to distill knowledge based on the combination of student substages andthe correlation of corresponding substages. We also employ the progressivecascade training method to address the accuracy loss caused by the largecapacity gap between the teacher and the student. Additionally, we propose apseudo-label optimization strategy to improve the initial data label. Tooptimize the loss functions of different distillation substages during thetraining process, we introduce a new loss method based on feature distribution.We conduct extensive experiments on both real and simulated data sets,demonstrating that our proposed approach outperforms existing distillationmethods in terms of knowledge distillation for video classification tasks. Ourproposed substage-based distillation approach has the potential to informfuture research on label-efficient learning for video data.</description><author>Chao Wang, Zheng Tang</author><pubDate>Thu, 13 Jul 2023 11:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05201v2</guid></item><item><title>Efficient Bayesian Policy Reuse with a Scalable Observation Model in Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2204.07729v3</link><description>Bayesian policy reuse (BPR) is a general policy transfer framework forselecting a source policy from an offline library by inferring the task beliefbased on some observation signals and a trained observation model. In thispaper, we propose an improved BPR method to achieve more efficient policytransfer in deep reinforcement learning (DRL). First, most BPR algorithms usethe episodic return as the observation signal that contains limited informationand cannot be obtained until the end of an episode. Instead, we employ thestate transition sample, which is informative and instantaneous, as theobservation signal for faster and more accurate task inference. Second, BPRalgorithms usually require numerous samples to estimate the probabilitydistribution of the tabular-based observation model, which may be expensive andeven infeasible to learn and maintain, especially when using the statetransition sample as the signal. Hence, we propose a scalable observation modelbased on fitting state transition functions of source tasks from only a smallnumber of samples, which can generalize to any signals observed in the targettask. Moreover, we extend the offline-mode BPR to the continual learningsetting by expanding the scalable observation model in a plug-and-play fashion,which can avoid negative transfer when faced with new unknown tasks.Experimental results show that our method can consistently facilitate fasterand more efficient policy transfer.</description><author>Jinmei Liu, Zhi Wang, Chunlin Chen, Daoyi Dong</author><pubDate>Thu, 13 Jul 2023 11:33:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.07729v3</guid></item><item><title>Generalized Laplacian Regularized Framelet Graph Neural Networks</title><link>http://arxiv.org/abs/2210.15092v2</link><description>This paper introduces a novel Framelet Graph approach based on p-LaplacianGNN. The proposed two models, named p-Laplacian undecimated framelet graphconvolution (pL-UFG) and generalized p-Laplacian undecimated framelet graphconvolution (pL-fUFG) inherit the nature of p-Laplacian with the expressivepower of multi-resolution decomposition of graph signals. The empirical studyhighlights the excellent performance of the pL-UFG and pL-fUFG in differentgraph learning tasks including node classification and signal denoising.</description><author>Zhiqi Shao, Andi Han, Dai Shi, Andrey Vasnev, Junbin Gao</author><pubDate>Thu, 13 Jul 2023 11:30:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.15092v2</guid></item><item><title>DGCNet: An Efficient 3D-Densenet based on Dynamic Group Convolution for Hyperspectral Remote Sensing Image Classification</title><link>http://arxiv.org/abs/2307.06667v1</link><description>Deep neural networks face many problems in the field of hyperspectral imageclassification, lack of effective utilization of spatial spectral information,gradient disappearance and overfitting as the model depth increases. In orderto accelerate the deployment of the model on edge devices with strict latencyrequirements and limited computing power, we introduce a lightweight modelbased on the improved 3D-Densenet model and designs DGCNet. It improves thedisadvantage of group convolution. Referring to the idea of dynamic network,dynamic group convolution(DGC) is designed on 3d convolution kernel. DGCintroduces small feature selectors for each grouping to dynamically decidewhich part of the input channel to connect based on the activations of allinput channels. Multiple groups can capture different and complementary visualand semantic features of input images, allowing convolution neural network(CNN)to learn rich features. 3D convolution extracts high-dimensional and redundanthyperspectral data, and there is also a lot of redundant information betweenconvolution kernels. DGC module allows 3D-Densenet to select channelinformation with richer semantic features and discard inactive regions. The3D-CNN passing through the DGC module can be regarded as a pruned network. DGCnot only allows 3D-CNN to complete sufficient feature extraction, but alsotakes into account the requirements of speed and calculation amount. Theinference speed and accuracy have been improved, with outstanding performanceon the IN, Pavia and KSC datasets, ahead of the mainstream hyperspectral imageclassification methods.</description><author>Guandong Li</author><pubDate>Thu, 13 Jul 2023 11:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06667v1</guid></item><item><title>Transformer-based end-to-end classification of variable-length volumetric data</title><link>http://arxiv.org/abs/2307.06666v1</link><description>The automatic classification of 3D medical data is memory-intensive. Also,variations in the number of slices between samples is common. Naive solutionssuch as subsampling can solve these problems, but at the cost of potentiallyeliminating relevant diagnosis information. Transformers have shown promisingperformance for sequential data analysis. However, their application forlong-sequences is data, computationally, and memory demanding. In this paper,we propose an end-to-end Transformer-based framework that allows to classifyvolumetric data of variable length in an efficient fashion. Particularly, byrandomizing the input slice-wise resolution during training, we enhance thecapacity of the learnable positional embedding assigned to each volume slice.Consequently, the accumulated positional information in each positionalembedding can be generalized to the neighbouring slices, even for highresolution volumes at the test time. By doing so, the model will be more robustto variable volume length and amenable to different computational budgets. Weevaluated the proposed approach in retinal OCT volume classification andachieved 21.96% average improvement in balanced accuracy on a 9-classdiagnostic task, compared to state-of-the-art video transformers. Our findingsshow that varying the slice-wise resolution of the input during trainingresults in more informative volume representation as compared to training withfixed number of slices per volume. Our code is available at:https://github.com/marziehoghbaie/VLFAT.</description><author>Marzieh Oghbaie, Teresa Araujo, Taha Emre, Ursula Schmidt-Erfurth, Hrvoje Bogunovic</author><pubDate>Thu, 13 Jul 2023 11:19:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06666v1</guid></item><item><title>A Comprehensive Analysis of Blockchain Applications for Securing Computer Vision Systems</title><link>http://arxiv.org/abs/2307.06659v1</link><description>Blockchain (BC) and Computer Vision (CV) are the two emerging fields with thepotential to transform various sectors.The ability of BC can help in offeringdecentralized and secure data storage, while CV allows machines to learn andunderstand visual data. This integration of the two technologies holds massivepromise for developing innovative applications that can provide solutions tothe challenges in various sectors such as supply chain management, healthcare,smart cities, and defense. This review explores a comprehensive analysis of theintegration of BC and CV by examining their combination and potentialapplications. It also provides a detailed analysis of the fundamental conceptsof both technologies, highlighting their strengths and limitations. This paperalso explores current research efforts that make use of the benefits offered bythis combination. The effort includes how BC can be used as an added layer ofsecurity in CV systems and also ensure data integrity, enabling decentralizedimage and video analytics using BC. The challenges and open issues associatedwith this integration are also identified, and appropriate potential futuredirections are also proposed.</description><author>Ramalingam M, Chemmalar Selvi, Nancy Victor, Rajeswari Chengoden, Sweta Bhattacharya, Praveen Kumar Reddy Maddikunta, Duehee Lee, Md. Jalil Piran, Neelu Khare, Gokul Yendri, Thippa Reddy Gadekallu</author><pubDate>Thu, 13 Jul 2023 11:03:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06659v1</guid></item><item><title>PEGG-Net: Pixel-Wise Efficient Grasp Generation in Complex Scenes</title><link>http://arxiv.org/abs/2203.16301v3</link><description>Vision-based grasp estimation is an essential part of robotic manipulationtasks in the real world. Existing planar grasp estimation algorithms have beendemonstrated to work well in relatively simple scenes. But when it comes tocomplex scenes, such as cluttered scenes with messy backgrounds and movingobjects, the algorithms from previous works are prone to generate inaccurateand unstable grasping contact points. In this work, we first study the existingplanar grasp estimation algorithms and analyze the related challenges incomplex scenes. Secondly, we design a Pixel-wise Efficient Grasp GenerationNetwork (PEGG-Net) to tackle the problem of grasping in complex scenes.PEGG-Net can achieve improved state-of-the-art performance on the Cornelldataset (98.9%) and second-best performance on the Jacquard dataset (93.8%),outperforming other existing algorithms without the introduction of complexstructures. Thirdly, PEGG-Net could operate in a closed-loop manner for addedrobustness in dynamic environments using position-based visual servoing (PBVS).Finally, we conduct real-world experiments on static, dynamic, and clutteredobjects in different complex scenes. The results show that our proposed networkachieves a high success rate in grasping irregular objects, household objects,and workshop tools. To benefit the community, our trained model andsupplementary materials are available at https://github.com/HZWang96/PEGG-Net.</description><author>Haozhe Wang, Zhiyang Liu, Lei Zhou, Huan Yin, Marcelo H Ang Jr</author><pubDate>Thu, 13 Jul 2023 10:52:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.16301v3</guid></item><item><title>Operational Support Estimator Networks</title><link>http://arxiv.org/abs/2307.06065v2</link><description>In this work, we propose a novel approach called Operational SupportEstimator Networks (OSENs) for the support estimation task. Support Estimation(SE) is defined as finding the locations of non-zero elements in a sparsesignal. By its very nature, the mapping between the measurement and sparsesignal is a non-linear operation. Traditional support estimators rely oncomputationally expensive iterative signal recovery techniques to achieve suchnon-linearity. Contrary to the convolution layers, the proposed OSEN approachconsists of operational layers that can learn such complex non-linearitieswithout the need for deep networks. In this way, the performance of thenon-iterative support estimation is greatly improved. Moreover, the operationallayers comprise so-called generative \textit{super neurons} with non-localkernels. The kernel location for each neuron/feature map is optimized jointlyfor the SE task during the training. We evaluate the OSENs in three differentapplications: i. support estimation from Compressive Sensing (CS) measurements,ii. representation-based classification, and iii. learning-aided CSreconstruction where the output of OSENs is used as prior knowledge to the CSalgorithm for an enhanced reconstruction. Experimental results show that theproposed approach achieves computational efficiency and outperforms competingmethods, especially at low measurement rates by a significant margin. Thesoftware implementation is publicly shared athttps://github.com/meteahishali/OSEN.</description><author>Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Moncef Gabbouj</author><pubDate>Thu, 13 Jul 2023 10:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06065v2</guid></item><item><title>Prospective Learning: Principled Extrapolation to the Future</title><link>http://arxiv.org/abs/2201.07372v2</link><description>Learning is a process which can update decision rules, based on pastexperience, such that future performance improves. Traditionally, machinelearning is often evaluated under the assumption that the future will beidentical to the past in distribution or change adversarially. But theseassumptions can be either too optimistic or pessimistic for many problems inthe real world. Real world scenarios evolve over multiple spatiotemporal scaleswith partially predictable dynamics. Here we reformulate the learning problemto one that centers around this idea of dynamic futures that are partiallylearnable. We conjecture that certain sequences of tasks are notretrospectively learnable (in which the data distribution is fixed), but areprospectively learnable (in which distributions may be dynamic), suggestingthat prospective learning is more difficult in kind than retrospectivelearning. We argue that prospective learning more accurately characterizes manyreal world problems that (1) currently stymie existing artificial intelligencesolutions and/or (2) lack adequate explanations for how natural intelligencessolve them. Thus, studying prospective learning will lead to deeper insightsand solutions to currently vexing challenges in both natural and artificialintelligences.</description><author>Ashwin De Silva, Rahul Ramesh, Lyle Ungar, Marshall Hussain Shuler, Noah J. Cowan, Michael Platt, Chen Li, Leyla Isik, Seung-Eon Roh, Adam Charles, Archana Venkataraman, Brian Caffo, Javier J. How, Justus M Kebschull, John W. Krakauer, Maxim Bichuch, Kaleab Alemayehu Kinfu, Eva Yezerets, Dinesh Jayaraman, Jong M. Shin, Soledad Villar, Ian Phillips, Carey E. Priebe, Thomas Hartung, Michael I. Miller, Jayanta Dey, Ningyuan, Huang, Eric Eaton, Ralph Etienne-Cummings, Elizabeth L. Ogburn, Randal Burns, Onyema Osuagwu, Brett Mensh, Alysson R. Muotri, Julia Brown, Chris White, Weiwei Yang, Andrei A. Rusu, Timothy Verstynen, Konrad P. Kording, Pratik Chaudhari, Joshua T. Vogelstein</author><pubDate>Thu, 13 Jul 2023 10:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.07372v2</guid></item><item><title>balance -- a Python package for balancing biased data samples</title><link>http://arxiv.org/abs/2307.06024v2</link><description>Surveys are an important research tool, providing unique measurements onsubjective experiences such as sentiment and opinions that cannot be measuredby other means. However, because survey data is collected from a self-selectedgroup of participants, directly inferring insights from it to a population ofinterest, or training ML models on such data, can lead to erroneous estimatesor under-performing models. In this paper we present balance, an open-sourcePython package by Meta, offering a simple workflow for analyzing and adjustingbiased data samples with respect to a population of interest. The balance workflow includes three steps: understanding the initial bias inthe data relative to a target we would like to infer, adjusting the data tocorrect for the bias by producing weights for each unit in the sample based onpropensity scores, and evaluating the final biases and the variance inflationafter applying the fitted weights. The package provides a simple API that canbe used by researchers and data scientists from a wide range of fields on avariety of data. The paper provides the relevant context, methodologicalbackground, and presents the package's API.</description><author>Tal Sarig, Tal Galili, Roee Eilat</author><pubDate>Thu, 13 Jul 2023 10:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06024v2</guid></item><item><title>EfficientNet Algorithm for Classification of Different Types of Cancer</title><link>http://arxiv.org/abs/2304.08715v3</link><description>Accurate and efficient classification of different types of cancer iscritical for early detection and effective treatment. In this paper, we presentthe results of our experiments using the EfficientNet algorithm forclassification of brain tumor, breast cancer mammography, chest cancer, andskin cancer. We used publicly available datasets and preprocessed the images toensure consistency and comparability. Our experiments show that theEfficientNet algorithm achieved high accuracy, precision, recall, and F1 scoreson each of the cancer datasets, outperforming other state-of-the-art algorithmsin the literature. We also discuss the strengths and weaknesses of theEfficientNet algorithm and its potential applications in clinical practice. Ourresults suggest that the EfficientNet algorithm is well-suited forclassification of different types of cancer and can be used to improve theaccuracy and efficiency of cancer diagnosis.</description><author>Romario Sameh Samir</author><pubDate>Thu, 13 Jul 2023 10:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08715v3</guid></item><item><title>GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition</title><link>http://arxiv.org/abs/2306.07848v4</link><description>Contrastive learning based pretraining methods have recently exhibitedimpressive success in diverse fields. In this paper, we propose GEmo-CLAP, akind of efficient gender-attribute-enhanced contrastive language-audiopretraining (CLAP) model for speech emotion recognition. To be specific, wefirst build an effective emotion CLAP model Emo-CLAP for emotion recognition,utilizing various self-supervised learning based pre-trained models. Then,considering the importance of the gender attribute in speech emotion modeling,two GEmo-CLAP approaches are further proposed to integrate the emotion andgender information of speech signals, forming more reasonable objectives.Extensive experiments on the IEMOCAP corpus demonstrate that our proposed twoGEmo-CLAP approaches consistently outperform the baseline Emo-CLAP withdifferent pre-trained models, while also achieving superior recognitionperformance compared with other state-of-the-art methods.</description><author>Yu Pan, Yanni Hu, Yuguang Yang, Jixun Yao, Wen Fei, Lei Ma, Heng Lu</author><pubDate>Thu, 13 Jul 2023 10:28:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07848v4</guid></item></channel></rss>