<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 21 Aug 2023 06:00:38 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training</title><link>http://arxiv.org/abs/2308.09718v1</link><description>The rapid advancement of deep learning models often attributes to theirability to leverage massive training data. In contrast, such privilege has notyet fully benefited 3D deep learning, mainly due to the limited availability oflarge-scale 3D datasets. Merging multiple available data sources and lettingthem collaboratively train a single model is a potential solution. However, dueto the large domain gap between 3D point cloud datasets, such mixed supervisioncould adversely affect the model's performance and lead to degeneratedperformance (i.e., negative transfer) compared to single-dataset training. Inview of this challenge, we introduce Point Prompt Training (PPT), a novelframework for multi-dataset synergistic learning in the context of 3Drepresentation learning that supports multiple pre-training paradigms. Based onthis framework, we propose Prompt-driven Normalization, which adapts the modelto different datasets with domain-specific prompts and Language-guidedCategorical Alignment that decently unifies the multiple-dataset label spacesby leveraging the relationship between label text. Extensive experiments verifythat PPT can overcome the negative transfer associated with synergisticlearning and produce generalizable representations. Notably, it achievesstate-of-the-art performance on each dataset using a single weight-shared modelwith supervised multi-dataset training. Moreover, when served as a pre-trainingframework, it outperforms other pre-training approaches regardingrepresentation quality and attains remarkable state-of-the-art performanceacross over ten diverse downstream tasks spanning both indoor and outdoor 3Dscenarios.</description><author>Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, Hengshuang Zhao</author><pubDate>Fri, 18 Aug 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09718v1</guid></item><item><title>Smoothness Similarity Regularization for Few-Shot GAN Adaptation</title><link>http://arxiv.org/abs/2308.09717v1</link><description>The task of few-shot GAN adaptation aims to adapt a pre-trained GAN model toa small dataset with very few training images. While existing methods performwell when the dataset for pre-training is structurally similar to the targetdataset, the approaches suffer from training instabilities or memorizationissues when the objects in the two domains have a very different structure. Tomitigate this limitation, we propose a new smoothness similarity regularizationthat transfers the inherently learned smoothness of the pre-trained GAN to thefew-shot target domain even if the two domains are very different. We evaluateour approach by adapting an unconditional and a class-conditional GAN todiverse few-shot target domains. Our proposed method significantly outperformsprior few-shot GAN adaptation methods in the challenging case of structurallydissimilar source-target domains, while performing on par with the state of theart for similar source-target domains.</description><author>Vadim Sushko, Ruyu Wang, Juergen Gall</author><pubDate>Fri, 18 Aug 2023 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09717v1</guid></item><item><title>Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization</title><link>http://arxiv.org/abs/2308.09716v1</link><description>The task of lip synchronization (lip-sync) seeks to match the lips of humanfaces with different audio. It has various applications in the film industry aswell as for creating virtual avatars and for video conferencing. This is achallenging problem as one needs to simultaneously introduce detailed,realistic lip movements while preserving the identity, pose, emotions, andimage quality. Many of the previous methods trying to solve this problem sufferfrom image quality degradation due to a lack of complete contextualinformation. In this paper, we present Diff2Lip, an audio-conditioneddiffusion-based model which is able to do lip synchronization in-the-wild whilepreserving these qualities. We train our model on Voxceleb2, a video datasetcontaining in-the-wild talking face videos. Extensive studies show that ourmethod outperforms popular methods like Wav2Lip and PC-AVS in Fr\'echetinception distance (FID) metric and Mean Opinion Scores (MOS) of the users. Weshow results on both reconstruction (same audio-video inputs) as well as cross(different audio-video inputs) settings on Voxceleb2 and LRW datasets. Videoresults and code can be accessed from our project page (https://soumik-kanad.github.io/diff2lip ).</description><author>Soumik Mukhopadhyay, Saksham Suri, Ravi Teja Gadde, Abhinav Shrivastava</author><pubDate>Fri, 18 Aug 2023 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09716v1</guid></item><item><title>Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis</title><link>http://arxiv.org/abs/2308.09713v1</link><description>We present a method that simultaneously addresses the tasks of dynamic scenenovel-view synthesis and six degree-of-freedom (6-DOF) tracking of all densescene elements. We follow an analysis-by-synthesis framework, inspired byrecent work that models scenes as a collection of 3D Gaussians which areoptimized to reconstruct input images via differentiable rendering. To modeldynamic scenes, we allow Gaussians to move and rotate over time while enforcingthat they have persistent color, opacity, and size. By regularizing Gaussians'motion and rotation with local-rigidity constraints, we show that our Dynamic3D Gaussians correctly model the same area of physical space over time,including the rotation of that space. Dense 6-DOF tracking and dynamicreconstruction emerges naturally from persistent dynamic view synthesis,without requiring any correspondence or flow as input. We demonstrate a largenumber of downstream applications enabled by our representation, includingfirst-person view synthesis, dynamic compositional scene synthesis, and 4Dvideo editing.</description><author>Jonathon Luiten, Georgios Kopanas, Bastian Leibe, Deva Ramanan</author><pubDate>Fri, 18 Aug 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09713v1</guid></item><item><title>HumanLiff: Layer-wise 3D Human Generation with Diffusion Model</title><link>http://arxiv.org/abs/2308.09712v1</link><description>3D human generation from 2D images has achieved remarkable progress throughthe synergistic utilization of neural rendering and generative models. Existing3D human generative models mainly generate a clothed 3D human as anundetectable 3D model in a single pass, while rarely considering the layer-wisenature of a clothed human body, which often consists of the human body andvarious clothes such as underwear, outerwear, trousers, shoes, etc. In thiswork, we propose HumanLiff, the first layer-wise 3D human generative model witha unified diffusion process. Specifically, HumanLiff firstly generatesminimal-clothed humans, represented by tri-plane features, in a canonicalspace, and then progressively generates clothes in a layer-wise manner. In thisway, the 3D human generation is thus formulated as a sequence ofdiffusion-based 3D conditional generation. To reconstruct more fine-grained 3Dhumans with tri-plane representation, we propose a tri-plane shift operationthat splits each tri-plane into three sub-planes and shifts these sub-planes toenable feature grid subdivision. To further enhance the controllability of 3Dgeneration with 3D layered conditions, HumanLiff hierarchically fuses tri-planefeatures and 3D layered conditions to facilitate the 3D diffusion modellearning. Extensive experiments on two layer-wise 3D human datasets, SynBody(synthetic) and TightCap (real-world), validate that HumanLiff significantlyoutperforms state-of-the-art methods in layer-wise 3D human generation. Ourcode will be available at https://skhu101.github.io/HumanLiff.</description><author>Shoukang Hu, Fangzhou Hong, Tao Hu, Liang Pan, Haiyi Mei, Weiye Xiao, Lei Yang, Ziwei Liu</author><pubDate>Fri, 18 Aug 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09712v1</guid></item><item><title>Robust Monocular Depth Estimation under Challenging Conditions</title><link>http://arxiv.org/abs/2308.09711v1</link><description>While state-of-the-art monocular depth estimation approaches achieveimpressive results in ideal settings, they are highly unreliable underchallenging illumination and weather conditions, such as at nighttime or in thepresence of rain. In this paper, we uncover these safety-critical issues andtackle them with md4all: a simple and effective solution that works reliablyunder both adverse and ideal conditions, as well as for different types oflearning supervision. We achieve this by exploiting the efficacy of existingmethods under perfect settings. Therefore, we provide valid training signalsindependently of what is in the input. First, we generate a set of complexsamples corresponding to the normal training ones. Then, we train the model byguiding its self- or full-supervision by feeding the generated samples andcomputing the standard losses on the corresponding original images. Doing soenables a single model to recover information across diverse conditions withoutmodifications at inference time. Extensive experiments on two challengingpublic datasets, namely nuScenes and Oxford RobotCar, demonstrate theeffectiveness of our techniques, outperforming prior works by a large margin inboth standard and challenging conditions. Source code and data are availableat: https://md4all.github.io.</description><author>Stefano Gasperini, Nils Morbitzer, HyunJun Jung, Nassir Navab, Federico Tombari</author><pubDate>Fri, 18 Aug 2023 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09711v1</guid></item><item><title>SimDA: Simple Diffusion Adapter for Efficient Video Generation</title><link>http://arxiv.org/abs/2308.09710v1</link><description>The recent wave of AI-generated content has witnessed the great developmentand success of Text-to-Image (T2I) technologies. By contrast, Text-to-Video(T2V) still falls short of expectations though attracting increasing interests.Existing works either train from scratch or adapt large T2I model to videos,both of which are computation and resource expensive. In this work, we proposea Simple Diffusion Adapter (SimDA) that fine-tunes only 24M out of 1.1Bparameters of a strong T2I model, adapting it to video generation in aparameter-efficient way. In particular, we turn the T2I model for T2V bydesigning light-weight spatial and temporal adapters for transfer learning.Besides, we change the original spatial attention to the proposed Latent-ShiftAttention (LSA) for temporal consistency. With similar model architecture, wefurther train a video super-resolution model to generate high-definition(1024x1024) videos. In addition to T2V generation in the wild, SimDA could alsobe utilized in one-shot video editing with only 2 minutes tuning. Doing so, ourmethod could minimize the training effort with extremely few tunable parametersfor model adaptation.</description><author>Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Fri, 18 Aug 2023 18:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09710v1</guid></item><item><title>Neural-network quantum state study of the long-range antiferromagnetic Ising chain</title><link>http://arxiv.org/abs/2308.09709v1</link><description>We investigate quantum phase transitions in the transverse field Ising chainwith algebraically decaying long-range antiferromagnetic interactions by usingthe variational Monte Carlo method with the restricted Boltzmann machine beingemployed as a trial wave function ansatz. In the finite-size scaling analysiswith the order parameter and the second R\'enyi entropy, we find that thecentral charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$in contrast to the critical exponents staying very close to the short-range(SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting thepreviously proposed scenario of conformal invariance breakdown. To identify thethreshold of the Ising universality and the conformal symmetry, we perform twoadditional tests for the universal Binder ratio and the conformal field theory(CFT) description of the correlation function. It turns out that both indicatea noticeable deviation from the SR Ising class at $\alpha_\mathrm{LR} &lt; 2$.However, a closer look at the scaled correlation function for$\alpha_\mathrm{LR} \ge 2$ shows a gradual change from the asymptotic line ofthe CFT verified at $\alpha_\mathrm{LR} = 3$, providing a rough estimate of thethreshold being in the range of $2 \lesssim \alpha_\mathrm{LR} &lt; 3$.</description><author>Jicheol Kim, Dongkyu Kim, Dong-Hee Kim</author><pubDate>Fri, 18 Aug 2023 18:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09709v1</guid></item><item><title>Training with Product Digital Twins for AutoRetail Checkout</title><link>http://arxiv.org/abs/2308.09708v1</link><description>Automating the checkout process is important in smart retail, where userseffortlessly pass products by hand through a camera, triggering automaticproduct detection, tracking, and counting. In this emerging area, due to thelack of annotated training data, we introduce a dataset comprised of product 3Dmodels, which allows for fast, flexible, and large-scale training datageneration through graphic engine rendering. Within this context, we discern anintriguing facet, because of the user "hands-on" approach, bias in userbehavior leads to distinct patterns in the real checkout process. The existenceof such patterns would compromise training effectiveness if training data failto reflect the same. To address this user bias problem, we propose a trainingdata optimization framework, i.e., training with digital twins (DtTrain).Specifically, we leverage the product 3D models and optimize their renderingviewpoint and illumination to generate "digital twins" that visually resemblerepresentative user images. These digital twins, inherit product labels and,when augmented, form the Digital Twin training set (DT set). Because thedigital twins individually mimic user bias, the resulting DT training setbetter reflects the characteristics of the target scenario and allows us totrain more effective product detection and tracking models. In our experiment,we show that DT set outperforms training sets created by existing datasetsynthesis methods in terms of counting accuracy. Moreover, by combining DT setwith pseudo-labeled real checkout data, further improvement is observed. Thecode is available at https://github.com/yorkeyao/Automated-Retail-Checkout.</description><author>Yue Yao, Xinyu Tian, Zheng Tang, Sujit Biswas, Huan Lei, Tom Gedeon, Liang Zheng</author><pubDate>Fri, 18 Aug 2023 18:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09708v1</guid></item><item><title>Segmenting Known Objects and Unseen Unknowns without Prior Knowledge</title><link>http://arxiv.org/abs/2209.05407v4</link><description>Panoptic segmentation methods assign a known class to each pixel given ininput. Even for state-of-the-art approaches, this inevitably enforces decisionsthat systematically lead to wrong predictions for objects outside the trainingcategories. However, robustness against out-of-distribution samples and cornercases is crucial in safety-critical settings to avoid dangerous consequences.Since real-world datasets cannot contain enough data points to adequatelysample the long tail of the underlying distribution, models must be able todeal with unseen and unknown scenarios as well. Previous methods targeted thisby re-identifying already-seen unlabeled objects. In this work, we propose thenecessary step to extend segmentation with a new setting which we term holisticsegmentation. Holistic segmentation aims to identify and separate objects ofunseen, unknown categories into instances without any prior knowledge aboutthem while performing panoptic segmentation of known classes. We tackle thisnew problem with U3HS, which finds unknowns as highly uncertain regions andclusters their corresponding instance-aware embeddings into individual objects.By doing so, for the first time in panoptic segmentation with unknown objects,our U3HS is trained without unknown categories, reducing assumptions andleaving the settings as unconstrained as in real-life scenarios. Extensiveexperiments on public data from MS COCO, Cityscapes, and Lost&amp;Found demonstratethe effectiveness of U3HS for this new, challenging, and assumptions-freesetting called holistic segmentation. Project page:https://holisticseg.github.io.</description><author>Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Nassir Navab, Benjamin Busam, Federico Tombari</author><pubDate>Fri, 18 Aug 2023 18:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05407v4</guid></item><item><title>Learnt Contrastive Concept Embeddings for Sign Recognition</title><link>http://arxiv.org/abs/2308.09515v1</link><description>In natural language processing (NLP) of spoken languages, word embeddingshave been shown to be a useful method to encode the meaning of words. Signlanguages are visual languages, which require sign embeddings to capture thevisual and linguistic semantics of sign. Unlike many common approaches to SignRecognition, we focus on explicitly creating sign embeddings that bridge thegap between sign language and spoken language. We propose a learning frameworkto derive LCC (Learnt Contrastive Concept) embeddings for sign language, aweakly supervised contrastive approach to learning sign embeddings. We train avocabulary of embeddings that are based on the linguistic labels for signvideo. Additionally, we develop a conceptual similarity loss which is able toutilise word embeddings from NLP methods to create sign embeddings that havebetter sign language to spoken language correspondence. These learntrepresentations allow the model to automatically localise the sign in time. Ourapproach achieves state-of-the-art keypoint-based sign recognition performanceon the WLASL and BOBSL datasets.</description><author>Ryan Wong, Necati Cihan Camgoz, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 13:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09515v1</guid></item><item><title>Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning</title><link>http://arxiv.org/abs/2308.09514v1</link><description>We present Spatial LibriSpeech, a spatial audio dataset with over 650 hoursof 19-channel audio, first-order ambisonics, and optional distractor noise.Spatial LibriSpeech is designed for machine learning model training, and itincludes labels for source position, speaking direction, room acoustics andgeometry. Spatial LibriSpeech is generated by augmenting LibriSpeech sampleswith 200k+ simulated acoustic conditions across 8k+ synthetic rooms. Todemonstrate the utility of our dataset, we train models on four spatial audiotasks, resulting in a median absolute error of 6.60{\deg} on 3D sourcelocalization, 0.43m on distance, 90.66ms on T30, and 2.74dB on DRR estimation.We show that the same models generalize well to widely-used evaluationdatasets, e.g., obtaining a median absolute error of 12.43{\deg} on 3D sourcelocalization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACEChallenge.</description><author>Miguel Sarabia, Elena Menyaylenko, Alessandro Toso, Skyler Seto, Zakaria Aldeneh, Shadi Pirhosseinloo, Luca Zappella, Barry-John Theobald, Nicholas Apostoloff, Jonathan Sheaffer</author><pubDate>Fri, 18 Aug 2023 13:45:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09514v1</guid></item><item><title>ResQ: Residual Quantization for Video Perception</title><link>http://arxiv.org/abs/2308.09511v1</link><description>This paper accelerates video perception, such as semantic segmentation andhuman pose estimation, by levering cross-frame redundancies. Unlike theexisting approaches, which avoid redundant computations by warping the pastfeatures using optical-flow or by performing sparse convolutions on framedifferences, we approach the problem from a new perspective: low-bitquantization. We observe that residuals, as the difference in networkactivations between two neighboring frames, exhibit properties that make themhighly quantizable. Based on this observation, we propose a novel quantizationscheme for video networks coined as Residual Quantization. ResQ extends thestandard, frame-by-frame, quantization scheme by incorporating temporaldependencies that lead to better performance in terms of accuracy vs.bit-width. Furthermore, we extend our model to dynamically adjust the bit-widthproportional to the amount of changes in the video. We demonstrate thesuperiority of our model, against the standard quantization and existingefficient video perception models, using various architectures on semanticsegmentation and human pose estimation benchmarks.</description><author>Davide Abati, Haitam Ben Yahia, Markus Nagel, Amirhossein Habibian</author><pubDate>Fri, 18 Aug 2023 13:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09511v1</guid></item><item><title>LVOS: A Benchmark for Long-term Video Object Segmentation</title><link>http://arxiv.org/abs/2211.10181v2</link><description>Existing video object segmentation (VOS) benchmarks focus on short-termvideos which just last about 3-5 seconds and where objects are visible most ofthe time. These videos are poorly representative of practical applications, andthe absence of long-term datasets restricts further investigation of VOS on theapplication in realistic scenarios. So, in this paper, we present a newbenchmark dataset named \textbf{LVOS}, which consists of 220 videos with atotal duration of 421 minutes. To the best of our knowledge, LVOS is the firstdensely annotated long-term VOS dataset. The videos in our LVOS last 1.59minutes on average, which is 20 times longer than videos in existing VOSdatasets. Each video includes various attributes, especially challengesderiving from the wild, such as long-term reappearing and cross-temporalsimilar objeccts.Based on LVOS, we assess existing video object segmentationalgorithms and propose a Diverse Dynamic Memory network (DDMemory) thatconsists of three complementary memory banks to exploit temporal informationadequately. The experimental results demonstrate the strength and weaknesses ofprior methods, pointing promising directions for further study. Data and codeare available at https://lingyihongfd.github.io/lvos.github.io/.</description><author>Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, Wenqiang Zhang</author><pubDate>Fri, 18 Aug 2023 13:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10181v2</guid></item><item><title>WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset</title><link>http://arxiv.org/abs/2308.03582v2</link><description>A fundamental challenge in the current NLP context, dominated by languagemodels, comes from the inflexibility of current architectures to 'learn' newinformation. While model-centric solutions like continual learning orparameter-efficient fine tuning are available, the question still remains ofhow to reliably identify changes in language or in the world. In this paper, wepropose WikiTiDe, a dataset derived from pairs of timestamped definitionsextracted from Wikipedia. We argue that such resource can be helpful foraccelerating diachronic NLP, specifically, for training models able to scanknowledge resources for core updates concerning a concept, an event, or a namedentity. Our proposed end-to-end method is fully automatic, and leverages abootstrapping algorithm for gradually creating a high-quality dataset. Ourresults suggest that bootstrapping the seed version of WikiTiDe leads to betterfine-tuned models. We also leverage fine-tuned models in a number of downstreamtasks, showing promising results with respect to competitive baselines.</description><author>Hsuvas Borkakoty, Luis Espinosa-Anke</author><pubDate>Fri, 18 Aug 2023 13:31:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03582v2</guid></item><item><title>Importance of Aligning Training Strategy with Evaluation for Diffusion Models in 3D Multiclass Segmentation</title><link>http://arxiv.org/abs/2303.06040v3</link><description>Recently, denoising diffusion probabilistic models (DDPM) have been appliedto image segmentation by generating segmentation masks conditioned on images,while the applications were mainly limited to 2D networks without exploitingpotential benefits from the 3D formulation. In this work, we studied theDDPM-based segmentation model for 3D multiclass segmentation on two largemulticlass data sets (prostate MR and abdominal CT). We observed that thedifference between training and test methods led to inferior performance forexisting DDPM methods. To mitigate the inconsistency, we proposed a recyclingmethod which generated corrupted masks based on the model's prediction at aprevious time step instead of using ground truth. The proposed method achievedstatistically significantly improved performance compared to existing DDPMs,independent of a number of other techniques for reducing train-testdiscrepancy, including performing mask prediction, using Dice loss, andreducing the number of diffusion time steps during training. The performance ofdiffusion models was also competitive and visually similar tonon-diffusion-based U-net, within the same compute budget. The JAX-baseddiffusion framework has been released athttps://github.com/mathpluscode/ImgX-DiffSeg.</description><author>Yunguan Fu, Yiwen Li, Shaheer U. Saeed, Matthew J. Clarkson, Yipeng Hu</author><pubDate>Fri, 18 Aug 2023 13:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06040v3</guid></item><item><title>Object Detection in Hyperspectral Image via Unified Spectral-Spatial Feature Aggregation</title><link>http://arxiv.org/abs/2306.08370v2</link><description>Deep learning-based hyperspectral image (HSI) classification and objectdetection techniques have gained significant attention due to their vital rolein image content analysis, interpretation, and wider HSI applications. However,current hyperspectral object detection approaches predominantly emphasizeeither spectral or spatial information, overlooking the valuable complementaryrelationship between these two aspects. In this study, we present a novel\textbf{S}pectral-\textbf{S}patial \textbf{A}ggregation (S2ADet) objectdetector that effectively harnesses the rich spectral and spatial complementaryinformation inherent in hyperspectral images. S2ADet comprises a hyperspectralinformation decoupling (HID) module, a two-stream feature extraction network,and a one-stage detection head. The HID module processes hyperspectral imagesby aggregating spectral and spatial information via band selection andprincipal components analysis, consequently reducing redundancy. Based on theacquired spatial and spectral aggregation information, we propose a featureaggregation two-stream network for interacting spectral-spatial features.Furthermore, to address the limitations of existing databases, we annotate anextensive dataset, designated as HOD3K, containing 3,242 hyperspectral imagescaptured across diverse real-world scenes and encompassing three objectclasses. These images possess a resolution of 512x256 pixels and cover 16 bandsranging from 470 nm to 620 nm. Comprehensive experiments on two datasetsdemonstrate that S2ADet surpasses existing state-of-the-art methods, achievingrobust and reliable results. The demo code and dataset of this work arepublicly available at \url{https://github.com/hexiao-cs/S2ADet}.</description><author>Xiao He, Chang Tang, Xinwang Liu, Wei Zhang, Kun Sun, Jiangfeng Xu</author><pubDate>Fri, 18 Aug 2023 13:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08370v2</guid></item><item><title>Semantic relatedness in DBpedia: A comparative and experimental assessment</title><link>http://arxiv.org/abs/2308.09502v1</link><description>Evaluating semantic relatedness of Web resources is still an open challenge.This paper focuses on knowledge-based methods, which represent an alternativeto corpus-based approaches, and rely in general on the availability ofknowledge graphs. In particular, we have selected 10 methods from the existingliterature, that have been organized according to it adjacent resources, triplepatterns, and triple weights-based methods. They have been implemented andevaluated by using DBpedia as reference RDF knowledge graph. Since DBpedia iscontinuously evolving, the experimental results provided by these methods inthe literature are not comparable. For this reason, in this work, such methodshave been experimented by running them all at once on the same DBpedia releaseand against 14 well-known golden datasets. On the basis of the correlationvalues with human judgment obtained according to the experimental results,weighting the RDF triples in combination with evaluating all the directed pathslinking the compared resources is the best strategy in order to computesemantic relatedness in DBpedia.</description><author>Anna Formica, Francesco Taglino</author><pubDate>Fri, 18 Aug 2023 13:26:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09502v1</guid></item><item><title>GP-PCS: One-shot Feature-Preserving Point Cloud Simplification with Gaussian Processes on Riemannian Manifolds</title><link>http://arxiv.org/abs/2303.15225v2</link><description>The processing, storage and transmission of large-scale point clouds is anongoing challenge in the computer vision community which hinders progress inthe application of 3D models to real-world settings, such as autonomousdriving, virtual reality and remote sensing. We propose a novel, one-shot pointcloud simplification method which preserves both the salient structuralfeatures and the overall shape of a point cloud without any prior surfacereconstruction step. Our method employs Gaussian processes suitable forfunctions defined on Riemannian manifolds, allowing us to model the surfacevariation function across any given point cloud. A simplified version of theoriginal cloud is obtained by sequentially selecting points using a greedysparsification scheme. The selection criterion used for this scheme ensuresthat the simplified cloud best represents the surface variation of the originalpoint cloud. We evaluate our method on several benchmark and self-acquiredpoint clouds, compare it to a range of existing methods, demonstrate itsapplication in downstream tasks of registration and surface reconstruction, andshow that our method is competitive both in terms of empirical performance andcomputational efficiency.</description><author>Stuti Pathak, Thomas M. McDonald, Rudi Penne</author><pubDate>Fri, 18 Aug 2023 13:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15225v2</guid></item><item><title>Bridged-GNN: Knowledge Bridge Learning for Effective Knowledge Transfer</title><link>http://arxiv.org/abs/2308.09499v1</link><description>The data-hungry problem, characterized by insufficiency and low-quality ofdata, poses obstacles for deep learning models. Transfer learning has been afeasible way to transfer knowledge from high-quality external data of sourcedomains to limited data of target domains, which follows a domain-levelknowledge transfer to learn a shared posterior distribution. However, they areusually built on strong assumptions, e.g., the domain invariant posteriordistribution, which is usually unsatisfied and may introduce noises, resultingin poor generalization ability on target domains. Inspired by Graph NeuralNetworks (GNNs) that aggregate information from neighboring nodes, we redefinethe paradigm as learning a knowledge-enhanced posterior distribution for targetdomains, namely Knowledge Bridge Learning (KBL). KBL first learns the scope ofknowledge transfer by constructing a Bridged-Graph that connects knowledgeablesamples to each target sample and then performs sample-wise knowledge transfervia GNNs.KBL is free from strong assumptions and is robust to noises in thesource data. Guided by KBL, we propose the Bridged-GNN} including an AdaptiveKnowledge Retrieval module to build Bridged-Graph and a Graph KnowledgeTransfer module. Comprehensive experiments on both un-relational and relationaldata-hungry scenarios demonstrate the significant improvements of Bridged-GNNcompared with SOTA methods</description><author>Wendong Bi, Xueqi Cheng, Bingbing Xu, Xiaoqian Sun, Li Xu, Huawei Shen</author><pubDate>Fri, 18 Aug 2023 13:14:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09499v1</guid></item><item><title>Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication</title><link>http://arxiv.org/abs/2308.09497v1</link><description>Individuals with complex communication needs (CCN) often rely on augmentativeand alternative communication (AAC) systems to have conversations andcommunique their wants. Such systems allow message authoring by arrangingpictograms in sequence. However, the difficulty of finding the desired item tocomplete a sentence can increase as the user's vocabulary increases. This paperproposes using BERTimbau, a Brazilian Portuguese version of BERT, for pictogramprediction in AAC systems. To finetune BERTimbau, we constructed an AAC corpusfor Brazilian Portuguese to use as a training corpus. We tested differentapproaches to representing a pictogram for prediction: as a word (usingpictogram captions), as a concept (using a dictionary definition), and as a setof synonyms (using related terms). We also evaluated the usage of images forpictogram prediction. The results demonstrate that using embeddings computedfrom the pictograms' caption, synonyms, or definitions have a similarperformance. Using synonyms leads to lower perplexity, but using captions leadsto the highest accuracies. This paper provides insight into how to represent apictogram for prediction using a BERT-like model and the potential of usingimages for pictogram prediction.</description><author>Jayr Pereira, Rodrigo Nogueira, Cleber Zanchettin, Robson Fidalgo</author><pubDate>Fri, 18 Aug 2023 13:14:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09497v1</guid></item><item><title>Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models</title><link>http://arxiv.org/abs/2308.09490v1</link><description>The field of artificial intelligence (AI) has experienced remarkable progressin recent years, driven by the widespread adoption of open-source machinelearning models in both research and industry. Considering theresource-intensive nature of training on vast datasets, many applications optfor models that have already been trained. Hence, a small number of key playersundertake the responsibility of training and publicly releasing largepre-trained models, providing a crucial foundation for a wide range ofapplications. However, the adoption of these open-source models carriesinherent privacy and security risks that are often overlooked. To provide aconcrete example, an inconspicuous model may conceal hidden functionalitiesthat, when triggered by specific input patterns, can manipulate the behavior ofthe system, such as instructing self-driving cars to ignore the presence ofother vehicles. The implications of successful privacy and security attacksencompass a broad spectrum, ranging from relatively minor damage like serviceinterruptions to highly alarming scenarios, including physical harm or theexposure of sensitive user data. In this work, we present a comprehensiveoverview of common privacy and security threats associated with the use ofopen-source models. By raising awareness of these dangers, we strive to promotethe responsible and secure use of AI systems.</description><author>Dominik Hintersdorf, Lukas Struppek, Kristian Kersting</author><pubDate>Fri, 18 Aug 2023 12:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09490v1</guid></item><item><title>Modelling Electricity Consumption in Irish Dairy Farms Using Agent-Based Modelling</title><link>http://arxiv.org/abs/2308.09488v1</link><description>Dairy farming can be an energy intensive form of farming. Understanding thefactors affecting electricity consumption on dairy farms is crucial for farmowners and energy providers. In order to accurately estimate electricitydemands in dairy farms, it is necessary to develop a model. In this researchpaper, an agent-based model is proposed to model the electricity consumption ofIrish dairy farms. The model takes into account various factors that affect theenergy consumption of dairy farms, including herd size, number of milkingmachines, and time of year. The outputs are validated using existingstate-of-the-art dairy farm modelling frameworks. The proposed agent-basedmodel is fully explainable, which is an advantage over other ArtificialIntelligence techniques, e.g. deep learning.</description><author>Hossein Khaleghy, Abdul Wahid, Eoghan Clifford, Karl Mason</author><pubDate>Fri, 18 Aug 2023 12:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09488v1</guid></item><item><title>PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification</title><link>http://arxiv.org/abs/2307.09066v2</link><description>Multi-label image classification is a prediction task that aims to identifymore than one label from a given image. This paper considers the semanticconsistency of the latent space between the visual patch and linguistic labeldomains and introduces the conditional transport (CT) theory to bridge theacknowledged gap. While recent cross-modal attention-based studies haveattempted to align such two representations and achieved impressiveperformance, they required carefully-designed alignment modules and extracomplex operations in the attention computation. We find that by formulatingthe multi-label classification as a CT problem, we can exploit the interactionsbetween the image and label efficiently by minimizing the bidirectional CTcost. Specifically, after feeding the images and textual labels into themodality-specific encoders, we view each image as a mixture of patch embeddingsand a mixture of label embeddings, which capture the local region features andthe class prototypes, respectively. CT is then employed to learn and alignthose two semantic sets by defining the forward and backward navigators.Importantly, the defined navigators in CT distance model the similaritiesbetween patches and labels, which provides an interpretable tool to visualizethe learned prototypes. Extensive experiments on three public image benchmarksshow that the proposed model consistently outperforms the previous methods.</description><author>Miaoge Li, Dongsheng Wang, Xinyang Liu, Zequn Zeng, Ruiying Lu, Bo Chen, Mingyuan Zhou</author><pubDate>Fri, 18 Aug 2023 12:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09066v2</guid></item><item><title>Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data</title><link>http://arxiv.org/abs/2308.09487v1</link><description>To successfully launch backdoor attacks, injected data needs to be correctlylabeled; otherwise, they can be easily detected by even basic data filters.Hence, the concept of clean-label attacks was introduced, which is moredangerous as it doesn't require changing the labels of injected data. To thebest of our knowledge, the existing clean-label backdoor attacks largely relieson an understanding of the entire training set or a portion of it. However, inpractice, it is very difficult for attackers to have it because of trainingdatasets often collected from multiple independent sources. Unlike all currentclean-label attacks, we propose a novel clean label method called 'Poison DartFrog'. Poison Dart Frog does not require access to any training data; it onlynecessitates knowledge of the target class for the attack, such as 'frog'. OnCIFAR10, Tiny-ImageNet, and TSRD, with a mere 0.1\%, 0.025\%, and 0.4\%poisoning rate of the training set size, respectively, Poison Dart Frogachieves a high Attack Success Rate compared to LC, HTBA, BadNets, and Blend.Furthermore, compared to the state-of-the-art attack, NARCISSUS, Poison DartFrog achieves similar attack success rates without any training data. Finally,we demonstrate that four typical backdoor defense algorithms struggle tocounter Poison Dart Frog.</description><author>Binhao Ma, Jiahui Wang, Dejun Wang, Bo Meng</author><pubDate>Fri, 18 Aug 2023 12:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09487v1</guid></item><item><title>Robust Evaluation of Diffusion-Based Adversarial Purification</title><link>http://arxiv.org/abs/2303.09051v2</link><description>We question the current evaluation practice on diffusion-based purificationmethods. Diffusion-based purification methods aim to remove adversarial effectsfrom an input data point at test time. The approach gains increasing attentionas an alternative to adversarial training due to the disentangling betweentraining and testing. Well-known white-box attacks are often employed tomeasure the robustness of the purification. However, it is unknown whetherthese attacks are the most effective for the diffusion-based purification sincethe attacks are often tailored for adversarial training. We analyze the currentpractices and provide a new guideline for measuring the robustness ofpurification methods against adversarial attacks. Based on our analysis, wefurther propose a new purification strategy improving robustness compared tothe current diffusion-based purification methods.</description><author>Minjong Lee, Dongwoo Kim</author><pubDate>Fri, 18 Aug 2023 12:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09051v2</guid></item><item><title>NASimEmu: Network Attack Simulator &amp; Emulator for Training Agents Generalizing to Novel Scenarios</title><link>http://arxiv.org/abs/2305.17246v2</link><description>Current frameworks for training offensive penetration testing agents withdeep reinforcement learning struggle to produce agents that perform well inreal-world scenarios, due to the reality gap in simulation-based frameworks andthe lack of scalability in emulation-based frameworks. Additionally, existingframeworks often use an unrealistic metric that measures the agents'performance on the training data. NASimEmu, a new framework introduced in thispaper, addresses these issues by providing both a simulator and an emulatorwith a shared interface. This approach allows agents to be trained insimulation and deployed in the emulator, thus verifying the realism of the usedabstraction. Our framework promotes the development of general agents that cantransfer to novel scenarios unseen during their training. For the simulationpart, we adopt an existing simulator NASim and enhance its realism. Theemulator is implemented with industry-level tools, such as Vagrant, VirtualBox,and Metasploit. Experiments demonstrate that a simulation-trained agent can bedeployed in emulation, and we show how to use the framework to train a generalagent that transfers into novel, structurally different scenarios. NASimEmu isavailable as open-source.</description><author>Jaromír Janisch, Tomáš Pevný, Viliam Lisý</author><pubDate>Fri, 18 Aug 2023 12:32:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17246v2</guid></item><item><title>Perceptions and Realities of Text-to-Image Generation</title><link>http://arxiv.org/abs/2306.08363v2</link><description>Generative artificial intelligence (AI) is a widely popular technology thatwill have a profound impact on society and individuals. Less than a decade ago,it was thought that creative work would be among the last to be automated - yettoday, we see AI encroaching on many creative domains. In this paper, wepresent the findings of a survey study on people's perceptions of text-to-imagegeneration. We touch on participants' technical understanding of the emergingtechnology, their fears and concerns, and thoughts about risks and dangers oftext-to-image generation to the individual and society. We find that whileparticipants were aware of the risks and dangers associated with thetechnology, only few participants considered the technology to be a personalrisk. The risks for others were more easy to recognize for participants.Artists were particularly seen at risk. Interestingly, participants who hadtried the technology rated its future importance lower than those who had nottried it. This result shows that many people are still oblivious of thepotential personal risks of generative artificial intelligence and theimpending societal changes associated with this technology.</description><author>Jonas Oppenlaender, Johanna Silvennoinen, Ville Paananen, Aku Visuri</author><pubDate>Fri, 18 Aug 2023 12:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08363v2</guid></item><item><title>Video-Instrument Synergistic Network for Referring Video Instrument Segmentation in Robotic Surgery</title><link>http://arxiv.org/abs/2308.09475v1</link><description>Robot-assisted surgery has made significant progress, with instrumentsegmentation being a critical factor in surgical intervention quality. Itserves as the building block to facilitate surgical robot navigation andsurgical education for the next generation of operating intelligence. Althoughexisting methods have achieved accurate instrument segmentation results, theysimultaneously generate segmentation masks for all instruments, without thecapability to specify a target object and allow an interactive experience. Thiswork explores a new task of Referring Surgical Video Instrument Segmentation(RSVIS), which aims to automatically identify and segment the correspondingsurgical instruments based on the given language expression. To achieve this,we devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn bothvideo-level and instrument-level knowledge to boost performance, while previouswork only used video-level information. Meanwhile, we design a Graph-basedRelation-aware Module (GRM) to model the correlation between multi-modalinformation (i.e., textual description and video frame) to facilitate theextraction of instrument-level information. We are also the first to producetwo RSVIS datasets to promote related research. Our method is verified on thesedatasets, and experimental results exhibit that the VIS-Net can significantlyoutperform existing state-of-the-art referring segmentation methods. Our codeand our datasets will be released upon the publication of this work.</description><author>Hongqiu Wang, Lei Zhu, Guang Yang, Yike Guo, Shichen Zhang, Bo Xu, Yueming Jin</author><pubDate>Fri, 18 Aug 2023 12:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09475v1</guid></item><item><title>AI Hilbert: From Data and Background Knowledge to Automated Scientific Discovery</title><link>http://arxiv.org/abs/2308.09474v1</link><description>The discovery of scientific formulae that parsimoniously explain naturalphenomena and align with existing background theory is a key goal in science.Historically, scientists have derived natural laws by manipulating equationsbased on existing knowledge, forming new equations, and verifying themexperimentally. In recent years, data-driven scientific discovery has emergedas a viable competitor in settings with large amounts of experimental data.Unfortunately, data-driven methods often fail to discover valid laws when datais noisy or scarce. Accordingly, recent works combine regression and reasoningto eliminate formulae inconsistent with background theory. However, the problemof searching over the space of formulae consistent with background theory tofind one that fits the data best is not well solved. We propose a solution tothis problem when all axioms and scientific laws are expressible via polynomialequalities and inequalities and argue that our approach is widely applicable.We further model notions of minimal complexity using binary variables andlogical constraints, solve polynomial optimization problems via mixed-integerlinear or semidefinite optimization, and automatically prove the validity ofour scientific discoveries via Positivestellensatz certificates. Remarkably,the optimization techniques leveraged in this paper allow our approach to runin polynomial time with fully correct background theory, or non-deterministicpolynomial (NP) time with partially correct background theory. Weexperimentally demonstrate that some famous scientific laws, including Kepler'sThird Law of Planetary Motion, the Hagen-Poiseuille Equation, and the RadiatedGravitational Wave Power equation, can be automatically derived from sets ofpartially correct background axioms.</description><author>Ryan Cory-Wright, Bachir El Khadir, Cristina Cornelio, Sanjeeb Dash, Lior Horesh</author><pubDate>Fri, 18 Aug 2023 12:19:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09474v1</guid></item><item><title>Vision Relation Transformer for Unbiased Scene Graph Generation</title><link>http://arxiv.org/abs/2308.09472v1</link><description>Recent years have seen a growing interest in Scene Graph Generation (SGG), acomprehensive visual scene understanding task that aims to predict entityrelationships using a relation encoder-decoder pipeline stacked on top of anobject encoder-decoder backbone. Unfortunately, current SGG methods suffer froman information loss regarding the entities local-level cues during the relationencoding process. To mitigate this, we introduce the Vision rElationTransfOrmer (VETO), consisting of a novel local-level entity relation encoder.We further observe that many existing SGG methods claim to be unbiased, but arestill biased towards either head or tail classes. To overcome this bias, weintroduce a Mutually Exclusive ExperT (MEET) learning strategy that capturesimportant relation features without bias towards head or tail classes.Experimental results on the VG and GQA datasets demonstrate that VETO + MEETboosts the predictive performance by up to 47 percentage over the state of theart while being 10 times smaller.</description><author>Gopika Sudhakaran, Devendra Singh Dhami, Kristian Kersting, Stefan Roth</author><pubDate>Fri, 18 Aug 2023 12:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09472v1</guid></item><item><title>Quantitative Susceptibility Mapping through Model-based Deep Image Prior (MoDIP)</title><link>http://arxiv.org/abs/2308.09467v1</link><description>The data-driven approach of supervised learning methods has limitedapplicability in solving dipole inversion in Quantitative SusceptibilityMapping (QSM) with varying scan parameters across different objects. To addressthis generalization issue in supervised QSM methods, we propose a noveltraining-free model-based unsupervised method called MoDIP (Model-based DeepImage Prior). MoDIP comprises a small, untrained network and a Data FidelityOptimization (DFO) module. The network converges to an interim state, acting asan implicit prior for image regularization, while the optimization processenforces the physical model of QSM dipole inversion. Experimental resultsdemonstrate MoDIP's excellent generalizability in solving QSM dipole inversionacross different scan parameters. It exhibits robustness against pathologicalbrain QSM, achieving over 32% accuracy improvement than supervised deeplearning and traditional iterative methods. It is also 33% more computationallyefficient and runs 4 times faster than conventional DIP-based approaches,enabling 3D high-resolution image reconstruction in under 4.5 minutes.</description><author>Zhuang Xiong, Yang Gao, Yin Liu, Amir Fazlollahi, Peter Nestor, Feng Liu, Hongfu Sun</author><pubDate>Fri, 18 Aug 2023 12:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09467v1</guid></item><item><title>Automated Semiconductor Defect Inspection in Scanning Electron Microscope Images: a Systematic Review</title><link>http://arxiv.org/abs/2308.08376v2</link><description>A growing need exists for efficient and accurate methods for detectingdefects in semiconductor materials and devices. These defects can have adetrimental impact on the efficiency of the manufacturing process, because theycause critical failures and wafer-yield limitations. As nodes and patterns getsmaller, even high-resolution imaging techniques such as Scanning ElectronMicroscopy (SEM) produce noisy images due to operating close to sensitivitylevels and due to varying physical properties of different underlayers orresist materials. This inherent noise is one of the main challenges for defectinspection. One promising approach is the use of machine learning algorithms,which can be trained to accurately classify and locate defects in semiconductorsamples. Recently, convolutional neural networks have proved to be particularlyuseful in this regard. This systematic review provides a comprehensive overviewof the state of automated semiconductor defect inspection on SEM images,including the most recent innovations and developments. 38 publications wereselected on this topic, indexed in IEEE Xplore and SPIE databases. For each ofthese, the application, methodology, dataset, results, limitations and futurework were summarized. A comprehensive overview and analysis of their methods isprovided. Finally, promising avenues for future work in the field of SEM-baseddefect inspection are suggested.</description><author>Thibault Lechien, Enrique Dehaerne, Bappaditya Dey, Victor Blanco, Sandip Halder, Stefan De Gendt, Wannes Meert</author><pubDate>Fri, 18 Aug 2023 12:03:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08376v2</guid></item><item><title>Data augmentation and explainability for bias discovery and mitigation in deep learning</title><link>http://arxiv.org/abs/2308.09464v1</link><description>This dissertation explores the impact of bias in deep neural networks andpresents methods for reducing its influence on model performance. The firstpart begins by categorizing and describing potential sources of bias and errorsin data and models, with a particular focus on bias in machine learningpipelines. The next chapter outlines a taxonomy and methods of Explainable AIas a way to justify predictions and control and improve the model. Then, as anexample of a laborious manual data inspection and bias discovery process, askin lesion dataset is manually examined. A Global Explanation for the BiasIdentification method is proposed as an alternative semi-automatic approach tomanual data exploration for discovering potential biases in data. Relevantnumerical methods and metrics are discussed for assessing the effects of theidentified biases on the model. Whereas identifying errors and bias iscritical, improving the model and reducing the number of flaws in the future isan absolute priority. Hence, the second part of the thesis focuses onmitigating the influence of bias on ML models. Three approaches are proposedand discussed: Style Transfer Data Augmentation, Targeted Data Augmentations,and Attribution Feedback. Style Transfer Data Augmentation aims to addressshape and texture bias by merging a style of a malignant lesion with aconflicting shape of a benign one. Targeted Data Augmentations randomly insertpossible biases into all images in the dataset during the training, as a way tomake the process random and, thus, destroy spurious correlations. Lastly,Attribution Feedback is used to fine-tune the model to improve its accuracy byeliminating obvious mistakes and teaching it to ignore insignificant inputparts via an attribution loss. The goal of these approaches is to reduce theinfluence of bias on machine learning models, rather than eliminate itentirely.</description><author>Agnieszka Mikołajczyk-Bareła</author><pubDate>Fri, 18 Aug 2023 12:02:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09464v1</guid></item><item><title>Accelerated Bayesian imaging by relaxed proximal-point Langevin sampling</title><link>http://arxiv.org/abs/2308.09460v1</link><description>This paper presents a new accelerated proximal Markov chain Monte Carlomethodology to perform Bayesian inference in imaging inverse problems with anunderlying convex geometry. The proposed strategy takes the form of astochastic relaxed proximal-point iteration that admits two complementaryinterpretations. For models that are smooth or regularised by Moreau-Yosidasmoothing, the algorithm is equivalent to an implicit midpoint discretisationof an overdamped Langevin diffusion targeting the posterior distribution ofinterest. This discretisation is asymptotically unbiased for Gaussian targetsand shown to converge in an accelerated manner for any target that is$\kappa$-strongly log-concave (i.e., requiring in the order of $\sqrt{\kappa}$iterations to converge, similarly to accelerated optimisation schemes),comparing favorably to [M. Pereyra, L. Vargas Mieles, K.C. Zygalakis, SIAM J.Imaging Sciences, 13, 2 (2020), pp. 905-935] which is only provably acceleratedfor Gaussian targets and has bias. For models that are not smooth, thealgorithm is equivalent to a Leimkuhler-Matthews discretisation of a Langevindiffusion targeting a Moreau-Yosida approximation of the posterior distributionof interest, and hence achieves a significantly lower bias than conventionalunadjusted Langevin strategies based on the Euler-Maruyama discretisation. Fortargets that are $\kappa$-strongly log-concave, the provided non-asymptoticconvergence analysis also identifies the optimal time step which maximizes theconvergence speed. The proposed methodology is demonstrated through a range ofexperiments related to image deconvolution with Gaussian and Poisson noise,with assumption-driven and data-driven convex priors.</description><author>Teresa Klatzer, Paul Dobson, Yoann Altmann, Marcelo Pereyra, Jesús María Sanz-Serna, Konstantinos C. Zygalakis</author><pubDate>Fri, 18 Aug 2023 11:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09460v1</guid></item><item><title>REAP: A Large-Scale Realistic Adversarial Patch Benchmark</title><link>http://arxiv.org/abs/2212.05680v2</link><description>Machine learning models are known to be susceptible to adversarialperturbation. One famous attack is the adversarial patch, a sticker with aparticularly crafted pattern that makes the model incorrectly predict theobject it is placed on. This attack presents a critical threat tocyber-physical systems that rely on cameras such as autonomous cars. Despitethe significance of the problem, conducting research in this setting has beendifficult; evaluating attacks and defenses in the real world is exceptionallycostly while synthetic data are unrealistic. In this work, we propose the REAP(REalistic Adversarial Patch) benchmark, a digital benchmark that allows theuser to evaluate patch attacks on real images, and under real-world conditions.Built on top of the Mapillary Vistas dataset, our benchmark contains over14,000 traffic signs. Each sign is augmented with a pair of geometric andlighting transformations, which can be used to apply a digitally generatedpatch realistically onto the sign. Using our benchmark, we perform the firstlarge-scale assessments of adversarial patch attacks under realisticconditions. Our experiments suggest that adversarial patch attacks may presenta smaller threat than previously believed and that the success rate of anattack on simpler digital simulations is not predictive of its actualeffectiveness in practice. We release our benchmark publicly athttps://github.com/wagner-group/reap-benchmark.</description><author>Nabeel Hingun, Chawin Sitawarin, Jerry Li, David Wagner</author><pubDate>Fri, 18 Aug 2023 11:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05680v2</guid></item><item><title>Artificial-Spiking Hierarchical Networks for Vision-Language Representation Learning</title><link>http://arxiv.org/abs/2308.09455v1</link><description>With the success of self-supervised learning, multimodal foundation modelshave rapidly adapted a wide range of downstream tasks driven by vision andlanguage (VL) pretraining. State-of-the-art methods achieve impressiveperformance by pre-training on large-scale datasets. However, bridging thesemantic gap between the two modalities remains a nonnegligible challenge forVL tasks. In this work, we propose an efficient computation framework formultimodal alignment by introducing a novel visual semantic module to furtherimprove the performance of the VL tasks. Specifically, we propose a flexiblemodel, namely Artificial-Spiking Hierarchical Networks (ASH-Nets), whichcombines the complementary advantages of Artificial neural networks (ANNs) andSpiking neural networks (SNNs) to enrich visual semantic representations. Inparticular, a visual concrete encoder and a semantic abstract encoder areconstructed to learn continuous and discrete latent variables to enhance theflexibility of semantic encoding. Considering the spatio-temporal properties ofSNNs modeling, we introduce a contrastive learning method to optimize theinputs of similar samples. This can improve the computational efficiency of thehierarchical network, while the augmentation of hard samples is beneficial tothe learning of visual representations. Furthermore, the Spiking to TextUni-Alignment Learning (STUA) pre-training method is proposed, which onlyrelies on text features to enhance the encoding ability of abstract semantics.We validate the performance on multiple well-established downstream VL tasks.Experiments show that the proposed ASH-Nets achieve competitive results.</description><author>Yeming Chen, Siyu Zhang, Yaoru Sun, Weijian Liang, Haoran Wang</author><pubDate>Fri, 18 Aug 2023 11:40:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09455v1</guid></item><item><title>Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model</title><link>http://arxiv.org/abs/2308.09454v1</link><description>Research in natural language processing has demonstrated that the quality ofgenerations from trained autoregressive language models is significantlyinfluenced by the used sampling strategy. In this study, we investigate theimpact of different sampling techniques on musical qualities such as diversityand structure. To accomplish this, we train a high-capacity transformer modelon a vast collection of highly-structured Irish folk melodies and analyze themusical qualities of the samples generated using distribution truncationsampling techniques. Specifically, we use nucleus sampling, the recentlyproposed "typical sampling", and conventional ancestral sampling. We evaluatethe effect of these sampling strategies in two scenarios: optimal circumstanceswith a well-calibrated model and suboptimal circumstances where wesystematically degrade the model's performance. We assess the generated samplesusing objective and subjective evaluations. We discover that probabilitytruncation techniques may restrict diversity and structural patterns in optimalcircumstances, but may also produce more musical samples in suboptimalcircumstances.</description><author>Mathias Rose Bjare, Stefan Lattner, Gerhard Widmer</author><pubDate>Fri, 18 Aug 2023 11:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09454v1</guid></item><item><title>Reconstructing $S$-matrix Phases with Machine Learning</title><link>http://arxiv.org/abs/2308.09451v1</link><description>An important element of the $S$-matrix bootstrap program is the relationshipbetween the modulus of an $S$-matrix element and its phase. Unitarity relatesthem by an integral equation. Even in the simplest case of elastic scattering,this integral equation cannot be solved analytically and numerical approachesare required. We apply modern machine learning techniques to studying theunitarity constraint. We find that for a given modulus, when a phase exists itcan generally be reconstructed to good accuracy with machine learning.Moreover, the loss of the reconstruction algorithm provides a good proxy forwhether a given modulus can be consistent with unitarity at all. In addition,we study the question of whether multiple phases can be consistent with asingle modulus, finding novel phase-ambiguous solutions. In particular, we finda new phase-ambiguous solution which pushes the known limit on such solutionssignificantly beyond the previous bound.</description><author>Aurélien Dersy, Matthew D. Schwartz, Alexander Zhiboedov</author><pubDate>Fri, 18 Aug 2023 11:29:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09451v1</guid></item><item><title>Defending Label Inference Attacks in Split Learning under Regression Setting</title><link>http://arxiv.org/abs/2308.09448v1</link><description>As a privacy-preserving method for implementing Vertical Federated Learning,Split Learning has been extensively researched. However, numerous studies haveindicated that the privacy-preserving capability of Split Learning isinsufficient. In this paper, we primarily focus on label inference attacks inSplit Learning under regression setting, which are mainly implemented throughthe gradient inversion method. To defend against label inference attacks, wepropose Random Label Extension (RLE), where labels are extended to obfuscatethe label information contained in the gradients, thereby preventing theattacker from utilizing gradients to train an attack model that can infer theoriginal labels. To further minimize the impact on the original task, wepropose Model-based adaptive Label Extension (MLE), where original labels arepreserved in the extended labels and dominate the training process. Theexperimental results show that compared to the basic defense methods, ourproposed defense methods can significantly reduce the attack model'sperformance while preserving the original task's performance.</description><author>Haoze Qiu, Fei Zheng, Chaochao Chen, Xiaolin Zheng</author><pubDate>Fri, 18 Aug 2023 11:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09448v1</guid></item><item><title>An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network</title><link>http://arxiv.org/abs/2308.09444v1</link><description>We propose an Gaussian Mixture Model (GMM) learning algorithm, based on ourprevious work of GMM expansion idea. The new algorithm brings more robustnessand simplicity than classic Expectation Maximization (EM) algorithm. It alsoimproves the accuracy and only take 1 iteration for learning. We theoreticallyproof that this new algorithm is guarantee to converge regardless theparameters initialisation. We compare our GMM expansion method with classicprobability layers in neural network leads to demonstrably better capability toovercome data uncertainty and inverse problem. Finally, we test GMM basedgenerator which shows a potential to build further application that able toutilized distribution random sampling for stochastic variation as well asvariation control.</description><author>Weiguo Lu, Xuan Wu, Deng Ding, Gangnan Yuan</author><pubDate>Fri, 18 Aug 2023 11:17:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09444v1</guid></item><item><title>Scope is all you need: Transforming LLMs for HPC Code</title><link>http://arxiv.org/abs/2308.09440v1</link><description>With easier access to powerful compute resources, there is a growing trend inthe field of AI for software development to develop larger and larger languagemodels (LLMs) to address a variety of programming tasks. Even LLMs applied totasks from the high-performance computing (HPC) domain are huge in size (e.g.,billions of parameters) and demand expensive compute resources for training. Wefound this design choice confusing - why do we need large LLMs trained onnatural languages and programming languages unrelated to HPC for HPC-specifictasks? In this line of work, we aim to question design choices made by existingLLMs by developing smaller LLMs for specific domains - we call themdomain-specific LLMs. Specifically, we start off with HPC as a domain andpropose a novel tokenizer named Tokompiler, designed specifically forpreprocessing code in HPC and compilation-centric tasks. Tokompiler leveragesknowledge of language primitives to generate language-oriented tokens,providing a context-aware understanding of code structure while avoiding humansemantics attributed to code structures completely. We applied Tokompiler topre-train two state-of-the-art models, SPT-Code and Polycoder, for a Fortrancode corpus mined from GitHub. We evaluate the performance of these modelsagainst the conventional LLMs. Results demonstrate that Tokompilersignificantly enhances code completion accuracy and semantic understandingcompared to traditional tokenizers in normalized-perplexity tests, down to ~1perplexity score. This research opens avenues for further advancements indomain-specific LLMs, catering to the unique demands of HPC and compilationtasks.</description><author>Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Abdul Wasay, Nesreen Ahmed, Ted Willke, Guy Tamir, Yuval Pinter, Timothy Mattson, Gal Oren</author><pubDate>Fri, 18 Aug 2023 11:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09440v1</guid></item><item><title>Generative AI Assistants in Software Development Education: A vision for integrating Generative AI into educational practice, not instinctively defending against it</title><link>http://arxiv.org/abs/2303.13936v3</link><description>The software development industry is amid another disruptive paradigm change- adopting the use of generative AI (GAI) assistants for programming. Whilst AIis already used in various areas of software engineering, GAI technologies,such as GitHub Copilot and ChatGPT, have ignited peoples' imaginations (andfears). It is unclear how the industry will adapt, but the move to integratethese technologies by large software companies, such as Microsoft (GitHub,Bing) and Google (Bard), is a clear indication of intent and direction. Weperformed exploratory interviews with industry professionals to understandcurrent practice and challenges, which we incorporate into our vision of afuture of software development education and make some pedagogicalrecommendations.</description><author>Christopher Bull, Ahmed Kharrufa</author><pubDate>Fri, 18 Aug 2023 11:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13936v3</guid></item><item><title>From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space</title><link>http://arxiv.org/abs/2308.09437v1</link><description>Deep Neural Networks are prone to learning spurious correlations embedded inthe training data, leading to potentially biased predictions. This poses riskswhen deploying these models for high-stake decision-making, such as in medicalapplications. Current methods for post-hoc model correction either requireinput-level annotations, which are only possible for spatially localizedbiases, or augment the latent feature space, thereby hoping to enforce theright reasons. We present a novel method ensuring the right reasons on theconcept level by reducing the model's sensitivity towards biases through thegradient. When modeling biases via Concept Activation Vectors, we highlight theimportance of choosing robust directions, as traditional regression-basedapproaches such as Support Vector Machines tend to result in divergingdirections. We effectively mitigate biases in controlled and real-worldsettings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNetand EfficientNet architectures.</description><author>Maximilian Dreyer, Frederik Pahde, Christopher J. Anders, Wojciech Samek, Sebastian Lapuschkin</author><pubDate>Fri, 18 Aug 2023 11:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09437v1</guid></item><item><title>Transformer-based Detection of Microorganismson High-Resolution Petri Dish Images</title><link>http://arxiv.org/abs/2308.09436v1</link><description>Many medical or pharmaceutical processes have strict guidelines regardingcontinuous hygiene monitoring. This often involves the labor-intensive task ofmanually counting microorganisms in Petri dishes by trained personnel.Automation attempts often struggle due to major challenges: significant scalingdifferences, low separation, low contrast, etc. To address these challenges, weintroduce AttnPAFPN, a high-resolution detection pipeline that leverages anovel transformer variation, the efficient-global self-attention mechanism. Ourstreamlined approach can be easily integrated in almost any multi-scale objectdetection pipeline. In a comprehensive evaluation on the publicly availableAGAR dataset, we demonstrate the superior accuracy of our network over thecurrent state-of-the-art. In order to demonstrate the task-independentperformance of our approach, we perform further experiments on COCO andLIVECell datasets.</description><author>Nikolas Ebert, Didier Stricker, Oliver Wasenmüller</author><pubDate>Fri, 18 Aug 2023 11:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09436v1</guid></item><item><title>A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages</title><link>http://arxiv.org/abs/2308.09435v1</link><description>Modern large language models demonstrate impressive capabilities in textgeneration and generalization. However, they often struggle with solving textediting tasks, particularly when it comes to correcting spelling errors andmistypings. In this paper, we present a methodology for generative spellingcorrection (SC), which was tested on English and Russian languages andpotentially can be extended to any language with minor changes. Our researchmainly focuses on exploring natural spelling errors and mistypings in texts andstudying the ways those errors can be emulated in correct sentences toeffectively enrich generative models' pre-train procedure. We investigate theimpact of such emulations and the models' abilities across different textdomains. In this work, we investigate two spelling corruption techniques: 1)first one mimics human behavior when making a mistake through leveragingstatistics of errors from particular dataset and 2) second adds the most commonspelling errors, keyboard miss clicks, and some heuristics within the texts. Weconducted experiments employing various corruption strategies, models'architectures and sizes on the pre-training and fine-tuning stages andevaluated the models using single-domain and multi-domain test sets. As apractical outcome of our work, we introduce SAGE (Spell checking viaAugmentation and Generative distribution Emulation) is a library for automaticgenerative SC that includes a family of pre-trained generative models andbuilt-in augmentation algorithms.</description><author>Nikita Martynov, Mark Baushenko, Anastasia Kozlova, Katerina Kolomeytseva, Aleksandr Abramov, Alena Fenogenova</author><pubDate>Fri, 18 Aug 2023 11:07:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09435v1</guid></item><item><title>Can ultrasound confidence maps predict sonographers' labeling variability?</title><link>http://arxiv.org/abs/2308.09433v1</link><description>Measuring cross-sectional areas in ultrasound images is a standard tool toevaluate disease progress or treatment response. Often addressed today withsupervised deep-learning segmentation approaches, existing solutions highlydepend upon the quality of experts' annotations. However, the annotationquality in ultrasound is anisotropic and position-variant due to the inherentphysical imaging principles, including attenuation, shadows, and missingboundaries, commonly exacerbated with depth. This work proposes a novelapproach that guides ultrasound segmentation networks to account forsonographers' uncertainties and generate predictions with variability similarto the experts. We claim that realistic variability can reduce overconfidentpredictions and improve physicians' acceptance of deep-learning cross-sectionalsegmentation solutions. Our method provides CM's certainty for each pixel forminimal computational overhead as it can be precalculated directly from theimage. We show that there is a correlation between low values in the confidencemaps and expert's label uncertainty. Therefore, we propose to give theconfidence maps as additional information to the networks. We study the effectof the proposed use of ultrasound CMs in combination with four state-of-the-artneural networks and in two configurations: as a second input channel and aspart of the loss. We evaluate our method on 3D ultrasound datasets of thethyroid and lower limb muscles. Our results show ultrasound CMs increase theDice score, improve the Hausdorff and Average Surface Distances, and decreasethe number of isolated pixel predictions. Furthermore, our findings suggestthat ultrasound CMs improve the penalization of uncertain areas in the groundtruth data, thereby improving problematic interpolations. Our code and exampledata will be made public athttps://github.com/IFL-CAMP/Confidence-segmentation.</description><author>Vanessa Gonzalez Duque, Leonhard Zirus, Yordanka Velikova, Nassir Navab, Diana Mateus</author><pubDate>Fri, 18 Aug 2023 11:07:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09433v1</guid></item><item><title>End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions</title><link>http://arxiv.org/abs/2308.09431v1</link><description>Computational models are an essential tool for understanding the origin andfunctions of the topographic organisation of the primate visual system. Yet,vision is most commonly modelled by convolutional neural networks that ignoretopography by learning identical features across space. Here, we overcome thislimitation by developing All-Topographic Neural Networks (All-TNNs). Trained onvisual input, several features of primate topography emerge in All-TNNs: smoothorientation maps and cortical magnification in their first layer, andcategory-selective areas in their final layer. In addition, we introduce anovel dataset of human spatial biases in object recognition, which enables usto directly link models to behaviour. We demonstrate that All-TNNssignificantly better align with human behaviour than previous state-of-the-artconvolutional models due to their topographic nature. All-TNNs thereby mark animportant step forward in understanding the spatial organisation of the visualbrain and how it mediates visual behaviour.</description><author>Zejin Lu, Adrien Doerig, Victoria Bosch, Bas Krahmer, Daniel Kaiser, Radoslaw M Cichy, Tim C Kietzmann</author><pubDate>Fri, 18 Aug 2023 11:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09431v1</guid></item><item><title>Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2308.09430v1</link><description>Stochastic gradient descent (SGD) performed in an asynchronous manner plays acrucial role in training large-scale machine learning models. However, thegeneralization performance of asynchronous delayed SGD, which is an essentialmetric for assessing machine learning algorithms, has rarely been explored.Existing generalization error bounds are rather pessimistic and cannot revealthe correlation between asynchronous delays and generalization. In this paper,we investigate sharper generalization error bound for SGD with asynchronousdelay $\tau$. Leveraging the generating function analysis tool, we firstestablish the average stability of the delayed gradient algorithm. Based onthis algorithmic stability, we provide upper bounds on the generalization errorof $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and$\tilde{\mathcal{O}}(\frac{1}{n})$ for quadratic convex and strongly convexproblems, respectively, where $T$ refers to the iteration number and $n$ is theamount of training data. Our theoretical results indicate that asynchronousdelays reduce the generalization error of the delayed SGD algorithm. Analogousanalysis can be generalized to the random delay setting, and the experimentalresults validate our theoretical findings.</description><author>Xiaoge Deng, Li Shen, Shengwei Li, Tao Sun, Dongsheng Li, Dacheng Tao</author><pubDate>Fri, 18 Aug 2023 11:00:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09430v1</guid></item><item><title>Self-Supervised Single-Image Deconvolution with Siamese Neural Networks</title><link>http://arxiv.org/abs/2308.09426v1</link><description>Inverse problems in image reconstruction are fundamentally complicated byunknown noise properties. Classical iterative deconvolution approaches amplifynoise and require careful parameter selection for an optimal trade-off betweensharpness and grain. Deep learning methods allow for flexible parametrizationof the noise and learning its properties directly from the data. Recently,self-supervised blind-spot neural networks were successfully adopted for imagedeconvolution by including a known point-spread function in the end-to-endtraining. However, their practical application has been limited to 2D images inthe biomedical domain because it implies large kernels that are poorlyoptimized. We tackle this problem with Fast Fourier Transform convolutions thatprovide training speed-up in 3D microscopy deconvolution tasks. Further, wepropose to adopt a Siamese invariance loss for deconvolution and empiricallyidentify its optimal position in the neural network between blind-spot and fullimage branches. The experimental results show that our improved frameworkoutperforms the previous state-of-the-art deconvolution methods with a knownpoint spread function.</description><author>Mikhail Papkov, Kaupo Palo, Leopold Parts</author><pubDate>Fri, 18 Aug 2023 10:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09426v1</guid></item><item><title>A Shift In Artistic Practices through Artificial Intelligence</title><link>http://arxiv.org/abs/2306.10054v2</link><description>The explosion of content generated by Artificial Intelligence models hasinitiated a cultural shift in arts, music, and media, where roles are changing,values are shifting, and conventions are challenged. The readily available,vast dataset of the internet has created an environment for AI models to betrained on any content on the web. With AI models shared openly, and used bymany, globally, how does this new paradigm shift challenge the status quo inartistic practices? What kind of changes will AI technology bring into music,arts, and new media?</description><author>Kıvanç Tatar, Petter Ericson, Kelsey Cotton, Paola Torres Núñez del Prado, Roser Batlle-Roca, Beatriz Cabrero-Daniel, Sara Ljungblad, Georgios Diapoulis, Jabbar Hussain</author><pubDate>Fri, 18 Aug 2023 10:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10054v2</guid></item><item><title>MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2308.09421v1</link><description>In the field of monocular 3D detection, it is common practice to utilizescene geometric clues to enhance the detector's performance. However, manyexisting works adopt these clues explicitly such as estimating a depth map andback-projecting it into 3D space. This explicit methodology induces sparsity in3D representations due to the increased dimensionality from 2D to 3D, and leadsto substantial information loss, especially for distant and occluded objects.To alleviate this issue, we propose MonoNeRD, a novel detection framework thatcan infer dense 3D geometry and occupancy. Specifically, we model scenes withSigned Distance Functions (SDF), facilitating the production of dense 3Drepresentations. We treat these representations as Neural Radiance Fields(NeRF) and then employ volume rendering to recover RGB images and depth maps.To the best of our knowledge, this work is the first to introduce volumerendering for M3D, and demonstrates the potential of implicit reconstructionfor image-based 3D perception. Extensive experiments conducted on the KITTI-3Dbenchmark and Waymo Open Dataset demonstrate the effectiveness of MonoNeRD.Codes are available at https://github.com/cskkxjk/MonoNeRD.</description><author>Junkai Xu, Liang Peng, Haoran Cheng, Hao Li, Wei Qian, Ke Li, Wenxiao Wang, Deng Cai</author><pubDate>Fri, 18 Aug 2023 10:39:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09421v1</guid></item><item><title>Reconstruction, forecasting, and stability of chaotic dynamics from partial data</title><link>http://arxiv.org/abs/2305.15111v2</link><description>The forecasting and computation of the stability of chaotic systems frompartial observations are tasks for which traditional equation-based methods maynot be suitable. In this computational paper, we propose data-driven methods to(i) infer the dynamics of unobserved (hidden) chaotic variables (full-statereconstruction); (ii) time forecast the evolution of the full state; and (iii)infer the stability properties of the full state. The tasks are performed withlong short-term memory (LSTM) networks, which are trained with observations(data) limited to only part of the state: (i) the low-to-high resolution LSTM(LH-LSTM), which takes partial observations as training input, and requiresaccess to the full system state when computing the loss; and (ii) thephysics-informed LSTM (PI-LSTM), which is designed to combine partialobservations with the integral formulation of the dynamical system's evolutionequations. First, we derive the Jacobian of the LSTMs. Second, we analyse achaotic partial differential equation, the Kuramoto-Sivashinsky (KS), and theLorenz-96 system. We show that the proposed networks can forecast the hiddenvariables, both time-accurately and statistically. The Lyapunov exponents andcovariant Lyapunov vectors, which characterize the stability of the chaoticattractors, are correctly inferred from partial observations. Third, thePI-LSTM outperforms the LH-LSTM by successfully reconstructing the hiddenchaotic dynamics when the input dimension is smaller or similar to theKaplan-Yorke dimension of the attractor. This work opens new opportunities forreconstructing the full state, inferring hidden variables, and computing thestability of chaotic systems from partial data.</description><author>Elise Özalp, Georgios Margazoglou, Luca Magri</author><pubDate>Fri, 18 Aug 2023 10:33:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15111v2</guid></item><item><title>Machine-Learning Solutions for the Analysis of Single-Particle Diffusion Trajectories</title><link>http://arxiv.org/abs/2308.09414v1</link><description>Single-particle traces of the diffusive motion of molecules, cells, oranimals are by-now routinely measured, similar to stochastic records of stockprices or weather data. Deciphering the stochastic mechanism behind therecorded dynamics is vital in understanding the observed systems. Typically,the task is to decipher the exact type of diffusion and/or to determine systemparameters. The tools used in this endeavor are currently revolutionized bymodern machine-learning techniques. In this Perspective we provide an overviewover recently introduced methods in machine-learning for diffusive time series,most notably, those successfully competing in theAnomalous-Diffusion-Challenge. As such methods are often criticized for theirlack of interpretability, we focus on means to include uncertainty estimatesand feature-based approaches, both improving interpretability and providingconcrete insight into the learning process of the machine. We expand thediscussion by examining predictions on different out-of-distribution data. Wealso comment on expected future developments.</description><author>Henrik Seckler, Janusz Szwabinski, Ralf Metzler</author><pubDate>Fri, 18 Aug 2023 10:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09414v1</guid></item><item><title>Metadata Improves Segmentation Through Multitasking Elicitation</title><link>http://arxiv.org/abs/2308.09411v1</link><description>Metainformation is a common companion to biomedical images. However, thispotentially powerful additional source of signal from image acquisition has hadlimited use in deep learning methods, for semantic segmentation in particular.Here, we incorporate metadata by employing a channel modulation mechanism inconvolutional networks and study its effect on semantic segmentation tasks. Wedemonstrate that metadata as additional input to a convolutional network canimprove segmentation results while being inexpensive in implementation as animble add-on to popular models. We hypothesize that this benefit of metadatacan be attributed to facilitating multitask switching. This aspect ofmetadata-driven systems is explored and discussed in detail.</description><author>Iaroslav Plutenko, Mikhail Papkov, Kaupo Palo, Leopold Parts, Dmytro Fishman</author><pubDate>Fri, 18 Aug 2023 10:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09411v1</guid></item><item><title>SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models</title><link>http://arxiv.org/abs/2305.05189v3</link><description>Diffusion models, which have emerged to become popular text-to-imagegeneration models, can produce high-quality and content-rich images guided bytextual prompts. However, there are limitations to semantic understanding andcommonsense reasoning in existing models when the input prompts are concisenarrative, resulting in low-quality image generation. To improve the capacitiesfor narrative prompts, we propose a simple-yet-effective parameter-efficientfine-tuning approach called the Semantic Understanding and Reasoning adapter(SUR-adapter) for pre-trained diffusion models. To reach this goal, we firstcollect and annotate a new dataset SURD which consists of more than 57,000semantically corrected multi-modal samples. Each sample contains a simplenarrative prompt, a complex keyword-based prompt, and a high-quality image.Then, we align the semantic representation of narrative prompts to the complexprompts and transfer knowledge of large language models (LLMs) to ourSUR-adapter via knowledge distillation so that it can acquire the powerfulsemantic understanding and reasoning capabilities to build a high-qualitytextual semantic representation for text-to-image generation. We conductexperiments by integrating multiple LLMs and popular pre-trained diffusionmodels to show the effectiveness of our approach in enabling diffusion modelsto understand and reason concise natural language without image qualitydegradation. Our approach can make text-to-image diffusion models easier to usewith better user experience, which demonstrates our approach has the potentialfor further advancing the development of user-friendly text-to-image generationmodels by bridging the semantic gap between simple narrative prompts andcomplex keyword-based prompts. The code is released athttps://github.com/Qrange-group/SUR-adapter.</description><author>Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin</author><pubDate>Fri, 18 Aug 2023 10:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05189v3</guid></item><item><title>Learning MDL logic programs from noisy data</title><link>http://arxiv.org/abs/2308.09393v1</link><description>Many inductive logic programming approaches struggle to learn programs fromnoisy data. To overcome this limitation, we introduce an approach that learnsminimal description length programs from noisy data, including recursiveprograms. Our experiments on several domains, including drug design, gameplaying, and program synthesis, show that our approach can outperform existingapproaches in terms of predictive accuracies and scale to moderate amounts ofnoise.</description><author>Céline Hocquette, Andreas Niskanen, Matti Järvisalo, Andrew Cropper</author><pubDate>Fri, 18 Aug 2023 09:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09393v1</guid></item><item><title>Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization</title><link>http://arxiv.org/abs/2308.09391v1</link><description>Domain generalization (DG) is proposed to deal with the issue of domainshift, which occurs when statistical differences exist between source andtarget domains. However, most current methods do not account for a commonrealistic scenario where the source and target domains have different classes.To overcome this deficiency, open set domain generalization (OSDG) then emergesas a more practical setting to recognize unseen classes in unseen domains. Anintuitive approach is to use multiple one-vs-all classifiers to define decisionboundaries for each class and reject the outliers as unknown. However, thesignificant class imbalance between positive and negative samples often causesthe boundaries biased towards positive ones, resulting in misclassification forknown samples in the unseen target domain. In this paper, we propose a novelmeta-learning-based framework called dualistic MEta-learning with jointDomaIn-Class matching (MEDIC), which considers gradient matching towardsinter-domain and inter-class splits simultaneously to find a generalizableboundary balanced for all tasks. Experimental results demonstrate that MEDICnot only outperforms previous methods in open set scenarios, but also maintainscompetitive close set generalization ability at the same time. Our code isavailable at https://github.com/zzwdx/MEDIC.</description><author>Xiran Wang, Jian Zhang, Lei Qi, Yinghuan Shi</author><pubDate>Fri, 18 Aug 2023 09:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09391v1</guid></item><item><title>Diffusion Models for Image Restoration and Enhancement -- A Comprehensive Survey</title><link>http://arxiv.org/abs/2308.09388v1</link><description>Image restoration (IR) has been an indispensable and challenging task in thelow-level vision field, which strives to improve the subjective quality ofimages distorted by various forms of degradation. Recently, the diffusion modelhas achieved significant advancements in the visual generation of AIGC, therebyraising an intuitive question, "whether diffusion model can boost imagerestoration". To answer this, some pioneering studies attempt to integratediffusion models into the image restoration task, resulting in superiorperformances than previous GAN-based methods. Despite that, a comprehensive andenlightening survey on diffusion model-based image restoration remains scarce.In this paper, we are the first to present a comprehensive review of recentdiffusion model-based methods on image restoration, encompassing the learningparadigm, conditional strategy, framework design, modeling strategy, andevaluation. Concretely, we first introduce the background of the diffusionmodel briefly and then present two prevalent workflows that exploit diffusionmodels in image restoration. Subsequently, we classify and emphasize theinnovative designs using diffusion models for both IR and blind/real-world IR,intending to inspire future development. To evaluate existing methodsthoroughly, we summarize the commonly-used dataset, implementation details, andevaluation metrics. Additionally, we present the objective comparison foropen-sourced methods across three tasks, including image super-resolution,deblurring, and inpainting. Ultimately, informed by the limitations in existingworks, we propose five potential and challenging directions for the futureresearch of diffusion model-based IR, including sampling efficiency, modelcompression, distortion simulation and estimation, distortion invariantlearning, and framework design.</description><author>Xin Li, Yulin Ren, Xin Jin, Cuiling Lan, Xingrui Wang, Wenjun Zeng, Xinchao Wang, Zhibo Chen</author><pubDate>Fri, 18 Aug 2023 09:40:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09388v1</guid></item><item><title>Multi-Level Compositional Reasoning for Interactive Instruction Following</title><link>http://arxiv.org/abs/2308.09387v1</link><description>Robotic agents performing domestic chores by natural language directives arerequired to master the complex job of navigating environment and interactingwith objects in the environments. The tasks given to the agents are oftencomposite thus are challenging as completing them require to reason aboutmultiple subtasks, e.g., bring a cup of coffee. To address the challenge, wepropose to divide and conquer it by breaking the task into multiple subgoalsand attend to them individually for better navigation and interaction. We callit Multi-level Compositional Reasoning Agent (MCR-Agent). Specifically, welearn a three-level action policy. At the highest level, we infer a sequence ofhuman-interpretable subgoals to be executed based on language instructions by ahigh-level policy composition controller. At the middle level, wediscriminatively control the agent's navigation by a master policy byalternating between a navigation policy and various independent interactionpolicies. Finally, at the lowest level, we infer manipulation actions with thecorresponding object masks using the appropriate interaction policy. Ourapproach not only generates human interpretable subgoals but also achieves2.03% absolute gain to comparable state of the arts in the efficiency metric(PLWSR in unseen set) without using rule-based planning or a semantic spatialmemory.</description><author>Suvaansh Bhambri, Byeonghwi Kim, Jonghyun Choi</author><pubDate>Fri, 18 Aug 2023 09:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09387v1</guid></item><item><title>DReg-NeRF: Deep Registration for Neural Radiance Fields</title><link>http://arxiv.org/abs/2308.09386v1</link><description>Although Neural Radiance Fields (NeRF) is popular in the computer visioncommunity recently, registering multiple NeRFs has yet to gain much attention.Unlike the existing work, NeRF2NeRF, which is based on traditional optimizationmethods and needs human annotated keypoints, we propose DReg-NeRF to solve theNeRF registration problem on object-centric scenes without human intervention.After training NeRF models, our DReg-NeRF first extracts features from theoccupancy grid in NeRF. Subsequently, our DReg-NeRF utilizes a transformerarchitecture with self-attention and cross-attention layers to learn therelations between pairwise NeRF blocks. In contrast to state-of-the-art (SOTA)point cloud registration methods, the decoupled correspondences are supervisedby surface fields without any ground truth overlapping labels. We construct anovel view synthesis dataset with 1,700+ 3D objects obtained from Objaverse totrain our network. When evaluated on the test set, our proposed method beatsthe SOTA point cloud registration methods by a large margin, with a mean$\text{RPE}=9.67^{\circ}$ and a mean $\text{RTE}=0.038$. Our code is available at https://github.com/AIBluefisher/DReg-NeRF.</description><author>Yu Chen, Gim Hee Lee</author><pubDate>Fri, 18 Aug 2023 09:37:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09386v1</guid></item><item><title>Streamlined Lensed Quasar Identification in Multiband Images via Ensemble Networks</title><link>http://arxiv.org/abs/2307.01090v2</link><description>Quasars experiencing strong lensing offer unique viewpoints on subjectsrelated to the cosmic expansion rate, the dark matter profile within theforeground deflectors, and the quasar host galaxies. Unfortunately, identifyingthem in astronomical images is challenging since they are overwhelmed by theabundance of non-lenses. To address this, we have developed a novel approach byensembling cutting-edge convolutional networks (CNNs) -- for instance, ResNet,Inception, NASNet, MobileNet, EfficientNet, and RegNet -- along with visiontransformers (ViTs) trained on realistic galaxy-quasar lens simulations basedon the Hyper Suprime-Cam (HSC) multiband images. While the individual modelexhibits remarkable performance when evaluated against the test dataset,achieving an area under the receiver operating characteristic curve of $&gt;$97.3%and a median false positive rate of 3.6%, it struggles to generalize in realdata, indicated by numerous spurious sources picked by each classifier. Asignificant improvement is achieved by averaging these CNNs and ViTs, resultingin the impurities being downsized by factors up to 50. Subsequently, combiningthe HSC images with the UKIRT, VISTA, and unWISE data, we retrieveapproximately 60 million sources as parent samples and reduce this to 892,609after employing a photometry preselection to discover $z&gt;1.5$ lensed quasarswith Einstein radii of $\theta_\mathrm{E}&lt;5$ arcsec. Afterward, the ensembleclassifier indicates 3080 sources with a high probability of being lenses, forwhich we visually inspect, yielding 210 prevailing candidates awaitingspectroscopic confirmation. These outcomes suggest that automated deep learningpipelines hold great potential in effectively detecting strong lenses in vastdatasets with minimal manual visual inspection involved.</description><author>Irham Taufik Andika, Sherry H. Suyu, Raoul Cañameras, Alejandra Melo, Stefan Schuldt, Yiping Shu, Anna-Christina Eilers, Anton Timur Jaelani, Minghao Yue</author><pubDate>Fri, 18 Aug 2023 09:31:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01090v2</guid></item><item><title>TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses</title><link>http://arxiv.org/abs/2306.05888v2</link><description>3D multi-object tracking (MOT) is vital for many applications includingautonomous driving vehicles and service robots. With the commonly usedtracking-by-detection paradigm, 3D MOT has made important progress in recentyears. However, these methods only use the detection boxes of the current frameto obtain trajectory-box association results, which makes it impossible for thetracker to recover objects missed by the detector. In this paper, we presentTrajectoryFormer, a novel point-cloud-based 3D MOT framework. To recover themissed object by detector, we generates multiple trajectory hypotheses withhybrid candidate boxes, including temporally predicted boxes and current-framedetection boxes, for trajectory-box association. The predicted boxes canpropagate object's history trajectory information to the current frame and thusthe network can tolerate short-term miss detection of the tracked objects. Wecombine long-term object motion feature and short-term object appearancefeature to create per-hypothesis feature embedding, which reduces thecomputational overhead for spatial-temporal encoding. Additionally, weintroduce a Global-Local Interaction Module to conduct information interactionamong all hypotheses and models their spatial relations, leading to accurateestimation of hypotheses. Our TrajectoryFormer achieves state-of-the-artperformance on the Waymo 3D MOT benchmarks. Code is available athttps://github.com/poodarchu/EFG .</description><author>Xuesong Chen, Shaoshuai Shi, Chao Zhang, Benjin Zhu, Qiang Wang, Ka Chun Cheung, Simon See, Hongsheng Li</author><pubDate>Fri, 18 Aug 2023 09:31:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05888v2</guid></item><item><title>Relation-aware graph structure embedding with co-contrastive learning for drug-drug interaction prediction</title><link>http://arxiv.org/abs/2307.01507v2</link><description>Relation-aware graph structure embedding is promising for predictingmulti-relational drug-drug interactions (DDIs). Typically, most existingmethods begin by constructing a multi-relational DDI graph and then learningrelation-aware graph structure embeddings (RaGSEs) of drugs from the DDI graph.Nevertheless, most existing approaches are usually limited in learning RaGSEsof new drugs, leading to serious over-fitting when the test DDIs involve suchdrugs. To alleviate this issue, we propose a novel DDI prediction method basedon relation-aware graph structure embedding with co-contrastive learning,RaGSECo. The proposed RaGSECo constructs two heterogeneous drug graphs: amulti-relational DDI graph and a multi-attribute drug-drug similarity (DDS)graph. The two graphs are used respectively for learning and propagating theRaGSEs of drugs, aiming to ensure all drugs, including new ones, can possesseffective RaGSEs. Additionally, we present a novel co-contrastive learningmodule to learn drug-pairs (DPs) representations. This mechanism learns DPrepresentations from two distinct views (interaction and similarity views) andencourages these views to supervise each other collaboratively to obtain morediscriminative DP representations. We evaluate the effectiveness of our RaGSECoon three different tasks using two real datasets. The experimental resultsdemonstrate that RaGSECo outperforms existing state-of-the-art predictionmethods.</description><author>Mengying Jiang, Guizhong Liu, Biao Zhao, Yuanchao Su, Weiqiang Jin</author><pubDate>Fri, 18 Aug 2023 09:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01507v2</guid></item><item><title>Label-Free Event-based Object Recognition via Joint Learning with Image Reconstruction from Events</title><link>http://arxiv.org/abs/2308.09383v1</link><description>Recognizing objects from sparse and noisy events becomes extremely difficultwhen paired images and category labels do not exist. In this paper, we studylabel-free event-based object recognition where category labels and pairedimages are not available. To this end, we propose a joint formulation of objectrecognition and image reconstruction in a complementary manner. Our methodfirst reconstructs images from events and performs object recognition throughContrastive Language-Image Pre-training (CLIP), enabling better recognitionthrough a rich context of images. Since the category information is essentialin reconstructing images, we propose category-guided attraction loss andcategory-agnostic repulsion loss to bridge the textual features of predictedcategories and the visual features of reconstructed images using CLIP.Moreover, we introduce a reliable data sampling strategy and local-globalreconstruction consistency to boost joint learning of two tasks. To enhance theaccuracy of prediction and quality of reconstruction, we also propose aprototype-based approach using unpaired images. Extensive experimentsdemonstrate the superiority of our method and its extensibility for zero-shotobject recognition. Our project code is available at\url{https://github.com/Chohoonhee/Ev-LaFOR}.</description><author>Hoonhee Cho, Hyeonseong Kim, Yujeong Chae, Kuk-Jin Yoon</author><pubDate>Fri, 18 Aug 2023 09:28:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09383v1</guid></item><item><title>On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box</title><link>http://arxiv.org/abs/2308.09381v1</link><description>Attribution methods shed light on the explainability of data-drivenapproaches such as deep learning models by revealing the most contributingfeatures to decisions that have been made. A widely accepted way of derivingfeature attributions is to analyze the gradients of the target function withrespect to input features. Analysis of gradients requires full access to thetarget system, meaning that solutions of this kind treat the target system as awhite-box. However, the white-box assumption may be untenable due to securityand safety concerns, thus limiting their practical applications. As an answerto the limited flexibility, this paper presents GEEX (gradient-estimation-basedexplanation), an explanation method that delivers gradient-like explanationsunder a black-box setting. Furthermore, we integrate the proposed method with apath method. The resulting approach iGEEX (integrated GEEX) satisfies the fourfundamental axioms of attribution methods: sensitivity, insensitivity,implementation invariance, and linearity. With a focus on image data, theexhaustive experiments empirically show that the proposed methods outperformstate-of-the-art black-box methods and achieve competitive performance comparedto the ones with full access.</description><author>Yi Cai, Gerhard Wunder</author><pubDate>Fri, 18 Aug 2023 09:24:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09381v1</guid></item><item><title>Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review</title><link>http://arxiv.org/abs/2308.09380v1</link><description>Existing artificial intelligence (AI) models for diagnosing kneeosteoarthritis (OA) have faced criticism for their lack of transparency andinterpretability, despite achieving medical-expert-like performance. Thisopacity makes them challenging to trust in clinical practice. Recently,explainable artificial intelligence (XAI) has emerged as a specializedtechnique that can provide confidence in the model's prediction by revealinghow the prediction is derived, thus promoting the use of AI systems inhealthcare. This paper presents the first survey of XAI techniques used forknee OA diagnosis. The XAI techniques are discussed from two perspectives: datainterpretability and model interpretability. The aim of this paper is toprovide valuable insights into XAI's potential towards a more reliable knee OAdiagnosis approach and encourage its adoption in clinical practice.</description><author>Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai</author><pubDate>Fri, 18 Aug 2023 09:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09380v1</guid></item><item><title>Too Large; Data Reduction for Vision-Language Pre-Training</title><link>http://arxiv.org/abs/2305.20087v3</link><description>This paper examines the problems of severe image-text misalignment and highredundancy in the widely-used large-scale Vision-Language Pre-Training (VLP)datasets. To address these issues, we propose an efficient and straightforwardVision-Language learning algorithm called TL;DR, which aims to compress theexisting large VLP data into a small, high-quality set. Our approach consistsof two major steps. First, a codebook-based encoder-decoder captioner isdeveloped to select representative samples. Second, a new caption is generatedto complement the original captions for selected samples, mitigating thetext-image misalignment problem while maintaining uniqueness. As the result,TL;DR enables us to reduce the large dataset into a small set of high-qualitydata, which can serve as an alternative pre-training dataset. This algorithmsignificantly speeds up the time-consuming pretraining process. Specifically,TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reducewell-cleaned CC3M dataset from 2.82M to 0.67M ($\sim$24\%) and noisy YFCC15Mfrom 15M to 2.5M ($\sim$16.7\%). Extensive experiments with three popular VLPmodels over seven downstream tasks show that VLP model trained on thecompressed dataset provided by TL;DR can perform similar or even better resultscompared with training on the full-scale dataset. The code will be madeavailable at \url{https://github.com/showlab/datacentric.vlp}.</description><author>Alex Jinpeng Wang, Kevin Qinghong Lin, David Junhao Zhang, Stan Weixian Lei, Mike Zheng Shou</author><pubDate>Fri, 18 Aug 2023 09:20:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.20087v3</guid></item><item><title>Deformable Mixer Transformer with Gating for Multi-Task Learning of Dense Prediction</title><link>http://arxiv.org/abs/2308.05721v3</link><description>CNNs and Transformers have their own advantages and both have been widelyused for dense prediction in multi-task learning (MTL). Most of the currentstudies on MTL solely rely on CNN or Transformer. In this work, we present anovel MTL model by combining both merits of deformable CNN and query-basedTransformer with shared gating for multi-task learning of dense prediction.This combination may offer a simple and efficient solution owing to itspowerful and flexible task-specific learning and advantages of lower cost, lesscomplexity and smaller parameters than the traditional MTL methods. Weintroduce deformable mixer Transformer with gating (DeMTG), a simple andeffective encoder-decoder architecture up-to-date that incorporates theconvolution and attention mechanism in a unified network for MTL. It isexquisitely designed to use advantages of each block, and provide deformableand comprehensive features for all tasks from local and global perspective.First, the deformable mixer encoder contains two types of operators: thechannel-aware mixing operator leveraged to allow communication among differentchannels, and the spatial-aware deformable operator with deformable convolutionapplied to efficiently sample more informative spatial locations. Second, thetask-aware gating transformer decoder is used to perform the task-specificpredictions, in which task interaction block integrated with self-attention isapplied to capture task interaction features, and the task query blockintegrated with gating attention is leveraged to select correspondingtask-specific features. Further, the experiment results demonstrate that theproposed DeMTG uses fewer GFLOPs and significantly outperforms currentTransformer-based and CNN-based competitive models on a variety of metrics onthree dense prediction datasets. Our code and models are available athttps://github.com/yangyangxu0/DeMTG.</description><author>Yangyang Xu, Yibo Yang, Bernard Ghanemm, Lefei Zhang, Du Bo, Dacheng Tao</author><pubDate>Fri, 18 Aug 2023 09:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05721v3</guid></item><item><title>Leveraging Large Language Models for DRL-Based Anti-Jamming Strategies in Zero Touch Networks</title><link>http://arxiv.org/abs/2308.09376v1</link><description>As the dawn of sixth-generation (6G) networking approaches, it promisesunprecedented advancements in communication and automation. Among the leadinginnovations of 6G is the concept of Zero Touch Networks (ZTNs), aiming toachieve fully automated, self-optimizing networks with minimal humanintervention. Despite the advantages ZTNs offer in terms of efficiency andscalability, challenges surrounding transparency, adaptability, and human trustremain prevalent. Concurrently, the advent of Large Language Models (LLMs)presents an opportunity to elevate the ZTN framework by bridging the gapbetween automated processes and human-centric interfaces. This paper exploresthe integration of LLMs into ZTNs, highlighting their potential to enhancenetwork transparency and improve user interactions. Through a comprehensivecase study on deep reinforcement learning (DRL)-based anti-jamming technique,we demonstrate how LLMs can distill intricate network operations intointuitive, human-readable reports. Additionally, we address the technical andethical intricacies of melding LLMs with ZTNs, with an emphasis on dataprivacy, transparency, and bias reduction. Looking ahead, we identify emergingresearch avenues at the nexus of LLMs and ZTNs, advocating for sustainedinnovation and interdisciplinary synergy in the domain of automated networks.</description><author>Abubakar S. Ali, Dimitrios Michael Manias, Abdallah Shami, Sami Muhaidat</author><pubDate>Fri, 18 Aug 2023 09:13:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09376v1</guid></item><item><title>Unsupervised Light Field Depth Estimation via Multi-view Feature Matching with Occlusion Prediction</title><link>http://arxiv.org/abs/2301.08433v2</link><description>Depth estimation from light field (LF) images is a fundamental step fornumerous applications. Recently, learning-based methods have achieved higheraccuracy and efficiency than the traditional methods. However, it is costly toobtain sufficient depth labels for supervised training. In this paper, wepropose an unsupervised framework to estimate depth from LF images. First, wedesign a disparity estimation network (DispNet) with a coarse-to-fine structureto predict disparity maps from different view combinations. It explicitlyperforms multi-view feature matching to learn the correspondences effectively.As occlusions may cause the violation of photo-consistency, we introduce anocclusion prediction network (OccNet) to predict the occlusion maps, which areused as the element-wise weights of photometric loss to solve the occlusionissue and assist the disparity learning. With the disparity maps estimated bymultiple input combinations, we then propose a disparity fusion strategy basedon the estimated errors with effective occlusion handling to obtain the finaldisparity map with higher accuracy. Experimental results demonstrate that ourmethod achieves superior performance on both the dense and sparse LF images,and also shows better robustness and generalization on the real-world LF imagescompared to the other methods.</description><author>Shansi Zhang, Nan Meng, Edmund Y. Lam</author><pubDate>Fri, 18 Aug 2023 09:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08433v2</guid></item><item><title>Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package</title><link>http://arxiv.org/abs/2308.09375v1</link><description>Spectral pixels are often a mixture of the pure spectra of the materials,called endmembers, due to the low spatial resolution of hyperspectral sensors,double scattering, and intimate mixtures of materials in the scenes. Unmixingestimates the fractional abundances of the endmembers within the pixel.Depending on the prior knowledge of endmembers, linear unmixing can be dividedinto three main groups: supervised, semi-supervised, and unsupervised (blind)linear unmixing. Advances in Image processing and machine learningsubstantially affected unmixing. This paper provides an overview of advancedand conventional unmixing approaches. Additionally, we draw a criticalcomparison between advanced and conventional techniques from the threecategories. We compare the performance of the unmixing techniques on threesimulated and two real datasets. The experimental results reveal the advantagesof different unmixing categories for different unmixing scenarios. Moreover, weprovide an open-source Python-based package available athttps://github.com/BehnoodRasti/HySUPP to reproduce the results.</description><author>Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot</author><pubDate>Fri, 18 Aug 2023 09:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09375v1</guid></item><item><title>Noise Sensitivity and Stability of Deep Neural Networks for Binary Classification</title><link>http://arxiv.org/abs/2308.09374v1</link><description>A first step is taken towards understanding often observed non-robustnessphenomena of deep neural net (DNN) classifiers. This is done from theperspective of Boolean functions by asking if certain sequences of Booleanfunctions represented by common DNN models are noise sensitive or noise stable,concepts defined in the Boolean function literature. Due to the naturalrandomness in DNN models, these concepts are extended to annealed and quenchedversions. Here we sort out the relation between these definitions andinvestigate the properties of two standard DNN architectures, the fullyconnected and convolutional models, when initiated with Gaussian weights.</description><author>Johan Jonasson, Jeffrey E. Steif, Olof Zetterqvist</author><pubDate>Fri, 18 Aug 2023 09:09:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09374v1</guid></item><item><title>Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers</title><link>http://arxiv.org/abs/2308.09372v1</link><description>The growing popularity of Vision Transformers as the go-to models for imageclassification has led to an explosion of architectural modifications claimingto be more efficient than the original ViT. However, a wide diversity ofexperimental conditions prevents a fair comparison between all of them, basedsolely on their reported results. To address this gap in comparability, weconduct a comprehensive analysis of more than 30 models to evaluate theefficiency of vision transformers and related architectures, consideringvarious performance metrics. Our benchmark provides a comparable baselineacross the landscape of efficiency-oriented transformers, unveiling a plethoraof surprising insights. For example, we discover that ViT is still Paretooptimal across multiple efficiency metrics, despite the existence of severalalternative approaches claiming to be more efficient. Results also indicatethat hybrid attention-CNN models fare particularly well when it comes to lowinference memory and number of parameters, and also that it is better to scalethe model size, than the image size. Furthermore, we uncover a strong positivecorrelation between the number of FLOPS and the training memory, which enablesthe estimation of required VRAM from theoretical measurements alone. Thanks to our holistic evaluation, this study offers valuable insights forpractitioners and researchers, facilitating informed decisions when selectingmodels for specific applications. We publicly release our code and data athttps://github.com/tobna/WhatTransformerToFavor</description><author>Tobias Christian Nauen, Sebastian Palacio, Andreas Dengel</author><pubDate>Fri, 18 Aug 2023 09:06:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09372v1</guid></item><item><title>TrOMR:Transformer-Based Polyphonic Optical Music Recognition</title><link>http://arxiv.org/abs/2308.09370v1</link><description>Optical Music Recognition (OMR) is an important technology in music and hasbeen researched for a long time. Previous approaches for OMR are usually basedon CNN for image understanding and RNN for music symbol classification. In thispaper, we propose a transformer-based approach with excellent global perceptualcapability for end-to-end polyphonic OMR, called TrOMR. We also introduce anovel consistency loss function and a reasonable approach for data annotationto improve recognition accuracy for complex music scores. Extensive experimentsdemonstrate that TrOMR outperforms current OMR methods, especially inreal-world scenarios. We also develop a TrOMR system and build a camera scenedataset for full-page music scores in real-world. The code and datasets will bemade available for reproducibility.</description><author>Yixuan Li, Huaping Liu, Qiang Jin, Miaomiao Cai, Peng Li</author><pubDate>Fri, 18 Aug 2023 09:06:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09370v1</guid></item><item><title>Single Frame Semantic Segmentation Using Multi-Modal Spherical Images</title><link>http://arxiv.org/abs/2308.09369v1</link><description>In recent years, the research community has shown a lot of interest topanoramic images that offer a 360-degree directional perspective. Multiple datamodalities can be fed, and complimentary characteristics can be utilized formore robust and rich scene interpretation based on semantic segmentation, tofully realize the potential. Existing research, however, mostly concentrated onpinhole RGB-X semantic segmentation. In this study, we propose atransformer-based cross-modal fusion architecture to bridge the gap betweenmulti-modal fusion and omnidirectional scene perception. We employdistortion-aware modules to address extreme object deformations and panoramadistortions that result from equirectangular representation. Additionally, weconduct cross-modal interactions for feature rectification and informationexchange before merging the features in order to communicate long-rangecontexts for bi-modal and tri-modal feature streams. In thorough tests usingcombinations of four different modality types in three indoor panoramic-viewdatasets, our technique achieved state-of-the-art mIoU performance: 60.60% onStanford2D3DS (RGB-HHA), 71.97% Structured3D (RGB-D-N), and 35.92% Matterport3D(RGB-D). We plan to release all codes and trained models soon.</description><author>Suresh Guttikonda, Jason Rambach</author><pubDate>Fri, 18 Aug 2023 09:06:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09369v1</guid></item><item><title>A tailored Handwritten-Text-Recognition System for Medieval Latin</title><link>http://arxiv.org/abs/2308.09368v1</link><description>The Bavarian Academy of Sciences and Humanities aims to digitize its MedievalLatin Dictionary. This dictionary entails record cards referring to lemmas inmedieval Latin, a low-resource language. A crucial step of the digitizationprocess is the Handwritten Text Recognition (HTR) of the handwritten lemmasfound on these record cards. In our work, we introduce an end-to-end pipeline,tailored to the medieval Latin dictionary, for locating, extracting, andtranscribing the lemmas. We employ two state-of-the-art (SOTA) imagesegmentation models to prepare the initial data set for the HTR task.Furthermore, we experiment with different transformer-based models and conducta set of experiments to explore the capabilities of different combinations ofvision encoders with a GPT-2 decoder. Additionally, we also apply extensivedata augmentation resulting in a highly competitive model. The best-performingsetup achieved a Character Error Rate (CER) of 0.015, which is even superior tothe commercial Google Cloud Vision model, and shows more stable performance.</description><author>Philipp Koch, Gilary Vera Nuñez, Esteban Garces Arias, Christian Heumann, Matthias Schöffel, Alexander Häberlin, Matthias Aßenmacher</author><pubDate>Fri, 18 Aug 2023 09:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09368v1</guid></item><item><title>On the Approximation of Bi-Lipschitz Maps by Invertible Neural Networks</title><link>http://arxiv.org/abs/2308.09367v1</link><description>Invertible neural networks (INNs) represent an important class of deep neuralnetwork architectures that have been widely used in several applications. Theuniversal approximation properties of INNs have also been established recently.However, the approximation rate of INNs is largely missing. In this work, weprovide an analysis of the capacity of a class of coupling-based INNs toapproximate bi-Lipschitz continuous mappings on a compact domain, and theresult shows that it can well approximate both forward and inverse mapssimultaneously. Furthermore, we develop an approach for approximatingbi-Lipschitz maps on infinite-dimensional spaces that simultaneouslyapproximate the forward and inverse maps, by combining model reduction withprincipal component analysis and INNs for approximating the reduced map, and weanalyze the overall approximation error of the approach. Preliminary numericalresults show the feasibility of the approach for approximating the solutionoperator for parameterized second-order elliptic problems.</description><author>Bangti Jin, Zehui Zhou, Jun Zou</author><pubDate>Fri, 18 Aug 2023 09:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09367v1</guid></item><item><title>UniVTG: Towards Unified Video-Language Temporal Grounding</title><link>http://arxiv.org/abs/2307.16715v2</link><description>Video Temporal Grounding (VTG), which aims to ground target clips from videos(such as consecutive intervals or disjoint shots) according to custom languagequeries (e.g., sentences or words), is key for video browsing on social media.Most methods in this direction develop taskspecific models that are trainedwith type-specific labels, such as moment retrieval (time interval) andhighlight detection (worthiness curve), which limits their abilities togeneralize to various VTG tasks and labels. In this paper, we propose to Unifythe diverse VTG labels and tasks, dubbed UniVTG, along three directions:Firstly, we revisit a wide range of VTG labels and tasks and define a unifiedformulation. Based on this, we develop data annotation schemes to createscalable pseudo supervision. Secondly, we develop an effective and flexiblegrounding model capable of addressing each task and making full use of eachlabel. Lastly, thanks to the unified framework, we are able to unlock temporalgrounding pretraining from large-scale diverse labels and develop strongergrounding abilities e.g., zero-shot grounding. Extensive experiments on threetasks (moment retrieval, highlight detection and video summarization) acrossseven datasets (QVHighlights, Charades-STA, TACoS, Ego4D, YouTube Highlights,TVSum, and QFVS) demonstrate the effectiveness and flexibility of our proposedframework. The codes are available at https://github.com/showlab/UniVTG.</description><author>Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, Mike Zheng Shou</author><pubDate>Fri, 18 Aug 2023 08:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16715v2</guid></item><item><title>Latent Jailbreak: A Test Suite for Evaluating Both Text Safety and Output Robustness of Large Language Models</title><link>http://arxiv.org/abs/2307.08487v2</link><description>Considerable research efforts have been devoted to ensuring that largelanguage models (LLMs) align with human values and generate safe text. However,an excessive focus on sensitivity to certain topics can compromise the model'srobustness in following instructions, thereby impacting its overall performancein completing tasks. Previous benchmarks for jailbreaking LLMs have primarilyfocused on evaluating the safety of the models without considering theirrobustness. In this paper, we propose a benchmark that assesses both the safetyand robustness of LLMs, emphasizing the need for a balanced approach. Tocomprehensively study text safety and output robustness, we introduce a latentjailbreak prompt dataset, each involving malicious instruction embedding.Specifically, we instruct the model to complete a regular task, such astranslation, with the text to be translated containing malicious instructions.To further analyze safety and robustness, we design a hierarchical annotationframework. We present a systematic analysis of the safety and robustness ofLLMs regarding the position of explicit normal instructions, word replacements(verbs in explicit normal instructions, target groups in maliciousinstructions, cue words for explicit normal instructions), and instructionreplacements (different explicit normal instructions). Our results demonstratethat current LLMs not only prioritize certain instruction verbs but alsoexhibit varying jailbreak rates for different instruction verbs in explicitnormal instructions. Code and data are available athttps://github.com/qiuhuachuan/latent-jailbreak.</description><author>Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, Zhenzhong Lan</author><pubDate>Fri, 18 Aug 2023 08:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08487v2</guid></item><item><title>Overlap Bias Matching is Necessary for Point Cloud Registration</title><link>http://arxiv.org/abs/2308.09364v1</link><description>Point cloud registration is a fundamental problem in many domains.Practically, the overlap between point clouds to be registered may berelatively small. Most unsupervised methods lack effective initial evaluationof overlap, leading to suboptimal registration accuracy. To address this issue,we propose an unsupervised network Overlap Bias Matching Network (OBMNet) forpartial point cloud registration. Specifically, we propose a plug-and-playOverlap Bias Matching Module (OBMM) comprising two integral components, overlapsampling module and bias prediction module. These two components are utilizedto capture the distribution of overlapping regions and predict biascoefficients of point cloud common structures, respectively. Then, we integrateOBMM with the neighbor map matching module to robustly identify correspondencesby precisely merging matching scores of points within the neighborhood, whichaddresses the ambiguities in single-point features. OBMNet can maintainefficacy even in pair-wise registration scenarios with low overlap ratios.Experimental results on extensive datasets demonstrate that our approach'sperformance achieves a significant improvement compared to the state-of-the-artregistration approach.</description><author>Pengcheng Shi, Jie Zhang, Haozhe Cheng, Junyang Wang, Yiyang Zhou, Chenlin Zhao, Jihua Zhu</author><pubDate>Fri, 18 Aug 2023 08:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09364v1</guid></item><item><title>Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models</title><link>http://arxiv.org/abs/2308.09363v1</link><description>Video Question Answering (VideoQA) is a challenging task that entails complexmulti-modal reasoning. In contrast to multiple-choice VideoQA which aims topredict the answer given several options, the goal of open-ended VideoQA is toanswer questions without restricting candidate answers. However, the majorityof previous VideoQA models formulate open-ended VideoQA as a classificationtask to classify the video-question pairs into a fixed answer set, i.e.,closed-vocabulary, which contains only frequent answers (e.g., top-1000answers). This leads the model to be biased toward only frequent answers andfail to generalize on out-of-vocabulary answers. We hence propose a newbenchmark, Open-vocabulary Video Question Answering (OVQA), to measure thegeneralizability of VideoQA models by considering rare and unseen answers. Inaddition, in order to improve the model's generalization power, we introduce anovel GNN-based soft verbalizer that enhances the prediction on rare and unseenanswers by aggregating the information from their similar words. Forevaluation, we introduce new baselines by modifying the existing(closed-vocabulary) open-ended VideoQA models and improve their performances byfurther taking into account rare and unseen answers. Our ablation studies andqualitative analyses demonstrate that our GNN-based soft verbalizer furtherimproves the model performance, especially on rare and unseen answers. We hopethat our benchmark OVQA can serve as a guide for evaluating thegeneralizability of VideoQA models and inspire future research. Code isavailable at https://github.com/mlvlab/OVQA.</description><author>Dohwan Ko, Ji Soo Lee, Miso Choi, Jaewon Chu, Jihwan Park, Hyunwoo J. Kim</author><pubDate>Fri, 18 Aug 2023 08:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09363v1</guid></item><item><title>Multi-feature concatenation and multi-classifier stacking: an interpretable and generalizable machine learning method for MDD discrimination with rsfMRI</title><link>http://arxiv.org/abs/2308.09360v1</link><description>Major depressive disorder is a serious and heterogeneous psychiatric disorderthat needs accurate diagnosis. Resting-state functional MRI (rsfMRI), whichcaptures multiple perspectives on brain structure, function, and connectivity,is increasingly applied in the diagnosis and pathological research of mentaldiseases. Different machine learning algorithms are then developed to exploitthe rich information in rsfMRI and discriminate MDD patients from normalcontrols. Despite recent advances reported, the discrimination accuracy hasroom for further improvement. The generalizability and interpretability of themethod are not sufficiently addressed either. Here, we propose a machinelearning method (MFMC) for MDD discrimination by concatenating multiplefeatures and stacking multiple classifiers. MFMC is tested on the REST-meta-MDDdata set that contains 2428 subjects collected from 25 different sites. MFMCyields 96.9% MDD discrimination accuracy, demonstrating a significantimprovement over existing methods. In addition, the generalizability of MFMC isvalidated by the good performance when the training and testing subjects arefrom independent sites. The use of XGBoost as the meta classifier allows us toprobe the decision process of MFMC. We identify 13 feature values related to 9brain regions including the posterior cingulate gyrus, superior frontal gyrusorbital part, and angular gyrus, which contribute most to the classificationand also demonstrate significant differences at the group level. The use ofthese 13 feature values alone can reach 87% of MFMC's full performance whentaking all feature values. These features may serve as clinically usefuldiagnostic and prognostic biomarkers for mental disorders in the future.</description><author>Yunsong Luo, Wenyu Chen, Ling Zhan, Jiang Qiu, Tao Jia</author><pubDate>Fri, 18 Aug 2023 08:40:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09360v1</guid></item><item><title>To Compute or not to Compute? Adaptive Smart Sensing in Resource-Constrained Edge Computing</title><link>http://arxiv.org/abs/2209.02166v3</link><description>We consider a network of smart sensors for an edge computing application thatsample a time-varying signal and send updates to a base station for remoteglobal monitoring. Sensors are equipped with sensing and compute, and caneither send raw data or process them on-board before transmission. Limitedhardware resources at the edge generate a fundamental latency-accuracytrade-off: raw measurements are inaccurate but timely, whereas accurateprocessed updates are available after processing delay. Hence, one needs todecide when sensors should transmit raw measurements or rely on localprocessing to maximize network monitoring performance. To tackle this sensingdesign problem, we model an estimation-theoretic optimization framework thatembeds both computation and communication latency, and propose a ReinforcementLearning-based approach that dynamically allocates computational resources ateach sensor. Effectiveness of our proposed approach is validated throughnumerical experiments motivated by smart sensing for the Internet of Drones andself-driving vehicles. In particular, we show that, under constrainedcomputation at the base station, monitoring performance can be further improvedby an online sensor selection.</description><author>Luca Ballotta, Giovanni Peserico, Francesco Zanini, Paolo Dini</author><pubDate>Fri, 18 Aug 2023 08:40:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.02166v3</guid></item><item><title>SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions</title><link>http://arxiv.org/abs/2306.05178v2</link><description>The remarkable capabilities of pretrained image diffusion models have beenutilized not only for generating fixed-size images but also for creatingpanoramas. However, naive stitching of multiple images often results in visibleseams. Recent techniques have attempted to address this issue by performingjoint diffusions in multiple windows and averaging latent features inoverlapping regions. However, these approaches, which focus on seamless montagegeneration, often yield incoherent outputs by blending different scenes withina single image. To overcome this limitation, we propose SyncDiffusion, aplug-and-play module that synchronizes multiple diffusions through gradientdescent from a perceptual similarity loss. Specifically, we compute thegradient of the perceptual loss using the predicted denoised images at eachdenoising step, providing meaningful guidance for achieving coherent montages.Our experimental results demonstrate that our method produces significantlymore coherent outputs compared to previous methods (66.35% vs. 33.65% in ouruser study) while still maintaining fidelity (as assessed by GIQA) andcompatibility with the input prompt (as measured by CLIP score).</description><author>Yuseung Lee, Kunho Kim, Hyunjin Kim, Minhyuk Sung</author><pubDate>Fri, 18 Aug 2023 08:38:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05178v2</guid></item><item><title>Multi-scale Target-Aware Framework for Constrained Image Splicing Detection and Localization</title><link>http://arxiv.org/abs/2308.09357v1</link><description>Constrained image splicing detection and localization (CISDL) is afundamental task of multimedia forensics, which detects splicing operationbetween two suspected images and localizes the spliced region on both images.Recent works regard it as a deep matching problem and have made significantprogress. However, existing frameworks typically perform feature extraction andcorrelation matching as separate processes, which may hinder the model'sability to learn discriminative features for matching and can be susceptible tointerference from ambiguous background pixels. In this work, we propose amulti-scale target-aware framework to couple feature extraction and correlationmatching in a unified pipeline. In contrast to previous methods, we design atarget-aware attention mechanism that jointly learns features and performscorrelation matching between the probe and donor images. Our approach caneffectively promote the collaborative learning of related patches, and performmutual promotion of feature learning and correlation matching. Additionally, inorder to handle scale transformations, we introduce a multi-scale projectionmethod, which can be readily integrated into our target-aware framework thatenables the attention process to be conducted between tokens containinginformation of varying scales. Our experiments demonstrate that our model,which uses a unified pipeline, outperforms state-of-the-art methods on severalbenchmark datasets and is robust against scale transformations.</description><author>Yuxuan Tan, Yuanman Li, Limin Zeng, Jiaxiong Ye, Wei wang, Xia Li</author><pubDate>Fri, 18 Aug 2023 08:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09357v1</guid></item><item><title>Accelerated materials language processing enabled by GPT</title><link>http://arxiv.org/abs/2308.09354v1</link><description>Materials language processing (MLP) is one of the key facilitators ofmaterials science research, as it enables the extraction of structuredinformation from massive materials science literature. Prior works suggestedhigh-performance MLP models for text classification, named entity recognition(NER), and extractive question answering (QA), which require complex modelarchitecture, exhaustive fine-tuning and a large number of human-labelleddatasets. In this study, we develop generative pretrained transformer(GPT)-enabled pipelines where the complex architectures of prior MLP models arereplaced with strategic designs of prompt engineering. First, we develop aGPT-enabled document classification method for screening relevant documents,achieving comparable accuracy and reliability compared to prior models, withonly small dataset. Secondly, for NER task, we design an entity-centricprompts, and learning few-shot of them improved the performance on most ofentities in three open datasets. Finally, we develop an GPT-enabled extractiveQA model, which provides improved performance and shows the possibility ofautomatically correcting annotations. While our findings confirm the potentialof GPT-enabled MLP models as well as their value in terms of reliability andpracticability, our scientific methods and systematic approach are applicableto any materials science domain to accelerate the information extraction ofscientific literature.</description><author>Jaewoong Choi, Byungju Lee</author><pubDate>Fri, 18 Aug 2023 08:31:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09354v1</guid></item><item><title>Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation</title><link>http://arxiv.org/abs/2307.09906v3</link><description>Talking head video generation aims to animate a human face in a still imagewith dynamic poses and expressions using motion information derived from atarget-driving video, while maintaining the person's identity in the sourceimage. However, dramatic and complex motions in the driving video causeambiguous generation, because the still source image cannot provide sufficientappearance information for occluded regions or delicate expression variations,which produces severe artifacts and significantly degrades the generationquality. To tackle this problem, we propose to learn a global facialrepresentation space, and design a novel implicit identity representationconditioned memory compensation network, coined as MCNet, for high-fidelitytalking head generation.~Specifically, we devise a network module to learn aunified spatial facial meta-memory bank from all training samples, which canprovide rich facial structure and appearance priors to compensate warped sourcefacial features for the generation. Furthermore, we propose an effective querymechanism based on implicit identity representations learned from the discretekeypoints of the source image. It can greatly facilitate the retrieval of morecorrelated information from the memory bank for the compensation. Extensiveexperiments demonstrate that MCNet can learn representative and complementaryfacial memory, and can clearly outperform previous state-of-the-art talkinghead generation methods on VoxCeleb1 and CelebV datasets. Please check our\href{https://github.com/harlanhong/ICCV2023-MCNET}{Project}.</description><author>Fa-Ting Hong, Dan Xu</author><pubDate>Fri, 18 Aug 2023 08:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09906v3</guid></item><item><title>RLIPv2: Fast Scaling of Relational Language-Image Pre-training</title><link>http://arxiv.org/abs/2308.09351v1</link><description>Relational Language-Image Pre-training (RLIP) aims to align visionrepresentations with relational texts, thereby advancing the capability ofrelational reasoning in computer vision tasks. However, hindered by the slowconvergence of RLIPv1 architecture and the limited availability of existingscene graph data, scaling RLIPv1 is challenging. In this paper, we proposeRLIPv2, a fast converging model that enables the scaling of relationalpre-training to large-scale pseudo-labelled scene graph data. To enable fastscaling, RLIPv2 introduces Asymmetric Language-Image Fusion (ALIF), a mechanismthat facilitates earlier and deeper gated cross-modal fusion with sparsifiedlanguage encoding layers. ALIF leads to comparable or better performance thanRLIPv1 in a fraction of the time for pre-training and fine-tuning. To obtainscene graph data at scale, we extend object detection datasets with free-formrelation labels by introducing a captioner (e.g., BLIP) and a designed RelationTagger. The Relation Tagger assigns BLIP-generated relation texts to regionpairs, thus enabling larger-scale relational pre-training. Through extensiveexperiments conducted on Human-Object Interaction Detection and Scene GraphGeneration, RLIPv2 shows state-of-the-art performance on three benchmarks underfully-finetuning, few-shot and zero-shot settings. Notably, the largest RLIPv2achieves 23.29mAP on HICO-DET without any fine-tuning, yields 32.22mAP withjust 1% data and yields 45.09mAP with 100% data. Code and models are publiclyavailable at https://github.com/JacobYuan7/RLIPv2.</description><author>Hangjie Yuan, Shiwei Zhang, Xiang Wang, Samuel Albanie, Yining Pan, Tao Feng, Jianwen Jiang, Dong Ni, Yingya Zhang, Deli Zhao</author><pubDate>Fri, 18 Aug 2023 08:17:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09351v1</guid></item><item><title>Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys</title><link>http://arxiv.org/abs/2308.03045v3</link><description>According to various estimates, brown dwarfs (BD) should account for up to 25percent of all objects in the Galaxy. However, few of them are discovered andwell-studied, both individually and as a population. Homogeneous and completesamples of brown dwarfs are needed for these kinds of studies. Due to theirweakness, spectral studies of brown dwarfs are rather laborious. For thisreason, creating a significant reliable sample of brown dwarfs, confirmed byspectroscopic observations, seems unattainable at the moment. Numerous attemptshave been made to search for and create a set of brown dwarfs using theircolours as a decision rule applied to a vast amount of survey data. In thiswork, we use machine learning methods such as Random Forest Classifier,XGBoost, SVM Classifier and TabNet on PanStarrs DR1, 2MASS and WISE data todistinguish L and T brown dwarfs from objects of other spectral and luminosityclasses. The explanation of the models is discussed. We also compare our modelswith classical decision rules, proving their efficiency and relevance.</description><author>Aleksandra Avdeeva</author><pubDate>Fri, 18 Aug 2023 08:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03045v3</guid></item><item><title>Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching</title><link>http://arxiv.org/abs/2308.09346v1</link><description>Class prototype construction and matching are core aspects of few-shot actionrecognition. Previous methods mainly focus on designing spatiotemporal relationmodeling modules or complex temporal alignment algorithms. Despite thepromising results, they ignored the value of class prototype construction andmatching, leading to unsatisfactory performance in recognizing similarcategories in every task. In this paper, we propose GgHM, a new framework withGraph-guided Hybrid Matching. Concretely, we learn task-oriented features bythe guidance of a graph neural network during class prototype construction,optimizing the intra- and inter-class feature correlation explicitly. Next, wedesign a hybrid matching strategy, combining frame-level and tuple-levelmatching to classify videos with multivariate styles. We additionally propose alearnable dense temporal modeling module to enhance the video feature temporalrepresentation to build a more solid foundation for the matching process. GgHMshows consistent improvements over other challenging baselines on severalfew-shot datasets, demonstrating the effectiveness of our method. The code willbe publicly available at https://github.com/jiazheng-xing/GgHM.</description><author>Jiazheng Xing, Mengmeng Wang, Yudi Ruan, Bofan Chen, Yaowei Guo, Boyu Mu, Guang Dai, Jingdong Wang, Yong Liu</author><pubDate>Fri, 18 Aug 2023 08:07:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09346v1</guid></item><item><title>Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations</title><link>http://arxiv.org/abs/2308.09345v1</link><description>Background: Automated segmentation of spinal MR images plays a vital roleboth scientifically and clinically. However, accurately delineating posteriorspine structures presents challenges. Methods: This retrospective study, approved by the ethical committee,involved translating T1w and T2w MR image series into CT images in a total ofn=263 pairs of CT/MR series. Landmark-based registration was performed to alignimage pairs. We compared 2D paired (Pix2Pix, denoising diffusion implicitmodels (DDIM) image mode, DDIM noise mode) and unpaired (contrastive unpairedtranslation, SynDiff) image-to-image translation using "peak signal to noiseratio" (PSNR) as quality measure. A publicly available segmentation networksegmented the synthesized CT datasets, and Dice scores were evaluated onin-house test sets and the "MRSpineSeg Challenge" volumes. The 2D findings wereextended to 3D Pix2Pix and DDIM. Results: 2D paired methods and SynDiff exhibited similar translationperformance and Dice scores on paired data. DDIM image mode achieved thehighest image quality. SynDiff, Pix2Pix, and DDIM image mode demonstratedsimilar Dice scores (0.77). For craniocaudal axis rotations, at least twolandmarks per vertebra were required for registration. The 3D translationoutperformed the 2D approach, resulting in improved Dice scores (0.80) andanatomically accurate segmentations in a higher resolution than the original MRimage. Conclusion: Two landmarks per vertebra registration enabled pairedimage-to-image translation from MR to CT and outperformed all unpairedapproaches. The 3D techniques provided anatomically correct segmentations,avoiding underprediction of small structures like the spinous process.</description><author>Robert Graf, Joachim Schmitt, Sarah Schlaeger, Hendrik Kristian Möller, Vasiliki Sideri-Lampretsa, Anjany Sekuboyina, Sandro Manuel Krieg, Benedikt Wiestler, Bjoern Menze, Daniel Rueckert, Jan Stefan Kirschke</author><pubDate>Fri, 18 Aug 2023 08:07:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09345v1</guid></item><item><title>Surprise machines: revealing Harvard Art Museums' image collection</title><link>http://arxiv.org/abs/2308.09343v1</link><description>Surprise Machines is a project of experimental museology that sets out tovisualize the entire image collection of the Harvard Art Museums, intending toopen up unexpected vistas on more than 200,000 objects usually inaccessible tovisitors. Part of the exhibition Curatorial A(i)gents organized by metaLAB (at)Harvard, the project explores the limits of artificial intelligence to displaya large set of images and create surprise among visitors. To achieve such afeeling of surprise, a choreographic interface was designed to connect theaudience's movement with several unique views of the collection.</description><author>Dario Rodighiero, Lins Derry, Douglas Duhaime, Jordan Kruguer, Maximilian C. Mueller, Christopher Pietsch, Jeffrey T. Schnapp, Jeff Steward</author><pubDate>Fri, 18 Aug 2023 08:05:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09343v1</guid></item><item><title>Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM</title><link>http://arxiv.org/abs/2308.03333v2</link><description>The analysis and mining of user heterogeneous behavior are of paramountimportance in recommendation systems. However, the conventional approach ofincorporating various types of heterogeneous behavior into recommendationmodels leads to feature sparsity and knowledge fragmentation issues. To addressthis challenge, we propose a novel approach for personalized recommendation viaLarge Language Model (LLM), by extracting and fusing heterogeneous knowledgefrom user heterogeneous behavior information. In addition, by combiningheterogeneous knowledge and recommendation tasks, instruction tuning isperformed on LLM for personalized recommendations. The experimental resultsdemonstrate that our method can effectively integrate user heterogeneousbehavior and significantly improve recommendation performance.</description><author>Bin Yin, Junjie Xie, Yu Qin, Zixiang Ding, Zhichao Feng, Xiang Li, Wei Lin</author><pubDate>Fri, 18 Aug 2023 08:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03333v2</guid></item><item><title>Document Automation Architectures: Updated Survey in Light of Large Language Models</title><link>http://arxiv.org/abs/2308.09341v1</link><description>This paper surveys the current state of the art in document automation (DA).The objective of DA is to reduce the manual effort during the generation ofdocuments by automatically creating and integrating input from differentsources and assembling documents conforming to defined templates. There havebeen reviews of commercial solutions of DA, particularly in the legal domain,but to date there has been no comprehensive review of the academic research onDA architectures and technologies. The current survey of DA reviews theacademic literature and provides a clearer definition and characterization ofDA and its features, identifies state-of-the-art DA architectures andtechnologies in academic research, and provides ideas that can lead to newresearch opportunities within the DA field in light of recent advances ingenerative AI and large language models.</description><author>Mohammad Ahmadi Achachlouei, Omkar Patil, Tarun Joshi, Vijayan N. Nair</author><pubDate>Fri, 18 Aug 2023 07:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09341v1</guid></item><item><title>GPS-GLASS: Learning Nighttime Semantic Segmentation Using Daytime Video and GPS data</title><link>http://arxiv.org/abs/2207.13297v5</link><description>Semantic segmentation for autonomous driving should be robust against variousin-the-wild environments. Nighttime semantic segmentation is especiallychallenging due to a lack of annotated nighttime images and a large domain gapfrom daytime images with sufficient annotation. In this paper, we propose anovel GPS-based training framework for nighttime semantic segmentation. GivenGPS-aligned pairs of daytime and nighttime images, we perform cross-domaincorrespondence matching to obtain pixel-level pseudo supervision. Moreover, weconduct flow estimation between daytime video frames and apply GPS-basedscaling to acquire another pixel-level pseudo supervision. Using these pseudosupervisions with a confidence map, we train a nighttime semantic segmentationnetwork without any annotation from nighttime images. Experimental resultsdemonstrate the effectiveness of the proposed method on several nighttimesemantic segmentation datasets. Our source code is available athttps://github.com/jimmy9704/GPS-GLASS.</description><author>Hongjae Lee, Changwoo Han, Jun-Sang Yoo, Seung-Won Jung</author><pubDate>Fri, 18 Aug 2023 07:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.13297v5</guid></item><item><title>PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer</title><link>http://arxiv.org/abs/2308.05115v2</link><description>Phosphorylation is central to numerous fundamental cellular processes,influencing the onset and progression of a variety of diseases. The correctidentification of these phosphorylation sites is of great importance to unravelthe intricate molecular mechanisms within cells and during viral infections,potentially leading to the discovery of new therapeutic targets. In this study,we introduce PTransIPs, a novel deep learning model for the identification ofphosphorylation sites. PTransIPs treat amino acids within protein sequences aswords, extracting unique encodings based on their type and sequential position.The model also incorporates embeddings from large pretrained protein models asadditional data inputs. PTransIPS is further trained on a combination model ofconvolutional neural network with residual connections and Transformer modelequipped with multi-head attention mechanisms. At last, the model outputsclassification results through a fully connected layer. The results ofindependent testing reveal that PTransIPs outperforms existingstate-of-the-art(SOTA) methods, achieving AUROCs of 0.9232 and 0.9660 foridentifying phosphorylated S/T and Y sites respectively. In addition, ablationstudies prove that pretrained model embeddings contribute to the performance ofPTransIPs. Furthermore, PTransIPs has interpretable amino acid preference,visible training process and shows generalizability on other bioactivityclassification tasks. To facilitate usage, our code and data are publiclyaccessible at \url{https://github.com/StatXzy7/PTransIPs}.</description><author>Ziyang Xu, Haitian Zhong</author><pubDate>Fri, 18 Aug 2023 07:35:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05115v2</guid></item><item><title>LSCD: A Large-Scale Screen Content Dataset for Video Compression</title><link>http://arxiv.org/abs/2308.09332v1</link><description>Multimedia compression allows us to watch videos, see pictures and hearsounds within a limited bandwidth, which helps the flourish of the internet.During the past decades, multimedia compression has achieved great successusing hand-craft features and systems. With the development of artificialintelligence and video compression, there emerges a lot of research workrelated to using the neural network on the video compression task to get rid ofthe complicated system. Not only producing the advanced algorithms, butresearchers also spread the compression to different content, such as UserGenerated Content(UGC). With the rapid development of mobile devices, screencontent videos become an important part of multimedia data. In contrast, wefind community lacks a large-scale dataset for screen content videocompression, which impedes the fast development of the correspondinglearning-based algorithms. In order to fulfill this blank and accelerate theresearch of this special type of videos, we propose the Large-scale ScreenContent Dataset(LSCD), which contains 714 source sequences. Meanwhile, weprovide the analysis of the proposed dataset to show some features of screencontent videos, which will help researchers have a better understanding of howto explore new algorithms. Besides collecting and post-processing the data toorganize the dataset, we also provide a benchmark containing the performance ofboth traditional codec and learning-based methods.</description><author>Yuhao Cheng, Siru Zhang, Yiqiang Yan, Rong Chen, Yun Zhang</author><pubDate>Fri, 18 Aug 2023 07:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09332v1</guid></item><item><title>SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT</title><link>http://arxiv.org/abs/2308.09331v1</link><description>The Segment Anything Model (SAM) has gained significant attention in thefield of image segmentation due to its impressive capabilities and prompt-basedinterface. While SAM has already been extensively evaluated in various domains,its adaptation to retinal OCT scans remains unexplored. To bridge this researchgap, we conduct a comprehensive evaluation of SAM and its adaptations on alarge-scale public dataset of OCTs from RETOUCH challenge. Our evaluationcovers diverse retinal diseases, fluid compartments, and device vendors,comparing SAM against state-of-the-art retinal fluid segmentation methods.Through our analysis, we showcase adapted SAM's efficacy as a powerfulsegmentation model in retinal OCT scans, although still lagging behindestablished methods in some circumstances. The findings highlight SAM'sadaptability and robustness, showcasing its utility as a valuable tool inretinal OCT image analysis and paving the way for further advancements in thisdomain.</description><author>Botond Fazekas, José Morano, Dmitrii Lachinov, Guilherme Aresta, Hrvoje Bogunović</author><pubDate>Fri, 18 Aug 2023 07:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09331v1</guid></item><item><title>KESDT: knowledge enhanced shallow and deep Transformer for detecting adverse drug reactions</title><link>http://arxiv.org/abs/2308.09329v1</link><description>Adverse drug reaction (ADR) detection is an essential task in the medicalfield, as ADRs have a gravely detrimental impact on patients' health and thehealthcare system. Due to a large number of people sharing information onsocial media platforms, an increasing number of efforts focus on social mediadata to carry out effective ADR detection. Despite having achieved impressiveperformance, the existing methods of ADR detection still suffer from three mainchallenges. Firstly, researchers have consistently ignored the interactionbetween domain keywords and other words in the sentence. Secondly, social mediadatasets suffer from the challenges of low annotated data. Thirdly, the issueof sample imbalance is commonly observed in social media datasets. To solvethese challenges, we propose the Knowledge Enhanced Shallow and DeepTransformer(KESDT) model for ADR detection. Specifically, to cope with thefirst issue, we incorporate the domain keywords into the Transformer modelthrough a shallow fusion manner, which enables the model to fully exploit theinteractive relationships between domain keywords and other words in thesentence. To overcome the low annotated data, we integrate the synonym setsinto the Transformer model through a deep fusion manner, which expands the sizeof the samples. To mitigate the impact of sample imbalance, we replace thestandard cross entropy loss function with the focal loss function for effectivemodel training. We conduct extensive experiments on three public datasetsincluding TwiMed, Twitter, and CADEC. The proposed KESDT outperformsstate-of-the-art baselines on F1 values, with relative improvements of 4.87%,47.83%, and 5.73% respectively, which demonstrates the effectiveness of ourproposed KESDT.</description><author>Yunzhi Qiu, Xiaokun Zhang, Weiwei Wang, Tongxuan Zhang, Bo Xu, Hongfei Lin</author><pubDate>Fri, 18 Aug 2023 07:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09329v1</guid></item></channel></rss>