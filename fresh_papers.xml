<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 09 Nov 2023 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models</title><link>http://arxiv.org/abs/2311.04902v1</link><description>Large Language Models (LLMs) with a billion or more parameters are primetargets for network pruning, which aims to reduce a portion of the networkweights without compromising performance. Prior approaches such as WeightsMagnitude, SparseGPT, and Wanda, either concentrated solely on weights orintegrated weights with activations for sparsity. However, they overlooked theinformative gradients derived from pretrained large language models. In thispaper, we present a novel sparsity-centric pruning method for pretrained LLMs,termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Prunerleverages the first-order term of the Taylor expansion, operating in atraining-free manner by harnessing properly normalized gradients from a fewcalibration samples to determine the importance pruning score, andsubstantially outperforms competitive counterparts like SparseGPT and Wanda inmultiple benchmarks. Intriguing, after incorporating gradients, theunstructured pruning method tends to reveal some structural patternspost-pruning, which mirrors the geometric interdependence inherent in the LLMs'parameter structure. Additionally, GBLM-Pruner functions without any subsequentretraining or weight updates to maintain its simplicity as other counterparts.Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarksand perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda(weights+activations) and SparseGPT (weights+activations+weight update) bysignificant margins. Our code and models are available athttps://github.com/RocktimJyotiDas/GBLM-Pruner.</description><author>Rocktim Jyoti Das, Liqun Ma, Zhiqiang Shen</author><pubDate>Wed, 08 Nov 2023 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04902v1</guid></item><item><title>GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs</title><link>http://arxiv.org/abs/2311.04901v1</link><description>Recent works have shown that Large Language Models (LLMs) could empowertraditional neuro-symbolic models via programming capabilities to translatelanguage into module descriptions, thus achieving strong visual reasoningresults while maintaining the model's transparency and efficiency. However,these models usually exhaustively generate the entire code snippet given eachnew instance of a task, which is extremely ineffective. We propose generativeneuro-symbolic visual reasoning by growing and reusing modules. Specifically,our model consists of three unique stages, module initialization, modulegeneration, and module execution. First, given a vision-language task, we adoptLLMs to examine whether we could reuse and grow over established modules tohandle this new task. If not, we initialize a new module needed by the task andspecify the inputs and outputs of this new module. After that, the new moduleis created by querying LLMs to generate corresponding code snippets that matchthe requirements. In order to get a better sense of the new module's ability,we treat few-shot training examples as test cases to see if our new modulecould pass these cases. If yes, the new module is added to the module libraryfor future reuse. Finally, we evaluate the performance of our model on thetesting set by executing the parsed programs with the newly made visual modulesto get the results. We find the proposed model possesses several advantages.First, it performs competitively on standard tasks like visual questionanswering and referring expression comprehension; Second, the modules learnedfrom one task can be seamlessly transferred to new tasks; Last but not least,it is able to adapt to new visual reasoning tasks by observing a few trainingexamples and reusing modules.</description><author>Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, Chuang Gan</author><pubDate>Wed, 08 Nov 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04901v1</guid></item><item><title>How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure</title><link>http://arxiv.org/abs/2311.04900v1</link><description>Language models are typically evaluated on their success at predicting thedistribution of specific words in specific contexts. Yet linguistic knowledgealso encodes relationships between contexts, allowing inferences between worddistributions. We investigate the degree to which pre-trained Transformer-basedlarge language models (LLMs) represent such relationships, focusing on thedomain of argument structure. We find that LLMs perform well in generalizingthe distribution of a novel noun argument between related contexts that wereseen during pre-training (e.g., the active object and passive subject of theverb spray), succeeding by making use of the semantically-organized structureof the embedding space for word embeddings. However, LLMs fail atgeneralizations between related contexts that have not been observed duringpre-training, but which instantiate more abstract, but well-attested structuralgeneralizations (e.g., between the active object and passive subject of anarbitrary verb). Instead, in this case, LLMs show a bias to generalize based onlinear order. This finding points to a limitation with current models andpoints to a reason for which their training is data-intensive.s reported hereare available at https://github.com/clay-lab/structural-alternations.</description><author>Michael Wilson, Jackson Petty, Robert Frank</author><pubDate>Wed, 08 Nov 2023 18:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04900v1</guid></item><item><title>Two Complementary Perspectives to Continual Learning: Ask Not Only What to Optimize, But Also How</title><link>http://arxiv.org/abs/2311.04898v1</link><description>Recent years have seen considerable progress in the continual training ofdeep neural networks, predominantly thanks to approaches that add replay orregularization terms to the loss function to approximate the joint loss overall tasks so far. However, we show that even with a perfect approximation tothe joint loss, these approaches still suffer from temporary but substantialforgetting when starting to train on a new task. Motivated by this 'stabilitygap', we propose that continual learning strategies should focus not only onthe optimization objective, but also on the way this objective is optimized.While there is some continual learning work that alters the optimizationtrajectory (e.g., using gradient projection techniques), this line of researchis positioned as alternative to improving the optimization objective, while weargue it should be complementary. To evaluate the merits of our proposition, weplan to combine replay-approximated joint objectives with gradientprojection-based optimization routines to test whether the addition of thelatter provides benefits in terms of (1) alleviating the stability gap, (2)increasing the learning efficiency and (3) improving the final learningoutcome.</description><author>Timm Hess, Tinne Tuytelaars, Gido M. van de Ven</author><pubDate>Wed, 08 Nov 2023 18:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04898v1</guid></item><item><title>Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences</title><link>http://arxiv.org/abs/2302.09043v3</link><description>Self-supervised feature learning enables perception systems to benefit fromthe vast raw data recorded by vehicle fleets worldwide. While video-levelself-supervised learning approaches have shown strong generalizability onclassification tasks, the potential to learn dense representations fromsequential data has been relatively unexplored. In this work, we propose TempO,a temporal ordering pretext task for pre-training region-level featurerepresentations for perception tasks. We embed each frame by an unordered setof proposal feature vectors, a representation that is natural for objectdetection or tracking systems, and formulate the sequential ordering bypredicting frame transition probabilities in a transformer-based multi-framearchitecture whose complexity scales less than quadratic with respect to thesequence length. Extensive evaluations on the BDD100K, nuImages, and MOT17datasets show that our TempO pre-training approach outperforms single-frameself-supervised learning methods as well as supervised transfer learninginitialization strategies, achieving an improvement of +0.7% in mAP for objectdetection and +2.0% in the HOTA score for multi-object tracking.</description><author>Christopher Lang, Alexander Braun, Lars Schillingmann, Karsten Haug, Abhinav Valada</author><pubDate>Wed, 08 Nov 2023 18:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09043v3</guid></item><item><title>Future Lens: Anticipating Subsequent Tokens from a Single Hidden State</title><link>http://arxiv.org/abs/2311.04897v1</link><description>We conjecture that hidden state vectors corresponding to individual inputtokens encode information sufficient to accurately predict several tokensahead. More concretely, in this paper we ask: Given a hidden (internal)representation of a single token at position $t$ in an input, can we reliablyanticipate the tokens that will appear at positions $\geq t + 2$? To test this,we measure linear approximation and causal intervention methods in GPT-J-6B toevaluate the degree to which individual hidden states in the network containsignal rich enough to predict future hidden states and, ultimately, tokenoutputs. We find that, at some layers, we can approximate a model's output withmore than 48% accuracy with respect to its prediction of subsequent tokensthrough a single hidden state. Finally we present a "Future Lens" visualizationthat uses these methods to create a new view of transformer states.</description><author>Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C. Wallace, David Bau</author><pubDate>Wed, 08 Nov 2023 18:56:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04897v1</guid></item><item><title>Optimized measurements of chaotic dynamical systems via the information bottleneck</title><link>http://arxiv.org/abs/2311.04896v1</link><description>Deterministic chaos permits a precise notion of a "perfect measurement" asone that, when obtained repeatedly, captures all of the information created bythe system's evolution with minimal redundancy. Finding an optimal measurementis challenging, and has generally required intimate knowledge of the dynamicsin the few cases where it has been done. We establish an equivalence between aperfect measurement and a variant of the information bottleneck. As aconsequence, we can employ machine learning to optimize measurement processesthat efficiently extract information from trajectory data. We obtainapproximately optimal measurements for multiple chaotic maps and lay thenecessary groundwork for efficient information extraction from general timeseries.</description><author>Kieran A. Murphy, Dani S. Bassett</author><pubDate>Wed, 08 Nov 2023 18:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04896v1</guid></item><item><title>Three Bricks to Consolidate Watermarks for Large Language Models</title><link>http://arxiv.org/abs/2308.00113v2</link><description>The task of discerning between generated and natural texts is increasinglychallenging. In this context, watermarking emerges as a promising technique forascribing generated text to a specific model. It alters the sampling generationprocess so as to leave an invisible trace in the generated output, facilitatinglater detection. This research consolidates watermarks for large languagemodels based on three theoretical and empirical considerations. First, weintroduce new statistical tests that offer robust theoretical guarantees whichremain valid even at low false-positive rates (less than 10$^{\text{-6}}$).Second, we compare the effectiveness of watermarks using classical benchmarksin the field of natural language processing, gaining insights into theirreal-world applicability. Third, we develop advanced detection schemes forscenarios where access to the LLM is available, as well as multi-bitwatermarking.</description><author>Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, Teddy Furon</author><pubDate>Wed, 08 Nov 2023 18:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00113v2</guid></item><item><title>DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets</title><link>http://arxiv.org/abs/2311.04894v1</link><description>Construction of a universal detector poses a crucial question: How can wemost effectively train a model on a large mixture of datasets? The answer liesin learning dataset-specific features and ensembling their knowledge but do allthis in a single model. Previous methods achieve this by having separatedetection heads on a common backbone but that results in a significant increasein parameters. In this work, we present Mixture-of-Experts as a solution,highlighting that MoEs are much more than a scalability tool. We proposeDataset-Aware Mixture-of-Experts, DAMEX where we train the experts to become an`expert' of a dataset by learning to route each dataset tokens to its mappedexpert. Experiments on Universal Object-Detection Benchmark show that weoutperform the existing state-of-the-art by average +10.2 AP score and improveover our non-MoE baseline by average +2.0 AP score. We also observe consistentgains while mixing datasets with (1) limited availability, (2) disparatedomains and (3) divergent label sets. Further, we qualitatively show that DAMEXis robust against expert representation collapse.</description><author>Yash Jain, Harkirat Behl, Zsolt Kira, Vibhav Vineet</author><pubDate>Wed, 08 Nov 2023 18:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04894v1</guid></item><item><title>Causal disentanglement of multimodal data</title><link>http://arxiv.org/abs/2310.18471v2</link><description>Causal representation learning algorithms discover lower-dimensionalrepresentations of data that admit a decipherable interpretation of cause andeffect; as achieving such interpretable representations is challenging, manycausal learning algorithms utilize elements indicating prior information, suchas (linear) structural causal models, interventional data, or weak supervision.Unfortunately, in exploratory causal representation learning, such elements andprior information may not be available or warranted. Alternatively, scientificdatasets often have multiple modalities or physics-based constraints, and theuse of such scientific, multimodal data has been shown to improvedisentanglement in fully unsupervised settings. Consequently, we introduce acausal representation learning algorithm (causalPIMA) that can use multimodaldata and known physics to discover important features with causalrelationships. Our innovative algorithm utilizes a new differentiableparametrization to learn a directed acyclic graph (DAG) together with a latentspace of a variational autoencoder in an end-to-end differentiable frameworkvia a single, tractable evidence lower bound loss function. We place a Gaussianmixture prior on the latent space and identify each of the mixtures with anoutcome of the DAG nodes; this novel identification enables feature discoverywith causal relationships. Tested against a synthetic and a scientific dataset,our results demonstrate the capability of learning an interpretable causalstructure while simultaneously discovering key features in a fully unsupervisedsetting.</description><author>Elise Walker, Jonas A. Actor, Carianne Martinez, Nathaniel Trask</author><pubDate>Wed, 08 Nov 2023 18:54:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18471v2</guid></item><item><title>Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs</title><link>http://arxiv.org/abs/2311.04892v1</link><description>Recent works have showcased the ability of large-scale language models (LLMs)to embody diverse personas in their responses, exemplified by prompts like 'Youare Yoda. Explain the Theory of Relativity.' While this ability allowspersonalization of LLMs and enables human behavior simulation, its effect onLLMs' capabilities remain unclear. To fill this gap, we present the firstextensive study of the unintended side-effects of persona assignment on theability of LLMs, specifically ChatGPT, to perform basic reasoning tasks. Ourstudy covers 24 reasoning datasets and 16 diverse personas spanning 5socio-demographic groups: race, gender, religion, disability, and politicalaffiliation. Our experiments unveil that ChatGPT carries deep rooted biasagainst various socio-demographics underneath a veneer of fairness. While itovertly rejects stereotypes when explicitly asked ('Are Black people lessskilled at mathematics?'), it manifests stereotypical and often erroneouspresumptions when prompted to answer questions while taking on a persona. Thesecan be observed as abstentions in the model responses, e.g., 'As a Blackperson, I am unable to answer this question as it requires math knowledge', andgenerally result in a substantial drop in performance on reasoning tasks. Wefind that this inherent deep bias is ubiquitous - 80% of our personasdemonstrated bias; it is significant - certain datasets had relative drops inperformance of 70%+; and can be especially harmful for certain groups - certainpersonas had stat. sign. drops on more than 80% of the datasets. Furtheranalysis shows that these persona-induced errors can be hard-to-discern andhard-to-avoid. Our findings serve as a cautionary tale that the practice ofassigning personas to LLMs - a trend on the rise - can surface theirdeep-rooted biases and have unforeseeable and detrimental side-effects.</description><author>Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, Tushar Khot</author><pubDate>Wed, 08 Nov 2023 18:52:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04892v1</guid></item><item><title>Towards Few-Annotation Learning in Computer Vision: Application to Image Classification and Object Detection tasks</title><link>http://arxiv.org/abs/2311.04888v1</link><description>In this thesis, we develop theoretical, algorithmic and experimentalcontributions for Machine Learning with limited labels, and more specificallyfor the tasks of Image Classification and Object Detection in Computer Vision.In a first contribution, we are interested in bridging the gap between theoryand practice for popular Meta-Learning algorithms used in Few-ShotClassification. We make connections to Multi-Task Representation Learning,which benefits from solid theoretical foundations, to verify the bestconditions for a more efficient meta-learning. Then, to leverage unlabeled datawhen training object detectors based on the Transformer architecture, wepropose both an unsupervised pretraining and a semi-supervised learning methodin two other separate contributions. For pretraining, we improve ContrastiveLearning for object detectors by introducing the localization information.Finally, our semi-supervised method is the first tailored to transformer-baseddetectors.</description><author>Quentin Bouniot</author><pubDate>Wed, 08 Nov 2023 18:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04888v1</guid></item><item><title>Robust Mean Estimation Without Moments for Symmetric Distributions</title><link>http://arxiv.org/abs/2302.10844v2</link><description>We study the problem of robustly estimating the mean or location parameterwithout moment assumptions. We show that for a large class of symmetricdistributions, the same error as in the Gaussian setting can be achievedefficiently. The distributions we study include products of arbitrary symmetricone-dimensional distributions, such as product Cauchy distributions, as well aselliptical distributions. For product distributions and elliptical distributions with known scatter(covariance) matrix, we show that given an $\varepsilon$-corrupted sample, wecan with probability at least $1-\delta$ estimate its location up to error$O(\varepsilon \sqrt{\log(1/\varepsilon)})$ using $\tfrac{d\log(d) +\log(1/\delta)}{\varepsilon^2 \log(1/\varepsilon)}$ samples. This resultmatches the best-known guarantees for the Gaussian distribution and known SQlower bounds (up to the $\log(d)$ factor). For elliptical distributions withunknown scatter (covariance) matrix, we propose a sequence of efficientalgorithms that approaches this optimal error. Specifically, for every $k \in\mathbb{N}$, we design an estimator using time and samples $\tilde{O}({d^k})$achieving error $O(\varepsilon^{1-\frac{1}{2k}})$. This matches the error andrunning time guarantees when assuming certifiably bounded moments of order upto $k$. For unknown covariance, such error bounds of $o(\sqrt{\varepsilon})$are not even known for (general) sub-Gaussian distributions. Our algorithms are based on a generalization of the well-known filteringtechnique. We show how this machinery can be combined with Huber-loss-basedtechniques to work with projections of the noise that behave more nicely thanthe initial noise. Moreover, we show how SoS proofs can be used to obtainalgorithmic guarantees even for distributions without a first moment. Webelieve that this approach may find other applications in future works.</description><author>Gleb Novikov, David Steurer, Stefan Tiegel</author><pubDate>Wed, 08 Nov 2023 18:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10844v2</guid></item><item><title>SEMQA: Semi-Extractive Multi-Source Question Answering</title><link>http://arxiv.org/abs/2311.04886v1</link><description>Recently proposed long-form question answering (QA) systems, supported bylarge language models (LLMs), have shown promising capabilities. Yet,attributing and verifying their generated abstractive answers can be difficult,and automatically evaluating their accuracy remains an ongoing challenge. In this work, we introduce a new QA task for answering multi-answer questionsby summarizing multiple diverse sources in a semi-extractive fashion.Specifically, Semi-extractive Multi-source QA (SEMQA) requires models to outputa comprehensive answer, while mixing factual quoted spans -- copied verbatimfrom given input sources -- and non-factual free-text connectors that gluethese spans together into a single cohesive passage. This setting bridges thegap between the outputs of well-grounded but constrained extractive QA systemsand more fluent but harder to attribute fully abstractive answers.Particularly, it enables a new mode for language models that leverages theiradvanced language generation capabilities, while also producing fine in-lineattributions by-design that are easy to verify, interpret, and evaluate. To study this task, we create the first dataset of this kind, QuoteSum, withhuman-written semi-extractive answers to natural and generated questions, anddefine text-based evaluation metrics. Experimenting with several LLMs invarious settings, we find this task to be surprisingly challenging,demonstrating the importance of QuoteSum for developing and studying suchconsolidation capabilities.</description><author>Tal Schuster, Adam D. Lelkes, Haitian Sun, Jai Gupta, Jonathan Berant, William W. Cohen, Donald Metzler</author><pubDate>Wed, 08 Nov 2023 18:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04886v1</guid></item><item><title>Survival Instinct in Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2306.03286v2</link><description>We present a novel observation about the behavior of offline reinforcementlearning (RL) algorithms: on many benchmark datasets, offline RL can producewell-performing and safe policies even when trained with "wrong" reward labels,such as those that are zero everywhere or are negatives of the true rewards.This phenomenon cannot be easily explained by offline RL's return maximizationobjective. Moreover, it gives offline RL a degree of robustness that isuncharacteristic of its online RL counterparts, which are known to be sensitiveto reward design. We demonstrate that this surprising robustness property isattributable to an interplay between the notion of pessimism in offline RLalgorithms and certain implicit biases in common data collection practices. Aswe prove in this work, pessimism endows the agent with a "survival instinct",i.e., an incentive to stay within the data support in the long term, while thelimited and biased data coverage further constrains the set of survivalpolicies. Formally, given a reward class -- which may not even contain the truereward -- we identify conditions on the training data distribution that enableoffline RL to learn a near-optimal and safe policy from any reward within theclass. We argue that the survival instinct should be taken into account wheninterpreting results from existing offline RL benchmarks and when creatingfuture ones. Our empirical and theoretical results suggest a new paradigm forRL, whereby an agent is nudged to learn a desirable behavior with imperfectreward but purposely biased data coverage.</description><author>Anqi Li, Dipendra Misra, Andrey Kolobov, Ching-An Cheng</author><pubDate>Wed, 08 Nov 2023 18:46:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03286v2</guid></item><item><title>Profiling Irony &amp; Stereotype: Exploring Sentiment, Topic, and Lexical Features</title><link>http://arxiv.org/abs/2311.04885v1</link><description>Social media has become a very popular source of information. With thispopularity comes an interest in systems that can classify the informationproduced. This study tries to create such a system detecting irony in Twitterusers. Recent work emphasize the importance of lexical features, sentimentfeatures and the contrast herein along with TF-IDF and topic models. Based on athorough feature selection process, the resulting model contains specificsub-features from these areas. Our model reaches an F1-score of 0.84, which isabove the baseline. We find that lexical features, especially TF-IDF,contribute the most to our models while sentiment and topic modeling featurescontribute less to overall performance. Lastly, we highlight multipleinteresting and important paths for further exploration.</description><author>Tibor L. R. Krols, Marie Mortensen, Ninell Oldenburg</author><pubDate>Wed, 08 Nov 2023 18:44:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04885v1</guid></item><item><title>Contextualizing Argument Quality Assessment with Relevant Knowledge</title><link>http://arxiv.org/abs/2305.12280v2</link><description>Automatic assessment of the quality of arguments has been recognized as achallenging task with significant implications for misinformation and targetedspeech. While real-world arguments are tightly anchored in context, existingcomputational methods analyze their quality in isolation, which affects theiraccuracy and generalizability. We propose SPARK: a novel method for scoringargument quality based on contextualization via relevant knowledge. We devisefour augmentations that leverage large language models to provide feedback,infer hidden assumptions, supply a similar-quality argument, or give acounter-argument. SPARK uses a dual-encoder Transformer architecture to enablethe original argument and its augmentation to be considered jointly. Ourexperiments in both in-domain and zero-shot setups show that SPARK consistentlyoutperforms existing techniques across multiple metrics.</description><author>Darshan Deshpande, Zhivar Sourati, Filip Ilievski, Fred Morstatter</author><pubDate>Wed, 08 Nov 2023 18:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12280v2</guid></item><item><title>LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models</title><link>http://arxiv.org/abs/2311.04879v1</link><description>We present LongQLoRA, an efficient and effective method to extend contextlength of large language models with less training resources. LongQLoRAcombines the advantages of Position Interpolation, QLoRA and Shift ShortAttention of LongLoRA. With a single 32GB V100 GPU, LongQLoRA can extend thecontext length of LLaMA2 7B and 13B from 4096 to 8192 and even to 12k within1000 finetuning steps. LongQLoRA achieves competitive perplexity performance onPG19 and Proof-pile datasets, our model outperforms LongLoRA and is very closeto MPT-7B-8K within the evaluation context length of 8192. We collect and build39k long instruction data to extend context length of Vicuna-13B from 4096 to8192 and achieve good performance both in long and short context generationtask. We also do some ablation experiments to study the effect of LoRA rank,finetuning steps and attention patterns in inference.The model weights,training data and code are avaliable athttps://github.com/yangjianxin1/LongQLoRA.</description><author>Jianxin Yang</author><pubDate>Wed, 08 Nov 2023 18:33:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04879v1</guid></item><item><title>Foundation Models for Generalist Geospatial Artificial Intelligence</title><link>http://arxiv.org/abs/2310.18660v2</link><description>Significant progress in the development of highly adaptable and reusableArtificial Intelligence (AI) models is expected to have a significant impact onEarth science and remote sensing. Foundation models are pre-trained on largeunlabeled datasets through self-supervision, and then fine-tuned for variousdownstream tasks with small labeled datasets. This paper introduces afirst-of-a-kind framework for the efficient pre-training and fine-tuning offoundational models on extensive geospatial data. We have utilized thisframework to create Prithvi, a transformer-based geospatial foundational modelpre-trained on more than 1TB of multispectral satellite imagery from theHarmonized Landsat-Sentinel 2 (HLS) dataset. Our study demonstrates theefficacy of our framework in successfully fine-tuning Prithvi to a range ofEarth observation tasks that have not been tackled by previous work onfoundation models involving multi-temporal cloud gap imputation, flood mapping,wildfire scar segmentation, and multi-temporal crop segmentation. Ourexperiments show that the pre-trained model accelerates the fine-tuning processcompared to leveraging randomly initialized weights. In addition, pre-trainedPrithvi compares well against the state-of-the-art, e.g., outperforming aconditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%)in the structural similarity index. Finally, due to the limited availability oflabeled data in the field of Earth observation, we gradually reduce thequantity of available labeled data for refining the model to evaluate dataefficiency and demonstrate that data can be decreased significantly withoutaffecting the model's accuracy. The pre-trained 100 million parameter model andcorresponding fine-tuning workflows have been released publicly as open sourcecontributions to the global Earth sciences community through Hugging Face.</description><author>Johannes Jakubik, Sujit Roy, C. E. Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, Daiki Kimura, Naomi Simumba, Linsong Chu, S. Karthik Mukkavilli, Devyani Lambhate, Kamal Das, Ranjini Bangalore, Dario Oliveira, Michal Muszynski, Kumar Ankur, Muthukumaran Ramasubramanian, Iksha Gurung, Sam Khallaghi, Hanxi, Li, Michael Cecil, Maryam Ahmadi, Fatemeh Kordi, Hamed Alemohammad, Manil Maskey, Raghu Ganti, Kommy Weldemariam, Rahul Ramachandran</author><pubDate>Wed, 08 Nov 2023 18:25:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18660v2</guid></item><item><title>Learning Performance-Improving Code Edits</title><link>http://arxiv.org/abs/2302.07867v4</link><description>With the waning of Moore's law, optimizing program performance has become amajor focus of software research. However, high-level optimizations such as APIand algorithm changes remain elusive due to the difficulty of understanding thesemantics of code. Simultaneously, pretrained large language models (LLMs) havedemonstrated strong capabilities at solving a wide range of programming tasks.To that end, we introduce a framework for adapting LLMs to high-level programoptimization. First, we curate a dataset of performance-improving edits made byhuman programmers of over 77K competitive C++ programming submission pairs,accompanied by extensive unit tests. A major challenge is the significantvariability of measuring performance on commodity hardware, which can lead tospurious "improvements". To isolate and reliably evaluate the impact of programoptimizations, we design an environment based on the gem5 full systemsimulator, the de facto simulator used in academia and industry. Next, wepropose a broad range of adaptation strategies for code optimization; forprompting, these include retrieval-based few-shot prompting andchain-of-thought, and for finetuning, these include performance-conditionedgeneration and synthetic data augmentation based on self-play. A combination ofthese techniques achieves an average speedup of 5.65X on CodeLlama-13B and6.86X on GPT-3.5, surpassing the best human performance (4.06X). We find ourproposed performance-conditioned generation is particularly effective atimproving performance as well as increasing the fraction of optimized programs.</description><author>Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh</author><pubDate>Wed, 08 Nov 2023 18:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07867v4</guid></item><item><title>Computing with Residue Numbers in High-Dimensional Representation</title><link>http://arxiv.org/abs/2311.04872v1</link><description>We introduce Residue Hyperdimensional Computing, a computing framework thatunifies residue number systems with an algebra defined over random,high-dimensional vectors. We show how residue numbers can be represented ashigh-dimensional vectors in a manner that allows algebraic operations to beperformed with component-wise, parallelizable operations on the vectorelements. The resulting framework, when combined with an efficient method forfactorizing high-dimensional vectors, can represent and operate on numericalvalues over a large dynamic range using vastly fewer resources than previousmethods, and it exhibits impressive robustness to noise. We demonstrate thepotential for this framework to solve computationally difficult problems invisual perception and combinatorial optimization, showing improvement overbaseline methods. More broadly, the framework provides a possible account forthe computational operations of grid cells in the brain, and it suggests newmachine learning architectures for representing and manipulating numericaldata.</description><author>Christopher J. Kymn, Denis Kleyko, E. Paxon Frady, Connor Bybee, Pentti Kanerva, Friedrich T. Sommer, Bruno A. Olshausen</author><pubDate>Wed, 08 Nov 2023 18:19:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04872v1</guid></item><item><title>Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression</title><link>http://arxiv.org/abs/2306.15063v2</link><description>Pretrained transformers exhibit the remarkable ability of in-context learning(ICL): they can learn tasks from just a few examples provided in the promptwithout updating any weights. This raises a foundational question: can ICLsolve fundamentally $\textit{new}$ tasks that are very different from thoseseen during pretraining? To probe this question, we examine ICL's performanceon linear regression while varying the diversity of tasks in the pretrainingdataset. We empirically demonstrate a $\textit{task diversity threshold}$ forthe emergence of ICL. Below this threshold, the pretrained transformer cannotsolve unseen regression tasks, instead behaving like a Bayesian estimator withthe $\textit{non-diverse pretraining task distribution}$ as the prior. Beyondthis threshold, the transformer significantly outperforms this estimator; itsbehavior aligns with that of ridge regression, corresponding to a Gaussianprior over $\textit{all tasks}$, including those not seen during pretraining.Thus, when pretrained on data with task diversity greater than the threshold,transformers $\textit{can}$ optimally solve fundamentally new tasks in-context.Importantly, this capability hinges on it deviating from the Bayes optimalestimator with the pretraining distribution as the prior. This study alsoexplores the effect of regularization, model capacity and task structure andunderscores, in a concrete example, the critical role of task diversity,alongside data and model scale, in the emergence of ICL. Code is available athttps://github.com/mansheej/icl-task-diversity.</description><author>Allan Raventós, Mansheej Paul, Feng Chen, Surya Ganguli</author><pubDate>Wed, 08 Nov 2023 18:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15063v2</guid></item><item><title>Robotic Learning the Sequence of Packing Irregular Objects from Human Demonstrations</title><link>http://arxiv.org/abs/2210.01645v2</link><description>We tackle the challenge of robotic bin packing with irregular objects, suchas groceries. Given the diverse physical attributes of these objects and thecomplex constraints governing their placement and manipulation, employingpreprogrammed strategies becomes unfeasible. Our approach is to learn directlyfrom expert demonstrations in order to extract implicit task knowledge andstrategies to ensure safe object positioning, efficient use of space, and thegeneration of human-like behaviors that enhance human-robot trust. We rely on human demonstrations to learn a Markov chain for predicting theobject packing sequence for a given set of items and then compare it with humanperformance. Our experimental results show that the model outperforms humanperformance by generating sequence predictions that humans classify ashuman-like more frequently than human-generated sequences. The human demonstrations were collected using our proposed VR platform,BoxED, which is a box packaging environment for simulating real-world objectsand scenarios for fast and streamlined data collection with the purpose ofteaching robots. We collected data from 43 participants packing a total of 263boxes with supermarket-like objects, yielding 4644 object manipulations. Our VRplatform can be easily adapted to new scenarios and objects, and is publiclyavailable, alongside our dataset, at https://github.com/andrejfsantos4/BoxED.</description><author>André Santos, Nuno Ferreira Duarte, Atabak Dehban, José Santos-Victor</author><pubDate>Wed, 08 Nov 2023 18:11:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01645v2</guid></item><item><title>Chain-of-Thought Reasoning is a Policy Improvement Operator</title><link>http://arxiv.org/abs/2309.08589v2</link><description>Large language models have astounded the world with fascinating newcapabilities. However, they currently lack the ability to teach themselves newskills, relying instead on large amounts of human-generated training data. Weintroduce SECToR (Self-Education via Chain-of-Thought Reasoning), aproof-of-concept demonstration that language models can teach themselves newskills using chain-of-thought reasoning. During the self-learning loop, SECToRasks models to solve addition problems using chain-of-thought reasoning beforetraining the next version of the model to solve those same problems directlywithout using such reasoning. This process often results in an improved modelwhich can, when again augmented with chain-of-thought reasoning, solve evenharder problems than the original model, allowing the self-learning loop tocontinue. Language models trained via SECToR autonomously learn to add up tothe longest-length-digit numbers without access to any ground truth examplesbeyond an initial supervised fine-tuning phase consisting only of numbers with6 or fewer digits. Our central hypothesis is that chain-of-thought reasoningcan act as a policy improvement operator, similarly to how Monte-Carlo TreeSearch is used in AlphaZero (Silver et al., 2017). We hope that this researchcan lead to new directions in which language models can learn to teachthemselves without the need for human demonstrations.</description><author>Hugh Zhang, David C. Parkes</author><pubDate>Wed, 08 Nov 2023 18:10:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08589v2</guid></item><item><title>Multitask Kernel-based Learning with First-Order Logic Constraints</title><link>http://arxiv.org/abs/2311.03340v2</link><description>In this paper we propose a general framework to integrate supervised andunsupervised examples with background knowledge expressed by a collection offirst-order logic clauses into kernel machines. In particular, we consider amulti-task learning scheme where multiple predicates defined on a set ofobjects are to be jointly learned from examples, enforcing a set of FOLconstraints on the admissible configurations of their values. The predicatesare defined on the feature spaces, in which the input objects are represented,and can be either known a priori or approximated by an appropriate kernel-basedlearner. A general approach is presented to convert the FOL clauses into acontinuous implementation that can deal with the outputs computed by thekernel-based predicates. The learning problem is formulated as asemi-supervised task that requires the optimization in the primal of a lossfunction that combines a fitting loss measure on the supervised examples, aregularization term, and a penalty term that enforces the constraints on boththe supervised and unsupervised examples. Unfortunately, the penalty term isnot convex and it can hinder the optimization process. However, it is possibleto avoid poor solutions by using a two stage learning schema, in which thesupervised examples are learned first and then the constraints are enforced.</description><author>Michelangelo Diligenti, Marco Gori, Marco Maggini, Leonardo Rigutini</author><pubDate>Wed, 08 Nov 2023 18:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03340v2</guid></item><item><title>Interpretability, Generalizability, and Memory of Reinforcement Learning Agents in Closed Drafting Games</title><link>http://arxiv.org/abs/2310.20654v2</link><description>Closed drafting or "pick and pass" is a popular game mechanic where eachround players select a card or other playable element from their hand and passthe rest to the next player. In this paper, we establish first-principleinterpretability, generalizability, and memory benchmarks for studyingmodel-free reinforcement learning (RL) algorithms playing closed draftinggames. Specifically in a popular family of closed drafting games called "SushiGo Party!", in which we achieve state-of-the-art performance. We fit decisionrules to interpret the strategy of trained RL agents and compare these to theranking preferences of different types of human players, finding easilyunderstandable explanations of the disparate performance of RL agents in thisenvironment. As Sushi Go Party! can be expressed as a set of closely-relatedgames based on the set of cards in play, we quantify the generalizability of RLmodels trained on various sets of cards, establishing key trends betweenperformance and the set distance between the train and evaluation gameconfigurations. Using the explicitly calculable memory of other player's handsin closed drafting games, we create measures of the ability of RL models tolearn memory.</description><author>Ryan Rezai, Jason Wang</author><pubDate>Wed, 08 Nov 2023 17:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20654v2</guid></item><item><title>Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values</title><link>http://arxiv.org/abs/2311.04855v1</link><description>Non-negative matrix factorization (NMF) is a dimensionality reductiontechnique that has shown promise for analyzing noisy data, especiallyastronomical data. For these datasets, the observed data may contain negativevalues due to noise even when the true underlying physical signal is strictlypositive. Prior NMF work has not treated negative data in a statisticallyconsistent manner, which becomes problematic for low signal-to-noise data withmany negative values. In this paper we present two algorithms, Shift-NMF andNearly-NMF, that can handle both the noisiness of the input data and also anyintroduced negativity. Both of these algorithms use the negative data spacewithout clipping, and correctly recover non-negative signals without anyintroduced positive offset that occurs when clipping negative data. Wedemonstrate this numerically on both simple and more realistic examples, andprove that both algorithms have monotonically decreasing update rules.</description><author>Dylan Green, Stephen Bailey</author><pubDate>Wed, 08 Nov 2023 17:50:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04855v1</guid></item><item><title>Variational Classification</title><link>http://arxiv.org/abs/2305.10406v3</link><description>We present a latent variable model for classification that provides a novelprobabilistic interpretation of neural network softmax classifiers. We derive avariational objective to train the model, analogous to the evidence lower bound(ELBO) used to train variational auto-encoders, that generalises thecross-entropy loss used to train classification models. Treating inputs to thesoftmax layer as samples of a latent variable, our abstracted perspectivereveals a potential inconsistency between their anticipated distribution,required for accurate label predictions to be output, and the empiricaldistribution found in practice. We augment the variational objective tomitigate such inconsistency and encourage a chosen latent distribution, insteadof the implicit assumption in off-the-shelf softmax classifiers. Overall, weprovide new theoretical insight into the inner workings of widely-used softmaxclassification. Empirical evaluation on image and text classification datasetsdemonstrates that our proposed approach, variational classification, maintainsclassification accuracy while the reshaped latent space improves otherdesirable properties of a classifier, such as calibration, adversarialrobustness, robustness to distribution shift and sample efficiency useful inlow data settings.</description><author>Shehzaad Dhuliawala, Mrinmaya Sachan, Carl Allen</author><pubDate>Wed, 08 Nov 2023 17:43:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10406v3</guid></item><item><title>Efficient and Equivariant Graph Networks for Predicting Quantum Hamiltonian</title><link>http://arxiv.org/abs/2306.04922v2</link><description>We consider the prediction of the Hamiltonian matrix, which finds use inquantum chemistry and condensed matter physics. Efficiency and equivariance aretwo important, but conflicting factors. In this work, we propose aSE(3)-equivariant network, named QHNet, that achieves efficiency andequivariance. Our key advance lies at the innovative design of QHNetarchitecture, which not only obeys the underlying symmetries, but also enablesthe reduction of number of tensor products by 92\%. In addition, QHNet preventsthe exponential growth of channel dimension when more atom types are involved.We perform experiments on MD17 datasets, including four molecular systems.Experimental results show that our QHNet can achieve comparable performance tothe state of the art methods at a significantly faster speed. Besides, ourQHNet consumes 50\% less memory due to its streamlined architecture. Our codeis publicly available as part of the AIRS library(\url{https://github.com/divelab/AIRS}).</description><author>Haiyang Yu, Zhao Xu, Xiaofeng Qian, Xiaoning Qian, Shuiwang Ji</author><pubDate>Wed, 08 Nov 2023 17:43:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04922v2</guid></item><item><title>Rethinking Benchmark and Contamination for Language Models with Rephrased Samples</title><link>http://arxiv.org/abs/2311.04850v1</link><description>Large language models are increasingly trained on all the data ever producedby humans. Many have raised concerns about the trustworthiness of publicbenchmarks due to potential contamination in pre-training or fine-tuningdatasets. While most data decontamination efforts apply string matching (e.g.,n-gram overlap) to remove benchmark data, we show that these methods areinsufficient, and simple variations of test data (e.g., paraphrasing,translation) can easily bypass these decontamination measures. Furthermore, wedemonstrate that if such variation of test data is not eliminated, a 13B modelcan easily overfit a test benchmark and achieve drastically high performance,on par with GPT-4. We validate such observations in widely used benchmarks suchas MMLU, GSK8k, and HumanEval. To address this growing risk, we propose astronger LLM-based decontamination method and apply it to widely usedpre-training and fine-tuning datasets, revealing significant previously unknowntest overlap. For example, in pre-training sets such as RedPajama-Data-1T andStarCoder-Data, we identified that 8-18\% of the HumanEval benchmark overlaps.Interestingly, we also find such contamination in synthetic dataset generatedby GPT-3.5/4, suggesting a potential risk of unintentional contamination. Weurge the community to adopt stronger decontamination approaches when usingpublic benchmarks. Moreover, we call for the community to actively developfresh one-time exams to evaluate models accurately. Our decontamination tool ispublicly available at https://github.com/lm-sys/llm-decontaminator.</description><author>Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica</author><pubDate>Wed, 08 Nov 2023 17:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04850v1</guid></item><item><title>Are foundation models efficient for medical image segmentation?</title><link>http://arxiv.org/abs/2311.04847v1</link><description>Foundation models are experiencing a surge in popularity. The SegmentAnything model (SAM) asserts an ability to segment a wide spectrum of objectsbut required supervised training at unprecedented scale. We compared SAM'sperformance (against clinical ground truth) and resources (labeling time,compute) to a modality-specific, label-free self-supervised learning (SSL)method on 25 measurements for 100 cardiac ultrasounds. SAM performed poorly andrequired significantly more labeling and computing resources, demonstratingworse efficiency than SSL.</description><author>Danielle Ferreira, Rima Arnaout</author><pubDate>Wed, 08 Nov 2023 17:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04847v1</guid></item><item><title>Incorporating temporal dynamics of mutations to enhance the prediction capability of antiretroviral therapy's outcome for HIV-1</title><link>http://arxiv.org/abs/2311.04846v1</link><description>Motivation: In predicting HIV therapy outcomes, a critical clinical questionis whether using historical information can enhance predictive capabilitiescompared with current or latest available data analysis. This study analyseswhether historical knowledge, which includes viral mutations detected in allgenotypic tests before therapy, their temporal occurrence, and concomitantviral load measurements, can bring improvements. We introduce a method to weighmutations, considering the previously enumerated factors and the referencemutation-drug Stanford resistance tables. We compare a model encompassinghistory (H) with one not using it (NH). Results: The H-model demonstratessuperior discriminative ability, with a higher ROC-AUC score (76.34%) than theNH-model (74.98%). Significant Wilcoxon test results confirm that incorporatinghistorical information improves consistently predictive accuracy for treatmentoutcomes. The better performance of the H-model might be attributed to itsconsideration of latent HIV reservoirs, probably obtained when leveraginghistorical information. The findings emphasize the importance of temporaldynamics in mutations, offering insights into HIV infection complexities.However, our result also shows that prediction accuracy remains relatively higheven when no historical information is available. Supplementary information:Supplementary material is available.</description><author>Giulia Di Teodoro, Martin Pirkl, Francesca Incardona, Ilaria Vicenti, Anders Sönnerborg, Rolf Kaiser, Laura Palagi, Maurizio Zazzi, Thomas Lengauer</author><pubDate>Wed, 08 Nov 2023 17:29:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04846v1</guid></item><item><title>Bridging Dimensions: Confident Reachability for High-Dimensional Controllers</title><link>http://arxiv.org/abs/2311.04843v1</link><description>Autonomous systems are increasingly implemented using end-end-end trainedcontrollers. Such controllers make decisions that are executed on the realsystem with images as one of the primary sensing modalities. Deep neuralnetworks form a fundamental building block of such controllers. Unfortunately,the existing neural-network verification tools do not scale to inputs withthousands of dimensions. Especially when the individual inputs (such as pixels)are devoid of clear physical meaning. This paper takes a step towardsconnecting exhaustive closed-loop verification with high-dimensionalcontrollers. Our key insight is that the behavior of a high-dimensionalcontroller can be approximated with several low-dimensional controllers indifferent regions of the state space. To balance approximation andverifiability, we leverage the latest verification-aware knowledgedistillation. Then, if low-dimensional reachability results are inflated withstatistical approximation errors, they yield a high-confidence reachabilityguarantee for the high-dimensional controller. We investigate two inflationtechniques -- based on trajectories and actions -- both of which showconvincing performance in two OpenAI gym benchmarks.</description><author>Yuang Geng, Souradeep Dutta, Ivan Ruchkin</author><pubDate>Wed, 08 Nov 2023 17:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04843v1</guid></item><item><title>Predictive Data Analytics with AI: assessing the need for post-editing of MT output by fine-tuning OpenAI LLMs</title><link>http://arxiv.org/abs/2308.00158v5</link><description>Translation Quality Evaluation (TQE) is an essential step of the moderntranslation production process. TQE is critical in assessing both machinetranslation (MT) and human translation (HT) quality without referencetranslations. The ability to evaluate or even simply estimate the quality oftranslation automatically may open significant efficiency gains through processoptimisation. This work examines whether the state-of-the-art large languagemodels (LLMs) can be used for this purpose. We take OpenAI models as the beststate-of-the-art technology and approach TQE as a binary classification task.On eight language pairs including English to Italian, German, French, Japanese,Dutch, Portuguese, Turkish, and Chinese, our experimental results show thatfine-tuned gpt3.5 can demonstrate good performance on translation qualityprediction tasks, i.e. whether the translation needs to be edited. Anotherfinding is that simply increasing the sizes of LLMs does not lead to apparentbetter performances on this task by comparing the performance of threedifferent versions of OpenAI models: curie, davinci, and gpt3.5 with 13B, 175B,and 175B parameters, respectively.</description><author>Serge Gladkoff, Gleb Erofeev, Irina Sorokina, Lifeng Han, Goran Nenadic</author><pubDate>Wed, 08 Nov 2023 17:25:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00158v5</guid></item><item><title>Physics informed machine learning with Smoothed Particle Hydrodynamics: Hierarchy of reduced Lagrangian models of turbulence</title><link>http://arxiv.org/abs/2110.13311v7</link><description>Building efficient, accurate and generalizable reduced order models ofdeveloped turbulence remains a major challenge. This manuscript approaches thisproblem by developing a hierarchy of parameterized reduced Lagrangian modelsfor turbulent flows, and investigates the effects of enforcing physicalstructure through Smoothed Particle Hydrodynamics (SPH) versus relying onneural networks (NN)s as universal function approximators. Starting from NeuralNetwork (NN) parameterizations of a Lagrangian acceleration operator, thishierarchy of models gradually incorporates a weakly compressible andparameterized SPH framework, which enforces physical symmetries, such asGalilean, rotational and translational invariances. Within this hierarchy, twonew parameterized smoothing kernels are developed in order to increase theflexibility of the learn-able SPH simulators. For each model we experiment withdifferent loss functions which are minimized using gradient based optimization,where efficient computations of gradients are obtained by using AutomaticDifferentiation (AD) and Sensitivity Analysis (SA). Each model within thehierarchy is trained on two data sets associated with weekly compressibleHomogeneous Isotropic Turbulence (HIT): (1) a validation set using weaklycompressible SPH; and (2) a high fidelity set from Direct Numerical Simulations(DNS). Numerical evidence shows that encoding more SPH structure improvesgeneralizability to different turbulent Mach numbers and time shifts, and thatincluding the novel parameterized smoothing kernels improves the accuracy ofSPH at the resolved scales.</description><author>Michael Woodward, Yifeng Tian, Criston Hyett, Chris Fryer, Daniel Livescu, Mikhail Stepanov, Michael Chertkov</author><pubDate>Wed, 08 Nov 2023 17:22:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.13311v7</guid></item><item><title>A Review on Car-Following Model</title><link>http://arxiv.org/abs/2304.07143v2</link><description>The car-following (CF) model is the core component for traffic simulationsand has been built-in in many production vehicles with Advanced DrivingAssistance Systems (ADAS). Research of CF behavior allows us to identify thesources of different macro phenomena induced by the basic process of pairwisevehicle interaction. The CF behavior and control model encompasses variousfields, such as traffic engineering, physics, cognitive science, machinelearning, and reinforcement learning. This paper provides a comprehensivesurvey highlighting differences, complementarities, and overlaps among variousCF models according to their underlying logic and principles. We reviewedrepresentative algorithms, ranging from the theory-based kinematic models,stimulus-response models, and cruise control models to data-driven BehaviorCloning (BC) and Imitation Learning (IL) and outlined their strengths andlimitations. This review categorizes CF models that are conceptualized invarying principles and summarize the vast literature with a holistic framework.</description><author>Tianya Zhang, Peter J. Jin, Alexandre Bayen, Ph. D., Benedetto Piccoli</author><pubDate>Wed, 08 Nov 2023 17:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07143v2</guid></item><item><title>Improving Fairness in Deepfake Detection</title><link>http://arxiv.org/abs/2306.16635v3</link><description>Despite the development of effective deepfake detectors in recent years,recent studies have demonstrated that biases in the data used to train thesedetectors can lead to disparities in detection accuracy across different racesand genders. This can result in different groups being unfairly targeted orexcluded from detection, allowing undetected deepfakes to manipulate publicopinion and erode trust in a deepfake detection model. While existing studieshave focused on evaluating fairness of deepfake detectors, to the best of ourknowledge, no method has been developed to encourage fairness in deepfakedetection at the algorithm level. In this work, we make the first attempt toimprove deepfake detection fairness by proposing novel loss functions thathandle both the setting where demographic information (eg, annotations of raceand gender) is available as well as the case where this information is absent.Fundamentally, both approaches can be used to convert many existing deepfakedetectors into ones that encourages fairness. Extensive experiments on fourdeepfake datasets and five deepfake detectors demonstrate the effectiveness andflexibility of our approach in improving deepfake detection fairness. Our codeis available at https://github.com/littlejuyan/DF_Fairness.</description><author>Yan Ju, Shu Hu, Shan Jia, George H. Chen, Siwei Lyu</author><pubDate>Wed, 08 Nov 2023 17:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16635v3</guid></item><item><title>Quality-Diversity through AI Feedback</title><link>http://arxiv.org/abs/2310.13032v3</link><description>In many text-generation problems, users may prefer not only a singleresponse, but a diverse range of high-quality outputs from which to choose.Quality-diversity (QD) search algorithms aim at such outcomes, by continuallyimproving and diversifying a population of candidates. However, theapplicability of QD to qualitative domains, like creative writing, has beenlimited by the difficulty of algorithmically specifying measures of quality anddiversity. Interestingly, recent developments in language models (LMs) haveenabled guiding search through AI feedback, wherein LMs are prompted in naturallanguage to evaluate qualitative aspects of text. Leveraging this development,we introduce Quality-Diversity through AI Feedback (QDAIF), wherein anevolutionary algorithm applies LMs to both generate variation and evaluate thequality and diversity of candidate text. When assessed on creative writingdomains, QDAIF covers more of a specified search space with high-qualitysamples than do non-QD controls. Further, human evaluation of QDAIF-generatedcreative texts validates reasonable agreement between AI and human evaluation.Our results thus highlight the potential of AI feedback to guide open-endedsearch for creative and original solutions, providing a recipe that seeminglygeneralizes to many domains and modalities. In this way, QDAIF is a steptowards AI systems that can independently search, diversify, evaluate, andimprove, which are among the core skills underlying human society's capacityfor innovation.</description><author>Herbie Bradley, Andrew Dai, Hannah Teufel, Jenny Zhang, Koen Oostermeijer, Marco Bellagente, Jeff Clune, Kenneth Stanley, Grégory Schott, Joel Lehman</author><pubDate>Wed, 08 Nov 2023 17:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13032v3</guid></item><item><title>Ultra-Long Sequence Distributed Transformer</title><link>http://arxiv.org/abs/2311.02382v2</link><description>Transformer models trained on long sequences often achieve higher accuracythan short sequences. Unfortunately, conventional transformers struggle withlong sequence training due to the overwhelming computation and memoryrequirements. Existing methods for long sequence training offer limited speedupand memory reduction, and may compromise accuracy. This paper presents a noveland efficient distributed training method, the Long Short-Sequence Transformer(LSS Transformer), for training transformer with long sequences. It distributesa long sequence into segments among GPUs, with each GPU computing a partialself-attention for its segment. Then, it uses a fused communication and a noveldouble gradient averaging technique to avoid the need to aggregate partialself-attention and minimize communication overhead. We evaluated theperformance between LSS Transformer and the state-of-the-art Nvidia sequenceparallelism on a Wikipedia enwik8 dataset. Results show that our proposedmethod lead to 5.6x faster and 10.2x more memory-efficient implementationcompared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs.Moreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32petaflops.</description><author>Xiao Wang, Isaac Lyngaas, Aristeidis Tsaris, Peng Chen, Sajal Dash, Mayanka Chandra Shekar, Tao Luo, Hong-Jun Yoon, Mohamed Wahib, John Gouley</author><pubDate>Wed, 08 Nov 2023 17:04:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02382v2</guid></item><item><title>Toward Rapid, Optimal, and Feasible Power Dispatch through Generalized Neural Mapping</title><link>http://arxiv.org/abs/2311.04838v1</link><description>The evolution towards a more distributed and interconnected grid necessitateslarge-scale decision-making within strict temporal constraints. Machinelearning (ML) paradigms have demonstrated significant potential in improvingthe efficacy of optimization processes. However, the feasibility of solutionsderived from ML models continues to pose challenges. It's imperative that MLmodels produce solutions that are attainable and realistic within the givensystem constraints of power systems. To address the feasibility issue andexpedite the solution search process, we proposed LOOP-LC 2.0(Learning toOptimize the Optimization Process with Linear Constraints version 2.0) as alearning-based approach for solving the power dispatch problem. A notableadvantage of the LOOP-LC 2.0 framework is its ability to ensure near-optimalityand strict feasibility of solutions without depending on computationallyintensive post-processing procedures, thus eliminating the need for iterativeprocesses. At the heart of the LOOP-LC 2.0 model lies the newly proposedgeneralized gauge map method, capable of mapping any infeasible solution to afeasible point within the linearly-constrained domain. The proposed generalizedgauge map method improves the traditional gauge map by exhibiting reducedsensitivity to input variances while increasing search speeds significantly.Utilizing the IEEE-200 test case as a benchmark, we demonstrate theeffectiveness of the LOOP-LC 2.0 methodology, confirming its superiorperformance in terms of training speed, computational time, optimality, andsolution feasibility compared to existing methodologies.</description><author>Meiyi Li, Javad Mohammadi</author><pubDate>Wed, 08 Nov 2023 17:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04838v1</guid></item><item><title>Identifying Semantic Component for Robust Molecular Property Prediction</title><link>http://arxiv.org/abs/2311.04837v1</link><description>Although graph neural networks have achieved great success in the task ofmolecular property prediction in recent years, their generalization abilityunder out-of-distribution (OOD) settings is still under-explored. Differentfrom existing methods that learn discriminative representations for prediction,we propose a generative model with semantic-components identifiability, namedSCI. We demonstrate that the latent variables in this generative model can beexplicitly identified into semantic-relevant (SR) and semantic-irrelevant (SI)components, which contributes to better OOD generalization by involving minimalchange properties of causal mechanisms. Specifically, we first formulate thedata generation process from the atom level to the molecular level, where thelatent space is split into SI substructures, SR substructures, and SR atomvariables. Sequentially, to reduce misidentification, we restrict the minimalchanges of the SR atom variables and add a semantic latent substructureregularization to mitigate the variance of the SR substructure under augmenteddomain changes. Under mild assumptions, we prove the block-wise identifiabilityof the SR substructure and the comment-wise identifiability of SR atomvariables. Experimental studies achieve state-of-the-art performance and showgeneral improvement on 21 datasets in 3 mainstream benchmarks. Moreover, thevisualization results of the proposed SCI method provide insightful casestudies and explanations for the prediction results. The code is available at:https://github.com/DMIRLAB-Group/SCI.</description><author>Zijian Li, Zunhong Xu, Ruichu Cai, Zhenhui Yang, Yuguang Yan, Zhifeng Hao, Guangyi Chen, Kun Zhang</author><pubDate>Wed, 08 Nov 2023 17:01:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04837v1</guid></item><item><title>Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction</title><link>http://arxiv.org/abs/2311.04834v1</link><description>We present a novel self-supervised approach for representation learning,particularly for the task of Visual Relationship Detection (VRD). Motivated bythe effectiveness of Masked Image Modeling (MIM), we propose Masked BoundingBox Reconstruction (MBBR), a variation of MIM where a percentage of theentities/objects within a scene are masked and subsequently reconstructed basedon the unmasked objects. The core idea is that, through object-level maskedmodeling, the network learns context-aware representations that capture theinteraction of objects within a scene and thus are highly predictive of visualobject relationships. We extensively evaluate learned representations, bothqualitatively and quantitatively, in a few-shot setting and demonstrate theefficacy of MBBR for learning robust visual representations, particularlytailored for VRD. The proposed method is able to surpass state-of-the-art VRDmethods on the Predicate Detection (PredDet) evaluation setting, using only afew annotated samples. We make our code available athttps://github.com/deeplab-ai/SelfSupervisedVRD.</description><author>Zacharias Anastasakis, Dimitrios Mallis, Markos Diomataris, George Alexandridis, Stefanos Kollias, Vassilis Pitsikalis</author><pubDate>Wed, 08 Nov 2023 16:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04834v1</guid></item><item><title>Anonymizing medical case-based explanations through disentanglement</title><link>http://arxiv.org/abs/2311.04833v1</link><description>Case-based explanations are an intuitive method to gain insight into thedecision-making process of deep learning models in clinical contexts. However,medical images cannot be shared as explanations due to privacy concerns. Toaddress this problem, we propose a novel method for disentangling identity andmedical characteristics of images and apply it to anonymize medical images. Thedisentanglement mechanism replaces some feature vectors in an image whileensuring that the remaining features are preserved, obtaining independentfeature vectors that encode the images' identity and medical characteristics.We also propose a model to manufacture synthetic privacy-preserving identitiesto replace the original image's identity and achieve anonymization. The modelsare applied to medical and biometric datasets, demonstrating their capacity togenerate realistic-looking anonymized images that preserve their originalmedical content. Additionally, the experiments show the network's inherentcapacity to generate counterfactual images through the replacement of medicalfeatures.</description><author>Helena Montenegro, Jaime S. Cardoso</author><pubDate>Wed, 08 Nov 2023 16:58:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04833v1</guid></item><item><title>A Quantum-Powered Photorealistic Rendering</title><link>http://arxiv.org/abs/2211.03418v5</link><description>Achieving photorealistic rendering of real-world scenes poses a significantchallenge with diverse applications, including mixed reality and virtualreality. Neural networks, extensively explored in solving differentialequations, have previously been introduced as implicit representations forphotorealistic rendering. However, achieving realism through traditionalcomputing methods is arduous due to the time-consuming optical ray tracing, asit necessitates extensive numerical integration of color, transparency, andopacity values for each sampling point during the rendering process. In thispaper, we introduce Quantum Radiance Fields (QRF), which incorporate quantumcircuits, quantum activation functions, and quantum volume rendering torepresent scenes implicitly. Our results demonstrate that QRF effectivelyconfronts the computational challenges associated with extensive numericalintegration by harnessing the parallelism capabilities of quantum computing.Furthermore, current neural networks struggle with capturing fine signaldetails and accurately modeling high-frequency information and higher-orderderivatives. Quantum computing's higher order of nonlinearity provides adistinct advantage in this context. Consequently, QRF leverages two keystrengths of quantum computing: highly non-linear processing and extensiveparallelism, making it a potent tool for achieving photorealistic rendering ofreal-world scenes.</description><author>YuanFu Yang, Min Sun</author><pubDate>Wed, 08 Nov 2023 16:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03418v5</guid></item><item><title>Real-Time Recurrent Reinforcement Learning</title><link>http://arxiv.org/abs/2311.04830v1</link><description>Recent advances in reinforcement learning, for partially-observable Markovdecision processes (POMDPs), rely on the biologically implausiblebackpropagation through time algorithm (BPTT) to perform gradient-descentoptimisation. In this paper we propose a novel reinforcement learning algorithmthat makes use of random feedback local online learning (RFLO), a biologicallyplausible approximation of realtime recurrent learning (RTRL) to compute thegradients of the parameters of a recurrent neural network in an online manner.By combining it with TD($\lambda$), a variant of temporaldifferencereinforcement learning with eligibility traces, we create a biologicallyplausible, recurrent actor-critic algorithm, capable of solving discrete andcontinuous control tasks in POMDPs. We compare BPTT, RTRL and RFLO as well asdifferent network architectures, and find that RFLO can perform just as well asRTRL while exceeding even BPTT in terms of complexity. The proposed method,called real-time recurrent reinforcement learning (RTRRL), serves as a model oflearning in biological neural networks mimicking reward pathways in themammalian brain.</description><author>Julian Lemmel, Radu Grosu</author><pubDate>Wed, 08 Nov 2023 16:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04830v1</guid></item><item><title>Image Captioners Are Scalable Vision Learners Too</title><link>http://arxiv.org/abs/2306.07915v2</link><description>Contrastive pretraining on image-text pairs from the web is one of the mostpopular large-scale pretraining strategies for vision backbones, especially inthe context of large multimodal models. At the same time, image captioning onthis type of data is commonly considered an inferior pretraining strategy. Inthis paper, we perform a fair comparison of these two pretraining strategies,carefully matching training data, compute, and model capacity. Using a standardencoder-decoder transformer, we find that captioning alone is surprisinglyeffective: on classification tasks, captioning produces vision encoderscompetitive with contrastively pretrained encoders, while surpassing them onvision &amp; language tasks. We further analyze the effect of the modelarchitecture and scale, as well as the pretraining data on the representationquality, and find that captioning exhibits the same or better scaling behavioralong these axes. Overall our results show that plain image captioning is amore powerful pretraining strategy than was previously believed.</description><author>Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, Lucas Beyer</author><pubDate>Wed, 08 Nov 2023 16:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07915v2</guid></item><item><title>Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data</title><link>http://arxiv.org/abs/2311.04829v1</link><description>Tucker decomposition is a powerful tensor model to handle multi-aspect data.It demonstrates the low-rank property by decomposing the grid-structured dataas interactions between a core tensor and a set of object representations(factors). A fundamental assumption of such decomposition is that there werefinite objects in each aspect or mode, corresponding to discrete indexes ofdata entries. However, many real-world data are not naturally posed in thesetting. For example, geographic data is represented as continuous indexes oflatitude and longitude coordinates, and cannot fit tensor models directly. Togeneralize Tucker decomposition to such scenarios, we propose FunctionalBayesian Tucker Decomposition (FunBaT). We treat the continuous-indexed data asthe interaction between the Tucker core and a group of latent functions. We useGaussian processes (GP) as functional priors to model the latent functions, andthen convert the GPs into a state-space prior by constructing an equivalentstochastic differential equation (SDE) to reduce computational cost. Anefficient inference algorithm is further developed for scalable posteriorapproximation based on advanced message-passing techniques. The advantage ofour method is shown in both synthetic data and several real-world applications.</description><author>Shikai Fang, Xin Yu, Zheng Wang, Shibo Li, Mike Kirby, Shandian Zhe</author><pubDate>Wed, 08 Nov 2023 16:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04829v1</guid></item><item><title>SODAWideNet -- Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training</title><link>http://arxiv.org/abs/2311.04828v1</link><description>Developing a new Salient Object Detection (SOD) model involves selecting anImageNet pre-trained backbone and creating novel feature refinement modules touse backbone features. However, adding new components to a pre-trained backboneneeds retraining the whole network on the ImageNet dataset, which requiressignificant time. Hence, we explore developing a neural network from scratchdirectly trained on SOD without ImageNet pre-training. Such a formulationoffers full autonomy to design task-specific components. To that end, wepropose SODAWideNet, an encoder-decoder-style network for Salient ObjectDetection. We deviate from the commonly practiced paradigm of narrow and deepconvolutional models to a wide and shallow architecture, resulting in aparameter-efficient deep neural network. To achieve a shallower network, weincrease the receptive field from the beginning of the network using acombination of dilated convolutions and self-attention. Therefore, we proposeMulti Receptive Field Feature Aggregation Module (MRFFAM) that efficientlyobtains discriminative features from farther regions at higher resolutionsusing dilated convolutions. Next, we propose Multi-Scale Attention (MSA), whichcreates a feature pyramid and efficiently computes attention across multipleresolutions to extract global features from larger feature maps. Finally, wepropose two variants, SODAWideNet-S (3.03M) and SODAWideNet (9.03M), thatachieve competitive performance against state-of-the-art models on fivedatasets.</description><author>Rohit Venkata Sai Dulam, Chandra Kambhamettu</author><pubDate>Wed, 08 Nov 2023 16:53:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04828v1</guid></item><item><title>Revisiting Table Detection Datasets for Visually Rich Documents</title><link>http://arxiv.org/abs/2305.04833v2</link><description>Table Detection has become a fundamental task for visually rich documentunderstanding with the surging number of electronic documents. However, popularpublic datasets widely used in related studies have inherent limitations,including noisy and inconsistent samples, limited training samples, and limiteddata sources. These limitations make these datasets unreliable to evaluate themodel performance and cannot reflect the actual capacity of models. Therefore,this study revisits some open datasets with high-quality annotations,identifies and cleans the noise, and aligns the annotation definitions of thesedatasets to merge a larger dataset, termed Open-Tables. Moreover, to enrich thedata sources, we propose a new ICT-TD dataset using the PDF files ofInformation and Communication Technologies (ICT) commodities, a differentdomain containing unique samples that hardly appear in open datasets. To ensurethe label quality of the dataset, we annotated the dataset manually followingthe guidance of a domain expert. The proposed dataset is challenging and can bea sample of actual cases in the business context. We built strong baselinesusing various state-of-the-art object detection models. Our experimentalresults show that the domain differences among existing open datasets are minordespite having different data sources. Our proposed Open-Tables and ICT-TD canprovide a more reliable evaluation for models because of their high quality andconsistent annotations. Besides, they are more suitable for cross-domainsettings. Our experimental results show that in the cross-domain setting,benchmark models trained with cleaned Open-Tables dataset can achieve0.6\%-2.6\% higher weighted average F1 than the corresponding ones trained withthe noisy version of Open-Tables, demonstrating the reliability of the proposeddatasets. The datasets are public available.</description><author>Bin Xiao, Murat Simsek, Burak Kantarci, Ala Abu Alkheir</author><pubDate>Wed, 08 Nov 2023 16:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04833v2</guid></item><item><title>Hierarchically Gated Recurrent Neural Network for Sequence Modeling</title><link>http://arxiv.org/abs/2311.04823v1</link><description>Transformers have surpassed RNNs in popularity due to their superiorabilities in parallel training and long-term dependency modeling. Recently,there has been a renewed interest in using linear RNNs for efficient sequencemodeling. These linear RNNs often employ gating mechanisms in the output of thelinear recurrence layer while ignoring the significance of using forget gateswithin the recurrence. In this paper, we propose a gated linear RNN modeldubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includesforget gates that are lower bounded by a learnable value. The lower boundincreases monotonically when moving up layers. This allows the upper layers tomodel long-term dependencies and the lower layers to model more local,short-term dependencies. Experiments on language modeling, imageclassification, and long-range arena benchmarks showcase the efficiency andeffectiveness of our proposed model. The source code is available athttps://github.com/OpenNLPLab/HGRN.</description><author>Zhen Qin, Songlin Yang, Yiran Zhong</author><pubDate>Wed, 08 Nov 2023 16:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04823v1</guid></item><item><title>On Robustness of Finetuned Transformer-based NLP Models</title><link>http://arxiv.org/abs/2305.14453v2</link><description>Transformer-based pretrained models like BERT, GPT-2 and T5 have beenfinetuned for a large number of natural language processing (NLP) tasks, andhave been shown to be very effective. However, while finetuning, what changesacross layers in these models with respect to pretrained checkpoints isunder-studied. Further, how robust are these models to perturbations in inputtext? Does the robustness vary depending on the NLP task for which the modelshave been finetuned? While there exists some work on studying the robustness ofBERT finetuned for a few NLP tasks, there is no rigorous study that comparesthis robustness across encoder only, decoder only and encoder-decoder models.In this paper, we characterize changes between pretrained and finetunedlanguage model representations across layers using two metrics: CKA and STIR.Further, we study the robustness of three language models (BERT, GPT-2 and T5)with eight different text perturbations on classification tasks from theGeneral Language Understanding Evaluation (GLUE) benchmark, and generationtasks like summarization, free-form generation and question generation. GPT-2representations are more robust than BERT and T5 across multiple types of inputperturbation. Although models exhibit good robustness broadly, dropping nouns,verbs or changing characters are the most impactful. Overall, this studyprovides valuable insights into perturbation-specific weaknesses of popularTransformer-based models, which should be kept in mind when passing inputs. Wemake the code and models publicly available[https://github.com/PavanNeerudu/Robustness-of-Transformers-models].</description><author>Pavan Kalyan Reddy Neerudu, Subba Reddy Oota, Mounika Marreddy, Venkateswara Rao Kagita, Manish Gupta</author><pubDate>Wed, 08 Nov 2023 16:46:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14453v2</guid></item><item><title>Sound and Relatively Complete Belief Hoare Logic for Statistical Hypothesis Testing Programs</title><link>http://arxiv.org/abs/2208.07074v3</link><description>We propose a new approach to formally describing the requirement forstatistical inference and checking whether a program uses the statisticalmethod appropriately. Specifically, we define belief Hoare logic (BHL) forformalizing and reasoning about the statistical beliefs acquired via hypothesistesting. This program logic is sound and relatively complete with respect to aKripke model for hypothesis tests. We demonstrate by examples that BHL isuseful for reasoning about practical issues in hypothesis testing. In ourframework, we clarify the importance of prior beliefs in acquiring statisticalbeliefs through hypothesis testing, and discuss the whole picture of thejustification of statistical inference inside and outside the program logic.</description><author>Yusuke Kawamoto, Tetsuya Sato, Kohei Suenaga</author><pubDate>Wed, 08 Nov 2023 16:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07074v3</guid></item><item><title>LoopTune: Optimizing Tensor Computations with Reinforcement Learning</title><link>http://arxiv.org/abs/2309.01825v3</link><description>Advanced compiler technology is crucial for enabling machine learningapplications to run on novel hardware, but traditional compilers fail todeliver performance, popular auto-tuners have long search times andexpert-optimized libraries introduce unsustainable costs. To address this, wedeveloped LoopTune, a deep reinforcement learning compiler that optimizestensor computations in deep learning models for the CPU. LoopTune optimizestensor traversal order while using the ultra-fast lightweight code generatorLoopNest to perform hardware-specific optimizations. With a novel graph-basedrepresentation and action space, LoopTune speeds up LoopNest by 3.2x,generating an order of magnitude faster code than TVM, 2.8x faster thanMetaSchedule, and 1.08x faster than AutoTVM, consistently performing at thelevel of the hand-tuned library Numpy. Moreover, LoopTune tunes code in orderof seconds.</description><author>Dejan Grubisic, Bram Wasti, Chris Cummins, John Mellor-Crummey, Aleksandar Zlateski</author><pubDate>Wed, 08 Nov 2023 16:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01825v3</guid></item><item><title>Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment</title><link>http://arxiv.org/abs/2311.04818v1</link><description>Learning from the collective knowledge of data dispersed across privatesources can provide neural networks with enhanced generalization capabilities.Federated learning, a method for collaboratively training a machine learningmodel across remote clients, achieves this by combining client models via theorchestration of a central server. However, current approaches face twocritical limitations: i) they struggle to converge when client domains aresufficiently different, and ii) current aggregation techniques produce anidentical global model for each client. In this work, we address these issuesby reformulating the typical federated learning setup: rather than learning asingle global model, we learn N models each optimized for a common objective.To achieve this, we apply a weighted distance minimization to model parametersshared in a peer-to-peer topology. The resulting framework, Iterative ParameterAlignment, applies naturally to the cross-silo setting, and has the followingproperties: (i) a unique solution for each participant, with the option toglobally converge each model in the federation, and (ii) an optionalearly-stopping mechanism to elicit fairness among peers in collaborativelearning settings. These characteristics jointly provide a flexible newframework for iteratively learning from peer models trained on disparatedatasets. We find that the technique achieves competitive results on a varietyof data partitions compared to state-of-the-art approaches. Further, we showthat the method is robust to divergent domains (i.e. disjoint classes acrosspeers) where existing approaches struggle.</description><author>Matt Gorbett, Hossein Shirazi, Indrakshi Ray</author><pubDate>Wed, 08 Nov 2023 16:42:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04818v1</guid></item><item><title>Decentralized Personalized Online Federated Learning</title><link>http://arxiv.org/abs/2311.04817v1</link><description>Vanilla federated learning does not support learning in an onlineenvironment, learning a personalized model on each client, and learning in adecentralized setting. There are existing methods extending federated learningin each of the three aspects. However, some important applications onenterprise edge servers (e.g. online item recommendation at global scale)involve the three aspects at the same time. Therefore, we propose a newlearning setting \textit{Decentralized Personalized Online Federated Learning}that considers all the three aspects at the same time. In this new setting for learning, the first technical challenge is how toaggregate the shared model parameters from neighboring clients to obtain apersonalized local model with good performance on each client. We propose todirectly learn an aggregation by optimizing the performance of the local modelwith respect to the aggregation weights. This not only improves personalizationof each local model but also helps the local model adapting to potential datashift by intelligently incorporating the right amount of information from itsneighbors. The second challenge is how to select the neighbors for each client.We propose a peer selection method based on the learned aggregation weightsenabling each client to select the most helpful neighbors and reducecommunication cost at the same time. We verify the effectiveness and robustnessof our proposed method on three real-world item recommendation datasets and oneair quality prediction dataset.</description><author>Renzhi Wu, Saayan Mitra, Xiang Chen, Anup Rao</author><pubDate>Wed, 08 Nov 2023 16:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04817v1</guid></item><item><title>Joint processing of linguistic properties in brains and language models</title><link>http://arxiv.org/abs/2212.08094v2</link><description>Language models have been shown to be very effective in predicting brainrecordings of subjects experiencing complex language stimuli. For a deeperunderstanding of this alignment, it is important to understand thecorrespondence between the detailed processing of linguistic information by thehuman brain versus language models. We investigate this correspondence via adirect approach, in which we eliminate information related to specificlinguistic properties in the language model representations and observe howthis intervention affects the alignment with fMRI brain recordings obtainedwhile participants listened to a story. We investigate a range of linguisticproperties (surface, syntactic, and semantic) and find that the elimination ofeach one results in a significant decrease in brain alignment. Specifically, wefind that syntactic properties (i.e. Top Constituents and Tree Depth) have thelargest effect on the trend of brain alignment across model layers. Thesefindings provide clear evidence for the role of specific linguistic informationin the alignment between brain and language models, and open new avenues formapping the joint information processing in both systems. We make the codepublicly available[https://github.com/subbareddy248/linguistic-properties-brain-alignment].</description><author>Subba Reddy Oota, Manish Gupta, Mariya Toneva</author><pubDate>Wed, 08 Nov 2023 16:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08094v2</guid></item><item><title>MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document</title><link>http://arxiv.org/abs/2311.04816v1</link><description>The facts and time in the document are intricately intertwined, makingtemporal reasoning over documents challenging. Previous work models timeimplicitly, making it difficult to handle such complex relationships. Toaddress this issue, we propose MTGER, a novel Multi-view Temporal GraphEnhanced Temporal Reasoning framework for temporal reasoning over time-involveddocuments. Concretely, MTGER explicitly models the temporal relationships amongfacts by multi-view temporal graphs. On the one hand, the heterogeneoustemporal graphs explicitly model the temporal and discourse relationships amongfacts; on the other hand, the multi-view mechanism captures both time-focusedand fact-focused information, allowing the two views to complement each otherthrough adaptive fusion. To further improve the implicit reasoning capabilityof the model, we design a self-supervised time-comparing objective. Extensiveexperimental results demonstrate the effectiveness of our method on the TimeQAand SituatedQA datasets. Furthermore, MTGER gives more consistent answers underquestion perturbations.</description><author>Zheng Chu, Zekun Wang, Jiafeng Liang, Ming Liu, Bing Qin</author><pubDate>Wed, 08 Nov 2023 16:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04816v1</guid></item><item><title>Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?</title><link>http://arxiv.org/abs/2309.12214v2</link><description>Photovoltaic (PV) energy is crucial for the decarbonization of energysystems. Due to the lack of centralized data, remote sensing of rooftop PVinstallations is the best option to monitor the evolution of the rooftop PVinstalled fleet at a regional scale. However, current techniques lackreliability and are notably sensitive to shifts in the acquisition conditions.To overcome this, we leverage the wavelet scale attribution method (WCAM),which decomposes a model's prediction in the space-scale domain. The WCAMenables us to assess on which scales the representation of a PV model rests andprovides insights to derive methods that improve the robustness to acquisitionconditions, thus increasing trust in deep learning systems to encourage theiruse for the safe integration of clean energy in electric systems.</description><author>Gabriel Kasmi, Laurent Dubus, Yves-Marie Saint-Drenan, Philippe Blanc</author><pubDate>Wed, 08 Nov 2023 16:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12214v2</guid></item><item><title>Domain Adaptive Object Detection via Balancing Between Self-Training and Adversarial Learning</title><link>http://arxiv.org/abs/2311.04815v1</link><description>Deep learning based object detectors struggle generalizing to a new targetdomain bearing significant variations in object and background. Most currentmethods align domains by using image or instance-level adversarial featurealignment. This often suffers due to unwanted background and lacksclass-specific alignment. A straightforward approach to promote class-levelalignment is to use high confidence predictions on unlabeled domain aspseudo-labels. These predictions are often noisy since model is poorlycalibrated under domain shift. In this paper, we propose to leverage model'spredictive uncertainty to strike the right balance between adversarial featurealignment and class-level alignment. We develop a technique to quantifypredictive uncertainty on class assignments and bounding-box predictions. Modelpredictions with low uncertainty are used to generate pseudo-labels forself-training, whereas the ones with higher uncertainty are used to generatetiles for adversarial feature alignment. This synergy between tiling arounduncertain object regions and generating pseudo-labels from highly certainobject regions allows capturing both image and instance-level context duringthe model adaptation. We report thorough ablation study to reveal the impact ofdifferent components in our approach. Results on five diverse and challengingadaptation scenarios show that our approach outperforms existingstate-of-the-art methods with noticeable margins.</description><author>Muhammad Akhtar Munir, Muhammad Haris Khan, M. Saquib Sarfraz, Mohsen Ali</author><pubDate>Wed, 08 Nov 2023 16:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04815v1</guid></item><item><title>Be Careful When Evaluating Explanations Regarding Ground Truth</title><link>http://arxiv.org/abs/2311.04813v1</link><description>Evaluating explanations of image classifiers regarding ground truth, e.g.segmentation masks defined by human perception, primarily evaluates the qualityof the models under consideration rather than the explanation methodsthemselves. Driven by this observation, we propose a framework for$\textit{jointly}$ evaluating the robustness of safety-critical systems that$\textit{combine}$ a deep neural network with an explanation method. These areincreasingly used in real-world applications like medical image analysis orrobotics. We introduce a fine-tuning procedure to (mis)alignmodel$\unicode{x2013}$explanation pipelines with ground truth and use it toquantify the potential discrepancy between worst and best-case scenarios ofhuman alignment. Experiments across various model architectures and post-hoclocal interpretation methods provide insights into the robustness of visiontransformers and the overall vulnerability of such AI systems to potentialadversarial attacks.</description><author>Hubert Baniecki, Maciej Chrabaszcz, Andreas Holzinger, Bastian Pfeifer, Anna Saranti, Przemyslaw Biecek</author><pubDate>Wed, 08 Nov 2023 16:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04813v1</guid></item><item><title>Image-Based Virtual Try-On: A Survey</title><link>http://arxiv.org/abs/2311.04811v1</link><description>Image-based virtual try-on aims to synthesize a naturally dressed personimage with a clothing image, which revolutionizes online shopping and inspiresrelated topics within image generation, showing both research significance andcommercial potentials. However, there is a great gap between current researchprogress and commercial applications and an absence of comprehensive overviewtowards this field to accelerate the development. In this survey, we provide acomprehensive analysis of the state-of-the-art techniques and methodologies inaspects of pipeline architecture, person representation and key modules such astry-on indication, clothing warping and try-on stage. We propose a new semanticcriteria with CLIP, and evaluate representative methods with uniformlyimplemented evaluation metrics on the same dataset. In addition to quantitativeand qualitative evaluation of current open-source methods, we also utilizeControlNet to fine-tune a recent large image generation model (PBE) to showfuture potentials of large-scale models on image-based virtual try-on task.Finally, unresolved issues are revealed and future research directions areprospected to identify key trends and inspire further exploration. Theuniformly implemented evaluation metrics, dataset and collected methods will bemade public available athttps://github.com/little-misfit/Survey-Of-Virtual-Try-On.</description><author>Dan Song, Xuanpu Zhang, Juan Zhou, Weizhi Nie, Ruofeng Tong, An-An Liu</author><pubDate>Wed, 08 Nov 2023 16:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04811v1</guid></item><item><title>VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models</title><link>http://arxiv.org/abs/2310.04655v2</link><description>Vision-Language (VL) pre-trained models have shown their superiority on manymultimodal tasks. However, the adversarial robustness of such models has notbeen fully explored. Existing approaches mainly focus on exploring theadversarial robustness under the white-box setting, which is unrealistic. Inthis paper, we aim to investigate a new yet practical task to craft image andtext perturbations using pre-trained VL models to attack black-box fine-tunedmodels on different downstream tasks. Towards this end, we propose VLAttack togenerate adversarial samples by fusing perturbations of images and texts fromboth single-modal and multimodal levels. At the single-modal level, we proposea new block-wise similarity attack (BSA) strategy to learn image perturbationsfor disrupting universal representations. Besides, we adopt an existing textattack strategy to generate text perturbations independent of the image-modalattack. At the multimodal level, we design a novel iterative cross-searchattack (ICSA) method to update adversarial image-text pairs periodically,starting with the outputs from the single-modal level. We conduct extensiveexperiments to attack three widely-used VL pretrained models for six tasks oneight datasets. Experimental results show that the proposed VLAttack frameworkachieves the highest attack success rates on all tasks compared withstate-of-the-art baselines, which reveals a significant blind spot in thedeployment of pre-trained VL models. Codes will be released soon.</description><author>Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma</author><pubDate>Wed, 08 Nov 2023 16:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04655v2</guid></item><item><title>A Lightweight Architecture for Real-Time Neuronal-Spike Classification</title><link>http://arxiv.org/abs/2311.04808v1</link><description>Electrophysiological recordings of neural activity in a mouse's brain arevery popular among neuroscientists for understanding brain function. Oneparticular area of interest is acquiring recordings from the Purkinje cells inthe cerebellum in order to understand brain injuries and the loss of motorfunctions. However, current setups for such experiments do not allow the mouseto move freely and, thus, do not capture its natural behaviour since they havea wired connection between the animal's head stage and an acquisition device.In this work, we propose a lightweight neuronal-spike detection andclassification architecture that leverages on the unique characteristics of thePurkinje cells to discard unneeded information from the sparse neural data inreal time. This allows the (condensed) data to be easily stored on a removablestorage device on the head stage, alleviating the need for wires. Our proposedimplementation shows a &gt;95% overall classification accuracy while stillresulting in a small-form-factor design, which allows for the free movement ofmice during experiments. Moreover, the power-efficient nature of the design andthe usage of STT-RAM (Spin Transfer Torque Magnetic Random Access Memory) asthe removable storage allows the head stage to easily operate on a tiny batteryfor up to approximately 4 days.</description><author>Muhammad Ali Siddiqi, David Vrijenhoek, Lennart P. L. Landsmeer, Job van der Kleij, Anteneh Gebregiorgis, Vincenzo Romano, Rajendra Bishnoi, Said Hamdioui, Christos Strydis</author><pubDate>Wed, 08 Nov 2023 16:30:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04808v1</guid></item><item><title>The PetShop Dataset -- Finding Causes of Performance Issues across Microservices</title><link>http://arxiv.org/abs/2311.04806v1</link><description>Identifying root causes for unexpected or undesirable behavior in complexsystems is a prevalent challenge. This issue becomes especially crucial inmodern cloud applications that employ numerous microservices. Although themachine learning and systems research communities have proposed varioustechniques to tackle this problem, there is currently a lack of standardizeddatasets for quantitative benchmarking. Consequently, research groups arecompelled to create their own datasets for experimentation. This paperintroduces a dataset specifically designed for evaluating root cause analysesin microservice-based applications. The dataset encompasses latency, requests,and availability metrics emitted in 5-minute intervals from a distributedapplication. In addition to normal operation metrics, the dataset includes 68injected performance issues, which increase latency and reduce availabilitythroughout the system. We showcase how this dataset can be used to evaluate theaccuracy of a variety of methods spanning different causal and non-causalcharacterisations of the root cause analysis problem. We hope the new dataset,available at https://github.com/amazon-science/petshop-root-cause-analysis/enables further development of techniques in this important area.</description><author>Michaela Hardt, William Orchard, Patrick Blöbaum, Shiva Kasiviswanathan, Elke Kirschbaum</author><pubDate>Wed, 08 Nov 2023 16:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04806v1</guid></item><item><title>DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert Pretraining</title><link>http://arxiv.org/abs/2311.04799v1</link><description>Building on the cost-efficient pretraining advancements brought about byCrammed BERT, we enhance its performance and interpretability further byintroducing a novel pretrained model Dependency Agreement Crammed BERT(DACBERT) and its two-stage pretraining framework - Dependency AgreementPretraining. This framework, grounded by linguistic theories, seamlessly weavessyntax and semantic information into the pretraining process. The first stageemploys four dedicated submodels to capture representative dependencyagreements at the chunk level, effectively converting these agreements intoembeddings. The second stage uses these refined embeddings, in tandem withconventional BERT embeddings, to guide the pretraining of the rest of themodel. Evaluated on the GLUE benchmark, our DACBERT demonstrates notableimprovement across various tasks, surpassing Crammed BERT by 3.13% in the RTEtask and by 2.26% in the MRPC task. Furthermore, our method boosts the averageGLUE score by 0.83%, underscoring its significant potential. The pretrainingprocess can be efficiently executed on a single GPU within a 24-hour cycle,necessitating no supplementary computational resources or extending thepretraining duration compared with the Crammed BERT. Extensive studies furtherilluminate our approach's instrumental role in bolstering the interpretabilityof pretrained language models for natural language understanding tasks.</description><author>Martin Kuo, Jianyi Zhang, Yiran Chen</author><pubDate>Wed, 08 Nov 2023 16:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04799v1</guid></item><item><title>Determination of toxic comments and unintended model bias minimization using Deep learning approach</title><link>http://arxiv.org/abs/2311.04789v1</link><description>Online conversations can be toxic and subjected to threats, abuse, orharassment. To identify toxic text comments, several deep learning and machinelearning models have been proposed throughout the years. However, recentstudies demonstrate that because of the imbalances in the training data, somemodels are more likely to show unintended biases including gender bias andidentity bias. In this research, our aim is to detect toxic comment and reducethe unintended bias concerning identity features such as race, gender, sex,religion by fine-tuning an attention based model called BERT(BidirectionalEncoder Representation from Transformers). We apply weighted loss to addressthe issue of unbalanced data and compare the performance of a fine-tuned BERTmodel with a traditional Logistic Regression model in terms of classificationand bias minimization. The Logistic Regression model with the TFIDF vectorizerachieve 57.1% accuracy, and fine-tuned BERT model's accuracy is 89%. Code isavailable athttps://github.com/zim10/Determine_Toxic_comment_and_identity_bias.git</description><author>Md Azim Khan</author><pubDate>Wed, 08 Nov 2023 16:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04789v1</guid></item><item><title>Why Do Clinical Probabilistic Models Fail To Transport Between Sites?</title><link>http://arxiv.org/abs/2311.04787v1</link><description>The rising popularity of artificial intelligence in healthcare ishighlighting the problem that a computational model achieving super-humanclinical performance at its training sites may perform substantially worse atnew sites. In this perspective, we present common sources for this failure totransport, which we divide into sources under the control of the experimenterand sources inherent to the clinical data-generating process. Of the inherentsources we look a little deeper into site-specific clinical practices that canaffect the data distribution, and propose a potential solution intended toisolate the imprint of those practices on the data from the patterns of diseasecause and effect that are the usual target of clinical models.</description><author>Thomas A. Lasko, Eric V. Strobl, William W. Stead</author><pubDate>Wed, 08 Nov 2023 16:09:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04787v1</guid></item><item><title>Hierarchical clustering with dot products recovers hidden tree structure</title><link>http://arxiv.org/abs/2305.15022v2</link><description>In this paper we offer a new perspective on the well establishedagglomerative clustering algorithm, focusing on recovery of hierarchicalstructure. We recommend a simple variant of the standard algorithm, in whichclusters are merged by maximum average dot product and not, for example, byminimum distance or within-cluster variance. We demonstrate that the treeoutput by this algorithm provides a bona fide estimate of generativehierarchical structure in data, under a generic probabilistic graphical model.The key technical innovations are to understand how hierarchical information inthis model translates into tree geometry which can be recovered from data, andto characterise the benefits of simultaneously growing sample size and datadimension. We demonstrate superior tree recovery performance with real dataover existing approaches such as UPGMA, Ward's method, and HDBSCAN.</description><author>Annie Gray, Alexander Modell, Patrick Rubin-Delanchy, Nick Whiteley</author><pubDate>Wed, 08 Nov 2023 16:07:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15022v2</guid></item><item><title>AvalonBench: Evaluating LLMs Playing the Game of Avalon</title><link>http://arxiv.org/abs/2310.05036v3</link><description>In this paper, we explore the potential of Large Language Models (LLMs)Agents in playing the strategic social deduction game, Resistance Avalon.Players in Avalon are challenged not only to make informed decisions based ondynamically evolving game phases, but also to engage in discussions where theymust deceive, deduce, and negotiate with other players. These characteristicsmake Avalon a compelling test-bed to study the decision-making andlanguage-processing capabilities of LLM Agents. To facilitate research in thisline, we introduce AvalonBench - a comprehensive game environment tailored forevaluating multi-agent LLM Agents. This benchmark incorporates: (1) a gameenvironment for Avalon, (2) rule-based bots as baseline opponents, and (3)ReAct-style LLM agents with tailored prompts for each role. Notably, ourevaluations based on AvalonBench highlight a clear capability gap. Forinstance, models like ChatGPT playing good-role got a win rate of 22.2% againstrule-based bots playing evil, while good-role bot achieves 38.2% win rate inthe same setting. We envision AvalonBench could be a good test-bed fordeveloping more advanced LLMs (with self-playing) and agent frameworks that caneffectively model the layered complexities of such game environments.</description><author>Jonathan Light, Min Cai, Sheng Shen, Ziniu Hu</author><pubDate>Wed, 08 Nov 2023 16:01:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05036v3</guid></item><item><title>VioLA: Aligning Videos to 2D LiDAR Scans</title><link>http://arxiv.org/abs/2311.04783v1</link><description>We study the problem of aligning a video that captures a local portion of anenvironment to the 2D LiDAR scan of the entire environment. We introduce amethod (VioLA) that starts with building a semantic map of the local scene fromthe image sequence, then extracts points at a fixed height for registering tothe LiDAR map. Due to reconstruction errors or partial coverage of the camerascan, the reconstructed semantic map may not contain sufficient information forregistration. To address this problem, VioLA makes use of a pre-trainedtext-to-image inpainting model paired with a depth completion model for fillingin the missing scene content in a geometrically consistent fashion to supportpose registration. We evaluate VioLA on two real-world RGB-D benchmarks, aswell as a self-captured dataset of a large office scene. Notably, our proposedscene completion module improves the pose registration performance by up to20%.</description><author>Jun-Jee Chao, Selim Engin, Nikhil Chavan-Dafle, Bhoram Lee, Volkan Isler</author><pubDate>Wed, 08 Nov 2023 16:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04783v1</guid></item><item><title>FetMRQC: an open-source machine learning framework for multi-centric fetal brain MRI quality control</title><link>http://arxiv.org/abs/2311.04780v1</link><description>Fetal brain MRI is becoming an increasingly relevant complement toneurosonography for perinatal diagnosis, allowing fundamental insights intofetal brain development throughout gestation. However, uncontrolled fetalmotion and heterogeneity in acquisition protocols lead to data of variablequality, potentially biasing the outcome of subsequent studies. We presentFetMRQC, an open-source machine-learning framework for automated image qualityassessment and quality control that is robust to domain shifts induced by theheterogeneity of clinical data. FetMRQC extracts an ensemble of quality metricsfrom unprocessed anatomical MRI and combines them to predict experts' ratingsusing random forests. We validate our framework on a pioneeringly large anddiverse dataset of more than 1600 manually rated fetal brain T2-weighted imagesfrom four clinical centers and 13 different scanners. Our study shows thatFetMRQC's predictions generalize well to unseen data while being interpretable.FetMRQC is a step towards more robust fetal brain neuroimaging, which has thepotential to shed new insights on the developing human brain.</description><author>Thomas Sanchez, Oscar Esteban, Yvan Gomez, Alexandre Pron, Mériam Koob, Vincent Dunet, Nadine Girard, Andras Jakab, Elisenda Eixarch, Guillaume Auzias, Meritxell Bach Cuadra</author><pubDate>Wed, 08 Nov 2023 15:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04780v1</guid></item><item><title>Optimal Deep Neural Network Approximation for Korobov Functions with respect to Sobolev Norms</title><link>http://arxiv.org/abs/2311.04779v1</link><description>This paper establishes the nearly optimal rate of approximation for deepneural networks (DNNs) when applied to Korobov functions, effectivelyovercoming the curse of dimensionality. The approximation results presented inthis paper are measured with respect to $L_p$ norms and $H^1$ norms. Ourachieved approximation rate demonstrates a remarkable "super-convergence" rate,outperforming traditional methods and any continuous function approximator.These results are non-asymptotic, providing error bounds that consider both thewidth and depth of the networks simultaneously.</description><author>Yahong Yang, Yulong Lu</author><pubDate>Wed, 08 Nov 2023 15:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04779v1</guid></item><item><title>On the Multiple Roles of Ontologies in Explainable AI</title><link>http://arxiv.org/abs/2311.04778v1</link><description>This paper discusses the different roles that explicit knowledge, inparticular ontologies, can play in Explainable AI and in the development ofhuman-centric explainable systems and intelligible explanations. We considerthree main perspectives in which ontologies can contribute significantly,namely reference modelling, common-sense reasoning, and knowledge refinementand complexity management. We overview some of the existing approaches in theliterature, and we position them according to these three proposedperspectives. The paper concludes by discussing what challenges still need tobe addressed to enable ontology-based approaches to explanation and to evaluatetheir human-understandability and effectiveness.</description><author>Roberto Confalonieri, Giancarlo Guizzardi</author><pubDate>Wed, 08 Nov 2023 15:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04778v1</guid></item><item><title>Lidar Annotation Is All You Need</title><link>http://arxiv.org/abs/2311.04777v1</link><description>In recent years, computer vision has transformed fields such as medicalimaging, object recognition, and geospatial analytics. One of the fundamentaltasks in computer vision is semantic image segmentation, which is vital forprecise object delineation. Autonomous driving represents one of the key areaswhere computer vision algorithms are applied. The task of road surfacesegmentation is crucial in self-driving systems, but it requires alabor-intensive annotation process in several data domains. The work describedin this paper aims to improve the efficiency of image segmentation using aconvolutional neural network in a multi-sensor setup. This approach leverageslidar (Light Detection and Ranging) annotations to directly train imagesegmentation models on RGB images. Lidar supplements the images by emittinglaser pulses and measuring reflections to provide depth information. However,lidar's sparse point clouds often create difficulties for accurate objectsegmentation. Segmentation of point clouds requires time-consuming preliminarydata preparation and a large amount of computational resources. The keyinnovation of our approach is the masked loss, addressing sparse ground-truthmasks from point clouds. By calculating loss exclusively where lidar pointsexist, the model learns road segmentation on images by using lidar points asground truth. This approach allows for blending of different ground-truth datatypes during model training. Experimental validation of the approach onbenchmark datasets shows comparable performance to a high-quality imagesegmentation model. Incorporating lidar reduces the load on annotations andenables training of image-segmentation models without loss of segmentationquality. The methodology is tested on diverse datasets, both publicly availableand proprietary. The strengths and weaknesses of the proposed method are alsodiscussed in the paper.</description><author>Dinar Sharafutdinov, Stanislav Kuskov, Saian Protasov, Alexey Voropaev</author><pubDate>Wed, 08 Nov 2023 15:55:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04777v1</guid></item><item><title>Towards a Unified Framework of Contrastive Learning for Disentangled Representations</title><link>http://arxiv.org/abs/2311.04774v1</link><description>Contrastive learning has recently emerged as a promising approach forlearning data representations that discover and disentangle the explanatoryfactors of the data. Previous analyses of such approaches have largely focusedon individual contrastive losses, such as noise-contrastive estimation (NCE)and InfoNCE, and rely on specific assumptions about the data generatingprocess. This paper extends the theoretical guarantees for disentanglement to abroader family of contrastive methods, while also relaxing the assumptionsabout the data distribution. Specifically, we prove identifiability of the truelatents for four contrastive losses studied in this paper, without imposingcommon independence assumptions. The theoretical findings are validated onseveral benchmark datasets. Finally, practical limitations of these methods arealso investigated.</description><author>Stefan Matthes, Zhiwei Han, Hao Shen</author><pubDate>Wed, 08 Nov 2023 15:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04774v1</guid></item><item><title>GCS-ICHNet: Assessment of Intracerebral Hemorrhage Prognosis using Self-Attention with Domain Knowledge Integration</title><link>http://arxiv.org/abs/2311.04772v1</link><description>Intracerebral Hemorrhage (ICH) is a severe condition resulting from damagedbrain blood vessel ruptures, often leading to complications and fatalities.Timely and accurate prognosis and management are essential due to its highmortality rate. However, conventional methods heavily rely on subjectiveclinician expertise, which can lead to inaccurate diagnoses and delays intreatment. Artificial intelligence (AI) models have been explored to assistclinicians, but many prior studies focused on model modification withoutconsidering domain knowledge. This paper introduces a novel deep learningalgorithm, GCS-ICHNet, which integrates multimodal brain CT image data and theGlasgow Coma Scale (GCS) score to improve ICH prognosis. The algorithm utilizesa transformer-based fusion module for assessment. GCS-ICHNet demonstrates highsensitivity 81.03% and specificity 91.59%, outperforming average clinicians andother state-of-the-art methods.</description><author>Xuhao Shan, Xinyang Li, Ruiquan Ge, Shibin Wu, Ahmed Elazab, Jichao Zhu, Lingyan Zhang, Gangyong Jia, Qingying Xiao, Xiang Wan, Changmiao Wang</author><pubDate>Wed, 08 Nov 2023 15:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04772v1</guid></item><item><title>Vital Sign Forecasting for Sepsis Patients in ICUs</title><link>http://arxiv.org/abs/2311.04770v1</link><description>Sepsis and septic shock are a critical medical condition affecting millionsglobally, with a substantial mortality rate. This paper uses state-of-the-artdeep learning (DL) architectures to introduce a multi-step forecasting systemto predict vital signs indicative of septic shock progression in Intensive CareUnits (ICUs). Our approach utilizes a short window of historical vital signdata to forecast future physiological conditions. We introduce a DL-based vitalsign forecasting system that predicts up to 3 hours of future vital signs from6 hours of past data. We further adopt the DILATE loss function to capturebetter the shape and temporal dynamics of vital signs, which are critical forclinical decision-making. We compare three DL models, N-BEATS, N-HiTS, andTemporal Fusion Transformer (TFT), using the publicly available eICUCollaborative Research Database (eICU-CRD), highlighting their forecastingcapabilities in a critical care setting. We evaluate the performance of ourmodels using mean squared error (MSE) and dynamic time warping (DTW) metrics.Our findings show that while TFT excels in capturing overall trends, N-HiTS issuperior in retaining short-term fluctuations within a predefined range. Thispaper demonstrates the potential of deep learning in transforming themonitoring systems in ICUs, potentially leading to significant improvements inpatient care and outcomes by accurately forecasting vital signs to assisthealthcare providers in detecting early signs of physiological instability andanticipating septic shock.</description><author>Anubhav Bhatti, Yuwei Liu, Chen Dan, Bingjie Shen, San Lee, Yonghwan Kim, Jang Yong Kim</author><pubDate>Wed, 08 Nov 2023 15:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04770v1</guid></item><item><title>An attention-based deep learning network for predicting Platinum resistance in ovarian cancer</title><link>http://arxiv.org/abs/2311.04769v1</link><description>Background: Ovarian cancer is among the three most frequent gynecologiccancers globally. High-grade serous ovarian cancer (HGSOC) is the most commonand aggressive histological type. Guided treatment for HGSOC typically involvesplatinum-based combination chemotherapy, necessitating an assessment of whetherthe patient is platinum-resistant. The purpose of this study is to propose adeep learning-based method to determine whether a patient is platinum-resistantusing multimodal positron emission tomography/computed tomography (PET/CT)images. Methods: 289 patients with HGSOC were included in this study. Anend-to-end SE-SPP-DenseNet model was built by adding Squeeze-Excitation Block(SE Block) and Spatial Pyramid Pooling Layer (SPPLayer) to Dense ConvolutionalNetwork (DenseNet). Multimodal data from PET/CT images of the regions ofinterest (ROI) were used to predict platinum resistance in patients. Results:Through five-fold cross-validation, SE-SPP-DenseNet achieved a high accuracyrate and an area under the curve (AUC) in predicting platinum resistance inpatients, which were 92.6% and 0.93, respectively. The importance ofincorporating SE Block and SPPLayer into the deep learning model, andconsidering multimodal data was substantiated by carrying out ablation studiesand experiments with single modality data. Conclusions: The obtainedclassification results indicate that our proposed deep learning frameworkperforms better in predicting platinum resistance in patients, which can helpgynecologists make better treatment decisions. Keywords: PET/CT, CNN, SE Block,SPP Layer, Platinum resistance, Ovarian cancer</description><author>Haoming Zhuang, Beibei Li, Jingtong Ma, Patrice Monkam, Shouliang Qi, Wei Qian, Dianning He</author><pubDate>Wed, 08 Nov 2023 15:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04769v1</guid></item><item><title>Causal Scoring: A Framework for Effect Estimation, Effect Ordering, and Effect Classification</title><link>http://arxiv.org/abs/2206.12532v3</link><description>This paper introduces causal scoring as a novel approach to frame causalestimation in the context of decision making. Causal scoring entails theestimation of scores that support decision making by providing insights intocausal effects. We present three valuable causal interpretations of thesescores: effect estimation (EE), effect ordering (EO), and effect classification(EC). In the EE interpretation, the causal score represents the effect itself.The EO interpretation implies that the score can serve as a proxy for themagnitude of the effect, enabling the sorting of individuals based on theircausal effects. The EC interpretation enables the classification of individualsinto high- and low-effect categories using a predefined threshold. Wedemonstrate the value of these alternative causal interpretations (EO and EC)through two key results. First, we show that aligning the statistical modelingwith the desired causal interpretation improves the accuracy of causalestimation. Second, we establish that more flexible causal interpretations areplausible in a wider range of data-generating processes and propose conditionsto assess their validity. We showcase the practical utility of the causalscoring framework through examples in diverse fields such as advertising,healthcare, and education, illustrating how it facilitates reasoning aboutflexible causal interpretations of statistical estimates in various contexts.The examples encompass confounded estimates, effect estimates on surrogateoutcomes, and even predictions about non-causal quantities as potential causalscores.</description><author>Carlos Fernández-Loría, Jorge Loría</author><pubDate>Wed, 08 Nov 2023 15:42:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12532v3</guid></item><item><title>DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation</title><link>http://arxiv.org/abs/2311.04766v1</link><description>In recent years, audio-driven 3D facial animation has gained significantattention, particularly in applications such as virtual reality, gaming, andvideo conferencing. However, accurately modeling the intricate and subtledynamics of facial expressions remains a challenge. Most existing studiesapproach the facial animation task as a single regression problem, which oftenfail to capture the intrinsic inter-modal relationship between speech signalsand 3D facial animation and overlook their inherent consistency. Moreover, dueto the limited availability of 3D-audio-visual datasets, approaches learningwith small-size samples have poor generalizability that decreases theperformance. To address these issues, in this study, we propose a cross-modaldual-learning framework, termed DualTalker, aiming at improving data usageefficiency as well as relating cross-modal dependencies. The framework istrained jointly with the primary task (audio-driven facial animation) and itsdual task (lip reading) and shares common audio/motion encoder components. Ourjoint training framework facilitates more efficient data usage by leveraginginformation from both tasks and explicitly capitalizing on the complementaryrelationship between facial motion and audio to improve performance.Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigatethe potential over-smoothing underlying the cross-modal complementaryrepresentations, enhancing the mapping of subtle facial expression dynamics.Through extensive experiments and a perceptual user study conducted on the VOCAand BIWI datasets, we demonstrate that our approach outperforms currentstate-of-the-art methods both qualitatively and quantitatively. We have madeour code and video demonstrations available athttps://github.com/sabrina-su/iadf.git.</description><author>Guinan Su, Yanwu Yang, Zhifeng Li</author><pubDate>Wed, 08 Nov 2023 15:39:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04766v1</guid></item><item><title>The voraus-AD Dataset for Anomaly Detection in Robot Applications</title><link>http://arxiv.org/abs/2311.04765v1</link><description>During the operation of industrial robots, unusual events may endanger thesafety of humans and the quality of production. When collecting data to detectsuch cases, it is not ensured that data from all potentially occurring errorsis included as unforeseeable events may happen over time. Therefore, anomalydetection (AD) delivers a practical solution, using only normal data to learnto detect unusual events. We introduce a dataset that allows training andbenchmarking of anomaly detection methods for robotic applications based onmachine data which will be made publicly available to the research community.As a typical robot task the dataset includes a pick-and-place application whichinvolves movement, actions of the end effector and interactions with theobjects of the environment. Since several of the contained anomalies are nottask-specific but general, evaluations on our dataset are transferable to otherrobotics applications as well. Additionally, we present MVT-Flow (multivariatetime-series flow) as a new baseline method for anomaly detection: It relies ondeep-learning-based density estimation with normalizing flows, tailored to thedata domain by taking its structure into account for the architecture. Ourevaluation shows that MVT-Flow outperforms baselines from previous work by alarge margin of 6.2% in area under ROC.</description><author>Jan Thieß Brockmann, Marco Rudolph, Bodo Rosenhahn, Bastian Wandt</author><pubDate>Wed, 08 Nov 2023 15:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04765v1</guid></item><item><title>Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling</title><link>http://arxiv.org/abs/2308.09078v2</link><description>Conditional sampling of variational autoencoders (VAEs) is needed in variousapplications, such as missing data imputation, but is computationallyintractable. A principled choice for asymptotically exact conditional samplingis Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEsto learn a structured latent space, a commonly desired property, can cause theMWG sampler to get "stuck" far from the target distribution. This papermitigates the limitations of MWG: we systematically outline the pitfalls in thecontext of VAEs, propose two original methods that address these pitfalls, anddemonstrate an improved performance of the proposed methods on a set ofsampling tasks.</description><author>Vaidotas Simkus, Michael U. Gutmann</author><pubDate>Wed, 08 Nov 2023 15:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09078v2</guid></item><item><title>Text Promptable Surgical Instrument Segmentation with Vision-Language Models</title><link>http://arxiv.org/abs/2306.09244v3</link><description>In this paper, we propose a novel text promptable surgical instrumentsegmentation approach to overcome challenges associated with diversity anddifferentiation of surgical instruments in minimally invasive surgeries. Weredefine the task as text promptable, thereby enabling a more nuancedcomprehension of surgical instruments and adaptability to new instrument types.Inspired by recent advancements in vision-language models, we leveragepretrained image and text encoders as our model backbone and design a textpromptable mask decoder consisting of attention- and convolution-basedprompting schemes for surgical instrument segmentation prediction. Our modelleverages multiple text prompts for each surgical instrument through a newmixture of prompts mechanism, resulting in enhanced segmentation performance.Additionally, we introduce a hard instrument area reinforcement module toimprove image feature comprehension and segmentation precision. Extensiveexperiments on several surgical instrument segmentation datasets demonstrateour model's superior performance and promising generalization capability. Toour knowledge, this is the first implementation of a promptable approach tosurgical instrument segmentation, offering significant potential for practicalapplication in the field of robotic-assisted surgery. Code is available athttps://github.com/franciszzj/TP-SIS.</description><author>Zijian Zhou, Oluwatosin Alabi, Meng Wei, Tom Vercauteren, Miaojing Shi</author><pubDate>Wed, 08 Nov 2023 15:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09244v3</guid></item><item><title>Towards Open-world Cross-Domain Sequential Recommendation: A Model-Agnostic Contrastive Denoising Approach</title><link>http://arxiv.org/abs/2311.04760v1</link><description>Cross-domain sequential recommendation (CDSR) aims to address the datasparsity problems that exist in traditional sequential recommendation (SR)systems. The existing approaches aim to design a specific cross-domain unit that cantransfer and propagate information across multiple domains by relying onoverlapping users with abundant behaviors. However, in real-world recommendersystems, CDSR scenarios usually consist of a majority of long-tailed users withsparse behaviors and cold-start users who only exist in one domain. This leadsto a drop in the performance of existing CDSR methods in the real-worldindustry platform. Therefore, improving the consistency and effectiveness ofmodels in open-world CDSR scenarios is crucial for constructing CDSR models(\textit{1st} CH). Recently, some SR approaches have utilized auxiliarybehaviors to complement the information for long-tailed users. However, thesemulti-behavior SR methods cannot deliver promising performance in CDSR, as theyoverlook the semantic gap between target and auxiliary behaviors, as well asuser interest deviation across domains (\textit{2nd} CH).</description><author>Wujiang Xu, Xuying Ning, Wenfang Lin, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, Minnan Luo</author><pubDate>Wed, 08 Nov 2023 15:33:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04760v1</guid></item><item><title>FetMRQC: Automated Quality Control for fetal brain MRI</title><link>http://arxiv.org/abs/2304.05879v2</link><description>Quality control (QC) has long been considered essential to guarantee thereliability of neuroimaging studies. It is particularly important for fetalbrain MRI, where large and unpredictable fetal motion can lead to substantialartifacts in the acquired images. Existing methods for fetal brain qualityassessment operate at the \textit{slice} level, and fail to get a comprehensivepicture of the quality of an image, that can only be achieved by looking at the\textit{entire} brain volume. In this work, we propose FetMRQC, a machinelearning framework for automated image quality assessment tailored to fetalbrain MRI, which extracts an ensemble of quality metrics that are then used topredict experts' ratings. Based on the manual ratings of more than 1000low-resolution stacks acquired across two different institutions, we show that,compared with existing quality metrics, FetMRQC is able to generalizeout-of-domain, while being interpretable and data efficient. We also release anovel manual quality rating tool designed to facilitate and optimize qualityrating of fetal brain images. Our tool, along with all the code to generate, train and evaluate the modelis available athttps://github.com/Medical-Image-Analysis-Laboratory/fetal_brain_qc/ .</description><author>Thomas Sanchez, Oscar Esteban, Yvan Gomez, Elisenda Eixarch, Meritxell Bach Cuadra</author><pubDate>Wed, 08 Nov 2023 15:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05879v2</guid></item><item><title>Evading Watermark based Detection of AI-Generated Content</title><link>http://arxiv.org/abs/2305.03807v5</link><description>A generative AI model can generate extremely realistic-looking content,posing growing challenges to the authenticity of information. To address thechallenges, watermark has been leveraged to detect AI-generated content.Specifically, a watermark is embedded into an AI-generated content before it isreleased. A content is detected as AI-generated if a similar watermark can bedecoded from it. In this work, we perform a systematic study on the robustnessof such watermark-based AI-generated content detection. We focus onAI-generated images. Our work shows that an attacker can post-process awatermarked image via adding a small, human-imperceptible perturbation to it,such that the post-processed image evades detection while maintaining itsvisual quality. We show the effectiveness of our attack both theoretically andempirically. Moreover, to evade detection, our adversarial post-processingmethod adds much smaller perturbations to AI-generated images and thus bettermaintain their visual quality than existing popular post-processing methodssuch as JPEG compression, Gaussian blur, and Brightness/Contrast. Our workshows the insufficiency of existing watermark-based detection of AI-generatedcontent, highlighting the urgent needs of new methods. Our code is publiclyavailable: https://github.com/zhengyuan-jiang/WEvade.</description><author>Zhengyuan Jiang, Jinghuai Zhang, Neil Zhenqiang Gong</author><pubDate>Wed, 08 Nov 2023 15:23:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03807v5</guid></item><item><title>Natural Bayesian Cramér-Rao Bound with an Application to Covariance Estimation</title><link>http://arxiv.org/abs/2311.04748v1</link><description>In this paper, we propose to develop a new Cram\'er-Rao Bound (CRB) when theparameter to estimate lies in a manifold and follows a prior distribution. Thisderivation leads to a natural inequality between an error criteria based ongeometrical properties and this new bound. This main contribution isillustrated in the problem of covariance estimation when the data follow aGaussian distribution and the prior distribution is an inverse Wishart.Numerical simulation shows new results where the proposed CRB allows to exhibitinteresting properties of the MAP estimator which are not observed with theclassical Bayesian CRB.</description><author>Florent Bouchard, Alexandre Renaux, Guillaume Ginolhac, Arnaud Breloy</author><pubDate>Wed, 08 Nov 2023 15:17:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04748v1</guid></item><item><title>Physics-Informed Graph Convolutional Networks: Towards a generalized framework for complex geometries</title><link>http://arxiv.org/abs/2310.14948v3</link><description>Since the seminal work of [9] and their Physics-Informed neural networks(PINNs), many efforts have been conducted towards solving partial differentialequations (PDEs) with Deep Learning models. However, some challenges remain,for instance the extension of such models to complex three-dimensionalgeometries, and a study on how such approaches could be combined to classicalnumerical solvers. In this work, we justify the use of graph neural networksfor these problems, based on the similarity between these architectures and themeshes used in traditional numerical techniques for solving partialdifferential equations. After proving an issue with the Physics-Informedframework for complex geometries, during the computation of PDE residuals, analternative procedure is proposed, by combining classical numerical solvers andthe Physics-Informed framework. Finally, we propose an implementation of thisapproach, that we test on a three-dimensional problem on an irregular geometry.</description><author>Marien Chenaud, José Alves, Frédéric Magoulès</author><pubDate>Wed, 08 Nov 2023 15:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14948v3</guid></item><item><title>Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers</title><link>http://arxiv.org/abs/2311.04744v1</link><description>The Geometric Algebra Transformer (GATr) is a versatile architecture forgeometric deep learning based on projective geometric algebra. We generalizethis architecture into a blueprint that allows one to construct a scalabletransformer architecture given any geometric (or Clifford) algebra. We studyversions of this architecture for Euclidean, projective, and conformalalgebras, all of which are suited to represent 3D data, and evaluate them intheory and practice. The simplest Euclidean architecture is computationallycheap, but has a smaller symmetry group and is not as sample-efficient, whilethe projective model is not sufficiently expressive. Both the conformal algebraand an improved version of the projective algebra define powerful, performantarchitectures.</description><author>Pim de Haan, Taco Cohen, Johann Brehmer</author><pubDate>Wed, 08 Nov 2023 15:12:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04744v1</guid></item><item><title>Using large language models to study human memory for meaningful narratives</title><link>http://arxiv.org/abs/2311.04742v1</link><description>One of the most impressive achievements of the AI revolution is thedevelopment of large language models that can generate meaningful text andrespond to instructions in plain English with no additional training necessary.Here we show that language models can be used as a scientific instrument forstudying human memory for meaningful material. We developed a pipeline fordesigning large scale memory experiments and analyzing the obtained results. Weperformed online memory experiments with a large number of participants andcollected recognition and recall data for narratives of different lengths. Wefound that both recall and recognition performance scale linearly withnarrative length. Furthermore, in order to investigate the role of narrativecomprehension in memory, we repeated these experiments using scrambled versionsof the presented stories. We found that even though recall performance declinedsignificantly, recognition remained largely unaffected. Interestingly, recallsin this condition seem to follow the original narrative order rather than thescrambled presentation, pointing to a contextual reconstruction of the story inmemory.</description><author>Antonios Georgiou Tankut Can, Mikhail Katkov, Misha Tsodyks</author><pubDate>Wed, 08 Nov 2023 15:11:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04742v1</guid></item><item><title>Enhancing Multi-Agent Coordination through Common Operating Picture Integration</title><link>http://arxiv.org/abs/2311.04740v1</link><description>In multi-agent systems, agents possess only local observations of theenvironment. Communication between teammates becomes crucial for enhancingcoordination. Past research has primarily focused on encoding local informationinto embedding messages which are unintelligible to humans. We find that usingthese messages in agent's policy learning leads to brittle policies when testedon out-of-distribution initial states. We present an approach to multi-agentcoordination, where each agent is equipped with the capability to integrate its(history of) observations, actions and messages received into a CommonOperating Picture (COP) and disseminate the COP. This process takes intoaccount the dynamic nature of the environment and the shared mission. Weconducted experiments in the StarCraft2 environment to validate our approach.Our results demonstrate the efficacy of COP integration, and show thatCOP-based training leads to robust policies compared to state-of-the-artMulti-Agent Reinforcement Learning (MARL) methods when faced without-of-distribution initial states.</description><author>Peihong Yu, Bhoram Lee, Aswin Raghavan, Supun Samarasekara, Pratap Tokekar, James Zachary Hare</author><pubDate>Wed, 08 Nov 2023 15:08:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04740v1</guid></item><item><title>Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions</title><link>http://arxiv.org/abs/2311.02985v2</link><description>In the last years, several variants of transformers have emerged. In thispaper, we compare different transformer-based models for solving the reversedictionary task and explore their use in the context of a serious game calledThe Dictionary Game.</description><author>Julien Guité-Vinet, Alexandre Blondin Massé, Fatiha Sadat</author><pubDate>Wed, 08 Nov 2023 15:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02985v2</guid></item><item><title>Robust Best-arm Identification in Linear Bandits</title><link>http://arxiv.org/abs/2311.04731v1</link><description>We study the robust best-arm identification problem (RBAI) in the case oflinear rewards. The primary objective is to identify a near-optimal robust arm,which involves selecting arms at every round and assessing their robustness byexploring potential adversarial actions. This approach is particularly relevantwhen utilizing a simulator and seeking to identify a robust solution forreal-world transfer. To this end, we present an instance-dependent lower boundfor the robust best-arm identification problem with linear rewards.Furthermore, we propose both static and adaptive bandit algorithms that achievesample complexity that matches the lower bound. In synthetic experiments, ouralgorithms effectively identify the best robust arm and perform similarly tothe oracle strategy. As an application, we examine diabetes care and theprocess of learning insulin dose recommendations that are robust with respectto inaccuracies in standard calculators. Our algorithms prove to be effectivein identifying robust dosage values across various age ranges of patients.</description><author>Wei Wang, Sattar Vakili, Ilija Bogunovic</author><pubDate>Wed, 08 Nov 2023 14:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04731v1</guid></item><item><title>Predicting Properties of Nodes via Community-Aware Features</title><link>http://arxiv.org/abs/2311.04730v1</link><description>A community structure that is often present in complex networks plays animportant role not only in their formation but also shapes dynamics of thesenetworks, affecting properties of their nodes. In this paper, we propose afamily of community-aware node features and then investigate their properties.We show that they have high predictive power for classification tasks. We alsoverify that they contain information that cannot be recovered neither byclassical node features nor by node embeddings (both classical as well asstructural).</description><author>Bogumił Kamiński, Paweł Prałat, François Théberge, Sebastian Zając</author><pubDate>Wed, 08 Nov 2023 14:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04730v1</guid></item><item><title>Fake It Without Making It: Conditioned Face Generation for Accurate 3D Face Reconstruction</title><link>http://arxiv.org/abs/2307.13639v2</link><description>Accurate 3D face reconstruction from 2D images is an enabling technology withapplications in healthcare, security, and creative industries. However, currentstate-of-the-art methods either rely on supervised training with very limited3D data or self-supervised training with 2D image data. To bridge this gap, wepresent a method to generate a large-scale synthesised dataset of 250Kphotorealistic images and their corresponding shape parameters and depth maps,which we call SynthFace. Our synthesis method conditions Stable Diffusion ondepth maps sampled from the FLAME 3D Morphable Model (3DMM) of the human face,allowing us to generate a diverse set of shape-consistent facial images that isdesigned to be balanced in race and gender. We further propose ControlFace, adeep neural network, trained on SynthFace, which achieves competitiveperformance on the NoW benchmark, without requiring 3D supervision or manual 3Dasset creation. The complete SynthFace dataset will be made publicly availableupon publication.</description><author>Will Rowan, Patrik Huber, Nick Pears, Andrew Keeling</author><pubDate>Wed, 08 Nov 2023 14:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13639v2</guid></item><item><title>Social Motion Prediction with Cognitive Hierarchies</title><link>http://arxiv.org/abs/2311.04726v1</link><description>Humans exhibit a remarkable capacity for anticipating the actions of othersand planning their own actions accordingly. In this study, we strive toreplicate this ability by addressing the social motion prediction problem. Weintroduce a new benchmark, a novel formulation, and a cognition-inspiredframework. We present Wusi, a 3D multi-person motion dataset under the contextof team sports, which features intense and strategic human interactions anddiverse pose distributions. By reformulating the problem from a multi-agentreinforcement learning perspective, we incorporate behavioral cloning andgenerative adversarial imitation learning to boost learning efficiency andgeneralization. Furthermore, we take into account the cognitive aspects of thehuman social action planning process and develop a cognitive hierarchyframework to predict strategic human social interactions. We conductcomprehensive experiments to validate the effectiveness of our proposed datasetand approach. Code and data are available athttps://walter0807.github.io/Social-CH/.</description><author>Wentao Zhu, Jason Qin, Yuke Lou, Hang Ye, Xiaoxuan Ma, Hai Ci, Yizhou Wang</author><pubDate>Wed, 08 Nov 2023 14:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04726v1</guid></item><item><title>Designing Robust Transformers using Robust Kernel Density Estimation</title><link>http://arxiv.org/abs/2210.05794v3</link><description>Recent advances in Transformer architectures have empowered their empiricalsuccess in a variety of tasks across different domains. However, existing worksmainly focus on predictive accuracy and computational cost, without consideringother practical issues, such as robustness to contaminated samples. Recent workby Nguyen et al., (2022) has shown that the self-attention mechanism, which isthe center of the Transformer architecture, can be viewed as a non-parametricestimator based on kernel density estimation (KDE). This motivates us toleverage a set of robust kernel density estimation methods for alleviating theissue of data contamination. Specifically, we introduce a series ofself-attention mechanisms that can be incorporated into different Transformerarchitectures and discuss the special properties of each method. We thenperform extensive empirical studies on language modeling and imageclassification tasks. Our methods demonstrate robust performance in multiplescenarios while maintaining competitive results on clean datasets.</description><author>Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, Nhat Ho</author><pubDate>Wed, 08 Nov 2023 14:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05794v3</guid></item><item><title>The Geometric Structure of Fully-Connected ReLU Layers</title><link>http://arxiv.org/abs/2310.03482v2</link><description>We formalize and interpret the geometric structure of $d$-dimensional fullyconnected ReLU layers in neural networks. The parameters of a ReLU layer inducea natural partition of the input domain, such that the ReLU layer can besignificantly simplified in each sector of the partition. This leads to ageometric interpretation of a ReLU layer as a projection onto a polyhedral conefollowed by an affine transformation, in line with the description in[doi:10.48550/arXiv.1905.08922] for convolutional networks with ReLUactivations. Further, this structure facilitates simplified expressions forpreimages of the intersection between partition sectors and hyperplanes, whichis useful when describing decision boundaries in a classification setting. Weinvestigate this in detail for a feed-forward network with one hiddenReLU-layer, where we provide results on the geometric complexity of thedecision boundary generated by such networks, as well as proving that modulo anaffine transformation, such a network can only generate $d$ different decisionboundaries. Finally, the effect of adding more layers to the network isdiscussed.</description><author>Jonatan Vallin, Karl Larsson, Mats G. Larson</author><pubDate>Wed, 08 Nov 2023 14:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03482v2</guid></item><item><title>3DoF Localization from a Single Image and an Object Map: the Flatlandia Problem and Dataset</title><link>http://arxiv.org/abs/2304.06373v4</link><description>Efficient visual localization is crucial to many applications, such aslarge-scale deployment of autonomous agents and augmented reality. Traditionalvisual localization, while achieving remarkable accuracy, relies on extensive3D models of the scene or large collections of geolocalized images, which areoften inefficient to store and to scale to novel environments. In contrast,humans orient themselves using very abstract 2D maps, using the location ofclearly identifiable landmarks. Drawing on this and on the success of recentworks that explored localization on 2D abstract maps, we propose Flatlandia, anovel visual localization challenge. With Flatlandia, we investigate whether itis possible to localize a visual query by comparing the layout of its commonobjects detected against the known spatial layout of objects in the map. Weformalize the challenge as two tasks at different levels of accuracy toinvestigate the problem and its possible limitations; for each, we proposeinitial baseline models and compare them against state-of-the-art 6DoF and 3DoFmethods. Code and dataset are publicly available atgithub.com/IIT-PAVIS/Flatlandia.</description><author>Matteo Toso, Matteo Taiana, Stuart James, Alessio Del Bue</author><pubDate>Wed, 08 Nov 2023 14:43:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06373v4</guid></item><item><title>Training CLIP models on Data from Scientific Papers</title><link>http://arxiv.org/abs/2311.04711v1</link><description>Contrastive Language-Image Pretraining (CLIP) models are able to capture thesemantic relationship of images and texts and have enabled a wide range ofapplications, from image retrieval to classification. These models are trainedwith datasets extracted from web crawls, which are of large quantity butlimited quality. This paper explores whether limited amounts higher qualitydata in a specific domain improve the general performance of CLIP models. Tothis purpose, we extract text-image data from scientific papers hosted in thearXiv and PubMed Central repositories. Experiments on small-scale CLIP models(ViT B/32) show that model performance increases on average, but onlymoderately. This result indicates that using the data sources considered in thepaper to train large-scale CLIP models is a worthwile research direction.</description><author>Calvin Metzger</author><pubDate>Wed, 08 Nov 2023 14:38:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04711v1</guid></item></channel></rss>