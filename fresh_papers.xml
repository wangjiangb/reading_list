<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 21 Sep 2023 06:00:28 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A Large-scale Dataset for Audio-Language Representation Learning</title><link>http://arxiv.org/abs/2309.11500v1</link><description>The AI community has made significant strides in developing powerfulfoundation models, driven by large-scale multimodal datasets. However, in theaudio representation learning community, the present audio-language datasetssuffer from limitations such as insufficient volume, simplistic content, andarduous collection procedures. To tackle these challenges, we present aninnovative and automatic audio caption generation pipeline based on a series ofpublic tools or APIs, and construct a large-scale, high-quality, audio-languagedataset, named as Auto-ACD, comprising over 1.9M audio-text pairs. Todemonstrate the effectiveness of the proposed dataset, we train popular modelson our dataset and show performance improvement on various downstream tasks,namely, audio-language retrieval, audio captioning, environment classification.In addition, we establish a novel test set and provide a benchmark foraudio-text tasks. The proposed dataset will be released athttps://auto-acd.github.io/.</description><author>Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie</author><pubDate>Wed, 20 Sep 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11500v1</guid></item><item><title>DreamLLM: Synergistic Multimodal Comprehension and Creation</title><link>http://arxiv.org/abs/2309.11499v1</link><description>This paper presents DreamLLM, a learning framework that first achievesversatile Multimodal Large Language Models (MLLMs) empowered with frequentlyoverlooked synergy between multimodal comprehension and creation. DreamLLMoperates on two fundamental principles. The first focuses on the generativemodeling of both language and image posteriors by direct sampling in the rawmultimodal space. This approach circumvents the limitations and informationloss inherent to external feature extractors like CLIP, and a more thoroughmultimodal understanding is obtained. Second, DreamLLM fosters the generationof raw, interleaved documents, modeling both text and image contents, alongwith unstructured layouts. This allows DreamLLM to learn all conditional,marginal, and joint multimodal distributions effectively. As a result, DreamLLMis the first MLLM capable of generating free-form interleaved content.Comprehensive experiments highlight DreamLLM's superior performance as azero-shot multimodal generalist, reaping from the enhanced learning synergy.</description><author>Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi</author><pubDate>Wed, 20 Sep 2023 18:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11499v1</guid></item><item><title>FreeU: Free Lunch in Diffusion U-Net</title><link>http://arxiv.org/abs/2309.11497v1</link><description>In this paper, we uncover the untapped potential of diffusion U-Net, whichserves as a "free lunch" that substantially improves the generation quality onthe fly. We initially investigate the key contributions of the U-Netarchitecture to the denoising process and identify that its main backboneprimarily contributes to denoising, whereas its skip connections mainlyintroduce high-frequency features into the decoder module, causing the networkto overlook the backbone semantics. Capitalizing on this discovery, we proposea simple yet effective method-termed "FreeU" - that enhances generation qualitywithout additional training or finetuning. Our key insight is to strategicallyre-weight the contributions sourced from the U-Net's skip connections andbackbone feature maps, to leverage the strengths of both components of theU-Net architecture. Promising results on image and video generation tasksdemonstrate that our FreeU can be readily integrated to existing diffusionmodels, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion,to improve the generation quality with only a few lines of code. All you needis to adjust two scaling factors during inference. Project page:https://chenyangsi.top/FreeU/.</description><author>Chenyang Si, Ziqi Huang, Yuming Jiang, Ziwei Liu</author><pubDate>Wed, 20 Sep 2023 18:56:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11497v1</guid></item><item><title>DeepAqua: Self-Supervised Semantic Segmentation of Wetland Surface Water Extent with SAR Images using Knowledge Distillation</title><link>http://arxiv.org/abs/2305.01698v2</link><description>Deep learning and remote sensing techniques have significantly advanced watermonitoring abilities; however, the need for annotated data remains a challenge.This is particularly problematic in wetland detection, where water extentvaries over time and space, demanding multiple annotations for the same area.In this paper, we present DeepAqua, a self-supervised deep learning model thatleverages knowledge distillation (a.k.a. teacher-student model) to eliminatethe need for manual annotations during the training phase. We utilize theNormalized Difference Water Index (NDWI) as a teacher model to train aConvolutional Neural Network (CNN) for segmenting water from Synthetic ApertureRadar (SAR) images, and to train the student model, we exploit cases whereoptical- and radar-based water masks coincide, enabling the detection of bothopen and vegetated water surfaces. DeepAqua represents a significantadvancement in computer vision techniques by effectively training semanticsegmentation models without any manually annotated data. Experimental resultsshow that DeepAqua outperforms other unsupervised methods by improving accuracyby 7%, Intersection Over Union by 27%, and F1 score by 14%. This approachoffers a practical solution for monitoring wetland water extent changes withoutneeding ground truth data, making it highly adaptable and scalable for wetlandconservation efforts.</description><author>Francisco J. Peña, Clara Hübinger, Amir H. Payberah, Fernando Jaramillo</author><pubDate>Wed, 20 Sep 2023 18:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01698v2</guid></item><item><title>CATR: Combinatorial-Dependence Audio-Queried Transformer for Audio-Visual Video Segmentation</title><link>http://arxiv.org/abs/2309.09709v2</link><description>Audio-visual video segmentation~(AVVS) aims to generate pixel-level maps ofsound-producing objects within image frames and ensure the maps faithfullyadhere to the given audio, such as identifying and segmenting a singing personin a video. However, existing methods exhibit two limitations: 1) they addressvideo temporal features and audio-visual interactive features separately,disregarding the inherent spatial-temporal dependence of combined audio andvideo, and 2) they inadequately introduce audio constraints and object-levelinformation during the decoding stage, resulting in segmentation outcomes thatfail to comply with audio directives. To tackle these issues, we propose adecoupled audio-video transformer that combines audio and video features fromtheir respective temporal and spatial dimensions, capturing their combineddependence. To optimize memory consumption, we design a block, which, whenstacked, enables capturing audio-visual fine-grained combinatorial-dependencein a memory-efficient manner. Additionally, we introduce audio-constrainedqueries during the decoding phase. These queries contain rich object-levelinformation, ensuring the decoded mask adheres to the sounds. Experimentalresults confirm our approach's effectiveness, with our framework achieving anew SOTA performance on all three datasets using two backbones. The code isavailable at \url{https://github.com/aspirinone/CATR.github.io}</description><author>Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, Jun Xiao</author><pubDate>Wed, 20 Sep 2023 18:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09709v2</guid></item><item><title>Chain-of-Verification Reduces Hallucination in Large Language Models</title><link>http://arxiv.org/abs/2309.11495v1</link><description>Generation of plausible yet incorrect factual information, termedhallucination, is an unsolved issue in large language models. We study theability of language models to deliberate on the responses they give in order tocorrect their mistakes. We develop the Chain-of-Verification (CoVe) methodwhereby the model first (i) drafts an initial response; then (ii) plansverification questions to fact-check its draft; (iii) answers those questionsindependently so the answers are not biased by other responses; and (iv)generates its final verified response. In experiments, we show CoVe decreaseshallucinations across a variety of tasks, from list-based questions fromWikidata, closed book MultiSpanQA and longform text generation.</description><author>Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston</author><pubDate>Wed, 20 Sep 2023 18:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11495v1</guid></item><item><title>Flow Annealed Kalman Inversion for Gradient-Free Inference in Bayesian Inverse Problems</title><link>http://arxiv.org/abs/2309.11490v1</link><description>For many scientific inverse problems we are required to evaluate an expensiveforward model. Moreover, the model is often given in such a form that it isunrealistic to access its gradients. In such a scenario, standard Markov ChainMonte Carlo algorithms quickly become impractical, requiring a large number ofserial model evaluations to converge on the target distribution. In this paperwe introduce Flow Annealed Kalman Inversion (FAKI). This is a generalization ofEnsemble Kalman Inversion (EKI), where we embed the Kalman filter updates in atemperature annealing scheme, and use normalizing flows (NF) to map theintermediate measures corresponding to each temperature level to the standardGaussian. In doing so, we relax the Gaussian ansatz for the intermediatemeasures used in standard EKI, allowing us to achieve higher fidelityapproximations to non-Gaussian targets. We demonstrate the performance of FAKIon two numerical benchmarks, showing dramatic improvements over standard EKI interms of accuracy whilst accelerating its already rapid convergence properties(typically in $\mathcal{O}(10)$ steps).</description><author>Richard D. P. Grumitt, Minas Karamanis, Uroš Seljak</author><pubDate>Wed, 20 Sep 2023 18:39:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11490v1</guid></item><item><title>Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning</title><link>http://arxiv.org/abs/2309.11489v1</link><description>Designing reward functions is a longstanding challenge in reinforcementlearning (RL); it requires specialized knowledge or domain data, leading tohigh costs for development. To address this, we introduce Text2Reward, adata-free framework that automates the generation of dense reward functionsbased on large language models (LLMs). Given a goal described in naturallanguage, Text2Reward generates dense reward functions as an executable programgrounded in a compact representation of the environment. Unlike inverse RL andrecent work that uses LLMs to write sparse reward codes, Text2Reward producesinterpretable, free-form dense reward codes that cover a wide range of tasks,utilize existing packages, and allow iterative refinement with human feedback.We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17manipulation tasks, policies trained with generated reward codes achievesimilar or better task success rates and convergence speed than expert-writtenreward codes. For locomotion tasks, our method learns six novel locomotionbehaviors with a success rate exceeding 94%. Furthermore, we show that thepolicies trained in the simulator with our method can be deployed in the realworld. Finally, Text2Reward further improves the policies by refining theirreward functions with human feedback. Video results are available athttps://text-to-reward.github.io</description><author>Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</author><pubDate>Wed, 20 Sep 2023 18:39:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11489v1</guid></item><item><title>Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs</title><link>http://arxiv.org/abs/2309.11478v1</link><description>We address the integration of storytelling and Large Language Models (LLMs)to develop engaging and believable Social Chatbots (SCs) in community settings.Motivated by the potential of fictional characters to enhance socialinteractions, we introduce Storytelling Social Chatbots (SSCs) and the conceptof story engineering to transform fictional game characters into "live" socialentities within player communities. Our story engineering process includesthree steps: (1) Character and story creation, defining the SC's personalityand worldview, (2) Presenting Live Stories to the Community, allowing the SC torecount challenges and seek suggestions, and (3) Communication with communitymembers, enabling interaction between the SC and users. We employed the LLMGPT-3 to drive our SSC prototypes, "David" and "Catherine," and evaluated theirperformance in an online gaming community, "DE (Alias)," on Discord. Ourmixed-method analysis, based on questionnaires (N=15) and interviews (N=8) withcommunity members, reveals that storytelling significantly enhances theengagement and believability of SCs in community settings.</description><author>Yuqian Sun, Hanyi Wang, Pok Man Chan, Morteza Tabibi, Yan Zhang, Huan Lu, Yuheng Chen, Chang Hee Lee, Ali Asadipour</author><pubDate>Wed, 20 Sep 2023 18:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11478v1</guid></item><item><title>Multiplying poles to avoid unwanted points in root finding and optimization</title><link>http://arxiv.org/abs/2309.11475v1</link><description>In root finding and optimization, there are many cases where there is aclosed set $A$ one does not the sequence constructed by one's favourite methodwill converge to A (here, we do not assume extra properties on $A$ such asbeing convex or connected). For example, if one wants to find roots, and onechooses initial points in the basin of attraction for 1 root $x^*$ (a factwhich one may not know before hand), then one will always end up in that root.In this case, one would like to have a mechanism to avoid this point $z^*$ inthe next runs of one's algorithm. In this paper, we propose a new method aiming to achieve this: we divide thecost function by an appropriate power of the distance function to $A$. Thisidea is inspired by how one would try to find all roots of a function in 1variable. We first explain the heuristic for this method in the case where theminimum of the cost function is exactly 0, and then explain how to proceed ifthe minimum is non-zero (allowing both positive and negative values). Themethod is very suitable for iterative algorithms which have the descentproperty. We also propose, based on this, an algorithm to escape the basin ofattraction of a component of positive dimension to reach another component. Along the way, we compare with main existing relevant methods in the currentliterature. We provide several examples to illustrate the usefulness of the newapproach.</description><author>Tuyen Trung Truong</author><pubDate>Wed, 20 Sep 2023 18:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11475v1</guid></item><item><title>Toward Dynamic Stability Assessment of Power Grid Topologies using Graph Neural Networks</title><link>http://arxiv.org/abs/2206.06369v4</link><description>To mitigate climate change, the share of renewable energies in powerproduction needs to be increased. Renewables introduce new challenges to powergrids regarding the dynamic stability due to decentralization, reduced inertia,and volatility in production. Since dynamic stability simulations areintractable and exceedingly expensive for large grids, graph neural networks(GNNs) are a promising method to reduce the computational effort of analyzingthe dynamic stability of power grids. As a testbed for GNN models, we generatenew, large datasets of dynamic stability of synthetic power grids, and providethem as an open-source resource to the research community. We find that GNNsare surprisingly effective at predicting the highly non-linear targets fromtopological information only. For the first time, performance that is suitablefor practical use cases is achieved. Furthermore, we demonstrate the ability ofthese models to accurately identify particular vulnerable nodes in power grids,so-called troublemakers. Last, we find that GNNs trained on small gridsgenerate accurate predictions on a large synthetic model of the Texan powergrid, which illustrates the potential for real-world applications.</description><author>Christian Nauck, Michael Lindner, Konstantin Schürholt, Frank Hellmann</author><pubDate>Wed, 20 Sep 2023 18:17:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.06369v4</guid></item><item><title>MasakhaNEWS: News Topic Classification for African languages</title><link>http://arxiv.org/abs/2304.09972v2</link><description>African languages are severely under-represented in NLP research due to lackof datasets covering several NLP tasks. While there are individual languagespecific datasets that are being expanded to different tasks, only a handful ofNLP tasks (e.g. named entity recognition and machine translation) havestandardized benchmark datasets covering several geographical andtypologically-diverse African languages. In this paper, we develop MasakhaNEWS-- a new benchmark dataset for news topic classification covering 16 languageswidely spoken in Africa. We provide an evaluation of baseline models bytraining classical machine learning models and fine-tuning several languagemodels. Furthermore, we explore several alternatives to full fine-tuning oflanguage models that are better suited for zero-shot and few-shot learning suchas cross-lingual parameter-efficient fine-tuning (like MAD-X), patternexploiting training (PET), prompting language models (like ChatGPT), andprompt-free sentence transformer fine-tuning (SetFit and Cohere Embedding API).Our evaluation in zero-shot setting shows the potential of prompting ChatGPTfor news topic classification in low-resource African languages, achieving anaverage performance of 70 F1 points without leveraging additional supervisionlike MAD-X. In few-shot setting, we show that with as little as 10 examples perlabel, we achieved more than 90\% (i.e. 86.0 F1 points) of the performance offull supervised training (92.6 F1 points) leveraging the PET approach.</description><author>David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba Alabi, Atnafu Lambebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure F. P. Dossou, Akintunde Oladipo, Doreen Nixdorf, Chris Chinenye Emezue, sana al-azzawi, Blessing Sibanda, Davis David, Lolwethu Ndolela, Jonathan Mukiibi, Tunde Ajayi, Tatiana Moteu, Brian Odhiambo, Abraham Owodunni, Nnaemeka Obiefuna, Muhidin Mohamed, Shamsuddeen Hassan Muhammad, Teshome Mulugeta Ababu, Saheed Abdullahi Salahudeen, Mesay Gemeda Yigezu, Tajuddeen Gwadabe, Idris Abdulmumin, Mahlet Taye, Oluwabusayo Awoyomi, Iyanuoluwa Shode, Tolulope Adelani, Habiba Abdulganiyu, Abdul-Hakeem Omotayo, Adetola Adeeko, Abeeb Afolabi, Anuoluwapo Aremu, Olanrewaju Samuel, Clemencia Siro, Wangari Kimotho, Onyekachi Ogbu, Chinedu Mbonu, Chiamaka Chukwuneke,</author><pubDate>Wed, 20 Sep 2023 18:14:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09972v2</guid></item><item><title>Multi-view Fuzzy Representation Learning with Rules based Model</title><link>http://arxiv.org/abs/2309.11473v1</link><description>Unsupervised multi-view representation learning has been extensively studiedfor mining multi-view data. However, some critical challenges remain. On theone hand, the existing methods cannot explore multi-view data comprehensivelysince they usually learn a common representation between views, given thatmulti-view data contains both the common information between views and thespecific information within each view. On the other hand, to mine the nonlinearrelationship between data, kernel or neural network methods are commonly usedfor multi-view representation learning. However, these methods are lacking ininterpretability. To this end, this paper proposes a new multi-view fuzzyrepresentation learning method based on the interpretable Takagi-Sugeno-Kang(TSK) fuzzy system (MVRL_FS). The method realizes multi-view representationlearning from two aspects. First, multi-view data are transformed into ahigh-dimensional fuzzy feature space, while the common information betweenviews and specific information of each view are explored simultaneously.Second, a new regularization method based on L_(2,1)-norm regression isproposed to mine the consistency information between views, while the geometricstructure of the data is preserved through the Laplacian graph. Finally,extensive experiments on many benchmark multi-view datasets are conducted tovalidate the superiority of the proposed method.</description><author>Wei Zhang, Zhaohong Deng, Te Zhang, Kup-Sze Choi, Shitong Wang</author><pubDate>Wed, 20 Sep 2023 18:13:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11473v1</guid></item><item><title>Model-free tracking control of complex dynamical trajectories with machine learning</title><link>http://arxiv.org/abs/2309.11470v1</link><description>Nonlinear tracking control enabling a dynamical system to track a desiredtrajectory is fundamental to robotics, serving a wide range of civil anddefense applications. In control engineering, designing tracking controlrequires complete knowledge of the system model and equations. We develop amodel-free, machine-learning framework to control a two-arm robotic manipulatorusing only partially observed states, where the controller is realized byreservoir computing. Stochastic input is exploited for training, which consistsof the observed partial state vector as the first and its immediate future asthe second component so that the neural machine regards the latter as thefuture state of the former. In the testing (deployment) phase, theimmediate-future component is replaced by the desired observational vector fromthe reference trajectory. We demonstrate the effectiveness of the controlframework using a variety of periodic and chaotic signals, and establish itsrobustness against measurement noise, disturbances, and uncertainties.</description><author>Zheng-Meng Zhai, Mohammadamin Moradi, Ling-Wei Kong, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai</author><pubDate>Wed, 20 Sep 2023 18:10:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11470v1</guid></item><item><title>Multi-Label Takagi-Sugeno-Kang Fuzzy System</title><link>http://arxiv.org/abs/2309.11469v1</link><description>Multi-label classification can effectively identify the relevant labels of aninstance from a given set of labels. However,the modeling of the relationshipbetween the features and the labels is critical to the classificationperformance. To this end, we propose a new multi-label classification method,called Multi-Label Takagi-Sugeno-Kang Fuzzy System (ML-TSK FS), to improve theclassification performance. The structure of ML-TSK FS is designed using fuzzyrules to model the relationship between features and labels. The fuzzy systemis trained by integrating fuzzy inference based multi-label correlationlearning with multi-label regression loss. The proposed ML-TSK FS is evaluatedexperimentally on 12 benchmark multi-label datasets. 1 The results show thatthe performance of ML-TSK FS is competitive with existing methods in terms ofvarious evaluation metrics, indicating that it is able to model thefeature-label relationship effectively using fuzzy inference rules and enhancesthe classification performance.</description><author>Qiongdan Lou, Zhaohong Deng, Zhiyong Xiao, Kup-Sze Choi, Shitong Wang</author><pubDate>Wed, 20 Sep 2023 18:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11469v1</guid></item><item><title>Graph Fuzzy System: Concepts, Models and Algorithms</title><link>http://arxiv.org/abs/2210.16730v2</link><description>Fuzzy systems (FSs) have enjoyed wide applications in various fields,including pattern recognition, intelligent control, data mining andbioinformatics, which is attributed to the strong interpretation and learningability. In traditional application scenarios, FSs are mainly applied to modelEuclidean space data and cannot be used to handle graph data of non-Euclideanstructure in nature, such as social networks and traffic route maps. Therefore,development of FS modeling method that is suitable for graph data and canretain the advantages of traditional FSs is an important research. To meet thischallenge, a new type of FS for graph data modeling called Graph Fuzzy System(GFS) is proposed in this paper, where the concepts, modeling framework andconstruction algorithms are systematically developed. First, GFS relatedconcepts, including graph fuzzy rule base, graph fuzzy sets and graphconsequent processing unit (GCPU), are defined. A GFS modeling framework isthen constructed and the antecedents and consequents of the GFS are presentedand analyzed. Finally, a learning framework of GFS is proposed, in which akernel K-prototype graph clustering (K2PGC) is proposed to develop theconstruction algorithm for the GFS antecedent generation, and then based ongraph neural network (GNNs), consequent parameters learning algorithm isproposed for GFS. Specifically, three different versions of the GFSimplementation algorithm are developed for comprehensive evaluations withexperiments on various benchmark graph classification datasets. The resultsdemonstrate that the proposed GFS inherits the advantages of both existingmainstream GNNs methods and conventional FSs methods while achieving betterperformance than the counterparts.</description><author>Fuping Hu, Zhaohong Deng, Zhenping Xie, Kup-Sze Choi, Shitong Wang</author><pubDate>Wed, 20 Sep 2023 18:02:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16730v2</guid></item><item><title>Budget-Aware Pruning: Handling Multiple Domains with Less Parameters</title><link>http://arxiv.org/abs/2309.11464v1</link><description>Deep learning has achieved state-of-the-art performance on several computervision tasks and domains. Nevertheless, it still has a high computational costand demands a significant amount of parameters. Such requirements hinder theuse in resource-limited environments and demand both software and hardwareoptimization. Another limitation is that deep models are usually specializedinto a single domain or task, requiring them to learn and store new parametersfor each new one. Multi-Domain Learning (MDL) attempts to solve this problem bylearning a single model that is capable of performing well in multiple domains.Nevertheless, the models are usually larger than the baseline for a singledomain. This work tackles both of these problems: our objective is to prunemodels capable of handling multiple domains according to a user-defined budget,making them more computationally affordable while keeping a similarclassification performance. We achieve this by encouraging all domains to use asimilar subset of filters from the baseline model, up to the amount defined bythe user's budget. Then, filters that are not used by any domain are prunedfrom the network. The proposed approach innovates by better adapting toresource-limited devices while, to our knowledge, being the only work thathandles multiple domains at test time with fewer parameters and lowercomputational complexity than the baseline model for a single domain.</description><author>Samuel Felipe dos Santos, Rodrigo Berriel, Thiago Oliveira-Santos, Nicu Sebe, Jurandy Almeida</author><pubDate>Wed, 20 Sep 2023 18:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11464v1</guid></item><item><title>AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition</title><link>http://arxiv.org/abs/2309.11462v1</link><description>Automatic Speech Recognition systems have been shown to be vulnerable toadversarial attacks that manipulate the command executed on the device. Recentresearch has focused on exploring methods to create such attacks, however, someissues relating to Over-The-Air (OTA) attacks have not been properly addressed.In our work, we examine the needed properties of robust attacks compatible withthe OTA model, and we design a method of generating attacks with arbitrary suchdesired properties, namely the invariance to synchronization, and therobustness to filtering: this allows a Denial-of-Service (DoS) attack againstASR systems. We achieve these characteristics by constructing attacks in amodified frequency domain through an inverse Fourier transform. We evaluate ourmethod on standard keyword classification tasks and analyze it in OTA, and weanalyze the properties of the cross-domain attacks to explain the efficiency ofthe approach.</description><author>Mohamad Fakih, Rouwaida Kanj, Fadi Kurdahi, Mohammed E. Fouda</author><pubDate>Wed, 20 Sep 2023 17:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11462v1</guid></item><item><title>A Robust Multilabel Method Integrating Rule-based Transparent Model, Soft Label Correlation Learning and Label Noise Resistance</title><link>http://arxiv.org/abs/2301.03283v2</link><description>Model transparency, label correlation learning and the robust-ness to labelnoise are crucial for multilabel learning. However, few existing methods studythese three characteristics simultaneously. To address this challenge, wepropose the robust multilabel Takagi-Sugeno-Kang fuzzy system (R-MLTSK-FS) withthree mechanisms. First, we design a soft label learning mechanism to reducethe effect of label noise by explicitly measuring the interactions betweenlabels, which is also the basis of the other two mechanisms. Second, therule-based TSK FS is used as the base model to efficiently model the inferencerelationship be-tween features and soft labels in a more transparent way thanmany existing multilabel models. Third, to further improve the performance ofmultilabel learning, we build a correlation enhancement learning mechanismbased on the soft label space and the fuzzy feature space. Extensiveexperiments are conducted to demonstrate the superiority of the proposedmethod.</description><author>Qiongdan Lou, Zhaohong Deng, Kup-Sze Choi, Shitong Wang</author><pubDate>Wed, 20 Sep 2023 17:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03283v2</guid></item><item><title>Digital twins of nonlinear dynamical systems: A perspective</title><link>http://arxiv.org/abs/2309.11461v1</link><description>Digital twins have attracted a great deal of recent attention from a widerange of fields. A basic requirement for digital twins of nonlinear dynamicalsystems is the ability to generate the system evolution and predict potentiallycatastrophic emergent behaviors so as to providing early warnings. The digitaltwin can then be used for system "health" monitoring in real time and forpredictive problem solving. In particular, if the digital twin forecasts apossible system collapse in the future due to parameter drifting as caused byenvironmental changes or perturbations, an optimal control strategy can bedevised and executed as early intervention to prevent the collapse. Twoapproaches exist for constructing digital twins of nonlinear dynamical systems:sparse optimization and machine learning. The basics of these two approachesare described and their advantages and caveats are discussed.</description><author>Ying-Cheng Lai</author><pubDate>Wed, 20 Sep 2023 17:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11461v1</guid></item><item><title>Evolving generalist controllers to handle a wide range of morphological variations</title><link>http://arxiv.org/abs/2309.10201v2</link><description>Neuro-evolutionary methods have proven effective in addressing a wide rangeof tasks. However, the study of the robustness and generalisability of evolvedartificial neural networks (ANNs) has remained limited. This has immenseimplications in the fields like robotics where such controllers are used incontrol tasks. Unexpected morphological or environmental changes duringoperation can risk failure if the ANN controllers are unable to handle thesechanges. This paper proposes an algorithm that aims to enhance the robustnessand generalisability of the controllers. This is achieved by introducingmorphological variations during the evolutionary process. As a results, it ispossible to discover generalist controllers that can handle a wide range ofmorphological variations sufficiently without the need of the informationregarding their morphologies or adaptation of their parameters. We perform anextensive experimental analysis on simulation that demonstrates the trade-offbetween specialist and generalist controllers. The results show thatgeneralists are able to control a range of morphological variations with a costof underperforming on a specific morphology relative to a specialist. Thisresearch contributes to the field by addressing the limited understanding ofrobustness and generalisability in neuro-evolutionary methods and proposes amethod by which to improve these properties.</description><author>Corinna Triebold, Anil Yaman</author><pubDate>Wed, 20 Sep 2023 17:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10201v2</guid></item><item><title>Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration</title><link>http://arxiv.org/abs/2108.06038v2</link><description>We present a method for learning a human-robot collaboration policy fromhuman-human collaboration demonstrations. An effective robot assistant mustlearn to handle diverse human behaviors shown in the demonstrations and berobust when the humans adjust their strategies during online task execution.Our method co-optimizes a human policy and a robot policy in an interactivelearning process: the human policy learns to generate diverse and plausiblecollaborative behaviors from demonstrations while the robot policy learns toassist by estimating the unobserved latent strategy of its human collaborator.Across a 2D strategy game, a human-robot handover task, and a multi-stepcollaborative manipulation task, our method outperforms the alternatives inboth simulated evaluations and when executing the tasks with a real humanoperator in-the-loop. Supplementary materials and videos athttps://sites.google.com/view/co-gail-web/home</description><author>Chen Wang, Claudia Pérez-D'Arpino, Danfei Xu, Li Fei-Fei, C. Karen Liu, Silvio Savarese</author><pubDate>Wed, 20 Sep 2023 17:45:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.06038v2</guid></item><item><title>Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence</title><link>http://arxiv.org/abs/2309.11456v1</link><description>We discuss the emerging new opportunity for building feedback-richcomputational models of social systems using generative artificialintelligence. Referred to as Generative Agent-Based Models (GABMs), suchindividual-level models utilize large language models such as ChatGPT torepresent human decision-making in social settings. We provide a GABM case inwhich human behavior can be incorporated in simulation models by coupling amechanistic model of human interactions with a pre-trained large languagemodel. This is achieved by introducing a simple GABM of social norm diffusionin an organization. For educational purposes, the model is intentionally keptsimple. We examine a wide range of scenarios and the sensitivity of the resultsto several changes in the prompt. We hope the article and the model serve as aguide for building useful diffusion models that include realistic humanreasoning and decision-making.</description><author>Navid Ghaffarzadegan, Aritra Majumdar, Ross Williams, Niyousha Hosseinichimeh</author><pubDate>Wed, 20 Sep 2023 17:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11456v1</guid></item><item><title>A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases</title><link>http://arxiv.org/abs/2208.05845v3</link><description>In recent years, image and video manipulations with Deepfake have become asevere concern for security and society. Many detection models and datasetshave been proposed to detect Deepfake data reliably. However, there is anincreased concern that these models and training databases might be biased and,thus, cause Deepfake detectors to fail. In this work, we investigate the biasissue caused by public Deepfake datasets by (a) providing large-scaledemographic and non-demographic attribute annotations of 47 differentattributes for five popular Deepfake datasets and (b) comprehensively analysingAI-bias of three state-of-the-art Deepfake detection backbone models on thesedatasets. The investigation analyses the influence of a large variety ofdistinctive attributes (from over 65M labels) on the detection performance,including demographic (age, gender, ethnicity) and non-demographic (hair, skin,accessories, etc.) information. The results indicate that investigateddatabases lack diversity and, more importantly, show that the utilised Deepfakedetection backbone models are strongly biased towards many investigatedattributes. The Deepfake detection backbone methods, which are trained withbiased datasets, might output incorrect detection results, thereby leading togeneralisability, fairness, and security issues. We hope that the findings ofthis study and the annotation databases will help to evaluate and mitigate biasin future Deepfake detection techniques. The annotation datasets and thecorresponding code are publicly available.</description><author>Ying Xu, Philipp Terhörst, Kiran Raja, Marius Pedersen</author><pubDate>Wed, 20 Sep 2023 17:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.05845v3</guid></item><item><title>Multi-Step Model Predictive Safety Filters: Reducing Chattering by Increasing the Prediction Horizon</title><link>http://arxiv.org/abs/2309.11453v1</link><description>Learning-based controllers have demonstrated superior performance compared toclassical controllers in various tasks. However, providing safety guarantees isnot trivial. Safety, the satisfaction of state and input constraints, can beguaranteed by augmenting the learned control policy with a safety filter. Modelpredictive safety filters (MPSFs) are a common safety filtering approach basedon model predictive control (MPC). MPSFs seek to guarantee safety whileminimizing the difference between the proposed and applied inputs in theimmediate next time step. This limited foresight can lead to jerky motions andundesired oscillations close to constraint boundaries, known as chattering. Inthis paper, we reduce chattering by considering input corrections over a longerhorizon. Under the assumption of bounded model uncertainties, we proverecursive feasibility using techniques from robust MPC. We verified theproposed approach in both extensive simulation and quadrotor experiments. Inexperiments with a Crazyflie 2.0 drone, we show that, in addition to preservingthe desired safety guarantees, the proposed MPSF reduces chattering by morethan a factor of 4 compared to previous MPSF formulations.</description><author>Federico Pizarro Bejarano, Lukas Brunke, Angela P. Schoellig</author><pubDate>Wed, 20 Sep 2023 17:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11453v1</guid></item><item><title>Using deep learning to construct stochastic local search SAT solvers with performance bounds</title><link>http://arxiv.org/abs/2309.11452v1</link><description>The Boolean Satisfiability problem (SAT) is the most prototypical NP-completeproblem and of great practical relevance. One important class of solvers forthis problem are stochastic local search (SLS) algorithms that iteratively andrandomly update a candidate assignment. Recent breakthrough results intheoretical computer science have established sufficient conditions under whichSLS solvers are guaranteed to efficiently solve a SAT instance, provided theyhave access to suitable "oracles" that provide samples from aninstance-specific distribution, exploiting an instance's local structure.Motivated by these results and the well established ability of neural networksto learn common structure in large datasets, in this work, we train oraclesusing Graph Neural Networks and evaluate them on two SLS solvers on random SATinstances of varying difficulty. We find that access to GNN-based oraclessignificantly boosts the performance of both solvers, allowing them, onaverage, to solve 17% more difficult instances (as measured by the ratiobetween clauses and variables), and to do so in 35% fewer steps, withimprovements in the median number of steps of up to a factor of 8. As such,this work bridges formal results from theoretical computer science andpractically motivated research on deep learning for constraint satisfactionproblems and establishes the promise of purpose-trained SAT solvers withperformance guarantees.</description><author>Maximilian Kramer, Paul Boes</author><pubDate>Wed, 20 Sep 2023 17:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11452v1</guid></item><item><title>Distribution and volume based scoring for Isolation Forests</title><link>http://arxiv.org/abs/2309.11450v1</link><description>We make two contributions to the Isolation Forest method for anomaly andoutlier detection. The first contribution is an information-theoreticallymotivated generalisation of the score function that is used to aggregate thescores across random tree estimators. This generalisation allows one to takeinto account not just the ensemble average across trees but instead the wholedistribution. The second contribution is an alternative scoring function at thelevel of the individual tree estimator, in which we replace the depth-basedscoring of the Isolation Forest with one based on hyper-volumes associated toan isolation tree's leaf nodes. We motivate the use of both of these methods on generated data and alsoevaluate them on 34 datasets from the recent and exhaustive ``ADBench''benchmark, finding significant improvement over the standard isolation forestfor both variants on some datasets and improvement on average across alldatasets for one of the two variants. The code to reproduce our results is madeavailable as part of the submission.</description><author>Hichem Dhouib, Alissa Wilms, Paul Boes</author><pubDate>Wed, 20 Sep 2023 17:27:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11450v1</guid></item><item><title>Weight Averaging Improves Knowledge Distillation under Domain Shift</title><link>http://arxiv.org/abs/2309.11446v1</link><description>Knowledge distillation (KD) is a powerful model compression technique broadlyused in practical deep learning applications. It is focused on training a smallstudent network to mimic a larger teacher network. While it is widely knownthat KD can offer an improvement to student generalization in i.i.d setting,its performance under domain shift, i.e. the performance of student networks ondata from domains unseen during training, has received little attention in theliterature. In this paper we make a step towards bridging the research fieldsof knowledge distillation and domain generalization. We show that weightaveraging techniques proposed in domain generalization literature, such as SWADand SMA, also improve the performance of knowledge distillation under domainshift. In addition, we propose a simplistic weight averaging strategy that doesnot require evaluation on validation data during training and show that itperforms on par with SWAD and SMA when applied to KD. We name our finaldistillation approach Weight-Averaged Knowledge Distillation (WAKD).</description><author>Valeriy Berezovskiy, Nikita Morozov</author><pubDate>Wed, 20 Sep 2023 17:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11446v1</guid></item><item><title>SkeleTR: Towrads Skeleton-based Action Recognition in the Wild</title><link>http://arxiv.org/abs/2309.11445v1</link><description>We present SkeleTR, a new framework for skeleton-based action recognition. Incontrast to prior work, which focuses mainly on controlled environments, wetarget more general scenarios that typically involve a variable number ofpeople and various forms of interaction between people. SkeleTR works with atwo-stage paradigm. It first models the intra-person skeleton dynamics for eachskeleton sequence with graph convolutions, and then uses stacked Transformerencoders to capture person interactions that are important for actionrecognition in general scenarios. To mitigate the negative impact of inaccurateskeleton associations, SkeleTR takes relative short skeleton sequences as inputand increases the number of sequences. As a unified solution, SkeleTR can bedirectly applied to multiple skeleton-based action tasks, including video-levelaction classification, instance-level action detection, and group-levelactivity recognition. It also enables transfer learning and joint trainingacross different action tasks and datasets, which result in performanceimprovement. When evaluated on various skeleton-based action recognitionbenchmarks, SkeleTR achieves the state-of-the-art performance.</description><author>Haodong Duan, Mingze Xu, Bing Shuai, Davide Modolo, Zhuowen Tu, Joseph Tighe, Alessandro Bergamo</author><pubDate>Wed, 20 Sep 2023 17:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11445v1</guid></item><item><title>GloPro: Globally-Consistent Uncertainty-Aware 3D Human Pose Estimation &amp; Tracking in the Wild</title><link>http://arxiv.org/abs/2309.10369v2</link><description>An accurate and uncertainty-aware 3D human body pose estimation is key toenabling truly safe but efficient human-robot interactions. Currentuncertainty-aware methods in 3D human pose estimation are limited to predictingthe uncertainty of the body posture, while effectively neglecting the bodyshape and root pose. In this work, we present GloPro, which to the best of ourknowledge the first framework to predict an uncertainty distribution of a 3Dbody mesh including its shape, pose, and root pose, by efficiently fusingvisual clues with a learned motion model. We demonstrate that it vastlyoutperforms state-of-the-art methods in terms of human trajectory accuracy in aworld coordinate system (even in the presence of severe occlusions), yieldsconsistent uncertainty distributions, and can run in real-time.</description><author>Simon Schaefer, Dorian F. Henning, Stefan Leutenegger</author><pubDate>Wed, 20 Sep 2023 17:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10369v2</guid></item><item><title>Signature Activation: A Sparse Signal View for Holistic Saliency</title><link>http://arxiv.org/abs/2309.11443v1</link><description>The adoption of machine learning in healthcare calls for model transparencyand explainability. In this work, we introduce Signature Activation, a saliencymethod that generates holistic and class-agnostic explanations forConvolutional Neural Network (CNN) outputs. Our method exploits the fact thatcertain kinds of medical images, such as angiograms, have clear foreground andbackground objects. We give theoretical explanation to justify our methods. Weshow the potential use of our method in clinical settings through evaluatingits efficacy for aiding the detection of lesions in coronary angiograms.</description><author>Jose Roberto Tello Ayala, Akl C. Fahed, Weiwei Pan, Eugene V. Pomerantsev, Patrick T. Ellinor, Anthony Philippakis, Finale Doshi-Velez</author><pubDate>Wed, 20 Sep 2023 17:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11443v1</guid></item><item><title>Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction</title><link>http://arxiv.org/abs/2309.11439v1</link><description>In Grammatical Error Correction (GEC), it is crucial to ensure the user'scomprehension of a reason for correction. Existing studies present tokens,examples, and hints as to the basis for correction but do not directly explainthe reasons for corrections. Although methods that use Large Language Models(LLMs) to provide direct explanations in natural language have been proposedfor various tasks, no such method exists for GEC. Generating explanations forGEC corrections involves aligning input and output tokens, identifyingcorrection points, and presenting corresponding explanations consistently.However, it is not straightforward to specify a complex format to generateexplanations, because explicit control of generation is difficult with prompts.This study introduces a method called controlled generation with PromptInsertion (PI) so that LLMs can explain the reasons for corrections in naturallanguage. In PI, LLMs first correct the input text, and then we automaticallyextract the correction points based on the rules. The extracted correctionpoints are sequentially inserted into the LLM's explanation output as prompts,guiding the LLMs to generate explanations for the correction points. We alsocreate an Explainable GEC (XGEC) dataset of correction reasons by annotatingNUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPTusing original prompts miss some correction points, the generation controlusing PI can explicitly guide to describe explanations for all correctionpoints, contributing to improved performance in generating correction reasons.</description><author>Masahiro Kaneko, Naoaki Okazaki</author><pubDate>Wed, 20 Sep 2023 17:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11439v1</guid></item><item><title>You Only Look at Screens: Multimodal Chain-of-Action Agents</title><link>http://arxiv.org/abs/2309.11436v1</link><description>Autonomous user interface (UI) agents aim to facilitate task automation byinteracting with the user interface without manual intervention. Recent studieshave investigated eliciting the capabilities of large language models (LLMs)for effective engagement in diverse environments. To align with theinput-output requirement of LLMs, existing approaches are developed under asandbox setting where they rely on external tools and application-specific APIsto parse the environment into textual elements and interpret the predictedactions. Consequently, those approaches often grapple with inferenceinefficiency and error propagation risks. To mitigate the challenges, weintroduce Auto-UI, a multimodal solution that directly interacts with theinterface, bypassing the need for environment parsing or reliance onapplication-dependent APIs. Moreover, we propose a chain-of-action technique --leveraging a series of intermediate previous action histories and future actionplans -- to help the agent decide what action to execute. We evaluate ourapproach on a new device-control benchmark AITW with 30K unique instructions,spanning multi-step tasks such as application operation, web searching, and webshopping. Experimental results show that Auto-UI achieves state-of-the-artperformance with an action type prediction accuracy of 90% and an overallaction success rate of 74%. Code is publicly available athttps://github.com/cooelf/Auto-UI.</description><author>Zhuosheng Zhan, Aston Zhang</author><pubDate>Wed, 20 Sep 2023 17:12:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11436v1</guid></item><item><title>A Systematic Review of Few-Shot Learning in Medical Imaging</title><link>http://arxiv.org/abs/2309.11433v1</link><description>The lack of annotated medical images limits the performance of deep learningmodels, which usually need large-scale labelled datasets. Few-shot learningtechniques can reduce data scarcity issues and enhance medical image analysis,especially with meta-learning. This systematic review gives a comprehensiveoverview of few-shot learning in medical imaging. We searched the literaturesystematically and selected 80 relevant articles published from 2018 to 2023.We clustered the articles based on medical outcomes, such as tumoursegmentation, disease classification, and image registration; anatomicalstructure investigated (i.e. heart, lung, etc.); and the meta-learning methodused. For each cluster, we examined the papers' distributions and the resultsprovided by the state-of-the-art. In addition, we identified a generic pipelineshared among all the studies. The review shows that few-shot learning canovercome data scarcity in most outcomes and that meta-learning is a popularchoice to perform few-shot learning because it can adapt to new tasks with fewlabelled samples. In addition, following meta-learning, supervised learning andsemi-supervised learning stand out as the predominant techniques employed totackle few-shot learning challenges in medical imaging and also bestperforming. Lastly, we observed that the primary application areaspredominantly encompass cardiac, pulmonary, and abdominal domains. Thissystematic review aims to inspire further research to improve medical imageanalysis and patient care.</description><author>Eva Pachetti, Sara Colantonio</author><pubDate>Wed, 20 Sep 2023 17:10:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11433v1</guid></item><item><title>Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing</title><link>http://arxiv.org/abs/2309.11427v1</link><description>This paper introduces TRACE-GPT, which stands for Time-seRiesAnomaly-detection with Convolutional Embedding and Generative Pre-trainedTransformers. TRACE-GPT is designed to pre-train univariate time-series sensordata and detect faults on unlabeled datasets in semiconductor manufacturing. Insemiconductor industry, classifying abnormal time-series sensor data fromnormal data is important because it is directly related to wafer defect.However, small, unlabeled, and even mixed training data without enoughanomalies make classification tasks difficult. In this research, we capturefeatures of time-series data with temporal convolutional embedding andGenerative Pre-trained Transformer (GPT) to classify abnormal sequences fromnormal sequences using cross entropy loss. We prove that our model shows betterperformance than previous unsupervised models with both an open dataset, theUniversity of California Riverside (UCR) time-series classification archive,and the process log of our Chemical Vapor Deposition (CVD) equipment. Our modelhas the highest F1 score at Equal Error Rate (EER) across all datasets and isonly 0.026 below the supervised state-of-the-art baseline on the open dataset.</description><author>Sewoong Lee, JinKyou Choi, Min Su Kim</author><pubDate>Wed, 20 Sep 2023 17:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11427v1</guid></item><item><title>Context is Environment</title><link>http://arxiv.org/abs/2309.09888v2</link><description>Two lines of work are taking the central stage in AI research. On the onehand, the community is making increasing efforts to build models that discardspurious correlations and generalize better in novel test environments.Unfortunately, the bitter lesson so far is that no proposal convincinglyoutperforms a simple empirical risk minimization baseline. On the other hand,large language models (LLMs) have erupted as algorithms able to learnin-context, generalizing on-the-fly to eclectic contextual circumstances thatusers enforce by means of prompting. In this paper, we argue that context isenvironment, and posit that in-context learning holds the key to better domaingeneralization. Via extensive theory and experiments, we show that payingattention to context$\unicode{x2013}\unicode{x2013}$unlabeled examples as theyarrive$\unicode{x2013}\unicode{x2013}$allows our proposed In-Context RiskMinimization (ICRM) algorithm to zoom-in on the test environment riskminimizer, leading to significant out-of-distribution performance improvements.From all of this, two messages are worth taking home. Researchers in domaingeneralization should consider environment as context, and harness the adaptivepower of in-context learning. Researchers in LLMs should consider context asenvironment, to better structure data towards generalization.</description><author>Sharut Gupta, Stefanie Jegelka, David Lopez-Paz, Kartik Ahuja</author><pubDate>Wed, 20 Sep 2023 16:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09888v2</guid></item><item><title>CalibFPA: A Focal Plane Array Imaging System based on Online Deep-Learning Calibration</title><link>http://arxiv.org/abs/2309.11421v1</link><description>Compressive focal plane arrays (FPA) enable cost-effective high-resolution(HR) imaging by acquisition of several multiplexed measurements on alow-resolution (LR) sensor. Multiplexed encoding of the visual scene istypically performed via electronically controllable spatial light modulators(SLM). An HR image is then reconstructed from the encoded measurements bysolving an inverse problem that involves the forward model of the imagingsystem. To capture system non-idealities such as optical aberrations, amainstream approach is to conduct an offline calibration scan to measure thesystem response for a point source at each spatial location on the imaginggrid. However, it is challenging to run calibration scans when using structuredSLMs as they cannot encode individual grid locations. In this study, we proposea novel compressive FPA system based on online deep-learning calibration ofmultiplexed LR measurements (CalibFPA). We introduce a piezo-stage thatlocomotes a pre-printed fixed coded aperture. A deep neural network is thenleveraged to correct for the influences of system non-idealities in multiplexedmeasurements without the need for offline calibration scans. Finally, a deepplug-and-play algorithm is used to reconstruct images from correctedmeasurements. On simulated and experimental datasets, we demonstrate thatCalibFPA outperforms state-of-the-art compressive FPA methods. We also reportanalyses to validate the design elements in CalibFPA and assess computationalcomplexity.</description><author>Alper Güngör, M. Umut Bahceci, Yasin Ergen, Ahmet Sözak, O. Oner Ekiz, Tolga Yelboga, Tolga Çukur</author><pubDate>Wed, 20 Sep 2023 16:53:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11421v1</guid></item><item><title>Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models</title><link>http://arxiv.org/abs/2309.11420v1</link><description>We investigate the approximation efficiency of score functions by deep neuralnetworks in diffusion-based generative modeling. While existing approximationtheories utilize the smoothness of score functions, they suffer from the curseof dimensionality for intrinsically high-dimensional data. This limitation ispronounced in graphical models such as Markov random fields, common for imagedistributions, where the approximation efficiency of score functions remainsunestablished. To address this, we observe score functions can often be well-approximated ingraphical models through variational inference denoising algorithms.Furthermore, these algorithms are amenable to efficient neural networkrepresentation. We demonstrate this in examples of graphical models, includingIsing models, conditional Ising models, restricted Boltzmann machines, andsparse encoding models. Combined with off-the-shelf discretization error boundsfor diffusion-based sampling, we provide an efficient sample complexity boundfor diffusion-based generative modeling when the score function is learned bydeep neural networks.</description><author>Song Mei, Yuchen Wu</author><pubDate>Wed, 20 Sep 2023 16:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11420v1</guid></item><item><title>Kosmos-2.5: A Multimodal Literate Model</title><link>http://arxiv.org/abs/2309.11419v1</link><description>We present Kosmos-2.5, a multimodal literate model for machine reading oftext-intensive images. Pre-trained on large-scale text-intensive images,Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1)generating spatially-aware text blocks, where each block of text is assignedits spatial coordinates within the image, and (2) producing structured textoutput that captures styles and structures into the markdown format. Thisunified multimodal literate capability is achieved through a shared Transformerarchitecture, task-specific prompts, and flexible text representations. Weevaluate Kosmos-2.5 on end-to-end document-level text recognition andimage-to-markdown text generation. Furthermore, the model can be readilyadapted for any text-intensive image understanding task with different promptsthrough supervised fine-tuning, making it a general-purpose tool for real-worldapplications involving text-rich images. This work also paves the way for thefuture scaling of multimodal large language models.</description><author>Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei</author><pubDate>Wed, 20 Sep 2023 16:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11419v1</guid></item><item><title>CNNs for JPEGs: A Study in Computational Cost</title><link>http://arxiv.org/abs/2309.11417v1</link><description>Convolutional neural networks (CNNs) have achieved astonishing advances overthe past decade, defining state-of-the-art in several computer vision tasks.CNNs are capable of learning robust representations of the data directly fromthe RGB pixels. However, most image data are usually available in compressedformat, from which the JPEG is the most widely used due to transmission andstorage purposes demanding a preliminary decoding process that have a highcomputational load and memory usage. For this reason, deep learning methodscapable of learning directly from the compressed domain have been gainingattention in recent years. Those methods usually extract a frequency domainrepresentation of the image, like DCT, by a partial decoding, and then makeadaptation to typical CNNs architectures to work with them. One limitation ofthese current works is that, in order to accommodate the frequency domain data,the modifications made to the original model increase significantly theiramount of parameters and computational complexity. On one hand, the methodshave faster preprocessing, since the cost of fully decoding the images isavoided, but on the other hand, the cost of passing the images though the modelis increased, mitigating the possible upside of accelerating the method. Inthis paper, we propose a further study of the computational cost of deep modelsdesigned for the frequency domain, evaluating the cost of decoding and passingthe images through the network. We also propose handcrafted and data-driventechniques for reducing the computational complexity and the number ofparameters for these models in order to keep them similar to their RGBbaselines, leading to efficient models with a better trade off betweencomputational cost and accuracy.</description><author>Samuel Felipe dos Santos, Nicu Sebe, Jurandy Almeida</author><pubDate>Wed, 20 Sep 2023 16:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11417v1</guid></item><item><title>Large Language Models Understand and Can be Enhanced by Emotional Stimuli</title><link>http://arxiv.org/abs/2307.11760v4</link><description>Emotional intelligence significantly impacts our daily behaviors andinteractions. Although Large Language Models (LLMs) are increasingly viewed asa stride toward artificial general intelligence, exhibiting impressiveperformance in numerous tasks, it is still uncertain if LLMs can genuinelygrasp psychological emotional stimuli. Understanding and responding toemotional cues gives humans a distinct advantage in problem-solving. In thispaper, we take the first step towards exploring the ability of LLMs tounderstand emotional stimuli. To this end, we first conduct automaticexperiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generativeapplications that represent comprehensive evaluation scenarios. Our automaticexperiments show that LLMs have a grasp of emotional intelligence, and theirperformance can be improved with emotional prompts (which we call"EmotionPrompt" that combines the original prompt with emotional stimuli),e.g., 8.00% relative performance improvement in Instruction Induction and 115%in BIG-Bench. In addition to those deterministic tasks that can beautomatically evaluated using existing metrics, we conducted a human study with106 participants to assess the quality of generative tasks using both vanillaand emotional prompts. Our human study results demonstrate that EmotionPromptsignificantly boosts the performance of generative tasks (10.9% averageimprovement in terms of performance, truthfulness, and responsibility metrics).We provide an in-depth discussion regarding why EmotionPrompt works for LLMsand the factors that may influence its performance. We posit that EmotionPromptheralds a novel avenue for exploring interdisciplinary knowledge for human-LLMsinteraction.</description><author>Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie</author><pubDate>Wed, 20 Sep 2023 16:46:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11760v4</guid></item><item><title>Proximal methods for point source localisation</title><link>http://arxiv.org/abs/2212.02991v4</link><description>Point source localisation is generally modelled as a Lasso-type problem onmeasures. However, optimisation methods in non-Hilbert spaces, such as thespace of Radon measures, are much less developed than in Hilbert spaces. Mostnumerical algorithms for point source localisation are based on the Frank-Wolfeconditional gradient method, for which ad hoc convergence theory is developed.We develop extensions of proximal-type methods to spaces of measures. Thisincludes forward-backward splitting, its inertial version, and primal-dualproximal splitting. Their convergence proofs follow standard patterns. Wedemonstrate their numerical efficacy.</description><author>Tuomo Valkonen</author><pubDate>Wed, 20 Sep 2023 16:41:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02991v4</guid></item><item><title>EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning</title><link>http://arxiv.org/abs/2309.11414v1</link><description>Classical motion planning for robotic manipulation includes a set of generalalgorithms that aim to minimize a scene-specific cost of executing a givenplan. This approach offers remarkable adaptability, as they can be directlyused off-the-shelf for any new scene without needing specific trainingdatasets. However, without a prior understanding of what diverse validtrajectories are and without specially designed cost functions for a givenscene, the overall solutions tend to have low success rates. Whiledeep-learning-based algorithms tremendously improve success rates, they aremuch harder to adopt without specialized training datasets. We propose EDMP, anEnsemble-of-costs-guided Diffusion for Motion Planning that aims to combine thestrengths of classical and deep-learning-based motion planning. Ourdiffusion-based network is trained on a set of diverse kinematically validtrajectories. Like classical planning, for any new scene at the time ofinference, we compute scene-specific costs such as "collision cost" and guidethe diffusion to generate valid trajectories that satisfy the scene-specificconstraints. Further, instead of a single cost function that may beinsufficient in capturing diversity across scenes, we use an ensemble of coststo guide the diffusion process, significantly improving the success ratecompared to classical planners. EDMP performs comparably with SOTAdeep-learning-based methods while retaining the generalization capabilitiesprimarily associated with classical planners.</description><author>Kallol Saha, Vishal Mandadi, Jayaram Reddy, Ajit Srikanth, Aditya Agarwal, Bipasha Sen, Arun Singh, Madhava Krishna</author><pubDate>Wed, 20 Sep 2023 16:40:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11414v1</guid></item><item><title>Enhancing motion trajectory segmentation of rigid bodies using a novel screw-based trajectory-shape representation</title><link>http://arxiv.org/abs/2309.11413v1</link><description>Trajectory segmentation refers to dividing a trajectory into meaningfulconsecutive sub-trajectories. This paper focuses on trajectory segmentation for3D rigid-body motions. Most segmentation approaches in the literature representthe body's trajectory as a point trajectory, considering only its translationand neglecting its rotation. We propose a novel trajectory representation forrigid-body motions that incorporates both translation and rotation, andadditionally exhibits several invariant properties. This representationconsists of a geometric progress rate and a third-order trajectory-shapedescriptor. Concepts from screw theory were used to make this representationtime-invariant and also invariant to the choice of body reference point. Thisnew representation is validated for a self-supervised segmentation approach,both in simulation and using real recordings of human-demonstrated pouringmotions. The results show a more robust detection of consecutive submotionswith distinct features and a more consistent segmentation compared toconventional representations. We believe that other existing segmentationmethods may benefit from using this trajectory representation to improve theirinvariance.</description><author>Arno Verduyn, Maxim Vochten, Joris De Schutter</author><pubDate>Wed, 20 Sep 2023 16:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11413v1</guid></item><item><title>Transformers versus LSTMs for electronic trading</title><link>http://arxiv.org/abs/2309.11400v1</link><description>With the rapid development of artificial intelligence, long short term memory(LSTM), one kind of recurrent neural network (RNN), has been widely applied intime series prediction. Like RNN, Transformer is designed to handle the sequential data. AsTransformer achieved great success in Natural Language Processing (NLP),researchers got interested in Transformer's performance on time seriesprediction, and plenty of Transformer-based solutions on long time seriesforecasting have come out recently. However, when it comes to financial timeseries prediction, LSTM is still a dominant architecture. Therefore, thequestion this study wants to answer is: whether the Transformer-based model canbe applied in financial time series prediction and beat LSTM. To answer this question, various LSTM-based and Transformer-based models arecompared on multiple financial prediction tasks based on high-frequency limitorder book data. A new LSTM-based model called DLSTM is built and newarchitecture for the Transformer-based model is designed to adapt for financialprediction. The experiment result reflects that the Transformer-based modelonly has the limited advantage in absolute price sequence prediction. TheLSTM-based models show better and more robust performance on differencesequence prediction, such as price difference and price movement.</description><author>Paul Bilokon, Yitao Qiu</author><pubDate>Wed, 20 Sep 2023 16:25:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11400v1</guid></item><item><title>Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization</title><link>http://arxiv.org/abs/2309.04810v2</link><description>Recent research indicates that the performance of machine learning models canbe improved by aligning the geometry of the latent space with the underlyingdata structure. Rather than relying solely on Euclidean space, researchers haveproposed using hyperbolic and spherical spaces with constant curvature, orcombinations thereof, to better model the latent space and enhance modelperformance. However, little attention has been given to the problem ofautomatically identifying the optimal latent geometry for the downstream task.We mathematically define this novel formulation and coin it as neural latentgeometry search (NLGS). More specifically, we introduce a principled methodthat searches for a latent geometry composed of a product of constant curvaturemodel spaces with minimal query evaluations. To accomplish this, we propose anovel notion of distance between candidate latent geometries based on theGromov-Hausdorff distance from metric geometry. In order to compute theGromov-Hausdorff distance, we introduce a mapping function that enables thecomparison of different manifolds by embedding them in a commonhigh-dimensional ambient space. Finally, we design a graph search space basedon the calculated distances between candidate manifolds and use Bayesianoptimization to search for the optimal latent geometry in a query-efficientmanner. This is a general method which can be applied to search for the optimallatent geometry for a variety of models and downstream tasks. Extensiveexperiments on synthetic and real-world datasets confirm the efficacy of ourmethod in identifying the optimal latent geometry for multiple machine learningproblems.</description><author>Haitz Saez de Ocariz Borde, Alvaro Arroyo, Ismael Morales, Ingmar Posner, Xiaowen Dong</author><pubDate>Wed, 20 Sep 2023 16:21:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04810v2</guid></item><item><title>Safurai 001: New Qualitative Approach for Code LLM Evaluation</title><link>http://arxiv.org/abs/2309.11385v1</link><description>This paper presents Safurai-001, a new Large Language Model (LLM) withsignificant potential in the domain of coding assistance. Driven by recentadvancements in coding LLMs, Safurai-001 competes in performance with thelatest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al.,2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a moreconversational interaction. By capitalizing on the progress in data engineering(including latest techniques of data transformation and prompt engineering) andinstruction tuning, this new model promises to stand toe-to-toe with recentclosed and open source developments. Recognizing the need for an efficaciousevaluation metric for coding LLMs, this paper also introduces GPT4-basedMultiParameters, an evaluation benchmark that harnesses varied parameters topresent a comprehensive insight into the models functioning and performance.Our assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% andWizardCoder by 18.78% in the Code Readability parameter and more.</description><author>Davide Cifarelli, Leonardo Boiardi, Alessandro Puppo</author><pubDate>Wed, 20 Sep 2023 16:11:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11385v1</guid></item><item><title>Long-Form End-to-End Speech Translation via Latent Alignment Segmentation</title><link>http://arxiv.org/abs/2309.11384v1</link><description>Current simultaneous speech translation models can process audio only up to afew seconds long. Contemporary datasets provide an oracle segmentation intosentences based on human-annotated transcripts and translations. However, thesegmentation into sentences is not available in the real world. Current speechsegmentation approaches either offer poor segmentation quality or have to tradelatency for quality. In this paper, we propose a novel segmentation approachfor a low-latency end-to-end speech translation. We leverage the existingspeech translation encoder-decoder architecture with ST CTC and show that itcan perform the segmentation task without supervision or additional parameters.To the best of our knowledge, our method is the first that allows an actualend-to-end simultaneous speech translation, as the same model is used fortranslation and segmentation at the same time. On a diverse set of languagepairs and in- and out-of-domain data, we show that the proposed approachachieves state-of-the-art quality at no additional computational cost.</description><author>Peter Polák, Ondřej Bojar</author><pubDate>Wed, 20 Sep 2023 16:10:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11384v1</guid></item><item><title>Adaptive PD Control using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays</title><link>http://arxiv.org/abs/2305.16979v2</link><description>Local-remote systems allow robots to execute complex tasks in hazardousenvironments such as space and nuclear power stations. However, establishingaccurate positional mapping between local and remote devices can be difficultdue to time delays that can compromise system performance and stability.Enhancing the synchronicity and stability of local-remote systems is vital forenabling robots to interact with environments at greater distances and underhighly challenging network conditions, including time delays. We introduce anadaptive control method employing reinforcement learning to tackle thetime-delayed control problem. By adjusting controller parameters in real-time,this adaptive controller compensates for stochastic delays and improvessynchronicity between local and remote robotic manipulators. To improve theadaptive PD controller's performance, we devise a model-based reinforcementlearning approach that effectively incorporates multi-step delays into thelearning framework. Utilizing this proposed technique, the local-remotesystem's performance is stabilized for stochastic communication time-delays ofup to 290ms. Our results demonstrate that the suggested model-basedreinforcement learning method surpasses the Soft-Actor Critic and augmentedstate Soft-Actor Critic techniques. Access the code at:https://github.com/CAV-Research-Lab/Predictive-Model-Delay-Correction</description><author>Luc McCutcheon, Saber Fallah</author><pubDate>Wed, 20 Sep 2023 16:09:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16979v2</guid></item><item><title>Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees</title><link>http://arxiv.org/abs/2210.07893v2</link><description>Gaussian processes are frequently deployed as part of larger machine learningand decision-making systems, for instance in geospatial modeling, Bayesianoptimization, or in latent Gaussian models. Within a system, the Gaussianprocess model needs to perform in a stable and reliable manner to ensure itinteracts correctly with other parts of the system. In this work, we study thenumerical stability of scalable sparse approximations based on inducing points.To do so, we first review numerical stability, and illustrate typicalsituations in which Gaussian process models can be unstable. Building onstability theory originally developed in the interpolation literature, wederive sufficient and in certain cases necessary conditions on the inducingpoints for the computations performed to be numerically stable. Forlow-dimensional tasks such as geospatial modeling, we propose an automatedmethod for computing inducing points satisfying these conditions. This is donevia a modification of the cover tree data structure, which is of independentinterest. We additionally propose an alternative sparse approximation forregression with a Gaussian likelihood which trades off a small amount ofperformance to further improve stability. We provide illustrative examplesshowing the relationship between stability of calculations and predictiveperformance of inducing point methods on spatial tasks.</description><author>Alexander Terenin, David R. Burt, Artem Artemev, Seth Flaxman, Mark van der Wilk, Carl Edward Rasmussen, Hong Ge</author><pubDate>Wed, 20 Sep 2023 16:08:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07893v2</guid></item><item><title>Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</title><link>http://arxiv.org/abs/2309.11382v1</link><description>Visual language navigation (VLN) is an embodied task demanding a wide rangeof skills encompassing understanding, perception, and planning. For such amultifaceted challenge, previous VLN methods totally rely on one model's ownthinking to make predictions within one round. However, existing models, eventhe most advanced large language model GPT4, still struggle with dealing withmultiple tasks by single-round self-thinking. In this work, drawing inspirationfrom the expert consultation meeting, we introduce a novel zero-shot VLNframework. Within this framework, large models possessing distinct abilitiesare served as domain experts. Our proposed navigation agent, namely DiscussNav,can actively discuss with these experts to collect essential information beforemoving at every step. These discussions cover critical navigation subtasks likeinstruction understanding, environment perception, and completion estimation.Through comprehensive experiments, we demonstrate that discussions with domainexperts can effectively facilitate navigation by perceivinginstruction-relevant information, correcting inadvertent errors, and siftingthrough in-consistent movement decisions. The performances on therepresentative VLN task R2R show that our method surpasses the leadingzero-shot VLN model by a large margin on all metrics. Additionally, real-robotexperiments display the obvious advantages of our method over single-roundself-thinking.</description><author>Yuxing Long, Xiaoqi Li, Wenzhe Cai, Hao Dong</author><pubDate>Wed, 20 Sep 2023 16:04:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11382v1</guid></item><item><title>Studying Lobby Influence in the European Parliament</title><link>http://arxiv.org/abs/2309.11381v1</link><description>We present a method based on natural language processing (NLP), for studyingthe influence of interest groups (lobbies) in the law-making process in theEuropean Parliament (EP). We collect and analyze novel datasets of lobbies'position papers and speeches made by members of the EP (MEPs). By comparingthese texts on the basis of semantic similarity and entailment, we are able todiscover interpretable links between MEPs and lobbies. In the absence of aground-truth dataset of such links, we perform an indirect validation bycomparing the discovered links with a dataset, which we curate, of retweetlinks between MEPs and lobbies, and with the publicly disclosed meetings ofMEPs. Our best method achieves an AUC score of 0.77 and performs significantlybetter than several baselines. Moreover, an aggregate analysis of thediscovered links, between groups of related lobbies and political groups ofMEPs, correspond to the expectations from the ideology of the groups (e.g.,center-left groups are associated with social causes). We believe that thiswork, which encompasses the methodology, datasets, and results, is a steptowards enhancing the transparency of the intricate decision-making processeswithin democratic institutions.</description><author>Aswin Suresh, Lazar Radojevic, Francesco Salvi, Antoine Magron, Victor Kristof, Matthias Grossglauser</author><pubDate>Wed, 20 Sep 2023 16:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11381v1</guid></item><item><title>Multi-Head Graph Convolutional Network for Structural Connectome Classification</title><link>http://arxiv.org/abs/2305.02199v2</link><description>We tackle classification based on brain connectivity derived from diffusionmagnetic resonance images. We propose a machine-learning model inspired bygraph convolutional networks (GCNs), which takes a brain connectivity inputgraph and processes the data separately through a parallel GCN mechanism withmultiple heads. The proposed network is a simple design that employs differentheads involving graph convolutions focused on edges and nodes, capturingrepresentations from the input data thoroughly. To test the ability of ourmodel to extract complementary and representative features from brainconnectivity data, we chose the task of sex classification. This quantifies thedegree to which the connectome varies depending on the sex, which is importantfor improving our understanding of health and disease in both sexes. We showexperiments on two publicly available datasets: PREVENT-AD (347 subjects) andOASIS3 (771 subjects). The proposed model demonstrates the highest performancecompared to the existing machine-learning algorithms we tested, includingclassical methods and (graph and non-graph) deep learning. We provide adetailed analysis of each component of our model.</description><author>Anees Kazi, Jocelyn Mora, Bruce Fischl, Adrian V. Dalca, Iman Aganj</author><pubDate>Wed, 20 Sep 2023 16:03:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02199v2</guid></item><item><title>Black-box Generalization of Machine Teaching</title><link>http://arxiv.org/abs/2206.15205v2</link><description>Hypothesis-pruning maximizes the hypothesis updates for active learning tofind those desired unlabeled data. An inherent assumption is that this learningmanner can derive those updates into the optimal hypothesis. However, itsconvergence may not be guaranteed well if those incremental updates arenegative and disordered. In this paper, we introduce a black-box teachinghypothesis $h^\mathcal{T}$ employing a tighter slack term$\left(1+\mathcal{F}^{\mathcal{T}}(\widehat{h}_t)\right)\Delta_t$ to replacethe typical $2\Delta_t$ for pruning. Theoretically, we prove that, under theguidance of this teaching hypothesis, the learner can converge into a tightergeneralization error and label complexity bound than those non-educatedlearners who do not receive any guidance from a teacher:1) the generalizationerror upper bound can be reduced from $R(h^*)+4\Delta_{T-1}$ to approximately$R(h^{\mathcal{T}})+2\Delta_{T-1}$, and 2) the label complexity upper bound canbe decreased from $4 \theta\left(TR(h^{*})+2O(\sqrt{T})\right)$ toapproximately $2\theta\left(2TR(h^{\mathcal{T}})+3 O(\sqrt{T})\right)$. To bestrict with our assumption, self-improvement of teaching is firstly proposedwhen $h^\mathcal{T}$ loosely approximates $h^*$. Against learning, we furtherconsider two teaching scenarios: teaching a white-box and black-box learner.Experiments verify this idea and show better generalization performance thanthe fundamental active learning strategies, such as IWAL, IWAL-D, etc.</description><author>Xiaofeng Cao, Yaming Guo, Ivor W. Tsang, James T. Kwok</author><pubDate>Wed, 20 Sep 2023 16:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.15205v2</guid></item><item><title>Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff</title><link>http://arxiv.org/abs/2309.11379v1</link><description>Blockwise self-attentional encoder models have recently emerged as onepromising end-to-end approach to simultaneous speech translation. These modelsemploy a blockwise beam search with hypothesis reliability scoring to determinewhen to wait for more input speech before translating further. However, thismethod maintains multiple hypotheses until the entire speech input is consumed-- this scheme cannot directly show a single \textit{incremental} translationto users. Further, this method lacks mechanisms for \textit{controlling} thequality vs. latency tradeoff. We propose a modified incremental blockwise beamsearch incorporating local agreement or hold-$n$ policies for quality-latencycontrol. We apply our framework to models trained for online or offlinetranslation and demonstrate that both types can be effectively used in onlinemode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changinglatency or 0.8-1.4 s latency improvement without changing quality.</description><author>Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel, Ondřej Bojar</author><pubDate>Wed, 20 Sep 2023 15:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11379v1</guid></item><item><title>Preconditioned Federated Learning</title><link>http://arxiv.org/abs/2309.11378v1</link><description>Federated Learning (FL) is a distributed machine learning approach thatenables model training in communication efficient and privacy-preservingmanner. The standard optimization method in FL is Federated Averaging (FedAvg),which performs multiple local SGD steps between communication rounds. FedAvghas been considered to lack algorithm adaptivity compared to modern first-orderadaptive optimizations. In this paper, we propose new communication-efficientFL algortithms based on two adaptive frameworks: local adaptivity (PreFed) andserver-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using anovel covariance matrix preconditioner. Theoretically, we provide convergenceguarantees for our algorithms. The empirical experiments show our methodsachieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.</description><author>Zeyi Tao, Jindi Wu, Qun Li</author><pubDate>Wed, 20 Sep 2023 15:58:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11378v1</guid></item><item><title>Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness</title><link>http://arxiv.org/abs/2309.11373v1</link><description>Recent work in machine learning for healthcare has raised concerns aboutpatient privacy and algorithmic fairness. For example, previous work has shownthat patient self-reported race can be predicted from medical data that doesnot explicitly contain racial information. However, the extent of dataidentification is unknown, and we lack ways to develop models whose outcomesare minimally affected by such information. Here we systematically investigatedthe ability of time-series electronic health record data to predict patientstatic information. We found that not only the raw time-series data, but alsolearned representations from machine learning models, can be trained to predicta variety of static information with area under the receiver operatingcharacteristic curve as high as 0.851 for biological sex, 0.869 for binarizedage and 0.810 for self-reported race. Such high predictive performance can beextended to a wide range of comorbidity factors and exists even when the modelwas trained for different tasks, using different cohorts, using different modelarchitectures and databases. Given the privacy and fairness concerns thesefindings pose, we develop a variational autoencoder-based approach that learnsa structured latent space to disentangle patient-sensitive attributes fromtime-series data. Our work thoroughly investigates the ability of machinelearning models to encode patient static information from time-serieselectronic health records and introduces a general approach to protectpatient-sensitive attribute information for downstream tasks.</description><author>Wei Liao, Joel Voldman</author><pubDate>Wed, 20 Sep 2023 15:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11373v1</guid></item><item><title>Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition</title><link>http://arxiv.org/abs/2309.11368v1</link><description>Human-robot collaboration has benefited users with higher efficiency towardsinteractive tasks. Nevertheless, most collaborative schemes rely on complicatedhuman-machine interfaces, which might lack the requisite intuitiveness comparedwith natural limb control. We also expect to understand human intent with lowtraining data requirements. In response to these challenges, this paperintroduces an innovative human-robot collaborative framework that seamlesslyintegrates hand gesture and dynamic movement recognition, voice recognition,and a switchable control adaptation strategy. These modules provide auser-friendly approach that enables the robot to deliver the tools as per userneed, especially when the user is working with both hands. Therefore, users canfocus on their task execution without additional training in the use ofhuman-machine interfaces, while the robot interprets their intuitive gestures.The proposed multimodal interaction framework is executed in the UR5e robotplatform equipped with a RealSense D435i camera, and the effectiveness isassessed through a soldering circuit board task. The experiment results havedemonstrated superior performance in hand gesture recognition, where the statichand gesture recognition module achieves an accuracy of 94.3\%, while thedynamic motion recognition module reaches 97.6\% accuracy. Compared with humansolo manipulation, the proposed approach facilitates higher efficiency tooldelivery, without significantly distracting from human intents.</description><author>Haolin Fei, Stefano Tedeschi, Yanpei Huang, Andrew Kennedy, Ziwei Wang</author><pubDate>Wed, 20 Sep 2023 15:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11368v1</guid></item><item><title>Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG)</title><link>http://arxiv.org/abs/2309.11361v1</link><description>We present a comprehensive benchmark dataset for Knowledge Graph QuestionAnswering in Materials Science (KGQA4MAT), with a focus on metal-organicframeworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) hasbeen constructed by integrating structured databases and knowledge extractedfrom the literature. To enhance MOF-KG accessibility for domain experts, we aimto develop a natural language interface for querying the knowledge graph. Wehave developed a benchmark comprised of 161 complex questions involvingcomparison, aggregation, and complicated graph structures. Each question isrephrased in three additional variations, resulting in 644 questions and 161 KGqueries. To evaluate the benchmark, we have developed a systematic approach forutilizing ChatGPT to translate natural language questions into formal KGqueries. We also apply the approach to the well-known QALD-9 dataset,demonstrating ChatGPT's potential in addressing KGQA issues for differentplatforms and query languages. The benchmark and the proposed approach aim tostimulate further research and development of user-friendly and efficientinterfaces for querying domain-specific materials science knowledge graphs,thereby accelerating the discovery of novel materials.</description><author>Yuan An, Jane Greenberg, Alex Kalinowski, Xintong Zhao, Xiaohua Hu, Fernando J. Uribe-Romo, Kyle Langlois, Jacob Furst, Diego A. Gómez-Gualdrón</author><pubDate>Wed, 20 Sep 2023 15:43:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11361v1</guid></item><item><title>Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability</title><link>http://arxiv.org/abs/2006.04787v2</link><description>In this paper we revisit some classic problems on classification undermisspecification. In particular, we study the problem of learning halfspacesunder Massart noise with rate $\eta$. In a recent work, Diakonikolas,Goulekakis, and Tzamos resolved a long-standing problem by giving the firstefficient algorithm for learning to accuracy $\eta + \epsilon$ for any$\epsilon &gt; 0$. However, their algorithm outputs a complicated hypothesis,which partitions space into $\text{poly}(d,1/\epsilon)$ regions. Here we give amuch simpler algorithm and in the process resolve a number of outstanding openquestions: (1) We give the first proper learner for Massart halfspaces that achieves$\eta + \epsilon$. We also give improved bounds on the sample complexityachievable by polynomial time algorithms. (2) Based on (1), we develop a blackbox knowledge distillation procedure toconvert an arbitrarily complex classifier to an equally good proper classifier. (3) By leveraging a simple but overlooked connection to evolvability, we showany SQ algorithm requires super-polynomially many queries to achieve$\mathsf{OPT} + \epsilon$. Moreover we study generalized linear models where $\mathbb{E}[Y|\mathbf{X}] =\sigma(\langle \mathbf{w}^*, \mathbf{X}\rangle)$ for any odd, monotone, andLipschitz function $\sigma$. This family includes the previously mentionedhalfspace models as a special case, but is much richer and includes otherfundamental models like logistic regression. We introduce a challenging newcorruption model that generalizes Massart noise, and give a general algorithmfor learning in this setting. Our algorithms are based on a small set of corerecipes for learning to classify in the presence of misspecification. Finally we study our algorithm for learning halfspaces under Massart noiseempirically and find that it exhibits some appealing fairness properties.</description><author>Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau</author><pubDate>Wed, 20 Sep 2023 15:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2006.04787v2</guid></item><item><title>3D Face Reconstruction: the Road to Forensics</title><link>http://arxiv.org/abs/2309.11357v1</link><description>3D face reconstruction algorithms from images and videos are applied to manyfields, from plastic surgery to the entertainment sector, thanks to theiradvantageous features. However, when looking at forensic applications, 3D facereconstruction must observe strict requirements that still make its possiblerole in bringing evidence to a lawsuit unclear. An extensive investigation ofthe constraints, potential, and limits of its application in forensics is stillmissing. Shedding some light on this matter is the goal of the present survey,which starts by clarifying the relation between forensic applications andbiometrics, with a focus on face recognition. Therefore, it provides ananalysis of the achievements of 3D face reconstruction algorithms fromsurveillance videos and mugshot images and discusses the current obstacles thatseparate 3D face reconstruction from an active role in forensic applications.Finally, it examines the underlying data sets, with their advantages andlimitations, while proposing alternatives that could substitute or complementthem.</description><author>Simone Maurizio La Cava, Giulia Orrù, Martin Drahansky, Gian Luca Marcialis, Fabio Roli</author><pubDate>Wed, 20 Sep 2023 15:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11357v1</guid></item><item><title>MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction</title><link>http://arxiv.org/abs/2308.06552v2</link><description>Cross-lingual open information extraction aims to extract structuredinformation from raw text across multiple languages. Previous work uses ashared cross-lingual pre-trained model to handle the different languages butunderuses the potential of the language-specific representation. In this paper,we propose an effective multi-stage tuning framework called MT4CrossIE,designed for enhancing cross-lingual open information extraction by injectinglanguage-specific knowledge into the shared model. Specifically, thecross-lingual pre-trained model is first tuned in a shared semantic space(e.g., embedding matrix) in the fixed encoder and then other components areoptimized in the second stage. After enough training, we freeze the pre-trainedmodel and tune the multiple extra low-rank language-specific modules usingmixture-of-LoRAs for model-based cross-lingual transfer. In addition, weleverage two-stage prompting to encourage the large language model (LLM) toannotate the multi-lingual raw data for data-based cross-lingual transfer. Themodel is trained with multi-lingual objectives on our proposed datasetOpenIE4++ by combing the model-based and data-based transfer techniques.Experimental results on various benchmarks emphasize the importance ofaggregating multiple plug-in-and-play language-specific modules and demonstratethe effectiveness of MT4CrossIE in cross-lingualOIE\footnote{\url{https://github.com/CSJianYang/Multilingual-Multimodal-NLP}}.</description><author>Tongliang Li, Zixiang Wang, Linzheng Chai, Jian Yang, Jiaqi Bai, Yuwei Yin, Jiaheng Liu, Hongcheng Guo, Liqun Yang, Hebboul Zine el-abidine, Zhoujun Li</author><pubDate>Wed, 20 Sep 2023 15:37:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06552v2</guid></item><item><title>A Comprehensive Survey on Rare Event Prediction</title><link>http://arxiv.org/abs/2309.11356v1</link><description>Rare event prediction involves identifying and forecasting events with a lowprobability using machine learning and data analysis. Due to the imbalanceddata distributions, where the frequency of common events vastly outweighs thatof rare events, it requires using specialized methods within each step of themachine learning pipeline, i.e., from data processing to algorithms toevaluation protocols. Predicting the occurrences of rare events is importantfor real-world applications, such as Industry 4.0, and is an active researcharea in statistical and machine learning. This paper comprehensively reviewsthe current approaches for rare event prediction along four dimensions: rareevent data, data processing, algorithmic approaches, and evaluation approaches.Specifically, we consider 73 datasets from different modalities (i.e.,numerical, image, text, and audio), four major categories of data processing,five major algorithmic groupings, and two broader evaluation approaches. Thispaper aims to identify gaps in the current literature and highlight thechallenges of predicting rare events. It also suggests potential researchdirections, which can help guide practitioners and researchers.</description><author>Chathurangi Shyalika, Ruwan Wickramarachchi, Amit Sheth</author><pubDate>Wed, 20 Sep 2023 15:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11356v1</guid></item><item><title>Multiclass Online Learnability under Bandit Feedback</title><link>http://arxiv.org/abs/2308.04620v2</link><description>We study online multiclass classification under bandit feedback. We extendthe results of Daniely and Helbertal [2013] by showing that the finiteness ofthe Bandit Littlestone dimension is necessary and sufficient for bandit onlinemulticlass learnability even when the label space is unbounded. Moreover, weshow that, unlike the full-information setting, sequential uniform convergenceis necessary but not sufficient for bandit online learnability. Our resultcomplements the recent work by Hanneke, Moran, Raman, Subedi, and Tewari [2023]who show that the Littlestone dimension characterizes online multiclasslearnability in the full-information setting even when the label space isunbounded.</description><author>Ananth Raman, Vinod Raman, Unique Subedi, Ambuj Tewari</author><pubDate>Wed, 20 Sep 2023 15:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04620v2</guid></item><item><title>Self-supervised learning unveils change in urban housing from street-level images</title><link>http://arxiv.org/abs/2309.11354v1</link><description>Cities around the world face a critical shortage of affordable and decenthousing. Despite its critical importance for policy, our ability to effectivelymonitor and track progress in urban housing is limited. Deep learning-basedcomputer vision methods applied to street-level images have been successful inthe measurement of socioeconomic and environmental inequalities but did notfully utilize temporal images to track urban change as time-varying labels areoften unavailable. We used self-supervised methods to measure change in Londonusing 15 million street images taken between 2008 and 2021. Our noveladaptation of Barlow Twins, Street2Vec, embeds urban structure while beinginvariant to seasonal and daily changes without manual annotations. Itoutperformed generic embeddings, successfully identified point-level change inLondon's housing supply from street-level images, and distinguished betweenmajor and minor change. This capability can provide timely information forurban planning and policy decisions toward more liveable, equitable, andsustainable cities.</description><author>Steven Stalder, Michele Volpi, Nicolas Büttner, Stephen Law, Kenneth Harttgen, Esra Suel</author><pubDate>Wed, 20 Sep 2023 15:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11354v1</guid></item><item><title>C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters</title><link>http://arxiv.org/abs/2309.11351v1</link><description>We present C$\cdot$ASE, an efficient and effective framework that learnsconditional Adversarial Skill Embeddings for physics-based characters. Ourphysically simulated character can learn a diverse repertoire of skills whileproviding controllability in the form of direct manipulation of the skills tobe performed. C$\cdot$ASE divides the heterogeneous skill motions into distinctsubsets containing homogeneous samples for training a low-level conditionalmodel to learn conditional behavior distribution. The skill-conditionedimitation learning naturally offers explicit control over the character'sskills after training. The training course incorporates the focal skillsampling, skeletal residual forces, and element-wise feature masking to balancediverse skills of varying complexities, mitigate dynamics mismatch to masteragile motions and capture more general behavior characteristics, respectively.Once trained, the conditional model can produce highly diverse and realisticskills, outperforming state-of-the-art models, and can be repurposed in variousdownstream tasks. In particular, the explicit skill control handle allows ahigh-level policy or user to direct the character with desired skillspecifications, which we demonstrate is advantageous for interactive characteranimation.</description><author>Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang</author><pubDate>Wed, 20 Sep 2023 15:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11351v1</guid></item><item><title>GECTurk: Grammatical Error Correction and Detection Dataset for Turkish</title><link>http://arxiv.org/abs/2309.11346v1</link><description>Grammatical Error Detection and Correction (GEC) tools have proven useful fornative speakers and second language learners. Developing such tools requires alarge amount of parallel, annotated data, which is unavailable for mostlanguages. Synthetic data generation is a common practice to overcome thescarcity of such data. However, it is not straightforward for morphologicallyrich languages like Turkish due to complex writing rules that requirephonological, morphological, and syntactic information. In this work, wepresent a flexible and extensible synthetic data generation pipeline forTurkish covering more than 20 expert-curated grammar and spelling rules(a.k.a., writing rules) implemented through complex transformation functions.Using this pipeline, we derive 130,000 high-quality parallel sentences fromprofessionally edited articles. Additionally, we create a more realistic testset by manually annotating a set of movie reviews. We implement three baselinesformulating the task as i) neural machine translation, ii) sequence tagging,and iii) prefix tuning with a pretrained decoder-only model, achieving strongresults. Furthermore, we perform exhaustive experiments on out-of-domaindatasets to gain insights on the transferability and robustness of the proposedapproaches. Our results suggest that our corpus, GECTurk, is high-quality andallows knowledge transfer for the out-of-domain setting. To encourage furtherresearch on Turkish GEC, we release our datasets, baseline models, and thesynthetic data generation pipeline at https://github.com/GGLAB-KU/gecturk.</description><author>Atakan Kara, Farrin Marouf Sofian, Andrew Bond, Gözde Gül Şahin</author><pubDate>Wed, 20 Sep 2023 15:25:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11346v1</guid></item><item><title>Using Property Elicitation to Understand the Impacts of Fairness Constraints</title><link>http://arxiv.org/abs/2309.11343v1</link><description>Predictive algorithms are often trained by optimizing some loss function, towhich regularization functions are added to impose a penalty for violatingconstraints. As expected, the addition of such regularization functions canchange the minimizer of the objective. It is not well-understood whichregularizers change the minimizer of the loss, and, when the minimizer doeschange, how it changes. We use property elicitation to take first steps towardsunderstanding the joint relationship between the loss and regularizationfunctions and the optimal decision for a given problem instance. In particular,we give a necessary and sufficient condition on loss and regularizer pairs forwhen a property changes with the addition of the regularizer, and examine someregularizers satisfying this condition standard in the fair machine learningliterature. We empirically demonstrate how algorithmic decision-making changesas a function of both data distribution changes and hardness of theconstraints.</description><author>Jessie Finocchiaro</author><pubDate>Wed, 20 Sep 2023 15:20:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11343v1</guid></item><item><title>Improving Article Classification with Edge-Heterogeneous Graph Neural Networks</title><link>http://arxiv.org/abs/2309.11341v1</link><description>Classifying research output into context-specific label taxonomies is achallenging and relevant downstream task, given the volume of existing andnewly published articles. We propose a method to enhance the performance ofarticle classification by enriching simple Graph Neural Networks (GNN)pipelines with edge-heterogeneous graph representations. SciBERT is used fornode feature generation to capture higher-order semantics within the articles'textual metadata. Fully supervised transductive node classification experimentsare conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and thePubMed diabetes dataset, augmented with additional metadata from MicrosoftAcademic Graph (MAG) and PubMed Central, respectively. The results demonstratethat edge-heterogeneous graphs consistently improve the performance of all GNNmodels compared to the edge-homogeneous graphs. The transformed data enablesimple and shallow GNN pipelines to achieve results on par with more complexarchitectures. On ogbn-arxiv, we achieve a top-15 result in the OGB competitionwith a 2-layer GCN (accuracy 74.61%), being the highest-scoring solution withsub-1 million parameters. On PubMed, we closely trail SOTA GNN architecturesusing a 2-layer GraphSAGE by including additional co-authorship edges in thegraph (accuracy 89.88%). The implementation is available at:$\href{https://github.com/lyvykhang/edgehetero-nodeproppred}{\text{https://github.com/lyvykhang/edgehetero-nodeproppred}}$.</description><author>Khang Ly, Yury Kashnitsky, Savvas Chamezopoulos, Valeria Krzhizhanovskaya</author><pubDate>Wed, 20 Sep 2023 15:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11341v1</guid></item><item><title>Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection</title><link>http://arxiv.org/abs/2304.04688v4</link><description>The goal of spatial-temporal action detection is to determine the time andplace where each person's action occurs in a video and classify thecorresponding action category. Most of the existing methods adoptfully-supervised learning, which requires a large amount of training data,making it very difficult to achieve zero-shot learning. In this paper, wepropose to utilize a pre-trained visual-language model to extract therepresentative image and text features, and model the relationship betweenthese features through different interaction modules to obtain the interactionfeature. In addition, we use this feature to prompt each label to obtain moreappropriate text features. Finally, we calculate the similarity between theinteraction feature and the text feature for each label to determine the actioncategory. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that theproposed interaction module and prompting make the visual-language featuresbetter aligned, thus achieving excellent accuracy for zero-shot spatio-temporalaction detection. The code will be available athttps://github.com/webber2933/iCLIP.</description><author>Wei-Jhe Huang, Jheng-Hsien Yeh, Min-Hung Chen, Gueter Josmy Faure, Shang-Hong Lai</author><pubDate>Wed, 20 Sep 2023 15:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04688v4</guid></item><item><title>TRAVID: An End-to-End Video Translation Framework</title><link>http://arxiv.org/abs/2309.11338v1</link><description>In today's globalized world, effective communication with people from diverselinguistic backgrounds has become increasingly crucial. While traditionalmethods of language translation, such as written text or voice-onlytranslations, can accomplish the task, they often fail to capture the completecontext and nuanced information conveyed through nonverbal cues like facialexpressions and lip movements. In this paper, we present an end-to-end videotranslation system that not only translates spoken language but alsosynchronizes the translated speech with the lip movements of the speaker. Oursystem focuses on translating educational lectures in various Indian languages,and it is designed to be effective even in low-resource system settings. Byincorporating lip movements that align with the target language and matchingthem with the speaker's voice using voice cloning techniques, our applicationoffers an enhanced experience for students and users. This additional featurecreates a more immersive and realistic learning environment, ultimately makingthe learning process more effective and engaging.</description><author>Prottay Kumar Adhikary, Bandaru Sugandhi, Subhojit Ghimire, Santanu Pal, Partha Pakray</author><pubDate>Wed, 20 Sep 2023 15:13:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11338v1</guid></item><item><title>You can have your ensemble and run it too -- Deep Ensembles Spread Over Time</title><link>http://arxiv.org/abs/2309.11333v1</link><description>Ensembles of independently trained deep neural networks yield uncertaintyestimates that rival Bayesian networks in performance. They also offer sizableimprovements in terms of predictive performance over single models. However,deep ensembles are not commonly used in environments with limited computationalbudget -- such as autonomous driving -- since the complexity grows linearlywith the number of ensemble members. An important observation that can be madefor robotics applications, such as autonomous driving, is that data istypically sequential. For instance, when an object is to be recognized, anautonomous vehicle typically observes a sequence of images, rather than asingle image. This raises the question, could the deep ensemble be spread overtime? In this work, we propose and analyze Deep Ensembles Spread Over Time (DESOT).The idea is to apply only a single ensemble member to each data point in thesequence, and fuse the predictions over a sequence of data points. We implementand experiment with DESOT for traffic sign classification, where sequences oftracked image patches are to be classified. We find that DESOT obtains thebenefits of deep ensembles, in terms of predictive and uncertainty estimationperformance, while avoiding the added computational cost. Moreover, DESOT issimple to implement and does not require sequences during training. Finally, wefind that DESOT, like deep ensembles, outperform single models forout-of-distribution detection.</description><author>Isak Meding, Alexander Bodin, Adam Tonderski, Joakim Johnander, Christoffer Petersson, Lennart Svensson</author><pubDate>Wed, 20 Sep 2023 15:09:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11333v1</guid></item><item><title>PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning</title><link>http://arxiv.org/abs/2306.06394v2</link><description>Hierarchical reinforcement learning (HRL) has the potential to solve complexlong horizon tasks using temporal abstraction and increased exploration.However, hierarchical agents are difficult to train due to inherentnon-stationarity. We present primitive enabled adaptive relabeling (PEAR), atwo-phase approach where we first perform adaptive relabeling on a few expertdemonstrations to generate efficient subgoal supervision, and then jointlyoptimize HRL agents by employing reinforcement learning (RL) and imitationlearning (IL). We perform theoretical analysis to $(i)$ bound thesub-optimality of our approach, and $(ii)$ derive a generalized plug-and-playframework for joint optimization using RL and IL. PEAR uses a handful of expertdemonstrations and makes minimal limiting assumptions on the task structure.Additionally, it can be easily integrated with typical model free RL algorithmsto produce a practical HRL algorithm. We perform experiments on challengingrobotic environments and show that PEAR is able to solve tasks that requirelong term decision making. We empirically show that PEAR exhibits improvedperformance and sample efficiency over previous hierarchical andnon-hierarchical approaches. We also perform real world robotic experiments oncomplex tasks and demonstrate that PEAR consistently outperforms the baselines.</description><author>Utsav Singh, Vinay P Namboodiri</author><pubDate>Wed, 20 Sep 2023 15:04:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06394v2</guid></item><item><title>Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism</title><link>http://arxiv.org/abs/2309.11331v1</link><description>In the past years, YOLO-series models have emerged as the leading approachesin the area of real-time object detection. Many studies pushed up the baselineto a higher level by modifying the architecture, augmenting data and designingnew losses. However, we find previous models still suffer from informationfusion problem, although Feature Pyramid Network (FPN) and Path AggregationNetwork (PANet) have alleviated this. Therefore, this study provides anadvanced Gatherand-Distribute mechanism (GD) mechanism, which is realized withconvolution and self-attention operations. This new designed model named asGold-YOLO, which boosts the multi-scale feature fusion capabilities andachieves an ideal balance between latency and accuracy across all model scales.Additionally, we implement MAE-style pretraining in the YOLO-series for thefirst time, allowing YOLOseries models could be to benefit from unsupervisedpretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017datasets and 1030 FPS on a T4 GPU, which outperforms the previous SOTA modelYOLOv6-3.0-N with similar FPS by +2.4%. The PyTorch code is available athttps://github.com/huaweinoah/Efficient-Computing/Detection/Gold-YOLO, and theMindSpore code is available athttps://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO.</description><author>Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Kai Han, Yunhe Wang</author><pubDate>Wed, 20 Sep 2023 15:03:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11331v1</guid></item><item><title>CRISP: Curriculum inducing Primitive Informed Subgoal Prediction</title><link>http://arxiv.org/abs/2304.03535v2</link><description>Hierarchical reinforcement learning is a promising approach that usestemporal abstraction to solve complex long horizon problems. However,simultaneously learning a hierarchy of policies is unstable as it ischallenging to train higher-level policy when the lower-level primitive isnon-stationary. In this paper, we propose a novel hierarchical algorithm CRISPto generate a curriculum of achievable subgoals for evolving lower-levelprimitives using reinforcement learning and imitation learning. The lower levelprimitive periodically performs data relabeling on a handful of expertdemonstrations using our primitive informed parsing approach to handlenon-stationarity. Since our approach uses a handful of expert demonstrations,it is suitable for most robotic control tasks. Experimental evaluations oncomplex robotic maze navigation and robotic manipulation environments show thatinducing hierarchical curriculum learning significantly improves sampleefficiency, and results in efficient goal conditioned policies for solvingtemporally extended tasks. We perform real world robotic experiments on complexmanipulation tasks and demonstrate that CRISP consistently outperforms thebaselines.</description><author>Utsav Singh, Vinay P Namboodiri</author><pubDate>Wed, 20 Sep 2023 14:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03535v2</guid></item><item><title>Detecting Objects with Graph Priors and Graph Refinement</title><link>http://arxiv.org/abs/2212.12395v2</link><description>The goal of this paper is to detect objects by exploiting theirinterrelationships. Rather than relying on predefined and labeled graphstructures, we infer a graph prior from object co-occurrence statistics. Thekey idea of our paper is to model object relations as a function of initialclass predictions and co-occurrence priors to generate a graph representationof an image for improved classification and bounding box regression. Weadditionally learn the object-relation joint distribution via energy basedmodeling. Sampling from this distribution generates a refined graphrepresentation of the image which in turn produces improved detectionperformance. Experiments on the Visual Genome and MS-COCO datasets demonstrateour method is detector agnostic, end-to-end trainable, and especiallybeneficial for rare object classes. What is more, we establish a consistentimprovement over object detectors like DETR and Faster-RCNN, as well asstate-of-the-art methods modeling object interrelationships.</description><author>Aritra Bhowmik, Martin R. Oswald, Yu Wang, Nora Baka, Cees G. M. Snoek</author><pubDate>Wed, 20 Sep 2023 14:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12395v2</guid></item><item><title>Towards Disentangling Information Paths with Coded ResNeXt</title><link>http://arxiv.org/abs/2202.05343v2</link><description>The conventional, widely used treatment of deep learning models as blackboxes provides limited or no insights into the mechanisms that guide neuralnetwork decisions. Significant research effort has been dedicated to buildinginterpretable models to address this issue. Most efforts either focus on thehigh-level features associated with the last layers, or attempt to interpretthe output of a single layer. In this paper, we take a novel approach toenhance the transparency of the function of the whole network. We propose aneural network architecture for classification, in which the information thatis relevant to each class flows through specific paths. These paths aredesigned in advance before training leveraging coding theory and withoutdepending on the semantic similarities between classes. A key property is thateach path can be used as an autonomous single-purpose model. This enables us toobtain, without any additional training and for any class, a lightweight binaryclassifier that has at least $60\%$ fewer parameters than the original network.Furthermore, our coding theory based approach allows the neural network to makeearly predictions at intermediate layers during inference, without requiringits full evaluation. Remarkably, the proposed architecture provides all theaforementioned properties while improving the overall accuracy. We demonstratethese properties on a slightly modified ResNeXt model tested on CIFAR-10/100and ImageNet-1k.</description><author>Apostolos Avranas, Marios Kountouris</author><pubDate>Wed, 20 Sep 2023 14:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05343v2</guid></item><item><title>Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition</title><link>http://arxiv.org/abs/2309.11327v1</link><description>Crafting an effective Automatic Speech Recognition (ASR) solution fordialects demands innovative approaches that not only address the data scarcityissue but also navigate the intricacies of linguistic diversity. In this paper,we address the aforementioned ASR challenge, focusing on the Tunisian dialect.First, textual and audio data is collected and in some cases annotated. Second,we explore self-supervision, semi-supervision and few-shot code-switchingapproaches to push the state-of-the-art on different Tunisian test sets;covering different acoustic, linguistic and prosodic conditions. Finally, andgiven the absence of conventional spelling, we produce a human evaluation ofour transcripts to avoid the noise coming from spelling inadequacies in ourtesting references. Our models, allowing to transcribe audio samples in alinguistic mix involving Tunisian Arabic, English and French, and all the dataused during training and testing are released for public use and furtherimprovements.</description><author>Ahmed Amine Ben Abdallah, Ata Kabboudi, Amir Kanoun, Salah Zaiem</author><pubDate>Wed, 20 Sep 2023 14:56:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11327v1</guid></item><item><title>How to turn your camera into a perfect pinhole model</title><link>http://arxiv.org/abs/2309.11326v1</link><description>Camera calibration is a first and fundamental step in various computer visionapplications. Despite being an active field of research, Zhang's method remainswidely used for camera calibration due to its implementation in populartoolboxes. However, this method initially assumes a pinhole model withoversimplified distortion models. In this work, we propose a novel approachthat involves a pre-processing step to remove distortions from images by meansof Gaussian processes. Our method does not need to assume any distortion modeland can be applied to severely warped images, even in the case of multipledistortion sources, e.g., a fisheye image of a curved mirror reflection. TheGaussian processes capture all distortions and camera imperfections, resultingin virtual images as though taken by an ideal pinhole camera with squarepixels. Furthermore, this ideal GP-camera only needs one image of a square gridcalibration pattern. This model allows for a serious upgrade of many algorithmsand applications that are designed in a pure projective geometry setting butwith a performance that is very sensitive to nonlinear lens distortions. Wedemonstrate the effectiveness of our method by simplifying Zhang's calibrationmethod, reducing the number of parameters and getting rid of the distortionparameters and iterative optimization. We validate by means of synthetic dataand real world images. The contributions of this work include the constructionof a virtual ideal pinhole camera using Gaussian processes, a simplifiedcalibration method and lens distortion removal.</description><author>Ivan De Boi, Stuti Pathak, Marina Oliveira, Rudi Penne</author><pubDate>Wed, 20 Sep 2023 14:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11326v1</guid></item><item><title>Contrastive Initial State Buffer for Reinforcement Learning</title><link>http://arxiv.org/abs/2309.09752v2</link><description>In Reinforcement Learning, the trade-off between exploration and exploitationposes a complex challenge for achieving efficient learning from limitedsamples. While recent works have been effective in leveraging past experiencesfor policy updates, they often overlook the potential of reusing pastexperiences for data collection. Independent of the underlying RL algorithm, weintroduce the concept of a Contrastive Initial State Buffer, whichstrategically selects states from past experiences and uses them to initializethe agent in the environment in order to guide it toward more informativestates. We validate our approach on two complex robotic tasks without relyingon any prior information about the environment: (i) locomotion of a quadrupedrobot traversing challenging terrains and (ii) a quadcopter drone racingthrough a track. The experimental results show that our initial state bufferachieves higher task performance than the nominal baseline while also speedingup training convergence.</description><author>Nico Messikommer, Yunlong Song, Davide Scaramuzza</author><pubDate>Wed, 20 Sep 2023 14:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09752v2</guid></item><item><title>Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models</title><link>http://arxiv.org/abs/2308.06534v3</link><description>Deep learning in medical imaging has the potential to minimize the risk ofdiagnostic errors, reduce radiologist workload, and accelerate diagnosis.Training such deep learning models requires large and accurate datasets, withannotations for all training samples. However, in the medical imaging domain,annotated datasets for specific tasks are often small due to the highcomplexity of annotations, limited access, or the rarity of diseases. Toaddress this challenge, deep learning models can be pre-trained on large imagedatasets without annotations using methods from the field of self-supervisedlearning. After pre-training, small annotated datasets are sufficient tofine-tune the models for a specific task. The most popular self-supervisedpre-training approaches in medical imaging are based on contrastive learning.However, recent studies in natural image processing indicate a strong potentialfor masked autoencoder approaches. Our work compares state-of-the-artcontrastive learning methods with the recently introduced masked autoencoderapproach "SparK" for convolutional neural networks (CNNs) on medical images.Therefore we pre-train on a large unannotated CT image dataset and fine-tune onseveral CT classification tasks. Due to the challenge of obtaining sufficientannotated training data in medical imaging, it is of particular interest toevaluate how the self-supervised pre-training methods perform when fine-tuningon small datasets. By experimenting with gradually reducing the trainingdataset size for fine-tuning, we find that the reduction has different effectsdepending on the type of pre-training chosen. The SparK pre-training method ismore robust to the training dataset size than the contrastive methods. Based onour results, we propose the SparK pre-training for medical imaging tasks withonly small annotated datasets.</description><author>Daniel Wolf, Tristan Payer, Catharina Silvia Lisson, Christoph Gerhard Lisson, Meinrad Beer, Michael Götz, Timo Ropinski</author><pubDate>Wed, 20 Sep 2023 14:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06534v3</guid></item><item><title>DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services</title><link>http://arxiv.org/abs/2309.11325v1</link><description>We propose DISC-LawLLM, an intelligent legal system utilizing large languagemodels (LLMs) to provide a wide range of legal services. We adopt legalsyllogism prompting strategies to construct supervised fine-tuning datasets inthe Chinese Judicial domain and fine-tune LLMs with legal reasoning capability.We augment LLMs with a retrieval module to enhance models' ability to accessand utilize external legal knowledge. A comprehensive legal benchmark,DISC-Law-Eval, is presented to evaluate intelligent legal systems from bothobjective and subjective dimensions. Quantitative and qualitative results onDISC-Law-Eval demonstrate the effectiveness of our system in serving varioususers across diverse legal scenarios. The detailed resources are available athttps://github.com/FudanDISC/DISC-LawLLM.</description><author>Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, Xuanjing Huang, Zhongyu Wei</author><pubDate>Wed, 20 Sep 2023 14:50:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11325v1</guid></item><item><title>Face Aging via Diffusion-based Editing</title><link>http://arxiv.org/abs/2309.11321v1</link><description>In this paper, we address the problem of face aging: generating past orfuture facial images by incorporating age-related changes to the given face.Previous aging methods rely solely on human facial image datasets and are thusconstrained by their inherent scale and bias. This restricts their applicationto a limited generatable age range and the inability to handle large age gaps.We propose FADING, a novel approach to address Face Aging via DIffusion-basededitiNG. We go beyond existing methods by leveraging the rich prior oflarge-scale language-image diffusion models. First, we specialize a pre-traineddiffusion model for the task of face age editing by using an age-awarefine-tuning scheme. Next, we invert the input image to latent noise and obtainoptimized null text embeddings. Finally, we perform text-guided local ageediting via attention control. The quantitative and qualitative analysesdemonstrate that our method outperforms existing approaches with respect toaging accuracy, attribute preservation, and aging quality.</description><author>Xiangyi Chen, Stéphane Lathuilière</author><pubDate>Wed, 20 Sep 2023 14:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11321v1</guid></item><item><title>WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting</title><link>http://arxiv.org/abs/2309.11319v1</link><description>Recent CNN and Transformer-based models tried to utilize frequency andperiodicity information for long-term time series forecasting. However, mostexisting work is based on Fourier transform, which cannot capture fine-grainedand local frequency structure. In this paper, we propose a Wavelet-FourierTransform Network (WFTNet) for long-term time series forecasting. WFTNetutilizes both Fourier and wavelet transforms to extract comprehensivetemporal-frequency information from the signal, where Fourier transformcaptures the global periodic patterns and wavelet transform captures the localones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) toadaptively balance the importance of global and local frequency patterns.Extensive experiments on various time series datasets show that WFTNetconsistently outperforms other state-of-the-art baseline.</description><author>Peiyuan Liu, Beiliang Wu, Naiqi Li, Tao Dai, Fengmao Lei, Jigang Bao, Yong Jiang, Shu-Tao Xia</author><pubDate>Wed, 20 Sep 2023 14:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11319v1</guid></item><item><title>Uncovering the effects of model initialization on deep model generalization: A study with adult and pediatric Chest X-ray images</title><link>http://arxiv.org/abs/2309.11318v1</link><description>Model initialization techniques are vital for improving the performance andreliability of deep learning models in medical computer vision applications.While much literature exists on non-medical images, the impacts on medicalimages, particularly chest X-rays (CXRs) are less understood. Addressing thisgap, our study explores three deep model initialization techniques: Cold-start,Warm-start, and Shrink and Perturb start, focusing on adult and pediatricpopulations. We specifically focus on scenarios with periodically arriving datafor training, thereby embracing the real-world scenarios of ongoing data influxand the need for model updates. We evaluate these models for generalizabilityagainst external adult and pediatric CXR datasets. We also propose novelensemble methods: F-score-weighted Sequential Least-Squares QuadraticProgramming (F-SLSQP) and Attention-Guided Ensembles with Learnable FuzzySoftmax to aggregate weight parameters from multiple models to capitalize ontheir collective knowledge and complementary representations. We performstatistical significance tests with 95% confidence intervals and p-values toanalyze model performance. Our evaluations indicate models initialized withImageNet-pre-trained weights demonstrate superior generalizability overrandomly initialized counterparts, contradicting some findings for non-medicalimages. Notably, ImageNet-pretrained models exhibit consistent performanceduring internal and external testing across different training scenarios.Weight-level ensembles of these models show significantly higher recall(p&lt;0.05) during testing compared to individual models. Thus, our studyaccentuates the benefits of ImageNet-pretrained weight initialization,especially when used with weight-level ensembles, for creating robust andgeneralizable deep learning solutions.</description><author>Sivaramakrishnan Rajaraman, Ghada Zamzmi, Feng Yang, Zhaohui Liang, Zhiyun Xue, Sameer Antani</author><pubDate>Wed, 20 Sep 2023 14:42:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11318v1</guid></item><item><title>Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory</title><link>http://arxiv.org/abs/2309.11316v1</link><description>The competitive nature of Cloud marketplaces as new concerns in delivery ofservices makes the pricing policies a crucial task for firms. so that, pricingstrategies has recently attracted many researchers. Since game theory canhandle such competing well this concern is addressed by designing a normal formgame between providers in current research. A committee is considered in whichproviders register for improving their competition based pricing policies. Thefunctionality of game theory is applied to design dynamic pricing policies. Theusage of the committee makes the game a complete information one, in which eachplayer is aware of every others payoff functions. The players enhance theirpricing policies to maximize their profits. The contribution of this paper isthe quantitative modeling of Cloud marketplaces in form of a game to providenovel dynamic pricing strategies; the model is validated by proving theexistence and the uniqueness of Nash equilibrium of the game.</description><author>Safiye Ghasemi, Mohammad Reza Meybodi, Mehdi Dehghan Takht-Fooladi, Amir Masoud Rahmani</author><pubDate>Wed, 20 Sep 2023 14:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11316v1</guid></item><item><title>A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques</title><link>http://arxiv.org/abs/2309.11312v1</link><description>Cloud computing as a fairly new commercial paradigm, widely investigated bydifferent researchers, already has a great range of challenges. Pricing is amajor problem in Cloud computing marketplace; as providers are competing toattract more customers without knowing the pricing policies of each other. Toovercome this lack of knowledge, we model their competition by anincomplete-information game. Considering the issue, this work proposes apricing policy related to the regret minimization algorithm and applies it tothe considered incomplete-information game. Based on the competition basedmarketplace of the Cloud, providers update the distribution of their strategiesusing the experienced regret. The idea of iteratively applying the algorithmfor updating probabilities of strategies causes the regret get minimizedfaster. The experimental results show much more increase in profits of theproviders in comparison with other pricing policies. Besides, the efficiency ofa variety of regret minimization techniques in a simulated marketplace of Cloudare discussed which have not been observed in the studied literature. Moreover,return on investment of providers in considered organizations is studied andpromising results appeared.</description><author>S. Ghasemi, M. R. Meybodi, M. Dehghan, A. M. Rahmani</author><pubDate>Wed, 20 Sep 2023 14:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11312v1</guid></item><item><title>Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features</title><link>http://arxiv.org/abs/2309.11307v1</link><description>Predicting the success of Conversational Task Assistants (CTA) can becritical to understand user behavior and act accordingly. In this paper, wepropose TB-Rater, a Transformer model which combines conversational-flowfeatures with user behavior features for predicting user ratings in a CTAscenario. In particular, we use real human-agent conversations and ratingscollected in the Alexa TaskBot challenge, a novel multimodal and multi-turnconversational context. Our results show the advantages of modeling both theconversational-flow and behavioral aspects of the conversation in a singlemodel for offline rating prediction. Additionally, an analysis of theCTA-specific behavioral features brings insights into this setting and can beused to bootstrap future systems.</description><author>Rafael Ferreira, David Semedo, João Magalhães</author><pubDate>Wed, 20 Sep 2023 14:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11307v1</guid></item><item><title>FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion</title><link>http://arxiv.org/abs/2309.11306v1</link><description>Speech-driven 3D facial animation synthesis has been a challenging task bothin industry and research. Recent methods mostly focus on deterministic deeplearning methods meaning that given a speech input, the output is always thesame. However, in reality, the non-verbal facial cues that reside throughoutthe face are non-deterministic in nature. In addition, majority of theapproaches focus on 3D vertex based datasets and methods that are compatiblewith existing facial animation pipelines with rigged characters is scarce. Toeliminate these issues, we present FaceDiffuser, a non-deterministic deeplearning model to generate speech-driven facial animations that is trained withboth 3D vertex and blendshape based datasets. Our method is based on thediffusion technique and uses the pre-trained large speech representation modelHuBERT to encode the audio input. To the best of our knowledge, we are thefirst to employ the diffusion method for the task of speech-driven 3D facialanimation synthesis. We have run extensive objective and subjective analysesand show that our approach achieves better or comparable results in comparisonto the state-of-the-art methods. We also introduce a new in-house dataset thatis based on a blendshape based rigged character. We recommend watching theaccompanying supplementary video. The code and the dataset will be publiclyavailable.</description><author>Stefan Stan, Kazi Injamamul Haque, Zerrin Yumak</author><pubDate>Wed, 20 Sep 2023 14:33:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11306v1</guid></item><item><title>Inferring effective couplings with Restricted Boltzmann Machines</title><link>http://arxiv.org/abs/2309.02292v2</link><description>Generative models offer a direct way to model complex data. Among them,energy-based models provide us with a neural network model that aims toaccurately reproduce all statistical correlations observed in the data at thelevel of the Boltzmann weight of the model. However, one challenge is tounderstand the physical interpretation of such models. In this study, wepropose a simple solution by implementing a direct mapping between the energyfunction of the Restricted Boltzmann Machine and an effective Ising spinHamiltonian that includes high-order interactions between spins. This mappingincludes interactions of all possible orders, going beyond the conventionalpairwise interactions typically considered in the inverse Ising approach, andallowing the description of complex datasets. Earlier works attempted toachieve this goal, but the proposed mappings did not do properly treat thecomplexity of the problem or did not contain direct prescriptions for practicalapplication. To validate our method, we performed several controlled numericalexperiments where we trained the RBMs using equilibrium samples of predefinedmodels containing local external fields, two-body and three-body interactionsin various low-dimensional topologies. The results demonstrate theeffectiveness of our proposed approach in learning the correct interactionnetwork and pave the way for its application in modeling interesting datasets.We also evaluate the quality of the inferred model based on different trainingmethods.</description><author>Aurélien Decelle, Cyril Furtlehner, Alfonso De Jesus Navas Gómez, Beatriz Seoane</author><pubDate>Wed, 20 Sep 2023 14:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02292v2</guid></item><item><title>Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning</title><link>http://arxiv.org/abs/2309.11305v1</link><description>Catastrophic forgetting remains a critical challenge in the field ofcontinual learning, where neural networks struggle to retain prior knowledgewhile assimilating new information. Most existing studies emphasize mitigatingthis issue only when encountering new tasks, overlooking the significance ofthe pre-task phase. Therefore, we shift the attention to the current tasklearning stage, presenting a novel framework, C&amp;F (Create and Find Flatness),which builds a flat training space for each task in advance. Specifically,during the learning of the current task, our framework adaptively creates aflat region around the minimum in the loss landscape. Subsequently, it findsthe parameters' importance to the current task based on their flatness degrees.When adapting the model to a new task, constraints are applied according to theflatness and a flat space is simultaneously prepared for the impending task. Wetheoretically demonstrate the consistency between the created and foundflatness. In this manner, our framework not only accommodates ample parameterspace for learning new tasks but also preserves the preceding knowledge ofearlier tasks. Experimental results exhibit C&amp;F's state-of-the-art performanceas a standalone continual learning approach and its efficacy as a frameworkincorporating other methods. Our work is available athttps://github.com/Eric8932/Create-and-Find-Flatness.</description><author>Wenhang Shi, Yiren Chen, Zhe Zhao, Wei Lu, Kimmo Yan, Xiaoyong Du</author><pubDate>Wed, 20 Sep 2023 14:30:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11305v1</guid></item><item><title>Generalizing Across Domains in Diabetic Retinopathy via Variational Autoencoders</title><link>http://arxiv.org/abs/2309.11301v1</link><description>Domain generalization for Diabetic Retinopathy (DR) classification allows amodel to adeptly classify retinal images from previously unseen domains withvarious imaging conditions and patient demographics, thereby enhancing itsapplicability in a wide range of clinical environments. In this study, weexplore the inherent capacity of variational autoencoders to disentangle thelatent space of fundus images, with an aim to obtain a more robust andadaptable domain-invariant representation that effectively tackles the domainshift encountered in DR datasets. Despite the simplicity of our approach, weexplore the efficacy of this classical method and demonstrate its ability tooutperform contemporary state-of-the-art approaches for this task usingpublicly available datasets. Our findings challenge the prevailing assumptionthat highly sophisticated methods for DR classification are inherently superiorfor domain generalization. This highlights the importance of considering simplemethods and adapting them to the challenging task of generalizing medicalimages, rather than solely relying on advanced techniques.</description><author>Sharon Chokuwa, Muhammad H. Khan</author><pubDate>Wed, 20 Sep 2023 14:29:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11301v1</guid></item><item><title>A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing</title><link>http://arxiv.org/abs/2309.11299v1</link><description>Due to the recent wide use of computational resources in cloud computing, newresource provisioning challenges have been emerged. Resource provisioningtechniques must keep total costs to a minimum while meeting the requirements ofthe requests. According to widely usage of cloud services, it seems morechallenging to develop effective schemes for provisioning servicescost-effectively; we have proposed a novel learning based resource provisioningapproach that achieves cost-reduction guarantees of demands. The contributionsof our optimized resource provisioning (ORP) approach are as follows. Firstly,it is designed to provide a cost-effective method to efficiently handle theprovisioning of requested applications; while most of the existing models allowonly workflows in general which cares about the dependencies of the tasks, ORPperforms based on services of which applications comprised and cares abouttheir efficient provisioning totally. Secondly, it is a learning automata-basedapproach which selects the most proper resources for hosting each service ofthe demanded application; our approach considers both cost and servicerequirements together for deploying applications. Thirdly, a comprehensiveevaluation is performed for three typical workloads: data-intensive,process-intensive and normal applications. The experimental results show thatour method adapts most of the requirements efficiently, and furthermore theresulting performance meets our design goals.</description><author>Safiye Ghasemi, Mohammad Reza Meybodi, Mehdi Dehghan Takht Fooladi, Amir Masoud Rahmani</author><pubDate>Wed, 20 Sep 2023 14:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11299v1</guid></item><item><title>Neural Superstatistics for Bayesian Estimation of Dynamic Cognitive Models</title><link>http://arxiv.org/abs/2211.13165v4</link><description>Mathematical models of cognition are often memoryless and ignore potentialfluctuations of their parameters. However, human cognition is inherentlydynamic. Thus, we propose to augment mechanistic cognitive models with atemporal dimension and estimate the resulting dynamics from a superstatisticsperspective. Such a model entails a hierarchy between a low-level observationmodel and a high-level transition model. The observation model describes thelocal behavior of a system, and the transition model specifies how theparameters of the observation model evolve over time. To overcome theestimation challenges resulting from the complexity of superstatistical models,we develop and validate a simulation-based deep learning method for Bayesianinference, which can recover both time-varying and time-invariant parameters.We first benchmark our method against two existing frameworks capable ofestimating time-varying parameters. We then apply our method to fit a dynamicversion of the diffusion decision model to long time series of human responsetimes data. Our results show that the deep learning approach is very efficientin capturing the temporal dynamics of the model. Furthermore, we show that theerroneous assumption of static or homogeneous parameters will hide importanttemporal information.</description><author>Lukas Schumacher, Paul-Christian Bürkner, Andreas Voss, Ullrich Köthe, Stefan T. Radev</author><pubDate>Wed, 20 Sep 2023 14:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13165v4</guid></item><item><title>The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation</title><link>http://arxiv.org/abs/2305.04718v3</link><description>In policy learning for robotic manipulation, sample efficiency is ofparamount importance. Thus, learning and extracting more compactrepresentations from camera observations is a promising avenue. However,current methods often assume full observability of the scene and struggle withscale invariance. In many tasks and settings, this assumption does not hold asobjects in the scene are often occluded or lie outside the field of view of thecamera, rendering the camera observation ambiguous with regard to theirlocation. To tackle this problem, we present BASK, a Bayesian approach totracking scale-invariant keypoints over time. Our approach successfullyresolves inherent ambiguities in images, enabling keypoint tracking onsymmetrical objects and occluded and out-of-view objects. We employ our methodto learn challenging multi-object robot manipulation tasks from wrist cameraobservations and demonstrate superior utility for policy learning compared toother representation learning techniques. Furthermore, we show outstandingrobustness towards disturbances such as clutter, occlusions, and noisy depthmeasurements, as well as generalization to unseen objects both in simulationand real-world robotic experiments.</description><author>Jan Ole von Hartz, Eugenio Chisari, Tim Welschehold, Wolfram Burgard, Joschka Boedecker, Abhinav Valada</author><pubDate>Wed, 20 Sep 2023 14:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04718v3</guid></item><item><title>CPLLM: Clinical Prediction with Large Language Models</title><link>http://arxiv.org/abs/2309.11295v1</link><description>We present Clinical Prediction with Large Language Models (CPLLM), a methodthat involves fine-tuning a pre-trained Large Language Model (LLM) for clinicaldisease prediction. We utilized quantization and fine-tuned the LLM usingprompts, with the task of predicting whether patients will be diagnosed with atarget disease during their next visit or in the subsequent diagnosis,leveraging their historical diagnosis records. We compared our results versusvarious baselines, including Logistic Regression, RETAIN, and Med-BERT, whichis the current state-of-the-art model for disease prediction using structuredEHR data. Our experiments have shown that CPLLM surpasses all the tested modelsin terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancementscompared to the baseline models.</description><author>Ofir Ben Shoham, Nadav Rappoport</author><pubDate>Wed, 20 Sep 2023 14:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11295v1</guid></item><item><title>Beyond Accuracy: Measuring Representation Capacity of Embeddings to Preserve Structural and Contextual Information</title><link>http://arxiv.org/abs/2309.11294v1</link><description>Effective representation of data is crucial in various machine learningtasks, as it captures the underlying structure and context of the data.Embeddings have emerged as a powerful technique for data representation, butevaluating their quality and capacity to preserve structural and contextualinformation remains a challenge. In this paper, we address this need byproposing a method to measure the \textit{representation capacity} ofembeddings. The motivation behind this work stems from the importance ofunderstanding the strengths and limitations of embeddings, enabling researchersand practitioners to make informed decisions in selecting appropriate embeddingmodels for their specific applications. By combining extrinsic evaluationmethods, such as classification and clustering, with t-SNE-based neighborhoodanalysis, such as neighborhood agreement and trustworthiness, we provide acomprehensive assessment of the representation capacity. Additionally, the useof optimization techniques (bayesian optimization) for weight optimization (forclassification, clustering, neighborhood agreement, and trustworthiness)ensures an objective and data-driven approach in selecting the optimalcombination of metrics. The proposed method not only contributes to advancingthe field of embedding evaluation but also empowers researchers andpractitioners with a quantitative measure to assess the effectiveness ofembeddings in capturing structural and contextual information. For theevaluation, we use $3$ real-world biological sequence (proteins and nucleotide)datasets and performed representation capacity analysis of $4$ embeddingmethods from the literature, namely Spike2Vec, Spaced $k$-mers, PWM2Vec, andAutoEncoder.</description><author>Sarwan Ali</author><pubDate>Wed, 20 Sep 2023 14:21:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11294v1</guid></item><item><title>Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains</title><link>http://arxiv.org/abs/2309.11285v1</link><description>This paper presents the overview of the AuTexTification shared task as partof the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within theframework of the SEPLN 2023 conference. AuTexTification consists of twosubtasks: for Subtask 1, participants had to determine whether a text ishuman-authored or has been generated by a large language model. For Subtask 2,participants had to attribute a machine-generated text to one of six differenttext generation models. Our AuTexTification 2023 dataset contains more than160.000 texts across two languages (English and Spanish) and five domains(tweets, reviews, news, legal, and how-to articles). A total of 114 teamssigned up to participate, of which 36 sent 175 runs, and 20 of them sent theirworking notes. In this overview, we present the AuTexTification dataset andtask, the submitted participating systems, and the results.</description><author>Areg Mikael Sarvazyan, José Ángel González, Marc Franco-Salvador, Francisco Rangel, Berta Chulvi, Paolo Rosso</author><pubDate>Wed, 20 Sep 2023 14:10:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11285v1</guid></item><item><title>Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting</title><link>http://arxiv.org/abs/2309.11284v1</link><description>With the acceleration of urbanization, traffic forecasting has become anessential role in smart city construction. In the context of spatio-temporalprediction, the key lies in how to model the dependencies of sensors. However,existing works basically only consider the micro relationships between sensors,where the sensors are treated equally, and their macroscopic dependencies areneglected. In this paper, we argue to rethink the sensor's dependency modelingfrom two hierarchies: regional and global perspectives. Particularly, we mergeoriginal sensors with high intra-region correlation as a region node topreserve the inter-region dependency. Then, we generate representative andcommon spatio-temporal patterns as global nodes to reflect a global dependencybetween sensors and provide auxiliary information for spatio-temporaldependency learning. In pursuit of the generality and reality of noderepresentations, we incorporate a Meta GCN to calibrate the regional and globalnodes in the physical data space. Furthermore, we devise the cross-hierarchygraph convolution to propagate information from different hierarchies. In anutshell, we propose a Hierarchical Information Enhanced Spatio-Temporalprediction method, HIEST, to create and utilize the regional dependency andcommon spatio-temporal patterns. Extensive experiments have verified theleading performance of our HIEST against state-of-the-art baselines. Wepublicize the code to ease reproducibility.</description><author>Qian Ma, Zijian Zhang, Xiangyu Zhao, Haoliang Li, Hongwei Zhao, Yiqi Wang, Zitao Liu, Wanyu Wang</author><pubDate>Wed, 20 Sep 2023 14:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11284v1</guid></item><item><title>The Wizard of Curiosities: Enriching Dialogues with Fun Facts</title><link>http://arxiv.org/abs/2309.11283v1</link><description>Introducing curiosities in a conversation is a way to teach something new tothe person in a pleasant and enjoyable way. Enriching dialogues withcontextualized curiosities can improve the users' perception of a dialog systemand their overall user experience. In this paper, we introduce a set of curatedcuriosities, targeting dialogues in the cooking and DIY domains. In particular,we use real human-agent conversations collected in the context of the AmazonAlexa TaskBot challenge, a multimodal and multi-turn conversational setting.According to an A/B test with over 1000 conversations, curiosities not onlyincrease user engagement, but provide an average relative rating improvement of9.7%.</description><author>Frederico Vicente, Rafael Ferreira, David Semedo, João Magalhães</author><pubDate>Wed, 20 Sep 2023 14:07:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11283v1</guid></item></channel></rss>