<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 30 May 2023 06:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>BITE: Textual Backdoor Attacks with Iterative Trigger Injection</title><link>http://arxiv.org/abs/2205.12700v3</link><description>Backdoor attacks have become an emerging threat to NLP systems. By providingpoisoned training data, the adversary can embed a "backdoor" into the victimmodel, which allows input instances satisfying certain textual patterns (e.g.,containing a keyword) to be predicted as a target label of the adversary'schoice. In this paper, we demonstrate that it is possible to design a backdoorattack that is both stealthy (i.e., hard to notice) and effective (i.e., has ahigh attack success rate). We propose BITE, a backdoor attack that poisons thetraining data to establish strong correlations between the target label and aset of "trigger words". These trigger words are iteratively identified andinjected into the target-label instances through natural word-levelperturbations. The poisoned training data instruct the victim model to predictthe target label on inputs containing trigger words, forming the backdoor.Experiments on four text classification datasets show that our proposed attackis significantly more effective than baseline methods while maintaining decentstealthiness, raising alarm on the usage of untrusted training data. We furtherpropose a defense method named DeBITE based on potential trigger word removal,which outperforms existing methods in defending against BITE and generalizeswell to handling other backdoor attacks.</description><author>Jun Yan, Vansh Gupta, Xiang Ren</author><pubDate>Mon, 29 May 2023 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.12700v3</guid></item><item><title>RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</title><link>http://arxiv.org/abs/2305.18295v1</link><description>Text-to-image generation has recently witnessed remarkable achievements. Weintroduce a text-conditional image diffusion model, termed RAPHAEL, to generatehighly artistic images, which accurately portray the text prompts, encompassingmultiple nouns, adjectives, and verbs. This is achieved by stacking tens ofmixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enablingbillions of diffusion paths (routes) from the network input to the output. Eachpath intuitively functions as a "painter" for depicting a particular textualconcept onto a specified image region at a diffusion timestep. Comprehensiveexperiments reveal that RAPHAEL outperforms recent cutting-edge models, such asStable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of bothimage quality and aesthetic appeal. Firstly, RAPHAEL exhibits superiorperformance in switching images across diverse styles, such as Japanese comics,realism, cyberpunk, and ink illustration. Secondly, a single model with threebillion parameters, trained on 1,000 A100 GPUs for two months, achieves astate-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,RAPHAEL significantly surpasses its counterparts in human evaluation on theViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel thefrontiers of image generation research in both academia and industry, pavingthe way for future breakthroughs in this rapidly evolving field. More detailscan be found on a project webpage: https://raphael-painter.github.io/.</description><author>Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, Ping Luo</author><pubDate>Mon, 29 May 2023 18:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18295v1</guid></item><item><title>Transformer Language Models Handle Word Frequency in Prediction Head</title><link>http://arxiv.org/abs/2305.18294v1</link><description>Prediction head is a crucial component of Transformer language models.Despite its direct impact on prediction, this component has often beenoverlooked in analyzing Transformers. In this study, we investigate the innerworkings of the prediction head, specifically focusing on bias parameters. Ourexperiments with BERT and GPT-2 models reveal that the biases in their wordprediction heads play a significant role in the models' ability to reflect wordfrequency in a corpus, aligning with the logit adjustment method commonly usedin long-tailed learning. We also quantify the effect of controlling the biasesin practical auto-regressive text generation scenarios; under a particularsetting, more diverse text can be generated without compromising text quality.</description><author>Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui</author><pubDate>Mon, 29 May 2023 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18294v1</guid></item><item><title>Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</title><link>http://arxiv.org/abs/2305.18292v1</link><description>Public large-scale text-to-image diffusion models, such as Stable Diffusion,have gained significant attention from the community. These models can beeasily customized for new concepts using low-rank adaptations (LoRAs). However,the utilization of multiple concept LoRAs to jointly support multiplecustomized concepts presents a challenge. We refer to this scenario asdecentralized multi-concept customization, which involves single-client concepttuning and center-node concept fusion. In this paper, we propose a newframework called Mix-of-Show that addresses the challenges of decentralizedmulti-concept customization, including concept conflicts resulting fromexisting single-client LoRA tuning and identity loss during model fusion.Mix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-clienttuning and gradient fusion for the center node to preserve the in-domainessence of single concepts and support theoretically limitless concept fusion.Additionally, we introduce regionally controllable sampling, which extendsspatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to addressattribute binding and missing object problems in multi-concept sampling.Extensive experiments demonstrate that Mix-of-Show is capable of composingmultiple customized concepts with high fidelity, including characters, objects,and scenes.</description><author>Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, Yixiao Ge, Ying Shan, Mike Zheng Shou</author><pubDate>Mon, 29 May 2023 18:58:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18292v1</guid></item><item><title>Learning Large Causal Structures from Inverse Covariance Matrix via Matrix Decomposition</title><link>http://arxiv.org/abs/2211.14221v2</link><description>Learning causal structures from observational data is a fundamental yethighly complex problem when the number of variables is large. In this paper, westart from linear structural equation models (SEMs) and investigate ways oflearning causal structures from the inverse covariance matrix. The proposedmethod, called $\mathcal{O}$-ICID (for {\it Independence-preserving}Decomposition from Oracle Inverse Covariance matrix), is based on continuousoptimization of a type of matrix decomposition that preserves the nonzeropatterns of the inverse covariance matrix. We show that $\mathcal{O}$-ICIDprovides an efficient way for identifying the true directed acyclic graph (DAG)under the knowledge of noise variances. With weaker prior information, theproposed method gives directed graph solutions that are useful for making morerefined causal discovery. The proposed method enjoys a low complexity when thetrue DAG has bounded node degrees, as reflected by its time efficiency inexperiments in comparison with state-of-the-art algorithms.</description><author>Shuyu Dong, Kento Uemura, Akito Fujii, Shuang Chang, Yusuke Koyanagi, Koji Maruhashi, Mich√®le Sebag</author><pubDate>Mon, 29 May 2023 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14221v2</guid></item><item><title>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</title><link>http://arxiv.org/abs/2305.18290v1</link><description>While large-scale unsupervised language models (LMs) learn broad worldknowledge and some reasoning skills, achieving precise control of theirbehavior is difficult due to the completely unsupervised nature of theirtraining. Existing methods for gaining such steerability collect human labelsof the relative quality of model generations and fine-tune the unsupervised LMto align with these preferences, often with reinforcement learning from humanfeedback (RLHF). However, RLHF is a complex and often unstable procedure, firstfitting a reward model that reflects the human preferences, and thenfine-tuning the large unsupervised LM using reinforcement learning to maximizethis estimated reward without drifting too far from the original model. In thispaper, we leverage a mapping between reward functions and optimal policies toshow that this constrained reward maximization problem can be optimized exactlywith a single stage of policy training, essentially solving a classificationproblem on the human preference data. The resulting algorithm, which we callDirect Preference Optimization (DPO), is stable, performant and computationallylightweight, eliminating the need for fitting a reward model, sampling from theLM during fine-tuning, or performing significant hyperparameter tuning. Ourexperiments show that DPO can fine-tune LMs to align with human preferences aswell as or better than existing methods. Notably, fine-tuning with DPO exceedsRLHF's ability to control sentiment of generations and improves responsequality in summarization and single-turn dialogue while being substantiallysimpler to implement and train.</description><author>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn</author><pubDate>Mon, 29 May 2023 18:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18290v1</guid></item><item><title>LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections</title><link>http://arxiv.org/abs/2305.18287v1</link><description>Recently, large-scale pre-trained Vision and Language (VL) models have set anew state-of-the-art (SOTA) in zero-shot visual classification enablingopen-vocabulary recognition of potentially unlimited set of categories definedas simple language prompts. However, despite these great advances, theperformance of these zeroshot classifiers still falls short of the results ofdedicated (closed category set) classifiers trained with supervised finetuning. In this paper we show, for the first time, how to reduce this gapwithout any labels and without any paired VL data, using an unlabeled imagecollection and a set of texts auto-generated using a Large Language Model (LLM)describing the categories of interest and effectively substituting labeledvisual instances of those categories. Using our label-free approach, we areable to attain significant performance improvements over the zero-shotperformance of the base VL model and other contemporary methods and baselineson a wide variety of datasets, demonstrating absolute improvement of up to11.7% (3.8% on average) in the label-free setting. Moreover, despite ourapproach being label-free, we observe 1.3% average gains over leading few-shotprompting baselines that do use 5-shot supervision.</description><author>M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Mateusz Kozinski, Horst Possegger, Rogerio Feris, Horst Bischof</author><pubDate>Mon, 29 May 2023 18:56:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18287v1</guid></item><item><title>Photoswap: Personalized Subject Swapping in Images</title><link>http://arxiv.org/abs/2305.18286v1</link><description>In an era where images and visual content dominate our digital landscape, theability to manipulate and personalize these images has become a necessity.Envision seamlessly substituting a tabby cat lounging on a sunlit window sillin a photograph with your own playful puppy, all while preserving the originalcharm and composition of the image. We present Photoswap, a novel approach thatenables this immersive image editing experience through personalized subjectswapping in existing images. Photoswap first learns the visual concept of thesubject from reference images and then swaps it into the target image usingpre-trained diffusion models in a training-free manner. We establish that awell-conceptualized visual subject can be seamlessly transferred to any imagewith appropriate self-attention and cross-attention manipulation, maintainingthe pose of the swapped subject and the overall coherence of the image.Comprehensive experiments underscore the efficacy and controllability ofPhotoswap in personalized subject swapping. Furthermore, Photoswapsignificantly outperforms baseline methods in human ratings across subjectswapping, background preservation, and overall quality, revealing its vastapplication potential, from entertainment to professional editing.</description><author>Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, Xin Eric Wang</author><pubDate>Mon, 29 May 2023 18:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18286v1</guid></item><item><title>Partially Personalized Federated Learning: Breaking the Curse of Data Heterogeneity</title><link>http://arxiv.org/abs/2305.18285v1</link><description>We present a partially personalized formulation of Federated Learning (FL)that strikes a balance between the flexibility of personalization andcooperativeness of global training. In our framework, we split the variablesinto global parameters, which are shared across all clients, and individuallocal parameters, which are kept private. We prove that under the right splitof parameters, it is possible to find global parameters that allow each clientto fit their data perfectly, and refer to the obtained problem asoverpersonalized. For instance, the shared global parameters can be used tolearn good data representations, whereas the personalized layers are fine-tunedfor a specific client. Moreover, we present a simple algorithm for thepartially personalized formulation that offers significant benefits to allclients. In particular, it breaks the curse of data heterogeneity in severalsettings, such as training with local steps, asynchronous training, andByzantine-robust training.</description><author>Konstantin Mishchenko, Rustem Islamov, Eduard Gorbunov, Samuel Horv√°th</author><pubDate>Mon, 29 May 2023 18:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18285v1</guid></item><item><title>CommonAccent: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice</title><link>http://arxiv.org/abs/2305.18283v1</link><description>Despite the recent advancements in Automatic Speech Recognition (ASR), therecognition of accented speech still remains a dominant problem. In order tocreate more inclusive ASR systems, research has shown that the integration ofaccent information, as part of a larger ASR framework, can lead to themitigation of accented speech errors. We address multilingual accentclassification through the ECAPA-TDNN and Wav2Vec 2.0/XLSR architectures whichhave been proven to perform well on a variety of speech-related downstreamtasks. We introduce a simple-to-follow recipe aligned to the SpeechBraintoolkit for accent classification based on Common Voice 7.0 (English) andCommon Voice 11.0 (Italian, German, and Spanish). Furthermore, we establish newstate-of-the-art for English accent classification with as high as 95%accuracy. We also study the internal categorization of the Wav2Vev 2.0embeddings through t-SNE, noting that there is a level of clustering based onphonological similarity. (Our recipe is open-source in the SpeechBrain toolkit,see: https://github.com/speechbrain/speechbrain/tree/develop/recipes)</description><author>Juan Zuluaga-Gomez, Sara Ahmed, Danielius Visockas, Cem Subakan</author><pubDate>Mon, 29 May 2023 18:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18283v1</guid></item><item><title>HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition</title><link>http://arxiv.org/abs/2305.18281v1</link><description>State-of-the-art ASR systems have achieved promising results by modelinglocal and global interactions separately. While the former can be computedefficiently, global interactions are usually modeled via attention mechanisms,which are expensive for long input sequences. Here, we address this byextending HyperMixer, an efficient alternative to attention exhibiting linearcomplexity, to the Conformer architecture for speech recognition, leading toHyperConformer. In particular, multi-head HyperConformer achieves comparable orhigher recognition performance while being more efficient than Conformer interms of inference speed, memory, parameter count, and available training data.HyperConformer achieves a word error rate of 2.9% on Librispeech test-cleanwith less than 8M neural parameters and a peak memory during training of 5.7GB,hence trainable with accessible hardware. Encoder speed is between 38% onmid-length speech and 56% on long speech faster than an equivalent Conformer.(The HyperConformer recipe is publicly available in:https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech/ASR/transformer/)</description><author>Florian Mai, Juan Zuluaga-Gomez, Titouan Parcollet, Petr Motlicek</author><pubDate>Mon, 29 May 2023 18:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18281v1</guid></item><item><title>Contextual Object Detection with Multimodal Large Language Models</title><link>http://arxiv.org/abs/2305.18279v1</link><description>Recent Multimodal Large Language Models (MLLMs) are remarkable invision-language tasks, such as image captioning and question answering, butlack the essential perception ability, i.e., object detection. In this work, weaddress this limitation by introducing a novel research problem of contextualobject detection -- understanding visible objects within different human-AIinteractive contexts. Three representative scenarios are investigated,including the language cloze test, visual captioning, and question answering.Moreover, we present ContextDET, a unified multimodal model that is capable ofend-to-end differentiable modeling of visual-language contexts, so as tolocate, identify, and associate visual objects with language inputs forhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visualencoder for extracting visual representations, (ii) a pre-trained LLM formultimodal context decoding, and (iii) a visual decoder for predicting boundingboxes given contextual object words. The new generate-then-detect frameworkenables us to detect object words within human vocabulary. Extensiveexperiments show the advantages of ContextDET on our proposed CODE benchmark,open-vocabulary detection, and referring image segmentation. Github:https://github.com/yuhangzang/ContextDET.</description><author>Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy</author><pubDate>Mon, 29 May 2023 18:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18279v1</guid></item><item><title>Mathematical Structure of Syntactic Merge</title><link>http://arxiv.org/abs/2305.18278v1</link><description>The syntactic Merge operation of the Minimalist Program in linguistics can bedescribed mathematically in terms of Hopf algebras, with a formalism similar tothe one arising in the physics of renormalization. This mathematicalformulation of Merge has good descriptive power, as phenomena empiricallyobserved in linguistics can be justified from simple mathematical arguments. Italso provides a possible mathematical model for externalization and for therole of syntactic parameters.</description><author>Matilde Marcolli, Noam Chomsky, Robert Berwick</author><pubDate>Mon, 29 May 2023 18:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18278v1</guid></item><item><title>3DTeethSeg'22: 3D Teeth Scan Segmentation and Labeling Challenge</title><link>http://arxiv.org/abs/2305.18277v1</link><description>Teeth localization, segmentation, and labeling from intra-oral 3D scans areessential tasks in modern dentistry to enhance dental diagnostics, treatmentplanning, and population-based studies on oral health. However, developingautomated algorithms for teeth analysis presents significant challenges due tovariations in dental anatomy, imaging protocols, and limited availability ofpublicly accessible data. To address these challenges, the 3DTeethSeg'22challenge was organized in conjunction with the International Conference onMedical Image Computing and Computer Assisted Intervention (MICCAI) in 2022,with a call for algorithms tackling teeth localization, segmentation, andlabeling from intraoral 3D scans. A dataset comprising a total of 1800 scansfrom 900 patients was prepared, and each tooth was individually annotated by ahuman-machine hybrid algorithm. A total of 6 algorithms were evaluated on thisdataset. In this study, we present the evaluation results of the 3DTeethSeg'22challenge. The 3DTeethSeg'22 challenge code can be accessed at:https://github.com/abenhamadou/3DTeethSeg22_challenge</description><author>Achraf Ben-Hamadou, Oussama Smaoui, Ahmed Rekik, Sergi Pujades, Edmond Boyer, Hoyeon Lim, Minchang Kim, Minkyung Lee, Minyoung Chung, Yeong-Gil Shin, Mathieu Leclercq, Lucia Cevidanes, Juan Carlos Prieto, Shaojie Zhuang, Guangshun Wei, Zhiming Cui, Yuanfeng Zhou, Tudor Dascalu, Bulat Ibragimov, Tae-Hoon Yong, Hong-Gi Ahn, Wan Kim, Jae-Hwan Han, Byungsun Choi, Niels van Nistelrooij, Steven Kempers, Shankeeth Vinayahalingam, Julien Strippoli, Aur√©lien Thollot, Hugo Setbon, Cyril Trosset, Edouard Ladroit</author><pubDate>Mon, 29 May 2023 18:49:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18277v1</guid></item><item><title>Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors</title><link>http://arxiv.org/abs/2305.18274v1</link><description>We present MindEye, a novel fMRI-to-image approach to retrieve andreconstruct viewed images from brain activity. Our model comprises two parallelsubmodules that are specialized for retrieval (using contrastive learning) andreconstruction (using a diffusion prior). MindEye can map fMRI brain activityto any high dimensional multimodal latent space, like CLIP image space,enabling image reconstruction using generative models that accept embeddingsfrom this latent space. We comprehensively compare our approach with otherexisting methods, using both qualitative side-by-side comparisons andquantitative evaluations, and show that MindEye achieves state-of-the-artperformance in both reconstruction and retrieval tasks. In particular, MindEyecan retrieve the exact original image even among highly similar candidatesindicating that its brain embeddings retain fine-grained image-specificinformation. This allows us to accurately retrieve images even from large-scaledatabases like LAION-5B. We demonstrate through ablations that MindEye'sperformance improvements over previous methods result from specializedsubmodules for retrieval and reconstruction, improved training techniques, andtraining models with orders of magnitude more parameters. Furthermore, we showthat MindEye can better preserve low-level image features in thereconstructions by using img2img, with outputs from a separate autoencoder. Allcode is available on GitHub.</description><author>Paul S. Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan J. Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth A. Norman, Tanishq Mathew Abraham</author><pubDate>Mon, 29 May 2023 18:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18274v1</guid></item><item><title>Pix2Repair: Implicit Shape Restoration from Images</title><link>http://arxiv.org/abs/2305.18273v1</link><description>We present Pix2Repair, an automated shape repair approach that generatesrestoration shapes from images to repair fractured objects. Prior repairapproaches require a high-resolution watertight 3D mesh of the fractured objectas input. Input 3D meshes must be obtained using expensive 3D scanners, andscanned meshes require manual cleanup, limiting accessibility and scalability.Pix2Repair takes an image of the fractured object as input and automaticallygenerates a 3D printable restoration shape. We contribute a novel shapefunction that deconstructs a latent code representing the fractured object intoa complete shape and a break surface. We show restorations for syntheticfractures from the Geometric Breaks and Breaking Bad datasets, and culturalheritage objects from the QP dataset, and for real fractures from the FantasticBreaks dataset. We overcome challenges in restoring axially symmetric objectsby predicting view-centered restorations. Our approach outperforms shapecompletion approaches adapted for shape repair in terms of chamfer distance,earth mover's distance, normal consistency, and percent restorations generated.</description><author>Nikolas Lamb, Sean Banerjee, Natasha Kholgade Banerjee</author><pubDate>Mon, 29 May 2023 18:48:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18273v1</guid></item><item><title>Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity</title><link>http://arxiv.org/abs/2301.12867v4</link><description>Recent breakthroughs in natural language processing (NLP) have permitted thesynthesis and comprehension of coherent text in an open-ended way, thereforetranslating the theoretical algorithms into practical applications. The largelanguage models (LLMs) have significantly impacted businesses such as reportsummarization software and copywriters. Observations indicate, however, thatLLMs may exhibit social prejudice and toxicity, posing ethical and societaldangers of consequences resulting from irresponsibility. Large-scale benchmarksfor accountable LLMs should consequently be developed. Although severalempirical investigations reveal the existence of a few ethical difficulties inadvanced LLMs, there is little systematic examination and user study of therisks and harmful behaviors of current LLM usage. To further educate futureefforts on constructing ethical LLMs responsibly, we perform a qualitativeresearch method called ``red teaming'' on OpenAI's ChatGPT\footnote{In thispaper, ChatGPT refers to the version released on Dec 15th.} to betterunderstand the practical features of ethical dangers in recent LLMs. We analyzeChatGPT comprehensively from four perspectives: 1) \textit{Bias} 2)\textit{Reliability} 3) \textit{Robustness} 4) \textit{Toxicity}. In accordancewith our stated viewpoints, we empirically benchmark ChatGPT on multiple sampledatasets. We find that a significant number of ethical risks cannot beaddressed by existing benchmarks, and hence illustrate them via additional casestudies. In addition, we examine the implications of our findings on AI ethicsand harmal behaviors of ChatGPT, as well as future problems and practicaldesign considerations for responsible LLMs. We believe that our findings maygive light on future efforts to determine and mitigate the ethical hazardsposed by machines in LLM applications.</description><author>Terry Yue Zhuo, Yujin Huang, Chunyang Chen, Zhenchang Xing</author><pubDate>Mon, 29 May 2023 18:46:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12867v4</guid></item><item><title>Learning Two-Layer Neural Networks, One (Giant) Step at a Time</title><link>http://arxiv.org/abs/2305.18270v1</link><description>We study the training dynamics of shallow neural networks, investigating theconditions under which a limited number of large batch gradient descent stepscan facilitate feature learning beyond the kernel regime. We compare theinfluence of batch size and that of multiple (but finitely many) steps. Ouranalysis of a single-step process reveals that while a batch size of $n = O(d)$enables feature learning, it is only adequate for learning a single direction,or a single-index model. In contrast, $n = O(d^2)$ is essential for learningmultiple directions and specialization. Moreover, we demonstrate that ``hard''directions, which lack the first $\ell$ Hermite coefficients, remain unobservedand require a batch size of $n = O(d^\ell)$ for being captured by gradientdescent. Upon iterating a few steps, the scenario changes: a batch-size of $n =O(d)$ is enough to learn new target directions spanning the subspace linearlyconnected in the Hermite basis to the previously learned directions, thereby astaircase property. Our analysis utilizes a blend of techniques related toconcentration, projection-based conditioning, and Gaussian equivalence that areof independent interest. By determining the conditions necessary for learningand specialization, our results highlight the interaction between batch sizeand number of iterations, and lead to a hierarchical depiction where learningperformance exhibits a stairway to accuracy over time and batch size, sheddingnew light on feature learning in neural networks.</description><author>Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, Ludovic Stephan</author><pubDate>Mon, 29 May 2023 18:43:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18270v1</guid></item><item><title>Doing the right thing for the right reason: Evaluating artificial moral cognition by probing cost insensitivity</title><link>http://arxiv.org/abs/2305.18269v1</link><description>Is it possible to evaluate the moral cognition of complex artificial agents?In this work, we take a look at one aspect of morality: `doing the right thingfor the right reasons.' We propose a behavior-based analysis of artificialmoral cognition which could also be applied to humans to facilitatelike-for-like comparison. Morally-motivated behavior should persist despitemounting cost; by measuring an agent's sensitivity to this cost, we gain deeperinsight into underlying motivations. We apply this evaluation to a particularset of deep reinforcement learning agents, trained by memory-basedmeta-reinforcement learning. Our results indicate that agents trained with areward function that includes other-regarding preferences perform helpingbehavior in a way that is less sensitive to increasing cost than agents trainedwith more self-interested preferences.</description><author>Yiran Mao, Madeline G. Reinecke, Markus Kunesch, Edgar A. Du√©√±ez-Guzm√°n, Ramona Comanescu, Julia Haas, Joel Z. Leibo</author><pubDate>Mon, 29 May 2023 18:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18269v1</guid></item><item><title>Analysis of the (1+1) EA on LeadingOnes with Constraints</title><link>http://arxiv.org/abs/2305.18267v1</link><description>Understanding how evolutionary algorithms perform on constrained problems hasgained increasing attention in recent years. In this paper, we study howevolutionary algorithms optimize constrained versions of the classicalLeadingOnes problem. We first provide a run time analysis for the classical(1+1) EA on the LeadingOnes problem with a deterministic cardinalityconstraint, giving $\Theta(n (n-B)\log(B) + n^2)$ as the tight bound. Ourresults show that the behaviour of the algorithm is highly dependent on theconstraint bound of the uniform constraint. Afterwards, we consider the problemin the context of stochastic constraints and provide insights usingexperimental studies on how the ($\mu$+1) EA is able to deal with theseconstraints in a sampling-based setting.</description><author>Tobias Friedrich, Timo K√∂tzing, Aneta Neumann, Frank Neumann, Aishwarya Radhakrishnan</author><pubDate>Mon, 29 May 2023 18:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18267v1</guid></item><item><title>Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evidence</title><link>http://arxiv.org/abs/2305.18265v1</link><description>We present a new fact-checking benchmark, Check-COVID, that requires systemsto verify claims about COVID-19 from news using evidence from scientificarticles. This approach to fact-checking is particularly challenging as itrequires checking internet text written in everyday language against evidencefrom journal articles written in formal academic language. Check-COVID contains1, 504 expert-annotated news claims about the coronavirus paired withsentence-level evidence from scientific journal articles and veracity labels.It includes both extracted (journalist-written) and composed(annotator-written) claims. Experiments using both a fact-checking specificsystem and GPT-3.5, which respectively achieve F1 scores of 76.99 and 69.90 onthis task, reveal the difficulty of automatically fact-checking both claimtypes and the importance of in-domain data for good performance. Our data andmodels are released publicly at https://github.com/posuer/Check-COVID.</description><author>Gengyu Wang, Kate Harwood, Lawrence Chillrud, Amith Ananthram, Melanie Subbiah, Kathleen McKeown</author><pubDate>Mon, 29 May 2023 18:39:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18265v1</guid></item><item><title>Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising</title><link>http://arxiv.org/abs/2305.18264v1</link><description>Leveraging large-scale image-text datasets and advancements in diffusionmodels, text-driven generative models have made remarkable strides in the fieldof image generation and editing. This study explores the potential of extendingthe text-driven ability to the generation and editing of multi-text conditionedlong videos. Current methodologies for video generation and editing, whileinnovative, are often confined to extremely short videos (typically less than24 frames) and are limited to a single text condition. These constraintssignificantly limit their applications given that real-world videos usuallyconsist of multiple segments, each bearing different semantic information. Toaddress this challenge, we introduce a novel paradigm dubbed as Gen-L-Video,capable of extending off-the-shelf short video diffusion models for generatingand editing videos comprising hundreds of frames with diverse semantic segmentswithout introducing additional training, all while preserving contentconsistency. We have implemented three mainstream text-driven video generationand editing methodologies and extended them to accommodate longer videos imbuedwith a variety of semantic segments with our proposed paradigm. Ourexperimental outcomes reveal that our approach significantly broadens thegenerative and editing capabilities of video diffusion models, offering newpossibilities for future research and applications. The code is available athttps://github.com/G-U-N/Gen-L-Video.</description><author>Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, Hongsheng Li</author><pubDate>Mon, 29 May 2023 18:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18264v1</guid></item><item><title>Beyond Confidence: Reliable Models Should Also Consider Atypicality</title><link>http://arxiv.org/abs/2305.18262v1</link><description>While most machine learning models can provide confidence in theirpredictions, confidence is insufficient to understand a prediction'sreliability. For instance, the model may have a low confidence prediction ifthe input is not well-represented in the training dataset or if the input isinherently ambiguous. In this work, we investigate the relationship between howatypical(rare) a sample or a class is and the reliability of a model'spredictions. We first demonstrate that atypicality is strongly related tomiscalibration and accuracy. In particular, we empirically show thatpredictions for atypical inputs or atypical classes are more overconfident andhave lower accuracy. Using these insights, we show incorporating atypicalityimproves uncertainty quantification and model performance for discriminativeneural networks and large language models. In a case study, we show that usingatypicality improves the performance of a skin lesion classifier acrossdifferent skin tone groups without having access to the group attributes.Overall, we propose that models should use not only confidence but alsoatypicality to improve uncertainty quantification and performance. Our resultsdemonstrate that simple post-hoc atypicality estimators can provide significantvalue.</description><author>Mert Yuksekgonul, Linjun Zhang, James Zou, Carlos Guestrin</author><pubDate>Mon, 29 May 2023 18:37:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18262v1</guid></item><item><title>Fundamental Limitations of Alignment in Large Language Models</title><link>http://arxiv.org/abs/2304.11082v2</link><description>An important aspect in developing language models that interact with humansis aligning their behavior to be useful and unharmful for their human users.This is usually achieved by tuning the model in a way that enhances desiredbehaviors and inhibits undesired ones, a process referred to as alignment. Inthis paper, we propose a theoretical approach called Behavior ExpectationBounds (BEB) which allows us to formally investigate several inherentcharacteristics and limitations of alignment in large language models.Importantly, we prove that for any behavior that has a finite probability ofbeing exhibited by the model, there exist prompts that can trigger the modelinto outputting this behavior, with probability that increases with the lengthof the prompt. This implies that any alignment process that attenuatesundesired behavior but does not remove it altogether, is not safe againstadversarial prompting attacks. Furthermore, our framework hints at themechanism by which leading alignment approaches such as reinforcement learningfrom human feedback increase the LLM's proneness to being prompted into theundesired behaviors. Moreover, we include the notion of personas in our BEBframework, and find that behaviors which are generally very unlikely to beexhibited by the model can be brought to the front by prompting the model tobehave as specific persona. This theoretical result is being experimentallydemonstrated in large scale by the so called contemporary "chatGPT jailbreaks",where adversarial users trick the LLM into breaking its alignment guardrails bytriggering it into acting as a malicious persona. Our results exposefundamental limitations in alignment of LLMs and bring to the forefront theneed to devise reliable mechanisms for ensuring AI safety.</description><author>Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua</author><pubDate>Mon, 29 May 2023 18:31:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11082v2</guid></item><item><title>Synfeal: A Data-Driven Simulator for End-to-End Camera Localization</title><link>http://arxiv.org/abs/2305.18260v1</link><description>Collecting real-world data is often considered the bottleneck of ArtificialIntelligence, stalling the research progress in several fields, one of which iscamera localization. End-to-end camera localization methods are stilloutperformed by traditional methods, and we argue that the inconsistenciesassociated with the data collection techniques are restraining the potential ofend-to-end methods. Inspired by the recent data-centric paradigm, we propose aframework that synthesizes large localization datasets based on realistic 3Dreconstructions of the real world. Our framework, termed Synfeal: Syntheticfrom Real, is an open-source, data-driven simulator that synthesizes RGB imagesby moving a virtual camera through a realistic 3D textured mesh, whilecollecting the corresponding ground-truth camera poses. The results validatethat the training of camera localization algorithms on datasets generated bySynfeal leads to better results when compared to datasets generated bystate-of-the-art methods. Using Synfeal, we conducted the first analysis of therelationship between the size of the dataset and the performance of cameralocalization algorithms. Results show that the performance significantlyincreases with the dataset size. Our results also suggest that when a largelocalization dataset with high quality is available, training from scratchleads to better performances. Synfeal is publicly available athttps://github.com/DanielCoelho112/synfeal.</description><author>Daniel Coelho, Miguel Oliveira, Paulo Dias</author><pubDate>Mon, 29 May 2023 18:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18260v1</guid></item><item><title>GlyphControl: Glyph Conditional Control for Visual Text Generation</title><link>http://arxiv.org/abs/2305.18259v1</link><description>Recently, there has been a growing interest in developing diffusion-basedtext-to-image generative models capable of generating coherent and well-formedvisual text. In this paper, we propose a novel and efficient approach calledGlyphControl to address this task. Unlike existing methods that rely oncharacter-aware text encoders like ByT5 and require retraining of text-to-imagemodels, our approach leverages additional glyph conditional information toenhance the performance of the off-the-shelf Stable-Diffusion model ingenerating accurate visual text. By incorporating glyph instructions, users cancustomize the content, location, and size of the generated text according totheir specific requirements. To facilitate further research in visual textgeneration, we construct a training benchmark dataset called LAION-Glyph. Weevaluate the effectiveness of our approach by measuring OCR-based metrics andCLIP scores of the generated visual text. Our empirical evaluations demonstratethat GlyphControl outperforms the recent DeepFloyd IF approach in terms of OCRaccuracy and CLIP scores, highlighting the efficacy of our method.</description><author>Yukang Yang, Dongnan Gui, Yuhui Yuan, Haisong Ding, Han Hu, Kai Chen</author><pubDate>Mon, 29 May 2023 18:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18259v1</guid></item><item><title>One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration</title><link>http://arxiv.org/abs/2305.18258v1</link><description>In online reinforcement learning (online RL), balancing exploration andexploitation is crucial for finding an optimal policy in a sample-efficientway. To achieve this, existing sample-efficient online RL algorithms typicallyconsist of three components: estimation, planning, and exploration. However, inorder to cope with general function approximators, most of them involveimpractical algorithmic components to incentivize exploration, such asoptimization within data-dependent level-sets or complicated samplingprocedures. To address this challenge, we propose an easy-to-implement RLframework called \textit{Maximize to Explore} (\texttt{MEX}), which only needsto optimize \emph{unconstrainedly} a single objective that integrates theestimation and planning components while balancing exploration and exploitationautomatically. Theoretically, we prove that \texttt{MEX} achieves a sublinearregret with general function approximations for Markov decision processes (MDP)and is further extendable to two-player zero-sum Markov games (MG). Meanwhile,we adapt deep RL baselines to design practical versions of \texttt{MEX}, inboth model-free and model-based manners, which can outperform baselines by astable margin in various MuJoCo environments with sparse rewards. Compared withexisting sample-efficient online RL algorithms with general functionapproximations, \texttt{MEX} achieves similar sample efficiency while enjoyinga lower computational cost and is more compatible with modern deep RL methods.</description><author>Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran Wang</author><pubDate>Mon, 29 May 2023 18:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18258v1</guid></item><item><title>Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers</title><link>http://arxiv.org/abs/2305.18256v1</link><description>A hyper-relational knowledge graph has been recently studied where a tripletis associated with a set of qualifiers; a qualifier is composed of a relationand an entity, providing auxiliary information for a triplet. While existinghyper-relational knowledge graph embedding methods assume that the entities arediscrete objects, some information should be represented using numeric values,e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, OxfordUniv.) can be associated with a qualifier such as (start time, 1911). In thispaper, we propose a unified framework named HyNT that learns representations ofa hyper-relational knowledge graph containing numeric literals in eithertriplets or qualifiers. We define a context transformer and a predictiontransformer to learn the representations based not only on the correlationsbetween a triplet and its qualifiers but also on the numeric information. Bylearning compact representations of triplets and qualifiers and feeding theminto the transformers, we reduce the computation cost of using transformers.Using HyNT, we can predict missing numeric values in addition to missingentities or relations in a hyper-relational knowledge graph. Experimentalresults show that HyNT significantly outperforms state-of-the-art methods onreal-world datasets.</description><author>Chanyoung Chung, Jaejun Lee, Joyce Jiyoung Whang</author><pubDate>Mon, 29 May 2023 18:23:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18256v1</guid></item><item><title>Do Language Models Know When They're Hallucinating References?</title><link>http://arxiv.org/abs/2305.18248v1</link><description>Current state-of-the-art language models (LMs) are notorious for generatingtext with "hallucinations," a primary example being book and paper referencesthat lack any solid basis in their training data. However, we find that many ofthese fabrications can be identified using the same LM, using only black-boxqueries without consulting any external resources. Consistency checks done withdirect queries about whether the generated reference title is real (inspired byKadavath et al. 2022, Lin et al. 2022, Manakul et al. 2023) are compared toconsistency checks with indirect queries which ask for ancillary details suchas the authors of the work. These consistency checks are found to be partiallyreliable indicators of whether or not the reference is a hallucination. Inparticular, we find that LMs in the GPT-series will hallucinate differingauthors of hallucinated references when queried in independent sessions, whileit will consistently identify authors of real references. This suggests thatthe hallucination may be more a result of generation techniques than theunderlying representation.</description><author>Ayush Agrawal, Lester Mackey, Adam Tauman Kalai</author><pubDate>Mon, 29 May 2023 18:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18248v1</guid></item><item><title>TaleCrafter: Interactive Story Visualization with Multiple Characters</title><link>http://arxiv.org/abs/2305.18247v1</link><description>Accurate Story visualization requires several necessary elements, such asidentity consistency across frames, the alignment between plain text and visualcontent, and a reasonable layout of objects in images. Most previous worksendeavor to meet these requirements by fitting a text-to-image (T2I) model on aset of videos in the same style and with the same characters, e.g., theFlintstonesSV dataset. However, the learned T2I models typically struggle toadapt to new characters, scenes, and styles, and often lack the flexibility torevise the layout of the synthesized images. This paper proposes a system forgeneric interactive story visualization, capable of handling multiple novelcharacters and supporting the editing of layout and local structure. It isdeveloped by leveraging the prior knowledge of large language and T2I models,trained on massive corpora. The system comprises four interconnectedcomponents: story-to-prompt generation (S2P), text-to-layout generation (T2L),controllable text-to-image generation (C-T2I), and image-to-video animation(I2V). First, the S2P module converts concise story information into detailedprompts required for subsequent stages. Next, T2L generates diverse andreasonable layouts based on the prompts, offering users the ability to adjustand refine the layout to their preference. The core component, C-T2I, enablesthe creation of images guided by layouts, sketches, and actor-specificidentifiers to maintain consistency and detail across visualizations. Finally,I2V enriches the visualization process by animating the generated images.Extensive experiments and a user study are conducted to validate theeffectiveness and flexibility of interactive editing of the proposed system.</description><author>Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Haoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, Yujiu Yang</author><pubDate>Mon, 29 May 2023 18:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18247v1</guid></item><item><title>Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo</title><link>http://arxiv.org/abs/2305.18246v1</link><description>We present a scalable and effective exploration strategy based on Thompsonsampling for reinforcement learning (RL). One of the key shortcomings ofexisting Thompson sampling algorithms is the need to perform a Gaussianapproximation of the posterior distribution, which is not a good surrogate inmost practical settings. We instead directly sample the Q function from itsposterior distribution, by using Langevin Monte Carlo, an efficient type ofMarkov Chain Monte Carlo (MCMC) method. Our method only needs to perform noisygradient descent updates to learn the exact posterior distribution of the Qfunction, which makes our approach easy to deploy in deep RL. We provide arigorous theoretical analysis for the proposed method and demonstrate that, inthe linear Markov decision process (linear MDP) setting, it has a regret boundof $\tilde{O}(d^{3/2}H^{5/2}\sqrt{T})$, where $d$ is the dimension of thefeature mapping, $H$ is the planning horizon, and $T$ is the total number ofsteps. We apply this approach to deep RL, by using Adam optimizer to performgradient updates. Our approach achieves better or similar results compared withstate-of-the-art deep RL algorithms on several challenging exploration tasksfrom the Atari57 suite.</description><author>Haque Ishfaq, Qingfeng Lan, Pan Xu, A. Rupam Mahmood, Doina Precup, Anima Anandkumar, Kamyar Azizzadenesheli</author><pubDate>Mon, 29 May 2023 18:11:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18246v1</guid></item><item><title>Black Box Adversarial Prompting for Foundation Models</title><link>http://arxiv.org/abs/2302.04237v2</link><description>Prompting interfaces allow users to quickly adjust the output of generativemodels in both vision and language. However, small changes and design choicesin the prompt can lead to significant differences in the output. In this work,we develop a black-box framework for generating adversarial prompts forunstructured image and text generation. These prompts, which can be standaloneor prepended to benign prompts, induce specific behaviors into the generativeprocess, such as generating images of a particular object or generating highperplexity text.</description><author>Natalie Maus, Patrick Chao, Eric Wong, Jacob Gardner</author><pubDate>Mon, 29 May 2023 18:06:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04237v2</guid></item><item><title>GazeGNN: A Gaze-Guided Graph Neural Network for Disease Classification</title><link>http://arxiv.org/abs/2305.18221v1</link><description>The application of eye-tracking techniques in medical image analysis hasbecome increasingly popular in recent years. It collects the visual searchpatterns of the domain experts, containing much important information abouthealth and disease. Therefore, how to efficiently integrate radiologists' gazepatterns into the diagnostic analysis turns into a critical question. Existingworks usually transform gaze information into visual attention maps (VAMs) tosupervise the learning process. However, this time-consuming procedure makes itdifficult to develop end-to-end algorithms. In this work, we propose a novelgaze-guided graph neural network (GNN), GazeGNN, to perform diseaseclassification from medical scans. In GazeGNN, we create a unifiedrepresentation graph that models both the image and gaze pattern information.Hence, the eye-gaze information is directly utilized without being convertedinto VAMs. With this benefit, we develop a real-time, real-world, end-to-enddisease classification algorithm for the first time and avoid the noise andtime consumption introduced during the VAM preparation. To our best knowledge,GazeGNN is the first work that adopts GNN to integrate image and eye-gaze data.Our experiments on the public chest X-ray dataset show that our proposed methodexhibits the best classification performance compared to existing methods.</description><author>Bin Wang, Hongyi Pan, Armstrong Aboah, Zheyuan Zhang, Ahmet Cetin, Drew Torigian, Baris Turkbey, Elizabeth Krupinski, Jayaram Udupa, Ulas Bagci</author><pubDate>Mon, 29 May 2023 18:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18221v1</guid></item><item><title>Towards minimizing efforts for Morphing Attacks -- Deep embeddings for morphing pair selection and improved Morphing Attack Detection</title><link>http://arxiv.org/abs/2305.18216v1</link><description>Face Morphing Attacks pose a threat to the security of identity documents,especially with respect to a subsequent access control process, because itenables both individuals involved to exploit the same document. In this study,face embeddings serve two purposes: pre-selecting images for large-scaleMorphing Attack generation and detecting potential Morphing Attacks. We buildupon previous embedding studies in both use cases using the MagFace model. Forthe first objective, we employ an pre-selection algorithm that pairsindividuals based on face embedding similarity. We quantify the attackpotential of differently morphed face images to compare the usability ofpre-selection in automatically generating numerous successful Morphing Attacks.Regarding the second objective, we compare embeddings from two state-of-the-artface recognition systems in terms of their ability to detect Morphing Attacks.Our findings demonstrate that ArcFace and MagFace provide valuable faceembeddings for image pre-selection. Both open-source and COTS face recognitionsystems are susceptible to generated attacks, particularly when pre-selectionis based on embeddings rather than random pairing which was only constrained bysoft biometrics. More accurate face recognition systems exhibit greatervulnerability to attacks, with COTS systems being the most susceptible.Additionally, MagFace embeddings serve as a robust alternative for detectingmorphed face images compared to the previously used ArcFace embeddings. Theresults endorse the advantages of face embeddings in more effective imagepre-selection for face morphing and accurate detection of morphed face images.This is supported by extensive analysis of various designed attacks. TheMagFace model proves to be a powerful alternative to the commonly used ArcFacemodel for both objectives, pre-selection and attack detection.</description><author>Roman Kessler, Kiran Raja, Juan Tapia, Christoph Busch</author><pubDate>Mon, 29 May 2023 18:00:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18216v1</guid></item><item><title>Gaussian Process Probes (GPP) for Uncertainty-Aware Probing</title><link>http://arxiv.org/abs/2305.18213v1</link><description>Understanding which concepts models can and cannot represent has beenfundamental to many tasks: from effective and responsible use of models todetecting out of distribution data. We introduce Gaussian process probes (GPP),a unified and simple framework for probing and measuring uncertainty aboutconcepts represented by models. As a Bayesian extension of linear probingmethods, GPP asks what kind of distribution over classifiers (of concepts) isinduced by the model. This distribution can be used to measure both what themodel represents and how confident the probe is about what the modelrepresents. GPP can be applied to any pre-trained model with vectorrepresentations of inputs (e.g., activations). It does not require access totraining data, gradients, or the architecture. We validate GPP on datasetscontaining both synthetic and real images. Our experiments show it can (1)probe a model's representations of concepts even with a very small number ofexamples, (2) accurately measure both epistemic uncertainty (how confident theprobe is) and aleatory uncertainty (how fuzzy the concepts are to the model),and (3) detect out of distribution data using those uncertainty measures aswell as classic methods do. By using Gaussian processes to expand what probingcan offer, GPP provides a data-efficient, versatile and uncertainty-aware toolfor understanding and evaluating the capabilities of machine learning models.</description><author>Zi Wang, Alexander Ku, Jason Baldridge, Thomas L. Griffiths, Been Kim</author><pubDate>Mon, 29 May 2023 18:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18213v1</guid></item><item><title>Concept Decomposition for Visual Exploration and Inspiration</title><link>http://arxiv.org/abs/2305.18203v1</link><description>A creative idea is often born from transforming, combining, and modifyingideas from existing visual examples capturing various concepts. However, onecannot simply copy the concept as a whole, and inspiration is achieved byexamining certain aspects of the concept. Hence, it is often necessary toseparate a concept into different aspects to provide new perspectives. In thispaper, we propose a method to decompose a visual concept, represented as a setof images, into different visual aspects encoded in a hierarchical treestructure. We utilize large vision-language models and their rich latent spacefor concept decomposition and generation. Each node in the tree represents asub-concept using a learned vector embedding injected into the latent space ofa pretrained text-to-image model. We use a set of regularizations to guide theoptimization of the embedding vectors encoded in the nodes to follow thehierarchical structure of the tree. Our method allows to explore and discovernew concepts derived from the original one. The tree provides the possibilityof endless visual sampling at each node, allowing the user to explore thehidden sub-concepts of the object of interest. The learned aspects in each nodecan be combined within and across trees to create new visual ideas, and can beused in natural language sentences to apply such aspects to new designs.</description><author>Yael Vinker, Andrey Voynov, Daniel Cohen-Or, Ariel Shamir</author><pubDate>Mon, 29 May 2023 17:56:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18203v1</guid></item><item><title>A Critical Evaluation of Evaluations for Long-form Question Answering</title><link>http://arxiv.org/abs/2305.18201v1</link><description>Long-form question answering (LFQA) enables answering a wide range ofquestions, but its flexibility poses enormous challenges for evaluation. Weperform the first targeted study of the evaluation of long-form answers,covering both human and automatic evaluation practices. We hire domain expertsin seven areas to provide preference judgments over pairs of answers, alongwith free-form justifications for their choices. We present a careful analysisof experts' evaluation, which focuses on new aspects such as thecomprehensiveness of the answer. Next, we examine automatic text generationmetrics, finding that no existing metrics are predictive of human preferencejudgments. However, some metrics correlate with fine-grained aspects of answers(e.g., coherence). We encourage future work to move away from a single "overallscore" of the answer and adopt a multi-faceted evaluation, targeting aspectssuch as factuality and completeness. We publicly release all of our annotationsand code to spur future work into LFQA evaluation.</description><author>Fangyuan Xu, Yixiao Song, Mohit Iyyer, Eunsol Choi</author><pubDate>Mon, 29 May 2023 17:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18201v1</guid></item><item><title>Contextual Knowledge Learning For Dialogue Generation</title><link>http://arxiv.org/abs/2305.18200v1</link><description>Incorporating conversational context and knowledge into dialogue generationmodels has been essential for improving the quality of the generated responses.The context, comprising utterances from previous dialogue exchanges, is used asa source of content for response generation and as a means of selectingexternal knowledge. However, to avoid introducing irrelevant content, it is keyto enable fine-grained scoring of context and knowledge. In this paper, wepresent a novel approach to context and knowledge weighting as an integral partof model training. We guide the model training through a Contextual KnowledgeLearning (CKL) process which involves Latent Vectors for context and knowledge,respectively. CKL Latent Vectors capture the relationship between context,knowledge, and responses through weak supervision and enable differentialweighting of context utterances and knowledge sentences during the trainingprocess. Experiments with two standard datasets and human evaluationdemonstrate that CKL leads to a significant improvement compared with theperformance of six strong baseline models and shows robustness with regard toreduced sizes of training sets.</description><author>Wen Zheng, Natasa Milic-Frayling, Ke Zhou</author><pubDate>Mon, 29 May 2023 17:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18200v1</guid></item><item><title>Geometric Clifford Algebra Networks</title><link>http://arxiv.org/abs/2302.06594v2</link><description>We propose Geometric Clifford Algebra Networks (GCANs) for modeling dynamicalsystems. GCANs are based on symmetry group transformations using geometric(Clifford) algebras. We first review the quintessence of modern (plane-based)geometric algebra, which builds on isometries encoded as elements of the$\mathrm{Pin}(p,q,r)$ group. We then propose the concept of group actionlayers, which linearly combine object transformations using pre-specified groupactions. Together with a new activation and normalization scheme, these layersserve as adjustable $\textit{geometric templates}$ that can be refined viagradient descent. Theoretical advantages are strongly reflected in the modelingof three-dimensional rigid body transformations as well as large-scale fluiddynamics simulations, showing significantly improved performance overtraditional methods.</description><author>David Ruhe, Jayesh K. Gupta, Steven de Keninck, Max Welling, Johannes Brandstetter</author><pubDate>Mon, 29 May 2023 17:51:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06594v2</guid></item><item><title>Introduction to Online Nonstochastic Control</title><link>http://arxiv.org/abs/2211.09619v2</link><description>This text presents an introduction to an emerging paradigm in control ofdynamical systems and differentiable reinforcement learning called onlinenonstochastic control. The new approach applies techniques from online convexoptimization and convex relaxations to obtain new methods with provableguarantees for classical settings in optimal and robust control. The primary distinction between online nonstochastic control and otherframeworks is the objective. In optimal control, robust control, and othercontrol methodologies that assume stochastic noise, the goal is to performcomparably to an offline optimal strategy. In online nonstochastic control,both the cost functions as well as the perturbations from the assumed dynamicalmodel are chosen by an adversary. Thus the optimal policy is not defined apriori. Rather, the target is to attain low regret against the best policy inhindsight from a benchmark class of policies. This objective suggests the use of the decision making framework of onlineconvex optimization as an algorithmic methodology. The resulting methods arebased on iterative mathematical optimization algorithms, and are accompanied byfinite-time regret and computational complexity guarantees.</description><author>Elad Hazan, Karan Singh</author><pubDate>Mon, 29 May 2023 17:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09619v2</guid></item><item><title>Local Convergence of Gradient Descent-Ascent for Training Generative Adversarial Networks</title><link>http://arxiv.org/abs/2305.08277v2</link><description>Generative Adversarial Networks (GANs) are a popular formulation to traingenerative models for complex high dimensional data. The standard method fortraining GANs involves a gradient descent-ascent (GDA) procedure on a minimaxoptimization problem. This procedure is hard to analyze in general due to thenonlinear nature of the dynamics. We study the local dynamics of GDA fortraining a GAN with a kernel-based discriminator. This convergence analysis isbased on a linearization of a non-linear dynamical system that describes theGDA iterations, under an \textit{isolated points model} assumption from [Beckeret al. 2022]. Our analysis brings out the effect of the learning rates,regularization, and the bandwidth of the kernel discriminator, on the localconvergence rate of GDA. Importantly, we show phase transitions that indicatewhen the system converges, oscillates, or diverges. We also provide numericalsimulations that verify our claims.</description><author>Evan Becker, Parthe Pandit, Sundeep Rangan, Alyson K. Fletcher</author><pubDate>Mon, 29 May 2023 17:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08277v2</guid></item><item><title>Crosslingual Generalization through Multitask Finetuning</title><link>http://arxiv.org/abs/2211.01786v2</link><description>Multitask prompted finetuning (MTF) has been shown to help large languagemodels generalize to new tasks in a zero-shot setting, but so far explorationsof MTF have focused on English data and models. We apply MTF to the pretrainedmultilingual BLOOM and mT5 model families to produce finetuned variants calledBLOOMZ and mT0. We find finetuning large multilingual language models onEnglish tasks with English prompts allows for task generalization tonon-English languages that appear only in the pretraining corpus. Finetuning onmultilingual tasks with English prompts further improves performance on Englishand non-English tasks leading to various state-of-the-art zero-shot results. Wealso investigate finetuning on multilingual tasks with prompts that have beenmachine-translated from English to match the language of each dataset. We findtraining on these machine-translated prompts leads to better performance onhuman-written prompts in the respective languages. Surprisingly, we find modelsare capable of zero-shot generalization to tasks in languages they have neverintentionally seen. We conjecture that the models are learning higher-levelcapabilities that are both task- and language-agnostic. In addition, weintroduce xP3, a composite of supervised datasets in 46 languages with Englishand machine-translated prompts. Our code, datasets and models are freelyavailable at https://github.com/bigscience-workshop/xmtf.</description><author>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel</author><pubDate>Mon, 29 May 2023 17:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01786v2</guid></item><item><title>Solving High-Dimensional PDEs with Latent Spectral Models</title><link>http://arxiv.org/abs/2301.12664v3</link><description>Deep models have achieved impressive progress in solving partial differentialequations (PDEs). A burgeoning paradigm is learning neural operators toapproximate the input-output mappings of PDEs. While previous deep models haveexplored the multiscale architectures and various operator designs, they arelimited to learning the operators as a whole in the coordinate space. In realphysical science problems, PDEs are complex coupled equations with numericalsolvers relying on discretization into high-dimensional coordinate space, whichcannot be precisely approximated by a single operator nor efficiently learneddue to the curse of dimensionality. We present Latent Spectral Models (LSM)toward an efficient and precise solver for high-dimensional PDEs. Going beyondthe coordinate space, LSM enables an attention-based hierarchical projectionnetwork to reduce the high-dimensional data into a compact latent space inlinear time. Inspired by classical spectral methods in numerical analysis, wedesign a neural spectral block to solve PDEs in the latent space thatapproximates complex input-output mappings via learning multiple basisoperators, enjoying nice theoretical guarantees for convergence andapproximation. Experimentally, LSM achieves consistent state-of-the-art andyields a relative gain of 11.5% averaged on seven benchmarks covering bothsolid and fluid physics. Code is available athttps://github.com/thuml/Latent-Spectral-Models.</description><author>Haixu Wu, Tengge Hu, Huakun Luo, Jianmin Wang, Mingsheng Long</author><pubDate>Mon, 29 May 2023 17:30:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12664v3</guid></item><item><title>Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</title><link>http://arxiv.org/abs/2305.18189v1</link><description>To recognize and mitigate harms from large language models (LLMs), we need tounderstand the prevalence and nuances of stereotypes in LLM outputs. Towardthis end, we present Marked Personas, a prompt-based method to measurestereotypes in LLMs for intersectional demographic groups without any lexiconor data labeling. Grounded in the sociolinguistic concept of markedness (whichcharacterizes explicitly linguistically marked categories versus unmarkeddefaults), our proposed method is twofold: 1) prompting an LLM to generatepersonas, i.e., natural language descriptions, of the target demographic groupalongside personas of unmarked, default groups; 2) identifying the words thatsignificantly distinguish personas of the target group from correspondingunmarked ones. We find that the portrayals generated by GPT-3.5 and GPT-4contain higher rates of racial stereotypes than human-written portrayals usingthe same prompts. The words distinguishing personas of marked (non-white,non-male) groups reflect patterns of othering and exoticizing thesedemographics. An intersectional lens further reveals tropes that dominateportrayals of marginalized groups, such as tropicalism and thehypersexualization of minoritized women. These representational harms haveconcerning implications for downstream applications like story generation.</description><author>Myra Cheng, Esin Durmus, Dan Jurafsky</author><pubDate>Mon, 29 May 2023 17:29:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18189v1</guid></item><item><title>Composite Goodness-of-fit Tests with Kernels</title><link>http://arxiv.org/abs/2111.10275v2</link><description>Model misspecification can create significant challenges for theimplementation of probabilistic models, and this has led to development of arange of robust methods which directly account for this issue. However, whetherthese more involved methods are required will depend on whether the model isreally misspecified, and there is a lack of generally applicable methods toanswer this question. In this paper, we propose one such method. Moreprecisely, we propose kernel-based hypothesis tests for the challengingcomposite testing problem, where we are interested in whether the data comesfrom any distribution in some parametric family. Our tests make use of minimumdistance estimators based on the maximum mean discrepancy and the kernel Steindiscrepancy. They are widely applicable, including whenever the density of theparametric model is known up to normalisation constant, or if the model takesthe form of a simulator. As our main result, we show that we are able toestimate the parameter and conduct our test on the same data (without datasplitting), while maintaining a correct test level. Our approach is illustratedon a range of problems, including testing for goodness-of-fit of anunnormalised non-parametric density model, and an intractable generative modelof a biological cellular network.</description><author>Oscar Key, Arthur Gretton, Fran√ßois-Xavier Briol, Tamara Fernandez</author><pubDate>Mon, 29 May 2023 17:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.10275v2</guid></item><item><title>Understanding Predictive Coding as an Adaptive Trust-Region Method</title><link>http://arxiv.org/abs/2305.18188v1</link><description>Predictive coding (PC) is a brain-inspired local learning algorithm that hasrecently been suggested to provide advantages over backpropagation (BP) inbiologically relevant scenarios. While theoretical work has mainly focused onshowing how PC can approximate BP in various limits, the putative benefits of"natural" PC are less understood. Here we develop a theory of PC as an adaptivetrust-region (TR) algorithm that uses second-order information. We show thatthe learning dynamics of PC can be interpreted as interpolating between BP'sloss gradient direction and a TR direction found by the PC inference dynamics.Our theory suggests that PC should escape saddle points faster than BP, aprediction which we prove in a shallow linear model and support withexperiments on deeper networks. This work lays a foundation for understandingPC in deep and wide networks.</description><author>Francesco Innocenti, Ryan Singh, Christopher L. Buckley</author><pubDate>Mon, 29 May 2023 17:25:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18188v1</guid></item><item><title>Syntax and Semantics Meet in the "Middle": Probing the Syntax-Semantics Interface of LMs Through Agentivity</title><link>http://arxiv.org/abs/2305.18185v1</link><description>Recent advances in large language models have prompted researchers to examinetheir abilities across a variety of linguistic tasks, but little has been doneto investigate how models handle the interactions in meaning across words andlarger syntactic forms -- i.e. phenomena at the intersection of syntax andsemantics. We present the semantic notion of agentivity as a case study forprobing such interactions. We created a novel evaluation dataset by utilitizingthe unique linguistic properties of a subset of optionally transitive Englishverbs. This dataset was used to prompt varying sizes of three model classes tosee if they are sensitive to agentivity at the lexical level, and if they canappropriately employ these word-level priors given a specific syntacticcontext. Overall, GPT-3 text-davinci-003 performs extremely well across allexperiments, outperforming all other models tested by far. In fact, the resultsare even better correlated with human judgements than both syntactic andsemantic corpus statistics. This suggests that LMs may potentially serve asmore useful tools for linguistic annotation, theory testing, and discovery thanselect corpora for certain tasks.</description><author>Lindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig</author><pubDate>Mon, 29 May 2023 17:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18185v1</guid></item><item><title>On the Efficacy of Generalization Error Prediction Scoring Functions</title><link>http://arxiv.org/abs/2303.13589v2</link><description>Generalization error predictors (GEPs) aim to predict model performance onunseen distributions by deriving dataset-level error estimates fromsample-level scores. However, GEPs often utilize disparate mechanisms (e.g.,regressors, thresholding functions, calibration datasets, etc), to derive sucherror estimates, which can obfuscate the benefits of a particular scoringfunction. Therefore, in this work, we rigorously study the effectiveness ofpopular scoring functions (confidence, local manifold smoothness, modelagreement), independent of mechanism choice. We find, absent complexmechanisms, that state-of-the-art confidence- and smoothness- based scores failto outperform simple model-agreement scores when estimating error underdistribution shifts and corruptions. Furthermore, on realistic settings wherethe training data has been compromised (e.g., label noise, measurement noise,undersampling), we find that model-agreement scores continue to perform welland that ensemble diversity is important for improving its performance.Finally, to better understand the limitations of scoring functions, wedemonstrate that simplicity bias, or the propensity of deep neural networks torely upon simple but brittle features, can adversely affect GEP performance.Overall, our work carefully studies the effectiveness of popular scoringfunctions in realistic settings and helps to better understand theirlimitations.</description><author>Puja Trivedi, Danai Koutra, Jayaraman J. Thiagarajan</author><pubDate>Mon, 29 May 2023 17:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13589v2</guid></item><item><title>LAMBADA: Backward Chaining for Automated Reasoning in Natural Language</title><link>http://arxiv.org/abs/2212.13894v2</link><description>Remarkable progress has been made on automated reasoning with natural text,by using Language Models (LMs) and methods such as Chain-of-Thought andSelection-Inference. These techniques search for proofs in the forwarddirection from axioms to the conclusion, which suffers from a combinatorialexplosion of the search space, and thus high failure rates for problemsrequiring longer chains of reasoning. The classical automated reasoningliterature has shown that reasoning in the backward direction (i.e. from theintended conclusion to supporting axioms) is significantly more efficient atproof-finding. Importing this intuition into the LM setting, we develop aBackward Chaining algorithm, called LAMBADA, that decomposes reasoning intofour sub-modules. These sub-modules are simply implemented by few-shot promptedLM inference. We show that LAMBADA achieves sizable accuracy boosts overstate-of-the-art forward reasoning methods on challenging logical reasoningdatasets, particularly when deep and accurate proof chains are required.</description><author>Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran</author><pubDate>Mon, 29 May 2023 17:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.13894v2</guid></item><item><title>Global Convergence Rate Analysis of Nonsmooth Nonconvex-Nonconcave Minimax Optimization</title><link>http://arxiv.org/abs/2209.10825v2</link><description>Nonconvex-nonconcave minimax optimization has gained widespread interest overthe last decade. However, most existing work focuses on variants of gradientdescent-ascent (GDA) algorithms, which are only applicable in smoothnonconvex-concave settings. To address this limitation, we propose a novelalgorithm named smoothed proximal linear descent-ascent (smoothed PLDA), whichcan effectively handle a broad range of structured nonsmoothnonconvex-nonconcave minimax problems. Specifically, we consider the settingwhere the primal function has a nonsmooth composite structure and the dualfunction possesses the Kurdyka-\L{}ojasiewicz (K\L{}) property with exponent$\theta \in [0,1)$. We introduce a novel convergence analysis framework forsmoothed PLDA, the key components of which are our newly developed nonsmoothprimal error bound and dual error bound properties. Using this framework, weshow that smoothed PLDA can find both $\epsilon$-game-stationary points and$\epsilon$-optimization-stationary points of the problems of interest in$\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ iterations. Furthermore, when$\theta \in [0,1/2]$, smoothed PLDA achieves the optimal iteration complexityof $\mathcal{O}(\epsilon^{-2})$. To further demonstrate the effectiveness andwide applicability of our analysis framework, we show that certainmax-structure problem possesses the K\L{} property with exponent $\theta=0$under mild assumptions. As a by-product, we establish algorithm-independentquantitative relationships among various stationarity concepts, which may be ofindependent interest.</description><author>Jiajin Li, Linglingzhi Zhu, Anthony Man-Cho So</author><pubDate>Mon, 29 May 2023 17:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.10825v2</guid></item><item><title>Rethinking Counterfactual Data Augmentation Under Confounding</title><link>http://arxiv.org/abs/2305.18183v1</link><description>Counterfactual data augmentation has recently emerged as a method to mitigateconfounding biases in the training data for a machine learning model. Thesebiases, such as spurious correlations, arise due to various observed andunobserved confounding variables in the data generation process. In this paper,we formally analyze how confounding biases impact downstream classifiers andpresent a causal viewpoint to the solutions based on counterfactual dataaugmentation. We explore how removing confounding biases serves as a means tolearn invariant features, ultimately aiding in generalization beyond theobserved data distribution. Additionally, we present a straightforward yetpowerful algorithm for generating counterfactual images, which effectivelymitigates the influence of confounding effects on downstream classifiers.Through experiments on MNIST variants and the CelebA datasets, we demonstratethe effectiveness and practicality of our approach.</description><author>Abbavaram Gowtham Reddy, Saketh Bachu, Saloni Dash, Charchit Sharma, Amit Sharma, Vineeth N Balasubramanian</author><pubDate>Mon, 29 May 2023 17:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18183v1</guid></item><item><title>Perceived Trustworthiness of Natural Language Generators</title><link>http://arxiv.org/abs/2305.18176v1</link><description>Natural Language Generation tools, such as chatbots that can generatehuman-like conversational text, are becoming more common both for personal andprofessional use. However, there are concerns about their trustworthiness andethical implications. The paper addresses the problem of understanding howdifferent users (e.g., linguists, engineers) perceive and adopt these tools andtheir perception of machine-generated text quality. It also discusses theperceived advantages and limitations of Natural Language Generation tools, aswell as users' beliefs on governance strategies. The main findings of thisstudy include the impact of users' field and level of expertise on theperceived trust and adoption of Natural Language Generation tools, the users'assessment of the accuracy, fluency, and potential biases of machine-generatedtext in comparison to human-written text, and an analysis of the advantages andethical risks associated with these tools as identified by the participants.Moreover, this paper discusses the potential implications of these findings forenhancing the AI development process. The paper sheds light on how differentuser characteristics shape their beliefs on the quality and overalltrustworthiness of machine-generated text. Furthermore, it examines thebenefits and risks of these tools from the perspectives of different users.</description><author>Beatriz Cabrero-Daniel, Andrea Sanagust√≠n Cabrero</author><pubDate>Mon, 29 May 2023 17:09:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18176v1</guid></item><item><title>Machine learning based iterative learning control for non-repetitive time-varying systems</title><link>http://arxiv.org/abs/2107.00421v2</link><description>The repetitive tracking task for time-varying systems (TVSs) withnon-repetitive time-varying parameters, which is also called non-repetitiveTVSs, is realized in this paper using iterative learning control (ILC). Amachine learning (ML) based nominal model update mechanism, which utilizes thelinear regression technique to update the nominal model at each ILC trial onlyusing the current trial information, is proposed for non-repetitive TVSs inorder to enhance the ILC performance. Given that the ML mechanism forces themodel uncertainties to remain within the ILC robust tolerance, an ILC updatelaw is proposed to deal with non-repetitive TVSs. How to tune parameters insideML and ILC algorithms to achieve the desired aggregate performance is alsoprovided. The robustness and reliability of the proposed method are verified bysimulations. Comparison with current state-of-the-art demonstrates its superiorcontrol performance in terms of controlling precision. This paper broadens ILCapplications from time-invariant systems to non-repetitive TVSs, adopts MLregression technique to estimate non-repetitive time-varying parameters betweentwo ILC trials and proposes a detailed parameter tuning mechanism to achievedesired performance, which are the main contributions.</description><author>Yiyang Chen, Wei Jiang, Themistoklis Charalambous</author><pubDate>Mon, 29 May 2023 17:04:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.00421v2</guid></item><item><title>Improved Probabilistic Image-Text Representations</title><link>http://arxiv.org/abs/2305.18171v1</link><description>Image-Text Matching (ITM) task, a fundamental vision-language (VL) task,suffers from the inherent ambiguity arising from multiplicity and imperfectannotations. Deterministic functions are not sufficiently powerful to captureambiguity, prompting the exploration of probabilistic embeddings to tackle thechallenge. However, the existing probabilistic ITM approach encounters two keyshortcomings; the burden of heavy computations due to the Monte Carloapproximation, and the loss saturation issue in the face of abundant falsenegatives. To overcome the issues, this paper presents an improvedProbabilistic Cross-Modal Embeddings (named PCME++) by introducing a newprobabilistic distance with a closed-form solution. In addition, twooptimization techniques are proposed to enhance PCME++ further; first, theincorporation of pseudo-positives to prevent the loss saturation problem undermassive false negatives; second, mixed sample data augmentation forprobabilistic matching. Experimental results on MS-COCO Caption and twoextended benchmarks, CxC and ECCV Caption, demonstrate the effectiveness ofPCME++ compared to state-of-the-art ITM methods. The robustness of PCME++ isalso evaluated under noisy image-text correspondences. In addition, thepotential applicability of PCME++ in automatic prompt tuning for zero-shotclassification is shown. The code is available athttps://naver-ai.github.io/pcmepp/.</description><author>Sanghyuk Chun</author><pubDate>Mon, 29 May 2023 17:02:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18171v1</guid></item><item><title>Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning</title><link>http://arxiv.org/abs/2305.18170v1</link><description>Chain-of-thought (CoT) prompting with large language models has proveneffective in numerous natural language processing tasks, but designing promptsthat generalize well to diverse problem types can be challenging, especially inthe context of math word problem (MWP) solving. Additionally, it is common tohave a large amount of training data that have a better diversity coverage butCoT annotations are not available, which limits the use of supervised learningtechniques. To address these issues, we investigate two approaches to leveragethe training data in a few-shot prompting scenario: dynamic program promptingand program distillation. Our approach is largely inspired by Gao et al.,(2022), where they proposed to replace the CoT with the programs as theintermediate reasoning step. Such a prompting strategy allows us to accuratelyverify the answer correctness through program execution in MWP solving. Ourdynamic program prompting involves annotating the training data by samplingcorrect programs from a large language model, while program distillationinvolves adapting a smaller model to the program-annotated training data. Ourexperiments on three standard MWP datasets demonstrate the effectiveness ofthese approaches, yielding significant improvements over previous baselines forprompting and fine-tuning. Our results suggest that leveraging a large amountof training data can improve the generalization ability of prompts and boostthe performance of fine-tuned small models in MWP solving</description><author>Zhanming Jie, Wei Lu</author><pubDate>Mon, 29 May 2023 17:01:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18170v1</guid></item><item><title>LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning</title><link>http://arxiv.org/abs/2305.18169v1</link><description>In recent years, there has been significant progress in developingpre-trained language models for NLP. However, these models often struggle whenfine-tuned on small datasets. To address this issue, researchers have proposedvarious adaptation approaches. Prompt-based tuning is arguably the most commonway, especially for larger models. Previous research shows that addingcontrastive learning to prompt-based fine-tuning is effective as it helps themodel generate embeddings that are more distinguishable between classes, and itcan also be more sample-efficient as the model learns from positive andnegative examples simultaneously. One of the most important components ofcontrastive learning is data augmentation, but unlike computer vision,effective data augmentation for NLP is still challenging. This paper proposesLM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of LanguageModels, which leverages prompt-based few-shot paraphrasing using generativelanguage models, especially large language models such as GPT-3 and OPT-175B,for data augmentation. Our experiments on multiple text classificationbenchmarks show that this augmentation method outperforms other methods, suchas easy data augmentation, back translation, and multiple templates.</description><author>Amirhossein Abaskohi, Sascha Rothe, Yadollah Yaghoobzadeh</author><pubDate>Mon, 29 May 2023 16:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18169v1</guid></item><item><title>Evaluating and Detecting ChatGPT's Responses on Abstractive Summarization</title><link>http://arxiv.org/abs/2303.17650v2</link><description>Large Language Models (LLMs) have gathered significant attention due to theirimpressive performance on a variety of tasks. ChatGPT, developed by OpenAI, isa recent addition to the family of language models and is being called adisruptive technology by a few, owing to its human-like text-generationcapabilities. Although, many anecdotal examples across the internet haveevaluated ChatGPT's strength and weakness, only a few systematic researchstudies exist. To contribute to the body of literature of systematic researchon ChatGPT, we evaluate the performance of ChatGPT on Abstractive Summarizationby the means of automated metrics and blinded human reviewers. We also buildautomatic text classifiers to detect ChatGPT generated summaries. We found thatwhile text classification algorithms can distinguish between real and generatedsummaries, humans are unable to distinguish between real summaries and thoseproduced by ChatGPT.</description><author>Mayank Soni, Vincent Wade</author><pubDate>Mon, 29 May 2023 16:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17650v2</guid></item><item><title>An Emergency Disposal Decision-making Method with Human--Machine Collaboration</title><link>http://arxiv.org/abs/2305.18165v1</link><description>Rapid developments in artificial intelligence technology have led to unmannedsystems replacing human beings in many fields requiring high-precisionpredictions and decisions. In modern operational environments, all job plansare affected by emergency events such as equipment failures and resourceshortages, making a quick resolution critical. The use of unmanned systems toassist decision-making can improve resolution efficiency, but theirdecision-making is not interpretable and may make the wrong decisions. Currentunmanned systems require human supervision and control. Based on this, wepropose a collaborative human--machine method for resolving unplanned eventsusing two phases: task filtering and task scheduling. In the task filteringphase, we propose a human--machine collaborative decision-making algorithm fordynamic tasks. The GACRNN model is used to predict the state of the job nodes,locate the key nodes, and generate a machine-predicted resolution task list. Ahuman decision-maker supervises the list in real time and modifies and confirmsthe machine-predicted list through the human--machine interface. In the taskscheduling phase, we propose a scheduling algorithm that integrates humanexperience constraints. The steps to resolve an event are inserted into thenormal job sequence to schedule the resolution. We propose severalhuman--machine collaboration methods in each phase to generate steps to resolvean unplanned event while minimizing the impact on the original job plan.</description><author>Yibo Guo, Jingyi Xue, Yingkang Zhang, Mingliang Xu</author><pubDate>Mon, 29 May 2023 16:53:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18165v1</guid></item><item><title>Generative Adversarial Networks based Skin Lesion Segmentation</title><link>http://arxiv.org/abs/2305.18164v1</link><description>Skin cancer is a serious condition that requires accurate identification andtreatment. One way to assist clinicians in this task is by using computer-aideddiagnosis (CAD) tools that can automatically segment skin lesions fromdermoscopic images. To this end, a new adversarial learning-based frameworkcalled EGAN has been developed. This framework uses an unsupervised generativenetwork to generate accurate lesion masks. It consists of a generator modulewith a top-down squeeze excitation-based compound scaled path and an asymmetriclateral connection-based bottom-up path, and a discriminator module thatdistinguishes between original and synthetic masks. Additionally, amorphology-based smoothing loss is implemented to encourage the network tocreate smooth semantic boundaries of lesions. The framework is evaluated on theInternational Skin Imaging Collaboration (ISIC) Lesion Dataset 2018 andoutperforms the current state-of-the-art skin lesion segmentation approacheswith a Dice coefficient, Jaccard similarity, and Accuracy of 90.1%, 83.6%, and94.5%, respectively. This represents a 2% increase in Dice Coefficient, 1%increase in Jaccard Index, and 1% increase in Accuracy.</description><author>Shubham Innani, Prasad Dutande, Bhakti Baheti, Venu Pokuri, Ujjwal Baid, Sanjay Talbar, Sharath Chandra Guntuku</author><pubDate>Mon, 29 May 2023 16:51:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18164v1</guid></item><item><title>Compressed Empirical Measures (in finite dimensions)</title><link>http://arxiv.org/abs/2204.08847v2</link><description>We study approaches for compressing the empirical measure in the context offinite dimensional reproducing kernel Hilbert spaces (RKHSs).In this context,the empirical measure is contained within a natural convex set and can beapproximated using convex optimization methods.Such an approximation givesunder certain conditions rise to a coreset of data points. A key quantity thatcontrols how large such a coreset has to be is the size of the largest ballaround the empirical measure that is contained within the empirical convex set.The bulk of our work is concerned with deriving high probability lower boundson the size of such a ball under various conditions. We complement thisderivation of the lower bound by developing techniques that allow us to applythe compression approach to concrete inference problems such as kernel ridgeregression. We conclude with a construction of an infinite dimensional RKHS forwhich the compression is poor, highlighting some of the difficulties one faceswhen trying to move to infinite dimensional RKHSs.</description><author>Steffen Gr√ºnew√§lder</author><pubDate>Mon, 29 May 2023 16:50:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.08847v2</guid></item><item><title>Compact Real-time Radiance Fields with Neural Codebook</title><link>http://arxiv.org/abs/2305.18163v1</link><description>Reconstructing neural radiance fields with explicit volumetricrepresentations, demonstrated by Plenoxels, has shown remarkable advantages ontraining and rendering efficiency, while grid-based representations typicallyinduce considerable overhead for storage and transmission. In this work, wepresent a simple and effective framework for pursuing compact radiance fieldsfrom the perspective of compression methodology. By exploiting intrinsicproperties exhibiting in grid models, a non-uniform compression stem isdeveloped to significantly reduce model complexity and a novel parameterizedmodule, named Neural Codebook, is introduced for better encoding high-frequencydetails specific to per-scene models via a fast optimization. Our approach canachieve over 40 $\times$ reduction on grid model storage with competitiverendering quality. In addition, the method can achieve real-time renderingspeed with 180 fps, realizing significant advantage on storage cost compared toreal-time rendering methods.</description><author>Lingzhi Li, Zhongshu Wang, Zhen Shen, Li Shen, Ping Tan</author><pubDate>Mon, 29 May 2023 16:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18163v1</guid></item><item><title>Supervised learning with probabilistic morphisms and kernel mean embeddings</title><link>http://arxiv.org/abs/2305.06348v3</link><description>In this paper I propose a concept of a correct loss function in a generativemodel of supervised learning for an input space $\mathcal{X}$ and a label space$\mathcal{Y}$, both of which are measurable spaces. A correct loss function ina generative model of supervised learning must accurately measure thediscrepancy between elements of a hypothesis space $\mathcal{H}$ of possiblepredictors and the supervisor operator, even when the supervisor operator doesnot belong to $\mathcal{H}$. To define correct loss functions, I propose acharacterization of a regular conditional probability measure$\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X}\times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}:\mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operatorequation. If $\mathcal{Y}$ is a separable metrizable topological space with theBorel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose an additionalcharacterization of a regular conditional probability measure$\mu_{\mathcal{Y}|\mathcal{X}}$ as a minimizer of mean square error on thespace of Markov kernels, referred to as probabilistic morphisms, from$\mathcal{X}$ to $\mathcal{Y}$. This characterization utilizes kernel meanembeddings. Building upon these results and employing inner measure to quantifythe generalizability of a learning algorithm, I extend a result due toCucker-Smale, which addresses the learnability of a regression model, to thesetting of a conditional probability estimation problem. Additionally, Ipresent a variant of Vapnik's regularization method for solving stochasticill-posed problems, incorporating inner measure, and showcase its applications.</description><author>H√¥ng V√¢n L√™</author><pubDate>Mon, 29 May 2023 16:48:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06348v3</guid></item><item><title>Autonomous GIS: the next-generation AI-powered GIS</title><link>http://arxiv.org/abs/2305.06453v4</link><description>Large Language Models (LLMs), such as ChatGPT, demonstrate a strongunderstanding of human natural language and have been explored and applied invarious fields, including reasoning, creative writing, code generation,translation, and information retrieval. By adopting LLM as the reasoning core,we introduce Autonomous GIS as an AI-powered geographic information system(GIS) that leverages the LLM's general abilities in natural languageunderstanding, reasoning, and coding for addressing spatial problems withautomatic spatial data collection, analysis, and visualization. We envisionthat autonomous GIS will need to achieve five autonomous goals:self-generating, self-organizing, self-verifying, self-executing, andself-growing. We developed a prototype system called LLM-Geo using the GPT-4API in a Python environment, demonstrating what an autonomous GIS looks likeand how it delivers expected results without human intervention using threecase studies. For all case studies, LLM-Geo was able to return accurateresults, including aggregated numbers, graphs, and maps, significantly reducingmanual operation time. Although still in its infancy and lacking severalimportant modules such as logging and code testing, LLM-Geo demonstrates apotential path toward the next-generation AI-powered GIS. We advocate for theGIScience community to dedicate more effort to the research and development ofautonomous GIS, making spatial analysis easier, faster, and more accessible toa broader audience.</description><author>Zhenlong Li, Huan Ning</author><pubDate>Mon, 29 May 2023 16:45:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06453v4</guid></item><item><title>VA-learning as a more efficient alternative to Q-learning</title><link>http://arxiv.org/abs/2305.18161v1</link><description>In reinforcement learning, the advantage function is critical for policyimprovement, but is often extracted from a learned Q-function. A naturalquestion is: Why not learn the advantage function directly? In this work, weintroduce VA-learning, which directly learns advantage function and valuefunction using bootstrapping, without explicit reference to Q-functions.VA-learning learns off-policy and enjoys similar theoretical guarantees asQ-learning. Thanks to the direct learning of advantage function and valuefunction, VA-learning improves the sample efficiency over Q-learning both intabular implementations and deep RL agents on Atari-57 games. We also identifya close connection between VA-learning and the dueling architecture, whichpartially explains why a simple architectural change to DQN agents tends toimprove performance.</description><author>Yunhao Tang, R√©mi Munos, Mark Rowland, Michal Valko</author><pubDate>Mon, 29 May 2023 16:44:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18161v1</guid></item><item><title>lilGym: Natural Language Visual Reasoning with Reinforcement Learning</title><link>http://arxiv.org/abs/2211.01994v3</link><description>We present lilGym, a new benchmark for language-conditioned reinforcementlearning in visual environments. lilGym is based on 2,661 highly-compositionalhuman-written natural language statements grounded in an interactive visualenvironment. We introduce a new approach for exact reward computation in everypossible world state by annotating all statements with executable Pythonprograms. Each statement is paired with multiple start states and rewardfunctions to form thousands of distinct Markov Decision Processes of varyingdifficulty. We experiment with lilGym with different models and learningregimes. Our results and analysis show that while existing methods are able toachieve non-trivial performance, lilGym forms a challenging open problem.lilGym is available at https://lil.nlp.cornell.edu/lilgym/.</description><author>Anne Wu, Kiant√© Brantley, Noriyuki Kojima, Yoav Artzi</author><pubDate>Mon, 29 May 2023 16:44:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01994v3</guid></item><item><title>Counterpart Fairness -- Addressing Systematic between-group Differences in Fairness Evaluation</title><link>http://arxiv.org/abs/2305.18160v1</link><description>When using machine learning (ML) to aid decision-making, it is critical toensure that an algorithmic decision is fair, i.e., it does not discriminateagainst specific individuals/groups, particularly those from underprivilegedpopulations. Existing group fairness methods require equal group-wise measures,which however fails to consider systematic between-group differences. Theconfounding factors, which are non-sensitive variables but manifest systematicdifferences, can significantly affect fairness evaluation. To mitigate thisproblem, we believe that a fairness measurement should be based on thecomparison between counterparts (i.e., individuals who are similar to eachother with respect to the task of interest) from different groups, whose groupidentities cannot be distinguished algorithmically by exploring confoundingfactors. We have developed a propensity-score-based method for identifyingcounterparts, which prevents fairness evaluation from comparing "oranges" with"apples". In addition, we propose a counterpart-based statistical fairnessindex, termed Counterpart-Fairness (CFair), to assess fairness of ML models.Empirical studies on the Medical Information Mart for Intensive Care (MIMIC)-IVdatabase were conducted to validate the effectiveness of CFair. We publish ourcode at \url{https://github.com/zhengyjo/CFair}.</description><author>Yifei Wang, Zhengyang Zhou, Liqin Wang, John Laurentiev, Peter Hou, Li Zhou, Pengyu Hong</author><pubDate>Mon, 29 May 2023 16:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18160v1</guid></item><item><title>Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning</title><link>http://arxiv.org/abs/2305.18158v1</link><description>Recent advances in robust semi-supervised learning (SSL) typically filterout-of-distribution (OOD) information at the sample level. We argue that anoverlooked problem of robust SSL is its corrupted information on semanticlevel, practically limiting the development of the field. In this paper, wetake an initial step to explore and propose a unified framework termed OODSemantic Pruning (OSP), which aims at pruning OOD semantics out fromin-distribution (ID) features. Specifically, (i) we propose an aliasing OODmatching module to pair each ID sample with an OOD sample with semanticoverlap. (ii) We design a soft orthogonality regularization, which firsttransforms each ID feature by suppressing its semantic component that iscollinear with paired OOD sample. It then forces the predictions before andafter soft orthogonality decomposition to be consistent. Being practicallysimple, our method shows a strong performance in OOD detection and IDclassification on challenging benchmarks. In particular, OSP surpasses theprevious state-of-the-art by 13.7% on accuracy for ID classification and 5.9%on AUROC for OOD detection on TinyImageNet dataset. The source codes arepublicly available at https://github.com/rain305f/OSP.</description><author>Yu Wang, Pengchong, Qiao, Chang Liu, Guoli Song, Xiawu Zheng, Jie Chen</author><pubDate>Mon, 29 May 2023 16:37:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18158v1</guid></item><item><title>Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods</title><link>http://arxiv.org/abs/2305.18156v1</link><description>Large-scale pre-trained language models such as GPT-3 have shown remarkableperformance across various natural language processing tasks. However, applyingprompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasksand their controllability remains underexplored. Controllability in GEC iscrucial for real-world applications, particularly in educational settings,where the ability to tailor feedback according to learner levels and specificerror types can significantly enhance the learning process. This paperinvestigates the performance and controllability of prompt-based methods withGPT-3 for GEC tasks using zero-shot and few-shot setting. We explore the impactof task instructions and examples on GPT-3's output, focusing on controllingaspects such as minimal edits, fluency edits, and learner levels. Our findingsdemonstrate that GPT-3 could effectively perform GEC tasks, outperformingexisting supervised and unsupervised approaches. We also showed that GPT-3could achieve controllability when appropriate task instructions and examplesare given.</description><author>Mengsay Loem, Masahiro Kaneko, Sho Takase, Naoaki Okazaki</author><pubDate>Mon, 29 May 2023 16:31:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18156v1</guid></item><item><title>Do Large Language Models Know What They Don't Know?</title><link>http://arxiv.org/abs/2305.18153v1</link><description>Large language models (LLMs) have a wealth of knowledge that allows them toexcel in various Natural Language Processing (NLP) tasks. Current researchfocuses on enhancing their performance within their existing knowledge. Despitetheir vast knowledge, LLMs are still limited by the amount of information theycan accommodate and comprehend. Therefore, the ability to understand their ownlimitations on the unknows, referred to as self-knowledge, is of paramountimportance. This study aims to evaluate LLMs' self-knowledge by assessing theirability to identify unanswerable or unknowable questions. We introduce anautomated methodology to detect uncertainty in the responses of these models,providing a novel measure of their self-knowledge. We further introduce aunique dataset, SelfAware, consisting of unanswerable questions from fivediverse categories and their answerable counterparts. Our extensive analysis,involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering anintrinsic capacity for self-knowledge within these models. Moreover, wedemonstrate that in-context learning and instruction tuning can further enhancethis self-knowledge. Despite this promising insight, our findings alsohighlight a considerable gap between the capabilities of these models and humanproficiency in recognizing the limits of their knowledge.</description><author>Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuanjing Huang</author><pubDate>Mon, 29 May 2023 16:30:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18153v1</guid></item><item><title>Extrinsic Factors Affecting the Accuracy of Biomedical NER</title><link>http://arxiv.org/abs/2305.18152v1</link><description>Biomedical named entity recognition (NER) is a critial task that aims toidentify structured information in clinical text, which is often replete withcomplex, technical terms and a high degree of variability. Accurate andreliable NER can facilitate the extraction and analysis of important biomedicalinformation, which can be used to improve downstream applications including thehealthcare system. However, NER in the biomedical domain is challenging due tolimited data availability, as the high expertise, time, and expenses arerequired to annotate its data. In this paper, by using the limited data, weexplore various extrinsic factors including the corpus annotation scheme, dataaugmentation techniques, semi-supervised learning and Brill transformation, toimprove the performance of a NER model on a clinical text dataset (i2b2 2012,\citet{sun-rumshisky-uzuner:2013}). Our experiments demonstrate that theseapproaches can significantly improve the model's F1 score from original 73.74to 77.55. Our findings suggest that considering different extrinsic factors andcombining these techniques is a promising approach for improving NERperformance in the biomedical domain where the size of data is limited.</description><author>Zhiyi Li, Shengjie Zhang, Yujie Song, Jungyeul Park</author><pubDate>Mon, 29 May 2023 16:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18152v1</guid></item><item><title>On the Privacy-Robustness-Utility Trilemma in Distributed Learning</title><link>http://arxiv.org/abs/2302.04787v2</link><description>The ubiquity of distributed machine learning (ML) in sensitive public domainapplications calls for algorithms that protect data privacy, while being robustto faults and adversarial behaviors. Although privacy and robustness have beenextensively studied independently in distributed ML, their synthesis remainspoorly understood. We present the first tight analysis of the error incurred byany algorithm ensuring robustness against a fraction of adversarial machines,as well as differential privacy (DP) for honest machines' data against anyother curious entity. Our analysis exhibits a fundamental trade-off betweenprivacy, robustness, and utility. To prove our lower bound, we consider thecase of mean estimation, subject to distributed DP and robustness constraints,and devise reductions to centralized estimation of one-way marginals. We proveour matching upper bound by presenting a new distributed ML algorithm using ahigh-dimensional robust aggregation rule. The latter amortizes the dependenceon the dimension in the error (caused by adversarial workers and DP), whilebeing agnostic to the statistical properties of the data.</description><author>Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, John Stephan</author><pubDate>Mon, 29 May 2023 16:27:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04787v2</guid></item><item><title>Multiscale Positive-Unlabeled Detection of AI-Generated Texts</title><link>http://arxiv.org/abs/2305.18149v1</link><description>Recent releases of Large Language Models (LLMs), e.g. ChatGPT, areastonishing at generating human-like texts, but they may get misused for fakescholarly texts, fake news, fake tweets, et cetera. Previous works haveproposed methods to detect these multiscale AI-generated texts, includingsimple ML classifiers, pretrained-model-based training-agnostic methods, andfinetuned language classification models. However, mainstream detectors areformulated without considering the factor of corpus length: shorter corpusesare harder to detect compared with longer ones for shortage of informativefeatures. In this paper, a Multiscale Positive-Unlabeled (MPU) trainingframework is proposed to address the challenge of multiscale text detection.Firstly, we acknowledge the human-resemblance property of short machine texts,and rephrase text classification as a Positive-Unlabeled (PU) problem bymarking these short machine texts as "unlabeled" during training. In this PUcontext, we propose the length-sensitive Multiscale PU Loss, where we use arecurrent model in abstraction to estimate positive priors of scale-variantcorpuses. Additionally, we introduce a Text Multiscaling module to enrichtraining corpuses. Experiments show that our MPU method augments detectionperformance on long AI-generated text, and significantly improves short-corpusdetection of language model detectors. Language Models trained with MPU couldoutcompete existing detectors by large margins on multiscale AI-generatedtexts. The codes are available athttps://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgptand https://github.com/huawei-noah/Efficient-Computing/AIGC_text_detector.</description><author>Yuchuan Tian, Hanting Chen, Xutao Wang, Zheyuan Bai, Qinghua Zhang, Ruifeng Li, Chao Xu, Yunhe Wang</author><pubDate>Mon, 29 May 2023 16:25:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18149v1</guid></item><item><title>ME-GAN: Learning Panoptic Electrocardio Representations for Multi-view ECG Synthesis Conditioned on Heart Diseases</title><link>http://arxiv.org/abs/2207.10670v2</link><description>Electrocardiogram (ECG) is a widely used non-invasive diagnostic tool forheart diseases. Many studies have devised ECG analysis models (e.g.,classifiers) to assist diagnosis. As an upstream task, researches have builtgenerative models to synthesize ECG data, which are beneficial to providingtraining samples, privacy protection, and annotation reduction. However,previous generative methods for ECG often neither synthesized multi-view data,nor dealt with heart disease conditions. In this paper, we propose a noveldisease-aware generative adversarial network for multi-view ECG synthesiscalled ME-GAN, which attains panoptic electrocardio representations conditionedon heart diseases and projects the representations onto multiple standard viewsto yield ECG signals. Since ECG manifestations of heart diseases are oftenlocalized in specific waveforms, we propose a new "mixup normalization" toinject disease information precisely into suitable locations. In addition, wepropose a view discriminator to revert disordered ECG views into apre-determined order, supervising the generator to obtain ECG representingcorrect view characteristics. Besides, a new metric, rFID, is presented toassess the quality of the synthesized ECG signals. Comprehensive experimentsverify that our ME-GAN performs well on multi-view ECG signal synthesis withtrusty morbid manifestations.</description><author>Jintai Chen, Kuanlun Liao, Kun Wei, Haochao Ying, Danny Z. Chen, Jian Wu</author><pubDate>Mon, 29 May 2023 16:22:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.10670v2</guid></item><item><title>GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking</title><link>http://arxiv.org/abs/2305.18144v1</link><description>Retrieval-enhanced text generation, which aims to leverage passages retrievedfrom a large passage corpus for delivering a proper answer given the inputquery, has shown remarkable progress on knowledge-intensive language tasks suchas open-domain question answering and knowledge-enhanced dialogue generation.However, the retrieved passages are not ideal for guiding answer generationbecause of the discrepancy between retrieval and generation, i.e., thecandidate passages are all treated equally during the retrieval procedurewithout considering their potential to generate the proper answers. Thisdiscrepancy makes a passage retriever deliver a sub-optimal collection ofcandidate passages to generate answers. In this paper, we propose theGeneRative Knowledge Improved Passage Ranking (GripRank) approach, addressingthe above challenge by distilling knowledge from a generative passage estimator(GPE) to a passage ranker, where the GPE is a generative language model used tomeasure how likely the candidate passages can generate the proper answer. Werealize the distillation procedure by teaching the passage ranker learning torank the passages ordered by the GPE. Furthermore, we improve the distillationquality by devising a curriculum knowledge distillation mechanism, which allowsthe knowledge provided by the GPE can be progressively distilled to the rankerthrough an easy-to-hard curriculum, enabling the passage ranker to correctlyrecognize the provenance of the answer from many plausible candidates. Weconduct extensive experiments on four datasets across three knowledge-intensivelanguage tasks. Experimental results show advantages over the state-of-the-artmethods for both passage ranking and answer generation on the KILT benchmark.</description><author>Jiaqi Bai, Hongcheng Guo, Jiaheng Liu, Jian Yang, Xinnian Liang, Zhao Yan, Zhoujun Li</author><pubDate>Mon, 29 May 2023 16:15:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18144v1</guid></item><item><title>Reason to explain: Interactive contrastive explanations (REASONX)</title><link>http://arxiv.org/abs/2305.18143v1</link><description>Many high-performing machine learning models are not interpretable. As theyare increasingly used in decision scenarios that can critically affectindividuals, it is necessary to develop tools to better understand theiroutputs. Popular explanation methods include contrastive explanations. However,they suffer several shortcomings, among others an insufficient incorporation ofbackground knowledge, and a lack of interactivity. While (dialogue-like)interactivity is important to better communicate an explanation, backgroundknowledge has the potential to significantly improve their quality, e.g., byadapting the explanation to the needs of the end-user. To close this gap, wepresent REASONX, an explanation tool based on Constraint Logic Programming(CLP). REASONX provides interactive contrastive explanations that can beaugmented by background knowledge, and allows to operate under a setting ofunder-specified information, leading to increased flexibility in the providedexplanations. REASONX computes factual and constrative decision rules, as wellas closest constrative examples. It provides explanations for decision trees,which can be the ML models under analysis, or global/local surrogate models ofany ML model. While the core part of REASONX is built on CLP, we also provide aprogram layer that allows to compute the explanations via Python, making thetool accessible to a wider audience. We illustrate the capability of REASONX ona synthetic data set, and on a a well-developed example in the credit domain.In both cases, we can show how REASONX can be flexibly used and tailored to theneeds of the user.</description><author>Laura State, Salvatore Ruggieri, Franco Turini</author><pubDate>Mon, 29 May 2023 16:13:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18143v1</guid></item><item><title>Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration</title><link>http://arxiv.org/abs/2104.01231v6</link><description>Deep neural networks achieve high prediction accuracy when the train and testdistributions coincide. In practice though, various types of corruptions occurwhich deviate from this setup and cause severe performance degradations. Fewmethods have been proposed to address generalization in the presence ofunforeseen domain shifts. In particular, digital noise corruptions arisecommonly in practice during the image acquisition stage and present asignificant challenge for current methods. In this paper, we propose a diverseGaussian noise consistency regularization method for improving robustness ofimage classifiers under a variety of corruptions while still maintaining highclean accuracy. We derive bounds to motivate and understand the behavior of ourGaussian noise consistency regularization using a local loss landscapeanalysis. Our approach improves robustness against unforeseen noise corruptionsby 4.2-18.4% over adversarial training and other strong diverse dataaugmentation baselines across several benchmarks. Furthermore, it improvesrobustness and uncertainty calibration by 3.7% and 5.5%, respectively, againstall common corruptions (weather, digital, blur, noise) when combined withstate-of-the-art diverse data augmentations.</description><author>Theodoros Tsiligkaridis, Athanasios Tsiligkaridis</author><pubDate>Mon, 29 May 2023 16:06:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.01231v6</guid></item><item><title>Alignment-free HDR Deghosting with Semantics Consistent Transformer</title><link>http://arxiv.org/abs/2305.18135v1</link><description>High dynamic range (HDR) imaging aims to retrieve information from multiplelow-dynamic range inputs to generate realistic output. The essence is toleverage the contextual information, including both dynamic and staticsemantics, for better image generation. Existing methods often focus on thespatial misalignment across input frames caused by the foreground and/or cameramotion. However, there is no research on jointly leveraging the dynamic andstatic context in a simultaneous manner. To delve into this problem, we proposea novel alignment-free network with a Semantics Consistent Transformer (SCTNet)with both spatial and channel attention modules in the network. The spatialattention aims to deal with the intra-image correlation to model the dynamicmotion, while the channel attention enables the inter-image intertwining toenhance the semantic consistency across frames. Aside from this, we introduce anovel realistic HDR dataset with more variations in foreground objects,environmental factors, and larger motions. Extensive comparisons on bothconventional datasets and ours validate the effectiveness of our method,achieving the best trade-off on the performance and the computational cost.</description><author>Steven Tel, Zongwei Wu, Yulun Zhang, Barth√©l√©my Heyrman, C√©dric Demonceaux, Radu Timofte, Dominique Ginhac</author><pubDate>Mon, 29 May 2023 16:03:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18135v1</guid></item><item><title>Network-Aided Intelligent Traffic Steering in 6G O-RAN: A Multi-Layer Optimization Framework</title><link>http://arxiv.org/abs/2302.02711v2</link><description>To enable an intelligent, programmable and multi-vendor radio access network(RAN) for 6G networks, considerable efforts have been made in standardizationand development of open RAN (O-RAN). So far, however, the applicability ofO-RAN in controlling and optimizing RAN functions has not been widelyinvestigated. In this paper, we jointly optimize the flow-split distribution,congestion control and scheduling (JFCS) to enable an intelligent trafficsteering application in O-RAN. Combining tools from network utilitymaximization and stochastic optimization, we introduce a multi-layeroptimization framework that provides fast convergence, long-termutility-optimality and significant delay reduction compared to thestate-of-the-art and baseline RAN approaches. Our main contributions arethree-fold: i) we propose the novel JFCS framework to efficiently andadaptively direct traffic to appropriate radio units; ii) we developlow-complexity algorithms based on the reinforcement learning, innerapproximation and bisection search methods to effectively solve the JFCSproblem in different time scales; and iii) the rigorous theoretical performanceresults are analyzed to show that there exists a scaling factor to improve thetradeoff between delay and utility-optimization. Collectively, the insights inthis work will open the door towards fully automated networks with enhancedcontrol and flexibility. Numerical results are provided to demonstrate theeffectiveness of the proposed algorithms in terms of the convergence rate,long-term utility-optimality and delay reduction.</description><author>Van-Dinh Nguyen, Thang X. Vu, Nhan Thanh Nguyen, Dinh C. Nguyen, Markku Juntti, Nguyen Cong Luong, Dinh Thai Hoang, Diep N. Nguyen, Symeon Chatzinotas</author><pubDate>Mon, 29 May 2023 15:54:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02711v2</guid></item><item><title>Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization</title><link>http://arxiv.org/abs/2212.12978v3</link><description>Nonconvex-nonconcave minimax optimization has received intense attention overthe last decade due to its broad applications in machine learning.Unfortunately, most existing algorithms cannot be guaranteed to convergeglobally and even suffer from limit cycles. To address this issue, we propose anovel single-loop algorithm called doubly smoothed gradient descent ascentmethod (DSGDA), which naturally balances the primal and dual updates. Theproposed DSGDA can get rid of limit cycles in various challengingnonconvex-nonconcave examples in the literature, including Forsaken,Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We furthershow that under an one-sided Kurdyka-\L{}ojasiewicz condition with exponent$\theta\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find agame-stationary point with an iteration complexity of$\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ (resp.$\mathcal{O}(\epsilon^{-4})$). These match the best results for single-loopalgorithms that solve nonconvex-concave or convex-nonconcave minimax problems,or problems satisfying the rather restrictive one-sided Polyak-\L{}ojasiewiczcondition. Our work demonstrates, for the first time, the possibility of havinga simple and unified single-loop algorithm for solving nonconvex-nonconcave,nonconvex-concave, and convex-nonconcave minimax problems.</description><author>Taoli Zheng, Linglingzhi Zhu, Anthony Man-Cho So, Jose Blanchet, Jiajin Li</author><pubDate>Mon, 29 May 2023 15:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12978v3</guid></item><item><title>ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback</title><link>http://arxiv.org/abs/2305.18090v1</link><description>Recent advancements in conversational large language models (LLMs), such asChatGPT, have demonstrated remarkable promise in various domains, includingdrug discovery. However, existing works mainly focus on investigating thecapabilities of conversational LLMs on chemical reaction and retrosynthesis.While drug editing, a critical task in the drug discovery pipeline, remainslargely unexplored. To bridge this gap, we propose ChatDrug, a framework tofacilitate the systematic investigation of drug editing using LLMs. ChatDrugjointly leverages a prompt module, a retrieval and domain feedback (ReDF)module, and a conversation module to streamline effective drug editing. Weempirically show that ChatDrug reaches the best performance on 33 out of 39drug editing tasks, encompassing small molecules, peptides, and proteins. Wefurther demonstrate, through 10 case studies, that ChatDrug can successfullyidentify the key substructures (e.g., the molecule functional groups, peptidemotifs, and protein structures) for manipulation, generating diverse and validsuggestions for drug editing. Promisingly, we also show that ChatDrug can offerinsightful explanations from a domain-specific perspective, enhancinginterpretability and enabling informed decision-making. This research shedslight on the potential of ChatGPT and conversational LLMs for drug editing. Itpaves the way for a more efficient and collaborative drug discovery pipeline,contributing to the advancement of pharmaceutical research and development.</description><author>Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao</author><pubDate>Mon, 29 May 2023 15:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18090v1</guid></item><item><title>Detecting hidden confounding in observational data using multiple environments</title><link>http://arxiv.org/abs/2205.13935v3</link><description>A common assumption in causal inference from observational data is that thereis no hidden confounding. Yet it is, in general, impossible to verify thisassumption from a single dataset. Under the assumption of independent causalmechanisms underlying the data-generating process, we demonstrate a way todetect unobserved confounders when having multiple observational datasetscoming from different environments. We present a theory for testableconditional independencies that are only absent when there is hiddenconfounding and examine cases where we violate its assumptions: degenerate &amp;dependent mechanisms, and faithfulness violations. Additionally, we propose aprocedure to test these independencies and study its empirical finite-samplebehavior using simulation studies and semi-synthetic data based on a real-worlddataset. In most cases, the proposed procedure correctly predicts the presenceof hidden confounding, particularly when the confounding bias is large.</description><author>Rickard K. A. Karlsson, Jesse H. Krijthe</author><pubDate>Mon, 29 May 2023 15:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.13935v3</guid></item><item><title>SketchFFusion: Sketch-guided image editing with diffusion model</title><link>http://arxiv.org/abs/2304.03174v2</link><description>Sketch-guided image editing aims to achieve local fine-tuning of the imagebased on the sketch information provided by the user, while maintaining theoriginal status of the unedited areas. Due to the high cost of acquiring humansketches, previous works mostly relied on edge maps as a substitute forsketches, but sketches possess more rich structural information. In this paper,we propose a sketch generation scheme that can preserve the main contours of animage and closely adhere to the actual sketch style drawn by the user.Simultaneously, current image editing methods often face challenges such asimage distortion, training cost, and loss of fine details in the sketch. Toaddress these limitations, We propose a conditional diffusion model(SketchFFusion) based on the sketch structure vector. We evaluate thegenerative performance of our model and demonstrate that it outperformsexisting methods.</description><author>Weihang Mao, Bo Han, Zihao Wang</author><pubDate>Mon, 29 May 2023 15:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03174v2</guid></item><item><title>Extremal Domain Translation with Neural Optimal Transport</title><link>http://arxiv.org/abs/2301.12874v2</link><description>We propose the extremal transport (ET) which is a mathematical formalizationof the theoretically best possible unpaired translation between a pair ofdomains w.r.t. the given similarity function. Inspired by the recent advancesin neural optimal transport (OT), we propose a scalable algorithm toapproximate ET maps as a limit of partial OT maps. We test our algorithm on toyexamples and on the unpaired image-to-image translation task.</description><author>Milena Gazdieva, Alexander Korotin, Daniil Selikhanovych, Evgeny Burnaev</author><pubDate>Mon, 29 May 2023 15:33:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12874v2</guid></item><item><title>TD-GEM: Text-Driven Garment Editing Mapper</title><link>http://arxiv.org/abs/2305.18120v1</link><description>Language-based fashion image editing allows users to try out variations ofdesired garments through provided text prompts. Inspired by research onmanipulating latent representations in StyleCLIP and HairCLIP, we focus onthese latent spaces for editing fashion items of full-body human datasets.Currently, there is a gap in handling fashion image editing due to thecomplexity of garment shapes and textures and the diversity of human poses. Inthis paper, we propose an editing optimizer scheme method called Text-DrivenGarment Editing Mapper (TD-GEM), aiming to edit fashion items in a disentangledway. To this end, we initially obtain a latent representation of an imagethrough generative adversarial network inversions such as Encoder for Editing(e4e) or Pivotal Tuning Inversion (PTI) for more accurate results. Anoptimization-based Contrasive Language-Image Pre-training (CLIP) is thenutilized to guide the latent representation of a fashion image in the directionof a target attribute expressed in terms of a text prompt. Our TD-GEMmanipulates the image accurately according to the target attribute, while otherparts of the image are kept untouched. In the experiments, we evaluate TD-GEMon two different attributes (i.e., "color" and "sleeve length"), whicheffectively generates realistic images compared to the recent manipulationschemes.</description><author>Reza Dadfar, Sanaz Sabzevari, M√•rten Bj√∂rkman, Danica Kragic</author><pubDate>Mon, 29 May 2023 15:31:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18120v1</guid></item><item><title>The minimax risk in testing the histogram of discrete distributions for uniformity under missing ball alternatives</title><link>http://arxiv.org/abs/2305.18111v1</link><description>We consider the problem of testing the fit of a discrete sample of items frommany categories to the uniform distribution over the categories. As a class ofalternative hypotheses, we consider the removal of an $\ell_p$ ball of radius$\epsilon$ around the uniform rate sequence for $p \leq 2$. We deliver a sharpcharacterization of the asymptotic minimax risk when $\epsilon \to 0$ as thenumber of samples and number of dimensions go to infinity, for testing based onthe occurrences' histogram (number of absent categories, singletons,collisions, ...). For example, for $p=1$ and in the limit of a small expectednumber of samples $n$ compared to the number of categories $N$ (aka"sub-linear" regime), the minimax risk $R^*_\epsilon$ asymptotes to $2\bar{\Phi}\left(n \epsilon^2/\sqrt{8N}\right) $, with $\bar{\Phi}(x)$ thenormal survival function. Empirical studies over a range of problem parametersshow that this estimate is accurate in finite samples, and that our test issignificantly better than the chisquared test or a test that only usescollisions. Our analysis is based on the asymptotic normality of histogramordinates, the equivalence between the minimax setting to a Bayesian one, andthe reduction of a multi-dimensional optimization problem to a one-dimensionalproblem.</description><author>Alon Kipnis</author><pubDate>Mon, 29 May 2023 15:26:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18111v1</guid></item><item><title>IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers</title><link>http://arxiv.org/abs/2304.14400v3</link><description>Scalable Vector Graphics (SVG) is a popular vector image format that offersgood support for interactivity and animation. Despite its appealingcharacteristics, creating custom SVG content can be challenging for users dueto the steep learning curve required to understand SVG grammars or get familiarwith professional editing software. Recent advancements in text-to-imagegeneration have inspired researchers to explore vector graphics synthesis usingeither image-based methods (i.e., text -&gt; raster image -&gt; vector graphics)combining text-to-image generation models with image vectorization, orlanguage-based methods (i.e., text -&gt; vector graphics script) throughpretrained large language models. However, these methods still suffer fromlimitations in terms of generation quality, diversity, and flexibility. In thispaper, we introduce IconShop, a text-guided vector icon synthesis method usingautoregressive transformers. The key to success of our approach is tosequentialize and tokenize SVG paths (and textual descriptions as guidance)into a uniquely decodable token sequence. With that, we are able to fullyexploit the sequence learning power of autoregressive transformers, whileenabling both unconditional and text-conditioned icon synthesis. Throughstandard training to predict the next token on a large-scale vector icondataset accompanied by textural descriptions, the proposed IconShopconsistently exhibits better icon synthesis capability than existingimage-based and language-based methods both quantitatively and qualitatively.Meanwhile, we observe a dramatic improvement in generation diversity, which isvalidated by the objective Uniqueness and Novelty measures. More importantly,we demonstrate the flexibility of IconShop with multiple novel icon synthesistasks, including icon editing, icon interpolation, icon semantic combination,and icon design auto-suggestion.</description><author>Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao</author><pubDate>Mon, 29 May 2023 15:26:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14400v3</guid></item><item><title>OverFlow: Putting flows on top of neural transducers for better TTS</title><link>http://arxiv.org/abs/2211.06892v2</link><description>Neural HMMs are a type of neural transducer recently proposed forsequence-to-sequence modelling in text-to-speech. They combine the bestfeatures of classic statistical speech synthesis and modern neural TTS,requiring less data and fewer training updates, and are less prone to gibberishoutput caused by neural attention failures. In this paper, we combine neuralHMM TTS with normalising flows for describing the highly non-Gaussiandistribution of speech acoustics. The result is a powerful, fully probabilisticmodel of durations and acoustics that can be trained using exact maximumlikelihood. Experiments show that a system based on our proposal needs fewerupdates than comparable methods to produce accurate pronunciations and asubjective speech quality close to natural speech. Please seehttps://shivammehta25.github.io/OverFlow/ for audio examples and code.</description><author>Shivam Mehta, Ambika Kirkland, Harm Lameris, Jonas Beskow, √âva Sz√©kely, Gustav Eje Henter</author><pubDate>Mon, 29 May 2023 15:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06892v2</guid></item><item><title>Medical Dialogue Generation via Dual Flow Modeling</title><link>http://arxiv.org/abs/2305.18109v1</link><description>Medical dialogue systems (MDS) aim to provide patients with medical services,such as diagnosis and prescription. Since most patients cannot preciselydescribe their symptoms, dialogue understanding is challenging for MDS.Previous studies mainly addressed this by extracting the mentioned medicalentities as critical dialogue history information. In this work, we argue thatit is also essential to capture the transitions of the medical entities and thedoctor's dialogue acts in each turn, as they help the understanding of how thedialogue flows and enhance the prediction of the entities and dialogue acts tobe adopted in the following turn. Correspondingly, we propose a Dual Flowenhanced Medical (DFMed) dialogue generation framework. It extracts the medicalentities and dialogue acts used in the dialogue history and models theirtransitions with an entity-centric graph flow and a sequential act flow,respectively. We employ two sequential models to encode them and devise aninterweaving component to enhance their interactions. Experiments on twodatasets demonstrate that our method exceeds baselines in both automatic andmanual evaluations.</description><author>Kaishuai Xu, Wenjun Hou, Yi Cheng, Jian Wang, Wenjie Li</author><pubDate>Mon, 29 May 2023 15:23:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18109v1</guid></item><item><title>Strategyproof Decision-Making in Panel Data Settings and Beyond</title><link>http://arxiv.org/abs/2211.14236v3</link><description>We consider the classical problem of decision-making using panel data, inwhich a decision-maker gets noisy, repeated measurements of multiple units (oragents). We consider a setup where there is a pre-intervention period, when theprincipal observes the outcomes of each unit, after which the principal usesthese observations to assign a treatment to each unit. Unlike this classicalsetting, we permit the units generating the panel data to be strategic, i.e.units may modify their pre-intervention outcomes in order to receive a moredesirable intervention. The principal's goal is to design a strategyproofintervention policy, i.e. a policy that assigns units to their correctinterventions despite their potential strategizing. We first identify anecessary and sufficient condition under which a strategyproof interventionpolicy exists, and provide a strategyproof mechanism with a simple closed formwhen one does exist. Along the way, we prove impossibility results forstrategic multiclass classification, which may be of independent interest. Whenthere are two interventions, we establish that there always exists astrategyproof mechanism, and provide an algorithm for learning such amechanism. For three or more interventions, we provide an algorithm forlearning a strategyproof mechanism if there exists a sufficiently large gap inthe principal's rewards between different interventions. Finally, weempirically evaluate our model using real-world panel data collected fromproduct sales over 18 months. We find that our methods compare favorably tobaselines which do not take strategic interactions into consideration, even inthe presence of model misspecification.</description><author>Keegan Harris, Anish Agarwal, Chara Podimata, Zhiwei Steven Wu</author><pubDate>Mon, 29 May 2023 15:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14236v3</guid></item><item><title>Crafting Training Degradation Distribution for the Accuracy-Generalization Trade-off</title><link>http://arxiv.org/abs/2305.18107v1</link><description>Super-resolution (SR) techniques designed for real-world applicationscommonly encounter two primary challenges: generalization performance andrestoration accuracy. We demonstrate that when methods are trained usingcomplex, large-range degradations to enhance generalization, a decline inaccuracy is inevitable. However, since the degradation in a certain real-worldapplications typically exhibits a limited variation range, it becomes feasibleto strike a trade-off between generalization performance and testing accuracywithin this scope. In this work, we introduce a novel approach to crafttraining degradation distributions using a small set of reference images. Ourstrategy is founded upon the binned representation of the degradation space andthe Fr\'echet distance between degradation distributions. Our results indicatethat the proposed technique significantly improves the performance of testimages while preserving generalization capabilities in real-world applications.</description><author>Ruofan Zhang, Jinjin Gu, Haoyu Chen, Chao Dong, Yulun Zhang, Wenming Yang</author><pubDate>Mon, 29 May 2023 15:22:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18107v1</guid></item><item><title>Metaheuristic conditional neural network for harvesting skyrmionic metastable states</title><link>http://arxiv.org/abs/2303.02876v2</link><description>We present a metaheuristic conditional neural-network-based method aimed atidentifying physically interesting metastable states in a potential energysurface of high rugosity. To demonstrate how this method works, we identify andanalyze spin textures with topological charge $Q$ ranging from 1 to $-13$(where antiskyrmions have $Q&lt;0$) in the Pd/Fe/Ir(111) system, which we modelusing a classical atomistic spin Hamiltonian based on parameters computed fromdensity functional theory. To facilitate the harvest of relevant spin textures,we make use of the newly developed Segment Anything Model (SAM). Spin textureswith $Q$ ranging from $-3$ to $-6$ are further analyzed usingfinite-temperature spin-dynamics simulations. We observe that for temperaturesup to around 20\,K, lifetimes longer than 200\,ps are predicted, and that whenthese textures decay, new topological spin textures are formed. We also findthat the relative stability of the spin textures depend linearly on thetopological charge, but only when comparing the most stable antiskyrmions foreach topological charge. In general, the number of holes (i.e.,non-self-intersecting curves that define closed domain walls in the structure)in the spin texture is an important predictor of stability -- the more holes,the less stable is the texture. Methods for systematic identification andcharacterization of complex metastable skyrmionic textures -- such as the onedemonstrated here -- are highly relevant for advancements in the field oftopological spintronics.</description><author>Qichen Xu, I. P. Miranda, Manuel Pereiro, Filipp N. Rybakov, Danny Thonig, Erik Sj√∂qvist, Pavel Bessarab, Anders Bergman, Olle Eriksson, Pawel Herman, Anna Delin</author><pubDate>Mon, 29 May 2023 15:13:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02876v2</guid></item><item><title>Writing user personas with Large Language Models: Testing phase 6 of a Thematic Analysis of semi-structured interviews</title><link>http://arxiv.org/abs/2305.18099v1</link><description>The goal of this paper is establishing if we can satisfactorily perform aThematic Analysis (TA) of semi-structured interviews using a Large LanguageModel (more precisely GPT3.5-Turbo). Building on previous work by the author,which established an embryonal process for conducting a TA with the model, thispaper will perform a further analysis and then cover the last phase of a TA(phase 6), which entails the writing up of the result. This phase was notcovered by the previous work. In particular, the focus will be on using theresults of a TA done with the LLM on a dataset of user interviews, for writinguser personas, with the model building on the TA to produce the personasnarratives. User personas are models of real users, usually built from a dataanalysis like interviews with a sample of users. User personas are tools oftenused in User Centered Design processes. The paper shows that the model canbuild basic user personas with an acceptable quality deriving them from themes,and that the model can serve for the generation of ideas for user personas.</description><author>Stefano De Paoli</author><pubDate>Mon, 29 May 2023 15:09:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18099v1</guid></item><item><title>BigTrans: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages</title><link>http://arxiv.org/abs/2305.18098v1</link><description>Large language models (LLMs) demonstrate promising translation performanceamong various natural languages. However, many LLMs especially the open-sourcedones, such as BLOOM and LLaMA, are English-dominant and support only dozens ofnatural languages, making the potential of LLMs on language translation lessexplored. In this work, we present BigTrans which adapts LLaMA that covers only20 languages and enhances it with multilingual translation capability on morethan 100 languages. BigTrans is built upon LLaMA-13B and it is optimized inthree steps. First, we continue training LLaMA with massive Chinese monolingualdata. Second, we continue training the model with a large-scale paralleldataset that covers 102 natural languages. Third, we instruct-tune thefoundation model with multilingual translation instructions, leading to ourBigTrans model. The preliminary experiments on multilingual translation showthat BigTrans performs comparably with ChatGPT and Google Translate in manylanguages and even outperforms ChatGPT in 8 language pairs. We release theBigTrans model and hope it can advance the research progress.</description><author>Wen Yang, Chong Li, Jiajun Zhang, Chengqing Zong</author><pubDate>Mon, 29 May 2023 15:07:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18098v1</guid></item><item><title>Personalized Algorithmic Recourse with Preference Elicitation</title><link>http://arxiv.org/abs/2205.13743v3</link><description>Algorithmic Recourse (AR) is the problem of computing a sequence of actionsthat -- once performed by a user -- overturns an undesirable machine decision.It is paramount that the sequence of actions does not require too much effortfor users to implement. Yet, most approaches to AR assume that actions cost thesame for all users, and thus may recommend unfairly expensive recourse plans tocertain users. Prompted by this observation, we introduce PEAR, the firsthuman-in-the-loop approach capable of providing personalized algorithmicrecourse tailored to the needs of any end-user. PEAR builds on insights fromBayesian Preference Elicitation to iteratively refine an estimate of the costsof actions by asking choice set queries to the target user. The queriesthemselves are computed by maximizing the Expected Utility of Selection, aprincipled measure of information gain accounting for uncertainty on both thecost estimate and the user's responses. PEAR integrates elicitation into aReinforcement Learning agent coupled with Monte Carlo Tree Search to quicklyidentify promising recourse plans. Our empirical evaluation on real-worlddatasets highlights how PEAR produces high-quality personalized recourse inonly a handful of iterations.</description><author>Giovanni De Toni, Paolo Viappiani, Stefano Teso, Bruno Lepri, Andrea Passerini</author><pubDate>Mon, 29 May 2023 15:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.13743v3</guid></item><item><title>Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target</title><link>http://arxiv.org/abs/2305.18096v1</link><description>Spoken Language Understanding (SLU) is a task that aims to extract semanticinformation from spoken utterances. Previous research has made progress inend-to-end SLU by using paired speech-text data, such as pre-trained AutomaticSpeech Recognition (ASR) models or paired text as intermediate targets.However, acquiring paired transcripts is expensive and impractical forunwritten languages. On the other hand, Textless SLU extracts semanticinformation from speech without utilizing paired transcripts. However, theabsence of intermediate targets and training guidance for textless SLU oftenresults in suboptimal performance. In this work, inspired by thecontent-disentangled discrete units from self-supervised speech models, weproposed to use discrete units as intermediate guidance to improve textless SLUperformance. Our method surpasses the baseline method on five SLU benchmarkcorpora. Additionally, we find that unit guidance facilitates few-shot learningand enhances the model's ability to handle noise.</description><author>Guan-Wei Wu, Guan-Ting Lin, Shang-Wen Li, Hung-yi Lee</author><pubDate>Mon, 29 May 2023 15:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18096v1</guid></item><item><title>Are Random Decompositions all we need in High Dimensional Bayesian Optimisation?</title><link>http://arxiv.org/abs/2301.12844v2</link><description>Learning decompositions of expensive-to-evaluate black-box functions promisesto scale Bayesian optimisation (BO) to high-dimensional problems. However, thesuccess of these techniques depends on finding proper decompositions thataccurately represent the black-box. While previous works learn thosedecompositions based on data, we investigate data-independent decompositionsampling rules in this paper. We find that data-driven learners ofdecompositions can be easily misled towards local decompositions that do nothold globally across the search space. Then, we formally show that a randomtree-based decomposition sampler exhibits favourable theoretical guaranteesthat effectively trade off maximal information gain and functional mismatchbetween the actual black-box and its surrogate as provided by thedecomposition. Those results motivate the development of the randomdecomposition upper-confidence bound algorithm (RDUCB) that is straightforwardto implement - (almost) plug-and-play - and, surprisingly, yields significantempirical gains compared to the previous state-of-the-art on a comprehensiveset of benchmarks. We also confirm the plug-and-play nature of our modellingcomponent by integrating our method with HEBO, showing improved practical gainsin the highest dimensional tasks from Bayesmark.</description><author>Juliusz Ziomek, Haitham Bou-Ammar</author><pubDate>Mon, 29 May 2023 14:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12844v2</guid></item><item><title>Massively Multilingual Lexical Specialization of Multilingual Transformers</title><link>http://arxiv.org/abs/2208.01018v3</link><description>While pretrained language models (PLMs) primarily serve as general-purposetext encoders that can be fine-tuned for a wide variety of downstream tasks,recent work has shown that they can also be rewired to produce high-qualityword representations (i.e., static word embeddings) and yield good performancein type-level lexical tasks. While existing work primarily focused on thelexical specialization of monolingual PLMs with immense quantities ofmonolingual constraints, in this work we expose massively multilingualtransformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge atscale, leveraging BabelNet as the readily available rich source of multilingualand cross-lingual type-level lexical knowledge. Concretely, we use BabelNet'smultilingual synsets to create synonym pairs (or synonym-gloss pairs) across 50languages and then subject the MMTs (mBERT and XLM-R) to a lexicalspecialization procedure guided by a contrastive objective. We show that suchmassively multilingual lexical specialization brings substantial gains in twostandard cross-lingual lexical tasks, bilingual lexicon induction andcross-lingual word similarity, as well as in cross-lingual sentence retrieval.Crucially, we observe gains for languages unseen in specialization, indicatingthat multilingual lexical specialization enables generalization to languageswith no lexical constraints. In a series of subsequent controlled experiments,we show that the number of specialization constraints plays a much greater rolethan the set of languages from which they originate.</description><author>Tommaso Green, Simone Paolo Ponzetto, Goran Glava≈°</author><pubDate>Mon, 29 May 2023 14:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.01018v3</guid></item><item><title>SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation</title><link>http://arxiv.org/abs/2211.07044v2</link><description>Self-supervised pre-training bears potential to generate expressiverepresentations without human annotation. Most pre-training in Earthobservation (EO) are based on ImageNet or medium-size, labeled remote sensing(RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-SupervisedLearning for Earth Observation - Sentinel-1/2) to assemble a large-scale,global, multimodal, and multi-seasonal corpus of satellite imagery from the ESASentinel-1 \&amp; -2 satellite missions. For EO applications we demonstrateSSL4EO-S12 to succeed in self-supervised pre-training for a set of methods:MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performanceclose to, or surpassing accuracy measures of supervised learning. In addition,pre-training on SSL4EO-S12 excels compared to existing datasets. We make openlyavailable the dataset, related source code, and pre-trained models athttps://github.com/zhu-xlab/SSL4EO-S12.</description><author>Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad M Albrecht, Xiao Xiang Zhu</author><pubDate>Mon, 29 May 2023 14:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07044v2</guid></item><item><title>Contrastive Learning Based Recursive Dynamic Multi-Scale Network for Image Deraining</title><link>http://arxiv.org/abs/2305.18092v1</link><description>Rain streaks significantly decrease the visibility of captured images and arealso a stumbling block that restricts the performance of subsequent computervision applications. The existing deep learning-based image deraining methodsemploy manually crafted networks and learn a straightforward projection fromrainy images to clear images. In pursuit of better deraining performance, theyfocus on elaborating a more complicated architecture rather than exploiting theintrinsic properties of the positive and negative information. In this paper,we propose a contrastive learning-based image deraining method thatinvestigates the correlation between rainy and clear images and leverages acontrastive prior to optimize the mutual information of the rainy and restoredcounterparts. Given the complex and varied real-world rain patterns, we developa recursive mechanism. It involves multi-scale feature extraction and dynamiccross-level information recruitment modules. The former advances the portrayalof diverse rain patterns more precisely, while the latter can selectivelycompensate high-level features for shallow-level information. We term theproposed recursive dynamic multi-scale network with a contrastive prior, RDMC.Extensive experiments on synthetic benchmarks and real-world images demonstratethat the proposed RDMC delivers strong performance on the depiction of rainstreaks and outperforms the state-of-the-art methods. Moreover, a practicalevaluation of object detection and semantic segmentation shows theeffectiveness of the proposed method.</description><author>Zhiying Jiang, Risheng Liu, Shuzhou Yang, Zengxi Zhang, Xin Fan</author><pubDate>Mon, 29 May 2023 14:51:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18092v1</guid></item><item><title>NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects</title><link>http://arxiv.org/abs/2305.14345v2</link><description>Deep generative models have been recently extended to synthesizing 3D digitalhumans. However, previous approaches treat clothed humans as a single chunk ofgeometry without considering the compositionality of clothing and accessories.As a result, individual items cannot be naturally composed into novelidentities, leading to limited expressiveness and controllability of generative3D avatars. While several methods attempt to address this by leveragingsynthetic data, the interaction between humans and objects is not authentic dueto the domain gap, and manual asset creation is difficult to scale for a widevariety of objects. In this work, we present a novel framework for learning acompositional generative model of humans and objects (backpacks, coats,scarves, and more) from real-world 3D scans. Our compositional model isinteraction-aware, meaning the spatial relationship between humans and objects,and the mutual shape change by physical contact is fully incorporated. The keychallenge is that, since humans and objects are in contact, their 3D scans aremerged into a single piece. To decompose them without manual annotations, wepropose to leverage two sets of 3D scans of a single person with and withoutobjects. Our approach learns to decompose objects and naturally compose themback into a generative human model in an unsupervised manner. Despite oursimple setup requiring only the capture of a single subject with objects, ourexperiments demonstrate the strong generalization of our model by enabling thenatural composition of objects to diverse identities in various poses and thecomposition of multiple objects, which is unseen in training data.https://taeksuu.github.io/ncho/</description><author>Taeksoo Kim, Shunsuke Saito, Hanbyul Joo</author><pubDate>Mon, 29 May 2023 14:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14345v2</guid></item></channel></rss>