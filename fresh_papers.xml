<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 23 Aug 2023 06:00:39 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>GRIP: Generating Interaction Poses Using Latent Consistency and Spatial Cues</title><link>http://arxiv.org/abs/2308.11617v1</link><description>Hands are dexterous and highly versatile manipulators that are central to howhumans interact with objects and their environment. Consequently, modelingrealistic hand-object interactions, including the subtle motion of individualfingers, is critical for applications in computer graphics, computer vision,and mixed reality. Prior work on capturing and modeling humans interacting withobjects in 3D focuses on the body and object motion, often ignoring hand pose.In contrast, we introduce GRIP, a learning-based method that takes, as input,the 3D motion of the body and the object, and synthesizes realistic motion forboth hands before, during, and after object interaction. As a preliminary stepbefore synthesizing the hand motion, we first use a network, ANet, to denoisethe arm motion. Then, we leverage the spatio-temporal relationship between thebody and the object to extract two types of novel temporal interaction cues,and use them in a two-stage inference pipeline to generate the hand motion. Inthe first stage, we introduce a new approach to enforce motion temporalconsistency in the latent space (LTC), and generate consistent interactionmotions. In the second stage, GRIP generates refined hand poses to avoidhand-object penetrations. Given sequences of noisy body and object motion, GRIPupgrades them to include hand-object interaction. Quantitative experiments andperceptual studies demonstrate that GRIP outperforms baseline methods andgeneralizes to unseen objects and motions from different motion-capturedatasets.</description><author>Omid Taheri, Yi Zhou, Dimitrios Tzionas, Yang Zhou, Duygu Ceylan, Soren Pirk, Michael J. Black</author><pubDate>Tue, 22 Aug 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11617v1</guid></item><item><title>Self-Training: A Survey</title><link>http://arxiv.org/abs/2202.12040v3</link><description>Semi-supervised algorithms aim to learn prediction functions from a small setof labeled observations and a large set of unlabeled observations. Because thisframework is relevant in many applications, they have received a lot ofinterest in both academia and industry. Among the existing techniques,self-training methods have undoubtedly attracted greater attention in recentyears. These models are designed to find the decision boundary on low densityregions without making additional assumptions about the data distribution, anduse the unsigned output score of a learned classifier, or its margin, as anindicator of confidence. The working principle of self-training algorithms isto learn a classifier iteratively by assigning pseudo-labels to the set ofunlabeled training samples with a margin greater than a certain threshold. Thepseudo-labeled examples are then used to enrich the labeled training data andto train a new classifier in conjunction with the labeled training set. In thispaper, we present self-training methods for binary and multi-classclassification; as well as their variants and two related approaches, namelyconsistency-based approaches and transductive learning. We examine the impactof significant self-training features on various methods, using differentgeneral and image classification benchmarks, and we discuss our ideas forfuture research in self-training. To the best of our knowledge, this is thefirst thorough and complete survey on this subject.</description><author>Massih-Reza Amini, Vasilii Feofanov, Loic Pauletto, Emilie Devijver, Yury Maximov</author><pubDate>Tue, 22 Aug 2023 18:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.12040v3</guid></item><item><title>Delving into Motion-Aware Matching for Monocular 3D Object Tracking</title><link>http://arxiv.org/abs/2308.11607v1</link><description>Recent advances of monocular 3D object detection facilitate the 3Dmulti-object tracking task based on low-cost camera sensors. In this paper, wefind that the motion cue of objects along different time frames is critical in3D multi-object tracking, which is less explored in existing monocular-basedapproaches. In this paper, we propose a motion-aware framework for monocular 3DMOT. To this end, we propose MoMA-M3T, a framework that mainly consists ofthree motion-aware components. First, we represent the possible movement of anobject related to all object tracklets in the feature space as its motionfeatures. Then, we further model the historical object tracklet along the timeframe in a spatial-temporal perspective via a motion transformer. Finally, wepropose a motion-aware matching module to associate historical object trackletsand current observations as final tracking results. We conduct extensiveexperiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3Tachieves competitive performance against state-of-the-art methods. Moreover,the proposed tracker is flexible and can be easily plugged into existingimage-based 3D object detectors without re-training. Code and models areavailable at https://github.com/kuanchihhuang/MoMA-M3T.</description><author>Kuan-Chih Huang, Ming-Hsuan Yang, Yi-Hsuan Tsai</author><pubDate>Tue, 22 Aug 2023 18:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11607v1</guid></item><item><title>StoryBench: A Multifaceted Benchmark for Continuous Story Visualization</title><link>http://arxiv.org/abs/2308.11606v1</link><description>Generating video stories from text prompts is a complex task. In addition tohaving high visual quality, videos need to realistically adhere to a sequenceof text prompts whilst being consistent throughout the frames. Creating abenchmark for video generation requires data annotated over time, whichcontrasts with the single caption used often in video datasets. To fill thisgap, we collect comprehensive human annotations on three existing datasets, andintroduce StoryBench: a new, challenging multi-task benchmark to reliablyevaluate forthcoming text-to-video models. Our benchmark includes three videogeneration tasks of increasing difficulty: action execution, where the nextaction must be generated starting from a conditioning video; storycontinuation, where a sequence of actions must be executed starting from aconditioning video; and story generation, where a video must be generated fromonly text prompts. We evaluate small yet strong text-to-video baselines, andshow the benefits of training on story-like data algorithmically generated fromexisting video captions. Finally, we establish guidelines for human evaluationof video stories, and reaffirm the need of better automatic metrics for videogeneration. StoryBench aims at encouraging future research efforts in thisexciting new area.</description><author>Emanuele Bugliarello, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, Paul Voigtlaender</author><pubDate>Tue, 22 Aug 2023 18:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11606v1</guid></item><item><title>GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning</title><link>http://arxiv.org/abs/2308.11605v1</link><description>Large-scale foundation models, such as CLIP, have demonstrated remarkablesuccess in visual recognition tasks by embedding images in a semantically richspace. Self-supervised learning (SSL) has also shown promise in improvingvisual recognition by learning invariant features. However, the combination ofCLIP with SSL is found to face challenges due to the multi-task framework thatblends CLIP's contrastive loss and SSL's loss, including difficulties with lossweighting and inconsistency among different views of images in CLIP's outputspace. To overcome these challenges, we propose a prompt learning-based modelcalled GOPro, which is a unified framework that ensures similarity betweenvarious augmented views of input images in a shared image-text embedding space,using a pair of learnable image and text projectors atop CLIP, to promoteinvariance and generalizability. To automatically learn such prompts, weleverage the visual content and style primitives extracted from pre-trainedCLIP and adapt them to the target task. In addition to CLIP's cross-domaincontrastive loss, we introduce a visual contrastive loss and a novel promptconsistency loss, considering the different views of the images. GOPro istrained end-to-end on all three loss objectives, combining the strengths ofCLIP and SSL in a principled manner. Empirical evaluations demonstrate thatGOPro outperforms the state-of-the-art prompting techniques on threechallenging domain generalization tasks across multiple benchmarks by asignificant margin. Our code is available athttps://github.com/mainaksingha01/GOPro.</description><author>Mainak Singha, Ankit Jha, Biplab Banerjee</author><pubDate>Tue, 22 Aug 2023 18:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11605v1</guid></item><item><title>Semantic Multi-Resolution Communications</title><link>http://arxiv.org/abs/2308.11604v1</link><description>Deep learning based joint source-channel coding (JSCC) has demonstratedsignificant advancements in data reconstruction compared to separatesource-channel coding (SSCC). This superiority arises from the suboptimality ofSSCC when dealing with finite block-length data. Moreover, SSCC falls short inreconstructing data in a multi-user and/or multi-resolution fashion, as it onlytries to satisfy the worst channel and/or the highest quality data. To overcomethese limitations, we propose a novel deep learning multi-resolution JSCCframework inspired by the concept of multi-task learning (MTL). This proposedframework excels at encoding data for different resolutions throughhierarchical layers and effectively decodes it by leveraging both current andpast layers of encoded data. Moreover, this framework holds great potential forsemantic communication, where the objective extends beyond data reconstructionto preserving specific semantic attributes throughout the communicationprocess. These semantic features could be crucial elements such as classlabels, essential for classification tasks, or other key attributes thatrequire preservation. Within this framework, each level of encoded data can becarefully designed to retain specific data semantics. As a result, theprecision of a semantic classifier can be progressively enhanced acrosssuccessive layers, emphasizing the preservation of targeted semanticsthroughout the encoding and decoding stages. We conduct experiments on MNISTand CIFAR10 dataset. The experiment with both datasets illustrates that ourproposed method is capable of surpassing the SSCC method in reconstructing datawith different resolutions, enabling the extraction of semantic features withheightened confidence in successive layers. This capability is particularlyadvantageous for prioritizing and preserving more crucial semantic featureswithin the datasets.</description><author>Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, Sennur Ulukus</author><pubDate>Tue, 22 Aug 2023 18:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11604v1</guid></item><item><title>Deep Learning Approaches on Image Captioning: A Review</title><link>http://arxiv.org/abs/2201.12944v5</link><description>Image captioning is a research area of immense importance, aiming to generatenatural language descriptions for visual content in the form of still images.The advent of deep learning and more recently vision-language pre-trainingtechniques has revolutionized the field, leading to more sophisticated methodsand improved performance. In this survey paper, we provide a structured reviewof deep learning methods in image captioning by presenting a comprehensivetaxonomy and discussing each method category in detail. Additionally, weexamine the datasets commonly employed in image captioning research, as well asthe evaluation metrics used to assess the performance of different captioningmodels. We address the challenges faced in this field by emphasizing issuessuch as object hallucination, missing context, illumination conditions,contextual understanding, and referring expressions. We rank different deeplearning methods' performance according to widely used evaluation metrics,giving insight into the current state of the art. Furthermore, we identifyseveral potential future directions for research in this area, which includetackling the information misalignment problem between image and textmodalities, mitigating dataset bias, incorporating vision-language pre-trainingmethods to enhance caption generation, and developing improved evaluation toolsto accurately measure the quality of image captions.</description><author>Taraneh Ghandi, Hamidreza Pourreza, Hamidreza Mahyar</author><pubDate>Tue, 22 Aug 2023 18:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.12944v5</guid></item><item><title>Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model</title><link>http://arxiv.org/abs/2308.11601v1</link><description>The introduction of the transformer architecture and the self-attentionmechanism has led to an explosive production of language models trained onspecific downstream tasks and data domains. With over 200, 000 models in theHugging Face ecosystem, users grapple with selecting and optimizing models tosuit multifaceted workflows and data domains while addressing computational,security, and recency concerns. There is an urgent need for machine learningframeworks that can eliminate the burden of model selection and customizationand unleash the incredible power of the vast emerging model library for endusers. Here, we propose a context-aware routing system, Tryage, that leveragesa language model router for optimal selection of expert models from a modellibrary based on analysis of individual input prompts. Inspired by the thalamicrouter in the brain, Tryage employs a perceptive router to predict down-streammodel performance on prompts and, then, makes a routing decision using anobjective function that integrates performance predictions with user goals andconstraints that are incorporated through flags (e.g., model size, modelrecency). Tryage allows users to explore a Pareto front and automaticallytrade-off between task accuracy and secondary goals including minimization ofmodel size, recency, security, verbosity, and readability. Across heterogeneousdata sets that include code, text, clinical data, and patents, the Tryageframework surpasses Gorilla and GPT3.5 turbo in dynamic model selectionidentifying the optimal model with an accuracy of 50.9% , compared to 23.6% byGPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates howrouting models can be applied to program and control the behavior ofmulti-model LLM systems to maximize efficient use of the expanding and evolvinglanguage model ecosystem.</description><author>Surya Narayanan Hari, Matt Thomson</author><pubDate>Tue, 22 Aug 2023 18:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11601v1</guid></item><item><title>SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation</title><link>http://arxiv.org/abs/2308.11596v1</link><description>What does it take to create the Babel Fish, a tool that can help individualstranslate speech between any two languages? While recent breakthroughs intext-based models have pushed machine translation coverage beyond 200languages, unified speech-to-speech translation models have yet to achievesimilar strides. More specifically, conventional speech-to-speech translationsystems rely on cascaded systems that perform translation progressively,putting high-performing unified systems out of reach. To address these gaps, weintroduce SeamlessM4T, a single model that supports speech-to-speechtranslation, speech-to-text translation, text-to-speech translation,text-to-text translation, and automatic speech recognition for up to 100languages. To build this, we used 1 million hours of open speech audio data tolearn self-supervised speech representations with w2v-BERT 2.0. Subsequently,we created a multimodal corpus of automatically aligned speech translations.Filtered and combined with human-labeled and pseudo-labeled data, we developedthe first multilingual system capable of translating from and into English forboth speech and text. On FLEURS, SeamlessM4T sets a new standard fortranslations into multiple target languages, achieving an improvement of 20%BLEU over the previous SOTA in direct speech-to-text translation. Compared tostrong cascaded models, SeamlessM4T improves the quality of into-Englishtranslation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points inspeech-to-speech. Tested for robustness, our system performs better againstbackground noises and speaker variations in speech-to-text tasks compared tothe current SOTA model. Critically, we evaluated SeamlessM4T on gender bias andadded toxicity to assess translation safety. Finally, all contributions in thiswork are open-sourced at this httpshttps://github.com/facebookresearch/seamless_communication.</description><author>Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-juss</author><pubDate>Tue, 22 Aug 2023 18:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11596v1</guid></item><item><title>Consciousness in Artificial Intelligence: Insights from the Science of Consciousness</title><link>http://arxiv.org/abs/2308.08708v3</link><description>Whether current or near-term AI systems could be conscious is a topic ofscientific interest and increasing public concern. This report argues for, andexemplifies, a rigorous and empirically grounded approach to AI consciousness:assessing existing AI systems in detail, in light of our best-supportedneuroscientific theories of consciousness. We survey several prominentscientific theories of consciousness, including recurrent processing theory,global workspace theory, higher-order theories, predictive processing, andattention schema theory. From these theories we derive "indicator properties"of consciousness, elucidated in computational terms that allow us to assess AIsystems for these properties. We use these indicator properties to assessseveral recent AI systems, and we discuss how future systems might implementthem. Our analysis suggests that no current AI systems are conscious, but alsosuggests that there are no obvious technical barriers to building AI systemswhich satisfy these indicators.</description><author>Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, Stephen M. Fleming, Chris Frith, Xu Ji, Ryota Kanai, Colin Klein, Grace Lindsay, Matthias Michel, Liad Mudrik, Megan A. K. Peters, Eric Schwitzgebel, Jonathan Simon, Rufin VanRullen</author><pubDate>Tue, 22 Aug 2023 18:33:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08708v3</guid></item><item><title>MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models</title><link>http://arxiv.org/abs/2308.09729v2</link><description>LLMs usually exhibit limitations in their ability to incorporate newknowledge, the generation of hallucinations, and the transparency of theirdecision-making process. In this paper, we explore how to prompt LLMs withknowledge graphs (KG), working as a remedy to engage LLMs with up-to-dateknowledge and elicit the reasoning pathways from LLMs. Specifically, we build aprompting pipeline that endows LLMs with the capability of comprehending KGinputs and inferring with a combined implicit knowledge and the retrievedexternal knowledge. In addition, we investigate eliciting the mind map on whichLLMs perform the reasoning and generate the answers. It is identified that theproduced mind map exhibits the reasoning pathways of LLMs grounded on theontology of knowledge, hence bringing the prospects of probing and gauging LLMinference in production. The experiments on three question &amp; answering datasetsalso show that MindMap prompting leads to a striking empirical gain. Forinstance, prompting a GPT-3.5 with MindMap yields an overwhelming performanceover GPT-4 consistently. We also demonstrate that with structured factsretrieved from KG, MindMap can outperform a series ofprompting-with-document-retrieval methods, benefiting from more accurate,concise, and comprehensive knowledge from KGs.</description><author>Yilin Wen, Zifeng Wang, Jimeng Sun</author><pubDate>Tue, 22 Aug 2023 18:32:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09729v2</guid></item><item><title>Phenotype-preserving metric design for high-content image reconstruction by generative inpainting</title><link>http://arxiv.org/abs/2307.14436v3</link><description>In the past decades, automated high-content microscopy demonstrated itsability to deliver large quantities of image-based data powering theversatility of phenotypic drug screening and systems biology applications.However, as the sizes of image-based datasets grew, it became infeasible forhumans to control, avoid and overcome the presence of imaging and samplepreparation artefacts in the images. While novel techniques like machinelearning and deep learning may address these shortcomings through generativeimage inpainting, when applied to sensitive research data this may come at thecost of undesired image manipulation. Undesired manipulation may be caused byphenomena such as neural hallucinations, to which some artificial neuralnetworks are prone. To address this, here we evaluate the state-of-the-artinpainting methods for image restoration in a high-content fluorescencemicroscopy dataset of cultured cells with labelled nuclei. We show thatarchitectures like DeepFill V2 and Edge Connect can faithfully restoremicroscopy images upon fine-tuning with relatively little data. Our resultsdemonstrate that the area of the region to be restored is of higher importancethan shape. Furthermore, to control for the quality of restoration, we proposea novel phenotype-preserving metric design strategy. In this strategy, the sizeand count of the restored biological phenotypes like cell nuclei are quantifiedto penalise undesirable manipulation. We argue that the design principles ofour approach may also generalise to other applications.</description><author>Vaibhav Sharma, Artur Yakimovich</author><pubDate>Tue, 22 Aug 2023 18:31:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14436v3</guid></item><item><title>G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid Model</title><link>http://arxiv.org/abs/2308.11573v1</link><description>This study introduces a novel framework, G3Reg, for fast and robust globalregistration of LiDAR point clouds. In contrast to conventional complexkeypoints and descriptors, we extract fundamental geometric primitivesincluding planes, clusters, and lines (PCL) from the raw point cloud to obtainlow-level semantic segments. Each segment is formulated as a unified GaussianEllipsoid Model (GEM) by employing a probability ellipsoid to ensure the groundtruth centers are encompassed with a certain degree of probability. Utilizingthese GEMs, we then present a distrust-and-verify scheme based on a PyramidCompatibility Graph for Global Registration (PAGOR). Specifically, we establishan upper bound, which can be traversed based on the confidence level forcompatibility testing to construct the pyramid graph. Gradually, we solvemultiple maximum cliques (MAC) for each level of the graph, generating numeroustransformation candidates. In the verification phase, we adopt a precise andefficient metric for point cloud alignment quality, founded on geometricprimitives, to identify the optimal candidate. The performance of the algorithmis extensively validated on three publicly available datasets and aself-collected multi-session dataset, without changing any parameter settingsin the experimental evaluation. The results exhibit superior robustness andreal-time performance of the G3Reg framework compared to state-of-the-artmethods. Furthermore, we demonstrate the potential for integrating individualGEM and PAGOR components into other algorithmic frameworks to enhance theirefficacy. To advance further research and promote community understanding, wehave publicly shared the source code.</description><author>Zhijian Qiao, Zehuan Yu, Binqian Jiang, Huan Yin, Shaojie Shen</author><pubDate>Tue, 22 Aug 2023 18:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11573v1</guid></item><item><title>MMD-Regularized Unbalanced Optimal Transport</title><link>http://arxiv.org/abs/2011.05001v7</link><description>We study the unbalanced optimal transport (UOT) problem, where the marginalconstraints are enforced using Maximum Mean Discrepancy (MMD) regularization.Our work is motivated by the observation that the literature on UOT is focusedon regularization based on $\phi$-divergence (e.g., KL divergence). Despite thepopularity of MMD, its role as a regularizer in the context of UOT seems lessunderstood. We begin by deriving the dual of MMD-regularized UOT (MMD-UOT),which helps us prove other useful properties. One interesting outcome of thisduality result is that MMD-UOT induces novel metrics, which not only lift theground metric like the Wasserstein but are also efficient to estimate like theMMD. Further, we present finite-dimensional convex programs for estimatingMMD-UOT and the corresponding barycenter solely based on the samples from themeasures being transported. Under mild conditions, we prove that ourconvex-program-based estimators are consistent and the estimation error decaysat a rate $\mathcal{O}\left(m^{-\frac{1}{2}}\right)$, where $m$ is the numberof samples. As far as we know, such error bounds that are free from the curseof dimensionality are not known for $\phi$-divergence regularized UOT. Finally,we discuss how the proposed convex programs can be solved efficiently usingaccelerated projected gradient descent. Our experiments show that MMD-UOTconsistently outperforms popular baselines, including KL-regularized UOT andMMD, in diverse machine learning applications.</description><author>Piyushi Manupriya, J. Saketha Nath, Pratik Jawanpuria</author><pubDate>Tue, 22 Aug 2023 18:20:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.05001v7</guid></item><item><title>Skin Lesion Correspondence Localization in Total Body Photography</title><link>http://arxiv.org/abs/2307.09642v2</link><description>Longitudinal tracking of skin lesions - finding correspondence, changes inmorphology, and texture - is beneficial to the early detection of melanoma.However, it has not been well investigated in the context of full-body imaging.We propose a novel framework combining geometric and texture information tolocalize skin lesion correspondence from a source scan to a target scan intotal body photography (TBP). Body landmarks or sparse correspondence are firstcreated on the source and target 3D textured meshes. Every vertex on each ofthe meshes is then mapped to a feature vector characterizing the geodesicdistances to the landmarks on that mesh. Then, for each lesion of interest(LOI) on the source, its corresponding location on the target is first coarselyestimated using the geometric information encoded in the feature vectors andthen refined using the texture information. We evaluated the frameworkquantitatively on both a public and a private dataset, for which our successrates (at 10 mm criterion) are comparable to the only reported longitudinalstudy. As full-body 3D capture becomes more prevalent and has higher quality,we expect the proposed method to constitute a valuable step in the longitudinaltracking of skin lesions.</description><author>Wei-Lun Huang, Davood Tashayyod, Jun Kang, Amir Gandjbakhche, Michael Kazhdan, Mehran Armand</author><pubDate>Tue, 22 Aug 2023 18:19:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09642v2</guid></item><item><title>Discovering Conservation Laws using Optimal Transport and Manifold Learning</title><link>http://arxiv.org/abs/2208.14995v2</link><description>Conservation laws are key theoretical and practical tools for understanding,characterizing, and modeling nonlinear dynamical systems. However, for manycomplex systems, the corresponding conserved quantities are difficult toidentify, making it hard to analyze their dynamics and build stable predictivemodels. Current approaches for discovering conservation laws often depend ondetailed dynamical information or rely on black box parametric deep learningmethods. We instead reformulate this task as a manifold learning problem andpropose a non-parametric approach for discovering conserved quantities. We testthis new approach on a variety of physical systems and demonstrate that ourmethod is able to both identify the number of conserved quantities and extracttheir values. Using tools from optimal transport theory and manifold learning,our proposed method provides a direct geometric approach to identifyingconservation laws that is both robust and interpretable without requiring anexplicit model of the system nor accurate time information.</description><author>Peter Y. Lu, Rumen Dangovski, Marin Soljačić</author><pubDate>Tue, 22 Aug 2023 18:16:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.14995v2</guid></item><item><title>SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation</title><link>http://arxiv.org/abs/2308.11568v1</link><description>Recent studies show that self-attentions behave like low-pass filters (asopposed to convolutions) and enhancing their high-pass filtering capabilityimproves model performance. Contrary to this idea, we investigate existingconvolution-based models with spectral analysis and observe that improving thelow-pass filtering in convolution operations also leads to performanceimprovement. To account for this observation, we hypothesize that utilizingoptimal token mixers that capture balanced representations of both high- andlow-frequency components can enhance the performance of models. We verify thisby decomposing visual features into the frequency domain and combining them ina balanced manner. To handle this, we replace the balancing problem with a maskfiltering problem in the frequency domain. Then, we introduce a noveltoken-mixer named SPAM and leverage it to derive a MetaFormer model termed asSPANet. Experimental results show that the proposed method provides a way toachieve this balance, and the balanced representations of both high- andlow-frequency components can improve the performance of models on multiplecomputer vision tasks. Our code is available at$\href{https://doranlyong.github.io/projects/spanet/}{\text{https://doranlyong.github.io/projects/spanet/}}$.</description><author>Guhnoo Yun, Juhan Yoo, Kijung Kim, Jeongho Lee, Dong Hwan Kim</author><pubDate>Tue, 22 Aug 2023 18:14:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11568v1</guid></item><item><title>Low Tensor Rank Learning of Neural Dynamics</title><link>http://arxiv.org/abs/2308.11567v1</link><description>Learning relies on coordinated synaptic changes in recurrently connectedpopulations of neurons. Therefore, understanding the collective evolution ofsynaptic connectivity over learning is a key challenge in neuroscience andmachine learning. In particular, recent work has shown that the weight matricesof task-trained RNNs are typically low rank, but how this low rank structureunfolds over learning is unknown. To address this, we investigate the rank ofthe 3-tensor formed by the weight matrices throughout learning. By fitting RNNsof varying rank to large-scale neural recordings during a motor learning task,we find that the inferred weights are low-tensor-rank and therefore evolve overa fixed low-dimensional subspace throughout the entire course of learning. Wenext validate the observation of low-tensor-rank learning on an RNN trained tosolve the same task by performing a low-tensor-rank decomposition directly onthe ground truth weights, and by showing that the method we applied to the datafaithfully recovers this low rank structure. Finally, we present a set ofmathematical results bounding the matrix and tensor ranks of gradient descentlearning dynamics which show that low-tensor-rank weights emerge naturally inRNNs trained to solve low-dimensional tasks. Taken together, our findingsprovide novel constraints on the evolution of population connectivity overlearning in both biological and artificial neural networks, and enable reverseengineering of learning-induced changes in recurrent network dynamics fromlarge-scale neural recordings.</description><author>Arthur Pellegrino, N Alex Cayco-Gajic, Angus Chadwick</author><pubDate>Tue, 22 Aug 2023 18:08:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11567v1</guid></item><item><title>Using ChatGPT as a CAT tool in Easy Language translation</title><link>http://arxiv.org/abs/2308.11563v1</link><description>This study sets out to investigate the feasibility of using ChatGPT totranslate citizen-oriented administrative texts into German Easy Language, asimplified, controlled language variety that is adapted to the needs of peoplewith reading impairments. We use ChatGPT to translate selected texts fromwebsites of German public authorities using two strategies, i.e. linguistic andholistic. We analyse the quality of the generated texts based on differentcriteria, such as correctness, readability, and syntactic complexity. Theresults indicated that the generated texts are easier than the standard texts,but that they still do not fully meet the established Easy Language standards.Additionally, the content is not always rendered correctly.</description><author>Silvana Deilen, Sergio Hernández Garrido, Ekaterina Lapshinova-Koltunski, Christiane Maaß</author><pubDate>Tue, 22 Aug 2023 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11563v1</guid></item><item><title>Type-to-Track: Retrieve Any Object via Prompt-based Tracking</title><link>http://arxiv.org/abs/2305.13495v2</link><description>One of the recent trends in vision problems is to use natural languagecaptions to describe the objects of interest. This approach can overcome somelimitations of traditional methods that rely on bounding boxes or categoryannotations. This paper introduces a novel paradigm for Multiple ObjectTracking called Type-to-Track, which allows users to track objects in videos bytyping natural language descriptions. We present a new dataset for thatGrounded Multiple Object Tracking task, called GroOT, that contains videos withvarious types of objects and their corresponding textual captions describingtheir appearance and action in detail. Additionally, we introduce two newevaluation protocols and formulate evaluation metrics specifically for thistask. We develop a new efficient method that models a transformer-basedeMbed-ENcoDE-extRact framework (MENDER) using the third-order tensordecomposition. The experiments in five scenarios show that our MENDER approachoutperforms another two-stage design in terms of accuracy and efficiency, up to14.7% accuracy and 4$\times$ speed faster.</description><author>Pha Nguyen, Kha Gia Quach, Kris Kitani, Khoa Luu</author><pubDate>Tue, 22 Aug 2023 17:49:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13495v2</guid></item><item><title>Two Approaches to Supervised Image Segmentation</title><link>http://arxiv.org/abs/2307.10123v3</link><description>Though performed almost effortlessly by humans, segmenting 2D gray-scale orcolor images into respective regions of interest (e.g.~background, objects, orportions of objects) constitutes one of the greatest challenges in science andtechnology as a consequence of several effects including dimensionalityreduction(3D to 2D), noise, reflections, shades, and occlusions, among manyother possibilities. While a large number of interesting related approacheshave been suggested along the last decades, it was mainly thanks to the recentdevelopment of deep learning that more effective and general solutions havebeen obtained, currently constituting the basic comparison reference for thistype of operation. Also developed recently, a multiset-based methodology hasbeen described that is capable of encouraging image segmentation performancecombining spatial accuracy, stability, and robustness while requiring littlecomputational resources (hardware and/or training and recognition time). Theinteresting features of the multiset neurons methodology mostly follow from theenhanced selectivity and sensitivity, as well as good robustness to dataperturbations and outliers, allowed by the coincidence similarity index onwhich the multiset approach to supervised image segmentation is founded. Afterdescribing the deep learning and multiset neurons approaches, the present workdevelops comparison experiments between them which are primarily aimed atillustrating their respective main interesting features when applied to theadopted specific type of data and parameter configurations. While the deeplearning approach confirmed its potential for performing image segmentation,the alternative multiset methodology allowed for enhanced accuracy whilerequiring little computational resources.</description><author>Alexandre Benatti, Luciano da F. Costa</author><pubDate>Tue, 22 Aug 2023 17:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10123v3</guid></item><item><title>EndoNet: model for automatic calculation of H-score on histological slides</title><link>http://arxiv.org/abs/2308.11562v1</link><description>H-score is a semi-quantitative method used to assess the presence anddistribution of proteins in tissue samples by combining the intensity ofstaining and percentage of stained nuclei. It is widely used but time-consumingand can be limited in accuracy and precision. Computer-aided methods may helpovercome these limitations and improve the efficiency of pathologists'workflows. In this work, we developed a model EndoNet for automatic calculationof H-score on histological slides. Our proposed method uses neural networks andconsists of two main parts. The first is a detection model which predictskeypoints of centers of nuclei. The second is a H-score module which calculatesthe value of the H-score using mean pixel values of predicted keypoints. Ourmodel was trained and validated on 1780 annotated tiles with a shape of 100x100$\mu m$ and performed 0.77 mAP on a test dataset. Moreover, the model can beadjusted to a specific specialist or whole laboratory to reproduce the mannerof calculating the H-score. Thus, EndoNet is effective and robust in theanalysis of histology slides, which can improve and significantly acceleratethe work of pathologists.</description><author>Egor Ushakov, Anton Naumov, Vladislav Fomberg, Polina Vishnyakova, Aleksandra Asaturova, Alina Badlaeva, Anna Tregubova, Evgeny Karpulevich, Gennady Sukhikh, Timur Fatkhudinov</author><pubDate>Tue, 22 Aug 2023 17:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11562v1</guid></item><item><title>Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation</title><link>http://arxiv.org/abs/2308.11561v1</link><description>This report details the method of the winning entry of the AVDN Challenge inICCV 2023. The competition addresses the Aerial Navigation from Dialog History(ANDH) task, which requires a drone agent to associate dialog history withaerial observations to reach the destination. For better cross-modal groundingabilities of the drone agent, we propose a Target-Grounded Graph-AwareTransformer (TG-GAT) framework. Concretely, TG-GAT first leverages agraph-aware transformer to capture spatiotemporal dependency, which isbeneficial for navigation state tracking and robust action planning. TG-GATfirst leverages a graph-aware transformer to capture spatiotemporaldependencies for more robust action planning. In addition, an auxiliary visualgrounding task is devised to boost the agent's awareness of referred landmarks.Moreover, a hybrid augmentation strategy based on large language models isutilized to mitigate data scarcity limitations. Our TG-GAT framework won theAVDN Challenge 2023, with 2.2% and 3.0% absolute improvements over the baselineon SPL and SR metrics, respectively. The code is available athttps://github.com/yifeisu/avdn-challenge.</description><author>Yifei Su, Dong An, Yuan Xu, Kehan Chen, Yan Huang</author><pubDate>Tue, 22 Aug 2023 17:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11561v1</guid></item><item><title>SAFE: Machine Unlearning With Shard Graphs</title><link>http://arxiv.org/abs/2304.13169v2</link><description>We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt largemodels on a diverse collection of data while minimizing the expected cost toremove the influence of training samples from the trained model. This process,also known as selective forgetting or unlearning, is often conducted bypartitioning a dataset into shards, training fully independent models on each,then ensembling the resulting models. Increasing the number of shards reducesthe expected cost to forget but at the same time it increases inference costand reduces the final accuracy of the model since synergistic informationbetween samples is lost during the independent model training. Rather thantreating each shard as independent, SAFE introduces the notion of a shardgraph, which allows incorporating limited information from other shards duringtraining, trading off a modest increase in expected forgetting cost with asignificant increase in accuracy, all while still attaining complete removal ofresidual influence after forgetting. SAFE uses a lightweight system of adapterswhich can be trained while reusing most of the computations. This allows SAFEto be trained on shards an order-of-magnitude smaller than currentstate-of-the-art methods (thus reducing the forgetting costs) while alsomaintaining high accuracy, as we demonstrate empirically on fine-grainedcomputer vision datasets.</description><author>Yonatan Dukler, Benjamin Bowman, Alessandro Achille, Aditya Golatkar, Ashwin Swaminathan, Stefano Soatto</author><pubDate>Tue, 22 Aug 2023 17:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13169v2</guid></item><item><title>Open Set Synthetic Image Source Attribution</title><link>http://arxiv.org/abs/2308.11557v1</link><description>AI-generated images have become increasingly realistic and have garneredsignificant public attention. While synthetic images are intriguing due totheir realism, they also pose an important misinformation threat. To addressthis new threat, researchers have developed multiple algorithms to detectsynthetic images and identify their source generators. However, most existingsource attribution techniques are designed to operate in a closed-set scenario,i.e. they can only be used to discriminate between known image generators. Bycontrast, new image-generation techniques are rapidly emerging. To contend withthis, there is a great need for open-set source attribution techniques that canidentify when synthetic images have originated from new, unseen generators. Toaddress this problem, we propose a new metric learning-based approach. Ourtechnique works by learning transferrable embeddings capable of discriminatingbetween generators, even when they are not seen during training. An image isfirst assigned to a candidate generator, then is accepted or rejected based onits distance in the embedding space from known generators' learned referencepoints. Importantly, we identify that initializing our source attributionembedding network by pretraining it on image camera identification can improveour embeddings' transferability. Through a series of experiments, wedemonstrate our approach's ability to attribute the source of synthetic imagesin open-set scenarios.</description><author>Shengbang Fang, Tai D. Nguyen, Matthew C. Stamm</author><pubDate>Tue, 22 Aug 2023 17:37:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11557v1</guid></item><item><title>Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting</title><link>http://arxiv.org/abs/2308.10425v2</link><description>With the rapid development of the Intelligent Transportation System (ITS),accurate traffic forecasting has emerged as a critical challenge. The keybottleneck lies in capturing the intricate spatio-temporal traffic patterns. Inrecent years, numerous neural networks with complicated architectures have beenproposed to address this issue. However, the advancements in networkarchitectures have encountered diminishing performance gains. In this study, wepresent a novel component called spatio-temporal adaptive embedding that canyield outstanding results with vanilla transformers. Our proposedSpatio-Temporal Adaptive Embedding transformer (STAEformer) achievesstate-of-the-art performance on five real-world traffic forecasting datasets.Further experiments demonstrate that spatio-temporal adaptive embedding plays acrucial role in traffic forecasting by effectively capturing intrinsicspatio-temporal relations and chronological information in traffic time series.</description><author>Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quanjun Chen, Xuan Song</author><pubDate>Tue, 22 Aug 2023 17:36:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10425v2</guid></item><item><title>Multi-event Video-Text Retrieval</title><link>http://arxiv.org/abs/2308.11551v1</link><description>Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massivevideo-text data on the Internet. A plethora of work characterized by using atwo-stream Vision-Language model architecture that learns a jointrepresentation of video-text pairs has become a prominent approach for the VTRtask. However, these models operate under the assumption of bijectivevideo-text correspondences and neglect a more practical scenario where videocontent usually encompasses multiple events, while texts like user queries orwebpage metadata tend to be specific and correspond to single events. Thisestablishes a gap between the previous training objective and real-worldapplications, leading to the potential performance degradation of earliermodels during inference. In this study, we introduce the Multi-event Video-TextRetrieval (MeVTR) task, addressing scenarios in which each video containsmultiple different events, as a niche scenario of the conventional Video-TextRetrieval Task. We present a simple model, Me-Retriever, which incorporates keyevent video representation and a new MeVTR loss for the MeVTR task.Comprehensive experiments show that this straightforward framework outperformsother models in the Video-to-Text and Text-to-Video tasks, effectivelyestablishing a robust baseline for the MeVTR task. We believe this work servesas a strong foundation for future studies. Code is available athttps://github.com/gengyuanmax/MeVTR.</description><author>Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</author><pubDate>Tue, 22 Aug 2023 17:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11551v1</guid></item><item><title>Exploring the Landscape of Natural Language Processing Research</title><link>http://arxiv.org/abs/2307.10652v3</link><description>As an efficient approach to understand, generate, and process naturallanguage texts, research in natural language processing (NLP) has exhibited arapid spread and wide adoption in recent years. Given the increasing researchwork in this area, several NLP-related approaches have been surveyed in theresearch community. However, a comprehensive study that categorizes establishedtopics, identifies trends, and outlines areas for future research remainsabsent. Contributing to closing this gap, we have systematically classified andanalyzed research papers in the ACL Anthology. As a result, we present astructured overview of the research landscape, provide a taxonomy of fields ofstudy in NLP, analyze recent developments in NLP, summarize our findings, andhighlight directions for future work.</description><author>Tim Schopf, Karim Arabi, Florian Matthes</author><pubDate>Tue, 22 Aug 2023 17:30:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10652v3</guid></item><item><title>An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography</title><link>http://arxiv.org/abs/2307.00904v2</link><description>Purpose: To develop an open-source, fully-automatic deep learning algorithm,DeepGPET, for choroid region segmentation in optical coherence tomography (OCT)data. Methods: We used a dataset of 715 OCT B-scans (82 subjects, 115 eyes)from 3 clinical studies related to systemic disease. Ground truth segmentationswere generated using a clinically validated, semi-automatic choroidsegmentation method, Gaussian Process Edge Tracing (GPET). We finetuned a UNetwith MobileNetV3 backbone pre-trained on ImageNet. Standard segmentationagreement metrics, as well as derived measures of choroidal thickness and area,were used to evaluate DeepGPET, alongside qualitative evaluation from aclinical ophthalmologist. Results: DeepGPET achieves excellent agreement withGPET on data from 3 clinical studies (AUC=0.9994, Dice=0.9664; Pearsoncorrelation of 0.8908 for choroidal thickness and 0.9082 for choroidal area),while reducing the mean processing time per image on a standard laptop CPU from34.49s ($\pm$15.09) using GPET to 1.25s ($\pm$0.10) using DeepGPET. Bothmethods performed similarly according to a clinical ophthalmologist, whoqualitatively judged a subset of segmentations by GPET and DeepGPET, based onsmoothness and accuracy of segmentations. Conclusions :DeepGPET, afully-automatic, open-source algorithm for choroidal segmentation, will enableresearchers to efficiently extract choroidal measurements, even for largedatasets. As no manual interventions are required, DeepGPET is less subjectivethan semi-automatic methods and could be deployed in clinical practice withoutnecessitating a trained operator. DeepGPET addresses the lack of open-source,fully-automatic and clinically relevant choroid segmentation algorithms, andits subsequent public release will facilitate future choroidal research both inophthalmology and wider systemic health.</description><author>Jamie Burke, Justin Engelmann, Charlene Hamid, Megan Reid-Schachter, Tom Pearson, Dan Pugh, Neeraj Dhaun, Stuart King, Tom MacGillivray, Miguel O. Bernabeu, Amos Storkey, Ian J. C. MacCormick</author><pubDate>Tue, 22 Aug 2023 17:20:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00904v2</guid></item><item><title>H4VDM: H.264 Video Device Matching</title><link>http://arxiv.org/abs/2210.11549v3</link><description>Methods that can determine if two given video sequences are captured by thesame device (e.g., mobile telephone or digital camera) can be used in manyforensics tasks. In this paper we refer to this as "video device matching". Inopen-set video forensics scenarios it is easier to determine if two videosequences were captured with the same device than identifying the specificdevice. In this paper, we propose a technique for open-set video devicematching. Given two H.264 compressed video sequences, our method can determineif they are captured by the same device, even if our method has neverencountered the device in training. We denote our proposed technique as H.264Video Device Matching (H4VDM). H4VDM uses H.264 compression informationextracted from video sequences to make decisions. It is more robust againstartifacts that alter camera sensor fingerprints, and it can be used to analyzerelatively small fragments of the H.264 sequence. We trained and tested ourmethod on a publicly available video forensics dataset consisting of 35devices, where our proposed method demonstrated good performance.</description><author>Ziyue Xiang, Paolo Bestagini, Stefano Tubaro, Edward J. Delp</author><pubDate>Tue, 22 Aug 2023 17:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.11549v3</guid></item><item><title>An Instance Segmentation Dataset of Yeast Cells in Microstructures</title><link>http://arxiv.org/abs/2304.07597v3</link><description>Extracting single-cell information from microscopy data requires accurateinstance-wise segmentations. Obtaining pixel-wise segmentations from microscopyimagery remains a challenging task, especially with the added complexity ofmicrostructured environments. This paper presents a novel dataset forsegmenting yeast cells in microstructures. We offer pixel-wise instancesegmentation labels for both cells and trap microstructures. In total, werelease 493 densely annotated microscopy images. To facilitate a unifiedcomparison between novel segmentation algorithms, we propose a standardizedevaluation strategy for our dataset. The aim of the dataset and evaluationstrategy is to facilitate the development of new cell segmentation approaches.The dataset is publicly available athttps://christophreich1996.github.io/yeast_in_microstructures_dataset/ .</description><author>Christoph Reich, Tim Prangemeier, André O. Françani, Heinz Koeppl</author><pubDate>Tue, 22 Aug 2023 17:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07597v3</guid></item><item><title>Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing</title><link>http://arxiv.org/abs/2305.03881v2</link><description>Multi-modal search engines have experienced significant growth and widespreaduse in recent years, making them the second most common internet use. Whilesearch engine systems offer a range of services, the image search field hasrecently become a focal point in the information retrieval community, as theadage goes, "a picture is worth a thousand words". Although popular searchengines like Google excel at image search accuracy and agility, there is anongoing debate over whether their search results can be biased in terms ofgender, language, demographics, socio-cultural aspects, and stereotypes. Thispotential for bias can have a significant impact on individuals' perceptionsand influence their perspectives. In this paper, we present our study on bias and fairness in web search, witha focus on keyword-based image search. We first discuss several kinds of biasesthat exist in search systems and why it is important to mitigate them. Wenarrow down our study to assessing and mitigating occupational stereotypes inimage search, which is a prevalent fairness issue in image retrieval. For theassessment of stereotypes, we take gender as an indicator. We explore variousopen-source and proprietary APIs for gender identification from images. Withthese, we examine the extent of gender bias in top-tanked image search resultsobtained for several occupational keywords. To mitigate the bias, we thenpropose a fairness-aware re-ranking algorithm that optimizes (a) relevance ofthe search result with the keyword and (b) fairness w.r.t genders identified.We experiment on 100 top-ranked images obtained for 10 occupational keywordsand consider random re-ranking and re-ranking based on relevance as baselines.Our experimental results show that the fairness-aware re-ranking algorithmproduces rankings with better fairness scores and competitive relevance scoresthan the baselines.</description><author>Swagatika Dash</author><pubDate>Tue, 22 Aug 2023 17:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03881v2</guid></item><item><title>FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection</title><link>http://arxiv.org/abs/2308.10236v2</link><description>Lack of generalization to unseen domains/attacks is the Achilles heel of mostface presentation attack detection (FacePAD) algorithms. Existing attempts toenhance the generalizability of FacePAD solutions assume that data frommultiple source domains are available with a single entity to enablecentralized training. In practice, data from different source domains may becollected by diverse entities, who are often unable to share their data due tolegal and privacy constraints. While collaborative learning paradigms such asfederated learning (FL) can overcome this problem, standard FL methods areill-suited for domain generalization because they struggle to surmount the twinchallenges of handling non-iid client data distributions during training andgeneralizing to unseen domains during inference. In this work, a novelframework called Federated Split learning with Intermediate representationSampling (FedSIS) is introduced for privacy-preserving domain generalization.In FedSIS, a hybrid Vision Transformer (ViT) architecture is learned using acombination of FL and split learning to achieve robustness against statisticalheterogeneity in the client data distributions without any sharing of raw data(thereby preserving privacy). To further improve generalization to unseendomains, a novel feature augmentation strategy called intermediaterepresentation sampling is employed, and discriminative information fromintermediate blocks of a ViT is distilled using a shared adapter network. TheFedSIS approach has been evaluated on two well-known benchmarks forcross-domain FacePAD to demonstrate that it is possible to achievestate-of-the-art generalization performance without data sharing. Code:https://github.com/Naiftt/FedSIS</description><author>Naif Alkhunaizi, Koushik Srivatsan, Faris Almalik, Ibrahim Almakky, Karthik Nandakumar</author><pubDate>Tue, 22 Aug 2023 17:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10236v2</guid></item><item><title>BELB: a Biomedical Entity Linking Benchmark</title><link>http://arxiv.org/abs/2308.11537v1</link><description>Biomedical entity linking (BEL) is the task of grounding entity mentions to aknowledge base. It plays a vital role in information extraction pipelines forthe life sciences literature. We review recent work in the field and find that,as the task is absent from existing benchmarks for biomedical text mining,different studies adopt different experimental setups making comparisons basedon published numbers problematic. Furthermore, neural systems are testedprimarily on instances linked to the broad coverage knowledge base UMLS,leaving their performance to more specialized ones, e.g. genes or variants,understudied. We therefore developed BELB, a Biomedical Entity LinkingBenchmark, providing access in a unified format to 11 corpora linked to 7knowledge bases and spanning six entity types: gene, disease, chemical,species, cell line and variant. BELB greatly reduces preprocessing overhead intesting BEL systems on multiple corpora offering a standardized testbed forreproducible experiments. Using BELB we perform an extensive evaluation of sixrule-based entity-specific systems and three recent neural approachesleveraging pre-trained language models. Our results reveal a mixed pictureshowing that neural approaches fail to perform consistently across entitytypes, highlighting the need of further studies towards entity-agnostic models.</description><author>Samuele Garda, Leon Weber-Genzel, Robert Martin, Ulf Leser</author><pubDate>Tue, 22 Aug 2023 17:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11537v1</guid></item><item><title>A free from local minima algorithm for training regressive MLP neural networks</title><link>http://arxiv.org/abs/2308.11532v1</link><description>In this article an innovative method for training regressive MLP networks ispresented, which is not subject to local minima. The Error-Back-Propagationalgorithm, proposed by William-Hinton-Rummelhart, has had the merit offavouring the development of machine learning techniques, which has permeatedevery branch of research and technology since the mid-1980s. This extraordinarysuccess is largely due to the black-box approach, but this same factor was alsoseen as a limitation, as soon more challenging problems were approached. One ofthe most critical aspects of the training algorithms was that of local minimaof the loss function, typically the mean squared error of the output on thetraining set. In fact, as the most popular training algorithms are driven bythe derivatives of the loss function, there is no possibility to evaluate if areached minimum is local or global. The algorithm presented in this paperavoids the problem of local minima, as the training is based on the propertiesof the distribution of the training set, or better on its image internal to theneural network. The performance of the algorithm is shown for a well-knownbenchmark.</description><author>Augusto Montisci</author><pubDate>Tue, 22 Aug 2023 16:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11532v1</guid></item><item><title>Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law</title><link>http://arxiv.org/abs/2308.11531v1</link><description>Our project aims at helping and supporting stakeholders in refugee statusadjudications, such as lawyers, judges, governing bodies, and claimants, inorder to make better decisions through data-driven intelligence and increasethe understanding and transparency of the refugee application process for allinvolved parties. This PhD project has two primary objectives: (1) to retrievepast cases, and (2) to analyze legal decision-making processes on a dataset ofCanadian cases. In this paper, we present the current state of our work, whichincludes a completed experiment on part (1) and ongoing efforts related to part(2). We believe that NLP-based solutions are well-suited to address thesechallenges, and we investigate the feasibility of automating all stepsinvolved. In addition, we introduce a novel benchmark for future NLP researchin refugee law. Our methodology aims to be inclusive to all end-users andstakeholders, with expected benefits including reduced time-to-decision, fairerand more transparent outcomes, and improved decision quality.</description><author>Claire Barale</author><pubDate>Tue, 22 Aug 2023 16:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11531v1</guid></item><item><title>Furnishing Sound Event Detection with Language Model Abilities</title><link>http://arxiv.org/abs/2308.11530v1</link><description>Recently, the ability of language models (LMs) has attracted increasingattention in visual cross-modality. In this paper, we further explore thegeneration capacity of LMs for sound event detection (SED), beyond the visualdomain. Specifically, we propose an elegant method that aligns audio featuresand text features to accomplish sound event classification and temporallocation. The framework consists of an acoustic encoder, a contrastive modulethat align the corresponding representations of the text and audio, and adecoupled language decoder that generates temporal and event sequences from theaudio characteristic. Compared with conventional works that require complicatedprocessing and barely utilize limited audio features, our model is more conciseand comprehensive since language model directly leverage its semanticcapabilities to generate the sequences. We investigate different decouplingmodules to demonstrate the effectiveness for timestamps capture and eventclassification. Evaluation results show that the proposed method achievesaccurate sequences of sound event detection.</description><author>Hualei Wang, Jianguo Mao, Zhifang Guo, Jiarui Wan, Hong Liu, Xiangdong Wang</author><pubDate>Tue, 22 Aug 2023 16:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11530v1</guid></item><item><title>Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis</title><link>http://arxiv.org/abs/2306.10168v2</link><description>How can we tell whether two neural networks are utilizing the same internalprocesses for a particular computation? This question is pertinent for multiplesubfields of both neuroscience and machine learning, including neuroAI,mechanistic interpretability, and brain-machine interfaces. Standard approachesfor comparing neural networks focus on the spatial geometry of latent states.Yet in recurrent networks, computations are implemented at the level of neuraldynamics, which do not have a simple one-to-one mapping with geometry. Tobridge this gap, we introduce a novel similarity metric that compares twosystems at the level of their dynamics. Our method incorporates two components:Using recent advances in data-driven dynamical systems theory, we learn ahigh-dimensional linear system that accurately captures core features of theoriginal nonlinear dynamics. Next, we compare these linear approximations via anovel extension of Procrustes Analysis that accounts for how vector fieldschange under orthogonal transformation. Via four case studies, we demonstratethat our method effectively identifies and distinguishes dynamic structure inrecurrent neural networks (RNNs), whereas geometric methods fall short. Weadditionally show that our method can distinguish learning rules in anunsupervised manner. Our method therefore opens the door to novel data-drivenanalyses of the temporal structure of neural computation, and to more rigoroustesting of RNNs as models of the brain.</description><author>Mitchell Ostrow, Adam Eisen, Leo Kozachkov, Ila Fiete</author><pubDate>Tue, 22 Aug 2023 16:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10168v2</guid></item><item><title>Taken by Surprise: Contrast effect for Similarity Scores</title><link>http://arxiv.org/abs/2308.09765v2</link><description>Accurately evaluating the similarity of object vector embeddings is ofcritical importance for natural language processing, information retrieval andclassification tasks. Popular similarity scores (e.g cosine similarity) arebased on pairs of embedding vectors and disregard the distribution of theensemble from which objects are drawn. Human perception of object similaritysignificantly depends on the context in which the objects appear. In this workwe propose the $\textit{surprise score}$, an ensemble-normalized similaritymetric that encapsulates the contrast effect of human perception andsignificantly improves the classification performance on zero- and few-shotdocument classification tasks. This score quantifies the surprise to find agiven similarity between two elements relative to the pairwise ensemblesimilarities. We evaluate this metric on zero/few shot classification andclustering tasks and typically find 10-15 % better performance compared to rawcosine similarity. Our code is available athttps://github.com/MeetElise/surprise-similarity.</description><author>Thomas C. Bachlechner, Mario Martone, Marjorie Schillo</author><pubDate>Tue, 22 Aug 2023 16:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09765v2</guid></item><item><title>ReLiCADA -- Reservoir Computing using Linear Cellular Automata Design Algorithm</title><link>http://arxiv.org/abs/2308.11522v1</link><description>In this paper, we present a novel algorithm to optimize the design ofReservoir Computing using Cellular Automata models for time seriesapplications. Besides selecting the models' hyperparameters, the proposedalgorithm particularly solves the open problem of linear Cellular Automatonrule selection. The selection method pre-selects only a few promising candidaterules out of an exponentially growing rule space. When applied to relevantbenchmark datasets, the selected rules achieve low errors, with the best rulesbeing among the top 5% of the overall rule space. The algorithm was developedbased on mathematical analysis of linear Cellular Automaton properties and isbacked by almost one million experiments, adding up to a computational runtimeof nearly one year. Comparisons to other state-of-the-art time series modelsshow that the proposed Reservoir Computing using Cellular Automata models havelower computational complexity, at the same time, achieve lower errors. Hence,our approach reduces the time needed for training and hyperparameteroptimization by up to several orders of magnitude.</description><author>Jonas Kantic, Fabian C. Legl, Walter Stechele, Jakob Hermann</author><pubDate>Tue, 22 Aug 2023 16:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11522v1</guid></item><item><title>EM for Mixture of Linear Regression with Clustered Data</title><link>http://arxiv.org/abs/2308.11518v1</link><description>Modern data-driven and distributed learning frameworks deal with diversemassive data generated by clients spread across heterogeneous environments.Indeed, data heterogeneity is a major bottleneck in scaling up many distributedlearning paradigms. In many settings however, heterogeneous data may begenerated in clusters with shared structures, as is the case in severalapplications such as federated learning where a common latent variable governsthe distribution of all the samples generated by a client. It is thereforenatural to ask how the underlying clustered structures in distributed data canbe exploited to improve learning schemes. In this paper, we tackle thisquestion in the special case of estimating $d$-dimensional parameters of atwo-component mixture of linear regressions problem where each of $m$ nodesgenerates $n$ samples with a shared latent variable. We employ the well-knownExpectation-Maximization (EM) method to estimate the maximum likelihoodparameters from $m$ batches of dependent samples each containing $n$measurements. Discarding the clustered structure in the mixture model, EM isknown to require $O(\log(mn/d))$ iterations to reach the statistical accuracyof $O(\sqrt{d/(mn)})$. In contrast, we show that if initialized properly, EM onthe structured data requires only $O(1)$ iterations to reach the samestatistical accuracy, as long as $m$ grows up as $e^{o(n)}$. Our analysisestablishes and combines novel asymptotic optimization and generalizationguarantees for population and empirical EM with dependent samples, which may beof independent interest.</description><author>Amirhossein Reisizadeh, Khashayar Gatmiry, Asuman Ozdaglar</author><pubDate>Tue, 22 Aug 2023 16:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11518v1</guid></item><item><title>Exploiting Inter-Sample Affinity for Knowability-Aware Universal Domain Adaptation</title><link>http://arxiv.org/abs/2207.09280v5</link><description>Universal domain adaptation (UniDA) aims to transfer the knowledge of commonclasses from the source domain to the target domain without any prior knowledgeon the label set, which requires distinguishing in the target domain theunknown samples from the known ones. Recent methods usually focused oncategorizing a target sample into one of the source classes rather thandistinguishing known and unknown samples, which ignores the inter-sampleaffinity between known and unknown samples and may lead to suboptimalperformance. Aiming at this issue, we propose a novel UDA framework where suchinter-sample affinity is exploited. Specifically, we introduce aknowability-based labeling scheme which can be divided into two steps: 1)Knowability-guided detection of known and unknown samples based on theintrinsic structure of the neighborhoods of samples, where we leverage thefirst singular vectors of the affinity matrices to obtain the knowability ofevery target sample. 2) Label refinement based on neighborhood consistency torelabel the target samples, where we refine the labels of each target samplebased on its neighborhood consistency of predictions. Then, auxiliary lossesbased on the two steps are used to reduce the inter-sample affinity between theunknown and the known target samples. Finally, experiments on four publicdatasets demonstrate that our method significantly outperforms existingstate-of-the-art methods.</description><author>Yifan Wang, Lin Zhang, Ran Song, Hongliang Li, Paul L. Rosin, Wei Zhang</author><pubDate>Tue, 22 Aug 2023 16:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.09280v5</guid></item><item><title>TrackFlow: Multi-Object Tracking with Normalizing Flows</title><link>http://arxiv.org/abs/2308.11513v1</link><description>The field of multi-object tracking has recently seen a renewed interest inthe good old schema of tracking-by-detection, as its simplicity and strongpriors spare it from the complex design and painful babysitting oftracking-by-attention approaches. In view of this, we aim at extendingtracking-by-detection to multi-modal settings, where a comprehensive cost hasto be computed from heterogeneous information e.g., 2D motion cues, visualappearance, and pose estimates. More precisely, we follow a case study where arough estimate of 3D information is also available and must be merged withother traditional metrics (e.g., the IoU). To achieve that, recent approachesresort to either simple rules or complex heuristics to balance the contributionof each cost. However, i) they require careful tuning of tailoredhyperparameters on a hold-out set, and ii) they imply these costs to beindependent, which does not hold in reality. We address these issues bybuilding upon an elegant probabilistic formulation, which considers the cost ofa candidate association as the negative log-likelihood yielded by a deepdensity estimator, trained to model the conditional joint probabilitydistribution of correct associations. Our experiments, conducted on bothsimulated and real benchmarks, show that our approach consistently enhances theperformance of several tracking-by-detection algorithms.</description><author>Gianluca Mancusi, Aniello Panariello, Angelo Porrello, Matteo Fabbri, Simone Calderara, Rita Cucchiara</author><pubDate>Tue, 22 Aug 2023 16:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11513v1</guid></item><item><title>Tensor Principal Component Analysis</title><link>http://arxiv.org/abs/2212.12981v2</link><description>In this paper, we develop new methods for analyzing high-dimensional tensordatasets. A tensor factor model describes a high-dimensional dataset as a sumof a low-rank component and an idiosyncratic noise, generalizing traditionalfactor models for panel data. We propose an estimation algorithm, called tensorprincipal component analysis (TPCA), which generalizes the traditional PCAapplicable to panel data. The algorithm involves unfolding the tensor into asequence of matrices along different dimensions and applying PCA to theunfolded matrices. We provide theoretical results on the consistency andasymptotic distribution for the TPCA estimator of loadings and factors. We alsointroduce a novel test for the number of factors in a tensor factor model. TheTPCA and the test feature good performance in Monte Carlo experiments and areapplied to sorted portfolios.</description><author>Andrii Babii, Eric Ghysels, Junsu Pan</author><pubDate>Tue, 22 Aug 2023 16:39:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12981v2</guid></item><item><title>Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models</title><link>http://arxiv.org/abs/2308.11511v1</link><description>We explore element-wise convex combinations of two permutation-aligned neuralnetwork parameter vectors $\Theta_A$ and $\Theta_B$ of size $d$. We conductextensive experiments by examining various distributions of such modelcombinations parametrized by elements of the hypercube $[0,1]^{d}$ and itsvicinity. Our findings reveal that broad regions of the hypercube form surfacesof low loss values, indicating that the notion of linear mode connectivityextends to a more general phenomenon which we call mode combinability. We alsomake several novel observations regarding linear mode connectivity and modelre-basin. We demonstrate a transitivity property: two models re-based to acommon third model are also linear mode connected, and a robustness property:even with significant perturbations of the neuron matchings the resultingcombinations continue to form a working model. Moreover, we analyze thefunctional and weight similarity of model combinations and show that suchcombinations are non-vacuous in the sense that there are significant functionaldifferences between the resulting models.</description><author>Adrián Csiszárik, Melinda F. Kiss, Péter Kőrösi-Szabó, Márton Muntag, Gergely Papp, Dániel Varga</author><pubDate>Tue, 22 Aug 2023 16:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11511v1</guid></item><item><title>SwinFace: A Multi-task Transformer for Face Recognition, Expression Recognition, Age Estimation and Attribute Estimation</title><link>http://arxiv.org/abs/2308.11509v1</link><description>In recent years, vision transformers have been introduced into facerecognition and analysis and have achieved performance breakthroughs. However,most previous methods generally train a single model or an ensemble of modelsto perform the desired task, which ignores the synergy among different tasksand fails to achieve improved prediction accuracy, increased data efficiency,and reduced training time. This paper presents a multi-purpose algorithm forsimultaneous face recognition, facial expression recognition, age estimation,and face attribute estimation (40 attributes including gender) based on asingle Swin Transformer. Our design, the SwinFace, consists of a single sharedbackbone together with a subnet for each set of related tasks. To address theconflicts among multiple tasks and meet the different demands of tasks, aMulti-Level Channel Attention (MLCA) module is integrated into eachtask-specific analysis subnet, which can adaptively select the features fromoptimal levels and channels to perform the desired tasks. Extensive experimentsshow that the proposed model has a better understanding of the face andachieves excellent performance for all tasks. Especially, it achieves 90.97%accuracy on RAF-DB and 0.22 $\epsilon$-error on CLAP2015, which arestate-of-the-art results on facial expression recognition and age estimationrespectively. The code and models will be made publicly available athttps://github.com/lxq1000/SwinFace.</description><author>Lixiong Qin, Mei Wang, Chao Deng, Ke Wang, Xi Chen, Jiani Hu, Weihong Deng</author><pubDate>Tue, 22 Aug 2023 16:38:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11509v1</guid></item><item><title>On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks</title><link>http://arxiv.org/abs/2306.05557v2</link><description>Graph Neural Network (GNN) research has highlighted a relationship betweenhigh homophily (i.e., the tendency of nodes of the same class to connect) andstrong predictive performance in node classification. However, recent work hasfound the relationship to be more nuanced, demonstrating that simple GNNs canlearn in certain heterophilous settings. To resolve these conflicting findingsand align closer to real-world datasets, we go beyond the assumption of aglobal graph homophily level and study the performance of GNNs when the localhomophily level of a node deviates from the global homophily level. Throughtheoretical and empirical analysis, we systematically demonstrate how shifts inlocal homophily can introduce performance degradation, leading to performancediscrepancies across local homophily levels. We ground the practicalimplications of this work through granular analysis on five real-world datasetswith varying global homophily levels, demonstrating that (a) GNNs can fail togeneralize to test nodes that deviate from the global homophily of a graph, and(b) high local homophily does not necessarily confer high performance for anode. We further show that GNNs designed for globally heterophilous graphs canalleviate performance discrepancy by improving performance across localhomophily levels, offering a new perspective on how these GNNs achieve strongerglobal performance.</description><author>Donald Loveland, Jiong Zhu, Mark Heimann, Benjamin Fish, Michael T. Shaub, Danai Koutra</author><pubDate>Tue, 22 Aug 2023 16:37:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05557v2</guid></item><item><title>DClEVerNet: Deep Combinatorial Learning for Efficient EV Charging Scheduling in Large-scale Networked Facilities</title><link>http://arxiv.org/abs/2305.11195v2</link><description>With the electrification of transportation, the rising uptake of electricvehicles (EVs) might stress distribution networks significantly, leaving theirperformance degraded and stability jeopardized. To accommodate these new loadscost-effectively, modern power grids require coordinated or ``smart'' chargingstrategies capable of optimizing EV charging scheduling in a scalable andefficient fashion. With this in view, the present work focuses on reservationmanagement programs for large-scale, networked EV charging stations. Weformulate a time-coupled binary optimization problem that maximizes EV users'total welfare gain while accounting for the network's available power capacityand stations' occupancy limits. To tackle the problem at scale while retaininghigh solution quality, a data-driven optimization framework combiningtechniques from the fields of Deep Learning and Approximation Algorithms isintroduced. The framework's key ingredient is a novel input-output processingscheme for neural networks that allows direct extrapolation to problem sizessubstantially larger than those included in the training set. Extensivenumerical simulations based on synthetic and real-world data traces verify theeffectiveness and superiority of the presented approach over two representativescheduling algorithms. Lastly, we round up the contributions by listing severalimmediate extensions to the proposed framework and outlining the prospects forfurther exploration.</description><author>Bushra Alshehhi, Areg Karapetyan, Khaled Elbassioni, Sid Chi-Kin Chau, Majid Khonji</author><pubDate>Tue, 22 Aug 2023 16:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11195v2</guid></item><item><title>Unsupervised Prototype Adapter for Vision-Language Models</title><link>http://arxiv.org/abs/2308.11507v1</link><description>Recently, large-scale pre-trained vision-language models (e.g. CLIP andALIGN) have demonstrated remarkable effectiveness in acquiring transferablevisual representations. To leverage the valuable knowledge encoded within thesemodels for downstream tasks, several fine-tuning approaches, including prompttuning methods and adapter-based methods, have been developed to adaptvision-language models effectively with supervision. However, these methodsrely on the availability of annotated samples, which can be labor-intensive andtime-consuming to acquire, thus limiting scalability. To address this issue, inthis work, we design an unsupervised fine-tuning approach for vision-languagemodels called Unsupervised Prototype Adapter (UP-Adapter). Specifically, forthe unannotated target datasets, we leverage the text-image aligning capabilityof CLIP to automatically select the most confident samples for each class.Utilizing these selected samples, we generate class prototypes, which serve asthe initialization for the learnable prototype model. After fine-tuning, theprototype model prediction is combined with the original CLIP's prediction by aresidual connection to perform downstream recognition tasks. Our extensiveexperimental results on image recognition and domain generalization show thatthe proposed unsupervised method outperforms 8-shot CoOp, 8-shot Tip-Adapter,and also the state-of-the-art UPL method by large margins.</description><author>Yi Zhang, Ce Zhang, Xueting Hu, Zhihai He</author><pubDate>Tue, 22 Aug 2023 16:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11507v1</guid></item><item><title>LCCo: Lending CLIP to Co-Segmentation</title><link>http://arxiv.org/abs/2308.11506v1</link><description>This paper studies co-segmenting the common semantic object in a set ofimages. Existing works either rely on carefully engineered networks to mine theimplicit semantic information in visual features or require extra data (i.e.,classification labels) for training. In this paper, we leverage the contrastivelanguage-image pre-training framework (CLIP) for the task. With a backbonesegmentation network that independently processes each image from the set, weintroduce semantics from CLIP into the backbone features, refining them in acoarse-to-fine manner with three key modules: i) an image set featurecorrespondence module, encoding global consistent semantic information of theimage set; ii) a CLIP interaction module, using CLIP-mined common semantics ofthe image set to refine the backbone feature; iii) a CLIP regularizationmodule, drawing CLIP towards this co-segmentation task, identifying the bestCLIP semantic and using it to regularize the backbone feature. Experiments onfour standard co-segmentation benchmark datasets show that the performance ofour method outperforms state-of-the-art methods.</description><author>Xin Duan, Yan Yang, Liyuan Pan, Xiabi Liu</author><pubDate>Tue, 22 Aug 2023 16:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11506v1</guid></item><item><title>Improving automatic endoscopic stone recognition using a multi-view fusion approach enhanced with two-step transfer learning</title><link>http://arxiv.org/abs/2304.03193v2</link><description>This contribution presents a deep-learning method for extracting and fusingimage information acquired from different viewpoints, with the aim to producemore discriminant object features for the identification of the type of kidneystones seen in endoscopic images. The model was further improved with atwo-step transfer learning approach and by attention blocks to refine thelearned feature maps. Deep feature fusion strategies improved the results ofsingle view extraction backbone models by more than 6% in terms of accuracy ofthe kidney stones classification.</description><author>Francisco Lopez-Tiro, Elias Villalvazo-Avila, Juan Pablo Betancur-Rengifo, Ivan Reyes-Amezcua, Jacques Hubert, Gilberto Ochoa-Ruiz, Christian Daul</author><pubDate>Tue, 22 Aug 2023 16:23:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03193v2</guid></item><item><title>Can Authorship Representation Learning Capture Stylistic Features?</title><link>http://arxiv.org/abs/2308.11490v1</link><description>Automatically disentangling an author's style from the content of theirwriting is a longstanding and possibly insurmountable problem in computationallinguistics. At the same time, the availability of large text corpora furnishedwith author labels has recently enabled learning authorship representations ina purely data-driven manner for authorship attribution, a task that ostensiblydepends to a greater extent on encoding writing style than encoding content.However, success on this surrogate task does not ensure that suchrepresentations capture writing style since authorship could also be correlatedwith other latent variables, such as topic. In an effort to better understandthe nature of the information these representations convey, and specifically tovalidate the hypothesis that they chiefly encode writing style, wesystematically probe these representations through a series of targetedexperiments. The results of these experiments suggest that representationslearned for the surrogate authorship prediction task are indeed sensitive towriting style. As a consequence, authorship representations may be expected tobe robust to certain kinds of data shift, such as topic drift over time.Additionally, our findings may open the door to downstream applications thatrequire stylistic representations, such as style transfer.</description><author>Andrew Wang, Cristina Aggazzotti, Rebecca Kotula, Rafael Rivera Soto, Marcus Bishop, Nicholas Andrews</author><pubDate>Tue, 22 Aug 2023 16:10:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11490v1</guid></item><item><title>Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition</title><link>http://arxiv.org/abs/2308.11489v1</link><description>We are concerned with a challenging scenario in unpaired multiview videolearning. In this case, the model aims to learn comprehensive multiviewrepresentations while the cross-view semantic information exhibits variations.We propose Semantics-based Unpaired Multiview Learning (SUM-L) to tackle thisunpaired multiview learning problem. The key idea is to build cross-viewpseudo-pairs and do view-invariant alignment by leveraging the semanticinformation of videos. To facilitate the data efficiency of multiview learning,we further perform video-text alignment for first-person and third-personvideos, to fully leverage the semantic knowledge to improve videorepresentations. Extensive experiments on multiple benchmark datasets verifythe effectiveness of our framework. Our method also outperforms multipleexisting view-alignment methods, under the more challenging scenario thantypical paired or unpaired multimodal or multiview learning. Our code isavailable at https://github.com/wqtwjt1996/SUM-L.</description><author>Qitong Wang, Long Zhao, Liangzhe Yuan, Ting Liu, Xi Peng</author><pubDate>Tue, 22 Aug 2023 16:10:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11489v1</guid></item><item><title>Opening the Vocabulary of Egocentric Actions</title><link>http://arxiv.org/abs/2308.11488v1</link><description>Human actions in egocentric videos are often hand-object interactionscomposed from a verb (performed by the hand) applied to an object. Despitetheir extensive scaling up, egocentric datasets still face two limitations -sparsity of action compositions and a closed set of interacting objects. Thispaper proposes a novel open vocabulary action recognition task. Given a set ofverbs and objects observed during training, the goal is to generalize the verbsto an open vocabulary of actions with seen and novel objects. To this end, wedecouple the verb and object predictions via an object-agnostic verb encoderand a prompt-based object encoder. The prompting leverages CLIP representationsto predict an open vocabulary of interacting objects. We create open vocabularybenchmarks on the EPIC-KITCHENS-100 and Assembly101 datasets; whereasclosed-action methods fail to generalize, our proposed method is effective. Inaddition, our object encoder significantly outperforms existing open-vocabularyvisual recognition methods in recognizing novel interacting objects.</description><author>Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</author><pubDate>Tue, 22 Aug 2023 16:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11488v1</guid></item><item><title>Free Lunch for Gait Recognition: A Novel Relation Descriptor</title><link>http://arxiv.org/abs/2308.11487v1</link><description>Gait recognition is to seek correct matches for query individuals by theirunique walking patterns at a long distance. However, current methods focussolely on individual gait features, disregarding inter-personal relationships.In this paper, we reconsider gait representation, asserting that gait is notjust an aggregation of individual features, but also the relationships amongdifferent subjects' gait features once reference gaits are established. Fromthis perspective, we redefine classifier weights as reference-anchored gaits,allowing each person's gait to be described by their relationship with thesereferences. In our work, we call this novel descriptor Relationship Descriptor(RD). This Relationship Descriptor offers two benefits: emphasizing meaningfulfeatures and enhancing robustness. To be specific, The normalized dot productbetween gait features and classifier weights signifies a similarity relation,where each dimension indicates the similarity between the test sample and eachtraining ID's gait prototype, respectively. Despite its potential, the directuse of relationship descriptors poses dimensionality challenges since thedimension of RD depends on the training set's identity count. To address this,we propose a Farthest Anchored gaits Selection algorithm and a dimensionreduction method to boost gait recognition performance. Our method can be builton top of off-the-shelf pre-trained classification-based models without extraparameters. We show that RD achieves higher recognition performance thandirectly using extracted features. We evaluate the effectiveness of our methodon the popular GREW, Gait3D, CASIA-B, and OU-MVLP, showing that our methodconsistently outperforms the baselines and achieves state-of-the-artperformances.</description><author>Jilong Wang, Saihui Hou, Yan Huang, Chunshui Cao, Xu Liu, Yongzhen Huang, Liang Wang</author><pubDate>Tue, 22 Aug 2023 16:06:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11487v1</guid></item><item><title>Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features</title><link>http://arxiv.org/abs/2308.11485v1</link><description>Given a query composed of a reference image and a relative caption, theComposed Image Retrieval goal is to retrieve images visually similar to thereference one that integrates the modifications expressed by the caption. Giventhat recent research has demonstrated the efficacy of large-scale vision andlanguage pre-trained (VLP) models in various tasks, we rely on features fromthe OpenAI CLIP model to tackle the considered task. We initially perform atask-oriented fine-tuning of both CLIP encoders using the element-wise sum ofvisual and textual features. Then, in the second stage, we train a Combinernetwork that learns to combine the image-text features integrating the bimodalinformation and providing combined features used to perform the retrieval. Weuse contrastive learning in both stages of training. Starting from the bareCLIP features as a baseline, experimental results show that the task-orientedfine-tuning and the carefully crafted Combiner network are highly effective andoutperform more complex state-of-the-art approaches on FashionIQ and CIRR, twopopular and challenging datasets for composed image retrieval. Code andpre-trained models are available at https://github.com/ABaldrati/CLIP4Cir</description><author>Alberto Baldrati, Marco Bertini, Tiberio Uricchio, Alberto del Bimbo</author><pubDate>Tue, 22 Aug 2023 16:03:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11485v1</guid></item><item><title>Pose2Gait: Extracting Gait Features from Monocular Video of Individuals with Dementia</title><link>http://arxiv.org/abs/2308.11484v1</link><description>Video-based ambient monitoring of gait for older adults with dementia has thepotential to detect negative changes in health and allow clinicians andcaregivers to intervene early to prevent falls or hospitalizations. Computervision-based pose tracking models can process video data automatically andextract joint locations; however, publicly available models are not optimizedfor gait analysis on older adults or clinical populations. In this work wetrain a deep neural network to map from a two dimensional pose sequence,extracted from a video of an individual walking down a hallway toward awall-mounted camera, to a set of three-dimensional spatiotemporal gait featuresaveraged over the walking sequence. The data of individuals with dementia usedin this work was captured at two sites using a wall-mounted system to collectthe video and depth information used to train and evaluate our model. OurPose2Gait model is able to extract velocity and step length values from thevideo that are correlated with the features from the depth camera, withSpearman's correlation coefficients of .83 and .60 respectively, showing thatthree dimensional spatiotemporal features can be predicted from monocularvideo. Future work remains to improve the accuracy of other features, such asstep time and step width, and test the utility of the predicted values fordetecting meaningful changes in gait during longitudinal ambient monitoring.</description><author>Caroline Malin-Mayor, Vida Adeli, Andrea Sabo, Sergey Noritsyn, Carolina Gorodetsky, Alfonso Fasano, Andrea Iaboni, Babak Taati</author><pubDate>Tue, 22 Aug 2023 15:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11484v1</guid></item><item><title>Constrained Probabilistic Mask Learning for Task-specific Undersampled MRI Reconstruction</title><link>http://arxiv.org/abs/2305.16376v2</link><description>Undersampling is a common method in Magnetic Resonance Imaging (MRI) tosubsample the number of data points in k-space, reducing acquisition times atthe cost of decreased image quality. A popular approach is to employundersampling patterns following various strategies, e.g., variable densitysampling or radial trajectories. In this work, we propose a method thatdirectly learns the undersampling masks from data points, thereby alsoproviding task- and domain-specific patterns. To solve the resulting discreteoptimization problem, we propose a general optimization routine called ProM: Afully probabilistic, differentiable, versatile, and model-free framework formask optimization that enforces acceleration factors through a convexconstraint. Analyzing knee, brain, and cardiac MRI datasets with our method, wediscover that different anatomic regions reveal distinct optimal undersamplingmasks, demonstrating the benefits of using custom masks, tailored for adownstream task. For example, ProM can create undersampling masks that maximizeperformance in downstream tasks like segmentation with networks trained onfully-sampled MRIs. Even with extreme acceleration factors, ProM yieldsreasonable performance while being more versatile than existing methods, pavingthe way for data-driven all-purpose mask generation.</description><author>Tobias Weber, Michael Ingrisch, Bernd Bischl, David Rügamer</author><pubDate>Tue, 22 Aug 2023 15:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16376v2</guid></item><item><title>Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</title><link>http://arxiv.org/abs/2308.11483v1</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities invarious NLP tasks. However, previous works have shown these models aresensitive towards prompt wording, and few-shot demonstrations and their order,posing challenges to fair assessment of these models. As these models becomemore powerful, it becomes imperative to understand and address theselimitations. In this paper, we focus on LLMs robustness on the task ofmultiple-choice questions -- commonly adopted task to study reasoning andfact-retrieving capability of LLMs. Investigating the sensitivity of LLMstowards the order of options in multiple-choice questions, we demonstrate aconsiderable performance gap of approximately 13% to 75% in LLMs on differentbenchmarks, when answer options are reordered, even when using demonstrationsin a few-shot setting. Through a detailed analysis, we conjecture that thissensitivity arises when LLMs are uncertain about the prediction between thetop-2/3 choices, and specific options placements may favor certain predictionbetween those top choices depending on the question caused by positional bias.We also identify patterns in top-2 choices that amplify or mitigate the model'sbias toward option placement. We found that for amplifying bias, the optimalstrategy involves positioning the top two choices as the first and lastoptions. Conversely, to mitigate bias, we recommend placing these choices amongthe adjacent options. To validate our conjecture, we conduct variousexperiments and adopt two approaches to calibrate LLMs' predictions, leading toup to 8 percentage points improvement across different models and benchmarks.</description><author>Pouya Pezeshkpour, Estevam Hruschka</author><pubDate>Tue, 22 Aug 2023 15:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11483v1</guid></item><item><title>Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection</title><link>http://arxiv.org/abs/2308.11480v1</link><description>Improving the reliability of deployed machine learning systems often involvesdeveloping methods to detect out-of-distribution (OOD) inputs. However,existing research often narrowly focuses on samples from classes that areabsent from the training set, neglecting other types of plausible distributionshifts. This limitation reduces the applicability of these methods inreal-world scenarios, where systems encounter a wide variety of anomalousinputs. In this study, we categorize five distinct types of distribution shiftsand critically evaluate the performance of recent OOD detection methods on eachof them. We publicly release our benchmark under the name BROAD (BenchmarkingResilience Over Anomaly Diversity). Our findings reveal that while thesemethods excel in detecting unknown classes, their performance is inconsistentwhen encountering other types of distribution shifts. In other words, they onlyreliably detect unexpected inputs that they have been specifically designed toexpect. As a first step toward broad OOD detection, we learn a generative modelof existing detection scores with a Gaussian mixture. By doing so, we presentan ensemble approach that offers a more consistent and comprehensive solutionfor broad OOD detection, demonstrating superior performance compared toexisting methods. Our code to download BROAD and reproduce our experiments ispublicly available.</description><author>Charles Guille-Escuret, Pierre-André Noël, Ioannis Mitliagkas, David Vazquez, Joao Monteiro</author><pubDate>Tue, 22 Aug 2023 15:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11480v1</guid></item><item><title>Revisiting column-generation-based matheuristic for learning classification trees</title><link>http://arxiv.org/abs/2308.11477v1</link><description>Decision trees are highly interpretable models for solving classificationproblems in machine learning (ML). The standard ML algorithms for trainingdecision trees are fast but generate suboptimal trees in terms of accuracy.Other discrete optimization models in the literature address the optimalityproblem but only work well on relatively small datasets. \cite{firat2020column}proposed a column-generation-based heuristic approach for learning decisiontrees. This approach improves scalability and can work with large datasets. Inthis paper, we describe improvements to this column generation approach. First,we modify the subproblem model to significantly reduce the number ofsubproblems in multiclass classification instances. Next, we show that thedata-dependent constraints in the master problem are implied, and use them ascutting planes. Furthermore, we describe a separation model to generate datapoints for which the linear programming relaxation solution violates theircorresponding constraints. We conclude by presenting computational results thatshow that these modifications result in better scalability.</description><author>Krunal Kishor Patel, Guy Desaulniers, Andrea Lodi</author><pubDate>Tue, 22 Aug 2023 15:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11477v1</guid></item><item><title>IT3D: Improved Text-to-3D Generation with Explicit View Synthesis</title><link>http://arxiv.org/abs/2308.11473v1</link><description>Recent strides in Text-to-3D techniques have been propelled by distillingknowledge from powerful large text-to-image diffusion models (LDMs).Nonetheless, existing Text-to-3D approaches often grapple with challenges suchas over-saturation, inadequate detailing, and unrealistic outputs. This studypresents a novel strategy that leverages explicitly synthesized multi-viewimages to address these issues. Our approach involves the utilization ofimage-to-image pipelines, empowered by LDMs, to generate posed high-qualityimages based on the renderings of coarse 3D models. Although the generatedimages mostly alleviate the aforementioned issues, challenges such as viewinconsistency and significant content variance persist due to the inherentgenerative nature of large diffusion models, posing extensive difficulties inleveraging these images effectively. To overcome this hurdle, we advocateintegrating a discriminator alongside a novel Diffusion-GAN dual trainingstrategy to guide the training of 3D models. For the incorporateddiscriminator, the synthesized multi-view images are considered real data,while the renderings of the optimized 3D models function as fake data. Weconduct a comprehensive set of experiments that demonstrate the effectivenessof our method over baseline approaches.</description><author>Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, Guosheng Lin</author><pubDate>Tue, 22 Aug 2023 15:39:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11473v1</guid></item><item><title>Stabilizing Unsupervised Environment Design with a Learned Adversary</title><link>http://arxiv.org/abs/2308.10797v2</link><description>A key challenge in training generally-capable agents is the design oftraining tasks that facilitate broad generalization and robustness toenvironment variations. This challenge motivates the problem setting ofUnsupervised Environment Design (UED), whereby a student agent trains on anadaptive distribution of tasks proposed by a teacher agent. A pioneeringapproach for UED is PAIRED, which uses reinforcement learning (RL) to train ateacher policy to design tasks from scratch, making it possible to directlygenerate tasks that are adapted to the agent's current capabilities. Despiteits strong theoretical backing, PAIRED suffers from a variety of challengesthat hinder its practical performance. Thus, state-of-the-art methods currentlyrely on curation and mutation rather than generation of new tasks. In thiswork, we investigate several key shortcomings of PAIRED and propose solutionsfor each shortcoming. As a result, we make it possible for PAIRED to match orexceed state-of-the-art methods, producing robust agents in several establishedchallenging procedurally-generated environments, including a partially-observedmaze navigation task and a continuous-control car racing environment. Webelieve this work motivates a renewed emphasis on UED methods based on learnedmodels that directly generate challenging environments, potentially unlockingmore open-ended RL training and, as a result, more general agents.</description><author>Ishita Mediratta, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, Tim Rocktäschel</author><pubDate>Tue, 22 Aug 2023 15:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10797v2</guid></item><item><title>Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)</title><link>http://arxiv.org/abs/2308.11471v1</link><description>This work targets what we consider to be the foundational step for urbanairborne robots, a safe landing. Our attention is directed toward what we deemthe most crucial aspect of the safe landing perception stack: segmentation. Wepresent a streamlined reactive UAV system that employs visual servoing byharnessing the capabilities of open vocabulary image segmentation. Thisapproach can adapt to various scenarios with minimal adjustments, bypassing thenecessity for extensive data accumulation for refining internal models, thanksto its open vocabulary methodology. Given the limitations imposed by localauthorities, our primary focus centers on operations originating from altitudesof 100 meters. This choice is deliberate, as numerous preceding works havedealt with altitudes up to 30 meters, aligning with the capabilities of smallstereo cameras. Consequently, we leave the remaining 20m to be navigated usingconventional 3D path planning methods. Utilizing monocular cameras and imagesegmentation, our findings demonstrate the system's capability to successfullyexecute landing maneuvers at altitudes as low as 20 meters. However, thisapproach is vulnerable to intermittent and occasionally abrupt fluctuations inthe segmentation between frames in a video stream. To address this challenge,we enhance the image segmentation output by introducing what we call a dynamicfocus: a masking mechanism that self adjusts according to the current landingstage. This dynamic focus guides the control system to avoid regions beyond thedrone's safety radius projected onto the ground, thus mitigating the problemswith fluctuations. Through the implementation of this supplementary layer, ourexperiments have reached improvements in the landing success rate of almosttenfold when compared to global segmentation. All the source code is opensource and available online (github.com/MISTLab/DOVESEI).</description><author>Haechan Mark Bon, Rongge Zhang, Ricardo de Azambuja, Giovanni Beltrame</author><pubDate>Tue, 22 Aug 2023 15:36:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11471v1</guid></item><item><title>An Effective Method using Phrase Mechanism in Neural Machine Translation</title><link>http://arxiv.org/abs/2308.10482v2</link><description>Machine Translation is one of the essential tasks in Natural LanguageProcessing (NLP), which has massive applications in real life as well ascontributing to other tasks in the NLP research community. Recently,Transformer -based methods have attracted numerous researchers in this domainand achieved state-of-the-art results in most of the pair languages. In thispaper, we report an effective method using a phrase mechanism,PhraseTransformer, to improve the strong baseline model Transformer inconstructing a Neural Machine Translation (NMT) system for parallel corporaVietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022competition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2BLEU scores on Chinese to Vietnamese data. Our code is available athttps://github.com/phuongnm94/PhraseTransformer.</description><author>Phuong Minh Nguyen, Le Minh Nguyen</author><pubDate>Tue, 22 Aug 2023 15:33:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10482v2</guid></item><item><title>Normative Conditional Reasoning as a Fragment of HOL</title><link>http://arxiv.org/abs/2308.10686v2</link><description>We report some results regarding the mechanization of normative(preference-based) conditional reasoning. Our focus is on Aqvist's system E forconditional obligation (and its extensions). Our mechanization is achieved viaa shallow semantical embedding in Isabelle/HOL. We consider two possible usesof the framework. The first one is as a tool for meta-reasoning about theconsidered logic. We employ it for the automated verification of deonticcorrespondences (broadly conceived) and related matters, analogous to what hasbeen previously achieved for the modal logic cube. The second use is as a toolfor assessing ethical arguments. We provide a computer encoding of a well-knownparadox in population ethics, Parfit's repugnant conclusion. Whether thepresented encoding increases or decreases the attractiveness and persuasivenessof the repugnant conclusion is a question we would like to pass on tophilosophy and ethics.</description><author>Xavier Parent, Christoph Benzmüller</author><pubDate>Tue, 22 Aug 2023 15:29:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10686v2</guid></item><item><title>Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms</title><link>http://arxiv.org/abs/2308.11468v1</link><description>The aim of this work is to perform a multitemporal analysis using the GoogleEarth Engine (GEE) platform for the detection of changes in urban areas usingoptical data and specific machine learning (ML) algorithms. As a case study,Cairo City has been identified, in Egypt country, as one of the five mostpopulous megacities of the last decade in the world. Classification and changedetection analysis of the region of interest (ROI) have been carried out fromJuly 2013 to July 2021. Results demonstrate the validity of the proposed methodin identifying changed and unchanged urban areas over the selected period.Furthermore, this work aims to evidence the growing significance of GEE as anefficient cloud-based solution for managing large quantities of satellite data.</description><author>Mariapia Rita Iandolo, Francesca Razzano, Chiara Zarro, G. S. Yogesh, Silvia Liberata Ullo</author><pubDate>Tue, 22 Aug 2023 15:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11468v1</guid></item><item><title>Evading Watermark based Detection of AI-Generated Content</title><link>http://arxiv.org/abs/2305.03807v3</link><description>A generative AI model can generate extremely realistic-looking content,posing growing challenges to the authenticity of information. To address thechallenges, watermark has been leveraged to detect AI-generated content.Specifically, a watermark is embedded into an AI-generated content before it isreleased. A content is detected as AI-generated if a similar watermark can bedecoded from it. In this work, we perform a systematic study on the robustnessof such watermark-based AI-generated content detection. We focus onAI-generated images. Our work shows that an attacker can post-process awatermarked image via adding a small, human-imperceptible perturbation to it,such that the post-processed image evades detection while maintaining itsvisual quality. We show the effectiveness of our attack both theoretically andempirically. Moreover, to evade detection, our adversarial post-processingmethod adds much smaller perturbations to AI-generated images and thus bettermaintain their visual quality than existing popular post-processing methodssuch as JPEG compression, Gaussian blur, and Brightness/Contrast. Our workshows the insufficiency of existing watermark-based detection of AI-generatedcontent, highlighting the urgent needs of new methods. Our code is publiclyavailable: https://github.com/zhengyuan-jiang/WEvade.</description><author>Zhengyuan Jiang, Jinghuai Zhang, Neil Zhenqiang Gong</author><pubDate>Tue, 22 Aug 2023 15:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03807v3</guid></item><item><title>Sentence-Level Multimodal and Language-Agnostic Representations</title><link>http://arxiv.org/abs/2308.11466v1</link><description>We introduce SONAR, a new multilingual and multimodal fixed-size sentenceembedding space. Our single text encoder, covering 200 languages, substantiallyoutperforms existing sentence embeddings such as LASER3 and LabSE on the xsimand xsim++ multilingual similarity search tasks. Speech segments can beembedded in the same SONAR embedding space using language-specific speechencoders trained in a teacher-student setting on speech transcription data. Ourencoders outperform existing speech encoders on similarity search tasks. Wealso provide a text decoder for 200 languages, which allows us to performtext-to-text and speech-to-text machine translation, including for zero-shotlanguage and modality combinations. Our text-to-text results are competitivecompared to the state-of-the-art NLLB~1B model, despite the fixed-sizebottleneck representation. Our zero-shot speech-to-text translation resultscompare favorably with strong supervised baselines such as Whisper.</description><author>Paul-Ambroise Duquenne, Holger Schwenk, Benoît Sagot</author><pubDate>Tue, 22 Aug 2023 15:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11466v1</guid></item><item><title>Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning</title><link>http://arxiv.org/abs/2308.11464v1</link><description>Federated learning (FL) inevitably confronts the challenge of systemheterogeneity in practical scenarios. To enhance the capabilities of mostmodel-homogeneous FL methods in handling system heterogeneity, we propose atraining scheme that can extend their capabilities to cope with this challenge.In this paper, we commence our study with a detailed exploration of homogeneousand heterogeneous FL settings and discover three key observations: (1) apositive correlation between client performance and layer similarities, (2)higher similarities in the shallow layers in contrast to the deep layers, and(3) the smoother gradients distributions indicate the higher layersimilarities. Building upon these observations, we propose InCo Aggregationthat leverags internal cross-layer gradients, a mixture of gradients fromshallow and deep layers within a server model, to augment the similarity in thedeep layers without requiring additional communication between clients.Furthermore, our methods can be tailored to accommodate model-homogeneous FLmethods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand theircapabilities to handle the system heterogeneity. Copious experimental resultsvalidate the effectiveness of InCo Aggregation, spotlighting internalcross-layer gradients as a promising avenue to enhance the performance inheterogenous FL.</description><author>Yun-Hin Chan, Rui Zhou, Running Zhao, Zhihan Jiang, Edith C. -H. Ngai</author><pubDate>Tue, 22 Aug 2023 15:23:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11464v1</guid></item><item><title>Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage</title><link>http://arxiv.org/abs/2305.09659v2</link><description>In this paper, we study distributionally robust offline reinforcementlearning (robust offline RL), which seeks to find an optimal policy purely froman offline dataset that can perform well in perturbed environments. Inspecific, we propose a generic algorithm framework called Doubly PessimisticModel-based Policy Optimization ($P^2MPO$), which features a novel combinationof a flexible model estimation subroutine and a doubly pessimistic policyoptimization step. Notably, the double pessimism principle is crucial toovercome the distributional shifts incurred by (i) the mismatch between thebehavior policy and the target policies; and (ii) the perturbation of thenominal model. Under certain accuracy conditions on the model estimationsubroutine, we prove that $P^2MPO$ is sample-efficient with robust partialcoverage data, which only requires the offline data to have good coverage ofthe distributions induced by the optimal robust policy and the perturbed modelsaround the nominal model. By tailoring specific model estimation subroutines for concrete examples ofRMDPs, including tabular RMDPs, factored RMDPs, kernel and neural RMDPs, weprove that $P^2MPO$ enjoys a $\tilde{\mathcal{O}}(n^{-1/2})$ convergence rate,where $n$ is the dataset size. We highlight that all these examples, excepttabular RMDPs, are first identified and proven tractable by this work.Furthermore, we continue our study of robust offline RL in the robust Markovgames (RMGs). By extending the double pessimism principle identified forsingle-agent RMDPs, we propose another algorithm framework that can efficientlyfind the robust Nash equilibria among players using only robust unilateral(partial) coverage data. To our best knowledge, this work proposes the firstgeneral learning principle -- double pessimism -- for robust offline RL andshows that it is provably efficient with general function approximation.</description><author>Jose Blanchet, Miao Lu, Tong Zhang, Han Zhong</author><pubDate>Tue, 22 Aug 2023 15:23:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09659v2</guid></item><item><title>Active Exploration for Inverse Reinforcement Learning</title><link>http://arxiv.org/abs/2207.08645v4</link><description>Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring areward function from expert demonstrations. Many IRL algorithms require a knowntransition model and sometimes even a known expert policy, or they at leastrequire access to a generative model. However, these assumptions are too strongfor many real-world applications, where the environment can be accessed onlythrough sequential interaction. We propose a novel IRL algorithm: Activeexploration for Inverse Reinforcement Learning (AceIRL), which activelyexplores an unknown environment and expert policy to quickly learn the expert'sreward function and identify a good policy. AceIRL uses previous observationsto construct confidence intervals that capture plausible reward functions andfind exploration policies that focus on the most informative regions of theenvironment. AceIRL is the first approach to active IRL with sample-complexitybounds that does not require a generative model of the environment. AceIRLmatches the sample complexity of active IRL with a generative model in theworst case. Additionally, we establish a problem-dependent bound that relatesthe sample complexity of AceIRL to the suboptimality gap of a given IRLproblem. We empirically evaluate AceIRL in simulations and find that itsignificantly outperforms more naive exploration strategies.</description><author>David Lindner, Andreas Krause, Giorgia Ramponi</author><pubDate>Tue, 22 Aug 2023 15:11:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.08645v4</guid></item><item><title>Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs</title><link>http://arxiv.org/abs/2307.07312v2</link><description>In any system that uses structured knowledge graph (KG) data as itsunderlying knowledge representation, KG-to-text generation is a useful tool forturning parts of the graph data into text that can be understood by humans.Recent work has shown that models that make use of pretraining on large amountsof text data can perform well on the KG-to-text task even with relatively smallsets of training data on the specific graph-to-text task. In this paper, webuild on this concept by using large language models to perform zero-shotgeneration based on nothing but the model's understanding of the triplestructure from what it can read. We show that ChatGPT achieves nearstate-of-the-art performance on some measures of the WebNLG 2020 challenge, butfalls behind on others. Additionally, we compare factual, counter-factual andfictional statements, and show that there is a significant connection betweenwhat the LLM already knows about the data it is parsing and the quality of theoutput text.</description><author>Agnes Axelsson, Gabriel Skantze</author><pubDate>Tue, 22 Aug 2023 15:10:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07312v2</guid></item><item><title>What Can Simple Arithmetic Operations Do for Temporal Modeling?</title><link>http://arxiv.org/abs/2307.08908v2</link><description>Temporal modeling plays a crucial role in understanding video content. Totackle this problem, previous studies built complicated temporal relationsthrough time sequence thanks to the development of computationally powerfuldevices. In this work, we explore the potential of four simple arithmeticoperations for temporal modeling. Specifically, we first capture auxiliarytemporal cues by computing addition, subtraction, multiplication, and divisionbetween pairs of extracted frame features. Then, we extract correspondingfeatures from these cues to benefit the original temporal-irrespective domain.We term such a simple pipeline as an Arithmetic Temporal Module (ATM), whichoperates on the stem of a visual backbone with a plug-and-play style. Weconduct comprehensive ablation studies on the instantiation of ATMs anddemonstrate that this module provides powerful temporal modeling capability ata low computational cost. Moreover, the ATM is compatible with both CNNs- andViTs-based architectures. Our results show that ATM achieves superiorperformance over several popular video benchmarks. Specifically, onSomething-Something V1, V2 and Kinetics-400, we reach top-1 accuracy of 65.6%,74.6%, and 89.4% respectively. The code is available athttps://github.com/whwu95/ATM.</description><author>Wenhao Wu, Yuxin Song, Zhun Sun, Jingdong Wang, Chang Xu, Wanli Ouyang</author><pubDate>Tue, 22 Aug 2023 15:10:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08908v2</guid></item><item><title>On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code</title><link>http://arxiv.org/abs/2305.04106v2</link><description>Pre-trained language models (PLMs) have become a prevalent technique in deeplearning for code, utilizing a two-stage pre-training and fine-tuning procedureto acquire general knowledge about code and specialize in a variety ofdownstream tasks. However, the dynamic nature of software codebases poses achallenge to the effectiveness and robustness of PLMs. In particular,world-realistic scenarios potentially lead to significant differences betweenthe distribution of the pre-training and test data, i.e., distribution shift,resulting in a degradation of the PLM's performance on downstream tasks. Inthis paper, we stress the need for adapting PLMs of code to software data whosedistribution changes over time, a crucial problem that has been overlooked inprevious works. The motivation of this work is to consider the PLM in anon-stationary environment, where fine-tuning data evolves over time accordingto a software evolution scenario. Specifically, we design a scenario where themodel needs to learn from a stream of programs containing new, unseen APIs overtime. We study two widely used PLM architectures, i.e., a GPT2 decoder and aRoBERTa encoder, on two downstream tasks, API call and API usage prediction. Wedemonstrate that the most commonly used fine-tuning technique from prior workis not robust enough to handle the dynamic nature of APIs, leading to the lossof previously acquired knowledge i.e., catastrophic forgetting. To addressthese issues, we implement five continual learning approaches, includingreplay-based and regularization-based methods. Our findings demonstrate thatutilizing these straightforward methods effectively mitigates catastrophicforgetting in PLMs across both downstream tasks while achieving comparable orsuperior performance.</description><author>Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, Houari Sahraoui</author><pubDate>Tue, 22 Aug 2023 15:10:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04106v2</guid></item><item><title>High-Fidelity Eye Animatable Neural Radiance Fields for Human Face</title><link>http://arxiv.org/abs/2308.00773v2</link><description>Face rendering using neural radiance fields (NeRF) is a rapidly developingresearch area in computer vision. While recent methods primarily focus oncontrolling facial attributes such as identity and expression, they oftenoverlook the crucial aspect of modeling eyeball rotation, which holdsimportance for various downstream tasks. In this paper, we aim to learn a faceNeRF model that is sensitive to eye movements from multi-view images. Weaddress two key challenges in eye-aware face NeRF learning: how to effectivelycapture eyeball rotation for training and how to construct a manifold forrepresenting eyeball rotation. To accomplish this, we first fit FLAME, awell-established parametric face model, to the multi-view images consideringmulti-view consistency. Subsequently, we introduce a new Dynamic Eye-aware NeRF(DeNeRF). DeNeRF transforms 3D points from different views into a canonicalspace to learn a unified face NeRF model. We design an eye deformation fieldfor the transformation, including rigid transformation, e.g., eyeball rotation,and non-rigid transformation. Through experiments conducted on the ETH-XGazedataset, we demonstrate that our model is capable of generating high-fidelityimages with accurate eyeball rotation and non-rigid periocular deformation,even under novel viewing angles. Furthermore, we show that utilizing therendered images can effectively enhance gaze estimation performance.</description><author>Hengfei Wang, Zhongqun Zhang, Yihua Cheng, Hyung Jin Chang</author><pubDate>Tue, 22 Aug 2023 15:09:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00773v2</guid></item><item><title>Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis</title><link>http://arxiv.org/abs/2308.10511v2</link><description>Understanding digital documents is like solving a puzzle, especiallyhistorical ones. Document Layout Analysis (DLA) helps with this puzzle bydividing documents into sections like paragraphs, images, and tables. This iscrucial for machines to read and understand these documents. In the DL Sprint2.0 competition, we worked on understanding Bangla documents. We used a datasetcalled BaDLAD with lots of examples. We trained a special model called MaskR-CNN to help with this understanding. We made this model better bystep-by-step hyperparameter tuning, and we achieved a good dice score of 0.889.However, not everything went perfectly. We tried using a model trained forEnglish documents, but it didn't fit well with Bangla. This showed us that eachlanguage has its own challenges. Our solution for the DL Sprint 2.0 is publiclyavailable at https://www.kaggle.com/competitions/dlsprint2/discussion/432201along with notebooks, weights, and inference notebook.</description><author>Shrestha Datta, Md Adith Mollah, Raisa Fairooz, Tariful Islam Fahim</author><pubDate>Tue, 22 Aug 2023 15:08:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10511v2</guid></item><item><title>A Survey on Self-Supervised Representation Learning</title><link>http://arxiv.org/abs/2308.11455v1</link><description>Learning meaningful representations is at the heart of many tasks in thefield of modern machine learning. Recently, a lot of methods were introducedthat allow learning of image representations without supervision. Theserepresentations can then be used in downstream tasks like classification orobject detection. The quality of these representations is close to supervisedlearning, while no labeled images are needed. This survey paper provides acomprehensive review of these methods in a unified notation, points outsimilarities and differences of these methods, and proposes a taxonomy whichsets these methods in relation to each other. Furthermore, our surveysummarizes the most-recent experimental results reported in the literature inform of a meta-study. Our survey is intended as a starting point forresearchers and practitioners who want to dive into the field of representationlearning.</description><author>Tobias Uelwer, Jan Robine, Stefan Sylvius Wagner, Marc Höftmann, Eric Upschulte, Sebastian Konietzny, Maike Behrendt, Stefan Harmeling</author><pubDate>Tue, 22 Aug 2023 15:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11455v1</guid></item><item><title>Label-free timing analysis of SiPM-based modularized detectors with physics-constrained deep learning</title><link>http://arxiv.org/abs/2304.11930v3</link><description>Pulse timing is an important topic in nuclear instrumentation, withfar-reaching applications from high energy physics to radiation imaging. Whilehigh-speed analog-to-digital converters become more and more developed andaccessible, their potential uses and merits in nuclear detector signalprocessing are still uncertain, partially due to associated timing algorithmswhich are not fully understood and utilized. In this paper, we propose a novelmethod based on deep learning for timing analysis of modularized detectorswithout explicit needs of labelling event data. By taking advantage of theintrinsic time correlations, a label-free loss function with a speciallydesigned regularizer is formed to supervise the training of neural networkstowards a meaningful and accurate mapping function. We mathematicallydemonstrate the existence of the optimal function desired by the method, andgive a systematic algorithm for training and calibration of the model. Theproposed method is validated on two experimental datasets based on siliconphotomultipliers (SiPM) as main transducers. In the toy experiment, the neuralnetwork model achieves the single-channel time resolution of 8.8 ps andexhibits robustness against concept drift in the dataset. In theelectromagnetic calorimeter experiment, several neural network models (FC, CNNand LSTM) are tested to show their conformance to the underlying physicalconstraint and to judge their performance against traditional methods. Intotal, the proposed method works well in either ideal or noisy experimentalcondition and recovers the time information from waveform samples successfullyand precisely.</description><author>Pengcheng Ai, Le Xiao, Zhi Deng, Yi Wang, Xiangming Sun, Guangming Huang, Dong Wang, Yulei Li, Xinchi Ran</author><pubDate>Tue, 22 Aug 2023 15:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11930v3</guid></item><item><title>Food Image Classification and Segmentation with Attention-based Multiple Instance Learning</title><link>http://arxiv.org/abs/2308.11452v1</link><description>The demand for accurate food quantification has increased in the recentyears, driven by the needs of applications in dietary monitoring. At the sametime, computer vision approaches have exhibited great potential in automatingtasks within the food domain. Traditionally, the development of machinelearning models for these problems relies on training data sets withpixel-level class annotations. However, this approach introduces challengesarising from data collection and ground truth generation that quickly becomecostly and error-prone since they must be performed in multiple settings andfor thousands of classes. To overcome these challenges, the paper presents aweakly supervised methodology for training food image classification andsemantic segmentation models without relying on pixel-level annotations. Theproposed methodology is based on a multiple instance learning approach incombination with an attention-based mechanism. At test time, the models areused for classification and, concurrently, the attention mechanism generatessemantic heat maps which are used for food class segmentation. In the paper, weconduct experiments on two meta-classes within the FoodSeg103 data set toverify the feasibility of the proposed approach and we explore the functioningproperties of the attention mechanism.</description><author>Valasia Vlachopoulou, Ioannis Sarafis, Alexandros Papadopoulos</author><pubDate>Tue, 22 Aug 2023 14:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11452v1</guid></item><item><title>Towards Discriminative Representations with Contrastive Instances for Real-Time UAV Tracking</title><link>http://arxiv.org/abs/2308.11450v1</link><description>Maintaining high efficiency and high precision are two fundamental challengesin UAV tracking due to the constraints of computing resources, batterycapacity, and UAV maximum load. Discriminative correlation filters (DCF)-basedtrackers can yield high efficiency on a single CPU but with inferior precision.Lightweight Deep learning (DL)-based trackers can achieve a good balancebetween efficiency and precision but performance gains are limited by thecompression rate. High compression rate often leads to poor discriminativerepresentations. To this end, this paper aims to enhance the discriminativepower of feature representations from a new feature-learning perspective.Specifically, we attempt to learn more disciminative representations withcontrastive instances for UAV tracking in a simple yet effective manner, whichnot only requires no manual annotations but also allows for developing anddeploying a lightweight model. We are the first to explore contrastive learningfor UAV tracking. Extensive experiments on four UAV benchmarks, includingUAV123@10fps, DTB70, UAVDT and VisDrone2018, show that the proposed DRCItracker significantly outperforms state-of-the-art UAV tracking methods.</description><author>Dan Zeng, Mingliang Zou, Xucheng Wang, Shuiwang Li</author><pubDate>Tue, 22 Aug 2023 14:58:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11450v1</guid></item><item><title>h-analysis and data-parallel physics-informed neural networks</title><link>http://arxiv.org/abs/2302.08835v2</link><description>We explore the data-parallel acceleration of physics-informed machinelearning (PIML) schemes, with a focus on physics-informed neural networks(PINNs) for multiple graphics processing units (GPUs) architectures. In orderto develop scale-robust and high-throughput PIML models for sophisticatedapplications which may require a large number of training points (e.g.,involving complex and high-dimensional domains, non-linear operators ormulti-physics), we detail a novel protocol based on $h$-analysis anddata-parallel acceleration through the Horovod training framework. The protocolis backed by new convergence bounds for the generalization error and thetrain-test gap. We show that the acceleration is straightforward to implement,does not compromise training, and proves to be highly efficient andcontrollable, paving the way towards generic scale-robust PIML. Extensivenumerical experiments with increasing complexity illustrate its robustness andconsistency, offering a wide range of possibilities for real-world simulations.</description><author>Paul Escapil-Inchauspé, Gonzalo A. Ruz</author><pubDate>Tue, 22 Aug 2023 14:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08835v2</guid></item><item><title>Convergence guarantee for consistency models</title><link>http://arxiv.org/abs/2308.11449v1</link><description>We provide the first convergence guarantees for the Consistency Models (CMs),a newly emerging type of one-step generative models that can generatecomparable samples to those generated by Diffusion Models. Our main result isthat, under the basic assumptions on score-matching errors, consistency errorsand smoothness of the data distribution, CMs can efficiently sample from anyrealistic data distribution in one step with small $W_2$ error. Our results (1)hold for $L^2$-accurate score and consistency assumption (rather than$L^\infty$-accurate); (2) do note require strong assumptions on the datadistribution such as log-Sobelev inequality; (3) scale polynomially in allparameters; and (4) match the state-of-the-art convergence guarantee forscore-based generative models (SGMs). We also provide the result that theMultistep Consistency Sampling procedure can further reduce the error comparingto one step sampling, which support the original statement of "ConsistencyModels, Yang Song 2023". Our result further imply a TV error guarantee whentake some Langevin-based modifications to the output distributions.</description><author>Junlong Lyu, Zhitang Chen, Shoubo Feng</author><pubDate>Tue, 22 Aug 2023 14:57:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11449v1</guid></item><item><title>Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding</title><link>http://arxiv.org/abs/2308.11448v1</link><description>Self-supervised pretraining (SSP) has emerged as a popular technique inmachine learning, enabling the extraction of meaningful feature representationswithout labelled data. In the realm of computer vision, pretrained visiontransformers (ViTs) have played a pivotal role in advancing transfer learning.Nonetheless, the escalating cost of finetuning these large models has posed achallenge due to the explosion of model size. This study endeavours to evaluatethe effectiveness of pure self-supervised learning (SSL) techniques in computervision tasks, obviating the need for finetuning, with the intention ofemulating human-like capabilities in generalisation and recognition of unseenobjects. To this end, we propose an evaluation protocol for zero-shotsegmentation based on a prompting patch. Given a point on the target object asa prompt, the algorithm calculates the similarity map between the selectedpatch and other patches, upon that, a simple thresholding is applied to segmentthe target. Another evaluation is intra-object and inter-object similarity togauge discriminatory ability of SSP ViTs. Insights from zero-shot segmentationfrom prompting and discriminatory abilities of SSP led to the design of asimple SSP approach, termed MMC. This approaches combines Masked imagemodelling for encouraging similarity of local features, Momentum basedself-distillation for transferring semantics from global to local features, andglobal Contrast for promoting semantics of global features, to enhancediscriminative representations of SSP ViTs. Consequently, our proposed methodsignificantly reduces the overlap of intra-object and inter-objectsimilarities, thereby facilitating effective object segmentation within animage. Our experiments reveal that MMC delivers top-tier results in zero-shotsemantic segmentation across various datasets.</description><author>Jiantao Wu, Shentong Mo, Muhammad Awais, Sara Atito, Zhenhua Feng, Josef Kittler</author><pubDate>Tue, 22 Aug 2023 14:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11448v1</guid></item><item><title>Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification</title><link>http://arxiv.org/abs/2308.11447v1</link><description>Aspect-based sentiment classification is a crucial problem in fine-grainedsentiment analysis, which aims to predict the sentiment polarity of the givenaspect according to its context. Previous works have made remarkable progressin leveraging attention mechanism to extract opinion words for differentaspects. However, a persistent challenge is the effective management ofsemantic mismatches, which stem from attention mechanisms that fall short inadequately aligning opinions words with their corresponding aspect inmulti-aspect sentences. To address this issue, we propose a novelAspect-oriented Opinion Alignment Network (AOAN) to capture the contextualassociation between opinion words and the corresponding aspect. Specifically,we first introduce a neighboring span enhanced module which highlights variouscompositions of neighboring words and given aspects. In addition, we design amulti-perspective attention mechanism that align relevant opinion informationwith respect to the given aspect. Extensive experiments on three benchmarkdatasets demonstrate that our model achieves state-of-the-art results. Thesource code is available at https://github.com/AONE-NLP/ABSA-AOAN.</description><author>Xueyi Liu, Rui Hou, Yanglei Gan, Da Luo, Changlin Li, Xiaojun Shi, Qiao Liu</author><pubDate>Tue, 22 Aug 2023 14:55:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11447v1</guid></item><item><title>Exploration of Rashomon Set Assists Explanations for Medical Data</title><link>http://arxiv.org/abs/2308.11446v1</link><description>The machine learning modeling process conventionally culminates in selectinga single model that maximizes a selected performance metric. However, thisapproach leads to abandoning a more profound analysis of slightly inferiormodels. Particularly in medical and healthcare studies, where the objectiveextends beyond predictions to valuable insight generation, relying solely onperformance metrics can result in misleading or incomplete conclusions. Thisproblem is particularly pertinent when dealing with a set of models withperformance close to maximum one, known as $\textit{Rashomon set}$. Such a setcan be numerous and may contain models describing the data in a different way,which calls for comprehensive analysis. This paper introduces a novel processto explore Rashomon set models, extending the conventional modeling approach.The cornerstone is the identification of the most different models within theRashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$algorithm. This algorithm compares profiles illustrating predictiondependencies on variable values generated by eXplainable ArtificialIntelligence (XAI) techniques. To quantify differences in variable effectsamong models, we introduce the Profile Disparity Index (PDI) based on measuresfrom functional data analysis. To illustrate the effectiveness of our approach,we showcase its application in predicting survival among hemophagocyticlymphohistiocytosis (HLH) patients - a foundational case study. Additionally,we benchmark our approach on other medical data sets, demonstrating itsversatility and utility in various contexts.</description><author>Katarzyna Kobylińska, Mateusz Krzyziński, Rafał Machowicz, Mariusz Adamek, Przemysław Biecek</author><pubDate>Tue, 22 Aug 2023 14:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11446v1</guid></item><item><title>Revisiting and Exploring Efficient Fast Adversarial Training via LAW: Lipschitz Regularization and Auto Weight Averaging</title><link>http://arxiv.org/abs/2308.11443v1</link><description>Fast Adversarial Training (FAT) not only improves the model robustness butalso reduces the training cost of standard adversarial training. However, fastadversarial training often suffers from Catastrophic Overfitting (CO), whichresults in poor robustness performance. Catastrophic Overfitting describes thephenomenon of a sudden and significant decrease in robust accuracy during thetraining of fast adversarial training. Many effective techniques have beendeveloped to prevent Catastrophic Overfitting and improve the model robustnessfrom different perspectives. However, these techniques adopt inconsistenttraining settings and require different training costs, i.e, training time andmemory costs, leading to unfair comparisons. In this paper, we conduct acomprehensive study of over 10 fast adversarial training methods in terms ofadversarial robustness and training costs. We revisit the effectiveness andefficiency of fast adversarial training techniques in preventing CatastrophicOverfitting from the perspective of model local nonlinearity and propose aneffective Lipschitz regularization method for fast adversarial training.Furthermore, we explore the effect of data augmentation and weight averaging infast adversarial training and propose a simple yet effective auto weightaveraging method to improve robustness further. By assembling these techniques,we propose a FGSM-based fast adversarial training method equipped withLipschitz regularization and Auto Weight averaging, abbreviated as FGSM-LAW.Experimental evaluations on four benchmark databases demonstrate thesuperiority of the proposed method over state-of-the-art fast adversarialtraining methods and the advanced standard adversarial training methods.</description><author>Xiaojun Jia, Yuefeng Chen, Xiaofeng Mao, Ranjie Duan, Jindong Gu, Rong Zhang, Hui Xue, Xiaochun Cao</author><pubDate>Tue, 22 Aug 2023 14:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11443v1</guid></item><item><title>SDeMorph: Towards Better Facial De-morphing from Single Morph</title><link>http://arxiv.org/abs/2308.11442v1</link><description>Face Recognition Systems (FRS) are vulnerable to morph attacks. A face morphis created by combining multiple identities with the intention to fool FRS andmaking it match the morph with multiple identities. Current Morph AttackDetection (MAD) can detect the morph but are unable to recover the identitiesused to create the morph with satisfactory outcomes. Existing work inde-morphing is mostly reference-based, i.e. they require the availability ofone identity to recover the other. Sudipta et al. \cite{ref9} proposed areference-free de-morphing technique but the visual realism of outputs producedwere feeble. In this work, we propose SDeMorph (Stably Diffused De-morpher), anovel de-morphing method that is reference-free and recovers the identities ofbona fides. Our method produces feature-rich outputs that are of significantlyhigh quality in terms of definition and facial fidelity. Our method utilizesDenoising Diffusion Probabilistic Models (DDPM) by destroying the input morphedsignal and then reconstructing it back using a branched-UNet. Experiments onASML, FRLL-FaceMorph, FRLL-MorDIFF, and SMDD datasets support the effectivenessof the proposed method.</description><author>Nitish Shukla</author><pubDate>Tue, 22 Aug 2023 14:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11442v1</guid></item><item><title>Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection</title><link>http://arxiv.org/abs/2308.11441v1</link><description>Latest methods represent shapes with open surfaces using unsigned distancefunctions (UDFs). They train neural networks to learn UDFs and reconstructsurfaces with the gradients around the zero level set of the UDF. However, thedifferential networks struggle from learning the zero level set where the UDFis not differentiable, which leads to large errors on unsigned distances andgradients around the zero level set, resulting in highly fragmented anddiscontinuous surfaces. To resolve this problem, we propose to learn a morecontinuous zero level set in UDFs with level set projections. Our insight is toguide the learning of zero level set using the rest non-zero level sets via aprojection procedure. Our idea is inspired from the observations that thenon-zero level sets are much smoother and more continuous than the zero levelset. We pull the non-zero level sets onto the zero level set with gradientconstraints which align gradients over different level sets and correctunsigned distance errors on the zero level set, leading to a smoother and morecontinuous unsigned distance field. We conduct comprehensive experiments insurface reconstruction for point clouds, real scans or depth maps, and furtherexplore the performance in unsupervised point cloud upsampling and unsupervisedpoint normal estimation with the learned UDF, which demonstrate our non-trivialimprovements over the state-of-the-art methods. Code is available athttps://github.com/junshengzhou/LevelSetUDF .</description><author>Junsheng Zhou, Baorui Ma, Shujuan Li, Yu-Shen Liu, Zhizhong Han</author><pubDate>Tue, 22 Aug 2023 14:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11441v1</guid></item><item><title>PoseGraphNet++: Enriching 3D Human Pose with Orientation Estimation</title><link>http://arxiv.org/abs/2308.11440v1</link><description>Existing kinematic skeleton-based 3D human pose estimation methods onlypredict joint positions. Although this is sufficient to compute the yaw andpitch of the bone rotations, the roll around the axis of the bones remainsunresolved by these methods. In this paper, we propose a novel 2D-to-3D liftingGraph Convolution Network named PoseGraphNet++ to predict the complete humanpose including the joint positions and the bone orientations. We employ nodeand edge convolutions to utilize the joint and bone features. Our model isevaluated on multiple benchmark datasets, and its performance is either on parwith or better than the state-of-the-art in terms of both position and rotationmetrics. Through extensive ablation studies, we show that PoseGraphNet++benefits from exploiting the mutual relationship between the joints and thebones.</description><author>Soubarna Banik, Edvard Avagyan, Alejandro Mendoza Gracia, Alois Knoll</author><pubDate>Tue, 22 Aug 2023 14:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11440v1</guid></item><item><title>SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes</title><link>http://arxiv.org/abs/2305.13998v2</link><description>The Surrogate Modeling Toolbox (SMT) is an open-source Python package thatoffers a collection of surrogate modeling methods, sampling techniques, and aset of sample problems. This paper presents SMT 2.0, a major new release of SMTthat introduces significant upgrades and new features to the toolbox. Thisrelease adds the capability to handle mixed-variable surrogate models andhierarchical variables. These types of variables are becoming increasinglyimportant in several surrogate modeling applications. SMT 2.0 also improves SMTby extending sampling methods, adding new surrogate models, and computingvariance and kernel derivatives for Kriging. This release also includes newfunctions to handle noisy and use multifidelity data. To the best of ourknowledge, SMT 2.0 is the first open-source surrogate library to proposesurrogate models for hierarchical and mixed inputs. This open-source softwareis distributed under the New BSD license.</description><author>Paul Saves, Remi Lafage, Nathalie Bartoli, Youssef Diouane, Jasper Bussemaker, Thierry Lefebvre, John T. Hwang, Joseph Morlier, Joaquim R. R. A. Martins</author><pubDate>Tue, 22 Aug 2023 14:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13998v2</guid></item><item><title>A Survey on Large Language Model based Autonomous Agents</title><link>http://arxiv.org/abs/2308.11432v1</link><description>Autonomous agents have long been a prominent research topic in the academiccommunity. Previous research in this field often focuses on training agentswith limited knowledge within isolated environments, which divergessignificantly from the human learning processes, and thus makes the agents hardto achieve human-like decisions. Recently, through the acquisition of vastamounts of web knowledge, large language models (LLMs) have demonstratedremarkable potential in achieving human-level intelligence. This has sparked anupsurge in studies investigating autonomous agents based on LLMs. To harnessthe full potential of LLMs, researchers have devised diverse agentarchitectures tailored to different applications. In this paper, we present acomprehensive survey of these studies, delivering a systematic review of thefield of autonomous agents from a holistic perspective. More specifically, ourfocus lies in the construction of LLM-based agents, for which we propose aunified framework that encompasses a majority of the previous work.Additionally, we provide a summary of the various applications of LLM-based AIagents in the domains of social science, natural science, and engineering.Lastly, we discuss the commonly employed evaluation strategies for LLM-based AIagents. Based on the previous studies, we also present several challenges andfuture directions in this field. To keep track of this field and continuouslyupdate our survey, we maintain a repository for the related references athttps://github.com/Paitesanshi/LLM-Agent-Survey.</description><author>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen</author><pubDate>Tue, 22 Aug 2023 14:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11432v1</guid></item><item><title>AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block</title><link>http://arxiv.org/abs/2308.11424v1</link><description>The future of the arts and artificial intelligence (AI) is promising astechnology advances. As the use of AI in design becomes more widespread, artpractice may not be a human-only art form and could instead become a digitallyintegrated experience. With enhanced creativity and collaboration, arts and AIcould work together towards creating artistic outputs that are visuallyappealing and meet the needs of the artist and viewer. While it is uncertainhow far the integration will go, arts and AI will likely influence one another.This workshop pictorial puts forward first-person research that sharesinteractions between an HCI researcher and AI as they try to escape thecreative block. The pictorial paper explores two questions: How can AI supportartists' creativity, and what does it mean to be explainable in this context?HIs, ChatGPT and Midjourney were engaged; the result was a series ofreflections that require further discussion and explorations in the XAIxArtscommunity: Transparency of attribution, the creation process, ethics of asking,and inspiration vs copying.</description><author>Makayla Lewis</author><pubDate>Tue, 22 Aug 2023 14:15:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11424v1</guid></item><item><title>TurboViT: Generating Fast Vision Transformers via Generative Architecture Search</title><link>http://arxiv.org/abs/2308.11421v1</link><description>Vision transformers have shown unprecedented levels of performance intackling various visual perception tasks in recent years. However, thearchitectural and computational complexity of such network architectures havemade them challenging to deploy in real-world applications withhigh-throughput, low-memory requirements. As such, there has been significantresearch recently on the design of efficient vision transformer architectures.In this study, we explore the generation of fast vision transformerarchitecture designs via generative architecture search (GAS) to achieve astrong balance between accuracy and architectural and computational efficiency.Through this generative architecture search process, we create TurboViT, ahighly efficient hierarchical vision transformer architecture design that isgenerated around mask unit attention and Q-pooling design patterns. Theresulting TurboViT architecture design achieves significantly lowerarchitectural computational complexity (&gt;2.47$\times$ smaller than FasterViT-0while achieving same accuracy) and computational complexity (&gt;3.4$\times$ fewerFLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 otherstate-of-the-art efficient vision transformer network architecture designswithin a similar range of accuracy on the ImageNet-1K dataset. Furthermore,TurboViT demonstrated strong inference latency and throughput in bothlow-latency and batch processing scenarios (&gt;3.21$\times$ lower latency and&gt;3.18$\times$ higher throughput compared to FasterViT-0 for low-latencyscenario). These promising results demonstrate the efficacy of leveraginggenerative architecture search for generating efficient transformerarchitecture designs for high-throughput scenarios.</description><author>Alexander Wong, Saad Abbasi, Saeejith Nair</author><pubDate>Tue, 22 Aug 2023 14:08:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11421v1</guid></item><item><title>Tensor Regression</title><link>http://arxiv.org/abs/2308.11419v1</link><description>Regression analysis is a key area of interest in the field of data analysisand machine learning which is devoted to exploring the dependencies betweenvariables, often using vectors. The emergence of high dimensional data intechnologies such as neuroimaging, computer vision, climatology and socialnetworks, has brought challenges to traditional data representation methods.Tensors, as high dimensional extensions of vectors, are considered as naturalrepresentations of high dimensional data. In this book, the authors provide asystematic study and analysis of tensor-based regression models and theirapplications in recent years. It groups and illustrates the existingtensor-based regression methods and covers the basics, core ideas, andtheoretical characteristics of most tensor-based regression methods. Inaddition, readers can learn how to use existing tensor-based regression methodsto solve specific regression tasks with multiway data, what datasets can beselected, and what software packages are available to start related work assoon as possible. Tensor Regression is the first thorough overview of thefundamentals, motivations, popular algorithms, strategies for efficientimplementation, related applications, available datasets, and softwareresources for tensor-based regression analysis. It is essential reading for allstudents, researchers and practitioners of working on high dimensional data.</description><author>Jiani Liu, Ce Zhu, Zhen Long, Yipeng Liu</author><pubDate>Tue, 22 Aug 2023 14:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11419v1</guid></item><item><title>ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes</title><link>http://arxiv.org/abs/2308.11417v1</link><description>We present ScanNet++, a large-scale dataset that couples together capture ofhigh-quality and commodity-level geometry and color of indoor scenes. Eachscene is captured with a high-end laser scanner at sub-millimeter resolution,along with registered 33-megapixel images from a DSLR camera, and RGB-D streamsfrom an iPhone. Scene reconstructions are further annotated with an openvocabulary of semantics, with label-ambiguous scenarios explicitly annotatedfor comprehensive semantic understanding. ScanNet++ enables a new real-worldbenchmark for novel view synthesis, both from high-quality RGB capture, andimportantly also from commodity-level images, in addition to a new benchmarkfor 3D semantic scene understanding that comprehensively encapsulates diverseand ambiguous semantic labeling scenarios. Currently, ScanNet++ contains 460scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.</description><author>Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, Angela Dai</author><pubDate>Tue, 22 Aug 2023 14:02:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11417v1</guid></item><item><title>Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm</title><link>http://arxiv.org/abs/2308.11411v1</link><description>Extracting relational triples (subject, predicate, object) from text enablesthe transformation of unstructured text data into structured knowledge. Thenamed entity recognition (NER) and the relation extraction (RE) are twofoundational subtasks in this knowledge generation pipeline. The integration ofsubtasks poses a considerable challenge due to their disparate nature. Thispaper presents a novel approach that converts the triple extraction task into agraph labeling problem, capitalizing on the structural information ofdependency parsing and graph recursive neural networks (GRNNs). To integratesubtasks, this paper proposes a dynamic feedback forest algorithm that connectsthe representations of subtasks by inference operations during model training.Experimental results demonstrate the effectiveness of the proposed method.</description><author>Hongyin Zhu</author><pubDate>Tue, 22 Aug 2023 14:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11411v1</guid></item><item><title>MatFuse: Controllable Material Generation with Diffusion Models</title><link>http://arxiv.org/abs/2308.11408v1</link><description>Creating high quality and realistic materials in computer graphics is achallenging and time-consuming task, which requires great expertise. In thispaper, we present MatFuse, a novel unified approach that harnesses thegenerative power of diffusion models (DM) to simplify the creation of SVBRDFmaps. Our DM-based pipeline integrates multiple sources of conditioning, suchas color palettes, sketches, and pictures, enabling fine-grained control andflexibility in material synthesis. This design allows for the combination ofdiverse information sources (e.g., sketch + image embedding), enhancingcreative possibilities in line with the principle of compositionality. Wedemonstrate the generative capabilities of the proposed method under variousconditioning settings; on the SVBRDF estimation task, we show that our methodyields performance comparable to state-of-the-art approaches, bothqualitatively and quantitatively.</description><author>Giuseppe Vecchio, Renato Sortino, Simone Palazzo, Concetto Spampinato</author><pubDate>Tue, 22 Aug 2023 13:54:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11408v1</guid></item><item><title>DynaMITe: Dynamic Query Bootstrapping for Multi-object Interactive Segmentation Transformer</title><link>http://arxiv.org/abs/2304.06668v2</link><description>Most state-of-the-art instance segmentation methods rely on large amounts ofpixel-precise ground-truth annotations for training, which are expensive tocreate. Interactive segmentation networks help generate such annotations basedon an image and the corresponding user interactions such as clicks. Existingmethods for this task can only process a single instance at a time and eachuser interaction requires a full forward pass through the entire deep network.We introduce a more efficient approach, called DynaMITe, in which we representuser interactions as spatio-temporal queries to a Transformer decoder with apotential to segment multiple object instances in a single iteration. Ourarchitecture also alleviates any need to re-compute image features duringrefinement, and requires fewer interactions for segmenting multiple instancesin a single image when compared to other methods. DynaMITe achievesstate-of-the-art results on multiple existing interactive segmentationbenchmarks, and also on the new multi-instance benchmark that we propose inthis paper.</description><author>Amit Kumar Rana, Sabarinath Mahadevan, Alexander Hermans, Bastian Leibe</author><pubDate>Tue, 22 Aug 2023 13:53:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06668v2</guid></item><item><title>Designing an attack-defense game: how to increase robustness of financial transaction models via a competition</title><link>http://arxiv.org/abs/2308.11406v1</link><description>Given the escalating risks of malicious attacks in the finance sector and theconsequential severe damage, a thorough understanding of adversarial strategiesand robust defense mechanisms for machine learning models is critical. Thethreat becomes even more severe with the increased adoption in banks moreaccurate, but potentially fragile neural networks. We aim to investigate thecurrent state and dynamics of adversarial attacks and defenses for neuralnetwork models that use sequential financial data as the input. To achieve this goal, we have designed a competition that allows realisticand detailed investigation of problems in modern financial transaction data.The participants compete directly against each other, so possible attacks anddefenses are examined in close-to-real-life conditions. Our main contributionsare the analysis of the competition dynamics that answers the questions on howimportant it is to conceal a model from malicious users, how long does it taketo break it, and what techniques one should use to make it more robust, andintroduction additional way to attack models or increase their robustness. Our analysis continues with a meta-study on the used approaches with theirpower, numerical experiments, and accompanied ablations studies. We show thatthe developed attacks and defenses outperform existing alternatives from theliterature while being practical in terms of execution, proving the validity ofthe competition as a tool for uncovering vulnerabilities of machine learningmodels and mitigating them in various domains.</description><author>Alexey Zaytsev, Alex Natekin, Evgeni Vorsin, Valerii Smirnov, Oleg Sidorshin, Alexander Senin, Alexander Dudin, Dmitry Berestnev</author><pubDate>Tue, 22 Aug 2023 13:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11406v1</guid></item></channel></rss>