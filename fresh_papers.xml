<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 04 Nov 2023 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Idempotent Generative Network</title><link>http://arxiv.org/abs/2311.01462v1</link><description>We propose a new approach for generative modeling based on training a neuralnetwork to be idempotent. An idempotent operator is one that can be appliedsequentially without changing the result beyond the initial application, namely$f(f(z))=f(z)$. The proposed model $f$ is trained to map a source distribution(e.g, Gaussian noise) to a target distribution (e.g. realistic images) usingthe following objectives: (1) Instances from the target distribution should mapto themselves, namely $f(x)=x$. We define the target manifold as the set of allinstances that $f$ maps to themselves. (2) Instances that form the sourcedistribution should map onto the defined target manifold. This is achieved byoptimizing the idempotence term, $f(f(z))=f(z)$ which encourages the range of$f(z)$ to be on the target manifold. Under ideal assumptions such a processprovably converges to the target distribution. This strategy results in a modelcapable of generating an output in one step, maintaining a consistent latentspace, while also allowing sequential applications for refinement.Additionally, we find that by processing inputs from both target and sourcedistributions, the model adeptly projects corrupted or modified data back tothe target manifold. This work is a first step towards a ``global projector''that enables projecting any input into a target data distribution.</description><author>Assaf Shocher, Amil Dravid, Yossi Gandelsman, Inbar Mosseri, Michael Rubinstein, Alexei A. Efros</author><pubDate>Thu, 02 Nov 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01462v1</guid></item><item><title>Implicit Chain of Thought Reasoning via Knowledge Distillation</title><link>http://arxiv.org/abs/2311.01460v1</link><description>To augment language models with the ability to reason, researchers usuallyprompt or finetune them to produce chain of thought reasoning steps beforeproducing the final answer. However, although people use natural language toreason effectively, it may be that LMs could reason more effectively with someintermediate computation that is not in natural language. In this work, weexplore an alternative reasoning approach: instead of explicitly producing thechain of thought reasoning steps, we use the language model's internal hiddenstates to perform implicit reasoning. The implicit reasoning steps aredistilled from a teacher model trained on explicit chain-of-thought reasoning,and instead of doing reasoning "horizontally" by producing intermediate wordsone-by-one, we distill it such that the reasoning happens "vertically" amongthe hidden states in different layers. We conduct experiments on a multi-digitmultiplication task and a grade school math problem dataset and find that thisapproach enables solving tasks previously not solvable without explicitchain-of-thought, at a speed comparable to no chain-of-thought.</description><author>Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, Stuart Shieber</author><pubDate>Thu, 02 Nov 2023 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01460v1</guid></item><item><title>Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization</title><link>http://arxiv.org/abs/2311.01459v1</link><description>The promising zero-shot generalization of vision-language models such as CLIPhas led to their adoption using prompt learning for numerous downstream tasks.Previous works have shown test-time prompt tuning using entropy minimization toadapt text prompts for unseen domains. While effective, this overlooks the keycause for performance degradation to unseen domains -- distribution shift. Inthis work, we explicitly handle this problem by aligning theout-of-distribution (OOD) test sample statistics to those of the source datausing prompt tuning. We use a single test sample to adapt multi-modal promptsat test time by minimizing the feature distribution shift to bridge the gap inthe test domain. Evaluating against the domain generalization benchmark, ourmethod improves zero-shot top- 1 accuracy beyond existing prompt-learningtechniques, with a 3.08% improvement over the baseline MaPLe. In cross-datasetgeneralization with unseen categories across 10 datasets, our method improvesconsistently across all datasets compared to the existing state-of-the-art. Oursource code and models are available athttps://jameelhassan.github.io/promptalign.</description><author>Jameel Hassan, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Shahbaz Khan, Salman Khan</author><pubDate>Thu, 02 Nov 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01459v1</guid></item><item><title>Detecting Deepfakes Without Seeing Any</title><link>http://arxiv.org/abs/2311.01458v1</link><description>Deepfake attacks, malicious manipulation of media containing people, are aserious concern for society. Conventional deepfake detection methods trainsupervised classifiers to distinguish real media from previously encountereddeepfakes. Such techniques can only detect deepfakes similar to thosepreviously seen, but not zero-day (previously unseen) attack types. As currentdeepfake generation techniques are changing at a breathtaking pace, new attacktypes are proposed frequently, making this a major issue. Our main observationsare that: i) in many effective deepfake attacks, the fake media must beaccompanied by false facts i.e. claims about the identity, speech, motion, orappearance of the person. For instance, when impersonating Obama, the attackerexplicitly or implicitly claims that the fake media show Obama; ii) currentgenerative techniques cannot perfectly synthesize the false facts claimed bythe attacker. We therefore introduce the concept of "fact checking", adaptedfrom fake news detection, for detecting zero-day deepfake attacks. Factchecking verifies that the claimed facts (e.g. identity is Obama), agree withthe observed media (e.g. is the face really Obama's?), and thus candifferentiate between real and fake media. Consequently, we introduce FACTOR, apractical recipe for deepfake fact checking and demonstrate its power incritical attack settings: face swapping and audio-visual synthesis. Although itis training-free, relies exclusively on off-the-shelf features, is very easy toimplement, and does not see any deepfakes, it achieves better thanstate-of-the-art accuracy.</description><author>Tal Reiss, Bar Cavia, Yedid Hoshen</author><pubDate>Thu, 02 Nov 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01458v1</guid></item><item><title>Conformal Policy Learning for Sensorimotor Control Under Distribution Shifts</title><link>http://arxiv.org/abs/2311.01457v1</link><description>This paper focuses on the problem of detecting and reacting to changes in thedistribution of a sensorimotor controller's observables. The key idea is thedesign of switching policies that can take conformal quantiles as input, whichwe define as conformal policy learning, that allows robots to detectdistribution shifts with formal statistical guarantees. We show how to designsuch policies by using conformal quantiles to switch between base policies withdifferent characteristics, e.g. safety or speed, or directly augmenting apolicy observation with a quantile and training it with reinforcement learning.Theoretically, we show that such policies achieve the formal convergenceguarantees in finite time. In addition, we thoroughly evaluate their advantagesand limitations on two compelling use cases: simulated autonomous driving andactive perception with a physical quadruped. Empirical results demonstrate thatour approach outperforms five baselines. It is also the simplest of thebaseline strategies besides one ablation. Being easy to use, flexible, and withformal guarantees, our work demonstrates how conformal prediction can be aneffective tool for sensorimotor learning under uncertainty.</description><author>Huang Huang, Satvik Sharma, Antonio Loquercio, Anastasios Angelopoulos, Ken Goldberg, Jitendra Malik</author><pubDate>Thu, 02 Nov 2023 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01457v1</guid></item><item><title>RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation</title><link>http://arxiv.org/abs/2311.01455v1</link><description>We present RoboGen, a generative robotic agent that automatically learnsdiverse robotic skills at scale via generative simulation. RoboGen leveragesthe latest advancements in foundation and generative models. Instead ofdirectly using or adapting these models to produce policies or low-levelactions, we advocate for a generative scheme, which uses these models toautomatically generate diversified tasks, scenes, and training supervisions,thereby scaling up robotic skill learning with minimal human supervision. Ourapproach equips a robotic agent with a self-guided propose-generate-learncycle: the agent first proposes interesting tasks and skills to develop, andthen generates corresponding simulation environments by populating pertinentobjects and assets with proper spatial configurations. Afterwards, the agentdecomposes the proposed high-level task into sub-tasks, selects the optimallearning approach (reinforcement learning, motion planning, or trajectoryoptimization), generates required training supervision, and then learnspolicies to acquire the proposed skill. Our work attempts to extract theextensive and versatile knowledge embedded in large-scale models and transferthem to the field of robotics. Our fully generative pipeline can be queriedrepeatedly, producing an endless stream of skill demonstrations associated withdiverse tasks and environments.</description><author>Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, Chuang Gan</author><pubDate>Thu, 02 Nov 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01455v1</guid></item><item><title>UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild</title><link>http://arxiv.org/abs/2305.11147v3</link><description>Achieving machine autonomy and human control often represent divergentobjectives in the design of interactive AI systems. Visual generativefoundation models such as Stable Diffusion show promise in navigating thesegoals, especially when prompted with arbitrary languages. However, they oftenfall short in generating images with spatial, structural, or geometriccontrols. The integration of such controls, which can accommodate variousvisual conditions in a single unified model, remains an unaddressed challenge.In response, we introduce UniControl, a new generative foundation model thatconsolidates a wide array of controllable condition-to-image (C2I) tasks withina singular framework, while still allowing for arbitrary language prompts.UniControl enables pixel-level-precise image generation, where visualconditions primarily influence the generated structures and language promptsguide the style and context. To equip UniControl with the capacity to handlediverse visual conditions, we augment pretrained text-to-image diffusion modelsand introduce a task-aware HyperNet to modulate the diffusion models, enablingthe adaptation to different C2I tasks simultaneously. Trained on nine uniqueC2I tasks, UniControl demonstrates impressive zero-shot generation abilitieswith unseen visual conditions. Experimental results show that UniControl oftensurpasses the performance of single-task-controlled methods of comparable modelsizes. This control versatility positions UniControl as a significantadvancement in the realm of controllable visual generation.</description><author>Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, Ran Xu</author><pubDate>Thu, 02 Nov 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11147v3</guid></item><item><title>NOIR: Neural Signal Operated Intelligent Robots for Everyday Activities</title><link>http://arxiv.org/abs/2311.01454v1</link><description>We present Neural Signal Operated Intelligent Robots (NOIR), ageneral-purpose, intelligent brain-robot interface system that enables humansto command robots to perform everyday activities through brain signals. Throughthis interface, humans communicate their intended objects of interest andactions to the robots using electroencephalography (EEG). Our novel systemdemonstrates success in an expansive array of 20 challenging, everydayhousehold activities, including cooking, cleaning, personal care, andentertainment. The effectiveness of the system is improved by its synergisticintegration of robot learning algorithms, allowing for NOIR to adapt toindividual users and predict their intentions. Our work enhances the way humansinteract with robots, replacing traditional channels of interaction withdirect, neural communication. Project website: https://noir-corl.github.io/.</description><author>Ruohan Zhang, Sharon Lee, Minjune Hwang, Ayano Hiranaka, Chen Wang, Wensi Ai, Jin Jie Ryan Tan, Shreya Gupta, Yilun Hao, Gabrael Levine, Ruohan Gao, Anthony Norcia, Li Fei-Fei, Jiajun Wu</author><pubDate>Thu, 02 Nov 2023 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01454v1</guid></item><item><title>PPI++: Efficient Prediction-Powered Inference</title><link>http://arxiv.org/abs/2311.01453v1</link><description>We present PPI++: a computationally lightweight methodology for estimationand inference based on a small labeled dataset and a typically much largerdataset of machine-learning predictions. The methods automatically adapt to thequality of available predictions, yielding easy-to-compute confidence sets --for parameters of any dimensionality -- that always improve on classicalintervals using only the labeled data. PPI++ builds on prediction-poweredinference (PPI), which targets the same problem setting, improving itscomputational and statistical efficiency. Real and synthetic experimentsdemonstrate the benefits of the proposed adaptations.</description><author>Anastasios N. Angelopoulos, John C. Duchi, Tijana Zrnic</author><pubDate>Thu, 02 Nov 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01453v1</guid></item><item><title>Time Series Anomaly Detection using Diffusion-based Models</title><link>http://arxiv.org/abs/2311.01452v1</link><description>Diffusion models have been recently used for anomaly detection (AD) inimages. In this paper we investigate whether they can also be leveraged for ADon multivariate time series (MTS). We test two diffusion-based models andcompare them to several strong neural baselines. We also extend the PA%Kprotocol, by computing a ROCK-AUC metric, which is agnostic to both thedetection threshold and the ratio K of correctly detected points. Our modelsoutperform the baselines on synthetic datasets and are competitive onreal-world datasets, illustrating the potential of diffusion-based methods forAD in multivariate time series.</description><author>Ioana Pintilie, Andrei Manolache, Florin Brad</author><pubDate>Thu, 02 Nov 2023 18:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01452v1</guid></item><item><title>Self-Supervised Image Captioning with CLIP</title><link>http://arxiv.org/abs/2306.15111v2</link><description>Image captioning, a fundamental task in vision-language understanding, seeksto generate accurate natural language descriptions for provided images. Currentimage captioning approaches heavily rely on high-quality image-caption pairs,which can be hard to obtain for many domains. To address this, we introduce aself-supervised image captioning method. After learning an initial signal froma small labeled dataset, our method transitions to self-supervised learning onunlabeled data, leveraging the auxiliary task of enhancing the CLIP relevancebetween images and generated captions. Remarkably, despite utilizing less than2% of the labeled COCO dataset, our method delivers a performance comparable tostate-of-the-art models trained on the complete dataset. Human evaluationsfurther reveal that our method produces captions with greater distinctivenessand informativeness, two attributes inherently challenging to achieve throughsupervised learning.</description><author>Chuanyang Jin</author><pubDate>Thu, 02 Nov 2023 18:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15111v2</guid></item><item><title>DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing</title><link>http://arxiv.org/abs/2311.01450v1</link><description>Model-based reinforcement learning (MBRL) has gained much attention for itsability to learn complex behaviors in a sample-efficient way: planning actionsby generating imaginary trajectories with predicted rewards. Despite itssuccess, we found that surprisingly, reward prediction is often a bottleneck ofMBRL, especially for sparse rewards that are challenging (or even ambiguous) topredict. Motivated by the intuition that humans can learn from rough rewardestimates, we propose a simple yet effective reward smoothing approach,DreamSmooth, which learns to predict a temporally-smoothed reward, instead ofthe exact reward at the given timestep. We empirically show that DreamSmoothachieves state-of-the-art performance on long-horizon sparse-reward tasks bothin sample efficiency and final performance without losing performance on commonbenchmarks, such as Deepmind Control Suite and Atari benchmarks.</description><author>Vint Lee, Pieter Abbeel, Youngwoon Lee</author><pubDate>Thu, 02 Nov 2023 18:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01450v1</guid></item><item><title>TopicGPT: A Prompt-based Topic Modeling Framework</title><link>http://arxiv.org/abs/2311.01449v1</link><description>Topic modeling is a well-established technique for exploring text corpora.Conventional topic models (e.g., LDA) represent topics as bags of words thatoften require "reading the tea leaves" to interpret; additionally, they offerusers minimal semantic control over topics. To tackle these issues, weintroduce TopicGPT, a prompt-based framework that uses large language models(LLMs) to uncover latent topics within a provided text collection. TopicGPTproduces topics that align better with human categorizations compared tocompeting methods: for example, it achieves a harmonic mean purity of 0.74against human-annotated Wikipedia topics compared to 0.64 for the strongestbaseline. Its topics are also more interpretable, dispensing with ambiguousbags of words in favor of topics with natural language labels and associatedfree-form descriptions. Moreover, the framework is highly adaptable, allowingusers to specify constraints and modify topics without the need for modelretraining. TopicGPT can be further extended to hierarchical topical modeling,enabling users to explore topics at various levels of granularity. Bystreamlining access to high-quality and interpretable topics, TopicGPTrepresents a compelling, human-centered approach to topic modeling.</description><author>Chau Minh Pham, Alexander Hoyle, Simeng Sun, Mohit Iyyer</author><pubDate>Thu, 02 Nov 2023 18:57:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01449v1</guid></item><item><title>UltraLiDAR: Learning Compact Representations for LiDAR Completion and Generation</title><link>http://arxiv.org/abs/2311.01448v1</link><description>LiDAR provides accurate geometric measurements of the 3D world.Unfortunately, dense LiDARs are very expensive and the point clouds captured bylow-beam LiDAR are often sparse. To address these issues, we presentUltraLiDAR, a data-driven framework for scene-level LiDAR completion, LiDARgeneration, and LiDAR manipulation. The crux of UltraLiDAR is a compact,discrete representation that encodes the point cloud's geometric structure, isrobust to noise, and is easy to manipulate. We show that by aligning therepresentation of a sparse point cloud to that of a dense point cloud, we candensify the sparse point clouds as if they were captured by a real high-densityLiDAR, drastically reducing the cost. Furthermore, by learning a prior over thediscrete codebook, we can generate diverse, realistic LiDAR point clouds forself-driving. We evaluate the effectiveness of UltraLiDAR on sparse-to-denseLiDAR completion and LiDAR generation. Experiments show that densifyingreal-world point clouds with our approach can significantly improve theperformance of downstream perception systems. Compared to prior art on LiDARgeneration, our approach generates much more realistic point clouds. Accordingto A/B test, over 98.5\% of the time human participants prefer our results overthose of previous methods.</description><author>Yuwen Xiong, Wei-Chiu Ma, Jingkang Wang, Raquel Urtasun</author><pubDate>Thu, 02 Nov 2023 18:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01448v1</guid></item><item><title>CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation</title><link>http://arxiv.org/abs/2311.01447v1</link><description>Realistic simulation is key to enabling safe and scalable development of %self-driving vehicles. A core component is simulating the sensors so that theentire autonomy system can be tested in simulation. Sensor simulation involvesmodeling traffic participants, such as vehicles, with high quality appearanceand articulated geometry, and rendering them in real time. The self-drivingindustry has typically employed artists to build these assets. However, this isexpensive, slow, and may not reflect reality. Instead, reconstructing assetsautomatically from sensor data collected in the wild would provide a betterpath to generating a diverse and large set with good real-world coverage.Nevertheless, current reconstruction approaches struggle on in-the-wild sensordata, due to its sparsity and noise. To tackle these issues, we present CADSim,which combines part-aware object-class priors via a small set of CAD modelswith differentiable rendering to automatically reconstruct vehicle geometry,including articulated wheels, with high-quality appearance. Our experimentsshow our method recovers more accurate shapes from sparse data compared toexisting approaches. Importantly, it also trains and renders efficiently. Wedemonstrate our reconstructed vehicles in several applications, includingaccurate testing of autonomy perception systems.</description><author>Jingkang Wang, Sivabalan Manivasagam, Yun Chen, Ze Yang, Ioan Andrei Bârsan, Anqi Joyce Yang, Wei-Chiu Ma, Raquel Urtasun</author><pubDate>Thu, 02 Nov 2023 18:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01447v1</guid></item><item><title>Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop Simulation</title><link>http://arxiv.org/abs/2311.01446v1</link><description>Self-driving vehicles (SDVs) must be rigorously tested on a wide range ofscenarios to ensure safe deployment. The industry typically relies onclosed-loop simulation to evaluate how the SDV interacts on a corpus ofsynthetic and real scenarios and verify it performs properly. However, theyprimarily only test the system's motion planning module, and only considerbehavior variations. It is key to evaluate the full autonomy system inclosed-loop, and to understand how variations in sensor data based on sceneappearance, such as the shape of actors, affect system performance. In thispaper, we propose a framework, Adv3D, that takes real world scenarios andperforms closed-loop sensor simulation to evaluate autonomy performance, andfinds vehicle shapes that make the scenario more challenging, resulting inautonomy failures and uncomfortable SDV maneuvers. Unlike prior works that addcontrived adversarial shapes to vehicle roof-tops or roadside to harmperception only, we optimize a low-dimensional shape representation to modifythe vehicle shape itself in a realistic manner to degrade autonomy performance(e.g., perception, prediction, and motion planning). Moreover, we find that theshape variations found with Adv3D optimized in closed-loop are much moreeffective than those in open-loop, demonstrating the importance of findingscene appearance variations that affect autonomy in the interactive setting.</description><author>Jay Sarva, Jingkang Wang, James Tu, Yuwen Xiong, Sivabalan Manivasagam, Raquel Urtasun</author><pubDate>Thu, 02 Nov 2023 18:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01446v1</guid></item><item><title>LabelFormer: Object Trajectory Refinement for Offboard Perception from LiDAR Point Clouds</title><link>http://arxiv.org/abs/2311.01444v1</link><description>A major bottleneck to scaling-up training of self-driving perception systemsare the human annotations required for supervision. A promising alternative isto leverage "auto-labelling" offboard perception models that are trained toautomatically generate annotations from raw LiDAR point clouds at a fraction ofthe cost. Auto-labels are most commonly generated via a two-stage approach --first objects are detected and tracked over time, and then each objecttrajectory is passed to a learned refinement model to improve accuracy. Sinceexisting refinement models are overly complex and lack advanced temporalreasoning capabilities, in this work we propose LabelFormer, a simple,efficient, and effective trajectory-level refinement approach. Our approachfirst encodes each frame's observations separately, then exploitsself-attention to reason about the trajectory with full temporal context, andfinally decodes the refined object size and per-frame poses. Evaluation on bothurban and highway datasets demonstrates that LabelFormer outperforms existingworks by a large margin. Finally, we show that training on a dataset augmentedwith auto-labels generated by our method leads to improved downstream detectionperformance compared to existing methods. Please visit the project website fordetails https://waabi.ai/labelformer</description><author>Anqi Joyce Yang, Sergio Casas, Nikita Dvornik, Sean Segal, Yuwen Xiong, Jordan Sir Kwang Hu, Carter Fang, Raquel Urtasun</author><pubDate>Thu, 02 Nov 2023 18:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01444v1</guid></item><item><title>Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models</title><link>http://arxiv.org/abs/2311.01442v1</link><description>Deep learning models, particularly Transformers, have achieved impressiveresults in various domains, including time series forecasting. While existingtime series literature primarily focuses on model architecture modificationsand data augmentation techniques, this paper explores the training schema ofdeep learning models for time series; how models are trained regardless oftheir architecture. We perform extensive experiments to investigate theoccurrence of deep double descent in several Transformer models trained onpublic time series data sets. We demonstrate epoch-wise deep double descent andthat overfitting can be reverted using more epochs. Leveraging these findings,we achieve state-of-the-art results for long sequence time series forecastingin nearly 70% of the 72 benchmarks tested. This suggests that many models inthe literature may possess untapped potential. Additionally, we introduce ataxonomy for classifying training schema modifications, covering dataaugmentation, model inputs, model targets, time series per model, andcomputational budget.</description><author>Valentino Assandri, Sam Heshmati, Burhaneddin Yaman, Anton Iakovlev, Ariel Emiliano Repetur</author><pubDate>Thu, 02 Nov 2023 18:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01442v1</guid></item><item><title>Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models</title><link>http://arxiv.org/abs/2311.01441v1</link><description>We propose a conceptually simple and lightweight framework for improving therobustness of vision models through the combination of knowledge distillationand data augmentation. We address the conjecture that larger models do not makefor better teachers by showing strong gains in out-of-distribution robustnesswhen distilling from pretrained foundation models. Following this finding, wepropose Discrete Adversarial Distillation (DAD), which leverages a robustteacher to generate adversarial examples and a VQGAN to discretize them,creating more informative samples than standard data augmentation techniques.We provide a theoretical framework for the use of a robust teacher in theknowledge distillation with data augmentation setting and demonstrate stronggains in out-of-distribution robustness and clean accuracy across differentstudent architectures. Notably, our method adds minor computational overheadcompared to similar techniques and can be easily combined with other dataaugmentations for further improvements.</description><author>Andy Zhou, Jindong Wang, Yu-Xiong Wang, Haohan Wang</author><pubDate>Thu, 02 Nov 2023 18:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01441v1</guid></item><item><title>Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time</title><link>http://arxiv.org/abs/2311.01435v1</link><description>We give a polynomial-time algorithm for learning high-dimensional halfspaceswith margins in $d$-dimensional space to within desired TV distance when theambient distribution is an unknown affine transformation of the $d$-foldproduct of an (unknown) symmetric one-dimensional logconcave distribution, andthe halfspace is introduced by deleting at least an $\epsilon$ fraction of thedata in one of the component distributions. Notably, our algorithm does notneed labels and establishes the unique (and efficient) identifiability of thehidden halfspace under this distributional assumption. The sample and timecomplexity of the algorithm are polynomial in the dimension and $1/\epsilon$.The algorithm uses only the first two moments of suitable re-weightings of theempirical distribution, which we call contrastive moments; its analysis usesclassical facts about generalized Dirichlet polynomials and relies crucially ona new monotonicity property of the moment ratio of truncations of logconcavedistributions. Such algorithms, based only on first and second moments weresuggested in earlier work, but hitherto eluded rigorous guarantees. Prior work addressed the special case when the underlying distribution isGaussian via Non-Gaussian Component Analysis. We improve on this by providingpolytime guarantees based on Total Variation (TV) distance, in place ofexisting moment-bound guarantees that can be super-polynomial. Our work is alsothe first to go beyond Gaussians in this setting.</description><author>Xinyuan Cao, Santosh S. Vempala</author><pubDate>Thu, 02 Nov 2023 18:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01435v1</guid></item><item><title>Getting aligned on representational alignment</title><link>http://arxiv.org/abs/2310.13018v2</link><description>Biological and artificial information processing systems form representationsthat they can use to categorize, reason, plan, navigate, and make decisions.How can we measure the extent to which the representations formed by thesediverse systems agree? Do similarities in representations then translate intosimilar behavior? How can a system's representations be modified to bettermatch those of another system? These questions pertaining to the study ofrepresentational alignment are at the heart of some of the most active researchareas in cognitive science, neuroscience, and machine learning. For example,cognitive scientists measure the representational alignment of multipleindividuals to identify shared cognitive priors, neuroscientists align fMRIresponses from multiple individuals into a shared representational space forgroup-level analyses, and ML researchers distill knowledge from teacher modelsinto student models by increasing their alignment. Unfortunately, there islimited knowledge transfer between research communities interested inrepresentational alignment, so progress in one field often ends up beingrediscovered independently in another. Thus, greater cross-field communicationwould be advantageous. To improve communication between these fields, wepropose a unifying framework that can serve as a common language betweenresearchers studying representational alignment. We survey the literature fromall three fields and demonstrate how prior work fits into this framework.Finally, we lay out open problems in representational alignment where progresscan benefit all three of these fields. We hope that our work can catalyzecross-disciplinary collaboration and accelerate progress for all communitiesstudying and developing information processing systems. We note that this is aworking paper and encourage readers to reach out with their suggestions forfuture revisions.</description><author>Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love, Erin Grant, Iris Groen, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nori Jacoby, Qiuyi Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. O'Connell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert Müller, Mariya Toneva, Thomas L. Griffiths</author><pubDate>Thu, 02 Nov 2023 18:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13018v2</guid></item><item><title>Tailoring Mixup to Data using Kernel Warping functions</title><link>http://arxiv.org/abs/2311.01434v1</link><description>Data augmentation is an essential building block for learning efficient deeplearning models. Among all augmentation techniques proposed so far, linearinterpolation of training data points, also called mixup, has found to beeffective for a large panel of applications. While the majority of works havefocused on selecting the right points to mix, or applying complex non-linearinterpolation, we are interested in mixing similar points more frequently andstrongly than less similar ones. To this end, we propose to dynamically changethe underlying distribution of interpolation coefficients through warpingfunctions, depending on the similarity between data points to combine. Wedefine an efficient and flexible framework to do so without losing indiversity. We provide extensive experiments for classification and regressiontasks, showing that our proposed method improves both performance andcalibration of models. Code available inhttps://github.com/ENSTA-U2IS/torch-uncertainty</description><author>Quentin Bouniot, Pavlo Mozharovskyi, Florence d'Alché-Buc</author><pubDate>Thu, 02 Nov 2023 18:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01434v1</guid></item><item><title>Transformation Decoupling Strategy based on Screw Theory for Deterministic Point Cloud Registration with Gravity Prior</title><link>http://arxiv.org/abs/2311.01432v1</link><description>Point cloud registration is challenging in the presence of heavy outliercorrespondences. This paper focuses on addressing the robustcorrespondence-based registration problem with gravity prior that often arisesin practice. The gravity directions are typically obtained by inertialmeasurement units (IMUs) and can reduce the degree of freedom (DOF) of rotationfrom 3 to 1. We propose a novel transformation decoupling strategy byleveraging screw theory. This strategy decomposes the original 4-DOF probleminto three sub-problems with 1-DOF, 2-DOF, and 1-DOF, respectively, therebyenhancing the computation efficiency. Specifically, the first 1-DOF representsthe translation along the rotation axis and we propose an intervalstabbing-based method to solve it. The second 2-DOF represents the pole whichis an auxiliary variable in screw theory and we utilize a branch-and-boundmethod to solve it. The last 1-DOF represents the rotation angle and we proposea global voting method for its estimation. The proposed method sequentiallysolves three consensus maximization sub-problems, leading to efficient anddeterministic registration. In particular, it can even handle thecorrespondence-free registration problem due to its significant robustness.Extensive experiments on both synthetic and real-world datasets demonstratethat our method is more efficient and robust than state-of-the-art methods,even when dealing with outlier rates exceeding 99%.</description><author>Xinyi Li, Zijian Ma, Yinlong Liu, Walter Zimmer, Hu Cao, Feihu Zhang, Alois Knoll</author><pubDate>Thu, 02 Nov 2023 18:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01432v1</guid></item><item><title>Efficient Vision Transformer for Accurate Traffic Sign Detection</title><link>http://arxiv.org/abs/2311.01429v1</link><description>This research paper addresses the challenges associated with traffic signdetection in self-driving vehicles and driver assistance systems. Thedevelopment of reliable and highly accurate algorithms is crucial for thewidespread adoption of traffic sign recognition and detection (TSRD) in diversereal-life scenarios. However, this task is complicated by suboptimal trafficimages affected by factors such as camera movement, adverse weather conditions,and inadequate lighting. This study specifically focuses on traffic signdetection methods and introduces the application of the Transformer model,particularly the Vision Transformer variants, to tackle this task. TheTransformer's attention mechanism, originally designed for natural languageprocessing, offers improved parallel efficiency. Vision Transformers havedemonstrated success in various domains, including autonomous driving, objectdetection, healthcare, and defense-related applications. To enhance theefficiency of the Transformer model, the research proposes a novel strategythat integrates a locality inductive bias and a transformer module. Thisincludes the introduction of the Efficient Convolution Block and the LocalTransformer Block, which effectively capture short-term and long-termdependency information, thereby improving both detection speed and accuracy.Experimental evaluations demonstrate the significant advancements achieved bythis approach, particularly when applied to the GTSDB dataset.</description><author>Javad Mirzapour Kaleybar, Hooman Khaloo, Avaz Naghipour</author><pubDate>Thu, 02 Nov 2023 18:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01429v1</guid></item><item><title>Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods</title><link>http://arxiv.org/abs/2311.01428v1</link><description>Dementia, a prevalent neurodegenerative condition, is a major manifestationof Alzheimer's disease (AD). As the condition progresses from mild to severe,it significantly impairs the individual's ability to perform daily tasksindependently, necessitating the need for timely and accurate ADclassification. Machine learning or deep learning models have emerged aseffective tools for this purpose. In this study, we suggested an approach forclassifying the four stages of dementia using RF, SVM, and CNN algorithms,augmented with watershed segmentation for feature extraction from MRI images.Our results reveal that SVM with watershed features achieves an impressiveaccuracy of 96.25%, surpassing other classification methods. The ADNI datasetis utilized to evaluate the effectiveness of our method, and we observed thatthe inclusion of watershed segmentation contributes to the enhanced performanceof the models.</description><author>Md Gulzar Hussain, Ye Shiren</author><pubDate>Thu, 02 Nov 2023 18:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01428v1</guid></item><item><title>Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery</title><link>http://arxiv.org/abs/2307.10943v2</link><description>Recent advances in deep learning have significantly improved the performanceof various computer vision applications. However, discovering novel categoriesin an incremental learning scenario remains a challenging problem due to thelack of prior knowledge about the number and nature of new categories. Existingmethods for novel category discovery are limited by their reliance on labeleddatasets and prior knowledge about the number of novel categories and theproportion of novel samples in the batch. To address the limitations and moreaccurately reflect real-world scenarios, in this paper, we propose a novelunsupervised class incremental learning approach for discovering novelcategories on unlabeled sets without prior knowledge. The proposed methodfine-tunes the feature extractor and proxy anchors on labeled sets, then splitssamples into old and novel categories and clusters on the unlabeled dataset.Furthermore, the proxy anchors-based exemplar generates representative categoryvectors to mitigate catastrophic forgetting. Experimental results demonstratethat our proposed approach outperforms the state-of-the-art methods onfine-grained datasets under real-world scenarios.</description><author>Hyungmin Kim, Sungho Suh, Daehwan Kim, Daun Jeong, Hansang Cho, Junmo Kim</author><pubDate>Thu, 02 Nov 2023 18:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10943v2</guid></item><item><title>Exploring Deep Learning Techniques for Glaucoma Detection: A Comprehensive Review</title><link>http://arxiv.org/abs/2311.01425v1</link><description>Glaucoma is one of the primary causes of vision loss around the world,necessitating accurate and efficient detection methods. Traditional manualdetection approaches have limitations in terms of cost, time, and subjectivity.Recent developments in deep learning approaches demonstrate potential inautomating glaucoma detection by detecting relevant features from retinalfundus images. This article provides a comprehensive overview of cutting-edgedeep learning methods used for the segmentation, classification, and detectionof glaucoma. By analyzing recent studies, the effectiveness and limitations ofthese techniques are evaluated, key findings are highlighted, and potentialareas for further research are identified. The use of deep learning algorithmsmay significantly improve the efficacy, usefulness, and accuracy of glaucomadetection. The findings from this research contribute to the ongoingadvancements in automated glaucoma detection and have implications forimproving patient outcomes and reducing the global burden of glaucoma.</description><author>Aized Amin Soofi, Fazal-e-Amin</author><pubDate>Thu, 02 Nov 2023 18:39:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01425v1</guid></item><item><title>Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models</title><link>http://arxiv.org/abs/2309.04316v2</link><description>Natural-language dialog is key for intuitive human-robot interaction. It canbe used not only to express humans' intents, but also to communicateinstructions for improvement if a robot does not understand a commandcorrectly. Of great importance is to endow robots with the ability to learnfrom such interaction experience in an incremental way to allow them to improvetheir behaviors or avoid mistakes in the future. In this paper, we propose asystem to achieve incremental learning of complex behavior from naturalinteraction, and demonstrate its implementation on a humanoid robot. Buildingon recent advances, we present a system that deploys Large Language Models(LLMs) for high-level orchestration of the robot's behavior, based on the ideaof enabling the LLM to generate Python statements in an interactive console toinvoke both robot perception and action. The interaction loop is closed byfeeding back human instructions, environment observations, and executionresults to the LLM, thus informing the generation of the next statement.Specifically, we introduce incremental prompt learning, which enables thesystem to interactively learn from its mistakes. For that purpose, the LLM cancall another LLM responsible for code-level improvements of the currentinteraction based on human feedback. The improved interaction is then saved inthe robot's memory, and thus retrieved on similar requests. We integrate thesystem in the robot cognitive architecture of the humanoid robot ARMAR-6 andevaluate our methods both quantitatively (in simulation) and qualitatively (insimulation and real-world) by demonstrating generalized incrementally-learnedknowledge.</description><author>Leonard Bärmann, Rainer Kartmann, Fabian Peller-Konrad, Alex Waibel, Tamim Asfour</author><pubDate>Thu, 02 Nov 2023 18:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04316v2</guid></item><item><title>Socratis: Are large multimodal models emotionally aware?</title><link>http://arxiv.org/abs/2308.16741v3</link><description>Existing emotion prediction benchmarks contain coarse emotion labels which donot consider the diversity of emotions that an image and text can elicit inhumans due to various reasons. Learning diverse reactions to multimodal contentis important as intelligent machines take a central role in generating anddelivering content to society. To address this gap, we propose Socratis, asocietal reactions benchmark, where each image-caption (IC) pair is annotatedwith multiple emotions and the reasons for feeling them. Socratis contains 18Kfree-form reactions for 980 emotions on 2075 image-caption pairs from 5widely-read news and image-caption (IC) datasets. We benchmark the capabilityof state-of-the-art multimodal large language models to generate the reasonsfor feeling an emotion given an IC pair. Based on a preliminary human study, weobserve that humans prefer human-written reasons over 2 times more often thanmachine-generated ones. This shows our task is harder than standard generationtasks because it starkly contrasts recent findings where humans cannot tellapart machine vs human-written news articles, for instance. We further see thatcurrent captioning metrics based on large vision-language models also fail tocorrelate with human preferences. We hope that these findings and our benchmarkwill inspire further research on training emotionally aware models.</description><author>Katherine Deng, Arijit Ray, Reuben Tan, Saadia Gabriel, Bryan A. Plummer, Kate Saenko</author><pubDate>Thu, 02 Nov 2023 18:37:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16741v3</guid></item><item><title>CenterRadarNet: Joint 3D Object Detection and Tracking Framework using 4D FMCW Radar</title><link>http://arxiv.org/abs/2311.01423v1</link><description>Robust perception is a vital component for ensuring safe autonomous andassisted driving. Automotive radar (77 to 81 GHz), which offersweather-resilient sensing, provides a complementary capability to the vision-or LiDAR-based autonomous driving systems. Raw radio-frequency (RF) radartensors contain rich spatiotemporal semantics besides 3D location information.The majority of previous methods take in 3D (Doppler-range-azimuth) RF radartensors, allowing prediction of an object's location, heading angle, and sizein bird's-eye-view (BEV). However, they lack the ability to at the same timeinfer objects' size, orientation, and identity in the 3D space. To overcomethis limitation, we propose an efficient joint architecture calledCenterRadarNet, designed to facilitate high-resolution representation learningfrom 4D (Doppler-range-azimuth-elevation) radar data for 3D object detectionand re-identification (re-ID) tasks. As a single-stage 3D object detector,CenterRadarNet directly infers the BEV object distribution confidence maps,corresponding 3D bounding box attributes, and appearance embedding for eachpixel. Moreover, we build an online tracker utilizing the learned appearanceembedding for re-ID. CenterRadarNet achieves the state-of-the-art result on theK-Radar 3D object detection benchmark. In addition, we present the first 3Dobject-tracking result using radar on the K-Radar dataset V2. In diversedriving scenarios, CenterRadarNet shows consistent, robust performance,emphasizing its wide applicability.</description><author>Jen-Hao Cheng, Sheng-Yao Kuan, Hugo Latapie, Gaowen Liu, Jenq-Neng Hwang</author><pubDate>Thu, 02 Nov 2023 18:36:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01423v1</guid></item><item><title>Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting</title><link>http://arxiv.org/abs/2211.15856v2</link><description>Producing high-quality forecasts of key climate variables such as temperatureand precipitation on subseasonal time scales has long been a gap in operationalforecasting. Recent studies have shown promising results using machine learning(ML) models to advance subseasonal forecasting (SSF), but several openquestions remain. First, several past approaches use the average of an ensembleof physics-based forecasts as an input feature of these models. However,ensemble forecasts contain information that can aid prediction beyond only theensemble mean. Second, past methods have focused on average performance,whereas forecasts of extreme events are far more important for planning andmitigation purposes. Third, climate forecasts correspond to a spatially-varyingcollection of forecasts, and different methods account for spatial variabilityin the response differently. Trade-offs between different approaches may bemitigated with model stacking. This paper describes the application of avariety of ML methods used to predict monthly average precipitation and twometer temperature using physics-based predictions (ensemble forecasts) andobservational data such as relative humidity, pressure at sea level, orgeopotential height, two weeks in advance for the whole continental UnitedStates. Regression, quantile regression, and tercile classification tasks usinglinear models, random forests, convolutional neural networks, and stackedmodels are considered. The proposed models outperform common baselines such ashistorical averages (or quantiles) and ensemble averages (or quantiles). Thispaper further includes an investigation of feature importance, trade-offsbetween using the full ensemble or only the ensemble average, and differentmodes of accounting for spatial variability.</description><author>Elena Orlova, Haokun Liu, Raphael Rossellini, Benjamin Cash, Rebecca Willett</author><pubDate>Thu, 02 Nov 2023 18:35:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15856v2</guid></item><item><title>Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data</title><link>http://arxiv.org/abs/2311.01420v1</link><description>We propose a learning problem involving adapting a pre-trained source modelto the target domain for classifying all classes that appeared in the sourcedata, using target data that covers only a partial label space. This problem ispractical, as it is unrealistic for the target end-users to collect data forall classes prior to adaptation. However, it has received limited attention inthe literature. To shed light on this issue, we construct benchmark datasetsand conduct extensive experiments to uncover the inherent challenges. We founda dilemma -- on the one hand, adapting to the new target domain is important toclaim better performance; on the other hand, we observe that preserving theclassification accuracy of classes missing in the target adaptation data ishighly challenging, let alone improving them. To tackle this, we identify twokey directions: 1) disentangling domain gradients from classificationgradients, and 2) preserving class relationships. We present several effectivesolutions that maintain the accuracy of the missing classes and enhance theoverall performance, establishing solid baselines for holistic transfer ofpre-trained models with partial target data.</description><author>Cheng-Hao Tu, Hong-You Chen, Zheda Mai, Jike Zhong, Vardaan Pahuja, Tanya Berger-Wolf, Song Gao, Charles Stewart, Yu Su, Wei-Lun Chao</author><pubDate>Thu, 02 Nov 2023 18:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01420v1</guid></item><item><title>CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society</title><link>http://arxiv.org/abs/2303.17760v2</link><description>The rapid advancement of chat-based language models has led to remarkableprogress in complex task-solving. However, their success heavily relies onhuman input to guide the conversation, which can be challenging andtime-consuming. This paper explores the potential of building scalabletechniques to facilitate autonomous cooperation among communicative agents, andprovides insight into their "cognitive" processes. To address the challenges ofachieving autonomous cooperation, we propose a novel communicative agentframework named role-playing. Our approach involves using inception promptingto guide chat agents toward task completion while maintaining consistency withhuman intentions. We showcase how role-playing can be used to generateconversational data for studying the behaviors and capabilities of a society ofagents, providing a valuable resource for investigating conversational languagemodels. In particular, we conduct comprehensive studies oninstruction-following cooperation in multi-agent settings. Our contributionsinclude introducing a novel communicative agent framework, offering a scalableapproach for studying the cooperative behaviors and capabilities of multi-agentsystems, and open-sourcing our library to support research on communicativeagents and beyond: https://github.com/camel-ai/camel.</description><author>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem</author><pubDate>Thu, 02 Nov 2023 18:34:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17760v2</guid></item><item><title>On Learning Gaussian Multi-index Models with Gradient Flow</title><link>http://arxiv.org/abs/2310.19793v2</link><description>We study gradient flow on the multi-index regression problem forhigh-dimensional Gaussian data. Multi-index functions consist of a compositionof an unknown low-rank linear projection and an arbitrary unknown,low-dimensional link function. As such, they constitute a natural template forfeature learning in neural networks. We consider a two-timescale algorithm, whereby the low-dimensional linkfunction is learnt with a non-parametric model infinitely faster than thesubspace parametrizing the low-rank projection. By appropriately exploiting thematrix semigroup structure arising over the subspace correlation matrices, weestablish global convergence of the resulting Grassmannian population gradientflow dynamics, and provide a quantitative description of its associated`saddle-to-saddle' dynamics. Notably, the timescales associated with eachsaddle can be explicitly characterized in terms of an appropriate Hermitedecomposition of the target link function. In contrast with these positiveresults, we also show that the related \emph{planted} problem, where the linkfunction is known and fixed, in fact has a rough optimization landscape, inwhich gradient flow dynamics might get trapped with high probability.</description><author>Alberto Bietti, Joan Bruna, Loucas Pillaud-Vivien</author><pubDate>Thu, 02 Nov 2023 18:33:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19793v2</guid></item><item><title>Castor: Causal Temporal Regime Structure Learning</title><link>http://arxiv.org/abs/2311.01412v1</link><description>The task of uncovering causal relationships among multivariate time seriesdata stands as an essential and challenging objective that cuts across a broadarray of disciplines ranging from climate science to healthcare. Such dataentails linear or non-linear relationships, and usually follow multiple apriori unknown regimes. Existing causal discovery methods can infer summarycausal graphs from heterogeneous data with known regimes, but they fall shortin comprehensively learning both regimes and the corresponding causal graph. Inthis paper, we introduce CASTOR, a novel framework designed to learn causalrelationships in heterogeneous time series data composed of various regimes,each governed by a distinct causal graph. Through the maximization of a scorefunction via the EM algorithm, CASTOR infers the number of regimes and learnslinear or non-linear causal relationships in each regime. We demonstrate therobust convergence properties of CASTOR, specifically highlighting itsproficiency in accurately identifying unique regimes. Empirical evidence,garnered from exhaustive synthetic experiments and two real-world benchmarks,confirm CASTOR's superior performance in causal discovery compared to baselinemethods. By learning a full temporal causal graph for each regime, CASTORestablishes itself as a distinctly interpretable method for causal discovery inheterogeneous time series.</description><author>Abdellah Rahmani, Pascal Frossard</author><pubDate>Thu, 02 Nov 2023 18:26:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01412v1</guid></item><item><title>The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing</title><link>http://arxiv.org/abs/2311.01410v1</link><description>We present a unified probabilistic formulation for diffusion-based imageediting, where a latent variable is edited in a task-specific manner andgenerally deviates from the corresponding marginal distribution induced by theoriginal stochastic or ordinary differential equation (SDE or ODE). Instead, itdefines a corresponding SDE or ODE for editing. In the formulation, we provethat the Kullback-Leibler divergence between the marginal distributions of thetwo SDEs gradually decreases while that for the ODEs remains as the timeapproaches zero, which shows the promise of SDE in image editing. Inspired byit, we provide the SDE counterparts for widely used ODE baselines in varioustasks including inpainting and image-to-image translation, where SDE shows aconsistent and substantial improvement. Moreover, we propose SDE-Drag -- asimple yet effective method built upon the SDE formulation for point-basedcontent dragging. We build a challenging benchmark (termed DragBench) withopen-set natural, art, and AI-generated images for evaluation. A user study onDragBench indicates that SDE-Drag significantly outperforms our ODE baseline,existing diffusion-based methods, and the renowned DragGAN. Our resultsdemonstrate the superiority and versatility of SDE in image editing and pushthe boundary of diffusion-based editing methods.</description><author>Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, Chongxuan Li</author><pubDate>Thu, 02 Nov 2023 18:23:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01410v1</guid></item><item><title>Parting with Misconceptions about Learning-based Vehicle Motion Planning</title><link>http://arxiv.org/abs/2306.07962v2</link><description>The release of nuPlan marks a new era in vehicle motion planning research,offering the first large-scale real-world dataset and evaluation schemesrequiring both precise short-term planning and long-horizon ego-forecasting.Existing systems struggle to simultaneously meet both requirements. Indeed, wefind that these tasks are fundamentally misaligned and should be addressedindependently. We further assess the current state of closed-loop planning inthe field, revealing the limitations of learning-based methods in complexreal-world scenarios and the value of simple rule-based priors such ascenterline selection through lane graph search algorithms. More surprisingly,for the open-loop sub-task, we observe that the best results are achieved whenusing only this centerline as scene context (i.e., ignoring all informationregarding the map and other agents). Combining these insights, we propose anextremely simple and efficient planner which outperforms an extensive set ofcompetitors, winning the nuPlan planning challenge 2023.</description><author>Daniel Dauner, Marcel Hallgarten, Andreas Geiger, Kashyap Chitta</author><pubDate>Thu, 02 Nov 2023 18:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07962v2</guid></item><item><title>A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference</title><link>http://arxiv.org/abs/2311.01409v1</link><description>We present a novel stochastic variational Gaussian process ($\mathcal{GP}$)inference method, based on a posterior over a learnable set of weighted pseudoinput-output points (coresets). Instead of a free-form variational family, theproposed coreset-based, variational tempered family for $\mathcal{GP}$s (CVTGP)is defined in terms of the $\mathcal{GP}$ prior and the data-likelihood; hence,accommodating the modeling inductive biases. We derive CVTGP's lower bound forthe log-marginal likelihood via marginalization of the proposed posterior overlatent $\mathcal{GP}$ coreset variables, and show it is amenable to stochasticoptimization. CVTGP reduces the learnable parameter size to $\mathcal{O}(M)$,enjoys numerical stability, and maintains $\mathcal{O}(M^3)$ time- and$\mathcal{O}(M^2)$ space-complexity, by leveraging a coreset-based temperedposterior that, in turn, provides sparse and explainable representations of thedata. Results on simulated and real-world regression problems with Gaussianobservation noise validate that CVTGP provides better evidence lower-boundestimates and predictive root mean squared error than alternative stochastic$\mathcal{GP}$ inference methods.</description><author>Mert Ketenci, Adler Perotte, Noémie Elhadad, Iñigo Urteaga</author><pubDate>Thu, 02 Nov 2023 18:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01409v1</guid></item><item><title>MiliPoint: A Point Cloud Dataset for mmWave Radar</title><link>http://arxiv.org/abs/2309.13425v2</link><description>Millimetre-wave (mmWave) radar has emerged as an attractive andcost-effective alternative for human activity sensing compared to traditionalcamera-based systems. mmWave radars are also non-intrusive, providing betterprotection for user privacy. However, as a Radio Frequency (RF) basedtechnology, mmWave radars rely on capturing reflected signals from objects,making them more prone to noise compared to cameras. This raises an intriguingquestion for the deep learning community: Can we develop more effective pointset-based deep learning methods for such attractive sensors? To answer this question, our work, termed MiliPoint, delves into this idea byproviding a large-scale, open dataset for the community to explore how mmWaveradars can be utilised for human activity recognition. Moreover, MiliPointstands out as it is larger in size than existing datasets, has more diversehuman actions represented, and encompasses all three key tasks in humanactivity recognition. We have also established a range of point-based deepneural networks such as DGCNN, PointNet++ and PointTransformer, on MiliPoint,which can serve to set the ground baseline for further development.</description><author>Han Cui, Shu Zhong, Jiacheng Wu, Zichao Shen, Naim Dahnoun, Yiren Zhao</author><pubDate>Thu, 02 Nov 2023 18:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13425v2</guid></item><item><title>Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability</title><link>http://arxiv.org/abs/2311.01406v1</link><description>Blockchain technology has revolutionized the way information is propagated indecentralized networks. Ethereum plays a pivotal role in facilitating smartcontracts and decentralized applications. Understanding information propagationdynamics in Ethereum is crucial for ensuring network efficiency, security, andscalability. In this study, we propose an innovative approach that utilizesGraph Convolutional Networks (GCNs) to analyze the information propagationpatterns in the Ethereum network. The first phase of our research involves datacollection from the Ethereum blockchain, consisting of blocks, transactions,and node degrees. We construct a transaction graph representation usingadjacency matrices to capture the node embeddings; while our major contributionis to develop a combined Graph Attention Network (GAT) and ReinforcementLearning (RL) model to optimize the network efficiency and scalability. Itlearns the best actions to take in various network states, ultimately leadingto improved network efficiency, throughput, and optimize gas limits for blockprocessing. In the experimental evaluation, we analyze the performance of ourmodel on a large-scale Ethereum dataset. We investigate effectively aggregatinginformation from neighboring nodes capturing graph structure and updating nodeembeddings using GCN with the objective of transaction pattern prediction,accounting for varying network loads and number of blocks. Not only we design agas limit optimization model and provide the algorithm, but also to addressscalability, we demonstrate the use and implementation of sparse matrices inGraphConv, GraphSAGE, and GAT. The results indicate that our designed GAT-RLmodel achieves superior results compared to other GCN models in terms ofperformance. It effectively propagates information across the network,optimizing gas limits for block processing and improving network efficiency.</description><author>Stefan Kambiz Behfar, Jon Crowcroft</author><pubDate>Thu, 02 Nov 2023 18:19:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01406v1</guid></item><item><title>Learning to See Physical Properties with Active Sensing Motor Policies</title><link>http://arxiv.org/abs/2311.01405v1</link><description>Knowledge of terrain's physical properties inferred from color images can aidin making efficient robotic locomotion plans. However, unlike imageclassification, it is unintuitive for humans to label image patches withphysical properties. Without labeled data, building a vision system that takesas input the observed terrain and predicts physical properties remainschallenging. We present a method that overcomes this challenge byself-supervised labeling of images captured by robots during real-worldtraversal with physical property estimators trained in simulation. To ensureaccurate labeling, we introduce Active Sensing Motor Policies (ASMP), which aretrained to explore locomotion behaviors that increase the accuracy ofestimating physical parameters. For instance, the quadruped robot learns toswipe its foot against the ground to estimate the friction coefficientaccurately. We show that the visual system trained with a small amount ofreal-world traversal data accurately predicts physical parameters. The trainedsystem is robust and works even with overhead images captured by a dronedespite being trained on data collected by cameras attached to a quadrupedrobot walking on the ground.</description><author>Gabriel B. Margolis, Xiang Fu, Yandong Ji, Pulkit Agrawal</author><pubDate>Thu, 02 Nov 2023 18:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01405v1</guid></item><item><title>Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds</title><link>http://arxiv.org/abs/2310.06157v2</link><description>Manifolds discovered by machine learning models provide a compactrepresentation of the underlying data. Geodesics on these manifolds definelocally length-minimising curves and provide a notion of distance, which arekey for reduced-order modelling, statistical inference, and interpolation. Inthis work, we propose a model-based parameterisation for distance fields andgeodesic flows on manifolds, exploiting solutions of a manifold-augmentedEikonal equation. We demonstrate how the geometry of the manifold impacts thedistance field, and exploit the geodesic flow to obtain globallylength-minimising curves directly. This work opens opportunities for statisticsand reduced-order modelling on differentiable manifolds.</description><author>Daniel Kelshaw, Luca Magri</author><pubDate>Thu, 02 Nov 2023 18:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06157v2</guid></item><item><title>Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs</title><link>http://arxiv.org/abs/2311.01404v1</link><description>The term "Normalizing Flows" is related to the task of constructinginvertible transport maps between probability measures by means of deep neuralnetworks. In this paper, we consider the problem of recovering the$W_2$-optimal transport map $T$ between absolutely continuous measures$\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neuralODE. We first show that, under suitable assumptions on $\mu,\nu$ and on thecontrolled vector fields, the optimal transport map is contained in the$C^0_c$-closure of the flows generated by the system. Assuming that discreteapproximations $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available,we use a discrete optimal coupling $\gamma_N$ to define an optimal controlproblem. With a $\Gamma$-convergence argument, we prove that its solutionscorrespond to flows that approximate the optimal transport map $T$. Finally,taking advantage of the Pontryagin Maximum Principle, we propose an iterativenumerical scheme for the resolution of the optimal control problem, resultingin an algorithm for the practical computation of the approximated optimaltransport map.</description><author>Alessandro Scagliotti, Sara Farinelli</author><pubDate>Thu, 02 Nov 2023 18:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01404v1</guid></item><item><title>Server-side Rescoring of Spoken Entity-centric Knowledge Queries for Virtual Assistants</title><link>http://arxiv.org/abs/2311.01398v1</link><description>On-device Virtual Assistants (VAs) powered by Automatic Speech Recognition(ASR) require effective knowledge integration for the challenging entity-richquery recognition. In this paper, we conduct an empirical study of modelingstrategies for server-side rescoring of spoken information domain queries usingvarious categories of Language Models (LMs) (N-gram word LMs, sub-word neuralLMs). We investigate the combination of on-device and server-side signals, anddemonstrate significant WER improvements of 23%-35% on various entity-centricquery subpopulations by integrating various server-side LMs compared toperforming ASR on-device only. We also perform a comparison between LMs trainedon domain data and a GPT-3 variant offered by OpenAI as a baseline.Furthermore, we also show that model fusion of multiple server-side LMs trainedfrom scratch most effectively combines complementary strengths of each modeland integrates knowledge learned from domain-specific data to a VA ASR system.</description><author>Youyuan Zhang, Sashank Gondala, Thiago Fraga-Silva, Christophe Van Gysel</author><pubDate>Thu, 02 Nov 2023 18:07:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01398v1</guid></item><item><title>Learning Realistic Traffic Agents in Closed-loop</title><link>http://arxiv.org/abs/2311.01394v1</link><description>Realistic traffic simulation is crucial for developing self-driving softwarein a safe and scalable manner prior to real-world deployment. Typically,imitation learning (IL) is used to learn human-like traffic agents directlyfrom real-world observations collected offline, but without explicitspecification of traffic rules, agents trained from IL alone frequently displayunrealistic infractions like collisions and driving off the road. This problemis exacerbated in out-of-distribution and long-tail scenarios. On the otherhand, reinforcement learning (RL) can train traffic agents to avoidinfractions, but using RL alone results in unhuman-like driving behaviors. Wepropose Reinforcing Traffic Rules (RTR), a holistic closed-loop learningobjective to match expert demonstrations under a traffic compliance constraint,which naturally gives rise to a joint IL + RL approach, obtaining the best ofboth worlds. Our method learns in closed-loop simulations of both nominalscenarios from real-world datasets as well as procedurally generated long-tailscenarios. Our experiments show that RTR learns more realistic andgeneralizable traffic simulation policies, achieving significantly bettertradeoffs between human-like driving and traffic compliance in both nominal andlong-tail scenarios. Moreover, when used as a data generation tool for trainingprediction models, our learned traffic policy leads to considerably improveddownstream prediction metrics compared to baseline traffic agents. For moreinformation, visit the project website: https://waabi.ai/rtr</description><author>Chris Zhang, James Tu, Lunjun Zhang, Kelvin Wong, Simon Suo, Raquel Urtasun</author><pubDate>Thu, 02 Nov 2023 17:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01394v1</guid></item><item><title>You Only Look at Once for Real-time and Generic Multi-Task</title><link>http://arxiv.org/abs/2310.01641v3</link><description>High precision, lightweight, and real-time responsiveness are three essentialrequirements for implementing autonomous driving. In this study, we present anadaptive, real-time, and lightweight multi-task model designed to concurrentlyaddress object detection, drivable area segmentation, and lane linesegmentation tasks. Specifically, we developed an end-to-end multi-task modelwith a unified and streamlined segmentation structure. We introduced alearnable parameter that adaptively concatenate features in segmentation necks,using the same loss function for all segmentation tasks. This eliminates theneed for customizations and enhances the model's generalization capabilities.We also introduced a segmentation head composed only of a series ofconvolutional layers, which reduces the inference time. We achieved competitiveresults on the BDD100k dataset, particularly in visualization outcomes. Theperformance results show a mAP50 of 81.1% for object detection, a mIoU of 91.0%for drivable area segmentation, and an IoU of 28.8% for lane line segmentation.Additionally, we introduced real-world scenarios to evaluate our model'sperformance in a real scene, which significantly outperforms competitors. Thisdemonstrates that our model not only exhibits competitive performance but isalso more flexible and faster than existing multi-task models. The source codesand pre-trained models are released athttps://github.com/JiayuanWang-JW/YOLOv8-multi-task</description><author>Jiayuan Wang, Q. M. Jonathan Wu, Ning Zhang</author><pubDate>Thu, 02 Nov 2023 17:52:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01641v3</guid></item><item><title>Time-series Generation by Contrastive Imitation</title><link>http://arxiv.org/abs/2311.01388v1</link><description>Consider learning a generative model for time-series data. The sequentialsetting poses a unique challenge: Not only should the generator capture theconditional dynamics of (stepwise) transitions, but its open-loop rolloutsshould also preserve the joint distribution of (multi-step) trajectories. Onone hand, autoregressive models trained by MLE allow learning and computingexplicit transition distributions, but suffer from compounding error duringrollouts. On the other hand, adversarial models based on GAN training alleviatesuch exposure bias, but transitions are implicit and hard to assess. In thiswork, we study a generative framework that seeks to combine the strengths ofboth: Motivated by a moment-matching objective to mitigate compounding error,we optimize a local (but forward-looking) transition policy, where thereinforcement signal is provided by a global (but stepwise-decomposable) energymodel trained by contrastive estimation. At training, the two components arelearned cooperatively, avoiding the instabilities typical of adversarialobjectives. At inference, the learned policy serves as the generator foriterative sampling, and the learned energy serves as a trajectory-level measurefor evaluating sample quality. By expressly training a policy to imitatesequential behavior of time-series features in a dataset, this approachembodies "generation by imitation". Theoretically, we illustrate thecorrectness of this formulation and the consistency of the algorithm.Empirically, we evaluate its ability to generate predictively useful samplesfrom real-world datasets, verifying that it performs at the standard ofexisting benchmarks.</description><author>Daniel Jarrett, Ioana Bica, Mihaela van der Schaar</author><pubDate>Thu, 02 Nov 2023 17:45:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01388v1</guid></item><item><title>Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics</title><link>http://arxiv.org/abs/2311.01386v1</link><description>Language models (LMs) have been argued to overlap substantially with humanbeings in grammaticality judgment tasks. But when humans systematically makeerrors in language processing, should we expect LMs to behave like cognitivemodels of language and mimic human behavior? We answer this question byinvestigating LMs' more subtle judgments associated with "language illusions"-- sentences that are vague in meaning, implausible, or ungrammatical butreceive unexpectedly high acceptability judgments by humans. We looked at threeillusions: the comparative illusion (e.g. "More people have been to Russia thanI have"), the depth-charge illusion (e.g. "No head injury is too trivial to beignored"), and the negative polarity item (NPI) illusion (e.g. "The hunter whono villager believed to be trustworthy will ever shoot a bear"). We found thatprobabilities represented by LMs were more likely to align with human judgmentsof being "tricked" by the NPI illusion which examines a structural dependency,compared to the comparative and the depth-charge illusions which requiresophisticated semantic understanding. No single LM or metric yielded resultsthat are entirely consistent with human behavior. Ultimately, we show that LMsare limited both in their construal as cognitive models of human languageprocessing and in their capacity to recognize nuanced but critical informationin complicated language materials.</description><author>Yuhan Zhang, Edward Gibson, Forrest Davis</author><pubDate>Thu, 02 Nov 2023 17:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01386v1</guid></item><item><title>Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models</title><link>http://arxiv.org/abs/2306.17820v2</link><description>Neural-symbolic methods have shown their effectiveness in enhancing thereasoning abilities of large language models (LLMs). However, existing methodsprimarily rely on mapping natural languages to more syntactically completeformal languages (e.g., Python and SQL). Those approaches necessitate thatreasoning tasks be convertible into programs, which cater more to the computerexecution mindset and deviate from human reasoning habits. To expand thereal-world applicability and flexibility of symbolic methods, we proposeMeta-Reasoning from the scope of linguistics itself. This method empowers LLMsto deconstruct questions and effectively capture more generalized knowledgeautonomously. We find that Meta-Reasoning achieves improved in-context learningefficiency, reasoning accuracy, and output stability in six arithmetic andsymbolic reasoning tasks. In particular, when applied to symbolic reasoningtasks such as Tracking Shuffled Objects, GPT-3 (text-davinci-002) surpasses thefew-shot Chain-of-Thought prompting approach (+37.7%), with 99% accuracy aftera single demonstration of Meta-Reasoning.</description><author>Yiming Wang, Zhuosheng Zhang, Rui Wang</author><pubDate>Thu, 02 Nov 2023 17:38:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17820v2</guid></item><item><title>Sim2Real Bilevel Adaptation for Object Surface Classification using Vision-Based Tactile Sensors</title><link>http://arxiv.org/abs/2311.01380v1</link><description>In this paper, we address the Sim2Real gap in the field of vision-basedtactile sensors for classifying object surfaces. We train a Diffusion Model tobridge this gap using a relatively small dataset of real-world images randomlycollected from unlabeled everyday objects via the DIGIT sensor. Subsequently,we employ a simulator to generate images by uniformly sampling the surface ofobjects from the YCB Model Set. These simulated images are then translated intothe real domain using the Diffusion Model and automatically labeled to train aclassifier. During this training, we further align features of the two domainsusing an adversarial procedure. Our evaluation is conducted on a dataset oftactile images obtained from a set of ten 3D printed YCB objects. The resultsreveal a total accuracy of 81.9%, a significant improvement compared to the34.7% achieved by the classifier trained solely on simulated images. Thisdemonstrates the effectiveness of our approach. We further validate ourapproach using the classifier on a 6D object pose estimation task from tactiledata.</description><author>Gabriele M. Caddeo, Andrea Maracani, Paolo D. Alfano, Nicola A. Piga, Lorenzo Rosasco, Lorenzo Natale</author><pubDate>Thu, 02 Nov 2023 17:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01380v1</guid></item><item><title>Vision-Language Foundation Models as Effective Robot Imitators</title><link>http://arxiv.org/abs/2311.01378v1</link><description>Recent progress in vision language foundation models has shown their abilityto understand multimodal data and resolve complicated vision language tasks,including robotics manipulation. We seek a straightforward way of making use ofexisting vision-language models (VLMs) with simple fine-tuning on roboticsdata. To this end, we derive a simple and novel vision-language manipulationframework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo.Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-stepvision-language comprehension, models sequential history information with anexplicit policy head, and is slightly fine-tuned by imitation learning only onlanguage-conditioned manipulation datasets. Such a decomposition providesRoboFlamingo the flexibility for open-loop control and deployment onlow-performance platforms. By exceeding the state-of-the-art performance with alarge margin on the tested benchmark, we show RoboFlamingo can be an effectiveand competitive alternative to adapt VLMs to robot control. Our extensiveexperimental results also reveal several interesting conclusions regarding thebehavior of different pre-trained VLMs on manipulation tasks. We believeRoboFlamingo has the potential to be a cost-effective and easy-to-use solutionfor robotics manipulation, empowering everyone with the ability to fine-tunetheir own robotics policy.</description><author>Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong</author><pubDate>Thu, 02 Nov 2023 17:34:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01378v1</guid></item><item><title>Analysis of tidal flows through the Strait of Gibraltar using Dynamic Mode Decomposition</title><link>http://arxiv.org/abs/2311.01377v1</link><description>The Strait of Gibraltar is a region characterized by intricate oceanicsub-mesoscale features, influenced by topography, tidal forces, instabilities,and nonlinear hydraulic processes, all governed by the nonlinear equations offluid motion. In this study, we aim to uncover the underlying physics of thesephenomena within 3D MIT general circulation model simulations, including waves,eddies, and gyres. To achieve this, we employ Dynamic Mode Decomposition (DMD)to break down simulation snapshots into Koopman modes, with distinctexponential growth/decay rates and oscillation frequencies. Our objectivesencompass evaluating DMD's efficacy in capturing known features, unveiling newelements, ranking modes, and exploring order reduction. We also introducemodifications to enhance DMD's robustness, numerical accuracy, and robustnessof eigenvalues. DMD analysis yields a comprehensive understanding of flowpatterns, internal wave formation, and the dynamics of the Strait of Gibraltar,its meandering behaviors, and the formation of a secondary gyre, notably theWestern Alboran Gyre, as well as the propagation of Kelvin and coastal-trappedwaves along the African coast. In doing so, it significantly advances ourcomprehension of intricate oceanographic phenomena and underscores the immenseutility of DMD as an analytical tool for such complex datasets, suggesting thatDMD could serve as a valuable addition to the toolkit of oceanographers.</description><author>Sathsara Dias, Sudam Surasinghe, Kanaththa Priyankara, Marko Budišić, Larry Pratt, José C. Sanchez-Garrido, Erik M. Bollt</author><pubDate>Thu, 02 Nov 2023 17:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01377v1</guid></item><item><title>Monotone Generative Modeling via a Gromov-Monge Embedding</title><link>http://arxiv.org/abs/2311.01375v1</link><description>Generative Adversarial Networks (GANs) are powerful tools for creating newcontent, but they face challenges such as sensitivity to starting conditionsand mode collapse. To address these issues, we propose a deep generative modelthat utilizes the Gromov-Monge embedding (GME). It helps identify thelow-dimensional structure of the underlying measure of the data and then mapsit, while preserving its geometry, into a measure in a low-dimensional latentspace, which is then optimally transported to the reference measure. Weguarantee the preservation of the underlying geometry by the GME and$c$-cyclical monotonicity of the generative map, where $c$ is an intrinsicembedding cost employed by the GME. The latter property is a first step inguaranteeing better robustness to initialization of parameters and modecollapse. Numerical experiments demonstrate the effectiveness of our approachin generating high-quality images, avoiding mode collapse, and exhibitingrobustness to different starting conditions.</description><author>Wonjun Lee, Yifei Yang, Dongmian Zou, Gilad Lerman</author><pubDate>Thu, 02 Nov 2023 17:33:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01375v1</guid></item><item><title>Recognize Any Regions</title><link>http://arxiv.org/abs/2311.01373v1</link><description>Understanding the semantics of individual regions or patches withinunconstrained images, such as in open-world object detection, represents acritical yet challenging task in computer vision. Building on the success ofpowerful image-level vision-language (ViL) foundation models like CLIP, recentefforts have sought to harness their capabilities by either training acontrastive model from scratch with an extensive collection of region-labelpairs or aligning the outputs of a detection model with image-levelrepresentations of region proposals. Despite notable progress, these approachesare plagued by computationally intensive training requirements, susceptibilityto data noise, and deficiency in contextual information. To address theselimitations, we explore the synergistic potential of off-the-shelf foundationmodels, leveraging their respective strengths in localization and semantics. Weintroduce a novel, generic, and efficient region recognition architecture,named RegionSpot, designed to integrate position-aware localization knowledgefrom a localization foundation model (e.g., SAM) with semantic informationextracted from a ViL model (e.g., CLIP). To fully exploit pretrained knowledgewhile minimizing training overhead, we keep both foundation models frozen,focusing optimization efforts solely on a lightweight attention-based knowledgeintegration module. Through extensive experiments in the context of open-worldobject recognition, our RegionSpot demonstrates significant performanceimprovements over prior alternatives, while also providing substantialcomputational savings. For instance, training our model with 3 million data ina single day using 8 V100 GPUs. Our model outperforms GLIP by 6.5 % in meanaverage precision (mAP), with an even larger margin by 14.8 % for morechallenging and rare categories.</description><author>Haosen Yang, Chuofan Ma, Bin Wen, Yi Jiang, Zehuan Yuan, Xiatian Zhu</author><pubDate>Thu, 02 Nov 2023 17:31:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01373v1</guid></item><item><title>Respiratory Anomaly Detection using Reflected Infrared Light-wave Signals</title><link>http://arxiv.org/abs/2311.01367v1</link><description>In this study, we present a non-contact respiratory anomaly detection methodusing incoherent light-wave signals reflected from the chest of a mechanicalrobot that can breathe like human beings. In comparison to existing radar andcamera-based sensing systems for vitals monitoring, this technology uses only alow-cost ubiquitous light source (e.g., infrared light emitting diode) andsensor (e.g., photodetector). This light-wave sensing (LWS) system recognizesdifferent breathing anomalies from the variations of light intensity reflectedfrom the chest of the robot within a 0.5m-1.5m range. The anomaly detectionmodel demonstrates up to 96.6% average accuracy in classifying 7 differenttypes of breathing data using machine learning. The model can also detectfaulty data collected by the system that does not contain breathinginformation. The developed system can be utilized at home or healthcarefacilities as a smart, non-contact and discreet respiration monitoring method.</description><author>Md Zobaer Islam, Brenden Martin, Carly Gotcher, Tyler Martinez, John F. O'Hara, Sabit Ekin</author><pubDate>Thu, 02 Nov 2023 17:23:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01367v1</guid></item><item><title>GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks</title><link>http://arxiv.org/abs/2311.01361v1</link><description>Automatically evaluating vision-language tasks is challenging, especiallywhen it comes to reflecting human judgments due to limitations in accountingfor fine-grained details. Although GPT-4V has shown promising results invarious multi-modal tasks, leveraging GPT-4V as a generalist evaluator forthese tasks has not yet been systematically explored. We comprehensivelyvalidate GPT-4V's capabilities for evaluation purposes, addressing tasksranging from foundational image-to-text and text-to-image synthesis tohigh-level image-to-image translations and multi-images to text alignment. Weemploy two evaluation methods, single-answer grading and pairwise comparison,using GPT-4V. Notably, GPT-4V shows promising agreement with humans acrossvarious tasks and evaluation methods, demonstrating immense potential formulti-modal LLMs as evaluators. Despite limitations like restricted visualclarity grading and real-world complex reasoning, its ability to providehuman-aligned scores enriched with detailed explanations is promising foruniversal automatic evaluator.</description><author>Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, Linda Ruth Petzold</author><pubDate>Thu, 02 Nov 2023 17:11:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01361v1</guid></item><item><title>The Universal Statistical Structure and Scaling Laws of Chaos and Turbulence</title><link>http://arxiv.org/abs/2311.01358v1</link><description>Turbulence is a complex spatial and temporal structure created by the strongnon-linear dynamics of fluid flows at high Reynolds numbers. Despite being anubiquitous phenomenon that has been studied for centuries, a full understandingof turbulence remained a formidable challenge. Here, we introduce tools fromthe fields of quantum chaos and Random Matrix Theory (RMT) and present adetailed analysis of image datasets generated from turbulence simulations ofincompressible and compressible fluid flows. Focusing on two observables: thedata Gram matrix and the single image distribution, we study both the local andglobal eigenvalue statistics and compare them to classical chaos, uncorrelatednoise and natural images. We show that from the RMT perspective, the turbulenceGram matrices lie in the same universality class as quantum chaotic rather thanintegrable systems, and the data exhibits power-law scalings in the bulk of itseigenvalues which are vastly different from uncorrelated classical chaos,random data, natural images. Interestingly, we find that the single sampledistribution only appears as fully RMT chaotic, but deviates from chaos atlarger correlation lengths, as well as exhibiting different scaling properties.</description><author>Noam Levi, Yaron Oz</author><pubDate>Thu, 02 Nov 2023 17:04:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01358v1</guid></item><item><title>Robust Identity Perceptual Watermark Against Deepfake Face Swapping</title><link>http://arxiv.org/abs/2311.01357v1</link><description>Notwithstanding offering convenience and entertainment to society, Deepfakeface swapping has caused critical privacy issues with the rapid development ofdeep generative models. Due to imperceptible artifacts in high-qualitysynthetic images, passive detection models against face swapping in recentyears usually suffer performance damping regarding the generalizability issue.Therefore, several studies have been attempted to proactively protect theoriginal images against malicious manipulations by inserting invisible signalsin advance. However, the existing proactive defense approaches demonstrateunsatisfactory results with respect to visual quality, detection accuracy, andsource tracing ability. In this study, we propose the first robust identityperceptual watermarking framework that concurrently performs detection andsource tracing against Deepfake face swapping proactively. We assign identitysemantics regarding the image contents to the watermarks and devise anunpredictable and unreversible chaotic encryption system to ensure watermarkconfidentiality. The watermarks are encoded and recovered by jointly trainingan encoder-decoder framework along with adversarial image manipulations.Extensive experiments demonstrate state-of-the-art performance against Deepfakeface swapping under both cross-dataset and cross-manipulation settings.</description><author>Tianyi Wang, Mengxiao Huang, Harry Cheng, Bin Ma, Yinglong Wang</author><pubDate>Thu, 02 Nov 2023 17:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01357v1</guid></item><item><title>Discovering Universal Geometry in Embeddings with ICA</title><link>http://arxiv.org/abs/2305.13175v2</link><description>This study utilizes Independent Component Analysis (ICA) to unveil aconsistent semantic structure within embeddings of words or images. Ourapproach extracts independent semantic components from the embeddings of apre-trained model by leveraging anisotropic information that remains after thewhitening process in Principal Component Analysis (PCA). We demonstrate thateach embedding can be expressed as a composition of a few intrinsicinterpretable axes and that these semantic axes remain consistent acrossdifferent languages, algorithms, and modalities. The discovery of a universalsemantic structure in the geometric patterns of embeddings enhances ourunderstanding of the representations in embeddings.</description><author>Hiroaki Yamagiwa, Momose Oyama, Hidetoshi Shimodaira</author><pubDate>Thu, 02 Nov 2023 17:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13175v2</guid></item><item><title>On the Lipschitz constant of random neural networks</title><link>http://arxiv.org/abs/2311.01356v1</link><description>Empirical studies have widely demonstrated that neural networks are highlysensitive to small, adversarial perturbations of the input. The worst-caserobustness against these so-called adversarial examples can be quantified bythe Lipschitz constant of the neural network. However, only few theoreticalresults regarding this quantity exist in the literature. In this paper, weinitiate the study of the Lipschitz constant of random ReLU neural networks,i.e., neural networks whose weights are chosen at random and which employ theReLU activation function. For shallow neural networks, we characterize theLipschitz constant up to an absolute numerical constant. Moreover, we extendour analysis to deep neural networks of sufficiently large width where we proveupper and lower bounds for the Lipschitz constant. These bounds match up to alogarithmic factor that depends on the depth.</description><author>Paul Geuchen, Thomas Heindl, Dominik Stöger, Felix Voigtlaender</author><pubDate>Thu, 02 Nov 2023 17:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01356v1</guid></item><item><title>Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation</title><link>http://arxiv.org/abs/2307.02598v2</link><description>We tackle the problems of latent variables identification and``out-of-support'' image generation in representation learning. We show thatboth are possible for a class of decoders that we call additive, which arereminiscent of decoders used for object-centric representation learning (OCRL)and well suited for images that can be decomposed as a sum of object-specificimages. We provide conditions under which exactly solving the reconstructionproblem using an additive decoder is guaranteed to identify the blocks oflatent variables up to permutation and block-wise invertible transformations.This guarantee relies only on very weak assumptions about the distribution ofthe latent factors, which might present statistical dependencies and have analmost arbitrarily shaped support. Our result provides a new setting wherenonlinear independent component analysis (ICA) is possible and adds to ourtheoretical understanding of OCRL methods. We also show theoretically thatadditive decoders can generate novel images by recombining observed factors ofvariations in novel ways, an ability we refer to as Cartesian-productextrapolation. We show empirically that additivity is crucial for bothidentifiability and extrapolation on simulated data.</description><author>Sébastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, Simon Lacoste-Julien</author><pubDate>Thu, 02 Nov 2023 17:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02598v2</guid></item><item><title>Norm of Word Embedding Encodes Information Gain</title><link>http://arxiv.org/abs/2212.09663v3</link><description>Distributed representations of words encode lexical semantic information, butwhat type of information is encoded and how? Focusing on the skip-gram withnegative-sampling method, we found that the squared norm of static wordembedding encodes the information gain conveyed by the word; the informationgain is defined by the Kullback-Leibler divergence of the co-occurrencedistribution of the word to the unigram distribution. Our findings areexplained by the theoretical framework of the exponential family of probabilitydistributions and confirmed through precise experiments that remove spuriouscorrelations arising from word frequency. This theory also extends tocontextualized word embeddings in language models or any neural networks withthe softmax output layer. We also demonstrate that both the KL divergence andthe squared norm of embedding provide a useful metric of the informativeness ofa word in tasks such as keyword extraction, proper-noun discrimination, andhypernym discrimination.</description><author>Momose Oyama, Sho Yokoi, Hidetoshi Shimodaira</author><pubDate>Thu, 02 Nov 2023 17:01:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09663v3</guid></item><item><title>Deep learning based Image Compression for Microscopy Images: An Empirical Study</title><link>http://arxiv.org/abs/2311.01352v1</link><description>With the fast development of modern microscopes and bioimaging techniques, anunprecedentedly large amount of imaging data are being generated, stored,analyzed, and even shared through networks. The size of the data poses greatchallenges for current data infrastructure. One common way to reduce the datasize is by image compression. This present study analyzes classic and deeplearning based image compression methods, and their impact on deep learningbased image processing models. Deep learning based label-free prediction models(i.e., predicting fluorescent images from bright field images) are used as anexample application for comparison and analysis. Effective image compressionmethods could help reduce the data size significantly without losing necessaryinformation, and therefore reduce the burden on data management infrastructureand permit fast transmission through the network for data sharing or cloudcomputing. To compress images in such a wanted way, multiple classical lossyimage compression techniques are compared to several AI-based compressionmodels provided by and trained with the CompressAI toolbox using python. Thesedifferent compression techniques are compared in compression ratio, multipleimage similarity measures and, most importantly, the prediction accuracy fromlabel-free models on compressed images. We found that AI-based compressiontechniques largely outperform the classic ones and will minimally affect thedownstream label-free task in 2D cases. In the end, we hope the present studycould shed light on the potential of deep learning based image compression andthe impact of image compression on downstream deep learning based imageanalysis models.</description><author>Yu Zhou, Jan Sollman, Jianxu Chen</author><pubDate>Thu, 02 Nov 2023 17:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01352v1</guid></item><item><title>Simplicial Models for the Epistemic Logic of Faulty Agents</title><link>http://arxiv.org/abs/2311.01351v1</link><description>In recent years, several authors have been investigating simplicial models, amodel of epistemic logic based on higher-dimensional structures calledsimplicial complexes. In the original formulation, simplicial models werealways assumed to be pure, meaning that all worlds have the same dimension.This is equivalent to the standard S5n semantics of epistemic logic, based onKripke models. By removing the assumption that models must be pure, we can gobeyond the usual Kripke semantics and study epistemic logics where the numberof agents participating in a world can vary. This approach has been developedin a number of papers, with applications in fault-tolerant distributedcomputing where processes may crash during the execution of a system. Adifficulty that arises is that subtle design choices in the definition ofimpure simplicial models can result in different axioms of the resulting logic.In this paper, we classify those design choices systematically, and axiomatizethe corresponding logics. We illustrate them via distributed computing examplesof synchronous systems where processes may crash.</description><author>Eric Goubault, Roman Kniazev, Jeremy Ledent, Sergio Rajsbaum</author><pubDate>Thu, 02 Nov 2023 17:00:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01351v1</guid></item><item><title>Unreading Race: Purging Protected Features from Chest X-ray Embeddings</title><link>http://arxiv.org/abs/2311.01349v1</link><description>Purpose: To analyze and remove protected feature effects in chest radiographembeddings of deep learning models. Materials and Methods: An orthogonalization is utilized to remove theinfluence of protected features (e.g., age, sex, race) in chest radiographembeddings, ensuring feature-independent results. To validate the efficacy ofthe approach, we retrospectively study the MIMIC and CheXpert datasets usingthree pre-trained models, namely a supervised contrastive, a self-supervisedcontrastive, and a baseline classifier model. Our statistical analysis involvescomparing the original versus the orthogonalized embeddings by estimatingprotected feature influences and evaluating the ability to predict race, age,or sex using the two types of embeddings. Results: Our experiments reveal a significant influence of protected featureson predictions of pathologies. Applying orthogonalization removes these featureeffects. Apart from removing any influence on pathology classification, whilemaintaining competitive predictive performance, orthogonalized embeddingsfurther make it infeasible to directly predict protected attributes andmitigate subgroup disparities. Conclusion: The presented work demonstrates the successful application andevaluation of the orthogonalization technique in the domain of chest X-rayclassification.</description><author>Tobias Weber, Michael Ingrisch, Bernd Bischl, David Rügamer</author><pubDate>Thu, 02 Nov 2023 16:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01349v1</guid></item><item><title>Improving word mover's distance by leveraging self-attention matrix</title><link>http://arxiv.org/abs/2211.06229v2</link><description>Measuring the semantic similarity between two sentences is still an importanttask. The word mover's distance (WMD) computes the similarity via the optimalalignment between the sets of word embeddings. However, WMD does not utilizeword order, making it challenging to distinguish sentences with significantoverlaps of similar words, even if they are semantically very different. Here,we attempt to improve WMD by incorporating the sentence structure representedby BERT's self-attention matrix (SAM). The proposed method is based on theFused Gromov-Wasserstein distance, which simultaneously considers thesimilarity of the word embedding and the SAM for calculating the optimaltransport between two sentences. Experiments demonstrate the proposed methodenhances WMD and its variants in paraphrase identification with near-equivalentperformance in semantic textual similarity. Our code is available at\url{https://github.com/ymgw55/WSMD}.</description><author>Hiroaki Yamagiwa, Sho Yokoi, Hidetoshi Shimodaira</author><pubDate>Thu, 02 Nov 2023 16:58:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06229v2</guid></item><item><title>Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers</title><link>http://arxiv.org/abs/2311.01344v1</link><description>Model extraction is a growing concern for the security of AI systems. Fordeep neural network models, the architecture is the most important informationan adversary aims to recover. Being a sequence of repeated computation blocks,neural network models deployed on edge-devices will generate distinctiveside-channel leakages. The latter can be exploited to extract criticalinformation when targeted platforms are physically accessible. By combiningtheoretical knowledge about deep learning practices and analysis of awidespread implementation library (ARM CMSIS-NN), our purpose is to answer thiscritical question: how far can we extract architecture information by simplyexamining an EM side-channel trace? For the first time, we propose anextraction methodology for traditional MLP and CNN models running on a high-end32-bit microcontroller (Cortex-M7) that relies only on simple patternrecognition analysis. Despite few challenging cases, we claim that, contrary toparameters extraction, the complexity of the attack is relatively low and wehighlight the urgent need for practicable protections that could fit the strongmemory and latency requirements of such platforms.</description><author>Raphael Joud, Pierre-Alain Moellic, Simon Pontie, Jean-Baptiste Rigaud</author><pubDate>Thu, 02 Nov 2023 16:55:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01344v1</guid></item><item><title>1D-CapsNet-LSTM: A Deep Learning-Based Model for Multi-Step Stock Index Forecasting</title><link>http://arxiv.org/abs/2310.02090v2</link><description>Multi-step stock index forecasting is vital in finance for informeddecision-making. Current forecasting methods on this task frequently produceunsatisfactory results due to the inherent data randomness and instability,thereby underscoring the demand for advanced forecasting models. Given thesuperiority of capsule network (CapsNet) over CNN in various forecasting andclassification tasks, this study investigates the potential of integrating a 1DCapsNet with an LSTM network for multi-step stock index forecasting. To thisend, a hybrid 1D-CapsNet-LSTM model is introduced, which utilizes a 1D CapsNetto generate high-level capsules from sequential data and a LSTM network tocapture temporal dependencies. To maintain stochastic dependencies overdifferent forecasting horizons, a multi-input multi-output (MIMO) strategy isemployed. The model's performance is evaluated on real-world stock marketindices, including S&amp;P 500, DJIA, IXIC, and NYSE, and compared to baselinemodels, including LSTM, RNN, and CNN-LSTM, using metrics such as RMSE, MAE,MAPE, and TIC. The proposed 1D-CapsNet-LSTM model consistently outperformsbaseline models in two key aspects. It exhibits significant reductions inforecasting errors compared to baseline models. Furthermore, it displays aslower rate of error increase with lengthening forecast horizons, indicatingincreased robustness for multi-step forecasting tasks.</description><author>Cheng Zhang, Nilam Nur Amir Sjarif, Roslina Ibrahim</author><pubDate>Thu, 02 Nov 2023 16:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02090v2</guid></item><item><title>Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching</title><link>http://arxiv.org/abs/2311.01331v1</link><description>In real-world scenarios, arbitrary interactions with the environment canoften be costly, and actions of expert demonstrations are not always available.To reduce the need for both, Offline Learning from Observations (LfO) isextensively studied, where the agent learns to solve a task with only expertstates and \textit{task-agnostic} non-expert state-action pairs. Thestate-of-the-art DIstribution Correction Estimation (DICE) methods minimize thestate occupancy divergence between the learner and expert policies. However,they are limited to either $f$-divergences (KL and $\chi^2$) or Wassersteindistance with Rubinstein duality, the latter of which constrains the underlyingdistance metric crucial to the performance of Wasserstein-based solutions. Toaddress this problem, we propose Primal Wasserstein DICE (PW-DICE), whichminimizes the primal Wasserstein distance between the expert and learner stateoccupancies with a pessimistic regularizer and leverages a contrastivelylearned distance as the underlying metric for the Wasserstein distance.Theoretically, we prove that our framework is a generalization of thestate-of-the-art, SMODICE, and unifies $f$-divergence and Wassersteinminimization. Empirically, we find that PW-DICE improves upon severalstate-of-the-art methods on multiple testbeds.</description><author>Kai Yan, Alexander G. Schwing, Yu-xiong Wang</author><pubDate>Thu, 02 Nov 2023 16:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01331v1</guid></item><item><title>A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories</title><link>http://arxiv.org/abs/2311.01329v1</link><description>Offline imitation from observations aims to solve MDPs where onlytask-specific expert states and task-agnostic non-expert state-action pairs areavailable. Offline imitation is useful in real-world scenarios where arbitraryinteractions are costly and expert actions are unavailable. Thestate-of-the-art "DIstribution Correction Estimation" (DICE) methods minimizedivergence of state occupancy between expert and learner policies and retrievea policy with weighted behavior cloning; however, their results are unstablewhen learning from incomplete trajectories, due to a non-robust optimization inthe dual domain. To address the issue, in this paper, we proposeTrajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses adiscounted sum along the future trajectory as the weight for weighted behaviorcloning. The terms for the sum are scaled by the output of a discriminator,which aims to identify expert states. Despite simplicity, TAILO works well ifthere exist trajectories or segments of expert behavior in the task-agnosticdata, a common assumption in prior work. In experiments across multipletestbeds, we find TAILO to be more robust and effective, particularly withincomplete trajectories.</description><author>Kai Yan, Alexander G. Schwing, Yu-Xiong Wang</author><pubDate>Thu, 02 Nov 2023 16:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01329v1</guid></item><item><title>High-dimensional Linear Bandits with Knapsacks</title><link>http://arxiv.org/abs/2311.01327v1</link><description>We study the contextual bandits with knapsack (CBwK) problem under thehigh-dimensional setting where the dimension of the feature is large. Thereward of pulling each arm equals the multiplication of a sparsehigh-dimensional weight vector and the feature of the current arrival, withadditional random noise. In this paper, we investigate how to exploit thissparsity structure to achieve improved regret for the CBwK problem. To thisend, we first develop an online variant of the hard thresholding algorithm thatperforms the sparse estimation in an online manner. We further combine ouronline estimator with a primal-dual framework, where we assign a dual variableto each knapsack constraint and utilize an online learning algorithm to updatethe dual variable, thereby controlling the consumption of the knapsackcapacity. We show that this integrated approach allows us to achieve asublinear regret that depends logarithmically on the feature dimension, thusimproving the polynomial dependency established in the previous literature. Wealso apply our framework to the high-dimension contextual bandit problemwithout the knapsack constraint and achieve optimal regret in both thedata-poor regime and the data-rich regime. We finally conduct numericalexperiments to show the efficient empirical performance of our algorithms underthe high dimensional setting.</description><author>Wanteng Ma, Dong Xia, Jiashuo Jiang</author><pubDate>Thu, 02 Nov 2023 16:40:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01327v1</guid></item><item><title>Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information</title><link>http://arxiv.org/abs/2311.01326v1</link><description>Real-world Knowledge Graphs (KGs) often suffer from incompleteness, whichlimits their potential performance. Knowledge Graph Completion (KGC) techniquesaim to address this issue. However, traditional KGC methods are computationallyintensive and impractical for large-scale KGs, necessitating the learning ofdense node embeddings and computing pairwise distances. Generativetransformer-based language models (e.g., T5 and recent KGT5) offer a promisingsolution as they can predict the tail nodes directly. In this study, we proposeto include node neighborhoods as additional information to improve KGC methodsbased on language models. We examine the effects of this imputation and showthat, on both inductive and transductive Wikidata subsets, our methodoutperforms KGT5 and conventional KGC approaches. We also provide an extensiveanalysis of the impact of neighborhood on model prediction and show itsimportance. Furthermore, we point the way to significantly improve KGC throughmore effective neighborhood selection.</description><author>Alla Chepurova, Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev</author><pubDate>Thu, 02 Nov 2023 16:38:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01326v1</guid></item><item><title>Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly</title><link>http://arxiv.org/abs/2311.01323v1</link><description>The adversarial vulnerability of deep neural networks (DNNs) has drawn greatattention due to the security risk of applying these models in real-worldapplications. Based on transferability of adversarial examples, an increasingnumber of transfer-based methods have been developed to fool black-box DNNmodels whose architecture and parameters are inaccessible. Although tremendouseffort has been exerted, there still lacks a standardized benchmark that couldbe taken advantage of to compare these methods systematically, fairly, andpractically. Our investigation shows that the evaluation of some methods needsto be more reasonable and more thorough to verify their effectiveness, toavoid, for example, unfair comparison and insufficient consideration ofpossible substitute/victim models. Therefore, we establish a transfer-basedattack benchmark (TA-Bench) which implements 30+ methods. In this paper, weevaluate and compare them comprehensively on 25 popular substitute/victimmodels on ImageNet. New insights about the effectiveness of these methods aregained and guidelines for future evaluations are provided. Code at:https://github.com/qizhangli/TA-Bench.</description><author>Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen</author><pubDate>Thu, 02 Nov 2023 16:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01323v1</guid></item><item><title>Deep learning-based Edge-aware pre and post-processing methods for JPEG compressed images</title><link>http://arxiv.org/abs/2104.04926v2</link><description>We propose a learning-based compression scheme that envelopes a standardcodec between pre and post-processing deep CNNs. Specifically, we demonstrateimprovements over prior approaches utilizing a compression-decompressionnetwork by introducing: (a) an edge-aware loss function to prevent blurringthat is commonly occurred in prior works &amp; (b) a super-resolution convolutionalneural network (CNN) for post-processing along with a correspondingpre-processing network for improved rate-distortion performance in the low rateregime. The algorithm is assessed on a variety of datasets varying from low tohigh resolution namely Set 5, Set 7, Classic 5, Set 14, Live 1, Kodak, General100, CLIC 2019. When compared to JPEG, JPEG2000, BPG, and recent CNN approach,the proposed algorithm contributes significant improvement in PSNR with anapproximate gain of 20.75%, 8.47%, 3.22%, 3.23% and 24.59%, 14.46%, 10.14%,8.57% at low and high bit-rates respectively. Similarly, this improvement inMS-SSIM is approximately 71.43%, 50%, 36.36%, 23.08%, 64.70% and 64.47%,61.29%, 47.06%, 51.52%, 16.28% at low and high bit-rates respectively. WithCLIC 2019 dataset, PSNR is found to be superior with approximately 16.67%,10.53%, 6.78%, and 24.62%, 17.39%, 14.08% at low and high bit-ratesrespectively, over JPEG2000, BPG, and recent CNN approach. Similarly, theMS-SSIM is found to be superior with approximately 72%, 45.45%, 39.13%, 18.52%,and 71.43%, 50%, 41.18%, 17.07% at low and high bit-rates respectively,compared to the same approaches. A similar type of improvement is achieved withother datasets also.</description><author>Dipti Mishra, Satish Kumar Singh, Rajat Kumar Singh</author><pubDate>Thu, 02 Nov 2023 16:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.04926v2</guid></item><item><title>Continual atlas-based segmentation of prostate MRI</title><link>http://arxiv.org/abs/2311.00548v2</link><description>Continual learning (CL) methods designed for natural image classificationoften fail to reach basic quality standards for medical image segmentation.Atlas-based segmentation, a well-established approach in medical imaging,incorporates domain knowledge on the region of interest, leading tosemantically coherent predictions. This is especially promising for CL, as itallows us to leverage structural information and strike an optimal balancebetween model rigidity and plasticity over time. When combined withprivacy-preserving prototypes, this process offers the advantages ofrehearsal-based CL without compromising patient privacy. We propose AtlasReplay, an atlas-based segmentation approach that uses prototypes to generatehigh-quality segmentation masks through image registration that maintainconsistency even as the training distribution changes. We explore how ourproposed method performs compared to state-of-the-art CL methods in terms ofknowledge transferability across seven publicly available prostate segmentationdatasets. Prostate segmentation plays a vital role in diagnosing prostatecancer, however, it poses challenges due to substantial anatomical variations,benign structural differences in older age groups, and fluctuating acquisitionparameters. Our results show that Atlas Replay is both robust and generalizeswell to yet-unseen domains while being able to maintain knowledge, unlikeend-to-end segmentation methods. Our code base is available underhttps://github.com/MECLabTUDA/Atlas-Replay.</description><author>Amin Ranem, Camila González, Daniel Pinto dos Santos, Andreas Michael Bucher, Ahmed Ezzat Othman, Anirban Mukhopadhyay</author><pubDate>Thu, 02 Nov 2023 16:30:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00548v2</guid></item><item><title>Lookaround Optimizer: $k$ steps around, 1 step average</title><link>http://arxiv.org/abs/2306.07684v3</link><description>Weight Average (WA) is an active research topic due to its simplicity inensembling deep networks and the effectiveness in promoting generalization.Existing weight average approaches, however, are often carried out along onlyone training trajectory in a post-hoc manner (i.e., the weights are averagedafter the entire training process is finished), which significantly degradesthe diversity between networks and thus impairs the effectiveness. In thispaper, inspired by weight average, we propose Lookaround, a straightforward yeteffective SGD-based optimizer leading to flatter minima with bettergeneralization. Specifically, Lookaround iterates two steps during the wholetraining period: the around step and the average step. In each iteration, 1)the around step starts from a common point and trains multiple networkssimultaneously, each on transformed data by a different data augmentation, and2) the average step averages these trained networks to get the averagednetwork, which serves as the starting point for the next iteration. The aroundstep improves the functionality diversity while the average step guarantees theweight locality of these networks during the whole training, which is essentialfor WA to work. We theoretically explain the superiority of Lookaround byconvergence analysis, and make extensive experiments to evaluate Lookaround onpopular benchmarks including CIFAR and ImageNet with both CNNs and ViTs,demonstrating clear superiority over state-of-the-arts. Our code is availableat https://github.com/Ardcy/Lookaround.</description><author>Jiangtao Zhang, Shunyu Liu, Jie Song, Tongtian Zhu, Zhengqi Xu, Mingli Song</author><pubDate>Thu, 02 Nov 2023 16:24:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07684v3</guid></item><item><title>Scattering Vision Transformer: Spectral Mixing Matters</title><link>http://arxiv.org/abs/2311.01310v1</link><description>Vision transformers have gained significant attention and achievedstate-of-the-art performance in various computer vision tasks, including imageclassification, instance segmentation, and object detection. However,challenges remain in addressing attention complexity and effectively capturingfine-grained information within images. Existing solutions often resort todown-sampling operations, such as pooling, to reduce computational cost.Unfortunately, such operations are non-invertible and can result in informationloss. In this paper, we present a novel approach called Scattering VisionTransformer (SVT) to tackle these challenges. SVT incorporates a spectrallyscattering network that enables the capture of intricate image details. SVTovercomes the invertibility issue associated with down-sampling operations byseparating low-frequency and high-frequency components. Furthermore, SVTintroduces a unique spectral gating network utilizing Einstein multiplicationfor token and channel mixing, effectively reducing complexity. We show that SVTachieves state-of-the-art performance on the ImageNet dataset with asignificant reduction in a number of parameters and FLOPS. SVT shows 2\%improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy,while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-Lreaches 85.7\% (again state-of-art for large versions). SVT also showscomparable results in other vision tasks such as instance segmentation. SVTalso outperforms other transformers in transfer learning on standard datasetssuch as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. Theproject page is available on thiswebpage.\url{https://badripatro.github.io/svt/}.</description><author>Badri N. Patro, Vijay Srinivas Agneeswaran</author><pubDate>Thu, 02 Nov 2023 16:24:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01310v1</guid></item><item><title>Hybrid-Fusion Transformer for Multisequence MRI</title><link>http://arxiv.org/abs/2311.01308v1</link><description>Medical segmentation has grown exponentially through the advent of a fullyconvolutional network (FCN), and we have now reached a turning point throughthe success of Transformer. However, the different characteristics of themodality have not been fully integrated into Transformer for medicalsegmentation. In this work, we propose the novel hybrid fusion Transformer(HFTrans) for multisequence MRI image segmentation. We take advantage of thedifferences among multimodal MRI sequences and utilize the Transformer layersto integrate the features extracted from each modality as well as the featuresof the early fused modalities. We validate the effectiveness of ourhybrid-fusion method in three-dimensional (3D) medical segmentation.Experiments on two public datasets, BraTS2020 and MRBrainS18, show that theproposed method outperforms previous state-of-the-art methods on the task ofbrain tumor segmentation and brain structure segmentation.</description><author>Jihoon Cho, Jinah Park</author><pubDate>Thu, 02 Nov 2023 16:22:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01308v1</guid></item><item><title>Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models</title><link>http://arxiv.org/abs/2305.08776v3</link><description>Foundation models have achieved remarkable results in 2D and language taskslike image segmentation, object detection, and visual-language understanding.However, their potential to enrich 3D scene representation learning is largelyuntapped due to the existence of the domain gap. In this work, we propose aninnovative methodology called Bridge3D to address this gap by pre-training 3Dmodels using features, semantic masks, and captions sourced from foundationmodels. Specifically, our method employs semantic masks from foundation modelsto guide the masking and reconstruction process for the masked autoencoder,enabling more focused attention on foreground representations. Moreover, webridge the 3D-text gap at the scene level using image captioning foundationmodels, thereby facilitating scene-level knowledge distillation. We furtherextend this bridging effort by introducing an innovative object-level knowledgedistillation method that harnesses highly accurate object-level masks andsemantic text data from foundation models. Our methodology significantlysurpasses the performance of existing state-of-the-art methods in 3D objectdetection and semantic segmentation tasks. For instance, on the ScanNetdataset, Bridge3D improves the baseline by a notable margin of 6.3%. Code willbe available at: https://github.com/Zhimin-C/Bridge3D</description><author>Zhimin Chen, Longlong Jing, Yingwei Li, Bing Li</author><pubDate>Thu, 02 Nov 2023 16:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08776v3</guid></item><item><title>The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models</title><link>http://arxiv.org/abs/2311.01307v1</link><description>Large Language Models (LLMs) make natural interfaces to factual knowledge,but their usefulness is limited by their tendency to deliver inconsistentanswers to semantically equivalent questions. For example, a model mightpredict both "Anne Redpath passed away in Edinburgh." and "Anne Redpath's lifeended in London." In this work, we identify potential causes of inconsistencyand evaluate the effectiveness of two mitigation strategies: up-scaling andaugmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlasmodels show that both strategies reduce inconsistency while retrievalaugmentation is considerably more efficient. We further consider anddisentangle the consistency contributions of different components of Atlas. Forall LMs evaluated we find that syntactical form and other evaluation taskartifacts impact consistency. Taken together, our results provide a betterunderstanding of the factors affecting the factual consistency of languagemodels.</description><author>Lovisa Hagström, Denitsa Saynova, Tobias Norlund, Moa Johansson, Richard Johansson</author><pubDate>Thu, 02 Nov 2023 16:20:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01307v1</guid></item><item><title>Improving Adversarial Transferability via Intermediate-level Perturbation Decay</title><link>http://arxiv.org/abs/2304.13410v3</link><description>Intermediate-level attacks that attempt to perturb feature representationsfollowing an adversarial direction drastically have shown favorable performancein crafting transferable adversarial examples. Existing methods in thiscategory are normally formulated with two separate stages, where a directionalguide is required to be determined at first and the scalar projection of theintermediate-level perturbation onto the directional guide is enlargedthereafter. The obtained perturbation deviates from the guide inevitably in thefeature space, and it is revealed in this paper that such a deviation may leadto sub-optimal attack. To address this issue, we develop a novelintermediate-level method that crafts adversarial examples within a singlestage of optimization. In particular, the proposed method, namedintermediate-level perturbation decay (ILPD), encourages the intermediate-levelperturbation to be in an effective adversarial direction and to possess a greatmagnitude simultaneously. In-depth discussion verifies the effectiveness of ourmethod. Experimental results show that it outperforms state-of-the-arts bylarge margins in attacking various victim models on ImageNet (+10.07% onaverage) and CIFAR-10 (+3.88% on average). Our code is athttps://github.com/qizhangli/ILPD-attack.</description><author>Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen</author><pubDate>Thu, 02 Nov 2023 16:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13410v3</guid></item><item><title>GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation</title><link>http://arxiv.org/abs/2303.10056v2</link><description>Text-to-image (T2I) models based on diffusion processes have achievedremarkable success in controllable image generation using user-providedcaptions. However, the tight coupling between the current text encoder andimage decoder in T2I models makes it challenging to replace or upgrade. Suchchanges often require massive fine-tuning or even training from scratch withthe prohibitive expense. To address this problem, we propose GlueGen, whichapplies a newly proposed GlueNet model to align features from single-modal ormulti-modal encoders with the latent space of an existing T2I model. Theapproach introduces a new training objective that leverages parallel corpora toalign the representation spaces of different encoders. Empirical results showthat GlueNet can be trained efficiently and enables various capabilities beyondprevious state-of-the-art models: 1) multilingual language models such asXLM-Roberta can be aligned with existing T2I models, allowing for thegeneration of high-quality images from captions beyond English; 2) GlueNet canalign multi-modal encoders such as AudioCLIP with the Stable Diffusion model,enabling sound-to-image generation; 3) it can also upgrade the current textencoder of the latent diffusion model for challenging case generation. By thealignment of various feature representations, the GlueNet allows for flexibleand efficient integration of new functionality into existing T2I models andsheds light on X-to-image (X2I) generation.</description><author>Can Qin, Ning Yu, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming Xiong, Ran Xu</author><pubDate>Thu, 02 Nov 2023 16:19:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10056v2</guid></item><item><title>AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models</title><link>http://arxiv.org/abs/2311.01305v1</link><description>Large language models(LLMs) exhibit excellent performance across a variety oftasks, but they come with significant computational and storage costs.Quantizing these models is an effective way to alleviate this issue. However,existing methods struggle to strike a balance between model accuracy andhardware efficiency. This is where we introduce AWEQ, a post-training methodthat requires no additional training overhead. AWEQ excels in bothultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.There is an observation that weight quantization is less challenging thanactivation quantization. AWEQ transfers the difficulty of activationquantization to weights using channel equalization, achieving a balance betweenthe quantization difficulties of both, and thereby maximizing performance. Wehave further refined the equalization method to mitigate quantization biaserror, ensuring the robustness of the model. Extensive experiments on popularmodels such as LLaMA and OPT demonstrate that AWEQ outperforms all existingpost-training quantization methods for large models.</description><author>Baisong Li, Xingwang Wang, Haixiao Xu</author><pubDate>Thu, 02 Nov 2023 16:18:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01305v1</guid></item><item><title>TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models</title><link>http://arxiv.org/abs/2311.01301v1</link><description>The rapid digitization of real-world data offers an unprecedented opportunityfor optimizing healthcare delivery and accelerating biomedical discovery. Inpractice, however, such data is most abundantly available in unstructuredforms, such as clinical notes in electronic medical records (EMRs), and it isgenerally plagued by confounders. In this paper, we present TRIALSCOPE, aunifying framework for distilling real-world evidence from population-levelobservational data. TRIALSCOPE leverages biomedical language models tostructure clinical text at scale, employs advanced probabilistic modeling fordenoising and imputation, and incorporates state-of-the-art causal inferencetechniques to combat common confounders. Using clinical trial specification asgeneric representation, TRIALSCOPE provides a turn-key solution to generate andreason with clinical hypotheses using observational data. In extensiveexperiments and analyses on a large-scale real-world dataset with over onemillion cancer patients from a large US healthcare network, we show thatTRIALSCOPE can produce high-quality structuring of real-world data andgenerates comparable results to marquee cancer trials. In addition tofacilitating in-silicon clinical trial design and optimization, TRIALSCOPE maybe used to empower synthetic controls, pragmatic trials, post-marketsurveillance, as well as support fine-grained patient-like-me reasoning inprecision diagnosis and treatment.</description><author>Javier González, Cliff Wong, Zelalem Gero, Jass Bagga, Risa Ueno, Isabel Chien, Eduard Orakvin, Emre Kiciman, Aditya Nori, Roshanthi Weerasinghe, Rom S. Leidner, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon</author><pubDate>Thu, 02 Nov 2023 16:15:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01301v1</guid></item><item><title>DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning</title><link>http://arxiv.org/abs/2311.01295v1</link><description>Data augmentation techniques, such as simple image transformations andcombinations, are highly effective at improving the generalization of computervision models, especially when training data is limited. However, suchtechniques are fundamentally incompatible with differentially private learningapproaches, due to the latter's built-in assumption that each training image'scontribution to the learned model is bounded. In this paper, we investigate whynaive applications of multi-sample data augmentation techniques, such as mixup,fail to achieve good performance and propose two novel data augmentationtechniques specifically designed for the constraints of differentially privatelearning. Our first technique, DP-Mix_Self, achieves SoTA classificationperformance across a range of datasets and settings by performing mixup onself-augmented data. Our second technique, DP-Mix_Diff, further improvesperformance by incorporating synthetic data from a pre-trained diffusion modelinto the mixup process. We open-source the code athttps://github.com/wenxuan-Bao/DP-Mix.</description><author>Wenxuan Bao, Francesco Pittaluga, Vijay Kumar B G, Vincent Bindschaedler</author><pubDate>Thu, 02 Nov 2023 16:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01295v1</guid></item><item><title>Collaborative Learning via Prediction Consensus</title><link>http://arxiv.org/abs/2305.18497v2</link><description>We consider a collaborative learning setting where the goal of each agent isto improve their own model by leveraging the expertise of collaborators, inaddition to their own training data. To facilitate the exchange of expertiseamong agents, we propose a distillation-based method leveraging sharedunlabeled auxiliary data, which is pseudo-labeled by the collective. Central toour method is a trust weighting scheme that serves to adaptively weigh theinfluence of each collaborator on the pseudo-labels until a consensus on how tolabel the auxiliary data is reached. We demonstrate empirically that ourcollaboration scheme is able to significantly boost individual models'performance in the target domain from which the auxiliary data is sampled. Atthe same time, it can provably mitigate the negative impact of bad models onthe collective. By design, our method adeptly accommodates heterogeneity inmodel architectures and substantially reduces communication overhead comparedto typical collaborative learning methods.</description><author>Dongyang Fan, Celestine Mendler-Dünner, Martin Jaggi</author><pubDate>Thu, 02 Nov 2023 16:10:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18497v2</guid></item><item><title>Efficient Spiking Neural Networks with Radix Encoding</title><link>http://arxiv.org/abs/2105.06943v2</link><description>Spiking neural networks (SNNs) have advantages in latency and energyefficiency over traditional artificial neural networks (ANNs) due to itsevent-driven computation mechanism and replacement of energy-consuming weightmultiplications with additions. However, in order to reach accuracy of its ANNcounterpart, it usually requires long spike trains to ensure the accuracy.Traditionally, a spike train needs around one thousand time steps to approachsimilar accuracy as its ANN counterpart. This offsets the computationefficiency brought by SNNs because longer spike trains mean a larger number ofoperations and longer latency. In this paper, we propose a radix encoded SNNwith ultra-short spike trains. In the new model, the spike train takes lessthan ten time steps. Experiments show that our method demonstrates 25X speedupand 1.1% increment on accuracy, compared with the state-of-the-art work onVGG-16 network architecture and CIFAR-10 dataset.</description><author>Zhehui Wang, Xiaozhe Gu, Rick Goh, Joey Tianyi Zhou, Tao Luo</author><pubDate>Thu, 02 Nov 2023 16:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2105.06943v2</guid></item><item><title>Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer</title><link>http://arxiv.org/abs/2207.01964v4</link><description>The increasing capabilities of quantum computing hardware and the challengeof realizing deep quantum circuits require fully automated and efficient toolsfor compiling quantum circuits. To express arbitrary circuits in a sequence ofnative gates specific to the quantum computer architecture, it is necessary tomake algorithms portable across the landscape of quantum hardware providers. Inthis work, we present a compiler capable of transforming and optimizing aquantum circuit targeting a shuttling-based trapped-ion quantum processor. Itconsists of custom algorithms set on top of the quantum circuit frameworkPytket. The performance was evaluated for a wide range of quantum circuits andthe results show that the gate counts can be reduced by factors up to 5.1compared to standard Pytket and up to 2.2 compared to standard Qiskitcompilation.</description><author>Fabian Kreppel, Christian Melzer, Diego Olvera Millán, Janis Wagner, Janine Hilder, Ulrich Poschinger, Ferdinand Schmidt-Kaler, André Brinkmann</author><pubDate>Thu, 02 Nov 2023 16:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.01964v4</guid></item><item><title>Joint 3D Shape and Motion Estimation from Rolling Shutter Light-Field Images</title><link>http://arxiv.org/abs/2311.01292v1</link><description>In this paper, we propose an approach to address the problem of 3Dreconstruction of scenes from a single image captured by a light-field cameraequipped with a rolling shutter sensor. Our method leverages the 3D informationcues present in the light-field and the motion information provided by therolling shutter effect. We present a generic model for the imaging process ofthis sensor and a two-stage algorithm that minimizes the re-projection errorwhile considering the position and motion of the camera in a motion-shapebundle adjustment estimation strategy. Thereby, we provide an instantaneous 3Dshape-and-pose-and-velocity sensing paradigm. To the best of our knowledge,this is the first study to leverage this type of sensor for this purpose. Wealso present a new benchmark dataset composed of different light-fields showingrolling shutter effects, which can be used as a common base to improve theevaluation and tracking the progress in the field. We demonstrate theeffectiveness and advantages of our approach through several experimentsconducted for different scenes and types of motions. The source code anddataset are publicly available at: https://github.com/ICB-Vision-AI/RSLF</description><author>Hermes McGriff, Renato Martins, Nicolas Andreff, Cédric Demonceaux</author><pubDate>Thu, 02 Nov 2023 16:08:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01292v1</guid></item><item><title>Predict-AI-bility of how humans balance self-interest with the interest of others</title><link>http://arxiv.org/abs/2307.12776v2</link><description>Generative artificial intelligence holds enormous potential to revolutionizedecision-making processes, from everyday to high-stake scenarios. However, asmany decisions carry social implications, for AI to be a reliable assistant fordecision-making it is crucial that it is able to capture the balance betweenself-interest and the interest of others. We investigate the ability of threeof the most advanced chatbots to predict dictator game decisions across 108experiments with human participants from 12 countries. We find that only GPT-4(not Bard nor Bing) correctly captures qualitative behavioral patterns,identifying three major classes of behavior: self-interested, inequity-averse,and fully altruistic. Nonetheless, GPT-4 consistently underestimatesself-interest and inequity-aversion, while overestimating altruistic behavior.This bias has significant implications for AI developers and users.</description><author>Valerio Capraro, Roberto Di Paolo, Veronica Pizziol</author><pubDate>Thu, 02 Nov 2023 16:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12776v2</guid></item><item><title>Distilling Knowledge from CNN-Transformer Models for Enhanced Human Action Recognition</title><link>http://arxiv.org/abs/2311.01283v1</link><description>This paper presents a study on improving human action recognition through theutilization of knowledge distillation, and the combination of CNN and ViTmodels. The research aims to enhance the performance and efficiency of smallerstudent models by transferring knowledge from larger teacher models. Theproposed method employs a Transformer vision network as the student model,while a convolutional network serves as the teacher model. The teacher modelextracts local image features, whereas the student model focuses on globalfeatures using an attention mechanism. The Vision Transformer (ViT)architecture is introduced as a robust framework for capturing globaldependencies in images. Additionally, advanced variants of ViT, namely PVT,Convit, MVIT, Swin Transformer, and Twins, are discussed, highlighting theircontributions to computer vision tasks. The ConvNeXt model is introduced as ateacher model, known for its efficiency and effectiveness in computer vision.The paper presents performance results for human action recognition on theStanford 40 dataset, comparing the accuracy and mAP of student models trainedwith and without knowledge distillation. The findings illustrate that thesuggested approach significantly improves the accuracy and mAP when compared totraining networks under regular settings. These findings emphasize thepotential of combining local and global features in action recognition tasks.</description><author>Hamid Ahmadabadi, Omid Nejati Manzari, Ahmad Ayatollahi</author><pubDate>Thu, 02 Nov 2023 15:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01283v1</guid></item><item><title>FlashDecoding++: Faster Large Language Model Inference on GPUs</title><link>http://arxiv.org/abs/2311.01282v1</link><description>As the Large Language Model (LLM) becomes increasingly important in variousdomains. However, the following challenges still remain unsolved inaccelerating LLM inference: (1) Synchronized partial softmax update. Thesoftmax operation requires a synchronized update operation among each partialsoftmax result, leading to ~20% overheads for the attention computation inLLMs. (2) Under-utilized computation of flat GEMM. The shape of matricesperforming GEMM in LLM inference is flat, leading to under-utilized computationand &gt;50% performance loss after padding zeros in previous designs. (3)Performance loss due to static dataflow. Kernel performance in LLM depends onvaried input data features, hardware configurations, etc. A single and staticdataflow may lead to a 50.25% performance loss for GEMMs of different shapes inLLM inference. We present FlashDecoding++, a fast LLM inference engine supporting mainstreamLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++creatively proposes: (1) Asynchronized softmax with unified max value.FlashDecoding++ introduces a unified max value technique for different partialsoftmax computations to avoid synchronization. (2) Flat GEMM optimization withdouble buffering. FlashDecoding++ points out that flat GEMMs with differentshapes face varied bottlenecks. Then, techniques like double buffering areintroduced. (3) Heuristic dataflow with hardware resource adaptation.FlashDecoding++ heuristically optimizes dataflow using different hardwareresource considering input dynamics. Due to the versatility of optimizations inFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup onboth NVIDIA and AMD GPUs compared to Hugging Face implementations.FlashDecoding++ also achieves an average speedup of 1.37x compared tostate-of-the-art LLM inference engines on mainstream LLMs.</description><author>Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, Yu Wang</author><pubDate>Thu, 02 Nov 2023 15:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01282v1</guid></item><item><title>Long Sequence Hopfield Memory</title><link>http://arxiv.org/abs/2306.04532v2</link><description>Sequence memory is an essential attribute of natural and artificialintelligence that enables agents to encode, store, and retrieve complexsequences of stimuli and actions. Computational models of sequence memory havebeen proposed where recurrent Hopfield-like neural networks are trained withtemporally asymmetric Hebbian rules. However, these networks suffer fromlimited sequence capacity (maximal length of the stored sequence) due tointerference between the memories. Inspired by recent work on Dense AssociativeMemories, we expand the sequence capacity of these models by introducing anonlinear interaction term, enhancing separation between the patterns. Wederive novel scaling laws for sequence capacity with respect to network size,significantly outperforming existing scaling laws for models based ontraditional Hopfield networks, and verify these theoretical results withnumerical simulation. Moreover, we introduce a generalized pseudoinverse ruleto recall sequences of highly correlated patterns. Finally, we extend thismodel to store sequences with variable timing between states' transitions anddescribe a biologically-plausible implementation, with connections to motorneuroscience.</description><author>Hamza Tahir Chaudhry, Jacob A. Zavatone-Veth, Dmitry Krotov, Cengiz Pehlevan</author><pubDate>Thu, 02 Nov 2023 15:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04532v2</guid></item><item><title>Long-Range Neural Atom Learning for Molecular Graphs</title><link>http://arxiv.org/abs/2311.01276v1</link><description>Graph Neural Networks (GNNs) have been widely adopted for drug discovery withmolecular graphs. Nevertheless, current GNNs are mainly good at leveragingshort-range interactions (SRI) but struggle to capture long-range interactions(LRI), both of which are crucial for determining molecular properties. Totackle this issue, we propose a method that implicitly projects all originalatoms into a few Neural Atoms, which abstracts the collective information ofatomic groups within a molecule. Specifically, we explicitly exchange theinformation among neural atoms and project them back to the atoms'representations as an enhancement. With this mechanism, neural atoms establishthe communication channels among distant nodes, effectively reducing theinteraction scope of arbitrary node pairs into a single hop. To provide aninspection of our method from a physical perspective, we reveal its connectionwith the traditional LRI calculation method, Ewald Summation. We conductextensive experiments on three long-range graph benchmarks, covering bothgraph-level and link-level tasks on molecular graphs. We empirically justifythat our method can be equipped with an arbitrary GNN and help to capture LRI.</description><author>Xuan Li, Zhanke Zhou, Jiangchao Yao, Yu Rong, Lu Zhang, Bo Han</author><pubDate>Thu, 02 Nov 2023 15:44:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01276v1</guid></item><item><title>Sample-efficient Multi-objective Molecular Optimization with GFlowNets</title><link>http://arxiv.org/abs/2302.04040v2</link><description>Many crucial scientific problems involve designing novel molecules withdesired properties, which can be formulated as a black-box optimization problemover the discrete chemical space. In practice, multiple conflicting objectivesand costly evaluations (e.g., wet-lab experiments) make the diversity ofcandidates paramount. Computational methods have achieved initial success butstill struggle with considering diversity in both objective and search space.To fill this gap, we propose a multi-objective Bayesian optimization (MOBO)algorithm leveraging the hypernetwork-based GFlowNets (HN-GFN) as anacquisition function optimizer, with the purpose of sampling a diverse batch ofcandidate molecular graphs from an approximate Pareto front. Using a singlepreference-conditioned hypernetwork, HN-GFN learns to explore varioustrade-offs between objectives. We further propose a hindsight-like off-policystrategy to share high-performing molecules among different preferences inorder to speed up learning for HN-GFN. We empirically illustrate that HN-GFNhas adequate capacity to generalize over preferences. Moreover, experiments invarious real-world MOBO settings demonstrate that our framework predominantlyoutperforms existing methods in terms of candidate quality and sampleefficiency. The code is available at https://github.com/violet-sto/HN-GFN.</description><author>Yiheng Zhu, Jialu Wu, Chaowen Hu, Jiahuan Yan, Chang-Yu Hsieh, Tingjun Hou, Jian Wu</author><pubDate>Thu, 02 Nov 2023 15:44:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04040v2</guid></item><item><title>Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations</title><link>http://arxiv.org/abs/2311.01273v1</link><description>When we communicate with other humans, we do not simply generate a sequenceof words. Rather, we use our cognitive state (beliefs, desires, intentions) andour model of the audience's cognitive state to create utterances that affectthe audience's cognitive state in the intended manner. An important part ofcognitive state is the common ground, which is the content the speakerbelieves, and the speaker believes the audience believes, and so on. While muchattention has been paid to common ground in cognitive science, there has notbeen much work in natural language processing. In this paper, we introduce anew annotation and corpus to capture common ground. We then describe someinitial experiments extracting propositions from dialog and tracking theirstatus in the common ground from the perspective of each speaker.</description><author>Magdalena Markowska, Mohammad Taghizadeh, Adil Soubki, Seyed Abolghasem Mirroshandel, Owen Rambow</author><pubDate>Thu, 02 Nov 2023 15:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01273v1</guid></item><item><title>Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning</title><link>http://arxiv.org/abs/2305.19913v2</link><description>Recently, operator learning, or learning mappings betweeninfinite-dimensional function spaces, has garnered significant attention,notably in relation to learning partial differential equations from data.Conceptually clear when outlined on paper, neural operators necessitatediscretization in the transition to computer implementations. This step cancompromise their integrity, often causing them to deviate from the underlyingoperators. This research offers a fresh take on neural operators with aframework Representation equivalent Neural Operators (ReNO) designed to addressthese issues. At its core is the concept of operator aliasing, which measuresinconsistency between neural operators and their discrete representations. Weexplore this for widely-used operator learning techniques. Our findings detailhow aliasing introduces errors when handling different discretizations andgrids and loss of crucial continuous structures. More generally, this frameworknot only sheds light on existing challenges but, given its constructive andbroad nature, also potentially offers tools for developing new neuraloperators.</description><author>Francesca Bartolucci, Emmanuel de Bézenac, Bogdan Raonić, Roberto Molinaro, Siddhartha Mishra, Rima Alaifari</author><pubDate>Thu, 02 Nov 2023 15:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19913v2</guid></item><item><title>People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection</title><link>http://arxiv.org/abs/2311.01270v1</link><description>NLP models are used in a variety of critical social computing tasks, such asdetecting sexist, racist, or otherwise hateful content. Therefore, it isimperative that these models are robust to spurious features. Past work hasattempted to tackle such spurious features using training data augmentation,including Counterfactually Augmented Data (CADs). CADs introduce minimalchanges to existing training data points and flip their labels; training onthem may reduce model dependency on spurious features. However, manuallygenerating CADs can be time-consuming and expensive. Hence in this work, weassess if this task can be automated using generative NLP models. Weautomatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluatetheir usefulness in improving model robustness compared to manually-generatedCADs. By testing both model performance on multiple out-of-domain test sets andindividual data point efficacy, our results show that while manual CADs arestill the most effective, CADs generated by ChatGPT come a close second. Onekey reason for the lower performance of automated methods is that the changesthey introduce are often insufficient to flip the original label.</description><author>Indira Sen, Dennis Assenmacher, Mattia Samory, Isabelle Augenstein, Wil van der Aalst, Claudia Wagne</author><pubDate>Thu, 02 Nov 2023 15:31:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01270v1</guid></item><item><title>QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language Models</title><link>http://arxiv.org/abs/2310.09259v2</link><description>Large Language Models (LLMs) from the GPT family have become extremelypopular, leading to a race towards reducing their inference costs to allow forefficient local computation. Yet, the vast majority of existing work focuses onweight-only quantization, which can reduce runtime costs in the memory-boundone-token-at-a-time generative setting, but does not address them incompute-bound scenarios, such as batched inference or prompt processing. Inthis paper, we address the general quantization problem, where both weights andactivations should be quantized. We show, for the first time, that the majorityof inference computations for large generative models such as LLaMA, OPT, andFalcon can be performed with both weights and activations being cast to 4 bits,in a way that leads to practical speedups, while at the same time maintaininggood accuracy. We achieve this via a hybrid quantization strategy called QUIK,which compresses most of the weights and activations to 4-bit, while keepingsome outlier weights and activations in higher-precision. The key feature ofour scheme is that it is designed with computational efficiency in mind: weprovide GPU kernels matching the QUIK format with highly-efficient layer-wiseruntimes, which lead to practical end-to-end throughput improvements of up to3.4x relative to FP16 execution. We provide detailed studies for models fromthe OPT, LLaMA-2 and Falcon families, as well as a first instance of accurateinference using quantization plus 2:4 sparsity. Code is available at:https://github.com/IST-DASLab/QUIK.</description><author>Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, Dan Alistarh</author><pubDate>Thu, 02 Nov 2023 15:26:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09259v2</guid></item><item><title>UniFolding: Towards Sample-efficient, Scalable, and Generalizable Robotic Garment Folding</title><link>http://arxiv.org/abs/2311.01267v1</link><description>This paper explores the development of UniFolding, a sample-efficient,scalable, and generalizable robotic system for unfolding and folding variousgarments. UniFolding employs the proposed UFONet neural network to integrateunfolding and folding decisions into a single policy model that is adaptable todifferent garment types and states. The design of UniFolding is based on agarment's partial point cloud, which aids in generalization and reducessensitivity to variations in texture and shape. The training pipelineprioritizes low-cost, sample-efficient data collection. Training data iscollected via a human-centric process with offline and online stages. Theoffline stage involves human unfolding and folding actions via Virtual Reality,while the online stage utilizes human-in-the-loop learning to fine-tune themodel in a real-world setting. The system is tested on two garment types:long-sleeve and short-sleeve shirts. Performance is evaluated on 20 shirts withsignificant variations in textures, shapes, and materials. More experiments andvideos can be found in the supplementary materials and on the website:https://unifolding.robotflow.ai</description><author>Han Xue, Yutong Li, Wenqiang Xu, Huanyu Li, Dongzhe Zheng, Cewu Lu</author><pubDate>Thu, 02 Nov 2023 15:25:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01267v1</guid></item></channel></rss>