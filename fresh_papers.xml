<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 30 Jun 2023 06:00:25 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>An Efficient General-Purpose Modular Vision Model via Multi-Task Heterogeneous Training</title><link>http://arxiv.org/abs/2306.17165v1</link><description>We present a model that can perform multiple vision tasks and can be adaptedto other downstream tasks efficiently. Despite considerable progress inmulti-task learning, most efforts focus on learning from multi-label data: asingle image set with multiple task labels. Such multi-label data sets arerare, small, and expensive. We say heterogeneous to refer to image sets withdifferent task labels, or to combinations of single-task datasets. Few haveexplored training on such heterogeneous datasets. General-purpose vision modelsare still dominated by single-task pretraining, and it remains unclear how toscale up multi-task models by leveraging mainstream vision datasets designedfor different purposes. The challenges lie in managing large intrinsicdifferences among vision tasks, including data distribution, architectures,task-specific modules, dataset scales, and sampling strategies. To addressthese challenges, we propose to modify and scale up mixture-of-experts (MoE)vision transformers, so that they can simultaneously learn classification,detection, and segmentation on diverse mainstream vision datasets includingImageNet, COCO, and ADE20K. Our approach achieves comparable results tosingle-task state-of-the-art models and demonstrates strong generalization ondownstream tasks. Due to its emergent modularity, this general-purpose modeldecomposes into high-performing components, efficiently adapting to downstreamtasks. We can fine-tune it with fewer training parameters, fewer modelparameters, and less computation. Additionally, its modularity allows for easyexpansion in continual-learning-without-forgetting scenarios. Finally, thesefunctions can be controlled and combined to meet various demands of downstreamtasks.</description><author>Zitian Chen, Mingyu Ding, Yikang Shen, Wei Zhan, Masayoshi Tomizuka, Erik Learned-Miller, Chuang Gan</author><pubDate>Thu, 29 Jun 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17165v1</guid></item><item><title>Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors</title><link>http://arxiv.org/abs/2306.17156v1</link><description>Generative AI and large language models hold great promise in enhancingcomputing education by powering next-generation educational technologies forintroductory programming. Recent works have studied these models for differentscenarios relevant to programming education; however, these works are limitedfor several reasons, as they typically consider already outdated models or onlyspecific scenario(s). Consequently, there is a lack of a systematic study thatbenchmarks state-of-the-art models for a comprehensive set of programmingeducation scenarios. In our work, we systematically evaluate two models,ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with humantutors for a variety of scenarios. We evaluate using five introductory Pythonprogramming problems and real-world buggy programs from an online platform, andassess performance using expert-based annotations. Our results show that GPT-4drastically outperforms ChatGPT (based on GPT-3.5) and comes close to humantutors' performance for several scenarios. These results also highlightsettings where GPT-4 still struggles, providing exciting future directions ondeveloping techniques to improve the performance of these models.</description><author>Tung Phung, Victor-Alexandru Pădurean, José Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, Gustavo Soares</author><pubDate>Thu, 29 Jun 2023 18:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17156v1</guid></item><item><title>Generate Anything Anywhere in Any Scene</title><link>http://arxiv.org/abs/2306.17154v1</link><description>Text-to-image diffusion models have attracted considerable interest due totheir wide applicability across diverse fields. However, challenges persist increating controllable models for personalized object generation. In this paper,we first identify the entanglement issues in existing personalized generativemodels, and then propose a straightforward and efficient data augmentationtraining strategy that guides the diffusion model to focus solely on objectidentity. By inserting the plug-and-play adapter layers from a pre-trainedcontrollable diffusion model, our model obtains the ability to control thelocation and size of each generated personalized object. During inference, wepropose a regionally-guided sampling technique to maintain the quality andfidelity of the generated images. Our method achieves comparable or superiorfidelity for personalized objects, yielding a robust, versatile, andcontrollable text-to-image diffusion model that is capable of generatingrealistic and personalized images. Our approach demonstrates significantpotential for various applications, such as those in art, entertainment, andadvertising design.</description><author>Yuheng Li, Haotian Liu, Yangming Wen, Yong Jae Lee</author><pubDate>Thu, 29 Jun 2023 18:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17154v1</guid></item><item><title>On Computational Mechanisms for Shared Intentionality, and Speculation on Rationality and Consciousness</title><link>http://arxiv.org/abs/2306.13657v2</link><description>A singular attribute of humankind is our ability to undertake novel,cooperative behavior, or teamwork. This requires that we can communicate goals,plans, and ideas between the brains of individuals to create sharedintentionality. Using the information processing model of David Marr, I derivenecessary characteristics of basic mechanisms to enable shared intentionalitybetween prelinguistic computational agents and indicate how these could beimplemented in present-day AI-based robots. More speculatively, I suggest the mechanisms derived by this thoughtexperiment apply to humans and extend to provide explanations for humanrationality and aspects of intentional and phenomenal consciousness that accordwith observation. This yields what I call the Shared Intentionality FirstTheory (SIFT) for rationality and consciousness. The significance of shared intentionality has been recognized and advocatedpreviously, but typically from a sociological or behavioral point of view. SIFTcomplements prior work by applying a computer science perspective to theunderlying mechanisms.</description><author>John Rushby</author><pubDate>Thu, 29 Jun 2023 18:54:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13657v2</guid></item><item><title>Local Risk Bounds for Statistical Aggregation</title><link>http://arxiv.org/abs/2306.17151v1</link><description>In the problem of aggregation, the aim is to combine a given class of basepredictors to achieve predictions nearly as accurate as the best one. In thisflexible framework, no assumption is made on the structure of the class or thenature of the target. Aggregation has been studied in both sequential andstatistical contexts. Despite some important differences between the twoproblems, the classical results in both cases feature the same globalcomplexity measure. In this paper, we revisit and tighten classical results inthe theory of aggregation in the statistical setting by replacing the globalcomplexity with a smaller, local one. Some of our proofs build on the PAC-Bayeslocalization technique introduced by Catoni. Among other results, we provelocalized versions of the classical bound for the exponential weights estimatordue to Leung and Barron and deviation-optimal bounds for the Q-aggregationestimator. These bounds improve over the results of Dai, Rigollet and Zhang forfixed design regression and the results of Lecu\'e and Rigollet for randomdesign regression.</description><author>Jaouad Mourtada, Tomas Vaškevičius, Nikita Zhivotovskiy</author><pubDate>Thu, 29 Jun 2023 18:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17151v1</guid></item><item><title>KDEformer: Accelerating Transformers via Kernel Density Estimation</title><link>http://arxiv.org/abs/2302.02451v2</link><description>Dot-product attention mechanism plays a crucial role in modern deeparchitectures (e.g., Transformer) for sequence modeling, however, na\"ive exactcomputation of this model incurs quadratic time and memory complexities insequence length, hindering the training of long-sequence models. Criticalbottlenecks are due to the computation of partition functions in thedenominator of softmax function as well as the multiplication of the softmaxmatrix with the matrix of values. Our key observation is that the former can bereduced to a variant of the kernel density estimation (KDE) problem, and anefficient KDE solver can be further utilized to accelerate the latter viasubsampling-based fast matrix products. Our proposed KDEformer can approximatethe attention in sub-quadratic time with provable spectral norm bounds, whileall prior results merely provide entry-wise error bounds. Empirically, weverify that KDEformer outperforms other attention approximations in terms ofaccuracy, memory, and runtime on various pre-trained models. On BigGAN imagegeneration, we achieve better generative scores than the exact computation withover $4\times$ speedup. For ImageNet classification with T2T-ViT, KDEformershows over $18\times$ speedup while the accuracy drop is less than $0.5\%$.</description><author>Amir Zandieh, Insu Han, Majid Daliri, Amin Karbasi</author><pubDate>Thu, 29 Jun 2023 18:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02451v2</guid></item><item><title>Filtered-Guided Diffusion: Fast Filter Guidance for Black-Box Diffusion Models</title><link>http://arxiv.org/abs/2306.17141v1</link><description>Recent advances in diffusion-based generative models have shown incrediblepromise for Image-to-Image translation and editing. Most recent work in thisspace relies on additional training or architecture-specific adjustments to thediffusion process. In this work, we show that much of this low-level controlcan be achieved without additional training or any access to features of thediffusion model. Our method simply applies a filter to the input of eachdiffusion step based on the output of the previous step in an adaptive manner.Notably, this approach does not depend on any specific architecture or samplerand can be done without access to internal features of the network, making iteasy to combine with other techniques, samplers, and diffusion architectures.Furthermore, it has negligible cost to performance, and allows for morecontinuous adjustment of guidance strength than other approaches. We show FGDoffers a fast and strong baseline that is competitive with recentarchitecture-dependent approaches. Furthermore, FGD can also be used as asimple add-on to enhance the structural guidance of other state-of-the-art I2Imethods. Finally, our derivation of this method helps to understand the impactof self attention, a key component of other recent architecture-specific I2Iapproaches, in a more architecture-independent way. Project page:https://github.com/jaclyngu/FilteredGuidedDiffusion</description><author>Zeqi Gu, Abe Davis</author><pubDate>Thu, 29 Jun 2023 18:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17141v1</guid></item><item><title>Measured Albedo in the Wild: Filling the Gap in Intrinsics Evaluation</title><link>http://arxiv.org/abs/2306.15662v2</link><description>Intrinsic image decomposition and inverse rendering are long-standingproblems in computer vision. To evaluate albedo recovery, most algorithmsreport their quantitative performance with a mean Weighted Human DisagreementRate (WHDR) metric on the IIW dataset. However, WHDR focuses only on relativealbedo values and often fails to capture overall quality of the albedo. Inorder to comprehensively evaluate albedo, we collect a new dataset, MeasuredAlbedo in the Wild (MAW), and propose three new metrics that complement WHDR:intensity, chromaticity and texture metrics. We show that existing algorithmsoften improve WHDR metric but perform poorly on other metrics. We then finetunedifferent algorithms on our MAW dataset to significantly improve the quality ofthe reconstructed albedo both quantitatively and qualitatively. Since theproposed intensity, chromaticity, and texture metrics and the WHDR are allcomplementary we further introduce a relative performance measure that capturesaverage performance. By analysing existing algorithms we show that there issignificant room for improvement. Our dataset and evaluation metrics willenable researchers to develop algorithms that improve albedo reconstruction.Code and Data available at: https://measuredalbedo.github.io/</description><author>Jiaye Wu, Sanjoy Chowdhury, Hariharmano Shanmugaraja, David Jacobs, Soumyadip Sengupta</author><pubDate>Thu, 29 Jun 2023 18:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15662v2</guid></item><item><title>ID-Pose: Sparse-view Camera Pose Estimation by Inverting Diffusion Models</title><link>http://arxiv.org/abs/2306.17140v1</link><description>Given sparse views of an object, estimating their camera poses is along-standing and intractable problem. We harness the pre-trained diffusionmodel of novel views conditioned on viewpoints (Zero-1-to-3). We presentID-Pose which inverses the denoising diffusion process to estimate the relativepose given two input images. ID-Pose adds a noise on one image, and predictsthe noise conditioned on the other image and a decision variable for the pose.The prediction error is used as the objective to find the optimal pose with thegradient descent method. ID-Pose can handle more than two images and estimateeach of the poses with multiple image pairs from triangular relationships.ID-Pose requires no training and generalizes to real-world images. We conductexperiments using high-quality real-scanned 3D objects, where ID-Posesignificantly outperforms state-of-the-art methods.</description><author>Weihao Cheng, Yan-Pei Cao, Ying Shan</author><pubDate>Thu, 29 Jun 2023 18:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17140v1</guid></item><item><title>Simulation of Human and Artificial Emotion (SHArE)</title><link>http://arxiv.org/abs/2011.02151v2</link><description>The framework for Simulation of Human and Artificial Emotion (SHArE)describes the architecture of emotion in terms of parameters transferablebetween psychology, neuroscience, and artificial intelligence. These parameterscan be defined as abstract concepts or granularized down to the voltage levelsof individual neurons. This model enables emotional trajectory design forhumans which may lead to novel therapeutic solutions for various mental healthconcerns. For artificial intelligence, this work provides a compact notationwhich can be applied to neural networks as a means to observe the emotions andmotivations of machines.</description><author>Kwadwo Opong-Mensah</author><pubDate>Thu, 29 Jun 2023 18:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.02151v2</guid></item><item><title>AutoML in Heavily Constrained Applications</title><link>http://arxiv.org/abs/2306.16913v1</link><description>Optimizing a machine learning pipeline for a task at hand requires carefulconfiguration of various hyperparameters, typically supported by an AutoMLsystem that optimizes the hyperparameters for the given training dataset. Yet,depending on the AutoML system's own second-order meta-configuration, theperformance of the AutoML process can vary significantly. Current AutoMLsystems cannot automatically adapt their own configuration to a specific usecase. Further, they cannot compile user-defined application constraints on theeffectiveness and efficiency of the pipeline and its generation. In this paper,we propose Caml, which uses meta-learning to automatically adapt its own AutoMLparameters, such as the search strategy, the validation strategy, and thesearch space, for a task at hand. The dynamic AutoML strategy of Caml takesuser-defined constraints into account and obtains constraint-satisfyingpipelines with high predictive performance.</description><author>Felix Neutatz, Marius Lindauer, Ziawasch Abedjan</author><pubDate>Thu, 29 Jun 2023 14:05:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16913v1</guid></item><item><title>"That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks</title><link>http://arxiv.org/abs/2204.04636v2</link><description>Adversarial attacks are a major challenge faced by current machine learningresearch. These purposely crafted inputs fool even the most advanced models,precluding their deployment in safety-critical applications. Extensive researchin computer vision has been carried to develop reliable defense strategies.However, the same issue remains less explored in natural language processing.Our work presents a model-agnostic detector of adversarial text examples. Theapproach identifies patterns in the logits of the target classifier whenperturbing the input text. The proposed detector improves the currentstate-of-the-art performance in recognizing adversarial inputs and exhibitsstrong generalization capabilities across different NLP models, datasets, andword-level attacks.</description><author>Edoardo Mosca, Shreyash Agarwal, Javier Rando, Georg Groh</author><pubDate>Thu, 29 Jun 2023 14:02:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.04636v2</guid></item><item><title>Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models</title><link>http://arxiv.org/abs/2304.07396v2</link><description>Physicians considering clinical trials for their patients are met with thelaborious process of checking many text based eligibility criteria. LargeLanguage Models (LLMs) have shown to perform well for clinical informationextraction and clinical reasoning, including medical tests, but not yet inreal-world scenarios. This paper investigates the use of InstructGPT to assistphysicians in determining eligibility for clinical trials based on a patient'ssummarised medical profile. Using a prompting strategy combining one-shot,selection-inference and chain-of-thought techniques, we investigate theperformance of LLMs on 10 synthetically created patient profiles. Performanceis evaluated at four levels: ability to identify screenable eligibilitycriteria from a trial given a medical profile; ability to classify for eachindividual criterion whether the patient qualifies; the overall classificationwhether a patient is eligible for a clinical trial and the percentage ofcriteria to be screened by physician. We evaluated against 146 clinical trialsand a total of 4,135 eligibility criteria. The LLM was able to correctlyidentify the screenability of 72% (2,994/4,135) of the criteria. Additionally,72% (341/471) of the screenable criteria were evaluated correctly. Theresulting trial level classification as eligible or ineligible resulted in arecall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0and precision of 0.71 on clinical trial level can be achieved while reducingthe amount of criteria to be checked by an estimated 90%. LLMs can be used toassist physicians with pre-screening of patients for clinical trials. Byforcing instruction-tuned LLMs to produce chain-of-thought responses, thereasoning can be made transparent to and the decision process becomes amenableby physicians, thereby making such a system feasible for use in real-worldscenarios.</description><author>Danny M. den Hamer, Perry Schoor, Tobias B. Polak, Daniel Kapitan</author><pubDate>Thu, 29 Jun 2023 13:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07396v2</guid></item><item><title>Numerical Data Imputation for Multimodal Data Sets: A Probabilistic Nearest-Neighbor Kernel Density Approach</title><link>http://arxiv.org/abs/2306.16906v1</link><description>Numerical data imputation algorithms replace missing values by estimates toleverage incomplete data sets. Current imputation methods seek to minimize theerror between the unobserved ground truth and the imputed values. But thisstrategy can create artifacts leading to poor imputation in the presence ofmultimodal or complex distributions. To tackle this problem, we introduce the$k$NN$\times$KDE algorithm: a data imputation method combining nearest neighborestimation ($k$NN) and density estimation with Gaussian kernels (KDE). Wecompare our method with previous data imputation methods using artificial andreal-world data with different data missing scenarios and various data missingrates, and show that our method can cope with complex original data structure,yields lower data imputation errors, and provides probabilistic estimates withhigher likelihood than current methods. We release the code in open-source forthe community: https://github.com/DeltaFloflo/knnxkde</description><author>Floria Lalande, Kenji Doya</author><pubDate>Thu, 29 Jun 2023 13:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16906v1</guid></item><item><title>TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction</title><link>http://arxiv.org/abs/2306.14406v2</link><description>When deep neural network has been proposed to assist the dentist in designingthe location of dental implant, most of them are targeting simple cases whereonly one missing tooth is available. As a result, literature works do not workwell when there are multiple missing teeth and easily generate falsepredictions when the teeth are sparsely distributed. In this paper, we aretrying to integrate a weak supervision text, the target region, to the implantposition regression network, to address above issues. We propose a textcondition embedded implant position regression network (TCEIP), to embed thetext condition into the encoder-decoder framework for improvement of theregression performance. A cross-modal interaction that consists of cross-modalattention (CMA) and knowledge alignment module (KAM) is proposed to facilitatethe interaction between features of images and texts. The CMA module performs across-attention between the image feature and the text condition, and the KAMmitigates the knowledge gap between the image feature and the image encoder ofthe CLIP. Extensive experiments on a dental implant dataset through five-foldcross-validation demonstrated that the proposed TCEIP achieves superiorperformance than existing methods.</description><author>Xinquan Yang, Jinheng Xie, Xuguang Li, Xuechen Li, Xin Li, Linlin Shen, Yongqiang Deng</author><pubDate>Thu, 29 Jun 2023 13:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14406v2</guid></item><item><title>A geometrically aware auto-encoder for multi-texture synthesis</title><link>http://arxiv.org/abs/2302.01616v3</link><description>We propose an auto-encoder architecture for multi-texture synthesis. Theapproach relies on both a compact encoder accounting for second order neuralstatistics and a generator incorporating adaptive periodic content. Images areembedded in a compact and geometrically consistent latent space, where thetexture representation and its spatial organisation are disentangled. Texturesynthesis and interpolation tasks can be performed directly from these latentcodes. Our experiments demonstrate that our model outperforms state-of-the-artfeed-forward methods in terms of visual quality and various texture relatedmetrics.</description><author>Pierrick Chatillon, Yann Gousseau, Sidonie Lefebvre</author><pubDate>Thu, 29 Jun 2023 13:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01616v3</guid></item><item><title>Leveraging Cross-Utterance Context For ASR Decoding</title><link>http://arxiv.org/abs/2306.16903v1</link><description>While external language models (LMs) are often incorporated into the decodingstage of automated speech recognition systems, these models usually operatewith limited context. Cross utterance information has been shown to bebeneficial during second pass re-scoring, however this limits the hypothesisspace based on the local information available to the first pass LM. In thiswork, we investigate the incorporation of long-context transformer LMs forcross-utterance decoding of acoustic models via beam search, and compareagainst results from n-best rescoring. Results demonstrate that beam searchallows for an improved use of cross-utterance context. When evaluating on thelong-format dataset AMI, results show a 0.7\% and 0.3\% absolute reduction ondev and test sets compared to the single-utterance setting, with improvementswhen including up to 500 tokens of prior context. Evaluations are also providedfor Tedlium-1 with less significant improvements of around 0.1\% absolute.</description><author>Robert Flynn, Anton Ragni</author><pubDate>Thu, 29 Jun 2023 13:48:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16903v1</guid></item><item><title>From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data</title><link>http://arxiv.org/abs/2306.16902v1</link><description>Large Language Models (LLMs) exhibit exceptional abilities for causalanalysis between concepts in numerous societally impactful domains, includingmedicine, science, and law. Recent research on LLM performance in variouscausal discovery and inference tasks has given rise to a new ladder in theclassical three-stage framework of causality. In this paper, we advance thecurrent research of LLM-driven causal discovery by proposing a novel frameworkthat combines knowledge-based LLM causal analysis with data-driven causalstructure learning. To make LLM more than a query tool and to leverage itspower in discovering natural and new laws of causality, we integrate thevaluable LLM expertise on existing causal mechanisms into statistical analysisof objective data to build a novel and practical baseline for causal structurelearning. We introduce a universal set of prompts designed to extract causal graphsfrom given variables and assess the influence of LLM prior causality onrecovering causal structures from data. We demonstrate the significantenhancement of LLM expertise on the quality of recovered causal structures fromdata, while also identifying critical challenges and issues, along withpotential approaches to address them. As a pioneering study, this paper aims toemphasize the new frontier that LLMs are opening for classical causal discoveryand inference, and to encourage the widespread adoption of LLM capabilities indata-driven causal analysis.</description><author>Taiyu Ban, Lyvzhou Chen, Xiangyu Wang, Huanhuan Chen</author><pubDate>Thu, 29 Jun 2023 13:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16902v1</guid></item><item><title>Transformers Meet Directed Graphs</title><link>http://arxiv.org/abs/2302.00049v2</link><description>Transformers were originally proposed as a sequence-to-sequence model fortext but have become vital for a wide range of modalities, including images,audio, video, and undirected graphs. However, transformers for directed graphsare a surprisingly underexplored topic, despite their applicability toubiquitous domains, including source code and logic circuits. In this work, wepropose two direction- and structure-aware positional encodings for directedgraphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-awaregeneralization of the combinatorial Laplacian; (2) directional random walkencodings. Empirically, we show that the extra directionality information isuseful in various downstream tasks, including correctness testing of sortingnetworks and source code understanding. Together with a data-flow-centric graphconstruction, our model outperforms the prior state of the art on the OpenGraph Benchmark Code2 relatively by 14.7%.</description><author>Simon Geisler, Yujia Li, Daniel Mankowitz, Ali Taylan Cemgil, Stephan Günnemann, Cosmin Paduraru</author><pubDate>Thu, 29 Jun 2023 13:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00049v2</guid></item><item><title>Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research</title><link>http://arxiv.org/abs/2306.16900v1</link><description>Many recent improvements in NLP stem from the development and use of largepre-trained language models (PLMs) with billions of parameters. Large modelsizes makes computational cost one of the main limiting factors for trainingand evaluating such models; and has raised severe concerns about thesustainability, reproducibility, and inclusiveness for researching PLMs. Theseconcerns are often based on personal experiences and observations. However,there had not been any large-scale surveys that investigate them. In this work,we provide a first attempt to quantify these concerns regarding three topics,namely, environmental impact, equity, and impact on peer reviewing. Byconducting a survey with 312 participants from the NLP community, we captureexisting (dis)parities between different and within groups with respect toseniority, academia, and industry; and their impact on the peer reviewingprocess. For each topic, we provide an analysis and devise recommendations tomitigate found disparities, some of which already successfully implemented.Finally, we discuss additional concerns raised by many participants infree-text responses.</description><author>Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, Jessica Zosa Forde, Leon Derczynski, Andreas Rücklé, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge</author><pubDate>Thu, 29 Jun 2023 13:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16900v1</guid></item><item><title>A Comparison of Reinforcement Learning Frameworks for Software Testing Tasks</title><link>http://arxiv.org/abs/2208.12136v3</link><description>Software testing activities scrutinize the artifacts and the behavior of asoftware product to find possible defects and ensure that the product meets itsexpected requirements. Recently, Deep Reinforcement Learning (DRL) has beensuccessfully employed in complex testing tasks such as game testing, regressiontesting, and test case prioritization to automate the process and providecontinuous adaptation. Practitioners can employ DRL by implementing fromscratch a DRL algorithm or using a DRL framework. DRL frameworks offerwell-maintained implemented state-of-the-art DRL algorithms to facilitate andspeed up the development of DRL applications. Developers have widely used theseframeworks to solve problems in various domains including software testing.However, to the best of our knowledge, there is no study that empiricallyevaluates the effectiveness and performance of implemented algorithms in DRLframeworks. Moreover, some guidelines are lacking from the literature thatwould help practitioners choose one DRL framework over another. In this paper,we empirically investigate the applications of carefully selected DRLalgorithms on two important software testing tasks: test case prioritization inthe context of Continuous Integration (CI) and game testing. For the gametesting task, we conduct experiments on a simple game and use DRL algorithms toexplore the game to detect bugs. Results show that some of the selected DRLframeworks such as Tensorforce outperform recent approaches in the literature.To prioritize test cases, we run experiments on a CI environment where DRLalgorithms from different frameworks are used to rank the test cases. Ourresults show that the performance difference between implemented algorithms insome cases is considerable, motivating further investigation.</description><author>Paulina Stevia Nouwou Mindom, Amin Nikanjam, Foutse Khomh</author><pubDate>Thu, 29 Jun 2023 13:36:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12136v3</guid></item><item><title>Traceable Group-Wise Self-Optimizing Feature Transformation Learning: A Dual Optimization Perspective</title><link>http://arxiv.org/abs/2306.16893v1</link><description>Feature transformation aims to reconstruct an effective representation spaceby mathematically refining the existing features. It serves as a pivotalapproach to combat the curse of dimensionality, enhance model generalization,mitigate data sparsity, and extend the applicability of classical models.Existing research predominantly focuses on domain knowledge-based featureengineering or learning latent representations. However, these methods, whileinsightful, lack full automation and fail to yield a traceable and optimalrepresentation space. An indispensable question arises: Can we concurrentlyaddress these limitations when reconstructing a feature space for amachine-learning task? Our initial work took a pioneering step towards thischallenge by introducing a novel self-optimizing framework. This frameworkleverages the power of three cascading reinforced agents to automaticallyselect candidate features and operations for generating improved featuretransformation combinations. Despite the impressive strides made, there wasroom for enhancing its effectiveness and generalization capability. In thisextended journal version, we advance our initial work from two distinct yetinterconnected perspectives: 1) We propose a refinement of the originalframework, which integrates a graph-based state representation method tocapture the feature interactions more effectively and develop differentQ-learning strategies to alleviate Q-value overestimation further. 2) Weutilize a new optimization technique (actor-critic) to train the entireself-optimizing framework in order to accelerate the model convergence andimprove the feature transformation performance. Finally, to validate theimproved effectiveness and generalization capability of our framework, weperform extensive experiments and conduct comprehensive analyses.</description><author>Meng Xiao, Dongjie Wang, Min Wu, Kunpeng Liu, Hui Xiong, Yuanchun Zhou, Yanjie Fu</author><pubDate>Thu, 29 Jun 2023 13:29:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16893v1</guid></item><item><title>Harnessing the Power of Hugging Face Transformers for Predicting Mental Health Disorders in Social Networks</title><link>http://arxiv.org/abs/2306.16891v1</link><description>Early diagnosis of mental disorders and intervention can facilitate theprevention of severe injuries and the improvement of treatment results. Usingsocial media and pre-trained language models, this study explores howuser-generated data can be used to predict mental disorder symptoms. Our studycompares four different BERT models of Hugging Face with standard machinelearning techniques used in automatic depression diagnosis in recentliterature. The results show that new models outperform the previous approachwith an accuracy rate of up to 97%. Analyzing the results while complementingpast findings, we find that even tiny amounts of data (like users' biodescriptions) have the potential to predict mental disorders. We conclude thatsocial media data is an excellent source of mental health screening, andpre-trained models can effectively automate this critical task.</description><author>Alireza Pourkeyvan, Ramin Safa, Ali Sorourkhah</author><pubDate>Thu, 29 Jun 2023 13:25:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16891v1</guid></item><item><title>Trajectory Poisson multi-Bernoulli mixture filter for traffic monitoring using a drone</title><link>http://arxiv.org/abs/2306.16890v1</link><description>This paper proposes a multi-object tracking (MOT) algorithm for trafficmonitoring using a drone equipped with optical and thermal cameras. Objectdetections on the images are obtained using a neural network for each type ofcamera. The cameras are modelled as direction-of-arrival (DOA) sensors. EachDOA detection follows a von-Mises Fisher distribution, whose mean direction isobtain by projecting a vehicle position on the ground to the camera. We thenuse the trajectory Poisson multi-Bernoulli mixture filter (TPMBM), which is aBayesian MOT algorithm, to optimally estimate the set of vehicle trajectories.We have also developed a parameter estimation algorithm for the measurementmodel. We have tested the accuracy of the resulting TPMBM filter in syntheticand experimental data sets.</description><author>Ángel F. García-Fernández, Jimin Xiao</author><pubDate>Thu, 29 Jun 2023 13:22:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16890v1</guid></item><item><title>Multi-IMU with Online Self-Consistency for Freehand 3D Ultrasound Reconstruction</title><link>http://arxiv.org/abs/2306.16197v2</link><description>Ultrasound (US) imaging is a popular tool in clinical diagnosis, offeringsafety, repeatability, and real-time capabilities. Freehand 3D US is atechnique that provides a deeper understanding of scanned regions withoutincreasing complexity. However, estimating elevation displacement andaccumulation error remains challenging, making it difficult to infer therelative position using images alone. The addition of external lightweightsensors has been proposed to enhance reconstruction performance without addingcomplexity, which has been shown to be beneficial. We propose a novel onlineself-consistency network (OSCNet) using multiple inertial measurement units(IMUs) to improve reconstruction performance. OSCNet utilizes a modal-levelself-supervised strategy to fuse multiple IMU information and reducedifferences between reconstruction results obtained from each IMU data.Additionally, a sequence-level self-consistency strategy is proposed to improvethe hierarchical consistency of prediction results among the scanning sequenceand its sub-sequences. Experiments on large-scale arm and carotid datasets withmultiple scanning tactics demonstrate that our OSCNet outperforms previousmethods, achieving state-of-the-art reconstruction performance.</description><author>Mingyuan Luo, Xin Yang, Zhongnuo Yan, Yuanji Zhang, Junyu Li, Jiongquan Chen, Xindi Hu, Jikuan Qian, Jun Cheng, Dong Ni</author><pubDate>Thu, 29 Jun 2023 13:17:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16197v2</guid></item><item><title>Inter-Instance Similarity Modeling for Contrastive Learning</title><link>http://arxiv.org/abs/2306.12243v3</link><description>The existing contrastive learning methods widely adopt one-hot instancediscrimination as pretext task for self-supervised learning, which inevitablyneglects rich inter-instance similarities among natural images, then leading topotential representation degeneration. In this paper, we propose a novel imagemix method, PatchMix, for contrastive learning in Vision Transformer (ViT), tomodel inter-instance similarities among images. Following the nature of ViT, werandomly mix multiple images from mini-batch in patch level to construct mixedimage patch sequences for ViT. Compared to the existing sample mix methods, ourPatchMix can flexibly and efficiently mix more than two images and simulatemore complicated similarity relations among natural images. In this manner, ourcontrastive framework can significantly reduce the gap between contrastiveobjective and ground truth in reality. Experimental results demonstrate thatour proposed method significantly outperforms the previous state-of-the-art onboth ImageNet-1K and CIFAR datasets, e.g., 3.0% linear accuracy improvement onImageNet-1K and 8.7% kNN accuracy improvement on CIFAR100. Moreover, our methodachieves the leading transfer performance on downstream tasks, object detectionand instance segmentation on COCO dataset. The code is available athttps://github.com/visresearch/patchmix</description><author>Chengchao Shen, Dawei Liu, Hao Tang, Zhe Qu, Jianxin Wang</author><pubDate>Thu, 29 Jun 2023 13:14:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12243v3</guid></item><item><title>Meta-Calibration Regularized Neural Networks</title><link>http://arxiv.org/abs/2303.15057v2</link><description>Miscalibration-the mismatch between predicted probability and the truecorrectness likelihood-has been frequently identified in modern deep neuralnetworks. Recent work in the field aims to address this problem by trainingcalibrated models directly by optimizing a proxy of the calibration erroralongside the conventional objective. Recently, Meta-Calibration (MC) showedthe effectiveness of using meta-learning for learning better calibrated models.In this work, we extend MC with two main components: (1) gamma network(gamma-net), a meta network to learn a sample-wise gamma at a continuous spacefor focal loss for optimizing backbone network; (2) smooth expected calibrationerror (SECE), a Gaussian-kernel based unbiased and differentiable ECE whichaims to smoothly optimizing gamma-net. The proposed method regularizes neuralnetwork towards better calibration meanwhile retain predictive performance. Ourexperiments show that (a) learning sample-wise gamma at continuous space caneffectively perform calibration; (b) SECE smoothly optimise gamma-net towardsbetter robustness to binning schemes; (c) the combination of gamma-net and SECEachieve the best calibration performance across various calibration metrics andretain very competitive predictive performance as compared to multiple recentlyproposed methods on three datasets.</description><author>Cheng Wang, Jacek Golebiowski</author><pubDate>Thu, 29 Jun 2023 13:11:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15057v2</guid></item><item><title>Policy Space Diversity for Non-Transitive Games</title><link>http://arxiv.org/abs/2306.16884v1</link><description>Policy-Space Response Oracles (PSRO) is an influential algorithm frameworkfor approximating a Nash Equilibrium (NE) in multi-agent non-transitive games.Many previous studies have been trying to promote policy diversity in PSRO. Amajor weakness in existing diversity metrics is that a more diverse (accordingto their diversity metrics) population does not necessarily mean (as we provedin the paper) a better approximation to a NE. To alleviate this problem, wepropose a new diversity metric, the improvement of which guarantees a betterapproximation to a NE. Meanwhile, we develop a practical and well-justifiedmethod to optimize our diversity metric using only state-action samples. Byincorporating our diversity regularization into the best response solving inPSRO, we obtain a new PSRO variant, Policy Space Diversity PSRO (PSD-PSRO). Wepresent the convergence property of PSD-PSRO. Empirically, extensiveexperiments on various games demonstrate that PSD-PSRO is more effective inproducing significantly less exploitable policies than state-of-the-art PSROvariants.</description><author>Jian Yao, Weiming Liu, Haobo Fu, Yaodong Yang, Stephen McAleer, Qiang Fu, Wei Yang</author><pubDate>Thu, 29 Jun 2023 13:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16884v1</guid></item><item><title>Surgical Phase and Instrument Recognition: How to identify appropriate Dataset Splits</title><link>http://arxiv.org/abs/2306.16879v1</link><description>Purpose: The development of machine learning models for surgical workflow andinstrument recognition from temporal data represents a challenging task due tothe complex nature of surgical workflows. In particular, the imbalanceddistribution of data is one of the major challenges in the domain of surgicalworkflow recognition. In order to obtain meaningful results, carefulpartitioning of data into training, validation, and test sets, as well as theselection of suitable evaluation metrics are crucial. Methods: In this work, wepresent an openly available web-based application that enables interactiveexploration of dataset partitions. The proposed visual framework facilitatesthe assessment of dataset splits for surgical workflow recognition, especiallywith regard to identifying sub-optimal dataset splits. Currently, it supportsvisualization of surgical phase and instrument annotations. Results: In orderto validate the dedicated interactive visualizations, we use a dataset split ofthe Cholec80 dataset. This dataset split was specifically selected to reflect acase of strong data imbalance. Using our software, we were able to identifyphases, phase transitions, and combinations of surgical instruments that werenot represented in one of the sets. Conclusion: In order to obtain meaningfulresults in highly unbalanced class distributions, special care should be takenwith respect to the selection of an appropriate split. Interactive datavisualization represents a promising approach for the assessment of machinelearning datasets. The source code is available athttps://github.com/Cardio-AI/endovis-ml</description><author>Georgii Kostiuchik, Lalith Sharan, Benedikt Mayer, Ivo Wolf, Bernhard Preim, Sandy Engelhardt</author><pubDate>Thu, 29 Jun 2023 13:02:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16879v1</guid></item><item><title>Toward a Perspectivist Turn in Ground Truthing for Predictive Computing</title><link>http://arxiv.org/abs/2109.04270v3</link><description>Most Artificial Intelligence applications are based on supervised machinelearning (ML), which ultimately grounds on manually annotated data. Theannotation process is often performed in terms of a majority vote and this hasbeen proved to be often problematic, as highlighted by recent studies on theevaluation of ML models. In this article we describe and advocate for adifferent paradigm, which we call data perspectivism, which moves away fromtraditional gold standard datasets, towards the adoption of methods thatintegrate the opinions and perspectives of the human subjects involved in theknowledge representation step of ML processes. Drawing on previous works whichinspired our proposal we describe the potential of our proposal for not onlythe more subjective tasks (e.g. those related to human language) but also totasks commonly understood as objective (e.g. medical decision making), andpresent the main advantages of adopting a perspectivist stance in ML, as wellas possible disadvantages, and various ways in which such a stance can beimplemented in practice. Finally, we share a set of recommendations and outlinea research agenda to advance the perspectivist stance in ML.</description><author>Valerio Basile, Federico Cabitza, Andrea Campagner, Michael Fell</author><pubDate>Thu, 29 Jun 2023 12:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.04270v3</guid></item><item><title>Query-based Hard-Image Retrieval for Object Detection at Test Time</title><link>http://arxiv.org/abs/2209.11559v2</link><description>There is a longstanding interest in capturing the error behaviour of objectdetectors by finding images where their performance is likely to beunsatisfactory. In real-world applications such as autonomous driving, it isalso crucial to characterise potential failures beyond simple requirements ofdetection performance. For example, a missed detection of a pedestrian close toan ego vehicle will generally require closer inspection than a missed detectionof a car in the distance. The problem of predicting such potential failures attest time has largely been overlooked in the literature and conventionalapproaches based on detection uncertainty fall short in that they are agnosticto such fine-grained characterisation of errors. In this work, we propose toreformulate the problem of finding "hard" images as a query-based hard imageretrieval task, where queries are specific definitions of "hardness", and offera simple and intuitive method that can solve this task for a large family ofqueries. Our method is entirely post-hoc, does not require ground-truthannotations, is independent of the choice of a detector, and relies on anefficient Monte Carlo estimation that uses a simple stochastic model in placeof the ground-truth. We show experimentally that it can be applied successfullyto a wide variety of queries for which it can reliably identify hard images fora given detector without any labelled data. We provide results on ranking andclassification tasks using the widely used RetinaNet, Faster-RCNN, Mask-RCNN,and Cascade Mask-RCNN object detectors. The code for this project is availableat https://github.com/fiveai/hardest.</description><author>Edward Ayers, Jonathan Sadeghi, John Redford, Romain Mueller, Puneet K. Dokania</author><pubDate>Thu, 29 Jun 2023 12:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.11559v2</guid></item><item><title>Understanding the Overfitting of the Episodic Meta-training</title><link>http://arxiv.org/abs/2306.16873v1</link><description>Despite the success of two-stage few-shot classification methods, in theepisodic meta-training stage, the model suffers severe overfitting. Wehypothesize that it is caused by over-discrimination, i.e., the model learns toover-rely on the superficial features that fit for base class discriminationwhile suppressing the novel class generalization. To penalizeover-discrimination, we introduce knowledge distillation techniques to keepnovel generalization knowledge from the teacher model during training.Specifically, we select the teacher model as the one with the best validationaccuracy during meta-training and restrict the symmetric Kullback-Leibler (SKL)divergence between the output distribution of the linear classifier of theteacher model and that of the student model. This simple approach outperformsthe standard meta-training process. We further propose the Nearest NeighborSymmetric Kullback-Leibler (NNSKL) divergence for meta-training to push thelimits of knowledge distillation techniques. NNSKL takes few-shot tasks asinput and penalizes the output of the nearest neighbor classifier, whichpossesses an impact on the relationships between query embedding and supportcenters. By combining SKL and NNSKL in meta-training, the model achieves evenbetter performance and surpasses state-of-the-art results on severalbenchmarks.</description><author>Siqi Hui, Sanping Zhou, Ye deng, Jinjun Wang</author><pubDate>Thu, 29 Jun 2023 12:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16873v1</guid></item><item><title>Towards a Self-Replicating Turing Machine</title><link>http://arxiv.org/abs/2306.16872v1</link><description>We provide partial implementations of von Neumann's universal constructor anduniversal copier, starting out with three types of simple building blocks usingminimal assumptions. Using the same principles, we also construct Turingmachines. Combining both, we arrive at a proposal for a self-replicating Turingmachine. Our construction allows for mutations if desired, and we give a simpledescription language.</description><author>Ralph P. Lano</author><pubDate>Thu, 29 Jun 2023 12:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16872v1</guid></item><item><title>NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes</title><link>http://arxiv.org/abs/2306.16869v1</link><description>Deep neural networks (DNNs) have become ubiquitous in machine learning, buttheir energy consumption remains a notable issue. Lowering the supply voltageis an effective strategy for reducing energy consumption. However, aggressivelyscaling down the supply voltage can lead to accuracy degradation due to randombit flips in static random access memory (SRAM) where model parameters arestored. To address this challenge, we introduce NeuralFuse, a novel add-onmodule that addresses the accuracy-energy tradeoff in low-voltage regimes bylearning input transformations to generate error-resistant datarepresentations. NeuralFuse protects DNN accuracy in both nominal andlow-voltage scenarios. Moreover, NeuralFuse is easy to implement and can bereadily applied to DNNs with limited access, such as non-configurable hardwareor remote access to cloud-based APIs. Experimental results demonstrate that, ata 1% bit error rate, NeuralFuse can reduce SRAM memory access energy by up to24% while improving accuracy by up to 57%. To the best of our knowledge, thisis the first model-agnostic approach (i.e., no model retraining) to addresslow-voltage-induced bit errors. The source code is available athttps://github.com/IBM/NeuralFuse.</description><author>Hao-Lun Sun, Lei Hsiung, Nandhini Chandramoorthy, Pin-Yu Chen, Tsung-Yi Ho</author><pubDate>Thu, 29 Jun 2023 12:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16869v1</guid></item><item><title>Sustainable Palm Tree Farming: Leveraging IoT and Multi-Modal Data for Early Detection and Mapping of Red Palm Weevil</title><link>http://arxiv.org/abs/2306.16862v1</link><description>The Red Palm Weevil (RPW) is a highly destructive insect causing economiclosses and impacting palm tree farming worldwide. This paper proposes aninnovative approach for sustainable palm tree farming by utilizing advancedtechnologies for the early detection and management of RPW. Our approachcombines computer vision, deep learning (DL), the Internet of Things (IoT), andgeospatial data to detect and classify RPW-infested palm trees effectively. Themain phases include; (1) DL classification using sound data from IoT devices,(2) palm tree detection using YOLOv8 on UAV images, and (3) RPW mapping usinggeospatial data. Our custom DL model achieves 100% precision and recall indetecting and localizing infested palm trees. Integrating geospatial dataenables the creation of a comprehensive RPW distribution map for efficientmonitoring and targeted management strategies. This technology-driven approachbenefits agricultural authorities, farmers, and researchers in managing RPWinfestations and safeguarding palm tree plantations' productivity.</description><author>Yosra Hajjaji, Ayyub Alzahem, Wadii Boulila, Imed Riadh Farah, Anis Koubaa</author><pubDate>Thu, 29 Jun 2023 12:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16862v1</guid></item><item><title>VAD: Vectorized Scene Representation for Efficient Autonomous Driving</title><link>http://arxiv.org/abs/2303.12077v2</link><description>Autonomous driving requires a comprehensive understanding of the surroundingenvironment for reliable trajectory planning. Previous works rely on denserasterized scene representation (e.g., agent occupancy and semantic map) toperform planning, which is computationally intensive and misses theinstance-level structure information. In this paper, we propose VAD, anend-to-end vectorized paradigm for autonomous driving, which models the drivingscene as a fully vectorized representation. The proposed vectorized paradigmhas two significant advantages. On one hand, VAD exploits the vectorized agentmotion and map elements as explicit instance-level planning constraints whicheffectively improves planning safety. On the other hand, VAD runs much fasterthan previous end-to-end planning methods by getting rid ofcomputation-intensive rasterized representation and hand-designedpost-processing steps. VAD achieves state-of-the-art end-to-end planningperformance on the nuScenes dataset, outperforming the previous best method bya large margin. Our base model, VAD-Base, greatly reduces the average collisionrate by 29.0% and runs 2.5x faster. Besides, a lightweight variant, VAD-Tiny,greatly improves the inference speed (up to 9.3x) while achieving comparableplanning performance. We believe the excellent performance and the highefficiency of VAD are critical for the real-world deployment of an autonomousdriving system. Code and models will be released for facilitating futureresearch.</description><author>Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, Xinggang Wang</author><pubDate>Thu, 29 Jun 2023 12:08:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12077v2</guid></item><item><title>ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch</title><link>http://arxiv.org/abs/2306.16857v1</link><description>We present ArrayBot, a distributed manipulation system consisting of a $16\times 16$ array of vertically sliding pillars integrated with tactile sensors,which can simultaneously support, perceive, and manipulate the tabletopobjects. Towards generalizable distributed manipulation, we leveragereinforcement learning (RL) algorithms for the automatic discovery of controlpolicies. In the face of the massively redundant actions, we propose to reshapethe action space by considering the spatially local action patch and thelow-frequency actions in the frequency domain. With this reshaped action space,we train RL agents that can relocate diverse objects through tactileobservations only. Surprisingly, we find that the discovered policy can notonly generalize to unseen object shapes in the simulator but also transfer tothe physical robot without any domain randomization. Leveraging the deployedpolicy, we present abundant real-world manipulation tasks, illustrating thevast potential of RL on ArrayBot for distributed manipulation.</description><author>Zhengrong Xue, Han Zhang, Jingwen Cheng, Zhengmao He, Yuanchen Ju, Changyi Lin, Gu Zhang, Huazhe Xu</author><pubDate>Thu, 29 Jun 2023 12:07:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16857v1</guid></item><item><title>On the Relationship Between RNN Hidden State Vectors and Semantic Ground Truth</title><link>http://arxiv.org/abs/2306.16854v1</link><description>We examine the assumption that the hidden-state vectors of recurrent neuralnetworks (RNNs) tend to form clusters of semantically similar vectors, which wedub the clustering hypothesis. While this hypothesis has been assumed in theanalysis of RNNs in recent years, its validity has not been studied thoroughlyon modern neural network architectures. We examine the clustering hypothesis inthe context of RNNs that were trained to recognize regular languages. Thisenables us to draw on perfect ground-truth automata in our evaluation, againstwhich we can compare the RNN's accuracy and the distribution of thehidden-state vectors. We start with examining the (piecewise linear) separability of an RNN'shidden-state vectors into semantically different classes. We continue theanalysis by computing clusters over the hidden-state vector space with multiplestate-of-the-art unsupervised clustering approaches. We formally analyze theaccuracy of computed clustering functions and the validity of the clusteringhypothesis by determining whether clusters group semantically similar vectorsto the same state in the ground-truth model. Our evaluation supports the validity of the clustering hypothesis in themajority of examined cases. We observed that the hidden-state vectors ofwell-trained RNNs are separable, and that the unsupervised clusteringtechniques succeed in finding clusters of similar state vectors.</description><author>Edi Muškardin, Martin Tappler, Ingo Pill, Bernhard K. Aichernig, Thomas Pock</author><pubDate>Thu, 29 Jun 2023 12:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16854v1</guid></item><item><title>iSmallNet: Densely Nested Network with Label Decoupling for Infrared Small Target Detection</title><link>http://arxiv.org/abs/2210.16561v2</link><description>Small targets are often submerged in cluttered backgrounds of infraredimages. Conventional detectors tend to generate false alarms, while CNN-baseddetectors lose small targets in deep layers. To this end, we propose iSmallNet,a multi-stream densely nested network with label decoupling for infrared smallobject detection. On the one hand, to fully exploit the shape information ofsmall targets, we decouple the original labeled ground-truth (GT) map into aninterior map and a boundary one. The GT map, in collaboration with the twoadditional maps, tackles the unbalanced distribution of small objectboundaries. On the other hand, two key modules are delicately designed andincorporated into the proposed network to boost the overall performance. First,to maintain small targets in deep layers, we develop a multi-scale nestedinteraction module to explore a wide range of context information. Second, wedevelop an interior-boundary fusion module to integrate multi-granularityinformation. Experiments on NUAA-SIRST and NUDT-SIRST clearly show thesuperiority of iSmallNet over 11 state-of-the-art detectors.</description><author>Zhiheng Hu, Yongzhen Wang, Peng Li, Jie Qin, Haoran Xie, Mingqiang Wei</author><pubDate>Thu, 29 Jun 2023 11:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16561v2</guid></item><item><title>DreamEditor: Text-Driven 3D Scene Editing with Neural Fields</title><link>http://arxiv.org/abs/2306.13455v2</link><description>Neural fields have achieved impressive advancements in view synthesis andscene reconstruction. However, editing these neural fields remains challengingdue to the implicit encoding of geometry and texture information. In thispaper, we propose DreamEditor, a novel framework that enables users to performcontrolled editing of neural fields using text prompts. By representing scenesas mesh-based neural fields, DreamEditor allows localized editing withinspecific regions. DreamEditor utilizes the text encoder of a pretrainedtext-to-Image diffusion model to automatically identify the regions to beedited based on the semantics of the text prompts. Subsequently, DreamEditoroptimizes the editing region and aligns its geometry and texture with the textprompts through score distillation sampling [29]. Extensive experiments havedemonstrated that DreamEditor can accurately edit neural fields of real-worldscenes according to the given text prompts while ensuring consistency inirrelevant areas. DreamEditor generates highly realistic textures and geometry,significantly surpassing previous works in both quantitative and qualitativeevaluations.</description><author>Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, Guanbin Li</author><pubDate>Thu, 29 Jun 2023 11:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.13455v2</guid></item><item><title>ICDaeLST: Intensity-Controllable Detail Attention-enhanced for Lightweight Fast Style Transfer</title><link>http://arxiv.org/abs/2306.16846v1</link><description>The mainstream style transfer methods usually use pre-trained deepconvolutional neural network (VGG) models as encoders, or use more complexmodel structures to achieve better style transfer effects. This leads toextremely slow processing speeds for practical tasks due to limited resourcesor higher resolution image processing, such as 4K images, severely hinderingthe practical application value of style transfer models. We introduce alightweight and fast styletransfer model with controllable detail attentionenhancement, named ICDaeLST. The model adopts a minimal, shallow, and smallarchitecture, forming a very compact lightweight model for efficient forwardinference. Although its structure is simple and has limited parameters, weachieve better overall color and texture structure matching by introducing astyle discriminator, design additional global semantic invariance loss topreserve the semantic and structural information of the content image from ahigh-level global perspective, and design a shallow detail attentionenhancement module to preserve the detail information of the content image froma low-level detail perspective. We also achieve controllable intensity duringinference for the first time (adjusting the degree of detail retention andtexture structure transfer based on subjective judgment) to meet differentusers' subjective evaluation of stylization effects. Compared with the currentbest-performing and most lightweight models, our model achieves better styletransfer quality and better content structure and detail retention, whilehaving a smaller model size (17-250 times smaller) and faster speed (0.26-6.5times faster), and achieves the fastest processing speed of 0.38s on 4Khigh-resolution images.</description><author>Jiang Shi Qi</author><pubDate>Thu, 29 Jun 2023 11:37:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16846v1</guid></item><item><title>Macro Placement by Wire-Mask-Guided Black-Box Optimization</title><link>http://arxiv.org/abs/2306.16844v1</link><description>The development of very large-scale integration (VLSI) technology has posednew challenges for electronic design automation (EDA) techniques in chipfloorplanning. During this process, macro placement is an important subproblem,which tries to determine the positions of all macros with the aim of minimizinghalf-perimeter wirelength (HPWL) and avoiding overlapping. Previous methodsinclude packing-based, analytical and reinforcement learning methods. In thispaper, we propose a new black-box optimization (BBO) framework (calledWireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedurefor objective evaluation. Equipped with different BBO algorithms, WireMask-BBOempirically achieves significant improvements over previous methods, i.e.,achieves significantly shorter HPWL by using much less time. Furthermore, itcan fine-tune existing placements by treating them as initial solutions, whichcan bring up to 50% improvement in HPWL. WireMask-BBO has the potential tosignificantly improve the quality and efficiency of chip floorplanning, whichmakes it appealing to researchers and practitioners in EDA and will alsopromote the application of BBO.</description><author>Yunqi Shi, Ke Xue, Lei Song, Chao Qian</author><pubDate>Thu, 29 Jun 2023 11:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16844v1</guid></item><item><title>Tokenization and the Noiseless Channel</title><link>http://arxiv.org/abs/2306.16842v1</link><description>Subword tokenization is a key part of many NLP pipelines. However, little isknown about why some tokenizer and hyperparameter combinations lead to betterdownstream model performance than others. We propose that good tokenizers leadto \emph{efficient} channel usage, where the channel is the means by which someinput is conveyed to the model and efficiency can be quantified ininformation-theoretic terms as the ratio of the Shannon entropy to the maximumpossible entropy of the token distribution. Yet, an optimal encoding accordingto Shannon entropy assigns extremely long codes to low-frequency tokens andvery short codes to high-frequency tokens. Defining efficiency in terms ofR\'enyi entropy, on the other hand, penalizes distributions with either veryhigh or very low-frequency tokens. In machine translation, we find that acrossmultiple tokenizers, the R\'enyi entropy with $\alpha = 2.5$ has a very strongcorrelation with \textsc{Bleu}: $0.78$ in comparison to just $-0.32$ forcompressed length.</description><author>Vilém Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Mrinmaya Sachan, Ryan Cotterell</author><pubDate>Thu, 29 Jun 2023 11:32:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16842v1</guid></item><item><title>Solving Kernel Ridge Regression with Gradient-Based Optimization Methods</title><link>http://arxiv.org/abs/2306.16838v1</link><description>Kernel ridge regression, KRR, is a non-linear generalization of linear ridgeregression. Here, we introduce an equivalent formulation of the objectivefunction of KRR, opening up both for using other penalties than the ridgepenalty and for studying kernel ridge regression from the perspective ofgradient descent. Using a continuous-time perspective, we derive a closed-formsolution, kernel gradient flow, KGF, with regularization through earlystopping, which allows us to theoretically bound the differences between KGFand KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and$\ell_\infty$ penalties and utilize the fact that analogously to thesimilarities between KGF and KRR, the solutions obtained when using thesepenalties are very similar to those obtained from forward stagewise regression(also known as coordinate descent) and sign gradient descent in combinationwith early stopping. Thus the need for computationally heavy proximal gradientdescent algorithms can be alleviated. We show theoretically and empirically howthese penalties, and corresponding gradient-based optimization algorithms,produce signal-driven and robust regression solutions, respectively. We alsoinvestigate kernel gradient descent where the kernel is allowed to changeduring training, and theoretically address the effects this has ongeneralization. Based on our findings, we propose an update scheme for thebandwidth of translational-invariant kernels, where we let the bandwidthdecrease to zero during training, thus circumventing the need forhyper-parameter selection. We demonstrate on real and synthetic data howdecreasing the bandwidth during training outperforms using a constantbandwidth, selected by cross-validation and marginal likelihood maximization.We also show that using a decreasing bandwidth, we are able to achieve bothzero training error and a double descent behavior.</description><author>Oskar Allerbo, Rebecka Jörnsten</author><pubDate>Thu, 29 Jun 2023 11:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16838v1</guid></item><item><title>A Formal Perspective on Byte-Pair Encoding</title><link>http://arxiv.org/abs/2306.16837v1</link><description>Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data inNLP, despite being devised initially as a compression method. BPE appears to bea greedy algorithm at face value, but the underlying optimization problem thatBPE seeks to solve has not yet been laid down. We formalize BPE as acombinatorial optimization problem. Via submodular functions, we prove that theiterative greedy version is a$\frac{1}{{\sigma(\boldsymbol{\mu}^\star)}}(1-e^{-{\sigma(\boldsymbol{\mu}^\star)}})$-approximationof an optimal merge sequence, where ${\sigma(\boldsymbol{\mu}^\star)}$ is thetotal backward curvature with respect to the optimal merge sequence$\boldsymbol{\mu}^\star$. Empirically the lower bound of the approximation is$\approx 0.37$. We provide a faster implementation of BPE which improves the runtimecomplexity from $\mathcal{O}\left(N M\right)$ to $\mathcal{O}\left(N \logM\right)$, where $N$ is the sequence length and $M$ is the merge count.Finally, we optimize the brute-force algorithm for optimal BPE usingmemoization.</description><author>Vilém Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Tim Vieira, Mrinmaya Sachan, Ryan Cotterell</author><pubDate>Thu, 29 Jun 2023 11:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16837v1</guid></item><item><title>Intelligence of Astronomical Optical Telescope: Present Status and Future Perspectives</title><link>http://arxiv.org/abs/2306.16834v1</link><description>Artificial intelligence technology has been widely used in astronomy, and newartificial intelligence technologies and application scenarios are constantlyemerging. There have been a large number of papers reviewing the application ofartificial intelligence technology in astronomy. However, relevant articlesseldom mention telescope intelligence separately, and it is difficult tounderstand the current development status and research hotspots of telescopeintelligence from these papers. This paper combines the development history ofartificial intelligence technology and the difficulties of criticaltechnologies of telescopes, comprehensively introduces the development andresearch hotspots of telescope intelligence, then conducts statistical analysison various research directions of telescope intelligence and defines theresearch directions' merits. All kinds of research directions are evaluated,and the research trend of each telescope's intelligence is pointed out.Finally, according to the advantages of artificial intelligence technology andthe development trend of telescopes, future research hotspots of telescopeintelligence are given.</description><author>Tianzhu Hu, Kang Huang, Jingyi Cai, Xiushan Pang, Yonghui Hou, Yong Zhang, Huaiqing Wang, Xiangqun Cui</author><pubDate>Thu, 29 Jun 2023 11:17:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16834v1</guid></item><item><title>Energy-based Detection of Adverse Weather Effects in LiDAR Data</title><link>http://arxiv.org/abs/2305.16129v3</link><description>Autonomous vehicles rely on LiDAR sensors to perceive the environment.Adverse weather conditions like rain, snow, and fog negatively affect thesesensors, reducing their reliability by introducing unwanted noise in themeasurements. In this work, we tackle this problem by proposing a novelapproach for detecting adverse weather effects in LiDAR data. We reformulatethis problem as an outlier detection task and use an energy-based framework todetect outliers in point clouds. More specifically, our method learns toassociate low energy scores with inlier points and high energy scores withoutliers allowing for robust detection of adverse weather effects. In extensiveexperiments, we show that our method performs better in adverse weatherdetection and has higher robustness to unseen weather effects than previousstate-of-the-art methods. Furthermore, we show how our method can be used toperform simultaneous outlier detection and semantic segmentation. Finally, tohelp expand the research field of LiDAR perception in adverse weather, werelease the SemanticSpray dataset, which contains labeled vehicle spray data inhighway-like scenarios. The dataset is available athttps://semantic-spray-dataset.github.io .</description><author>Aldi Piroli, Vinzenz Dallabetta, Johannes Kopp, Marc Walessa, Daniel Meissner, Klaus Dietmayer</author><pubDate>Thu, 29 Jun 2023 11:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16129v3</guid></item><item><title>Sampling weights of deep neural networks</title><link>http://arxiv.org/abs/2306.16830v1</link><description>We introduce a probability distribution, combined with an efficient samplingalgorithm, for weights and biases of fully-connected neural networks. In asupervised learning context, no iterative optimization or gradient computationsof internal network parameters are needed to obtain a trained network. Thesampling is based on the idea of random feature models. However, instead of adata-agnostic distribution, e.g., a normal distribution, we use both the inputand the output training data of the supervised learning problem to sample bothshallow and deep networks. We prove that the sampled networks we construct areuniversal approximators. We also show that our sampling scheme is invariant torigid body transformations and scaling of the input data. This implies manypopular pre-processing techniques are no longer required. For Barron functions,we show that the $L^2$-approximation error of sampled shallow networksdecreases with the square root of the number of neurons. In numericalexperiments, we demonstrate that sampled networks achieve comparable accuracyas iteratively trained ones, but can be constructed orders of magnitude faster.Our test cases involve a classification benchmark from OpenML, sampling ofneural operators to represent maps in function spaces, and transfer learningusing well-known architectures.</description><author>Erik Lien Bolager, Iryna Burak, Chinmay Datar, Qing Sun, Felix Dietrich</author><pubDate>Thu, 29 Jun 2023 11:13:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16830v1</guid></item><item><title>SaGess: Sampling Graph Denoising Diffusion Model for Scalable Graph Generation</title><link>http://arxiv.org/abs/2306.16827v1</link><description>Over recent years, denoising diffusion generative models have come to beconsidered as state-of-the-art methods for synthetic data generation,especially in the case of generating images. These approaches have also provedsuccessful in other applications such as tabular and graph data generation.However, due to computational complexity, to this date, the application ofthese techniques to graph data has been restricted to small graphs, such asthose used in molecular modeling. In this paper, we propose SaGess, a discretedenoising diffusion approach, which is able to generate large real-worldnetworks by augmenting a diffusion model (DiGress) with a generalizeddivide-and-conquer framework. The algorithm is capable of generating largergraphs by sampling a covering of subgraphs of the initial graph in order totrain DiGress. SaGess then constructs a synthetic graph using the subgraphsthat have been generated by DiGress. We evaluate the quality of the syntheticdata sets against several competitor methods by comparing graph statisticsbetween the original and synthetic samples, as well as evaluating the utilityof the synthetic data set produced by using it to train a task-driven model,namely link prediction. In our experiments, SaGess, outperforms most of theone-shot state-of-the-art graph generating methods by a significant factor,both on the graph metrics and on the link prediction task.</description><author>Stratis Limnios, Praveen Selvaraj, Mihai Cucuringu, Carsten Maple, Gesine Reinert, Andrew Elliott</author><pubDate>Thu, 29 Jun 2023 11:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16827v1</guid></item><item><title>Length of Stay prediction for Hospital Management using Domain Adaptation</title><link>http://arxiv.org/abs/2306.16823v1</link><description>Inpatient length of stay (LoS) is an important managerial metric which ifknown in advance can be used to efficiently plan admissions, allocate resourcesand improve care. Using historical patient data and machine learningtechniques, LoS prediction models can be developed. Ethically, these models cannot be used for patient discharge in lieu of unit heads but are of utmostnecessity for hospital management systems in charge of effective hospitalplanning. Therefore, the design of the prediction system should be adapted towork in a true hospital setting. In this study, we predict early hospital LoSat the granular level of admission units by applying domain adaptation toleverage information learned from a potential source domain. Time-varying datafrom 110,079 and 60,492 patient stays to 8 and 9 intensive care units wererespectively extracted from eICU-CRD and MIMIC-IV. These were fed into aLong-Short Term Memory and a Fully connected network to train a source domainmodel, the weights of which were transferred either partially or fully toinitiate training in target domains. Shapley Additive exPlanations (SHAP)algorithms were used to study the effect of weight transfer on modelexplanability. Compared to the benchmark, the proposed weight transfer modelshowed statistically significant gains in prediction accuracy (between 1% and5%) as well as computation time (up to 2hrs) for some target domains. Theproposed method thus provides an adapted clinical decision support system forhospital management that can ease processes of data access via ethicalcommittee, computation infrastructures and time.</description><author>Lyse Naomi Wamba Momo, Nyalleng Moorosi, Elaine O. Nsoesie, Frank Rademakers, Bart De Moor</author><pubDate>Thu, 29 Jun 2023 10:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16823v1</guid></item><item><title>Graph Denoising Diffusion for Inverse Protein Folding</title><link>http://arxiv.org/abs/2306.16819v1</link><description>Inverse protein folding is challenging due to its inherent one-to-manymapping characteristic, where numerous possible amino acid sequences can foldinto a single, identical protein backbone. This task involves not onlyidentifying viable sequences but also representing the sheer diversity ofpotential solutions. However, existing discriminative models, such astransformer-based auto-regressive models, struggle to encapsulate the diverserange of plausible solutions. In contrast, diffusion probabilistic models, asan emerging genre of generative approaches, offer the potential to generate adiverse set of sequence candidates for determined protein backbones. We proposea novel graph denoising diffusion model for inverse protein folding, where agiven protein backbone guides the diffusion process on the corresponding aminoacid residue types. The model infers the joint distribution of amino acidsconditioned on the nodes' physiochemical properties and local environment.Moreover, we utilize amino acid replacement matrices for the diffusion forwardprocess, encoding the biologically-meaningful prior knowledge of amino acidsfrom their spatial and sequential neighbors as well as themselves, whichreduces the sampling space of the generative process. Our model achievesstate-of-the-art performance over a set of popular baseline methods in sequencerecovery and exhibits great potential in generating diverse protein sequencesfor a determined protein backbone structure.</description><author>Kai Yi, Bingxin Zhou, Yiqing Shen, Pietro Liò, Yu Guang Wang</author><pubDate>Thu, 29 Jun 2023 10:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16819v1</guid></item><item><title>Improving Online Continual Learning Performance and Stability with Temporal Ensembles</title><link>http://arxiv.org/abs/2306.16817v1</link><description>Neural networks are very effective when trained on large datasets for a largenumber of iterations. However, when they are trained on non-stationary streamsof data and in an online fashion, their performance is reduced (1) by theonline setup, which limits the availability of data, (2) due to catastrophicforgetting because of the non-stationary nature of the data. Furthermore,several recent works (Caccia et al., 2022; Lange et al., 2023)arXiv:2205.1345(2) showed that replay methods used in continual learning sufferfrom the stability gap, encountered when evaluating the model continually(rather than only on task boundaries). In this article, we study the effect ofmodel ensembling as a way to improve performance and stability in onlinecontinual learning. We notice that naively ensembling models coming from avariety of training tasks increases the performance in online continuallearning considerably. Starting from this observation, and drawing inspirationsfrom semi-supervised learning ensembling methods, we use a lightweight temporalensemble that computes the exponential moving average of the weights (EMA) attest time, and show that it can drastically increase the performance andstability when used in combination with several methods from the literature.</description><author>Albin Soutif--Cormerais, Antonio Carta, Joost Van de Weijer</author><pubDate>Thu, 29 Jun 2023 10:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16817v1</guid></item><item><title>Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning</title><link>http://arxiv.org/abs/2306.14522v2</link><description>The widely used stochastic gradient methods for minimizing nonconvexcomposite objective functions require the Lipschitz smoothness of thedifferentiable part. But the requirement does not hold true for problem classesincluding quadratic inverse problems and training neural networks. To addressthis issue, we investigate a family of stochastic Bregman proximal gradient(SBPG) methods, which only require smooth adaptivity of the differentiablepart. SBPG replaces the upper quadratic approximation used in SGD with theBregman proximity measure, resulting in a better approximation model thatcaptures the non-Lipschitz gradients of the nonconvex objective. We formulatethe vanilla SBPG and establish its convergence properties under nonconvexsetting without finite-sum structure. Experimental results on quadratic inverseproblems testify the robustness of SBPG. Moreover, we propose a momentum-basedversion of SBPG (MSBPG) and prove it has improved convergence properties. Weapply MSBPG to the training of deep neural networks with a polynomial kernelfunction, which ensures the smooth adaptivity of the loss function.Experimental results on representative benchmarks demonstrate the effectivenessand robustness of MSBPG in training neural networks. Since the additionalcomputation cost of MSBPG compared with SGD is negligible in large-scaleoptimization, MSBPG can potentially be employed as an universal open-sourceoptimizer in the future.</description><author>Kuangyu Ding, Jingyang Li, Kim-Chuan Toh</author><pubDate>Thu, 29 Jun 2023 10:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.14522v2</guid></item><item><title>On-device modeling of user's social context and familiar places from smartphone-embedded sensor data</title><link>http://arxiv.org/abs/2205.08790v2</link><description>Context modeling and recognition represent complex tasks that allow mobileand ubiquitous computing applications to adapt to the user's situation. Currentsolutions mainly focus on limited context information generally processed oncentralized architectures, potentially exposing users' personal data to privacyleakage, and missing personalization features. For these reasons on-devicecontext modeling and recognition represent the current research trend in thisarea. Among the different information characterizing the user's context inmobile environments, social interactions and visited locations remarkablycontribute to the characterization of daily life scenarios. In this paper wepropose a novel, unsupervised and lightweight approach to model the user'ssocial context and her locations based on ego networks directly on the usermobile device. Relying on this model, the system is able to extract high-leveland semantic-rich context features from smartphone-embedded sensors data.Specifically, for the social context it exploits data related to both physicaland cyber social interactions among users and their devices. As far as locationcontext is concerned, we assume that it is more relevant to model thefamiliarity degree of a specific location for the user's context than the rawlocation data, both in terms of GPS coordinates and proximity devices. By using5 real-world datasets, we assess the structure of the social and location egonetworks, we provide a semantic evaluation of the proposed models and acomplexity evaluation in terms of mobile computing performance. Finally, wedemonstrate the relevance of the extracted features by showing the performanceof 3 machine learning algorithms to recognize daily-life situations, obtainingan improvement of 3% of AUROC, 9% of Precision, and 5% in terms of Recall withrespect to use only features related to physical context.</description><author>Mattia Giovanni Campana, Franca Delmastro</author><pubDate>Thu, 29 Jun 2023 10:48:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.08790v2</guid></item><item><title>Explainability in Practice: Estimating Electrification Rates from Mobile Phone Data in Senegal</title><link>http://arxiv.org/abs/2211.06277v2</link><description>Explainable artificial intelligence (XAI) provides explanations for notinterpretable machine learning (ML) models. While many technical approachesexist, there is a lack of validation of these techniques on real-worlddatasets. In this work, we present a use-case of XAI: an ML model which istrained to estimate electrification rates based on mobile phone data inSenegal. The data originate from the Data for Development challenge by Orangein 2014/15. We apply two model-agnostic, local explanation techniques and findthat while the model can be verified, it is biased with respect to thepopulation density. We conclude our paper by pointing to the two mainchallenges we encountered during our work: data processing and model designthat might be restricted by currently available XAI methods, and the importanceof domain knowledge to interpret explanations.</description><author>Laura State, Hadrien Salat, Stefania Rubrichi, Zbigniew Smoreda</author><pubDate>Thu, 29 Jun 2023 10:43:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06277v2</guid></item><item><title>CLIPAG: Towards Generator-Free Text-to-Image Generation</title><link>http://arxiv.org/abs/2306.16805v1</link><description>Perceptually Aligned Gradients (PAG) refer to an intriguing property observedin robust image classification models, wherein their input gradients align withhuman perception and pose semantic meanings. While this phenomenon has gainedsignificant research attention, it was solely studied in the context ofunimodal vision-only architectures. In this work, we extend the study of PAG toVision-Language architectures, which form the foundations for diverseimage-text tasks and applications. Through an adversarial robustificationfinetuning of CLIP, we demonstrate that robust Vision-Language models exhibitPAG in contrast to their vanilla counterparts. This work reveals the merits ofCLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, weshow that seamlessly integrating CLIPAG in a "plug-n-play" manner leads tosubstantial improvements in vision-language generative applications.Furthermore, leveraging its PAG property, CLIPAG enables text-to-imagegeneration without any generative model, which typically requires hugegenerators.</description><author>Roy Ganz, Michael Elad</author><pubDate>Thu, 29 Jun 2023 10:35:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16805v1</guid></item><item><title>Multi-source adversarial transfer learning based on similar source domains with local features</title><link>http://arxiv.org/abs/2305.19067v2</link><description>Transfer learning leverages knowledge from other domains and has beensuccessful in many applications. Transfer learning methods rely on the overallsimilarity of the source and target domains. However, in some cases, it isimpossible to provide an overall similar source domain, and only some sourcedomains with similar local features can be provided. Can transfer learning beachieved? In this regard, we propose a multi-source adversarial transferlearning method based on local feature similarity to the source domain tohandle transfer scenarios where the source and target domains have only localsimilarities. This method extracts transferable local features between a singlesource domain and the target domain through a sub-network. Specifically, thefeature extractor of the sub-network is induced by the domain discriminator tolearn transferable knowledge between the source domain and the target domain.The extracted features are then weighted by an attention module to suppressnon-transferable local features while enhancing transferable local features. Inorder to ensure that the data from the target domain in different sub-networksin the same batch is exactly the same, we designed a multi-source domainindependent strategy to provide the possibility for later local feature fusionto complete the key features required. In order to verify the effectiveness ofthe method, we made the dataset "Local Carvana Image Masking Dataset". Applyingthe proposed method to the image segmentation task of the proposed datasetachieves better transfer performance than other multi-source transfer learningmethods. It is shown that the designed transfer learning method is feasible fortransfer scenarios where the source and target domains have only localsimilarities.</description><author>Yifu Zhang, Hongru Li, Shimeng Shi, Youqi Li, Jiansong Zhang</author><pubDate>Thu, 29 Jun 2023 10:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19067v2</guid></item><item><title>Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems</title><link>http://arxiv.org/abs/2304.14712v4</link><description>In this article, a benchmark for real-world bin packing problems is proposed.This dataset consists of 12 instances of varying levels of complexity regardingsize (with the number of packages ranging from 38 to 53) and user-definedrequirements. In fact, several real-world-oriented restrictions were taken intoaccount to build these instances: i) item and bin dimensions, ii) weightrestrictions, iii) affinities among package categories iv) preferences forpackage ordering and v) load balancing. Besides the data, we also offer an owndeveloped Python script for the dataset generation, coined Q4RealBPP-DataGen.The benchmark was initially proposed to evaluate the performance of quantumsolvers. Therefore, the characteristics of this set of instances were designedaccording to the current limitations of quantum devices. Additionally, thedataset generator is included to allow the construction of general-purposebenchmarks. The data introduced in this article provides a baseline that willencourage quantum computing researchers to work on real-world bin packingproblems.</description><author>Eneko Osaba, Esther Villar-Rodriguez, Sebastián V. Romero</author><pubDate>Thu, 29 Jun 2023 10:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14712v4</guid></item><item><title>Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis</title><link>http://arxiv.org/abs/2306.16803v1</link><description>To make reinforcement learning more sample efficient, we need better creditassignment methods that measure an action's influence on future rewards.Building upon Hindsight Credit Assignment (HCA), we introduce CounterfactualContribution Analysis (COCOA), a new family of model-based credit assignmentalgorithms. Our algorithms achieve precise credit assignment by measuring thecontribution of actions upon obtaining subsequent rewards, by quantifying acounterfactual query: "Would the agent still have reached this reward if it hadtaken another action?". We show that measuring contributions w.r.t. rewardingstates, as is done in HCA, results in spurious estimates of contributions,causing HCA to degrade towards the high-variance REINFORCE estimator in manyrelevant environments. Instead, we measure contributions w.r.t. rewards orlearned representations of the rewarding objects, resulting in gradientestimates with lower variance. We run experiments on a suite of problemsspecifically designed to evaluate long-term credit assignment capabilities. Byusing dynamic programming, we measure ground-truth policy gradients and showthat the improved performance of our new model-based credit assignment methodsis due to lower bias and variance compared to HCA and common baselines. Ourresults demonstrate how modeling action contributions towards rewardingoutcomes can be leveraged for credit assignment, opening a new path towardssample-efficient reinforcement learning.</description><author>Alexander Meulemans, Simon Schug, Seijin Kobayashi, Nathaniel Daw, Gregory Wayne</author><pubDate>Thu, 29 Jun 2023 10:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16803v1</guid></item><item><title>Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2303.01170v2</link><description>Transfer learning in Reinforcement Learning (RL) has been widely studied toovercome training issues of Deep-RL, i.e., exploration cost, data availabilityand convergence time, by introducing a way to enhance training phase withexternal knowledge. Generally, knowledge is transferred from expert-agents tonovices. While this fixes the issue for a novice agent, a good understanding ofthe task on expert agent is required for such transfer to be effective. As analternative, in this paper we propose Expert-Free Online Transfer Learning(EF-OnTL), an algorithm that enables expert-free real-time dynamic transferlearning in multi-agent system. No dedicated expert exists, and transfer sourceagent and knowledge to be transferred are dynamically selected at each transferstep based on agents' performance and uncertainty. To improve uncertaintyestimation, we also propose State Action Reward Next-State Random NetworkDistillation (sars-RND), an extension of RND that estimates uncertainty from RLagent-environment interaction. We demonstrate EF-OnTL effectiveness against ano-transfer scenario and advice-based baselines, with and without expertagents, in three benchmark tasks: Cart-Pole, a grid-based Multi-TeamPredator-Prey (mt-pp) and Half Field Offense (HFO). Our results show thatEF-OnTL achieve overall comparable performance when compared againstadvice-based baselines while not requiring any external input nor thresholdtuning. EF-OnTL outperforms no-transfer with an improvement related to thecomplexity of the task addressed.</description><author>Alberto Castagna, Ivana Dusparic</author><pubDate>Thu, 29 Jun 2023 10:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01170v2</guid></item><item><title>ORA3D: Overlap Region Aware Multi-view 3D Object Detection</title><link>http://arxiv.org/abs/2207.00865v4</link><description>Current multi-view 3D object detection methods often fail to detect objectsin the overlap region properly, and the networks' understanding of the scene isoften limited to that of a monocular detection network. Moreover, objects inthe overlap region are often largely occluded or suffer from deformation due tocamera distortion, causing a domain shift. To mitigate this issue, we proposeusing the following two main modules: (1) Stereo Disparity Estimation for WeakDepth Supervision and (2) Adversarial Overlap Region Discriminator. The formerutilizes the traditional stereo disparity estimation method to obtain reliabledisparity information from the overlap region. Given the disparity estimates assupervision, we propose regularizing the network to fully utilize the geometricpotential of binocular images and improve the overall detection accuracyaccordingly. Further, the latter module minimizes the representational gapbetween non-overlap and overlapping regions. We demonstrate the effectivenessof the proposed method with the nuScenes large-scale multi-view 3D objectdetection data. Our experiments show that our proposed method outperformscurrent state-of-the-art models, i.e., DETR3D and BEVDet.</description><author>Wonseok Roh, Gyusam Chang, Seokha Moon, Giljoo Nam, Chanyoung Kim, Younghyun Kim, Jinkyu Kim, Sangpil Kim</author><pubDate>Thu, 29 Jun 2023 10:20:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.00865v4</guid></item><item><title>LeanAI: A method for AEC practitioners to effectively plan AI implementations</title><link>http://arxiv.org/abs/2306.16799v1</link><description>Recent developments in Artificial Intelligence (AI) provide unprecedentedautomation opportunities in the Architecture, Engineering, and Construction(AEC) industry. However, despite the enthusiasm regarding the use of AI, 85% ofcurrent big data projects fail. One of the main reasons for AI project failuresin the AEC industry is the disconnect between those who plan or decide to useAI and those who implement it. AEC practitioners often lack a clearunderstanding of the capabilities and limitations of AI, leading to a failureto distinguish between what AI should solve, what it can solve, and what itwill solve, treating these categories as if they are interchangeable. This lackof understanding results in the disconnect between AI planning andimplementation because the planning is based on a vision of what AI shouldsolve without considering if it can or will solve it. To address thischallenge, this work introduces the LeanAI method. The method has beendeveloped using data from several ongoing longitudinal studies analyzing AIimplementations in the AEC industry, which involved 50+ hours of interviewdata. The LeanAI method delineates what AI should solve, what it can solve, andwhat it will solve, forcing practitioners to clearly articulate thesecomponents early in the planning process itself by involving the relevantstakeholders. By utilizing the method, practitioners can effectively plan AIimplementations, thus increasing the likelihood of success and ultimatelyspeeding up the adoption of AI. A case example illustrates the usefulness ofthe method.</description><author>Ashwin Agrawal, Vishal Singh, Martin Fischer</author><pubDate>Thu, 29 Jun 2023 10:18:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16799v1</guid></item><item><title>Evaluation of Environmental Conditions on Object Detection using Oriented Bounding Boxes for AR Applications</title><link>http://arxiv.org/abs/2306.16798v1</link><description>The objective of augmented reality (AR) is to add digital content to naturalimages and videos to create an interactive experience between the user and theenvironment. Scene analysis and object recognition play a crucial role in AR,as they must be performed quickly and accurately. In this study, a new approachis proposed that involves using oriented bounding boxes with a detection andrecognition deep network to improve performance and processing time. Theapproach is evaluated using two datasets: a real image dataset (DOTA dataset)commonly used for computer vision tasks, and a synthetic dataset that simulatesdifferent environmental, lighting, and acquisition conditions. The focus of theevaluation is on small objects, which are difficult to detect and recognise.The results indicate that the proposed approach tends to produce better AveragePrecision and greater accuracy for small objects in most of the testedconditions.</description><author>Vladislav Li, Barbara Villarini, Jean-Christophe Nebel, Thomas Lagkas, Panagiotis Sarigiannidis, Vasileios Argyriou</author><pubDate>Thu, 29 Jun 2023 10:17:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16798v1</guid></item><item><title>Log-linear Guardedness and its Implications</title><link>http://arxiv.org/abs/2210.10012v2</link><description>Methods for erasing human-interpretable concepts from neural representationsthat assume linearity have been found to be tractable and useful. However, theimpact of this removal on the behavior of downstream classifiers trained on themodified representations is not fully understood. In this work, we formallydefine the notion of log-linear guardedness as the inability of an adversary topredict the concept directly from the representation, and study itsimplications. We show that, in the binary case, under certain assumptions, adownstream log-linear model cannot recover the erased concept. However, wedemonstrate that a multiclass log-linear model \emph{can} be constructed thatindirectly recovers the concept in some cases, pointing to the inherentlimitations of log-linear guardedness as a downstream bias mitigationtechnique. These findings shed light on the theoretical limitations of linearerasure methods and highlight the need for further research on the connectionsbetween intrinsic and extrinsic bias in neural models.</description><author>Shauli Ravfogel, Yoav Goldberg, Ryan Cotterell</author><pubDate>Thu, 29 Jun 2023 10:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10012v2</guid></item><item><title>DeciLS-PBO: an Effective Local Search Method for Pseudo-Boolean Optimization</title><link>http://arxiv.org/abs/2301.12251v2</link><description>Local search is an effective method for solving large-scale combinatorialoptimization problems, and it has made remarkable progress in recent yearsthrough several subtle mechanisms. In this paper, we found two ways to improvethe local search algorithms in solving Pseudo-Boolean Optimization (PBO):Firstly, some of those mechanisms such as unit propagation are merely used insolving MaxSAT before, which can be generalized to solve PBO as well; Secondly,the existing local search algorithms utilize the heuristic on variables,so-called score, to mainly guide the search. We attempt to gain more insightsinto the clause, as it plays the role of a middleman who builds a bridgebetween variables and the given formula. Hence, we first extended thecombination of unit propagation-based decimation algorithm to PBO problem,giving a further generalized definition of unit clause for PBO problem, andapply it to the existing solver LS-PBO for constructing an initial assignment;then, we introduced a new heuristic on clauses, dubbed care, to set a higherpriority for the clauses that are less satisfied in current iterations.Experiments on benchmarks from the most recent PB Competition, as well as threereal-world application benchmarks including minimum-width confidence band,wireless sensor network optimization, and seating arrangement problems showthat our algorithm DeciLS-PBO has a promising performance compared to thestate-of-the-art algorithms.</description><author>Luyu Jiang, Dantong Ouyang, Qi Zhang, Liming Zhang</author><pubDate>Thu, 29 Jun 2023 10:03:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12251v2</guid></item><item><title>Benchmarking Large Language Model Capabilities for Conditional Generation</title><link>http://arxiv.org/abs/2306.16793v1</link><description>Pre-trained large language models (PLMs) underlie most new developments innatural language processing. They have shifted the field fromapplication-specific model pipelines to a single model that is adapted to awide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongsidetechniques like few-shot learning, have additionally shifted the outputmodality to generation instead of classification or regression. Despite theirubiquitous use, the generation quality of language models is rarely evaluatedwhen these models are introduced. Additionally, it is unclear how existinggeneration tasks--while they can be used to compare systems at a highlevel--relate to the real world use cases for which people have been adoptingthem. In this work, we discuss how to adapt existing application-specificgeneration benchmarks to PLMs and provide an in-depth, empirical study of thelimitations and capabilities of PLMs in natural language generation tasks alongdimensions such as scale, architecture, input and output language. Our resultsshow that PLMs differ in their applicability to different data regimes andtheir generalization to multiple languages and inform which PLMs to use for agiven generation task setup. We share best practices to be taken intoconsideration when benchmarking generation capabilities during the developmentof upcoming PLMs.</description><author>Joshua Maynez, Priyanka Agrawal, Sebastian Gehrmann</author><pubDate>Thu, 29 Jun 2023 09:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16793v1</guid></item><item><title>Continual Learning for Predictive Maintenance: Overview and Challenges</title><link>http://arxiv.org/abs/2301.12467v2</link><description>Deep learning techniques have become one of the main propellers for solvingengineering problems effectively and efficiently. For instance, PredictiveMaintenance methods have been used to improve predictions of when maintenanceis needed on different machines and operative contexts. However, deep learningmethods are not without limitations, as these models are normally trained on afixed distribution that only reflects the current state of the problem. Due tointernal or external factors, the state of the problem can change, and theperformance decreases due to the lack of generalization and adaptation.Contrary to this stationary training set, real-world applications change theirenvironments constantly, creating the need to constantly adapt the model toevolving scenarios. To aid in this endeavor, Continual Learning methods proposeways to constantly adapt prediction models and incorporate new knowledge afterdeployment. Despite the advantages of these techniques, there are stillchallenges to applying them to real-world problems. In this work, we present abrief introduction to predictive maintenance, non-stationary environments, andcontinual learning, together with an extensive review of the current state ofapplying continual learning in real-world applications and specifically inpredictive maintenance. We then discuss the current challenges of bothpredictive maintenance and continual learning, proposing future directions atthe intersection of both areas. Finally, we propose a novel way to createbenchmarks that favor the application of continuous learning methods in morerealistic environments, giving specific examples of predictive maintenance.</description><author>Julio Hurtado, Dario Salvati, Rudy Semola, Mattia Bosio, Vincenzo Lomonaco</author><pubDate>Thu, 29 Jun 2023 09:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12467v2</guid></item><item><title>Did AI get more negative recently?</title><link>http://arxiv.org/abs/2202.13610v3</link><description>In this paper, we classify scientific articles in the domain of naturallanguage processing (NLP) and machine learning (ML), as core subfields ofartificial intelligence (AI), into whether (i) they extend the currentstate-of-the-art by the introduction of novel techniques which beat existingmodels or whether (ii) they mainly criticize the existing state-of-the-art,i.e. that it is deficient with respect to some property (e.g. wrong evaluation,wrong datasets, misleading task specification). We refer to contributions under(i) as having a 'positive stance' and contributions under (ii) as having a'negative stance' (to related work). We annotate over 1.5 k papers from NLP andML to train a SciBERT-based model to automatically predict the stance of apaper based on its title and abstract. We then analyse large-scale trends onover 41 k papers from the last approximately 35 years in NLP and ML, findingthat papers have become substantially more positive over time, but negativepapers also got more negative and we observe considerably more negative papersin recent years. Negative papers are also more influential in terms ofcitations they receive.</description><author>Dominik Beese, Begüm Altunbaş, Görkem Güzeler, Steffen Eger</author><pubDate>Thu, 29 Jun 2023 09:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.13610v3</guid></item><item><title>Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging</title><link>http://arxiv.org/abs/2306.16788v1</link><description>Neural networks can be significantly compressed by pruning, leading to sparsemodels requiring considerably less storage and floating-point operations whilemaintaining predictive performance. Model soups (Wortsman et al., 2022) improvegeneralization and out-of-distribution performance by averaging the parametersof multiple models into a single one without increased inference time. However,identifying models in the same loss basin to leverage both sparsity andparameter averaging is challenging, as averaging arbitrary sparse modelsreduces the overall sparsity due to differing sparse connectivities. In thiswork, we address these challenges by demonstrating that exploring a singleretraining phase of Iterative Magnitude Pruning (IMP) with varyinghyperparameter configurations, such as batch ordering or weight decay, producesmodels that are suitable for averaging and share the same sparse connectivityby design. Averaging these models significantly enhances generalizationperformance compared to their individual components. Building on this idea, weintroduce Sparse Model Soups (SMS), a novel method for merging sparse models byinitiating each prune-retrain cycle with the averaged model of the previousphase. SMS maintains sparsity, exploits sparse network benefits being modularand fully parallelizable, and substantially improves IMP's performance.Additionally, we demonstrate that SMS can be adapted to enhance the performanceof state-of-the-art pruning during training approaches.</description><author>Max Zimmer, Christoph Spiegel, Sebastian Pokutta</author><pubDate>Thu, 29 Jun 2023 09:49:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16788v1</guid></item><item><title>An Efficient Virtual Data Generation Method for Reducing Communication in Federated Learning</title><link>http://arxiv.org/abs/2306.12088v3</link><description>Communication overhead is one of the major challenges in FederatedLearning(FL). A few classical schemes assume the server can extract theauxiliary information about training data of the participants from the localmodels to construct a central dummy dataset. The server uses the dummy datasetto finetune aggregated global model to achieve the target test accuracy infewer communication rounds. In this paper, we summarize the above solutionsinto a data-based communication-efficient FL framework. The key of the proposedframework is to design an efficient extraction module(EM) which ensures thedummy dataset has a positive effect on finetuning aggregated global model.Different from the existing methods that use generator to design EM, ourproposed method, FedINIBoost borrows the idea of gradient match to constructEM. Specifically, FedINIBoost builds a proxy dataset of the real dataset in twosteps for each participant at each communication round. Then the serveraggregates all the proxy datasets to form a central dummy dataset, which isused to finetune aggregated global model. Extensive experiments verify thesuperiority of our method compared with the existing classical method, FedAVG,FedProx, Moon and FedFTG. Moreover, FedINIBoost plays a significant role infinetuning the performance of aggregated global model at the initial stage ofFL.</description><author>Cheng Yang, Xue Yang, Dongxian Wu, Xiaohu Tang</author><pubDate>Thu, 29 Jun 2023 09:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12088v3</guid></item><item><title>A Survey on Datasets for Decision-making of Autonomous Vehicle</title><link>http://arxiv.org/abs/2306.16784v1</link><description>Autonomous vehicles (AV) are expected to reshape future transportationsystems, and decision-making is one of the critical modules toward high-levelautomated driving. To overcome those complicated scenarios that rule-basedmethods could not cope with well, data-driven decision-making approaches havearoused more and more focus. The datasets to be used in developing data-drivenmethods dramatically influences the performance of decision-making, hence it isnecessary to have a comprehensive insight into the existing datasets. From theaspects of collection sources, driving data can be divided into vehicle,environment, and driver related data. This study compares the state-of-the-artdatasets of these three categories and summarizes their features includingsensors used, annotation, and driving scenarios. Based on the characteristicsof the datasets, this survey also concludes the potential applications ofdatasets on various aspects of AV decision-making, assisting researchers tofind appropriate ones to support their own research. The future trends of AVdataset development are summarized.</description><author>Yuning Wang, Zeyu Han, Yining Xing, Shaobing Xu, Jianqiang Wang</author><pubDate>Thu, 29 Jun 2023 09:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16784v1</guid></item><item><title>Low-Light Enhancement in the Frequency Domain</title><link>http://arxiv.org/abs/2306.16782v1</link><description>Decreased visibility, intensive noise, and biased color are the commonproblems existing in low-light images. These visual disturbances further reducethe performance of high-level vision tasks, such as object detection, andtracking. To address this issue, some image enhancement methods have beenproposed to increase the image contrast. However, most of them are implementedonly in the spatial domain, which can be severely influenced by noise signalswhile enhancing. Hence, in this work, we propose a novel residual recurrentmulti-wavelet convolutional neural network R2-MWCNN learned in the frequencydomain that can simultaneously increase the image contrast and reduce noisesignals well. This end-to-end trainable network utilizes a multi-level discretewavelet transform to divide input feature maps into distinct frequencies,resulting in a better denoise impact. A channel-wise loss function is proposedto correct the color distortion for more realistic results. Extensiveexperiments demonstrate that our proposed R2-MWCNN outperforms thestate-of-the-art methods quantitively and qualitatively.</description><author>Hao Chen, Zhi Jin</author><pubDate>Thu, 29 Jun 2023 09:39:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16782v1</guid></item><item><title>Deep Learning for Energy Time-Series Analysis and Forecasting</title><link>http://arxiv.org/abs/2306.09129v2</link><description>Energy time-series analysis describes the process of analyzing past energyobservations and possibly external factors so as to predict the future.Different tasks are involved in the general field of energy time-seriesanalysis and forecasting, with electric load demand forecasting, personalizedenergy consumption forecasting, as well as renewable energy generationforecasting being among the most common ones. Following the exceptionalperformance of Deep Learning (DL) in a broad area of vision tasks, DL modelshave successfully been utilized in time-series forecasting tasks. This paperaims to provide insight into various DL methods geared towards improving theperformance in energy time-series forecasting tasks, with special emphasis inGreek Energy Market, and equip the reader with the necessary knowledge to applythese methods in practice.</description><author>Maria Tzelepi, Charalampos Symeonidis, Paraskevi Nousi, Efstratios Kakaletsis, Theodoros Manousis, Pavlos Tosidis, Nikos Nikolaidis, Anastasios Tefas</author><pubDate>Thu, 29 Jun 2023 09:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09129v2</guid></item><item><title>Graph Sampling-based Meta-Learning for Molecular Property Prediction</title><link>http://arxiv.org/abs/2306.16780v1</link><description>Molecular property is usually observed with a limited number of samples, andresearchers have considered property prediction as a few-shot problem. Oneimportant fact that has been ignored by prior works is that each molecule canbe recorded with several different properties simultaneously. To effectivelyutilize many-to-many correlations of molecules and properties, we propose aGraph Sampling-based Meta-learning (GS-Meta) framework for few-shot molecularproperty prediction. First, we construct a Molecule-Property relation Graph(MPG): molecule and properties are nodes, while property labels decide edges.Then, to utilize the topological information of MPG, we reformulate an episodein meta-learning as a subgraph of the MPG, containing a target property node,molecule nodes, and auxiliary property nodes. Third, as episodes in the form ofsubgraphs are no longer independent of each other, we propose to schedule thesubgraph sampling process with a contrastive loss function, which considers theconsistency and discrimination of subgraphs. Extensive experiments on 5commonly-used benchmarks show GS-Meta consistently outperforms state-of-the-artmethods by 5.71%-6.93% in ROC-AUC and verify the effectiveness of each proposedmodule. Our code is available at https://github.com/HICAI-ZJU/GS-Meta.</description><author>Xiang Zhuang, Qiang Zhang, Bin Wu, Keyan Ding, Yin Fang, Huajun Chen</author><pubDate>Thu, 29 Jun 2023 09:34:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16780v1</guid></item><item><title>A Generalized Multi-Modal Fusion Detection Framework</title><link>http://arxiv.org/abs/2303.07064v2</link><description>LiDAR point clouds have become the most common data source in autonomousdriving. However, due to the sparsity of point clouds, accurate and reliabledetection cannot be achieved in specific scenarios. Because of theircomplementarity with point clouds, images are getting increasing attention.Although with some success, existing fusion methods either perform hard fusionor do not fuse in a direct manner. In this paper, we propose a generic 3Ddetection framework called MMFusion, using multi-modal features. The frameworkaims to achieve accurate fusion between LiDAR and images to improve 3Ddetection in complex scenes. Our framework consists of two separate streams:the LiDAR stream and the camera stream, which can be compatible with anysingle-modal feature extraction network. The Voxel Local Perception Module inthe LiDAR stream enhances local feature representation, and then theMulti-modal Feature Fusion Module selectively combines feature output fromdifferent streams to achieve better fusion. Extensive experiments have shownthat our framework not only outperforms existing benchmarks but also improvestheir detection, especially for detecting cyclists and pedestrians on KITTIbenchmarks, with strong robustness and generalization capabilities. Hopefully,our work will stimulate more research into multi-modal fusion for autonomousdriving tasks.</description><author>Leichao Cui, Xiuxian Li, Min Meng, Xiaoyu Mo</author><pubDate>Thu, 29 Jun 2023 09:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07064v2</guid></item><item><title>Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages</title><link>http://arxiv.org/abs/2306.16774v1</link><description>Vision-Language Pre-training (VLP) has advanced the performance of manyvision-language tasks, such as image-text retrieval, visual entailment, andvisual reasoning. The pre-training mostly utilizes lexical databases and imagequeries in English. Previous work has demonstrated that the pre-training inEnglish does not transfer well to other languages in a zero-shot setting.However, multilingual pre-trained language models (MPLM) have excelled at avariety of single-modal language tasks. In this paper, we propose a simple yetefficient approach to adapt VLP to unseen languages using MPLM. We utilize across-lingual contextualized token embeddings alignment approach to train textencoders for non-English languages. Our approach does not require image inputand primarily uses machine translation, eliminating the need for targetlanguage data. Our evaluation across three distinct tasks (image-textretrieval, visual entailment, and natural language visual reasoning)demonstrates that this approach outperforms the state-of-the-art multilingualvision-language models without requiring large parallel corpora. Our code isavailable at https://github.com/Yasminekaroui/CliCoTea.</description><author>Yasmine Karoui, Rémi Lebret, Negar Foroutan, Karl Aberer</author><pubDate>Thu, 29 Jun 2023 09:20:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16774v1</guid></item><item><title>Learning from Synthetic Human Group Activities</title><link>http://arxiv.org/abs/2306.16772v1</link><description>The understanding of complex human interactions and group activities hasgarnered attention in human-centric computer vision. However, the advancementof the related tasks is hindered due to the difficulty of obtaining large-scalelabeled real-world datasets. To mitigate the issue, we propose M3Act, amulti-view multi-group multi-person human atomic action and group activity datagenerator. Powered by the Unity engine, M3Act contains simulation-ready 3Dscenes and human assets, configurable lighting and camera systems, highlyparameterized modular group activities, and a large degree of domainrandomization during the data generation process. Our data generator is capableof generating large-scale datasets of human activities with multipleviewpoints, modalities (RGB images, 2D poses, 3D motions), and high-qualityannotations for individual persons and multi-person groups (2D bounding boxes,instance segmentation masks, individual actions and group activity categories).Using M3Act, we perform synthetic data pre-training for 2D skeleton-based groupactivity recognition and RGB-based multi-person pose tracking. The resultsindicate that learning from our synthetic datasets largely improves the modelperformances on real-world datasets, with the highest gain of 5.59% and 7.32%respectively in group and person recognition accuracy on CAD2, as well as animprovement of 6.63 in MOTP on HiEve. Pre-training with our synthetic data alsoleads to faster model convergence on downstream tasks (up to 6.8% faster).Moreover, M3Act opens new research problems for 3D group activity generation.We release M3Act3D, an 87.6-hour 3D motion dataset of human activities withlarger group sizes and higher complexity of inter-person interactions thanprevious multi-person datasets. We define multiple metrics and propose acompetitive baseline for the novel task.</description><author>Che-Jui Chang, Honglu Zhou, Parth Goel, Aditya Bhat, Seonghyeon Moon, Samuel S. Sohn, Sejong Yoon, Vladimir Pavlovic, Mubbasir Kapadia</author><pubDate>Thu, 29 Jun 2023 09:13:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16772v1</guid></item><item><title>DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations</title><link>http://arxiv.org/abs/2306.16770v1</link><description>In open-domain dialogue generation tasks, contexts and responses in mostdatasets are one-to-one mapped, violating an important many-to-manycharacteristic: a context leads to various responses, and a response answersmultiple contexts. Without such patterns, models poorly generalize and preferresponding safely. Many attempts have been made in either multi-turn settingsfrom a one-to-many perspective or in a many-to-many perspective but limited tosingle-turn settings. The major challenge to many-to-many augment multi-turndialogues is that discretely replacing each turn with semantic similaritybreaks fragile context coherence. In this paper, we propose DialoGue PathSampling (DialoGPS) method in continuous semantic space, the first many-to-manyaugmentation method for multi-turn dialogues. Specifically, we map a dialogueto our extended Brownian Bridge, a special Gaussian process. We sample latentvariables to form coherent dialogue paths in the continuous space. A dialoguepath corresponds to a new multi-turn dialogue and is used as augmented trainingdata. We show the effect of DialoGPS with both automatic and human evaluation.</description><author>Ang Lv, Jinpeng Li, Yuhan Chen, Xing Gao, Ji Zhang, Rui Yan</author><pubDate>Thu, 29 Jun 2023 09:12:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16770v1</guid></item><item><title>Performance Analysis of DNN Inference/Training with Convolution and non-Convolution Operations</title><link>http://arxiv.org/abs/2306.16767v1</link><description>Today's performance analysis frameworks for deep learning accelerators sufferfrom two significant limitations. First, although modern convolutional neuralnetwork (CNNs) consist of many types of layers other than convolution,especially during training, these frameworks largely focus on convolutionlayers only. Second, these frameworks are generally targeted towards inference,and lack support for training operations. This work proposes a novelperformance analysis framework, SimDIT, for general ASIC-based systolichardware accelerator platforms. The modeling effort of SimDIT comprehensivelycovers convolution and non-convolution operations of both CNN inference andtraining on a highly parameterizable hardware substrate. SimDIT is integratedwith a backend silicon implementation flow and provides detailed end-to-endperformance statistics (i.e., data access cost, cycle counts, energy, andpower) for executing CNN inference and training workloads. SimDIT-enabledperformance analysis reveals that on a 64X64 processing array, non-convolutionoperations constitute 59.5% of total runtime for ResNet-50 training workload.In addition, by optimally distributing available off-chip DRAM bandwidth andon-chip SRAM resources, SimDIT achieves 18X performance improvement over ageneric static resource allocation for ResNet-50 inference.</description><author>Hadi Esmaeilzadeh, Soroush Ghodrati, Andrew B. Kahng, Sean Kinzer, Susmita Dey Manasi, Sachin S. Sapatnekar, Zhiang Wang</author><pubDate>Thu, 29 Jun 2023 09:11:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16767v1</guid></item><item><title>Unified Language Representation for Question Answering over Text, Tables, and Images</title><link>http://arxiv.org/abs/2306.16762v1</link><description>When trying to answer complex questions, people often rely on multiplesources of information, such as visual, textual, and tabular data. Previousapproaches to this problem have focused on designing input features or modelstructure in the multi-modal space, which is inflexible for cross-modalreasoning or data-efficient training. In this paper, we call for an alternativeparadigm, which transforms the images and tables into unified languagerepresentations, so that we can simplify the task into a simpler textual QAproblem that can be solved using three steps: retrieval, ranking, andgeneration, all within a language space. This idea takes advantage of the powerof pre-trained language models and is implemented in a framework called Solar.Our experimental results show that Solar outperforms all existing methods by10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten differentmetrics. Additionally, Solar achieves the best performance on the WebQAleaderboard</description><author>Bowen Yu, Cheng Fu, Haiyang Yu, Fei Huang, Yongbin Li</author><pubDate>Thu, 29 Jun 2023 09:02:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16762v1</guid></item><item><title>Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs</title><link>http://arxiv.org/abs/2306.16761v1</link><description>Recently, Ye et al. (Mathematical Programming 2023) designed an algorithm forsolving a specific class of bilevel programs with an emphasis on applicationsrelated to hyperparameter selection, utilizing the difference of convexalgorithm based on the value function approach reformulation. The proposedalgorithm is particularly powerful when the lower level problem is fully convex, such as a support vector machine model or a least absolute shrinkage andselection operator model. In this paper, to suit more applications related tomachine learning and statistics, we substantially weaken the underlyingassumption from lower level full convexity to weak convexity. Accordingly, wepropose a new reformulation using Moreau envelope of the lower level problemand demonstrate that this reformulation is a difference of weakly convexprogram. Subsequently, we develop a sequentially convergent algorithm forsolving this difference of weakly convex program. To evaluate the effectivenessof our approach, we conduct numerical experiments on the bilevel hyperparameterselection problem from elastic net, sparse group lasso, and RBF kernel supportvector machine models.</description><author>Lucy L. Gao, Jane J. Ye, Haian Yin, Shangzhi Zeng, Jin Zhang</author><pubDate>Thu, 29 Jun 2023 08:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16761v1</guid></item><item><title>Transfer Learning with Semi-Supervised Dataset Annotation for Birdcall Classification</title><link>http://arxiv.org/abs/2306.16760v1</link><description>We present working notes on transfer learning with semi-supervised datasetannotation for the BirdCLEF 2023 competition, focused on identifying Africanbird species in recorded soundscapes. Our approach utilizes existingoff-the-shelf models, BirdNET and MixIT, to address representation and labelingchallenges in the competition. We explore the embedding space learned byBirdNET and propose a process to derive an annotated dataset for supervisedlearning. Our experiments involve various models and feature engineeringapproaches to maximize performance on the competition leaderboard. The resultsdemonstrate the effectiveness of our approach in classifying bird species andhighlight the potential of transfer learning and semi-supervised datasetannotation in similar tasks.</description><author>Anthony Miyaguchi, Nathan Zhong, Murilo Gustineli, Chris Hayduk</author><pubDate>Thu, 29 Jun 2023 08:56:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16760v1</guid></item><item><title>An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning</title><link>http://arxiv.org/abs/2306.15786v2</link><description>The Rashomon Effect describes the following phenomenon: for a given datasetthere may exist many models with equally good performance but with differentsolution strategies. The Rashomon Effect has implications for ExplainableMachine Learning, especially for the comparability of explanations. We providea unified view on three different comparison scenarios and conduct aquantitative evaluation across different datasets, models, attribution methods,and metrics. We find that hyperparameter-tuning plays a role and that metricselection matters. Our results provide empirical support for previouslyanecdotal evidence and exhibit challenges for both scientists andpractitioners.</description><author>Sebastian Müller, Vanessa Toborek, Katharina Beckh, Matthias Jakobs, Christian Bauckhage, Pascal Welke</author><pubDate>Thu, 29 Jun 2023 08:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15786v2</guid></item><item><title>SaaFormer: Spectral-spatial Axial Aggregation Transformer for Hyperspectral Image Classification</title><link>http://arxiv.org/abs/2306.16759v1</link><description>Hyperspectral images (HSI) captured from earth observing satellites andaircraft is becoming increasingly important for applications in agriculture,environmental monitoring, mining, etc. Due to the limited availablehyperspectral datasets, the pixel-wise random sampling is the most commonlyused training-test dataset partition approach, which has significant overlapbetween samples in training and test datasets. Furthermore, our experimentalobservations indicates that regions with larger overlap often exhibit higherclassification accuracy. Consequently, the pixel-wise random sampling approachposes a risk of data leakage. Thus, we propose a block-wise sampling method tominimize the potential for data leakage. Our experimental findings also confirmthe presence of data leakage in models such as 2DCNN. Further, We propose aspectral-spatial axial aggregation transformer model, namely SaaFormer, toaddress the challenges associated with hyperspectral image classifier thatconsiders HSI as long sequential three-dimensional images. The model comprisestwo primary components: axial aggregation attention and multi-levelspectral-spatial extraction. The axial aggregation attention mechanismeffectively exploits the continuity and correlation among spectral bands ateach pixel position in hyperspectral images, while aggregating spatialdimension features. This enables SaaFormer to maintain high precision evenunder block-wise sampling. The multi-level spectral-spatial extractionstructure is designed to capture the sensitivity of different materialcomponents to specific spectral bands, allowing the model to focus on a broaderrange of spectral details. The results on six publicly available datasetsdemonstrate that our model exhibits comparable performance when using randomsampling, while significantly outperforming other methods when employingblock-wise sampling partition.</description><author>Enzhe Zhao, Zhichang Guo, Yao Li, Dazhi Zhang</author><pubDate>Thu, 29 Jun 2023 08:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16759v1</guid></item><item><title>N$^2$M$^2$: Learning Navigation for Arbitrary Mobile Manipulation Motions in Unseen and Dynamic Environments</title><link>http://arxiv.org/abs/2206.08737v2</link><description>Despite its importance in both industrial and service robotics, mobilemanipulation remains a significant challenge as it requires a seamlessintegration of end-effector trajectory generation with navigation skills aswell as reasoning over long-horizons. Existing methods struggle to control thelarge configuration space, and to navigate dynamic and unknown environments. Inprevious work, we proposed to decompose mobile manipulation tasks into asimplified motion generator for the end-effector in task space and a trainedreinforcement learning agent for the mobile base to account for kinematicfeasibility of the motion. In this work, we introduce Neural Navigation forMobile Manipulation (N$^2$M$^2$) which extends this decomposition to complexobstacle environments and enables it to tackle a broad range of tasks in realworld settings. The resulting approach can perform unseen, long-horizon tasksin unexplored environments while instantly reacting to dynamic obstacles andenvironmental changes. At the same time, it provides a simple way to define newmobile manipulation tasks. We demonstrate the capabilities of our proposedapproach in extensive simulation and real-world experiments on multiplekinematically diverse mobile manipulators. Code and videos are publiclyavailable at http://mobile-rl.cs.uni-freiburg.de.</description><author>Daniel Honerkamp, Tim Welschehold, Abhinav Valada</author><pubDate>Thu, 29 Jun 2023 08:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08737v2</guid></item><item><title>Eigensubspace of Temporal-Difference Dynamics and How It Improves Value Approximation in Reinforcement Learning</title><link>http://arxiv.org/abs/2306.16750v1</link><description>We propose a novel value approximation method, namely EigensubspaceRegularized Critic (ERC) for deep reinforcement learning (RL). ERC is motivatedby an analysis of the dynamics of Q-value approximation error in theTemporal-Difference (TD) method, which follows a path defined by the1-eigensubspace of the transition kernel associated with the Markov DecisionProcess (MDP). It reveals a fundamental property of TD learning that hasremained unused in previous deep RL approaches. In ERC, we propose aregularizer that guides the approximation error tending towards the1-eigensubspace, resulting in a more efficient and stable path of valueapproximation. Moreover, we theoretically prove the convergence of the ERCmethod. Besides, theoretical analysis and experiments demonstrate that ERCeffectively reduces the variance of value functions. Among 26 tasks in theDMControl benchmark, ERC outperforms state-of-the-art methods for 20. Besides,it shows significant advantages in Q-value approximation and variancereduction. Our code is available at https://sites.google.com/view/erc-ecml23/.</description><author>Qiang He, Tianyi Zhou, Meng Fang, Setareh Maghsudi</author><pubDate>Thu, 29 Jun 2023 08:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16750v1</guid></item><item><title>Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train</title><link>http://arxiv.org/abs/2306.16741v1</link><description>Foundation models have exhibited remarkable success in various applications,such as disease diagnosis and text report generation. To date, a foundationmodel for endoscopic video analysis is still lacking. In this paper, we proposeEndo-FM, a foundation model specifically developed using massive endoscopicvideo data. First, we build a video transformer, which captures both local andglobal long-range dependencies across spatial and temporal dimensions. Second,we pre-train our transformer model using global and local views via aself-supervised manner, aiming to make it robust to spatial-temporal variationsand discriminative across different scenes. To develop the foundation model, weconstruct a large-scale endoscopy video dataset by combining 9 publiclyavailable datasets and a privately collected dataset from Baoshan Branch ofRenji Hospital in Shanghai, China. Our dataset overall consists of over 33Kvideo clips with up to 5 million frames, encompassing various protocols, targetorgans, and disease types. Our pre-trained Endo-FM can be easily adopted for agiven downtream task via fine-tuning by serving as the backbone. Withexperiments on 3 different types of downstream tasks, including classification,segmentation, and detection, our Endo-FM surpasses the current state-of-the-artself-supervised pre-training and adapter-based transfer learning methods by asignificant margin, such as VCL (3.1% F1 for classification, 4.8% Dice forsegmentation, and 5.5% F1 for detection) and ST-Adapter (5.9% F1 forclassification, 9.6% Dice for segmentation, and 9.9% F1 for detection). Code,datasets, and models are released at https://github.com/med-air/Endo-FM.</description><author>Zhao Wang, Chang Liu, Shaoting Zhang, Qi Dou</author><pubDate>Thu, 29 Jun 2023 08:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16741v1</guid></item><item><title>Principles and Guidelines for Evaluating Social Robot Navigation Algorithms</title><link>http://arxiv.org/abs/2306.16740v1</link><description>A major challenge to deploying robots widely is navigation in human-populatedenvironments, commonly referred to as social robot navigation. While the fieldof social navigation has advanced tremendously in recent years, the fairevaluation of algorithms that tackle social navigation remains hard because itinvolves not just robotic agents moving in static environments but also dynamichuman agents and their perceptions of the appropriateness of robot behavior. Incontrast, clear, repeatable, and accessible benchmarks have acceleratedprogress in fields like computer vision, natural language processing andtraditional robot navigation by enabling researchers to fairly comparealgorithms, revealing limitations of existing solutions and illuminatingpromising new directions. We believe the same approach can benefit socialnavigation. In this paper, we pave the road towards common, widely accessible,and repeatable benchmarking criteria to evaluate social robot navigation. Ourcontributions include (a) a definition of a socially navigating robot as onethat respects the principles of safety, comfort, legibility, politeness, socialcompetency, agent understanding, proactivity, and responsiveness to context,(b) guidelines for the use of metrics, development of scenarios, benchmarks,datasets, and simulators to evaluate social navigation, and (c) a design of asocial navigation metrics framework to make it easier to compare results fromdifferent simulators, robots and datasets.</description><author>Anthony Francis, Claudia Perez-D'Arpino, Chengshu Li, Fei Xia, Alexandre Alahi, Rachid Alami, Aniket Bera, Abhijat Biswas, Joydeep Biswas, Rohan Chandra, Hao-Tien Lewis Chiang, Michael Everett, Sehoon Ha, Justin Hart, Jonathan P. How, Haresh Karnan, Tsang-Wei Edward Lee, Luis J. Manso, Reuth Mirksy, Soeren Pirk, Phani Teja Singamaneni, Peter Stone, Ada V. Taylor, Peter Trautman, Nathan Tsoi, Marynel Vazquez, Xuesu Xiao, Peng Xu, Naoki Yokoyama, Alexander Toshev, Roberto Martin-Martin</author><pubDate>Thu, 29 Jun 2023 08:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16740v1</guid></item><item><title>Towards Optimal Randomized Strategies in Adversarial Example Game</title><link>http://arxiv.org/abs/2306.16738v1</link><description>The vulnerability of deep neural network models to adversarial exampleattacks is a practical challenge in many artificial intelligence applications.A recent line of work shows that the use of randomization in adversarialtraining is the key to find optimal strategies against adversarial exampleattacks. However, in a fully randomized setting where both the defender and theattacker can use randomized strategies, there are no efficient algorithm forfinding such an optimal strategy. To fill the gap, we propose the firstalgorithm of its kind, called FRAT, which models the problem with a newinfinite-dimensional continuous-time flow on probability distribution spaces.FRAT maintains a lightweight mixture of models for the defender, withflexibility to efficiently update mixing weights and model parameters at eachiteration. Furthermore, FRAT utilizes lightweight sampling subroutines toconstruct a random strategy for the attacker. We prove that the continuous-timelimit of FRAT converges to a mixed Nash equilibria in a zero-sum game formed bya defender and an attacker. Experimental results also demonstrate theefficiency of FRAT on CIFAR-10 and CIFAR-100 datasets.</description><author>Jiahao Xie, Chao Zhang, Weijie Liu, Wensong Bai, Hui Qian</author><pubDate>Thu, 29 Jun 2023 08:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16738v1</guid></item><item><title>GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction</title><link>http://arxiv.org/abs/2306.16736v1</link><description>Demystifying complex human-ground interactions is essential for accurate andrealistic 3D human motion reconstruction from RGB videos, as it ensuresconsistency between the humans and the ground plane. Prior methods have modeledhuman-ground interactions either implicitly or in a sparse manner, oftenresulting in unrealistic and incorrect motions when faced with noise anduncertainty. In contrast, our approach explicitly represents these interactionsin a dense and continuous manner. To this end, we propose a novel Ground-awareMotion Model for 3D Human Motion Reconstruction, named GraMMaR, which jointlylearns the distribution of transitions in both pose and interaction betweenevery joint and ground plane at each time step of a motion sequence. It istrained to explicitly promote consistency between the motion and distancechange towards the ground. After training, we establish a joint optimizationstrategy that utilizes GraMMaR as a dual-prior, regularizing the optimizationtowards the space of plausible ground-aware motions. This leads to realisticand coherent motion reconstruction, irrespective of the assumed or learnedground plane. Through extensive evaluation on the AMASS and AIST++ datasets,our model demonstrates good generalization and discriminating abilities inchallenging cases including complex and ambiguous human-ground interactions.The code will be released.</description><author>Sihan Ma, Qiong Cao, Hongwei Yi, Jing Zhang, Dacheng Tao</author><pubDate>Thu, 29 Jun 2023 08:22:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16736v1</guid></item><item><title>Unified View of Damage leaves Planimetry &amp; Analysis Using Digital Images Processing Techniques</title><link>http://arxiv.org/abs/2306.16734v1</link><description>The detection of leaf diseases in plants generally involves visualobservation of patterns appearing on the leaf surface. However, there are manydiseases that are distinguished based on very subtle changes in these visuallyobservable patterns. This paper attempts to identify plant leaf diseases usingimage processing techniques. The focus of this study is on the detection ofcitrus leaf canker disease. Canker is a bacterial infection of leaves. Symptomsof citrus cankers include brown spots on the leaves, often with a watery oroily appearance. The spots (called lesions in botany) are usually yellow. It issurrounded by a halo of the leaves and is found on both the top and bottom ofthe leaf. This paper describes various methods that have been used to detectcitrus leaf canker disease. The methods used are histogram comparison andk-means clustering. Using these methods, citrus canker development was detectedbased on histograms generated based on leaf patterns. The results thus obtainedcan be used, after consultation with experts in the field of agriculture, toidentify suitable treatments for the processes used.</description><author>Pijush Kanti Kumar, DeepKiran Munjal, Sunita Rani, Anurag Dutta, Liton Chandra Voumik, A. Ramamoorthy</author><pubDate>Thu, 29 Jun 2023 08:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16734v1</guid></item><item><title>Multi-Scenario Ranking with Adaptive Feature Learning</title><link>http://arxiv.org/abs/2306.16732v1</link><description>Recently, Multi-Scenario Learning (MSL) is widely used in recommendation andretrieval systems in the industry because it facilitates transfer learning fromdifferent scenarios, mitigating data sparsity and reducing maintenance cost.These efforts produce different MSL paradigms by searching more optimal networkstructure, such as Auxiliary Network, Expert Network, and Multi-Tower Network.It is intuitive that different scenarios could hold their specificcharacteristics, activating the user's intents quite differently. In otherwords, different kinds of auxiliary features would bear varying importanceunder different scenarios. With more discriminative feature representationsrefined in a scenario-aware manner, better ranking performance could be easilyobtained without expensive search for the optimal network structure.Unfortunately, this simple idea is mainly overlooked but much desired inreal-world systems.Further analysis also validates the rationality of adaptivefeature learning under a multi-scenario scheme. Moreover, our A/B test resultson the Alibaba search advertising platform also demonstrate that Maria issuperior in production environments.</description><author>Yu Tian, Bofang Li, Si Chen, Xubin Li, Hongbo Deng, Jian Xu, Bo Zheng, Qian Wang, Chenliang Li</author><pubDate>Thu, 29 Jun 2023 08:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16732v1</guid></item><item><title>Language Models as Knowledge Embeddings</title><link>http://arxiv.org/abs/2206.12617v3</link><description>Knowledge embeddings (KE) represent a knowledge graph (KG) by embeddingentities and relations into continuous vector spaces. Existing methods aremainly structure-based or description-based. Structure-based methods learnrepresentations that preserve the inherent structure of KGs. They cannot wellrepresent abundant long-tail entities in real-world KGs with limited structuralinformation. Description-based methods leverage textual information andlanguage models. Prior approaches in this direction barely outperformstructure-based ones, and suffer from problems like expensive negative samplingand restrictive description demand. In this paper, we propose LMKE, whichadopts Language Models to derive Knowledge Embeddings, aiming at both enrichingrepresentations of long-tail entities and solving problems of priordescription-based methods. We formulate description-based KE learning with acontrastive learning framework to improve efficiency in training andevaluation. Experimental results show that LMKE achieves state-of-the-artperformance on KE benchmarks of link prediction and triple classification,especially for long-tail entities.</description><author>Xintao Wang, Qianyu He, Jiaqing Liang, Yanghua Xiao</author><pubDate>Thu, 29 Jun 2023 08:11:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12617v3</guid></item><item><title>Covariance-aware Feature Alignment with Pre-computed Source Statistics for Test-time Adaptation to Multiple Image Corruptions</title><link>http://arxiv.org/abs/2204.13263v2</link><description>Real-world image recognition systems often face corrupted input images, whichcause distribution shifts and degrade the performance of models. These systemsoften use a single prediction model in a central server and process images sentfrom various environments, such as cameras distributed in cities or cars. Suchsingle models face images corrupted in heterogeneous ways in test time. Thus,they require to instantly adapt to the multiple corruptions during testingrather than being re-trained at a high cost. Test-time adaptation (TTA), whichaims to adapt models without accessing the training dataset, is one of thesettings that can address this problem. Existing TTA methods indeed work wellon a single corruption. However, the adaptation ability is limited whenmultiple types of corruption occur, which is more realistic. We hypothesizethis is because the distribution shift is more complicated, and the adaptationbecomes more difficult in case of multiple corruptions. In fact, weexperimentally found that a larger distribution gap remains after TTA. Toaddress the distribution gap during testing, we propose a novel TTA methodnamed Covariance-Aware Feature alignment (CAFe). We empirically show that CAFeoutperforms prior TTA methods on image corruptions, including multiple types ofcorruptions.</description><author>Kazuki Adachi, Shin'ya Yamaguchi, Atsutoshi Kumagai</author><pubDate>Thu, 29 Jun 2023 08:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.13263v2</guid></item><item><title>Improving Identity-Robustness for Face Models</title><link>http://arxiv.org/abs/2304.03838v2</link><description>Despite the success of deep-learning models in many tasks, there have beenconcerns about such models learning shortcuts, and their lack of robustness toirrelevant confounders. When it comes to models directly trained on humanfaces, a sensitive confounder is that of human identities. Many face-relatedtasks should ideally be identity-independent, and perform uniformly acrossdifferent individuals (i.e. be fair). One way to measure and enforce suchrobustness and performance uniformity is through enforcing it during training,assuming identity-related information is available at scale. However, due toprivacy concerns and also the cost of collecting such information, this isoften not the case, and most face datasets simply contain input images andtheir corresponding task-related labels. Thus, improving identity-relatedrobustness without the need for such annotations is of great importance. Here,we explore using face-recognition embedding vectors, as proxies for identities,to enforce such robustness. We propose to use the structure in theface-recognition embedding space, to implicitly emphasize rare samples withineach class. We do so by weighting samples according to their conditionalinverse density (CID) in the proxy embedding space. Our experiments suggestthat such a simple sample weighting scheme, not only improves the trainingrobustness, it often improves the overall performance as a result of suchrobustness. We also show that employing such constraints during trainingresults in models that are significantly less sensitive to different levels ofbias in the dataset.</description><author>Qi Qi, Shervin Ardeshir</author><pubDate>Thu, 29 Jun 2023 08:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03838v2</guid></item><item><title>Learned Video Compression via Heterogeneous Deformable Compensation Network</title><link>http://arxiv.org/abs/2207.04589v3</link><description>Learned video compression has recently emerged as an essential research topicin developing advanced video compression technologies, where motioncompensation is considered one of the most challenging issues. In this paper,we propose a learned video compression framework via heterogeneous deformablecompensation strategy (HDCVC) to tackle the problems of unstable compressionperformance caused by single-size deformable kernels in downsampled featuredomain. More specifically, instead of utilizing optical flow warping orsingle-size-kernel deformable alignment, the proposed algorithm extractsfeatures from the two adjacent frames to estimate content-adaptiveheterogeneous deformable (HetDeform) kernel offsets. Then we transform thereference features with the HetDeform convolution to accomplish motioncompensation. Moreover, we design a Spatial-Neighborhood-Conditioned DivisiveNormalization (SNCDN) to achieve more effective data Gaussianization combinedwith the Generalized Divisive Normalization. Furthermore, we propose amulti-frame enhanced reconstruction module for exploiting context and temporalinformation for final quality enhancement. Experimental results indicate thatHDCVC achieves superior performance than the recent state-of-the-art learnedvideo compression approaches.</description><author>Huairui Wang, Zhenzhong Chen, Chang Wen Chen</author><pubDate>Thu, 29 Jun 2023 08:05:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.04589v3</guid></item><item><title>Bridging the Gap: Neural Collapse Inspired Prompt Tuning for Generalization under Class Imbalance</title><link>http://arxiv.org/abs/2306.15955v2</link><description>Large-scale vision-language (V-L) models have demonstrated remarkablegeneralization capabilities for downstream tasks through prompt tuning.However, their performance suffers significantly in the presence of classimbalance, a common issue in real-world scenarios. In this paper, weinvestigate the effects of class imbalance on the generalization performance ofV-L models and extend Neural Collapse phenomenon to these models, revealing thegeometric reasons behind the impact of class imbalance on their generalizationability. To address this problem, we propose Neural Collapse based PromptTuning (NPT), a novel method that optimizes prompts so that both text and imagefeatures satisfy the same simplex ETF structure. NPT incorporates tworegularization terms, geometric de-biasing and multi-modal isomorphism, toenhance the robustness of V-L models under class imbalance conditions whilemaintaining their generalization capabilities. Our comprehensive experimentsshow that NPT outperforms existing prompt learning techniques across 11 diverseimage recognition datasets, achieving an absolute average gain of 2.63\% fornovel classes and 2.47\% for harmonic mean when facing imbalanced data.</description><author>Didi Zhu, Yinchuan Li, Min Zhang, Junkun Yuan, Jiashuo Liu, Zexi Li, Kun Kuang, Chao Wu</author><pubDate>Thu, 29 Jun 2023 08:02:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15955v2</guid></item><item><title>A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023</title><link>http://arxiv.org/abs/2306.16125v2</link><description>This paper describes our participation in the MentalRiskES task at IberLEF2023. The task involved predicting the likelihood of an individual experiencingdepression based on their social media activity. The dataset consisted ofconversations from 175 Telegram users, each labeled according to their evidenceof suffering from the disorder. We used a combination of traditional machinelearning and deep learning techniques to solve four predictive subtasks: binaryclassification, simple regression, multiclass classification, and multi-outputregression. We approached this by training a model to solve the multi-output regressioncase and then transforming the predictions to work for the other threesubtasks. We compare the performance of two modeling approaches: fine-tuning aBERT-based model directly for the task or using its embeddings as inputs to alinear regressor, with the latter yielding better results. The code toreproduce our results can be found at:https://github.com/simonsanvil/EarlyDepression-MentalRiskES</description><author>Simon Sanchez Viloria, Daniel Peix del Río, Rubén Bermúdez Cabo, Guillermo Arturo Arrojo Fuentes, Isabel Segura-Bedmar</author><pubDate>Thu, 29 Jun 2023 08:02:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16125v2</guid></item><item><title>A Restarted Large-Scale Spectral Clustering with Self-Guiding and Block Diagonal Representation</title><link>http://arxiv.org/abs/2306.15138v2</link><description>Spectral clustering is one of the most popular unsupervised machine learningmethods. Constructing similarity matrix is crucial to this type of method. Inmost existing works, the similarity matrix is computed once for all or isupdated alternatively. However, the former is difficult to reflectcomprehensive relationships among data points, and the latter is time-consumingand is even infeasible for large-scale problems. In this work, we propose arestarted clustering framework with self-guiding and block diagonalrepresentation. An advantage of the strategy is that some useful clusteringinformation obtained from previous cycles could be preserved as much aspossible. To the best of our knowledge, this is the first work that appliesrestarting strategy to spectral clustering. The key difference is that wereclassify the samples in each cycle of our method, while they are classifiedonly once in existing methods. To further release the overhead, we introduce ablock diagonal representation with Nystr\"{o}m approximation for constructingthe similarity matrix. Theoretical results are established to show therationality of inexact computations in spectral clustering. Comprehensiveexperiments are performed on some benchmark databases, which show thesuperiority of our proposed algorithms over many state-of-the-art algorithmsfor large-scale problems. Specifically, our framework has a potential boost forclustering algorithms and works well even using an initial guess chosenrandomly.</description><author>Yongyan Guo, Gang Wu</author><pubDate>Thu, 29 Jun 2023 07:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15138v2</guid></item><item><title>Evaluating Paraphrastic Robustness in Textual Entailment Models</title><link>http://arxiv.org/abs/2306.16722v1</link><description>We present PaRTE, a collection of 1,126 pairs of Recognizing TextualEntailment (RTE) examples to evaluate whether models are robust toparaphrasing. We posit that if RTE models understand language, theirpredictions should be consistent across inputs that share the same meaning. Weuse the evaluation set to determine if RTE models' predictions change whenexamples are paraphrased. In our experiments, contemporary models change theirpredictions on 8-16\% of paraphrased examples, indicating that there is stillroom for improvement.</description><author>Dhruv Verma, Yash Kumar Lal, Shreyashee Sinha, Benjamin Van Durme, Adam Poliak</author><pubDate>Thu, 29 Jun 2023 07:48:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16722v1</guid></item></channel></rss>