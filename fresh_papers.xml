<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 04 Oct 2023 06:01:16 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DREAM: Visual Decoding from Reversing Human Visual System</title><link>http://arxiv.org/abs/2310.02265v1</link><description>In this work we present DREAM, an fMRI-to-image method for reconstructingviewed images from brain activities, grounded on fundamental knowledge of thehuman visual system. We craft reverse pathways that emulate the hierarchicaland parallel nature of how humans perceive the visual world. These tailoredpathways are specialized to decipher semantics, color, and depth cues from fMRIdata, mirroring the forward pathways from visual stimuli to fMRI recordings. Todo so, two components mimic the inverse processes within the human visualsystem: the Reverse Visual Association Cortex (R-VAC) which reverses pathwaysof this brain region, extracting semantics from fMRI data; the Reverse ParallelPKM (R-PKM) component simultaneously predicting color and depth from fMRIsignals. The experiments indicate that our method outperforms the currentstate-of-the-art models in terms of the consistency of appearance, structure,and semantics. Code will be made publicly available to facilitate furtherresearch in this field.</description><author>Weihao Xia, Raoul de Charette, Cengiz Öztireli, Jing-Hao Xue</author><pubDate>Tue, 03 Oct 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02265v1</guid></item><item><title>Generalizable Long-Horizon Manipulations with Large Language Models</title><link>http://arxiv.org/abs/2310.02264v1</link><description>This work introduces a framework harnessing the capabilities of LargeLanguage Models (LLMs) to generate primitive task conditions for generalizablelong-horizon manipulations with novel objects and unseen tasks. These taskconditions serve as guides for the generation and adjustment of DynamicMovement Primitives (DMP) trajectories for long-horizon task execution. Wefurther create a challenging robotic manipulation task suite based on Pybulletfor long-horizon task evaluation. Extensive experiments in both simulated andreal-world environments demonstrate the effectiveness of our framework on bothfamiliar tasks involving new objects and novel but related tasks, highlightingthe potential of LLMs in enhancing robotic system versatility and adaptability.Project website: https://object814.github.io/Task-Condition-With-LLM/</description><author>Haoyu Zhou, Mingyu Ding, Weikun Peng, Masayoshi Tomizuka, Lin Shao, Chuang Gan</author><pubDate>Tue, 03 Oct 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02264v1</guid></item><item><title>Contrastive Post-training Large Language Models on Data Curriculum</title><link>http://arxiv.org/abs/2310.02263v1</link><description>Alignment serves as an important step to steer large language models (LLMs)towards human preferences. In this paper, we explore contrastive post-trainingtechniques for alignment by automatically constructing preference pairs frommultiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). Wecarefully compare the contrastive techniques of SLiC and DPO to SFT baselinesand find that DPO provides a step-function improvement even after continueingSFT saturates. We also explore a data curriculum learning scheme forcontrastive post-training, which starts by learning from "easier" pairs andtransitioning to "harder" ones, which further improves alignment. Finally, wescale up our experiments to train with more data and larger models like Orca.Remarkably, contrastive post-training further improves the performance of Orca,already a state-of-the-art instruction learning model tuned with GPT-4 outputs,to exceed that of ChatGPT.</description><author>Canwen Xu, Corby Rosset, Luciano Del Corro, Shweti Mahajan, Julian McAuley, Jennifer Neville, Ahmed Hassan Awadallah, Nikhil Rao</author><pubDate>Tue, 03 Oct 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02263v1</guid></item><item><title>RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and Comfortable Autonomous Driving</title><link>http://arxiv.org/abs/2310.02262v1</link><description>This paper addresses the growing demands for safety and comfort inintelligent robot systems, particularly autonomous vehicles, where roadconditions play a pivotal role in overall driving performance. For example,reconstructing road surfaces helps to enhance the analysis and prediction ofvehicle responses for motion planning and control systems. We introduce theRoad Surface Reconstruction Dataset (RSRD), a real-world, high-resolution, andhigh-precision dataset collected with a specialized platform in diverse drivingconditions. It covers common road types containing approximately 16,000 pairsof stereo images, original point clouds, and ground-truth depth/disparity maps,with accurate post-processing pipelines to ensure its quality. Based on RSRD,we further build a comprehensive benchmark for recovering road profiles throughdepth estimation and stereo matching. Preliminary evaluations with variousstate-of-the-art methods reveal the effectiveness of our dataset and thechallenge of the task, underscoring substantial opportunities of RSRD as avaluable resource for advancing techniques, e.g., multi-view stereo towardssafe autonomous driving. The dataset and demo videos are available athttps://thu-rsxd.com/rsrd/</description><author>Tong Zhao, Chenfeng Xu, Mingyu Ding, Masayoshi Tomizuka, Wei Zhan, Yintao Wei</author><pubDate>Tue, 03 Oct 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02262v1</guid></item><item><title>TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View Radar Semantic Segmentation</title><link>http://arxiv.org/abs/2310.02260v1</link><description>Scene understanding plays an essential role in enabling autonomous drivingand maintaining high standards of performance and safety. To address this task,cameras and laser scanners (LiDARs) have been the most commonly used sensors,with radars being less popular. Despite that, radars remain low-cost,information-dense, and fast-sensing techniques that are resistant to adverseweather conditions. While multiple works have been previously presented forradar-based scene semantic segmentation, the nature of the radar data stillposes a challenge due to the inherent noise and sparsity, as well as thedisproportionate foreground and background. In this work, we propose a novelapproach to the semantic segmentation of radar scenes using a multi-inputfusion of radar data through a novel architecture and loss functions that aretailored to tackle the drawbacks of radar perception. Our novel architectureincludes an efficient attention block that adaptively captures importantfeature information. Our method, TransRadar, outperforms state-of-the-artmethods on the CARRADA and RADIal datasets while having smaller model sizes.https://github.com/YahiDar/TransRadar</description><author>Yahia Dalbah, Jean Lahoud, Hisham Cholakkal</author><pubDate>Tue, 03 Oct 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02260v1</guid></item><item><title>A Neural Scaling Law from Lottery Ticket Ensembling</title><link>http://arxiv.org/abs/2310.02258v1</link><description>Neural scaling laws (NSL) refer to the phenomenon where model performanceimproves with scale. Sharma &amp; Kaplan analyzed NSL using approximation theoryand predict that MSE losses decay as $N^{-\alpha}$, $\alpha=4/d$, where $N$ isthe number of model parameters, and $d$ is the intrinsic input dimension.Although their theory works well for some cases (e.g., ReLU networks), wesurprisingly find that a simple 1D problem $y=x^2$ manifests a differentscaling law ($\alpha=1$) from their predictions ($\alpha=4$). We opened theneural networks and found that the new scaling law originates from lotteryticket ensembling: a wider network on average has more "lottery tickets", whichare ensembled to reduce the variance of outputs. We support the ensemblingmechanism by mechanistically interpreting single neural networks, as well asstudying them statistically. We attribute the $N^{-1}$ scaling law to the"central limit theorem" of lottery tickets. Finally, we discuss its potentialimplications for large language models and statistical physics-type theories oflearning.</description><author>Ziming Liu, Max Tegmark</author><pubDate>Tue, 03 Oct 2023 18:58:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02258v1</guid></item><item><title>MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts</title><link>http://arxiv.org/abs/2310.02255v1</link><description>Although Large Language Models (LLMs) and Large Multimodal Models (LMMs)exhibit impressive skills in various domains, their ability for mathematicalreasoning within visual contexts has not been formally examined. Equipping LLMsand LMMs with this capability is vital for general-purpose AI assistants andshowcases promising potential in education, data analysis, and scientificdiscovery. To bridge this gap, we present MathVista, a benchmark designed toamalgamate challenges from diverse mathematical and visual tasks. We firsttaxonomize the key task types, reasoning skills, and visual contexts from theliterature to guide our selection from 28 existing math-focused and visualquestion answering datasets. Then, we construct three new datasets, IQTest,FunctionQA, and PaperQA, to accommodate for missing types of visual contexts.The problems featured often require deep visual understanding beyond OCR orimage captioning, and compositional reasoning with rich domain-specific tools,thus posing a notable challenge to existing models. We conduct a comprehensiveevaluation of 11 prominent open-source and proprietary foundation models (LLMs,LLMs augmented with tools, and LMMs), and early experiments with GPT-4V. Thebest-performing model, Multimodal Bard, achieves only 58% of human performance(34.8% vs 60.3%), indicating ample room for further improvement. Given thissignificant gap, MathVista fuels future research in the development ofgeneral-purpose AI agents capable of tackling mathematically intensive andvisually rich real-world tasks. Preliminary tests show that MathVista alsopresents challenges to GPT-4V, underscoring the benchmark's importance. Theproject is available at https://mathvista.github.io/.</description><author>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao</author><pubDate>Tue, 03 Oct 2023 18:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02255v1</guid></item><item><title>Learning unitaries with quantum statistical queries</title><link>http://arxiv.org/abs/2310.02254v1</link><description>We propose several algorithms for learning unitary operators from quantumstatistical queries (QSQs) with respect to their Choi-Jamiolkowski state.Quantum statistical queries capture the capabilities of a learner with limitedquantum resources, which receives as input only noisy estimates of expectedvalues of measurements. Our methods hinge on a novel technique for estimatingthe Fourier mass of a unitary on a subset of Pauli strings with a singlequantum statistical query, generalizing a previous result for uniform quantumexamples. Exploiting this insight, we show that the quantum Goldreich-Levinalgorithm can be implemented with quantum statistical queries, whereas theprior version of the algorithm involves oracle access to the unitary and itsinverse. Moreover, we prove that $\mathcal{O}(\log n)$-juntas and quantumBoolean functions with constant total influence are efficiently learnable inour model, and constant-depth circuits are learnable sample-efficiently withquantum statistical queries. On the other hand, all previous algorithms forthese tasks require direct access to the Choi-Jamiolkowski state or oracleaccess to the unitary. In addition, our upper bounds imply that the actions ofthose classes of unitaries on locally scrambled ensembles can be efficientlylearned. We also demonstrate that, despite these positive results, quantumstatistical queries lead to an exponentially larger sample complexity forcertain tasks, compared to separable measurements to the Choi-Jamiolkowskistate. In particular, we show an exponential lower bound for learning a classof phase-oracle unitaries and a double exponential lower bound for testing theunitarity of channels, adapting to our setting previous arguments for quantumstates. Finally, we propose a new definition of average-case surrogate models,showing a potential application of our results to hybrid quantum machinelearning.</description><author>Armando Angrisani</author><pubDate>Tue, 03 Oct 2023 18:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02254v1</guid></item><item><title>Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving</title><link>http://arxiv.org/abs/2310.02251v1</link><description>Talk2BEV is a large vision-language model (LVLM) interface for bird's-eyeview (BEV) maps in autonomous driving contexts. While existing perceptionsystems for autonomous driving scenarios have largely focused on a pre-defined(closed) set of object categories and driving scenarios, Talk2BEV blends recentadvances in general-purpose language and vision models with BEV-structured maprepresentations, eliminating the need for task-specific models. This enables asingle system to cater to a variety of autonomous driving tasks encompassingvisual and spatial reasoning, predicting the intents of traffic actors, anddecision-making based on visual cues. We extensively evaluate Talk2BEV on alarge number of scene understanding tasks that rely on both the ability tointerpret free-form natural language queries, and in grounding these queries tothe visual context embedded into the language-enhanced BEV map. To enablefurther research in LVLMs for autonomous driving scenarios, we develop andrelease Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEVscenarios, with more than 20,000 questions and ground-truth responses from theNuScenes dataset.</description><author>Vikrant Dewangan, Tushar Choudhary, Shivam Chandhok, Shubham Priyadarshan, Anushka Jain, Arun K. Singh, Siddharth Srivastava, Krishna Murthy Jatavallabhula, K. Madhava Krishna</author><pubDate>Tue, 03 Oct 2023 18:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02251v1</guid></item><item><title>Why do autoencoders work?</title><link>http://arxiv.org/abs/2310.02250v1</link><description>Deep neural network autoencoders are routinely used computationally for modelreduction. They allow recognizing the intrinsic dimension of data that lie in a$k$-dimensional subset $K$ of an input Euclidean space $\R^n$. The underlyingidea is to obtain both an encoding layer that maps $\R^n$ into $\R^k$ (calledthe bottleneck layer or the space of latent variables) and a decoding layerthat maps $\R^k$ back into $\R^n$, in such a way that the input data from theset $K$ is recovered when composing the two maps. This is achieved by adjustingparameters (weights) in the network to minimize the discrepancy between theinput and the reconstructed output. Since neural networks (with continuousactivation functions) compute continuous maps, the existence of a network thatachieves perfect reconstruction would imply that $K$ is homeomorphic to a$k$-dimensional subset of $\R^k$, so clearly there are topological obstructionsto finding such a network. On the other hand, in practice the technique isfound to ``work'' well, which leads one to ask if there is a way to explainthis effectiveness. We show that, up to small errors, indeed the method isguaranteed to work. This is done by appealing to certain facts fromdifferential geometry. A computational example is also included to illustratethe ideas.</description><author>Matthew D. Kvalheim, Eduardo D. Sontag</author><pubDate>Tue, 03 Oct 2023 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02250v1</guid></item><item><title>Harnessing Pre-Trained Sentence Transformers for Offensive Language Detection in Indian Languages</title><link>http://arxiv.org/abs/2310.02249v1</link><description>In our increasingly interconnected digital world, social media platforms haveemerged as powerful channels for the dissemination of hate speech and offensivecontent. This work delves into the domain of hate speech detection, placingspecific emphasis on three low-resource Indian languages: Bengali, Assamese,and Gujarati. The challenge is framed as a text classification task, aimed atdiscerning whether a tweet contains offensive or non-offensive content.Leveraging the HASOC 2023 datasets, we fine-tuned pre-trained BERT and SBERTmodels to evaluate their effectiveness in identifying hate speech. Our findingsunderscore the superiority of monolingual sentence-BERT models, particularly inthe Bengali language, where we achieved the highest ranking. However, theperformance in Assamese and Gujarati languages signifies ongoing opportunitiesfor enhancement. Our goal is to foster inclusive online spaces by counteringhate speech proliferation.</description><author>Ananya Joshi, Raviraj Joshi</author><pubDate>Tue, 03 Oct 2023 18:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02249v1</guid></item><item><title>Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances</title><link>http://arxiv.org/abs/2310.02246v1</link><description>Solving a linear system $Ax=b$ is a fundamental scientific computingprimitive for which numerous solvers and preconditioners have been developed.These come with parameters whose optimal values depend on the system beingsolved and are often impossible or too expensive to identify; thus in practicesub-optimal heuristics are used. We consider the common setting in which manyrelated linear systems need to be solved, e.g. during a single numericalsimulation. In this scenario, can we sequentially choose parameters that attaina near-optimal overall number of iterations, without extra matrix computations?We answer in the affirmative for Successive Over-Relaxation (SOR), a standardsolver whose parameter $\omega$ has a strong impact on its runtime. For thismethod, we prove that a bandit online learning algorithm -- using only thenumber of iterations as feedback -- can select parameters for a sequence ofinstances such that the overall cost approaches that of the best fixed $\omega$as the sequence length increases. Furthermore, when given additional structuralinformation, we show that a contextual bandit method asymptotically achievesthe performance of the instance-optimal policy, which selects the best $\omega$for each instance. Our work provides the first learning-theoretic treatment ofhigh-precision linear system solvers and the first end-to-end guarantees fordata-driven scientific computing, demonstrating theoretically the potential tospeed up numerical methods using well-understood learning algorithms.</description><author>Mikhail Khodak, Edmond Chow, Maria-Florina Balcan, Ameet Talwalkar</author><pubDate>Tue, 03 Oct 2023 18:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02246v1</guid></item><item><title>Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks</title><link>http://arxiv.org/abs/2310.02244v1</link><description>By classifying infinite-width neural networks and identifying the *optimal*limit, Tensor Programs IV and V demonstrated a universal way, called $\mu$P,for *widthwise hyperparameter transfer*, i.e., predicting optimalhyperparameters of wide neural networks from narrow ones. Here we investigatethe analogous classification for *depthwise parametrizations* of deep residualnetworks (resnets). We classify depthwise parametrizations of block multiplierand learning rate by their infinite-width-then-depth limits. In resnets whereeach block has only one layer, we identify a unique optimal parametrization,called Depth-$\mu$P that extends $\mu$P and show empirically it admitsdepthwise hyperparameter transfer. We identify *feature diversity* as a crucialfactor in deep networks, and Depth-$\mu$P can be characterized as maximizingboth feature learning and feature diversity. Exploiting this, we find thatabsolute value, among all homogeneous nonlinearities, maximizes featurediversity and indeed empirically leads to significant better performance.However, if each block is deeper (such as modern transformers), then we findfundamental limitations in all possible infinite-depth limits of suchparametrizations, which we illustrate both theoretically and empirically onsimple networks as well as Megatron transformer trained on Common Crawl.</description><author>Greg Yang, Dingli Yu, Chen Zhu, Soufiane Hayou</author><pubDate>Tue, 03 Oct 2023 18:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02244v1</guid></item><item><title>Learning quantum Hamiltonians at any temperature in polynomial time</title><link>http://arxiv.org/abs/2310.02243v1</link><description>We study the problem of learning a local quantum Hamiltonian $H$ given copiesof its Gibbs state $\rho = e^{-\beta H}/\textrm{tr}(e^{-\beta H})$ at a knowninverse temperature $\beta&gt;0$. Anshu, Arunachalam, Kuwahara, and Soleimanifar(arXiv:2004.07266) gave an algorithm to learn a Hamiltonian on $n$ qubits toprecision $\epsilon$ with only polynomially many copies of the Gibbs state, butwhich takes exponential time. Obtaining a computationally efficient algorithmhas been a major open problem [Alhambra'22 (arXiv:2204.08349)], [Anshu,Arunachalam'22 (arXiv:2204.08349)], with prior work only resolving this in thelimited cases of high temperature [Haah, Kothari, Tang'21 (arXiv:2108.04842)]or commuting terms [Anshu, Arunachalam, Kuwahara, Soleimanifar'21]. We fullyresolve this problem, giving a polynomial time algorithm for learning $H$ toprecision $\epsilon$ from polynomially many copies of the Gibbs state at anyconstant $\beta &gt; 0$. Our main technical contribution is a new flat polynomial approximation to theexponential function, and a translation between multi-variate scalarpolynomials and nested commutators. This enables us to formulate Hamiltonianlearning as a polynomial system. We then show that solving a low-degreesum-of-squares relaxation of this polynomial system suffices to accuratelylearn the Hamiltonian.</description><author>Ainesh Bakshi, Allen Liu, Ankur Moitra, Ewin Tang</author><pubDate>Tue, 03 Oct 2023 18:50:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02243v1</guid></item><item><title>Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models</title><link>http://arxiv.org/abs/2310.02242v1</link><description>This paper presents a novel approach to generating the 3D motion of a humaninteracting with a target object, with a focus on solving the challenge ofsynthesizing long-range and diverse motions, which could not be fulfilled byexisting auto-regressive models or path planning-based methods. We propose ahierarchical generation framework to solve this challenge. Specifically, ourframework first generates a set of milestones and then synthesizes the motionalong them. Therefore, the long-range motion generation could be reduced tosynthesizing several short motion sequences guided by milestones. Theexperiments on the NSM, COUCH, and SAMP datasets show that our approachoutperforms previous methods by a large margin in both quality and diversity.The source code is available on our project pagehttps://zju3dv.github.io/hghoi.</description><author>Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, Hujun Bao</author><pubDate>Tue, 03 Oct 2023 18:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02242v1</guid></item><item><title>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens</title><link>http://arxiv.org/abs/2310.02239v1</link><description>Large Language Models (LLMs) have garnered significant attention for theiradvancements in natural language processing, demonstrating unparalleled prowessin text comprehension and generation. Yet, the simultaneous generation ofimages with coherent textual narratives remains an evolving frontier. Inresponse, we introduce an innovative interleaved vision-and-language generationtechnique anchored by the concept of "generative vokens," acting as the bridgefor harmonized image-text outputs. Our approach is characterized by adistinctive two-staged training strategy focusing on description-freemultimodal generation, where the training requires no comprehensivedescriptions of images. To bolster model integrity, classifier-free guidance isincorporated, enhancing the effectiveness of vokens on image generation. Ourmodel, MiniGPT-5, exhibits substantial improvement over the baseline Divtermodel on the MMDialog dataset and consistently delivers superior or comparablemultimodal outputs in human evaluations on the VIST dataset, highlighting itsefficacy across diverse benchmarks.</description><author>Kaizhi Zheng, Xuehai He, Xin Eric Wang</author><pubDate>Tue, 03 Oct 2023 18:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02239v1</guid></item><item><title>On Data Fabrication in Collaborative Vehicular Perception: Attacks and Countermeasures</title><link>http://arxiv.org/abs/2309.12955v2</link><description>Collaborative perception, which greatly enhances the sensing capability ofconnected and autonomous vehicles (CAVs) by incorporating data from externalresources, also brings forth potential security risks. CAVs' driving decisionsrely on remote untrusted data, making them susceptible to attacks carried outby malicious participants in the collaborative perception system. However,security analysis and countermeasures for such threats are absent. Tounderstand the impact of the vulnerability, we break the ground by proposingvarious real-time data fabrication attacks in which the attacker deliverscrafted malicious data to victims in order to perturb their perception results,leading to hard brakes or increased collision risks. Our attacks demonstrate ahigh success rate of over 86% on high-fidelity simulated scenarios and arerealizable in real-world experiments. To mitigate the vulnerability, we presenta systematic anomaly detection approach that enables benign vehicles to jointlyreveal malicious fabrication. It detects 91.5% of attacks with a false positiverate of 3% in simulated scenarios and significantly mitigates attack impacts inreal-world scenarios.</description><author>Qingzhao Zhang, Shuowei Jin, Ruiyang Zhu, Jiachen Sun, Xumiao Zhang, Qi Alfred Chen, Z. Morley Mao</author><pubDate>Tue, 03 Oct 2023 18:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12955v2</guid></item><item><title>Who's Harry Potter? Approximate Unlearning in LLMs</title><link>http://arxiv.org/abs/2310.02238v1</link><description>Large language models (LLMs) are trained on massive internet corpora thatoften contain copyrighted content. This poses legal and ethical challenges forthe developers and users of these models, as well as the original authors andpublishers. In this paper, we propose a novel technique for unlearning a subsetof the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter booksfrom the Llama2-7b model (a generative language model recently open-sourced byMeta). While the model took over 184K GPU-hours to pretrain, we show that inabout 1 GPU hour of finetuning, we effectively erase the model's ability togenerate or recall Harry Potter-related content, while its performance oncommon benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remainsalmost unaffected. We make our fine-tuned model publicly available onHuggingFace for community evaluation. To the best of our knowledge, this is thefirst paper to present an effective technique for unlearning in generativelanguage models. Our technique consists of three main components: First, we use a reinforcedmodel that is further trained on the target data to identify the tokens thatare most related to the unlearning target, by comparing its logits with thoseof a baseline model. Second, we replace idiosyncratic expressions in the targetdata with generic counterparts, and leverage the model's own predictions togenerate alternative labels for every token. These labels aim to approximatethe next-token predictions of a model that has not been trained on the targetdata. Third, we finetune the model on these alternative labels, whicheffectively erases the original text from the model's memory whenever it isprompted with its context.</description><author>Ronen Eldan, Mark Russinovich</author><pubDate>Tue, 03 Oct 2023 18:48:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02238v1</guid></item><item><title>Exploring Model Learning Heterogeneity for Boosting Ensemble Robustness</title><link>http://arxiv.org/abs/2310.02237v1</link><description>Deep neural network ensembles hold the potential of improving generalizationperformance for complex learning tasks. This paper presents formal analysis andempirical evaluation to show that heterogeneous deep ensembles with highensemble diversity can effectively leverage model learning heterogeneity toboost ensemble robustness. We first show that heterogeneous DNN models trainedfor solving the same learning problem, e.g., object detection, cansignificantly strengthen the mean average precision (mAP) through our weightedbounding box ensemble consensus method. Second, we further compose ensembles ofheterogeneous models for solving different learning problems, e.g., objectdetection and semantic segmentation, by introducing the connected componentlabeling (CCL) based alignment. We show that this two-tier heterogeneity drivenensemble construction method can compose an ensemble team that promotes highensemble diversity and low negative correlation among member models of theensemble, strengthening ensemble robustness against both negative examples andadversarial attacks. Third, we provide a formal analysis of the ensemblerobustness in terms of negative correlation. Extensive experiments validate theenhanced robustness of heterogeneous ensembles in both benign and adversarialsettings. The source codes are available on GitHub athttps://github.com/git-disl/HeteRobust.</description><author>Yanzhao Wu, Ka-Ho Chow, Wenqi Wei, Ling Liu</author><pubDate>Tue, 03 Oct 2023 18:47:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02237v1</guid></item><item><title>Automatic Quality Assessment of Wikipedia Articles -- A Systematic Literature Review</title><link>http://arxiv.org/abs/2310.02235v1</link><description>Wikipedia is the world's largest online encyclopedia, but maintaining articlequality through collaboration is challenging. Wikipedia designed a qualityscale, but with such a manual assessment process, many articles remainunassessed. We review existing methods for automatically measuring the qualityof Wikipedia articles, identifying and comparing machine learning algorithms,article features, quality metrics, and used datasets, examining 149 distinctstudies, and exploring commonalities and gaps in them. The literature isextensive, and the approaches follow past technological trends. However,machine learning is still not widely used by Wikipedia, and we hope that ouranalysis helps future researchers change that reality.</description><author>Pedro Miguel Moás, Carla Teixeira Lopes</author><pubDate>Tue, 03 Oct 2023 18:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02235v1</guid></item><item><title>On CNF formulas irredundant with respect to unit clause propagation</title><link>http://arxiv.org/abs/2309.01750v2</link><description>Two CNF formulas are called ucp-equivalent, if they behave in the same waywith respect to the unit clause propagation (UCP). A formula is calleducp-irredundant, if removing any clause leads to a formula which is notucp-equivalent to the original one. As a consequence of known results, theratio of the size of a ucp-irredundant formula and the size of a smallestucp-equivalent formula is at most $n^2$, where $n$ is the number of thevariables. We demonstrate an example of a ucp-irredundant formula for asymmetric definite Horn function which is larger than a smallest ucp-equivalentformula by a factor $\Omega(n/\ln n)$ and, hence, a general upper bound on theabove ratio cannot be smaller than this.</description><author>Petr Savický</author><pubDate>Tue, 03 Oct 2023 18:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01750v2</guid></item><item><title>MIS-AVioDD: Modality Invariant and Specific Representation for Audio-Visual Deepfake Detection</title><link>http://arxiv.org/abs/2310.02234v1</link><description>Deepfakes are synthetic media generated using deep generative algorithms andhave posed a severe societal and political threat. Apart from facialmanipulation and synthetic voice, recently, a novel kind of deepfakes hasemerged with either audio or visual modalities manipulated. In this regard, anew generation of multimodal audio-visual deepfake detectors is beinginvestigated to collectively focus on audio and visual data for multimodalmanipulation detection. Existing multimodal (audio-visual) deepfake detectorsare often based on the fusion of the audio and visual streams from the video.Existing studies suggest that these multimodal detectors often obtainequivalent performances with unimodal audio and visual deepfake detectors. Weconjecture that the heterogeneous nature of the audio and visual signalscreates distributional modality gaps and poses a significant challenge toeffective fusion and efficient performance. In this paper, we tackle theproblem at the representation level to aid the fusion of audio and visualstreams for multimodal deepfake detection. Specifically, we propose the jointuse of modality (audio and visual) invariant and specific representations. Thisensures that the common patterns and patterns specific to each modalityrepresenting pristine or fake content are preserved and fused for multimodaldeepfake manipulation detection. Our experimental results on FakeAVCeleb andKoDF audio-visual deepfake datasets suggest the enhanced accuracy of ourproposed method over SOTA unimodal and multimodal audio-visual deepfakedetectors by $17.8$% and $18.4$%, respectively. Thus, obtainingstate-of-the-art performance.</description><author>Vinaya Sree Katamneni, Ajita Rattani</author><pubDate>Tue, 03 Oct 2023 18:43:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02234v1</guid></item><item><title>Generalized Schrödinger Bridge Matching</title><link>http://arxiv.org/abs/2310.02233v1</link><description>Modern distribution matching algorithms for training diffusion or flow modelsdirectly prescribe the time evolution of the marginal distributions between twoboundary distributions. In this work, we consider a generalized distributionmatching setup, where these marginals are only implicitly described as asolution to some task-specific objective function. The problem setup, known asthe Generalized Schr\"odinger Bridge (GSB), appears prevalently in manyscientific areas both within and without machine learning. We proposeGeneralized Schr\"odinger Bridge Matching (GSBM), a new matching algorithminspired by recent advances, generalizing them beyond kinetic energyminimization and to account for task-specific state costs. We show that such ageneralization can be cast as solving conditional stochastic optimal control,for which efficient variational approximations can be used, and furtherdebiased with the aid of path integral theory. Compared to prior methods forsolving GSB problems, our GSBM algorithm always preserves a feasible transportmap between the boundary distributions throughout training, thereby enablingstable convergence and significantly improved scalability. We empiricallyvalidate our claims on an extensive suite of experimental setups, includingcrowd navigation, opinion depolarization, LiDAR manifolds, and image domaintransfer. Our work brings new algorithmic opportunities for training diffusionmodels enhanced with task-specific optimality structures.</description><author>Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos A. Theodorou, Ricky T. Q. Chen</author><pubDate>Tue, 03 Oct 2023 18:42:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02233v1</guid></item><item><title>HoloNets: Spectral Convolutions do extend to Directed Graphs</title><link>http://arxiv.org/abs/2310.02232v1</link><description>Within the graph learning community, conventional wisdom dictates thatspectral convolutional networks may only be deployed on undirected graphs: Onlythere could the existence of a well-defined graph Fourier transform beguaranteed, so that information may be translated between spatial- and spectraldomains. Here we show this traditional reliance on the graph Fourier transformto be superfluous and -- making use of certain advanced tools from complexanalysis and spectral theory -- extend spectral convolutions to directedgraphs. We provide a frequency-response interpretation of newly developedfilters, investigate the influence of the basis used to express filters anddiscuss the interplay with characteristic operators on which networks arebased. In order to thoroughly test the developed theory, we conduct experimentsin real world settings, showcasing that directed spectral convolutionalnetworks provide new state of the art results for heterophilic nodeclassification on many datasets and -- as opposed to baselines -- may berendered stable to resolution-scale varying topological perturbations.</description><author>Christian Koke, Daniel Cremers</author><pubDate>Tue, 03 Oct 2023 18:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02232v1</guid></item><item><title>Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks</title><link>http://arxiv.org/abs/2310.02230v1</link><description>Spurious correlations in the data, where multiple cues are predictive of thetarget labels, often lead to shortcut learning phenomena, where a model mayrely on erroneous, easy-to-learn, cues while ignoring reliable ones. In thiswork, we propose an ensemble diversification framework exploiting thegeneration of synthetic counterfactuals using Diffusion Probabilistic Models(DPMs). We discover that DPMs have the inherent capability to representmultiple visual cues independently, even when they are largely correlated inthe training data. We leverage this characteristic to encourage model diversityand empirically show the efficacy of the approach with respect to severaldiversification objectives. We show that diffusion-guided diversification canlead models to avert attention from shortcut cues, achieving ensemble diversityperformance comparable to previous methods requiring additional datacollection.</description><author>Luca Scimeca, Alexander Rubinstein, Armand Nicolicioiu, Damien Teney, Yoshua Bengio</author><pubDate>Tue, 03 Oct 2023 18:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02230v1</guid></item><item><title>Extraction of Medication and Temporal Relation from Clinical Text by Harnessing Different Deep Learning Models</title><link>http://arxiv.org/abs/2310.02229v1</link><description>Clinical texts, represented in electronic medical records (EMRs), containrich medical information and are essential for disease prediction, personalisedinformation recommendation, clinical decision support, and medication patternmining and measurement. Relation extractions between medication mentions andtemporal information can further help clinicians better understand thepatients' treatment history. To evaluate the performances of deep learning (DL)and large language models (LLMs) in medication extraction and temporalrelations classification, we carry out an empirical investigation of\textbf{MedTem} project using several advanced learning structures includingBiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER),and BERT-CNN for temporal relation extraction (RE), in addition to theexploration of different word embedding techniques. Furthermore, we alsodesigned a set of post-processing roles to generate structured output onmedications and the temporal relation. Our experiments show that CNN-BiLSTMslightly wins the BiLSTM-CRF model on the i2b2-2009 clinical NER task yielding75.67, 77.83, and 78.17 for precision, recall, and F1 scores using MacroAverage. BERT-CNN model also produced reasonable evaluation scores 64.48,67.17, and 65.03 for P/R/F1 using Macro Avg on the temporal relation extractiontest set from i2b2-2012 challenges. Code and Tools from MedTem will be hostedat \url{https://github.com/HECTA-UoM/MedTem}</description><author>Hangyu Tu, Lifeng Han, Goran Nenadic</author><pubDate>Tue, 03 Oct 2023 18:37:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02229v1</guid></item><item><title>SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training</title><link>http://arxiv.org/abs/2310.02227v1</link><description>In an era where symbolic mathematical equations are indispensable formodeling complex natural phenomena, scientific inquiry often involvescollecting observations and translating them into mathematical expressions.Recently, deep learning has emerged as a powerful tool for extracting insightsfrom data. However, existing models typically specialize in either numeric orsymbolic domains, and are usually trained in a supervised manner tailored tospecific tasks. This approach neglects the substantial benefits that couldarise from a task-agnostic unified understanding between symbolic equations andtheir numeric counterparts. To bridge the gap, we introduce SNIP, aSymbolic-Numeric Integrated Pre-training, which employs joint contrastivelearning between symbolic and numeric domains, enhancing their mutualsimilarities in the pre-trained embeddings. By performing latent spaceanalysis, we observe that SNIP provides cross-domain insights into therepresentations, revealing that symbolic supervision enhances the embeddings ofnumeric data and vice versa. We evaluate SNIP across diverse tasks, includingsymbolic-to-numeric mathematical property prediction and numeric-to-symbolicequation discovery, commonly known as symbolic regression. Results show thatSNIP effectively transfers to various tasks, consistently outperforming fullysupervised baselines and competing strongly with established task-specificmethods, especially in few-shot learning scenarios where available data islimited.</description><author>Kazem Meidani, Parshin Shojaee, Chandan K. Reddy, Amir Barati Farimani</author><pubDate>Tue, 03 Oct 2023 18:32:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02227v1</guid></item><item><title>Think before you speak: Training Language Models With Pause Tokens</title><link>http://arxiv.org/abs/2310.02226v1</link><description>Language models generate responses by producing a series of tokens inimmediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$hidden vectors per layer, one vector per preceding token. What if instead wewere to let the model manipulate say, $K+10$ hidden vectors, before it outputsthe $(K+1)^{th}$ token? We operationalize this idea by performing training andinference on language models with a (learnable) $\textit{pause}$ token, asequence of which is appended to the input prefix. We then delay extracting themodel's outputs until the last pause token is seen, thereby allowing the modelto process extra computation before committing to an answer. We empiricallyevaluate $\textit{pause-training}$ on decoder-only models of 1B and 130Mparameters with causal pretraining on C4, and on downstream tasks coveringreasoning, question-answering, general understanding and fact recall. Our mainfinding is that inference-time delays show gains when the model is bothpre-trained and finetuned with delays. For the 1B model, we witness gains on 8of 9 tasks, most prominently, a gain of $18\%$ EM score on the QA task ofSQuAD, $8\%$ on CommonSenseQA and $1\%$ accuracy on the reasoning task ofGSM8k. Our work raises a range of conceptual and practical future researchquestions on making delayed next-token prediction a widely applicable newparadigm.</description><author>Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh Nagarajan</author><pubDate>Tue, 03 Oct 2023 18:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02226v1</guid></item><item><title>Can Language Models be Instructed to Protect Personal Information?</title><link>http://arxiv.org/abs/2310.02224v1</link><description>Large multimodal language models have proven transformative in numerousapplications. However, these models have been shown to memorize and leakpre-training data, raising serious user privacy and information securityconcerns. While data leaks should be prevented, it is also crucial to examinethe trade-off between the privacy protection and model utility of proposedapproaches. In this paper, we introduce PrivQA -- a multimodal benchmark toassess this privacy/utility trade-off when a model is instructed to protectspecific categories of personal information in a simulated scenario. We alsopropose a technique to iteratively self-moderate responses, which significantlyimproves privacy. However, through a series of red-teaming experiments, we findthat adversaries can also easily circumvent these protections with simplejailbreaking methods through textual and/or image inputs. We believe PrivQA hasthe potential to support the development of new models with improved privacyprotections, as well as the adversarial robustness of these protections. Werelease the entire PrivQA dataset at https://llm-access-control.github.io/.</description><author>Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter</author><pubDate>Tue, 03 Oct 2023 18:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02224v1</guid></item><item><title>Structurally guided task decomposition in spatial navigation tasks</title><link>http://arxiv.org/abs/2310.02221v1</link><description>How are people able to plan so efficiently despite limited cognitiveresources? We aimed to answer this question by extending an existing model ofhuman task decomposition that can explain a wide range of simple planningproblems by adding structure information to the task to facilitate planning inmore complex tasks. The extended model was then applied to a more complexplanning domain of spatial navigation. Our results suggest that our frameworkcan correctly predict the navigation strategies of the majority of theparticipants in an online experiment.</description><author>Ruiqi He, Carlos G. Correa, Thomas L. Griffiths, Mark K. Ho</author><pubDate>Tue, 03 Oct 2023 18:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02221v1</guid></item><item><title>What do we learn from a large-scale study of pre-trained visual representations in sim and real environments?</title><link>http://arxiv.org/abs/2310.02219v1</link><description>We present a large empirical investigation on the use of pre-trained visualrepresentations (PVRs) for training downstream policies that execute real-worldtasks. Our study spans five different PVRs, two different policy-learningparadigms (imitation and reinforcement learning), and three different robotsfor 5 distinct manipulation and indoor navigation tasks. From this effort, wecan arrive at three insights: 1) the performance trends of PVRs in thesimulation are generally indicative of their trends in the real world, 2) theuse of PVRs enables a first-of-its-kind result with indoor ImageNav (zero-shottransfer to a held-out scene in the real world), and 3) the benefits fromvariations in PVRs, primarily data-augmentation and fine-tuning, also transferto the real-world performance. See project website for additional details andvisuals.</description><author>Sneha Silwal, Karmesh Yadav, Tingfan Wu, Jay Vakil, Arjun Majumdar, Sergio Arnaud, Claire Chen, Vincent-Pierre Berges, Dhruv Batra, Aravind Rajeswaran, Mrinal Kalakrishnan, Franziska Meier, Oleksandr Maksymets</author><pubDate>Tue, 03 Oct 2023 18:27:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02219v1</guid></item><item><title>FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation</title><link>http://arxiv.org/abs/2210.00193v3</link><description>We present FRMT, a new dataset and evaluation benchmark for Few-shotRegion-aware Machine Translation, a type of style-targeted translation. Thedataset consists of professional translations from English into two regionalvariants each of Portuguese and Mandarin Chinese. Source documents are selectedto enable detailed analysis of phenomena of interest, including lexicallydistinct terms and distractor terms. We explore automatic evaluation metricsfor FRMT and validate their correlation with expert human evaluation acrossboth region-matched and mismatched rating scenarios. Finally, we present anumber of baseline models for this task, and offer guidelines for howresearchers can train, evaluate, and compare their own models. Our dataset andevaluation code are publicly available: https://bit.ly/frmt-task</description><author>Parker Riley, Timothy Dozat, Jan A. Botha, Xavier Garcia, Dan Garrette, Jason Riesa, Orhan Firat, Noah Constant</author><pubDate>Tue, 03 Oct 2023 18:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.00193v3</guid></item><item><title>An experimental system for detection and localization of hemorrhage using ultra-wideband microwaves with deep learning</title><link>http://arxiv.org/abs/2310.02215v1</link><description>Stroke is a leading cause of mortality and disability. Emergent diagnosis andintervention are critical, and predicated upon initial brain imaging; however,existing clinical imaging modalities are generally costly, immobile, and demandhighly specialized operation and interpretation. Low-energy microwaves havebeen explored as low-cost, small form factor, fast, and safe probes of tissuedielectric properties, with both imaging and diagnostic potential.Nevertheless, challenges inherent to microwave reconstruction have impededprogress, hence microwave imaging (MWI) remains an elusive scientific aim.Herein, we introduce a dedicated experimental framework comprising a roboticnavigation system to translate blood-mimicking phantoms within an anatomicallyrealistic human head model. An 8-element ultra-wideband (UWB) array of modifiedantipodal Vivaldi antennas was developed and driven by a two-port vectornetwork analyzer spanning 0.6-9.0 GHz at an operating power of 1 mw. Complexscattering parameters were measured, and dielectric signatures of hemorrhagewere learned using a dedicated deep neural network for prediction of hemorrhageclasses and localization. An overall sensitivity and specificity for detection&gt;0.99 was observed, with Rayliegh mean localization error of 1.65 mm. The studyestablishes the feasibility of a robust experimental model and deep learningsolution for UWB microwave stroke detection.</description><author>Eisa Hedayati, Fatemeh Safari, George Verghese, Vito R. Ciancia, Daniel K. Sodickson, Seena Dehkharghani, Leeor Alon</author><pubDate>Tue, 03 Oct 2023 18:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02215v1</guid></item><item><title>Language Models Represent Space and Time</title><link>http://arxiv.org/abs/2310.02207v1</link><description>The capabilities of large language models (LLMs) have sparked debate overwhether such systems just learn an enormous collection of superficialstatistics or a coherent model of the data generating process -- a world model.We find evidence for the latter by analyzing the learned representations ofthree spatial datasets (world, US, NYC places) and three temporal datasets(historical figures, artworks, news headlines) in the Llama-2 family of models.We discover that LLMs learn linear representations of space and time acrossmultiple scales. These representations are robust to prompting variations andunified across different entity types (e.g. cities and landmarks). In addition,we identify individual ``space neurons'' and ``time neurons'' that reliablyencode spatial and temporal coordinates. Our analysis demonstrates that modernLLMs acquire structured knowledge about fundamental dimensions such as spaceand time, supporting the view that they learn not merely superficialstatistics, but literal world models.</description><author>Wes Gurnee, Max Tegmark</author><pubDate>Tue, 03 Oct 2023 18:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02207v1</guid></item><item><title>Chunking: Forgetting Matters in Continual Learning even without Changing Tasks</title><link>http://arxiv.org/abs/2310.02206v1</link><description>Work on continual learning (CL) has largely focused on the problems arisingfrom the dynamically-changing data distribution. However, CL can be decomposedinto two sub-problems: (a) shifts in the data distribution, and (b) dealingwith the fact that the data is split into chunks and so only a part of the datais available to be trained on at any point in time. In this work, we look atthe latter sub-problem -- the chunking of data -- and note that previousanalysis of chunking in the CL literature is sparse. We show that chunking isan important part of CL, accounting for around half of the performance dropfrom offline learning in our experiments. Furthermore, our results reveal thatcurrent CL algorithms do not address the chunking sub-problem, only performingas well as plain SGD training when there is no shift in the data distribution.We analyse why performance drops when learning occurs on chunks of data, andfind that forgetting, which is often seen to be a problem due to distributionshift, still arises and is a significant problem. Motivated by an analysis ofthe linear case, we show that per-chunk weight averaging improves performancein the chunking setting and that this performance transfers to the full CLsetting. Hence, we argue that work on chunking can help advance CL in general.</description><author>Thomas L. Lee, Amos Storkey</author><pubDate>Tue, 03 Oct 2023 18:04:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02206v1</guid></item><item><title>Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs</title><link>http://arxiv.org/abs/2307.10490v4</link><description>We demonstrate how images and sounds can be used for indirect prompt andinstruction injection in multi-modal LLMs. An attacker generates an adversarialperturbation corresponding to the prompt and blends it into an image or audiorecording. When the user asks the (unmodified, benign) model about theperturbed image or audio, the perturbation steers the model to output theattacker-chosen text and/or make the subsequent dialog follow the attacker'sinstruction. We illustrate this attack with several proof-of-concept examplestargeting LLaVa and PandaGPT.</description><author>Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</author><pubDate>Tue, 03 Oct 2023 18:03:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10490v4</guid></item><item><title>Unifying supervised learning and VAEs -- coverage, systematics and goodness-of-fit in normalizing-flow based neural network models for astro-particle reconstructions</title><link>http://arxiv.org/abs/2008.05825v4</link><description>Neural-network based predictions of event properties in astro-particlephysics are getting more and more common. However, in many cases the result isjust utilized as a point prediction. Statistical uncertainties and coverage(1), systematic uncertainties (2) or a goodness-of-fit measure (3) are oftennot calculated. Here we describe a certain choice of training and networkarchitecture that allows to incorporate all these properties into a singlenetwork model. We show that a KL-divergence objective of the joint distributionof data and labels allows to unify supervised learning and variationalautoencoders (VAEs) under one umbrella of stochastic variational inference. Theunification motivates an extended supervised learning scheme which allows tocalculate a goodness-of-fit p-value for the neural network model. Conditionalnormalizing flows amortized with a neural network are crucial in thisconstruction. We discuss how they allow to rigorously define coverage forposteriors defined jointly on a product space, e.g. $\mathbb{R}^n \times\mathcal{S}^m$, which encompasses posteriors over directions. Finally,systematic uncertainties are naturally included in the variational viewpoint.The proposed extended supervised training with amortized normalizing flowsincorporates (1) coverage calculation, (2) systematics and (3) agoodness-of-fit measure in a single machine-learning model. There are noconstraints on the shape of the involved distributions (e.g. Gaussianity) forthese properties to hold, in fact it works with complex multi-modaldistributions defined on product spaces like $\mathbb{R}^n \times\mathcal{S}^m$. We see great potential for exploiting this per-eventinformation in event selections or for fast astronomical alerts which requireuncertainty guarantees.</description><author>Thorsten Glüsenkamp</author><pubDate>Tue, 03 Oct 2023 18:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2008.05825v4</guid></item><item><title>Learnable Data Augmentation for One-Shot Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2310.02201v1</link><description>This paper presents a classification framework based on learnable dataaugmentation to tackle the One-Shot Unsupervised Domain Adaptation (OS-UDA)problem. OS-UDA is the most challenging setting in Domain Adaptation, as onlyone single unlabeled target sample is assumed to be available for modeladaptation. Driven by such single sample, our method LearnAug-UDA learns how toaugment source data, making it perceptually similar to the target. As a result,a classifier trained on such augmented data will generalize well for the targetdomain. To achieve this, we designed an encoder-decoder architecture thatexploits a perceptual loss and style transfer strategies to augment the sourcedata. Our method achieves state-of-the-art performance on two well-known DomainAdaptation benchmarks, DomainNet and VisDA. The project code is available athttps://github.com/IIT-PAVIS/LearnAug-UDA</description><author>Julio Ivan Davila Carrazco, Pietro Morerio, Alessio Del Bue, Vittorio Murino</author><pubDate>Tue, 03 Oct 2023 17:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02201v1</guid></item><item><title>Minimax Estimation of Distances on a Surface and Minimax Manifold Learning in the Isometric-to-Convex Setting</title><link>http://arxiv.org/abs/2011.12478v2</link><description>We start by considering the problem of estimating intrinsic distances on asmooth submanifold. We show that minimax optimality can be obtained via areconstruction of the surface, and discuss the use of a particular meshconstruction -- the tangential Delaunay complex -- for that purpose. We thenturn to manifold learning and argue that a variant of Isomap where thedistances are instead computed on a reconstructed surface is minimax optimalfor the isometric variant of the problem.</description><author>Ery Arias-Castro, Phong Alain Chau</author><pubDate>Tue, 03 Oct 2023 17:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.12478v2</guid></item><item><title>Learning Task Automata for Reinforcement Learning using Hidden Markov Models</title><link>http://arxiv.org/abs/2208.11838v4</link><description>Training reinforcement learning (RL) agents using scalar reward signals isoften infeasible when an environment has sparse and non-Markovian rewards.Moreover, handcrafting these reward functions before training is prone tomisspecification, especially when the environment's dynamics are only partiallyknown. This paper proposes a novel pipeline for learning non-Markovian taskspecifications as succinct finite-state `task automata' from episodes of agentexperience within unknown environments. We leverage two key algorithmicinsights. First, we learn a product MDP, a model composed of thespecification's automaton and the environment's MDP (both initially unknown),by treating the product MDP as a partially observable MDP and using thewell-known Baum-Welch algorithm for learning hidden Markov models. Second, wepropose a novel method for distilling the task automaton (assumed to be adeterministic finite automaton) from the learnt product MDP. Our learnt taskautomaton enables the decomposition of a task into its constituent sub-tasks,which improves the rate at which an RL agent can later synthesise an optimalpolicy. It also provides an interpretable encoding of high-level environmentaland task features, so a human can readily verify that the agent has learntcoherent tasks with no misspecifications. In addition, we take steps towardsensuring that the learnt automaton is environment-agnostic, making itwell-suited for use in transfer learning. Finally, we provide experimentalresults compared with two baselines to illustrate our algorithm's performancein different environments and tasks.</description><author>Alessandro Abate, Yousif Almulla, James Fox, David Hyland, Michael Wooldridge</author><pubDate>Tue, 03 Oct 2023 17:46:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.11838v4</guid></item><item><title>Efficient Online Scheduling and Routing for Automated Guided Vehicles: Comparing a Novel Loop-Based Algorithm Against Existing Methods</title><link>http://arxiv.org/abs/2310.02195v1</link><description>Automated guided vehicles (AGVs) are widely used in various industries, andscheduling and routing them in a conflict-free manner is crucial to theirefficient operation. We propose a loop-based algorithm that solves the online,conflict-free scheduling and routing problem for AGVs. The proposed algorithmis compared against an exact method, a greedy heuristic and a metaheuristic. Weexperimentally show that this algorithm either outperforms the other algorithmsor gets an equally good solution in less computing time.</description><author>Louis Stubbe, Jens Goemaere, Jan Goedgebeur</author><pubDate>Tue, 03 Oct 2023 17:41:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02195v1</guid></item><item><title>On the Generalization of Training-based ChatGPT Detection Methods</title><link>http://arxiv.org/abs/2310.01307v2</link><description>ChatGPT is one of the most popular language models which achieve amazingperformance on various natural language tasks. Consequently, there is also anurgent need to detect the texts generated ChatGPT from human written. One ofthe extensively studied methods trains classification models to distinguishboth. However, existing studies also demonstrate that the trained models maysuffer from distribution shifts (during test), i.e., they are ineffective topredict the generated texts from unseen language tasks or topics. In this work,we aim to have a comprehensive investigation on these methods' generalizationbehaviors under distribution shift caused by a wide range of factors, includingprompts, text lengths, topics, and language tasks. To achieve this goal, wefirst collect a new dataset with human and ChatGPT texts, and then we conductextensive studies on the collected dataset. Our studies unveil insightfulfindings which provide guidance for developing future methodologies or datacollection strategies for ChatGPT detection.</description><author>Han Xu, Jie Ren, Pengfei He, Shenglai Zeng, Yingqian Cui, Amy Liu, Hui Liu, Jiliang Tang</author><pubDate>Tue, 03 Oct 2023 17:40:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01307v2</guid></item><item><title>Uncertainty Quantification in Inverse Models in Hydrology</title><link>http://arxiv.org/abs/2310.02193v1</link><description>In hydrology, modeling streamflow remains a challenging task due to thelimited availability of basin characteristics information such as soil geologyand geomorphology. These characteristics may be noisy due to measurement errorsor may be missing altogether. To overcome this challenge, we propose aknowledge-guided, probabilistic inverse modeling method for recovering physicalcharacteristics from streamflow and weather data, which are more readilyavailable. We compare our framework with state-of-the-art inverse models forestimating river basin characteristics. We also show that these estimates offerimprovement in streamflow modeling as opposed to using the original basincharacteristic values. Our inverse model offers 3\% improvement in R$^2$ forthe inverse model (basin characteristic estimation) and 6\% for the forwardmodel (streamflow prediction). Our framework also offers improvedexplainability since it can quantify uncertainty in both the inverse and theforward model. Uncertainty quantification plays a pivotal role in improving theexplainability of machine learning models by providing additional insights intothe reliability and limitations of model predictions. In our analysis, weassess the quality of the uncertainty estimates. Compared to baselineuncertainty quantification methods, our framework offers 10\% improvement inthe dispersion of epistemic uncertainty and 13\% improvement in coverage rate.This information can help stakeholders understand the level of uncertaintyassociated with the predictions and provide a more comprehensive view of thepotential outcomes.</description><author>Somya Sharma Chatterjee, Rahul Ghosh, Arvind Renganathan, Xiang Li, Snigdhansu Chatterjee, John Nieber, Christopher Duffy, Vipin Kumar</author><pubDate>Tue, 03 Oct 2023 17:39:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02193v1</guid></item><item><title>A Case for AI Safety via Law</title><link>http://arxiv.org/abs/2309.12321v2</link><description>How to make artificial intelligence (AI) systems safe and aligned with humanvalues is an open research question. Proposed solutions tend toward relying onhuman intervention in uncertain situations, learning human values andintentions through training or observation, providing off-switches,implementing isolation or simulation environments, or extrapolating what peoplewould want if they had more knowledge and more time to think. Law-basedapproaches--such as inspired by Isaac Asimov--have not been well regarded. Thispaper makes a case that effective legal systems are the best way to address AIsafety. Law is defined as any rules that codify prohibitions and prescriptionsapplicable to particular agents in specified domains/contexts and includesprocesses for enacting, managing, enforcing, and litigating such rules.</description><author>Jeffrey W. Johnston</author><pubDate>Tue, 03 Oct 2023 17:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12321v2</guid></item><item><title>Photonic Accelerators for Image Segmentation in Autonomous Driving and Defect Detection</title><link>http://arxiv.org/abs/2309.16783v2</link><description>Photonic computing promises faster and more energy-efficient deep neuralnetwork (DNN) inference than traditional digital hardware. Advances in photoniccomputing can have profound impacts on applications such as autonomous drivingand defect detection that depend on fast, accurate and energy efficientexecution of image segmentation models. In this paper, we investigate imagesegmentation on photonic accelerators to explore: a) the types of imagesegmentation DNN architectures that are best suited for photonic accelerators,and b) the throughput and energy efficiency of executing the different imagesegmentation models on photonic accelerators, along with the trade-offsinvolved therein. Specifically, we demonstrate that certain segmentation modelsexhibit negligible loss in accuracy (compared to digital float32 models) whenexecuted on photonic accelerators, and explore the empirical reasoning fortheir robustness. We also discuss techniques for recovering accuracy in thecase of models that do not perform well. Further, we compare throughput(inferences-per-second) and energy consumption estimates for different imagesegmentation workloads on photonic accelerators. We discuss the challenges andpotential optimizations that can help improve the application of photonicaccelerators to such computer vision tasks.</description><author>Lakshmi Nair, David Widemann, Brad Turcott, Nick Moore, Alexandra Wleklinski, Darius Bunandar, Ioannis Papavasileiou, Shihu Wang, Eric Logan</author><pubDate>Tue, 03 Oct 2023 17:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16783v2</guid></item><item><title>Self-supervised Learning of Contextualized Local Visual Embeddings</title><link>http://arxiv.org/abs/2310.00527v2</link><description>We present Contextualized Local Visual Embeddings (CLoVE), a self-supervisedconvolutional-based method that learns representations suited for denseprediction tasks. CLoVE deviates from current methods and optimizes a singleloss function that operates at the level of contextualized local embeddingslearned from output feature maps of convolution neural network (CNN) encoders.To learn contextualized embeddings, CLoVE proposes a normalized mult-headself-attention layer that combines local features from different parts of animage based on similarity. We extensively benchmark CLoVE's pre-trainedrepresentations on multiple datasets. CLoVE reaches state-of-the-artperformance for CNN-based architectures in 4 dense prediction downstream tasks,including object detection, instance segmentation, keypoint detection, anddense pose estimation. Code:$\href{https://github.com/sthalles/CLoVE}{\text{https://github.com/sthalles/CLoVE}}$.</description><author>Thalles Santos Silva, Helio Pedrini, Adín Ramírez Rivera</author><pubDate>Tue, 03 Oct 2023 17:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00527v2</guid></item><item><title>Expanding Small-Scale Datasets with Guided Imagination</title><link>http://arxiv.org/abs/2211.13976v4</link><description>The power of DNNs relies heavily on the quantity and quality of trainingdata. However, collecting and annotating data on a large scale is oftenexpensive and time-consuming. To address this issue, we explore a new task,termed dataset expansion, aimed at expanding a ready-to-use small dataset byautomatically creating new labeled samples. To this end, we present a GuidedImagination Framework (GIF) that leverages cutting-edge generative models likeDALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new datafrom the input seed data. Specifically, GIF conducts data imagination byoptimizing the latent features of the seed data in the semantically meaningfulspace of the prior model, resulting in the creation of photo-realistic imageswith new content. To guide the imagination towards creating informative samplesfor model training, we introduce two key criteria, i.e., class-maintainedinformation boosting and sample diversity promotion. These criteria areverified to be essential for effective dataset expansion: GIF-SD obtains 13.5%higher model accuracy on natural image datasets than unguided expansion withSD. With these essential criteria, GIF successfully expands small datasets invarious scenarios, boosting model accuracy by 36.9% on average over six naturalimage datasets and by 13.5% on average over three medical datasets. The sourcecode is available at https://github.com/Vanint/DatasetExpansion.</description><author>Yifan Zhang, Daquan Zhou, Bryan Hooi, Kai Wang, Jiashi Feng</author><pubDate>Tue, 03 Oct 2023 17:13:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.13976v4</guid></item><item><title>Ask Again, Then Fail: Large Language Models' Vacillations in Judgement</title><link>http://arxiv.org/abs/2310.02174v1</link><description>With the emergence of generative conversational large language models (LLMs)like ChatGPT, serving as virtual assistants in various fields, the stabilityand reliability of their responses have become crucial. However, during usage,it has been observed that these models tend to waver in their judgements whenconfronted with follow-up questions from users expressing skepticism ordisagreement. In this work, we draw inspiration from questioning strategies ineducation and propose a \textsc{Follow-up Questioning Mechanism} along with twoevaluation metrics to assess the judgement consistency of LLMs before and afterexposure to disturbances. We evaluate the judgement consistency of ChatGPT,PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoningbenchmarks. Empirical results show that even when the initial answers arecorrect, judgement consistency sharply decreases when LLMs face disturbancessuch as questioning, negation, or misleading. Additionally, we study thesemodels' judgement consistency under various settings (sampling temperature andprompts) to validate this issue further, observing the impact of prompt toneand conducting an in-depth error analysis for deeper behavioral insights.Furthermore, we also explore several prompting methods to mitigate this issueand demonstrate theireffectiveness\footnote{\url{https://github.com/NUSTM/LLMs-Waver-In-Judgements}}.</description><author>Qiming Xie, Zengzhi Wang, Yi Feng, Rui Xia</author><pubDate>Tue, 03 Oct 2023 17:08:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02174v1</guid></item><item><title>Lyfe Agents: Generative agents for low-cost real-time social interactions</title><link>http://arxiv.org/abs/2310.02172v1</link><description>Highly autonomous generative agents powered by large language models promiseto simulate intricate social behaviors in virtual societies. However, achievingreal-time interactions with humans at a low computational cost remainschallenging. Here, we introduce Lyfe Agents. They combine low-cost withreal-time responsiveness, all while remaining intelligent and goal-oriented.Key innovations include: (1) an option-action framework, reducing the cost ofhigh-level decisions; (2) asynchronous self-monitoring for betterself-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizingcritical memory items at a low cost. We evaluate Lyfe Agents' self-motivationand sociability across several multi-agent scenarios in our custom LyfeGame 3Dvirtual environment platform. When equipped with our brain-inspired techniques,Lyfe Agents can exhibit human-like self-motivated social reasoning. Forexample, the agents can solve a crime (a murder mystery) through autonomouscollaboration and information exchange. Meanwhile, our techniques enabled LyfeAgents to operate at a computational cost 10-100 times lower than existingalternatives. Our findings underscore the transformative potential ofautonomous generative agents to enrich human social experiences in virtualworlds.</description><author>Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, Andrew Ahn</author><pubDate>Tue, 03 Oct 2023 17:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02172v1</guid></item><item><title>Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization</title><link>http://arxiv.org/abs/2310.02170v1</link><description>Large language model (LLM) agents have been shown effective on a wide rangeof tasks, and by ensembling multiple LLM agents, their performances could befurther improved. Existing approaches employ a fixed set of agents to interactwith each other in a static architecture, which limits their generalizabilityto various tasks and requires strong human prior in designing these agents. Inthis work, we propose to construct a strategic team of agents communicating ina dynamic interaction architecture based on the task query. Specifically, webuild a framework named Dynamic LLM-Agent Network ($\textbf{DyLAN}$) forLLM-agent collaboration on complicated tasks like reasoning and codegeneration. DyLAN enables agents to interact for multiple rounds in a dynamicarchitecture with inference-time agent selection and an early-stoppingmechanism to improve performance and efficiency. We further design an automaticagent team optimization algorithm based on an unsupervised metric termed$\textit{Agent Importance Score}$, enabling the selection of best agents basedon the contribution each agent makes. Empirically, we demonstrate that DyLANperforms well in both reasoning and code generation tasks with reasonablecomputational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH andHumanEval, respectively, compared to a single execution on GPT-35-turbo. Onspecific subjects of MMLU, agent team optimization in DyLAN increases accuracyby up to 25.0%.</description><author>Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang</author><pubDate>Tue, 03 Oct 2023 17:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02170v1</guid></item><item><title>Editing Personality for LLMs</title><link>http://arxiv.org/abs/2310.02168v1</link><description>This paper introduces an innovative task focused on editing the personalitytraits of Large Language Models (LLMs). This task seeks to adjust the models'responses to opinion-related questions on specified topics since anindividual's personality often manifests in the form of their expressedopinions, thereby showcasing different personality traits. Specifically, weconstruct a new benchmark dataset PersonalityEdit to address this task. Drawingon the theory in Social Psychology, we isolate three representative traits,namely Neuroticism, Extraversion, and Agreeableness, as the foundation for ourbenchmark. We then gather data using GPT-4, generating responses that not onlyalign with a specified topic but also embody the targeted personality trait. Weconduct comprehensive experiments involving various baselines and discuss therepresentation of personality behavior in LLMs. Our intriguing findings uncoverpotential challenges of the proposed task, illustrating several remainingissues. We anticipate that our work can provide the NLP community withinsights. Code and datasets will be released athttps://github.com/zjunlp/EasyEdit.</description><author>Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</author><pubDate>Tue, 03 Oct 2023 17:02:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02168v1</guid></item><item><title>Towards a Unified Framework for Sequential Decision Making</title><link>http://arxiv.org/abs/2310.02167v1</link><description>In recent years, the integration of Automated Planning (AP) and ReinforcementLearning (RL) has seen a surge of interest. To perform this integration, ageneral framework for Sequential Decision Making (SDM) would prove immenselyuseful, as it would help us understand how AP and RL fit together. In thispreliminary work, we attempt to provide such a framework, suitable for anymethod ranging from Classical Planning to Deep RL, by drawing on concepts fromProbability Theory and Bayesian inference. We formulate an SDM task as a set oftraining and test Markov Decision Processes (MDPs), to account forgeneralization. We provide a general algorithm for SDM which we hypothesizeevery SDM method is based on. According to it, every SDM algorithm can be seenas a procedure that iteratively improves its solution estimate by leveragingthe task knowledge available. Finally, we derive a set of formulas andalgorithms for calculating interesting properties of SDM tasks and methods,which make possible their empirical evaluation and comparison.</description><author>Carlos Núñez-Molina, Pablo Mesejo, Juan Fernández-Olivares</author><pubDate>Tue, 03 Oct 2023 17:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02167v1</guid></item><item><title>A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression</title><link>http://arxiv.org/abs/2310.00987v2</link><description>Existing statistical learning guarantees for general kernel regressors oftenyield loose bounds when used with finite-rank kernels. Yet, finite-rank kernelsnaturally appear in several machine learning problems, e.g.\ when fine-tuning apre-trained deep neural network's last layer to adapt it to a novel task whenperforming transfer learning. We address this gap for finite-rank kernel ridgeregression (KRR) by deriving sharp non-asymptotic upper and lower bounds forthe KRR test error of any finite-rank KRR. Our bounds are tighter thanpreviously derived bounds on finite-rank KRR, and unlike comparable results,they also remain valid for any regularization parameters.</description><author>Tin Sum Cheng, Aurelien Lucchi, Ivan Dokmanić, Anastasis Kratsios, David Belius</author><pubDate>Tue, 03 Oct 2023 17:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00987v2</guid></item><item><title>Large Language Models Meet Knowledge Graphs to Answer Factoid Questions</title><link>http://arxiv.org/abs/2310.02166v1</link><description>Recently, it has been shown that the incorporation of structured knowledgeinto Large Language Models significantly improves the results for a variety ofNLP tasks. In this paper, we propose a method for exploring pre-trainedText-to-Text Language Models enriched with additional information fromKnowledge Graphs for answering factoid questions. More specifically, we proposean algorithm for subgraphs extraction from a Knowledge Graph based on questionentities and answer candidates. Then, we procure easily interpreted informationwith Transformer-based models through the linearization of the extractedsubgraphs. Final re-ranking of the answer candidates with the extractedinformation boosts Hits@1 scores of the pre-trained text-to-text languagemodels by 4-6%.</description><author>Mikhail Salnikov, Hai Le, Prateek Rajput, Irina Nikishina, Pavel Braslavski, Valentin Malykh, Alexander Panchenko</author><pubDate>Tue, 03 Oct 2023 16:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02166v1</guid></item><item><title>CAMP-Net: Consistency-Aware Multi-Prior Network for Accelerated MRI Reconstruction</title><link>http://arxiv.org/abs/2306.11238v2</link><description>Despite promising advances in deep learning-based MRI reconstruction methods,restoring high-frequency image details and textures remains a challengingproblem for accelerated MRI. To tackle this challenge, we propose a novelconsistency-aware multi-prior network (CAMP-Net) for MRI reconstruction.CAMP-Net leverages the complementary nature of multiple prior knowledge andexplores data redundancy between adjacent slices in the hybrid domain toimprove image quality. It incorporates three interleaved modules respectivelyfor image enhancement, k-space restoration, and calibration consistency tojointly learn consistency-aware multiple priors in an end-to-end fashion. Theimage enhancement module learns a coil-combined image prior to suppressnoise-like artifacts, while the k-space restoration module explores multi-coilk-space correlations to recover high-frequency details. The calibrationconsistency module embeds the known physical properties of MRI acquisition toensure consistency of k-space correlations extracted from measurements and theartifact-free image intermediate. The resulting low- and high-frequencyreconstructions are hierarchically aggregated in a frequency fusion module anditeratively refined to progressively reconstruct the final image. We evaluatedthe generalizability and robustness of our method on three large publicdatasets with various accelerations and sampling patterns. Comprehensiveexperiments demonstrate that CAMP-Net outperforms state-of-the-art methods interms of reconstruction quality and quantitative $T_2$ mapping.</description><author>Liping Zhang, Xiaobo Li, Weitian Chen</author><pubDate>Tue, 03 Oct 2023 16:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11238v2</guid></item><item><title>Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data</title><link>http://arxiv.org/abs/2302.00674v4</link><description>Few-shot learning is valuable in many real-world applications, but learning ageneralizable model without overfitting to the few labeled datapoints ischallenging. In this work, we focus on Few-shot Learning with Auxiliary Data(FLAD), a training paradigm that assumes access to auxiliary data duringfew-shot learning in hopes of improving generalization. Previous works haveproposed automated methods for mixing auxiliary and target data, but thesemethods typically scale linearly (or worse) with the number of auxiliarydatasets, limiting their practicality. In this work we relate FLAD to theexplore-exploit dilemma that is central to the multi-armed bandit setting andderive algorithms whose computational complexity is independent of the numberof auxiliary datasets, allowing us to scale to 100x more auxiliary datasetsthan prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- andcompare them with prior FLAD methods that either explore or exploit, findingthat the combination of exploration and exploitation is crucial. Throughextensive experimentation we find that our methods outperform all pre-existingFLAD methods by 4% and lead to the first 3 billion parameter language modelsthat outperform the 175 billion parameter GPT-3. Overall, our work suggeststhat the discovery of better, more efficient mixing strategies for FLAD mayprovide a viable path towards substantially improving generalization infew-shot learning.</description><author>Alon Albalak, Colin Raffel, William Yang Wang</author><pubDate>Tue, 03 Oct 2023 16:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00674v4</guid></item><item><title>Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis</title><link>http://arxiv.org/abs/2306.00814v2</link><description>Recent advancements in neural vocoding are predominantly driven by GenerativeAdversarial Networks (GANs) operating in the time-domain. While effective, thisapproach neglects the inductive bias offered by time-frequency representations,resulting in reduntant and computionally-intensive upsampling operations.Fourier-based time-frequency representation is an appealing alternative,aligning more accurately with human auditory perception, and benefitting fromwell-established fast algorithms for its computation. Nevertheless, directreconstruction of complex-valued spectrograms has been historicallyproblematic, primarily due to phase recovery issues. This study seeks to closethis gap by presenting Vocos, a new model that directly generates Fourierspectral coefficients. Vocos not only matches the state-of-the-art in audioquality, as demonstrated in our evaluations, but it also substantially improvescomputational efficiency, achieving an order of magnitude increase in speedcompared to prevailing time-domain neural vocoding approaches. The source codeand model weights have been open-sourced athttps://github.com/charactr-platform/vocos.</description><author>Hubert Siuzdak</author><pubDate>Tue, 03 Oct 2023 16:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00814v2</guid></item><item><title>Selenite: Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models</title><link>http://arxiv.org/abs/2310.02161v1</link><description>Decision-making in unfamiliar domains can be challenging, demandingconsiderable user effort to compare different options with respect to variouscriteria. Prior research and our formative study found that people wouldbenefit from seeing an overview of the information space upfront, such as thecriteria that others have previously found useful. However, existingsensemaking tools struggle with the "cold-start" problem -- it not onlyrequires significant input from previous users to generate and share theseoverviews, but such overviews may also be biased and incomplete. In this work,we introduce a novel system, Selenite, which leverages LLMs as reasoningmachines and knowledge retrievers to automatically produce a comprehensiveoverview of options and criteria to jumpstart users' sensemaking processes.Subsequently, Selenite also adapts as people use it, helping users find, read,and navigate unfamiliar information in a systematic yet personalized manner.Through three studies, we found that Selenite produced accurate andhigh-quality overviews reliably, significantly accelerated users' informationprocessing, and effectively improved their overall comprehension andsensemaking experience.</description><author>Michael Xieyang Liu, Tongshuang Wu, Tianying Chen, Franklin Mingzhe Li, Aniket Kittur, Brad A. Myers</author><pubDate>Tue, 03 Oct 2023 16:48:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02161v1</guid></item><item><title>Probabilistically Rewired Message-Passing Neural Networks</title><link>http://arxiv.org/abs/2310.02156v1</link><description>Message-passing graph neural networks (MPNNs) emerged as powerful tools forprocessing graph-structured input. However, they operate on a fixed input graphstructure, ignoring potential noise and missing information. Furthermore, theirlocal aggregation mechanism can lead to problems such as over-squashing andlimited expressive power in capturing relevant graph structures. Existingsolutions to these challenges have primarily relied on heuristic methods, oftendisregarding the underlying data distribution. Hence, devising principledapproaches for learning to infer graph structures relevant to the givenprediction task remains an open challenge. In this work, leveraging recentprogress in exact and differentiable $k$-subset sampling, we deviseprobabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edgeswhile omitting less beneficial ones. For the first time, our theoreticalanalysis explores how PR-MPNNs enhance expressive power, and we identifyprecise conditions under which they outperform purely randomized approaches.Empirically, we demonstrate that our approach effectively mitigates issues likeover-squashing and under-reaching. In addition, on established real-worlddatasets, our method exhibits competitive or superior predictive performancecompared to traditional MPNN models and recent graph transformer architectures.</description><author>Chendi Qian, Andrei Manolache, Kareem Ahmed, Zhe Zeng, Guy Van den Broeck, Mathias Niepert, Christopher Morris</author><pubDate>Tue, 03 Oct 2023 16:43:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02156v1</guid></item><item><title>Graph Neural Network-based EEG Classification: A Survey</title><link>http://arxiv.org/abs/2310.02152v1</link><description>Graph neural networks (GNN) are increasingly used to classify EEG for taskssuch as emotion recognition, motor imagery and neurological diseases anddisorders. A wide range of methods have been proposed to design GNN-basedclassifiers. Therefore, there is a need for a systematic review andcategorisation of these approaches. We exhaustively search the publishedliterature on this topic and derive several categories for comparison. Thesecategories highlight the similarities and differences among the methods. Theresults suggest a prevalence of spectral graph convolutional layers overspatial. Additionally, we identify standard forms of node features, with themost popular being the raw EEG signal and differential entropy. Our resultssummarise the emerging trends in GNN-based approaches for EEG classification.Finally, we discuss several promising research directions, such as exploringthe potential of transfer learning methods and appropriate modelling ofcross-frequency interactions.</description><author>Dominik Klepl, Min Wu, Fei He</author><pubDate>Tue, 03 Oct 2023 16:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02152v1</guid></item><item><title>Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation</title><link>http://arxiv.org/abs/2310.02147v1</link><description>Whittle index policy is a heuristic to the intractable restless multi-armedbandits (RMAB) problem. Although it is provably asymptotically optimal, findingWhittle indices remains difficult. In this paper, we present Neural-Q-Whittle,a Whittle index based Q-learning algorithm for RMAB with neural networkfunction approximation, which is an example of nonlinear two-timescalestochastic approximation with Q-function values updated on a faster timescaleand Whittle indices on a slower timescale. Despite the empirical success ofdeep Q-learning, the non-asymptotic convergence rate of Neural-Q-Whittle, whichcouples neural networks with two-timescale Q-learning largely remains unclear.This paper provides a finite-time analysis of Neural-Q-Whittle, where data aregenerated from a Markov chain, and Q-function is approximated by a ReLU neuralnetwork. Our analysis leverages a Lyapunov drift approach to capture theevolution of two coupled parameters, and the nonlinearity in value functionapproximation further requires us to characterize the approximation error.Combing these provide Neural-Q-Whittle with $\mathcal{O}(1/k^{2/3})$convergence rate, where $k$ is the number of iterations.</description><author>Guojun Xiong, Jian Li</author><pubDate>Tue, 03 Oct 2023 16:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02147v1</guid></item><item><title>PAD-Phys: Exploiting Physiology for Presentation Attack Detection in Face Biometrics</title><link>http://arxiv.org/abs/2310.02140v1</link><description>Presentation Attack Detection (PAD) is a crucial stage in facial recognitionsystems to avoid leakage of personal information or spoofing of identity toentities. Recently, pulse detection based on remote photoplethysmography (rPPG)has been shown to be effective in face presentation attack detection. This work presents three different approaches to the presentation attackdetection based on rPPG: (i) The physiological domain, a domain usingrPPG-based models, (ii) the Deepfakes domain, a domain where models wereretrained from the physiological domain to specific Deepfakes detection tasks;and (iii) a new Presentation Attack domain was trained by applying transferlearning from the two previous domains to improve the capability todifferentiate between bona-fides and attacks. The results show the efficiency of the rPPG-based models for presentationattack detection, evidencing a 21.70% decrease in average classification errorrate (ACER) (from 41.03% to 19.32%) when the presentation attack domain iscompared to the physiological and Deepfakes domains. Our experiments highlightthe efficiency of transfer learning in rPPG-based models and perform well inpresentation attack detection in instruments that do not allow copying of thisphysiological feature.</description><author>Luis F. Gomez, Julian Fierrez, Aythami Morales, Mahdi Ghafourian, Ruben Tolosana, Imanol Solano, Alejandro Garcia, Francisco Zamora-Martinez</author><pubDate>Tue, 03 Oct 2023 16:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02140v1</guid></item><item><title>Learning Reliable Logical Rules with SATNet</title><link>http://arxiv.org/abs/2310.02133v1</link><description>Bridging logical reasoning and deep learning is crucial for advanced AIsystems. In this work, we present a new framework that addresses this goal bygenerating interpretable and verifiable logical rules through differentiablelearning, without relying on pre-specified logical structures. Our approachbuilds upon SATNet, a differentiable MaxSAT solver that learns the underlyingrules from input-output examples. Despite its efficacy, the learned weights inSATNet are not straightforwardly interpretable, failing to producehuman-readable rules. To address this, we propose a novel specification methodcalled "maximum equality", which enables the interchangeability between thelearned weights of SATNet and a set of propositional logical rules in weightedMaxSAT form. With the decoded weighted MaxSAT formula, we further introduceseveral effective verification techniques to validate it against the groundtruth rules. Experiments on stream transformations and Sudoku problems showthat our decoded rules are highly reliable: using exact solvers on them couldachieve 100% accuracy, whereas the original SATNet fails to give correctsolutions in many cases. Furthermore, we formally verify that our decodedlogical rules are functionally equivalent to the ground truth ones.</description><author>Zhaoyu Li, Jinpei Guo, Yuhe Jiang, Xujie Si</author><pubDate>Tue, 03 Oct 2023 16:14:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02133v1</guid></item><item><title>Unveiling the Pitfalls of Knowledge Editing for Large Language Models</title><link>http://arxiv.org/abs/2310.02129v1</link><description>As the cost associated with fine-tuning Large Language Models (LLMs)continues to rise, recent research efforts have pivoted towards developingmethodologies to edit implicit knowledge embedded within LLMs. Yet, there'sstill a dark cloud lingering overhead -- will knowledge editing triggerbutterfly effect? since it is still unclear whether knowledge editing mightintroduce side effects that pose potential risks or not. This paper pioneersthe investigation into the potential pitfalls associated with knowledge editingfor LLMs. To achieve this, we introduce new benchmark datasets and proposeinnovative evaluation metrics. Our results underline two pivotal concerns: (1)Knowledge Conflict: Editing groups of facts that logically clash can magnifythe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)Knowledge Distortion: Altering parameters with the aim of editing factualknowledge can irrevocably warp the innate knowledge structure of LLMs.Experimental results vividly demonstrate that knowledge editing mightinadvertently cast a shadow of unintended consequences on LLMs, which warrantattention and efforts for future works. Code will be released athttps://github.com/zjunlp/PitfallsKnowledgeEditing.</description><author>Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen</author><pubDate>Tue, 03 Oct 2023 16:10:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02129v1</guid></item><item><title>Faster and Accurate Neural Networks with Semantic Inference</title><link>http://arxiv.org/abs/2310.01259v2</link><description>Deep neural networks (DNN) usually come with a significant computationalburden. While approaches such as structured pruning and mobile-specific DNNshave been proposed, they incur drastic accuracy loss. In this paper we leveragethe intrinsic redundancy in latent representations to reduce the computationalload with limited loss in performance. We show that semantically similar inputsshare many filters, especially in the earlier layers. Thus, semanticallysimilar classes can be clustered to create cluster-specific subgraphs. To thisend, we propose a new framework called Semantic Inference (SINF). In short,SINF (i) identifies the semantic cluster the object belongs to using a smalladditional classifier and (ii) executes the subgraph extracted from the baseDNN related to that semantic cluster for inference. To extract eachcluster-specific subgraph, we propose a new approach named DiscriminativeCapability Score (DCS) that finds the subgraph with the capability todiscriminate among the members of a specific semantic cluster. DCS isindependent from SINF and can be applied to any DNN. We benchmark theperformance of DCS on the VGG16, VGG19, and ResNet50 DNNs trained on theCIFAR100 dataset against 6 state-of-the-art pruning approaches. Our resultsshow that (i) SINF reduces the inference time of VGG19, VGG16, and ResNet50respectively by up to 35%, 29% and 15% with only 0.17%, 3.75%, and 6.75%accuracy loss (ii) DCS achieves respectively up to 3.65%, 4.25%, and 2.36%better accuracy with VGG16, VGG19, and ResNet50 with respect to existingdiscriminative scores (iii) when used as a pruning criterion, DCS achieves upto 8.13% accuracy gain with 5.82% less parameters than the existing state ofthe art work published at ICLR 2023 (iv) when considering per-cluster accuracy,SINF performs on average 5.73%, 8.38% and 6.36% better than the base VGG16,VGG19, and ResNet50.</description><author>Sazzad Sayyed, Jonathan Ashdown, Francesco Restuccia</author><pubDate>Tue, 03 Oct 2023 16:08:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01259v2</guid></item><item><title>Corruption-Robust Lipschitz Contextual Search</title><link>http://arxiv.org/abs/2307.13903v2</link><description>I study the problem of learning a Lipschitz function with corrupted binarysignals. The learner tries to learn a $L$-Lipschitz function $f: [0,1]^d\rightarrow [0, L]$ that the adversary chooses. There is a total of $T$ rounds.In each round $t$, the adversary selects a context vector $x_t$ in the inputspace, and the learner makes a guess to the true function value $f(x_t)$ andreceives a binary signal indicating whether the guess is high or low. In atotal of $C$ rounds, the signal may be corrupted, though the value of $C$ is\emph{unknown} to the learner. The learner's goal is to incur a smallcumulative loss. This work introduces the new algorithmic technique\emph{agnostic checking} as well as new analysis techniques. I designalgorithms which: for the symmetric loss, the learner achieves regret $L\cdotO(C\log T)$ with $d = 1$ and $L\cdot O_d(C\log T + T^{(d-1)/d})$ with $d &gt; 1$;for the pricing loss, the learner achieves regret $L\cdot \widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.</description><author>Shiliang Zuo</author><pubDate>Tue, 03 Oct 2023 16:06:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13903v2</guid></item><item><title>Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View</title><link>http://arxiv.org/abs/2310.02124v1</link><description>As Natural Language Processing (NLP) systems are increasingly employed inintricate social environments, a pressing query emerges: Can these NLP systemsmirror human-esque collaborative intelligence, in a multi-agent societyconsisting of multiple large language models (LLMs)? This paper probes thecollaboration mechanisms among contemporary NLP systems by melding practicalexperiments with theoretical insights. We fabricate four unique `societies'comprised of LLM agents, where each agent is characterized by a specific`trait' (easy-going or overconfident) and engages in collaboration with adistinct `thinking pattern' (debate or reflection). Evaluating thesemulti-agent societies on three benchmark datasets, we discern that LLM agentsnavigate tasks by leveraging diverse social behaviors, from active debates tointrospective reflections. Notably, certain collaborative strategies onlyoptimize efficiency (using fewer API tokens), but also outshine previoustop-tier approaches. Moreover, our results further illustrate that LLM agentsmanifest human-like social behaviors, such as conformity or majority rule,mirroring foundational Social Psychology theories. In conclusion, we integrateinsights from Social Psychology to contextualize the collaboration of LLMagents, inspiring further investigations into the collaboration mechanism forLLMs. We commit to sharing our code and datasets (already submitted insupplementary materials), hoping to catalyze further research in this promisingavenue (All code and data are available at\url{https://github.com/zjunlp/MachineSoM}.).</description><author>Jintian Zhang, Xin Xu, Shumin Deng</author><pubDate>Tue, 03 Oct 2023 16:05:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02124v1</guid></item><item><title>TWIZ: The Wizard of Multimodal Conversational-Stimulus</title><link>http://arxiv.org/abs/2310.02118v1</link><description>In this report, we describe the vision, challenges, and scientificcontributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBotChallenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal,knowledgeable, and engaging assistant that can guide users towards thesuccessful completion of complex manual tasks. To achieve this, we focus ourefforts on three main research questions: (1) Humanly-Shaped Conversations, byproviding information in a knowledgeable way; (2) Multimodal Stimulus, makinguse of various modalities including voice, images, and videos; and (3)Zero-shot Conversational Flows, to improve the robustness of the interaction tounseen scenarios. TWIZ is an assistant capable of supporting a wide range oftasks, with several innovative features such as creative cooking, videonavigation through voice, and the robust TWIZ-LLM, a Large Language Modeltrained for dialoguing about complex manual tasks. Given ratings and feedbackprovided by users, we observed that TWIZ bot is an effective and robust system,capable of guiding users through tasks while providing several multimodalstimuli.</description><author>Rafael Ferreira, Diogo Tavares, Diogo Silva, Rodrigo Valério, João Bordalo, Inês Simões, Vasco Ramos, David Semedo, João Magalhães</author><pubDate>Tue, 03 Oct 2023 15:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02118v1</guid></item><item><title>Is Simple Uniform Sampling Effective for Center-Based Clustering with Outliers: When and Why?</title><link>http://arxiv.org/abs/2103.00558v5</link><description>Real-world datasets often contain outliers, and the presence of outliers canmake the clustering problems to be much more challenging. In this paper, wepropose a simple uniform sampling framework for solving three representativecenter-based clustering with outliers problems: $k$-center/median/meansclustering with outliers. Our analysis is fundamentally different from theprevious (uniform and non-uniform) sampling based ideas. To explain theeffectiveness of uniform sampling in theory, we introduce a measure of"significance" and prove that the performance of our framework depends on thesignificance degree of the given instance. In particular, the sample size canbe independent of the input data size $n$ and the dimensionality $d$, if weassume the given instance is "significant", which is in fact a fairlyreasonable assumption in practice. Due to its simplicity, the uniform samplingapproach also enjoys several significant advantages over the non-uniformsampling approaches in practice. To the best of our knowledge, this is thefirst work that systematically studies the effectiveness of uniform samplingfrom both theoretical and experimental aspects.</description><author>Jiawei Huang, Wenjie Liu, Hu Ding</author><pubDate>Tue, 03 Oct 2023 15:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.00558v5</guid></item><item><title>Symmetric Single Index Learning</title><link>http://arxiv.org/abs/2310.02117v1</link><description>Few neural architectures lend themselves to provable learning with gradientbased methods. One popular model is the single-index model, in which labels areproduced by composing an unknown linear projection with a possibly unknownscalar link function. Learning this model with SGD is relativelywell-understood, whereby the so-called information exponent of the linkfunction governs a polynomial sample complexity rate. However, extending thisanalysis to deeper or more complicated architectures remains challenging. In this work, we consider single index learning in the setting of symmetricneural networks. Under analytic assumptions on the activation and maximumdegree assumptions on the link function, we prove that gradient flow recoversthe hidden planted direction, represented as a finitely supported vector in thefeature space of power sum polynomials. We characterize a notion of informationexponent adapted to our setting that controls the efficiency of learning.</description><author>Aaron Zweig, Joan Bruna</author><pubDate>Tue, 03 Oct 2023 15:59:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02117v1</guid></item><item><title>Deep Operator Learning Lessens the Curse of Dimensionality for PDEs</title><link>http://arxiv.org/abs/2301.12227v3</link><description>Deep neural networks (DNNs) have achieved remarkable success in numerousdomains, and their application to PDE-related problems has been rapidlyadvancing. This paper provides an estimate for the generalization error oflearning Lipschitz operators over Banach spaces using DNNs with applications tovarious PDE solution operators. The goal is to specify DNN width, depth, andthe number of training samples needed to guarantee a certain testing error.Under mild assumptions on data distributions or operator structures, ouranalysis shows that deep operator learning can have a relaxed dependence on thediscretization resolution of PDEs and, hence, lessen the curse ofdimensionality in many PDE-related problems including elliptic equations,parabolic equations, and Burgers equations. Our results are also applied togive insights about discretization-invariance in operator learning.</description><author>Ke Chen, Chunmei Wang, Haizhao Yang</author><pubDate>Tue, 03 Oct 2023 15:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.12227v3</guid></item><item><title>Hierarchical Concept Discovery Models: A Concept Pyramid Scheme</title><link>http://arxiv.org/abs/2310.02116v1</link><description>Deep Learning algorithms have recently gained significant attention due totheir impressive performance. However, their high complexity andun-interpretable mode of operation hinders their confident deployment inreal-world safety-critical tasks. This work targets ante hoc interpretability,and specifically Concept Bottleneck Models (CBMs). Our goal is to design aframework that admits a highly interpretable decision making process withrespect to human understandable concepts, on multiple levels of granularity. Tothis end, we propose a novel hierarchical concept discovery formulationleveraging: (i) recent advances in image-text models, and (ii) an innovativeformulation for multi-level concept selection via data-driven and sparsityinducing Bayesian arguments. Within this framework, concept information doesnot solely rely on the similarity between the whole image and generalunstructured concepts; instead, we introduce the notion of concept hierarchy touncover and exploit more granular concept information residing inpatch-specific regions of the image scene. As we experimentally show, theproposed construction not only outperforms recent CBM approaches, but alsoyields a principled framework towards interpetability.</description><author>Konstantinos P. Panousis, Dino Ienco, Diego Marcos</author><pubDate>Tue, 03 Oct 2023 15:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02116v1</guid></item><item><title>Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices</title><link>http://arxiv.org/abs/2303.00492v2</link><description>Graph neural networks (GNN) have been widely deployed in real-world networkedapplications and systems due to their capability to handle graph-structureddata. However, the growing awareness of data privacy severely challenges thetraditional centralized model training paradigm, where a server holds all thegraph information. Federated learning is an emerging collaborative computingparadigm that allows model training without data centralization. Existingfederated GNN studies mainly focus on systems where clients hold distinctivegraphs or sub-graphs. The practical node-level federated situation, where eachclient is only aware of its direct neighbors, has yet to be studied. In thispaper, we propose the first federated GNN framework called Lumos that supportssupervised and unsupervised learning with feature and degree protection onnode-level federated graphs. We first design a tree constructor to improve therepresentation capability given the limited structural information. We furtherpresent a Monte Carlo Markov Chain-based algorithm to mitigate the workloadimbalance caused by degree heterogeneity with theoretically-guaranteedperformance. Based on the constructed tree for each client, a decentralizedtree-based GNN trainer is proposed to support versatile training. Extensiveexperiments demonstrate that Lumos outperforms the baseline with significantlyhigher accuracy and greatly reduced communication cost and training time.</description><author>Qiying Pan, Yifei Zhu, Lingyang Chu</author><pubDate>Tue, 03 Oct 2023 15:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00492v2</guid></item><item><title>FLEDGE: Ledger-based Federated Learning Resilient to Inference and Backdoor Attacks</title><link>http://arxiv.org/abs/2310.02113v1</link><description>Federated learning (FL) is a distributed learning process that uses a trustedaggregation server to allow multiple parties (or clients) to collaborativelytrain a machine learning model without having them share their private data.Recent research, however, has demonstrated the effectiveness of inference andpoisoning attacks on FL. Mitigating both attacks simultaneously is verychallenging. State-of-the-art solutions have proposed the use of poisoningdefenses with Secure Multi-Party Computation (SMPC) and/or Differential Privacy(DP). However, these techniques are not efficient and fail to address themalicious intent behind the attacks, i.e., adversaries (curious servers and/orcompromised clients) seek to exploit a system for monetization purposes. Toovercome these limitations, we present a ledger-based FL framework known asFLEDGE that allows making parties accountable for their behavior and achievereasonable efficiency for mitigating inference and poisoning attacks. Oursolution leverages crypto-currency to increase party accountability bypenalizing malicious behavior and rewarding benign conduct. We conduct anextensive evaluation on four public datasets: Reddit, MNIST, Fashion-MNIST, andCIFAR-10. Our experimental results demonstrate that (1) FLEDGE provides strongprivacy guarantees for model updates without sacrificing model utility; (2)FLEDGE can successfully mitigate different poisoning attacks without degradingthe performance of the global model; and (3) FLEDGE offers unique rewardmechanisms to promote benign behavior during model training and/or modelaggregation.</description><author>Jorge Castillo, Phillip Rieger, Hossein Fereidooni, Qian Chen, Ahmad Sadeghi</author><pubDate>Tue, 03 Oct 2023 15:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02113v1</guid></item><item><title>Dual Correction Strategy for Ranking Distillation in Top-N Recommender System</title><link>http://arxiv.org/abs/2109.03459v3</link><description>Knowledge Distillation (KD), which transfers the knowledge of a well-trainedlarge model (teacher) to a small model (student), has become an important areaof research for practical deployment of recommender systems. Recently, RelaxedRanking Distillation (RRD) has shown that distilling the ranking information inthe recommendation list significantly improves the performance. However, themethod still has limitations in that 1) it does not fully utilize theprediction errors of the student model, which makes the training not fullyefficient, and 2) it only distills the user-side ranking information, whichprovides an insufficient view under the sparse implicit feedback. This paperpresents Dual Correction strategy for Distillation (DCD), which transfers theranking information from the teacher model to the student model in a moreefficient manner. Most importantly, DCD uses the discrepancy between theteacher model and the student model predictions to decide which knowledge to bedistilled. By doing so, DCD essentially provides the learning guidance tailoredto "correcting" what the student model has failed to accurately predict. Thisprocess is applied for transferring the ranking information from the user-sideas well as the item-side to address sparse implicit user feedback. Ourexperiments show that the proposed method outperforms the state-of-the-artbaselines, and ablation studies validate the effectiveness of each component.</description><author>Youngjune Lee, Kee-Eung Kim</author><pubDate>Tue, 03 Oct 2023 15:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.03459v3</guid></item><item><title>SIEVE: Multimodal Dataset Pruning Using Image Captioning Models</title><link>http://arxiv.org/abs/2310.02110v1</link><description>Vision-Language Models (VLMs) are pretrained on large, diverse, and noisyweb-crawled datasets. This underscores the critical need for dataset pruning,as the quality of these datasets is strongly correlated with the performance ofVLMs on downstream tasks. Using CLIPScore from a pretrained model to only trainmodels using highly-aligned samples is one of the most successful methods forpruning.We argue that this approach suffers from multiple limitationsincluding: 1) false positives due to spurious correlations captured by thepretrained CLIP model, 2) false negatives due to poor discrimination betweenhard and bad samples, and 3) biased ranking towards samples similar to thepretrained CLIP dataset. We propose a pruning method, SIEVE, that employssynthetic captions generated by image-captioning models pretrained on small,diverse, and well-aligned image-text pairs to evaluate the alignment of noisyimage-text pairs. To bridge the gap between the limited diversity of generatedcaptions and the high diversity of alternative text (alt-text), we estimate thesemantic textual similarity in the embedding space of a language modelpretrained on billions of sentences. Using DataComp, a multimodal datasetfiltering benchmark, we achieve state-of-the-art performance on the large scalepool, and competitive results on the medium scale pool, surpassingCLIPScore-based filtering by 1.7% and 2.6% on average, on 38 downstream tasks.</description><author>Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, Ari Morcos</author><pubDate>Tue, 03 Oct 2023 15:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02110v1</guid></item><item><title>Towards Effective Human-AI Decision-Making: The Role of Human Learning in Appropriate Reliance on AI Advice</title><link>http://arxiv.org/abs/2310.02108v1</link><description>The true potential of human-AI collaboration lies in exploiting thecomplementary capabilities of humans and AI to achieve a joint performancesuperior to that of the individual AI or human, i.e., to achieve complementaryteam performance (CTP). To realize this complementarity potential, humans needto exercise discretion in following AI 's advice, i.e., appropriately relyingon the AI's advice. While previous work has focused on building a mental modelof the AI to assess AI recommendations, recent research has shown that themental model alone cannot explain appropriate reliance. We hypothesize that, inaddition to the mental model, human learning is a key mediator of appropriatereliance and, thus, CTP. In this study, we demonstrate the relationship betweenlearning and appropriate reliance in an experiment with 100 participants. Thiswork provides fundamental concepts for analyzing reliance and derivesimplications for the effective design of human-AI decision-making.</description><author>Max Schemmer, Andrea Bartos, Philipp Spitzer, Patrick Hemmer, Niklas Kühl, Jonas Liebschner, Gerhard Satzger</author><pubDate>Tue, 03 Oct 2023 15:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02108v1</guid></item><item><title>Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance</title><link>http://arxiv.org/abs/2310.02107v1</link><description>Enabling large language models (LLMs) to perform tasks in zero-shot has beenan appealing goal owing to its labor-saving (i.e., requiring no task-specificannotations); as such, zero-shot prompting approaches also enjoy better taskgeneralizability. To improve LLMs' zero-shot performance, prior work hasfocused on devising more effective task instructions (e.g., ``let's think stepby step'' ). However, we argue that, in order for an LLM to solve themcorrectly in zero-shot, individual test instances need more carefully designedand customized instructions. To this end, we propose PRoMPTd, an approach thatrewrites the task prompt for each individual test input to be more specific,unambiguous, and complete, so as to provide better guidance to the task LLM. Weevaluated PRoMPTd on eight datasets covering tasks including arithmetics,logical reasoning, and code generation, using GPT-4 as the task LLM. Notably,\algoname achieves an absolute improvement of around 10\% on the complex MATHdataset and 5\% on the code generation task on HumanEval, outperformingconventional zero-shot methods. In addition, we also showed that the rewrittenprompt can provide better interpretability of how the LLM resolves each testinstance, which can potentially be leveraged as a defense mechanism againstadversarial prompting. The source code and dataset can be obtained fromhttps://github.com/salokr/PRoMPTd</description><author>Saurabh Srivastava, Chengyue Huang, Weiguo Fan, Ziyu Yao</author><pubDate>Tue, 03 Oct 2023 15:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02107v1</guid></item><item><title>Deep Contrastive Patch-Based Subspace Learning for Camera Image Signal Processing</title><link>http://arxiv.org/abs/2104.00253v4</link><description>Camera Image Signal Processing (ISP) pipelines can get appealing results indifferent image signal processing tasks. Nonetheless, the majority of thesemethods, including those employing an encoder-decoder deep architecture for thetask, typically utilize a uniform filter applied consistently across the entireimage. However, it is natural to view a camera image as heterogeneous, as thecolor intensity and the artificial noise are distributed vastly differently,even across the two-dimensional domain of a single image. Varied Moire ringing,motion blur, color-bleaching, or lens-based projection distortions can allpotentially lead to a heterogeneous image artifact filtering problem. In thispaper, we present a specific patch-based, local subspace deep neural networkthat improves Camera ISP to be robust to heterogeneous artifacts (especiallyimage denoising). We call our three-fold deep-trained model the Patch SubspaceLearning Autoencoder (PSL-AE). The PSL-AE model does not make assumptionsregarding uniform levels of image distortion. Instead, it first encodes patchesextracted from noisy a nd clean image pairs, with different artifact types ordistortion levels, by contrastive learning. Then, the patches of each image areencoded into corresponding soft clusters within their suitable latentsub-space, utilizing a prior mixture model. Furthermore, the decoders undergotraining in an unsupervised manner, specifically trained for the image patchespresent in each cluster. The experiments highlight the adaptability andefficacy through enhanced heterogeneous filtering, both from synthesizedartifacts but also realistic SIDD image pairs.</description><author>Yunhao Yang, Yi Wang, Chandrajit Bajaj</author><pubDate>Tue, 03 Oct 2023 15:48:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.00253v4</guid></item><item><title>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</title><link>http://arxiv.org/abs/2307.16789v2</link><description>Despite the advancements of open-source large language models (LLMs), e.g.,LLaMA, they remain significantly limited in tool-use capabilities, i.e., usingexternal tools (APIs) to fulfill human instructions. The reason is that currentinstruction tuning largely focuses on basic language tasks but ignores thetool-use domain. This is in contrast to the excellent tool-use capabilities ofstate-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap,we introduce ToolLLM, a general tool-use framework encompassing dataconstruction, model training, and evaluation. We first present ToolBench, aninstruction-tuning dataset for tool use, which is constructed automaticallyusing ChatGPT. Specifically, the construction can be divided into three stages:(i) API collection: we collect 16,464 real-world RESTful APIs spanning 49categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT togenerate diverse instructions involving these APIs, covering both single-tooland multi-tool scenarios; (iii) solution path annotation: we use ChatGPT tosearch for a valid solution path (chain of API calls) for each instruction. Toenhance the reasoning capabilities of LLMs, we develop a novel depth-firstsearch-based decision tree algorithm. It enables LLMs to evaluate multiplereasoning traces and expand the search space. Moreover, to evaluate thetool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval.Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip itwith a neural API retriever to recommend appropriate APIs for each instruction.Experiments show that ToolLLaMA demonstrates a remarkable ability to executecomplex instructions and generalize to unseen APIs, and exhibits comparableperformance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shotgeneralization ability in an out-of-distribution tool-use dataset: APIBench.</description><author>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun</author><pubDate>Tue, 03 Oct 2023 15:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16789v2</guid></item><item><title>Set-Type Belief Propagation with Applications to Poisson Multi-Bernoulli SLAM</title><link>http://arxiv.org/abs/2305.04797v2</link><description>Belief propagation (BP) is a useful probabilistic inference algorithm forefficiently computing approximate marginal probability densities of randomvariables. However, in its standard form, BP is only applicable to thevector-type random variables with a fixed and known number of vector elements,while certain applications rely on RFSs with an unknown number of vectorelements. In this paper, we develop BP rules for factor graphs defined onsequences of RFSs where each RFS has an unknown number of elements, with theintention of deriving novel inference methods for RFSs. Furthermore, we showthat vector-type BP is a special case of set-type BP, where each RFS followsthe Bernoulli process. To demonstrate the validity of developed set-type BP, weapply it to the PMB filter for SLAM, which naturally leads to new set-typeBP-mapping, SLAM, multi-target tracking, and simultaneous localization andtracking filters. Finally, we explore the relationships between the vector-typeBP and the proposed set-type BP PMB-SLAM implementations and show a performancegain of the proposed set-type BP PMB-SLAM filter in comparison with thevector-type BP-SLAM filter.</description><author>Hyowon Kim, Angel F. García-Fernández, Yu Ge, Yuxuan Xia, Lennart Svensson, Henk Wymeersch</author><pubDate>Tue, 03 Oct 2023 15:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04797v2</guid></item><item><title>Leveraging Classic Deconvolution and Feature Extraction in Zero-Shot Image Restoration</title><link>http://arxiv.org/abs/2310.02097v1</link><description>Non-blind deconvolution aims to restore a sharp image from its blurredcounterpart given an obtained kernel. Existing deep neural architectures areoften built based on large datasets of sharp ground truth images and trainedwith supervision. Sharp, high quality ground truth images, however, are notalways available, especially for biomedical applications. This severely hampersthe applicability of current approaches in practice. In this paper, we proposea novel non-blind deconvolution method that leverages the power of deeplearning and classic iterative deconvolution algorithms. Our approach combinesa pre-trained network to extract deep features from the input image withiterative Richardson-Lucy deconvolution steps. Subsequently, a zero-shotoptimisation process is employed to integrate the deconvolved features,resulting in a high-quality reconstructed image. By performing the preliminaryreconstruction with the classic iterative deconvolution method, we caneffectively utilise a smaller network to produce the final image, thusaccelerating the reconstruction whilst reducing the demand for valuablecomputational resources. Our method demonstrates significant improvements invarious real-world applications non-blind deconvolution tasks.</description><author>Tomáš Chobola, Gesine Müller, Veit Dausmann, Anton Theileis, Jan Taucher, Jan Huisken, Tingying Peng</author><pubDate>Tue, 03 Oct 2023 15:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02097v1</guid></item><item><title>CoNO: Complex Neural Operator for Continuous Dynamical Systems</title><link>http://arxiv.org/abs/2310.02094v1</link><description>Neural operators extend data-driven models to map betweeninfinite-dimensional functional spaces. These models have successfully solvedcontinuous dynamical systems represented by differential equations, viz weatherforecasting, fluid flow, or solid mechanics. However, the existing operatorsstill rely on real space, thereby losing rich representations potentiallycaptured in the complex space by functional transforms. In this paper, weintroduce a Complex Neural Operator (CoNO), that parameterizes the integralkernel in the complex fractional Fourier domain. Additionally, the modelemploying a complex-valued neural network along with aliasing-free activationfunctions preserves the complex values and complex algebraic properties,thereby enabling improved representation, robustness to noise, andgeneralization. We show that the model effectively captures the underlyingpartial differential equation with a single complex fractional Fouriertransform. We perform an extensive empirical evaluation of CoNO on severaldatasets and additional tasks such as zero-shot super-resolution, evaluation ofout-of-distribution data, data efficiency, and robustness to noise. CoNOexhibits comparable or superior performance to all the state-of-the-art modelsin these tasks. Altogether, CoNO presents a robust and superior model formodeling continuous dynamical systems, providing a fillip to scientific machinelearning.</description><author>Karn Tiwari, N M Anoop Krishnan, Prathosh A P</author><pubDate>Tue, 03 Oct 2023 15:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02094v1</guid></item><item><title>Stochastic Gradient Descent with Preconditioned Polyak Step-size</title><link>http://arxiv.org/abs/2310.02093v1</link><description>Stochastic Gradient Descent (SGD) is one of the many iterative optimizationmethods that are widely used in solving machine learning problems. Thesemethods display valuable properties and attract researchers and industrialmachine learning engineers with their simplicity. However, one of theweaknesses of this type of methods is the necessity to tune learning rate(step-size) for every loss function and dataset combination to solve anoptimization problem and get an efficient performance in a given time budget.Stochastic Gradient Descent with Polyak Step-size (SPS) is a method that offersan update rule that alleviates the need of fine-tuning the learning rate of anoptimizer. In this paper, we propose an extension of SPS that employspreconditioning techniques, such as Hutchinson's method, Adam, and AdaGrad, toimprove its performance on badly scaled and/or ill-conditioned datasets.</description><author>Farshed Abdukhakimov, Chulu Xiang, Dmitry Kamzolov, Martin Takáč</author><pubDate>Tue, 03 Oct 2023 15:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02093v1</guid></item><item><title>1D-CapsNet-LSTM: A Deep Learning-Based Model for Multi-Step Stock Index Forecasting</title><link>http://arxiv.org/abs/2310.02090v1</link><description>Multi-step forecasting of stock market index prices is a crucial task in thefinancial sector, playing a pivotal role in decision-making across variousfinancial activities. However, forecasting results are often unsatisfactoryowing to the stochastic and volatile nature of the data. Researchers have madevarious attempts, and this process is ongoing. Inspired by convolutional neuralnetwork long short-term memory (CNN-LSTM) networks that utilize a 1D CNN forfeature extraction to boost model performance, this study explores the use of acapsule network (CapsNet) as an advanced feature extractor in an LSTM-basedforecasting model to enhance multi-step predictions. To this end, a novelneural architecture called 1D-CapsNet-LSTM was introduced, which combines a 1DCapsNet to extract high-level features from 1D sequential data and an LSTMlayer to capture the temporal dependencies between the previously extractedfeatures and uses a multi-input multi-output (MIMO) strategy to maintain thestochastic dependencies between the predicted values at different time steps.The proposed model was evaluated based on several real-world stock marketindices, including Standard &amp; Poor's 500 (S&amp;P 500), Dow Jones IndustrialAverage (DJIA), Nasdaq Composite Index (IXIC), and New York Stock Exchange(NYSE), and was compared with baseline models such as LSTM, recurrent neuralnetwork (RNN), and CNN-LSTM in terms of various evaluation metrics. Thecomparison results suggest that the 1D-CapsNet-LSTM model outperforms thebaseline models and has immense potential for the effective handling of complexprediction tasks.</description><author>Cheng Zhang, Nilam Nur Amir Sjarif, Roslina Ibrahim</author><pubDate>Tue, 03 Oct 2023 15:33:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02090v1</guid></item><item><title>On the Unlikelihood of D-Separation</title><link>http://arxiv.org/abs/2303.05628v2</link><description>Causal discovery aims to recover a causal graph from data generated by it;constraint based methods do so by searching for a d-separating conditioning setof nodes in the graph via an oracle. In this paper, we provide analyticevidence that on large graphs, d-separation is a rare phenomenon, even whenguaranteed to exist, unless the graph is extremely sparse. We then provide ananalytic average case analysis of the PC Algorithm for causal discovery, aswell as a variant of the SGS Algorithm we call UniformSGS. We consider a set$V=\{v_1,\ldots,v_n\}$ of nodes, and generate a random DAG $G=(V,E)$ where$(v_a, v_b) \in E$ with i.i.d. probability $p_1$ if $a&lt;b$ and $0$ if $a &gt; b$.We provide upper bounds on the probability that a subset of $V-\{x,y\}$d-separates $x$ and $y$, conditional on $x$ and $y$ being d-separable; ourupper bounds decay exponentially fast to $0$ as $|V| \rightarrow \infty$. Forthe PC Algorithm, while it is known that its worst-case guarantees fail onnon-sparse graphs, we show that the same is true for the average case, and thatthe sparsity requirement is quite demanding: for good performance, the densitymust go to $0$ as $|V| \rightarrow \infty$ even in the average case. ForUniformSGS, while it is known that the running time is exponential for existingedges, we show that in the average case, that is the expected running time formost non-existing edges as well.</description><author>Itai Feigenbaum, Huan Wang, Shelby Heinecke, Juan Carlos Niebles, Weiran Yao, Caiming Xiong, Devansh Arpit</author><pubDate>Tue, 03 Oct 2023 15:32:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05628v2</guid></item><item><title>Point Neighborhood Embeddings</title><link>http://arxiv.org/abs/2310.02083v1</link><description>Point convolution operations rely on different embedding mechanisms to encodethe neighborhood information of each point in order to detect patterns in 3Dspace. However, as convolutions are usually evaluated as a whole, not much workhas been done to investigate which is the ideal mechanism to encode suchneighborhood information. In this paper, we provide the first extensive studythat analyzes such Point Neighborhood Embeddings (PNE) alone in a controlledexperimental setup. From our experiments, we derive a set of recommendationsfor PNE that can help to improve future designs of neural network architecturesfor point clouds. Our most surprising finding shows that the most commonly usedembedding based on a Multi-layer Perceptron (MLP) with ReLU activationfunctions provides the lowest performance among all embeddings, even beingsurpassed on some tasks by a simple linear combination of the pointcoordinates. Additionally, we show that a neural network architecture usingsimple convolutions based on such embeddings is able to achievestate-of-the-art results on several tasks, outperforming recent and morecomplex operations. Lastly, we show that these findings extrapolate to othermore complex convolution operations, where we show how following ourrecommendations we are able to improve recent state-of-the-art architectures.</description><author>Pedro Hermosilla</author><pubDate>Tue, 03 Oct 2023 15:26:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02083v1</guid></item><item><title>Transforming Transformers for Resilient Lifelong Learning</title><link>http://arxiv.org/abs/2303.08250v3</link><description>Lifelong learning without catastrophic forgetting (i.e., resiliency) remainsan open problem for deep neural networks. The prior art mostly focuses onconvolutional neural networks. With the increasing dominance of Transformers indeep learning, it is a pressing need to study lifelong learning withTransformers. Due to the complexity of training Transformers in practice, forlifelong learning, a question naturally arises: Can Transformers be learned togrow in a task aware way, that is to be dynamically transformed by introducinglightweight learnable plastic components to the architecture, while retainingthe parameter-heavy, but stable components at streaming tasks? To that end,motivated by the lifelong learning capability maintained by the functionalityof Hippocampi in human brain, we explore what would be, and how to implement,Artificial Hippocampi (ArtiHippo) in Transformers. We present a method toidentify, and learn to grow, ArtiHippo in Vision Transformers (ViTs) forresilient lifelong learning in four aspects: (i) Where to place ArtiHippo toenable plasticity while preserving the core function of ViTs at streamingtasks? (ii) How to represent and realize ArtiHippo to ensure expressivity andadaptivity for tackling tasks of different nature in lifelong learning? (iii)How to learn to grow ArtiHippo to exploit task synergies (i.e., the learnedknowledge) and overcome catastrophic forgetting? (iv) How to harness the bestof our proposed ArtiHippo and prompting-based approaches? In experiments, wetest the proposed method on the challenging Visual Domain Decathlon (VDD)benchmark and the 5-Dataset benchmark under the task-incremental lifelonglearning setting. It obtains consistently better performance than the prior artwith sensible ArtiHippo learned continually. To our knowledge, it is the firstattempt of lifelong learning with ViTs on the challenging VDD benchmark.</description><author>Chinmay Savadikar, Michelle Dai, Tianfu Wu</author><pubDate>Tue, 03 Oct 2023 15:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08250v3</guid></item><item><title>A Survey on Reinforcement Learning for Combinatorial Optimization</title><link>http://arxiv.org/abs/2008.12248v3</link><description>This paper gives a detailed review of reinforcement learning (RL) incombinatorial optimization, introduces the history of combinatorialoptimization starting in the 1950s, and compares it with the RL algorithms ofrecent years. This paper explicitly looks at a famous combinatorialproblem-traveling salesperson problem (TSP). It compares the approach of modernRL algorithms for the TSP with an approach published in the 1970s. By comparingthe similarities and variances between these methodologies, the paperdemonstrates how RL algorithms are optimized due to the evolution of machinelearning techniques and computing power. The paper then briefly introduces thedeep learning approach to the TSP named deep RL, which is an extension of thetraditional mathematical framework. In deep RL, attention and feature encodingmechanisms are introduced to generate near-optimal solutions. The survey showsthat integrating the deep learning mechanism, such as attention with RL, caneffectively approximate the TSP. The paper also argues that deep learning couldbe a generic approach that can be integrated with any traditional RL algorithmto enhance the outcomes of the TSP.</description><author>Yunhao Yang, Andrew Whinston</author><pubDate>Tue, 03 Oct 2023 15:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2008.12248v3</guid></item><item><title>Learning Quantum Processes with Quantum Statistical Queries</title><link>http://arxiv.org/abs/2310.02075v1</link><description>Learning complex quantum processes is a central challenge in many areas ofquantum computing and quantum machine learning, with applications in quantumbenchmarking, cryptanalysis, and variational quantum algorithms. This paperintroduces the first learning framework for studying quantum process learningwithin the Quantum Statistical Query (QSQ) model, providing the first formaldefinition of statistical queries to quantum processes (QPSQs). The frameworkallows us to propose an efficient QPSQ learner for arbitrary quantum processesaccompanied by a provable performance guarantee. We also provide numericalsimulations to demonstrate the efficacy of this algorithm. The practicalrelevance of this framework is exemplified through application incryptanalysis, highlighting vulnerabilities of Classical-Readout QuantumPhysical Unclonable Functions (CR-QPUFs), addressing an important open questionin the field of quantum hardware security. This work marks a significant steptowards understanding the learnability of quantum processes and shedding lighton their security implications.</description><author>Chirag Wadhwa, Mina Doosti</author><pubDate>Tue, 03 Oct 2023 15:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02075v1</guid></item><item><title>ACE: A fast, skillful learned global atmospheric model for climate prediction</title><link>http://arxiv.org/abs/2310.02074v1</link><description>Existing ML-based atmospheric models are not suitable for climate prediction,which requires long-term stability and physical consistency. We present ACE(AI2 Climate Emulator), a 200M-parameter, autoregressive machine learningemulator of an existing comprehensive 100-km resolution global atmosphericmodel. The formulation of ACE allows evaluation of physical laws such as theconservation of mass and moisture. The emulator is stable for 10 years, nearlyconserves column moisture without explicit constraints and faithfullyreproduces the reference model's climate, outperforming a challenging baselineon over 80% of tracked variables. ACE requires nearly 100x less wall clock timeand is 100x more energy efficient than the reference model using typicallyavailable resources.</description><author>Oliver Watt-Meyer, Gideon Dresdner, Jeremy McGibbon, Spencer K. Clark, Brian Henn, James Duncan, Noah D. Brenowitz, Karthik Kashinath, Michael S. Pritchard, Boris Bonev, Matthew E. Peters, Christopher S. Bretherton</author><pubDate>Tue, 03 Oct 2023 15:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02074v1</guid></item><item><title>Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond</title><link>http://arxiv.org/abs/2310.02071v1</link><description>In this study, we explore the potential of Multimodal Large Language Models(MLLMs) in improving embodied decision-making processes for agents. While LargeLanguage Models (LLMs) have been widely used due to their advanced reasoningskills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visualunderstanding and reasoning capabilities. We investigate whetherstate-of-the-art MLLMs can handle embodied decision-making in an end-to-endmanner and whether collaborations between LLMs and MLLMs can enhancedecision-making. To address these questions, we introduce a new benchmarkcalled PCA-EVAL, which evaluates embodied decision-making from the perspectivesof Perception, Cognition, and Action. Additionally, we propose HOLMES, amulti-agent cooperation framework that allows LLMs to leverage MLLMs and APIsto gather multimodal information for informed decision-making. We compareend-to-end embodied decision-making and HOLMES on our benchmark and find thatthe GPT4-Vision model demonstrates strong end-to-end embodied decision-makingabilities, outperforming GPT4-HOLMES in terms of average decision accuracy(+3%). However, this performance is exclusive to the latest GPT4-Vision model,surpassing the open-source state-of-the-art MLLM by 26%. Our results indicatethat powerful MLLMs like GPT4-Vision hold promise for decision-making inembodied agents, offering new avenues for MLLM research.</description><author>Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Tianyu Liu, Baobao Chang</author><pubDate>Tue, 03 Oct 2023 15:13:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02071v1</guid></item><item><title>ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values</title><link>http://arxiv.org/abs/2309.16916v2</link><description>Understanding why a neural network model makes certain decisions can be asimportant as the inference performance. Various methods have been proposed tohelp practitioners explain the prediction of a neural network model, of whichShapley values are most popular. SHAP package is a leading implementation ofShapley values to explain neural networks implemented in TensorFlow or PyTorchbut lacks cross-platform support, one-shot deployment and is highlyinefficient. To address these problems, we present the ONNXExplainer, which isa generic framework to explain neural networks using Shapley values in the ONNXecosystem. In ONNXExplainer, we develop its own automatic differentiation andoptimization approach, which not only enables One-Shot Deployment of neuralnetworks inference and explanations, but also significantly improves theefficiency to compute explanation with less memory consumption. For faircomparison purposes, we also implement the same optimization in TensorFlow andPyTorch and measure its performance against the current state of the artopen-source counterpart, SHAP. Extensive benchmarks demonstrate that theproposed optimization approach improves the explanation latency of VGG19,ResNet50, DenseNet201, and EfficientNetB0 by as much as 500%.</description><author>Yong Zhao, Runxin He, Nicholas Kersting, Can Liu, Shubham Agrawal, Chiranjeet Chetia, Yu Gu</author><pubDate>Tue, 03 Oct 2023 15:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16916v2</guid></item><item><title>Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability</title><link>http://arxiv.org/abs/2310.02067v1</link><description>In the context of temporal image forensics, it is not evident that a neuralnetwork, trained on images from different time-slots (classes), exploit solelyage related features. Usually, images taken in close temporal proximity (e.g.,belonging to the same age class) share some common content properties. Suchcontent bias can be exploited by a neural network. In this work, a novelapproach that evaluates the influence of image content is proposed. Thisapproach is verified using synthetic images (where content bias can be ruledout) with an age signal embedded. Based on the proposed approach, it is shownthat a `standard' neural network trained in the context of age classificationis strongly dependent on image content. As a potential countermeasure, twodifferent techniques are applied to mitigate the influence of the image contentduring training, and they are also evaluated by the proposed method.</description><author>Robert Jöchl, Andreas Uhl</author><pubDate>Tue, 03 Oct 2023 15:09:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02067v1</guid></item><item><title>De Novo Drug Design with Joint Transformers</title><link>http://arxiv.org/abs/2310.02066v1</link><description>De novo drug design requires simultaneously generating novel moleculesoutside of training data and predicting their target properties, making it ahard task for generative models. To address this, we propose Joint Transformerthat combines a Transformer decoder, a Transformer encoder, and a predictor ina joint generative model with shared weights. We show that training the modelwith a penalized log-likelihood objective results in state-of-the-artperformance in molecule generation, while decreasing the prediction error onnewly sampled molecules, as compared to a fine-tuned decoder-only Transformer,by 42%. Finally, we propose a probabilistic black-box optimization algorithmthat employs Joint Transformer to generate novel molecules with improved targetproperties, as compared to the training data, outperforming other SMILES-basedoptimization methods in de novo drug design.</description><author>Adam Izdebski, Ewelina Weglarz-Tomczak, Ewa Szczurek, Jakub M. Tomczak</author><pubDate>Tue, 03 Oct 2023 15:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02066v1</guid></item><item><title>VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores</title><link>http://arxiv.org/abs/2310.02065v1</link><description>The increasing success and scaling of Deep Learning models demands highercomputational efficiency and power. Sparsification can lead to both smallermodels as well as higher compute efficiency, and accelerated hardware isbecoming available. However, exploiting it efficiently requires kernelimplementations, pruning algorithms, and storage formats, to utilize hardwaresupport of specialized sparse vector units. An example of those are theNVIDIA's Sparse Tensor Cores (SPTCs), which promise a 2x speedup. However,SPTCs only support the 2:4 format, limiting achievable sparsity ratios to 50%.We present the V:N:M format, which enables the execution of arbitrary N:Mratios on SPTCs. To efficiently exploit the resulting format, we proposeSpatha, a high-performance sparse-library for DL routines. We show that Spathaachieves up to 37x speedup over cuBLAS. We also demonstrate a second-orderpruning technique that enables sparsification to high sparsity ratios withV:N:M and little to no loss in accuracy in modern transformers.</description><author>Roberto L. Castro, Andrei Ivanov, Diego Andrade, Tal Ben-Nun, Basilio B. Fraguela, Torsten Hoefler</author><pubDate>Tue, 03 Oct 2023 15:08:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02065v1</guid></item><item><title>Lessons Learned from EXMOS User Studies: A Technical Report Summarizing Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform</title><link>http://arxiv.org/abs/2310.02063v1</link><description>In the realm of interactive machine-learning systems, the provision ofexplanations serves as a vital aid in the processes of debugging and enhancingprediction models. However, the extent to which various global model-centricand data-centric explanations can effectively assist domain experts indetecting and resolving potential data-related issues for the purpose of modelimprovement has remained largely unexplored. In this technical report, wesummarise the key findings of our two user studies. Our research involved acomprehensive examination of the impact of global explanations rooted in bothdata-centric and model-centric perspectives within systems designed to supporthealthcare experts in optimising machine learning models through both automatedand manual data configurations. To empirically investigate these dynamics, weconducted two user studies, comprising quantitative analysis involving a samplesize of 70 healthcare experts and qualitative assessments involving 30healthcare experts. These studies were aimed at illuminating the influence ofdifferent explanation types on three key dimensions: trust, understandability,and model improvement. Results show that global model-centric explanationsalone are insufficient for effectively guiding users during the intricateprocess of data configuration. In contrast, data-centric explanations exhibitedtheir potential by enhancing the understanding of system changes that occurpost-configuration. However, a combination of both showed the highest level ofefficacy for fostering trust, improving understandability, and facilitatingmodel enhancement among healthcare experts. We also present essentialimplications for developing interactive machine-learning systems driven byexplanations. These insights can guide the creation of more effective systemsthat empower domain experts to harness the full potential of machine learning</description><author>Aditya Bhattacharya, Simone Stumpf, Lucija Gosak, Gregor Stiglic, Katrien Verbert</author><pubDate>Tue, 03 Oct 2023 15:04:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02063v1</guid></item><item><title>Global Attractor for a Reaction-Diffusion Model Arising in Biological Dynamic in 3D Soil Structure</title><link>http://arxiv.org/abs/2310.02060v1</link><description>Partial Differential Equations (PDEs) play a crucial role as tools formodeling and comprehending intricate natural processes, notably within thedomain of biology. This research explores the domain of microbial activitywithin the complex matrix of 3D soil structures, providing valuableunderstanding into both the existence and uniqueness of solutions and theasymptotic behavior of the corresponding PDE model. Our investigation resultsin the discovery of a global attractor, a fundamental feature with significantimplications for long-term system behavior. To enhance the clarity of ourfindings, numerical simulations are employed to visually illustrate theattributes of this global attractor.</description><author>Mohamed Elghandouri, Khalil Ezzinbi, Mouad Klai, Olivier Monga</author><pubDate>Tue, 03 Oct 2023 15:03:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02060v1</guid></item><item><title>Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning</title><link>http://arxiv.org/abs/2201.12143v2</link><description>Locally interpretable model agnostic explanations (LIME) method is one of themost popular methods used to explain black-box models at a per example level.Although many variants have been proposed, few provide a simple way to producehigh fidelity explanations that are also stable and intuitive. In this work, weprovide a novel perspective by proposing a model agnostic local explanationmethod inspired by the invariant risk minimization (IRM) principle --originally proposed for (global) out-of-distribution generalization -- toprovide such high fidelity explanations that are also stable and unidirectionalacross nearby examples. Our method is based on a game theoretic formulationwhere we theoretically show that our approach has a strong tendency toeliminate features where the gradient of the black-box function abruptlychanges sign in the locality of the example we want to explain, while in othercases it is more careful and will choose a more conservative (feature)attribution, a behavior which can be highly desirable for recourse.Empirically, we show on tabular, image and text data that the quality of ourexplanations with neighborhoods formed using random perturbations are muchbetter than LIME and in some cases even comparable to other methods that userealistic neighbors sampled from the data manifold. This is desirable giventhat learning a manifold to either create realistic neighbors or to projectexplanations is typically expensive or may even be impossible. Moreover, ouralgorithm is simple and efficient to train, and can ascertain stable inputfeatures for local decisions of a black-box without access to side informationsuch as a (partial) causal graph as has been seen in some recent works.</description><author>Amit Dhurandhar, Karthikeyan Ramamurthy, Kartik Ahuja, Vijay Arya</author><pubDate>Tue, 03 Oct 2023 14:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.12143v2</guid></item><item><title>The Probabilistic Stability of Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2303.13093v2</link><description>Characterizing and understanding the stability of Stochastic Gradient Descent(SGD) remains an open problem in deep learning. A common method is to utilizethe convergence of statistical moments, esp. the variance, of the parameters toquantify the stability. We revisit the definition of stability for SGD andpropose using the $\textit{convergence in probability}$ condition to define the$\textit{probabilistic stability}$ of SGD. The probabilistic stability shedslight on a fundamental question in deep learning theory: how SGD selects ameaningful solution for a neural network from an enormous number of possiblesolutions that may severely overfit. We show that only through the lens ofprobabilistic stability does SGD exhibit rich and practically relevant phasesof learning, such as the phases of the complete loss of stability, incorrectlearning where the model captures incorrect data correlation, convergence tolow-rank saddles, and correct learning where the model captures the correctcorrelation. These phase boundaries are precisely quantified by the Lyapunovexponents of the dynamics. The obtained phase diagrams imply that SGD preferslow-rank saddles in a neural network when the underlying gradient is noisy,thereby influencing the learning performance.</description><author>Liu Ziyin, Botao Li, Tomer Galanti, Masahito Ueda</author><pubDate>Tue, 03 Oct 2023 14:56:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13093v2</guid></item></channel></rss>