<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 15 Nov 2024 13:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MagicQuill: An Intelligent Interactive Image Editing System</title><link>http://arxiv.org/abs/2411.09703v1</link><description>Image editing involves a variety of complex tasks and requires efficient andprecise manipulation techniques. In this paper, we present MagicQuill, anintegrated image editing system that enables swift actualization of creativeideas. Our system features a streamlined yet functionally robust interface,allowing for the articulation of editing operations (e.g., inserting elements,erasing objects, altering color) with minimal input. These interactions aremonitored by a multimodal large language model (MLLM) to anticipate editingintentions in real time, bypassing the need for explicit prompt entry. Finally,we apply a powerful diffusion prior, enhanced by a carefully learned two-branchplug-in module, to process editing requests with precise control. Experimentalresults demonstrate the effectiveness of MagicQuill in achieving high-qualityimage edits. Please visit https://magic-quill.github.io to try out our system.</description><author>Zichen Liu, Yue Yu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Wen Wang, Zhiheng Liu, Qifeng Chen, Yujun Shen</author><pubDate>Thu, 14 Nov 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09703v1</guid></item><item><title>On the Surprising Effectiveness of Attention Transfer for Vision Transformers</title><link>http://arxiv.org/abs/2411.09702v1</link><description>Conventional wisdom suggests that pre-training Vision Transformers (ViT)improves downstream performance by learning useful representations. Is thisactually true? We investigate this question and find that the features andrepresentations learned during pre-training are not essential. Surprisingly,using only the attention patterns from pre-training (i.e., guiding howinformation flows between tokens) is sufficient for models to learn highquality features from scratch and achieve comparable downstream performance. Weshow this by introducing a simple method called attention transfer, where onlythe attention patterns from a pre-trained teacher ViT are transferred to astudent, either by copying or distilling the attention maps. Since attentiontransfer lets the student learn its own features, ensembling it with afine-tuned teacher also further improves accuracy on ImageNet. Wesystematically study various aspects of our findings on the sufficiency ofattention maps, including distribution shift settings where they underperformfine-tuning. We hope our exploration provides a better understanding of whatpre-training accomplishes and leads to a useful alternative to the standardpractice of fine-tuning</description><author>Alexander C. Li, Yuandong Tian, Beidi Chen, Deepak Pathak, Xinlei Chen</author><pubDate>Thu, 14 Nov 2024 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09702v1</guid></item><item><title>A Bayesian Optimization Approach to Machine Translation Reranking</title><link>http://arxiv.org/abs/2411.09694v1</link><description>Reranking a list of candidates from a machine translation system with anexternal scoring model and returning the highest-scoring candidate remains asimple and effective method for improving the overall output quality.Translation scoring models continue to grow in size, with the best models beingcomparable to generation models. Thus, reranking can add substantialcomputational cost to the translation pipeline. In this work, we pose rerankingas a Bayesian optimization (BayesOpt) problem. By strategically selectingcandidates to score based on a balance of exploration and exploitation, we showthat it is possible to find top-scoring candidates when scoring only a fractionof the candidate list. For instance, our method achieves the same CometKiwiscore using only 70 scoring evaluations compared a baseline system using 180.We present a multi-fidelity setting for BayesOpt, where the candidates arefirst scored with a cheaper but noisier proxy scoring model, which furtherimproves the cost-performance tradeoff when using smaller but well-traineddistilled proxy scorers.</description><author>Julius Cheng, Maike Züfle, Vilém Zouhar, Andreas Vlachos</author><pubDate>Thu, 14 Nov 2024 18:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09694v1</guid></item><item><title>CropCraft: Inverse Procedural Modeling for 3D Reconstruction of Crop Plants</title><link>http://arxiv.org/abs/2411.09693v1</link><description>The ability to automatically build 3D digital twins of plants from images hascountless applications in agriculture, environmental science, robotics, andother fields. However, current 3D reconstruction methods fail to recovercomplete shapes of plants due to heavy occlusion and complex geometries. Inthis work, we present a novel method for 3D reconstruction of agriculturalcrops based on optimizing a parametric model of plant morphology via inverseprocedural modeling. Our method first estimates depth maps by fitting a neuralradiance field and then employs Bayesian optimization to estimate plantmorphological parameters that result in consistent depth renderings. Theresulting 3D model is complete and biologically plausible. We validate ourmethod on a dataset of real images of agricultural fields, and demonstrate thatthe reconstructions can be used for a variety of monitoring and simulationapplications.</description><author>Albert J. Zhai, Xinlei Wang, Kaiyuan Li, Zhao Jiang, Junxiong Zhou, Sheng Wang, Zhenong Jin, Kaiyu Guan, Shenlong Wang</author><pubDate>Thu, 14 Nov 2024 18:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09693v1</guid></item><item><title>Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)</title><link>http://arxiv.org/abs/2405.09596v2</link><description>The prediction of ship trajectories is a growing field of study in artificialintelligence. Traditional methods rely on the use of LSTM, GRU networks, andeven Transformer architectures for the prediction of spatio-temporal series.This study proposes a viable alternative for predicting these trajectoriesusing only GNSS positions. It considers this spatio-temporal problem as anatural language processing problem. The latitude/longitude coordinates of AISmessages are transformed into cell identifiers using the H3 index. Thanks tothe pseudo-octal representation, it becomes easier for language models to learnthe spatial hierarchy of the H3 index. The method is compared with a classicalKalman filter, widely used in the maritime domain, and introduces the Fr\'echetdistance as the main evaluation metric. We show that it is possible to predictship trajectories quite precisely up to 8 hours ahead with 30 minutes ofcontext, using solely GNSS positions, without relying on any additionalinformation such as speed, course, or external conditions - unlike manytraditional methods. We demonstrate that this alternative works well enough topredict trajectories worldwide.</description><author>Nicolas Drapier, Aladine Chetouani, Aurélien Chateigner</author><pubDate>Thu, 14 Nov 2024 18:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09596v2</guid></item><item><title>Advancing Fine-Grained Visual Understanding with Multi-Scale Alignment in Multi-Modal Models</title><link>http://arxiv.org/abs/2411.09691v1</link><description>Multi-modal large language models (MLLMs) have achieved remarkable success infine-grained visual understanding across a range of tasks. However, they oftenencounter significant challenges due to inadequate alignment for fine-grainedknowledge, which restricts their ability to accurately capture local detailsand attain a comprehensive global perception. While recent advancements havefocused on aligning object expressions with grounding information, theytypically lack explicit integration of object images, which contain affluentinformation beyond mere texts or coordinates. To bridge this gap, we introducea novel fine-grained visual knowledge alignment method that effectively alignsand integrates multi-scale knowledge of objects, including texts, coordinates,and images. This innovative method is underpinned by our multi-scalefine-grained enhancement data synthesis pipeline, which provides over 300Kessential training data to enhance alignment and improve overall performance.Furthermore, we present TinyGroundingGPT, a series of compact models optimizedfor high-level alignments. With a scale of approximately 3B parameters,TinyGroundingGPT achieves outstanding results in grounding tasks whiledelivering performance comparable to larger MLLMs in complex visual scenarios.</description><author>Wei Wang, Zhaowei Li, Qi Xu, Linfeng Li, YiQing Cai, Botian Jiang, Hang Song, Xingcan Hu, Pengyu Wang, Li Xiao</author><pubDate>Thu, 14 Nov 2024 18:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09691v1</guid></item><item><title>LLM Hallucination Reasoning with Zero-shot Knowledge Test</title><link>http://arxiv.org/abs/2411.09689v1</link><description>LLM hallucination, where LLMs occasionally generate unfaithful text, posessignificant challenges for their practical applications. Most existingdetection methods rely on external knowledge, LLM fine-tuning, orhallucination-labeled datasets, and they do not distinguish between differenttypes of hallucinations, which are crucial for improving detection performance.We introduce a new task, Hallucination Reasoning, which classifiesLLM-generated text into one of three categories: aligned, misaligned, andfabricated. Our novel zero-shot method assesses whether LLM has enoughknowledge about a given prompt and text. Our experiments conducted on newdatasets demonstrate the effectiveness of our method in hallucination reasoningand underscore its importance for enhancing detection performance.</description><author>Seongmin Lee, Hsiang Hsu, Chun-Fu Chen</author><pubDate>Thu, 14 Nov 2024 18:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09689v1</guid></item><item><title>Squeezed Attention: Accelerating Long Context Length LLM Inference</title><link>http://arxiv.org/abs/2411.09688v1</link><description>Emerging Large Language Model (LLM) applications require long input promptsto perform complex downstream tasks like document analysis and code generation.For these long context length applications, the length of the input promptposes a significant challenge in terms of inference efficiency since theinference costs increase linearly with sequence length. However, for many ofthese applications, much of the context in the prompt is fixed across differentuser inputs, thereby providing the opportunity to perform offline optimizationsto process user inputs quickly, as they are received. In this work, we proposeSqueezed Attention as a mechanism to accelerate LLM applications where a largeportion of the input prompt is fixed. We first leverage K-means clusteringoffline to group the keys for the fixed context based on semantic similarityand represent each cluster with a single centroid value. During inference, wecompare query tokens from the user input with the centroids to predict which ofthe keys from the fixed context are semantically relevant and need to be loadedduring inference. We then compute exact attention using only these importantkeys from the fixed context, thereby reducing bandwidth and computationalcosts. We also extend our method to use a hierarchical centroid lookup toidentify important keys, which can reduce the complexity of attention fromlinear to logarithmic with respect to the context length. We implementoptimized Triton kernels for centroid comparison and sparse FlashAttention withimportant keys, achieving more than 4x speedups during both the prefill andgeneration phases for long-context inference. Furthermore, we have extensivelyevaluated our method on various long-context benchmarks including LongBench,where it achieves a 3x reduction in KV cache budget without accuracy loss andup to an 8x reduction with &lt;0.5 point accuracy gap for various models.</description><author>Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael W. Mahoney, Kurt Keutzer, Amir Gholami</author><pubDate>Thu, 14 Nov 2024 18:54:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09688v1</guid></item><item><title>Conditional regression for the Nonlinear Single-Variable Model</title><link>http://arxiv.org/abs/2411.09686v1</link><description>Several statistical models for regression of a function $F$ on $\mathbb{R}^d$without the statistical and computational curse of dimensionality exist, forexample by imposing and exploiting geometric assumptions on the distribution ofthe data (e.g. that its support is low-dimensional), or strong smoothnessassumptions on $F$, or a special structure $F$. Among the latter, compositionalmodels assume $F=f\circ g$ with $g$ mapping to $\mathbb{R}^r$ with $r\ll d$,have been studied, and include classical single- and multi-index models andrecent works on neural networks. While the case where $g$ is linear is ratherwell-understood, much less is known when $g$ is nonlinear, and in particularfor which $g$'s the curse of dimensionality in estimating $F$, or both $f$ and$g$, may be circumvented. In this paper, we consider a model$F(X):=f(\Pi_\gamma X) $ where $\Pi_\gamma:\mathbb{R}^d\to[0,\rm{len}_\gamma]$is the closest-point projection onto the parameter of a regular curve $\gamma:[0,\rm{len}_\gamma]\to\mathbb{R}^d$ and $f:[0,\rm{len}_\gamma]\to\mathbb{R}^1$.The input data $X$ is not low-dimensional, far from $\gamma$, conditioned on$\Pi_\gamma(X)$ being well-defined. The distribution of the data, $\gamma$ and$f$ are unknown. This model is a natural nonlinear generalization of thesingle-index model, which corresponds to $\gamma$ being a line. We propose anonparametric estimator, based on conditional regression, and show that undersuitable assumptions, the strongest of which being that $f$ is coarselymonotone, it can achieve the $one$-$dimensional$ optimal min-max rate fornon-parametric regression, up to the level of noise in the observations, and beconstructed in time $\mathcal{O}(d^2n\log n)$. All the constants in thelearning bounds, in the minimal number of samples required for our bounds tohold, and in the computational complexity are at most low-order polynomials in$d$.</description><author>Yantao Wu, Mauro Maggioni</author><pubDate>Thu, 14 Nov 2024 18:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09686v1</guid></item><item><title>Towards a Classification of Open-Source ML Models and Datasets for Software Engineering</title><link>http://arxiv.org/abs/2411.09683v1</link><description>Background: Open-Source Pre-Trained Models (PTMs) and datasets provideextensive resources for various Machine Learning (ML) tasks, yet theseresources lack a classification tailored to Software Engineering (SE) needs.Aims: We apply an SE-oriented classification to PTMs and datasets on a popularopen-source ML repository, Hugging Face (HF), and analyze the evolution of PTMsover time. Method: We conducted a repository mining study. We started with asystematically gathered database of PTMs and datasets from the HF API. Ourselection was refined by analyzing model and dataset cards and metadata, suchas tags, and confirming SE relevance using Gemini 1.5 Pro. All analyses arereplicable, with a publicly accessible replication package. Results: The mostcommon SE task among PTMs and datasets is code generation, with a primary focuson software development and limited attention to software management. PopularPTMs and datasets mainly target software development. Among ML tasks, textgeneration is the most common in SE PTMs and datasets. There has been a markedincrease in PTMs for SE since 2023 Q2. Conclusions: This study underscores theneed for broader task coverage to enhance the integration of ML within SEpractices.</description><author>Alexandra González, Xavier Franch, David Lo, Silverio Martínez-Fernández</author><pubDate>Thu, 14 Nov 2024 18:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09683v1</guid></item><item><title>NeuralDEM - Real-time Simulation of Industrial Particulate Flows</title><link>http://arxiv.org/abs/2411.09678v1</link><description>Advancements in computing power have made it possible to numerically simulatelarge-scale fluid-mechanical and/or particulate systems, many of which areintegral to core industrial processes. Among the different numerical methodsavailable, the discrete element method (DEM) provides one of the most accuraterepresentations of a wide range of physical systems involving granular anddiscontinuous materials. Consequently, DEM has become a widely acceptedapproach for tackling engineering problems connected to granular flows andpowder mechanics. Additionally, DEM can be integrated with grid-basedcomputational fluid dynamics (CFD) methods, enabling the simulation of chemicalprocesses taking place, e.g., in fluidized beds. However, DEM iscomputationally intensive because of the intrinsic multiscale nature ofparticulate systems, restricting simulation duration or number of particles.Towards this end, NeuralDEM presents an end-to-end approach to replace slownumerical DEM routines with fast, adaptable deep learning surrogates. NeuralDEMis capable of picturing long-term transport processes across different regimesusing macroscopic observables without any reference to microscopic modelparameters. First, NeuralDEM treats the Lagrangian discretization of DEM as anunderlying continuous field, while simultaneously modeling macroscopic behaviordirectly as additional auxiliary fields. Second, NeuralDEM introducesmulti-branch neural operators scalable to real-time modeling ofindustrially-sized scenarios - from slow and pseudo-steady to fast andtransient. Such scenarios have previously posed insurmountable challenges fordeep learning models. Notably, NeuralDEM faithfully models coupled CFD-DEMfluidized bed reactors of 160k CFD cells and 500k DEM particles fortrajectories of 28s. NeuralDEM will open many new doors to advanced engineeringand much faster process cycles.</description><author>Benedikt Alkin, Tobias Kronlachner, Samuele Papa, Stefan Pirker, Thomas Lichtenegger, Johannes Brandstetter</author><pubDate>Thu, 14 Nov 2024 18:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09678v1</guid></item><item><title>I2I-Mamba: Multi-modal medical image synthesis via selective state space modeling</title><link>http://arxiv.org/abs/2405.14022v3</link><description>In recent years, deep learning models comprising transformer components havepushed the performance envelope in medical image synthesis tasks. Contrary toconvolutional neural networks (CNNs) that use static, local filters,transformers use self-attention mechanisms to permit adaptive, non-localfiltering to sensitively capture long-range context. However, this sensitivitycomes at the expense of substantial model complexity, which can compromiselearning efficacy particularly on relatively modest-sized imaging datasets.Here, we propose a novel adversarial model for multi-modal medical imagesynthesis, I2I-Mamba, that leverages selective state space modeling (SSM) toefficiently capture long-range context while maintaining local precision. To dothis, I2I-Mamba injects channel-mixed Mamba (cmMamba) blocks in the bottleneckof a convolutional backbone. In cmMamba blocks, SSM layers are used to learncontext across the spatial dimension and channel-mixing layers are used tolearn context across the channel dimension of feature maps. Comprehensivedemonstrations are reported for imputing missing images in multi-contrast MRIand MRI-CT protocols. Our results indicate that I2I-Mamba offers superiorperformance against state-of-the-art CNN- and transformer-based methods insynthesizing target-modality images.</description><author>Omer F. Atli, Bilal Kabas, Fuat Arslan, Mahmut Yurt, Onat Dalmaz, Tolga Çukur</author><pubDate>Thu, 14 Nov 2024 18:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14022v3</guid></item><item><title>Quantitative Assessment of Intersectional Empathetic Bias and Understanding</title><link>http://arxiv.org/abs/2411.05777v2</link><description>A growing amount of literature critiques the current operationalizations ofempathy based on loose definitions of the construct. Such definitionsnegatively affect dataset quality, model robustness, and evaluationreliability. We propose an empathy evaluation framework that operationalizesempathy close to its psychological origins. The framework measures the variancein responses of LLMs to prompts using existing metrics for empathy andemotional valence. The variance is introduced through the controlled generationof the prompts by varying social biases affecting context understanding, thusimpacting empathetic understanding. The control over generation ensures hightheoretical validity of the constructs in the prompt dataset. Also, it makeshigh-quality translation, especially into languages that currently havelittle-to-no way of evaluating empathy or bias, such as the Slavonic family,more manageable. Using chosen LLMs and various prompt types, we demonstrate theempathy evaluation with the framework, including multiple-choice answers andfree generation. The variance in our initial evaluation sample is small and wewere unable to measure convincing differences between the empatheticunderstanding in contexts given by different social groups. However, theresults are promising because the models showed significant alterations theirreasoning chains needed to capture the relatively subtle changes in theprompts. This provides the basis for future research into the construction ofthe evaluation sample and statistical methods for measuring the results.</description><author>Vojtech Formanek, Ondrej Sotolar</author><pubDate>Thu, 14 Nov 2024 18:35:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05777v2</guid></item><item><title>Adaptive Decoding via Latent Preference Optimization</title><link>http://arxiv.org/abs/2411.09661v1</link><description>During language model decoding, it is known that using higher temperaturesampling gives more creative responses, while lower temperatures are morefactually accurate. However, such models are commonly applied to generalinstruction following, which involves both creative and fact seeking tasks,using a single fixed temperature across all examples and tokens. In this work,we introduce Adaptive Decoding, a layer added to the model to select thesampling temperature dynamically at inference time, at either the token orexample level, in order to optimize performance. To learn its parameters weintroduce Latent Preference Optimization (LPO) a general approach to traindiscrete latent variables such as choices of temperature. Our methodoutperforms all fixed decoding temperatures across a range of tasks thatrequire different temperatures, including UltraFeedback, Creative StoryWriting, and GSM8K.</description><author>Shehzaad Dhuliawala, Ilia Kulikov, Ping Yu, Asli Celikyilmaz, Jason Weston, Sainbayar Sukhbaatar, Jack Lanchantin</author><pubDate>Thu, 14 Nov 2024 18:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09661v1</guid></item><item><title>Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data</title><link>http://arxiv.org/abs/2404.03862v3</link><description>To trust the fluent generations of large language models (LLMs), humans mustbe able to verify their correctness against trusted, external sources. Recentefforts, such as providing citations via retrieved documents or post-hocprovenance, enhance verifiability but provide no guarantees on theircorrectness. To address these limitations, we tackle the verifiability goalwith a different philosophy: trivializing the verification process bydeveloping models that quote verbatim statements from trusted sources in theirpre-training data. We propose Quote-Tuning, which demonstrates the feasibilityof aligning models to quote. The core of Quote-Tuning is a fast membershipinference function that efficiently verifies text against trusted corpora. Weleverage this tool to design a reward function to quantify quotes in modelresponses, and curate datasets for preference learning. Experiments show thatQuote-Tuning significantly increases verbatim quotes from high-qualitydocuments by up to 130% relative to base models while maintaining responsequality. Quote-Tuning is applicable in different tasks, generalizes toout-of-domain data and diverse model families, and provides additional benefitsto truthfulness. Our method not only serves as a hassle-free method to increasequoting but also opens up avenues for improving LLM trustworthiness throughbetter verifiability.</description><author>Jingyu Zhang, Marc Marone, Tianjian Li, Benjamin Van Durme, Daniel Khashabi</author><pubDate>Thu, 14 Nov 2024 18:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03862v3</guid></item><item><title>Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable Medical Information</title><link>http://arxiv.org/abs/2411.09648v1</link><description>This paper introduces Med-Bot, an AI-powered chatbot designed to provideusers with accurate and reliable medical information. Utilizing advancedlibraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,Med-Bot is built to handle the complexities of natural language understandingin a healthcare context. The integration of llamaassisted data processing andAutoGPT-Q provides enhanced performance in processing and responding to queriesbased on PDFs of medical literature, ensuring that users receive precise andtrustworthy information. This research details the methodologies employed indeveloping Med-Bot and evaluates its effectiveness in disseminating healthcareinformation.</description><author>Ahan Bhatt, Nandan Vaghela</author><pubDate>Thu, 14 Nov 2024 18:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09648v1</guid></item><item><title>How do Machine Learning Models Change?</title><link>http://arxiv.org/abs/2411.09645v1</link><description>The proliferation of Machine Learning (ML) models and their open-sourceimplementations has transformed Artificial Intelligence research andapplications. Platforms like Hugging Face (HF) enable the development, sharing,and deployment of these models, fostering an evolving ecosystem. While previousstudies have examined aspects of models hosted on platforms like HF, acomprehensive longitudinal study of how these models change remainsunderexplored. This study addresses this gap by utilizing both repositorymining and longitudinal analysis methods to examine over 200,000 commits and1,200 releases from over 50,000 models on HF. We replicate and extend an MLchange taxonomy for classifying commits and utilize Bayesian networks touncover patterns in commit and release activities over time. Our findingsindicate that commit activities align with established data sciencemethodologies, such as CRISP-DM, emphasizing iterative refinement andcontinuous improvement. Additionally, release patterns tend to consolidatesignificant updates, particularly in documentation, distinguishing betweengranular changes and milestone-based releases. Furthermore, projects withhigher popularity prioritize infrastructure enhancements early in theirlifecycle, and those with intensive collaboration practices exhibit improveddocumentation standards. These and other insights enhance the understanding ofmodel changes on community platforms and provide valuable guidance for bestpractices in model maintenance.</description><author>Joel Castaño, Rafael Cabañas, Antonio Salmerón, David Lo, Silverio Martínez-Fernández</author><pubDate>Thu, 14 Nov 2024 18:14:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09645v1</guid></item><item><title>AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks</title><link>http://arxiv.org/abs/2403.04783v2</link><description>Despite extensive pre-training in moral alignment to prevent generatingharmful information, large language models (LLMs) remain vulnerable tojailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defenseframework that filters harmful responses from LLMs. With the response-filteringmechanism, our framework is robust against different jailbreak attack prompts,and can be used to defend different victim models. AutoDefense assignsdifferent roles to LLM agents and employs them to complete the defense taskcollaboratively. The division in tasks enhances the overallinstruction-following of LLMs and enables the integration of other defensecomponents as tools. With AutoDefense, small open-source LMs can serve asagents and defend larger models against jailbreak attacks. Our experiments showthat AutoDefense can effectively defense against different jailbreak attacks,while maintaining the performance at normal user request. For example, wereduce the attack success rate on GPT-3.5 from 55.74% to 7.95% usingLLaMA-2-13b with a 3-agent system. Our code and data are publicly available athttps://github.com/XHMY/AutoDefense.</description><author>Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu</author><pubDate>Thu, 14 Nov 2024 18:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04783v2</guid></item><item><title>Neural Operators Can Play Dynamic Stackelberg Games</title><link>http://arxiv.org/abs/2411.09644v1</link><description>Dynamic Stackelberg games are a broad class of two-player games in which theleader acts first, and the follower chooses a response strategy to the leader'sstrategy. Unfortunately, only stylized Stackelberg games are explicitlysolvable since the follower's best-response operator (as a function of thecontrol of the leader) is typically analytically intractable. This paperaddresses this issue by showing that the \textit{follower's best-responseoperator} can be approximately implemented by an \textit{attention-based neuraloperator}, uniformly on compact subsets of adapted open-loop controls for theleader. We further show that the value of the Stackelberg game where thefollower uses the approximate best-response operator approximates the value ofthe original Stackelberg game. Our main result is obtained using our universalapproximation theorem for attention-based neural operators between spaces ofsquare-integrable adapted stochastic processes, as well as stability resultsfor a general class of Stackelberg games.</description><author>Guillermo Alvarez, Ibrahim Ekren, Anastasis Kratsios, Xuwei Yang</author><pubDate>Thu, 14 Nov 2024 18:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09644v1</guid></item><item><title>Super-resolution multi-contrast unbiased eye atlases with deep probabilistic refinement</title><link>http://arxiv.org/abs/2401.03060v3</link><description>Purpose: Eye morphology varies significantly across the population,especially for the orbit and optic nerve. These variations limit thefeasibility and robustness of generalizing population-wise features of eyeorgans to an unbiased spatial reference. Approach: To tackle these limitations, we propose a process for creatinghigh-resolution unbiased eye atlases. First, to restore spatial details fromscans with a low through-plane resolution compared to a high in-planeresolution, we apply a deep learning-based super-resolution algorithm. Then, wegenerate an initial unbiased reference with an iterative metric-basedregistration using a small portion of subject scans. We register the remainingscans to this template and refine the template using an unsupervised deepprobabilistic approach that generates a more expansive deformation field toenhance the organ boundary alignment. We demonstrate this framework usingmagnetic resonance images across four different tissue contrasts, generatingfour atlases in separate spatial alignments. Results: For each tissue contrast, we find a significant improvement usingthe Wilcoxon signed-rank test in the average Dice score across four labeledregions compared to a standard registration framework consisting of rigid,affine, and deformable transformations. These results highlight the effectivealignment of eye organs and boundaries using our proposed process. Conclusions: By combining super-resolution preprocessing and deepprobabilistic models, we address the challenge of generating an eye atlas toserve as a standardized reference across a largely variable population.</description><author>Ho Hin Lee, Adam M. Saunders, Michael E. Kim, Samuel W. Remedios, Lucas W. Remedios, Yucheng Tang, Qi Yang, Xin Yu, Shunxing Bao, Chloe Cho, Louise A. Mawn, Tonia S. Rex, Kevin L. Schey, Blake E. Dewey, Jeffrey M. Spraggins, Jerry L. Prince, Yuankai Huo, Bennett A. Landman</author><pubDate>Thu, 14 Nov 2024 18:09:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03060v3</guid></item><item><title>On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse</title><link>http://arxiv.org/abs/2411.09642v1</link><description>Specifying all desirable properties of a language model is challenging, butcertain requirements seem essential. Given samples from an unknown language,the trained model should produce valid strings not seen in training and beexpressive enough to capture the language's full richness. Otherwise,outputting invalid strings constitutes "hallucination," and failing to capturethe full range leads to "mode collapse." We ask if a language model can meetboth requirements. We investigate this within a statistical language generation setting buildingon Gold and Angluin. Here, the model receives random samples from adistribution over an unknown language K, which belongs to a possibly infinitecollection of languages. The goal is to generate unseen strings from K. We saythe model generates from K with consistency and breadth if, as training sizeincreases, its output converges to all unseen strings in K. Kleinberg and Mullainathan [KM24] asked if consistency and breadth inlanguage generation are possible. We answer this negatively: for a large classof language models, including next-token prediction models, this is impossiblefor most collections of candidate languages. This contrasts with [KM24]'sresult, showing consistent generation without breadth is possible for anycountable collection of languages. Our finding highlights that generation withbreadth fundamentally differs from generation without breadth. As a byproduct, we establish near-tight bounds on the number of samplesneeded for generation with or without breadth. Finally, our results offer hope: consistent generation with breadth isachievable for any countable collection of languages when negative examples(strings outside K) are available alongside positive ones. This suggests thatpost-training feedback, which encodes negative examples, can be crucial inreducing hallucinations while limiting mode collapse.</description><author>Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas</author><pubDate>Thu, 14 Nov 2024 18:06:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09642v1</guid></item><item><title>MCCE: Missingness-aware Causal Concept Explainer</title><link>http://arxiv.org/abs/2411.09639v1</link><description>Causal concept effect estimation is gaining increasing interest in the fieldof interpretable machine learning. This general approach explains the behaviorsof machine learning models by estimating the causal effect ofhuman-understandable concepts, which represent high-level knowledge morecomprehensibly than raw inputs like tokens. However, existing causal concepteffect explanation methods assume complete observation of all concepts involvedwithin the dataset, which can fail in practice due to incomplete annotations ormissing concept data. We theoretically demonstrate that unobserved concepts canbias the estimation of the causal effects of observed concepts. To address thislimitation, we introduce the Missingness-aware Causal Concept Explainer (MCCE),a novel framework specifically designed to estimate causal concept effects whennot all concepts are observable. Our framework learns to account for residualbias resulting from missing concepts and utilizes a linear predictor to modelthe relationships between these concepts and the outputs of black-box machinelearning models. It can offer explanations on both local and global levels. Weconduct validations using a real-world dataset, demonstrating that MCCEachieves promising performance compared to state-of-the-art explanation methodsin causal concept effect estimation.</description><author>Jifan Gao, Guanhua Chen</author><pubDate>Thu, 14 Nov 2024 18:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09639v1</guid></item><item><title>VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models</title><link>http://arxiv.org/abs/2407.04573v2</link><description>Vector retrieval algorithms are essential for semantic queries within therapidly evolving landscape of Large Language Models (LLMs). The ability toretrieve vectors that satisfy both similarity and diversity criteriasubstantially enhances the performance of LLMs. Although Maximal MarginalRelevance (MMR) is widely employed in retrieval scenarios requiring relevanceand diversity, variations in the parameter $\lambda$ lead to fluctuations thatcomplicate the optimization trajectory in vector spaces. This obscures thedirection of improvement and highlights the lack of a robust theoreticalanalysis regarding similarity and diversity constraints in retrieval processes.To address these challenges, this paper introduces a novel approach thatcharacterizes both constraints through the relationship between the sum vectorand the query vector. The proximity of these vectors ensures the similarityconstraint, while requiring individual vectors within the sum vector to divergein their alignment with the query vector satisfies the diversity constraint. Wefirst formulate a new combinatorial optimization problem, selecting k vectorsfrom a candidate set such that their sum vector maximally aligns with the queryvector, and demonstrate that this problem is NP-complete. This resultunderscores the inherent difficulty of simultaneously achieving similarity anddiversity in vector retrieval, thereby providing a theoretical foundation forfuture research. Subsequently, we present the heuristic algorithm VectorsRetrieval with Similarity and Diversity, VRSD, which features a clearoptimization objective and eliminates the need for preset parameters. VRSD alsoachieves a modest reduction in time complexity compared to MMR. Empiricalvalidation confirms that VRSD significantly outperforms MMR across variousdatasets.</description><author>Hang Gao, Yongfeng Zhang</author><pubDate>Thu, 14 Nov 2024 18:01:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04573v2</guid></item><item><title>Counterfactual Uncertainty Quantification of Factual Estimand of Efficacy from Before-and-After Treatment Repeated Measures Randomized Controlled Trials</title><link>http://arxiv.org/abs/2411.09635v1</link><description>The ideal estimand for comparing a new treatment $Rx$ with a control $C$ isthe $\textit{counterfactual}$ efficacy $Rx:C$, the expected differentialoutcome between $Rx$ and $C$ if each patient were given $\textit{both}$. Whilecounterfactual $\textit{point estimation}$ from $\textit{factual}$ RandomizedControlled Trials (RCTs) has been available, this article shows$\textit{counterfactual}$ uncertainty quantification (CUQ), quantifyinguncertainty for factual point estimates but in a counterfactual setting, issurprisingly achievable. We achieve CUQ whose variability is typically smallerthan factual UQ, by creating a new statistical modeling principle called ETZwhich is applicable to RCTs with $\textit{Before-and-After}$ treatment RepeatedMeasures, common in many therapeutic areas. We urge caution when estimate of the unobservable true condition of a patientbefore treatment has measurement error, because that violation of standardregression assumption can cause attenuation in estimating treatment effects.Fortunately, we prove that, for traditional medicine in general, and fortargeted therapy with efficacy defined as averaged over the population,counterfactual point estimation is unbiased. However, for targeted therapy,both Real Human and Digital Twins approaches should respect this limitation,lest predicted treatment effect in $\textit{subgroups}$ will have bias.</description><author>Xingya Wang, Yang Han, Yushi Liu, Szu-Yu Tang, Jason C. Hsu</author><pubDate>Thu, 14 Nov 2024 18:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09635v1</guid></item><item><title>One-Shot Manipulation Strategy Learning by Making Contact Analogies</title><link>http://arxiv.org/abs/2411.09627v1</link><description>We present a novel approach, MAGIC (manipulation analogies for generalizableintelligent contacts), for one-shot learning of manipulation strategies withfast and extensive generalization to novel objects. By leveraging a referenceaction trajectory, MAGIC effectively identifies similar contact points andsequences of actions on novel objects to replicate a demonstrated strategy,such as using different hooks to retrieve distant objects of different shapesand sizes. Our method is based on a two-stage contact-point matching processthat combines global shape matching using pretrained neural features with localcurvature analysis to ensure precise and physically plausible contact points.We experiment with three tasks including scooping, hanging, and hookingobjects. MAGIC demonstrates superior performance over existing methods,achieving significant improvements in runtime speed and generalization todifferent object categories. Website: https://magic-2024.github.io/ .</description><author>Yuyao Liu, Jiayuan Mao, Joshua Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling</author><pubDate>Thu, 14 Nov 2024 17:54:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09627v1</guid></item><item><title>Lifted Inference beyond First-Order Logic</title><link>http://arxiv.org/abs/2308.11738v3</link><description>Weighted First Order Model Counting (WFOMC) is fundamental to probabilisticinference in statistical relational learning models. As WFOMC is known to beintractable in general ($\#$P-complete), logical fragments that admitpolynomial time WFOMC are of significant interest. Such fragments are calleddomain liftable. Recent works have shown that the two-variable fragment offirst order logic extended with counting quantifiers ($\mathrm{C^2}$) isdomain-liftable. However, many properties of real-world data, like acyclicityin citation networks and connectivity in social networks, cannot be modeled in$\mathrm{C^2}$, or first order logic in general. In this work, we expand thedomain liftability of $\mathrm{C^2}$ with multiple such properties. We showthat any $\mathrm{C^2}$ sentence remains domain liftable when one of itsrelations is restricted to represent a directed acyclic graph, a connectedgraph, a tree (resp. a directed tree) or a forest (resp. a directed forest).All our results rely on a novel and general methodology of "counting bysplitting". Besides their application to probabilistic inference, our resultsprovide a general framework for counting combinatorial structures. We expand avast array of previous results in discrete mathematics literature on directedacyclic graphs, phylogenetic networks, etc.</description><author>Sagar Malhotra, Davide Bizzaro, Luciano Serafini</author><pubDate>Thu, 14 Nov 2024 17:50:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11738v3</guid></item><item><title>Local deployment of large-scale music AI models on commodity hardware</title><link>http://arxiv.org/abs/2411.09625v1</link><description>We present the MIDInfinite, a web application capable of generating symbolicmusic using a large-scale generative AI model locally on commodity hardware.Creating this demo involved porting the Anticipatory Music Transformer, a largelanguage model (LLM) pre-trained on the Lakh MIDI dataset, to the MachineLearning Compilation (MLC) framework. Once the model is ported, MLC facilitatesinference on a variety of runtimes including C++, mobile, and the browser. Weenvision that MLC has the potential to bridge the gap between the landscape ofincreasingly capable music AI models and technology more familiar to musicsoftware developers. As a proof of concept, we build a web application thatallows users to generate endless streams of multi-instrumental MIDI in thebrowser, either from scratch or conditioned on a prompt. On commodity hardware(an M3 Macbook Pro), our demo can generate 51 notes per second, which is fasterthan real-time playback for 72.9% of generations, and increases to 86.3% with 2seconds of upfront buffering.</description><author>Xun Zhou, Charlie Ruan, Zihe Zhao, Tianqi Chen, Chris Donahue</author><pubDate>Thu, 14 Nov 2024 17:49:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09625v1</guid></item><item><title>Vision-based Manipulation of Transparent Plastic Bags in Industrial Setups</title><link>http://arxiv.org/abs/2411.09623v1</link><description>This paper addresses the challenges of vision-based manipulation forautonomous cutting and unpacking of transparent plastic bags in industrialsetups, aligning with the Industry 4.0 paradigm. Industry 4.0, driven by data,connectivity, analytics, and robotics, promises enhanced accessibility andsustainability throughout the value chain. The integration of autonomoussystems, including collaborative robots (cobots), into industrial processes ispivotal for efficiency and safety. The proposed solution employs advancedMachine Learning algorithms, particularly Convolutional Neural Networks (CNNs),to identify transparent plastic bags under varying lighting and backgroundconditions. Tracking algorithms and depth sensing technologies are utilized for3D spatial awareness during pick and placement. The system addresses challengesin grasping and manipulation, considering optimal points, compliance controlwith vacuum gripping technology, and real-time automation for safe interactionin dynamic environments. The system's successful testing and validation in thelab with the FRANKA robot arm, showcases its potential for widespreadindustrial applications, while demonstrating effectiveness in automating theunpacking and cutting of transparent plastic bags for an 8-stack bulk-loaderbased on specific requirements and rigorous testing.</description><author>F. Adetunji, A. Karukayil, P. Samant, S. Shabana, F. Varghese, U. Upadhyay, R. A. Yadav, A. Partridge, E. Pendleton, R. Plant, Y. Petillot, M. Koskinopoulou</author><pubDate>Thu, 14 Nov 2024 17:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09623v1</guid></item><item><title>Value Residual Learning For Alleviating Attention Concentration In Transformers</title><link>http://arxiv.org/abs/2410.17897v2</link><description>Transformers can capture long-range dependencies using self-attention,allowing tokens to attend to all others directly. However, stacking multipleattention layers leads to attention concentration. One natural way to addressthis issue is to use cross-layer attention, allowing information from earlierlayers to be directly accessible to later layers. However, this approach iscomputationally expensive. To address this problem, we propose Transformer withresidual value (ResFormer) which approximates cross-layer attention throughadding a residual connection from the values of the the first layer to allsubsequent layers. Based on this method, one variant is the Transformer withsingle layer value (SVFormer), where all layers share the same value embeddingfrom first layer, reducing the $KV$ cache by nearly 50\%. Comprehensiveempirical evidence demonstrates that ResFormer mitigates attentionconcentration problem in deeper layers and enhances representation across mostlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO intraining error as well as downstream tasks. Further visualization resultssuggest that Resformer alleviates attention sinks through avoiding value-statedrains. SVFormer trains significantly faster than the vanilla Transformer andperforms better than other methods like GQA and CLA, with performanceinfluenced by sequence length and cumulative learning rate.</description><author>Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, Zhenzhong Lan</author><pubDate>Thu, 14 Nov 2024 17:46:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17897v2</guid></item><item><title>Information-driven design of imaging systems</title><link>http://arxiv.org/abs/2405.20559v2</link><description>Most modern imaging systems process the data they capture algorithmicallybefore-or instead of-human viewing. As a result, performance depends not on howinterpretable the measurements appear, but how effectively they encode detailsfor algorithmic processing. Information theory provides mathematical tools toanalyze this, but developing methods that can handle the complexity ofreal-world measurements yet remain practical enough for widespread use hasproven challenging. We introduce a data-driven approach for estimating theinformation content of imaging system measurements. Our framework requires onlyexperimental measurements and noise characterization, with no need for groundtruth data. We demonstrate that these information estimates reliably predictsystem performance across diverse imaging modalities, including colorphotography, radio astronomy, lensless imaging, and label-free microscopy. Toautomate the process of designing imaging systems that maximize informationcapture we introduce an optimization technique called Information-DrivenEncoder Analysis Learning (IDEAL). The tools we develop in this work unlockinformation theory as a powerful, practical tool for analyzing and designingimaging systems across a broad range of applications. A video summarizing this work can be found athttps://waller-lab.github.io/EncodingInformationWebsite/</description><author>Henry Pinkard, Leyla Kabuli, Eric Markley, Tiffany Chien, Jiantao Jiao, Laura Waller</author><pubDate>Thu, 14 Nov 2024 17:40:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20559v2</guid></item><item><title>MICCAI-CDMRI 2023 QuantConn Challenge Findings on Achieving Robust Quantitative Connectivity through Harmonized Preprocessing of Diffusion MRI</title><link>http://arxiv.org/abs/2411.09618v1</link><description>White matter alterations are increasingly implicated in neurological diseasesand their progression. International-scale studies use diffusion-weightedmagnetic resonance imaging (DW-MRI) to qualitatively identify changes in whitematter microstructure and connectivity. Yet, quantitative analysis of DW-MRIdata is hindered by inconsistencies stemming from varying acquisitionprotocols. There is a pressing need to harmonize the preprocessing of DW-MRIdatasets to ensure the derivation of robust quantitative diffusion metricsacross acquisitions. In the MICCAI-CDMRI 2023 QuantConn challenge, participantswere provided raw data from the same individuals collected on the same scannerbut with two different acquisitions and tasked with preprocessing the DW-MRI tominimize acquisition differences while retaining biological variation.Submissions are evaluated on the reproducibility and comparability ofcross-acquisition bundle-wise microstructure measures, bundle shape features,and connectomics. The key innovations of the QuantConn challenge are that (1)we assess bundles and tractography in the context of harmonization for thefirst time, (2) we assess connectomics in the context of harmonization for thefirst time, and (3) we have 10x additional subjects over prior harmonizationchallenge, MUSHAC and 100x over SuperMUDI. We find that bundle surface area,fractional anisotropy, connectome assortativity, betweenness centrality, edgecount, modularity, nodal strength, and participation coefficient measures aremost biased by acquisition and that machine learning voxel-wise correction,RISH mapping, and NeSH methods effectively reduce these biases. In addition,microstructure measures AD, MD, RD, bundle length, connectome density,efficiency, and path length are least biased by these acquisition differences.</description><author>Nancy R. Newlin, Kurt Schilling, Serge Koudoro, Bramsh Qamar Chandio, Praitayini Kanakaraj, Daniel Moyer, Claire E. Kelly, Sila Genc, Jian Chen, Joseph Yuan-Mou Yang, Ye Wu, Yifei He, Jiawei Zhang, Qingrun Zeng, Fan Zhang, Nagesh Adluru, Vishwesh Nath, Sudhir Pathak, Walter Schneider, Anurag Gade, Yogesh Rathi, Tom Hendriks, Anna Vilanova, Maxime Chamberland, Tomasz Pieciak, Dominika Ciupek, Antonio Tristán Vega, Santiago Aja-Fernández, Maciej Malawski, Gani Ouedraogo, Julia Machnio, Christian Ewert, Paul M. Thompson, Neda Jahanshad, Eleftherios Garyfallidis, Bennett A. Landman</author><pubDate>Thu, 14 Nov 2024 17:37:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09618v1</guid></item><item><title>PTR: Precision-Driven Tool Recommendation for Large Language Models</title><link>http://arxiv.org/abs/2411.09613v1</link><description>By augmenting Large Language Models (LLMs) with external tools, theircapacity to solve complex problems has been significantly enhanced. However,despite ongoing advancements in the parsing capabilities of LLMs, incorporatingall available tools simultaneously in the prompt remains impractical due to thevast number of external tools. Consequently, it is essential to provide LLMswith a precise set of tools tailored to the specific task, considering bothquantity and quality. Current tool retrieval methods primarily focus onrefining the ranking list of tools and directly packaging a fixed number oftop-ranked tools as the tool set. However, these approaches often fail to equipLLMs with the optimal set of tools prior to execution, since the optimal numberof tools for different tasks could be different, resulting in inefficienciessuch as redundant or unsuitable tools, which impede immediate access to themost relevant tools. This paper addresses the challenge of recommending precisetoolsets for LLMs. We introduce the problem of tool recommendation, define itsscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.PTR captures an initial, concise set of tools by leveraging historical toolbundle usage and dynamically adjusts the tool set by performing tool matching,culminating in a multi-view-based tool addition. Additionally, we present a newdataset, RecTools, and a metric, TRACC, designed to evaluate the effectivenessof tool recommendation for LLMs. We further validate our design choices throughcomprehensive experiments, demonstrating promising accuracy across two openbenchmarks and our RecTools dataset.</description><author>Hang Gao, Yongfeng Zhang</author><pubDate>Thu, 14 Nov 2024 17:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09613v1</guid></item><item><title>The Moral Foundations Weibo Corpus</title><link>http://arxiv.org/abs/2411.09612v1</link><description>Moral sentiments expressed in natural language significantly influence bothonline and offline environments, shaping behavioral styles and interactionpatterns, including social media selfpresentation, cyberbullying, adherence tosocial norms, and ethical decision-making. To effectively measure moralsentiments in natural language processing texts, it is crucial to utilizelarge, annotated datasets that provide nuanced understanding for accurateanalysis and modeltraining. However, existing corpora, while valuable, oftenface linguistic limitations. To address this gap in the Chinese languagedomain,we introduce the Moral Foundation Weibo Corpus. This corpus consists of25,671 Chinese comments on Weibo, encompassing six diverse topic areas. Eachcomment is manually annotated by at least three systematically trainedannotators based on ten moral categories derived from a grounded theory ofmorality. To assess annotator reliability, we present the kappa testresults, agold standard for measuring consistency. Additionally, we apply several thelatest large language models to supplement the manual annotations, conductinganalytical experiments to compare their performance and report baseline resultsfor moral sentiment classification.</description><author>Renjie Cao, Miaoyan Hu, Jiahan Wei, Baha Ihnaini</author><pubDate>Thu, 14 Nov 2024 17:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09612v1</guid></item><item><title>Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing</title><link>http://arxiv.org/abs/2411.07104v2</link><description>Recently, quadrupedal locomotion has achieved significant success, but theirmanipulation capabilities, particularly in handling large objects, remainlimited, restricting their usefulness in demanding real-world applications suchas search and rescue, construction, industrial automation, and roomorganization. This paper tackles the task of obstacle-aware, long-horizonpushing by multiple quadrupedal robots. We propose a hierarchical multi-agentreinforcement learning framework with three levels of control. The high-levelcontroller integrates an RRT planner and a centralized adaptive policy togenerate subgoals, while the mid-level controller uses a decentralizedgoal-conditioned policy to guide the robots toward these sub-goals. Apre-trained low-level locomotion policy executes the movement commands. Weevaluate our method against several baselines in simulation, demonstratingsignificant improvements over baseline approaches, with 36.0% higher successrates and 24.5% reduction in completion time than the best baseline. Ourframework successfully enables long-horizon, obstacle-aware manipulation taskslike Push-Cuboid and Push-T on Go1 robots in the real world.</description><author>Yuming Feng, Chuye Hong, Yaru Niu, Shiqi Liu, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao</author><pubDate>Thu, 14 Nov 2024 17:28:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07104v2</guid></item><item><title>Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the AutoNuggetizer Framework</title><link>http://arxiv.org/abs/2411.09607v1</link><description>This report provides an initial look at partial results from the TREC 2024Retrieval-Augmented Generation (RAG) Track. We have identified RAG evaluationas a barrier to continued progress in information access (and more broadly,natural language processing and artificial intelligence), and it is our hopethat we can contribute to tackling the many challenges in this space. Thecentral hypothesis we explore in this work is that the nugget evaluationmethodology, originally developed for the TREC Question Answering Track in2003, provides a solid foundation for evaluating RAG systems. As such, ourefforts have focused on "refactoring" this methodology, specifically applyinglarge language models to both automatically create nuggets and to automaticallyassign nuggets to system answers. We call this the AutoNuggetizer framework.Within the TREC setup, we are able to calibrate our fully automatic processagainst a manual process whereby nuggets are created by human assessorssemi-manually and then assigned manually to system answers. Based on initialresults across 21 topics from 45 runs, we observe a strong correlation betweenscores derived from a fully automatic nugget evaluation and a (mostly) manualnugget evaluation by human assessors. This suggests that our fully automaticevaluation process can be used to guide future iterations of RAG systems.</description><author>Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick Craswell, Jimmy Lin</author><pubDate>Thu, 14 Nov 2024 17:25:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09607v1</guid></item><item><title>Personalised dynamic super learning: an application in predicting hemodiafiltration convection volumes</title><link>http://arxiv.org/abs/2310.08479v2</link><description>Obtaining continuously updated predictions is a major challenge forpersonalised medicine. Leveraging combinations of parametric regressions andmachine learning approaches, the personalised online super learner (POSL) canachieve such dynamic and personalised predictions. We adapt POSL to predict arepeated continuous outcome dynamically and propose a new way to validate suchpersonalised or dynamic prediction models. We illustrate its performance bypredicting the convection volume of patients undergoing hemodiafiltration. POSLoutperformed its candidate learners with respect to median absolute error,calibration-in-the-large, discrimination, and net benefit. We finally discussthe choices and challenges underlying the use of POSL.</description><author>Arthur Chatton, Michèle Bally, Renée Lévesque, Ivana Malenica, Robert W. Platt, Mireille E. Schnitzer</author><pubDate>Thu, 14 Nov 2024 17:23:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08479v2</guid></item><item><title>Local-Global Attention: An Adaptive Mechanism for Multi-Scale Feature Integration</title><link>http://arxiv.org/abs/2411.09604v1</link><description>In recent years, attention mechanisms have significantly enhanced theperformance of object detection by focusing on key feature information.However, prevalent methods still encounter difficulties in effectivelybalancing local and global features. This imbalance hampers their ability tocapture both fine-grained details and broader contextual information-twocritical elements for achieving accurate object detection.To address thesechallenges, we propose a novel attention mechanism, termed Local-GlobalAttention, which is designed to better integrate both local and globalcontextual features. Specifically, our approach combines multi-scaleconvolutions with positional encoding, enabling the model to focus on localdetails while concurrently considering the broader global context.Additionally, we introduce a learnable parameters, which allow the model todynamically adjust the relative importance of local and global attention,depending on the specific requirements of the task, thereby optimizing featurerepresentations across multiple scales.We have thoroughly evaluated theLocal-Global Attention mechanism on several widely used object detection andclassification datasets. Our experimental results demonstrate that thisapproach significantly enhances the detection of objects at various scales,with particularly strong performance on multi-class and small object detectiontasks. In comparison to existing attention mechanisms, Local-Global Attentionconsistently outperforms them across several key metrics, all while maintainingcomputational efficiency.</description><author>Yifan Shao</author><pubDate>Thu, 14 Nov 2024 17:22:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09604v1</guid></item><item><title>Accelerating Knowledge Graph and Ontology Engineering with Large Language Models</title><link>http://arxiv.org/abs/2411.09601v1</link><description>Large Language Models bear the promise of significant acceleration of keyKnowledge Graph and Ontology Engineering tasks, including ontology modeling,extension, modification, population, alignment, as well as entitydisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineeringas a new and coming area of research, and argue that modular approaches toontologies will be of central importance.</description><author>Cogan Shimizu, Pascal Hitzler</author><pubDate>Thu, 14 Nov 2024 17:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09601v1</guid></item><item><title>Latency Optimization in LEO Satellite Communications with Hybrid Beam Pattern and Interference Control</title><link>http://arxiv.org/abs/2411.09600v1</link><description>The rapid advancement of low Earth orbit (LEO) satellite communicationsystems has significantly enhanced global connectivity, offering high-capacity,low-latency services crucial for next-generation applications. However, thedense configuration of LEO constellations poses challenges in resourceallocation optimization and interference management, complicating coexistencewith other communication systems. To address these limitations, this paperproposes a novel framework for optimizing the beam scheduling and resourceallocation in multi-beam LEO systems. To satisfy the uneven terrestrial trafficdemand, a hybrid beam pattern is employed to enhance the downlink quality ofservice and minimize the transmission latency from LEO satellites to grounduser terminals. Additionally, a dynamic co-channel interference (CCI) controlmechanism is developed to mitigate inter-beam interference within the LEOconstellation and limit cross-system interference affecting protected usersfrom other networks. The problem of user-beam-frequency allocation with poweroptimization is formulated as a mixed-integer dynamic programming model andsolved using a low-complexity neural network-based graph generation algorithm.Simulation results show that the proposed approach outperforms the baselinemethods of full frequency reuse and single-channel transmission, and highlightsthe potential for further performance improvement with multi-usertransmissions.</description><author>Qianqian Zhang, Ye Hu, Minchae Jung</author><pubDate>Thu, 14 Nov 2024 17:18:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09600v1</guid></item><item><title>Assessing the Performance of the DINOv2 Self-supervised Learning Vision Transformer Model for the Segmentation of the Left Atrium from MRI Images</title><link>http://arxiv.org/abs/2411.09598v1</link><description>Accurate left atrium (LA) segmentation from pre-operative scans is crucialfor diagnosing atrial fibrillation, treatment planning, and supporting surgicalinterventions. While deep learning models are key in medical imagesegmentation, they often require extensive manually annotated data. Foundationmodels trained on larger datasets have reduced this dependency, enhancinggeneralizability and robustness through transfer learning. We explore DINOv2, aself-supervised learning vision transformer trained on natural images, for LAsegmentation using MRI. The challenges for LA's complex anatomy, thinboundaries, and limited annotated data make accurate segmentation difficultbefore &amp; during the image-guided intervention. We demonstrate DINOv2's abilityto provide accurate &amp; consistent segmentation, achieving a mean Dice score of.871 &amp; a Jaccard Index of .792 for end-to-end fine-tuning. Through few-shotlearning across various data sizes &amp; patient counts, DINOv2 consistentlyoutperforms baseline models. These results suggest that DINOv2 effectivelyadapts to MRI with limited data, highlighting its potential as a competitivetool for segmentation &amp; encouraging broader use in medical imaging.</description><author>Bipasha Kundu, Bidur Khanal, Richard Simon, Cristian A. Linte</author><pubDate>Thu, 14 Nov 2024 17:15:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09598v1</guid></item><item><title>LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models</title><link>http://arxiv.org/abs/2411.09595v1</link><description>This work explores expanding the capabilities of large language models (LLMs)pretrained on text to generate 3D meshes within a unified model. This offerskey advantages of (1) leveraging spatial knowledge already embedded in LLMs,derived from textual sources like 3D tutorials, and (2) enabling conversational3D generation and mesh understanding. A primary challenge is effectivelytokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.To address this, we introduce LLaMA-Mesh, a novel approach that represents thevertex coordinates and face definitions of 3D meshes as plain text, allowingdirect integration with LLMs without expanding the vocabulary. We construct asupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputsas required, and (3) understand and interpret 3D meshes. Our work is the firstto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledgefor 3D mesh generation in a text-based format, effectively unifying the 3D andtext modalities. LLaMA-Mesh achieves mesh generation quality on par with modelstrained from scratch while maintaining strong text generation performance.</description><author>Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, Xiaohui Zeng</author><pubDate>Thu, 14 Nov 2024 17:08:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09595v1</guid></item><item><title>Stable Consistency Tuning: Understanding and Improving Consistency Models</title><link>http://arxiv.org/abs/2410.18958v2</link><description>Diffusion models achieve superior generation quality but suffer from slowgeneration speed due to the iterative nature of denoising. In contrast,consistency models, a new generative family, achieve competitive performancewith significantly faster sampling. These models are trained either throughconsistency distillation, which leverages pretrained diffusion models, orconsistency training/tuning directly from raw data. In this work, we propose anovel framework for understanding consistency models by modeling the denoisingprocess of the diffusion model as a Markov Decision Process (MDP) and framingconsistency model training as the value estimation through TemporalDifference~(TD) Learning. More importantly, this framework allows us to analyzethe limitations of current consistency training/tuning strategies. Built uponEasy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT),which incorporates variance-reduced learning using the score identity. SCTleads to significant performance improvements on benchmarks such as CIFAR-10and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID1.55, a new SoTA for consistency models.</description><author>Fu-Yun Wang, Zhengyang Geng, Hongsheng Li</author><pubDate>Thu, 14 Nov 2024 17:06:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18958v2</guid></item><item><title>SMILE-UHURA Challenge -- Small Vessel Segmentation at Mesoscopic Scale from Ultra-High Resolution 7T Magnetic Resonance Angiograms</title><link>http://arxiv.org/abs/2411.09593v1</link><description>The human brain receives nutrients and oxygen through an intricate network ofblood vessels. Pathology affecting small vessels, at the mesoscopic scale,represents a critical vulnerability within the cerebral blood supply and canlead to severe conditions, such as Cerebral Small Vessel Diseases. The adventof 7 Tesla MRI systems has enabled the acquisition of higher spatial resolutionimages, making it possible to visualise such vessels in the brain. However, thelack of publicly available annotated datasets has impeded the development ofrobust, machine learning-driven segmentation algorithms. To address this, theSMILE-UHURA challenge was organised. This challenge, held in conjunction withthe ISBI 2023, in Cartagena de Indias, Colombia, aimed to provide a platformfor researchers working on related topics. The SMILE-UHURA challenge addressesthe gap in publicly available annotated datasets by providing an annotateddataset of Time-of-Flight angiography acquired with 7T MRI. This dataset wascreated through a combination of automated pre-segmentation and extensivemanual refinement. In this manuscript, sixteen submitted methods and twobaseline methods are compared both quantitatively and qualitatively on twodifferent datasets: held-out test MRAs from the same dataset as the trainingdata (with labels kept secret) and a separate 7T ToF MRA dataset where bothinput volumes and labels are kept secret. The results demonstrate that most ofthe submitted deep learning methods, trained on the provided training dataset,achieved reliable segmentation performance. Dice scores reached up to 0.838$\pm$ 0.066 and 0.716 $\pm$ 0.125 on the respective datasets, with an averageperformance of up to 0.804 $\pm$ 0.15.</description><author>Soumick Chatterjee, Hendrik Mattern, Marc Dörner, Alessandro Sciarra, Florian Dubost, Hannes Schnurre, Rupali Khatun, Chun-Chih Yu, Tsung-Lin Hsieh, Yi-Shan Tsai, Yi-Zeng Fang, Yung-Ching Yang, Juinn-Dar Huang, Marshall Xu, Siyu Liu, Fernanda L. Ribeiro, Saskia Bollmann, Karthikesh Varma Chintalapati, Chethan Mysuru Radhakrishna, Sri Chandana Hudukula Ram Kumara, Raviteja Sutrave, Abdul Qayyum, Moona Mazher, Imran Razzak, Cristobal Rodero, Steven Niederren, Fengming Lin, Yan Xia, Jiacheng Wang, Riyu Qiu, Liansheng Wang, Arya Yazdan Panah, Rosana El Jurdi, Guanghui Fu, Janan Arslan, Ghislain Vaillant, Romain Valabregue, Didier Dormont, Bruno Stankoff, Olivier Colliot, Luisa Vargas, Isai Daniel Chacón, Ioannis Pitsiorlas, Pablo Arbeláez, Maria A. Zuluaga, Stefanie Schreiber, Oliver Speck, An</author><pubDate>Thu, 14 Nov 2024 17:06:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09593v1</guid></item><item><title>Expert Study on Interpretable Machine Learning Models with Missing Data</title><link>http://arxiv.org/abs/2411.09591v1</link><description>Inherently interpretable machine learning (IML) models provide valuableinsights for clinical decision-making but face challenges when features havemissing values. Classical solutions like imputation or excluding incompleterecords are often unsuitable in applications where values are missing at testtime. In this work, we conducted a survey with 71 clinicians from 29 traumacenters across France, including 20 complete responses to study the interactionbetween medical professionals and IML applied to data with missing values. Thisprovided valuable insights into how missing data is interpreted in clinicalmachine learning. We used the prediction of hemorrhagic shock as a concreteexample to gauge the willingness and readiness of the participants to adopt IMLmodels from three classes of methods. Our findings show that, while cliniciansvalue interpretability and are familiar with common IML methods, classicalimputation techniques often misalign with their intuition, and that models thatnatively handle missing values are preferred. These results emphasize the needto integrate clinical intuition into future IML models for betterhuman-computer interaction.</description><author>Lena Stempfle, Arthur James, Julie Josse, Tobias Gauss, Fredrik D. Johansson</author><pubDate>Thu, 14 Nov 2024 17:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09591v1</guid></item><item><title>Adopting RAG for LLM-Aided Future Vehicle Design</title><link>http://arxiv.org/abs/2411.09590v1</link><description>In this paper, we explore the integration of Large Language Models (LLMs)with Retrieval-Augmented Generation (RAG) to enhance automated design andsoftware development in the automotive industry. We present two case studies: astandardization compliance chatbot and a design copilot, both utilizing RAG toprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,LLAMA3, Mistral, and Mixtral- comparing their answering accuracy and executiontime. Our results demonstrate that while GPT-4 offers superior performance,LLAMA3 and Mistral also show promising capabilities for local deployment,addressing data privacy concerns in automotive applications. This studyhighlights the potential of RAG-augmented LLMs in improving design workflowsand compliance in automotive engineering.</description><author>Vahid Zolfaghari, Nenad Petrovic, Fengjunjie Pan, Krzysztof Lebioda, Alois Knoll</author><pubDate>Thu, 14 Nov 2024 17:01:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09590v1</guid></item><item><title>Spider: Any-to-Many Multimodal LLM</title><link>http://arxiv.org/abs/2411.09439v1</link><description>Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models(LLMs), enabling the integration of various modalities. However, Any-to-AnyMLLMs are limited to generating pairwise modalities 'Text + X' within a singleresponse, such as Text + {Image or Audio or Video}. To address this limitation,we introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)framework, which can generate an arbitrary combination of modalities 'Text +Xs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, ourSpider integrates three core components: a Base Model for basic X-to-X (i.e.,Any-to-Any) modality processing, a novel Efficient Decoders-Controller forcontrolling multimodal Decoders to generate Xs (many-modal) contents, and anAny-to-Many Instruction Template designed for producing Xs signal prompts. Totrain Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset,which facilitates the learning of the X-to-Xs (i.e., Any-to-Many) capabilitynecessary for AMMG. Ultimately, the well-trained Spider generates a pseudoX-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing thepotential for AMMG task in future research. Overall, this work not only pushesthe boundary of multimodal interaction but also provides rich data support foradvancing the field.</description><author>Jinxiang Lai, Jie Zhang, Jun Liu, Jian Li, Xiaocheng Lu, Song Guo</author><pubDate>Thu, 14 Nov 2024 16:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09439v1</guid></item><item><title>BabyLM Challenge: Exploring the Effect of Variation Sets on Language Model Training Efficiency</title><link>http://arxiv.org/abs/2411.09587v1</link><description>While current large language models have achieved a remarkable success, theirdata efficiency remains a challenge to overcome. Recently it has been suggestedthat child-directed speech (CDS) can improve training data efficiency of modernlanguage models based on Transformer neural networks. However, it is not yetunderstood which specific properties of CDS are effective for training thesemodels. In the context of the BabyLM Challenge, we focus on Variation Sets(VSs), sets of consecutive utterances expressing a similar intent with slightlydifferent words and structures, which are ubiquitous in CDS. To assess theimpact of VSs on training data efficiency, we augment CDS data with differentproportions of artificial VSs and use these datasets to train anauto-regressive model, GPT-2. We find that the best proportion of VSs dependson the evaluation benchmark: BLiMP and GLUE scores benefit from the presence ofVSs, but EWOK scores do not. Additionally, the results vary depending onmultiple factors such as the number of epochs and the order of utterancepresentation. Taken together, these findings suggest that VSs can have abeneficial influence on language models, while leaving room for furtherinvestigation.</description><author>Akari Haga, Akiyo Fukatsu, Miyu Oba, Arianna Bisazza, Yohei Oseki</author><pubDate>Thu, 14 Nov 2024 16:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09587v1</guid></item><item><title>From Imitation to Refinement -- Residual RL for Precise Assembly</title><link>http://arxiv.org/abs/2407.16677v3</link><description>Advances in behavior cloning (BC), like action-chunking and diffusion, haveenabled impressive capabilities. Still, imitation alone remains insufficientfor learning reliable policies for tasks requiring precise aligning andinserting of objects, like assembly. Our key insight is that chunked BCpolicies effectively function as trajectory planners, enabling long-horizontasks. Conversely, as they execute action chunks open-loop, they lack thefine-grained reactivity necessary for reliable execution. Further, we find thatthe performance of BC policies saturates despite increasing data. Reinforcementlearning (RL) is a natural way to overcome BC's limitations, but it is notstraightforward to apply directly to action-chunked models like diffusionpolicies. We present a simple yet effective method, ResiP (Residual for PreciseManipulation), that sidesteps these challenges by augmenting a frozen, chunkedBC model with a fully closed-loop residual policy trained with RL. The residualpolicy is trained via on-policy RL, addressing distribution shifts andintroducing reactive control without altering the BC trajectory planner.Evaluation on high-precision manipulation tasks demonstrates strong performanceof ResiP over BC methods and direct RL fine-tuning. Videos, code, and data areavailable at https://residual-assembly.github.io.</description><author>Lars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, Pulkit Agrawal</author><pubDate>Thu, 14 Nov 2024 16:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16677v3</guid></item><item><title>Software Performance Engineering for Foundation Model-Powered Software (FMware)</title><link>http://arxiv.org/abs/2411.09580v1</link><description>The rise of Foundation Models (FMs) like Large Language Models (LLMs) isrevolutionizing software development. Despite the impressive prototypes,transforming FMware into production-ready products demands complex engineeringacross various domains. A critical but overlooked aspect is performanceengineering, which aims at ensuring FMware meets performance goals such asthroughput and latency to avoid user dissatisfaction and financial loss. Often,performance considerations are an afterthought, leading to costly optimizationefforts post-deployment. FMware's high computational resource demands highlightthe need for efficient hardware use. Continuous performance engineering isessential to prevent degradation. This paper highlights the significance ofSoftware Performance Engineering (SPE) in FMware, identifying four keychallenges: cognitive architecture design, communication protocols, tuning andoptimization, and deployment. These challenges are based on literature surveysand experiences from developing an in-house FMware system. We discuss problems,current practices, and innovative paths for the software engineering community.</description><author>Haoxiang Zhang, Shi Chang, Arthur Leung, Kishanthan Thangarajah, Boyuan Chen, Hanan Lutfiyya, Ahmed E. Hassan</author><pubDate>Thu, 14 Nov 2024 16:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09580v1</guid></item><item><title>V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation Localization and Copyright Protection</title><link>http://arxiv.org/abs/2404.16824v4</link><description>AI-generated video has revolutionized short video production, filmmaking, andpersonalized media, making video local editing an essential tool. However, thisprogress also blurs the line between reality and fiction, posing challenges inmultimedia forensics. To solve this urgent issue, V2A-Mark is proposed toaddress the limitations of current video tampering forensics, such as poorgeneralizability, singular function, and single modality focus. Combining thefragility of video-into-video steganography with deep robust watermarking, ourmethod can embed invisible visual-audio localization watermarks and copyrightwatermarks into the original video frames and audio, enabling precisemanipulation localization and copyright protection. We also design a temporalalignment and fusion module and degradation prompt learning to enhance thelocalization accuracy and decoding robustness. Meanwhile, we introduce asample-level audio localization method and a cross-modal copyright extractionmechanism to couple the information of audio and video frames. Theeffectiveness of V2A-Mark has been verified on a visual-audio tamperingdataset, emphasizing its superiority in localization precision and copyrightaccuracy, crucial for the sustainable development of video editing in the AIGCvideo era.</description><author>Xuanyu Zhang, Youmin Xu, Runyi Li, Jiwen Yu, Weiqi Li, Zhipei Xu, Jian Zhang</author><pubDate>Thu, 14 Nov 2024 16:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16824v4</guid></item><item><title>Automating Reformulation of Essence Specifications via Graph Rewriting</title><link>http://arxiv.org/abs/2411.09576v1</link><description>Formulating an effective constraint model of a parameterised problem class iscrucial to the efficiency with which instances of the class can subsequently besolved. It is difficult to know beforehand which of a set of candidate modelswill perform best in practice. This paper presents a system that employs graphrewriting to reformulate an input model for improved performance automatically.By situating our work in the Essence abstract constraint specificationlanguage, we can use the structure in its high level variable types to triggerrewrites directly. We implement our system via rewrite rules expressed in theGraph Programs 2 language, applied to the abstract syntax tree of an inputspecification. We show how to automatically translate the solution of thereformulated problem into a solution of the original problem for verificationand presentation. We demonstrate the efficacy of our system with a detailedcase study.</description><author>Ian Miguel, András Z. Salamon, Christopher Stone</author><pubDate>Thu, 14 Nov 2024 16:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09576v1</guid></item><item><title>Equivariant Symmetry Breaking Sets</title><link>http://arxiv.org/abs/2402.02681v3</link><description>Equivariant neural networks (ENNs) have been shown to be extremely effectivein applications involving underlying symmetries. By construction ENNs cannotproduce lower symmetry outputs given a higher symmetry input. However, symmetrybreaking occurs in many physical systems and we may obtain a less symmetricstable state from an initial highly symmetric one. Hence, it is imperative thatwe understand how to systematically break symmetry in ENNs. In this work, wepropose a novel symmetry breaking framework that is fully equivariant and isthe first which fully addresses spontaneous symmetry breaking. We emphasizethat our approach is general and applicable to equivariance under any group. Toachieve this, we introduce the idea of symmetry breaking sets (SBS). Ratherthan redesign existing networks, we design sets of symmetry breaking objectswhich we feed into our network based on the symmetry of our inputs and outputs.We show there is a natural way to define equivariance on these sets, whichgives an additional constraint. Minimizing the size of these sets equates todata efficiency. We prove that minimizing these sets translates to a wellstudied group theory problem, and tabulate solutions to this problem for thepoint groups. Finally, we provide some examples of symmetry breaking todemonstrate how our approach works in practice. The code for these examples isavailable at \url{https://github.com/atomicarchitects/equivariant-SBS}.</description><author>YuQing Xie, Tess Smidt</author><pubDate>Thu, 14 Nov 2024 16:30:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02681v3</guid></item><item><title>Dynamic Reconstruction of Hand-Object Interaction with Distributed Force-aware Contact Representation</title><link>http://arxiv.org/abs/2411.09572v1</link><description>We present ViTaM-D, a novel visual-tactile framework for dynamic hand-objectinteraction reconstruction, integrating distributed tactile sensing for moreaccurate contact modeling. While existing methods focus primarily on visualinputs, they struggle with capturing detailed contact interactions such asobject deformation. Our approach leverages distributed tactile sensors toaddress this limitation by introducing DF-Field. This distributed force-awarecontact representation models both kinetic and potential energy in hand-objectinteraction. ViTaM-D first reconstructs hand-object interactions using avisual-only network, VDT-Net, and then refines contact details through aforce-aware optimization (FO) process, enhancing object deformation modeling.To benchmark our approach, we introduce the HOT dataset, which features 600sequences of hand-object interactions, including deformable objects, built in ahigh-precision simulation environment. Extensive experiments on both the DexYCBand HOT datasets demonstrate significant improvements in accuracy over previousstate-of-the-art methods such as gSDF and HOTrack. Our results highlight thesuperior performance of ViTaM-D in both rigid and deformable objectreconstruction, as well as the effectiveness of DF-Field in refining handposes. This work offers a comprehensive solution to dynamic hand-objectinteraction reconstruction by seamlessly integrating visual and tactile data.Codes, models, and datasets will be available.</description><author>Zhenjun Yu, Wenqiang Xu, Pengfei Xie, Yutong Li, Cewu Lu</author><pubDate>Thu, 14 Nov 2024 16:29:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09572v1</guid></item><item><title>FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI</title><link>http://arxiv.org/abs/2411.04872v3</link><description>We introduce FrontierMath, a benchmark of hundreds of original, exceptionallychallenging mathematics problems crafted and vetted by expert mathematicians.The questions cover most major branches of modern mathematics -- fromcomputationally intensive problems in number theory and real analysis toabstract questions in algebraic geometry and category theory. Solving a typicalproblem requires multiple hours of effort from a researcher in the relevantbranch of mathematics, and for the upper end questions, multiple days.FrontierMath uses new, unpublished problems and automated verification toreliably evaluate models while minimizing risk of data contamination. Currentstate-of-the-art AI models solve under 2% of problems, revealing a vast gapbetween AI capabilities and the prowess of the mathematical community. As AIsystems advance toward expert-level mathematical abilities, FrontierMath offersa rigorous testbed that quantifies their progress.</description><author>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Järviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, Mark Wildon</author><pubDate>Thu, 14 Nov 2024 16:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04872v3</guid></item><item><title>Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?</title><link>http://arxiv.org/abs/2411.06542v2</link><description>Designing planners and controllers for contact-rich manipulation is extremelychallenging as contact violates the smoothness conditions that manygradient-based controller synthesis tools assume. Contact smoothingapproximates a non-smooth system with a smooth one, allowing one to use thesesynthesis tools more effectively. However, applying classical control synthesismethods to smoothed contact dynamics remains relatively under-explored. Thispaper analyzes the efficacy of linear controller synthesis using differentialsimulators based on contact smoothing. We introduce natural baselines forleveraging contact smoothing to compute (a) open-loop plans robust to uncertainconditions and/or dynamics, and (b) feedback gains to stabilize aroundopen-loop plans. Using robotic bimanual whole-body manipulation as a testbed,we perform extensive empirical experiments on over 300 trajectories and analyzewhy LQR seems insufficient for stabilizing contact-rich plans. The videosummarizing this paper and hardware experiments is found here:https://youtu.be/HLaKi6qbwQg?si=_zCAmBBD6rGSitm9.</description><author>Yuki Shirai, Tong Zhao, H. J. Terry Suh, Huaijiang Zhu, Xinpei Ni, Jiuguang Wang, Max Simchowitz, Tao Pang</author><pubDate>Thu, 14 Nov 2024 16:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.06542v2</guid></item><item><title>VPBSD:Vessel-Pattern-Based Semi-Supervised Distillation for Efficient 3D Microscopic Cerebrovascular Segmentation</title><link>http://arxiv.org/abs/2411.09567v1</link><description>3D microscopic cerebrovascular images are characterized by their highresolution, presenting significant annotation challenges, large data volumes,and intricate variations in detail. Together, these factors make achievinghigh-quality, efficient whole-brain segmentation particularly demanding. Inthis paper, we propose a novel Vessel-Pattern-Based Semi-SupervisedDistillation pipeline (VpbSD) to address the challenges of 3D microscopiccerebrovascular segmentation. This pipeline initially constructs avessel-pattern codebook that captures diverse vascular structures fromunlabeled data during the teacher model's pretraining phase. In the knowledgedistillation stage, the codebook facilitates the transfer of rich knowledgefrom a heterogeneous teacher model to a student model, while thesemi-supervised approach further enhances the student model's exposure todiverse learning samples. Experimental results on real-world data, includingcomparisons with state-of-the-art methods and ablation studies, demonstratethat our pipeline and its individual components effectively address thechallenges inherent in microscopic cerebrovascular segmentation.</description><author>Xi Lin, Shixuan Zhao, Xinxu Wei, Amir Shmuel, Yongjie Li</author><pubDate>Thu, 14 Nov 2024 16:21:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09567v1</guid></item><item><title>Causal Discovery and Classification Using Lempel-Ziv Complexity</title><link>http://arxiv.org/abs/2411.01881v2</link><description>Inferring causal relationships in the decision-making processes of machinelearning algorithms is a crucial step toward achieving explainable ArtificialIntelligence (AI). In this research, we introduce a novel causality measure anda distance metric derived from Lempel-Ziv (LZ) complexity. We explore how theproposed causality measure can be used in decision trees by enabling splitsbased on features that most strongly \textit{cause} the outcome. We furtherevaluate the effectiveness of the causality-based decision tree and thedistance-based decision tree in comparison to a traditional decision tree usingGini impurity. While the proposed methods demonstrate comparable classificationperformance overall, the causality-based decision tree significantlyoutperforms both the distance-based decision tree and the Gini-based decisiontree on datasets generated from causal models. This result indicates that theproposed approach can capture insights beyond those of classical decisiontrees, especially in causally structured data. Based on the features used inthe LZ causal measure based decision tree, we introduce a causal strength foreach features in the dataset so as to infer the predominant causal variablesfor the occurrence of the outcome.</description><author>Dhruthi, Nithin Nagaraj, Harikrishnan N B</author><pubDate>Thu, 14 Nov 2024 16:17:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01881v2</guid></item><item><title>Diffusion Sampling Correction via Approximately 10 Parameters</title><link>http://arxiv.org/abs/2411.06503v2</link><description>Diffusion Probabilistic Models (DPMs) have demonstrated exceptionalperformance in generative tasks, but this comes at the expense of samplingefficiency. To enhance sampling speed without sacrificing quality, variousdistillation-based accelerated sampling algorithms have been recently proposed.However, they typically require significant additional training costs and modelparameter storage, which limit their practical application. In this work, wepropose PCA-based Adaptive Search (PAS), which optimizes existing solvers forDPMs with minimal learnable parameters and training costs. Specifically, wefirst employ PCA to obtain a few orthogonal unit basis vectors to span thehigh-dimensional sampling space, which enables us to learn just a set ofcoordinates to correct the sampling direction; furthermore, based on theobservation that the cumulative truncation error exhibits an ``S''-shape, wedesign an adaptive search strategy that further enhances the samplingefficiency and reduces the number of stored parameters to approximately 10.Extensive experiments demonstrate that PAS can significantly enhance existingfast solvers in a plug-and-play manner with negligible costs. For instance, onCIFAR10, PAS requires only 12 parameters and less than 1 minute of training ona single NVIDIA A100 GPU to optimize the DDIM from 15.69 FID (NFE=10) to 4.37.</description><author>Guangyi Wang, Wei Peng, Lijiang Li, Wenyu Chen, Yuren Cai, Songzhi Su</author><pubDate>Thu, 14 Nov 2024 16:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.06503v2</guid></item><item><title>Adaptive Deviation Learning for Visual Anomaly Detection with Data Contamination</title><link>http://arxiv.org/abs/2411.09558v1</link><description>Visual anomaly detection targets to detect images that notably differ fromnormal pattern, and it has found extensive application in identifying defectiveparts within the manufacturing industry. These anomaly detection paradigmspredominantly focus on training detection models using only clean, unlabelednormal samples, assuming an absence of contamination; a condition often unmetin real-world scenarios. The performance of these methods significantly dependson the quality of the data and usually decreases when exposed to noise. Weintroduce a systematic adaptive method that employs deviation learning tocompute anomaly scores end-to-end while addressing data contamination byassigning relative importance to the weights of individual instances. In thisapproach, the anomaly scores for normal instances are designed to approximatescalar scores obtained from the known prior distribution. Meanwhile, anomalyscores for anomaly examples are adjusted to exhibit statistically significantdeviations from these reference scores. Our approach incorporates a constrainedoptimization problem within the deviation learning framework to update instanceweights, resolving this problem for each mini-batch. Comprehensive experimentson the MVTec and VisA benchmark datasets indicate that our proposed methodsurpasses competing techniques and exhibits both stability and robustness inthe presence of data contamination.</description><author>Anindya Sundar Das, Guansong Pang, Monowar Bhuyan</author><pubDate>Thu, 14 Nov 2024 16:10:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09558v1</guid></item><item><title>Image Processing for Motion Magnification</title><link>http://arxiv.org/abs/2411.09555v1</link><description>Motion Magnification (MM) is a collection of relative recent techniqueswithin the realm of Image Processing. The main motivation of introducing thesetechniques in to support the human visual system to capture relevantdisplacements of an object of interest; these motions can be in object colorand in object location. In fact, the goal is to opportunely process a videosequence to obtain as output a new video in which motions are magnified andvisible to the viewer. We propose a numerical technique using the Phase-BasedMotion Magnification which analyses the video sequence in the Fourier Domainand rely on the Fourier Shifting Property. We describe the mathematicalfoundation of this method and the corresponding implementation in a numericalalgorithm. We present preliminary experiments, focusing on some basic test madeup using synthetic images.</description><author>Nadaniela Egidi, Josephin Giacomini, Paolo Leonesi, Pierluigi Maponi, Federico Mearelli, Edin Trebovic</author><pubDate>Thu, 14 Nov 2024 16:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09555v1</guid></item><item><title>OOD-SEG: Out-Of-Distribution detection for image SEGmentation with sparse multi-class positive-only annotations</title><link>http://arxiv.org/abs/2411.09553v1</link><description>Despite significant advancements, segmentation based on deep neural networksin medical and surgical imaging faces several challenges, two of which we aimto address in this work. First, acquiring complete pixel-level segmentationlabels for medical images is time-consuming and requires domain expertise.Second, typical segmentation pipelines cannot detect out-of-distribution (OOD)pixels, leaving them prone to spurious outputs during deployment. In this work,we propose a novel segmentation approach exploiting OOD detection that learnsonly from sparsely annotated pixels from multiple positive-only classes. %but\emph{no background class} annotation. These multi-class positive annotationsnaturally fall within the in-distribution (ID) set. Unlabelled pixels maycontain positive classes but also negative ones, including what is typicallyreferred to as \emph{background} in standard segmentation formulations. Here,we forgo the need for background annotation and consider these together withany other unseen classes as part of the OOD set. Our framework can integrate,at a pixel-level, any OOD detection approaches designed for classificationtasks. To address the lack of existing OOD datasets and established evaluationmetric for medical image segmentation, we propose a cross-validation strategythat treats held-out labelled classes as OOD. Extensive experiments on bothmulti-class hyperspectral and RGB surgical imaging datasets demonstrate therobustness and generalisation capability of our proposed framework.</description><author>Junwen Wang, Zhonghao Wang, Oscar MacCormac, Jonathan Shapey, Tom Vercauteren</author><pubDate>Thu, 14 Nov 2024 16:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09553v1</guid></item><item><title>MFTIQ: Multi-Flow Tracker with Independent Matching Quality Estimation</title><link>http://arxiv.org/abs/2411.09551v1</link><description>In this work, we present MFTIQ, a novel dense long-term tracking model thatadvances the Multi-Flow Tracker (MFT) framework to address challenges inpoint-level visual tracking in video sequences. MFTIQ builds upon theflow-chaining concepts of MFT, integrating an Independent Quality (IQ) modulethat separates correspondence quality estimation from optical flowcomputations. This decoupling significantly enhances the accuracy andflexibility of the tracking process, allowing MFTIQ to maintain reliabletrajectory predictions even in scenarios of prolonged occlusions and complexdynamics. Designed to be "plug-and-play", MFTIQ can be employed with anyoff-the-shelf optical flow method without the need for fine-tuning orarchitectural modifications. Experimental validations on the TAP-Vid Davisdataset show that MFTIQ with RoMa optical flow not only surpasses MFT but alsoperforms comparably to state-of-the-art trackers while having substantiallyfaster processing speed. Code and models available athttps://github.com/serycjon/MFTIQ .</description><author>Jonas Serych, Michal Neoral, Jiri Matas</author><pubDate>Thu, 14 Nov 2024 16:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09551v1</guid></item><item><title>Piecing It All Together: Verifying Multi-Hop Multimodal Claims</title><link>http://arxiv.org/abs/2411.09547v1</link><description>Existing claim verification datasets often do not require systems to performcomplex reasoning or effectively interpret multimodal evidence. To addressthis, we introduce a new task: multi-hop multimodal claim verification. Thistask challenges models to reason over multiple pieces of evidence from diversesources, including text, images, and tables, and determine whether the combinedmultimodal evidence supports or refutes a given claim. To study this task, weconstruct MMCV, a large-scale dataset comprising 16k multi-hop claims pairedwith multimodal evidence, generated and refined using large language models,with additional input from human feedback. We show that MMCV is challengingeven for the latest state-of-the-art multimodal large language models,especially as the number of reasoning hops increases. Additionally, weestablish a human performance benchmark on a subset of MMCV. We hope thisdataset and its evaluation task will encourage future research in multimodalmulti-hop claim verification.</description><author>Haoran Wang, Aman Rangapur, Xiongxiao Xu, Yueqing Liang, Haroon Gharwi, Carl Yang, Kai Shu</author><pubDate>Thu, 14 Nov 2024 16:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09547v1</guid></item><item><title>Equation-informed data-driven identification of flow budgets and dynamics</title><link>http://arxiv.org/abs/2411.09545v1</link><description>Computational Fluid Dynamics (CFD) is an indispensable method of fluidmodelling in engineering applications, reducing the need for physicalprototypes and testing for tasks such as design optimisation and performanceanalysis. Depending on the complexity of the system under consideration, modelsranging from low to high fidelity can be used for prediction, allowingsignificant speed-up. However, the choice of model requires information aboutthe actual dynamics of the flow regime. Correctly identifying theregions/clusters of flow that share the same dynamics has been a challengingresearch topic to date. In this study, we propose a novel hybrid approach toflow clustering. It consists of characterising each sample point of the systemwith equation-based features, i.e. features are budgets that represent thecontribution of each term from the original governing equation to the localdynamics at each sample point. This was achieved by applying the SparseIdentification of Nonlinear Dynamical systems (SINDy) method pointwise to timeevolution data. The method proceeds with equation-based clustering using theGirvan-Newman algorithm. This allows the detection of communities that sharethe same physical dynamics. The algorithm is implemented in both Eulerian andLagrangian frameworks. In the Lagrangian, i.e. dynamic approach, the clusteringis performed on the trajectory of each point, allowing the change of clustersto be represented also in time. The performance of the algorithm is firsttested on a flow around a cylinder. The construction of the dynamic clusters inthis test case clearly shows the evolution of the wake from the steady statesolution through the transient to the oscillatory solution. Dynamic clusteringwas then successfully tested on turbulent flow data. Two distinct andwell-defined clusters were identified and their temporal evolution wasreconstructed.</description><author>Nataliya Sevryugina, Serena Costanzo, Steve de Bruyn Kops, Colm-cille Caulfield, Iraj Mortazavi, Taraneh Sayadi</author><pubDate>Thu, 14 Nov 2024 15:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09545v1</guid></item><item><title>OpenGeMM: A High-Utilization GeMM Accelerator Generator with Lightweight RISC-V Control and Tight Memory Coupling</title><link>http://arxiv.org/abs/2411.09543v1</link><description>Deep neural networks (DNNs) face significant challenges when deployed onresource-constrained extreme edge devices due to their computational anddata-intensive nature. While standalone accelerators tailored for specificapplication scenarios suffer from inflexible control and limitedprogrammability, generic hardware acceleration platforms coupled with RISC-VCPUs can enable high reusability and flexibility, yet typically at the expenseof system level efficiency and low utilization. To fill this gap, we proposeOpenGeMM, an open-source acceleration platform, jointly demonstrating highefficiency and utilization, as well as ease of configurability andprogrammability. OpenGeMM encompasses a parameterized Chisel-coded GeMMaccelerator, a lightweight RISC-V processor, and a tightly coupled multi-bankedscratchpad memory. The GeMM core utilization and system efficiency are boostedthrough three mechanisms: configuration pre-loading, input pre-fetching withoutput buffering, and programmable strided memory access. Experimental resultsshow that OpenGeMM can consistently achieve hardware utilization ranging from81.89% to 99.34% across diverse CNN and Transformer workloads. Compared to theSotA open-source Gemmini accelerator, OpenGeMM demonstrates a 3.58x to 16.40xspeedup on normalized throughput across a wide variety ofGeMM workloads, whileachieving 4.68 TOPS/W system efficiency.</description><author>Xiaoling Yi, Ryan Antonio, Joren Dumoulin, Jiacong Sun, Josse Van Delm, Guilherme Paim, Marian Verhelst</author><pubDate>Thu, 14 Nov 2024 15:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09543v1</guid></item><item><title>Prompting the Unseen: Detecting Hidden Backdoors in Black-Box Models</title><link>http://arxiv.org/abs/2411.09540v1</link><description>Visual prompting (VP) is a new technique that adapts well-trained frozenmodels for source domain tasks to target domain tasks. This study examines VP'sbenefits for black-box model-level backdoor detection. The visual prompt in VPmaps class subspaces between source and target domains. We identify amisalignment, termed class subspace inconsistency, between clean and poisoneddatasets. Based on this, we introduce \textsc{BProm}, a black-box model-leveldetection method to identify backdoors in suspicious models, if any.\textsc{BProm} leverages the low classification accuracy of prompted modelswhen backdoors are present. Extensive experiments confirm \textsc{BProm}'seffectiveness.</description><author>Zi-Xuan Huang, Jia-Wei Chen, Zhi-Peng Zhang, Chia-Mu Yu</author><pubDate>Thu, 14 Nov 2024 15:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09540v1</guid></item><item><title>A Practical Guide to Fine-tuning Language Models with Limited Data</title><link>http://arxiv.org/abs/2411.09539v1</link><description>Employing pre-trained Large Language Models (LLMs) has become the de factostandard in Natural Language Processing (NLP) despite their extensive datarequirements. Motivated by the recent surge in research focused on trainingLLMs with limited data, particularly in low-resource domains and languages,this paper surveys recent transfer learning approaches to optimize modelperformance in downstream tasks where data is scarce. We first address initialand continued pre-training strategies to better leverage prior knowledge inunseen domains and languages. We then examine how to maximize the utility oflimited data during fine-tuning and few-shot learning. The final section takesa task-specific perspective, reviewing models and methods suited for differentlevels of data scarcity. Our goal is to provide practitioners with practicalguidelines for overcoming the challenges posed by constrained data while alsohighlighting promising directions for future research.</description><author>Márton Szép, Daniel Rueckert, Rüdiger von Eisenhart-Rothe, Florian Hinterwimmer</author><pubDate>Thu, 14 Nov 2024 15:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09539v1</guid></item><item><title>Marker-free Human Gait Analysis using a Smart Edge Sensor System</title><link>http://arxiv.org/abs/2411.09538v1</link><description>The human gait is a complex interplay between the neuronal and the muscularsystems, reflecting an individual's neurological and physiological condition.This makes gait analysis a valuable tool for biomechanics and medical experts.Traditional observational gait analysis is cost-effective but lacks reliabilityand accuracy, while instrumented gait analysis, particularly using marker-basedoptical systems, provides accurate data but is expensive and time-consuming. Inthis paper, we introduce a novel markerless approach for gait analysis using amulti-camera setup with smart edge sensors to estimate 3D body poses withoutfiducial markers. We propose a Siamese embedding network with triplet losscalculation to identify individuals by their gait pattern. This networkeffectively maps gait sequences to an embedding space that enables clusteringsequences from the same individual or activity closely together whileseparating those of different ones. Our results demonstrate the potential ofthe proposed system for efficient automated gait analysis in diverse real-worldenvironments, facilitating a wide range of applications.</description><author>Eva Katharina Bauer, Simon Bultmann, Sven Behnke</author><pubDate>Thu, 14 Nov 2024 15:55:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09538v1</guid></item><item><title>Knowledge Bases in Support of Large Language Models for Processing Web News</title><link>http://arxiv.org/abs/2411.08278v2</link><description>Large Language Models (LLMs) have received considerable interest in wideapplications lately. During pre-training via massive datasets, such a modelimplicitly memorizes the factual knowledge of trained datasets in its hiddenparameters. However, knowledge held implicitly in parameters often makes itsuse by downstream applications ineffective due to the lack of common-sensereasoning. In this article, we introduce a general framework that permits tobuild knowledge bases with an aid of LLMs, tailored for processing Web news.The framework applies a rule-based News Information Extractor (NewsIE) to newsitems for extracting their relational tuples, referred to as knowledge bases,which are then graph-convoluted with the implicit knowledge facts of news itemsobtained by LLMs, for their classification. It involves two lightweightcomponents: 1) NewsIE: for extracting the structural information of every newsitem, in the form of relational tuples; 2) BERTGraph: for graph convoluting theimplicit knowledge facts with relational tuples extracted by NewsIE. We haveevaluated our framework under different news-related datasets for news categoryclassification, with promising experimental results.</description><author>Yihe Zhang, Nabin Pakka, Nian-Feng Tzeng</author><pubDate>Thu, 14 Nov 2024 15:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.08278v2</guid></item><item><title>HyCoT: A Transformer-Based Autoencoder for Hyperspectral Image Compression</title><link>http://arxiv.org/abs/2408.08700v2</link><description>The development of learning-based hyperspectral image (HSI) compressionmodels has recently attracted significant interest. Existing modelspredominantly utilize convolutional filters, which capture only localdependencies. Furthermore,they often incur high training costs and exhibitsubstantial computational complexity. To address these limitations, in thispaper we propose Hyperspectral Compression Transformer (HyCoT) that is atransformer-based autoencoder for pixelwise HSI compression. Additionally, weapply a simple yet effective training set reduction approach to accelerate thetraining process. Experimental results on the HySpecNet-11k dataset demonstratethat HyCoT surpasses the state of the art across various compression ratios byover 1 dB of PSNR with significantly reduced computational requirements. Ourcode and pre-trained weights are publicly available athttps://git.tu-berlin.de/rsim/hycot .</description><author>Martin Hermann Paul Fuchs, Behnood Rasti, Begüm Demir</author><pubDate>Thu, 14 Nov 2024 15:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08700v2</guid></item><item><title>Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2405.14325v4</link><description>Recent studies highlighted a practical setting of unsupervised anomalydetection (UAD) that builds a unified model for multi-class images. Despitevarious advancements addressing this challenging task, the detectionperformance under the multi-class setting still lags far behindstate-of-the-art class-separated models. Our research aims to bridge thissubstantial performance gap. In this paper, we introduce a minimalisticreconstruction-based anomaly detection framework, namely Dinomaly, whichleverages pure Transformer architectures without relying on complex designs,additional modules, or specialized tricks. Given this powerful frameworkconsisted of only Attentions and MLPs, we found four simple components that areessential to multi-class anomaly detection: (1) Foundation Transformers thatextracts universal and discriminative features, (2) Noisy Bottleneck wherepre-existing Dropouts do all the noise injection tricks, (3) Linear Attentionthat naturally cannot focus, and (4) Loose Reconstruction that does not forcelayer-to-layer and point-by-point reconstruction. Extensive experiments areconducted across popular anomaly detection benchmarks including MVTec-AD, VisA,and Real-IAD. Our proposed Dinomaly achieves impressive image-level AUROC of99.6%, 98.7%, and 89.3% on the three datasets respectively, which is not onlysuperior to state-of-the-art multi-class UAD methods, but also achieves themost advanced class-separated UAD records.</description><author>Jia Guo, Shuai Lu, Weihang Zhang, Fang Chen, Hongen Liao, Huiqi Li</author><pubDate>Thu, 14 Nov 2024 15:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14325v4</guid></item><item><title>Breaking the Low-Rank Dilemma of Linear Attention</title><link>http://arxiv.org/abs/2411.07635v2</link><description>The Softmax attention mechanism in Transformer models is notoriouslycomputationally expensive, particularly due to its quadratic complexity, posingsignificant challenges in vision applications. In contrast, linear attentionprovides a far more efficient solution by reducing the complexity to linearlevels. However, compared to Softmax attention, linear attention oftenexperiences significant performance degradation. Our experiments indicate thatthis performance drop is due to the low-rank nature of linear attention'sfeature map, which hinders its ability to adequately model complex spatialinformation. In this paper, to break the low-rank dilemma of linear attention,we conduct rank analysis from two perspectives: the KV buffer and the outputfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),which rivals the performance of Softmax attention while maintaining linearcomplexity and high efficiency. Based on RALA, we construct the Rank-AugmentedVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLTachieves excellent performance across various vision tasks. Specifically,without using any additional labels, data, or supervision during training,RAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parametersand 4.6G FLOPs. This result significantly surpasses previous linear attentionmechanisms, fully illustrating the potential of RALA. Code will be available athttps://github.com/qhfan/RALA.</description><author>Qihang Fan, Huaibo Huang, Ran He</author><pubDate>Thu, 14 Nov 2024 15:40:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07635v2</guid></item><item><title>Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents</title><link>http://arxiv.org/abs/2411.09523v1</link><description>With the continuous development of large language models (LLMs),transformer-based models have made groundbreaking advances in numerous naturallanguage processing (NLP) tasks, leading to the emergence of a series of agentsthat use LLMs as their control hub. While LLMs have achieved success in varioustasks, they face numerous security and privacy threats, which become even moresevere in the agent scenarios. To enhance the reliability of LLM-basedapplications, a range of research has emerged to assess and mitigate theserisks from different perspectives. To help researchers gain a comprehensive understanding of various risks, thissurvey collects and analyzes the different threats faced by these agents. Toaddress the challenges posed by previous taxonomies in handling cross-moduleand cross-stage threats, we propose a novel taxonomy framework based on thesources and impacts. Additionally, we identify six key features of LLM-basedagents, based on which we summarize the current research progress and analyzetheir limitations. Subsequently, we select four representative agents as casestudies to analyze the risks they may face in practical use. Finally, based onthe aforementioned analyses, we propose future research directions from theperspectives of data, methodology, and policy, respectively.</description><author>Yuyou Gan, Yong Yang, Zhe Ma, Ping He, Rui Zeng, Yiming Wang, Qingming Li, Chunyi Zhou, Songze Li, Ting Wang, Yunjun Gao, Yingcai Wu, Shouling Ji</author><pubDate>Thu, 14 Nov 2024 15:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09523v1</guid></item><item><title>Generative Adversarial Networks for Spatio-Spectral Compression of Hyperspectral Images</title><link>http://arxiv.org/abs/2305.08514v4</link><description>The development of deep learning-based models for the compression ofhyperspectral images (HSIs) has recently attracted great attention in remotesensing due to the sharp growing of hyperspectral data archives. Most of theexisting models achieve either spectral or spatial compression, and do notjointly consider the spatio-spectral redundancies present in HSIs. To addressthis problem, in this paper we focus our attention on the High FidelityCompression (HiFiC) model (which is proven to be highly effective for spatialcompression problems) and adapt it to perform spatio-spectral compression ofHSIs. In detail, we introduce two new models: i) HiFiC using Squeeze andExcitation (SE) blocks (denoted as HiFiC$_{SE}$); and ii) HiFiC with 3Dconvolutions (denoted as HiFiC$_{3D}$) in the framework of compression of HSIs.We analyze the effectiveness of HiFiC$_{SE}$ and HiFiC$_{3D}$ in compressingthe spatio-spectral redundancies with channel attention and inter-dependencyanalysis. Experimental results show the efficacy of the proposed models inperforming spatio-spectral compression, while reconstructing images at reducedbitrates with higher reconstruction quality. The code of the proposed models ispublicly available at https://git.tu-berlin.de/rsim/HSI-SSC .</description><author>Martin Hermann Paul Fuchs, Akshara Preethy Byju, Alisa Walda, Behnood Rasti, Begüm Demir</author><pubDate>Thu, 14 Nov 2024 15:39:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08514v4</guid></item><item><title>CFCPalsy: Facial Image Synthesis with Cross-Fusion Cycle Diffusion Model for Facial Paralysis Individuals</title><link>http://arxiv.org/abs/2409.07271v3</link><description>Currently, the diagnosis of facial paralysis remains a challenging task,often relying heavily on the subjective judgment and experience of clinicians,which can introduce variability and uncertainty in the assessment process. Onepromising application in real-life situations is the automatic estimation offacial paralysis. However, the scarcity of facial paralysis datasets limits thedevelopment of robust machine learning models for automated diagnosis andtherapeutic interventions. To this end, this study aims to synthesize ahigh-quality facial paralysis dataset to address this gap, enabling moreaccurate and efficient algorithm training. Specifically, a novel Cross-FusionCycle Palsy Expression Generative Model (CFCPalsy) based on the diffusion modelis proposed to combine different features of facial information and enhance thevisual details of facial appearance and texture in facial regions, thuscreating synthetic facial images that accurately represent various degrees andtypes of facial paralysis. We have qualitatively and quantitatively evaluatedthe proposed method on the commonly used public clinical datasets of facialparalysis to demonstrate its effectiveness. Experimental results indicate thatthe proposed method surpasses state-of-the-art methods, generating morerealistic facial images and maintaining identity consistency.</description><author>Weixiang Gao, Yifan Xia</author><pubDate>Thu, 14 Nov 2024 15:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07271v3</guid></item><item><title>Randomized Truthful Auctions with Learning Agents</title><link>http://arxiv.org/abs/2411.09517v1</link><description>We study a setting where agents use no-regret learning algorithms toparticipate in repeated auctions. \citet{kolumbus2022auctions} showed, rathersurprisingly, that when bidders participate in second-price auctions usingno-regret bidding algorithms, no matter how large the number of interactions$T$ is, the runner-up bidder may not converge to bidding truthfully. Our firstresult shows that this holds for \emph{general deterministic} truthfulauctions. We also show that the ratio of the learning rates of the bidders can\emph{qualitatively} affect the convergence of the bidders. Next, we considerthe problem of revenue maximization in this environment. In the setting withfully rational bidders, \citet{myerson1981optimal} showed that revenue can bemaximized by using a second-price auction with reserves.We show that, in starkcontrast, in our setting with learning bidders, \emph{randomized} auctions canhave strictly better revenue guarantees than second-price auctions withreserves, when $T$ is large enough. Finally, we study revenue maximization inthe non-asymptotic regime. We define a notion of {\em auctioneer regret}comparing the revenue generated to the revenue of a second price auction withtruthful bids. When the auctioneer has to use the same auction throughout theinteraction, we show an (almost) tight regret bound of $\smash{\widetilde\Theta(T^{3/4})}.$ If the auctioneer can change auctions during theinteraction, but in a way that is oblivious to the bids, we show an (almost)tight bound of $\smash{\widetilde \Theta(\sqrt{T})}.$</description><author>Gagan Aggarwal, Anupam Gupta, Andres Perlroth, Grigoris Velegkas</author><pubDate>Thu, 14 Nov 2024 15:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09517v1</guid></item><item><title>Sharp Matrix Empirical Bernstein Inequalities</title><link>http://arxiv.org/abs/2411.09516v1</link><description>We present two sharp empirical Bernstein inequalities for symmetric randommatrices with bounded eigenvalues. By sharp, we mean that both inequalitiesadapt to the unknown variance in a tight manner: the deviation captured by thefirst-order $1/\sqrt{n}$ term asymptotically matches the matrix Bernsteininequality exactly, including constants, the latter requiring knowledge of thevariance. Our first inequality holds for the sample mean of independentmatrices, and our second inequality holds for a mean estimator under martingaledependence at stopping times.</description><author>Hongjian Wang, Aaditya Ramdas</author><pubDate>Thu, 14 Nov 2024 15:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09516v1</guid></item><item><title>GAN-Based Architecture for Low-dose Computed Tomography Imaging Denoising</title><link>http://arxiv.org/abs/2411.09512v1</link><description>Generative Adversarial Networks (GANs) have surfaced as a revolutionaryelement within the domain of low-dose computed tomography (LDCT) imaging,providing an advanced resolution to the enduring issue of reconciling radiationexposure with image quality. This comprehensive review synthesizes the rapidadvancements in GAN-based LDCT denoising techniques, examining the evolutionfrom foundational architectures to state-of-the-art models incorporatingadvanced features such as anatomical priors, perceptual loss functions, andinnovative regularization strategies. We critically analyze various GANarchitectures, including conditional GANs (cGANs), CycleGANs, andSuper-Resolution GANs (SRGANs), elucidating their unique strengths andlimitations in the context of LDCT denoising. The evaluation provides bothqualitative and quantitative results related to the improvements in performancein benchmark and clinical datasets with metrics such as PSNR, SSIM, and LPIPS.After highlighting the positive results, we discuss some of the challengespreventing a wider clinical use, including the interpretability of the imagesgenerated by GANs, synthetic artifacts, and the need for clinically relevantmetrics. The review concludes by highlighting the essential significance ofGAN-based methodologies in the progression of precision medicine via tailoredLDCT denoising models, underlining the transformative possibilities presentedby artificial intelligence within contemporary radiological practice.</description><author>Yunuo Wang, Ningning Yang, Jialin Li</author><pubDate>Thu, 14 Nov 2024 15:26:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09512v1</guid></item><item><title>Spatial Re-parameterization for N:M Sparsity</title><link>http://arxiv.org/abs/2306.05612v2</link><description>This paper presents a Spatial Re-parameterization (SpRe) method for the N:Msparsity in CNNs. SpRe is stemmed from an observation regarding the restrictedvariety in spatial sparsity present in N:M sparsity compared with unstructuredsparsity. Particularly, N:M sparsity exhibits a fixed sparsity rate within thespatial domains due to its distinctive pattern that mandates N non-zerocomponents among M successive weights in the input channel dimension ofconvolution filters. On the contrary, we observe that unstructured sparsitydisplays a substantial divergence in sparsity across the spatial domains, whichwe experimentally verified to be very crucial for its robust performanceretention compared with N:M sparsity. Therefore, SpRe employs thespatial-sparsity distribution of unstructured sparsity to assign an extrabranch in conjunction with the original N:M branch at training time, whichallows the N:M sparse network to sustain a similar distribution of spatialsparsity with unstructured sparsity. During inference, the extra branch can befurther re-parameterized into the main N:M branch, without exerting anydistortion on the sparse pattern or additional computation costs. SpRe hasachieved a commendable feat by matching the performance of N:M sparsity methodswith state-of-the-art unstructured sparsity methods across various benchmarks.Code and models are anonymously available at\url{https://github.com/zyxxmu/SpRe}.</description><author>Yuxin Zhang, Mingliang Xu, Yonghong Tian, Rongrong Ji</author><pubDate>Thu, 14 Nov 2024 15:22:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05612v2</guid></item><item><title>Communication Compression for Tensor Parallel LLM Inference</title><link>http://arxiv.org/abs/2411.09510v1</link><description>Large Language Models (LLMs) have pushed the frontier of artificialintelligence but are comprised of hundreds of billions of parameters andoperations. For faster inference latency, LLMs are deployed on multiplehardware accelerators through various Model Parallelism strategies. Our paperlooks into the details on one such strategy - Tensor Parallel - and proposes toreduce latency by compressing inter-accelerator communication. We leverage finegrained quantization techniques to compress selected activations by 3.5 - 4.5x.Our proposed method leads up to 2x reduction of time-to-first-token (TTFT) withnegligible model performance degradation.</description><author>Jan Hansen-Palmus, Michael Truong-Le, Oliver Hausdörfer, Alok Verma</author><pubDate>Thu, 14 Nov 2024 15:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09510v1</guid></item><item><title>Toward a Cohesive AI and Simulation Software Ecosystem for Scientific Innovation</title><link>http://arxiv.org/abs/2411.09507v1</link><description>In this paper, we discuss the need for an integrated software stack thatunites artificial intelligence (AI) and modeling and simulation (ModSim) toolsto advance scientific discovery. The authors advocate for a unified AI/ModSimsoftware ecosystem that ensures compatibility across a wide range of softwareon diverse high-performance computing systems, promoting ease of deployment,version management, and binary distribution. Key challenges highlighted includebalancing the distinct needs of AI and ModSim, especially in terms of softwarebuild practices, dependency management, and compatibility. The documentunderscores the importance of continuous integration, community-drivenstewardship, and collaboration with the Department of Energy (DOE) to develop aportable and cohesive scientific software ecosystem. Recommendations focus onsupporting standardized environments through initiatives like the Extreme-scaleScientific Software Stack (E4S) and Spack to foster interdisciplinaryinnovation and facilitate new scientific advancements.</description><author>Michael A. Heroux, Sameer Shende, Lois Curfman McInnes, Todd Gamblin, James M. Willenbring</author><pubDate>Thu, 14 Nov 2024 15:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09507v1</guid></item><item><title>Golden Noise for Diffusion Models: A Learning Framework</title><link>http://arxiv.org/abs/2411.09502v1</link><description>Text-to-image diffusion model is a popular paradigm that synthesizespersonalized images by providing a text prompt and a random Gaussian noise.While people observe that some noises are ``golden noises'' that can achievebetter text-image alignment and higher human preference than others, we stilllack a machine learning framework to obtain those golden noises. To learngolden noises for diffusion sampling, we mainly make three contributions inthis paper. First, we identify a new concept termed the \textit{noise prompt},which aims at turning a random Gaussian noise into a golden noise by adding asmall desirable perturbation derived from the text prompt. Following theconcept, we first formulate the \textit{noise prompt learning} framework thatsystematically learns ``prompted'' golden noise associated with a text promptfor diffusion models. Second, we design a noise prompt data collection pipelineand collect a large-scale \textit{noise prompt dataset}~(NPD) that contains100k pairs of random noises and golden noises with the associated text prompts.With the prepared NPD as the training dataset, we trained a small \textit{noiseprompt network}~(NPNet) that can directly learn to transform a random noiseinto a golden noise. The learned golden noise perturbation can be considered asa kind of prompt for noise, as it is rich in semantic information and tailoredto the given text prompt. Third, our extensive experiments demonstrate theimpressive effectiveness and generalization of NPNet on improving the qualityof synthesized images across various diffusion models, including SDXL,DreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small andefficient controller that acts as a plug-and-play module with very limitedadditional inference and computational costs, as it just provides a goldennoise instead of a random noise without accessing the original pipeline.</description><author>Zikai Zhou, Shitong Shao, Lichen Bai, Zhiqiang Xu, Bo Han, Zeke Xie</author><pubDate>Thu, 14 Nov 2024 15:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09502v1</guid></item><item><title>Developement of Reinforcement Learning based Optimisation Method for Side-Sill Design</title><link>http://arxiv.org/abs/2411.09499v1</link><description>Optimisation for crashworthiness is a critical part of the vehicledevelopment process. Due to stringent regulations and increasing marketdemands, multiple factors must be considered within a limited timeframe.However, for optimal crashworthiness design, multiobjective optimisation isnecessary, and for complex parts, multiple design parameters must be evaluated.This crashworthiness analysis requires computationally intensive finite elementsimulations. This challenge leads to the need for inverse multi-parametermulti-objective optimisation. This challenge leads to the need formulti-parameter, multi-objective inverse optimisation. This articleinvestigates a machine learning-based method for this type of optimisation,focusing on the design optimisation of a multi-cell side sill to improvecrashworthiness results. Furthermore, the optimiser is coupled with an FEsolver to achieve improved results.</description><author>Aditya Borse, Rutwik Gulakala, Marcus Stoffel</author><pubDate>Thu, 14 Nov 2024 15:06:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09499v1</guid></item><item><title>Generative Forests</title><link>http://arxiv.org/abs/2308.03648v3</link><description>We focus on generative AI for a type of data that still represent one of themost prevalent form of data: tabular data. Our paper introduces two keycontributions: a new powerful class of forest-based models fit for such tasksand a simple training algorithm with strong convergence guarantees in aboosting model that parallels that of the original weak / strong supervisedlearning setting. This algorithm can be implemented by a few tweaks to the mostpopular induction scheme for decision tree induction (i.e. supervised learning)with two classes. Experiments on the quality of generated data displaysubstantial improvements compared to the state of the art. The losses ouralgorithm minimize and the structure of our models make them practical forrelated tasks that require fast estimation of a density given a generativemodel and an observation (even partially specified): such tasks include missingdata imputation and density estimation. Additional experiments on these tasksreveal that our models can be notably good contenders to diverse state of theart methods, relying on models as diverse as (or mixing elements of) trees,neural nets, kernels or graphical models.</description><author>Richard Nock, Mathieu Guillame-Bert</author><pubDate>Thu, 14 Nov 2024 15:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03648v3</guid></item><item><title>Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling</title><link>http://arxiv.org/abs/2410.01440v3</link><description>In the endeavor to make autonomous robots take actions, task planning is amajor challenge that requires translating high-level task descriptions intolong-horizon action sequences. Despite recent advances in language modelagents, they remain prone to planning errors and limited in their ability toplan ahead. To address these limitations in robotic planning, we advocate aself-refining scheme that iteratively refines a draft plan until an equilibriumis reached. Remarkably, this process can be optimized end-to-end from ananalytical perspective without the need to curate additional verifiers orreward models, allowing us to train self-refining planners in a simplesupervised learning fashion. Meanwhile, a nested equilibrium sequence modelingprocedure is devised for efficient closed-loop planning that incorporatesuseful feedback from the environment (or an internal world model). Our methodis evaluated on the VirtualHome-Env benchmark, showing advanced performancewith better scaling for inference computation. Code is available athttps://github.com/Singularity0104/equilibrium-planner.</description><author>Jinghan Li, Zhicheng Sun, Fei Li, Cao Sheng, Jiazhong Yu, Yadong Mu</author><pubDate>Thu, 14 Nov 2024 15:04:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01440v3</guid></item><item><title>The Use of Readability Metrics in Legal Text: A Systematic Literature Review</title><link>http://arxiv.org/abs/2411.09497v1</link><description>Understanding the text in legal documents can be challenging due to theircomplex structure and the inclusion of domain-specific jargon. Laws andregulations are often crafted in such a manner that engagement with themrequires formal training, potentially leading to vastly differentinterpretations of the same texts. Linguistic complexity is an importantcontributor to the difficulties experienced by readers. Simplifying texts couldenhance comprehension across a broader audience, not just among trainedprofessionals. Various metrics have been developed to measure documentreadability. Therefore, we adopted a systematic review approach to examine thelinguistic and readability metrics currently employed for legal and regulatorytexts. A total of 3566 initial papers were screened, with 34 relevant studiesfound and further assessed. Our primary objective was to identify which currentmetrics were applied for evaluating readability within the legal field. Sixteendifferent metrics were identified, with the Flesch-Kincaid Grade Level beingthe most frequently used method. The majority of studies (73.5%) were found inthe domain of "informed consent forms". From the analysis, it is clear that notall legal domains are well represented in terms of readability metrics and thatthere is a further need to develop more consensus on which metrics should beapplied for legal documents.</description><author>Yu Han, Aaron Ceross, Jeroen H. M. Bergmann</author><pubDate>Thu, 14 Nov 2024 15:04:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09497v1</guid></item><item><title>MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in LLMs</title><link>http://arxiv.org/abs/2411.09492v1</link><description>Large language models (LLMs) excel in high-resource languages but facenotable challenges in low-resource languages like Mongolian. This paperaddresses these challenges by categorizing capabilities into language abilities(syntax and semantics) and cognitive abilities (knowledge and reasoning). Tosystematically evaluate these areas, we developed MM-Eval, a specializeddataset based on Modern Mongolian Language Textbook I and enriched with WebQSPand MGSM datasets. Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,Llama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all modelsperformed better on syntactic tasks than semantic tasks, highlighting a gap indeeper language understanding; and 2) knowledge tasks showed a moderatedecline, suggesting that models can transfer general knowledge fromhigh-resource to low-resource contexts. The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,and 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs inlow-resource languages like Mongolian. The dataset is available athttps://github.com/joenahm/MM-Eval.</description><author>Mengyuan Zhang, Ruihui Wang, Bo Xia, Yuan Sun, Xiaobing Zhao</author><pubDate>Thu, 14 Nov 2024 14:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09492v1</guid></item><item><title>DiffPAD: Denoising Diffusion-based Adversarial Patch Decontamination</title><link>http://arxiv.org/abs/2410.24006v2</link><description>In the ever-evolving adversarial machine learning landscape, developingeffective defenses against patch attacks has become a critical challenge,necessitating reliable solutions to safeguard real-world AI systems. Althoughdiffusion models have shown remarkable capacity in image synthesis and havebeen recently utilized to counter $\ell_p$-norm bounded attacks, theirpotential in mitigating localized patch attacks remains largely underexplored.In this work, we propose DiffPAD, a novel framework that harnesses the power ofdiffusion models for adversarial patch decontamination. DiffPAD first performssuper-resolution restoration on downsampled input images, then adoptsbinarization, dynamic thresholding scheme and sliding window for effectivelocalization of adversarial patches. Such a design is inspired by thetheoretically derived correlation between patch size and diffusion restorationerror that is generalized across diverse patch attack scenarios. Finally,DiffPAD applies inpainting techniques to the original input images with theestimated patch region being masked. By integrating closed-form solutions forsuper-resolution restoration and image inpainting into the conditional reversesampling process of a pre-trained diffusion model, DiffPAD obviates the needfor text guidance or fine-tuning. Through comprehensive experiments, wedemonstrate that DiffPAD not only achieves state-of-the-art adversarialrobustness against patch attacks but also excels in recovering naturalisticimages without patch remnants. The source code is available athttps://github.com/JasonFu1998/DiffPAD.</description><author>Jia Fu, Xiao Zhang, Sepideh Pashami, Fatemeh Rahimian, Anders Holst</author><pubDate>Thu, 14 Nov 2024 14:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24006v2</guid></item><item><title>Affordance-based Robot Manipulation with Flow Matching</title><link>http://arxiv.org/abs/2409.01083v2</link><description>We present a framework for assistive robot manipulation, which focuses on twofundamental challenges: first, efficiently adapting large-scale models todownstream scene affordance understanding tasks, especially in daily livingscenarios where gathering multi-task data involving humans requires strenuouseffort; second, effectively learning robot trajectories by grounding the visualaffordance model. We tackle the first challenge by employing aparameter-efficient prompt tuning method that prepends learnable text promptsto the frozen vision model to predict manipulation affordances in multi-taskscenarios. Then we propose to learn robot trajectories guided by affordances ina supervised Flow Matching method. Flow matching represents a robot visuomotorpolicy as a conditional process of flowing random waypoints to desired robottrajectories. Finally, we introduce a real-world dataset with 10 tasks acrossActivities of Daily Living to test our framework. Our extensive evaluationhighlights that the proposed prompt tuning method for learning manipulationaffordance with language prompter achieves competitive performance and evenoutperforms other finetuning protocols across data scales, while satisfyingparameter efficiency. Learning multi-task robot trajectories with flow matchingpolicy also leads to consistently better generalization performance and fasterinference than alternative behavior cloning methods, especially givenmultimodal robot action distributions. Our framework seamlessly unifiesaffordance model learning and trajectory generation with flow matching forrobot manipulation.</description><author>Fan Zhang, Michael Gienger</author><pubDate>Thu, 14 Nov 2024 14:52:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01083v2</guid></item><item><title>TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for Traversability Estimation</title><link>http://arxiv.org/abs/2406.01395v3</link><description>This paper presents TE-NeXt, a novel and efficient architecture forTraversability Estimation (TE) from sparse LiDAR point clouds based on aresidual convolution block. TE-NeXt block fuses notions of current trends suchas attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstratehigh capacity for generalisation in a variety of urban and naturalenvironments, using well-known and accessible datasets such as SemanticKITTI,Rellis-3D and SemanticUSL. Thus, the designed architecture ouperformsstate-of-the-art methods in the problem of semantic segmentation, demonstratingbetter results in unstructured environments and maintaining high reliabilityand robustness in urbans environments, which leads to better abstraction.Implementation is available in a open repository to the scientific communitywith the aim of ensuring the reproducibility of results.</description><author>Antonio Santo, Juan J. Cabrera, David Valiente, Carlos Viegas, Arturo Gil</author><pubDate>Thu, 14 Nov 2024 14:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01395v3</guid></item><item><title>Image Matching Filtering and Refinement by Planes and Beyond</title><link>http://arxiv.org/abs/2411.09484v1</link><description>This paper introduces a modular, non-deep learning method for filtering andrefining sparse correspondences in image matching. Assuming that motion flowwithin the scene can be approximated by local homography transformations,matches are aggregated into overlapping clusters corresponding to virtualplanes using an iterative RANSAC-based approach, with non-conformingcorrespondences discarded. Moreover, the underlying planar structural designprovides an explicit map between local patches associated with the matches,enabling optional refinement of keypoint positions through cross-correlationtemplate matching after patch reprojection. Finally, to enhance robustness andfault-tolerance against violations of the piece-wise planar approximationassumption, a further strategy is designed for minimizing relative patchdistortion in the plane reprojection by introducing an intermediate homographythat projects both patches into a common plane. The proposed method isextensively evaluated on standard datasets and image matching pipelines, andcompared with state-of-the-art approaches. Unlike other current comparisons,the proposed benchmark also takes into account the more general, real, andpractical cases where camera intrinsics are unavailable. Experimental resultsdemonstrate that our proposed non-deep learning, geometry-based approachachieves performances that are either superior to or on par with recentstate-of-the-art deep learning methods. Finally, this study suggests that thereare still development potential in actual image matching solutions in theconsidered research direction, which could be in the future incorporated innovel deep image matching architectures.</description><author>Fabio Bellavia, Zhenjun Zhao, Luca Morelli, Fabio Remondino</author><pubDate>Thu, 14 Nov 2024 14:37:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09484v1</guid></item><item><title>Sparse Bayesian Generative Modeling for Compressive Sensing</title><link>http://arxiv.org/abs/2411.09483v1</link><description>This work addresses the fundamental linear inverse problem in compressivesensing (CS) by introducing a new type of regularizing generative prior. Ourproposed method utilizes ideas from classical dictionary-based CS and, inparticular, sparse Bayesian learning (SBL), to integrate a strongregularization towards sparse solutions. At the same time, by leveraging thenotion of conditional Gaussianity, it also incorporates the adaptability fromgenerative models to training data. However, unlike most state-of-the-artgenerative models, it is able to learn from a few compressed and noisy datasamples and requires no optimization algorithm for solving the inverse problem.Additionally, similar to Dirichlet prior networks, our model parameterizes aconjugate prior enabling its application for uncertainty quantification. Wesupport our approach theoretically through the concept of variational inferenceand validate it empirically using different types of compressible signals.</description><author>Benedikt Böck, Sadaf Syed, Wolfgang Utschick</author><pubDate>Thu, 14 Nov 2024 14:37:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09483v1</guid></item><item><title>What makes a good BIM design: quantitative linking between design behavior and quality</title><link>http://arxiv.org/abs/2411.09481v1</link><description>In the Architecture Engineering &amp; Construction (AEC) industry, how designbehaviors impact design quality remains unclear. This study proposes a novelapproach, which, for the first time, identifies and quantitatively describesthe relationship between design behaviors and quality of design based onBuilding Information Modeling (BIM). Real-time collection and log mining areintegrated to collect raw data of design behaviors. Feature engineering andvarious machine learning models are then utilized for quantitative modeling andinterpretation. Results confirm an existing quantifiable relationship which canbe learned by various models. The best-performing model using Extremely RandomTrees achieved an R2 value of 0.88 on the test set. Behavioral features relatedto designer's skill level and changes of design intentions are identified tohave significant impacts on design quality. These findings deepen ourunderstanding of the design process and help forming BIM designs with betterquality.</description><author>Xiang-Rui Ni, Peng Pan, Jia-Rui Lin</author><pubDate>Thu, 14 Nov 2024 14:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09481v1</guid></item><item><title>Improving Arabic Multi-Label Emotion Classification using Stacked Embeddings and Hybrid Loss Function</title><link>http://arxiv.org/abs/2410.03979v3</link><description>In multi-label emotion classification, particularly for low-resourcelanguages like Arabic, the challenges of class imbalance and label correlationhinder model performance, especially in accurately predicting minorityemotions. To address these issues, this study proposes a novel approach thatcombines stacked embeddings, meta-learning, and a hybrid loss function toenhance multi-label emotion classification for the Arabic language. The studyextracts contextual embeddings from three fine-tuned languagemodels-ArabicBERT, MarBERT, and AraBERT-which are then stacked to form enrichedembeddings. A meta-learner is trained on these stacked embeddings, and theresulting concatenated representations are provided as input to a Bi-LSTMmodel, followed by a fully connected neural network for multi-labelclassification. To further improve performance, a hybrid loss function isintroduced, incorporating class weighting, label correlation matrix, andcontrastive learning, effectively addressing class imbalances and improving thehandling of label correlations. Extensive experiments validate the proposedmodel's performance across key metrics such as Precision, Recall, F1-Score,Jaccard Accuracy, and Hamming Loss. The class-wise performance analysisdemonstrates the hybrid loss function's ability to significantly reducedisparities between majority and minority classes, resulting in a more balancedemotion classification. An ablation study highlights the contribution of eachcomponent, showing the superiority of the model compared to baseline approachesand other loss functions. This study not only advances multi-label emotionclassification for Arabic but also presents a generalizable framework that canbe adapted to other languages and domains, providing a significant step forwardin addressing the challenges of low-resource emotion classification tasks.</description><author>Muhammad Azeem Aslam, Wang Jun, Nisar Ahmed, Muhammad Imran Zaman, Li Yanan, Hu Hongfei, Wang Shiyu, Xin Liu</author><pubDate>Thu, 14 Nov 2024 14:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03979v3</guid></item><item><title>Graph Neural Networks and Differential Equations: A hybrid approach for data assimilation of fluid flows</title><link>http://arxiv.org/abs/2411.09476v1</link><description>This study presents a novel hybrid approach that combines Graph NeuralNetworks (GNNs) with Reynolds-Averaged Navier Stokes (RANS) equations toenhance the accuracy of mean flow reconstruction across a range of fluiddynamics applications. Traditional purely data-driven Neural Networks (NNs)models, often struggle maintaining physical consistency. Moreover, theytypically require large datasets to achieve reliable performances. The GNNframework, which naturally handles unstructured data such as complex geometriesin Computational Fluid Dynamics (CFD), is here integrated with RANS equationsas a physical baseline model. The methodology leverages the adjoint method,enabling the use of RANS-derived gradients as optimization terms in the GNNtraining process. This ensures that the learned model adheres to the governingphysics, maintaining physical consistency while improving the predictionaccuracy. We test our approach on multiple CFD scenarios, including casesinvolving generalization with respect to the Reynolds number, sparsemeasurements, denoising and inpainting of missing portions of the mean flow.The results demonstrate significant improvements in the accuracy of thereconstructed mean flow compared to purely data-driven models, using limitedamounts of data in the training dataset. The key strengths of this study arethe integration of physical laws into the training process of the GNN, and theability to achieve high-accuracy predictions with a limited amount of data,making this approach particularly valuable for applications in fluid dynamicswhere data is often scarce.</description><author>M. Quattromini, M. A. Bucci, S. Cherubini, O. Semeraro</author><pubDate>Thu, 14 Nov 2024 14:31:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09476v1</guid></item><item><title>ResidualDroppath: Enhancing Feature Reuse over Residual Connections</title><link>http://arxiv.org/abs/2411.09475v1</link><description>Residual connections are one of the most important components in neuralnetwork architectures for mitigating the vanishing gradient problem andfacilitating the training of much deeper networks. One possible explanation forhow residual connections aid deeper network training is by promoting featurereuse. However, we identify and analyze the limitations of feature reuse withvanilla residual connections. To address these limitations, we proposemodifications in training methods. Specifically, we provide an additionalopportunity for the model to learn feature reuse with residual connectionsthrough two types of iterations during training. The first type of iterationinvolves using droppath, which enforces feature reuse by randomly dropping asubset of layers. The second type of iteration focuses on training the droppedparts of the model while freezing the undropped parts. As a result, the droppedparts learn in a way that encourages feature reuse, as the model relies on theundropped parts with feature reuse in mind. Overall, we demonstratedperformance improvements in models with residual connections for imageclassification in certain cases.</description><author>Sejik Park</author><pubDate>Thu, 14 Nov 2024 14:31:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09475v1</guid></item><item><title>Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric</title><link>http://arxiv.org/abs/2402.06900v5</link><description>In the pursuit of developing Large Language Models (LLMs) that adhere tosocietal standards, it is imperative to detect the toxicity in the generatedtext. The majority of existing toxicity metrics rely on encoder models trainedon specific toxicity datasets, which are susceptible to out-of-distribution(OOD) problems and depend on the dataset's definition of toxicity. In thispaper, we introduce a robust metric grounded on LLMs to flexibly measuretoxicity according to the given definition. We first analyze the toxicityfactors, followed by an examination of the intrinsic toxic attributes of LLMsto ascertain their suitability as evaluators. Finally, we evaluate theperformance of our metric with detailed analysis. Our empirical resultsdemonstrate outstanding performance in measuring toxicity within verifiedfactors, improving on conventional metrics by 12 points in the F1 score. Ourfindings also indicate that upstream toxicity significantly influencesdownstream metrics, suggesting that LLMs are unsuitable for toxicityevaluations within unverified factors.</description><author>Hyukhun Koh, Dohyung Kim, Minwoo Lee, Kyomin Jung</author><pubDate>Thu, 14 Nov 2024 14:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06900v5</guid></item><item><title>Terracorder: Sense Long and Prosper</title><link>http://arxiv.org/abs/2408.02407v3</link><description>In-situ sensing devices need to be deployed in remote environments for longperiods of time; minimizing their power consumption is vital for maximisingboth their operational lifetime and coverage. We introduce Terracorder -- aversatile multi-sensor device -- and showcase its exceptionally low powerconsumption using an on-device reinforcement learning scheduler. We prototype aunique device setup for biodiversity monitoring and compare its battery lifeusing our scheduler against a number of fixed schedules; the scheduler capturesmore than 80% of events at less than 50% of the number of activations of thebest-performing fixed schedule. We then explore how a collaborative schedulercan maximise the useful operation of a network of devices, improving overallnetwork power consumption and robustness.</description><author>Josh Millar, Sarab Sethi, Hamed Haddadi, Anil Madhavapeddy</author><pubDate>Thu, 14 Nov 2024 14:26:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02407v3</guid></item><item><title>A Similarity-Based Oversampling Method for Multi-label Imbalanced Text Data</title><link>http://arxiv.org/abs/2411.01013v2</link><description>In real-world applications, as data availability increases, obtaining labeleddata for machine learning (ML) projects remains challenging due to the highcosts and intensive efforts required for data annotation. Many ML projects,particularly those focused on multi-label classification, also grapple withdata imbalance issues, where certain classes may lack sufficient data to traineffective classifiers. This study introduces and examines a novel oversamplingmethod for multi-label text classification, designed to address performancechallenges associated with data imbalance. The proposed method identifiespotential new samples from unlabeled data by leveraging similarity measuresbetween instances. By iteratively searching the unlabeled dataset, the methodlocates instances similar to those in underrepresented classes and evaluatestheir contribution to classifier performance enhancement. Instances thatdemonstrate performance improvement are then added to the labeled dataset.Experimental results indicate that the proposed approach effectively enhancesclassifier performance post-oversampling.</description><author>Ismail Hakki Karaman, Gulser Koksal, Levent Eriskin, Salih Salihoglu</author><pubDate>Thu, 14 Nov 2024 14:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.01013v2</guid></item><item><title>Renal Cell Carcinoma subtyping: learning from multi-resolution localization</title><link>http://arxiv.org/abs/2411.09471v1</link><description>Renal Cell Carcinoma is typically asymptomatic at the early stages for manypatients. This leads to a late diagnosis of the tumor, where the curabilitylikelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high,with respect to its incidence rate. To increase the survival chance, a fast andcorrect categorization of the tumor subtype is paramount. Nowadays,computerized methods, based on artificial intelligence, represent aninteresting opportunity to improve the productivity and the objectivity of themicroscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of theirexploitation is hampered by the paucity of annotated dataset, essential for aproficient training of supervised machine learning technologies. This studysets out to investigate a novel self supervised training strategy for machinelearning diagnostic tools, based on the multi-resolution nature of thehistological samples. We aim at reducing the need of annotated dataset, withoutsignificantly reducing the accuracy of the tool. We demonstrate theclassification capability of our tool on a whole slide imaging dataset forRenal Cancer subtyping, and we compare our solution with severalstate-of-the-art classification counterparts.</description><author>Mohamad Mohamad, Francesco Ponzio, Santa Di Cataldo, Damien Ambrosetti, Xavier Descombes</author><pubDate>Thu, 14 Nov 2024 14:21:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09471v1</guid></item></channel></rss>