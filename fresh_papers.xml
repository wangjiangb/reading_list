<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 23 May 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion</title><link>http://arxiv.org/abs/2402.03309v2</link><description>Underwater perception and 3D surface reconstruction are challenging problemswith broad applications in construction, security, marine archaeology, andenvironmental monitoring. Treacherous operating conditions, fragilesurroundings, and limited navigation control often dictate that submersiblesrestrict their range of motion and, thus, the baseline over which they cancapture measurements. In the context of 3D scene reconstruction, it iswell-known that smaller baselines make reconstruction more challenging. Ourwork develops a physics-based multimodal acoustic-optical neural surfacereconstruction framework (AONeuS) capable of effectively integratinghigh-resolution RGB measurements with low-resolution depth-resolved imagingsonar measurements. By fusing these complementary modalities, our framework canreconstruct accurate high-resolution 3D surfaces from measurements capturedover heavily-restricted baselines. Through extensive simulations and in-labexperiments, we demonstrate that AONeuS dramatically outperforms recentRGB-only and sonar-only inverse-differentiable-rendering--based surfacereconstruction methods. A website visualizing the results of our paper islocated at this address: https://aoneus.github.io/</description><author>Mohamad Qadri, Kevin Zhang, Akshay Hinduja, Michael Kaess, Adithya Pediredla, Christopher A. Metzler</author><pubDate>Tue, 21 May 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03309v2</guid></item><item><title>Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</title><link>http://arxiv.org/abs/2405.12981v1</link><description>Key-value (KV) caching plays an essential role in accelerating decoding fortransformer-based autoregressive large language models (LLMs). However, theamount of memory required to store the KV cache can become prohibitive at longsequence lengths and large batch sizes. Since the invention of the transformer,two of the most effective interventions discovered for reducing the size of theKV cache have been Multi-Query Attention (MQA) and its generalization,Grouped-Query Attention (GQA). MQA and GQA both modify the design of theattention block so that multiple query heads can share a single key/value head,reducing the number of distinct key/value heads by a large factor while onlyminimally degrading accuracy. In this paper, we show that it is possible totake Multi-Query Attention a step further by also sharing key and value headsbetween adjacent layers, yielding a new attention design we call Cross-LayerAttention (CLA). With CLA, we find that it is possible to reduce the size ofthe KV cache by another 2x while maintaining nearly the same accuracy asunmodified MQA. In experiments training 1B- and 3B-parameter models fromscratch, we demonstrate that CLA provides a Pareto improvement over thememory/accuracy tradeoffs which are possible with traditional MQA, enablinginference with longer sequence lengths and larger batch sizes than wouldotherwise be possible</description><author>William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, Jonathan Ragan Kelly</author><pubDate>Tue, 21 May 2024 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12981v1</guid></item><item><title>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</title><link>http://arxiv.org/abs/2405.12979v1</link><description>The image matching field has been witnessing a continuous emergence of novellearnable feature matching techniques, with ever-improving performance onconventional benchmarks. However, our investigation shows that despite thesegains, their potential for real-world applications is restricted by theirlimited generalization capabilities to novel image domains. In this paper, weintroduce OmniGlue, the first learnable image matcher that is designed withgeneralization as a core principle. OmniGlue leverages broad knowledge from avision foundation model to guide the feature matching process, boostinggeneralization to domains not seen at training time. Additionally, we propose anovel keypoint position-guided attention mechanism which disentangles spatialand appearance information, leading to enhanced matching descriptors. Weperform comprehensive experiments on a suite of $7$ datasets with varied imagedomains, including scene-level, object-centric and aerial images. OmniGlue'snovel components lead to relative gains on unseen domains of $20.9\%$ withrespect to a directly comparable reference model, while also outperforming therecent LightGlue method by $9.5\%$ relatively.Code and model can be found athttps://hwjiang1510.github.io/OmniGlue</description><author>Hanwen Jiang, Arjun Karpur, Bingyi Cao, Qixing Huang, Andre Araujo</author><pubDate>Tue, 21 May 2024 18:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12979v1</guid></item><item><title>Auto-Linear Phenomenon in Subsurface Imaging</title><link>http://arxiv.org/abs/2305.13314v3</link><description>Subsurface imaging involves solving full waveform inversion (FWI) to predictgeophysical properties from measurements. This problem can be reframed as animage-to-image translation, with the usual approach being to train anencoder-decoder network using paired data from two domains: geophysicalproperty and measurement. A recent seminal work (InvLINT) demonstrates there isonly a linear mapping between the latent spaces of the two domains, and thedecoder requires paired data for training. This paper extends this direction by demonstrating that only linear mappingnecessitates paired data, while both the encoder and decoder can be learnedfrom their respective domains through self-supervised learning. This unveils anintriguing phenomenon (named Auto-Linear) where the self-learned features oftwo separate domains are automatically linearly correlated. Compared withexisting methods, our Auto-Linear has four advantages: (a) solving both forwardand inverse modeling simultaneously, (b) applicable to different subsurfaceimaging tasks and achieving markedly better results than previous methods,(c)enhanced performance, especially in scenarios with limited paired data andin the presence of noisy data, and (d) strong generalization ability of thetrained encoder and decoder.</description><author>Yinan Feng, Yinpeng Chen, Peng Jin, Shihang Feng, Zicheng Liu, Youzuo Lin</author><pubDate>Tue, 21 May 2024 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13314v3</guid></item><item><title>Personalized Residuals for Concept-Driven Text-to-Image Generation</title><link>http://arxiv.org/abs/2405.12978v1</link><description>We present personalized residuals and localized attention-guided sampling forefficient concept-driven generation using text-to-image diffusion models. Ourmethod first represents concepts by freezing the weights of a pretrainedtext-conditioned diffusion model and learning low-rank residuals for a smallsubset of the model's layers. The residual-based approach then directly enablesapplication of our proposed sampling technique, which applies the learnedresiduals only in areas where the concept is localized via cross-attention andapplies the original diffusion weights in all other regions. Localized samplingtherefore combines the learned identity of the concept with the existinggenerative prior of the underlying diffusion model. We show that personalizedresiduals effectively capture the identity of a concept in ~3 minutes on asingle GPU without the use of regularization images and with fewer parametersthan previous models, and localized sampling allows using the original model asstrong prior for large parts of the image.</description><author>Cusuh Ham, Matthew Fisher, James Hays, Nicholas Kolkin, Yuchen Liu, Richard Zhang, Tobias Hinz</author><pubDate>Tue, 21 May 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12978v1</guid></item><item><title>On the Efficiency of Convolutional Neural Networks</title><link>http://arxiv.org/abs/2404.03617v2</link><description>Since the breakthrough performance of AlexNet in 2012, convolutional neuralnetworks (convnets) have grown into extremely powerful vision models. Deeplearning researchers have used convnets to perform vision tasks with accuracythat was unachievable a decade ago. Confronted with the immense computationthat convnets use, deep learning researchers also became interested inefficiency. However, the engineers who deployed efficient convnets soonrealized that they were slower than the previous generation, despite usingfewer operations. Many reverted to older models that ran faster. Henceresearchers switched the objective of their search from arithmetic complexityto latency and produced a new wave of models that performed better.Paradoxically, these models also used more operations. Skepticism grew amongresearchers and engineers alike about the relevance of arithmetic complexity.Contrary to the prevailing view that latency and arithmetic complexity areirreconcilable, a simple formula relates both through computational efficiency.This insight enabled us to co-optimize the separate factors that determinelatency. We observed that the degenerate conv2d layers that produce the bestaccuracy--complexity trade-off also use significant memory resources and havelow computational efficiency. We devised block fusion algorithms to implementall the layers of a residual block in a single kernel, thereby creatingtemporal locality, avoiding communication, and reducing workspace size. OurConvFirst model with block-fusion kernels has less arithmetic complexity andgreater computational efficiency than baseline models and kernels, and ranapproximately four times as fast as ConvNeXt. We also created novel tools,including efficiency gap plots and waterline analysis. Our unified approach toconvnet efficiency envisions a new era of models and kernels that achievegreater accuracy at lower cost.</description><author>Andrew Lavin</author><pubDate>Tue, 21 May 2024 18:56:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03617v2</guid></item><item><title>BiomedParse: a biomedical foundation model for image parsing of everything everywhere all at once</title><link>http://arxiv.org/abs/2405.12971v1</link><description>Biomedical image analysis is fundamental for biomedical discovery in cellbiology, pathology, radiology, and many other biomedical domains. Holisticimage analysis comprises interdependent subtasks such as segmentation,detection, and recognition of relevant objects. Here, we propose BiomedParse, abiomedical foundation model for imaging parsing that can jointly conductsegmentation, detection, and recognition for 82 object types across 9 imagingmodalities. Through joint learning, we can improve accuracy for individualtasks and enable novel applications such as segmenting all relevant objects inan image through a text prompt, rather than requiring users to laboriouslyspecify the bounding box for each object. We leveraged readily availablenatural-language labels or descriptions accompanying those datasets and useGPT-4 to harmonize the noisy, unstructured text information with establishedbiomedical object ontologies. We created a large dataset comprising over sixmillion triples of image, segmentation mask, and textual description. On imagesegmentation, we showed that BiomedParse is broadly applicable, outperformingstate-of-the-art methods on 102,855 test image-mask-label triples across 9imaging modalities (everything). On object detection, which aims to locate aspecific object of interest, BiomedParse again attained state-of-the-artperformance, especially on objects with irregular shapes (everywhere). Onobject recognition, which aims to identify all objects in a given image alongwith their semantic types, we showed that BiomedParse can simultaneouslysegment and label all biomedical objects in an image (all at once). In summary,BiomedParse is an all-in-one tool for biomedical image analysis by jointlysolving segmentation, detection, and recognition for all major biomedical imagemodalities, paving the path for efficient and accurate image-based biomedicaldiscovery.</description><author>Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Brian Piening, Carlo Bifulco, Mu Wei, Hoifung Poon, Sheng Wang</author><pubDate>Tue, 21 May 2024 18:54:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12971v1</guid></item><item><title>Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control</title><link>http://arxiv.org/abs/2405.12970v1</link><description>Current face reenactment and swapping methods mainly rely on GAN frameworks,but recent focus has shifted to pre-trained diffusion models for their superiorgeneration capabilities. However, training these models is resource-intensive,and the results have not yet achieved satisfactory performance levels. Toaddress this issue, we introduce Face-Adapter, an efficient and effectiveadapter designed for high-precision and high-fidelity face editing forpre-trained diffusion models. We observe that both face reenactment/swappingtasks essentially involve combinations of target structure, ID and attribute.We aim to sufficiently decouple the control of these factors to achieve bothtasks in one model. Specifically, our method contains: 1) A Spatial ConditionGenerator that provides precise landmarks and background; 2) A Plug-and-playIdentity Encoder that transfers face embeddings to the text space by atransformer decoder. 3) An Attribute Controller that integrates spatialconditions and detailed attributes. Face-Adapter achieves comparable or evensuperior performance in terms of motion control precision, ID retentioncapability, and generation quality compared to fully fine-tuned facereenactment/swapping models. Additionally, Face-Adapter seamlessly integrateswith various StableDiffusion models.</description><author>Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, Yong Liu</author><pubDate>Tue, 21 May 2024 18:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12970v1</guid></item><item><title>Can We Treat Noisy Labels as Accurate?</title><link>http://arxiv.org/abs/2405.12969v1</link><description>Noisy labels significantly hinder the accuracy and generalization of machinelearning models, particularly due to ambiguous instance features. Traditionaltechniques that attempt to correct noisy labels directly, such as those usingtransition matrices, often fail to address the inherent complexities of theproblem sufficiently. In this paper, we introduce EchoAlign, a transformativeparadigm shift in learning from noisy labels. Instead of focusing on labelcorrection, EchoAlign treats noisy labels ($\tilde{Y}$) as accurate andmodifies corresponding instance features ($X$) to achieve better alignment with$\tilde{Y}$. EchoAlign's core components are (1) EchoMod: Employingcontrollable generative models, EchoMod precisely modifies instances whilemaintaining their intrinsic characteristics and ensuring alignment with thenoisy labels. (2) EchoSelect: Instance modification inevitably introducesdistribution shifts between training and test sets. EchoSelect maintains asignificant portion of clean original instances to mitigate these shifts. Itleverages the distinct feature similarity distributions between original andmodified instances as a robust tool for accurate sample selection. Thisintegrated approach yields remarkable results. In environments with 30%instance-dependent noise, even at 99% selection accuracy, EchoSelect retainsnearly twice the number of samples compared to the previous best method.Notably, on three datasets, EchoAlign surpasses previous state-of-the-arttechniques with a substantial improvement.</description><author>Yuxiang Zheng, Zhongyi Han, Yilong Yin, Xin Gao, Tongliang Liu</author><pubDate>Tue, 21 May 2024 18:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12969v1</guid></item><item><title>Scaling Down Deep Learning with MNIST-1D</title><link>http://arxiv.org/abs/2011.14439v4</link><description>Although deep learning models have taken on commercial and politicalrelevance, key aspects of their training and operation remain poorlyunderstood. This has sparked interest in science of deep learning projects,many of which require large amounts of time, money, and electricity. But howmuch of this research really needs to occur at scale? In this paper, weintroduce MNIST-1D: a minimalist, procedurally generated, low-memory, andlow-compute alternative to classic deep learning benchmarks. Although thedimensionality of MNIST-1D is only 40 and its default training set size only4000, MNIST-1D can be used to study inductive biases of different deeparchitectures, find lottery tickets, observe deep double descent, metalearn anactivation function, and demonstrate guillotine regularization inself-supervised learning. All these experiments can be conducted on a GPU oroften even on a CPU within minutes, allowing for fast prototyping, educationaluse cases, and cutting-edge research on a low budget.</description><author>Sam Greydanus, Dmitry Kobak</author><pubDate>Tue, 21 May 2024 18:48:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.14439v4</guid></item><item><title>The future of cosmological likelihood-based inference: accelerated high-dimensional parameter estimation and model comparison</title><link>http://arxiv.org/abs/2405.12965v1</link><description>We advocate for a new paradigm of cosmological likelihood-based inference,leveraging recent developments in machine learning and its underlyingtechnology, to accelerate Bayesian inference in high-dimensional settings.Specifically, we combine (i) emulation, where a machine learning model istrained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii)differentiable and probabilistic programming, e.g. JAX and NumPyro,respectively; (iii) scalable Markov chain Monte Carlo (MCMC) samplingtechniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv)decoupled and scalable Bayesian model selection techniques that compute theBayesian evidence purely from posterior samples, e.g. the learned harmonic meanimplemented in harmonic. This paradigm allows us to carry out a completeBayesian analysis, including both parameter estimation and model selection, ina fraction of the time of traditional approaches. First, we demonstrate theapplication of this paradigm on a simulated cosmic shear analysis for a StageIV survey in 37- and 39-dimensional parameter spaces, comparing $\Lambda$CDMand a dynamical dark energy model ($w_0w_a$CDM). We recover posterior contoursand evidence estimates that are in excellent agreement with those computed bythe traditional nested sampling approach while reducing the computational costfrom 8 months on 48 CPU cores to 2 days on 12 GPUs. Second, we consider a jointanalysis between three simulated next-generation surveys, each performing a3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces.Standard nested sampling techniques are simply not feasible in thishigh-dimensional setting, requiring a projected 12 years of compute time on 48CPU cores; on the other hand, the proposed approach only requires 8 days ofcompute time on 24 GPUs. All packages used in our analyses are publiclyavailable.</description><author>Davide Piras, Alicja Polanska, Alessio Spurio Mancini, Matthew A. Price, Jason D. McEwen</author><pubDate>Tue, 21 May 2024 18:45:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12965v1</guid></item><item><title>Comprehensive Multimodal Deep Learning Survival Prediction Enabled by a Transformer Architecture: A Multicenter Study in Glioblastoma</title><link>http://arxiv.org/abs/2405.12963v1</link><description>Background: This research aims to improve glioblastoma survival prediction byintegrating MR images, clinical and molecular-pathologic data in atransformer-based deep learning model, addressing data heterogeneity andperformance generalizability. Method: We propose and evaluate atransformer-based non-linear and non-proportional survival prediction model.The model employs self-supervised learning techniques to effectively encode thehigh-dimensional MRI input for integration with non-imaging data usingcross-attention. To demonstrate model generalizability, the model is assessedwith the time-dependent concordance index (Cdt) in two training setups usingthree independent public test sets: UPenn-GBM, UCSF-PDGM, and RHUH-GBM, eachcomprising 378, 366, and 36 cases, respectively. Results: The proposedtransformer model achieved promising performance for imaging as well asnon-imaging data, effectively integrating both modalities for enhancedperformance (UPenn-GBM test-set, imaging Cdt 0.645, multimodal Cdt 0.707) whileoutperforming state-of-the-art late-fusion 3D-CNN-based models. Consistentperformance was observed across the three independent multicenter test setswith Cdt values of 0.707 (UPenn-GBM, internal test set), 0.672 (UCSF-PDGM,first external test set) and 0.618 (RHUH-GBM, second external test set). Themodel achieved significant discrimination between patients with favorable andunfavorable survival for all three datasets (logrank p 1.9\times{10}^{-8},9.7\times{10}^{-3}, and 1.2\times{10}^{-2}). Conclusions: The proposedtransformer-based survival prediction model integrates complementaryinformation from diverse input modalities, contributing to improvedglioblastoma survival prediction compared to state-of-the-art methods.Consistent performance was observed across institutions supporting modelgeneralizability.</description><author>Ahmed Gomaa, Yixing Huang, Amr Hagag, Charlotte Schmitter, Daniel Höfler, Thomas Weissmann, Katharina Breininger, Manuel Schmidt, Jenny Stritzelberger, Daniel Delev, Roland Coras, Arnd Dörfler, Oliver Schnell, Benjamin Frey, Udo S. Gaipl, Sabine Semrau, Christoph Bert, Rainer Fietkau, Florian Putz</author><pubDate>Tue, 21 May 2024 18:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12963v1</guid></item><item><title>Budgeting Counterfactual for Offline RL</title><link>http://arxiv.org/abs/2307.06328v2</link><description>The main challenge of offline reinforcement learning, where data is limited,arises from a sequence of counterfactual reasoning dilemmas within the realm ofpotential actions: What if we were to choose a different course of action?These circumstances frequently give rise to extrapolation errors, which tend toaccumulate exponentially with the problem horizon. Hence, it becomes crucial toacknowledge that not all decision steps are equally important to the finaloutcome, and to budget the number of counterfactual decisions a policy make inorder to control the extrapolation. Contrary to existing approaches that useregularization on either the policy or value function, we propose an approachto explicitly bound the amount of out-of-distribution actions during training.Specifically, our method utilizes dynamic programming to decide where toextrapolate and where not to, with an upper bound on the decisions differentfrom behavior policy. It balances between the potential for improvement fromtaking out-of-distribution actions and the risk of making errors due toextrapolation. Theoretically, we justify our method by the constrainedoptimality of the fixed point solution to our $Q$ updating rules. Empirically,we show that the overall performance of our method is better than thestate-of-the-art offline RL methods on tasks in the widely-used D4RLbenchmarks.</description><author>Yao Liu, Pratik Chaudhari, Rasool Fakoor</author><pubDate>Tue, 21 May 2024 18:40:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06328v2</guid></item><item><title>Unsupervised Episode Generation for Graph Meta-learning</title><link>http://arxiv.org/abs/2306.15217v3</link><description>We propose Unsupervised Episode Generation method called Neighbors as Queries(NaQ) to solve the Few-Shot Node-Classification (FSNC) task by unsupervisedGraph Meta-learning. Doing so enables full utilization of the information ofall nodes in a graph, which is not possible in current supervised meta-learningmethods for FSNC due to the label-scarcity problem. In addition, unlikeunsupervised Graph Contrastive Learning (GCL) methods that overlook thedownstream task to be solved at the training phase resulting in vulnerabilityto class imbalance of a graph, we adopt the episodic learning framework thatallows the model to be aware of the downstream task format, i.e., FSNC. Theproposed NaQ is a simple but effective unsupervised episode generation methodthat randomly samples nodes from a graph to make a support set, followed bysimilarity-based sampling of nodes to make the corresponding query set. SinceNaQ is model-agnostic, any existing supervised graph meta-learning methods canbe trained in an unsupervised manner, while not sacrificing much of theirperformance or sometimes even improving them. Extensive experimental resultsdemonstrate the effectiveness of our proposed unsupervised episode generationmethod for graph meta-learning towards the FSNC task. Our code is available at:https://github.com/JhngJng/NaQ-PyTorch.</description><author>Jihyeong Jung, Sangwoo Seo, Sungwon Kim, Chanyoung Park</author><pubDate>Tue, 21 May 2024 18:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15217v3</guid></item><item><title>Investigating and Improving Latent Density Segmentation Models for Aleatoric Uncertainty Quantification in Medical Imaging</title><link>http://arxiv.org/abs/2307.16694v4</link><description>Data uncertainties, such as sensor noise, occlusions or limitations in theacquisition method can introduce irreducible ambiguities in images, whichresult in varying, yet plausible, semantic hypotheses. In Machine Learning,this ambiguity is commonly referred to as aleatoric uncertainty. In imagesegmentation, latent density models can be utilized to address this problem.The most popular approach is the Probabilistic U-Net (PU-Net), which useslatent Normal densities to optimize the conditional data log-likelihoodEvidence Lower Bound. In this work, we demonstrate that the PU-Net latent spaceis severely sparse and heavily under-utilized. To address this, we introducemutual information maximization and entropy-regularized Sinkhorn Divergence inthe latent space to promote homogeneity across all latent dimensions,effectively improving gradient-descent updates and latent spaceinformativeness. Our results show that by applying this on public datasets ofvarious clinical segmentation problems, our proposed methodology receives up to11% performance gains compared against preceding latent variable models forprobabilistic segmentation on the Hungarian-Matched Intersection over Union.The results indicate that encouraging a homogeneous latent space significantlyimproves latent density modeling for medical image segmentation.</description><author>M. M. Amaan Valiuddin, Christiaan G. A. Viviers, Ruud J. G. van Sloun, Peter H. N. de With, Fons van der Sommen</author><pubDate>Tue, 21 May 2024 18:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16694v4</guid></item><item><title>Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale</title><link>http://arxiv.org/abs/2405.12961v1</link><description>Searching through chemical space is an exceptionally challenging problembecause the number of possible molecules grows combinatorially with the numberof atoms. Large, autoregressive models trained on databases of chemicalcompounds have yielded powerful generators, but we still lack robust strategiesfor generating molecules with desired properties. This molecular search problemclosely resembles the "alignment" problem for large language models, though formany chemical tasks we have a specific and easily evaluable reward function.Here, we introduce an algorithm called energy rank alignment (ERA) thatleverages an explicit reward function to produce a gradient-based objectivethat we use to optimize autoregressive policies. We show theoretically thatthis algorithm is closely related to proximal policy optimization (PPO) anddirect preference optimization (DPO), but has a minimizer that converges to anideal Gibbs-Boltzmann distribution with the reward playing the role of anenergy function. Furthermore, this algorithm is highly scalable, does notrequire reinforcement learning, and performs well relative to DPO when thenumber of preference observations per pairing is small. We deploy this approachto align molecular transformers to generate molecules with externally specifiedproperties and find that it does so robustly, searching through diverse partsof chemical space. While our focus here is on chemical search, we also obtainexcellent results on an AI supervised task for LLM alignment, showing that themethod is scalable and general.</description><author>Shriram Chennakesavalu, Frank Hu, Sebastian Ibarraran, Grant M. Rotskoff</author><pubDate>Tue, 21 May 2024 18:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12961v1</guid></item><item><title>A Uniform Language to Explain Decision Trees</title><link>http://arxiv.org/abs/2310.11636v2</link><description>The formal XAI community has studied a plethora of interpretability queriesaiming to understand the classifications made by decision trees. However, amore uniform understanding of what questions we can hope to answer about thesemodels, traditionally deemed to be easily interpretable, has remained elusive.In an initial attempt to understand uniform languages for interpretability,Arenas et al. (2021) proposed FOIL, a logic for explaining black-box ML models,and showed that it can express a variety of interpretability queries. However,we show that FOIL is limited in two important senses: (i) it is not expressiveenough to capture some crucial queries, and (ii) its model agnostic natureresults in a high computational complexity for decision trees. In this paper,we carefully craft two fragments of first-order logic that allow forefficiently interpreting decision trees: Q-DT-FOIL and its optimization variantOPT-DT-FOIL. We show that our proposed logics can express not only a variety ofinterpretability queries considered by previous literature, but also elegantlyallows users to specify different objectives the sought explanations shouldoptimize for. Using finite model-theoretic techniques, we show that thedifferent ingredients of Q-DT-FOIL are necessary for its expressiveness, andyet that queries in Q-DT-FOIL can be evaluated with a polynomial number ofqueries to a SAT solver, as well as their optimization versions in OPT-DT-FOIL.Besides our theoretical results, we provide a SAT-based implementation of theevaluation for OPT-DT-FOIL that is performant on industry-size decision trees.</description><author>Marcelo Arenas, Pablo Barcelo, Diego Bustamante, Jose Caraball, Bernardo Subercaseaux</author><pubDate>Tue, 21 May 2024 18:34:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11636v2</guid></item><item><title>Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels</title><link>http://arxiv.org/abs/2404.14313v2</link><description>When prompting a language model (LM), users often expect the model to adhereto a set of behavioral principles across diverse tasks, such as producinginsightful content while avoiding harmful or biased language. Instilling suchprinciples (i.e., a constitution) into a model is resource-intensive,technically challenging, and generally requires human preference labels orexamples. We introduce SAMI, an iterative algorithm that finetunes a pretrainedlanguage model (without requiring preference labels or demonstrations) toincrease the conditional mutual information between constitutions andself-generated responses given queries from a dataset. On single-turn dialogueand summarization, a SAMI-trained mistral-7b outperforms the initial pretrainedmodel, with win rates between 66% and 77%. Strikingly, it also surpasses aninstruction-finetuned baseline (mistral-7b-instruct) with win rates between 55%and 57% on single-turn dialogue. SAMI requires a model that writes theprinciples. To avoid dependence on strong models for writing principles, wealign a strong pretrained model (mixtral-8x7b) using constitutions written by aweak instruction-finetuned model (mistral-7b-instruct), achieving a 65% winrate on summarization. Finally, we investigate whether SAMI generalizes todiverse summarization principles (e.g., "summaries should be scientific") andscales to stronger models (llama3-70b), finding that it achieves win rates ofup to 68% for learned and 67% for held-out principles compared to the basemodel. Our results show that a pretrained LM can learn to follow constitutionswithout using preference labels, demonstrations, or human oversight.</description><author>Jan-Philipp Fränken, Eric Zelikman, Rafael Rafailov, Kanishk Gandhi, Tobias Gerstenberg, Noah D. Goodman</author><pubDate>Tue, 21 May 2024 18:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14313v2</guid></item><item><title>Online Learning of Halfspaces with Massart Noise</title><link>http://arxiv.org/abs/2405.12958v1</link><description>We study the task of online learning in the presence of Massart noise.Instead of assuming that the online adversary chooses an arbitrary sequence oflabels, we assume that the context $\mathbf{x}$ is selected adversarially butthe label $y$ presented to the learner disagrees with the ground-truth label of$\mathbf{x}$ with unknown probability at most $\eta$. We study the fundamentalclass of $\gamma$-margin linear classifiers and present a computationallyefficient algorithm that achieves mistake bound $\eta T + o(T)$. Our mistakebound is qualitatively tight for efficient algorithms: it is known that even inthe offline setting achieving classification error better than $\eta$ requiressuper-polynomial time in the SQ model. We extend our online learning model to a $k$-arm contextual bandit settingwhere the rewards -- instead of satisfying commonly used realizabilityassumptions -- are consistent (in expectation) with some linear rankingfunction with weight vector $\mathbf{w}^\ast$. Given a list of contexts$\mathbf{x}_1,\ldots \mathbf{x}_k$, if $\mathbf{w}^*\cdot \mathbf{x}_i &gt;\mathbf{w}^* \cdot \mathbf{x}_j$, the expected reward of action $i$ must belarger than that of $j$ by at least $\Delta$. We use our Massart online learnerto design an efficient bandit algorithm that obtains expected reward at least$(1-1/k)~ \Delta T - o(T)$ bigger than choosing a random action at every round.</description><author>Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis</author><pubDate>Tue, 21 May 2024 18:31:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12958v1</guid></item><item><title>Truncated Variance Reduced Value Iteration</title><link>http://arxiv.org/abs/2405.12952v1</link><description>We provide faster randomized algorithms for computing an $\epsilon$-optimalpolicy in a discounted Markov decision process with$A_{\text{tot}}$-state-action pairs, bounded rewards, and discount factor$\gamma$. We provide an $\tilde{O}(A_{\text{tot}}[(1 -\gamma)^{-3}\epsilon^{-2} + (1 - \gamma)^{-2}])$-time algorithm in the samplingsetting, where the probability transition matrix is unknown but accessiblethrough a generative model which can be queried in $\tilde{O}(1)$-time, and an$\tilde{O}(s + (1-\gamma)^{-2})$-time algorithm in the offline setting wherethe probability transition matrix is known and $s$-sparse. These resultsimprove upon the prior state-of-the-art which either ran in$\tilde{O}(A_{\text{tot}}[(1 - \gamma)^{-3}\epsilon^{-2} + (1 - \gamma)^{-3}])$time [Sidford, Wang, Wu, Ye 2018] in the sampling setting, $\tilde{O}(s +A_{\text{tot}} (1-\gamma)^{-3})$ time [Sidford, Wang, Wu, Yang, Ye 2018] in theoffline setting, or time at least quadratic in the number of states usinginterior point methods for linear programming. We achieve our results bybuilding upon prior stochastic variance-reduced value iteration methods[Sidford, Wang, Wu, Yang, Ye 2018]. We provide a variant that carefullytruncates the progress of its iterates to improve the variance of newvariance-reduced sampling procedures that we introduce to implement the steps.Our method is essentially model-free and can be implemented in$\tilde{O}(A_{\text{tot}})$-space when given generative model access.Consequently, our results take a step in closing the sample-complexity gapbetween model-free and model-based methods.</description><author>Yujia Jin, Ishani Karmarkar, Aaron Sidford, Jiayi Wang</author><pubDate>Tue, 21 May 2024 18:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12952v1</guid></item><item><title>Improved Content Understanding With Effective Use of Multi-task Contrastive Learning</title><link>http://arxiv.org/abs/2405.11344v2</link><description>In enhancing LinkedIn core content recommendation models, a significantchallenge lies in improving their semantic understanding capabilities. Thispaper addresses the problem by leveraging multi-task learning, a method thathas shown promise in various domains. We fine-tune a pre-trained,transformer-based LLM using multi-task contrastive learning with data from adiverse set of semantic labeling tasks. We observe positive transfer, leadingto superior performance across all tasks when compared to trainingindependently on each. Our model outperforms the baseline on zero shot learningand offers improved multilingual support, highlighting its potential forbroader application. The specialized content embeddings produced by our modeloutperform generalized embeddings offered by OpenAI on Linkedin dataset andtasks. This work provides a robust foundation for vertical teams acrossLinkedIn to customize and fine-tune the LLM to their specific applications. Ourwork offers insights and best practices for the field to build on.</description><author>Akanksha Bindal, Sudarshan Ramanujam, Dave Golland, TJ Hazen, Tina Jiang, Fengyu Zhang, Peng Yan</author><pubDate>Tue, 21 May 2024 18:27:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11344v2</guid></item><item><title>Strategic Deployment of Honeypots in Blockchain-based IoT Systems</title><link>http://arxiv.org/abs/2405.12951v1</link><description>This paper addresses the challenge of enhancing cybersecurity inBlockchain-based Internet of Things (BIoTs) systems, which are increasinglyvulnerable to sophisticated cyberattacks. It introduces an AI-powered systemmodel for the dynamic deployment of honeypots, utilizing an Intrusion DetectionSystem (IDS) integrated with smart contract functionalities on IoT nodes. Thismodel enables the transformation of regular nodes into decoys in response tosuspicious activities, thereby strengthening the security of BIoT networks. Thepaper analyses strategic interactions between potential attackers and theAI-enhanced IDS through a game-theoretic model, specifically Bayesian games.The model focuses on understanding and predicting sophisticated attacks thatmay initially appear normal, emphasizing strategic decision-making, optimizedhoneypot deployment, and adaptive strategies in response to evolving attackpatterns.</description><author>Daniel Commey, Sena Hounsinou, Garth V. Crosby</author><pubDate>Tue, 21 May 2024 18:27:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12951v1</guid></item><item><title>Strategy-Proof Auctions through Conformal Prediction</title><link>http://arxiv.org/abs/2405.12016v2</link><description>Auctions are key for maximizing sellers' revenue and ensuring truthfulbidding among buyers. Recently, an approach known as differentiable economicsbased on deep learning shows promise in learning optimal auction mechanisms formultiple items and participants. However, this approach has no guarantee ofstrategy-proofness at test time. Strategy-proofness is crucial as it ensuresthat buyers are incentivized to bid their true valuations, leading to optimaland fair auction outcomes without the risk of manipulation. Building onconformal prediction, we introduce a novel approach to achievestrategy-proofness with rigorous statistical guarantees. The key novelties ofour method are: (i) the formulation of a regret prediction model, used toquantify at test time violations of strategy-proofness; and (ii) an auctionacceptance rule that leverages the predicted regret to ensure that for a newauction, the data-driven mechanism meets the strategy-proofness requirementwith high probability (e.g., 99\%). Numerical experiments demonstrate thenecessity for rigorous guarantees, the validity of our theoretical results, andthe applicability of our proposed method.</description><author>Roy Maor Lotan, Inbal Talgam-Cohen, Yaniv Romano</author><pubDate>Tue, 21 May 2024 18:20:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12016v2</guid></item><item><title>Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation</title><link>http://arxiv.org/abs/2402.19267v2</link><description>Low-resourced data presents a significant challenge for neural machinetranslation. In most cases, the low-resourced environment is caused by highcosts due to the need for domain experts or the lack of language experts.Therefore, identifying the most training-efficient data within an unsupervisedsetting emerges as a practical strategy. Recent research suggests that sucheffective data can be identified by selecting 'appropriately complex data'based on its volume, providing strong intuition for unsupervised dataselection. However, we have discovered that establishing criteria forunsupervised data selection remains a challenge, as the 'appropriate level ofdifficulty' may vary depending on the data domain. We introduce a novelunsupervised data selection method named 'Capturing Perplexing Named Entities,'which leverages the maximum inference entropy in translated named entities as ametric for selection. When tested with the 'Korean-English Parallel Corpus ofSpecialized Domains,' our method served as robust guidance for identifyingtraining-efficient data across different domains, in contrast to existingmethods.</description><author>Seunghyun Ji, Hagai Raja Sinulingga, Darongsae Kwon</author><pubDate>Tue, 21 May 2024 18:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19267v2</guid></item><item><title>GRACE-C: Generalized Rate Agnostic Causal Estimation via Constraints</title><link>http://arxiv.org/abs/2205.09235v4</link><description>Graphical structures estimated by causal learning algorithms from time seriesdata can provide misleading causal information if the causal timescale of thegenerating process fails to match the measurement timescale of the data.Existing algorithms provide limited resources to respond to this challenge, andso researchers must either use models that they know are likely misleading, orelse forego causal learning entirely. Existing methods face up-to-four distinctshortfalls, as they might 1) require that the difference between causal andmeasurement timescales is known; 2) only handle very small number of randomvariables when the timescale difference is unknown; 3) only apply to pairs ofvariables; or 4) be unable to find a solution given statistical noise in thedata. This research addresses these challenges. Our approach combinesconstraint programming with both theoretical insights into the problemstructure and prior information about admissible causal interactions to achievemultiple orders of magnitude in speed-up. The resulting system maintainstheoretical guarantees while scaling to significantly larger sets of randomvariables (&gt;100) without knowledge of timescale differences. This method isalso robust to edge misidentification and can use parametric connectionstrengths, while optionally finding the optimal solution among many possibleones.</description><author>Mohammadsajad Abavisani, David Danks, Sergey Plis</author><pubDate>Tue, 21 May 2024 18:19:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.09235v4</guid></item><item><title>Tutorly: Turning Programming Videos Into Apprenticeship Learning Environments with LLMs</title><link>http://arxiv.org/abs/2405.12946v1</link><description>Online programming videos, including tutorials and streamcasts, are widelypopular and contain a wealth of expert knowledge. However, effectivelyutilizing these resources to achieve targeted learning goals can bechallenging. Unlike direct tutoring, video content lacks tailored guidancebased on individual learning paces, personalized feedback, and interactiveengagement necessary for support and monitoring. Our work transformsprogramming videos into one-on-one tutoring experiences using the cognitiveapprenticeship framework. Tutorly, developed as a JupyterLab Plugin, allowslearners to (1) set personalized learning goals, (2) engage inlearning-by-doing through a conversational LLM-based mentor agent, (3) receiveguidance and feedback based on a student model that steers the mentor moves. Ina within-subject study with 16 participants learning exploratory data analysisfrom a streamcast, Tutorly significantly improved their performance from 61.9%to 76.6% based on a post-test questionnaire. Tutorly demonstrates the potentialfor enhancing programming video learning experiences with LLM and learnermodeling.</description><author>Wengxi Li, Roy Pea, Nick Haber, Hari Subramonyam</author><pubDate>Tue, 21 May 2024 18:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12946v1</guid></item><item><title>AMFD: Distillation via Adaptive Multimodal Fusion for Multispectral Pedestrian Detection</title><link>http://arxiv.org/abs/2405.12944v1</link><description>Multispectral pedestrian detection has been shown to be effective inimproving performance within complex illumination scenarios. However, prevalentdouble-stream networks in multispectral detection employ two separate featureextraction branches for multi-modal data, leading to nearly double theinference time compared to single-stream networks utilizing only one featureextraction branch. This increased inference time has hindered the widespreademployment of multispectral pedestrian detection in embedded devices forautonomous systems. To address this limitation, various knowledge distillationmethods have been proposed. However, traditional distillation methods focusonly on the fusion features and ignore the large amount of information in theoriginal multi-modal features, thereby restricting the student network'sperformance. To tackle the challenge, we introduce the Adaptive Modal FusionDistillation (AMFD) framework, which can fully utilize the original modalfeatures of the teacher network. Specifically, a Modal Extraction Alignment(MEA) module is utilized to derive learning weights for student networks,integrating focal and global attention mechanisms. This methodology enables thestudent network to acquire optimal fusion strategies independent from that ofteacher network without necessitating an additional feature fusion module.Furthermore, we present the SMOD dataset, a well-aligned challengingmultispectral dataset for detection. Extensive experiments on the challengingKAIST, LLVIP and SMOD datasets are conducted to validate the effectiveness ofAMFD. The results demonstrate that our method outperforms existingstate-of-the-art methods in both reducing log-average Miss Rate and improvingmean Average Precision. The code is available athttps://github.com/bigD233/AMFD.git.</description><author>Zizhao Chen, Yeqiang Qian, Xiaoxiao Yang, Chunxiang Wang, Ming Yang</author><pubDate>Tue, 21 May 2024 18:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12944v1</guid></item><item><title>Learning the Infinitesimal Generator of Stochastic Diffusion Processes</title><link>http://arxiv.org/abs/2405.12940v1</link><description>We address data-driven learning of the infinitesimal generator of stochasticdiffusion processes, essential for understanding numerical simulations ofnatural and physical systems. The unbounded nature of the generator posessignificant challenges, rendering conventional analysis techniques forHilbert-Schmidt operators ineffective. To overcome this, we introduce a novelframework based on the energy functional for these stochastic processes. Ourapproach integrates physical priors through an energy-based risk metric in bothfull and partial knowledge settings. We evaluate the statistical performance ofa reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in thepartial knowledge setting. Notably, our approach provides learning boundsindependent of the state space dimension and ensures non-spurious spectralestimation. Additionally, we elucidate how the distortion between the intrinsicenergy-induced metric of the stochastic diffusion and the RKHS metric used forgenerator estimation impacts the spectral learning bounds.</description><author>Vladimir R. Kostic, Karim Lounici, Helene Halconruy, Timothee Devergne, Massimiliano Pontil</author><pubDate>Tue, 21 May 2024 18:13:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12940v1</guid></item><item><title>Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models</title><link>http://arxiv.org/abs/2405.12939v1</link><description>Recent advancements in Chain-of-Thought prompting have facilitatedsignificant breakthroughs for Large Language Models (LLMs) in complex reasoningtasks. Current research enhances the reasoning performance of LLMs by samplingmultiple reasoning chains and ensembling based on the answer frequency.However, this approach fails in scenarios where the correct answers are in theminority. We identify this as a primary factor constraining the reasoningcapabilities of LLMs, a limitation that cannot be resolved solely based on thepredicted answers. To address this shortcoming, we introduce a hierarchicalreasoning aggregation framework AoR (Aggregation of Reasoning), which selectsanswers based on the evaluation of reasoning chains. Additionally, AoRincorporates dynamic sampling, adjusting the number of reasoning chains inaccordance with the complexity of the task. Experimental results on a series ofcomplex reasoning tasks show that AoR outperforms prominent ensemble methods.Further analysis reveals that AoR not only adapts various LLMs but alsoachieves a superior performance ceiling when compared to current methods.</description><author>Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Tianxiang Sun, Cheng Chang, Qinyuan Cheng, Ding Wang, Xiaofeng Mou, Xipeng Qiu, XuanJing Huang</author><pubDate>Tue, 21 May 2024 18:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12939v1</guid></item><item><title>Verifying message-passing neural networks via topology-based bounds tightening</title><link>http://arxiv.org/abs/2402.13937v2</link><description>Since graph neural networks (GNNs) are often vulnerable to attack, we need toknow when we can trust them. We develop a computationally effective approachtowards providing robust certificates for message-passing neural networks(MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because ourwork builds on mixed-integer optimization, it encodes a wide variety ofsubproblems, for example it admits (i) both adding and removing edges, (ii)both global and local budgets, and (iii) both topological perturbations andfeature modifications. Our key technology, topology-based bounds tightening,uses graph structure to tighten bounds. We also experiment with aggressivebounds tightening to dynamically change the optimization constraints bytightening variable bounds. To demonstrate the effectiveness of thesestrategies, we implement an extension to the open-source branch-and-cut solverSCIP. We test on both node and graph classification problems and considertopological attacks that both add and remove edges.</description><author>Christopher Hojny, Shiqiang Zhang, Juan S. Campos, Ruth Misener</author><pubDate>Tue, 21 May 2024 18:10:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13937v2</guid></item><item><title>Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy</title><link>http://arxiv.org/abs/2403.01218v3</link><description>The high cost of model training makes it increasingly desirable to developtechniques for unlearning. These techniques seek to remove the influence of atraining example without having to retrain the model from scratch. Intuitively,once a model has unlearned, an adversary that interacts with the model shouldno longer be able to tell whether the unlearned example was included in themodel's training set or not. In the privacy literature, this is known asmembership inference. In this work, we discuss adaptations of MembershipInference Attacks (MIAs) to the setting of unlearning (leading to their "U-MIA"counterparts). We propose a categorization of existing U-MIAs into "populationU-MIAs", where the same attacker is instantiated for all examples, and"per-example U-MIAs", where a dedicated attacker is instantiated for eachexample. We show that the latter category, wherein the attacker tailors itsmembership prediction to each example under attack, is significantly stronger.Indeed, our results show that the commonly used U-MIAs in the unlearningliterature overestimate the privacy protection afforded by existing unlearningtechniques on both vision and language models. Our investigation reveals alarge variance in the vulnerability of different examples to per-exampleU-MIAs. In fact, several unlearning algorithms lead to a reduced vulnerabilityfor some, but not all, examples that we wish to unlearn, at the expense ofincreasing it for other examples. Notably, we find that the privacy protectionfor the remaining training examples may worsen as a consequence of unlearning.We also discuss the fundamental difficulty of equally protecting all examplesusing existing unlearning schemes, due to the different rates at which examplesare unlearned. We demonstrate that naive attempts at tailoring unlearningstopping criteria to different examples fail to alleviate these issues.</description><author>Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, Nicolas Papernot</author><pubDate>Tue, 21 May 2024 18:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01218v3</guid></item><item><title>Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs</title><link>http://arxiv.org/abs/2405.12933v1</link><description>Large Language Models (LLMs) have shown remarkable capabilities in tasks suchas summarization, arithmetic reasoning, and question answering. However, theyencounter significant challenges in the domain of moral reasoning and ethicaldecision-making, especially in complex scenarios with multiple stakeholders.This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancingmoral reasoning in LLMs by exploring decisions' consequences from multiplestakeholder perspectives. Central to SKIG's mechanism is simulatingaccountability for actions, which, alongside empathy exercises and riskassessment, is pivotal to its effectiveness. We validate SKIG's performanceacross various moral reasoning benchmarks with proprietary and opensource LLMs,and investigate its crucial components through extensive ablation analyses.</description><author>Bilgehan Sel, Priya Shanmugasundaram, Mohammad Kachuee, Kun Zhou, Ruoxi Jia, Ming Jin</author><pubDate>Tue, 21 May 2024 18:04:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12933v1</guid></item><item><title>AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks</title><link>http://arxiv.org/abs/2403.13101v2</link><description>The increasing complexity of deep neural networks poses significant barriersto democratizing them to resource-limited edge devices. To address thischallenge, split federated learning (SFL) has emerged as a promising solutionby of floading the primary training workload to a server via model partitioningwhile enabling parallel training among edge devices. However, although systemoptimization substantially influences the performance of SFL underresource-constrained systems, the problem remains largely uncharted. In thispaper, we provide a convergence analysis of SFL which quantifies the impact ofmodel splitting (MS) and client-side model aggregation (MA) on the learningperformance, serving as a theoretical foundation. Then, we propose AdaptSFL, anovel resource-adaptive SFL framework, to expedite SFL underresource-constrained edge computing systems. Specifically, AdaptSFL adaptivelycontrols client-side MA and MS to balance communication-computing latency andtraining convergence. Extensive simulations across various datasets validatethat our proposed AdaptSFL framework takes considerably less time to achieve atarget accuracy than benchmarks, demonstrating the effectiveness of theproposed strategies.</description><author>Zheng Lin, Guanqiao Qu, Wei Wei, Xianhao Chen, Kin K. Leung</author><pubDate>Tue, 21 May 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13101v2</guid></item><item><title>Pytorch-Wildlife: A Collaborative Deep Learning Framework for Conservation</title><link>http://arxiv.org/abs/2405.12930v1</link><description>The alarming decline in global biodiversity, driven by various factors,underscores the urgent need for large-scale wildlife monitoring. In response,scientists have turned to automated deep learning methods for data processingin wildlife monitoring. However, applying these advanced methods in real-worldscenarios is challenging due to their complexity and the need for specializedknowledge, primarily because of technical challenges and interdisciplinarybarriers. To address these challenges, we introduce Pytorch-Wildlife, an open-sourcedeep learning platform built on PyTorch. It is designed for creating,modifying, and sharing powerful AI models. This platform emphasizes usabilityand accessibility, making it accessible to individuals with limited or notechnical background. It also offers a modular codebase to simplify featureexpansion and further development. Pytorch-Wildlife offers an intuitive,user-friendly interface, accessible through local installation or Hugging Face,for animal detection and classification in images and videos. As two real-worldapplications, Pytorch-Wildlife has been utilized to train animal classificationmodels for species recognition in the Amazon Rainforest and for invasiveopossum recognition in the Galapagos Islands. The Opossum model achieves 98%accuracy, and the Amazon model has 92% recognition accuracy for 36 animals in90% of the data. As Pytorch-Wildlife evolves, we aim to integrate moreconservation tasks, addressing various environmental challenges.Pytorch-Wildlife is available at https://github.com/microsoft/CameraTraps.</description><author>Andres Hernandez, Zhongqi Miao, Luisa Vargas, Rahul Dodhia, Juan Lavista</author><pubDate>Tue, 21 May 2024 17:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12930v1</guid></item><item><title>Code-mixed Sentiment and Hate-speech Prediction</title><link>http://arxiv.org/abs/2405.12929v1</link><description>Code-mixed discourse combines multiple languages in a single text. It iscommonly used in informal discourse in countries with several officiallanguages, but also in many other countries in combination with English orneighboring languages. As recently large language models have dominated mostnatural language processing tasks, we investigated their performance incode-mixed settings for relevant tasks. We first created four new bilingualpre-trained masked language models for English-Hindi and English-Slovenelanguages, specifically aimed to support informal language. Then we performedan evaluation of monolingual, bilingual, few-lingual, and massivelymultilingual models on several languages, using two tasks that frequentlycontain code-mixed text, in particular, sentiment analysis and offensivelanguage detection in social media texts. The results show that the mostsuccessful classifiers are fine-tuned bilingual models and multilingual models,specialized for social media texts, followed by non-specialized massivelymultilingual and monolingual models, while huge generative models are notcompetitive. For our affective problems, the models mostly perform slightlybetter on code-mixed data compared to non-code-mixed data.</description><author>Anjali Yadav, Tanya Garg, Matej Klemen, Matej Ulcar, Basant Agarwal, Marko Robnik Sikonja</author><pubDate>Tue, 21 May 2024 17:56:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12929v1</guid></item><item><title>Learning Explainable and Better Performing Representations of POMDP Strategies</title><link>http://arxiv.org/abs/2401.07656v3</link><description>Strategies for partially observable Markov decision processes (POMDP)typically require memory. One way to represent this memory is via automata. Wepresent a method to learn an automaton representation of a strategy using amodification of the L*-algorithm. Compared to the tabular representation of astrategy, the resulting automaton is dramatically smaller and thus also moreexplainable. Moreover, in the learning process, our heuristics may even improvethe strategy's performance. In contrast to approaches that synthesize anautomaton directly from the POMDP thereby solving it, our approach isincomparably more scalable.</description><author>Alexander Bork, Debraj Chakraborty, Kush Grover, Jan Kretinsky, Stefanie Mohr</author><pubDate>Tue, 21 May 2024 17:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07656v3</guid></item><item><title>On Image Registration and Subpixel Estimation</title><link>http://arxiv.org/abs/2405.12927v1</link><description>Image registration is a classical problem in machine vision which seeksmethods to align discrete images of the same scene to subpixel accuracy ingeneral situations. As with all estimation problems, the underlying difficultyis the partial information available about the ground truth. We consider abasic and idealized one-dimensional image registration problem motivated byquestions about measurement and about quantization, and we demonstrate that theextent to which subinterval/subpixel inferences can be made in this settingdepends on a type of complexity associated with the function of interest, therelationship between the function and the pixel size, and the number ofdistinct sampling count observations available.</description><author>Serap A. Savari</author><pubDate>Tue, 21 May 2024 17:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12927v1</guid></item><item><title>Trusting Fair Data: Leveraging Quality in Fairness-Driven Data Removal Techniques</title><link>http://arxiv.org/abs/2405.12926v1</link><description>In this paper, we deal with bias mitigation techniques that remove specificdata points from the training set to aim for a fair representation of thepopulation in that set. Machine learning models are trained on thesepre-processed datasets, and their predictions are expected to be fair. However,such approaches may exclude relevant data, making the attained subsets lesstrustworthy for further usage. To enhance the trustworthiness of prior methods,we propose additional requirements and objectives that the subsets must fulfillin addition to fairness: (1) group coverage, and (2) minimal data loss. Whileremoving entire groups may improve the measured fairness, this practice is veryproblematic as failing to represent every group cannot be considered fair. Inour second concern, we advocate for the retention of data while minimizingdiscrimination. By introducing a multi-objective optimization problem thatconsiders fairness and data loss, we propose a methodology to findPareto-optimal solutions that balance these objectives. By identifying suchsolutions, users can make informed decisions about the trade-off betweenfairness and data quality and select the most suitable subset for theirapplication.</description><author>Manh Khoi Duong, Stefan Conrad</author><pubDate>Tue, 21 May 2024 17:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12926v1</guid></item><item><title>Panmodal Information Interaction</title><link>http://arxiv.org/abs/2405.12923v1</link><description>The emergence of generative artificial intelligence (GenAI) is transforminginformation interaction. For decades, search engines such as Google and Binghave been the primary means of locating relevant information for the generalpopulation. They have provided search results in the same standard format (theso-called "10 blue links"). The recent ability to chat via natural languagewith AI-based agents and have GenAI automatically synthesize answers inreal-time (grounded in top-ranked results) is changing how people interact withand consume information at massive scale. These two information interactionmodalities (traditional search and AI-powered chat) coexist in current searchengines, either loosely coupled (e.g., as separate options/tabs) or tightlycoupled (e.g., integrated as a chat answer embedded directly within atraditional search result page). We believe that the existence of these twodifferent modalities, and potentially many others, is creating an opportunityto re-imagine the search experience, capitalize on the strengths of manymodalities, and develop systems and strategies to support seamless flow betweenthem. We refer to these as panmodal experiences. Unlike monomodal experiences,where only one modality is available and/or used for the task at hand, panmodalexperiences make multiple modalities available to users (multimodal), directlysupport transitions between modalities (crossmodal), and seamlessly combinemodalities to tailor task assistance (transmodal). While our focus is searchand chat, with learnings from insights from a survey of over 100 individualswho have recently performed common tasks on these two modalities, we alsopresent a more general vision for the future of information interaction usingmultiple modalities and the emergent capabilities of GenAI.</description><author>Chirag Shah, Ryen W. White</author><pubDate>Tue, 21 May 2024 17:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12923v1</guid></item><item><title>Lift-Attend-Splat: Bird's-eye-view camera-lidar fusion using transformers</title><link>http://arxiv.org/abs/2312.14919v3</link><description>Combining complementary sensor modalities is crucial to providing robustperception for safety-critical robotics applications such as autonomous driving(AD). Recent state-of-the-art camera-lidar fusion methods for AD rely onmonocular depth estimation which is a notoriously difficult task compared tousing depth information from the lidar directly. Here, we find that thisapproach does not leverage depth as expected and show that naively improvingdepth estimation does not lead to improvements in object detection performance.Strikingly, we also find that removing depth estimation altogether does notdegrade object detection performance substantially, suggesting that relying onmonocular depth could be an unnecessary architectural bottleneck duringcamera-lidar fusion. In this work, we introduce a novel fusion method thatbypasses monocular depth estimation altogether and instead selects and fusescamera and lidar features in a bird's-eye-view grid using a simple attentionmechanism. We show that our model can modulate its use of camera features basedon the availability of lidar features and that it yields better 3D objectdetection on the nuScenes dataset than baselines relying on monocular depthestimation.</description><author>James Gunn, Zygmunt Lenyk, Anuj Sharma, Andrea Donati, Alexandru Buburuzan, John Redford, Romain Mueller</author><pubDate>Tue, 21 May 2024 17:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14919v3</guid></item><item><title>E2E-MFD: Towards End-to-End Synchronous Multimodal Fusion Detection</title><link>http://arxiv.org/abs/2403.09323v2</link><description>Multimodal image fusion and object detection are crucial for autonomousdriving. While current methods have advanced the fusion of texture details andsemantic information, their complex training processes hinder broaderapplications. Addressing this challenge, we introduce E2E-MFD, a novelend-to-end algorithm for multimodal fusion detection. E2E-MFD streamlines theprocess, achieving high performance with a single training phase. It employssynchronous joint optimization across components to avoid suboptimal solutionstied to individual tasks. Furthermore, it implements a comprehensiveoptimization strategy in the gradient matrix for shared parameters, ensuringconvergence to an optimal fusion detection configuration. Our extensive testingon multiple public datasets reveals E2E-MFD's superior capabilities, showcasingnot only visually appealing image fusion but also impressive detectionoutcomes, such as a 3.9% and 2.0% mAP50 increase on horizontal object detectiondataset M3FD and oriented object detection dataset DroneVehicle, respectively,compared to state-of-the-art approaches.</description><author>Jiaqing Zhang, Mingxiang Cao, Xue Yang, Weiying Xie, Jie Lei, Daixun Li, Wenbo Huang, Yunsong Li</author><pubDate>Tue, 21 May 2024 17:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09323v2</guid></item><item><title>Ship in Sight: Diffusion Models for Ship-Image Super Resolution</title><link>http://arxiv.org/abs/2403.18370v2</link><description>In recent years, remarkable advancements have been achieved in the field ofimage generation, primarily driven by the escalating demand for high-qualityoutcomes across various image generation subtasks, such as inpainting,denoising, and super resolution. A major effort is devoted to exploring theapplication of super-resolution techniques to enhance the quality oflow-resolution images. In this context, our method explores in depth theproblem of ship image super resolution, which is crucial for coastal and portsurveillance. We investigate the opportunity given by the growing interest intext-to-image diffusion models, taking advantage of the prior knowledge thatsuch foundation models have already learned. In particular, we present adiffusion-model-based architecture that leverages text conditioning duringtraining while being class-aware, to best preserve the crucial details of theships during the generation of the super-resoluted image. Since the specificityof this task and the scarcity availability of off-the-shelf data, we alsointroduce a large labeled ship dataset scraped from online ship images, mostlyfrom ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our methodachieves more robust results than other deep learning models previouslyemployed for super resolution, as proven by the multiple experiments performed.Moreover, we investigate how this model can benefit downstream tasks, such asclassification and object detection, thus emphasizing practical implementationin a real-world scenario. Experimental results show flexibility, reliability,and impressive performance of the proposed framework over state-of-the-artmethods for different tasks. The code is available at:https://github.com/LuigiSigillo/ShipinSight .</description><author>Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello</author><pubDate>Tue, 21 May 2024 17:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18370v2</guid></item><item><title>G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation</title><link>http://arxiv.org/abs/2405.12915v1</link><description>Large Language Models (LLMs) have demonstrated remarkable abilities ingeneral scenarios. Instruction finetuning empowers them to align with humans invarious tasks. Nevertheless, the Diversity and Quality of the instruction dataremain two main challenges for instruction finetuning. With regard to this, inthis paper, we propose a novel gradient-based method to automatically selecthigh-quality and diverse instruction finetuning data for machine translation.Our key innovation centers around analyzing how individual training examplesinfluence the model during training. Specifically, we select training examplesthat exert beneficial influences on the model as high-quality ones by means ofInfluence Function plus a small high-quality seed dataset. Moreover, to enhancethe diversity of the training data we maximize the variety of influences theyhave on the model by clustering on their gradients and resampling. Extensiveexperiments on WMT22 and FLORES translation tasks demonstrate the superiorityof our methods, and in-depth analysis further validates their effectiveness andgeneralization.</description><author>Xingyuan Pan, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Shanbo Cheng</author><pubDate>Tue, 21 May 2024 17:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12915v1</guid></item><item><title>Probabilistic Forecasting of Irregular Time Series via Conditional Flows</title><link>http://arxiv.org/abs/2402.06293v2</link><description>Probabilistic forecasting of irregularly sampled multivariate time serieswith missing values is an important problem in many fields, including healthcare, astronomy, and climate. State-of-the-art methods for the task estimateonly marginal distributions of observations in single channels and at singletimepoints, assuming a fixed-shape parametric distribution. In this work, wepropose a novel model, ProFITi, for probabilistic forecasting of irregularlysampled time series with missing values using conditional normalizing flows.The model learns joint distributions over the future values of the time seriesconditioned on past observations and queried channels and times, withoutassuming any fixed shape of the underlying distribution. As model components,we introduce a novel invertible triangular attention layer and an invertiblenon-linear activation function on and onto the whole real line. We conductextensive experiments on four datasets and demonstrate that the proposed modelprovides $4$ times higher likelihood over the previously best model.</description><author>Vijaya Krishna Yalavarthi, Randolf Scholz, Stefan Born, Lars Schmidt-Thieme</author><pubDate>Tue, 21 May 2024 17:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06293v2</guid></item><item><title>Enriching Disentanglement: From Logical Definitions to Quantitative Metrics</title><link>http://arxiv.org/abs/2305.11512v2</link><description>Disentangling the explanatory factors in complex data is a promising approachfor generalizable and data-efficient representation learning. While a varietyof quantitative metrics for learning and evaluating disentangledrepresentations have been proposed, it remains unclear what properties thesemetrics truly quantify. In this work, we establish a theoretical connectionbetween logical definitions of disentanglement and quantitative metrics usingtopos theory and enriched category theory. We introduce a systematic approachfor converting a first-order predicate into a real-valued quantity by replacing(i) equality with a strict premetric, (ii) the Heyting algebra of binary truthvalues with a quantale of continuous values, and (iii) quantifiers withaggregators. The metrics induced by logical definitions have strong theoreticalguarantees, and some of them are easily differentiable and can be used aslearning objectives directly. Finally, we empirically demonstrate theeffectiveness of the proposed metrics by isolating different aspects ofdisentangled representations.</description><author>Yivan Zhang, Masashi Sugiyama</author><pubDate>Tue, 21 May 2024 17:37:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11512v2</guid></item><item><title>Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes</title><link>http://arxiv.org/abs/2402.05279v2</link><description>Learning-based approaches are emerging as an effective approach for safetyfilters for black-box dynamical systems. Existing methods have relied oncertificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi(HJ) reachability value functions. The primary motivation for our work is therecognition that ultimately, enforcing the safety constraint as a control inputconstraint at each state is what matters. By focusing on this constraint, wecan eliminate dependence on any specific certificate function-based design. Toachieve this, we define a discriminating hyperplane that shapes the half-spaceconstraint on control input at each state, serving as a sufficient conditionfor safety. This concept not only generalizes over traditional safety methodsbut also simplifies safety filter design by eliminating dependence on specificcertificate functions. We present two strategies to learn the discriminatinghyperplane: (a) a supervised learning approach, using pre-verified controlinvariant sets for labeling, and (b) a reinforcement learning (RL) approach,which does not require such labels. The main advantage of our method, unlikeconventional safe RL approaches, is the separation of performance and safety.This offers a reusable safety filter for learning new tasks, avoiding the needto retrain from scratch. As such, we believe that the new notion of thediscriminating hyperplane offers a more generalizable direction towardsdesigning safety filters, encompassing and extending existingcertificate-function-based or safe RL methodologies.</description><author>Will Lavanakul, Jason J. Choi, Koushil Sreenath, Claire J. Tomlin</author><pubDate>Tue, 21 May 2024 17:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05279v2</guid></item><item><title>An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation</title><link>http://arxiv.org/abs/2405.12914v1</link><description>One critical prerequisite for faithful text-to-image generation is theaccurate understanding of text inputs. Existing methods leverage the textencoder of the CLIP model to represent input prompts. However, the pre-trainedCLIP model can merely encode English with a maximum token length of 77.Moreover, the model capacity of the text encoder from CLIP is relativelylimited compared to Large Language Models (LLMs), which offer multilingualinput, accommodate longer context, and achieve superior text representation. Inthis paper, we investigate LLMs as the text encoder to improve the languageunderstanding in text-to-image generation. Unfortunately, trainingtext-to-image generative model with LLMs from scratch demands significantcomputational resources and data. To this end, we introduce a three-stagetraining pipeline that effectively and efficiently integrates the existingtext-to-image model with LLMs. Specifically, we propose a lightweight adapterthat enables fast training of the text-to-image model using the textualrepresentations from LLMs. Extensive experiments demonstrate that our modelsupports not only multilingual but also longer input context with superiorimage generation quality.</description><author>Zhiyu Tan, Mengping Yang, Luozheng Qin, Hao Yang, Ye Qian, Qiang Zhou, Cheng Zhang, Hao Li</author><pubDate>Tue, 21 May 2024 17:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12914v1</guid></item><item><title>Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment</title><link>http://arxiv.org/abs/2405.12910v1</link><description>This paper addresses a critical gap in legal analytics by developing andapplying a novel taxonomy for topic modelling summary judgment cases in theUnited Kingdom. Using a curated dataset of summary judgment cases, we use theLarge Language Model Claude 3 Opus to explore functional topics and trends. Wefind that Claude 3 Opus correctly classified the topic with an accuracy of87.10%. The analysis reveals distinct patterns in the application of summaryjudgments across various legal domains. As case law in the United Kingdom isnot originally labelled with keywords or a topic filtering option, the findingsnot only refine our understanding of the thematic underpinnings of summaryjudgments but also illustrate the potential of combining traditional andAI-driven approaches in legal classification. Therefore, this paper provides anew and general taxonomy for UK law. The implications of this work serve as afoundation for further research and policy discussions in the field of judicialadministration and computational legal research methodologies.</description><author>Holli Sargeant, Ahmed Izzidien, Felix Steffek</author><pubDate>Tue, 21 May 2024 17:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12910v1</guid></item><item><title>Confidence-Aware Multi-Field Model Calibration</title><link>http://arxiv.org/abs/2402.17655v2</link><description>Accurately predicting the probabilities of user feedback, such as clicks andconversions, is critical for advertisement ranking and bidding. However, thereoften exist unwanted mismatches between predicted probabilities and truelikelihoods due to the rapid shift of data distributions and intrinsic modelbiases. Calibration aims to address this issue by post-processing modelpredictions, and field-aware calibration can adjust model output on differentfeature field values to satisfy fine-grained advertising demands.Unfortunately, the observed samples corresponding to certain field values canbe seriously limited to make confident calibrations, which may yield biasamplification and online disturbance. In this paper, we propose aconfidence-aware multi-field calibration method, which adaptively adjusts thecalibration intensity based on confidence levels derived from samplestatistics. It also utilizes multiple fields for joint model calibrationaccording to their importance to mitigate the impact of data sparsity on asingle field. Extensive offline and online experiments show the superiority ofour method in boosting advertising performance and reducing predictiondeviations.</description><author>Yuang Zhao, Chuhan Wu, Qinglin Jia, Hong Zhu, Jia Yan, Libin Zong, Linxuan Zhang, Zhenhua Dong, Muyu Zhang</author><pubDate>Tue, 21 May 2024 17:22:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17655v2</guid></item><item><title>Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents</title><link>http://arxiv.org/abs/2405.12900v1</link><description>Recent advancements in open-domain dialogue systems have been propelled bythe emergence of high-quality large language models (LLMs) and variouseffective training methodologies. Nevertheless, the presence of toxicity withinthese models presents a significant challenge that can potentially diminish theuser experience. In this study, we introduce an innovative training algorithm,an improvement upon direct preference optimization (DPO), called adversarialDPO (ADPO). The ADPO algorithm is designed to train models to assign higherprobability distributions to preferred responses and lower distributions tounsafe responses, which are self-generated using the toxic control token. Wedemonstrate that ADPO enhances the model's resilience against harmfulconversations while minimizing performance degradation. Furthermore, weillustrate that ADPO offers a more stable training procedure compared to thetraditional DPO. To the best of our knowledge, this is the first adaptation ofthe DPO algorithm that directly incorporates harmful data into the generativemodel, thereby reducing the need to artificially create safe dialogue data.</description><author>San Kim, Gary Geunbae Lee</author><pubDate>Tue, 21 May 2024 17:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12900v1</guid></item><item><title>Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints</title><link>http://arxiv.org/abs/2402.07692v2</link><description>Bayesian optimization has been successfully applied to optimize black-boxfunctions where the number of evaluations is severely limited. However, in manyreal-world applications, it is hard or impossible to know in advance whichdesigns are feasible due to some physical or system limitations. These issueslead to an even more challenging problem of optimizing an unknown function withunknown constraints. In this paper, we observe that in such scenarios optimalsolution typically lies on the boundary between feasible and infeasible regionsof the design space, making it considerably more difficult than that withinterior optima. Inspired by this observation, we propose BE-CBO, a newBayesian optimization method that efficiently explores the boundary betweenfeasible and infeasible designs. To identify the boundary, we learn theconstraints with an ensemble of neural networks that outperform the standardGaussian Processes for capturing complex boundaries. Our method demonstratessuperior performance against state-of-the-art methods through comprehensiveexperiments on synthetic and real-world benchmarks. Code available at:https://github.com/yunshengtian/BE-CBO</description><author>Yunsheng Tian, Ane Zuniga, Xinwei Zhang, Johannes P. Dürholt, Payel Das, Jie Chen, Wojciech Matusik, Mina Konaković Luković</author><pubDate>Tue, 21 May 2024 17:08:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07692v2</guid></item><item><title>Decentralized Federated Learning Over Imperfect Communication Channels</title><link>http://arxiv.org/abs/2405.12894v1</link><description>This paper analyzes the impact of imperfect communication channels ondecentralized federated learning (D-FL) and subsequently determines the optimalnumber of local aggregations per training round, adapting to the networktopology and imperfect channels. We start by deriving the bias of locallyaggregated D-FL models under imperfect channels from the ideal global modelsrequiring perfect channels and aggregations. The bias reveals that excessivelocal aggregations can accumulate communication errors and degrade convergence.Another important aspect is that we analyze a convergence upper bound of D-FLbased on the bias. By minimizing the bound, the optimal number of localaggregations is identified to balance a trade-off with accumulation ofcommunication errors in the absence of knowledge of the channels. With thisknowledge, the impact of communication errors can be alleviated, allowing theconvergence upper bound to decrease throughout aggregations. Experimentsvalidate our convergence analysis and also identify the optimal number of localaggregations on two widely considered image classification tasks. It is seenthat D-FL, with an optimal number of local aggregations, can outperform itspotential alternatives by over 10% in training accuracy.</description><author>Weicai Li, Tiejun Lv, Wei Ni, Jingbo Zhao, Ekram Hossain, H. Vincent Poor</author><pubDate>Tue, 21 May 2024 17:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12894v1</guid></item><item><title>Implicit-ARAP: Efficient Handle-Guided Deformation of High-Resolution Meshes and Neural Fields via Local Patch Meshing</title><link>http://arxiv.org/abs/2405.12895v1</link><description>In this work, we present the local patch mesh representation for neuralsigned distance fields. This technique allows to discretize local regions ofthe level sets of an input SDF by projecting and deforming flat patch meshesonto the level set surface, using exclusively the SDF information and itsgradient. Our analysis reveals this method to be more accurate than thestandard marching cubes algorithm for approximating the implicit surface. Then,we apply this representation in the setting of handle-guided deformation: weintroduce two distinct pipelines, which make use of 3D neural fields to computeAs-Rigid-As-Possible deformations of both high-resolution meshes and neuralfields under a given set of constraints. We run a comprehensive evaluation ofour method and various baselines for neural field and mesh deformation whichshow both pipelines achieve impressive efficiency and notable improvements interms of quality of results and robustness. With our novel pipeline, weintroduce a scalable approach to solve a well-established geometry processingproblem on high-resolution meshes, and pave the way for extending othergeometric tasks to the domain of implicit surfaces via local patch meshing.</description><author>Daniele Baieri, Filippo Maggioli, Zorah Lähner, Simone Melzi, Emanuele Rodolà</author><pubDate>Tue, 21 May 2024 17:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12895v1</guid></item><item><title>Retrievable Domain-Sensitive Feature Memory for Multi-Domain Recommendation</title><link>http://arxiv.org/abs/2405.12892v1</link><description>With the increase in the business scale and number of domains in onlineadvertising, multi-domain ad recommendation has become a mainstream solution inthe industry. The core of multi-domain recommendation is effectively modelingthe commonalities and distinctions among domains. Existing works are dedicatedto designing model architectures for implicit multi-domain modeling whileoverlooking an in-depth investigation from a more fundamental perspective offeature distributions. This paper focuses on features with significantdifferences across various domains in both distributions and effects on modelpredictions. We refer to these features as domain-sensitive features, whichserve as carriers of domain distinctions and are crucial for multi-domainmodeling. Experiments demonstrate that existing multi-domain modeling methodsmay neglect domain-sensitive features, indicating insufficient learning ofdomain distinctions. To avoid this neglect, we propose a domain-sensitivefeature attribution method to identify features that best reflect domaindistinctions from the feature set. Further, we design a memory architecturethat extracts domain-specific information from domain-sensitive features forthe model to retrieve and integrate, thereby enhancing the awareness of domaindistinctions. Extensive offline and online experiments demonstrate thesuperiority of our method in capturing domain distinctions and improvingmulti-domain recommendation performance.</description><author>Yuang Zhao, Zhaocheng Du, Qinglin Jia, Linxuan Zhang, Zhenhua Dong, Ruiming Tang</author><pubDate>Tue, 21 May 2024 17:02:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12892v1</guid></item><item><title>DARK: Denoising, Amplification, Restoration Kit</title><link>http://arxiv.org/abs/2405.12891v1</link><description>This paper introduces a novel lightweight computational framework forenhancing images under low-light conditions, utilizing advanced machinelearning and convolutional neural networks (CNNs). Traditional enhancementtechniques often fail to adequately address issues like noise, colordistortion, and detail loss in challenging lighting environments. Our approachleverages insights from the Retinex theory and recent advances in imagerestoration networks to develop a streamlined model that efficiently processesillumination components and integrates context-sensitive enhancements throughoptimized convolutional blocks. This results in significantly improved imageclarity and color fidelity, while avoiding over-enhancement and unnatural colorshifts. Crucially, our model is designed to be lightweight, ensuring lowcomputational demand and suitability for real-time applications on standardconsumer hardware. Performance evaluations confirm that our model not onlysurpasses existing methods in enhancing low-light images but also maintains aminimal computational footprint.</description><author>Zhuoheng Li, Yuheng Pan, Houcheng Yu, Zhiheng Zhang</author><pubDate>Tue, 21 May 2024 17:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12891v1</guid></item><item><title>Keep the Momentum: Conservation Laws beyond Euclidean Gradient Flows</title><link>http://arxiv.org/abs/2405.12888v1</link><description>Conservation laws are well-established in the context of Euclidean gradientflow dynamics, notably for linear or ReLU neural network training. Yet, theirexistence and principles for non-Euclidean geometries and momentum-baseddynamics remain largely unknown. In this paper, we characterize "all"conservation laws in this general setting. In stark contrast to the case ofgradient flows, we prove that the conservation laws for momentum-based dynamicsexhibit temporal dependence. Additionally, we often observe a "conservationloss" when transitioning from gradient flow to momentum dynamics. Specifically,for linear networks, our framework allows us to identify all momentumconservation laws, which are less numerous than in the gradient flow caseexcept in sufficiently over-parameterized regimes. With ReLU networks, noconservation law remains. This phenomenon also manifests in non-Euclideanmetrics, used e.g. for Nonnegative Matrix Factorization (NMF): all conservationlaws can be determined in the gradient flow context, yet none persists in themomentum case.</description><author>Sibylle Marcotte, Rémi Gribonval, Gabriel Peyré</author><pubDate>Tue, 21 May 2024 16:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12888v1</guid></item><item><title>Few-sample Variational Inference of Bayesian Neural Networks with Arbitrary Nonlinearities</title><link>http://arxiv.org/abs/2405.02063v2</link><description>Bayesian Neural Networks (BNNs) extend traditional neural networks to provideuncertainties associated with their outputs. On the forward pass through a BNN,predictions (and their uncertainties) are made either by Monte Carlo samplingnetwork weights from the learned posterior or by analytically propagatingstatistical moments through the network. Though flexible, Monte Carlo samplingis computationally expensive and can be infeasible or impractical underresource constraints or for large networks. While moment propagation canameliorate the computational costs of BNN inference, it can be difficult orimpossible for networks with arbitrary nonlinearities, thereby restricting thepossible set of network layers permitted with such a scheme. In this work, wedemonstrate a simple yet effective approach for propagating statistical momentsthrough arbitrary nonlinearities with only 3 deterministic samples, enablingfew-sample variational inference of BNNs without restricting the set of networklayers used. Furthermore, we leverage this approach to demonstrate a novelnonlinear activation function that we use to inject physics-informed priorinformation into output nodes of a BNN.</description><author>David J. Schodt</author><pubDate>Tue, 21 May 2024 16:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02063v2</guid></item><item><title>Investigating Persuasion Techniques in Arabic: An Empirical Study Leveraging Large Language Models</title><link>http://arxiv.org/abs/2405.12884v1</link><description>In the current era of digital communication and widespread use of socialmedia, it is crucial to develop an understanding of persuasive techniquesemployed in written text. This knowledge is essential for effectivelydiscerning accurate information and making informed decisions. To address thisneed, this paper presents a comprehensive empirical study focused onidentifying persuasive techniques in Arabic social media content. To achievethis objective, we utilize Pre-trained Language Models (PLMs) and leverage theArAlEval dataset, which encompasses two tasks: binary classification todetermine the presence or absence of persuasion techniques, and multi-labelclassification to identify the specific types of techniques employed in thetext. Our study explores three different learning approaches by harnessing thepower of PLMs: feature extraction, fine-tuning, and prompt engineeringtechniques. Through extensive experimentation, we find that the fine-tuningapproach yields the highest results on the aforementioned dataset, achieving anf1-micro score of 0.865 and an f1-weighted score of 0.861. Furthermore, ouranalysis sheds light on an interesting finding. While the performance of theGPT model is relatively lower compared to the other approaches, we haveobserved that by employing few-shot learning techniques, we can enhance itsresults by up to 20\%. This offers promising directions for future research andexploration in this topic\footnote{Upon Acceptance, the source code will bereleased on GitHub.}.</description><author>Abdurahmman Alzahrani, Eyad Babkier, Faisal Yanbaawi, Firas Yanbaawi, Hassan Alhuzali</author><pubDate>Tue, 21 May 2024 16:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12884v1</guid></item><item><title>Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization</title><link>http://arxiv.org/abs/2310.09833v3</link><description>In multi-agent reinforcement learning (MARL), ensuring robustness againstunpredictable or worst-case actions by allies is crucial for real-worlddeployment. Existing robust MARL methods either approximate or enumerate allpossible threat scenarios against worst-case adversaries, leading tocomputational intensity and reduced robustness. In contrast, human learningefficiently acquires robust behaviors in daily life without preparing for everypossible threat. Inspired by this, we frame robust MARL as an inferenceproblem, with worst-case robustness implicitly optimized under all threatscenarios via off-policy evaluation. Within this framework, we demonstrate thatMutual Information Regularization as Robust Regularization (MIR3) duringroutine training is guaranteed to maximize a lower bound on robustness, withoutthe need for adversaries. Further insights show that MIR3 acts as aninformation bottleneck, preventing agents from over-reacting to others andaligning policies with robust action priors. In the presence of worst-caseadversaries, our MIR3 significantly surpasses baseline methods in robustnessand training efficiency while maintaining cooperative performance in StarCraftII and robot swarm control. When deploying the robot swarm control algorithm inthe real world, our method also outperforms the best baseline by 14.29%.</description><author>Simin Li, Ruixiao Xu, Jingqiao Xiu, Yuwei Zheng, Pu Feng, Yaodong Yang, Xianglong Liu</author><pubDate>Tue, 21 May 2024 16:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09833v3</guid></item><item><title>Explaining Expert Search and Team Formation Systems with ExES</title><link>http://arxiv.org/abs/2405.12881v1</link><description>Expert search and team formation systems operate on collaboration networks,with nodes representing individuals, labeled with their skills, and edgesdenoting collaboration relationships. Given a keyword query corresponding tothe desired skills, these systems identify experts that best match the query.However, state-of-the-art solutions to this problem lack transparency. Toaddress this issue, we propose ExES, a tool designed to explain expert searchand team formation systems using factual and counterfactual methods from thefield of explainable artificial intelligence (XAI). ExES uses factualexplanations to highlight important skills and collaborations, andcounterfactual explanations to suggest new skills and collaborations toincrease the likelihood of being identified as an expert. Towards a practicaldeployment as an interactive explanation tool, we present and experimentallyevaluate a suite of pruning strategies to speed up the explanation search. Inmany cases, our pruning strategies make ExES an order of magnitude faster thanexhaustive search, while still producing concise and actionable explanations.</description><author>Kiarash Golzadeh, Lukasz Golab, Jaroslaw Szlichta</author><pubDate>Tue, 21 May 2024 16:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12881v1</guid></item><item><title>Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in Remote Sensing Images</title><link>http://arxiv.org/abs/2405.12875v1</link><description>Remote sensing image change captioning (RSICC) aims at generating human-likelanguage to describe the semantic changes between bi-temporal remote sensingimage pairs. It provides valuable insights into environmental dynamics and landmanagement. Unlike conventional change captioning task, RSICC involves not onlyretrieving relevant information across different modalities and generatingfluent captions, but also mitigating the impact of pixel-level differences onterrain change localization. The pixel problem due to long time span decreasesthe accuracy of generated caption. Inspired by the remarkable generative powerof diffusion model, we propose a probabilistic diffusion model for RSICC tosolve the aforementioned problems. In training process, we construct a noisepredictor conditioned on cross modal features to learn the distribution fromthe real caption distribution to the standard Gaussian distribution under theMarkov chain. Meanwhile, a cross-mode fusion and a stacking self-attentionmodule are designed for noise predictor in the reverse process. In testingphase, the well-trained noise predictor helps to estimate the mean value of thedistribution and generate change captions step by step. Extensive experimentson the LEVIR-CC dataset demonstrate the effectiveness of our Diffusion-RSCC andits individual components. The quantitative results showcase superiorperformance over existing methods across both traditional and newly augmentedmetrics. The code and materials will be available online athttps://github.com/Fay-Y/Diffusion-RSCC.</description><author>Xiaofei Yu, Yitong Li, Jie Ma</author><pubDate>Tue, 21 May 2024 16:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12875v1</guid></item><item><title>Spatial-aware Attention Generative Adversarial Network for Semi-supervised Anomaly Detection in Medical Image</title><link>http://arxiv.org/abs/2405.12872v1</link><description>Medical anomaly detection is a critical research area aimed at recognizingabnormal images to aid in diagnosis.Most existing methods adopt syntheticanomalies and image restoration on normal samples to detect anomaly. Theunlabeled data consisting of both normal and abnormal data is not wellexplored. We introduce a novel Spatial-aware Attention Generative AdversarialNetwork (SAGAN) for one-class semi-supervised generation of health images.Ourcore insight is the utilization of position encoding and attention toaccurately focus on restoring abnormal regions and preserving normal regions.To fully utilize the unlabelled data, SAGAN relaxes the cyclic consistencyrequirement of the existing unpaired image-to-image conversion methods, andgenerates high-quality health images corresponding to unlabeled data, guided bythe reconstruction of normal images and restoration of pseudo-anomalyimages.Subsequently, the discrepancy between the generated healthy image andthe original image is utilized as an anomaly score.Extensive experiments onthree medical datasets demonstrate that the proposed SAGAN outperforms thestate-of-the-art methods.</description><author>Zerui Zhang, Zhichao Sun, Zelong Liu, Bo Du, Rui Yu, Zhou Zhao, Yongchao Xu</author><pubDate>Tue, 21 May 2024 16:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12872v1</guid></item><item><title>Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics</title><link>http://arxiv.org/abs/2405.12868v1</link><description>Learning to represent and simulate the dynamics of physical systems is acrucial yet challenging task. Existing equivariant Graph Neural Network (GNN)based methods have encapsulated the symmetry of physics, \emph{e.g.},translations, rotations, etc, leading to better generalization ability.Nevertheless, their frame-to-frame formulation of the task overlooks thenon-Markov property mainly incurred by unobserved dynamics in the environment.In this paper, we reformulate dynamics simulation as a spatio-temporalprediction task, by employing the trajectory in the past period to recover theNon-Markovian interactions. We propose Equivariant Spatio-Temporal AttentiveGraph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, tofulfill our purpose. At its core, we design a novel Equivariant DiscreteFourier Transform (EDFT) to extract periodic patterns from the history frames,and then construct an Equivariant Spatial Module (ESM) to accomplish spatialmessage passing, and an Equivariant Temporal Module (ETM) with the forwardattention and equivariant pooling mechanisms to aggregate temporal message. Weevaluate our model on three real datasets corresponding to the molecular-,protein- and macro-level. Experimental results verify the effectiveness ofESTAG compared to typical spatio-temporal GNNs and equivariant GNNs.</description><author>Liming Wu, Zhichao Hou, Jirui Yuan, Yu Rong, Wenbing Huang</author><pubDate>Tue, 21 May 2024 16:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12868v1</guid></item><item><title>Transparency Distortion Robustness for SOTA Image Segmentation Tasks</title><link>http://arxiv.org/abs/2405.12864v1</link><description>Semantic Image Segmentation facilitates a multitude of real-worldapplications ranging from autonomous driving over industrial processsupervision to vision aids for human beings. These models are usually trainedin a supervised fashion using example inputs. Distribution Shifts between theseexamples and the inputs in operation may cause erroneous segmentations. Therobustness of semantic segmentation models against distribution shifts causedby differing camera or lighting setups, lens distortions, adversarial inputsand image corruptions has been topic of recent research. However, robustnessagainst spatially varying radial distortion effects that can be caused byuneven glass structures (e.g. windows) or the chaotic refraction in heated airhas not been addressed by the research community yet. We propose a method tosynthetically augment existing datasets with spatially varying distortions. Ourexperiments show, that these distortion effects degrade the performance ofstate-of-the-art segmentation models. Pretraining and enlarged model capacitiesproof to be suitable strategies for mitigating performance degradation to somedegree, while fine-tuning on distorted images only leads to marginalperformance improvements.</description><author>Volker Knauthe, Arne Rak, Tristan Wirth, Thomas Pöllabauer, Simon Metzler, Arjan Kuijper, Dieter W. Fellner</author><pubDate>Tue, 21 May 2024 16:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12864v1</guid></item><item><title>Safe and Responsible Large Language Model Development</title><link>http://arxiv.org/abs/2404.01399v2</link><description>In light of the increasing concerns regarding the safety and risks associatedwith Large Language Models (LLMs), the imperative to design effectivemitigation strategies has never been more pressing. This paper introduces aSafety and Responsible Large Language Model (\textbf{SR}$_{\text{LLM}}$ ), anapproach designed to enhance the safety of LLM-generated content. Initially, wepropose a safety risk taxonomy to categorize the safety risks found in LLMresponses. Subsequently, we effectively collect high-quality instructions forLLM alignment, including the use of experts annotations and review thatresonate with this taxonomy. We present \textbf{SR}$_{\text{LLM}}$, an LLM thatis specifically designed to detect potential unsafe content and generate benignalternatives. We leverage parameter-efficient fine-tuning mechanisms to makethe model more usable and adaptable. The methods for evaluating theeffectiveness of this model, along with state-of-the-art methods, present amultifaceted approach towards their assessment. Through rigorous testing acrossfive benchmark datasets and two proprietary datasets, we observed a markeddecrease in the generation of unsafe content. We present the details of ourapproach, the fine-tuning methodologies, and safety evaluation to thecommunity. A GitHub link with associated data and code is publicly available at\url{ https://github.com/shainarazavi/Safe-Responsible-LLM}</description><author>Shaina Raza, Oluwanifemi Bamgbose, Shardul Ghuge, Deepak John Reji</author><pubDate>Tue, 21 May 2024 16:28:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01399v2</guid></item><item><title>Toward Constraint Compliant Goal Formulation and Planning</title><link>http://arxiv.org/abs/2405.12862v1</link><description>One part of complying with norms, rules, and preferences is incorporatingconstraints (such as knowledge of ethics) into one's goal formulation andplanning processing. We explore in a simple domain how the encoding ofknowledge in different ethical frameworks influences an agent's goalformulation and planning processing and demonstrate ability of an agent tosatisfy and satisfice when its collection of relevant constraints includes amix of "hard" and "soft" constraints of various types. How the agent attemptsto comply with ethical constraints depends on the ethical framing and weinvestigate tradeoffs between deontological framing and utilitarian framing forcomplying with an ethical norm. Representative scenarios highlight howperforming the same task with different framings of the same norm leads todifferent behaviors. Our explorations suggest an important role formetacognitive judgments in resolving ethical conflicts during goal formulationand planning.</description><author>Steven J. Jones Robert E. Wray</author><pubDate>Tue, 21 May 2024 16:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12862v1</guid></item><item><title>Influence of Water Droplet Contamination for Transparency Segmentation</title><link>http://arxiv.org/abs/2405.12861v1</link><description>Computer vision techniques are on the rise for industrial applications, likeprocess supervision and autonomous agents, e.g., in the healthcare domain anddangerous environments. While the general usability of these techniques ishigh, there are still challenging real-world use-cases. Especially transparentstructures, which can appear in the form of glass doors, protective casings oreveryday objects like glasses, pose a challenge for computer vision methods.This paper evaluates the combination of transparent objects in conjunction with(naturally occurring) contamination through environmental effects like hazing.We introduce a novel publicly available dataset containing 489 imagesincorporating three grades of water droplet contamination on transparentstructures and examine the resulting influence on transparency handling. Ourfindings show, that contaminated transparent objects are easier to segment andthat we are able to distinguish between different severity levels ofcontamination with a current state-of-the art machine-learning model. This inturn opens up the possibility to enhance computer vision systems regardingresilience against, e.g., datashifts through contaminated protection casings orimplement an automated cleaning alert.</description><author>Volker Knauthe, Paul Weitz, Thomas Pöllabauer, Tristan Wirth, Arne Rak, Arjan Kuijper, Dieter W. Fellner</author><pubDate>Tue, 21 May 2024 16:24:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12861v1</guid></item><item><title>CLIP in Medical Imaging: A Comprehensive Survey</title><link>http://arxiv.org/abs/2312.07353v4</link><description>Contrastive Language-Image Pre-training (CLIP), a simple yet effectivepre-training paradigm, successfully introduces text supervision to visionmodels. It has shown promising results across various tasks, attributable toits generalizability and interpretability. The use of CLIP has recently gainedincreasing interest in the medical imaging domain, serving both as apre-training paradigm for aligning medical vision and language, and as acritical component in diverse clinical tasks. With the aim of facilitating adeeper understanding of this promising direction, this survey offers anin-depth exploration of the CLIP paradigm within the domain of medical imaging,regarding both refined CLIP pre-training and CLIP-driven applications. In thisstudy, We (1) start with a brief introduction to the fundamentals of CLIPmethodology. (2) Then, we investigate the adaptation of CLIP pre-training inthe medical domain, focusing on how to optimize CLIP given characteristics ofmedical images and reports. (3) Furthermore, we explore the practicalutilization of CLIP pre-trained models in various tasks, includingclassification, dense prediction, and cross-modal tasks. (4) Finally, wediscuss existing limitations of CLIP in the context of medical imaging andpropose forward-looking directions to address the demands of medical imagingdomain. We expect that this comprehensive survey will provide researchers inthe field of medical image analysis with a holistic understanding of the CLIPparadigm and its potential implications. The project page can be found onhttps://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.</description><author>Zihao Zhao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, Dinggang Shen</author><pubDate>Tue, 21 May 2024 16:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07353v4</guid></item><item><title>Multiple-Choice Questions are Efficient and Robust LLM Evaluators</title><link>http://arxiv.org/abs/2405.11966v2</link><description>We present GSM-MC and MATH-MC, two multiple-choice (MC) datasets constructedby collecting answers and incorrect predictions on GSM8K and MATH from over 50open-source models. Through extensive experiments, we show that LLMs'performance on the MC versions of these two popular benchmarks is stronglycorrelated with their performance on the original versions, and is quite robustto distractor choices and option orders, while the evaluation time is reducedby a factor of up to 30. Following a similar procedure, we also introducePythonIO, a new program output prediction MC dataset constructed from two otherpopular LLM evaluation benchmarks HumanEval and MBPP. Our data and code areavailable at https://github.com/Geralt-Targaryen/MC-Evaluation.</description><author>Ziyin Zhang, Lizhen Xu, Zhaokun Jiang, Hongkun Hao, Rui Wang</author><pubDate>Tue, 21 May 2024 16:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11966v2</guid></item><item><title>LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language</title><link>http://arxiv.org/abs/2405.12856v1</link><description>Machine learning practitioners often face significant challenges in formallyintegrating their prior knowledge and beliefs into predictive models, limitingthe potential for nuanced and context-aware analyses. Moreover, the expertiseneeded to integrate this prior knowledge into probabilistic modeling typicallylimits the application of these models to specialists. Our goal is to build aregression model that can process numerical data and make probabilisticpredictions at arbitrary locations, guided by natural language text whichdescribes a user's prior knowledge. Large Language Models (LLMs) provide auseful starting point for designing such a tool since they 1) provide aninterface where users can incorporate expert insights in natural language and2) provide an opportunity for leveraging latent problem-relevant knowledgeencoded in LLMs that users may not have themselves. We start by exploringstrategies for eliciting explicit, coherent numerical predictive distributionsfrom LLMs. We examine these joint predictive distributions, which we call LLMProcesses, over arbitrarily-many quantities in settings such as forecasting,multi-dimensional regression, black-box optimization, and image modeling. Weinvestigate the practical details of prompting to elicit coherent predictivedistributions, and demonstrate their effectiveness at regression. Finally, wedemonstrate the ability to usefully incorporate text into numericalpredictions, improving predictive performance and giving quantitative structurethat reflects qualitative descriptions. This lets us begin to explore the rich,grounded hypothesis space that LLMs implicitly encode.</description><author>James Requeima, John Bronskill, Dami Choi, Richard E. Turner, David Duvenaud</author><pubDate>Tue, 21 May 2024 16:13:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12856v1</guid></item><item><title>Inconsistency-Aware Cross-Attention for Audio-Visual Fusion in Dimensional Emotion Recognition</title><link>http://arxiv.org/abs/2405.12853v1</link><description>Leveraging complementary relationships across modalities has recently drawn alot of attention in multimodal emotion recognition. Most of the existingapproaches explored cross-attention to capture the complementary relationshipsacross the modalities. However, the modalities may also exhibit weakcomplementary relationships, which may deteriorate the cross-attended features,resulting in poor multimodal feature representations. To address this problem,we propose Inconsistency-Aware Cross-Attention (IACA), which can adaptivelyselect the most relevant features on-the-fly based on the strong or weakcomplementary relationships across audio and visual modalities. Specifically,we design a two-stage gating mechanism that can adaptively select theappropriate relevant features to deal with weak complementary relationships.Extensive experiments are conducted on the challenging Aff-Wild2 dataset toshow the robustness of the proposed model.</description><author>R Gnana Praveen, Jahangir Alam</author><pubDate>Tue, 21 May 2024 16:11:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12853v1</guid></item><item><title>Gaussian Head &amp; Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping</title><link>http://arxiv.org/abs/2405.12069v2</link><description>By equipping the most recent 3D Gaussian Splatting representation with head3D morphable models (3DMM), existing methods manage to create head avatars withhigh fidelity. However, most existing methods only reconstruct a head withoutthe body, substantially limiting their application scenarios. We found thatnaively applying Gaussians to model the clothed chest and shoulders tends toresult in blurry reconstruction and noisy floaters under novel poses. This isbecause of the fundamental limitation of Gaussians and point clouds -- eachGaussian or point can only have a single directional radiance without spatialvariance, therefore an unnecessarily large number of them is required torepresent complicated spatially varying texture, even for simple geometry. Incontrast, we propose to model the body part with a neural texture that consistsof coarse and pose-dependent fine colors. To properly render the body texturefor each view and pose without accurate geometry nor UV mapping, we optimizeanother sparse set of Gaussians as anchors that constrain the neural warpingfield that maps image plane coordinates to the texture space. We demonstratethat Gaussian Head &amp; Shoulders can fit the high-frequency details on theclothed upper body with high fidelity and potentially improve the accuracy andfidelity of the head region. We evaluate our method with casual phone-capturedand internet videos and show our method archives superior reconstructionquality and robustness in both self and cross reenactment tasks. To fullyutilize the efficient rendering speed of Gaussian splatting, we additionallypropose an accelerated inference method of our trained model withoutMulti-Layer Perceptron (MLP) queries and reach a stable rendering speed ofaround 130 FPS for any subjects.</description><author>Tianhao Wu, Jing Yang, Zhilin Guo, Jingyi Wan, Fangcheng Zhong, Cengiz Oztireli</author><pubDate>Tue, 21 May 2024 16:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12069v2</guid></item><item><title>Weakly supervised alignment and registration of MR-CT for cervical cancer radiotherapy</title><link>http://arxiv.org/abs/2405.12850v1</link><description>Cervical cancer is one of the leading causes of death in women, andbrachytherapy is currently the primary treatment method. However, it isimportant to precisely define the extent of paracervical tissue invasion toimprove cancer diagnosis and treatment options. The fusion of the informationcharacteristics of both computed tomography (CT) and magnetic resonanceimaging(MRI) modalities may be useful in achieving a precise outline of theextent of paracervical tissue invasion. Registration is the initial step ininformation fusion. However, when aligning multimodal images with varyingdepths, manual alignment is prone to large errors and is time-consuming.Furthermore, the variations in the size of the Region of Interest (ROI) and theshape of multimodal images pose a significant challenge for achieving accurateregistration.In this paper, we propose a preliminary spatial alignmentalgorithm and a weakly supervised multimodal registration network. The spatialposition alignment algorithm efficiently utilizes the limited annotationinformation in the two modal images provided by the doctor to automaticallyalign multimodal images with varying depths. By utilizing aligned multimodalimages for weakly supervised registration and incorporating pyramidal featuresand cost volume to estimate the optical flow, the results indicate that theproposed method outperforms traditional volume rendering alignment methods andregistration networks in various evaluation metrics. This demonstrates theeffectiveness of our model in multimodal image registration.</description><author>Jjahao Zhang, Yin Gu, Deyu Sun, Yuhua Gao, Ming Gao, Ming Cui, Teng Zhang, He Ma</author><pubDate>Tue, 21 May 2024 16:05:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12850v1</guid></item><item><title>Learn Your Reference Model for Real Good Alignment</title><link>http://arxiv.org/abs/2404.09656v2</link><description>The complexity of the alignment problem stems from the fact that existingmethods are considered unstable. Reinforcement Learning from Human Feedback(RLHF) addresses this issue by minimizing the KL divergence between the trainedpolicy and the initial supervised fine-tuned policy (SFT) to avoid generatingout-of-domain samples for the reward model (RM). Recently, many methods haveemerged that shift from online to offline optimization, reformulating the RLHFobjective and removing the reward model (DPO, IPO, KTO). Despite eliminatingthe reward model and the challenges it posed, these algorithms are stillconstrained in terms of closeness of the trained policy to the SFT one. In ourpaper, we argue that this implicit limitation in the offline optimizationmethods leads to suboptimal results. To address this issue, we propose a classof new methods called Trust Region (TR-DPO, TR-IPO, TR-KTO), which update thereference policy during training. With this straightforward update approach, wedemonstrate the effectiveness of the new paradigm of language model alignmentagainst the classical one on the Anthropic-HH and Reddit TL;DR datasets. Mostnotably, when automatically comparing TR methods and baselines side by sideusing pretrained Pythia 6.9B models on the Reddit TL;DR task, the difference inwin rates reaches 8.4% for DPO, 14.3% for IPO, and 15% for KTO. Finally, byassessing model response ratings grounded on criteria such as coherence,correctness, helpfulness, and harmlessness, we demonstrate that our proposedmethods significantly outperform existing techniques.</description><author>Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, Daniil Gavrilov</author><pubDate>Tue, 21 May 2024 16:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09656v2</guid></item><item><title>Training and inference in the ReckON RSNN architecture implemented on a MPSoC</title><link>http://arxiv.org/abs/2405.12849v1</link><description>With the rise of artificial intelligence, biological neuron models are beingused to implement neural networks that can learn certain tasks after a trainingphase. One type of such networks are spiking neural networks (SNNs) that relyon a simplified model for biological neurons, the Integrate and Fire neuron.Several accelerators have emerged to implement SNNs with this kind of neuron.The ReckON system is one of these that allows both the training and executionof a recurrent SNN. The ReckON architecture, implemented on a custom ASIC, canbe fully described using a hardware description language. In this work, weadapt the Verilog description to implement it on a Xilinx Multiprocessor Systemon Chip system (MPSoC). We present the circuits required for the efficientoperation of the system, and a Python framework to use it on the Pynq ZUplatform. We validate the architecture and implementation in two differentscenarios, and show how the simulated accuracy is preserved with a peakperformance of 3.8M events processed per second.</description><author>Alejandro Linares-Barranco, Luciano Prono, Robert Lengenstein, Giacomo Indiveri, Charlotte Frenkel</author><pubDate>Tue, 21 May 2024 15:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12849v1</guid></item><item><title>A Dataset and Baselines for Measuring and Predicting the Music Piece Memorability</title><link>http://arxiv.org/abs/2405.12847v1</link><description>Nowadays, humans are constantly exposed to music, whether through voluntarystreaming services or incidental encounters during commercial breaks. Despitethe abundance of music, certain pieces remain more memorable and often gaingreater popularity. Inspired by this phenomenon, we focus on measuring andpredicting music memorability. To achieve this, we collect a new music piecedataset with reliable memorability labels using a novel interactiveexperimental procedure. We then train baselines to predict and analyze musicmemorability, leveraging both interpretable features and audio mel-spectrogramsas inputs. To the best of our knowledge, we are the first to explore musicmemorability using data-driven deep learning-based methods. Through a series ofexperiments and ablation studies, we demonstrate that while there is room forimprovement, predicting music memorability with limited data is possible.Certain intrinsic elements, such as higher valence, arousal, and faster tempo,contribute to memorable music. As prediction techniques continue to evolve,real-life applications like music recommendation systems and music styletransfer will undoubtedly benefit from this new area of research.</description><author>Li-Yang Tseng, Tzu-Ling Lin, Hong-Han Shuai, Jen-Wei Huang, Wen-Whei Chang</author><pubDate>Tue, 21 May 2024 15:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12847v1</guid></item><item><title>Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks</title><link>http://arxiv.org/abs/2312.03365v4</link><description>To reduce global carbon emissions and limit climate change, controllingenergy consumption in buildings is an important piece of the puzzle. Here, wespecifically focus on using a demand response (DR) algorithm to limit theenergy consumption of a residential building's heating system while respectinguser's thermal comfort. In that domain, Reinforcement learning (RL) methodshave been shown to be quite effective. One such RL method is Monte Carlo TreeSearch (MCTS), which has achieved impressive success in playing board games(go, chess). A particular advantage of MCTS is that its decision tree structurenaturally allows to integrate exogenous constraints (e.g., by trimming branchesthat violate them), while conventional RL solutions need more elaboratetechniques (e.g., indirectly by adding penalties in the cost/reward function,or through a backup controller that corrects constraint-violating actions). Themain aim of this paper is to study the adoption of MCTS for building control,since this (to the best of our knowledge) has remained largely unexplored. Aspecific property of MCTS is that it needs a simulator component that canpredict subsequent system states, based on actions taken. A straightforwarddata-driven solution is to use black-box neural networks (NNs). We will howeverextend a Physics-informed Neural Network (PiNN) model to deliver multi-timesteppredictions, and show the benefit it offers in terms of lower prediction errors($-$32\% MAE) as well as better MCTS performance ($-$4\% energy cost, $+$7\%thermal comfort) compared to a black-box NN. A second contribution will be toextend a vanilla MCTS version to adopt the ideas applied in AlphaZero (i.e.,using learned prior and value functions and an action selection heuristic) toobtain lower computational costs while maintaining control performance.</description><author>Fabio Pavirani, Gargya Gokhale, Bert Claessens, Chris Develder</author><pubDate>Tue, 21 May 2024 15:56:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03365v4</guid></item><item><title>OpenCarbonEval: A Unified Carbon Emission Estimation Framework in Large-Scale AI Models</title><link>http://arxiv.org/abs/2405.12843v1</link><description>In recent years, large-scale auto-regressive models have made significantprogress in various tasks, such as text or video generation. However, theenvironmental impact of these models has been largely overlooked, with a lackof assessment and analysis of their carbon footprint. To address this gap, weintroduce OpenCarbonEval, a unified framework for integrating large-scalemodels across diverse modalities to predict carbon emissions, which couldprovide AI service providers and users with a means to estimate emissionsbeforehand and help mitigate the environmental pressure associated with thesemodels. In OpenCarbonEval, we propose a dynamic throughput modeling approachthat could capture workload and hardware fluctuations in the training processfor more precise emissions estimates. Our evaluation results demonstrate thatOpenCarbonEval can more accurately predict training emissions than previousmethods, and can be seamlessly applied to different modal tasks. Specifically,we show that OpenCarbonEval achieves superior performance in predicting carbonemissions for both visual models and language models. By promoting sustainableAI development and deployment, OpenCarbonEval can help reduce the environmentalimpact of large-scale models and contribute to a more environmentallyresponsible future for the AI community.</description><author>Zhaojian Yu, Yinghao Wu, Zhuotao Deng, Yansong Tang, Xiao-Ping Zhang</author><pubDate>Tue, 21 May 2024 15:50:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12843v1</guid></item><item><title>SmartFlow: Robotic Process Automation using LLMs</title><link>http://arxiv.org/abs/2405.12842v1</link><description>Robotic Process Automation (RPA) systems face challenges in handling complexprocesses and diverse screen layouts that require advanced human-likedecision-making capabilities. These systems typically rely on pixel-levelencoding through drag-and-drop or automation frameworks such as Selenium tocreate navigation workflows, rather than visual understanding of screenelements. In this context, we present SmartFlow, an AI-based RPA system thatuses pre-trained large language models (LLMs) coupled with deep-learning basedimage understanding. Our system can adapt to new scenarios, including changesin the user interface and variations in input data, without the need for humanintervention. SmartFlow uses computer vision and natural language processing toperceive visible elements on the graphical user interface (GUI) and convertthem into a textual representation. This information is then utilized by LLMsto generate a sequence of actions that are executed by a scripting engine tocomplete an assigned task. To assess the effectiveness of SmartFlow, we havedeveloped a dataset that includes a set of generic enterprise applications withdiverse layouts, which we are releasing for research use. Our evaluations onthis dataset demonstrate that SmartFlow exhibits robustness across differentlayouts and applications. SmartFlow can automate a wide range of businessprocesses such as form filling, customer service, invoice processing, andback-office operations. SmartFlow can thus assist organizations in enhancingproductivity by automating an even larger fraction of screen-based workflows.The demo-video and dataset are available athttps://smartflow-4c5a0a.webflow.io/.</description><author>Arushi Jain, Shubham Paliwal, Monika Sharma, Lovekesh Vig, Gautam Shroff</author><pubDate>Tue, 21 May 2024 15:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12842v1</guid></item><item><title>GotFunding: A grant recommendation system based on scientific articles</title><link>http://arxiv.org/abs/2405.12840v1</link><description>Obtaining funding is an important part of becoming a successful scientist.Junior faculty spend a great deal of time finding the right agencies andprograms that best match their research profile. But what are the factors thatinfluence the best publication--grant matching? Some universities might employpre-award personnel to understand these factors, but not all institutions canafford to hire them. Historical records of publications funded by grants canhelp us understand the matching process and also help us develop recommendationsystems to automate it. In this work, we present \textsc{GotFunding} (GrantrecOmmendaTion based on past FUNDING), a recommendation system trained onNational Institutes of Health's (NIH) grant--publication records. Our systemachieves a high performance (NDCG@1 = 0.945) by casting the problem as learningto rank. By analyzing the features that make predictions effective, our resultsshow that the ranking considers most important 1) the year difference betweenpublication and grant grant, 2) the amount of information provided in thepublication, and 3) the relevance of the publication to the grant. We discussfuture improvements of the system and an online tool for scientists to try.</description><author>Tong Zeng, Daniel E. Acuna</author><pubDate>Tue, 21 May 2024 15:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12840v1</guid></item><item><title>What makes an image realistic?</title><link>http://arxiv.org/abs/2403.04493v4</link><description>The last decade has seen tremendous progress in our ability to generaterealistic-looking data, be it images, text, audio, or video. Here, we discussthe closely related problem of quantifying realism, that is, designingfunctions that can reliably tell realistic data from unrealistic data. Thisproblem turns out to be significantly harder to solve and remains poorlyunderstood, despite its prevalence in machine learning and recent breakthroughsin generative AI. Drawing on insights from algorithmic information theory, wediscuss why this problem is challenging, why a good generative model alone isinsufficient to solve it, and what a good solution would look like. Inparticular, we introduce the notion of a universal critic, which unlikeadversarial critics does not require adversarial training. While universalcritics are not immediately practical, they can serve both as a North Star forguiding practical implementations and as a tool for analyzing existing attemptsto capture realism.</description><author>Lucas Theis</author><pubDate>Tue, 21 May 2024 15:44:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04493v4</guid></item><item><title>CoVR: Learning Composed Video Retrieval from Web Video Captions</title><link>http://arxiv.org/abs/2308.14746v2</link><description>Composed Image Retrieval (CoIR) has recently gained popularity as a task thatconsiders both text and image queries together, to search for relevant imagesin a database. Most CoIR approaches require manually annotated datasets,comprising image-text-image triplets, where the text describes a modificationfrom the query image to the target image. However, manual curation of CoIRtriplets is expensive and prevents scalability. In this work, we insteadpropose a scalable automatic dataset creation methodology that generatestriplets given video-caption pairs, while also expanding the scope of the taskto include composed video retrieval (CoVR). To this end, we mine paired videoswith a similar caption from a large database, and leverage a large languagemodel to generate the corresponding modification text. Applying thismethodology to the extensive WebVid2M collection, we automatically constructour WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, weintroduce a new benchmark for CoVR with a manually annotated evaluation set,along with baseline results. Our experiments further demonstrate that traininga CoVR model on our dataset effectively transfers to CoIR, leading to improvedstate-of-the-art performance in the zero-shot setup on both the CIRR andFashionIQ benchmarks. Our code, datasets, and models are publicly available athttps://imagine.enpc.fr/~ventural/covr.</description><author>Lucas Ventura, Antoine Yang, Cordelia Schmid, Gül Varol</author><pubDate>Tue, 21 May 2024 15:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14746v2</guid></item><item><title>A Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data</title><link>http://arxiv.org/abs/2405.12833v1</link><description>Automatic radiology report generation can alleviate the workload forphysicians and minimize regional disparities in medical resources, thereforebecoming an important topic in the medical image analysis field. It is achallenging task, as the computational model needs to mimic physicians toobtain information from multi-modal input data (i.e., medical images, clinicalinformation, medical knowledge, etc.), and produce comprehensive and accuratereports. Recently, numerous works emerged to address this issue using deeplearning-based methods, such as transformers, contrastive learning, andknowledge-base construction. This survey summarizes the key techniquesdeveloped in the most recent works and proposes a general workflow for deeplearning-based report generation with five main components, includingmulti-modality data acquisition, data preparation, feature learning, featurefusion/interaction, and report generation. The state-of-the-art methods foreach of these components are highlighted. Additionally, training strategies,public datasets, evaluation methods, current challenges, and future directionsin this field are summarized. We have also conducted a quantitative comparisonbetween different methods under the same experimental setting. This is the mostup-to-date survey that focuses on multi-modality inputs and data fusion forradiology report generation. The aim is to provide comprehensive and richinformation for researchers interested in automatic clinical report generationand medical image analysis, especially when using multimodal inputs, and assistthem in developing new algorithms to advance the field.</description><author>Xinyi Wang, Grazziela Figueredo, Ruizhe Li, Wei Emma Zhang, Weitong Chen, Xin Chen</author><pubDate>Tue, 21 May 2024 15:37:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12833v1</guid></item><item><title>Wav-KAN: Wavelet Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2405.12832v1</link><description>In this paper , we introduce Wav-KAN, an innovative neural networkarchitecture that leverages the Wavelet Kolmogorov-Arnold Networks (Wav-KAN)framework to enhance interpretability and performance. Traditional multilayerperceptrons (MLPs) and even recent advancements like Spl-KAN face challengesrelated to interpretability, training speed, robustness, computationalefficiency, and performance. Wav-KAN addresses these limitations byincorporating wavelet functions into the Kolmogorov-Arnold network structure,enabling the network to capture both high-frequency and low-frequencycomponents of the input data efficiently. Wavelet-based approximations employorthogonal or semi-orthogonal basis and also maintains a balance betweenaccurately representing the underlying data structure and avoiding overfittingto the noise. Analogous to how water conforms to the shape of its container,Wav-KAN adapts to the data structure, resulting in enhanced accuracy, fastertraining speeds, and increased robustness compared to Spl-KAN and MLPs. Ourresults highlight the potential of Wav-KAN as a powerful tool for developinginterpretable and high-performance neural networks, with applications spanningvarious fields. This work sets the stage for further exploration andimplementation of Wav-KAN in frameworks such as PyTorch, TensorFlow, and alsoit makes wavelet in KAN in wide-spread usage like nowadays activation functionslike ReLU, sigmoid in universal approximation theory (UAT).</description><author>Zavareh Bozorgasl, Hao Chen</author><pubDate>Tue, 21 May 2024 15:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12832v1</guid></item><item><title>Graph Neural Networks over the Air for Decentralized Tasks in Wireless Networks</title><link>http://arxiv.org/abs/2302.08447v3</link><description>Graph neural networks (GNNs) model representations from networked data andallow for decentralized inference through localized communications. ExistingGNN architectures often assume ideal communications and ignore potentialchannel effects, such as fading and noise, leading to performance degradationin real-world implementation. Considering a GNN implemented over nodesconnected through wireless links, this paper conducts a stability analysis tostudy the impact of channel impairments on the performance of GNNs, andproposes graph neural networks over the air (AirGNNs), a novel GNN architecturethat incorporates the communication model. AirGNNs modify graph convolutionaloperations that shift graph signals over random communication graphs to takeinto account channel fading and noise when aggregating features from neighbors,thus, improving architecture robustness to channel impairments during testing.We develop a channel-inversion signal transmission strategy for AirGNNs whenchannel state information (CSI) is available, and propose a stochastic gradientdescent based method to train AirGNNs when CSI is unknown. The convergenceanalysis shows that the training procedure approaches a stationary solution ofan associated stochastic optimization problem and the variance analysischaracterizes the statistical behavior of the trained model. Experiments ondecentralized source localization and multi-robot flocking corroboratetheoretical findings and show superior performance of AirGNNs over wirelesscommunication channels.</description><author>Zhan Gao, Deniz Gunduz</author><pubDate>Tue, 21 May 2024 15:35:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08447v3</guid></item><item><title>Frequency-Adaptive Dilated Convolution for Semantic Segmentation</title><link>http://arxiv.org/abs/2403.05369v6</link><description>Dilated convolution, which expands the receptive field by inserting gapsbetween its consecutive elements, is widely employed in computer vision. Inthis study, we propose three strategies to improve individual phases of dilatedconvolution from the view of spectrum analysis. Departing from the conventionalpractice of fixing a global dilation rate as a hyperparameter, we introduceFrequency-Adaptive Dilated Convolution (FADC), which dynamically adjustsdilation rates spatially based on local frequency components. Subsequently, wedesign two plug-in modules to directly enhance effective bandwidth andreceptive field size. The Adaptive Kernel (AdaKern) module decomposesconvolution weights into low-frequency and high-frequency components,dynamically adjusting the ratio between these components on a per-channelbasis. By increasing the high-frequency part of convolution weights, AdaKerncaptures more high-frequency components, thereby improving effective bandwidth.The Frequency Selection (FreqSelect) module optimally balances high- andlow-frequency components in feature representations through spatially variantreweighting. It suppresses high frequencies in the background to encourage FADCto learn a larger dilation, thereby increasing the receptive field for anexpanded scope. Extensive experiments on segmentation and object detectionconsistently validate the efficacy of our approach. The code is publiclyavailable at https://github.com/Linwei-Chen/FADC.</description><author>Linwei Chen, Lin Gu, Ying Fu</author><pubDate>Tue, 21 May 2024 15:29:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05369v6</guid></item><item><title>Talk2Radar: Bridging Natural Language with 4D mmWave Radar for 3D Referring Expression Comprehension</title><link>http://arxiv.org/abs/2405.12821v1</link><description>Embodied perception is essential for intelligent vehicles and robots,enabling more natural interaction and task execution. However, theseadvancements currently embrace vision level, rarely focusing on using 3Dmodeling sensors, which limits the full understanding of surrounding objectswith multi-granular characteristics. Recently, as a promising automotive sensorwith affordable cost, 4D Millimeter-Wave radar provides denser point cloudsthan conventional radar and perceives both semantic and physicalcharacteristics of objects, thus enhancing the reliability of perceptionsystem. To foster the development of natural language-driven contextunderstanding in radar scenes for 3D grounding, we construct the first dataset,Talk2Radar, which bridges these two modalities for 3D Referring ExpressionComprehension. Talk2Radar contains 8,682 referring prompt samples with 20,558referred objects. Moreover, we propose a novel model, T-RadarNet for 3D RECupon point clouds, achieving state-of-the-art performances on Talk2Radardataset compared with counterparts, where Deformable-FPN and Gated Graph Fusionare meticulously designed for efficient point cloud feature modeling andcross-modal fusion between radar and text features, respectively. Further,comprehensive experiments are conducted to give a deep insight into radar-based3D REC. We release our project at https://github.com/GuanRunwei/Talk2Radar.</description><author>Runwei Guan, Ruixiao Zhang, Ningwei Ouyang, Jianan Liu, Ka Lok Man, Xiaohao Cai, Ming Xu, Jeremy Smith, Eng Gee Lim, Yutao Yue, Hui Xiong</author><pubDate>Tue, 21 May 2024 15:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12821v1</guid></item><item><title>Large Language Models Meet NLP: A Survey</title><link>http://arxiv.org/abs/2405.12819v1</link><description>While large language models (LLMs) like ChatGPT have shown impressivecapabilities in Natural Language Processing (NLP) tasks, a systematicinvestigation of their potential in this field remains largely unexplored. Thisstudy aims to address this gap by exploring the following questions: (1) Howare LLMs currently applied to NLP tasks in the literature? (2) Have traditionalNLP tasks already been solved with LLMs? (3) What is the future of the LLMs forNLP? To answer these questions, we take the first step to provide acomprehensive overview of LLMs in NLP. Specifically, we first introduce aunified taxonomy including (1) parameter-frozen application and (2)parameter-tuning application to offer a unified perspective for understandingthe current progress of LLMs in NLP. Furthermore, we summarize the newfrontiers and the associated challenges, aiming to inspire furthergroundbreaking advancements. We hope this work offers valuable insights intothe {potential and limitations} of LLMs in NLP, while also serving as apractical guide for building effective LLMs in NLP.</description><author>Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, Philip S. Yu</author><pubDate>Tue, 21 May 2024 15:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12819v1</guid></item><item><title>Leveraging Discourse Structure for Extractive Meeting Summarization</title><link>http://arxiv.org/abs/2405.11055v2</link><description>We introduce an extractive summarization system for meetings that leveragesdiscourse structure to better identify salient information from complexmulti-party discussions. Using discourse graphs to represent semantic relationsbetween the contents of utterances in a meeting, we train a GNN-based nodeclassification model to select the most important utterances, which are thencombined to create an extractive summary. Experimental results on AMI and ICSIdemonstrate that our approach surpasses existing text-based and graph-basedextractive summarization systems, as measured by both classification andsummarization metrics. Additionally, we conduct ablation studies on discoursestructure and relation type to provide insights for future NLP applicationsleveraging discourse analysis theory.</description><author>Virgile Rennard, Guokan Shang, Michalis Vazirgiannis, Julie Hunter</author><pubDate>Tue, 21 May 2024 15:23:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11055v2</guid></item><item><title>Efficient Data Collection for Robotic Manipulation via Compositional Generalization</title><link>http://arxiv.org/abs/2403.05110v2</link><description>Data collection has become an increasingly important problem in roboticmanipulation, yet there still lacks much understanding of how to effectivelycollect data to facilitate broad generalization. Recent works on large-scalerobotic data collection typically vary many environmental factors of variation(e.g., object types, table textures) during data collection, to cover a diverserange of scenarios. However, they do not explicitly account for the possiblecompositional abilities of policies trained on the data. If robot policies cancompose environmental factors from their data to succeed when encounteringunseen factor combinations, we can exploit this to avoid collecting data forsituations that composition would address. To investigate this possibility, weconduct thorough empirical studies both in simulation and on a real robot thatcompare data collection strategies and assess whether visual imitation learningpolicies can compose environmental factors. We find that policies do exhibitcomposition, although leveraging prior robotic datasets is critical for this ona real robot. We use these insights to propose better in-domain data collectionstrategies that exploit composition, which can induce better generalizationthan naive approaches for the same amount of effort during data collection. Wefurther demonstrate that a real robot policy trained on data from such astrategy achieves a success rate of 77.5% when transferred to entirely newenvironments that encompass unseen combinations of environmental factors,whereas policies trained using data collected without accounting forenvironmental variation fail to transfer effectively, with a success rate ofonly 2.5%. We provide videos at http://iliad.stanford.edu/robot-data-comp/.</description><author>Jensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, Dorsa Sadigh</author><pubDate>Tue, 21 May 2024 15:18:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05110v2</guid></item><item><title>FedLPA: One-shot Federated Learning with Layer-Wise Posterior Aggregation</title><link>http://arxiv.org/abs/2310.00339v3</link><description>Efficiently aggregating trained neural networks from local clients into aglobal model on a server is a widely researched topic in federated learning.Recently, motivated by diminishing privacy concerns, mitigating potentialattacks, and reducing communication overhead, one-shot federated learning(i.e., limiting client-server communication into a single round) has gainedpopularity among researchers. However, the one-shot aggregation performancesare sensitively affected by the non-identical training data distribution, whichexhibits high statistical heterogeneity in some real-world scenarios. Toaddress this issue, we propose a novel one-shot aggregation method withlayer-wise posterior aggregation, named FedLPA. FedLPA aggregates local modelsto obtain a more accurate global model without requiring extra auxiliarydatasets or exposing any private label information, e.g., label distributions.To effectively capture the statistics maintained in the biased local datasetsin the practical non-IID scenario, we efficiently infer the posteriors of eachlayer in each local model using layer-wise Laplace approximation and aggregatethem to train the global parameters. Extensive experimental results demonstratethat FedLPA significantly improves learning performance over state-of-the-artmethods across several metrics.</description><author>Xiang Liu, Liangxi Liu, Feiyang Ye, Yunheng Shen, Xia Li, Linshan Jiang, Jialin Li</author><pubDate>Tue, 21 May 2024 15:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00339v3</guid></item><item><title>Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark</title><link>http://arxiv.org/abs/2403.13502v3</link><description>Integrating machine learning into Automated Control Systems (ACS) enhancesdecision-making in industrial process management. One of the limitations to thewidespread adoption of these technologies in industry is the vulnerability ofneural networks to adversarial attacks. This study explores the threats indeploying deep learning models for fault diagnosis in ACS using the TennesseeEastman Process dataset. By evaluating three neural networks with differentarchitectures, we subject them to six types of adversarial attacks and explorefive different defense methods. Our results highlight the strong vulnerabilityof models to adversarial samples and the varying effectiveness of defensestrategies. We also propose a novel protection approach by combining multipledefense methods and demonstrate it's efficacy. This research contributesseveral insights into securing machine learning within ACS, ensuring robustfault diagnosis in industrial processes.</description><author>Vitaliy Pozdnyakov, Aleksandr Kovalenko, Ilya Makarov, Mikhail Drobyshevskiy, Kirill Lukyanov</author><pubDate>Tue, 21 May 2024 15:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13502v3</guid></item><item><title>Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation</title><link>http://arxiv.org/abs/2402.08845v2</link><description>We investigate the problem of explainability in machine learning. To addressthis problem, Feature Attribution Methods (FAMs) measure the contribution ofeach feature through a perturbation test, where the difference in prediction iscompared under different perturbations. However, such perturbation tests maynot accurately distinguish the contributions of different features, when theirchange in prediction is the same after perturbation. In order to enhance theability of FAMs to distinguish different features' contributions in thischallenging setting, we propose to utilize the Probability of Necessity andSufficiency (PNS) that perturbing a feature is a necessary and sufficient causefor the prediction to change as a measure of feature importance. Our approach,Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS viaa perturbation test involving two stages (factual and interventional). Inpractice, to generate counterfactual samples, we use a resampling-basedapproach on the observed samples to approximate the required conditionaldistribution. We demonstrate that FANS outperforms existing attribution methodson six benchmarks. Our source code is available at\url{https://github.com/DMIRLAB-Group/FANS}.</description><author>Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</author><pubDate>Tue, 21 May 2024 15:14:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08845v2</guid></item><item><title>VertiBayes: Learning Bayesian network parameters from vertically partitioned data with missing values</title><link>http://arxiv.org/abs/2210.17228v2</link><description>Federated learning makes it possible to train a machine learning model ondecentralized data. Bayesian networks are probabilistic graphical models thathave been widely used in artificial intelligence applications. Their popularitystems from the fact they can be built by combining existing expert knowledgewith data and are highly interpretable, which makes them useful for decisionsupport, e.g. in healthcare. While some research has been published on thefederated learning of Bayesian networks, publications on Bayesian networks in avertically partitioned or heterogeneous data setting (where different variablesare located in different datasets) are limited, and suffer from importantomissions, such as the handling of missing data. In this article, we propose anovel method called VertiBayes to train Bayesian networks (structure andparameters) on vertically partitioned data, which can handle missing values aswell as an arbitrary number of parties. For structure learning we adapted thewidely used K2 algorithm with a privacy-preserving scalar product protocol. Forparameter learning, we use a two-step approach: first, we learn an intermediatemodel using maximum likelihood by treating missing values as a special valueand then we train a model on synthetic data generated by the intermediate modelusing the EM algorithm. The privacy guarantees of our approach are equivalentto the ones provided by the privacy preserving scalar product protocol used. Weexperimentally show our approach produces models comparable to those learntusing traditional algorithms and we estimate the increase in complexity interms of samples, network size, and complexity. Finally, we propose twoalternative approaches to estimate the performance of the model usingvertically partitioned data and we show in experiments that they lead toreasonably accurate estimates.</description><author>Florian van Daalen, Lianne Ippel, Andre Dekker, Inigo Bermejo</author><pubDate>Tue, 21 May 2024 15:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.17228v2</guid></item><item><title>Empirical Sample Complexity of Neural Network Mixed State Reconstruction</title><link>http://arxiv.org/abs/2307.01840v2</link><description>Quantum state reconstruction using Neural Quantum States has been proposed asa viable tool to reduce quantum shot complexity in practical applications, andits advantage over competing techniques has been shown in numerical experimentsfocusing mainly on the noiseless case. In this work, we numerically investigatethe performance of different quantum state reconstruction techniques for mixedstates: the finite-temperature Ising model. We show how to systematicallyreduce the quantum resource requirement of the algorithms by applying variancereduction techniques. Then, we compare the two leading neural quantum stateencodings of the state, namely, the Neural Density Operator and the positiveoperator-valued measurement representation, and illustrate their differentperformance as the mixedness of the target state varies. We find that certainencodings are more efficient in different regimes of mixedness and point outthe need for designing more efficient encodings in terms of both classical andquantum resources.</description><author>Haimeng Zhao, Giuseppe Carleo, Filippo Vicentini</author><pubDate>Tue, 21 May 2024 15:04:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01840v2</guid></item><item><title>A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables</title><link>http://arxiv.org/abs/2311.11343v3</link><description>In materials science, the challenge of rapid prototyping materials withdesired properties often involves extensive experimentation to find suitablemicrostructures. Additionally, finding microstructures for given properties istypically an ill-posed problem where multiple solutions may exist. Usinggenerative machine learning models can be a viable solution which also reducesthe computational cost. This comes with new challenges because, e.g., acontinuous property variable as conditioning input to the model is required. Weinvestigate the shortcomings of an existing method and compare this to a novelembedding strategy for generative models that is based on the binaryrepresentation of floating point numbers. This eliminates the need fornormalization, preserves information, and creates a versatile embedding spacefor conditioning the generative model. This technique can be applied tocondition a network on any number, to provide fine control over generatedmicrostructure images, thereby contributing to accelerated materials design.</description><author>Sébastien Bompas, Stefan Sandfeld</author><pubDate>Tue, 21 May 2024 15:01:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11343v3</guid></item><item><title>FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information</title><link>http://arxiv.org/abs/2405.12807v1</link><description>This paper establishes a mathematical foundation for the Adam optimizer,elucidating its connection to natural gradient descent through Riemannian andinformation geometry. We rigorously analyze the diagonal empirical Fisherinformation matrix (FIM) in Adam, clarifying all detailed approximations andadvocating for the use of log probability functions as loss, which should bebased on discrete distributions, due to the limitations of empirical FIM. Ouranalysis uncovers flaws in the original Adam algorithm, leading to proposedcorrections such as enhanced momentum calculations, adjusted bias corrections,and gradient clipping. We refine the weight decay term based on our theoreticalframework. Our modified algorithm, Fisher Adam (FAdam), demonstrates superiorperformance across diverse domains including LLM, ASR, and VQ-VAE, achievingstate-of-the-art results in ASR.</description><author>Dongseong Hwang</author><pubDate>Tue, 21 May 2024 14:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12807v1</guid></item><item><title>MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video</title><link>http://arxiv.org/abs/2405.12806v1</link><description>Single-view clothed human reconstruction holds a central position in virtualreality applications, especially in contexts involving intricate human motions.It presents notable challenges in achieving realistic clothing deformation.Current methodologies often overlook the influence of motion on surfacedeformation, resulting in surfaces lacking the constraints imposed by globalmotion. To overcome these limitations, we introduce an innovative framework,Motion-Based 3D Clothed Humans Synthesis (MOSS), which employs kinematicinformation to achieve motion-aware Gaussian split on the human surface. Ourframework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS)and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisherdistribution to propagate global motion across the body surface. The densityand rotation factors of this distribution explicitly control the Gaussians,thereby enhancing the realism of the reconstructed surface. Additionally, toaddress local occlusions in single-view, based on KGAS, UID identifiessignificant surfaces, and geometric reconstruction is performed to compensatefor these deformations. Experimental results demonstrate that MOSS achievesstate-of-the-art visual quality in 3D clothed human synthesis from monocularvideos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94%and 16.75% in LPIPS* respectively. Codes are available athttps://wanghongsheng01.github.io/MOSS/.</description><author>Hongsheng Wang, Xiang Cai, Xi Sun, Jinhong Yue, Shengyu Zhang, Feng Lin, Fei Wu</author><pubDate>Tue, 21 May 2024 14:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12806v1</guid></item><item><title>Stochastic Inference of Plate Bending from Heterogeneous Data: Physics-informed Gaussian Processes via Kirchhoff-Love Theory</title><link>http://arxiv.org/abs/2405.12802v1</link><description>Advancements in machine learning and an abundance of structural monitoringdata have inspired the integration of mechanical models with probabilisticmodels to identify a structure's state and quantify the uncertainty of itsphysical parameters and response. In this paper, we propose an inferencemethodology for classical Kirchhoff-Love plates via physics-informed GaussianProcesses (GP). A probabilistic model is formulated as a multi-output GP byplacing a GP prior on the deflection and deriving the covariance function usingthe linear differential operators of the plate governing equations. Theposteriors of the flexural rigidity, hyperparameters, and plate response areinferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) samplingfrom noisy measurements. We demonstrate the applicability with two examples: asimply supported plate subjected to a sinusoidal load and a fixed platesubjected to a uniform load. The results illustrate how the proposedmethodology can be employed to perform stochastic inference for plate rigidityand physical quantities by integrating measurements from various sensor typesand qualities. Potential applications of the presented methodology are instructural health monitoring and uncertainty quantification of plate-likestructures.</description><author>Igor Kavrakov, Gledson Rodrigo Tondo, Guido Morgenthal</author><pubDate>Tue, 21 May 2024 14:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12802v1</guid></item><item><title>Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously</title><link>http://arxiv.org/abs/2311.13816v2</link><description>The endeavor to preserve the generalization of a fair and invariantclassifier across domains, especially in the presence of distribution shifts,becomes a significant and intricate challenge in machine learning. In responseto this challenge, numerous effective algorithms have been developed with afocus on addressing the problem of fairness-aware domain generalization. Thesealgorithms are designed to navigate various types of distribution shifts, witha particular emphasis on covariate and dependence shifts. In this context,covariate shift pertains to changes in the marginal distribution of inputfeatures, while dependence shift involves alterations in the joint distributionof the label variable and sensitive attributes. In this paper, we introduce asimple but effective approach that aims to learn a fair and invariantclassifier by simultaneously addressing both covariate and dependence shiftsacross domains. We assert the existence of an underlying transformation modelcan transform data from one domain to another, while preserving the semanticsrelated to non-sensitive attributes and classes. By augmenting varioussynthetic data domains through the model, we learn a fair and invariantclassifier in source domains. This classifier can then be generalized tounknown target domains, maintaining both model prediction and fairnessconcerns. Extensive empirical studies on four benchmark datasets demonstratethat our approach surpasses state-of-the-art methods.</description><author>Chen Zhao, Kai Jiang, Xintao Wu, Haoliang Wang, Latifur Khan, Christan Grant, Feng Chen</author><pubDate>Tue, 21 May 2024 14:51:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13816v2</guid></item></channel></rss>