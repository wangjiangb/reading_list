<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 12 Jul 2023 14:00:43 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives</title><link>http://arxiv.org/abs/2307.05473v1</link><description>Given a set of calibrated images of a scene, we present an approach thatproduces a simple, compact, and actionable 3D world representation by means of3D primitives. While many approaches focus on recovering high-fidelity 3Dscenes, we focus on parsing a scene into mid-level 3D representations made of asmall set of textured primitives. Such representations are interpretable, easyto manipulate and suited for physics-based simulations. Moreover, unlikeexisting primitive decomposition methods that rely on 3D input data, ourapproach operates directly on images through differentiable rendering.Specifically, we model primitives as textured superquadric meshes and optimizetheir parameters from scratch with an image rendering loss. We highlight theimportance of modeling transparency for each primitive, which is critical foroptimization and also enables handling varying numbers of primitives. We showthat the resulting textured primitives faithfully reconstruct the input imagesand accurately model the visible 3D points, while providing amodal shapecompletions of unseen object regions. We compare our approach to the state ofthe art on diverse scenes from DTU, and demonstrate its robustness on real-lifecaptures from BlendedMVS and Nerfstudio. We also showcase how our results canbe used to effortlessly edit a scene or perform physical simulations. Code andvideo results are available at https://www.tmonnier.com/DBW .</description><author>Tom Monnier, Jake Austin, Angjoo Kanazawa, Alexei A. Efros, Mathieu Aubry</author><pubDate>Tue, 11 Jul 2023 18:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05473v1</guid></item><item><title>TOAST: Transfer Learning via Attention Steering</title><link>http://arxiv.org/abs/2305.15542v2</link><description>Transfer learning involves adapting a pre-trained model to novel downstreamtasks. However, we observe that current transfer learning methods often fail tofocus on task-relevant features. In this work, we explore refocusing modelattention for transfer learning. We introduce Top-Down Attention Steering(TOAST), a novel transfer learning algorithm that keeps the pre-trainedbackbone frozen, selects task-relevant features in the output, and feeds thosefeatures back to the model to steer the attention to the task-specificfeatures. By refocusing the attention only, TOAST achieves state-of-the-artresults on a number of transfer learning benchmarks, while having a smallnumber of tunable parameters. Compared to fully fine-tuning, LoRA, and prompttuning, TOAST substantially improves performance across a range of fine-grainedvisual classification datasets (e.g., 81.1% -&gt; 86.2% on FGVC). TOAST alsooutperforms the fully fine-tuned Alpaca and Vicuna models oninstruction-following language generation. Code is available athttps://github.com/bfshi/TOAST.</description><author>Baifeng Shi, Siyu Gai, Trevor Darrell, Xin Wang</author><pubDate>Tue, 11 Jul 2023 18:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15542v2</guid></item><item><title>Scale Alone Does not Improve Mechanistic Interpretability in Vision Models</title><link>http://arxiv.org/abs/2307.05471v1</link><description>In light of the recent widespread adoption of AI systems, understanding theinternal information processing of neural networks has become increasinglycritical. Most recently, machine vision has seen remarkable progress by scalingneural networks to unprecedented levels in dataset and model size. We here askwhether this extraordinary increase in scale also positively impacts the fieldof mechanistic interpretability. In other words, has our understanding of theinner workings of scaled neural networks improved as well? We here use apsychophysical paradigm to quantify mechanistic interpretability for a diversesuite of models and find no scaling effect for interpretability - neither formodel nor dataset size. Specifically, none of the nine investigatedstate-of-the-art models are easier to interpret than the GoogLeNet model fromalmost a decade ago. Latest-generation vision models appear even lessinterpretable than older architectures, hinting at a regression rather thanimprovement, with modern models sacrificing interpretability for accuracy.These results highlight the need for models explicitly designed to bemechanistically interpretable and the need for more helpful interpretabilitymethods to increase our understanding of networks at an atomic level. Werelease a dataset containing more than 120'000 human responses from ourpsychophysical evaluation of 767 units across nine models. This dataset ismeant to facilitate research on automated instead of human-basedinterpretability evaluations that can ultimately be leveraged to directlyoptimize the mechanistic interpretability of models.</description><author>Roland S. Zimmermann, Thomas Klein, Wieland Brendel</author><pubDate>Tue, 11 Jul 2023 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05471v1</guid></item><item><title>My3DGen: Building Lightweight Personalized 3D Generative Model</title><link>http://arxiv.org/abs/2307.05468v1</link><description>Our paper presents My3DGen, a practical system for creating a personalizedand lightweight 3D generative prior using as few as 10 images. My3DGen canreconstruct multi-view consistent images from an input test image, and generatenovel appearances by interpolating between any two images of the sameindividual. While recent studies have demonstrated the effectiveness ofpersonalized generative priors in producing high-quality 2D portraitreconstructions and syntheses, to the best of our knowledge, we are the firstto develop a personalized 3D generative prior. Instead of fine-tuning a largepre-trained generative model with millions of parameters to achievepersonalization, we propose a parameter-efficient approach. Our method involvesutilizing a pre-trained model with fixed weights as a generic prior, whiletraining a separate personalized prior through low-rank decomposition of theweights in each convolution and fully connected layer. However,parameter-efficient few-shot fine-tuning on its own often leads to overfitting.To address this, we introduce a regularization technique based on symmetry ofhuman faces. This regularization enforces that novel view renderings of atraining sample, rendered from symmetric poses, exhibit the same identity. Byincorporating this symmetry prior, we enhance the quality of reconstruction andsynthesis, particularly for non-frontal (profile) faces. Our final systemcombines low-rank fine-tuning with symmetry regularization and significantlysurpasses the performance of pre-trained models, e.g. EG3D. It introduces onlyapproximately 0.6 million additional parameters per identity compared to 31million for full finetuning of the original model. As a result, our systemachieves a 50-fold reduction in model size without sacrificing the quality ofthe generated 3D faces. Code will be available at our project page:https://luchaoqi.github.io/my3dgen.</description><author>Luchao Qi, Jiaye Wu, Shengze Wang, Soumyadip Sengupta</author><pubDate>Tue, 11 Jul 2023 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05468v1</guid></item><item><title>EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</title><link>http://arxiv.org/abs/2307.05463v1</link><description>Video-language pre-training (VLP) has become increasingly important due toits ability to generalize to various vision and language tasks. However,existing egocentric VLP frameworks utilize separate video and language encodersand learn task-specific cross-modal information only during fine-tuning,limiting the development of a unified system. In this work, we introduce thesecond generation of egocentric video-language pre-training (EgoVLPv2), asignificant improvement from the previous generation, by incorporatingcross-modal fusion directly into the video and language backbones. EgoVLPv2learns strong video-text representation during pre-training and reuses thecross-modal attention modules to support different downstream tasks in aflexible and efficient manner, reducing fine-tuning costs. Moreover, ourproposed fusion in the backbone strategy is more lightweight andcompute-efficient than stacking additional fusion-specific layers. Extensiveexperiments on a wide range of VL tasks demonstrate the effectiveness ofEgoVLPv2 by achieving consistent state-of-the-art performance over strongbaselines across all downstream. Our project page can be found athttps://shramanpramanick.github.io/EgoVLPv2/.</description><author>Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Rama Chellappa, Pengchuan Zhang</author><pubDate>Tue, 11 Jul 2023 18:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05463v1</guid></item><item><title>Efficient 3D Articulated Human Generation with Layered Surface Volumes</title><link>http://arxiv.org/abs/2307.05462v1</link><description>Access to high-quality and diverse 3D articulated digital human assets iscrucial in various applications, ranging from virtual reality to socialplatforms. Generative approaches, such as 3D generative adversarial networks(GANs), are rapidly replacing laborious manual content creation tools. However,existing 3D GAN frameworks typically rely on scene representations thatleverage either template meshes, which are fast but offer limited quality, orvolumes, which offer high capacity but are slow to render, thereby limiting the3D fidelity in GAN settings. In this work, we introduce layered surface volumes(LSVs) as a new 3D object representation for articulated digital humans. LSVsrepresent a human body using multiple textured mesh layers around aconventional template. These layers are rendered using alpha compositing withfast differentiable rasterization, and they can be interpreted as a volumetricrepresentation that allocates its capacity to a manifold of finite thicknessaround the template. Unlike conventional single-layer templates that strugglewith representing fine off-surface details like hair or accessories, oursurface volumes naturally capture such details. LSVs can be articulated, andthey exhibit exceptional efficiency in GAN settings, where a 2D generatorlearns to synthesize the RGBA textures for the individual layers. Trained onunstructured, single-view 2D image datasets, our LSV-GAN generates high-qualityand view-consistent 3D articulated digital humans without the need forview-inconsistent 2D upsampling networks.</description><author>Yinghao Xu, Wang Yifan, Alexander W. Bergman, Menglei Chai, Bolei Zhou, Gordon Wetzstein</author><pubDate>Tue, 11 Jul 2023 18:50:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05462v1</guid></item><item><title>LegoNN: Building Modular Encoder-Decoder Models</title><link>http://arxiv.org/abs/2206.03318v2</link><description>State-of-the-art encoder-decoder models (e.g. for machine translation (MT) orautomatic speech recognition (ASR)) are constructed and trained end-to-end asan atomic unit. No component of the model can be (re-)used without the others,making it impossible to share parts, e.g. a high resourced decoder, acrosstasks. We describe LegoNN, a procedure for building encoder-decoderarchitectures in a way so that its parts can be applied to other tasks withoutthe need for any fine-tuning. To achieve this reusability, the interfacebetween encoder and decoder modules is grounded to a sequence of marginaldistributions over a pre-defined discrete vocabulary. We present two approachesfor ingesting these marginals; one is differentiable, allowing the flow ofgradients across the entire network, and the other is gradient-isolating. Toenable the portability of decoder modules between MT tasks for different sourcelanguages and across other tasks like ASR, we introduce a modality agnosticencoder which consists of a length control mechanism to dynamically adaptencoders' output lengths in order to match the expected input length range ofpre-trained decoders. We present several experiments to demonstrate theeffectiveness of LegoNN models: a trained language generation LegoNN decodermodule from German-English (De-En) MT task can be reused without anyfine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MTtasks, matching or beating the performance of baseline. After fine-tuning,LegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5%relative WER reduction on the Europarl ASR task. To show how the approachgeneralizes, we compose a LegoNN ASR model from three modules -- each has beenlearned within different end-to-end trained models on three different datasets-- achieving an overall WER reduction of 19.5%.</description><author>Siddharth Dalmia, Dmytro Okhonko, Mike Lewis, Sergey Edunov, Shinji Watanabe, Florian Metze, Luke Zettlemoyer, Abdelrahman Mohamed</author><pubDate>Tue, 11 Jul 2023 18:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.03318v2</guid></item><item><title>Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features</title><link>http://arxiv.org/abs/2307.05454v1</link><description>A challenge towards developing NLP systems for the world's languages isunderstanding how they generalize to typological differences relevant forreal-world applications. To this end, we propose M2C, a morphologically-awareframework for behavioral testing of NLP models. We use M2C to generate teststhat probe models' behavior in light of specific linguistic features in 12typologically diverse languages. We evaluate state-of-the-art language modelson the generated tests. While models excel at most tests in English, wehighlight generalization failures to specific typological characteristics suchas temporal expressions in Swahili and compounding possessives in Finish. Ourfindings motivate the development of models that address these blind spots.</description><author>Ester Hlavnova, Sebastian Ruder</author><pubDate>Tue, 11 Jul 2023 18:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05454v1</guid></item><item><title>Adversarial Cheap Talk</title><link>http://arxiv.org/abs/2211.11030v4</link><description>Adversarial attacks in reinforcement learning (RL) often assumehighly-privileged access to the victim's parameters, environment, or data.Instead, this paper proposes a novel adversarial setting called a Cheap TalkMDP in which an Adversary can merely append deterministic messages to theVictim's observation, resulting in a minimal range of influence. The Adversarycannot occlude ground truth, influence underlying environment dynamics orreward signals, introduce non-stationarity, add stochasticity, see the Victim'sactions, or access their parameters. Additionally, we present a simplemeta-learning algorithm called Adversarial Cheap Talk (ACT) to trainAdversaries in this setting. We demonstrate that an Adversary trained with ACTstill significantly influences the Victim's training and testing performance,despite the highly constrained setting. Affecting train-time performancereveals a new attack vector and provides insight into the success and failuremodes of existing RL algorithms. More specifically, we show that an ACTAdversary is capable of harming performance by interfering with the learner'sfunction approximation, or instead helping the Victim's performance byoutputting useful features. Finally, we show that an ACT Adversary canmanipulate messages during train-time to directly and arbitrarily control theVictim at test-time. Project video and code are available athttps://sites.google.com/view/adversarial-cheap-talk</description><author>Chris Lu, Timon Willi, Alistair Letcher, Jakob Foerster</author><pubDate>Tue, 11 Jul 2023 18:31:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11030v4</guid></item><item><title>Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and Denoising</title><link>http://arxiv.org/abs/2307.05447v1</link><description>Due to the low accuracy of object detection and recognition in manyintelligent surveillance systems at nighttime, the quality of night images iscrucial. Compared with the corresponding daytime image, nighttime image ischaracterized as low brightness, low contrast and high noise. In this paper, abio-inspired image enhancement algorithm is proposed to convert a lowilluminance image to a brighter and clear one. Different from existingbio-inspired algorithm, the proposed method doesn't use any training sequences,we depend on a novel chain of contrast enhancement and denoising algorithmswithout using any forms of recursive functions. Our method can largely improvethe brightness and contrast of night images, besides, suppress noise. Then weimplement on real experiment, and simulation experiment to test our algorithms.Both results show the advantages of proposed algorithm over contrast pair,Meylan and Retinex.</description><author>Xinyi Bai, Steffi Agino Priyanka, Hsiao-Jung Tung, Yuankai Wang</author><pubDate>Tue, 11 Jul 2023 18:22:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05447v1</guid></item><item><title>Neural Quantile Optimization for Edge-Cloud Computing</title><link>http://arxiv.org/abs/2307.05170v1</link><description>We seek the best traffic allocation scheme for the edge-cloud computingnetwork that satisfies constraints and minimizes the cost based on burstablebilling. First, for a fixed network topology, we formulate a family of integerprogramming problems with random parameters describing the various trafficdemands. Then, to overcome the difficulty caused by the discrete feature of theproblem, we generalize the Gumbel-softmax reparameterization method to inducean unconstrained continuous optimization problem as a regularized continuationof the discrete problem. Finally, we introduce the Gumbel-softmax samplingnetwork to solve the optimization problems via unsupervised learning. Thenetwork structure reflects the edge-cloud computing topology and is trained tominimize the expectation of the cost function for unconstrained continuousoptimization problems. The trained network works as an efficient trafficallocation scheme sampler, remarkably outperforming the random strategy infeasibility and cost function value. Besides testing the quality of the outputallocation scheme, we examine the generalization property of the network byincreasing the time steps and the number of users. We also feed the solution toexisting integer optimization solvers as initial conditions and verify thewarm-starts can accelerate the short-time iteration process. The framework isgeneral with solid performance, and the decoupled feature of the random neuralnetworks is adequate for practical implementations.</description><author>Bin Du, He Zhang, Xiangle Cheng, Lei Zhang</author><pubDate>Tue, 11 Jul 2023 12:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05170v1</guid></item><item><title>Application of data engineering approaches to address challenges in microbiome data for optimal medical decision-making</title><link>http://arxiv.org/abs/2307.00033v2</link><description>The human gut microbiota is known to contribute to numerous physiologicalfunctions of the body and also implicated in a myriad of pathologicalconditions. Prolific research work in the past few decades have yieldedvaluable information regarding the relative taxonomic distribution of gutmicrobiota. Unfortunately, the microbiome data suffers from class imbalance andhigh dimensionality issues that must be addressed. In this study, we haveimplemented data engineering algorithms to address the above-mentioned issuesinherent to microbiome data. Four standard machine learning classifiers(logistic regression (LR), support vector machines (SVM), random forests (RF),and extreme gradient boosting (XGB) decision trees) were implemented on apreviously published dataset. The issue of class imbalance and highdimensionality of the data was addressed through synthetic minorityoversampling technique (SMOTE) and principal component analysis (PCA). Ourresults indicate that ensemble classifiers (RF and XGB decision trees) exhibitsuperior classification accuracy in predicting the host phenotype. Theapplication of PCA significantly reduced testing time while maintaining highclassification accuracy. The highest classification accuracy was obtained atthe levels of species for most classifiers. The prototype employed in the studyaddresses the issues inherent to microbiome datasets and could be highlybeneficial for providing personalized medicine.</description><author>Isha Thombre, Pavan Kumar Perepu, Shyam Kumar Sudhakar</author><pubDate>Tue, 11 Jul 2023 12:01:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00033v2</guid></item><item><title>Enhanced Multi-level Features for Very High Resolution Remote Sensing Scene Classification</title><link>http://arxiv.org/abs/2305.00679v2</link><description>Very high-resolution (VHR) remote sensing (RS) scene classification is achallenging task due to the higher inter-class similarity and intra-classvariability problems. Recently, the existing deep learning (DL)-based methodshave shown great promise in VHR RS scene classification. However, they stillprovide an unstable classification performance. To address such a problem, we,in this letter, propose a novel DL-based approach. For this, we devise anenhanced VHR attention module (EAM), followed by the atrous spatial pyramidpooling (ASPP) and global average pooling (GAP). This procedure imparts theenhanced features from the corresponding level. Then, the multi-level featurefusion is performed. Experimental results on two widely-used VHR RS datasetsshow that the proposed approach yields a competitive and stable/robustclassification performance with the least standard deviation of 0.001. Further,the highest overall accuracies on the AID and the NWPU datasets are 95.39% and93.04%, respectively.</description><author>Chiranjibi Sitaula, Sumesh KC, Jagannath Aryal</author><pubDate>Tue, 11 Jul 2023 11:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00679v2</guid></item><item><title>A Mapping Study of Machine Learning Methods for Remaining Useful Life Estimation of Lead-Acid Batteries</title><link>http://arxiv.org/abs/2307.05163v1</link><description>Energy storage solutions play an increasingly important role in moderninfrastructure and lead-acid batteries are among the most commonly used in therechargeable category. Due to normal degradation over time, correctlydetermining the battery's State of Health (SoH) and Remaining Useful Life (RUL)contributes to enhancing predictive maintenance, reliability, and longevity ofbattery systems. Besides improving the cost savings, correct estimation of theSoH can lead to reduced pollution though reuse of retired batteries. This paperpresents a mapping study of the state-of-the-art in machine learning methodsfor estimating the SoH and RUL of lead-acid batteries. These two indicators arecritical in the battery management systems of electric vehicles, renewableenergy systems, and other applications that rely heavily on this batterytechnology. In this study, we analyzed the types of machine learning algorithmsemployed for estimating SoH and RUL, and evaluated their performance in termsof accuracy and inference time. Additionally, this mapping identifies andanalyzes the most commonly used combinations of sensors in specificapplications, such as vehicular batteries. The mapping concludes byhighlighting potential gaps and opportunities for future research, which laysthe foundation for further advancements in the field.</description><author>Sérgio F Chevtchenko, Elisson da Silva Rocha, Bruna Cruz, Ermeson Carneiro de Andrade, Danilo Ricardo Barbosa de Araújo</author><pubDate>Tue, 11 Jul 2023 11:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05163v1</guid></item><item><title>SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization</title><link>http://arxiv.org/abs/2307.05162v1</link><description>Finetuning Large Language Models helps improve the results fordomain-specific use cases. End-to-end finetuning of large language models istime and resource intensive and has high storage requirements to store thefinetuned version of the large language model. Parameter Efficient Fine Tuning(PEFT) methods address the time and resource challenges by keeping the largelanguage model as a fixed base and add additional layers, which the PEFTmethods finetune. This paper demonstrates the evaluation results for one suchPEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization.The evaluation results show that LoRA works at par with end-to-end finetuningfor a large language model. The paper presents the evaluations done for solvingboth the Subtask A and B from ImageCLEFmedical{https://www.imageclef.org/2023/medical}</description><author>Kunal Suri, Prakhar Mishra, Saumajit Saha, Atul Singh</author><pubDate>Tue, 11 Jul 2023 11:38:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05162v1</guid></item><item><title>On the Effectiveness of Speech Self-supervised Learning for Music</title><link>http://arxiv.org/abs/2307.05161v1</link><description>Self-supervised learning (SSL) has shown promising results in various speechand natural language processing applications. However, its efficacy in musicinformation retrieval (MIR) still remains largely unexplored. While previousSSL models pre-trained on music recordings may have been mostly closed-sourced,recent speech models such as wav2vec2.0 have shown promise in music modelling.Nevertheless, research exploring the effectiveness of applying speech SSLmodels to music recordings has been limited. We explore the music adaption ofSSL with two distinctive speech-related models, data2vec1.0 and Hubert, andrefer to them as music2vec and musicHuBERT, respectively. We train $12$ SSLmodels with 95M parameters under various pre-training configurations andsystematically evaluate the MIR task performances with 13 different MIR tasks.Our findings suggest that training with music data can generally improveperformance on MIR tasks, even when models are trained using paradigms designedfor speech. However, we identify the limitations of such existingspeech-oriented designs, especially in modelling polyphonic information. Basedon the experimental results, empirical suggestions are also given for designingfuture musical SSL strategies and paradigms.</description><author>Yinghao Ma, Ruibin Yuan, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Ruibo Liu, Gus Xia, Roger Dannenberg, Yike Guo, Jie Fu</author><pubDate>Tue, 11 Jul 2023 11:37:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05161v1</guid></item><item><title>A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings</title><link>http://arxiv.org/abs/2307.05158v1</link><description>Predicting where a person is looking is a complex task, requiring tounderstand not only the person's gaze and scene content, but also the 3D scenestructure and the person's situation (are they manipulating? interacting orobserving others? attentive?) to detect obstructions in the line of sight orapply attention priors that humans typically have when observing others. Inthis paper, we hypothesize that identifying and leveraging such priors can bebetter achieved through the exploitation of explicitly derived multimodal cuessuch as depth and pose. We thus propose a modular multimodal architectureallowing to combine these cues using an attention mechanism. The architecturecan naturally be exploited in privacy-sensitive situations such as surveillanceand health, where personally identifiable information cannot be released. Weperform extensive experiments on the GazeFollow and VideoAttentionTarget publicdatasets, obtaining state-of-the-art performance and demonstrating verycompetitive results in the privacy setting case.</description><author>Anshul Gupta, Samy Tafasca, Jean-Marc Odobez</author><pubDate>Tue, 11 Jul 2023 11:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05158v1</guid></item><item><title>Stable Normative Explanations: From Argumentation to Deontic Logic</title><link>http://arxiv.org/abs/2307.05156v1</link><description>This paper examines how a notion of stable explanation developed elsewhere inDefeasible Logic can be expressed in the context of formal argumentation. Withthis done, we discuss the deontic meaning of this reconstruction and show howto build from argumentation neighborhood structures for deontic logic wherethis notion of explanation can be characterised. Some direct complexity resultsare offered.</description><author>Cecilia Di Florio, Guido Governatori, Antonino Rotolo, Giovanni Sartor</author><pubDate>Tue, 11 Jul 2023 11:26:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05156v1</guid></item><item><title>DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification</title><link>http://arxiv.org/abs/2307.03500v2</link><description>Gradient sparsification is a widely adopted solution for reducing theexcessive communication traffic in distributed deep learning. However, mostexisting gradient sparsifiers have relatively poor scalability because ofconsiderable computational cost of gradient selection and/or increasedcommunication traffic owing to gradient build-up. To address these challenges,we propose a novel gradient sparsification scheme, DEFT, that partitions thegradient selection task into sub tasks and distributes them to workers. DEFTdiffers from existing sparsifiers, wherein every worker selects gradients amongall gradients. Consequently, the computational cost can be reduced as thenumber of workers increases. Moreover, gradient build-up can be eliminatedbecause DEFT allows workers to select gradients in partitions that arenon-intersecting (between workers). Therefore, even if the number of workersincreases, the communication traffic can be maintained as per user requirement. To avoid the loss of significance of gradient selection, DEFT selects moregradients in the layers that have a larger gradient norm than the other layers.Because every layer has a different computational load, DEFT allocates layersto workers using a bin-packing algorithm to maintain a balanced load ofgradient selection between workers. In our empirical evaluation, DEFT shows asignificant improvement in training performance in terms of speed in gradientselection over existing sparsifiers while achieving high convergenceperformance.</description><author>Daegun Yoon, Sangyoon Oh</author><pubDate>Tue, 11 Jul 2023 11:23:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03500v2</guid></item><item><title>Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders</title><link>http://arxiv.org/abs/2307.05152v1</link><description>Experimental particle physics demands a sophisticated trigger and acquisitionsystem capable to efficiently retain the collisions of interest for furtherinvestigation. Heterogeneous computing with the employment of FPGA cards mayemerge as a trending technology for the triggering strategy of the upcominghigh-luminosity program of the Large Hadron Collider at CERN. In this context,we present two machine-learning algorithms for selecting events where neutrallong-lived particles decay within the detector volume studying their accuracyand inference time when accelerated on commercially available Xilinx FPGAaccelerator cards. The inference time is also confronted with a CPU- andGPU-based hardware setup. The proposed new algorithms are proven efficient forthe considered benchmark physics scenario and their accuracy is found to notdegrade when accelerated on the FPGA cards. The results indicate that alltested architectures fit within the latency requirements of a second-leveltrigger farm and that exploiting accelerator technologies for real-timeprocessing of particle-physics collisions is a promising research field thatdeserves additional investigations, in particular with machine-learning modelswith a large number of trainable parameters.</description><author>Andrea Coccaro, Francesco Armando Di Bello, Stefano Giagu, Lucrezia Rambelli, Nicola Stocchetti</author><pubDate>Tue, 11 Jul 2023 11:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05152v1</guid></item><item><title>ExFaceGAN: Exploring Identity Directions in GAN's Learned Latent Space for Synthetic Identity Generation</title><link>http://arxiv.org/abs/2307.05151v1</link><description>Deep generative models have recently presented impressive results ingenerating realistic face images of random synthetic identities. To generatemultiple samples of a certain synthetic identity, several previous worksproposed to disentangle the latent space of GANs by incorporating additionalsupervision or regularization, enabling the manipulation of certain attributes,e.g. identity, hairstyle, pose, or expression. Most of these works requiredesigning special loss functions and training dedicated network architectures.Others proposed to disentangle specific factors in unconditional pretrainedGANs latent spaces to control their output, which also requires supervision byattribute classifiers. Moreover, these attributes are entangled in GAN's latentspace, making it difficult to manipulate them without affecting the identityinformation. We propose in this work a framework, ExFaceGAN, to disentangleidentity information in state-of-the-art pretrained GANs latent spaces,enabling the generation of multiple samples of any synthetic identity. Thevariations in our generated images are not limited to specific attributes asExFaceGAN explicitly aims at disentangling identity information, while othervisual attributes are randomly drawn from a learned GAN latent space. As anexample of the practical benefit of our ExFaceGAN, we empirically prove thatdata generated by ExFaceGAN can be successfully used to train face recognitionmodels.</description><author>Fadi Boutros, Marcel Klemt, Meiling Fang, Arjan Kuijper, Naser Damer</author><pubDate>Tue, 11 Jul 2023 11:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05151v1</guid></item><item><title>A Modal Logic for Explaining some Graph Neural Networks</title><link>http://arxiv.org/abs/2307.05150v1</link><description>In this paper, we propose a modal logic in which counting modalities appearin linear inequalities. We show that each formula can be transformed into anequivalent graph neural network (GNN). We also show that each GNN can betransformed into a formula. We show that the satisfiability problem isdecidable. We also discuss some variants that are in PSPACE.</description><author>Pierre Nunn, François Schwarzentruber</author><pubDate>Tue, 11 Jul 2023 11:13:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05150v1</guid></item><item><title>Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles</title><link>http://arxiv.org/abs/2307.04679v2</link><description>In this paper, we provide a novel framework for the analysis ofgeneralization error of first-order optimization algorithms for statisticallearning when the gradient can only be accessed through partial observationsgiven by an oracle. Our analysis relies on the regularity of the gradientw.r.t. the data samples, and allows to derive near matching upper and lowerbounds for the generalization error of multiple learning problems, includingsupervised learning, transfer learning, robust learning, distributed learningand communication efficient learning using gradient quantization. These resultshold for smooth and strongly-convex optimization problems, as well as smoothnon-convex optimization problems verifying a Polyak-Lojasiewicz assumption. Inparticular, our upper and lower bounds depend on a novel quantity that extendsthe notion of conditional standard deviation, and is a measure of the extent towhich the gradient can be approximated by having access to the oracle. As aconsequence, our analysis provides a precise meaning to the intuition thatoptimization of the statistical learning objective is as hard as the estimationof its gradient. Finally, we show that, in the case of standard supervisedlearning, mini-batch gradient descent with increasing batch sizes and a warmstart can reach a generalization error that is optimal up to a multiplicativefactor, thus motivating the use of this optimization scheme in practicalapplications.</description><author>Kevin Scaman, Mathieu Even, Laurent Massoulié</author><pubDate>Tue, 11 Jul 2023 11:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04679v2</guid></item><item><title>High Dimensional Quantum Machine Learning With Small Quantum Computers</title><link>http://arxiv.org/abs/2203.13739v3</link><description>Quantum computers hold great promise to enhance machine learning, but theircurrent qubit counts restrict the realisation of this promise. In an attempt toplacate this limitation techniques can be applied for evaluating a quantumcircuit using a machine with fewer qubits than the circuit naively requires.These techniques work by evaluating many smaller circuits on the smallermachine, that are then combined in a polynomial to replicate the output of thelarger machine. This scheme requires more circuit evaluations than arepractical for general circuits. However, we investigate the possibility thatfor certain applications many of these subcircuits are superfluous, and that amuch smaller sum is sufficient to estimate the full circuit. We construct amachine learning model that may be capable of approximating the outputs of thelarger circuit with much fewer circuit evaluations. We successfully apply ourmodel to the task of digit recognition, using simulated quantum computers muchsmaller than the data dimension. The model is also applied to the task ofapproximating a random 10 qubit PQC with simulated access to a 5 qubitcomputer, even with only relatively modest number of circuits our modelprovides an accurate approximation of the 10 qubit PQCs output, superior to aneural network attempt. The developed method might be useful for implementingquantum models on larger data throughout the NISQ era.</description><author>Simon C. Marshall, Casper Gyurik, Vedran Dunjko</author><pubDate>Tue, 11 Jul 2023 10:55:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.13739v3</guid></item><item><title>TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities</title><link>http://arxiv.org/abs/2212.06385v2</link><description>Recently, the success of pre-training in text domain has been fully extendedto vision, audio, and cross-modal scenarios. The proposed pre-training modelsof different modalities are showing a rising trend of homogeneity in theirmodel structures, which brings the opportunity to implement differentpre-training models within a uniform framework. In this paper, we presentTencentPretrain, a toolkit supporting pre-training models of differentmodalities. The core feature of TencentPretrain is the modular design. Thetoolkit uniformly divides pre-training models into 5 components: embedding,encoder, target embedding, decoder, and target. As almost all of common modulesare provided in each component, users can choose the desired modules fromdifferent components to build a complete pre-training model. The modular designenables users to efficiently reproduce existing pre-training models or buildbrand-new one. We test the toolkit on text, vision, and audio benchmarks andshow that it can match the performance of the original implementations.</description><author>Zhe Zhao, Yudong Li, Cheng Hou, Jing Zhao, Rong Tian, Weijie Liu, Yiren Chen, Ningyuan Sun, Haoyan Liu, Weiquan Mao, Han Guo, Weigang Guo, Taiqiang Wu, Tao Zhu, Wenhang Shi, Chen Chen, Shan Huang, Sihong Chen, Liqun Liu, Feifei Li, Xiaoshuai Chen, Xingwu Sun, Zhanhui Kang, Xiaoyong Du, Linlin Shen, Kimmo Yan</author><pubDate>Tue, 11 Jul 2023 10:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.06385v2</guid></item><item><title>Hybrid quantum-classical machine learning for generative chemistry and drug design</title><link>http://arxiv.org/abs/2108.11644v2</link><description>Deep generative chemistry models emerge as powerful tools to expedite drugdiscovery. However, the immense size and complexity of the structural space ofall possible drug-like molecules pose significant obstacles, which could beovercome with hybrid architectures combining quantum computers with deepclassical networks. As the first step toward this goal, we built a compactdiscrete variational autoencoder (DVAE) with a Restricted Boltzmann Machine(RBM) of reduced size in its latent layer. The size of the proposed model wassmall enough to fit on a state-of-the-art D-Wave quantum annealer and allowedtraining on a subset of the ChEMBL dataset of biologically active compounds.Finally, we generated 2331 novel chemical structures with medicinal chemistryand synthetic accessibility properties in the ranges typical for molecules fromChEMBL. The presented results demonstrate the feasibility of using alreadyexisting or soon-to-be-available quantum computing devices as testbeds forfuture drug discovery applications.</description><author>A. I. Gircha, A. S. Boev, K. Avchaciov, P. O. Fedichev, A. K. Fedorov</author><pubDate>Tue, 11 Jul 2023 10:42:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.11644v2</guid></item><item><title>Deep Probabilistic Movement Primitives with a Bayesian Aggregator</title><link>http://arxiv.org/abs/2307.05141v1</link><description>Movement primitives are trainable parametric models that reproduce roboticmovements starting from a limited set of demonstrations. Previous worksproposed simple linear models that exhibited high sample efficiency andgeneralization power by allowing temporal modulation of movements (reproducingmovements faster or slower), blending (merging two movements into one),via-point conditioning (constraining a movement to meet some particularvia-points) and context conditioning (generation of movements based on anobserved variable, e.g., position of an object). Previous works have proposedneural network-based motor primitive models, having demonstrated their capacityto perform tasks with some forms of input conditioning or time-modulationrepresentations. However, there has not been a single unified deep motorprimitive's model proposed that is capable of all previous operations, limitingneural motor primitive's potential applications. This paper proposes a deepmovement primitive architecture that encodes all the operations above and usesa Bayesian context aggregator that allows a more sound context conditioning andblending. Our results demonstrate our approach can scale to reproduce complexmotions on a larger variety of input choices compared to baselines whilemaintaining operations of linear movement primitives provide.</description><author>Michael Przystupa, Faezeh Haghverd, Martin Jagersand, Samuele Tosatto</author><pubDate>Tue, 11 Jul 2023 10:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05141v1</guid></item><item><title>RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</title><link>http://arxiv.org/abs/2305.18295v2</link><description>Text-to-image generation has recently witnessed remarkable achievements. Weintroduce a text-conditional image diffusion model, termed RAPHAEL, to generatehighly artistic images, which accurately portray the text prompts, encompassingmultiple nouns, adjectives, and verbs. This is achieved by stacking tens ofmixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enablingbillions of diffusion paths (routes) from the network input to the output. Eachpath intuitively functions as a "painter" for depicting a particular textualconcept onto a specified image region at a diffusion timestep. Comprehensiveexperiments reveal that RAPHAEL outperforms recent cutting-edge models, such asStable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of bothimage quality and aesthetic appeal. Firstly, RAPHAEL exhibits superiorperformance in switching images across diverse styles, such as Japanese comics,realism, cyberpunk, and ink illustration. Secondly, a single model with threebillion parameters, trained on 1,000 A100 GPUs for two months, achieves astate-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,RAPHAEL significantly surpasses its counterparts in human evaluation on theViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel thefrontiers of image generation research in both academia and industry, pavingthe way for future breakthroughs in this rapidly evolving field. More detailscan be found on a webpage: https://miaohua.sensetime.com/en.</description><author>Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, Ping Luo</author><pubDate>Tue, 11 Jul 2023 10:33:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18295v2</guid></item><item><title>DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer</title><link>http://arxiv.org/abs/2307.04157v2</link><description>Neural Style Transfer (NST) is the field of study applying neural techniquesto modify the artistic appearance of a content image to match the style of areference style image. Traditionally, NST methods have focused on texture-basedimage edits, affecting mostly low level information and keeping most imagestructures the same. However, style-based deformation of the content isdesirable for some styles, especially in cases where the style is abstract orthe primary concept of the style is in its deformed rendition of some content.With the recent introduction of diffusion models, such as Stable Diffusion, wecan access far more powerful image generation techniques, enabling newpossibilities. In our work, we propose using this new class of models toperform style transfer while enabling deformable style transfer, an elusivecapability in previous models. We show how leveraging the priors of thesemodels can expose new artistic controls at inference time, and we document ourfindings in exploring this new direction for the field of style transfer.</description><author>Dan Ruta, Gemma Canet Tarrés, Andrew Gilbert, Eli Shechtman, Nicholas Kolkin, John Collomosse</author><pubDate>Tue, 11 Jul 2023 10:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04157v2</guid></item><item><title>Unveiling the invisible: Enhanced detection and analysis deteriorated areas in solar PV modules using unsupervised sensing algorithms and 3D augmented reality</title><link>http://arxiv.org/abs/2307.05136v1</link><description>Solar Photovoltaic (PV) is increasingly being used to address the globalconcern of energy security. However, hot spot and snail trails in PV modulescaused mostly by crakes reduce their efficiency and power capacity. Thisarticle presents a groundbreaking methodology for automatically identifying andanalyzing anomalies like hot spots and snail trails in Solar Photovoltaic (PV)modules, leveraging unsupervised sensing algorithms and 3D Augmented Reality(AR) visualization. By transforming the traditional methods of diagnosis andrepair, our approach not only enhances efficiency but also substantially cutsdown the cost of PV system maintenance. Validated through computer simulationsand real-world image datasets, the proposed framework accurately identifiesdirty regions, emphasizing the critical role of regular maintenance inoptimizing the power capacity of solar PV modules. Our immediate objective isto leverage drone technology for real-time, automatic solar panel detection,significantly boosting the efficacy of PV maintenance. The proposed methodologycould revolutionize solar PV maintenance, enabling swift, precise anomalydetection without human intervention. This could result in significant costsavings, heightened energy production, and improved overall performance ofsolar PV systems. Moreover, the novel combination of unsupervised sensingalgorithms with 3D AR visualization heralds new opportunities for furtherresearch and development in solar PV maintenance.</description><author>Adel Oulefki, Yassine Himeur, Thaweesak Trongtiraku, Kahina Amara, Sos Agaian, Samir, Benbelkacem, Mohamed Amine Guerroudji, Mohamed Zemmouri, Sahla Ferhat, Nadia Zenati, Shadi Atalla, Wathiq Mansoor</author><pubDate>Tue, 11 Jul 2023 10:27:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05136v1</guid></item><item><title>TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation</title><link>http://arxiv.org/abs/2307.05134v1</link><description>The progress in the generation of synthetic images has made it crucial toassess their quality. While several metrics have been proposed to assess therendering of images, it is crucial for Text-to-Image (T2I) models, whichgenerate images based on a prompt, to consider additional aspects such as towhich extent the generated image matches the important content of the prompt.Moreover, although the generated images usually result from a random startingpoint, the influence of this one is generally not considered. In this article,we propose a new metric based on prompt templates to study the alignmentbetween the content specified in the prompt and the corresponding generatedimages. It allows us to better characterize the alignment in terms of the typeof the specified objects, their number, and their color. We conducted a studyon several recent T2I models about various aspects. An additional interestingresult we obtained with our approach is that image quality can vary drasticallydepending on the latent noise used as a seed for the images. We also quantifythe influence of the number of concepts in the prompt, their order as well astheir (color) attributes. Finally, our method allows us to identify some latentseeds that produce better images than others, opening novel directions ofresearch on this understudied topic.</description><author>Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille</author><pubDate>Tue, 11 Jul 2023 10:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05134v1</guid></item><item><title>On the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis</title><link>http://arxiv.org/abs/2307.05132v1</link><description>Self-supervised learning (SSL) speech representations learned from largeamounts of diverse, mixed-quality speech data without transcriptions aregaining ground in many speech technology applications. Prior work has shownthat SSL is an effective intermediate representation in two-stagetext-to-speech (TTS) for both read and spontaneous speech. However, it is stillnot clear which SSL and which layer from each SSL model is most suited forspontaneous TTS. We address this shortcoming by extending the scope ofcomparison for SSL in spontaneous TTS to 6 different SSLs and 3 layers withineach SSL. Furthermore, SSL has also shown potential in predicting the meanopinion scores (MOS) of synthesized speech, but this has only been done inread-speech MOS prediction. We extend an SSL-based MOS prediction frameworkpreviously developed for scoring read speech synthesis and evaluate itsperformance on synthesized spontaneous speech. All experiments are conductedtwice on two different spontaneous corpora in order to find generalizabletrends. Overall, we present comprehensive experimental results on the use ofSSL in spontaneous TTS and MOS prediction to further quantify and understandhow SSL can be used in spontaneous TTS. Audios samples:https://www.speech.kth.se/tts-demos/sp_ssl_tts</description><author>Siyang Wang, Gustav Eje Henter, Joakim Gustafson, Éva Székely</author><pubDate>Tue, 11 Jul 2023 10:22:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05132v1</guid></item><item><title>RGB-D And Thermal Sensor Fusion: A Systematic Literature Review</title><link>http://arxiv.org/abs/2305.11427v2</link><description>In the last decade, the computer vision field has seen significant progressin multimodal data fusion and learning, where multiple sensors, includingdepth, infrared, and visual, are used to capture the environment across diversespectral ranges. Despite these advancements, there has been no systematic andcomprehensive evaluation of fusing RGB-D and thermal modalities to date. Whileautonomous driving using LiDAR, radar, RGB, and other sensors has garneredsubstantial research interest, along with the fusion of RGB and depthmodalities, the integration of thermal cameras and, specifically, the fusion ofRGB-D and thermal data, has received comparatively less attention. This mightbe partly due to the limited number of publicly available datasets for suchapplications. This paper provides a comprehensive review of both,state-of-the-art and traditional methods used in fusing RGB-D and thermalcamera data for various applications, such as site inspection, human tracking,fault detection, and others. The reviewed literature has been categorised intotechnical areas, such as 3D reconstruction, segmentation, object detection,available datasets, and other related topics. Following a brief introductionand an overview of the methodology, the study delves into calibration andregistration techniques, then examines thermal visualisation and 3Dreconstruction, before discussing the application of classic feature-basedtechniques as well as modern deep learning approaches. The paper concludes witha discourse on current limitations and potential future research directions. Itis hoped that this survey will serve as a valuable reference for researcherslooking to familiarise themselves with the latest advancements and contributeto the RGB-DT research field.</description><author>Martin Brenner, Napoleon H. Reyes, Teo Susnjak, Andre L. C. Barczak</author><pubDate>Tue, 11 Jul 2023 10:20:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11427v2</guid></item><item><title>Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</title><link>http://arxiv.org/abs/2307.05131v1</link><description>This is an overview of the eleventh edition of the BioASQ challenge in thecontext of the Conference and Labs of the Evaluation Forum (CLEF) 2023. BioASQis a series of international challenges promoting advances in large-scalebiomedical semantic indexing and question answering. This year, BioASQconsisted of new editions of the two established tasks b and Synergy, and a newtask (MedProcNER) on semantic annotation of clinical content in Spanish withmedical procedures, which have a critical role in medical practice. In thisedition of BioASQ, 28 competing teams submitted the results of more than 150distinct systems in total for the three different shared tasks of thechallenge. Similarly to previous editions, most of the participating systemsachieved competitive performance, suggesting the continuous advancement of thestate-of-the-art in the field.</description><author>Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Salvador Lima López, Eulália Farré-Maduell, Luis Gasco, Martin Krallinger, Georgios Paliouras</author><pubDate>Tue, 11 Jul 2023 10:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05131v1</guid></item><item><title>DFR: Depth from Rotation by Uncalibrated Image Rectification with Latitudinal Motion Assumption</title><link>http://arxiv.org/abs/2307.05129v1</link><description>Despite the increasing prevalence of rotating-style capture (e.g.,surveillance cameras), conventional stereo rectification techniques frequentlyfail due to the rotation-dominant motion and small baseline between views. Inthis paper, we tackle the challenge of performing stereo rectification foruncalibrated rotating cameras. To that end, we propose Depth-from-Rotation(DfR), a novel image rectification solution that analytically rectifies twoimages with two-point correspondences and serves for further depth estimation.Specifically, we model the motion of a rotating camera as the camera rotates ona sphere with fixed latitude. The camera's optical axis lies perpendicular tothe sphere's surface. We call this latitudinal motion assumption. Then wederive a 2-point analytical solver from directly computing the rectifiedtransformations on the two images. We also present a self-adaptive strategy toreduce the geometric distortion after rectification. Extensive synthetic andreal data experiments demonstrate that the proposed method outperforms existingworks in effectiveness and efficiency by a significant margin.</description><author>Yongcong Zhang, Yifei Xue, Ming Liao, Huiqing Zhang, Yizhen Lao</author><pubDate>Tue, 11 Jul 2023 10:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05129v1</guid></item><item><title>One-Shot Learning for Periocular Recognition: Exploring the Effect of Domain Adaptation and Data Bias on Deep Representations</title><link>http://arxiv.org/abs/2307.05128v1</link><description>One weakness of machine-learning algorithms is the need to train the modelsfor a new task. This presents a specific challenge for biometric recognitiondue to the dynamic nature of databases and, in some instances, the reliance onsubject collaboration for data collection. In this paper, we investigate thebehavior of deep representations in widely used CNN models under extreme datascarcity for One-Shot periocular recognition, a biometric recognition task. Weanalyze the outputs of CNN layers as identity-representing feature vectors. Weexamine the impact of Domain Adaptation on the network layers' output forunseen data and evaluate the method's robustness concerning data normalizationand generalization of the best-performing layer. We improved state-of-the-artresults that made use of networks trained with biometric datasets with millionsof images and fine-tuned for the target periocular dataset by utilizingout-of-the-box CNNs trained for the ImageNet Recognition Challenge and standardcomputer vision algorithms. For example, for the Cross-Eyed dataset, we couldreduce the EER by 67% and 79% (from 1.70% and 3.41% to 0.56% and 0.71%) in theClose-World and Open-World protocols, respectively, for the periocular case. Wealso demonstrate that traditional algorithms like SIFT can outperform CNNs insituations with limited data or scenarios where the network has not beentrained with the test classes like the Open-World mode. SIFT alone was able toreduce the EER by 64% and 71.6% (from 1.7% and 3.41% to 0.6% and 0.97%) forCross-Eyed in the Close-World and Open-World protocols, respectively, and areduction of 4.6% (from 3.94% to 3.76%) in the PolyU database for theOpen-World and single biometric case.</description><author>Kevin Hernandez-Diaz, Fernando Alonso-Fernandez, Josef Bigun</author><pubDate>Tue, 11 Jul 2023 10:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05128v1</guid></item><item><title>Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs</title><link>http://arxiv.org/abs/2301.06862v2</link><description>Weighted finite-state automata (WSFAs) are commonly used in NLP. Failuretransitions are a useful extension for compactly representing backoffs orinterpolation in $n$-gram models and CRFs, which are special cases of WFSAs.The pathsum in ordinary acyclic WFSAs is efficiently computed by the backwardalgorithm in time $O(|E|)$, where $E$ is the set of transitions. However, thisdoes not allow failure transitions, and preprocessing the WFSA to eliminatefailure transitions could greatly increase $|E|$. We extend the backwardalgorithm to handle failure transitions directly. Our approach is efficientwhen the average state has outgoing arcs for only a small fraction $s \ll 1$ ofthe alphabet $\Sigma$. We propose an algorithm for general acyclic WFSAs whichruns in $O{\left(|E| + s |\Sigma| |Q| T_\text{max} \log{|\Sigma|}\right)}$,where $Q$ is the set of states and $T_\text{max}$ is the size of the largestconnected component of failure transitions. When the failure transitiontopology satisfies a condition exemplified by CRFs, the $T_\text{max}$ factorcan be dropped, and when the weight semiring is a ring, the $\log{|\Sigma|}$factor can be dropped. In the latter case (ring-weighted acyclic WFSAs), wealso give an alternative algorithm with complexity $\displaystyle O{\left(|E| +|\Sigma| |Q| \min(1,s\pi_\text{max}) \right)}$, where $\pi_\text{max}$ is thesize of the longest failure path.</description><author>Anej Svete, Benjamin Dayan, Tim Vieira, Ryan Cotterell, Jason Eisner</author><pubDate>Tue, 11 Jul 2023 10:08:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.06862v2</guid></item><item><title>Enhancing Continuous Time Series Modelling with a Latent ODE-LSTM Approach</title><link>http://arxiv.org/abs/2307.05126v1</link><description>Due to their dynamic properties such as irregular sampling rate andhigh-frequency sampling, Continuous Time Series (CTS) are found in manyapplications. Since CTS with irregular sampling rate are difficult to modelwith standard Recurrent Neural Networks (RNNs), RNNs have been generalised tohave continuous-time hidden dynamics defined by a Neural Ordinary DifferentialEquation (Neural ODE), leading to the ODE-RNN model. Another approach thatprovides a better modelling is that of the Latent ODE model, which constructs acontinuous-time model where a latent state is defined at all times. The LatentODE model uses a standard RNN as the encoder and a Neural ODE as the decoder.However, since the RNN encoder leads to difficulties with missing data andill-defined latent variables, a Latent ODE-RNN model has recently been proposedthat uses a ODE-RNN model as the encoder instead. Both the Latent ODE andLatent ODE-RNN models are difficult to train due to the vanishing and explodinggradients problem. To overcome this problem, the main contribution of thispaper is to propose and illustrate a new model based on a new Latent ODE usingan ODE-LSTM (Long Short-Term Memory) network as an encoder -- the LatentODE-LSTM model. To limit the growth of the gradients the Norm Gradient Clippingstrategy was embedded on the Latent ODE-LSTM model. The performance evaluationof the new Latent ODE-LSTM (with and without Norm Gradient Clipping) formodelling CTS with regular and irregular sampling rates is then demonstrated.Numerical experiments show that the new Latent ODE-LSTM performs better thanLatent ODE-RNNs and can avoid the vanishing and exploding gradients duringtraining.</description><author>C. Coelho, M. Fernanda P. Costa, L. L. Ferrás</author><pubDate>Tue, 11 Jul 2023 10:01:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05126v1</guid></item><item><title>An Inter-observer consistent deep adversarial training for visual scanpath prediction</title><link>http://arxiv.org/abs/2211.07336v2</link><description>The visual scanpath is a sequence of points through which the human gazemoves while exploring a scene. It represents the fundamental concepts uponwhich visual attention research is based. As a result, the ability to predictthem has emerged as an important task in recent years. In this paper, wepropose an inter-observer consistent adversarial training approach for scanpathprediction through a lightweight deep neural network. The adversarial methodemploys a discriminative neural network as a dynamic loss that is better suitedto model the natural stochastic phenomenon while maintaining consistencybetween the distributions related to the subjective nature of scanpathstraversed by different observers. Through extensive testing, we show thecompetitiveness of our approach in regard to state-of-the-art methods.</description><author>Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Alessandro Bruno</author><pubDate>Tue, 11 Jul 2023 10:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07336v2</guid></item><item><title>Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer</title><link>http://arxiv.org/abs/2307.05121v1</link><description>How to obtain informative representations of transactions and then performthe identification of fraudulent transactions is a crucial part of ensuringfinancial security. Recent studies apply Graph Neural Networks (GNNs) to thetransaction fraud detection problem. Nevertheless, they encounter challenges ineffectively learning spatial-temporal information due to structurallimitations. Moreover, few prior GNN-based detectors have recognized thesignificance of incorporating global information, which encompasses similarbehavioral patterns and offers valuable insights for discriminativerepresentation learning. Therefore, we propose a novel heterogeneous graphneural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) fortransaction fraud detection problems. Specifically, we design a temporalencoding strategy to capture temporal dependencies and incorporate it into thegraph neural network framework, enhancing spatial-temporal information modelingand improving expressive ability. Furthermore, we introduce a transformermodule to learn local and global information. Pairwise node-node interactionsovercome the limitation of the GNN structure and build up the interactions withthe target node and long-distance ones. Experimental results on two financialdatasets compared to general GNN models and GNN-based fraud detectorsdemonstrate that our proposed method STA-GT is effective on the transactionfraud detection task.</description><author>Yue Tian, Guanjun Liu</author><pubDate>Tue, 11 Jul 2023 09:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05121v1</guid></item><item><title>Tamed Warping Network for High-Resolution Semantic Video Segmentation</title><link>http://arxiv.org/abs/2005.01344v4</link><description>Recent approaches for fast semantic video segmentation have reducedredundancy by warping feature maps across adjacent frames, greatly speeding upthe inference phase. However, the accuracy drops seriously owing to the errorsincurred by warping. In this paper, we propose a novel framework and design asimple and effective correction stage after warping. Specifically, we build anon-key-frame CNN, fusing warped context features with current spatial details.Based on the feature fusion, our Context Feature Rectification~(CFR) modulelearns the model's difference from a per-frame model to correct the warpedfeatures. Furthermore, our Residual-Guided Attention~(RGA) module utilizes theresidual maps in the compressed domain to help CRF focus on error-proneregions. Results on Cityscapes show that the accuracy significantly increasesfrom $67.3\%$ to $71.6\%$, and the speed edges down from $65.5$ FPS to $61.8$FPS at a resolution of $1024\times 2048$. For non-rigid categories, e.g.,``human'' and ``object'', the improvements are even higher than 18 percentagepoints.</description><author>Songyuan Li, Junyi Feng, Xi Li</author><pubDate>Tue, 11 Jul 2023 09:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.01344v4</guid></item><item><title>$\ell_p$-Regression in the Arbitrary Partition Model of Communication</title><link>http://arxiv.org/abs/2307.05117v1</link><description>We consider the randomized communication complexity of the distributed$\ell_p$-regression problem in the coordinator model, for $p\in (0,2]$. In thisproblem, there is a coordinator and $s$ servers. The $i$-th server receives$A^i\in\{-M, -M+1, \ldots, M\}^{n\times d}$ and $b^i\in\{-M, -M+1, \ldots,M\}^n$ and the coordinator would like to find a $(1+\epsilon)$-approximatesolution to $\min_{x\in\mathbb{R}^n} \|(\sum_i A^i)x - (\sum_i b^i)\|_p$. Here$M \leq \mathrm{poly}(nd)$ for convenience. This model, where the data isadditively shared across servers, is commonly referred to as the arbitrarypartition model. We obtain significantly improved bounds for this problem. For $p = 2$, i.e.,least squares regression, we give the first optimal bound of$\tilde{\Theta}(sd^2 + sd/\epsilon)$ bits. For $p \in (1,2)$,we obtain an $\tilde{O}(sd^2/\epsilon +sd/\mathrm{poly}(\epsilon))$ upper bound. Notably, for $d$ sufficiently large,our leading order term only depends linearly on $1/\epsilon$ rather thanquadratically. We also show communication lower bounds of $\Omega(sd^2 +sd/\epsilon^2)$ for $p\in (0,1]$ and $\Omega(sd^2 + sd/\epsilon)$ for $p\in(1,2]$. Our bounds considerably improve previous bounds due to (Woodruff et al.COLT, 2013) and (Vempala et al., SODA, 2020).</description><author>Yi Li, Honghao Lin, David P. Woodruff</author><pubDate>Tue, 11 Jul 2023 09:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05117v1</guid></item><item><title>ResNeRF: Geometry-Guided Residual Neural Radiance Field for Indoor Scene Novel View Synthesis</title><link>http://arxiv.org/abs/2211.16211v3</link><description>We represent the ResNeRF, a novel geometry-guided two-stage framework forindoor scene novel view synthesis. Be aware of that a good geometry wouldgreatly boost the performance of novel view synthesis, and to avoid thegeometry ambiguity issue, we propose to characterize the density distributionof the scene based on a base density estimated from scene geometry and aresidual density parameterized by the geometry. In the first stage, we focus ongeometry reconstruction based on SDF representation, which would lead to a goodgeometry surface of the scene and also a sharp density. In the second stage,the residual density is learned based on the SDF learned in the first stage forencoding more details about the appearance. In this way, our method can betterlearn the density distribution with the geometry prior for high-fidelity novelview synthesis while preserving the 3D structures. Experiments on large-scaleindoor scenes with many less-observed and textureless areas show that with thegood 3D surface, our method achieves state-of-the-art performance for novelview synthesis.</description><author>Yuting Xiao, Yiqun Zhao, Yanyu Xu, Shenghua Gao</author><pubDate>Tue, 11 Jul 2023 09:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16211v3</guid></item><item><title>Beyond the Obvious: Evaluating the Reasoning Ability In Real-life Scenarios of Language Models on Life Scapes Reasoning Benchmark~(LSR-Benchmark)</title><link>http://arxiv.org/abs/2307.05113v1</link><description>This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), anovel dataset targeting real-life scenario reasoning, aiming to close the gapin artificial neural networks' ability to reason in everyday contexts. Incontrast to domain knowledge reasoning datasets, LSR-Benchmark comprisesfree-text formatted questions with rich information on real-life scenarios,human behaviors, and character roles. The dataset consists of 2,162 questionscollected from open-source online sources and is manually annotated to improveits quality. Experiments are conducted using state-of-the-art language models,such as gpt3.5-turbo and instruction fine-tuned llama models, to test theperformance in LSR-Benchmark. The results reveal that humans outperform thesemodels significantly, indicating a persisting challenge for machine learningmodels in comprehending daily human life.</description><author>Zhouhong Gu, Zihan Li, Lin Zhang, Zhuozhi Xiong, Sihang Jiang, Xiaoxuan Zhu, Shusen Wang, Zili Wang, Jianchen Wang, Haoning Ye, Wenhao Huang, Yikai Zhang, Hongwei Feng, Yanghua Xiao</author><pubDate>Tue, 11 Jul 2023 09:45:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05113v1</guid></item><item><title>Single-Model Attribution of Generative Models Through Final-Layer Inversion</title><link>http://arxiv.org/abs/2306.06210v2</link><description>Recent groundbreaking developments on generative modeling have sparkedinterest in practical single-model attribution. Such methods predict whether asample was generated by a specific generator or not, for instance, to proveintellectual property theft. However, previous works are either limited to theclosed-world setting or require undesirable changes of the generative model. Weaddress these shortcomings by proposing FLIPAD, a new approach for single-modelattribution in the open-world setting based on final-layer inversion andanomaly detection. We show that the utilized final-layer inversion can bereduced to a convex lasso optimization problem, making our approachtheoretically sound and computationally efficient. The theoretical findings areaccompanied by an experimental study demonstrating the effectiveness of ourapproach, outperforming the existing methods.</description><author>Mike Laszkiewicz, Jonas Ricker, Johannes Lederer, Asja Fischer</author><pubDate>Tue, 11 Jul 2023 09:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06210v2</guid></item><item><title>Conformalization of Sparse Generalized Linear Models</title><link>http://arxiv.org/abs/2307.05109v1</link><description>Given a sequence of observable variables $\{(x_1, y_1), \ldots, (x_n,y_n)\}$, the conformal prediction method estimates a confidence set for$y_{n+1}$ given $x_{n+1}$ that is valid for any finite sample size by merelyassuming that the joint distribution of the data is permutation invariant.Although attractive, computing such a set is computationally infeasible in mostregression problems. Indeed, in these cases, the unknown variable $y_{n+1}$ cantake an infinite number of possible candidate values, and generating conformalsets requires retraining a predictive model for each candidate. In this paper,we focus on a sparse linear model with only a subset of variables forprediction and use numerical continuation techniques to approximate thesolution path efficiently. The critical property we exploit is that the set ofselected variables is invariant under a small perturbation of the input data.Therefore, it is sufficient to enumerate and refit the model only at the changepoints of the set of active features and smoothly interpolate the rest of thesolution via a Predictor-Corrector mechanism. We show how our path-followingalgorithm accurately approximates conformal prediction sets and illustrate itsperformance using synthetic and real data examples.</description><author>Etash Kumar Guha, Eugene Ndiaye, Xiaoming Huo</author><pubDate>Tue, 11 Jul 2023 09:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05109v1</guid></item><item><title>A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI</title><link>http://arxiv.org/abs/2307.05104v1</link><description>Explainable Artificial Intelligence (XAI) has gained significant attentionrecently as the demand for transparency and interpretability of machinelearning models has increased. In particular, XAI for time series data hasbecome increasingly important in finance, healthcare, and climate science.However, evaluating the quality of explanations, such as attributions providedby XAI techniques, remains challenging. This paper provides an in-depthanalysis of using perturbations to evaluate attributions extracted from timeseries models. A perturbation analysis involves systematically modifying theinput data and evaluating the impact on the attributions generated by the XAImethod. We apply this approach to several state-of-the-art XAI techniques andevaluate their performance on three time series classification datasets. Ourresults demonstrate that the perturbation analysis approach can effectivelyevaluate the quality of attributions and provide insights into the strengthsand limitations of XAI techniques. Such an approach can guide the selection ofXAI methods for time series data, e.g., focusing on return time rather thanprecision, and facilitate the development of more reliable and interpretablemachine learning models for time series analysis.</description><author>Udo Schlegel, Daniel A. Keim</author><pubDate>Tue, 11 Jul 2023 09:26:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05104v1</guid></item><item><title>CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning</title><link>http://arxiv.org/abs/2307.04390v2</link><description>Background: MR-based subchondral bone effectively predicts kneeosteoarthritis. However, its clinical application is limited by the cost andtime of MR. Purpose: We aim to develop a novel distillation-learning-basedmethod named SRRD for subchondral bone microstructural analysis usingeasily-acquired CT images, which leverages paired MR images to enhance theCT-based analysis model during training. Materials and Methods: Knee jointimages of both CT and MR modalities were collected from October 2020 to May2021. Firstly, we developed a GAN-based generative model to transform MR imagesinto CT images, which was used to establish the anatomical correspondencebetween the two modalities. Next, we obtained numerous patches of subchondralbone regions of MR images, together with their trabecular parameters (BV / TV,Tb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression.The distillation-learning technique was used to train the regression model andtransfer MR structural information to the CT-based model. The regressedtrabecular parameters were further used for knee osteoarthritis classification.Results: A total of 80 participants were evaluated. CT-based regression resultsof trabecular parameters achieved intra-class correlation coefficients (ICCs)of 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N,respectively. The use of distillation learning significantly improved theperformance of the CT-based knee osteoarthritis classification method using theCNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of0.658 (95% CI, 0.574-0.742) (p&lt;.001). Conclusions: The proposed SRRD methodshowed high reliability and validity in MR-CT registration, regression, andknee osteoarthritis classification, indicating the feasibility of subchondralbone microstructural analysis based on CT images.</description><author>Yuqi Hu, Xiangyu Zhao, Gaowei Qing, Kai Xie, Chenglei Liu, Lichi Zhang</author><pubDate>Tue, 11 Jul 2023 09:15:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04390v2</guid></item><item><title>Exploring Structured Semantic Prior for Multi Label Recognition with Incomplete Labels</title><link>http://arxiv.org/abs/2303.13223v5</link><description>Multi-label recognition (MLR) with incomplete labels is very challenging.Recent works strive to explore the image-to-label correspondence in thevision-language model, \ie, CLIP, to compensate for insufficient annotations.In spite of promising performance, they generally overlook the valuable priorabout the label-to-label correspondence. In this paper, we advocate remedyingthe deficiency of label supervision for the MLR with incomplete labels byderiving a structured semantic prior about the label-to-label correspondencevia a semantic prior prompter. We then present a novel Semantic CorrespondencePrompt Network (SCPNet), which can thoroughly explore the structured semanticprior. A Prior-Enhanced Self-Supervised Learning method is further introducedto enhance the use of the prior. Comprehensive experiments and analyses onseveral widely used benchmark datasets show that our method significantlyoutperforms existing methods on all datasets, well demonstrating theeffectiveness and the superiority of our method. Our code will be available athttps://github.com/jameslahm/SCPNet.</description><author>Zixuan Ding, Ao Wang, Hui Chen, Qiang Zhang, Pengzhang Liu, Yongjun Bao, Weipeng Yan, Jungong Han</author><pubDate>Tue, 11 Jul 2023 09:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13223v5</guid></item><item><title>Realising Synthetic Active Inference Agents, Part II: Variational Message Updates</title><link>http://arxiv.org/abs/2306.02733v2</link><description>The Free Energy Principle (FEP) describes (biological) agents as minimising avariational Free Energy (FE) with respect to a generative model of theirenvironment. Active Inference (AIF) is a corollary of the FEP that describeshow agents explore and exploit their environment by minimising an expected FEobjective. In two related papers, we describe a scalable, epistemic approach tosynthetic AIF agents, by message passing on free-form Forney-style FactorGraphs (FFGs). A companion paper (part I) introduces a Constrained FFG (CFFG)notation that visually represents (generalised) FE objectives for AIF. Thecurrent paper (part II) derives message passing algorithms that minimise(generalised) FE objectives on a CFFG by variational calculus. A comparisonbetween simulated Bethe and generalised FE agents illustrates how synthetic AIFinduces epistemic behaviour on a T-maze navigation task. With a full messagepassing account of synthetic AIF agents, it becomes possible to derive andreuse message updates across models and move closer to industrial applicationsof synthetic AIF.</description><author>Thijs van de Laar, Magnus Koudahl, Bert de Vries</author><pubDate>Tue, 11 Jul 2023 09:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02733v2</guid></item><item><title>ATWM: Defense against adversarial malware based on adversarial training</title><link>http://arxiv.org/abs/2307.05095v1</link><description>Deep learning technology has made great achievements in the field of image.In order to defend against malware attacks, researchers have proposed manyWindows malware detection models based on deep learning. However, deep learningmodels are vulnerable to adversarial example attacks. Malware can generateadversarial malware with the same malicious function to attack the malwaredetection model and evade detection of the model. Currently, many adversarialdefense studies have been proposed, but existing adversarial defense studiesare based on image sample and cannot be directly applied to malware sample.Therefore, this paper proposes an adversarial malware defense method based onadversarial training. This method uses preprocessing to defend simpleadversarial examples to reduce the difficulty of adversarial training.Moreover, this method improves the adversarial defense capability of the modelthrough adversarial training. We experimented with three attack methods in twosets of datasets, and the results show that the method in this paper canimprove the adversarial defense capability of the model without reducing theaccuracy of the model.</description><author>Kun Li, Fan Zhang, Wei Guo</author><pubDate>Tue, 11 Jul 2023 09:07:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05095v1</guid></item><item><title>Continual Learning on Dynamic Graphs via Parameter Isolation</title><link>http://arxiv.org/abs/2305.13825v2</link><description>Many real-world graph learning tasks require handling dynamic graphs wherenew nodes and edges emerge. Dynamic graph learning methods commonly suffer fromthe catastrophic forgetting problem, where knowledge learned for previousgraphs is overwritten by updates for new graphs. To alleviate the problem,continual graph learning methods are proposed. However, existing continualgraph learning methods aim to learn new patterns and maintain old ones with thesame set of parameters of fixed size, and thus face a fundamental tradeoffbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)for continual learning on dynamic graphs that circumvents the tradeoff viaparameter isolation and expansion. Our motivation lies in that differentparameters contribute to learning different graph patterns. Based on the idea,we expand model parameters to continually learn emerging graph patterns.Meanwhile, to effectively preserve knowledge for unaffected patterns, we findparameters that correspond to them via optimization and freeze them to preventthem from being rewritten. Experiments on eight real-world datasets corroboratethe effectiveness of PI-GNN compared to state-of-the-art baselines.</description><author>Peiyan Zhang, Yuchen Yan, Chaozhuo Li, Senzhang Wang, Xing Xie, Guojie Song, Sunghun Kim</author><pubDate>Tue, 11 Jul 2023 09:02:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13825v2</guid></item><item><title>Offline and Online Optical Flow Enhancement for Deep Video Compression</title><link>http://arxiv.org/abs/2307.05092v1</link><description>Video compression relies heavily on exploiting the temporal redundancybetween video frames, which is usually achieved by estimating and using themotion information. The motion information is represented as optical flows inmost of the existing deep video compression networks. Indeed, these networksoften adopt pre-trained optical flow estimation networks for motion estimation.The optical flows, however, may be less suitable for video compression due tothe following two factors. First, the optical flow estimation networks weretrained to perform inter-frame prediction as accurately as possible, but theoptical flows themselves may cost too many bits to encode. Second, the opticalflow estimation networks were trained on synthetic data, and may not generalizewell enough to real-world videos. We address the twofold limitations byenhancing the optical flows in two stages: offline and online. In the offlinestage, we fine-tune a trained optical flow estimation network with the motioninformation provided by a traditional (non-deep) video compression scheme, e.g.H.266/VVC, as we believe the motion information of H.266/VVC achieves a betterrate-distortion trade-off. In the online stage, we further optimize the latentfeatures of the optical flows with a gradient descent-based algorithm for thevideo to be compressed, so as to enhance the adaptivity of the optical flows.We conduct experiments on a state-of-the-art deep video compression scheme,DCVC. Experimental results demonstrate that the proposed offline and onlineenhancement together achieves on average 12.8% bitrate saving on the testedvideos, without increasing the model or computational complexity of the decoderside.</description><author>Chuanbo Tang, Xihua Sheng, Zhuoyuan Li, Haotian Zhang, Li Li, Dong Liu</author><pubDate>Tue, 11 Jul 2023 08:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05092v1</guid></item><item><title>SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View Representation</title><link>http://arxiv.org/abs/2307.05087v1</link><description>SAR images are highly sensitive to observation configurations, and theyexhibit significant variations across different viewing angles, making itchallenging to represent and learn their anisotropic features. As a result,deep learning methods often generalize poorly across different view angles.Inspired by the concept of neural radiance fields (NeRF), this study combinesSAR imaging mechanisms with neural networks to propose a novel NeRF model forSAR image generation. Following the mapping and projection pinciples, a set ofSAR images is modeled implicitly as a function of attenuation coefficients andscattering intensities in the 3D imaging space through a differentiablerendering equation. SAR-NeRF is then constructed to learn the distribution ofattenuation coefficients and scattering intensities of voxels, where thevectorized form of 3D voxel SAR rendering equation and the samplingrelationship between the 3D space voxels and the 2D view ray grids areanalytically derived. Through quantitative experiments on various datasets, wethoroughly assess the multi-view representation and generalization capabilitiesof SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset cansignificantly improve SAR target classification performance under few-shotlearning setup, where a 10-type classification accuracy of 91.6\% can beachieved by using only 12 images per class.</description><author>Zhengxin Lei, Feng Xu, Jiangtao Wei, Feng Cai, Feng Wang, Ya-Qiu Jin</author><pubDate>Tue, 11 Jul 2023 08:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05087v1</guid></item><item><title>Vacaspati: A Diverse Corpus of Bangla Literature</title><link>http://arxiv.org/abs/2307.05083v1</link><description>Bangla (or Bengali) is the fifth most spoken language globally; yet, thestate-of-the-art NLP in Bangla is lagging for even simple tasks such aslemmatization, POS tagging, etc. This is partly due to lack of a varied qualitycorpus. To alleviate this need, we build Vacaspati, a diverse corpus of Banglaliterature. The literary works are collected from various websites; only thoseworks that are publicly available without copyright violations or restrictionsare collected. We believe that published literature captures the features of alanguage much better than newspapers, blogs or social media posts which tend tofollow only a certain literary pattern and, therefore, miss out on languagevariety. Our corpus Vacaspati is varied from multiple aspects, including typeof composition, topic, author, time, space, etc. It contains more than 11million sentences and 115 million words. We also built a word embedding model,Vac-FT, using FastText from Vacaspati as well as trained an Electra model,Vac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires onlya fraction of resources compared to other state-of-the-art transformer modelsand yet performs either better or similar on various downstream tasks. Onmultiple downstream tasks, Vac-FT outperforms other FastText-based models. Wealso demonstrate the efficacy of Vacaspati as a corpus by showing that similarmodels built from other corpora are not as effective. The models are availableat https://bangla.iitk.ac.in/.</description><author>Pramit Bhattacharyya, Joydeep Mondal, Subhadip Maji, Arnab Bhattacharya</author><pubDate>Tue, 11 Jul 2023 08:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05083v1</guid></item><item><title>OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning</title><link>http://arxiv.org/abs/2307.05082v1</link><description>This research presents a comprehensive methodology for utilizing anontology-driven structured prompts system in interplay with ChatGPT, a widelyused large language model (LLM). The study develops formal models, bothinformation and functional, and establishes the methodological foundations forintegrating ontology-driven prompts with ChatGPT's meta-learning capabilities.The resulting productive triad comprises the methodological foundations,advanced information technology, and the OntoChatGPT system, which collectivelyenhance the effectiveness and performance of chatbot systems. Theimplementation of this technology is demonstrated using the Ukrainian languagewithin the domain of rehabilitation. By applying the proposed methodology, theOntoChatGPT system effectively extracts entities from contexts, classifiesthem, and generates relevant responses. The study highlights the versatility ofthe methodology, emphasizing its applicability not only to ChatGPT but also toother chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2LLM. The underlying principles of meta-learning, structured prompts, andontology-driven information retrieval form the core of the proposedmethodology, enabling their adaptation and utilization in various LLM-basedsystems. This versatile approach opens up new possibilities for NLP anddialogue systems, empowering developers to enhance the performance andfunctionality of chatbot systems across different domains and languages.</description><author>Oleksandr Palagin, Vladislav Kaverinskiy, Anna Litvin, Kyrylo Malakhov</author><pubDate>Tue, 11 Jul 2023 08:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05082v1</guid></item><item><title>Argumentative Segmentation Enhancement for Legal Summarization</title><link>http://arxiv.org/abs/2307.05081v1</link><description>We use the combination of argumentative zoning [1] and a legal argumentativescheme to create legal argumentative segments. Based on the argumentativesegmentation, we propose a novel task of classifying argumentative segments oflegal case decisions. GPT-3.5 is used to generate summaries based onargumentative segments. In terms of automatic evaluation metrics, our methodgenerates higher quality argumentative summaries while leaving out lessrelevant context as compared to GPT-4 and non-GPT models.</description><author>Huihui Xu, Kevin Ashley</author><pubDate>Tue, 11 Jul 2023 08:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05081v1</guid></item><item><title>Estimating label quality and errors in semantic segmentation data via any model</title><link>http://arxiv.org/abs/2307.05080v1</link><description>The labor-intensive annotation process of semantic segmentation datasets isoften prone to errors, since humans struggle to label every pixel correctly. Westudy algorithms to automatically detect such annotation errors, in particularmethods to score label quality, such that the images with the lowest scores areleast likely to be correctly labeled. This helps prioritize what data to reviewin order to ensure a high-quality training/evaluation dataset, which iscritical in sensitive applications such as medical imaging and autonomousvehicles. Widely applicable, our label quality scores rely on probabilisticpredictions from a trained segmentation model -- any model architecture andtraining procedure can be utilized. Here we study 7 different label qualityscoring methods used in conjunction with a DeepLabV3+ or a FPN segmentationmodel to detect annotation errors in a version of the SYNTHIA dataset.Precision-recall evaluations reveal a score -- the soft-minimum of themodel-estimated likelihoods of each pixel's annotated class -- that isparticularly effective to identify images that are mislabeled, across multipletypes of annotation error.</description><author>Vedang Lad, Jonas Mueller</author><pubDate>Tue, 11 Jul 2023 08:29:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05080v1</guid></item><item><title>Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images</title><link>http://arxiv.org/abs/2307.05075v1</link><description>Removing multiple degradations, such as haze, rain, and blur, from real-worldimages poses a challenging and illposed problem. Recently, unified models thatcan handle different degradations have been proposed and yield promisingresults. However, these approaches focus on synthetic images and experience asignificant performance drop when applied to realworld images. In this paper,we introduce Uni-Removal, a twostage semi-supervised framework for addressingthe removal of multiple degradations in real-world images using a unified modeland parameters. In the knowledge transfer stage, Uni-Removal leverages asupervised multi-teacher and student architecture in the knowledge transferstage to facilitate learning from pretrained teacher networks specialized indifferent degradation types. A multi-grained contrastive loss is introduced toenhance learning from feature and image spaces. In the domain adaptation stage,unsupervised fine-tuning is performed by incorporating an adversarialdiscriminator on real-world images. The integration of an extendedmulti-grained contrastive loss and generative adversarial loss enables theadaptation of the student network from synthetic to real-world domains.Extensive experiments on real-world degraded datasets demonstrate theeffectiveness of our proposed method. We compare our Uni-Removal framework withstate-of-the-art supervised and unsupervised methods, showcasing its promisingresults in real-world image dehazing, deraining, and deblurring simultaneously.</description><author>Yongheng Zhang, Danfeng Yan, Yuanqiang Cai</author><pubDate>Tue, 11 Jul 2023 08:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05075v1</guid></item><item><title>Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with Sample-aware Prompting and Dynamic Revision Chain</title><link>http://arxiv.org/abs/2307.05074v1</link><description>Text-to-SQL aims at generating SQL queries for the given natural languagequestions and thus helping users to query databases. Prompt learning with largelanguage models (LLMs) has emerged as a recent approach, which designs promptsto lead LLMs to understand the input question and generate the correspondingSQL. However, it faces challenges with strict SQL syntax requirements. Existingwork prompts the LLMs with a list of demonstration examples (i.e. question-SQLpairs) to generate SQL, but the fixed prompts can hardly handle the scenariowhere the semantic gap between the retrieved demonstration and the inputquestion is large. In this paper, we propose a retrieval-augmented promptingmethod for a LLM-based Text-to-SQL framework, involving sample-aware promptingand a dynamic revision chain. Our approach incorporates sample-awaredemonstrations, which include the composition of SQL operators and fine-grainedinformation related to the given question. To retrieve questions sharingsimilar intents with input questions, we propose two strategies for assistingretrieval. Firstly, we leverage LLMs to simplify the original questions,unifying the syntax and thereby clarifying the users' intentions. To generateexecutable and accurate SQLs without human intervention, we design a dynamicrevision chain which iteratively adapts fine-grained feedback from thepreviously generated SQL. Experimental results on three Text-to-SQL benchmarksdemonstrate the superiority of our method over strong baseline models.</description><author>Chunxi Guo, Zhiliang Tian, Jintao Tang, Shasha Li, Zhihua Wen, Kaixuan Wang, Ting Wang</author><pubDate>Tue, 11 Jul 2023 08:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05074v1</guid></item><item><title>Aggregating Credences into Beliefs: Agenda Conditions for Impossibility Results</title><link>http://arxiv.org/abs/2307.05072v1</link><description>Binarizing belief aggregation addresses how to rationally aggregateindividual probabilistic beliefs into collective binary beliefs. Similar to thedevelopment of judgment aggregation theory, formulating axiomatic requirements,proving impossibility theorems, and identifying exact agenda conditions ofimpossibility theorems are natural and important research topics in binarizingbelief aggregation. Building on our previous research on impossibilitytheorems, we use an agenda-theoretic approach to generalize the results and todetermine the necessary and sufficient level of logical interconnection betweenthe issues in an agenda for the impossibility theorems to arise. We demonstratethat (1) path-connectedness and even-negatability constitute the exact agendacondition for the oligarchy result stating that binarizing belief aggregationsatisfying proposition-wise independence and deductive closure of collectivebeliefs yields the oligarchies under minor conditions; (2)negation-connectedness is the condition for the triviality result obtained byadding anonymity to the oligarchy result; and (3) blockedness is the conditionfor the impossibility result, which follows by adding completeness andconsistency of collective beliefs. Moreover, we compare these novel findingswith existing agenda-theoretic characterization theorems in judgmentaggregation and belief binarization.</description><author>Minkyung Wang, Chisu Kim</author><pubDate>Tue, 11 Jul 2023 08:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05072v1</guid></item><item><title>Mining for Unknown Unknowns</title><link>http://arxiv.org/abs/2307.05071v1</link><description>Unknown unknowns are future relevant contingencies that lack an ex antedescription. While there are numerous retrospective accounts showing thatsignificant gains or losses might have been achieved or avoided had suchcontingencies been previously uncovered, getting hold of unknown unknowns stillremains elusive, both in practice and conceptually. Using Formal ConceptAnalysis (FCA) - a subfield of lattice theory which is increasingly applied formining and organizing data - this paper introduces a simple framework tosystematically think out of the box and direct the search for unknown unknowns.</description><author>Bernard Sinclair-Desgagné</author><pubDate>Tue, 11 Jul 2023 08:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05071v1</guid></item><item><title>Cognitive Bias and Belief Revision</title><link>http://arxiv.org/abs/2307.05069v1</link><description>In this paper we formalise three types of cognitive bias within the frameworkof belief revision: confirmation bias, framing bias, and anchoring bias. Weinterpret them generally, as restrictions on the process of iterated revision,and we apply them to three well-known belief revision methods: conditioning,lexicographic revision, and minimal revision. We investigate the reliability ofbiased belief revision methods in truth tracking. We also run computersimulations to assess the performance of biased belief revision in randomscenarios.</description><author>Panagiotis Papadamos, Nina Gierasimczuk</author><pubDate>Tue, 11 Jul 2023 08:13:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05069v1</guid></item><item><title>A Theory of Bounded Inductive Rationality</title><link>http://arxiv.org/abs/2307.05068v1</link><description>The dominant theories of rational choice assume logical omniscience. That is,they assume that when facing a decision problem, an agent can perform allrelevant computations and determine the truth value of all relevantlogical/mathematical claims. This assumption is unrealistic when, for example,we offer bets on remote digits of pi or when an agent faces a computationallyintractable planning problem. Furthermore, the assumption of logicalomniscience creates contradictions in cases where the environment can containdescriptions of the agent itself. Importantly, strategic interactions asstudied in game theory are decision problems in which a rational agent ispredicted by its environment (the other players). In this paper, we develop atheory of rational decision making that does not assume logical omniscience. Weconsider agents who repeatedly face decision problems (including ones likebetting on digits of pi or games against other agents). The main contributionof this paper is to provide a sensible theory of rationality for such agents.Roughly, we require that a boundedly rational inductive agent tests eachefficiently computable hypothesis infinitely often and follows those hypothesesthat keep their promises of high rewards. We then prove that agents that arerational in this sense have other desirable properties. For example, they learnto value random and pseudo-random lotteries at their expected reward. Finally,we consider strategic interactions between different agents and prove a folktheorem for what strategies bounded rational inductive agents can converge to.</description><author>Caspar Oesterheld, Abram Demski, Vincent Conitzer</author><pubDate>Tue, 11 Jul 2023 08:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05068v1</guid></item><item><title>Exploiting Asymmetry in Logic Puzzles: Using ZDDs for Symbolic Model Checking Dynamic Epistemic Logic</title><link>http://arxiv.org/abs/2307.05067v1</link><description>Binary decision diagrams (BDDs) are widely used to mitigate thestate-explosion problem in model checking. A variation of BDDs areZero-suppressed Decision Diagrams (ZDDs) which omit variables that must befalse, instead of omitting variables that do not matter. We use ZDDs tosymbolically encode Kripke models used in Dynamic Epistemic Logic, a frameworkto reason about knowledge and information dynamics in multi-agent systems. Wecompare the memory usage of different ZDD variants for three well-knownexamples from the literature: the Muddy Children, the Sum and Product puzzleand the Dining Cryptographers. Our implementation is based on the existingmodel checker SMCDEL and the CUDD library. Our results show that replacing BDDswith the right variant of ZDDs can significantly reduce memory usage. Thissuggests that ZDDs are a useful tool for model checking multi-agent systems.</description><author>Daniel Miedema, Malvin Gattinger</author><pubDate>Tue, 11 Jul 2023 08:13:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05067v1</guid></item><item><title>Tableaux for the Logic of Strategically Knowing How</title><link>http://arxiv.org/abs/2307.05066v1</link><description>The logic of goal-directed knowing-how extends the standard epistemic logicwith an operator of knowing-how. The knowing-how operator is interpreted asthat there exists a strategy such that the agent knows that the strategy canmake sure that p. This paper presents a tableau procedure for the multi-agentversion of the logic of strategically knowing-how and shows the soundness andcompleteness of this tableau procedure. This paper also shows that thesatisfiability problem of the logic can be decided in PSPACE.</description><author>Yanjun Li</author><pubDate>Tue, 11 Jul 2023 08:12:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05066v1</guid></item><item><title>System of Spheres-based Two Level Credibility-limited Revisions</title><link>http://arxiv.org/abs/2307.05062v1</link><description>Two level credibility-limited revision is a non-prioritized revisionoperation. When revising by a two level credibility-limited revision, twolevels of credibility and one level of incredibility are considered. Whenrevising by a sentence at the highest level of credibility, the operatorbehaves as a standard revision, if the sentence is at the second level ofcredibility, then the outcome of the revision process coincides with a standardcontraction by the negation of that sentence. If the sentence is not credible,then the original belief set remains unchanged. In this paper, we propose aconstruction for two level credibility-limited revision operators based onGrove's systems of spheres and present an axiomatic characterization for theseoperators.</description><author>Marco Garapa, Eduardo Ferme, Maurício D. L. Reis</author><pubDate>Tue, 11 Jul 2023 08:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05062v1</guid></item><item><title>Joint Behavior and Common Belief</title><link>http://arxiv.org/abs/2303.07185v2</link><description>For over 25 years, common belief has been widely viewed as necessary forjoint behavior. But this is not quite correct. We show by example that what cannaturally be thought of as joint behavior can occur without common belief. Wethen present two variants of common belief that can lead to joint behavior,even without standard common belief ever being achieved, and show that one ofthem, action-stamped common belief, is in a sense necessary and sufficient forjoint behavior. These observations are significant because, as is well known,common belief is quite difficult to achieve in practice, whereas these variantsare more easily achievable.</description><author>Meir Friedenberg, Joseph Y. Halpern</author><pubDate>Tue, 11 Jul 2023 08:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07185v2</guid></item><item><title>Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction</title><link>http://arxiv.org/abs/2303.06681v2</link><description>Sparse-view cone-beam CT (CBCT) reconstruction is an important direction toreduce radiation dose and benefit clinical applications. Previous voxel-basedgeneration methods represent the CT as discrete voxels, resulting in highmemory requirements and limited spatial resolution due to the use of 3Ddecoders. In this paper, we formulate the CT volume as a continuous intensityfield and develop a novel DIF-Net to perform high-quality CBCT reconstructionfrom extremely sparse (fewer than 10) projection views at an ultrafast speed.The intensity field of a CT can be regarded as a continuous function of 3Dspatial points. Therefore, the reconstruction can be reformulated as regressingthe intensity value of an arbitrary 3D point from given sparse projections.Specifically, for a point, DIF-Net extracts its view-specific features fromdifferent 2D projection views. These features are subsequently aggregated by afusion module for intensity estimation. Notably, thousands of points can beprocessed in parallel to improve efficiency during training and testing. Inpractice, we collect a knee CBCT dataset to train and evaluate DIF-Net.Extensive experiments show that our approach can reconstruct CBCT with highimage quality and high spatial resolution from extremely sparse views within1.6 seconds, significantly outperforming state-of-the-art methods. Our codewill be available at https://github.com/xmed-lab/DIF-Net.</description><author>Yiqun Lin, Zhongjin Luo, Wei Zhao, Xiaomeng Li</author><pubDate>Tue, 11 Jul 2023 08:08:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06681v2</guid></item><item><title>On Imperfect Recall in Multi-Agent Influence Diagrams</title><link>http://arxiv.org/abs/2307.05059v1</link><description>Multi-agent influence diagrams (MAIDs) are a popular game-theoretic modelbased on Bayesian networks. In some settings, MAIDs offer significantadvantages over extensive-form game representations. Previous work on MAIDs hasassumed that agents employ behavioural policies, which set independentconditional probability distributions over actions for each of their decisions.In settings with imperfect recall, however, a Nash equilibrium in behaviouralpolicies may not exist. We overcome this by showing how to solve MAIDs withforgetful and absent-minded agents using mixed policies and two types ofcorrelated equilibrium. We also analyse the computational complexity of keydecision problems in MAIDs, and explore tractable cases. Finally, we describeapplications of MAIDs to Markov games and team situations, where imperfectrecall is often unavoidable.</description><author>James Fox, Matt MacDermott, Lewis Hammond, Paul Harrenstein, Alessandro Abate, Michael Wooldridge</author><pubDate>Tue, 11 Jul 2023 08:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05059v1</guid></item><item><title>Deep Ensemble Learning with Frame Skipping for Face Anti-Spoofing</title><link>http://arxiv.org/abs/2307.02858v2</link><description>Face presentation attacks (PA), also known as spoofing attacks, pose asubstantial threat to biometric systems that rely on facial recognitionsystems, such as access control systems, mobile payments, and identityverification systems. To mitigate the spoofing risk, several video-basedmethods have been presented in the literature that analyze facial motion insuccessive video frames. However, estimating the motion between adjacent framesis a challenging task and requires high computational cost. In this paper, werephrase the face anti-spoofing task as a motion prediction problem andintroduce a deep ensemble learning model with a frame skipping mechanism. Inparticular, the proposed frame skipping adopts a uniform sampling approach bydividing the original video into video clips of fixed size. By doing so, everynth frame of the clip is selected to ensure that the temporal patterns caneasily be perceived during the training of three different recurrent neuralnetworks (RNNs). Motivated by the performance of individual RNNs, a meta-modelis developed to improve the overall detection performance by combining theprediction of individual RNNs. Extensive experiments were performed on fourdatasets, and state-of-the-art performance is reported on MSU-MFSD (3.12%),Replay-Attack (11.19%), and OULU-NPU (12.23%) databases by using half totalerror rates (HTERs) in the most challenging cross-dataset testing scenario.</description><author>Usman Muhammad, Md Ziaul Hoque, Mourad Oussalah, Jorma Laaksonen</author><pubDate>Tue, 11 Jul 2023 08:06:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02858v2</guid></item><item><title>Strengthening Consistency Results in Modal Logic</title><link>http://arxiv.org/abs/2307.05053v1</link><description>A fundamental question asked in modal logic is whether a given theory isconsistent. But consistent with what? A typical way to address this questionidentifies a choice of background knowledge axioms (say, S4, D, etc.) and thenshows the assumptions codified by the theory in question to be consistent withthose background axioms. But determining the specific choice and division ofbackground axioms is, at least sometimes, little more than tradition. Thispaper introduces **generic theories** for propositional modal logic to addressconsistency results in a more robust way. As building blocks for backgroundknowledge, generic theories provide a standard for categorical determinationsof consistency. We argue that the results and methods of this paper help toelucidate problems in epistemology and enjoy sufficient scope and power to havepurchase on problems bearing on modalities in judgement, inference, anddecision making.</description><author>Samuel Allen Alexander, Arthur Paul Pedersen</author><pubDate>Tue, 11 Jul 2023 08:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05053v1</guid></item><item><title>Optimal Algorithms for Latent Bandits with Cluster Structure</title><link>http://arxiv.org/abs/2301.07040v3</link><description>We consider the problem of latent bandits with cluster structure where thereare multiple users, each with an associated multi-armed bandit problem. Theseusers are grouped into \emph{latent} clusters such that the mean reward vectorsof users within the same cluster are identical. At each round, a user, selecteduniformly at random, pulls an arm and observes a corresponding noisy reward.The goal of the users is to maximize their cumulative rewards. This problem iscentral to practical recommendation systems and has received wide attention oflate \cite{gentile2014online, maillard2014latent}. Now, if each user actsindependently, then they would have to explore each arm independently and aregret of $\Omega(\sqrt{\mathsf{MNT}})$ is unavoidable, where $\mathsf{M},\mathsf{N}$ are the number of arms and users, respectively. Instead, we proposeLATTICE (Latent bAndiTs via maTrIx ComplEtion) which allows exploitation of thelatent cluster structure to provide the minimax optimal regret of$\widetilde{O}(\sqrt{(\mathsf{M}+\mathsf{N})\mathsf{T}})$, when the number ofclusters is $\widetilde{O}(1)$. This is the first algorithm to guarantee suchstrong regret bound. LATTICE is based on a careful exploitation of arminformation within a cluster while simultaneously clustering users.Furthermore, it is computationally efficient and requires only$O(\log{\mathsf{T}})$ calls to an offline matrix completion oracle across all$\mathsf{T}$ rounds.</description><author>Soumyabrata Pal, Arun Sai Suggala, Karthikeyan Shanmugam, Prateek Jain</author><pubDate>Tue, 11 Jul 2023 08:04:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07040v3</guid></item><item><title>Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps</title><link>http://arxiv.org/abs/2307.05052v1</link><description>We investigate the role of various demonstration components in the in-contextlearning (ICL) performance of large language models (LLMs). Specifically, weexplore the impacts of ground-truth labels, input distribution, andcomplementary explanations, particularly when these are altered or perturbed.We build on previous work, which offers mixed findings on how these elementsinfluence ICL. To probe these questions, we employ explainable NLP (XNLP)methods and utilize saliency maps of contrastive demonstrations for bothqualitative and quantitative analysis. Our findings reveal that flippingground-truth labels significantly affects the saliency, though it's morenoticeable in larger LLMs. Our analysis of the input distribution at a granularlevel reveals that changing sentiment-indicative terms in a sentiment analysistask to neutral ones does not have as substantial an impact as alteringground-truth labels. Finally, we find that the effectiveness of complementaryexplanations in boosting ICL performance is task-dependent, with limitedbenefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.These insights are critical for understanding the functionality of LLMs andguiding the development of effective demonstrations, which is increasinglyrelevant in light of the growing use of LLMs in applications such as ChatGPT.Our research code is publicly available at https://github.com/paihengxu/XICL.</description><author>Zongxia Li, Paiheng Xu, Fuxiao Liu, Hyemi Song</author><pubDate>Tue, 11 Jul 2023 08:03:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05052v1</guid></item><item><title>Active Implicit Object Reconstruction using Uncertainty-guided Next-Best-View Optimziation</title><link>http://arxiv.org/abs/2303.16739v2</link><description>Actively planning sensor views during object reconstruction is crucial forautonomous mobile robots. An effective method should be able to strike abalance between accuracy and efficiency. In this paper, we propose a seamlessintegration of the emerging implicit representation with the activereconstruction task. We build an implicit occupancy field as our geometryproxy. While training, the prior object bounding box is utilized as auxiliaryinformation to generate clean and detailed reconstructions. To evaluate viewuncertainty, we employ a sampling-based approach that directly extracts entropyfrom the reconstructed occupancy probability field as our measure of viewinformation gain. This eliminates the need for additional uncertainty maps orlearning. Unlike previous methods that compare view uncertainty within a finiteset of candidates, we aim to find the next-best-view (NBV) on a continuousmanifold. Leveraging the differentiability of the implicit representation, theNBV can be optimized directly by maximizing the view uncertainty using gradientdescent. It significantly enhances the method's adaptability to differentscenarios. Simulation and real-world experiments demonstrate that our approacheffectively improves reconstruction accuracy and efficiency of view planning inactive reconstruction tasks. The proposed system will open source athttps://github.com/HITSZ-NRSL/ActiveImplicitRecon.git.</description><author>Dongyu Yan, Jianheng Liu, Fengyu Quan, Haoyao Chen, Mengmeng Fu</author><pubDate>Tue, 11 Jul 2023 07:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16739v2</guid></item><item><title>Portfolio Optimization: A Comparative Study</title><link>http://arxiv.org/abs/2307.05048v1</link><description>Portfolio optimization has been an area that has attracted considerableattention from the financial research community. Designing a profitableportfolio is a challenging task involving precise forecasting of future stockreturns and risks. This chapter presents a comparative study of three portfoliodesign approaches, the mean-variance portfolio (MVP), hierarchical risk parity(HRP)-based portfolio, and autoencoder-based portfolio. These three approachesto portfolio design are applied to the historical prices of stocks chosen fromten thematic sectors listed on the National Stock Exchange (NSE) of India. Theportfolios are designed using the stock price data from January 1, 2018, toDecember 31, 2021, and their performances are tested on the out-of-sample datafrom January 1, 2022, to December 31, 2022. Extensive results are analyzed onthe performance of the portfolios. It is observed that the performance of theMVP portfolio is the best on the out-of-sample data for the risk-adjustedreturns. However, the autoencoder portfolios outperformed their counterparts onannual returns.</description><author>Jaydip Sen, Subhasis Dasgupta</author><pubDate>Tue, 11 Jul 2023 07:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05048v1</guid></item><item><title>Epistemic Syllogistic: First Steps</title><link>http://arxiv.org/abs/2307.05043v1</link><description>Aristotle's discussions on modal syllogistic have often been viewed aserror-prone and have garnered significant attention in the literature due tohistorical and philosophical interests. However, from a contemporarystandpoint, they also introduced natural fragments of first-order modal logic,warranting a comprehensive technical analysis. In this paper, drawinginspiration from the natural logic program, we propose and examine severalvariants of modal syllogistic within the epistemic context, thereby coining theterm Epistemic Syllogistic. Specifically, we concentrate on the de reinterpretation of epistemic syllogisms containing non-trivial yet naturalexpressions such as "all things known to be A are also known to be not B." Weexplore the epistemic apodeictic syllogistic and its extensions, whichaccommodate more complex terms. Our main contributions include severalaxiomatizations of these logics, with completeness proofs that may be ofindependent interest.</description><author>Yipu Li, Yanjing Wang</author><pubDate>Tue, 11 Jul 2023 07:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05043v1</guid></item><item><title>DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification</title><link>http://arxiv.org/abs/2303.10610v3</link><description>Diffusion Probabilistic Models have recently shown remarkable performance ingenerative image modeling, attracting significant attention in the computervision community. However, while a substantial amount of diffusion-basedresearch has focused on generative tasks, few studies have applied diffusionmodels to general medical image classification. In this paper, we propose thefirst diffusion-based model (named DiffMIC) to address general medical imageclassification by eliminating unexpected noise and perturbations in medicalimages and robustly capturing semantic representation. To achieve this goal, wedevise a dual conditional guidance strategy that conditions each diffusion stepwith multiple granularities to improve step-wise regional attention.Furthermore, we propose learning the mutual information in each granularity byenforcing Maximum-Mean Discrepancy regularization during the diffusion forwardprocess. We evaluate the effectiveness of our DiffMIC on three medicalclassification tasks with different image modalities, including placentalmaturity grading on ultrasound images, skin lesion classification usingdermatoscopic images, and diabetic retinopathy grading using fundus images. Ourexperimental results demonstrate that DiffMIC outperforms state-of-the-artmethods by a significant margin, indicating the universality and effectivenessof the proposed model. Our code will be publicly available athttps://github.com/scott-yjyang/DiffMIC.</description><author>Yijun Yang, Huazhu Fu, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Lei Zhu</author><pubDate>Tue, 11 Jul 2023 07:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10610v3</guid></item><item><title>Disentangled Contrastive Image Translation for Nighttime Surveillance</title><link>http://arxiv.org/abs/2307.05038v1</link><description>Nighttime surveillance suffers from degradation due to poor illumination andarduous human annotations. It is challengable and remains a security risk atnight. Existing methods rely on multi-spectral images to perceive objects inthe dark, which are troubled by low resolution and color absence. We argue thatthe ultimate solution for nighttime surveillance is night-to-day translation,or Night2Day, which aims to translate a surveillance scene from nighttime tothe daytime while maintaining semantic consistency. To achieve this, this paperpresents a Disentangled Contrastive (DiCo) learning method. Specifically, toaddress the poor and complex illumination in the nighttime scenes, we propose alearnable physical prior, i.e., the color invariant, which provides a stableperception of a highly dynamic night environment and can be incorporated intothe learning pipeline of neural networks. Targeting the surveillance scenes, wedevelop a disentangled representation, which is an auxiliary pretext task thatseparates surveillance scenes into the foreground and background withcontrastive learning. Such a strategy can extract the semantics withoutsupervision and boost our model to achieve instance-aware translation. Finally,we incorporate all the modules above into generative adversarial networks andachieve high-fidelity translation. This paper also contributes a newsurveillance dataset called NightSuR. It includes six scenes to support thestudy on nighttime surveillance. This dataset collects nighttime images withdifferent properties of nighttime environments, such as flare and extremedarkness. Extensive experiments demonstrate that our method outperformsexisting works significantly. The dataset and source code will be released onGitHub soon.</description><author>Guanzhou Lan, Bin Zhao, Xuelong Li</author><pubDate>Tue, 11 Jul 2023 07:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05038v1</guid></item><item><title>Neural-Symbolic Recommendation with Graph-Enhanced Information</title><link>http://arxiv.org/abs/2307.05036v1</link><description>The recommendation system is not only a problem of inductive statistics fromdata but also a cognitive task that requires reasoning ability. The mostadvanced graph neural networks have been widely used in recommendation systemsbecause they can capture implicit structured information from graph-structureddata. However, like most neural network algorithms, they only learn matchingpatterns from a perception perspective. Some researchers use user behavior forlogic reasoning to achieve recommendation prediction from the perspective ofcognitive reasoning, but this kind of reasoning is a local one and ignoresimplicit information on a global scale. In this work, we combine the advantagesof graph neural networks and propositional logic operations to construct aneuro-symbolic recommendation model with both global implicit reasoning abilityand local explicit logic reasoning ability. We first build an item-item graphbased on the principle of adjacent interaction and use graph neural networks tocapture implicit information in global data. Then we transform user behaviorinto propositional logic expressions to achieve recommendations from theperspective of cognitive reasoning. Extensive experiments on five publicdatasets show that our proposed model outperforms several state-of-the-artmethods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].</description><author>Bang Chen, Wei Peng, Maonian Wu, Bo Zheng, Shaojun Zhu</author><pubDate>Tue, 11 Jul 2023 07:29:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05036v1</guid></item><item><title>Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition</title><link>http://arxiv.org/abs/2306.08249v2</link><description>Masked autoencoder (MAE) has attracted unprecedented attention and achievesremarkable performance in many vision tasks. It reconstructs random maskedimage patches (known as proxy task) during pretraining and learns meaningfulsemantic representations that can be transferred to downstream tasks. However,MAE has not been thoroughly explored in ultrasound imaging. In this work, weinvestigate the potential of MAE for ultrasound image recognition. Motivated bythe unique property of ultrasound imaging in high noise-to-signal ratio, wepropose a novel deblurring MAE approach that incorporates deblurring into theproxy task during pretraining. The addition of deblurring facilitates thepretraining to better recover the subtle details presented in the ultrasoundimages, thus improving the performance of the downstream classification task.Our experimental results demonstrate the effectiveness of our deblurring MAE,achieving state-of-the-art performance in ultrasound image classification.Overall, our work highlights the potential of MAE for ultrasound imagerecognition and presents a novel approach that incorporates deblurring tofurther improve its effectiveness.</description><author>Qingbo Kang, Jun Gao, Kang Li, Qicheng Lao</author><pubDate>Tue, 11 Jul 2023 07:26:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08249v2</guid></item><item><title>Number Systems for Deep Neural Network Architectures: A Survey</title><link>http://arxiv.org/abs/2307.05035v1</link><description>Deep neural networks (DNNs) have become an enabling component for a myriad ofartificial intelligence applications. DNNs have shown sometimes superiorperformance, even compared to humans, in cases such as self-driving, healthapplications, etc. Because of their computational complexity, deploying DNNs inresource-constrained devices still faces many challenges related to computingcomplexity, energy efficiency, latency, and cost. To this end, several researchdirections are being pursued by both academia and industry to accelerate andefficiently implement DNNs. One important direction is determining theappropriate data representation for the massive amount of data involved in DNNprocessing. Using conventional number systems has been found to be sub-optimalfor DNNs. Alternatively, a great body of research focuses on exploring suitablenumber systems. This article aims to provide a comprehensive survey anddiscussion about alternative number systems for more efficient representationsof DNN data. Various number systems (conventional/unconventional) exploited forDNNs are discussed. The impact of these number systems on the performance andhardware design of DNNs is considered. In addition, this paper highlights thechallenges associated with each number system and various solutions that areproposed for addressing them. The reader will be able to understand theimportance of an efficient number system for DNN, learn about the widely usednumber systems for DNN, understand the trade-offs between various numbersystems, and consider various design aspects that affect the impact of numbersystems on DNN performance. In addition, the recent trends and related researchopportunities will be highlighted</description><author>Ghada Alsuhli, Vasileios Sakellariou, Hani Saleh, Mahmoud Al-Qutayri, Baker Mohammad, Thanos Stouraitis</author><pubDate>Tue, 11 Jul 2023 07:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05035v1</guid></item><item><title>Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference</title><link>http://arxiv.org/abs/2307.05034v1</link><description>We introduce a synthetic dataset called Sentences Involving ComplexCompositional Knowledge (SICCK) and a novel analysis that investigates theperformance of Natural Language Inference (NLI) models to understandcompositionality in logic. We produce 1,304 sentence pairs by modifying 15examples from the SICK dataset (Marelli et al., 2014). To this end, we modifythe original texts using a set of phrases - modifiers that correspond touniversal quantifiers, existential quantifiers, negation, and other conceptmodifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases tomodify the subject, verb, and object parts of the premise and hypothesis.Lastly, we annotate these modified texts with the corresponding entailmentlabels following NL rules. We conduct a preliminary verification of how wellthe change in the structural and semantic composition is captured by neural NLImodels, in both zero-shot and fine-tuned scenarios. We found that theperformance of NLI models under the zero-shot setting is poor, especially formodified sentences with negation and existential quantifiers. After fine-tuningthis dataset, we observe that models continue to perform poorly over negation,existential and universal modifiers.</description><author>Sushma Anand Akoju, Robert Vacareanu, Haris Riaz, Eduardo Blanco, Mihai Surdeanu</author><pubDate>Tue, 11 Jul 2023 07:18:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05034v1</guid></item><item><title>Towards Anytime Optical Flow Estimation with Event Cameras</title><link>http://arxiv.org/abs/2307.05033v1</link><description>Event cameras are capable of responding to log-brightness changes inmicroseconds. Its characteristic of producing responses only to the changingregion is particularly suitable for optical flow estimation. In contrast to thesuper low-latency response speed of event cameras, existing datasets collectedvia event cameras, however, only provide limited frame rate optical flow groundtruth, (e.g., at 10Hz), greatly restricting the potential of event-drivenoptical flow. To address this challenge, we put forward a high-frame-rate,low-latency event representation Unified Voxel Grid, sequentially fed into thenetwork bin by bin. We then propose EVA-Flow, an EVent-based Anytime Flowestimation network to produce high-frame-rate event optical flow with onlylow-frame-rate optical flow ground truth for supervision. The key component ofour EVA-Flow is the stacked Spatiotemporal Motion Refinement (SMR) module,which predicts temporally-dense optical flow and enhances the accuracy viaspatial-temporal motion refinement. The time-dense feature warping utilized inthe SMR module provides implicit supervision for the intermediate optical flow.Additionally, we introduce the Rectified Flow Warp Loss (RFWL) for theunsupervised evaluation of intermediate optical flow in the absence of groundtruth. This is, to the best of our knowledge, the first work focusing onanytime optical flow estimation via event cameras. A comprehensive variety ofexperiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flowachieves competitive performance, super-low-latency (5ms), fastest inference(9.2ms), time-dense motion estimation (200Hz), and strong generalization. Ourcode will be available at https://github.com/Yaozhuwa/EVA-Flow.</description><author>Yaozu Ye, Hao Shi, Kailun Yang, Ze Wang, Xiaoting Yin, Yaonan Wang, Kaiwei Wang</author><pubDate>Tue, 11 Jul 2023 07:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05033v1</guid></item><item><title>FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms</title><link>http://arxiv.org/abs/2307.05029v1</link><description>This thesis explores open-sourced machine learning (ML) model explanationtools to understand whether these tools can allow a layman to visualize,understand, and suggest intuitive remedies to unfairness in ML-baseddecision-support systems. Machine learning models trained on datasets biasedagainst minority groups are increasingly used to guide life-altering socialdecisions, prompting the urgent need to study their logic for unfairness. Dueto this problem's impact on vast populations of the general public, it iscritical for the layperson -- not just subject matter experts in social justiceor machine learning experts -- to understand the nature of unfairness withinthese algorithms and the potential trade-offs. Existing research on fairness inmachine learning focuses mostly on the mathematical definitions and tools tounderstand and remedy unfair models, with some directly citing user-interactivetools as necessary for future work. This thesis presents FairLay-ML, aproof-of-concept GUI integrating some of the most promising tools to provideintuitive explanations for unfair logic in ML models by integrating existingresearch tools (e.g. Local Interpretable Model-Agnostic Explanations) withexisting ML-focused GUI (e.g. Python Streamlit). We test FairLay-ML usingmodels of various accuracy and fairness generated by an unfairness detectortool, Parfait-ML, and validate our results using Themis. Our study finds thatthe technology stack used for FairLay-ML makes it easy to install and providesreal-time black-box explanations of pre-trained models to users. Furthermore,the explanations provided translate to actionable remedies.</description><author>Normen Yu, Gang Tan, Saeid Tizpaz-Niari</author><pubDate>Tue, 11 Jul 2023 07:05:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05029v1</guid></item><item><title>Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain</title><link>http://arxiv.org/abs/2302.14302v2</link><description>Deep neural networks (DNNs) may suffer from significantly degeneratedperformance when the training and test data are of different underlyingdistributions. Despite the importance of model generalization toout-of-distribution (OOD) data, the accuracy of state-of-the-art (SOTA) modelson OOD data can plummet. Recent work has demonstrated that regular oroff-manifold adversarial examples, as a special case of data augmentation, canbe used to improve OOD generalization. Inspired by this, we theoretically provethat on-manifold adversarial examples can better benefit OOD generalization.Nevertheless, it is nontrivial to generate on-manifold adversarial examplesbecause the real manifold is generally complex. To address this issue, weproposed a novel method of Augmenting data with Adversarial examples via aWavelet module (AdvWavAug), an on-manifold adversarial data augmentationtechnique that is simple to implement. In particular, we project a benign imageinto a wavelet domain. With the assistance of the sparsity characteristic ofwavelet transformation, we can modify an image on the estimated data manifold.We conduct adversarial augmentation based on AdvProp training framework.Extensive experiments on different models and different datasets, includingImageNet and its distorted versions, demonstrate that our method can improvemodel generalization, especially on OOD data. By integrating AdvWavAug into thetraining process, we have achieved SOTA results on some recenttransformer-based models.</description><author>Chang Liu, Wenzhao Xiang, Yuan He, Hui Xue, Shibao Zheng, Hang Su</author><pubDate>Tue, 11 Jul 2023 07:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14302v2</guid></item><item><title>Action-based Early Autism Diagnosis Using Contrastive Feature Learning</title><link>http://arxiv.org/abs/2209.05379v3</link><description>Autism, also known as Autism Spectrum Disorder (or ASD), is a neurologicaldisorder. Its main symptoms include difficulty in (verbal and/or non-verbal)communication, and rigid/repetitive behavior. These symptoms are oftenindistinguishable from a normal (control) individual, due to which thisdisorder remains undiagnosed in early childhood leading to delayed treatment.Since the learning curve is steep during the initial age, an early diagnosis ofautism could allow to take adequate interventions at the right time, whichmight positively affect the growth of an autistic child. Further, thetraditional methods of autism diagnosis require multiple visits to aspecialized psychiatrist, however this process can be time-consuming. In thispaper, we present a learning based approach to automate autism diagnosis usingsimple and small action video clips of subjects. This task is particularlychallenging because the amount of annotated data available is small, and thevariations among samples from the two categories (ASD and control) aregenerally indistinguishable. This is also evident from poor performance of abinary classifier learned using the cross-entropy loss on top of a baselineencoder. To address this, we adopt contrastive feature learning in both selfsupervised and supervised learning frameworks, and show that these can lead toa significant increase in the prediction accuracy of a binary classifier onthis task. We further validate this by conducting thorough experimentalanalyses under different set-ups on two publicly available datasets.</description><author>Asha Rani, Pankaj Yadav, Yashaswi Verma</author><pubDate>Tue, 11 Jul 2023 06:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05379v3</guid></item><item><title>Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels</title><link>http://arxiv.org/abs/2307.05025v1</link><description>In recent years, research on learning with noisy labels has focused ondevising novel algorithms that can achieve robustness to noisy training labelswhile generalizing to clean data. These algorithms often incorporatesophisticated techniques, such as noise modeling, label correction, andco-training. In this study, we demonstrate that a simple baseline usingcross-entropy loss, combined with widely used regularization strategies likelearning rate decay, model weights average, and data augmentations, canoutperform state-of-the-art methods. Our findings suggest that employing acombination of regularization strategies can be more effective than intricatealgorithms in tackling the challenges of learning with noisy labels. While someof these regularization strategies have been utilized in previous noisy labellearning research, their full potential has not been thoroughly explored. Ourresults encourage a reevaluation of benchmarks for learning with noisy labelsand prompt reconsideration of the role of specialized learning algorithmsdesigned for training with noisy labels.</description><author>Hui Kang, Sheng Liu, Huaxi Huang, Jun Yu, Bo Han, Dadong Wang, Tongliang Liu</author><pubDate>Tue, 11 Jul 2023 06:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05025v1</guid></item><item><title>Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN</title><link>http://arxiv.org/abs/2011.12177v4</link><description>We present a novel algorithm to detect double nuclei galaxies (DNG) calledGOTHIC (Graph BOosted iterated HIll Climbing) - that detects whether a givenimage of a galaxy has two or more closely separated nuclei. Our aim is todetect samples of dual or multiple active galactic nuclei (AGN) in galaxies.Although galaxy mergers are common, the detection of dual AGN is rare. Theirdetection is very important as they help us understand the formation ofsupermassive black hole (SMBH) binaries, SMBH growth and AGN feedback effectsin multiple nuclei systems. There is thus a need for an algorithm to do asystematic survey of existing imaging data for the discovery of DNGs and dualAGN. We have tested GOTHIC on a known sample of DNGs and subsequently appliedit to a sample of a million SDSS DR16 galaxies lying in the redshift range of 0to 0.75 approximately, and have available spectroscopic data. We have detected159 dual AGN in this sample, of which 2 are triple AGN systems. Our resultsshow that dual AGN are not common, and triple AGN even rarer. The color (u-r)magnitude plots of the DNGs indicate that star formation is quenched as thenuclei come closer and as the AGN fraction increases. The quenching isespecially prominent for dual/triple AGN galaxies that lie in the extreme endof the red sequence.</description><author>Anwesh Bhattacharya, Nehal C. P., Mousumi Das, Abhishek Paswan, Snehanshu Saha, Francoise Combes</author><pubDate>Tue, 11 Jul 2023 06:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.12177v4</guid></item><item><title>Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification</title><link>http://arxiv.org/abs/2307.05017v1</link><description>Decisions made by convolutional neural networks(CNN) can be understood andexplained by visualizing discriminative regions on images. To this end, ClassActivation Map (CAM) based methods were proposed as powerful interpretationtools, making the prediction of deep learning models more explainable,transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM,Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models withfully-connected (FC) layers as a classifier. It is worth noting that many deeplearning models classify images without FC layers, e.g., few-shot learningimage classification, contrastive learning image classification, and imageretrieval tasks. In this work, a post-hoc interpretation tool named featureactivation map (FAM) is proposed, which can interpret deep learning modelswithout FC layers as a classifier. In the proposed FAM algorithm, thechannel-wise contribution weights are derived from the similarity scoresbetween two image embeddings. The activation maps are linearly combined withthe corresponding normalized contribution weights, forming the explanation mapfor visualization. The quantitative and qualitative experiments conducted onten deep learning models for few-shot image classification, contrastivelearning image classification and image retrieval tasks demonstrate theeffectiveness of the proposed FAM algorithm.</description><author>Yi Liao, Yongsheng Gao, Weichuan Zhang</author><pubDate>Tue, 11 Jul 2023 06:33:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05017v1</guid></item><item><title>TRansPose: Large-Scale Multispectral Dataset for Transparent Object</title><link>http://arxiv.org/abs/2307.05016v1</link><description>Transparent objects are encountered frequently in our daily lives, yetrecognizing them poses challenges for conventional vision sensors due to theirunique material properties, not being well perceived from RGB or depth cameras.Overcoming this limitation, thermal infrared cameras have emerged as asolution, offering improved visibility and shape information for transparentobjects. In this paper, we present TRansPose, the first large-scalemultispectral dataset that combines stereo RGB-D, thermal infrared (TIR)images, and object poses to promote transparent object research. The datasetincludes 99 transparent objects, encompassing 43 household items, 27 recyclabletrashes, 29 chemical laboratory equivalents, and 12 non-transparent objects. Itcomprises a vast collection of 333,819 images and 4,000,056 annotations,providing instance-level segmentation masks, ground-truth poses, and completeddepth information. The data was acquired using a FLIR A65 thermal infrared(TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Pandarobot manipulator. Spanning 87 sequences, TRansPose covers various challengingreal-life scenarios, including objects filled with water, diverse lightingconditions, heavy clutter, non-transparent or translucent containers, objectsin plastic bags, and multi-stacked objects. TRansPose dataset can be accessedfrom the following link: https://sites.google.com/view/transpose-dataset</description><author>Jeongyun Kim, Myung-Hwan Jeon, Sangwoo Jung, Wooseong Yang, Minwoo Jung, Jaeho Shin, Ayoung Kim</author><pubDate>Tue, 11 Jul 2023 06:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05016v1</guid></item><item><title>Test-Time Training on Video Streams</title><link>http://arxiv.org/abs/2307.05014v1</link><description>Prior work has established test-time training (TTT) as a general framework tofurther improve a trained model at test time. Before making a prediction oneach test instance, the model is trained on the same instance using aself-supervised task, such as image reconstruction with masked autoencoders. Weextend TTT to the streaming setting, where multiple test instances - videoframes in our case - arrive in temporal order. Our extension is online TTT: Thecurrent model is initialized from the previous model, then trained on thecurrent frame and a small window of frames immediately before. Online TTTsignificantly outperforms the fixed-model baseline for four tasks, on threereal-world datasets. The relative improvement is 45% and 66% for instance andpanoptic segmentation. Surprisingly, online TTT also outperforms its offlinevariant that accesses more information, training on all frames from the entiretest video regardless of temporal order. This differs from previous findingsusing synthetic videos. We conceptualize locality as the advantage of onlineover offline TTT. We analyze the role of locality with ablations and a theorybased on bias-variance trade-off.</description><author>Renhao Wang, Yu Sun, Yossi Gandelsman, Xinlei Chen, Alexei A. Efros, Xiaolong Wang</author><pubDate>Tue, 11 Jul 2023 06:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05014v1</guid></item><item><title>Explanation Regeneration via Information Bottleneck</title><link>http://arxiv.org/abs/2212.09603v2</link><description>Explaining the black-box predictions of NLP models naturally and accuratelyis an important open problem in natural language generation. These free-textexplanations are expected to contain sufficient and carefully-selected evidenceto form supportive arguments for predictions. Due to the superior generativecapacity of large pretrained language models, recent work built on promptengineering enables explanation generation without specific training. However,explanation generated through single-pass prompting often lacks sufficiency andconciseness. To address this problem, we develop an information bottleneckmethod EIB to produce refined explanations that are sufficient and concise. Ourapproach regenerates the free-text explanation by polishing the single-passoutput from the pretrained language model but retaining the information thatsupports the contents being explained. Experiments on two out-of-domain tasksverify the effectiveness of EIB through automatic evaluation andthoroughly-conducted human evaluation.</description><author>Qintong Li, Zhiyong Wu, Lingpeng Kong, Wei Bi</author><pubDate>Tue, 11 Jul 2023 06:17:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09603v2</guid></item><item><title>Referring Camouflaged Object Detection</title><link>http://arxiv.org/abs/2306.07532v2</link><description>We consider the problem of referring camouflaged object detection (Ref-COD),a new task that aims to segment specified camouflaged objects based on a smallset of referring images with salient target objects. We first assemble alarge-scale dataset, called R2C7K, which consists of 7K images covering 64object categories in real-world scenarios. Then, we develop a simple but strongdual-branch framework, dubbed R2CNet, with a reference branch embedding thecommon representations of target objects from referring images and asegmentation branch identifying and segmenting camouflaged objects under theguidance of the common representations. In particular, we design a ReferringMask Generation module to generate pixel-level prior mask and a ReferringFeature Enrichment module to enhance the capability of identifying specifiedcamouflaged objects. Extensive experiments show the superiority of our Ref-CODmethods over their COD counterparts in segmenting specified camouflaged objectsand identifying the main body of target objects. Our code and dataset arepublicly available at https://github.com/zhangxuying1004/RefCOD.</description><author>Xuying Zhang, Bowen Yin, Zheng Lin, Qibin Hou, Deng-Ping Fan, Ming-Ming Cheng</author><pubDate>Tue, 11 Jul 2023 06:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07532v2</guid></item><item><title>Improving RNN-Transducers with Acoustic LookAhead</title><link>http://arxiv.org/abs/2307.05006v1</link><description>RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-endmodel for speech to text conversion because of their high accuracy andstreaming capabilities. A typical RNN-T independently encodes the input audioand the text context, and combines the two encodings by a thin joint network.While this architecture provides SOTA streaming accuracy, it also makes themodel vulnerable to strong LM biasing which manifests as multi-stephallucination of text without acoustic evidence. In this paper we proposeLookAhead that makes text representations more acoustically grounded by lookingahead into the future within the audio input. This technique yields asignificant 5%-20% relative reduction in word error rate on both in-domain andout-of-domain evaluation sets.</description><author>Vinit S. Unni, Ashish Mittal, Preethi Jyothi, Sunita Sarawagi</author><pubDate>Tue, 11 Jul 2023 04:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05006v1</guid></item><item><title>Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2307.05004v1</link><description>This paper proposes a generative probabilistic model integrating emergentcommunication and multi-agent reinforcement learning. The agents plan theiractions by probabilistic inference, called control as inference, andcommunicate using messages that are latent variables and estimated based on theplanned actions. Through these messages, each agent can send information aboutits actions and know information about the actions of another agent. Therefore,the agents change their actions according to the estimated messages to achievecooperative tasks. This inference of messages can be considered ascommunication, and this procedure can be formulated by the Metropolis-Hastingnaming game. Through experiments in the grid world environment, we show thatthe proposed PGM can infer meaningful messages to achieve the cooperative task.</description><author>Tomoaki Nakamura, Akira Taniguchi, Tadahiro Taniguchi</author><pubDate>Tue, 11 Jul 2023 04:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05004v1</guid></item><item><title>Neural Point-based Volumetric Avatar: Surface-guided Neural Points for Efficient and Photorealistic Volumetric Head Avatar</title><link>http://arxiv.org/abs/2307.05000v1</link><description>Rendering photorealistic and dynamically moving human heads is crucial forensuring a pleasant and immersive experience in AR/VR and video conferencingapplications. However, existing methods often struggle to model challengingfacial regions (e.g., mouth interior, eyes, hair/beard), resulting inunrealistic and blurry results. In this paper, we propose {\fullname}({\name}), a method that adopts the neural point representation as well as theneural volume rendering process and discards the predefined connectivity andhard correspondence imposed by mesh-based approaches. Specifically, the neuralpoints are strategically constrained around the surface of the targetexpression via a high-resolution UV displacement map, achieving increasedmodeling capacity and more accurate control. We introduce three technicalinnovations to improve the rendering and training efficiency: a patch-wisedepth-guided (shading point) sampling strategy, a lightweight radiance decodingprocess, and a Grid-Error-Patch (GEP) ray sampling strategy during training. Bydesign, our {\name} is better equipped to handle topologically changing regionsand thin structures while also ensuring accurate expression control whenanimating avatars. Experiments conducted on three subjects from the Multifacedataset demonstrate the effectiveness of our designs, outperforming previousstate-of-the-art methods, especially in handling challenging facial regions.</description><author>Cong Wang, Di Kang, Yanpei Cao, Linchao Bao, Ying Shan, Song-Hai Zhang</author><pubDate>Tue, 11 Jul 2023 04:40:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05000v1</guid></item><item><title>Selective Sampling and Imitation Learning via Online Regression</title><link>http://arxiv.org/abs/2307.04998v1</link><description>We consider the problem of Imitation Learning (IL) by actively querying noisyexpert for feedback. While imitation learning has been empirically successful,much of prior work assumes access to noiseless expert feedback which is notpractical in many applications. In fact, when one only has access to noisyexpert feedback, algorithms that rely on purely offline data (non-interactiveIL) can be shown to need a prohibitively large number of samples to besuccessful. In contrast, in this work, we provide an interactive algorithm forIL that uses selective sampling to actively query the noisy expert forfeedback. Our contributions are twofold: First, we provide a new selectivesampling algorithm that works with general function classes and multipleactions, and obtains the best-known bounds for the regret and the number ofqueries. Next, we extend this analysis to the problem of IL with noisy expertfeedback and provide a new IL algorithm that makes limited queries. Our algorithm for selective sampling leverages function approximation, andrelies on an online regression oracle w.r.t.~the given model class to predictactions, and to decide whether to query the expert for its label. On thetheoretical side, the regret bound of our algorithm is upper bounded by theregret of the online regression oracle, while the query complexity additionallydepends on the eluder dimension of the model class. We complement this with alower bound that demonstrates that our results are tight. We extend ourselective sampling algorithm for IL with general function approximation andprovide bounds on both the regret and the number of queries made to the noisyexpert. A key novelty here is that our regret and query complexity bounds onlydepend on the number of times the optimal policy (and not the noisy expert, orthe learner) go to states that have a small margin.</description><author>Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</author><pubDate>Tue, 11 Jul 2023 04:32:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04998v1</guid></item><item><title>BTPK-based interpretable method for NER tasks based on Talmudic Public Announcement Logic</title><link>http://arxiv.org/abs/2201.09523v2</link><description>As one of the basic tasks in natural language processing (NLP), named entityrecognition (NER) is an important basic tool for downstream tasks of NLP, suchas information extraction, syntactic analysis, machine translation and so on.The internal operation logic of current name entity recognition model isblack-box to the user, so the user has no basis to determine which name entitymakes more sense. Therefore, a user-friendly explainable recognition processwould be very useful for many people. In this paper, we propose a novelinterpretable method, BTPK (Binary Talmudic Public Announcement Logic model),to help users understand the internal recognition logic of the name entityrecognition tasks based on Talmudic Public Announcement Logic. BTPK model canalso capture the semantic information in the input sentences, that is, thecontext dependency of the sentence. We observed the public announcement of BTPKpresents the inner decision logic of BRNNs, and the explanations obtained froma BTPK model show us how BRNNs essentially handle NER tasks.</description><author>Yulin Chen, Beishui Liao, Bruno Bentzen, Bo Yuan, Zelai Yao, Haixiao Chi, Dov Gabbay</author><pubDate>Tue, 11 Jul 2023 04:26:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.09523v2</guid></item><item><title>Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning</title><link>http://arxiv.org/abs/2307.04996v1</link><description>Personalized recommendations have a growing importance in direct marketing,which motivates research to enhance customer experiences by knowledge graph(KG) applications. For example, in financial services, companies may benefitfrom providing relevant financial articles to their customers to cultivaterelationships, foster client engagement and promote informed financialdecisions. While several approaches center on KG-based recommender systems forimproved content, in this study we focus on interpretable KG-based recommendersystems for decision making.To this end, we present two knowledge graph-basedapproaches for personalized article recommendations for a set of customers of alarge multinational financial services company. The first approach employsReinforcement Learning and the second approach uses the XGBoost algorithm forrecommending articles to the customers. Both approaches make use of a KGgenerated from both structured (tabular data) and unstructured data (a largebody of text data).Using the Reinforcement Learning-based recommender system wecould leverage the graph traversal path leading to the recommendation as a wayto generate interpretations (Path Directed Reasoning (PDR)). In theXGBoost-based approach, one can also provide explainable results using post-hocmethods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like Iam Five).Importantly, our approach offers explainable results, promoting betterdecision-making. This study underscores the potential of combining advancedmachine learning techniques with KG-driven insights to bolster experience incustomer relationship management.</description><author>Ghanshyam Verma, Shovon Sengupta, Simon Simanta, Huan Chen, Janos A. Perge, Devishree Pillai, John P. McCrae, Paul Buitelaar</author><pubDate>Tue, 11 Jul 2023 04:24:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04996v1</guid></item></channel></rss>