<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 26 Oct 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views</title><link>http://arxiv.org/abs/2410.18979v1</link><description>We propose PixelGaussian, an efficient feed-forward framework for learninggeneralizable 3D Gaussian reconstruction from arbitrary views. Most existingmethods rely on uniform pixel-wise Gaussian representations, which learn afixed number of 3D Gaussians for each view and cannot generalize well to moreinput views. Differently, our PixelGaussian dynamically adapts both theGaussian distribution and quantity based on geometric complexity, leading tomore efficient representations and significant improvements in reconstructionquality. Specifically, we introduce a Cascade Gaussian Adapter to adjustGaussian distribution according to local geometry complexity identified by akeypoint scorer. CGA leverages deformable attention in context-awarehypernetworks to guide Gaussian pruning and splitting, ensuring accuraterepresentation in complex regions while reducing redundancy. Furthermore, wedesign a transformer-based Iterative Gaussian Refiner module that refinesGaussian representations through direct image-Gaussian interactions. OurPixelGaussian can effectively reduce Gaussian redundancy as input viewsincrease. We conduct extensive experiments on the large-scale ACID andRealEstate10K datasets, where our method achieves state-of-the-art performancewith good generalization to various numbers of views. Code:https://github.com/Barrybarry-Smith/PixelGaussian.</description><author>Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, Jiwen Lu</author><pubDate>Thu, 24 Oct 2024 17:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18979v1</guid></item><item><title>Framer: Interactive Frame Interpolation</title><link>http://arxiv.org/abs/2410.18978v1</link><description>We propose Framer for interactive frame interpolation, which targetsproducing smoothly transitioning frames between two images as per usercreativity. Concretely, besides taking the start and end frames as inputs, ourapproach supports customizing the transition process by tailoring thetrajectory of some selected keypoints. Such a design enjoys two clear benefits.First, incorporating human interaction mitigates the issue arising fromnumerous possibilities of transforming one image to another, and in turnenables finer control of local motions. Second, as the most basic form ofinteraction, keypoints help establish the correspondence across frames,enhancing the model to handle challenging cases (e.g., objects on the start andend frames are of different shapes and styles). It is noteworthy that oursystem also offers an "autopilot" mode, where we introduce a module to estimatethe keypoints and refine the trajectory automatically, to simplify the usage inpractice. Extensive experimental results demonstrate the appealing performanceof Framer on various applications, such as image morphing, time-lapse videogeneration, cartoon interpolation, etc. The code, the model, and the interfacewill be released to facilitate further research.</description><author>Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, Chunhua Shen</author><pubDate>Thu, 24 Oct 2024 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18978v1</guid></item><item><title>MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms</title><link>http://arxiv.org/abs/2410.18977v1</link><description>This research delves into the problem of interactive editing of human motiongeneration. Previous motion diffusion models lack explicit modeling of theword-level text-motion correspondence and good explainability, hencerestricting their fine-grained editing ability. To address this issue, wepropose an attention-based motion diffusion model, namely MotionCLR, with CLeaRmodeling of attention mechanisms. Technically, MotionCLR models the in-modalityand cross-modality interactions with self-attention and cross-attention,respectively. More specifically, the self-attention mechanism aims to measurethe sequential similarity between frames and impacts the order of motionfeatures. By contrast, the cross-attention mechanism works to find thefine-grained word-sequence correspondence and activate the correspondingtimesteps in the motion sequence. Based on these key properties, we develop aversatile set of simple yet effective motion editing methods via manipulatingattention maps, such as motion (de-)emphasizing, in-place motion replacement,and example-based motion generation, etc. For further verification of theexplainability of the attention mechanism, we additionally explore thepotential of action-counting and grounded motion generation ability viaattention maps. Our experimental results show that our method enjoys goodgeneration and editing ability with good explainability.</description><author>Ling-Hao Chen, Wenxun Dai, Xuan Ju, Shunlin Lu, Lei Zhang</author><pubDate>Thu, 24 Oct 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18977v1</guid></item><item><title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title><link>http://arxiv.org/abs/2410.18976v1</link><description>Recent years have witnessed a significant interest in developing largemultimodal models (LMMs) capable of performing various visual reasoning andunderstanding tasks. This has led to the introduction of multiple LMMbenchmarks to evaluate LMMs on different tasks. However, most existing LMMevaluation benchmarks are predominantly English-centric. In this work, wedevelop a comprehensive LMM evaluation benchmark for the Arabic language torepresent a large population of over 400 million speakers. The proposedbenchmark, named CAMEL-Bench, comprises eight diverse domains and 38sub-domains including, multi-image understanding, complex visual perception,handwritten document understanding, video understanding, medical imaging, plantdiseases, and remote sensing-based land use understanding to evaluate broadscenario generalizability. Our CAMEL-Bench comprises around 29,036 questionsthat are filtered from a larger pool of samples, where the quality is manuallyverified by native speakers to ensure reliable model assessment. We conductevaluations of both closed-source, including GPT-4 series, and open-sourceLMMs. Our analysis reveals the need for substantial improvement, especiallyamong the best open-source models, with even the closed-source GPT-4o achievingan overall score of 62%. Our benchmark and evaluation scripts are open-sourced.</description><author>Sara Ghaboura, Ahmed Heakl, Omkar Thawakar, Ali Alharthi, Ines Riahi, Abduljalil Saif, Jorma Laaksonen, Fahad S. Khan, Salman Khan, Rao M. Anwer</author><pubDate>Thu, 24 Oct 2024 17:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18976v1</guid></item><item><title>Unbounded: A Generative Infinite Game of Character Life Simulation</title><link>http://arxiv.org/abs/2410.18975v1</link><description>We introduce the concept of a generative infinite game, a video game thattranscends the traditional boundaries of finite, hard-coded systems by usinggenerative models. Inspired by James P. Carse's distinction between finite andinfinite games, we leverage recent advances in generative AI to createUnbounded: a game of character life simulation that is fully encapsulated ingenerative models. Specifically, Unbounded draws inspiration from sandbox lifesimulations and allows you to interact with your autonomous virtual characterin a virtual world by feeding, playing with and guiding it - with open-endedmechanics generated by an LLM, some of which can be emergent. In order todevelop Unbounded, we propose technical innovations in both the LLM and visualgeneration domains. Specifically, we present: (1) a specialized, distilledlarge language model (LLM) that dynamically generates game mechanics,narratives, and character interactions in real-time, and (2) a new dynamicregional image prompt Adapter (IP-Adapter) for vision models that ensuresconsistent yet flexible visual generation of a character across multipleenvironments. We evaluate our system through both qualitative and quantitativeanalysis, showing significant improvements in character life simulation, userinstruction following, narrative coherence, and visual consistency for bothcharacters and the environments compared to traditional related approaches.</description><author>Jialu Li, Yuanzhen Li, Neal Wadhwa, Yael Pritch, David E. Jacobs, Michael Rubinstein, Mohit Bansal, Nataniel Ruiz</author><pubDate>Thu, 24 Oct 2024 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18975v1</guid></item><item><title>3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation</title><link>http://arxiv.org/abs/2410.18974v1</link><description>Multi-view image diffusion models have significantly advanced open-domain 3Dobject generation. However, most existing models rely on 2D networkarchitectures that lack inherent 3D biases, resulting in compromised geometricconsistency. To address this challenge, we introduce 3D-Adapter, a plug-inmodule designed to infuse 3D geometry awareness into pretrained image diffusionmodels. Central to our approach is the idea of 3D feedback augmentation: foreach denoising step in the sampling loop, 3D-Adapter decodes intermediatemulti-view features into a coherent 3D representation, then re-encodes therendered RGBD views to augment the pretrained base model through featureaddition. We study two variants of 3D-Adapter: a fast feed-forward versionbased on Gaussian splatting and a versatile training-free version utilizingneural fields and meshes. Our extensive experiments demonstrate that 3D-Adapternot only greatly enhances the geometry quality of text-to-multi-view modelssuch as Instant3D and Zero123++, but also enables high-quality 3D generationusing the plain text-to-image Stable Diffusion. Furthermore, we showcase thebroad application potential of 3D-Adapter by presenting high quality results intext-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.</description><author>Hansheng Chen, Bokui Shen, Yulin Liu, Ruoxi Shi, Linqi Zhou, Connor Z. Lin, Jiayuan Gu, Hao Su, Gordon Wetzstein, Leonidas Guibas</author><pubDate>Thu, 24 Oct 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18974v1</guid></item><item><title>Tuning-free coreset Markov chain Monte Carlo</title><link>http://arxiv.org/abs/2410.18973v1</link><description>A Bayesian coreset is a small, weighted subset of a data set that replacesthe full data during inference to reduce computational cost. Thestate-of-the-art coreset construction algorithm, Coreset Markov chain MonteCarlo (Coreset MCMC), uses draws from an adaptive Markov chain targeting thecoreset posterior to train the coreset weights via stochastic gradientoptimization. However, the quality of the constructed coreset, and thus thequality of its posterior approximation, is sensitive to the stochasticoptimization learning rate. In this work, we propose a learning-rate-freestochastic gradient optimization procedure, Hot-start Distance over Gradient(Hot DoG), for training coreset weights in Coreset MCMC without user tuningeffort. Empirical results demonstrate that Hot DoG provides higher qualityposterior approximations than other learning-rate-free stochastic gradientmethods, and performs competitively to optimally-tuned ADAM.</description><author>Naitong Chen, Jonathan H. Huggins, Trevor Campbell</author><pubDate>Thu, 24 Oct 2024 17:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18973v1</guid></item><item><title>Deep Insights into Cognitive Decline: A Survey of Leveraging Non-Intrusive Modalities with Deep Learning Techniques</title><link>http://arxiv.org/abs/2410.18972v1</link><description>Cognitive decline is a natural part of aging, often resulting in reducedcognitive abilities. In some cases, however, this decline is more pronounced,typically due to disorders such as Alzheimer's disease. Early detection ofanomalous cognitive decline is crucial, as it can facilitate timelyprofessional intervention. While medical data can help in this detection, itoften involves invasive procedures. An alternative approach is to employnon-intrusive techniques such as speech or handwriting analysis, which do notnecessarily affect daily activities. This survey reviews the most relevantmethodologies that use deep learning techniques to automate the cognitivedecline estimation task, including audio, text, and visual processing. Wediscuss the key features and advantages of each modality and methodology,including state-of-the-art approaches like Transformer architecture andfoundation models. In addition, we present works that integrate differentmodalities to develop multimodal models. We also highlight the most significantdatasets and the quantitative results from studies using these resources. Fromthis review, several conclusions emerge. In most cases, the textual modalityachieves the best results and is the most relevant for detecting cognitivedecline. Moreover, combining various approaches from individual modalities intoa multimodal model consistently enhances performance across nearly allscenarios.</description><author>David Ortiz-Perez, Manuel Benavent-Lledo, Jose Garcia-Rodriguez, David Tomás, M. Flores Vizcaya-Moreno</author><pubDate>Thu, 24 Oct 2024 17:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18972v1</guid></item><item><title>RET-LLM: Towards a General Read-Write Memory for Large Language Models</title><link>http://arxiv.org/abs/2305.14322v2</link><description>Large language models (LLMs) have significantly advanced the field of naturallanguage processing (NLP) through their extensive parameters and comprehensivedata utilization. However, existing LLMs lack a dedicated memory unit, limitingtheir ability to explicitly store and retrieve knowledge for various tasks. Inthis paper, we propose RET-LLM a novel framework that equips LLMs with ageneral write-read memory unit, allowing them to extract, store, and recallknowledge from the text as needed for task performance. Inspired by Davidsoniansemantics theory, we extract and save knowledge in the form of triplets. Thememory unit is designed to be scalable, aggregatable, updatable, andinterpretable. Through qualitative evaluations, we demonstrate the superiorityof our proposed framework over baseline approaches in question answering tasks.Moreover, our framework exhibits robust performance in handling temporal-basedquestion answering tasks, showcasing its ability to effectively managetime-dependent information.</description><author>Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schütze</author><pubDate>Thu, 24 Oct 2024 17:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14322v2</guid></item><item><title>ConceptDrift: Uncovering Biases through the Lens of Foundational Models</title><link>http://arxiv.org/abs/2410.18970v1</link><description>Datasets and pre-trained models come with intrinsic biases. Most methods relyon spotting them by analysing misclassified samples, in a semi-automatedhuman-computer validation. In contrast, we propose ConceptDrift, a method whichanalyzes the weights of a linear probe, learned on top a foundational model. Wecapitalize on the weight update trajectory, which starts from the embedding ofthe textual representation of the class, and proceeds to drift towardsembeddings that disclose hidden biases. Different from prior work, with thisapproach we can pin-point unwanted correlations from a dataset, providing morethan just possible explanations for the wrong predictions. We empirically provethe efficacy of our method, by significantly improving zero-shot performancewith biased-augmented prompting. Our method is not bounded to a singlemodality, and we experiment in this work with both image (Waterbirds, CelebA,Nico++) and text datasets (CivilComments).</description><author>Cristian Daniel Păduraru, Antonio Bărbălau, Radu Filipescu, Andrei Liviu Nicolicioiu, Elena Burceanu</author><pubDate>Thu, 24 Oct 2024 17:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18970v1</guid></item><item><title>PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches</title><link>http://arxiv.org/abs/2410.10870v2</link><description>As large language models (LLMs) increasingly shape the AI landscape,fine-tuning pretrained models has become more popular than in the pre-LLM erafor achieving optimal performance in domain-specific tasks. However, pretrainedLLMs such as ChatGPT are periodically evolved, i.e., model parameters arefrequently updated), making it challenging for downstream users with limitedresources to keep up with fine-tuning the newest LLMs for their domainapplication. Even though fine-tuning costs have nowadays been reduced thanks tothe innovations of parameter-efficient fine-tuning such as LoRA, not alldownstream users have adequate computing for frequent personalization.Moreover, access to fine-tuning datasets, particularly in sensitive domainssuch as healthcare, could be time-restrictive, making it crucial to retain theknowledge encoded in earlier fine-tuned rounds for future adaptation. In thispaper, we present PortLLM, a training-free framework that (i) creates aninitial lightweight model update patch to capture domain-specific knowledge,and (ii) allows a subsequent seamless plugging for the continualpersonalization of evolved LLM at minimal cost. Our extensive experiments coverseven representative datasets, from easier question-answering tasks {BoolQ,SST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including{Mistral-7B, Llama2, Llama3.1, and Gemma2}, validating the portability of ourdesigned model patches and showcasing the effectiveness of our proposedframework. For instance, PortLLM achieves comparable performance to LoRAfine-tuning with reductions of up to 12.2x in GPU memory usage. Finally, weprovide theoretical justifications to understand the portability of our modelupdate patches, which offers new insights into the theoretical dimension ofLLMs' personalization.</description><author>Rana Muhammad Shahroz Khan, Pingzhi Li, Sukwon Yun, Zhenyu Wang, Shahriar Nirjon, Chau-Wai Wong, Tianlong Chen</author><pubDate>Thu, 24 Oct 2024 17:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10870v2</guid></item><item><title>Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms</title><link>http://arxiv.org/abs/2410.18967v1</link><description>Building a generalist model for user interface (UI) understanding ischallenging due to various foundational issues, such as platform diversity,resolution variation, and data limitation. In this paper, we introduceFerret-UI 2, a multimodal large language model (MLLM) designed for universal UIunderstanding across a wide range of platforms, including iPhone, Android,iPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI2 introduces three key innovations: support for multiple platform types,high-resolution perception through adaptive scaling, and advanced task trainingdata generation powered by GPT-4o with set-of-mark visual prompting. Theseadvancements enable Ferret-UI 2 to perform complex, user-centered interactions,making it highly versatile and adaptable for the expanding diversity ofplatform ecosystems. Extensive empirical experiments on referring, grounding,user-centric advanced tasks (comprising 9 subtasks $\times$ 5 platforms), GUIDEnext-action prediction dataset, and GUI-World multi-platform benchmarkdemonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and alsoshows strong cross-platform transfer capabilities.</description><author>Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff Nichols, Yinfei Yang, Zhe Gan</author><pubDate>Thu, 24 Oct 2024 17:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18967v1</guid></item><item><title>Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions</title><link>http://arxiv.org/abs/2410.18966v1</link><description>Large language models (LLMs) have demonstrated great performance acrossvarious benchmarks, showing potential as general-purpose task solvers. However,as LLMs are typically trained on vast amounts of data, a significant concern intheir evaluation is data contamination, where overlap between training data andevaluation datasets inflates performance assessments. While multiple approacheshave been developed to identify data contamination, these approaches rely onspecific assumptions that may not hold universally across different settings.To bridge this gap, we systematically review 47 papers on data contaminationdetection, categorize the underlying assumptions, and assess whether they havebeen rigorously validated. We identify and analyze eight categories ofassumptions and test three of them as case studies. Our analysis reveals thatwhen classifying instances used for pretraining LLMs, detection approachesbased on these three assumptions perform close to random guessing, suggestingthat current LLMs learn data distributions rather than memorizing individualinstances. Overall, this work underscores the importance of approaches clearlystating their underlying assumptions and testing their validity across variousscenarios.</description><author>Yujuan Fu, Ozlem Uzuner, Meliha Yetisgen, Fei Xia</author><pubDate>Thu, 24 Oct 2024 17:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18966v1</guid></item><item><title>On the Crucial Role of Initialization for Matrix Factorization</title><link>http://arxiv.org/abs/2410.18965v1</link><description>This work revisits the classical low-rank matrix factorization problem andunveils the critical role of initialization in shaping convergence rates forsuch nonconvex and nonsmooth optimization. We introduce Nystrom initialization,which significantly improves the global convergence of Scaled Gradient Descent(ScaledGD) in both symmetric and asymmetric matrix factorization tasks.Specifically, we prove that ScaledGD with Nystrom initialization achievesquadratic convergence in cases where only linear rates were previously known.Furthermore, we extend this initialization to low-rank adapters (LoRA) commonlyused for finetuning foundation models. Our approach, NoRA, i.e., LoRA withNystrom initialization, demonstrates superior performance across variousdownstream tasks and model scales, from 1B to 7B parameters, in large languageand diffusion models.</description><author>Bingcong Li, Liang Zhang, Aryan Mokhtari, Niao He</author><pubDate>Thu, 24 Oct 2024 17:58:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18965v1</guid></item><item><title>Learning to Look: Seeking Information for Decision Making via Policy Factorization</title><link>http://arxiv.org/abs/2410.18964v1</link><description>Many robot manipulation tasks require active or interactive explorationbehavior in order to be performed successfully. Such tasks are ubiquitous inembodied domains, where agents must actively search for the informationnecessary for each stage of a task, e.g., moving the head of the robot to findinformation relevant to manipulation, or in multi-robot domains, where onescout robot may search for the information that another robot needs to makeinformed decisions. We identify these tasks with a new type of problem,factorized Contextual Markov Decision Processes, and propose DISaM, adual-policy solution composed of an information-seeking policy that exploresthe environment to find the relevant contextual information and aninformation-receiving policy that exploits the context to achieve themanipulation goal. This factorization allows us to train both policiesseparately, using the information-receiving one to provide reward to train theinformation-seeking policy. At test time, the dual agent balances explorationand exploitation based on the uncertainty the manipulation policy has on whatthe next best action is. We demonstrate the capabilities of our dual policysolution in five manipulation tasks that require information-seeking behaviors,both in simulation and in the real-world, where DISaM significantly outperformsexisting methods. More information athttps://robin-lab.cs.utexas.edu/learning2look/.</description><author>Shivin Dass, Jiaheng Hu, Ben Abbatematteo, Peter Stone, Roberto Martín-Martín</author><pubDate>Thu, 24 Oct 2024 17:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18964v1</guid></item><item><title>OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning</title><link>http://arxiv.org/abs/2410.18963v1</link><description>Large language models (LLMs) and large multimodal models (LMMs) have showngreat potential in automating complex tasks like web browsing and gaming.However, their ability to generalize across diverse applications remainslimited, hindering broader utility. To address this challenge, we presentOSCAR: Operating System Control via state-Aware reasoning and Re-planning.OSCAR is a generalist agent designed to autonomously navigate and interact withvarious desktop and mobile applications through standardized controls, such asmouse and keyboard inputs, while processing screen images to fulfill usercommands. OSCAR translates human instructions into executable Python code,enabling precise control over graphical user interfaces (GUIs). To enhancestability and adaptability, OSCAR operates as a state machine, equipped witherror-handling mechanisms and dynamic task re-planning, allowing it toefficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR'seffectiveness through extensive experiments on diverse benchmarks acrossdesktop and mobile platforms, where it transforms complex workflows into simplenatural language commands, significantly boosting user productivity. Our codewill be open-source upon publication.</description><author>Xiaoqiang Wang, Bang Liu</author><pubDate>Thu, 24 Oct 2024 17:58:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18963v1</guid></item><item><title>Where Am I and What Will I See: An Auto-Regressive Model for Spatial Localization and View Prediction</title><link>http://arxiv.org/abs/2410.18962v1</link><description>Spatial intelligence is the ability of a machine to perceive, reason, and actin three dimensions within space and time. Recent advancements in large-scaleauto-regressive models have demonstrated remarkable capabilities across variousreasoning tasks. However, these models often struggle with fundamental aspectsof spatial reasoning, particularly in answering questions like "Where am I?"and "What will I see?". While some attempts have been done, existing approachestypically treat them as separate tasks, failing to capture their interconnectednature. In this paper, we present Generative Spatial Transformer (GST), a novelauto-regressive framework that jointly addresses spatial localization and viewprediction. Our model simultaneously estimates the camera pose from a singleimage and predicts the view from a new camera pose, effectively bridging thegap between spatial awareness and visual prediction. The proposed innovativecamera tokenization method enables the model to learn the joint distribution of2D projections and their corresponding spatial perspectives in anauto-regressive manner. This unified training paradigm demonstrates that jointoptimization of pose estimation and novel view synthesis leads to improvedperformance in both tasks, for the first time, highlighting the inherentrelationship between spatial awareness and visual prediction.</description><author>Junyi Chen, Di Huang, Weicai Ye, Wanli Ouyang, Tong He</author><pubDate>Thu, 24 Oct 2024 17:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18962v1</guid></item><item><title>Taming Data and Transformers for Audio Generation</title><link>http://arxiv.org/abs/2406.19388v2</link><description>Generating ambient sounds is a challenging task due to data scarcity andoften insufficient caption quality, making it difficult to employ large-scalegenerative models for the task. In this work, we tackle this problem byintroducing two new models. First, we propose AutoCap, a high-quality andefficient automatic audio captioning model. By using a compact audiorepresentation and leveraging audio metadata, AutoCap substantially enhancescaption quality, reaching a CIDEr score of 83.2, marking a 3.2% improvementfrom the best available captioning model at four times faster inference speed.Second, we propose GenAu, a scalable transformer-based audio generationarchitecture that we scale up to 1.25B parameters. Using AutoCap to generatecaption clips from existing audio datasets, we demonstrate the benefits of datascaling with synthetic captions as well as model size scaling. When compared tostate-of-the-art audio generators trained at similar size and data scale, GenAuobtains significant improvements of 4.7% in FAD score, 22.7% in IS, and 13.5%in CLAP score, indicating significantly improved quality of generated audiocompared to previous works. Moreover, we propose an efficient and scalablepipeline for collecting audio datasets, enabling us to compile 57M ambientaudio clips, forming AutoReCap-XL, the largest available audio-text dataset, at90 times the scale of existing ones. Our code, model checkpoints, and datasetare publicly available.</description><author>Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, Vicente Ordonez</author><pubDate>Thu, 24 Oct 2024 17:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19388v2</guid></item><item><title>Scaling Law with Learning Rate Annealing</title><link>http://arxiv.org/abs/2408.11029v2</link><description>We find that the cross-entropy loss curves of neural language modelsempirically adhere to a scaling law with learning rate (LR) annealing overtraining steps: $$L(s) = L_0 + A\cdot S_1^{-\alpha} - C\cdot S_2,$$ where$L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LRcurve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\alpha$ areconstant parameters. This formulation takes into account two factors: (1)power-law scaling over data size, and (2) the additional loss reduction duringLR annealing. Therefore, this formulation can describe the full loss curve ateach step, rather than the single loss point at the end of training. Applyingthe scaling law with LR annealing and fitting only one or two training curves,we can accurately predict the loss at any given step across any learning ratescheduler (LRS). This approach significantly reduces computational cost informulating scaling laws while providing more accuracy and expressiveness fortraining dynamics. Extensive experiments demonstrate that our findings holdacross a range of hyper-parameters and model architectures, and our equationcan extend to scaling effect of model sizes. Moreover, our formulation providesaccurate theoretical verification and explanation for empirical resultsobserved in numerous previous studies, particularly those focusing on LRschedule and annealing. We believe that this work is promising to enhance theunderstanding of LLM training dynamics while greatly democratizing scalinglaws, and it can guide researchers in refining training strategies (e.g.critical LRS) for further LLMs.</description><author>Howe Tissue, Venus Wang, Lu Wang</author><pubDate>Thu, 24 Oct 2024 17:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11029v2</guid></item><item><title>Context is Key: A Benchmark for Forecasting with Essential Textual Information</title><link>http://arxiv.org/abs/2410.18959v1</link><description>Forecasting is a critical task in decision making across various domains.While numerical data provides a foundation, it often lacks crucial contextnecessary for accurate predictions. Human forecasters frequently rely onadditional information, such as background knowledge or constraints, which canbe efficiently communicated through natural language. However, the ability ofexisting forecasting models to effectively integrate this textual informationremains an open question. To address this, we introduce "Context is Key" (CiK),a time series forecasting benchmark that pairs numerical data with diversetypes of carefully crafted textual context, requiring models to integrate bothmodalities. We evaluate a range of approaches, including statistical models,time series foundation models, and LLM-based forecasters, and propose a simpleyet effective LLM prompting method that outperforms all other tested methods onour benchmark. Our experiments highlight the importance of incorporatingcontextual information, demonstrate surprising performance when using LLM-basedforecasting models, and also reveal some of their critical shortcomings. Bypresenting this benchmark, we aim to advance multimodal forecasting, promotingmodels that are both accurate and accessible to decision-makers with variedtechnical expertise. The benchmark can be visualized athttps://servicenow.github.io/context-is-key-forecasting/v0/ .</description><author>Andrew Robert Williams, Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Jithendaraa Subramanian, Roland Riachi, James Requeima, Alexandre Lacoste, Irina Rish, Nicolas Chapados, Alexandre Drouin</author><pubDate>Thu, 24 Oct 2024 17:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18959v1</guid></item><item><title>Stable Consistency Tuning: Understanding and Improving Consistency Models</title><link>http://arxiv.org/abs/2410.18958v1</link><description>Diffusion models achieve superior generation quality but suffer from slowgeneration speed due to the iterative nature of denoising. In contrast,consistency models, a new generative family, achieve competitive performancewith significantly faster sampling. These models are trained either throughconsistency distillation, which leverages pretrained diffusion models, orconsistency training/tuning directly from raw data. In this work, we propose anovel framework for understanding consistency models by modeling the denoisingprocess of the diffusion model as a Markov Decision Process (MDP) and framingconsistency model training as the value estimation through TemporalDifference~(TD) Learning. More importantly, this framework allows us to analyzethe limitations of current consistency training/tuning strategies. Built uponEasy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT),which incorporates variance-reduced learning using the score identity. SCTleads to significant performance improvements on benchmarks such as CIFAR-10and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID1.55, a new SoTA for consistency models.</description><author>Fu-Yun Wang, Zhengyang Geng, Hongsheng Li</author><pubDate>Thu, 24 Oct 2024 17:55:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18958v1</guid></item><item><title>Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in Low-Resource Code</title><link>http://arxiv.org/abs/2410.18957v1</link><description>Large Language Models (LLMs) demonstrate strong proficiency in generatingcode for high-resource programming languages (HRPLs) like Python but strugglesignificantly with low-resource programming languages (LRPLs) such as Racket orD. This performance gap deepens the digital divide, preventing developers usingLRPLs from benefiting equally from LLM advancements and reinforcing disparitiesin innovation within underrepresented programming communities. While generatingadditional training data for LRPLs is promising, it faces two key challenges:manual annotation is labor-intensive and costly, and LLM-generated LRPL code isoften of subpar quality. The underlying cause of this issue is the gap betweennatural language to programming language gap (NL-PL Gap), which is especiallypronounced in LRPLs due to limited aligned data. In this work, we introduce anovel approach called Bridge-Coder, which leverages LLMs' intrinsiccapabilities to enhance the performance on LRPLs. Our method consists of twokey stages. Bridge Generation, where we create high-quality dataset byutilizing LLMs' general knowledge understanding, proficiency in HRPLs, andin-context learning abilities. Then, we apply the Bridged Alignment, whichprogressively improves the alignment between NL instructions and LRPLs.Experimental results across multiple LRPLs show that Bridge-Coder significantlyenhances model performance, demonstrating the effectiveness and generalizationof our approach. Furthermore, we offer a detailed analysis of the keycomponents of our method, providing valuable insights for future work aimed ataddressing the challenges associated with LRPLs.</description><author>Jipeng Zhang, Jianshu Zhang, Yuanzhe Li, Renjie Pi, Rui Pan, Runtao Liu, Ziqiang Zheng, Tong Zhang</author><pubDate>Thu, 24 Oct 2024 17:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18957v1</guid></item><item><title>Large Spatial Model: End-to-end Unposed Images to Semantic 3D</title><link>http://arxiv.org/abs/2410.18956v1</link><description>Reconstructing and understanding 3D structures from a limited number ofimages is a well-established problem in computer vision. Traditional methodsusually break this task into multiple subtasks, each requiring complextransformations between different data representations. For instance, densereconstruction through Structure-from-Motion (SfM) involves converting imagesinto key points, optimizing camera parameters, and estimating structures.Afterward, accurate sparse reconstructions are required for further densemodeling, which is subsequently fed into task-specific neural networks. Thismulti-step process results in considerable processing time and increasedengineering complexity. In this work, we present the Large Spatial Model (LSM), which processesunposed RGB images directly into semantic radiance fields. LSM simultaneouslyestimates geometry, appearance, and semantics in a single feed-forwardoperation, and it can generate versatile label maps by interacting withlanguage at novel viewpoints. Leveraging a Transformer-based architecture, LSMintegrates global geometry through pixel-aligned point maps. To enhance spatialattribute regression, we incorporate local context aggregation with multi-scalefusion, improving the accuracy of fine local details. To tackle the scarcity oflabeled 3D semantic data and enable natural language-driven scene manipulation,we incorporate a pre-trained 2D language-based segmentation model into a3D-consistent semantic feature field. An efficient decoder then parameterizes aset of semantic anisotropic Gaussians, facilitating supervised end-to-endlearning. Extensive experiments across various tasks show that LSM unifiesmultiple 3D vision tasks directly from unposed images, achieving real-timesemantic 3D reconstruction for the first time.</description><author>Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang</author><pubDate>Thu, 24 Oct 2024 17:54:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18956v1</guid></item><item><title>TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks</title><link>http://arxiv.org/abs/2406.19380v4</link><description>Advances in machine learning research drive progress in real-worldapplications. To ensure this progress, it is important to understand thepotential pitfalls on the way from a novel method's success on academicbenchmarks to its practical deployment. In this work, we analyze existingtabular benchmarks and find two common characteristics of tabular data intypical industrial applications that are underrepresented in the datasetsusually used for evaluation in the literature. First, in real-world deploymentscenarios, distribution of data often changes over time. To account for thisdistribution drift, time-based train/test splits should be used in evaluation.However, popular tabular datasets often lack timestamp metadata to enable suchevaluation. Second, a considerable portion of datasets in production settingsstem from extensive data acquisition and feature engineering pipelines. Thiscan have an impact on the absolute and relative number of predictive,uninformative, and correlated features compared to academic datasets. In thiswork, we aim to understand how recent research advances in tabular deeplearning transfer to these underrepresented conditions. To this end, weintroduce TabReD -- a collection of eight industry-grade tabular datasets. Wereassess a large number of tabular ML models and techniques on TabReD. Wedemonstrate that evaluation on time-based data splits leads to differentmethods ranking, compared to evaluation on random splits, which are common incurrent benchmarks. Furthermore, simple MLP-like architectures and GBDT showthe best results on the TabReD datasets, while other methods are less effectivein the new setting.</description><author>Ivan Rubachev, Nikolay Kartashev, Yury Gorishniy, Artem Babenko</author><pubDate>Thu, 24 Oct 2024 17:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19380v4</guid></item><item><title>BioMistral-NLU: Towards More Generalizable Medical Language Understanding through Instruction Tuning</title><link>http://arxiv.org/abs/2410.18955v1</link><description>Large language models (LLMs) such as ChatGPT are fine-tuned on large anddiverse instruction-following corpora, and can generalize to new tasks.However, those instruction-tuned LLMs often perform poorly in specializedmedical natural language understanding (NLU) tasks that require domainknowledge, granular text comprehension, and structured data extraction. Tobridge the gap, we: (1) propose a unified prompting format for 7 important NLUtasks, % through span extraction and multi-choice question-answering (QA), (2)curate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existingopen-source medical NLU corpora, and (3) develop BioMistral-NLU, ageneralizable medical NLU model, through fine-tuning BioMistral onMNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6important NLU tasks, from two widely adopted medical NLU benchmarks: BiomedicalLanguage Understanding Evaluation (BLUE) and Biomedical Language Understandingand Reasoning Benchmark (BLURB). Our experiments show that our BioMistral-NLUoutperforms the original BioMistral, as well as the proprietary LLMs - ChatGPTand GPT-4. Our dataset-agnostic prompting strategy and instruction tuning stepover diverse NLU tasks enhance LLMs' generalizability across diverse medicalNLU tasks. Our ablation experiments show that instruction-tuning on a widervariety of tasks, even when the total number of training instances remainsconstant, enhances downstream zero-shot generalization.</description><author>Yujuan Velvin Fu, Giridhar Kaushik Ramachandran, Namu Park, Kevin Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen</author><pubDate>Thu, 24 Oct 2024 17:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18955v1</guid></item><item><title>Learning Structured Compressed Sensing with Automatic Resource Allocation</title><link>http://arxiv.org/abs/2410.18954v1</link><description>Multidimensional data acquisition often requires extensive time and posessignificant challenges for hardware and software regarding data storage andprocessing. Rather than designing a single compression matrix as inconventional compressed sensing, structured compressed sensing yieldsdimension-specific compression matrices, reducing the number of optimizableparameters. Recent advances in machine learning (ML) have enabled task-basedsupervised learning of subsampling matrices, albeit at the expense of complexdownstream models. Additionally, the sampling resource allocation acrossdimensions is often determined in advance through heuristics. To address thesechallenges, we introduce Structured COmpressed Sensing with Automatic ResourceAllocation (SCOSARA) with an information theory-based unsupervised learningstrategy. SCOSARA adaptively distributes samples across sampling dimensionswhile maximizing Fisher information content. Using ultrasound localization as acase study, we compare SCOSARA to state-of-the-art ML-based and greedy searchalgorithms. Simulation results demonstrate that SCOSARA can producehigh-quality subsampling matrices that achieve lower Cram\'er-Rao Bound valuesthan the baselines. In addition, SCOSARA outperforms other ML-based algorithmsin terms of the number of trainable parameters, computational complexity, andmemory requirements while automatically choosing the number of samples peraxis.</description><author>Han Wang, Eduardo Pérez, Iris A. M. Huijben, Hans van Gorp, Ruud van Sloun, Florian Römer</author><pubDate>Thu, 24 Oct 2024 17:53:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18954v1</guid></item><item><title>Dynamic Vocabulary Pruning in Early-Exit LLMs</title><link>http://arxiv.org/abs/2410.18952v1</link><description>Increasing the size of large language models (LLMs) has been shown to lead tobetter performance. However, this comes at the cost of slower and moreexpensive inference. Early-exiting is a promising approach for improving theefficiency of LLM inference by enabling next token prediction at intermediatelayers. Yet, the large vocabulary size in modern LLMs makes the confidenceestimation required for exit decisions computationally expensive, diminishingthe efficiency gains. To address this, we propose dynamically pruning thevocabulary at test time for each token. Specifically, the vocabulary is prunedat one of the initial layers, and the smaller vocabulary is then usedthroughout the rest of the forward pass. Our experiments demonstrate that suchpost-hoc dynamic vocabulary pruning improves the efficiency of confidenceestimation in early-exit LLMs while maintaining competitive performance.</description><author>Jort Vincenti, Karim Abdel Sadek, Joan Velja, Matteo Nulli, Metod Jazbec</author><pubDate>Thu, 24 Oct 2024 17:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18952v1</guid></item><item><title>Adjusted Overfitting Regression</title><link>http://arxiv.org/abs/2410.18950v1</link><description>In this paper, I will introduce a new form of regression, that can adjustoverfitting and underfitting through, "distance-based regression." Overfittingoften results in finding false patterns causing inaccurate results, so byhaving a new approach that minimizes overfitting, more accurate predictions canbe derived. Then I will proceed with a test of my regression form and showadditional ways to optimize the regression. Finally, I will apply my newtechnique to a specific data set to demonstrate its practical value.</description><author>Dylan Wilson</author><pubDate>Thu, 24 Oct 2024 17:50:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18950v1</guid></item><item><title>Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices</title><link>http://arxiv.org/abs/2410.17998v2</link><description>Analyzing the structure of sampled features from an input data distributionis challenging when constrained by limited measurements in both the number ofinputs and features. Traditional approaches often rely on the eigenvaluespectrum of the sample covariance matrix derived from finite measurementmatrices; however, these spectra are sensitive to the size of the measurementmatrix, leading to biased insights. In this paper, we introduce a novelalgorithm that provides unbiased estimates of the spectral moments of thekernel integral operator in the limit of infinite inputs and features fromfinitely sampled measurement matrices. Our method, based on dynamicprogramming, is efficient and capable of estimating the moments of the operatorspectrum. We demonstrate the accuracy of our estimator on radial basis function(RBF) kernels, highlighting its consistency with the theoretical spectra.Furthermore, we showcase the practical utility and robustness of our method inunderstanding the geometry of learned representations in neural networks.</description><author>Chanwoo Chun, SueYeon Chung, Daniel D. Lee</author><pubDate>Thu, 24 Oct 2024 17:47:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17998v2</guid></item><item><title>White Men Lead, Black Women Help? Benchmarking Language Agency Social Biases in LLMs</title><link>http://arxiv.org/abs/2404.10508v4</link><description>Social biases can manifest in language agency. While several studiesapproached agency-related bias in human-written language, very limited researchhas investigated such biases in Large Language Model (LLM)-generated content.In addition, previous works often rely on string-matching techniques toidentify agentic and communal words within texts, which fall short ofaccurately classifying language agency. We introduce the novel Language AgencyBias Evaluation (LABE) benchmark, which comprehensively evaluates biases inLLMs by analyzing agency levels attributed to different demographic groups inmodel generations. LABE leverages 5,400 template-based prompts, an accurateagency classifier, and corresponding bias metrics to test for gender, racial,and intersectional language agency biases in LLMs on 3 text generation tasks:biographies, professor reviews, and reference letters. We also contribute theLanguage Agency Classification (LAC) dataset, consisting of 3,724 agentic andcommunal sentences. Using LABE, we unveil language agency social biases in 3recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generationstend to demonstrate greater gender bias than human-written texts; (2) Modelsdemonstrate remarkably higher levels of intersectional bias than the other biasaspects. Those who are at the intersection of gender and racial minoritygroups--such as Black females--are consistently described by texts with lowerlevels of agency, aligning with real-world social inequalities; (3) Among the 3LLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not onlydoes prompt-based mitigation fail to resolve language agency bias in LLMs, butit frequently leads to the exacerbation of biases in generated texts.</description><author>Yixin Wan, Kai-Wei Chang</author><pubDate>Thu, 24 Oct 2024 17:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10508v4</guid></item><item><title>Influence Functions for Scalable Data Attribution in Diffusion Models</title><link>http://arxiv.org/abs/2410.13850v2</link><description>Diffusion models have led to significant advancements in generativemodelling. Yet their widespread adoption poses challenges regarding dataattribution and interpretability. In this paper, we aim to help address suchchallenges in diffusion models by developing an \textit{influence functions}framework. Influence function-based data attribution methods approximate how amodel's output would have changed if some training data were removed. Insupervised learning, this is usually used for predicting how the loss on aparticular example would change. For diffusion models, we focus on predictingthe change in the probability of generating a particular example via severalproxy measurements. We show how to formulate influence functions for suchquantities and how previously proposed methods can be interpreted as particulardesign choices in our framework. To ensure scalability of the Hessiancomputations in influence functions, we systematically develop K-FACapproximations based on generalised Gauss-Newton matrices specifically tailoredto diffusion models. We recast previously proposed methods as specific designchoices in our framework and show that our recommended method outperformsprevious data attribution approaches on common evaluations, such as the LinearData-modelling Score (LDS) or retraining without top influences, without theneed for method-specific hyperparameter tuning.</description><author>Bruno Mlodozeniec, Runa Eschenhagen, Juhan Bae, Alexander Immer, David Krueger, Richard Turner</author><pubDate>Thu, 24 Oct 2024 17:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13850v2</guid></item><item><title>A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU</title><link>http://arxiv.org/abs/2305.17473v3</link><description>Deep learning (DL) has emerged as a powerful subset of machine learning (ML)and artificial intelligence (AI), outperforming traditional ML methods,especially in handling unstructured and large datasets. Its impact spans acrossvarious domains, including speech recognition, healthcare, autonomous vehicles,cybersecurity, predictive analytics, and more. However, the complexity anddynamic nature of real-world problems present challenges in designing effectivedeep learning models. Consequently, several deep learning models have beendeveloped to address different problems and applications. In this article, weconduct a comprehensive survey of various deep learning models, includingConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),Generative Models, Deep Reinforcement Learning (DRL), and Deep TransferLearning. We examine the structure, applications, benefits, and limitations ofeach model. Furthermore, we perform an analysis using three publicly availabledatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowneddeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),Bidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.</description><author>Farhad Mortezapour Shiri, Thinagaran Perumal, Norwati Mustapha, Raihani Mohamed</author><pubDate>Thu, 24 Oct 2024 17:41:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17473v3</guid></item><item><title>Interação entre robôs humanoides: desenvolvendo a colaboração e comunicação autônoma</title><link>http://arxiv.org/abs/2410.17450v2</link><description>This study investigates the interaction between humanoid robots NAO andPepper, emphasizing their potential applications in educational settings. NAO,widely used in education, and Pepper, designed for social interactions, of ernew opportunities for autonomous communication and collaboration. Through aseries of programmed interactions, the robots demonstrated their ability tocommunicate and coordinate actions autonomously, highlighting their potentialas tools for enhancing learning environments. The research also explores theintegration of emerging technologies, such as artificial intelligence, intothese systems, allowing robots to learn from each other and adapt theirbehavior. The findings suggest that NAO and Pepper can significantly contributeto both technical learning and the development of social and emotional skillsin students, of ering innovative pedagogical approaches through the use ofhumanoid robotics.</description><author>Moraes Pablo, Rodríguez Mónica, Peters Christopher, Sodre Hiago, Mazondo Ahilen, Sandin Vincent, Barcelona Sebastian, Moraes William, Fernández Santiago, Assunção Nathalie, de Vargas Bruna, Dörnbach Tobias, Kelbouscas André, Grando Ricardo</author><pubDate>Thu, 24 Oct 2024 17:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17450v2</guid></item><item><title>Unearthing Skill-Level Insights for Understanding Trade-Offs of Foundation Models</title><link>http://arxiv.org/abs/2410.13826v2</link><description>With models getting stronger, evaluations have grown more complex, testingmultiple skills in one benchmark and even in the same instance at once.However, skill-wise performance is obscured when inspecting aggregate accuracy,under-utilizing the rich signal modern benchmarks contain. We propose anautomatic approach to recover the underlying skills relevant for any evaluationinstance, by way of inspecting model-generated rationales. After validating therelevance of rationale-parsed skills and inferring skills for $46$k instancesover $12$ benchmarks, we observe many skills to be common across benchmarks,resulting in the curation of hundreds of skill-slices (i.e. sets of instancestesting a common skill). Inspecting accuracy over these slices yields novelinsights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,on average, Gemini 1.5 Pro is $18\%$ more accurate in "computing molar mass",but $19\%$ less accurate in "applying constitutional law", despite the overallaccuracies of the three models differing by a mere $0.4\%$. Furthermore, wedemonstrate the practical utility of our approach by showing that insightsderived from skill slice analysis can generalize to held-out instances: whenrouting each instance to the model strongest on the relevant skills, we see a$3\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices andframework open a new avenue in model evaluation, leveraging skill-specificanalyses to unlock a more granular and actionable understanding of modelcapabilities.</description><author>Mazda Moayeri, Vidhisha Balachandran, Varun Chandrasekaran, Safoora Yousefi, Thomas Fel, Soheil Feizi, Besmira Nushi, Neel Joshi, Vibhav Vineet</author><pubDate>Thu, 24 Oct 2024 17:27:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13826v2</guid></item><item><title>Pointer Networks with Q-Learning for Combinatorial Optimization</title><link>http://arxiv.org/abs/2311.02629v4</link><description>We introduce the Pointer Q-Network (PQN), a hybrid neural architecture thatintegrates model-free Q-value policy approximation with Pointer Networks(Ptr-Nets) to enhance the optimality of attention-based sequence generation,focusing on long-term outcomes. This integration proves particularly effectivein solving combinatorial optimization (CO) tasks, especially the TravellingSalesman Problem (TSP), which is the focus of our study. We address thischallenge by defining a Markov Decision Process (MDP) compatible with PQN,which involves iterative graph embedding, encoding and decoding by anLSTM-based recurrent neural network. This process generates a context vectorand computes raw attention scores, which are dynamically adjusted by Q-valuescalculated for all available state-action pairs before applying softmax. Theresulting attention vector is utilized as an action distribution, with actionsselected hinged to exploration-exploitation dynamic adaptibility of PQN. Ourempirical results demonstrate the efficacy of this approach, also testing themodel in unstable environments.</description><author>Alessandro Barro</author><pubDate>Thu, 24 Oct 2024 17:25:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02629v4</guid></item><item><title>A Random Matrix Theory Perspective on the Spectrum of Learned Features and Asymptotic Generalization Capabilities</title><link>http://arxiv.org/abs/2410.18938v1</link><description>A key property of neural networks is their capacity of adapting to dataduring training. Yet, our current mathematical understanding of featurelearning and its relationship to generalization remain limited. In this work,we provide a random matrix analysis of how fully-connected two-layer neuralnetworks adapt to the target function after a single, but aggressive, gradientdescent step. We rigorously establish the equivalence between the updatedfeatures and an isotropic spiked random feature model, in the limit of largebatch size. For the latter model, we derive a deterministic equivalentdescription of the feature empirical covariance matrix in terms of certainlow-dimensional operators. This allows us to sharply characterize the impact oftraining in the asymptotic feature spectrum, and in particular, provides atheoretical grounding for how the tails of the feature spectrum modify withtraining. The deterministic equivalent further yields the exact asymptoticgeneralization error, shedding light on the mechanisms behind its improvementin the presence of feature learning. Our result goes beyond standard randommatrix ensembles, and therefore we believe it is of independent technicalinterest. Different from previous work, our result holds in the challengingmaximal learning rate regime, is fully rigorous and allows for finitelysupported second layer initialization, which turns out to be crucial forstudying the functional expressivity of the learned features. This provides asharp description of the impact of feature learning in the generalization oftwo-layer neural networks, beyond the random features and lazy trainingregimes.</description><author>Yatin Dandi, Luca Pesce, Hugo Cui, Florent Krzakala, Yue M. Lu, Bruno Loureiro</author><pubDate>Thu, 24 Oct 2024 17:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18938v1</guid></item><item><title>Schema-Guided Culture-Aware Complex Event Simulation with Multi-Agent Role-Play</title><link>http://arxiv.org/abs/2410.18935v1</link><description>Complex news events, such as natural disasters and socio-political conflicts,require swift responses from the government and society. Relying on historicalevents to project the future is insufficient as such events are sparse and donot cover all possible conditions and nuanced situations. Simulation of thesecomplex events can help better prepare and reduce the negative impact. Wedevelop a controllable complex news event simulator guided by both the eventschema representing domain knowledge about the scenario and user-providedassumptions representing case-specific conditions. As event dynamics depend onthe fine-grained social and cultural context, we further introduce ageo-diverse commonsense and cultural norm-aware knowledge enhancementcomponent. To enhance the coherence of the simulation, apart from the globaltimeline of events, we take an agent-based approach to simulate the individualcharacter states, plans, and actions. By incorporating the schema and culturalnorms, our generated simulations achieve much higher coherence andappropriateness and are received favorably by participants from a humanitarianassistance organization.</description><author>Sha Li, Revanth Gangi Reddy, Khanh Duy Nguyen, Qingyun Wang, May Fung, Chi Han, Jiawei Han, Kartik Natarajan, Clare R. Voss, Heng Ji</author><pubDate>Thu, 24 Oct 2024 17:21:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18935v1</guid></item><item><title>A Neural Phillips Curve and a Deep Output Gap</title><link>http://arxiv.org/abs/2202.04146v2</link><description>Many problems plague empirical Phillips curves (PCs). Among them is thehurdle that the two key components, inflation expectations and the output gap,are both unobserved. Traditional remedies include proxying for the absentees orextracting them via assumptions-heavy filtering procedures. I propose analternative route: a Hemisphere Neural Network (HNN) whose architecture yieldsa final layer where components can be interpreted as latent states within aNeural PC. There are benefits. First, HNN conducts the supervised estimation ofnonlinearities that arise when translating a high-dimensional set of observedregressors into latent states. Second, forecasts are economicallyinterpretable. Among other findings, the contribution of real activity toinflation appears understated in traditional PCs. In contrast, HNN captures the2021 upswing in inflation and attributes it to a large positive output gapstarting from late 2020. The unique path of HNN's gap comes from dispensingwith unemployment and GDP in favor of an amalgam of nonlinearly processedalternative tightness indicators.</description><author>Philippe Goulet Coulombe</author><pubDate>Thu, 24 Oct 2024 17:20:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.04146v2</guid></item><item><title>ANAVI: Audio Noise Awareness using Visuals of Indoor environments for NAVIgation</title><link>http://arxiv.org/abs/2410.18932v1</link><description>We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation forquieter robot path planning. While humans are naturally aware of the noise theymake and its impact on those around them, robots currently lack this awareness.A key challenge in achieving audio awareness for robots is estimating how loudwill the robot's actions be at a listener's location? Since sound depends uponthe geometry and material composition of rooms, we train the robot to passivelyperceive loudness using visual observations of indoor environments. To thisend, we generate data on how loud an 'impulse' sounds at different listenerlocations in simulated homes, and train our Acoustic Noise Predictor (ANP).Next, we collect acoustic profiles corresponding to different actions fornavigation. Unifying ANP with action acoustics, we demonstrate experiments withwheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that theserobots adhere to the noise constraints of the environment. See code and data athttps://anavi-corl24.github.io/</description><author>Vidhi Jain, Rishi Veerapaneni, Yonatan Bisk</author><pubDate>Thu, 24 Oct 2024 17:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18932v1</guid></item><item><title>Sort-free Gaussian Splatting via Weighted Sum Rendering</title><link>http://arxiv.org/abs/2410.18931v1</link><description>Recently, 3D Gaussian Splatting (3DGS) has emerged as a significantadvancement in 3D scene reconstruction, attracting considerable attention dueto its ability to recover high-fidelity details while maintaining lowcomplexity. Despite the promising results achieved by 3DGS, its renderingperformance is constrained by its dependence on costly non-commutativealpha-blending operations. These operations mandate complex view dependentsorting operations that introduce computational overhead, especially on theresource-constrained platforms such as mobile phones. In this paper, we proposeWeighted Sum Rendering, which approximates alpha blending with weighted sums,thereby removing the need for sorting. This simplifies implementation, deliverssuperior performance, and eliminates the "popping" artifacts caused by sorting.Experimental results show that optimizing a generalized Gaussian splattingformulation to the new differentiable rendering yields competitive imagequality. The method was implemented and tested in a mobile device GPU,achieving on average $1.23\times$ faster rendering.</description><author>Qiqi Hou, Randall Rauwendaal, Zifeng Li, Hoang Le, Farzad Farhadzadeh, Fatih Porikli, Alexei Bourd, Amir Said</author><pubDate>Thu, 24 Oct 2024 17:18:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18931v1</guid></item><item><title>AutoStep: Locally adaptive involutive MCMC</title><link>http://arxiv.org/abs/2410.18929v1</link><description>Many common Markov chain Monte Carlo (MCMC) kernels can be formulated using adeterministic involutive proposal with a step size parameter. Selecting anappropriate step size is often a challenging task in practice; and for complexmultiscale targets, there may not be one choice of step size that works wellglobally. In this work, we address this problem with a novel class ofinvolutive MCMC methods -- AutoStep MCMC -- that selects an appropriate stepsize at each iteration adapted to the local geometry of the targetdistribution. We prove that AutoStep MCMC is $\pi$-invariant and has otherdesirable properties under mild assumptions on the target distribution $\pi$and involutive proposal. Empirical results examine the effect of various stepsize selection design choices, and show that AutoStep MCMC is competitive withstate-of-the-art methods in terms of effective sample size per unit cost on arange of challenging target distributions.</description><author>Tiange Liu, Nikola Surjanovic, Miguel Biron-Lattes, Alexandre Bouchard-Côté, Trevor Campbell</author><pubDate>Thu, 24 Oct 2024 17:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18929v1</guid></item><item><title>Learning $k$-body Hamiltonians via compressed sensing</title><link>http://arxiv.org/abs/2410.18928v1</link><description>We study the problem of learning a $k$-body Hamiltonian with $M$ unknownPauli terms that are not necessarily geometrically local. We propose a protocolthat learns the Hamiltonian to precision $\epsilon$ with total evolution time${\mathcal{O}}(M^{1/2+1/p}/\epsilon)$ up to logarithmic factors, where theerror is quantified by the $\ell^p$-distance between Pauli coefficients. Ourlearning protocol uses only single-qubit control operations and a GHZ stateinitial state, is non-adaptive, is robust against SPAM errors, and performswell even if $M$ and $k$ are not precisely known in advance or if theHamiltonian is not exactly $M$-sparse. Methods from the classical theory ofcompressed sensing are used for efficiently identifying the $M$ terms in theHamiltonian from among all possible $k$-body Pauli operators. We also provide alower bound on the total evolution time needed in this learning task, and wediscuss the operational interpretations of the $\ell^1$ and $\ell^2$ errormetrics. In contrast to previous works, our learning protocol requires neithergeometric locality nor any other relaxed locality conditions.</description><author>Muzhou Ma, Steven T. Flammia, John Preskill, Yu Tong</author><pubDate>Thu, 24 Oct 2024 17:16:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18928v1</guid></item><item><title>LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor Search</title><link>http://arxiv.org/abs/2410.18926v1</link><description>Approximate nearest neighbor (ANN) search is a key component in many modernmachine learning pipelines; recent use cases include retrieval-augmentedgeneration (RAG) and vector databases. Clustering-based ANN algorithms, thatuse score computation methods based on product quantization (PQ), are oftenused in industrial-scale applications due to their scalability and suitabilityfor distributed and disk-based implementations. However, they have slower querytimes than the leading graph-based ANN algorithms. In this work, we propose anew supervised score computation method based on the observation that innerproduct approximation is a multivariate (multi-output) regression problem thatcan be solved efficiently by reduced-rank regression. Our experiments show thaton modern high-dimensional data sets, the proposed reduced-rank regression(RRR) method is superior to PQ in both query latency and memory usage. We alsointroduce LoRANN, a clustering-based ANN library that leverages the proposedscore computation method. LoRANN is competitive with the leading graph-basedalgorithms and outperforms the state-of-the-art GPU ANN methods onhigh-dimensional data sets.</description><author>Elias Jääsaari, Ville Hyvönen, Teemu Roos</author><pubDate>Thu, 24 Oct 2024 17:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18926v1</guid></item><item><title>SegLLM: Multi-round Reasoning Segmentation</title><link>http://arxiv.org/abs/2410.18923v1</link><description>We present SegLLM, a novel multi-round interactive reasoning segmentationmodel that enhances LLM-based segmentation by exploiting conversational memoryof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,SegLLM re-integrates previous segmentation results into its input stream,enabling it to reason about complex user intentions and segment objects inrelation to previously identified entities, including positional,interactional, and hierarchical relationships, across multiple interactions.This capability allows SegLLM to respond to visual and text queries in achat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLMoutperforms existing methods in multi-round interactive reasoning segmentationby over 20%. Additionally, we observed that training on multi-round reasoningsegmentation data enhances performance on standard single-round referringsegmentation and localization tasks, resulting in a 5.5% increase in cIoU forreferring expression segmentation and a 4.5% improvement in Acc@0.5 forreferring expression localization.</description><author>XuDong Wang, Shaolun Zhang, Shufan Li, Konstantinos Kallidromitis, Kehan Li, Yusuke Kato, Kazuki Kozuka, Trevor Darrell</author><pubDate>Thu, 24 Oct 2024 17:11:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18923v1</guid></item><item><title>From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical Integrity on Faulty Mathematical Problems</title><link>http://arxiv.org/abs/2410.18921v1</link><description>Consider the math problem: "Lily received 3 cookies from her best friendyesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies.How many cookies does Lily have now?" Many large language models (LLMs) inprevious research approach this problem by calculating the answer "1" using theequation "3 - 5 + 3." However, from a human perspective, we recognize theinherent flaw in this problem: Lily cannot eat 5 cookies if she initially onlyhad 3. This discrepancy prompts a key question: Are current LLMs merely BlindSolver that apply mathematical operations without deeper reasoning, or can theyfunction as Logical Thinker capable of identifying logical inconsistencies? To explore this question, we propose a benchmark dataset, FaultyMath, whichincludes faulty math problems of rich diversity: i) multiple mathematicalcategories, e.g., algebra, geometry, number theory, etc., ii) varying levels ofdifficulty, and iii) different origins of faultiness -- ranging from violationsof common sense and ambiguous statements to mathematical contradictions andmore. We evaluate a broad spectrum of LLMs, including open-source,closed-source, and math-specialized models, using FaultyMath across threedimensions: (i) How accurately can the models detect faulty math problemswithout being explicitly prompted to do so? (ii) When provided with hints --either correct or misleading -- about the validity of the problems, to whatextent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthyare the explanations generated by LLMs when they recognize a math problem asflawed? Through extensive experimentation and detailed analysis, our resultsdemonstrate that existing LLMs largely function as Blind Solver and fall shortof the reasoning capabilities required to perform as Logical Thinker.</description><author>A M Muntasir Rahman, Junyi Ye, Wei Yao, Wenpeng Yin, Guiling Wang</author><pubDate>Thu, 24 Oct 2024 17:10:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18921v1</guid></item><item><title>Optimizing Edge Offloading Decisions for Object Detection</title><link>http://arxiv.org/abs/2410.18919v1</link><description>Recent advances in machine learning and hardware have produced embeddeddevices capable of performing real-time object detection with commendableaccuracy. We consider a scenario in which embedded devices rely on an onboardobject detector, but have the option to offload detection to a more powerfuledge server when local accuracy is deemed too low. Resource constraints,however, limit the number of images that can be offloaded to the edge. Our goalis to identify which images to offload to maximize overall detection accuracyunder those constraints. To that end, the paper introduces a reward metricdesigned to quantify potential accuracy improvements from offloading individualimages, and proposes an efficient approach to make offloading decisions byestimating this reward based only on local detection results. The approach iscomputationally frugal enough to run on embedded devices, and empiricalfindings indicate that it outperforms existing alternatives in improvingdetection accuracy even when the fraction of offloaded images is small.</description><author>Jiaming Qiu, Ruiqi Wang, Brooks Hu, Roch Guerin, Chenyang Lu</author><pubDate>Thu, 24 Oct 2024 17:09:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18919v1</guid></item><item><title>MissNODAG: Differentiable Cyclic Causal Graph Learning from Incomplete Data</title><link>http://arxiv.org/abs/2410.18918v1</link><description>Causal discovery in real-world systems, such as biological networks, is oftencomplicated by feedback loops and incomplete data. Standard algorithms, whichassume acyclic structures or fully observed data, struggle with thesechallenges. To address this gap, we propose MissNODAG, a differentiableframework for learning both the underlying cyclic causal graph and themissingness mechanism from partially observed data, including data missing notat random. Our framework integrates an additive noise model with anexpectation-maximization procedure, alternating between imputing missing valuesand optimizing the observed data likelihood, to uncover both the cyclicstructures and the missingness mechanism. We demonstrate the effectiveness ofMissNODAG through synthetic experiments and an application to real-world geneperturbation data.</description><author>Muralikrishnna G. Sethuraman, Razieh Nabi, Faramarz Fekri</author><pubDate>Thu, 24 Oct 2024 17:09:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18918v1</guid></item><item><title>Scikit-fingerprints: easy and efficient computation of molecular fingerprints in Python</title><link>http://arxiv.org/abs/2407.13291v3</link><description>In this work, we present \skfp, a Python package for computation of molecularfingerprints for applications in chemoinformatics. Our library offers anindustry-standard scikit-learn interface, allowing intuitive usage and easyintegration with machine learning pipelines. It is also highly optimized,featuring parallel computation that enables efficient processing of largemolecular datasets. Currently, \skfp~stands as the most feature-rich library inthe open source Python ecosystem, offering over 30 molecular fingerprints. Ourlibrary simplifies chemoinformatics tasks based on molecular fingerprints,including molecular property prediction and virtual screening. It is alsoflexible, highly efficient, and fully open source.</description><author>Jakub Adamczyk, Piotr Ludynia</author><pubDate>Thu, 24 Oct 2024 17:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13291v3</guid></item><item><title>Using Parametric PINNs for Predicting Internal and External Turbulent Flows</title><link>http://arxiv.org/abs/2410.18917v1</link><description>Computational fluid dynamics (CFD) solvers employing two-equation eddyviscosity models are the industry standard for simulating turbulent flows usingthe Reynolds-averaged Navier-Stokes (RANS) formulation. While these methods arecomputationally less expensive than direct numerical simulations, they canstill incur significant computational costs to achieve the desired accuracy. Inthis context, physics-informed neural networks (PINNs) offer a promisingapproach for developing parametric surrogate models that leverage bothexisting, but limited CFD solutions and the governing differential equations topredict simulation outcomes in a computationally efficient, differentiable, andnear real-time manner. In this work, we build upon the previously proposedRANS-PINN framework, which only focused on predicting flow over a cylinder. Toinvestigate the efficacy of RANS-PINN as a viable approach to buildingparametric surrogate models, we investigate its accuracy in predicting relevantturbulent flow variables for both internal and external flows. To ensuretraining convergence with a more complex loss function, we adopt a novelsampling approach that exploits the domain geometry to ensure a proper balanceamong the contributions from various regions within the solution domain. Theeffectiveness of this framework is then demonstrated for two scenarios thatrepresent a broad class of internal and external flow problems.</description><author>Shinjan Ghosh, Amit Chakraborty, Georgia Olympia Brikis, Biswadip Dey</author><pubDate>Thu, 24 Oct 2024 17:08:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18917v1</guid></item><item><title>Testing Support Size More Efficiently Than Learning Histograms</title><link>http://arxiv.org/abs/2410.18915v1</link><description>Consider two problems about an unknown probability distribution $p$: 1. How many samples from $p$ are required to test if $p$ is supported on $n$elements or not? Specifically, given samples from $p$, determine whether it issupported on at most $n$ elements, or it is "$\epsilon$-far" (in totalvariation distance) from being supported on $n$ elements. 2. Given $m$ samples from $p$, what is the largest lower bound on its supportsize that we can produce? The best known upper bound for problem (1) uses a general algorithm forlearning the histogram of the distribution $p$, which requires$\Theta(\tfrac{n}{\epsilon^2 \log n})$ samples. We show that testing can bedone more efficiently than learning the histogram, using only$O(\tfrac{n}{\epsilon \log n} \log(1/\epsilon))$ samples, nearly matching thebest known lower bound of $\Omega(\tfrac{n}{\epsilon \log n})$. This algorithmalso provides a better solution to problem (2), producing larger lower boundson support size than what follows from previous work. The proof relies on ananalysis of Chebyshev polynomial approximations outside the range where theyare designed to be good approximations, and the paper is intended as anaccessible self-contained exposition of the Chebyshev polynomial method.</description><author>Renato Ferreira Pinto Jr., Nathaniel Harms</author><pubDate>Thu, 24 Oct 2024 17:05:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18915v1</guid></item><item><title>Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling</title><link>http://arxiv.org/abs/2410.18912v1</link><description>Videos of robots interacting with objects encode rich information about theobjects' dynamics. However, existing video prediction approaches typically donot explicitly account for the 3D information from videos, such as robotactions and objects' 3D states, limiting their use in real-world roboticapplications. In this work, we introduce a framework to learn object dynamicsdirectly from multi-view RGB videos by explicitly considering the robot'saction trajectories and their effects on scene dynamics. We utilize the 3DGaussian representation of 3D Gaussian Splatting (3DGS) to train aparticle-based dynamics model using Graph Neural Networks. This model operateson sparse control particles downsampled from the densely tracked 3D Gaussianreconstructions. By learning the neural dynamics model on offline robotinteraction data, our method can predict object motions under varying initialconfigurations and unseen robot actions. The 3D transformations of Gaussianscan be interpolated from the motions of control particles, enabling therendering of predicted future object states and achieving action-conditionedvideo prediction. The dynamics model can also be applied to model-basedplanning frameworks for object manipulation tasks. We conduct experiments onvarious kinds of deformable materials, including ropes, clothes, and stuffedanimals, demonstrating our framework's ability to model complex shapes anddynamics. Our project page is available at https://gs-dynamics.github.io.</description><author>Mingtong Zhang, Kaifeng Zhang, Yunzhu Li</author><pubDate>Thu, 24 Oct 2024 17:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18912v1</guid></item><item><title>SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment</title><link>http://arxiv.org/abs/2410.18907v1</link><description>Imitation learning from human demonstrations is an effective paradigm forrobot manipulation, but acquiring large datasets is costly andresource-intensive, especially for long-horizon tasks. To address this issue,we propose SkillMimicGen (SkillGen), an automated system for generatingdemonstration datasets from a few human demos. SkillGen segments human demosinto manipulation skills, adapts these skills to new contexts, and stitchesthem together through free-space transit and transfer motion. We also propose aHybrid Skill Policy (HSP) framework for learning skill initiation, control, andtermination components from SkillGen datasets, enabling skills to be sequencedusing motion planning at test-time. We demonstrate that SkillGen greatlyimproves data generation and policy learning performance over astate-of-the-art data generation framework, resulting in the capability toproduce data for large scene variations, including clutter, and agents that areon average 24% more successful. We demonstrate the efficacy of SkillGen bygenerating over 24K demonstrations across 18 task variants in simulation fromjust 60 human demonstrations, and training proficient, often near-perfect, HSPagents. Finally, we apply SkillGen to 3 real-world manipulation tasks and alsodemonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.Videos, and more at https://skillgen.github.io.</description><author>Caelan Garrett, Ajay Mandlekar, Bowen Wen, Dieter Fox</author><pubDate>Thu, 24 Oct 2024 16:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18907v1</guid></item><item><title>PRISM: A Methodology for Auditing Biases in Large Language Models</title><link>http://arxiv.org/abs/2410.18906v1</link><description>Auditing Large Language Models (LLMs) to discover their biases andpreferences is an emerging challenge in creating Responsible ArtificialIntelligence (AI). While various methods have been proposed to elicit thepreferences of such models, countermeasures have been taken by LLM trainers,such that LLMs hide, obfuscate or point blank refuse to disclosure theirpositions on certain subjects. This paper presents PRISM, a flexible,inquiry-based methodology for auditing LLMs - that seeks to illicit suchpositions indirectly through task-based inquiry prompting rather than directinquiry of said preferences. To demonstrate the utility of the methodology, weapplied PRISM on the Political Compass Test, where we assessed the politicalleanings of twenty-one LLMs from seven providers. We show LLMs, by default,espouse positions that are economically left and socially liberal (consistentwith prior work). We also show the space of positions that these models arewilling to espouse - where some models are more constrained and less compliantthan others - while others are more neutral and objective. In sum, PRISM canmore reliably probe and audit LLMs to understand their preferences, biases andconstraints.</description><author>Leif Azzopardi, Yashar Moshfeghi</author><pubDate>Thu, 24 Oct 2024 16:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18906v1</guid></item><item><title>GOAL: A Generalist Combinatorial Optimization Agent Learning</title><link>http://arxiv.org/abs/2406.15079v2</link><description>Machine Learning-based heuristics have recently shown impressive performancein solving a variety of hard combinatorial optimization problems (COPs).However they generally rely on a separate neural model, specialized and trainedfor each single problem. Any variation of a problem requires adjustment of itsmodel and re-training from scratch. In this paper, we propose GOAL (forGeneralist combinatorial Optimization Agent Learning), a generalist modelcapable of efficiently solving multiple COPs and which can be fine-tuned tosolve new COPs. GOAL consists of a single backbone plus light-weightproblem-specific adapters for input and output processing. The backbone isbased on a new form of mixed-attention blocks which allows to handle problemsdefined on graphs with arbitrary combinations of node, edge and instance-levelfeatures. Additionally, problems which involve heterogeneous types of nodes oredges are handled through a novel multi-type transformer architecture, wherethe attention blocks are duplicated to attend the meaningful combinations oftypes while relying on the same shared parameters. We train GOAL on a set ofrouting, scheduling and classic graph problems and show that it is onlyslightly inferior to the specialized baselines while being the first multi-taskmodel that solves a wide range of COPs. Finally we showcase the strong transferlearning capacity of GOAL by fine-tuning it on several new problems. Our codeis available at https://github.com/naver/goal-co/.</description><author>Darko Drakulic, Sofia Michel, Jean-Marc Andreoli</author><pubDate>Thu, 24 Oct 2024 16:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.15079v2</guid></item><item><title>Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2403.02107v3</link><description>The vast majority of Reinforcement Learning methods is largely impacted bythe computation effort and data requirements needed to obtain effectiveestimates of action-value functions, which in turn determine the quality of theoverall performance and the sample-efficiency of the learning procedure.Typically, action-value functions are estimated through an iterative schemethat alternates the application of an empirical approximation of the Bellmanoperator and a subsequent projection step onto a considered function space. Ithas been observed that this scheme can be potentially generalized to carry outmultiple iterations of the Bellman operator at once, benefiting the underlyinglearning algorithm. However, till now, it has been challenging to effectivelyimplement this idea, especially in high-dimensional problems. In this paper, weintroduce iterated $Q$-Network (i-QN), a novel principled approach that enablesmultiple consecutive Bellman updates by learning a tailored sequence ofaction-value functions where each serves as the target for the next. We showthat i-QN is theoretically grounded and that it can be seamlessly used invalue-based and actor-critic methods. We empirically demonstrate the advantagesof i-QN in Atari $2600$ games and MuJoCo continuous control problems.</description><author>Théo Vincent, Daniel Palenicek, Boris Belousov, Jan Peters, Carlo D'Eramo</author><pubDate>Thu, 24 Oct 2024 16:50:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02107v3</guid></item><item><title>Disentangled Representation Learning with the Gromov-Monge Gap</title><link>http://arxiv.org/abs/2407.07829v2</link><description>Learning disentangled representations from unlabelled data is a fundamentalchallenge in machine learning. Solving it may unlock other problems, such asgeneralization, interpretability, or fairness. Although remarkably challengingto solve in theory, disentanglement is often achieved in practice through priormatching. Furthermore, recent works have shown that prior matching approachescan be enhanced by leveraging geometrical considerations, e.g., by learningrepresentations that preserve geometric features of the data, such as distancesor angles between points. However, matching the prior while preservinggeometric features is challenging, as a mapping that fully preserves thesefeatures while aligning the data distribution with the prior does not exist ingeneral. To address these challenges, we introduce a novel approach todisentangled representation learning based on quadratic optimal transport. Weformulate the problem using Gromov-Monge maps that transport one distributiononto another with minimal distortion of predefined geometric features,preserving them as much as can be achieved. To compute such maps, we proposethe Gromov-Monge-Gap (GMG), a regularizer quantifying whether a map moves areference distribution with minimal geometry distortion. We demonstrate theeffectiveness of our approach for disentanglement across four standardbenchmarks, outperforming other methods leveraging geometric considerations.</description><author>Théo Uscidda, Luca Eyring, Karsten Roth, Fabian Theis, Zeynep Akata, Marco Cuturi</author><pubDate>Thu, 24 Oct 2024 16:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07829v2</guid></item><item><title>Localizing and Mitigating Errors in Long-form Question Answering</title><link>http://arxiv.org/abs/2407.11930v2</link><description>Long-form question answering (LFQA) aims to provide thorough and in-depthanswers to complex questions, enhancing comprehension. However, such detailedresponses are prone to hallucinations and factual inconsistencies, challengingtheir faithful evaluation. This work introduces HaluQuestQA, the firsthallucination dataset with localized error annotations for human-written andmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8kspan-level error annotations for five different error types by expertannotators, along with preference judgments. Using our collected data, wethoroughly analyze the shortcomings of long-form answers and find that theylack comprehensiveness and provide unhelpful references. We train an automaticfeedback model on this dataset that predicts error spans with incompleteinformation and provides associated explanations. Finally, we propose aprompt-based approach, Error-informed refinement, that uses signals from thelearned feedback model to refine generated answers, which we show reduceserrors and improves answer quality across multiple models. Furthermore, humansfind answers generated by our approach comprehensive and highly prefer them(84%) over the baseline answers.</description><author>Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych</author><pubDate>Thu, 24 Oct 2024 16:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11930v2</guid></item><item><title>Modulated Adaptive Fourier Neural Operators for Temporal Interpolation of Weather Forecasts</title><link>http://arxiv.org/abs/2410.18904v1</link><description>Weather and climate data are often available at limited temporal resolution,either due to storage limitations, or in the case of weather forecast modelsbased on deep learning, their inherently long time steps. The coarse temporalresolution makes it difficult to capture rapidly evolving weather events. Toaddress this limitation, we introduce an interpolation model that reconstructsthe atmospheric state between two points in time for which the state is known.The model makes use of a novel network layer that modifies the adaptive Fourierneural operator (AFNO), which has been previously used in weather predictionand other applications of machine learning to physics problems. The modulatedAFNO (ModAFNO) layer takes an embedding, here computed from the interpolationtarget time, as an additional input and applies a learned shift-scale operationinside the AFNO layers to adapt them to the target time. Thus, one model can beused to produce all intermediate time steps. Trained to interpolate between twotime steps 6 h apart, the ModAFNO-based interpolation model produces 1 hresolution intermediate time steps that are visually nearly indistinguishablefrom the actual corresponding 1 h resolution data. The model reduces the RMSEloss of reconstructing the intermediate steps by approximately 50% compared tolinear interpolation. We also demonstrate its ability to reproduce thestatistics of extreme weather events such as hurricanes and heat waves betterthan 6 h resolution data. The ModAFNO layer is generic and is expected to beapplicable to other problems, including weather forecasting with tunable leadtime.</description><author>Jussi Leinonen, Boris Bonev, Thorsten Kurth, Yair Cohen</author><pubDate>Thu, 24 Oct 2024 16:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18904v1</guid></item><item><title>LLMs for Extremely Low-Resource Finno-Ugric Languages</title><link>http://arxiv.org/abs/2410.18902v1</link><description>The advancement of large language models (LLMs) has predominantly focused onhigh-resource languages, leaving low-resource languages, such as those in theFinno-Ugric family, significantly underrepresented. This paper addresses thisgap by focusing on V\~oro, Livonian, and Komi. We cover almost the entire cycleof LLM creation, from data collection to instruction tuning and evaluation. Ourcontributions include developing multilingual base and instruction-tunedmodels; creating evaluation benchmarks, including the smugri-MT-benchmulti-turn conversational benchmark; and conducting human evaluation. We intendfor this work to promote linguistic diversity, ensuring that lesser-resourcedlanguages can benefit from advancements in NLP.</description><author>Taido Purason, Hele-Andra Kuulmets, Mark Fishel</author><pubDate>Thu, 24 Oct 2024 16:48:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18902v1</guid></item><item><title>Comparative Analysis of Indicators for Multiobjective Diversity Optimization</title><link>http://arxiv.org/abs/2410.18900v1</link><description>Indicator-based (multiobjective) diversity optimization aims at finding a setof near (Pareto-)optimal solutions that maximizes a diversity indicator, wherediversity is typically interpreted as the number of essentially differentsolutions. Whereas, in the first diversity-oriented evolutionary multiobjectiveoptimization algorithm, the NOAH algorithm by Ulrich and Thiele, the SolowPolasky Diversity (also related to Magnitude) served as a metric, otherdiversity indicators might be considered, such as the parameter-free Max-MinDiversity, and the Riesz s-Energy, which features uniformly distributedsolution sets. In this paper, focusing on multiobjective diversityoptimization, we discuss different diversity indicators from the perspective ofindicator-based evolutionary algorithms (IBEA) with multiple objectives. Weexamine theoretical, computational, and practical properties of theseindicators, such as monotonicity in species, twinning, monotonicity indistance, strict monotonicity in distance, uniformity of maximizing point sets,computational effort for a set of size~n, single-point contributions, subsetselection, and submodularity. We present new theorems -- including a proof ofthe NP-hardness of the Riesz s-Energy Subset Selection Problem -- andconsolidate existing results from the literature. In the second part, we applythese indicators in the NOAH algorithm and analyze search dynamics through anexample. We examine how optimizing with one indicator affects the performanceof others and propose NOAH adaptations specific to the Max-Min indicator.</description><author>Ksenia Pereverdieva, André Deutz, Tessa Ezendam, Thomas Bäck, Hèrm Hofmeyer, Michael T. M. Emmerich</author><pubDate>Thu, 24 Oct 2024 16:40:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18900v1</guid></item><item><title>XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference</title><link>http://arxiv.org/abs/2404.15420v2</link><description>In-context learning (ICL) approaches typically leverage prompting tocondition decoder-only language model generation on reference information.Just-in-time processing of a context is inefficient due to the quadratic costof self-attention operations, and caching is desirable. However, cachingtransformer states can easily require almost as much space as the modelparameters. When the right context isn't known in advance, caching ICL can bechallenging. This work addresses these limitations by introducing models that,inspired by the encoder-decoder architecture, use cross-attention to conditiongeneration on reference text without the prompt. More precisely, we leveragepre-trained decoder-only models and only train a small number of added layers.We use Question-Answering (QA) as a testbed to evaluate the ability of ourmodels to perform conditional generation and observe that they outperform ICL,are comparable to fine-tuned prompted LLMs, and drastically reduce the spacefootprint relative to standard KV caching by two orders of magnitude.</description><author>João Monteiro, Étienne Marcotte, Pierre-André Noël, Valentina Zantedeschi, David Vázquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian</author><pubDate>Thu, 24 Oct 2024 16:40:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15420v2</guid></item><item><title>ArterialNet: Reconstructing Arterial Blood Pressure Waveform with Wearable Pulsatile Signals, a Cohort-Aware Approach</title><link>http://arxiv.org/abs/2410.18895v1</link><description>Continuous arterial blood pressure (ABP) monitoring is invasive but essentialfor hemodynamic monitoring. Recent techniques have reconstructed ABPnon-invasively using pulsatile signals but produced inaccurate systolic anddiastolic blood pressure (SBP and DBP) values and were sensitive to individualvariability. ArterialNet integrates generalized pulsatile-to-ABP signaltranslation and personalized feature extraction using hybrid loss functions andregularization. We validated ArterialNet using the MIMIC-III dataset andachieved a root mean square error (RMSE) of 5.41 mmHg, with at least a 58%lower standard deviation. ArterialNet reconstructed ABP with an RMSE of 7.99mmHg in remote health scenarios. ArterialNet achieved superior performance inABP reconstruction and SBP and DBP estimations, with significantly reducedsubject variance, demonstrating its potential in remote health settings. Wealso ablated ArterialNet architecture to investigate the contributions of eachcomponent and evaluated its translational impact and robustness by conducting aseries of ablations on data quality and availability.</description><author>Sicong Huang, Roozbeh Jafari, Bobak J. Mortazavi</author><pubDate>Thu, 24 Oct 2024 16:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18895v1</guid></item><item><title>AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models</title><link>http://arxiv.org/abs/2403.00953v3</link><description>Rare diseases affect millions worldwide but often face limited research focusdue to their low prevalence. This results in prolonged diagnoses and a lack ofapproved therapies. Recent advancements in Large Language Models (LLMs) haveshown promise in automating the extraction of medical information, offeringpotential to improve medical diagnosis and management. However, most LLMs lackprofessional medical knowledge, especially concerning rare diseases, andstruggle to handle the latest rare disease information. They also cannoteffectively manage rare disease data and are not directly suitable fordiagnosis and management tasks. Our objective is to create an end-to-end systemcalled AutoRD, which automates the extraction of information from medical textsabout rare diseases, focusing on entities and their relations. AutoRDintegrates up-to-date structured knowledge and demonstrates superiorperformance in rare disease extraction tasks. We conduct various experiments toevaluate AutoRD's performance, aiming to surpass common LLMs and traditionalmethods.</description><author>Lang Cao, Jimeng Sun, Adam Cross</author><pubDate>Thu, 24 Oct 2024 16:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00953v3</guid></item><item><title>Meta-Learning with Heterogeneous Tasks</title><link>http://arxiv.org/abs/2410.18894v1</link><description>Meta-learning is a general approach to equip machine learning models with theability to handle few-shot scenarios when dealing with many tasks. Mostexisting meta-learning methods work based on the assumption that all tasks areof equal importance. However, real-world applications often presentheterogeneous tasks characterized by varying difficulty levels, noise intraining samples, or being distinctively different from most other tasks. Inthis paper, we introduce a novel meta-learning method designed to effectivelymanage such heterogeneous tasks by employing rank-based task-level learningobjectives, Heterogeneous Tasks Robust Meta-learning (HeTRoM). HeTRoM isproficient in handling heterogeneous tasks, and it prevents easy tasks fromoverwhelming the meta-learner. The approach allows for an efficient iterativeoptimization algorithm based on bi-level optimization, which is then improvedby integrating statistical guidance. Our experimental results demonstrate thatour method provides flexibility, enabling users to adapt to diverse tasksettings and enhancing the meta-learner's overall performance.</description><author>Zhaofeng Si, Shu Hu, Kaiyi Ji, Siwei Lyu</author><pubDate>Thu, 24 Oct 2024 16:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18894v1</guid></item><item><title>Creating and Repairing Robot Programs in Open-World Domains</title><link>http://arxiv.org/abs/2410.18893v1</link><description>Using Large Language Models (LLMs) to produce robot programs from naturallanguage has allowed for robot systems that can complete a higher diversity oftasks. However, LLM-generated programs may be faulty, either due to ambiguityin instructions, misinterpretation of the desired task, or missing informationabout the world state. As these programs run, the state of the world changesand they gather new information. When a failure occurs, it is important thatthey recover from the current world state and avoid repeating steps that theythey previously completed successfully. We propose RoboRepair, a system whichtraces the execution of a program up until error, and then runs an LLM-producedrecovery program that minimizes repeated actions. To evaluate the efficacy of our system, we create a benchmark consisting ofeleven tasks with various error conditions that require the generation of arecovery program. We compare the efficiency of the recovery program to a planbuilt with an oracle that has foreknowledge of future errors.</description><author>Claire Schlesinger, Arjun Guha, Joydeep Biswas</author><pubDate>Thu, 24 Oct 2024 16:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18893v1</guid></item><item><title>Improving Small-Scale Large Language Models Function Calling for Reasoning Tasks</title><link>http://arxiv.org/abs/2410.18890v1</link><description>Recent advancements in Large Language Models (LLMs) have demonstratedexceptional capabilities in natural language understanding and generation.While these models excel in general complex reasoning tasks, they still facechallenges in mathematical problem-solving and logical reasoning. To addressthese limitations, researchers have explored function calling abilities,allowing LLMs to execute provided functions and utilize their outputs for taskcompletion. However, concentrating on specific tasks can be very inefficientfor large-scale LLMs to be used, because of the expensive cost of training andinference stages they need in terms of computational resources. This studyintroduces a novel framework for training smaller language models in functioncalling, focusing on specific logical and mathematical reasoning tasks. Theapproach aims to improve performances of small-scale models for these tasksusing function calling, ensuring a high level of accuracy. Our frameworkemploys an agent that, given a problem and a set of callable functions, queriesthe LLM by injecting a description and examples of the usable functions intothe prompt and managing their calls in a step-by-step reasoning chain. Thisprocess is used to create a dataset of correct and incorrect reasoning chainchat completions from a large-scale LLM. This dataset is used to train asmaller LLM using Reinforcement Learning from Human Feedback (RLHF),specifically employing the Direct Preference Optimization (DPO) technique.Experimental results demonstrate how the proposed approach balances thetrade-off between model size and performance, improving the ability of functioncalling for reasoning tasks, in smaller models.</description><author>Graziano A. Manduzio, Federico A. Galatolo, Mario G. C. A. Cimino, Enzo Pasquale Scilingo, Lorenzo Cominelli</author><pubDate>Thu, 24 Oct 2024 16:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18890v1</guid></item><item><title>Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance</title><link>http://arxiv.org/abs/2410.18889v1</link><description>NLP benchmarks rely on standardized datasets for training and evaluatingmodels and are crucial for advancing the field. Traditionally, expertannotations ensure high-quality labels; however, the cost of expert annotationdoes not scale well with the growing demand for larger datasets required bymodern models. While crowd-sourcing provides a more scalable solution, it oftencomes at the expense of annotation precision and consistency. Recentadvancements in large language models (LLMs) offer new opportunities to enhancethe annotation process, particularly for detecting label errors in existingdatasets. In this work, we consider the recent approach of LLM-as-a-judge,leveraging an ensemble of LLMs to flag potentially mislabeled examples. Througha case study of four datasets from the TRUE benchmark, covering different tasksand domains, we empirically analyze the labeling quality of existing datasets,and compare expert, crowd-sourced, and our LLM-based annotations in terms ofagreement, label quality, and efficiency, demonstrating the strengths andlimitations of each annotation method. Our findings reveal a substantial numberof label errors, which, when corrected, induce a significant upward shift inreported model performance. This suggests that many of the LLMs so-calledmistakes are due to label errors rather than genuine model failures.Additionally, we discuss the implications of mislabeled data and proposemethods to mitigate them in training to improve model performance.</description><author>Omer Nahum, Nitay Calderon, Orgad Keller, Idan Szpektor, Roi Reichart</author><pubDate>Thu, 24 Oct 2024 16:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18889v1</guid></item><item><title>Generalizable, Fast, and Accurate DeepQSPR with fastprop</title><link>http://arxiv.org/abs/2404.02058v4</link><description>Quantitative Structure Property Relationship studies aim to define a mappingbetween molecular structure and arbitrary quantities of interest. This washistorically accomplished via the development of descriptors which requiressignificant domain expertise and struggles to generalize. Thus the field hasmorphed into Molecular Property Prediction and been given over to learnedrepresentations which are highly generalizable. The paper introduces fastprop,a DeepQSPR framework which uses a cogent set of molecular level descriptors tomeet and exceed the performance of learned representations on diverse datasetsin dramatically less time. fastprop is freely available on github atgithub.com/JacksonBurns/fastprop.</description><author>Jackson Burns, William Green</author><pubDate>Thu, 24 Oct 2024 16:18:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02058v4</guid></item><item><title>A Survey of Multimodal Sarcasm Detection</title><link>http://arxiv.org/abs/2410.18882v1</link><description>Sarcasm is a rhetorical device that is used to convey the opposite of theliteral meaning of an utterance. Sarcasm is widely used on social media andother forms of computer-mediated communication motivating the use ofcomputational models to identify it automatically. While the clear majority ofapproaches to sarcasm detection have been carried out on text only, sarcasmdetection often requires additional information present in tonality, facialexpression, and contextual images. This has led to the introduction ofmultimodal models, opening the possibility to detect sarcasm in multiplemodalities such as audio, images, text, and video. In this paper, we presentthe first comprehensive survey on multimodal sarcasm detection - henceforth MSD- to date. We survey papers published between 2018 and 2023 on the topic, anddiscuss the models and datasets used for this task. We also present futureresearch directions in MSD.</description><author>Shafkat Farabi, Tharindu Ranasinghe, Diptesh Kanojia, Yu Kong, Marcos Zampieri</author><pubDate>Thu, 24 Oct 2024 16:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18882v1</guid></item><item><title>Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences</title><link>http://arxiv.org/abs/2410.18881v1</link><description>One-step text-to-image generator models offer advantages such as swiftinference efficiency, flexible architectures, and state-of-the-art generationperformance. In this paper, we study the problem of aligning one-step generatormodels with human preferences for the first time. Inspired by the success ofreinforcement learning using human feedback (RLHF), we formulate the alignmentproblem as maximizing expected human reward functions while adding an IntegralKullback-Leibler divergence term to prevent the generator from diverging. Byovercoming technical challenges, we introduce Diff-Instruct++ (DI++), thefirst, fast-converging and image data-free human preference alignment methodfor one-step text-to-image generators. We also introduce novel theoreticalinsights, showing that using CFG for diffusion distillation is secretly doingRLHF with DI++. Such an interesting finding brings understanding and potentialcontributions to future research involving CFG. In the experiment sections, wealign both UNet-based and DiT-based one-step generators using DI++, which usethe Stable Diffusion 1.5 and the PixelArt-$\alpha$ as the reference diffusionprocesses. The resulting DiT-based one-step text-to-image model achieves astrong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCOvalidation prompt dataset. It also achieves a leading Human preference Score(HPSv2.0) of 28.48, outperforming other open-sourced models such as StableDiffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\alpha$. Both theoreticalcontributions and empirical evidence indicate that DI++ is a stronghuman-preference alignment approach for one-step text-to-image models.</description><author>Weijian Luo</author><pubDate>Thu, 24 Oct 2024 16:17:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18881v1</guid></item><item><title>Multi-Class Abnormality Classification in Video Capsule Endoscopy Using Deep Learning</title><link>http://arxiv.org/abs/2410.18879v1</link><description>This report outlines Team Seq2Cure's deep learning approach for the CapsuleVision 2024 Challenge, leveraging an ensemble of convolutional neural networks(CNNs) and transformer-based architectures for multi-class abnormalityclassification in video capsule endoscopy frames. The dataset comprised over50,000 frames from three public sources and one private dataset, labeled across10 abnormality classes. To overcome the limitations of traditional CNNs incapturing global context, we integrated CNN and transformer models within amulti-model ensemble. Our approach achieved a balanced accuracy of 86.34percent and a mean AUC-ROC score of 0.9908 on the validation set, withsignificant improvements in classifying complex abnormalities. Code isavailable at http://github.com/arnavs04/capsule-vision-2024 .</description><author>Arnav Samal, Ranya</author><pubDate>Thu, 24 Oct 2024 16:13:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18879v1</guid></item><item><title>Guiding Empowerment Model: Liberating Neurodiversity in Online Higher Education</title><link>http://arxiv.org/abs/2410.18876v1</link><description>In this innovative practice full paper, we address the equity gap forneurodivergent and situationally limited learners by identifying the spectrumof dynamic factors that impact learning and function. Educators have shown agrowing interest in identifying learners' cognitive abilities and learningpreferences to measure their impact on academic achievement. Often institutionsemploy one-size-fits-all approaches leaving the burden on disabled students toself-advocate or tolerate inadequate support. Emerging frameworks guideneurodivergent learners through instructional approaches, such as onlineeducation. However, these frameworks fail to address holistic environmentalneeds or recommend technology interventions, particularly for those withundisclosed learning or developmental disabilities and situational limitations.In this article, we integrate a neurodivergent perspective through secondaryresearch of around 100 articles to introduce a Guiding Empowerment Modelinvolving key cognitive and situational factors that contextualize day-to-dayexperiences affecting learner ability. We synthesize three sample studentprofiles that highlight user problems in functioning. We use this model toevaluate sample learning platform features and other supportive technologysolutions. The proposed approach augments frameworks such as Universal Designfor Learning to consider factors including various sensory processingdifferences, social connection challenges, and environmental limitations. Wesuggest that by applying the mode through technology-enabled features such ascustomizable task management, guided varied content access, and guidedmulti-modal collaboration, major learning barriers of neurodivergent andsituationally limited learners will be removed to activate the successfulpursuit of their academic goals.</description><author>Hannah Beaux, Pegah Karimi, Otilia Pop, Rob Clark</author><pubDate>Thu, 24 Oct 2024 16:05:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18876v1</guid></item><item><title>Exploring the Universe with SNAD: Anomaly Detection in Astronomy</title><link>http://arxiv.org/abs/2410.18875v1</link><description>SNAD is an international project with a primary focus on detectingastronomical anomalies within large-scale surveys, using active learning andother machine learning algorithms. The work carried out by SNAD not onlycontributes to the discovery and classification of various astronomicalphenomena but also enhances our understanding and implementation of machinelearning techniques within the field of astrophysics. This paper provides areview of the SNAD project and summarizes the advancements and achievementsmade by the team over several years.</description><author>Alina A. Volnova, Patrick D. Aleo, Anastasia Lavrukhina, Etienne Russeil, Timofey Semenikhin, Emmanuel Gangler, Emille E. O. Ishida, Matwey V. Kornilov, Vladimir Korolev, Konstantin Malanchev, Maria V. Pruzhinskaya, Sreevarsha Sreejith</author><pubDate>Thu, 24 Oct 2024 16:05:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18875v1</guid></item><item><title>A Novel Interpretability Metric for Explaining Bias in Language Models: Applications on Multilingual Models from Southeast Asia</title><link>http://arxiv.org/abs/2410.15464v2</link><description>Work on bias in pretrained language models (PLMs) focuses on bias evaluationand mitigation and fails to tackle the question of bias attribution andexplainability. We propose a novel metric, the $\textit{bias attributionscore}$, which draws from information theory to measure token-levelcontributions to biased behavior in PLMs. We then demonstrate the utility ofthis metric by applying it on multilingual PLMs, including models fromSoutheast Asia which have not yet been thoroughly examined in bias evaluationliterature. Our results confirm the presence of sexist and homophobic bias inSoutheast Asian PLMs. Interpretability and semantic analyses also reveal thatPLM bias is strongly induced by words relating to crime, intimaterelationships, and helping among other discursive categories, suggesting thatthese are topics where PLMs strongly reproduce bias from pretraining data andwhere PLMs should be used with more caution.</description><author>Lance Calvin Lim Gamboa, Mark Lee</author><pubDate>Thu, 24 Oct 2024 15:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15464v2</guid></item><item><title>Hierarchical Multi-agent Reinforcement Learning for Cyber Network Defense</title><link>http://arxiv.org/abs/2410.17351v2</link><description>Recent advances in multi-agent reinforcement learning (MARL) have createdopportunities to solve complex real-world tasks. Cybersecurity is a notableapplication area, where defending networks against sophisticated adversariesremains a challenging task typically performed by teams of security operators.In this work, we explore novel MARL strategies for building autonomous cybernetwork defenses that address challenges such as large policy spaces, partialobservability, and stealthy, deceptive adversarial strategies. To facilitateefficient and generalized learning, we propose a hierarchical Proximal PolicyOptimization (PPO) architecture that decomposes the cyber defense task intospecific sub-tasks like network investigation and host recovery. Our approachinvolves training sub-policies for each sub-task using PPO enhanced with domainexpertise. These sub-policies are then leveraged by a master defense policythat coordinates their selection to solve complex network defense tasks.Furthermore, the sub-policies can be fine-tuned and transferred with minimalcost to defend against shifts in adversarial behavior or changes in networksettings. We conduct extensive experiments using CybORG Cage 4, thestate-of-the-art MARL environment for cyber defense. Comparisons with multiplebaselines across different adversaries show that our hierarchical learningapproach achieves top performance in terms of convergence speed, episodicreturn, and several interpretable metrics relevant to cybersecurity, includingthe fraction of clean machines on the network, precision, and false positiveson recoveries.</description><author>Aditya Vikram Singh, Ethan Rathbun, Emma Graham, Lisa Oakley, Simona Boboila, Alina Oprea, Peter Chin</author><pubDate>Thu, 24 Oct 2024 15:57:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17351v2</guid></item><item><title>End-to-end Training for Recommendation with Language-based User Profiles</title><link>http://arxiv.org/abs/2410.18870v1</link><description>Many online platforms maintain user profiles for personalization.Unfortunately, these profiles are typically not interpretable or easilymodifiable by the user. To remedy this shortcoming, we explore naturallanguage-based user profiles, as they promise enhanced transparency andscrutability of recommender systems. While existing work has shown thatlanguage-based profiles from standard LLMs can be effective, such generalistLLMs are unlikely to be optimal for this task. In this paper, we introduceLangPTune, the first end-to-end learning method for training LLMs to producelanguage-based user profiles that optimize recommendation effectiveness.Through comprehensive evaluations of LangPTune across various trainingconfigurations and benchmarks, we demonstrate that our approach significantlyoutperforms existing profile-based methods. In addition, it approachesperformance levels comparable to state-of-the-art, less transparent recommendersystems, providing a robust and interpretable alternative to conventionalsystems. Finally, we validate the relative interpretability of theselanguage-based user profiles through user studies involving crowdworkers andGPT-4-based evaluations. Implementation of LangPTune can be found athttps://github.com/ZhaolinGao/LangPTune.</description><author>Zhaolin Gao, Joyce Zhou, Yijia Dai, Thorsten Joachims</author><pubDate>Thu, 24 Oct 2024 15:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18870v1</guid></item><item><title>A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics</title><link>http://arxiv.org/abs/2410.18868v1</link><description>By incorporating physical consistency as inductive bias, deep neural networksdisplay increased generalization capabilities and data efficiency in learningnonlinear dynamic models. However, the complexity of these models generallyincreases with the system dimensionality, requiring larger datasets, morecomplex deep networks, and significant computational effort. We propose a novelgeometric network architecture to learn physically-consistent reduced-orderdynamic parameters that accurately describe the original high-dimensionalsystem behavior. This is achieved by building on recent advances in model-orderreduction and by adopting a Riemannian perspective to jointly learn astructure-preserving latent space and the associated low-dimensional dynamics.Our approach enables accurate long-term predictions of the high-dimensionaldynamics of rigid and deformable systems with increased data efficiency byinferring interpretable and physically plausible reduced Lagrangian models.</description><author>Katharina Friedl, Noémie Jaquier, Jens Lundell, Tamim Asfour, Danica Kragic</author><pubDate>Thu, 24 Oct 2024 15:53:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18868v1</guid></item><item><title>The Cat and Mouse Game: The Ongoing Arms Race Between Diffusion Models and Detection Methods</title><link>http://arxiv.org/abs/2410.18866v1</link><description>The emergence of diffusion models has transformed synthetic media generation,offering unmatched realism and control over content creation. Theseadvancements have driven innovation across fields such as art, design, andscientific visualization. However, they also introduce significant ethical andsocietal challenges, particularly through the creation of hyper-realisticimages that can facilitate deepfakes, misinformation, and unauthorizedreproduction of copyrighted material. In response, the need for effectivedetection mechanisms has become increasingly urgent. This review examines theevolving adversarial relationship between diffusion model development and theadvancement of detection methods. We present a thorough analysis ofcontemporary detection strategies, including frequency and spatial domaintechniques, deep learning-based approaches, and hybrid models that combinemultiple methodologies. We also highlight the importance of diverse datasetsand standardized evaluation metrics in improving detection accuracy andgeneralizability. Our discussion explores the practical applications of thesedetection systems in copyright protection, misinformation prevention, andforensic analysis, while also addressing the ethical implications of syntheticmedia. Finally, we identify key research gaps and propose future directions toenhance the robustness and adaptability of detection methods in line with therapid advancements of diffusion models. This review emphasizes the necessity ofa comprehensive approach to mitigating the risks associated with AI-generatedcontent in an increasingly digital world.</description><author>Linda Laurier, Ave Giulietta, Arlo Octavia, Meade Cleti</author><pubDate>Thu, 24 Oct 2024 15:51:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18866v1</guid></item><item><title>Omics-driven hybrid dynamic modeling of bioprocesses with uncertainty estimation</title><link>http://arxiv.org/abs/2410.18864v1</link><description>This work presents an omics-driven modeling pipeline that integratesmachine-learning tools to facilitate the dynamic modeling of multiscalebiological systems. Random forests and permutation feature importance areproposed to mine omics datasets, guiding feature selection and dimensionalityreduction for dynamic modeling. Continuous and differentiable machine-learningfunctions can be trained to link the reduced omics feature set to keycomponents of the dynamic model, resulting in a hybrid model. As proof ofconcept, we apply this framework to a high-dimensional proteomics dataset of$\textit{Saccharomyces cerevisiae}$. After identifying key intracellularproteins that correlate with cell growth, targeted dynamic experiments aredesigned, and key model parameters are captured as functions of the selectedproteins using Gaussian processes. This approach captures the dynamic behaviorof yeast strains under varying proteome profiles while estimating theuncertainty in the hybrid model's predictions. The outlined modeling frameworkis adaptable to other scenarios, such as integrating additional layers of omicsdata for more advanced multiscale biological systems, or employing alternativemachine-learning methods to handle larger datasets. Overall, this studyoutlines a strategy for leveraging omics data to inform multiscale dynamicmodeling in systems biology and bioprocess engineering.</description><author>Sebastián Espinel-Ríos, José Montaño López, José L. Avalos</author><pubDate>Thu, 24 Oct 2024 15:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18864v1</guid></item><item><title>Learning Mathematical Rules with Large Language Models</title><link>http://arxiv.org/abs/2410.16973v2</link><description>In this paper, we study the ability of large language models to learnspecific mathematical rules such as distributivity or simplifying equations. Wepresent an empirical analysis of their ability to generalize these rules, aswell as to reuse them in the context of word problems. For this purpose, weprovide a rigorous methodology to build synthetic data incorporating suchrules, and perform fine-tuning of large language models on such data. Ourexperiments show that our model can learn and generalize these rules to someextent, as well as suitably reuse them in the context of word problems.</description><author>Nelson Vadori, Antoine Gorceix, Bastien Le Chenadec, Ahmad Rammal, Manuela Veloso</author><pubDate>Thu, 24 Oct 2024 15:49:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16973v2</guid></item><item><title>Data Augmentation of Multivariate Sensor Time Series using Autoregressive Models and Application to Failure Prognostics</title><link>http://arxiv.org/abs/2410.16419v2</link><description>This work presents a novel data augmentation solution for non-stationarymultivariate time series and its application to failure prognostics. The methodextends previous work from the authors which is based on time-varyingautoregressive processes. It can be employed to extract key information from alimited number of samples and generate new synthetic samples in a way thatpotentially improves the performance of PHM solutions. This is especiallyvaluable in situations of data scarcity which are very usual in PHM, especiallyfor failure prognostics. The proposed approach is tested based on the CMAPSSdataset, commonly employed for prognostics experiments and benchmarks. AnAutoML approach from PHM literature is employed for automating the design ofthe prognostics solution. The empirical evaluation provides evidence that theproposed method can substantially improve the performance of PHM solutions.</description><author>Douglas Baptista de Souza, Bruno Paes Leao</author><pubDate>Thu, 24 Oct 2024 15:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16419v2</guid></item><item><title>RClicks: Realistic Click Simulation for Benchmarking Interactive Segmentation</title><link>http://arxiv.org/abs/2410.11722v2</link><description>The emergence of Segment Anything (SAM) sparked research interest in thefield of interactive segmentation, especially in the context of image editingtasks and speeding up data annotation. Unlike common semantic segmentation,interactive segmentation methods allow users to directly influence their outputthrough prompts (e.g. clicks). However, click patterns in real-worldinteractive segmentation scenarios remain largely unexplored. Most methods relyon the assumption that users would click in the center of the largest erroneousarea. Nevertheless, recent studies show that this is not always the case. Thus,methods may have poor performance in real-world deployment despite high metricsin a baseline benchmark. To accurately simulate real-user clicks, we conducteda large crowdsourcing study of click patterns in an interactive segmentationscenario and collected 475K real-user clicks. Drawing on ideas from saliencytasks, we develop a clickability model that enables sampling clicks, whichclosely resemble actual user inputs. Using our model and dataset, we proposeRClicks benchmark for a comprehensive comparison of existing interactivesegmentation methods on realistic clicks. Specifically, we evaluate not onlythe average quality of methods, but also the robustness w.r.t. click patterns.According to our benchmark, in real-world usage interactive segmentation modelsmay perform worse than it has been reported in the baseline benchmark, and mostof the methods are not robust. We believe that RClicks is a significant steptowards creating interactive segmentation methods that provide the best userexperience in real-world cases.</description><author>Anton Antonov, Andrey Moskalenko, Denis Shepelev, Alexander Krapukhin, Konstantin Soshin, Anton Konushin, Vlad Shakhuro</author><pubDate>Thu, 24 Oct 2024 15:48:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11722v2</guid></item><item><title>FedSPD: A Soft-clustering Approach for Personalized Decentralized Federated Learning</title><link>http://arxiv.org/abs/2410.18862v1</link><description>Federated learning has recently gained popularity as a framework fordistributed clients to collaboratively train a machine learning model usinglocal data. While traditional federated learning relies on a central server formodel aggregation, recent advancements adopt a decentralized framework,enabling direct model exchange between clients and eliminating the single pointof failure. However, existing decentralized frameworks often assume all clientstrain a shared model. Personalizing each client's model can enhanceperformance, especially with heterogeneous client data distributions. Wepropose FedSPD, an efficient personalized federated learning algorithm for thedecentralized setting, and show that it learns accurate models even inlow-connectivity networks. To provide theoretical guarantees on convergence, weintroduce a clustering-based framework that enables consensus on models fordistinct data clusters while personalizing to unique mixtures of these clustersat different clients. This flexibility, allowing selective model updates basedon data distribution, substantially reduces communication costs compared toprior work on personalized federated learning in decentralized settings.Experimental results on real-world datasets show that FedSPD outperformsmultiple decentralized variants of personalized federated learning algorithms,especially in scenarios with low-connectivity networks.</description><author>I-Cheng Lin, Osman Yagan, Carlee Joe-Wong</author><pubDate>Thu, 24 Oct 2024 15:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18862v1</guid></item><item><title>On high-dimensional modifications of the nearest neighbor classifier</title><link>http://arxiv.org/abs/2407.05145v3</link><description>Nearest neighbor classifier is arguably the most simple and popularnonparametric classifier available in the literature. However, due to theconcentration of pairwise distances and the violation of the neighborhoodstructure, this classifier often suffers in high-dimension, low-sample size(HDLSS) situations, especially when the scale difference between the competingclasses dominates their location difference. Several attempts have been made inthe literature to take care of this problem. In this article, we discuss someof these existing methods and propose some new ones. We carry out sometheoretical investigations in this regard and analyze several simulated andbenchmark datasets to compare the empirical performances of proposed methodswith some of the existing ones.</description><author>Annesha Ghosh, Deep Ghoshal, Bilol Banerjee, Anil K. Ghosh</author><pubDate>Thu, 24 Oct 2024 15:47:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05145v3</guid></item><item><title>Predicting the Performance of Foundation Models via Agreement-on-the-Line</title><link>http://arxiv.org/abs/2404.01542v2</link><description>Estimating the out-of-distribution performance in regimes where labels arescarce is critical to safely deploy foundation models. Recently, it was shownthat ensembles of neural networks observe the phenomena"agreement-on-the-line", which can be leveraged to reliably predict OODperformance without labels. However, in contrast to classical neural networksthat are trained on in-distribution data from scratch for numerous epochs,foundation models undergo minimal finetuning from heavily pretrained weights,which may reduce the ensemble diversity needed to observeagreement-on-the-line. In our work, we demonstrate that when lightly finetuningmultiple runs from a single foundation model, the choice of randomness duringtraining (linear head initialization, data ordering, and data subsetting) canlead to drastically different levels of agreement-on-the-line in the resultingensemble. Surprisingly, only random head initialization is able to reliablyinduce agreement-on-the-line in finetuned foundation models across vision andlanguage benchmarks. Second, we demonstrate that ensembles of multiplefoundation models pretrained on different datasets but finetuned on the sametask can also show agreement-on-the-line. In total, by careful construction ofa diverse ensemble, we can utilize agreement-on-the-line-based methods topredict the OOD performance of foundation models with high precision.</description><author>Rahul Saxena, Taeyoun Kim, Aman Mehra, Christina Baek, Zico Kolter, Aditi Raghunathan</author><pubDate>Thu, 24 Oct 2024 15:47:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01542v2</guid></item><item><title>Provably Robust Watermarks for Open-Source Language Models</title><link>http://arxiv.org/abs/2410.18861v1</link><description>The recent explosion of high-quality language models has necessitated newmethods for identifying AI-generated text. Watermarking is a leading solutionand could prove to be an essential tool in the age of generative AI. Existingapproaches embed watermarks at inference and crucially rely on the largelanguage model (LLM) specification and parameters being secret, which makesthem inapplicable to the open-source setting. In this work, we introduce thefirst watermarking scheme for open-source LLMs. Our scheme works by modifyingthe parameters of the model, but the watermark can be detected from just theoutputs of the model. Perhaps surprisingly, we prove that our watermarks areunremovable under certain assumptions about the adversary's knowledge. Todemonstrate the behavior of our construction under concrete parameterinstantiations, we present experimental results with OPT-6.7B and OPT-1.3B. Wedemonstrate robustness to both token substitution and perturbation of the modelparameters. We find that the stronger of these attacks, the model-perturbationattack, requires deteriorating the quality score to 0 out of 100 in order tobring the detection rate down to 50%.</description><author>Miranda Christ, Sam Gunn, Tal Malkin, Mariana Raykova</author><pubDate>Thu, 24 Oct 2024 15:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18861v1</guid></item><item><title>DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations</title><link>http://arxiv.org/abs/2410.18860v1</link><description>Large Language Models (LLMs) often hallucinate, producing unfaithful orfactually incorrect outputs by misrepresenting the provided context orincorrectly recalling internal knowledge. Recent studies have identifiedspecific attention heads within the Transformer architecture, known asretrieval heads, responsible for extracting relevant contextual information. Wehypothesise that masking these retrieval heads can induce hallucinations andthat contrasting the outputs of the base LLM and the masked LLM can reducehallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads(DeCoRe), a novel training-free decoding strategy that amplifies informationfound in the context and model parameters. DeCoRe mitigates potentiallyhallucinated responses by dynamically contrasting the outputs of the base LLMand the masked LLM, using conditional entropy as a guide. Our extensiveexperiments confirm that DeCoRe significantly improves performance on tasksrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),instruction following (MemoTrap by 10.9%), and open-book question answering(NQ-Open by 2.4% and NQ-Swap by 5.5%).</description><author>Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, Tom Diethe, Philip Teare, Beatrice Alex, Pasquale Minervini, Amrutha Saseendran</author><pubDate>Thu, 24 Oct 2024 15:44:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18860v1</guid></item><item><title>Bilinear Sequence Regression: A Model for Learning from Long Sequences of High-dimensional Tokens</title><link>http://arxiv.org/abs/2410.18858v1</link><description>Current progress in artificial intelligence is centered around so-calledlarge language models that consist of neural networks processing long sequencesof high-dimensional vectors called tokens. Statistical physics providespowerful tools to study the functioning of learning with neural networks andhas played a recognized role in the development of modern machine learning. Thestatistical physics approach relies on simplified and analytically tractablemodels of data. However, simple tractable models for long sequences ofhigh-dimensional tokens are largely underexplored. Inspired by the crucial rolemodels such as the single-layer teacher-student perceptron (aka generalizedlinear regression) played in the theory of fully connected neural networks, inthis paper, we introduce and study the bilinear sequence regression (BSR) asone of the most basic models for sequences of tokens. We note that modernarchitectures naturally subsume the BSR model due to the skip connections.Building on recent methodological progress, we compute the Bayes-optimalgeneralization error for the model in the limit of long sequences ofhigh-dimensional tokens, and provide a message-passing algorithm that matchesthis performance. We quantify the improvement that optimal learning brings withrespect to vectorizing the sequence of tokens and learning via simple linearregression. We also unveil surprising properties of the gradient descentalgorithms in the BSR model.</description><author>Vittorio Erba, Emanuele Troiani, Luca Biggio, Antoine Maillard, Lenka Zdeborová</author><pubDate>Thu, 24 Oct 2024 15:44:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18858v1</guid></item><item><title>Probabilistic Language-Image Pre-Training</title><link>http://arxiv.org/abs/2410.18857v1</link><description>Vision-language models (VLMs) embed aligned image-text pairs into a jointspace but often rely on deterministic embeddings, assuming a one-to-onecorrespondence between images and texts. This oversimplifies real-worldrelationships, which are inherently many-to-many, with multiple captionsdescribing a single image and vice versa. We introduce ProbabilisticLanguage-Image Pre-training (ProLIP), the first probabilistic VLM pre-trainedon a billion-scale image-text dataset using only probabilistic objectives,achieving a strong zero-shot capability (e.g., 74.6% ImageNet zero-shotaccuracy with ViT-B/16). ProLIP efficiently estimates uncertainty by an"uncertainty token" without extra parameters. We also introduce a novelinclusion loss that enforces distributional inclusion relationships betweenimage-text pairs and between original and masked inputs. Experimentsdemonstrate that, by leveraging uncertainty estimates, ProLIP benefitsdownstream tasks and aligns with intuitive notions of uncertainty, e.g.,shorter texts being more uncertain and more general inputs including specificones. Utilizing text uncertainties, we further improve ImageNet accuracy from74.6% to 75.8% (under a few-shot setting), supporting the practical advantagesof our probabilistic approach. The code is available athttps://github.com/naver-ai/prolip</description><author>Sanghyuk Chun, Wonjae Kim, Song Park, Sangdoo Yun</author><pubDate>Thu, 24 Oct 2024 15:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18857v1</guid></item><item><title>Demystifying Large Language Models for Medicine: A Primer</title><link>http://arxiv.org/abs/2410.18856v1</link><description>Large language models (LLMs) represent a transformative class of AI toolscapable of revolutionizing various aspects of healthcare by generatinghuman-like responses across diverse contexts and adapting to novel tasksfollowing human instructions. Their potential application spans a broad rangeof medical tasks, such as clinical documentation, matching patients to clinicaltrials, and answering medical questions. In this primer paper, we propose anactionable guideline to help healthcare professionals more efficiently utilizeLLMs in their work, along with a set of best practices. This approach consistsof several main phases, including formulating the task, choosing LLMs, promptengineering, fine-tuning, and deployment. We start with the discussion ofcritical considerations in identifying healthcare tasks that align with thecore capabilities of LLMs and selecting models based on the selected task anddata, performance requirements, and model interface. We then review thestrategies, such as prompt engineering and fine-tuning, to adapt standard LLMsto specialized medical tasks. Deployment considerations, including regulatorycompliance, ethical guidelines, and continuous monitoring for fairness andbias, are also discussed. By providing a structured step-by-step methodology,this tutorial aims to equip healthcare professionals with the tools necessaryto effectively integrate LLMs into clinical practice, ensuring that thesepowerful technologies are applied in a safe, reliable, and impactful manner.</description><author>Qiao Jin, Nicholas Wan, Robert Leaman, Shubo Tian, Zhizheng Wang, Yifan Yang, Zifeng Wang, Guangzhi Xiong, Po-Ting Lai, Qingqing Zhu, Benjamin Hou, Maame Sarfo-Gyamfi, Gongbo Zhang, Aidan Gilson, Balu Bhasuran, Zhe He, Aidong Zhang, Jimeng Sun, Chunhua Weng, Ronald M. Summers, Qingyu Chen, Yifan Peng, Zhiyong Lu</author><pubDate>Thu, 24 Oct 2024 15:41:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18856v1</guid></item><item><title>ONCOPILOT: A Promptable CT Foundation Model For Solid Tumor Evaluation</title><link>http://arxiv.org/abs/2410.07908v3</link><description>Carcinogenesis is a proteiform phenomenon, with tumors emerging in variouslocations and displaying complex, diverse shapes. At the crucial intersectionof research and clinical practice, it demands precise and flexible assessment.However, current biomarkers, such as RECIST 1.1's long and short axismeasurements, fall short of capturing this complexity, offering an approximateestimate of tumor burden and a simplistic representation of a more intricateprocess. Additionally, existing supervised AI models face challenges inaddressing the variability in tumor presentations, limiting their clinicalutility. These limitations arise from the scarcity of annotations and themodels' focus on narrowly defined tasks. To address these challenges, we developed ONCOPILOT, an interactiveradiological foundation model trained on approximately 7,500 CT scans coveringthe whole body, from both normal anatomy and a wide range of oncological cases.ONCOPILOT performs 3D tumor segmentation using visual prompts like point-clickand bounding boxes, outperforming state-of-the-art models (e.g., nnUnet) andachieving radiologist-level accuracy in RECIST 1.1 measurements. The keyadvantage of this foundation model is its ability to surpass state-of-the-artperformance while keeping the radiologist in the loop, a capability thatprevious models could not achieve. When radiologists interactively refine thesegmentations, accuracy improves further. ONCOPILOT also acceleratesmeasurement processes and reduces inter-reader variability, facilitatingvolumetric analysis and unlocking new biomarkers for deeper insights. This AI assistant is expected to enhance the precision of RECIST 1.1measurements, unlock the potential of volumetric biomarkers, and improvepatient stratification and clinical care, while seamlessly integrating into theradiological workflow.</description><author>Léo Machado, Hélène Philippe, Élodie Ferreres, Julien Khlaut, Julie Dupuis, Korentin Le Floch, Denis Habip Gatenyo, Pascal Roux, Jules Grégory, Maxime Ronot, Corentin Dancette, Daniel Tordjman, Pierre Manceron, Paul Hérent</author><pubDate>Thu, 24 Oct 2024 15:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07908v3</guid></item><item><title>Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets</title><link>http://arxiv.org/abs/2410.01779v2</link><description>We prove rich algebraic structures of the solution space for 2-layer neuralnetworks with quadratic activation and $L_2$ loss, trained on reasoning tasksin Abelian group (e.g., modular addition). Such a rich structure enablesanalytical construction of global optimal solutions from partial solutions thatonly satisfy part of the loss, despite its high nonlinearity. We coin theframework as CoGO (Composing Global Optimizers). Specifically, we show that theweight space over different numbers of hidden nodes of the 2-layer network isequipped with a semi-ring algebraic structure, and the loss function to beoptimized consists of monomial potentials, which are ring homomorphism,allowing partial solutions to be composed into global ones by ring addition andmultiplication. Our experiments show that around $95\%$ of the solutionsobtained by gradient descent match exactly our theoretical constructions.Although the global optimizers constructed only required a small number ofhidden nodes, our analysis on gradient dynamics shows thatover-parameterization asymptotically decouples training dynamics and isbeneficial. We further show that training dynamics favors simpler solutionsunder weight decay, and thus high-order global optimizers such as perfectmemorization are unfavorable.</description><author>Yuandong Tian</author><pubDate>Thu, 24 Oct 2024 15:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01779v2</guid></item><item><title>DL-Polycube: Deep learning enhanced polycube method for high-quality hexahedral mesh generation and volumetric spline construction</title><link>http://arxiv.org/abs/2410.18852v1</link><description>In this paper, we present a novel algorithm that integrates deep learningwith the polycube method (DL-Polycube) to generate high-quality hexahedral(hex) meshes, which are then used to construct volumetric splines forisogeometric analysis. Our DL-Polycube algorithm begins by establishing aconnection between surface triangular meshes and polycube structures. We employdeep neural network to classify surface triangular meshes into theircorresponding polycube structures. Following this, we combine the acquiredpolycube structural information with unsupervised learning to perform surfacesegmentation of triangular meshes. This step addresses the issue ofsegmentation not corresponding to a polycube while reducing manualintervention. Quality hex meshes are then generated from the polycubestructures, with employing octree subdivision, parametric mapping and qualityimprovement techniques. The incorporation of deep learning for creatingpolycube structures, combined with unsupervised learning for segmentation ofsurface triangular meshes, substantially accelerates hex mesh generation.Finally, truncated hierarchical B-splines are constructed on the generated hexmeshes. We extract trivariate B\'ezier elements from these splines and applythem directly in isogeometric analysis. We offer several examples todemonstrate the robustness of our DL-Polycube algorithm.</description><author>Yuxuan Yu, Yuzhuo Fang, Hua Tong, Yongjie Jessica Zhang</author><pubDate>Thu, 24 Oct 2024 15:35:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18852v1</guid></item><item><title>We Augmented Whisper With kNN and You Won't Believe What Came Next</title><link>http://arxiv.org/abs/2410.18850v1</link><description>Speech recognition performance varies by language, domain, and speakercharacteristics such as accent, and fine-tuning a model on any of thesecategories may lead to catastrophic forgetting. $k$ nearest neighbor search($k$NN), first proposed for neural sequence decoders for natural languagegeneration (NLG) and machine translation (MT), is a non-parametric method thatcan instead adapt by building an external datastore that can then be searchedduring inference time, without training the underlying model. We show thatWhisper, a transformer end-to-end speech model, benefits from $k$NN. Weinvestigate the differences between the speech and text setups. We discussimplications for speaker adaptation, and analyze improvements by gender,accent, and age.</description><author>Maya K. Nachesa, Vlad Niculae</author><pubDate>Thu, 24 Oct 2024 15:32:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18850v1</guid></item><item><title>Ensemble architecture in polyp segmentation</title><link>http://arxiv.org/abs/2408.07262v2</link><description>In this research, we revisit the architecture of semantic segmentation andevaluate the models excelling in polyp segmentation. We introduce an integratedframework that harnesses the advantages of different models to attain anoptimal outcome. More specifically, we fuse the learned features fromconvolutional and transformer models for prediction, and we view this approachas an ensemble technique to enhance model performance. Our experiments on polypsegmentation reveal that the proposed architecture surpasses other top models,exhibiting improved learning capacity and resilience. The code is available athttps://github.com/HuangDLab/EnFormer.</description><author>Hao-Yun Hsu, Yi-Ching Cheng, Guan-Hua Huang</author><pubDate>Thu, 24 Oct 2024 15:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07262v2</guid></item><item><title>Expanding AI Awareness Through Everyday Interactions with AI: A Reflective Journal Study</title><link>http://arxiv.org/abs/2410.18845v1</link><description>As the application of AI continues to expand, students in technology programsare poised to be both producers and users of the technologies. They are alsopositioned to engage with AI applications within and outside the classroom.While focusing on the curriculum when examining students' AI knowledge iscommon, extending this connection to students' everyday interactions with AIprovides a more complete picture of their learning. In this paper, we explorestudent's awareness and engagement with AI in the context of school and theirdaily lives. Over six weeks, 22 undergraduate students participated in areflective journal study and submitted a weekly journal entry about theirinteractions with AI. The participants were recruited from a technology andsociety course that focuses on the implications of technology on people,communities, and processes. In their weekly journal entries, participantsreflected on interactions with AI on campus (coursework, advertises campusevents, or seminars) and beyond (social media, news, or conversations withfriends and family). The journal prompts were designed to help them thinkthrough what they had read, watched, or been told and reflect on thedevelopment of their own perspectives, knowledge, and literacy on the topic.Overall, students described nine categories of interactions: coursework, newsand current events, using software and applications, university events, socialmedia related to their work, personal discussions with friends and family,interacting with content, and gaming. Students reported that completing thediaries allowed them time for reflection and made them more aware of thepresence of AI in their daily lives and of its potential benefits anddrawbacks. This research contributes to the ongoing work on AI awareness andliteracy by bringing in perspectives from beyond a formal educational context.</description><author>Ashish Hingle, Aditya Johri</author><pubDate>Thu, 24 Oct 2024 15:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18845v1</guid></item><item><title>Learning to Explore with Lagrangians for Bandits under Unknown Linear Constraints</title><link>http://arxiv.org/abs/2410.18844v1</link><description>Pure exploration in bandits models multiple real-world problems, such astuning hyper-parameters or conducting user studies, where different safety,resource, and fairness constraints on the decision space naturally appear. Westudy these problems as pure exploration in multi-armed bandits with unknownlinear constraints, where the aim is to identify an $r$$\textit{-good feasiblepolicy}$. First, we propose a Lagrangian relaxation of the sample complexitylower bound for pure exploration under constraints. We show how this lowerbound evolves with the sequential estimation of constraints. Second, weleverage the Lagrangian lower bound and the properties of convex optimisationto propose two computationally efficient extensions of Track-and-Stop andGamified Explorer, namely LATS and LAGEX. To this end, we propose aconstraint-adaptive stopping rule, and while tracking the lower bound, usepessimistic estimate of the feasible set at each step. We show that thesealgorithms achieve asymptotically optimal sample complexity upper bounds up toconstraint-dependent constants. Finally, we conduct numerical experiments withdifferent reward distributions and constraints that validate efficientperformance of LAGEX and LATS with respect to baselines.</description><author>Udvas Das, Debabrota Basu</author><pubDate>Thu, 24 Oct 2024 15:26:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18844v1</guid></item><item><title>From Efficiency to Equity: Measuring Fairness in Preference Learning</title><link>http://arxiv.org/abs/2410.18841v1</link><description>As AI systems, particularly generative models, increasingly influencedecision-making, ensuring that they are able to fairly represent diverse humanpreferences becomes crucial. This paper introduces a novel framework forevaluating epistemic fairness in preference learning models inspired byeconomic theories of inequality and Rawlsian justice. We propose metricsadapted from the Gini Coefficient, Atkinson Index, and Kuznets Ratio toquantify fairness in these models. We validate our approach using two datasets:a custom visual preference dataset (AI-EDI-Space) and the Jester Jokes dataset.Our analysis reveals variations in model performance across users, highlightingpotential epistemic injustices. We explore pre-processing and in-processingtechniques to mitigate these inequalities, demonstrating a complex relationshipbetween model efficiency and fairness. This work contributes to AI ethics byproviding a framework for evaluating and improving epistemic fairness inpreference learning models, offering insights for developing more inclusive AIsystems in contexts where diverse human preferences are crucial.</description><author>Shreeyash Gowaikar, Hugo Berard, Rashid Mushkani, Shin Koseki</author><pubDate>Thu, 24 Oct 2024 15:25:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18841v1</guid></item><item><title>High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws</title><link>http://arxiv.org/abs/2410.18837v1</link><description>A growing number of machine learning scenarios rely on knowledge distillationwhere one uses the output of a surrogate model as labels to supervise thetraining of a target model. In this work, we provide a sharp characterizationof this process for ridgeless, high-dimensional regression, under two settings:(i) model shift, where the surrogate model is arbitrary, and (ii) distributionshift, where the surrogate model is the solution of empirical risk minimizationwith out-of-distribution data. In both cases, we characterize the precise riskof the target model through non-asymptotic bounds in terms of sample size anddata distribution under mild conditions. As a consequence, we identify the formof the optimal surrogate model, which reveals the benefits and limitations ofdiscarding weak features in a data-dependent fashion. In the context ofweak-to-strong (W2S) generalization, this has the interpretation that (i) W2Straining, with the surrogate as the weak model, can provably outperformtraining with strong labels under the same data budget, but (ii) it is unableto improve the data scaling law. We validate our results on numericalexperiments both on ridgeless regression and on neural network architectures.</description><author>M. Emrullah Ildiz, Halil Alperen Gozeten, Ege Onur Taga, Marco Mondelli, Samet Oymak</author><pubDate>Thu, 24 Oct 2024 15:22:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18837v1</guid></item><item><title>From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers for Underrepresented Languages</title><link>http://arxiv.org/abs/2410.18836v1</link><description>In this paper, we propose a model-agnostic cost-effective approach todeveloping bilingual base large language models (LLMs) to support English andany target language. The method includes vocabulary expansion, initializationof new embeddings, model training and evaluation. We performed our experimentswith three languages, each using a non-Latin script - Ukrainian, Arabic, andGeorgian. Our approach demonstrates improved language performance while reducingcomputational costs. It mitigates the disproportionate penalization ofunderrepresented languages, promoting fairness and minimizing adverse phenomenasuch as code-switching and broken grammar. Additionally, we introduce newmetrics to evaluate language quality, revealing that vocabulary sizesignificantly impacts the quality of generated text.</description><author>Artur Kiulian, Anton Polishko, Mykola Khandoga, Yevhen Kostiuk, Guillermo Gabrielli, Łukasz Gagała, Fadi Zaraket, Qusai Abu Obaida, Hrishikesh Garud, Wendy Wing Yee Mak, Dmytro Chaplynskyi, Selma Belhadj Amor, Grigol Peradze</author><pubDate>Thu, 24 Oct 2024 15:20:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18836v1</guid></item></channel></rss>