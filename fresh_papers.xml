<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 14 May 2024 14:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MambaOut: Do We Really Need Mamba for Vision?</title><link>http://arxiv.org/abs/2405.07992v1</link><description>Mamba, an architecture with RNN-like token mixer of state space model (SSM),was recently introduced to address the quadratic complexity of the attentionmechanism and subsequently applied to vision tasks. Nevertheless, theperformance of Mamba for vision is often underwhelming when compared withconvolutional and attention-based models. In this paper, we delve into theessence of Mamba, and conceptually conclude that Mamba is ideally suited fortasks with long-sequence and autoregressive characteristics. For vision tasks,as image classification does not align with either characteristic, wehypothesize that Mamba is not necessary for this task; Detection andsegmentation tasks are also not autoregressive, yet they adhere to thelong-sequence characteristic, so we believe it is still worthwhile to exploreMamba's potential for these tasks. To empirically verify our hypotheses, weconstruct a series of models named \emph{MambaOut} through stacking Mambablocks while removing their core token mixer, SSM. Experimental resultsstrongly support our hypotheses. Specifically, our MambaOut model surpasses allvisual Mamba models on ImageNet image classification, indicating that Mamba isindeed unnecessary for this task. As for detection and segmentation, MambaOutcannot match the performance of state-of-the-art visual Mamba models,demonstrating the potential of Mamba for long-sequence visual tasks. The codeis available at https://github.com/yuweihao/MambaOut</description><author>Weihao Yu, Xinchao Wang</author><pubDate>Mon, 13 May 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07992v1</guid></item><item><title>Auto-Linear Phenomenon in Subsurface Imaging</title><link>http://arxiv.org/abs/2305.13314v2</link><description>Subsurface imaging involves solving full waveform inversion (FWI) to predictgeophysical properties from measurements. This problem can be reframed as animage-to-image translation, with the usual approach being to train anencoder-decoder network using paired data from two domains: geophysicalproperty and measurement. A recent seminal work (InvLINT) demonstrates there isonly a linear mapping between the latent spaces of the two domains, and thedecoder requires paired data for training. This paper extends this direction by demonstrating that only linear mappingnecessitates paired data, while both the encoder and decoder can be learnedfrom their respective domains through self-supervised learning. This unveils anintriguing phenomenon (named Auto-Linear) where the self-learned features oftwo separate domains are automatically linearly correlated. Compared withexisting methods, our Auto-Linear has four advantages: (a) solving both forwardand inverse modeling simultaneously, (b) applicable to different subsurfaceimaging tasks and achieving markedly better results than previous methods,(c)enhanced performance, especially in scenarios with limited paired data andin the presence of noisy data, and (d) strong generalization ability of thetrained encoder and decoder.</description><author>Yinan Feng, Yinpeng Chen, Peng Jin, Shihang Feng, Zicheng Liu, Youzuo Lin</author><pubDate>Mon, 13 May 2024 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13314v2</guid></item><item><title>SPIN: Simultaneous Perception, Interaction and Navigation</title><link>http://arxiv.org/abs/2405.07991v1</link><description>While there has been remarkable progress recently in the fields ofmanipulation and locomotion, mobile manipulation remains a long-standingchallenge. Compared to locomotion or static manipulation, a mobile system mustmake a diverse range of long-horizon tasks feasible in unstructured and dynamicenvironments. While the applications are broad and interesting, there are aplethora of challenges in developing these systems such as coordination betweenthe base and arm, reliance on onboard perception for perceiving and interactingwith the environment, and most importantly, simultaneously integrating allthese parts together. Prior works approach the problem using disentangledmodular skills for mobility and manipulation that are trivially tied together.This causes several limitations such as compounding errors, delays indecision-making, and no whole-body coordination. In this work, we present areactive mobile manipulation framework that uses an active visual system toconsciously perceive and react to its environment. Similar to how humansleverage whole-body and hand-eye coordination, we develop a mobile manipulatorthat exploits its ability to move and see, more specifically -- to move inorder to see and to see in order to move. This allows it to not only movearound and interact with its environment but also, choose "when" to perceive"what" using an active visual system. We observe that such an agent learns tonavigate around complex cluttered scenarios while displaying agile whole-bodycoordination using only ego-vision without needing to create environment maps.Results visualizations and videos at https://spin-robot.github.io/</description><author>Shagun Uppal, Ananye Agarwal, Haoyu Xiong, Kenneth Shaw, Deepak Pathak</author><pubDate>Mon, 13 May 2024 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07991v1</guid></item><item><title>Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots</title><link>http://arxiv.org/abs/2405.07990v1</link><description>The remarkable progress of Multi-modal Large Language Models (MLLMs) hasattracted significant attention due to their superior performance in visualcontexts. However, their capabilities in turning visual figure to executablecode, have not been evaluated thoroughly. To address this, we introducePlot2Code, a comprehensive visual coding benchmark designed for a fair andin-depth assessment of MLLMs. We carefully collect 132 manually selectedhigh-quality matplotlib plots across six plot types from publicly availablematplotlib galleries. For each plot, we carefully offer its source code, and andescriptive instruction summarized by GPT-4. This approach enables Plot2Code toextensively evaluate MLLMs' code capabilities across various input modalities.Furthermore, we propose three automatic evaluation metrics, including code passrate, text-match ratio, and GPT-4V overall rating, for a fine-grainedassessment of the output code and rendered images. Instead of simply judgingpass or fail, we employ GPT-4V to make an overall judgement between thegenerated and reference images, which has been shown to be consistent withhuman evaluation. The evaluation results, which include analyses of 14 MLLMssuch as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini,highlight the substantial challenges presented by Plot2Code. With Plot2Code, wereveal that most existing MLLMs struggle with visual coding for text-denseplots, heavily relying on textual instruction. We hope that the evaluationresults from Plot2Code on visual coding will guide the future development ofMLLMs. All data involved with Plot2Code are available athttps://huggingface.co/datasets/TencentARC/Plot2Code.</description><author>Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, Ping Luo</author><pubDate>Mon, 13 May 2024 18:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07990v1</guid></item><item><title>A Generalist Learner for Multifaceted Medical Image Interpretation</title><link>http://arxiv.org/abs/2405.07988v1</link><description>Current medical artificial intelligence systems are often limited to narrowapplications, hindering their widespread adoption in clinical practice. Toaddress this limitation, we propose MedVersa, a generalist learner that enablesflexible learning and tasking for medical image interpretation. By leveraging alarge language model as a learnable orchestrator, MedVersa can learn from bothvisual and linguistic supervision, support multimodal inputs, and performreal-time task specification. This versatility allows MedVersa to adapt tovarious clinical scenarios and perform multifaceted medical image analysis. Weintroduce MedInterp, the largest multimodal dataset to date for medical imageinterpretation, consisting of over 13 million annotated instances spanning 11tasks across 3 modalities, to support the development of MedVersa. Ourexperiments demonstrate that MedVersa achieves state-of-the-art performance in9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersais the first to showcase the viability of multimodal generative medical AI inimplementing multimodal outputs, inputs, and dynamic task specification,highlighting its potential as a multifunctional system for comprehensivemedical image analysis. This generalist approach to medical imageinterpretation paves the way for more adaptable and efficient AI-assistedclinical decision-making.</description><author>Hong-Yu Zhou, Subathra Adithan, Julián Nicolás Acosta, Eric J. Topol, Pranav Rajpurkar</author><pubDate>Mon, 13 May 2024 18:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07988v1</guid></item><item><title>The Platonic Representation Hypothesis</title><link>http://arxiv.org/abs/2405.07987v1</link><description>We argue that representations in AI models, particularly deep networks, areconverging. First, we survey many examples of convergence in the literature:over time and across multiple domains, the ways by which different neuralnetworks represent data are becoming more aligned. Next, we demonstrateconvergence across data modalities: as vision models and language models getlarger, they measure distance between datapoints in a more and more alike way.We hypothesize that this convergence is driving toward a shared statisticalmodel of reality, akin to Plato's concept of an ideal reality. We term such arepresentation the platonic representation and discuss several possibleselective pressures toward it. Finally, we discuss the implications of thesetrends, their limitations, and counterexamples to our analysis.</description><author>Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola</author><pubDate>Mon, 13 May 2024 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07987v1</guid></item><item><title>A Demographic-Conditioned Variational Autoencoder for fMRI Distribution Sampling and Removal of Confounds</title><link>http://arxiv.org/abs/2405.07977v1</link><description>Objective: fMRI and derived measures such as functional connectivity (FC)have been used to predict brain age, general fluid intelligence, psychiatricdisease status, and preclinical neurodegenerative disease. However, it is notalways clear that all demographic confounds, such as age, sex, and race, havebeen removed from fMRI data. Additionally, many fMRI datasets are restricted toauthorized researchers, making dissemination of these valuable data sourceschallenging. Methods: We create a variational autoencoder (VAE)-based model,DemoVAE, to decorrelate fMRI features from demographics and generatehigh-quality synthetic fMRI data based on user-supplied demographics. We trainand validate our model using two large, widely used datasets, the PhiladelphiaNeurodevelopmental Cohort (PNC) and Bipolar and Schizophrenia Network forIntermediate Phenotypes (BSNIP). Results: We find that DemoVAE recapitulatesgroup differences in fMRI data while capturing the full breadth of individualvariations. Significantly, we also find that most clinical and computerizedbattery fields that are correlated with fMRI data are not correlated withDemoVAE latents. An exception are several fields related to schizophreniamedication and symptom severity. Conclusion: Our model generates fMRI data thatcaptures the full distribution of FC better than traditional VAE or GAN models.We also find that most prediction using fMRI data is dependent on correlationwith, and prediction of, demographics. Significance: Our DemoVAE model allowsfor generation of high quality synthetic data conditioned on subjectdemographics as well as the removal of the confounding effects of demographics.We identify that FC-based prediction tasks are highly influenced by demographicconfounds.</description><author>Anton Orlichenko, Gang Qu, Ziyu Zhou, Anqi Liu, Hong-Wen Deng, Zhengming Ding, Julia M. Stephen, Tony W. Wilson, Vince D. Calhoun, Yu-Ping Wang</author><pubDate>Mon, 13 May 2024 18:49:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07977v1</guid></item><item><title>Localized Adaptive Risk Control</title><link>http://arxiv.org/abs/2405.07976v1</link><description>Adaptive Risk Control (ARC) is an online calibration strategy based on setprediction that offers worst-case deterministic long-term risk control, as wellas statistical marginal coverage guarantees. ARC adjusts the size of theprediction set by varying a single scalar threshold based on feedback from pastdecisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC),an online calibration scheme that targets statistical localized risk guaranteesranging from conditional risk to marginal risk, while preserving the worst-caseperformance of ARC. L-ARC updates a threshold function within a reproducingkernel Hilbert space (RKHS), with the kernel determining the level oflocalization of the statistical risk guarantee. The theoretical resultshighlight a trade-off between localization of the statistical risk andconvergence speed to the long-term risk target. Thanks to localization, L-ARCis demonstrated via experiments to produce prediction sets with risk guaranteesacross different data subpopulations, significantly improving the fairness ofthe calibrated model for tasks such as image segmentation and beam selection inwireless networks.</description><author>Matteo Zecchin, Osvaldo Simeone</author><pubDate>Mon, 13 May 2024 18:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07976v1</guid></item><item><title>SignAvatar: Sign Language 3D Motion Reconstruction and Generation</title><link>http://arxiv.org/abs/2405.07974v1</link><description>Achieving expressive 3D motion reconstruction and automatic generation forisolated sign words can be challenging, due to the lack of real-world 3Dsign-word data, the complex nuances of signing motions, and the cross-modalunderstanding of sign language semantics. To address these challenges, weintroduce SignAvatar, a framework capable of both word-level sign languagereconstruction and generation. SignAvatar employs a transformer-basedconditional variational autoencoder architecture, effectively establishingrelationships across different semantic modalities. Additionally, this approachincorporates a curriculum learning strategy to enhance the model's robustnessand generalization, resulting in more realistic motions. Furthermore, wecontribute the ASL3DWord dataset, composed of 3D joint rotation data for thebody, hands, and face, for unique sign words. We demonstrate the effectivenessof SignAvatar through extensive experiments, showcasing its superiorreconstruction and automatic generation capabilities. The code and dataset areavailable on the project page.</description><author>Lu Dong, Lipisha Chaudhary, Fei Xu, Xiao Wang, Mason Lary, Ifeoma Nwogu</author><pubDate>Mon, 13 May 2024 18:48:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07974v1</guid></item><item><title>Sensitivity Analysis for Active Sampling, with Applications to the Simulation of Analog Circuits</title><link>http://arxiv.org/abs/2405.07971v1</link><description>We propose an active sampling flow, with the use-case of simulating theimpact of combined variations on analog circuits. In such a context, given thelarge number of parameters, it is difficult to fit a surrogate model and toefficiently explore the space of design features. By combining a drastic dimension reduction using sensitivity analysis andBayesian surrogate modeling, we obtain a flexible active sampling flow. Onsynthetic and real datasets, this flow outperforms the usual Monte-Carlosampling which often forms the foundation of design space exploration.</description><author>Reda Chhaibi, Fabrice Gamboa, Christophe Oger, Vinicius Oliveira, Clément Pellegrini, Damien Remot</author><pubDate>Mon, 13 May 2024 18:47:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07971v1</guid></item><item><title>Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly Segmentation</title><link>http://arxiv.org/abs/2405.07969v1</link><description>Zero-shot anomaly segmentation using pre-trained foundation models is apromising approach that enables effective algorithms without expensive,domain-specific training or fine-tuning. Ensuring that these methods workacross various environmental conditions and are robust to distribution shiftsis an open problem. We investigate the performance of WinCLIP [14] zero-shotanomaly segmentation algorithm by perturbing test data using three semantictransformations: bounded angular rotations, bounded saturation shifts, and hueshifts. We empirically measure a lower performance bound by aggregating acrossper-sample worst-case perturbations and find that average performance drops byup to 20% in area under the ROC curve and 40% in area under the per-regionoverlap curve. We find that performance is consistently lowered on three CLIPbackbones, regardless of model architecture or learning objective,demonstrating a need for careful performance evaluation.</description><author>Kevin Stangl, Marius Arvinte, Weilin Xu, Cory Cornelius</author><pubDate>Mon, 13 May 2024 18:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07969v1</guid></item><item><title>OverlapMamba: Novel Shift State Space Model for LiDAR-based Place Recognition</title><link>http://arxiv.org/abs/2405.07966v1</link><description>Place recognition is the foundation for enabling autonomous systems toachieve independent decision-making and safe operations. It is also crucial intasks such as loop closure detection and global localization within SLAM.Previous methods utilize mundane point cloud representations as input and deeplearning-based LiDAR-based Place Recognition (LPR) approaches employingdifferent point cloud image inputs with convolutional neural networks (CNNs) ortransformer architectures. However, the recently proposed Mamba deep learningmodel, combined with state space models (SSMs), holds great potential for longsequence modeling. Therefore, we developed OverlapMamba, a novel network forplace recognition, which represents input range views (RVs) as sequences. In anovel way, we employ a stochastic reconstruction approach to build shift statespace models, compressing the visual representation. Evaluated on threedifferent public datasets, our method effectively detects loop closures,showing robustness even when traversing previously visited locations fromdifferent directions. Relying on raw range view inputs, it outperforms typicalLiDAR and multi-view combination methods in time complexity and speed,indicating strong place recognition capabilities and real-time efficiency.</description><author>Qiuchi Xiang, Jintao Cheng, Jiehao Luo, Jin Wu, Rui Fan, Xieyuanli Chen, Xiaoyu Tang</author><pubDate>Mon, 13 May 2024 18:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07966v1</guid></item><item><title>Fast Computation of Superquantile-Constrained Optimization Through Implicit Scenario Reduction</title><link>http://arxiv.org/abs/2405.07965v1</link><description>Superquantiles have recently gained significant interest as a risk-awaremetric for addressing fairness and distribution shifts in statistical learningand decision making problems. This paper introduces a fast, scalable and robustsecond-order computational framework to solve large-scale optimization problemswith superquantile-based constraints. Unlike empirical risk minimization,superquantile-based optimization requires ranking random functions evaluatedacross all scenarios to compute the tail conditional expectation. While thistail-based feature might seem computationally unfriendly, it provides anadvantageous setting for a semismooth-Newton-based augmented Lagrangian method.The superquantile operator effectively reduces the dimensions of the Newtonsystems since the tail expectation involves considerably fewer scenarios.Notably, the extra cost of obtaining relevant second-order information andperforming matrix inversions is often comparable to, and sometimes even lessthan, the effort required for gradient computation. Our developed solver isparticularly effective when the number of scenarios substantially exceeds thenumber of decision variables. In synthetic problems with linear and convexdiagonal quadratic objectives, numerical experiments demonstrate that ourmethod outperforms existing approaches by a large margin: It achieves speedsmore than 750 times faster for linear and quadratic objectives than thealternating direction method of multipliers as implemented by OSQP forcomputing low-accuracy solutions. Additionally, it is up to 25 times faster forlinear objectives and 70 times faster for quadratic objectives than thecommercial solver Gurobi, and 20 times faster for linear objectives and 30times faster for quadratic objectives than the Portfolio Safeguard optimizationsuite for high-accuracy solution computations.</description><author>Jake Roth, Ying Cui</author><pubDate>Mon, 13 May 2024 18:46:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07965v1</guid></item><item><title>AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments</title><link>http://arxiv.org/abs/2405.07960v1</link><description>Diagnosing and managing a patient is a complex, sequential decision makingprocess that requires physicians to obtain information -- such as which teststo perform -- and to act upon it. Recent advances in artificial intelligence(AI) and large language models (LLMs) promise to profoundly impact clinicalcare. However, current evaluation schemes overrely on static medicalquestion-answering benchmarks, falling short on interactive decision-makingthat is required in real-life clinical work. Here, we present AgentClinic: amultimodal benchmark to evaluate LLMs in their ability to operate as agents insimulated clinical environments. In our benchmark, the doctor agent mustuncover the patient's diagnosis through dialogue and active data collection. Wepresent two open benchmarks: a multimodal image and dialogue environment,AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embedcognitive and implicit biases both in patient and doctor agents to emulaterealistic interactions between biased agents. We find that introducing biasleads to large reductions in diagnostic accuracy of the doctor agents, as wellas reduced compliance, confidence, and follow-up consultation willingness inpatient agents. Evaluating a suite of state-of-the-art LLMs, we find thatseveral models that excel in benchmarks like MedQA are performing poorly inAgentClinic-MedQA. We find that the LLM used in the patient agent is animportant factor for performance in the AgentClinic benchmark. We show thatboth having limited interactions as well as too many interaction reducesdiagnostic accuracy in doctor agents. The code and data for this work ispublicly available at https://AgentClinic.github.io.</description><author>Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, Michael Moor</author><pubDate>Mon, 13 May 2024 18:38:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07960v1</guid></item><item><title>Kreyòl-MT: Building MT for Latin American, Caribbean and Colonial African Creole Languages</title><link>http://arxiv.org/abs/2405.05376v2</link><description>A majority of language technologies are tailored for a small number ofhigh-resource languages, while relatively many low-resource languages areneglected. One such group, Creole languages, have long been marginalized inacademic study, though their speakers could benefit from machine translation(MT). These languages are predominantly used in much of Latin America, Africaand the Caribbean. We present the largest cumulative dataset to date for Creolelanguage MT, including 14.5M unique Creole sentences with parallel translations-- 11.6M of which we release publicly, and the largest bitexts gathered to datefor 41 languages -- the first ever for 21. In addition, we provide MT modelssupporting all 41 Creole languages in 172 translation directions. Given ourdiverse dataset, we produce a model for Creole language MT exposed to moregenre diversity than ever before, which outperforms a genre-specific Creole MTmodel on its own benchmark for 26 of 34 translation directions.</description><author>Nathaniel R. Robinson, Raj Dabre, Ammon Shurtz, Rasul Dent, Onenamiyi Onesi, Claire Bizon Monroc, Loïc Grobol, Hasan Muhammad, Ashi Garg, Naome A. Etori, Vijay Murari Tiyyala, Olanrewaju Samuel, Matthew Dean Stutzman, Bismarck Bamfo Odoom, Sanjeev Khudanpur, Stephen D. Richardson, Kenton Murray</author><pubDate>Mon, 13 May 2024 18:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05376v2</guid></item><item><title>Hierarchical Decision Mamba</title><link>http://arxiv.org/abs/2405.07943v1</link><description>Recent advancements in imitation learning have been largely fueled by theintegration of sequence models, which provide a structured flow of informationto effectively mimic task behaviours. Currently, Decision Transformer (DT) andsubsequently, the Hierarchical Decision Transformer (HDT), presentedTransformer-based approaches to learn task policies. Recently, the Mambaarchitecture has shown to outperform Transformers across various task domains.In this work, we introduce two novel methods, Decision Mamba (DM) andHierarchical Decision Mamba (HDM), aimed at enhancing the performance of theTransformer models. Through extensive experimentation across diverseenvironments such as OpenAI Gym and D4RL, leveraging varying demonstration datasets, we demonstrate the superiority of Mamba models over their Transformercounterparts in a majority of tasks. Results show that HDM outperforms othermethods in most settings. The code can be found athttps://github.com/meowatthemoon/HierarchicalDecisionMamba.</description><author>André Correia, Luís A. Alexandre</author><pubDate>Mon, 13 May 2024 18:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07943v1</guid></item><item><title>RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors</title><link>http://arxiv.org/abs/2405.07940v1</link><description>Many commercial and open-source models claim to detect machine-generated textwith very high accuracy (99\% or higher). However, very few of these detectorsare evaluated on shared benchmark datasets and even when they are, the datasetsused for evaluation are insufficiently challenging -- lacking variations insampling strategy, adversarial attacks, and open-source generative models. Inthis work we present RAID: the largest and most challenging benchmark datasetfor machine-generated text detection. RAID includes over 6 million generationsspanning 11 models, 8 domains, 11 adversarial attacks and 4 decodingstrategies. Using RAID, we evaluate the out-of-domain and adversarialrobustness of 8 open- and 4 closed-source detectors and find that currentdetectors are easily fooled by adversarial attacks, variations in samplingstrategies, repetition penalties, and unseen generative models. We release ourdataset and tools to encourage further exploration into detector robustness.</description><author>Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh Magnus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ippolito, Chris Callison-Burch</author><pubDate>Mon, 13 May 2024 18:15:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07940v1</guid></item><item><title>EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning</title><link>http://arxiv.org/abs/2405.07938v1</link><description>In this paper, we introduce EconLogicQA, a rigorous benchmark designed toassess the sequential reasoning capabilities of large language models (LLMs)within the intricate realms of economics, business, and supply chainmanagement. Diverging from traditional benchmarks that predict subsequentevents individually, EconLogicQA poses a more challenging task: it requiresmodels to discern and sequence multiple interconnected events, capturing thecomplexity of economic logics. EconLogicQA comprises an array of multi-eventscenarios derived from economic articles, which necessitate an insightfulunderstanding of both temporal and logical event relationships. Throughcomprehensive evaluations, we exhibit that EconLogicQA effectively gauges aLLM's proficiency in navigating the sequential complexities inherent ineconomic contexts. We provide a detailed description of EconLogicQA dataset andshows the outcomes from evaluating the benchmark across various leading-edgeLLMs, thereby offering a thorough perspective on their sequential reasoningpotential in economic contexts. Our benchmark dataset is available athttps://huggingface.co/datasets/yinzhu-quan/econ_logic_qa.</description><author>Yinzhu Quan, Zefang Liu</author><pubDate>Mon, 13 May 2024 18:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07938v1</guid></item><item><title>Active Learning with Simple Questions</title><link>http://arxiv.org/abs/2405.07937v1</link><description>We consider an active learning setting where a learner is presented with apool S of n unlabeled examples belonging to a domain X and asks queries to findthe underlying labeling that agrees with a target concept h^* \in H. In contrast to traditional active learning that queries a single example forits label, we study more general region queries that allow the learner to picka subset of the domain T \subset X and a target label y and ask a labelerwhether h^*(x) = y for every example in the set T \cap S. Such more powerful queries allow us to bypass the limitations of traditionalactive learning and use significantly fewer rounds of interactions to learn butcan potentially lead to a significantly more complex query language. Our maincontribution is quantifying the trade-off between the number of queries and thecomplexity of the query language used by the learner. We measure the complexity of the region queries via the VC dimension of thefamily of regions. We show that given any hypothesis class H with VC dimensiond, one can design a region query family Q with VC dimension O(d) such that forevery set of n examples S \subset X and every h^* \in H, a learner can submitO(d log n) queries from Q to a labeler and perfectly label S. We show amatching lower bound by designing a hypothesis class H with VC dimension d anda dataset S \subset X of size n such that any learning algorithm using anyquery class with VC dimension O(d) must make poly(n) queries to label Sperfectly. Finally, we focus on well-studied hypothesis classes including unions ofintervals, high-dimensional boxes, and d-dimensional halfspaces, and obtainstronger results. In particular, we design learning algorithms that (i) arecomputationally efficient and (ii) work even when the queries are not answeredbased on the learner's pool of examples S but on some unknown superset L of S</description><author>Vasilis Kontonis, Mingchen Ma, Christos Tzamos</author><pubDate>Mon, 13 May 2024 18:13:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07937v1</guid></item><item><title>Authentic Hand Avatar from a Phone Scan via Universal Hand Model</title><link>http://arxiv.org/abs/2405.07933v1</link><description>The authentic 3D hand avatar with every identifiable information, such ashand shapes and textures, is necessary for immersive experiences in AR/VR. Inthis paper, we present a universal hand model (UHM), which 1) can universallyrepresent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) canbe adapted to each person with a short phone scan for the authentic handavatar. For effective universal hand modeling, we perform tracking and modelingat the same time, while previous 3D hand models perform them separately. Theconventional separate pipeline suffers from the accumulated errors from thetracking stage, which cannot be recovered in the modeling stage. On the otherhand, ours does not suffer from the accumulated errors while having a much moreconcise overall pipeline. We additionally introduce a novel image matching lossfunction to address a skin sliding during the tracking and modeling, whileexisting works have not focused on it much. Finally, using learned priors fromour UHM, we effectively adapt our UHM to each person's short phone scan for theauthentic hand avatar.</description><author>Gyeongsik Moon, Weipeng Xu, Rohan Joshi, Chenglei Wu, Takaaki Shiratori</author><pubDate>Mon, 13 May 2024 18:09:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07933v1</guid></item><item><title>PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition</title><link>http://arxiv.org/abs/2405.07932v1</link><description>Large language models (LLMs) have shown success in many natural languageprocessing tasks. Despite rigorous safety alignment processes, supposedlysafety-aligned LLMs like Llama 2 and Claude 2 are still susceptible tojailbreaks, leading to security risks and abuse of the models. One option tomitigate such risks is to augment the LLM with a dedicated "safeguard", whichchecks the LLM's inputs or outputs for undesired behaviour. A promisingapproach is to use the LLM itself as the safeguard. Nonetheless, baselinemethods, such as prompting the LLM to self-classify toxic content, demonstratelimited efficacy. We hypothesise that this is due to domain shift: thealignment training imparts a self-censoring behaviour to the model ("Sorry Ican't do that"), while the self-classify approach shifts it to a classificationformat ("Is this prompt malicious"). In this work, we propose PARDEN, whichavoids this domain shift by simply asking the model to repeat its own outputs.PARDEN neither requires finetuning nor white box access to the model. Weempirically verify the effectiveness of our method and show that PARDENsignificantly outperforms existing jailbreak detection baselines for Llama-2and Claude-2. Code and data are available at https://github.com/Ed-Zh/PARDEN. We find that PARDEN is particularly powerful in the relevant regime of highTrue Positive Rate (TPR) and low False Positive Rate (FPR). For instance, forLlama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction inthe FPR from 24.8% to 2.0% on the harmful behaviours dataset.</description><author>Ziyang Zhang, Qizhen Zhang, Jakob Foerster</author><pubDate>Mon, 13 May 2024 18:08:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07932v1</guid></item><item><title>Look Once to Hear: Target Speech Hearing with Noisy Examples</title><link>http://arxiv.org/abs/2405.06289v2</link><description>In crowded settings, the human brain can focus on speech from a targetspeaker, given prior knowledge of how they sound. We introduce a novelintelligent hearable system that achieves this capability, enabling targetspeech hearing to ignore all interfering speech and noise, but the targetspeaker. A naive approach is to require a clean speech example to enroll thetarget speaker. This is however not well aligned with the hearable applicationdomain since obtaining a clean example is challenging in real world scenarios,creating a unique user interface problem. We present the first enrollmentinterface where the wearer looks at the target speaker for a few seconds tocapture a single, short, highly noisy, binaural example of the target speaker.This noisy example is used for enrollment and subsequent speech extraction inthe presence of interfering speakers and noise. Our system achieves a signalquality improvement of 7.01 dB using less than 5 seconds of noisy enrollmentaudio and can process 8 ms of audio chunks in 6.24 ms on an embedded CPU. Ouruser studies demonstrate generalization to real-world static and mobilespeakers in previously unseen indoor and outdoor multipath environments.Finally, our enrollment interface for noisy examples does not cause performancedegradation compared to clean examples, while being convenient anduser-friendly. Taking a step back, this paper takes an important step towardsenhancing the human auditory perception with artificial intelligence. Weprovide code and data at: https://github.com/vb000/LookOnceToHear.</description><author>Bandhav Veluri, Malek Itani, Tuochao Chen, Takuya Yoshioka, Shyamnath Gollakota</author><pubDate>Mon, 13 May 2024 18:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06289v2</guid></item><item><title>Improving Multimodal Learning with Multi-Loss Gradient Modulation</title><link>http://arxiv.org/abs/2405.07930v1</link><description>Learning from multiple modalities, such as audio and video, offersopportunities for leveraging complementary information, enhancing robustness,and improving contextual understanding and performance. However, combining suchmodalities presents challenges, especially when modalities differ in datastructure, predictive contribution, and the complexity of their learningprocesses. It has been observed that one modality can potentially dominate thelearning process, hindering the effective utilization of information from othermodalities and leading to sub-optimal model performance. To address this issuethe vast majority of previous works suggest to assess the unimodalcontributions and dynamically adjust the training to equalize them. We improveupon previous work by introducing a multi-loss objective and further refiningthe balancing process, allowing it to dynamically adjust the learning pace ofeach modality in both directions, acceleration and deceleration, with theability to phase out balancing effects upon convergence. We achieve superiorresults across three audio-video datasets: on CREMA-D, models with ResNetbackbone encoders surpass the previous best by 1.9% to 12.4%, and Conformerbackbone models deliver improvements ranging from 2.8% to 14.1% acrossdifferent fusion methods. On AVE, improvements range from 2.7% to 7.7%, whileon UCF101, gains reach up to 6.1%.</description><author>Konstantinos Kontras, Christos Chatzichristos, Matthew Blaschko, Maarten De Vos</author><pubDate>Mon, 13 May 2024 18:01:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07930v1</guid></item><item><title>Stable Diffusion-based Data Augmentation for Federated Learning with Non-IID Data</title><link>http://arxiv.org/abs/2405.07925v1</link><description>The proliferation of edge devices has brought Federated Learning (FL) to theforefront as a promising paradigm for decentralized and collaborative modeltraining while preserving the privacy of clients' data. However, FL struggleswith a significant performance reduction and poor convergence when confrontedwith Non-Independent and Identically Distributed (Non-IID) data distributionsamong participating clients. While previous efforts, such as client driftmitigation and advanced server-side model fusion techniques, have shown somesuccess in addressing this challenge, they often overlook the root cause of theperformance reduction - the absence of identical data accurately mirroring theglobal data distribution among clients. In this paper, we introduce Gen-FedSD,a novel approach that harnesses the powerful capability of state-of-the-arttext-to-image foundation models to bridge the significant Non-IID performancegaps in FL. In Gen-FedSD, each client constructs textual prompts for each classlabel and leverages an off-the-shelf state-of-the-art pre-trained StableDiffusion model to synthesize high-quality data samples. The generatedsynthetic data is tailored to each client's unique local data gaps anddistribution disparities, effectively making the final augmented local dataIID. Through extensive experimentation, we demonstrate that Gen-FedSD achievesstate-of-the-art performance and significant communication cost savings acrossvarious datasets and Non-IID settings.</description><author>Mahdi Morafah, Matthias Reisser, Bill Lin, Christos Louizos</author><pubDate>Mon, 13 May 2024 17:57:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07925v1</guid></item><item><title>Can Better Text Semantics in Prompt Tuning Improve VLM Generalization?</title><link>http://arxiv.org/abs/2405.07921v1</link><description>Going beyond mere fine-tuning of vision-language models (VLMs), learnableprompt tuning has emerged as a promising, resource-efficient alternative.Despite their potential, effectively learning prompts faces the followingchallenges: (i) training in a low-shot scenario results in overfitting,limiting adaptability and yielding weaker performance on newer classes ordatasets; (ii) prompt-tuning's efficacy heavily relies on the label space, withdecreased performance in large class spaces, signaling potential gaps inbridging image and class concepts. In this work, we ask the question if bettertext semantics can help address these concerns. In particular, we introduce aprompt-tuning method that leverages class descriptions obtained from largelanguage models (LLMs). Our approach constructs part-level description-guidedviews of both image and text features, which are subsequently aligned to learnmore generalizable prompts. Our comprehensive experiments, conducted across 11benchmark datasets, outperform established methods, demonstrating substantialimprovements.</description><author>Hari Chandana Kuchibhotla, Sai Srinivas Kancheti, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian</author><pubDate>Mon, 13 May 2024 17:52:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07921v1</guid></item><item><title>Exploring the Low-Pass Filtering Behavior in Image Super-Resolution</title><link>http://arxiv.org/abs/2405.07919v1</link><description>Deep neural networks for image super-resolution have shown significantadvantages over traditional approaches like interpolation. However, they areoften criticized as `black boxes' compared to traditional approaches which havesolid mathematical foundations. In this paper, we attempt to interpret thebehavior of deep neural networks using theories from signal processingtheories. We first report an intriguing phenomenon, referred to as `the sincphenomenon,' which occurs when an impulse input is fed to a neural network.Building on this observation, we propose a method named Hybird ResponseAnalysis (HyRA) to analyze the behavior of neural networks in imagesuper-resolution tasks. In details, HyRA decomposes a neural network into aparallel connection of a linear system and a non-linear system, demonstratingthat the linear system functions as a low-pass filter, while the non-linearsystem injects high-frequency information. Furthermore, to quantify theinjected high-frequency information, we introduce a metric for image-to-imagetasks called Frequency Spectrum Distribution Similarity (FSDS). FSDS reflectsthe distribution similarity of different frequency components, capturingnuances that traditional metrics may overlook. Code for this work can be foundin: https://github.com/RisingEntropy/LPFInISR.</description><author>Haoyu Deng, Zijing Xu, Yule Duan, Xiao Wu, Wenjie Shu, Liang-Jian Deng</author><pubDate>Mon, 13 May 2024 17:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07919v1</guid></item><item><title>IMAFD: An Interpretable Multi-stage Approach to Flood Detection from time series Multispectral Data</title><link>http://arxiv.org/abs/2405.07916v1</link><description>In this paper, we address two critical challenges in the domain of flooddetection: the computational expense of large-scale time series changedetection and the lack of interpretable decision-making processes onexplainable AI (XAI). To overcome these challenges, we proposed aninterpretable multi-stage approach to flood detection, IMAFD has been proposed.It provides an automatic, efficient and interpretable solution suitable forlarge-scale remote sensing tasks and offers insight into the decision-makingprocess. The proposed IMAFD approach combines the analysis of the dynamic timeseries image sequences to identify images with possible flooding with thestatic, within-image semantic segmentation. It combines anomaly detection (atboth image and pixel level) with semantic segmentation. The flood detectionproblem is addressed through four stages: (1) at a sequence level: identifyingthe suspected images (2) at a multi-image level: detecting change withinsuspected images (3) at an image level: semantic segmentation of images intoLand, Water or Cloud class (4) decision making. Our contributions are twofolder. First, we efficiently reduced the number of frames to be processed fordense change detection by providing a multi-stage holistic approach to flooddetection. Second, the proposed semantic change detection method (stage 3)provides human users with an interpretable decision-making process, while mostof the explainable AI (XAI) methods provide post hoc explanations. Theevaluation of the proposed IMAFD framework was performed on three datasets,WorldFloods, RavAEn and MediaEval. For all the above datasets, the proposedframework demonstrates a competitive performance compared to other methodsoffering also interpretability and insight.</description><author>Ziyang Zhang, Plamen Angelov, Dmitry Kangin, Nicolas Longépé</author><pubDate>Mon, 13 May 2024 17:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07916v1</guid></item><item><title>Distribution Learning Meets Graph Structure Sampling</title><link>http://arxiv.org/abs/2405.07914v1</link><description>This work establishes a novel link between the problem of PAC-learninghigh-dimensional graphical models and the task of (efficient) counting andsampling of graph structures, using an online learning framework. We observe that if we apply the exponentially weighted average (EWA) orrandomized weighted majority (RWM) forecasters on a sequence of samples from adistribution P using the log loss function, the average regret incurred by theforecaster's predictions can be used to bound the expected KL divergencebetween P and the predictions. Known regret bounds for EWA and RWM then yieldnew sample complexity bounds for learning Bayes nets. Moreover, thesealgorithms can be made computationally efficient for several interestingclasses of Bayes nets. Specifically, we give a new sample-optimal andpolynomial time learning algorithm with respect to trees of unknown structureand the first polynomial sample and time algorithm for learning with respect toBayes nets over a given chordal skeleton.</description><author>Arnab Bhattacharyya, Sutanu Gayen, Philips George John, Sayantan Sen, N. V. Vinodchandran</author><pubDate>Mon, 13 May 2024 17:47:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07914v1</guid></item><item><title>CTRLorALTer: Conditional LoRAdapter for Efficient 0-Shot Control &amp; Altering of T2I Models</title><link>http://arxiv.org/abs/2405.07913v1</link><description>Text-to-image generative models have become a prominent and powerful toolthat excels at generating high-resolution realistic images. However, guidingthe generative process of these models to consider detailed forms ofconditioning reflecting style and/or structure information remains an openproblem. In this paper, we present LoRAdapter, an approach that unifies bothstyle and structure conditioning under the same formulation using a novelconditional LoRA block that enables zero-shot control. LoRAdapter is anefficient, powerful, and architecture-agnostic approach to conditiontext-to-image diffusion models, which enables fine-grained control conditioningduring generation and outperforms recent state-of-the-art approaches</description><author>Nick Stracke, Stefan Andreas Baumann, Joshua M. Susskind, Miguel Angel Bautista, Björn Ommer</author><pubDate>Mon, 13 May 2024 17:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07913v1</guid></item><item><title>MRSegmentator: Robust Multi-Modality Segmentation of 40 Classes in MRI and CT Sequences</title><link>http://arxiv.org/abs/2405.06463v2</link><description>Purpose: To introduce a deep learning model capable of multi-organsegmentation in MRI scans, offering a solution to the current limitations inMRI analysis due to challenges in resolution, standardized intensity values,and variability in sequences. Materials and Methods: he model was trained on 1,200 manually annotated MRIscans from the UK Biobank, 221 in-house MRI scans and 1228 CT scans, leveragingcross-modality transfer learning from CT segmentation models. Ahuman-in-the-loop annotation workflow was employed to efficiently createhigh-quality segmentations. The model's performance was evaluated on NAKO andthe AMOS22 dataset containing 600 and 60 MRI examinations. Dice SimilarityCoefficient (DSC) and Hausdorff Distance (HD) was used to assess segmentationaccuracy. The model will be open sourced. Results: The model showcased high accuracy in segmenting well-defined organs,achieving Dice Similarity Coefficient (DSC) scores of 0.97 for the right andleft lungs, and 0.95 for the heart. It also demonstrated robustness in organslike the liver (DSC: 0.96) and kidneys (DSC: 0.95 left, 0.95 right), whichpresent more variability. However, segmentation of smaller and complexstructures such as the portal and splenic veins (DSC: 0.54) and adrenal glands(DSC: 0.65 left, 0.61 right) revealed the need for further model optimization. Conclusion: The proposed model is a robust, tool for accurate segmentation of40 anatomical structures in MRI and CT images. By leveraging cross-modalitylearning and interactive annotation, the model achieves strong performance andgeneralizability across diverse datasets, making it a valuable resource forresearchers and clinicians. It is open source and can be downloaded fromhttps://github.com/hhaentze/MRSegmentator.</description><author>Hartmut Häntze, Lina Xu, Felix J. Dorfner, Leonhard Donle, Daniel Truhn, Hugo Aerts, Mathias Prokop, Bram van Ginneken, Alessa Hering, Lisa C. Adams, Keno K. Bressem</author><pubDate>Mon, 13 May 2024 17:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06463v2</guid></item><item><title>PLUTO: Pathology-Universal Transformer</title><link>http://arxiv.org/abs/2405.07905v1</link><description>Pathology is the study of microscopic inspection of tissue, and a pathologydiagnosis is often the medical gold standard to diagnose disease. Pathologyimages provide a unique challenge for computer-vision-based analysis: a singlepathology Whole Slide Image (WSI) is gigapixel-sized and often containshundreds of thousands to millions of objects of interest across multipleresolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO):a light-weight pathology FM that is pre-trained on a diverse dataset of 195million image tiles collected from multiple sites and extracts meaningfulrepresentations across multiple WSI scales that enable a large variety ofdownstream pathology tasks. In particular, we design task-specific adaptationheads that utilize PLUTO's output embeddings for tasks which span pathologyscales ranging from subcellular to slide-scale, including instancesegmentation, tile classification, and slide-level prediction. We comparePLUTO's performance to other state-of-the-art methods on a diverse set ofexternal and internal benchmarks covering multiple biologically relevant tasks,tissue types, resolutions, stains, and scanners. We find that PLUTO matches oroutperforms existing task-specific baselines and pathology-specific foundationmodels, some of which use orders-of-magnitude larger datasets and model sizeswhen compared to PLUTO. Our findings present a path towards a universalembedding to power pathology image analysis, and motivate further explorationaround pathology foundation models in terms of data diversity, architecturalimprovements, sample efficiency, and practical deployability in real-worldapplications.</description><author>Dinkar Juyal, Harshith Padigela, Chintan Shah, Daniel Shenker, Natalia Harguindeguy, Yi Liu, Blake Martin, Yibo Zhang, Michael Nercessian, Miles Markey, Isaac Finberg, Kelsey Luu, Daniel Borders, Syed Ashar Javed, Emma Krause, Raymond Biju, Aashish Sood, Allen Ma, Jackson Nyman, John Shamshoian, Guillaume Chhor, Darpan Sanghavi, Marc Thibault, Limin Yu, Fedaa Najdawi, Jennifer A. Hipp, Darren Fahy, Benjamin Glass, Eric Walk, John Abel, Harsha Pokkalla, Andrew H. Beck, Sean Grullon</author><pubDate>Mon, 13 May 2024 17:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07905v1</guid></item><item><title>Science based AI model certification for new operational environments with application in traffic state estimation</title><link>http://arxiv.org/abs/2405.07893v1</link><description>The expanding role of Artificial Intelligence (AI) in diverse engineeringdomains highlights the challenges associated with deploying AI models in newoperational environments, involving substantial investments in data collectionand model training. Rapid application of AI necessitates evaluating thefeasibility of utilizing pre-trained models in unobserved operational settingswith minimal or no additional data. However, interpreting the opaque nature ofAI's black-box models remains a persistent challenge. Addressing this issue,this paper proposes a science-based certification methodology to assess theviability of employing pre-trained data-driven models in new operationalenvironments. The methodology advocates a profound integration of domainknowledge, leveraging theoretical and analytical models from physics andrelated disciplines, with data-driven AI models. This novel approach introducestools to facilitate the development of secure engineering systems, providingdecision-makers with confidence in the trustworthiness and safety of AI-basedmodels across diverse environments characterized by limited training data anddynamic, uncertain conditions. The paper demonstrates the efficacy of thismethodology in real-world safety-critical scenarios, particularly in thecontext of traffic state estimation. Through simulation results, the studyillustrates how the proposed methodology efficiently quantifies physicalinconsistencies exhibited by pre-trained AI models. By utilizing analyticalmodels, the methodology offers a means to gauge the applicability ofpre-trained AI models in new operational environments. This researchcontributes to advancing the understanding and deployment of AI models,offering a robust certification framework that enhances confidence in theirreliability and safety across a spectrum of operational conditions.</description><author>Daryl Mupupuni, Anupama Guntu, Liang Hong, Kamrul Hasan, Leehyun Keel</author><pubDate>Mon, 13 May 2024 17:28:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07893v1</guid></item><item><title>All Nodes are created Not Equal: Node-Specific Layer Aggregation and Filtration for GNN</title><link>http://arxiv.org/abs/2405.07892v1</link><description>The ever-designed Graph Neural Networks, though opening a promising path forthe modeling of the graph-structure data, unfortunately introduce two dauntingobstacles to their deployment on devices. (I) Most of existing GNNs areshallow, due mostly to the over-smoothing and gradient-vanish problem as theygo deeper as convolutional architectures. (II) The vast majority of GNNs adhereto the homophily assumption, where the central node and its adjacent nodesshare the same label. This assumption often poses challenges for many GNNsworking with heterophilic graphs. Addressing the aforementioned issue hasbecome a looming challenge in enhancing the robustness and scalability of GNNapplications. In this paper, we take a comprehensive and systematic approach toovercoming the two aforementioned challenges for the first time. We propose aNode-Specific Layer Aggregation and Filtration architecture, termed NoSAF, aframework capable of filtering and processing information from each individualnodes. NoSAF introduces the concept of "All Nodes are Created Not Equal" intoevery layer of deep networks, aiming to provide a reliable information filterfor each layer's nodes to sieve out information beneficial for the subsequentlayer. By incorporating a dynamically updated codebank, NoSAF dynamicallyoptimizes the optimal information outputted downwards at each layer. Thiseffectively overcomes heterophilic issues and aids in deepening the network. Tocompensate for the information loss caused by the continuous filtering inNoSAF, we also propose NoSAF-D (Deep), which incorporates a compensationmechanism that replenishes information in every layer of the model, allowingNoSAF to perform meaningful computations even in very deep layers.</description><author>Shilong Wang, Hao Wu, Yifan Duan, Guibin Zhang, Guohao Li, Yuxuan Liang, Shirui Pan, Kun Wang, Yang Wang</author><pubDate>Mon, 13 May 2024 17:27:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07892v1</guid></item><item><title>Russian-Language Multimodal Dataset for Automatic Summarization of Scientific Papers</title><link>http://arxiv.org/abs/2405.07886v1</link><description>The paper discusses the creation of a multimodal dataset of Russian-languagescientific papers and testing of existing language models for the task ofautomatic text summarization. A feature of the dataset is its multimodal data,which includes texts, tables and figures. The paper presents the results ofexperiments with two language models: Gigachat from SBER and YandexGPT fromYandex. The dataset consists of 420 papers and is publicly available onhttps://github.com/iis-research-team/summarization-dataset.</description><author>Alena Tsanda, Elena Bruches</author><pubDate>Mon, 13 May 2024 17:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07886v1</guid></item><item><title>Soft Merging of Experts with Adaptive Routing</title><link>http://arxiv.org/abs/2306.03745v2</link><description>Sparsely activated neural networks with conditional computation learn toroute their inputs through different "expert" subnetworks, providing a form ofmodularity that densely activated models lack. Despite their possible benefits,models with learned routing often underperform their parameter-matched denselyactivated counterparts as well as models that use non-learned heuristic routingstrategies. In this paper, we hypothesize that these shortcomings stem from thegradient estimation techniques used to train sparsely activated models that usenon-differentiable discrete routing decisions. To address this issue, weintroduce Soft Merging of Experts with Adaptive Routing (SMEAR), which avoidsdiscrete routing by using a single "merged" expert constructed via a weightedaverage of all of the experts' parameters. By routing activations through asingle merged expert, SMEAR does not incur a significant increase incomputational costs and enables standard gradient-based training. Weempirically validate that models using SMEAR outperform models that route basedon metadata or learn sparse routing through gradient estimation. Furthermore,we provide qualitative analysis demonstrating that the experts learned viaSMEAR exhibit a significant amount of specialization. All of the code used inour experiments is publicly available.</description><author>Mohammed Muqeeth, Haokun Liu, Colin Raffel</author><pubDate>Mon, 13 May 2024 17:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03745v2</guid></item><item><title>Lai Loss: A Novel Loss Integrating Regularization</title><link>http://arxiv.org/abs/2405.07884v1</link><description>In the field of machine learning, traditional regularization methodsgenerally tend to directly add regularization terms to the loss function. Thispaper introduces the "Lai loss", a novel loss design that integrates theregularization terms (gradient component) into the traditional loss functionthrough a straightforward geometric ideation. This design innovativelypenalizes the gradient vectors through the loss, effectively controlling themodel's smoothness and offering the dual benefits of reducing overfitting andavoiding underfitting. Subsequently, we proposed a random sampling method thatsuccessfully addresses the challenges associated with its application underlarge sample conditions. We conducted preliminary experiments using publiclyavailable datasets from Kaggle, demonstrating that the design of Lai loss cancontrol the model's smoothness while ensuring maximum accuracy.</description><author>YuFei Lai</author><pubDate>Mon, 13 May 2024 17:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07884v1</guid></item><item><title>OMPGPT: A Generative Pre-trained Transformer Model for OpenMP</title><link>http://arxiv.org/abs/2401.16445v2</link><description>Large language models (LLMs)such as ChatGPT have significantly advanced thefield of Natural Language Processing (NLP). This trend led to the developmentof code-based large language models such as StarCoder, WizardCoder, andCodeLlama, which are trained extensively on vast repositories of code andprogramming languages. While the generic abilities of these code LLMs areuseful for many programmers in tasks like code generation, the area ofhigh-performance computing (HPC) has a narrower set of requirements that make asmaller and more domain-specific model a smarter choice. This paper presentsOMPGPT, a novel domain-specific model meticulously designed to harness theinherent strengths of language models for OpenMP pragma generation.Furthermore, we leverage prompt engineering techniques from the NLP domain tocreate Chain-of-OMP, an innovative strategy designed to enhance OMPGPT'seffectiveness. Our extensive evaluations demonstrate that OMPGPT outperformsexisting large language models specialized in OpenMP tasks and maintains anotably smaller size, aligning it more closely with the typical hardwareconstraints of HPC environments. We consider our contribution as a pivotalbridge, connecting the advantage of language models with the specific demandsof HPC tasks.</description><author>Le Chen, Arijit Bhattacharjee, Nesreen Ahmed, Niranjan Hasabnis, Gal Oren, Vy Vo, Ali Jannesari</author><pubDate>Mon, 13 May 2024 17:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16445v2</guid></item><item><title>Zero-Shot Tokenizer Transfer</title><link>http://arxiv.org/abs/2405.07883v1</link><description>Language models (LMs) are bound to their tokenizer, which maps raw text to asequence of vocabulary items (tokens). This restricts their flexibility: forexample, LMs trained primarily on English may still perform well in othernatural and programming languages, but have vastly decreased efficiency due totheir English-centric tokenizer. To mitigate this, we should be able to swapthe original LM tokenizer with an arbitrary one, on the fly, without degradingperformance. Hence, in this work we define a new problem: Zero-Shot TokenizerTransfer (ZeTT). The challenge at the core of ZeTT is finding embeddings forthe tokens in the vocabulary of the new tokenizer. Since prior heuristics forinitializing embeddings often perform at chance level in a ZeTT setting, wepropose a new solution: we train a hypernetwork taking a tokenizer as input andpredicting the corresponding embeddings. We empirically demonstrate that thehypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) anddecoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'performance in cross-lingual and coding tasks while markedly reducing thelength of the tokenized sequence. We also find that the remaining gap can bequickly closed by continued training on less than 1B tokens. Finally, we showthat a ZeTT hypernetwork trained for a base (L)LM can also be applied tofine-tuned variants without extra training. Overall, our results makesubstantial strides toward detaching LMs from their tokenizer.</description><author>Benjamin Minixhofer, Edoardo Maria Ponti, Ivan Vulić</author><pubDate>Mon, 13 May 2024 17:17:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07883v1</guid></item><item><title>Prospects for AI-Enhanced ECG as a Unified Screening Tool for Cardiac and Non-Cardiac Conditions -- An Explorative Study in Emergency Care</title><link>http://arxiv.org/abs/2312.11050v2</link><description>Current deep learning algorithms designed for automatic ECG analysis haveexhibited notable accuracy. However, akin to traditional electrocardiography,they tend to be narrowly focused and typically address a singular diagnosticcondition. In this exploratory study, we specifically investigate thecapability of a single model to predict a diverse range of both cardiac andnon-cardiac discharge diagnoses based on a sole ECG collected in the emergencydepartment. We find that 253, 81 cardiac, and 172 non-cardiac, ICD codes can bereliably predicted in the sense of exceeding an AUROC score of 0.8 in astatistically significant manner. This underscores the model's proficiency inhandling a wide array of cardiac and non-cardiac diagnostic scenarios whichdemonstrates potential as a screening tool for diverse medical encounters.</description><author>Nils Strodthoff, Juan Miguel Lopez Alcaraz, Wilhelm Haverkamp</author><pubDate>Mon, 13 May 2024 17:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11050v2</guid></item><item><title>Multi-scale Wasserstein Shortest-path Graph Kernels for Graph Classification</title><link>http://arxiv.org/abs/2206.00979v5</link><description>Graph kernels are conventional methods for computing graph similarities.However, the existing R-convolution graph kernels cannot resolve both of thetwo challenges: 1) Comparing graphs at multiple different scales, and 2)Considering the distributions of substructures when computing the kernelmatrix. These two challenges limit their performances. To mitigate both of thetwo challenges, we propose a novel graph kernel called the Multi-scaleWasserstein Shortest-Path graph kernel (MWSP), at the heart of which is themulti-scale shortest-path node feature map, of which each element denotes thenumber of occurrences of the shortest path around a node. The shortest path isrepresented by the concatenation of all the labels of nodes in it. Since theshortest-path node feature map can only compare graphs at local scales, weincorporate into it the multiple different scales of the graph structure, whichare captured by the truncated BFS trees of different depths rooted at each nodein a graph. We use the Wasserstein distance to compute the similarity betweenthe multi-scale shortest-path node feature maps of two graphs, considering thedistributions of shortest paths. We empirically validate MWSP on variousbenchmark graph datasets and demonstrate that it achieves state-of-the-artperformance on most datasets.</description><author>Wei Ye, Hao Tian, Qijun Chen</author><pubDate>Mon, 13 May 2024 17:14:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.00979v5</guid></item><item><title>Fully Embedded Time-Series Generative Adversarial Networks</title><link>http://arxiv.org/abs/2308.15730v2</link><description>Generative Adversarial Networks (GANs) should produce synthetic data thatfits the underlying distribution of the data being modeled. For real valuedtime-series data, this implies the need to simultaneously capture the staticdistribution of the data, but also the full temporal distribution of the datafor any potential time horizon. This temporal element produces a more complexproblem that can potentially leave current solutions under-constrained,unstable during training, or prone to varying degrees of mode collapse. InFETSGAN, entire sequences are translated directly to the generator's samplingspace using a seq2seq style adversarial auto encoder (AAE), where adversarialtraining is used to match the training distribution in both the feature spaceand the lower dimensional sampling space. This additional constraint provides aloose assurance that the temporal distribution of the synthetic samples willnot collapse. In addition, the First Above Threshold (FAT) operator isintroduced to supplement the reconstruction of encoded sequences, whichimproves training stability and the overall quality of the synthetic data beinggenerated. These novel contributions demonstrate a significant improvement tothe current state of the art for adversarial learners in qualitative measuresof temporal similarity and quantitative predictive ability of data generatedthrough FETSGAN.</description><author>Joe Beck, Subhadeep Chakraborty</author><pubDate>Mon, 13 May 2024 17:11:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15730v2</guid></item><item><title>On the Relation Between Autoencoders and Non-negative Matrix Factorization, and Their Application for Mutational Signature Extraction</title><link>http://arxiv.org/abs/2405.07879v1</link><description>The aim of this study is to provide a foundation to understand therelationship between non-negative matrix factorization (NMF) and non-negativeautoencoders enabling proper interpretation and understanding ofautoencoder-based alternatives to NMF. Since its introduction, NMF has been apopular tool for extracting interpretable, low-dimensional representations ofhigh-dimensional data. However, recently, several studies have proposed toreplace NMF with autoencoders. This increasing popularity of autoencoderswarrants an investigation on whether this replacement is in general valid andreasonable. Moreover, the exact relationship between non-negative autoencodersand NMF has not been thoroughly explored. Thus, a main aim of this study is toinvestigate in detail the relationship between non-negative autoencoders andNMF. We find that the connection between the two models can be establishedthrough convex NMF, which is a restricted case of NMF. In particular, convexNMF is a special case of an autoencoder. The performance of NMF andautoencoders is compared within the context of extraction of mutationalsignatures from cancer genomics data. We find that the reconstructions based onNMF are more accurate compared to autoencoders, while the signatures extractedusing both methods show comparable consistencies and values when externallyvalidated. These findings suggest that the non-negative autoencodersinvestigated in this article do not provide an improvement of NMF in the fieldof mutational signature extraction.</description><author>Ida Egendal, Rasmus Froberg Brøndum, Marta Pelizzola, Asger Hobolth, Martin Bøgsted</author><pubDate>Mon, 13 May 2024 17:09:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07879v1</guid></item><item><title>Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs</title><link>http://arxiv.org/abs/2310.02619v2</link><description>Generating realistic time series data is important for many engineering andscientific applications. Existing work tackles this problem using generativeadversarial networks (GANs). However, GANs are unstable during training, andthey can suffer from mode collapse. While variational autoencoders (VAEs) areknown to be more robust to the these issues, they are (surprisingly) lessconsidered for time series generation. In this work, we introduce Koopman VAE(KoVAE), a new generative framework that is based on a novel design for themodel prior, and that can be optimized for either regular and irregulartraining data. Inspired by Koopman theory, we represent the latent conditionalprior dynamics using a linear map. Our approach enhances generative modelingwith two desired features: (i) incorporating domain knowledge can be achievedby leveraging spectral tools that prescribe constraints on the eigenvalues ofthe linear map; and (ii) studying the qualitative behavior and stability of thesystem can be performed using tools from dynamical systems theory. Our resultsshow that KoVAE outperforms state-of-the-art GAN and VAE methods across severalchallenging synthetic and real-world time series generation benchmarks. Whethertrained on regular or irregular data, KoVAE generates time series that improveboth discriminative and predictive metrics. We also present visual evidencesuggesting that KoVAE learns probability density functions that betterapproximate the empirical ground truth distribution.</description><author>Ilan Naiman, N. Benjamin Erichson, Pu Ren, Michael W. Mahoney, Omri Azencot</author><pubDate>Mon, 13 May 2024 17:07:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02619v2</guid></item><item><title>Reproducing the Metric-Based Evaluation of a Set of Controllable Text Generation Techniques</title><link>http://arxiv.org/abs/2405.07875v1</link><description>Rerunning a metric-based evaluation should be more straightforward, andresults should be closer, than in a human-based evaluation, especially wherecode and model checkpoints are made available by the original authors. As thisreport of our efforts to rerun a metric-based evaluation of a set ofsingle-attribute and multiple-attribute controllable text generation (CTG)techniques shows however, such reruns of evaluations do not always produceresults that are the same as the original results, and can reveal errors in thereporting of the original work.</description><author>Michela Lorandi, Anya Belz</author><pubDate>Mon, 13 May 2024 17:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07875v1</guid></item><item><title>Enhancing Clinically Significant Prostate Cancer Prediction in T2-weighted Images through Transfer Learning from Breast Cancer</title><link>http://arxiv.org/abs/2405.07869v1</link><description>In 2020, prostate cancer saw a staggering 1.4 million new cases, resulting inover 375,000 deaths. The accurate identification of clinically significantprostate cancer is crucial for delivering effective treatment to patients.Consequently, there has been a surge in research exploring the application ofdeep neural networks to predict clinical significance based on magneticresonance images. However, these networks demand extensive datasets to attainoptimal performance. Recently, transfer learning emerged as a technique thatleverages acquired features from a domain with richer data to enhance theperformance of a domain with limited data. In this paper, we investigate theimprovement of clinically significant prostate cancer prediction in T2-weightedimages through transfer learning from breast cancer. The results demonstrate aremarkable improvement of over 30% in leave-one-out cross-validation accuracy.</description><author>Chi-en Amy Tai, Alexander Wong</author><pubDate>Mon, 13 May 2024 16:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07869v1</guid></item><item><title>Boostlet.js: Image processing plugins for the web via JavaScript injection</title><link>http://arxiv.org/abs/2405.07868v1</link><description>Can web-based image processing and visualization tools easily integrate intoexisting websites without significant time and effort? Our Boostlet.js libraryaddresses this challenge by providing an open-source, JavaScript-based webframework to enable additional image processing functionalities. Boostletexamples include kernel filtering, image captioning, data visualization,segmentation, and web-optimized machine-learning models. To achieve this,Boostlet.js uses a browser bookmark to inject a user-friendly plugin selectiontool called PowerBoost into any host website. Boostlet also provides on-siteaccess to a standard API independent of any visualization framework for pixeldata and scene manipulation. Web-based Boostlets provide a modular architectureand client-side processing capabilities to apply advanced image-processingtechniques using consumer-level hardware. The code is open-source andavailable.</description><author>Edward Gaibor, Shruti Varade, Rohini Deshmukh, Tim Meyer, Mahsa Geshvadi, SangHyuk Kim, Vidhya Sree Narayanappa, Daniel Haehn</author><pubDate>Mon, 13 May 2024 16:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07868v1</guid></item><item><title>Approximating Numerical Fluxes Using Fourier Neural Operators for Hyperbolic Conservation Laws</title><link>http://arxiv.org/abs/2401.01783v4</link><description>Traditionally, classical numerical schemes have been employed to solvepartial differential equations (PDEs) using computational methods. Recently,neural network-based methods have emerged. Despite these advancements, neuralnetwork-based methods, such as physics-informed neural networks (PINNs) andneural operators, exhibit deficiencies in robustness and generalization. Toaddress these issues, numerous studies have integrated classical numericalframeworks with machine learning techniques, incorporating neural networks intoparts of traditional numerical methods. In this study, we focus on hyperbolicconservation laws by replacing traditional numerical fluxes with neuraloperators. To this end, we developed loss functions inspired by establishednumerical schemes related to conservation laws and approximated numericalfluxes using Fourier neural operators (FNOs). Our experiments demonstrated thatour approach combines the strengths of both traditional numerical schemes andFNOs, outperforming standard FNO methods in several respects. For instance, wedemonstrate that our method is robust, has resolution invariance, and isfeasible as a data-driven method. In particular, our method can make continuouspredictions over time and exhibits superior generalization capabilities without-of-distribution (OOD) samples, which are challenges that existing neuraloperator methods encounter.</description><author>Taeyoung Kim, Myungjoo Kang</author><pubDate>Mon, 13 May 2024 16:53:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01783v4</guid></item><item><title>AnoVox: A Benchmark for Multimodal Anomaly Detection in Autonomous Driving</title><link>http://arxiv.org/abs/2405.07865v1</link><description>The scale-up of autonomous vehicles depends heavily on their ability to dealwith anomalies, such as rare objects on the road. In order to handle suchsituations, it is necessary to detect anomalies in the first place. Anomalydetection for autonomous driving has made great progress in the past years butsuffers from poorly designed benchmarks with a strong focus on camera data. Inthis work, we propose AnoVox, the largest benchmark for ANOmaly detection inautonomous driving to date. AnoVox incorporates large-scale multimodal sensordata and spatial VOXel ground truth, allowing for the comparison of methodsindependent of their used sensor. We propose a formal definition of normalityand provide a compliant training dataset. AnoVox is the first benchmark tocontain both content and temporal anomalies.</description><author>Daniel Bogdoll, Iramm Hamdard, Lukas Namgyu Rößler, Felix Geisler, Muhammed Bayram, Felix Wang, Jan Imhof, Miguel de Campos, Anushervon Tabarov, Yitian Yang, Hanno Gottschalk, J. Marius Zöllner</author><pubDate>Mon, 13 May 2024 16:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07865v1</guid></item><item><title>RLHF Workflow: From Reward Modeling to Online RLHF</title><link>http://arxiv.org/abs/2405.07863v1</link><description>We present the workflow of Online Iterative Reinforcement Learning from HumanFeedback (RLHF) in this technical report, which is widely reported tooutperform its offline counterpart by a large margin in the recent largelanguage model (LLM) literature. However, existing open-source RLHF projectsare still largely confined to the offline learning setting. In this technicalreport, we aim to fill in this gap and provide a detailed recipe that is easyto reproduce for online iterative RLHF. In particular, since online humanfeedback is usually infeasible for open-source communities with limitedresources, we start by constructing preference models using a diverse set ofopen-source datasets and use the constructed proxy preference model toapproximate human feedback. Then, we discuss the theoretical insights andalgorithmic principles behind online iterative RLHF, followed by a detailedpractical implementation. Our trained LLM, SFR-Iterative-DPO-LLaMA-3-8B-R,achieves impressive performance on LLM chatbot benchmarks, includingAlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarkssuch as HumanEval and TruthfulQA. We have shown that supervised fine-tuning(SFT) and iterative RLHF can obtain state-of-the-art performance with fullyopen-source datasets. Further, we have made our models, curated datasets, andcomprehensive step-by-step code guidebooks publicly available. Please refer tohttps://github.com/RLHFlow/RLHF-Reward-Modeling andhttps://github.com/RLHFlow/Online-RLHF for more detailed information.</description><author>Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, Tong Zhang</author><pubDate>Mon, 13 May 2024 16:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07863v1</guid></item><item><title>Improving Breast Cancer Grade Prediction with Multiparametric MRI Created Using Optimized Synthetic Correlated Diffusion Imaging</title><link>http://arxiv.org/abs/2405.07861v1</link><description>Breast cancer was diagnosed for over 7.8 million women between 2015 to 2020.Grading plays a vital role in breast cancer treatment planning. However, thecurrent tumor grading method involves extracting tissue from patients, leadingto stress, discomfort, and high medical costs. A recent paper leveragingvolumetric deep radiomic features from synthetic correlated diffusion imaging(CDI$^s$) for breast cancer grade prediction showed immense promise fornoninvasive methods for grading. Motivated by the impact of CDI$^s$optimization for prostate cancer delineation, this paper examines usingoptimized CDI$^s$ to improve breast cancer grade prediction. We fuse theoptimized CDI$^s$ signal with diffusion-weighted imaging (DWI) to create amultiparametric MRI for each patient. Using a larger patient cohort andtraining across all the layers of a pretrained MONAI model, we achieve aleave-one-out cross-validation accuracy of 95.79%, over 8% higher compared tothat previously reported.</description><author>Chi-en Amy Tai, Alexander Wong</author><pubDate>Mon, 13 May 2024 16:48:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07861v1</guid></item><item><title>Learning to Plan and Generate Text with Citations</title><link>http://arxiv.org/abs/2404.03381v2</link><description>The increasing demand for the deployment of LLMs in information-seekingscenarios has spurred efforts in creating verifiable systems, which generateresponses to queries along with supporting evidence. In this paper, we explorethe attribution capabilities of plan-based models which have been recentlyshown to improve the faithfulness, grounding, and controllability of generatedtext. We conceptualize plans as a sequence of questions which serve asblueprints of the generated content and its organization. We propose twoattribution models that utilize different variants of blueprints, anabstractive model where questions are generated from scratch, and an extractivemodel where questions are copied from the input. Experiments on long-formquestion-answering show that planning consistently improves attributionquality. Moreover, the citations generated by blueprint models are moreaccurate compared to those obtained from LLM-based pipelines lacking a planningcomponent.</description><author>Constanza Fierro, Reinald Kim Amplayo, Fantine Huot, Nicola De Cao, Joshua Maynez, Shashi Narayan, Mirella Lapata</author><pubDate>Mon, 13 May 2024 16:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03381v2</guid></item><item><title>Uniform Inference for Subsampled Moment Regression</title><link>http://arxiv.org/abs/2405.07860v1</link><description>We propose a method for constructing a confidence region for the solution toa conditional moment equation. The method is built around a class of algorithmsfor nonparametric regression based on subsampled kernels. This class includesrandom forest regression. We bound the error in the confidence region's nominalcoverage probability, under the restriction that the conditional momentequation of interest satisfies a local orthogonality condition. The method isapplicable to the construction of confidence regions for conditional averagetreatment effects in randomized experiments, among many other similar problemsencountered in applied economics and causal inference. As a by-product, weobtain several new order-explicit results on the concentration and normalapproximation of high-dimensional $U$-statistics.</description><author>David M. Ritzwoller, Vasilis Syrgkanis</author><pubDate>Mon, 13 May 2024 16:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07860v1</guid></item><item><title>Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs</title><link>http://arxiv.org/abs/2405.07857v1</link><description>The multi-plane representation has been highlighted for its fast training andinference across static and dynamic neural radiance fields. This approachconstructs relevant features via projection onto learnable grids andinterpolating adjacent vertices. However, it has limitations in capturinglow-frequency details and tends to overuse parameters for low-frequencyfeatures due to its bias toward fine details, despite its multi-resolutionconcept. This phenomenon leads to instability and inefficiency when trainingposes are sparse. In this work, we propose a method that synergisticallyintegrates multi-plane representation with a coordinate-based network known forstrong bias toward low-frequency signals. The coordinate-based network isresponsible for capturing low-frequency details, while the multi-planerepresentation focuses on capturing fine-grained details. We demonstrate thatusing residual connections between them seamlessly preserves their own inherentproperties. Additionally, the proposed progressive training scheme acceleratesthe disentanglement of these two features. We empirically show that theproposed method achieves comparable results to explicit encoding with fewerparameters, and particularly, it outperforms others for the static and dynamicNeRFs under sparse inputs.</description><author>Mingyu Kim, Jun-Seong Kim, Se-Young Yun, Jin-Hwa Kim</author><pubDate>Mon, 13 May 2024 16:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07857v1</guid></item><item><title>Using Multiparametric MRI with Optimized Synthetic Correlated Diffusion Imaging to Enhance Breast Cancer Pathologic Complete Response Prediction</title><link>http://arxiv.org/abs/2405.07854v1</link><description>In 2020, 685,000 deaths across the world were attributed to breast cancer,underscoring the critical need for innovative and effective breast cancertreatment. Neoadjuvant chemotherapy has recently gained popularity as apromising treatment strategy for breast cancer, attributed to its efficacy inshrinking large tumors and leading to pathologic complete response. However,the current process to recommend neoadjuvant chemotherapy relies on thesubjective evaluation of medical experts which contain inherent biases andsignificant uncertainty. A recent study, utilizing volumetric deep radiomicfeatures extracted from synthetic correlated diffusion imaging (CDI$^s$),demonstrated significant potential in noninvasive breast cancer pathologiccomplete response prediction. Inspired by the positive outcomes of optimizingCDI$^s$ for prostate cancer delineation, this research investigates theapplication of optimized CDI$^s$ to enhance breast cancer pathologic completeresponse prediction. Using multiparametric MRI that fuses optimized CDI$^s$with diffusion-weighted imaging (DWI), we obtain a leave-one-outcross-validation accuracy of 93.28%, over 5.5% higher than that previouslyreported.</description><author>Chi-en Amy Tai, Alexander Wong</author><pubDate>Mon, 13 May 2024 16:40:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07854v1</guid></item><item><title>SceneFactory: A Workflow-centric and Unified Framework for Incremental Scene Modeling</title><link>http://arxiv.org/abs/2405.07847v1</link><description>We present SceneFactory, a workflow-centric and unified framework forincremental scene modeling, that supports conveniently a wide range ofapplications, such as (unposed and/or uncalibrated) multi-view depthestimation, LiDAR completion, (dense) RGB-D/RGB-L/Mono//Depth-onlyreconstruction and SLAM. The workflow-centric design uses multiple blocks asthe basis for building different production lines. The supported applications,i.e., productions avoid redundancy in their designs. Thus, the focus is on eachblock itself for independent expansion. To support all input combinations, ourimplementation consists of four building blocks in SceneFactory: (1) Mono-SLAM,(2) depth estimation, (3) flexion and (4) scene reconstruction. Furthermore, wepropose an unposed &amp; uncalibrated multi-view depth estimation model (U2-MVD) toestimate dense geometry. U2-MVD exploits dense bundle adjustment for solvingfor poses, intrinsics, and inverse depth. Then a semantic-awared ScaleCov stepis introduced to complete the multi-view depth. Relying on U2-MVD, SceneFactoryboth supports user-friendly 3D creation (with just images) and bridges theapplications of Dense RGB-D and Dense Mono. For high quality surface and colorreconstruction, we propose due-purpose Multi-resolutional Neural Points(DM-NPs) for the first surface accessible Surface Color Field design, where weintroduce Improved Point Rasterization (IPR) for point cloud based surfacequery. We implement and experiment with SceneFactory to demonstrate its broadpracticability and high flexibility. Its quality also competes or exceeds thetightly-coupled state of the art approaches in all tasks. We contribute thecode to the community (https://jarrome.github.io/).</description><author>Yijun Yuan, Michael Bleier, Andreas Nüchter</author><pubDate>Mon, 13 May 2024 16:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07847v1</guid></item><item><title>Multi-Task Learning for Fatigue Detection and Face Recognition of Drivers via Tree-Style Space-Channel Attention Fusion Network</title><link>http://arxiv.org/abs/2405.07845v1</link><description>In driving scenarios, automobile active safety systems are increasinglyincorporating deep learning technology. These systems typically need to handlemultiple tasks simultaneously, such as detecting fatigue driving andrecognizing the driver's identity. However, the traditional parallel-styleapproach of combining multiple single-task models tends to waste resources whendealing with similar tasks. Therefore, we propose a novel tree-style multi-taskmodeling approach for multi-task learning, which rooted at a shared backbone,more dedicated separate module branches are appended as the model pipeline goesdeeper. Following the tree-style approach, we propose a multi-task learningmodel for simultaneously performing driver fatigue detection and facerecognition for identifying a driver. This model shares a common featureextraction backbone module, with further separated feature extraction andclassification module branches. The dedicated branches exploit and combinespatial and channel attention mechanisms to generate space-channelfused-attention enhanced features, leading to improved detection performance.As only single-task datasets are available, we introduce techniques includingalternating updation and gradient accumulation for training our multi-taskmodel using only the single-task datasets. The effectiveness of our tree-stylemulti-task learning model is verified through extensive validations.</description><author>Shulei Qu, Zhenguo Gao, Xiaowei Chen, Na Li, Yakai Wang, Xiaoxiao Wu</author><pubDate>Mon, 13 May 2024 16:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07845v1</guid></item><item><title>Delta Tensor: Efficient Vector and Tensor Storage in Delta Lake</title><link>http://arxiv.org/abs/2405.03708v3</link><description>The exponential growth of artificial intelligence (AI) and machine learning(ML) applications has necessitated the development of efficient storagesolutions for vector and tensor data. This paper presents a novel approach fortensor storage in a Lakehouse architecture using Delta Lake. By adopting themultidimensional array storage strategy from array databases and sparseencoding methods to Delta Lake tables, experiments show that this approach hasdemonstrated notable improvements in both space and time efficiencies whencompared to traditional serialization of tensors. These results providevaluable insights for the development and implementation of optimized vectorand tensor storage solutions in data-intensive applications, contributing tothe evolution of efficient data management practices in AI and ML domains incloud-native environments</description><author>Zhiwei Bao, Liu Liao-Liao, Zhiyu Wu, Yifan Zhou, Dan Fan, Michal Aibin, Yvonne Coady, Andrew Brownsword</author><pubDate>Mon, 13 May 2024 16:30:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03708v3</guid></item><item><title>Ground-based Image Deconvolution with Swin Transformer UNet</title><link>http://arxiv.org/abs/2405.07842v1</link><description>As ground-based all-sky astronomical surveys will gather millions of imagesin the coming years, a critical requirement emerges for the development of fastdeconvolution algorithms capable of efficiently improving the spatialresolution of these images. By successfully recovering clean andhigh-resolution images from these surveys, our objective is to help deepen ourunderstanding of galaxy formation and evolution through accurate photometricmeasurements. We introduce a two-step deconvolution framework using a SwinTransformer architecture. Our study reveals that the deep learning-basedsolution introduces a bias, constraining the scope of scientific analysis. Toaddress this limitation, we propose a novel third step relying on the activecoefficients in the sparsity wavelet framework. By conducting a performancecomparison between our deep learning-based method and Firedec, a classicaldeconvolution algorithm, we analyze a subset of the EDisCS cluster samples. Wedemonstrate the advantage of our method in terms of resolution recovery,generalization to different noise properties, and computational efficiency. Notonly does the analysis of this cluster sample assess the efficiency of ourmethod, but it also enables us to quantify the number of clumps within thesegalaxies in relation to their disc colour. This robust technique holds promisefor identifying structures in the distant universe from ground-based images.</description><author>Utsav Akhaury, Pascale Jablonka, Jean-Luc Starck, Frédéric Courbin</author><pubDate>Mon, 13 May 2024 16:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07842v1</guid></item><item><title>Sample Selection Bias in Machine Learning for Healthcare</title><link>http://arxiv.org/abs/2405.07841v1</link><description>While machine learning algorithms hold promise for personalised medicine,their clinical adoption remains limited. One critical factor contributing tothis restraint is sample selection bias (SSB) which refers to the studypopulation being less representative of the target population, leading tobiased and potentially harmful decisions. Despite being well-known in theliterature, SSB remains scarcely studied in machine learning for healthcare.Moreover, the existing techniques try to correct the bias by balancingdistributions between the study and the target populations, which may result ina loss of predictive performance. To address these problems, our studyillustrates the potential risks associated with SSB by examining SSB's impacton the performance of machine learning algorithms. Most importantly, we proposea new research direction for addressing SSB, based on the target populationidentification rather than the bias correction. Specifically, we propose twoindependent networks (T-Net) and a multitasking network (MT-Net) for addressingSSB, where one network/task identifies the target subpopulation which isrepresentative of the study population and the second makes predictions for theidentified subpopulation. Our empirical results with synthetic andsemi-synthetic datasets highlight that SSB can lead to a large drop in theperformance of an algorithm for the target population as compared with thestudy population, as well as a substantial difference in the performance forthe target subpopulations that are representative of the selected and thenon-selected patients from the study population. Furthermore, our proposedtechniques demonstrate robustness across various settings, including differentdataset sizes, event rates, and selection rates, outperforming the existingbias correction techniques.</description><author>Vinod Kumar Chauhan, Lei Clifton, Achille Salaün, Huiqi Yvonne Lu, Kim Branson, Patrick Schwab, Gaurav Nigam, David A. Clifton</author><pubDate>Mon, 13 May 2024 16:30:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07841v1</guid></item><item><title>Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM</title><link>http://arxiv.org/abs/2405.07840v1</link><description>Decoding language information from brain signals represents a vital researcharea within brain-computer interfaces, particularly in the context ofdeciphering the semantic information from the fMRI signal. However, manyexisting efforts concentrate on decoding small vocabulary sets, leaving spacefor the exploration of open vocabulary continuous text decoding. In this paper,we introduce a novel method, the \textbf{Brain Prompt GPT (BP-GPT)}. By usingthe brain representation that is extracted from the fMRI as a prompt, ourmethod can utilize GPT-2 to decode fMRI signals into stimulus text. Further, weintroduce a text-to-text baseline and align the fMRI prompt to the text prompt.By introducing the text-to-text baseline, our BP-GPT can extract a more robustbrain prompt and promote the decoding of pre-trained LLM. We evaluate ourBP-GPT on the open-source auditory semantic decoding dataset and achieve asignificant improvement up to $4.61\%$ on METEOR and $2.43\%$ on BERTScoreacross all the subjects compared to the state-of-the-art method. Theexperimental results demonstrate that using brain representation as a prompt tofurther drive LLM for auditory neural decoding is feasible and effective.</description><author>Xiaoyu Chen, Changde Du, Che Liu, Yizhe Wang, Huiguang He</author><pubDate>Mon, 13 May 2024 16:25:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07840v1</guid></item><item><title>Constrained Exploration via Reflected Replica Exchange Stochastic Gradient Langevin Dynamics</title><link>http://arxiv.org/abs/2405.07839v1</link><description>Replica exchange stochastic gradient Langevin dynamics (reSGLD) is aneffective sampler for non-convex learning in large-scale datasets. However, thesimulation may encounter stagnation issues when the high-temperature chaindelves too deeply into the distribution tails. To tackle this issue, we proposereflected reSGLD (r2SGLD): an algorithm tailored for constrained non-convexexploration by utilizing reflection steps within a bounded domain.Theoretically, we observe that reducing the diameter of the domain enhancesmixing rates, exhibiting a \emph{quadratic} behavior. Empirically, we test itsperformance through extensive experiments, including identifying dynamicalsystems with physical constraints, simulations of constrained multi-modaldistributions, and image classification tasks. The theoretical and empiricalfindings highlight the crucial role of constrained exploration in improving thesimulation efficiency.</description><author>Haoyang Zheng, Hengrong Du, Qi Feng, Wei Deng, Guang Lin</author><pubDate>Mon, 13 May 2024 16:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07839v1</guid></item><item><title>Adaptive Exploration for Data-Efficient General Value Function Evaluations</title><link>http://arxiv.org/abs/2405.07838v1</link><description>General Value Functions (GVFs) (Sutton et al, 2011) are an established way torepresent predictive knowledge in reinforcement learning. Each GVF computes theexpected return for a given policy, based on a unique pseudo-reward. MultipleGVFs can be estimated in parallel using off-policy learning from a singlestream of data, often sourced from a fixed behavior policy or pre-collecteddataset. This leaves an open question: how can behavior policy be chosen fordata-efficient GVF learning? To address this gap, we propose GVFExplorer, whichaims at learning a behavior policy that efficiently gathers data for evaluatingmultiple GVFs in parallel. This behavior policy selects actions in proportionto the total variance in the return across all GVFs, reducing the number ofenvironmental interactions. To enable accurate variance estimation, we use arecently proposed temporal-difference-style variance estimator. We prove thateach behavior policy update reduces the mean squared error in the summedpredictions over all GVFs. We empirically demonstrate our method's performancein both tabular representations and nonlinear function approximation.</description><author>Arushi Jain, Josiah P. Hanna, Doina Precup</author><pubDate>Mon, 13 May 2024 16:24:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07838v1</guid></item><item><title>Domain Generalisation for Object Detection under Covariate and Concept Shift</title><link>http://arxiv.org/abs/2203.05294v3</link><description>Domain generalisation aims to promote the learning of domain-invariantfeatures while suppressing domain-specific features, so that a model cangeneralise better to previously unseen target domains. An approach to domaingeneralisation for object detection is proposed, the first such approachapplicable to any object detection architecture. Based on a rigorousmathematical analysis, we extend approaches based on feature alignment with anovel component for performing class conditional alignment at the instancelevel, in addition to aligning the marginal feature distributions acrossdomains at the image level. This allows us to fully address both components ofdomain shift, i.e. covariate and concept shift, and learn a domain agnosticfeature representation. We perform extensive evaluation with both one-stage(FCOS, YOLO) and two-stage (FRCNN) detectors, on a newly proposed benchmarkcomprising several different datasets for autonomous driving applications(Cityscapes, BDD10K, ACDC, IDD) as well as the GWHD dataset for precisionagriculture, and show consistent improvements to the generalisation andlocalisation performance over baselines and state-of-the-art.</description><author>Karthik Seemakurthy, Erchan Aptoula, Charles Fox, Petra Bosilj</author><pubDate>Mon, 13 May 2024 16:23:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.05294v3</guid></item><item><title>Forecasting with Hyper-Trees</title><link>http://arxiv.org/abs/2405.07836v1</link><description>This paper introduces the concept of Hyper-Trees and offers a new directionin applying tree-based models to time series data. Unlike conventionalapplications of decision trees that forecast time series directly, Hyper-Treesare designed to learn the parameters of a target time series model. Ourframework leverages the gradient-based nature of boosted trees, which allows usto extend the concept of Hyper-Networks to Hyper-Trees and to induce atime-series inductive bias to tree models. By relating the parameters of atarget time series model to features, Hyper-Trees address the challenge ofparameter non-stationarity and enable tree-based forecasts to extend beyondtheir initial training range. With our research, we aim to explore theeffectiveness of Hyper-Trees across various forecasting scenarios and to expandthe application of gradient boosted decision trees past their conventional usein time series forecasting.</description><author>Alexander März, Kashif Rasul</author><pubDate>Mon, 13 May 2024 16:22:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07836v1</guid></item><item><title>A Comprehensive Overview of Fish-Eye Camera Distortion Correction Methods</title><link>http://arxiv.org/abs/2401.00442v2</link><description>The fisheye camera, with its unique wide field of view and othercharacteristics, has found extensive applications in various fields. However,the fisheye camera suffers from significant distortion compared to pinholecameras, resulting in distorted images of captured objects. Fish-eye cameradistortion is a common issue in digital image processing, requiring effectivecorrection techniques to enhance image quality. This review provides acomprehensive overview of various methods used for fish-eye camera distortioncorrection. The article explores the polynomial distortion model, whichutilizes polynomial functions to model and correct radial distortions.Additionally, alternative approaches such as panorama mapping, grid mapping,direct methods, and deep learning-based methods are discussed. The reviewhighlights the advantages, limitations, and recent advancements of each method,enabling readers to make informed decisions based on their specific needs.</description><author>Jian Xu, De-Wei Han, Kang Li, Jun-Jie Li, Zhao-Yuan Ma</author><pubDate>Mon, 13 May 2024 16:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00442v2</guid></item><item><title>Automatic Recognition of Food Ingestion Environment from the AIM-2 Wearable Sensor</title><link>http://arxiv.org/abs/2405.07827v1</link><description>Detecting an ingestion environment is an important aspect of monitoringdietary intake. It provides insightful information for dietary assessment.However, it is a challenging problem where human-based reviewing can betedious, and algorithm-based review suffers from data imbalance and perceptualaliasing problems. To address these issues, we propose a neural network-basedmethod with a two-stage training framework that tactfully combines fine-tuningand transfer learning techniques. Our method is evaluated on a newly collecteddataset called ``UA Free Living Study", which uses an egocentric wearablecamera, AIM-2 sensor, to simulate food consumption in free-living conditions.The proposed training framework is applied to common neural network backbones,combined with approaches in the general imbalanced classification field.Experimental results on the collected dataset show that our proposed method forautomatic ingestion environment recognition successfully addresses thechallenging data imbalance problem in the dataset and achieves a promisingoverall classification accuracy of 96.63%.</description><author>Yuning Huang, Mohamed Abul Hassan, Jiangpeng He, Janine Higgins, Megan McCrory, Heather Eicher-Miller, Graham Thomas, Edward O Sazonov, Fengqing Maggie Zhu</author><pubDate>Mon, 13 May 2024 16:12:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07827v1</guid></item><item><title>Integrating Multi-Physics Simulations and Machine Learning to Define the Spatter Mechanism and Process Window in Laser Powder Bed Fusion</title><link>http://arxiv.org/abs/2405.07823v1</link><description>Laser powder bed fusion (LPBF) has shown promise for wide range ofapplications due to its ability to fabricate freeform geometries and generate acontrolled microstructure. However, components generated by LPBF still possesssub-optimal mechanical properties due to the defects that are created duringlaser-material interactions. In this work, we investigate mechanism of spatterformation, using a high-fidelity modelling tool that was built to simulate themulti-physics phenomena in LPBF. The modelling tool have the capability tocapture the 3D resolution of the meltpool and the spatter behavior. Tounderstand spatter behavior and formation, we reveal its properties at ejectionand evaluate its variation from the meltpool, the source where it is formed.The dataset of the spatter and the meltpool collected consist of 50 % spatterand 50 % melt pool samples, with features that include position components,velocity components, velocity magnitude, temperature, density and pressure. Therelationship between the spatter and the meltpool were evaluated viacorrelation analysis and machine learning (ML) algorithms for classificationtasks. Upon screening different ML algorithms on the dataset, a high accuracywas observed for all the ML models, with ExtraTrees having the highest at 96 %and KNN having the lowest at 94 %.</description><author>Olabode T. Ajenifujah, Francis Ogoke, Florian Wirth, Jack Beuth, Amir Barati Farimani</author><pubDate>Mon, 13 May 2024 16:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07823v1</guid></item><item><title>Synthetic Tabular Data Validation: A Divergence-Based Approach</title><link>http://arxiv.org/abs/2405.07822v1</link><description>The ever-increasing use of generative models in various fields where tabulardata is used highlights the need for robust and standardized validation metricsto assess the similarity between real and synthetic data. Current methods lacka unified framework and rely on diverse and often inconclusive statisticalmeasures. Divergences, which quantify discrepancies between data distributions,offer a promising avenue for validation. However, traditional approachescalculate divergences independently for each feature due to the complexity ofjoint distribution modeling. This paper addresses this challenge by proposing anovel approach that uses divergence estimation to overcome the limitations ofmarginal comparisons. Our core contribution lies in applying a divergenceestimator to build a validation metric considering the joint distribution ofreal and synthetic data. We leverage a probabilistic classifier to approximatethe density ratio between datasets, allowing the capture of complexrelationships. We specifically calculate two divergences: the well-knownKullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence. KLdivergence offers an established use in the field, while JS divergence issymmetric and bounded, providing a reliable metric. The efficacy of thisapproach is demonstrated through a series of experiments with varyingdistribution complexities. The initial phase involves comparing estimateddivergences with analytical solutions for simple distributions, setting abenchmark for accuracy. Finally, we validate our method on a real-world datasetand its corresponding synthetic counterpart, showcasing its effectiveness inpractical applications. This research offers a significant contribution withapplicability beyond tabular data and the potential to improve synthetic datavalidation in various fields.</description><author>Patricia A. Apellániz, Ana Jiménez, Borja Arroyo Galende, Juan Parras, Santiago Zazo</author><pubDate>Mon, 13 May 2024 16:07:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07822v1</guid></item><item><title>Regularized Q-learning</title><link>http://arxiv.org/abs/2202.05404v6</link><description>Q-learning is widely used algorithm in reinforcement learning community.Under the lookup table setting, its convergence is well established. However,its behavior is known to be unstable with the linear function approximationcase. This paper develops a new Q-learning algorithm that converges when linearfunction approximation is used. We prove that simply adding an appropriateregularization term ensures convergence of the algorithm. We prove itsstability using a recent analysis tool based on switching system models.Moreover, we experimentally show that it converges in environments whereQ-learning with linear function approximation has known to diverge. We alsoprovide an error bound on the solution where the algorithm converges.</description><author>Han-Dong Lim, Donghwan Lee</author><pubDate>Mon, 13 May 2024 16:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05404v6</guid></item><item><title>The Power of Combined Modalities in Interactive Robot Learning</title><link>http://arxiv.org/abs/2405.07817v1</link><description>This study contributes to the evolving field of robot learning in interactionwith humans, examining the impact of diverse input modalities on learningoutcomes. It introduces the concept of "meta-modalities" which encapsulateadditional forms of feedback beyond the traditional preference and scalarfeedback mechanisms. Unlike prior research that focused on individualmeta-modalities, this work evaluates their combined effect on learningoutcomes. Through a study with human participants, we explore user preferencesfor these modalities and their impact on robot learning performance. Ourfindings reveal that while individual modalities are perceived differently,their combination significantly improves learning behavior and usability. Thisresearch not only provides valuable insights into the optimization ofhuman-robot interactive task learning but also opens new avenues for enhancingthe interactive freedom and scaffolding capabilities provided to users in suchsettings.</description><author>Helen Beierling, Anna-Lisa Vollmer</author><pubDate>Mon, 13 May 2024 15:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07817v1</guid></item><item><title>Quick and Accurate Affordance Learning</title><link>http://arxiv.org/abs/2405.07816v1</link><description>Infants learn actively in their environments, shaping their own learningcurricula. They learn about their environments' affordances, that is, how localcircumstances determine how their behavior can affect the environment. Here wemodel this type of behavior by means of a deep learning architecture. Thearchitecture mediates between global cognitive map exploration and localaffordance learning. Inference processes actively move the simulated agenttowards regions where they expect affordance-related knowledge gain. Wecontrast three measures of uncertainty to guide this exploration: predicteduncertainty of a model, standard deviation between the means of several models(SD), and the Jensen-Shannon Divergence (JSD) between several models. We showthat the first measure gets fooled by aleatoric uncertainty inherent in theenvironment, while the two other measures focus learning on epistemicuncertainty. JSD exhibits the most balanced exploration strategy. From acomputational perspective, our model suggests three key ingredients forcoordinating the active generation of learning curricula: (1) Navigationbehavior needs to be coordinated with local motor behavior for enabling activeaffordance learning. (2) Affordances need to be encoded locally for acquiringgeneralized knowledge. (3) Effective active affordance learning mechanismsshould use density comparison techniques for estimating expected knowledgegain. Future work may seek collaborations with developmental psychology tomodel active play in children in more realistic scenarios.</description><author>Fedor Scholz, Erik Ayari, Johannes Bertram, Martin V. Butz</author><pubDate>Mon, 13 May 2024 15:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07816v1</guid></item><item><title>DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning</title><link>http://arxiv.org/abs/2403.14421v3</link><description>Text-to-image diffusion models have been shown to suffer from sample-levelmemorization, possibly reproducing near-perfect replica of images that they aretrained on, which may be undesirable. To remedy this issue, we develop thefirst differentially private (DP) retrieval-augmented generation algorithm thatis capable of generating high-quality image samples while providing provableprivacy guarantees. Specifically, we assume access to a text-to-image diffusionmodel trained on a small amount of public data, and design a DP retrievalmechanism to augment the text prompt with samples retrieved from a privateretrieval dataset. Our \emph{differentially private retrieval-augmenteddiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset toadapt to another domain, and can use state-of-the-art generative models togenerate high-quality image samples while satisfying rigorous DP guarantees.For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with aprivacy budget of $\epsilon=10$, while providing a $3.5$ point improvement inFID compared to public-only retrieval for up to $10,000$ queries.</description><author>Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo</author><pubDate>Mon, 13 May 2024 15:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14421v3</guid></item><item><title>NutritionVerse-Direct: Exploring Deep Neural Networks for Multitask Nutrition Prediction from Food Images</title><link>http://arxiv.org/abs/2405.07814v1</link><description>Many aging individuals encounter challenges in effectively tracking theirdietary intake, exacerbating their susceptibility to nutrition-related healthcomplications. Self-reporting methods are often inaccurate and suffer fromsubstantial bias; however, leveraging intelligent prediction methods canautomate and enhance precision in this process. Recent work has explored usingcomputer vision prediction systems to predict nutritional information from foodimages. Still, these methods are often tailored to specific situations, requireother inputs in addition to a food image, or do not provide comprehensivenutritional information. This paper aims to enhance the efficacy of dietary intake estimation byleveraging various neural network architectures to directly predict a meal'snutritional content from its image. Through comprehensive experimentation andevaluation, we present NutritionVerse-Direct, a model utilizing a visiontransformer base architecture with three fully connected layers that lead tofive regression heads predicting calories (kcal), mass (g), protein (g), fat(g), and carbohydrates (g) present in a meal. NutritionVerse-Direct yields acombined mean average error score on the NutritionVerse-Real dataset of 412.6,an improvement of 25.5% over the Inception-ResNet model, demonstrating itspotential for improving dietary intake estimation accuracy.</description><author>Matthew Keller, Chi-en Amy Tai, Yuhao Chen, Pengcheng Xi, Alexander Wong</author><pubDate>Mon, 13 May 2024 15:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07814v1</guid></item><item><title>Localizing Task Information for Improved Model Merging and Compression</title><link>http://arxiv.org/abs/2405.07813v1</link><description>Model merging and task arithmetic have emerged as promising scalableapproaches to merge multiple single-task checkpoints to one multi-task model,but their applicability is reduced by significant performance loss. Previousworks have linked these drops to interference in the weight space and erasureof important task-specific features. Instead, in this work we show that theinformation required to solve each task is still preserved after merging asdifferent tasks mostly use non-overlapping sets of weights. We proposeTALL-masks, a method to identify these task supports given a collection of taskvectors and show that one can retrieve &gt;99% of the single task accuracy byapplying our masks to the multi-task vector, effectively compressing theindividual checkpoints. We study the statistics of intersections amongconstructed masks and reveal the existence of selfish and catastrophic weights,i.e., parameters that are important exclusively to one task and irrelevant toall tasks but detrimental to multi-task fusion. For this reason, we proposeConsensus Merging, an algorithm that eliminates such weights and improves thegeneral performance of existing model merging approaches. Our experiments invision and NLP benchmarks with up to 20 tasks, show that Consensus Mergingconsistently improves existing approaches. Furthermore, our proposedcompression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% oforiginal performance.</description><author>Ke Wang, Nikolaos Dimitriadis, Guillermo Ortiz-Jimenez, François Fleuret, Pascal Frossard</author><pubDate>Mon, 13 May 2024 15:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07813v1</guid></item><item><title>Guided Interpretable Facial Expression Recognition via Spatial Action Unit Cues</title><link>http://arxiv.org/abs/2402.00281v4</link><description>Although state-of-the-art classifiers for facial expression recognition (FER)can achieve a high level of accuracy, they lack interpretability, an importantfeature for end-users. Experts typically associate spatial action units (\aus)from a codebook to facial regions for the visual interpretation of expressions.In this paper, the same expert steps are followed. A new learning strategy isproposed to explicitly incorporate \au cues into classifier training, allowingto train deep interpretable models. During training, this \au codebook is used,along with the input image expression label, and facial landmarks, to constructa \au heatmap that indicates the most discriminative image regions of interestw.r.t the facial expression. This valuable spatial cue is leveraged to train adeep interpretable classifier for FER. This is achieved by constraining thespatial layer features of a classifier to be correlated with \au heatmaps.Using a composite loss, the classifier is trained to correctly classify animage while yielding interpretable visual layer-wise attention correlated with\au maps, simulating the expert decision process. Our strategy only relies onimage class expression for supervision, without additional manual annotations.Our new strategy is generic, and can be applied to any deep CNN- ortransformer-based classifier without requiring any architectural change orsignificant additional training time. Our extensive evaluation on two publicbenchmarks \rafdb, and \affectnet datasets shows that our proposed strategy canimprove layer-wise interpretability without degrading classificationperformance. In addition, we explore a common type of interpretable classifiersthat rely on class activation mapping (CAM) methods, and show that our approachcan also improve CAM interpretability.</description><author>Soufiane Belharbi, Marco Pedersoli, Alessandro Lameiras Koerich, Simon Bacon, Eric Granger</author><pubDate>Mon, 13 May 2024 15:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00281v4</guid></item><item><title>Decoding Geometric Properties in Non-Random Data from First Information-Theoretic Principles</title><link>http://arxiv.org/abs/2405.07803v1</link><description>Based on the principles of information theory, measure theory, andtheoretical computer science, we introduce a univariate signal deconvolutionmethod with a wide range of applications to coding theory, particularly inzero-knowledge one-way communication channels, such as in deciphering messagesfrom unknown generating sources about which no prior knowledge is available andto which no return message can be sent. Our multidimensional spacereconstruction method from an arbitrary received signal is proven to beagnostic vis-a-vis the encoding-decoding scheme, computation model, programminglanguage, formal theory, the computable (or semi-computable) method ofapproximation to algorithmic complexity, and any arbitrarily chosen(computable) probability measure of the events. The method derives from theprinciples of an approach to Artificial General Intelligence capable ofbuilding a general-purpose model of models independent of any arbitrarilyassumed prior probability distribution. We argue that this optimal anduniversal method of decoding non-random data has applications to signalprocessing, causal deconvolution, topological and geometric propertiesencoding, cryptography, and bio- and technosignature detection.</description><author>Hector Zenil, Felipe S. Abrahão</author><pubDate>Mon, 13 May 2024 15:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07803v1</guid></item><item><title>Deep Learning-Based Object Pose Estimation: A Comprehensive Survey</title><link>http://arxiv.org/abs/2405.07801v1</link><description>Object pose estimation is a fundamental computer vision problem with broadapplications in augmented reality and robotics. Over the past decade, deeplearning models, due to their superior accuracy and robustness, haveincreasingly supplanted conventional algorithms reliant on engineered pointpair features. Nevertheless, several challenges persist in contemporarymethods, including their dependency on labeled training data, modelcompactness, robustness under challenging conditions, and their ability togeneralize to novel unseen objects. A recent survey discussing the progressmade on different aspects of this area, outstanding challenges, and promisingfuture directions, is missing. To fill this gap, we discuss the recent advancesin deep learning-based object pose estimation, covering all three formulationsof the problem, i.e., instance-level, category-level, and unseen object poseestimation. Our survey also covers multiple input data modalities,degrees-of-freedom of output poses, object properties, and downstream tasks,providing readers with a holistic understanding of this field. Additionally, itdiscusses training paradigms of different domains, inference modes, applicationareas, evaluation metrics, and benchmark datasets, as well as reports theperformance of current state-of-the-art methods on these benchmarks, therebyfacilitating readers in selecting the most suitable method for theirapplication. Finally, the survey identifies key challenges, reviews prevailingtrends along with their pros and cons, and identifies promising directions forfuture research. We also keep tracing the latest works athttps://github.com/CNJianLiu/Awesome-Object-Pose-Estimation.</description><author>Jian Liu, Wei Sun, Hui Yang, Zhiwen Zeng, Chongpei Liu, Jin Zheng, Xingyu Liu, Hossein Rahmani, Nicu Sebe, Ajmal Mian</author><pubDate>Mon, 13 May 2024 15:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07801v1</guid></item><item><title>Data Imputation by Pursuing Better Classification: A Supervised Kernel-Based Method</title><link>http://arxiv.org/abs/2405.07800v1</link><description>Data imputation, the process of filling in missing feature elements forincomplete data sets, plays a crucial role in data-driven learning. Afundamental belief is that data imputation is helpful for learning performance,and it follows that the pursuit of better classification can guide the dataimputation process. While some works consider using label information to assistin this task, their simplistic utilization of labels lacks flexibility and mayrely on strict assumptions. In this paper, we propose a new framework thateffectively leverages supervision information to complete missing data in amanner conducive to classification. Specifically, this framework operates intwo stages. Firstly, it leverages labels to supervise the optimization ofsimilarity relationships among data, represented by the kernel matrix, with thegoal of enhancing classification accuracy. To mitigate overfitting that mayoccur during this process, a perturbation variable is introduced to improve therobustness of the framework. Secondly, the learned kernel matrix serves asadditional supervision information to guide data imputation through regression,utilizing the block coordinate descent method. The superiority of the proposedmethod is evaluated on four real-world data sets by comparing it withstate-of-the-art imputation methods. Remarkably, our algorithm significantlyoutperforms other methods when the data is missing more than 60\% of thefeatures</description><author>Ruikai Yang, Fan He, Mingzhen He, Kaijie Wang, Xiaolin Huang</author><pubDate>Mon, 13 May 2024 15:44:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07800v1</guid></item><item><title>FreeVA: Offline MLLM as Training-Free Video Assistant</title><link>http://arxiv.org/abs/2405.07798v1</link><description>This paper undertakes an empirical study to revisit the latest advancementsin Multimodal Large Language Models (MLLMs): Video Assistant. This study,namely FreeVA, aims to extend existing image-based MLLM to the video domain ina training-free manner. The study provides an essential, yet must-knowbaseline, and reveals several surprising findings: 1) FreeVA, leveraging onlyoffline image-based MLLM without additional training, excels in zero-shot videoquestion-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), evensurpassing state-of-the-art methods that involve video instruction tuning. 2)While mainstream video-based MLLMs typically initialize with an image-basedMLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the studyindicates that utilizing the widely adopted VideoInstruct-100K for videoinstruction tuning doesn't actually lead to better performance compared to nottraining at all. 3) The commonly used evaluation metrics in existing works aresignificantly influenced by changes in the GPT API version over time. Ifignored, this could affect the fairness and uniformity of comparisons betweendifferent methods and impact the analysis and judgment of researchers in thefield. The advancement of MLLMs is currently thriving, drawing numerousresearchers into the field. We aim for this work to serve as a plug-and-play,simple yet effective baseline, encouraging the direct evaluation of existingMLLMs in video domain while also standardizing the field of videoconversational models to a certain extent. Also, we encourage researchers toreconsider: Have current video MLLM methods truly acquired knowledge beyondimage MLLM? Code is available at https://github.com/whwu95/FreeVA</description><author>Wenhao Wu</author><pubDate>Mon, 13 May 2024 15:42:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07798v1</guid></item><item><title>Improved Bound for Robust Causal Bandits with Linear Models</title><link>http://arxiv.org/abs/2405.07795v1</link><description>This paper investigates the robustness of causal bandits (CBs) in the face oftemporal model fluctuations. This setting deviates from the existingliterature's widely-adopted assumption of constant causal models. The focus ison causal systems with linear structural equation models (SEMs). The SEMs andthe time-varying pre- and post-interventional statistical models are allunknown and subject to variations over time. The goal is to design a sequenceof interventions that incur the smallest cumulative regret compared to anoracle aware of the entire causal model and its fluctuations. A robust CBalgorithm is proposed, and its cumulative regret is analyzed by establishingboth upper and lower bounds on the regret. It is shown that in a graph withmaximum in-degree $d$, length of the largest causal path $L$, and an aggregatemodel deviation $C$, the regret is upper bounded by$\tilde{\mathcal{O}}(d^{L-\frac{1}{2}}(\sqrt{T} + C))$ and lower bounded by$\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T}\; ,\; d^2C\})$. The proposed algorithmachieves nearly optimal $\tilde{\mathcal{O}}(\sqrt{T})$ regret when $C$ is$o(\sqrt{T})$, maintaining sub-linear regret for a broad range of $C$.</description><author>Zirui Yan, Arpan Mukherjee, Burak Varıcı, Ali Tajer</author><pubDate>Mon, 13 May 2024 15:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07795v1</guid></item><item><title>SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models</title><link>http://arxiv.org/abs/2405.06219v2</link><description>Large language models (LLMs) can now handle longer sequences of tokens,enabling complex tasks like book understanding and generating lengthy novels.However, the key-value (KV) cache required for LLMs consumes substantial memoryas context length increasing, becoming the bottleneck for deployment. In thispaper, we present a strategy called SKVQ, which stands for sliding-window KVcache quantization, to address the issue of extremely low bitwidth KV cachequantization. To achieve this, SKVQ rearranges the channels of the KV cache inorder to improve the similarity of channels in quantization groups, and appliesclipped dynamic quantization at the group level. Additionally, SKVQ ensuresthat the most recent window tokens in the KV cache are preserved with highprecision. This helps maintain the accuracy of a small but important portion ofthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantizationapproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bitvalues with minimal loss of accuracy. With SKVQ, it is possible to processcontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7times faster decoding.</description><author>Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, Dahua Lin</author><pubDate>Mon, 13 May 2024 15:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06219v2</guid></item><item><title>Properties of Discrete Sliced Wasserstein Losses</title><link>http://arxiv.org/abs/2307.10352v5</link><description>The Sliced Wasserstein (SW) distance has become a popular alternative to theWasserstein distance for comparing probability measures. Widespreadapplications include image processing, domain adaptation and generativemodelling, where it is common to optimise some parameters in order to minimiseSW, which serves as a loss function between discrete probability measures(since measures admitting densities are numerically unattainable). All theseoptimisation problems bear the same sub-problem, which is minimising the SlicedWasserstein energy. In this paper we study the properties of $\mathcal{E}: Y\longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance betweentwo uniform discrete measures with the same amount of points as a function ofthe support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. Weinvestigate the regularity and optimisation properties of this energy, as wellas its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation inSW using only $p$ samples) and show convergence results on the critical pointsof $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniformconvergence and a uniform Central Limit result on the process$\mathcal{E}_p(Y)$. Finally, we show that in a certain sense, StochasticGradient Descent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ convergetowards (Clarke) critical points of these energies.</description><author>Eloi Tanguy, Rémi Flamary, Julie Delon</author><pubDate>Mon, 13 May 2024 15:38:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10352v5</guid></item><item><title>Optimal Matrix Sketching over Sliding Windows</title><link>http://arxiv.org/abs/2405.07792v1</link><description>Matrix sketching, aimed at approximating a matrix $\boldsymbol{A} \in\mathbb{R}^{N\times d}$ consisting of vector streams of length $N$ with asmaller sketching matrix $\boldsymbol{B} \in \mathbb{R}^{\ell\times d}, \ell\ll N$, has garnered increasing attention in fields such as large-scale dataanalytics and machine learning. A well-known deterministic matrix sketchingmethod is the Frequent Directions algorithm, which achieves the optimal$O\left(\frac{d}{\varepsilon}\right)$ space bound and provides a covarianceerror guarantee of $\varepsilon = \lVert \boldsymbol{A}^\top \boldsymbol{A} -\boldsymbol{B}^\top \boldsymbol{B} \rVert_2/\lVert \boldsymbol{A} \rVert_F^2$.The matrix sketching problem becomes particularly interesting in the context ofsliding windows, where the goal is to approximate the matrix$\boldsymbol{A}_W$, formed by input vectors over the most recent $N$ timeunits. However, despite recent efforts, whether achieving the optimal$O\left(\frac{d}{\varepsilon}\right)$ space bound on sliding windows ispossible has remained an open question. In this paper, we introduce the DS-FD algorithm, which achieves the optimal$O\left(\frac{d}{\varepsilon}\right)$ space bound for matrix sketching overrow-normalized, sequence-based sliding windows. We also present matching upperand lower space bounds for time-based and unnormalized sliding windows,demonstrating the generality and optimality of \dsfd across various slidingwindow models. This conclusively answers the open question regarding theoptimal space bound for matrix sketching over sliding windows. Furthermore, weconduct extensive experiments with both synthetic and real-world datasets,validating our theoretical claims and thus confirming the correctness andeffectiveness of our algorithm, both theoretically and empirically.</description><author>Hanyan Yin, Dongxie Wen, Jiajun Li, Zhewei Wei, Xiao Zhang, Zengfeng Huang, Feifei Li</author><pubDate>Mon, 13 May 2024 15:38:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07792v1</guid></item><item><title>Decentralized Kernel Ridge Regression Based on Data-dependent Random Feature</title><link>http://arxiv.org/abs/2405.07791v1</link><description>Random feature (RF) has been widely used for node consistency indecentralized kernel ridge regression (KRR). Currently, the consistency isguaranteed by imposing constraints on coefficients of features, necessitatingthat the random features on different nodes are identical. However, in manyapplications, data on different nodes varies significantly on the number ordistribution, which calls for adaptive and data-dependent methods that generatedifferent RFs. To tackle the essential difficulty, we propose a newdecentralized KRR algorithm that pursues consensus on decision functions, whichallows great flexibility and well adapts data on nodes. The convergence isrigorously given and the effectiveness is numerically verified: by capturingthe characteristics of the data on each node, while maintaining the samecommunication costs as other methods, we achieved an average regressionaccuracy improvement of 25.5\% across six real-world data sets.</description><author>Ruikai Yang, Fan He, Mingzhen He, Jie Yang, Xiaolin Huang</author><pubDate>Mon, 13 May 2024 15:37:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07791v1</guid></item><item><title>Hamiltonian-based Quantum Reinforcement Learning for Neural Combinatorial Optimization</title><link>http://arxiv.org/abs/2405.07790v1</link><description>Advancements in Quantum Computing (QC) and Neural Combinatorial Optimization(NCO) represent promising steps in tackling complex computational challenges.On the one hand, Variational Quantum Algorithms such as QAOA can be used tosolve a wide range of combinatorial optimization problems. On the other hand,the same class of problems can be solved by NCO, a method that has shownpromising results, particularly since the introduction of Graph NeuralNetworks. Given recent advances in both research areas, we introduceHamiltonian-based Quantum Reinforcement Learning (QRL), an approach at theintersection of QC and NCO. We model our ansatzes directly on the combinatorialoptimization problem's Hamiltonian formulation, which allows us to apply ourapproach to a broad class of problems. Our ansatzes show favourabletrainability properties when compared to the hardware efficient ansatzes, whilealso not being limited to graph-based problems, unlike previous works. In thiswork, we evaluate the performance of Hamiltonian-based QRL on a diverse set ofcombinatorial optimization problems to demonstrate the broad applicability ofour approach and compare it to QAOA.</description><author>Georg Kruse, Rodrigo Coehlo, Andreas Rosskopf, Robert Wille, Jeanette Miriam Lorenz</author><pubDate>Mon, 13 May 2024 15:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07790v1</guid></item><item><title>DEPTH: Discourse Education through Pre-Training Hierarchically</title><link>http://arxiv.org/abs/2405.07788v1</link><description>Language Models (LMs) often struggle with linguistic understanding at thediscourse level, even though discourse patterns such as coherence, cohesion,and narrative flow are prevalent in their pre-training data. Current methodsaddress these challenges only after the pre-training phase, relying onexpensive human annotated data to align the model. To improve the discoursecapabilities of LMs already at the pre-training stage, we introduce DEPTH, anencoder-decoder model that learns to represent sentences using adiscourse-oriented pre-training objective. DEPTH combines hierarchical sentencerepresentations with two objectives: (1) Sentence Un-Shuffling, and (2)Span-Corruption. This approach trains the model to represent bothsub-word-level and sentence-level dependencies over a massive amount ofunstructured text. When trained either from scratch or continuing from apre-trained T5 checkpoint, DEPTH learns semantic and discourse-levelrepresentations faster than T5, outperforming it in span-corruption lossdespite the additional sentence-un-shuffling objective. Evaluations on theGLUE, DiscoEval, and NI benchmarks demonstrate DEPTH's ability to quickly learndiverse downstream tasks, which require syntactic, semantic, and discoursecapabilities. Overall, our approach extends the discourse capabilities of T5,while minimally impacting other natural language understanding (NLU)capabilities in the resulting LM.</description><author>Zachary Bamberger, Ofek Glick, Chaim Baskin, Yonatan Belinkov</author><pubDate>Mon, 13 May 2024 15:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07788v1</guid></item><item><title>The Update-Equivalence Framework for Decision-Time Planning</title><link>http://arxiv.org/abs/2304.13138v3</link><description>The process of revising (or constructing) a policy at execution time -- knownas decision-time planning -- has been key to achieving superhuman performancein perfect-information games like chess and Go. A recent line of work hasextended decision-time planning to imperfect-information games, leading tosuperhuman performance in poker. However, these methods involve solvingsubgames whose sizes grow quickly in the amount of non-public information,making them unhelpful when the amount of non-public information is large.Motivated by this issue, we introduce an alternative framework fordecision-time planning that is not based on solving subgames, but rather onupdate equivalence. In this update-equivalence framework, decision-timeplanning algorithms replicate the updates of last-iterate algorithms, whichneed not rely on public information. This facilitates scalability to games withlarge amounts of non-public information. Using this framework, we derive aprovably sound search algorithm for fully cooperative games based on mirrordescent and a search algorithm for adversarial games based on magnetic mirrordescent. We validate the performance of these algorithms in cooperative andadversarial domains, notably in Hanabi, the standard benchmark for search infully cooperative imperfect-information games. Here, our mirror descentapproach exceeds or matches the performance of public information-based searchwhile using two orders of magnitude less search time. This is the firstinstance of a non-public-information-based algorithm outperformingpublic-information-based approaches in a domain they have historicallydominated.</description><author>Samuel Sokota, Gabriele Farina, David J. Wu, Hengyuan Hu, Kevin A. Wang, J. Zico Kolter, Noam Brown</author><pubDate>Mon, 13 May 2024 15:34:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13138v3</guid></item><item><title>Generating Human Motion in 3D Scenes from Text Descriptions</title><link>http://arxiv.org/abs/2405.07784v1</link><description>Generating human motions from textual descriptions has gained growingresearch interest due to its wide range of applications. However, only a fewworks consider human-scene interactions together with text conditions, which iscrucial for visual and physical realism. This paper focuses on the task ofgenerating human motions in 3D indoor scenes given text descriptions of thehuman-scene interactions. This task presents challenges due to themulti-modality nature of text, scene, and motion, as well as the need forspatial reasoning. To address these challenges, we propose a new approach thatdecomposes the complex problem into two more manageable sub-problems: (1)language grounding of the target object and (2) object-centric motiongeneration. For language grounding of the target object, we leverage the powerof large language models. For motion generation, we design an object-centricscene representation for the generative model to focus on the target object,thereby reducing the scene complexity and facilitating the modeling of therelationship between human motions and the object. Experiments demonstrate thebetter motion quality of our approach compared to baselines and validate ourdesign choices.</description><author>Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, Xiaowei Zhou</author><pubDate>Mon, 13 May 2024 15:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07784v1</guid></item><item><title>Harnessing Hierarchical Label Distribution Variations in Test Agnostic Long-tail Recognition</title><link>http://arxiv.org/abs/2405.07780v1</link><description>This paper explores test-agnostic long-tail recognition, a challenginglong-tail task where the test label distributions are unknown and arbitrarilyimbalanced. We argue that the variation in these distributions can be brokendown hierarchically into global and local levels. The global ones reflect abroad range of diversity, while the local ones typically arise from milderchanges, often focused on a particular neighbor. Traditional methodspredominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixedtest label distributions that exhibit substantial global variations. However,the local variations are left unconsidered. To address this issue, we propose anew MoE strategy, $\mathsf{DirMixE}$, which assigns experts to differentDirichlet meta-distributions of the label distribution, each targeting aspecific aspect of local variations. Additionally, the diversity among theseDirichlet meta-distributions inherently captures global variations. Thisdual-level approach also leads to a more stable objective function, allowing usto sample different test distributions better to quantify the mean and varianceof performance outcomes. Theoretically, we show that our proposed objectivebenefits from enhanced generalization by virtue of the variance-basedregularization. Comprehensive experiments across multiple benchmarks confirmthe effectiveness of $\mathsf{DirMixE}$. The code is available at\url{https://github.com/scongl/DirMixE}.</description><author>Zhiyong Yang, Qianqian Xu, Zitai Wang, Sicong Li, Boyu Han, Shilong Bao, Xiaochun Cao, Qingming Huang</author><pubDate>Mon, 13 May 2024 15:24:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07780v1</guid></item><item><title>Single-seed generation of Brownian paths and integrals for adaptive and high order SDE solvers</title><link>http://arxiv.org/abs/2405.06464v2</link><description>Despite the success of adaptive time-stepping in ODE simulation, it has sofar seen few applications for Stochastic Differential Equations (SDEs). Tosimulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) havebeen developed, which can generate Brownian motion (BM) non-chronologically.However, in most applications, knowing only the values of Brownian motion isnot enough to achieve a high order of convergence; for that, we must computetime-integrals of BM such as $\int_s^t W_r \, dr$. With the aim of using highorder SDE solvers adaptively, we extend the VBT to generate these integrals ofBM in addition to the Brownian increments. A JAX-based implementation of ourconstruction is included in the popular Diffrax library(https://github.com/patrick-kidger/diffrax). Since the entire Brownian path produced by VBT is uniquely determined by asingle PRNG seed, previously generated samples need not be stored, whichresults in a constant memory footprint and enables experiment repeatability andstrong error estimation. Based on binary search, the VBT's time complexity islogarithmic in the tolerance parameter $\varepsilon$. Unlike the original VBTalgorithm, which was only precise at some dyadic times, we prove that ourconstruction exactly matches the joint distribution of the Brownian motion andits time integrals at any query times, provided they are at least $\varepsilon$apart. We present two applications of adaptive high order solvers enabled by our newVBT. Using adaptive solvers to simulate a high-volatility CIR model, we achievemore than twice the convergence order of constant stepping. We apply anadaptive third order underdamped or kinetic Langevin solver to an MCMC problem,where our approach outperforms the No U-Turn Sampler, while using only a tenthof its function evaluations.</description><author>Andraž Jelinčič, James Foster, Patrick Kidger</author><pubDate>Mon, 13 May 2024 15:23:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06464v2</guid></item><item><title>A Comprehensive Analysis of Static Word Embeddings for Turkish</title><link>http://arxiv.org/abs/2405.07778v1</link><description>Word embeddings are fixed-length, dense and distributed word representationsthat are used in natural language processing (NLP) applications. There arebasically two types of word embedding models which are non-contextual (static)models and contextual models. The former method generates a single embeddingfor a word regardless of its context, while the latter method produces distinctembeddings for a word based on the specific contexts in which it appears. Thereare plenty of works that compare contextual and non-contextual embedding modelswithin their respective groups in different languages. However, the number ofstudies that compare the models in these two groups with each other is very fewand there is no such study in Turkish. This process necessitates convertingcontextual embeddings into static embeddings. In this paper, we compare andevaluate the performance of several contextual and non-contextual models inboth intrinsic and extrinsic evaluation settings for Turkish. We make afine-grained comparison by analyzing the syntactic and semantic capabilities ofthe models separately. The results of the analyses provide insights about thesuitability of different embedding models in different types of NLP tasks. Wealso build a Turkish word embedding repository comprising the embedding modelsused in this work, which may serve as a valuable resource for researchers andpractitioners in the field of Turkish NLP. We make the word embeddings,scripts, and evaluation datasets publicly available.</description><author>Karahan Sarıtaş, Cahid Arda Öz, Tunga Güngör</author><pubDate>Mon, 13 May 2024 15:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07778v1</guid></item><item><title>Parallax-Tolerant Image Stitching with Epipolar Displacement Field</title><link>http://arxiv.org/abs/2311.16637v2</link><description>Image stitching with parallax is still a challenging task. Existing methodsoften struggle to maintain both the local and global structures of the imagewhile reducing alignment artifacts and warping distortions. In this paper, wepropose a novel approach that utilizes epipolar geometry to establish a warpingtechnique based on the epipolar displacement field. Initially, the warping rulefor pixels in the epipolar geometry is established through the infinitehomography. Subsequently, the epipolar displacement field, which represents thesliding distance of the warped pixel along the epipolar line, is formulated bythin-plate splines based on the principle of local elastic deformation. Thestitching result can be generated by inversely warping the pixels according tothe epipolar displacement field. This method incorporates the epipolarconstraints in the warping rule, which ensures high-quality alignment andmaintains the projectivity of the panorama. Qualitative and quantitativecomparative experiments demonstrate the competitiveness of the proposed methodfor stitching images with large parallax.</description><author>Jian Yu, Feipeng Da</author><pubDate>Mon, 13 May 2024 15:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16637v2</guid></item><item><title>GMSR:Gradient-Guided Mamba for Spectral Reconstruction from RGB Images</title><link>http://arxiv.org/abs/2405.07777v1</link><description>Mainstream approaches to spectral reconstruction (SR) primarily focus ondesigning Convolution- and Transformer-based architectures. However, CNNmethods often face challenges in handling long-range dependencies, whereasTransformers are constrained by computational efficiency limitations. Recentbreakthroughs in state-space model (e.g., Mamba) has attracted significantattention due to its near-linear computational efficiency and superiorperformance, prompting our investigation into its potential for SR problem. Tothis end, we propose the Gradient-guided Mamba for Spectral Reconstruction fromRGB Images, dubbed GMSR-Net. GMSR-Net is a lightweight model characterized by aglobal receptive field and linear computational complexity. Its core comprisesmultiple stacked Gradient Mamba (GM) blocks, each featuring a tri-branchstructure. In addition to benefiting from efficient global featurerepresentation by Mamba block, we further innovatively introduce spatialgradient attention and spectral gradient attention to guide the reconstructionof spatial and spectral cues. GMSR-Net demonstrates a significantaccuracy-efficiency trade-off, achieving state-of-the-art performance whilemarkedly reducing the number of parameters and computational burdens. Comparedto existing approaches, GMSR-Net slashes parameters and FLOPS by substantialmargins of 10 times and 20 times, respectively. Code is available athttps://github.com/wxy11-27/GMSR.</description><author>Xinying Wang, Zhixiong Huang, Sifan Zhang, Jiawen Zhu, Lin Feng</author><pubDate>Mon, 13 May 2024 15:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07777v1</guid></item><item><title>SAR Image Synthesis with Diffusion Models</title><link>http://arxiv.org/abs/2405.07776v1</link><description>In recent years, diffusion models (DMs) have become a popular method forgenerating synthetic data. By achieving samples of higher quality, they quicklybecame superior to generative adversarial networks (GANs) and the currentstate-of-the-art method in generative modeling. However, their potential hasnot yet been exploited in radar, where the lack of available training data is along-standing problem. In this work, a specific type of DMs, namely denoisingdiffusion probabilistic model (DDPM) is adapted to the SAR domain. Weinvestigate the network choice and specific diffusion parameters forconditional and unconditional SAR image generation. In our experiments, we showthat DDPM qualitatively and quantitatively outperforms state-of-the-artGAN-based methods for SAR image generation. Finally, we show that DDPM profitsfrom pretraining on largescale clutter data, generating SAR images of evenhigher quality.</description><author>Denisa Qosja, Simon Wagner, Daniel O'Hagan</author><pubDate>Mon, 13 May 2024 15:21:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07776v1</guid></item><item><title>PeFLL: Personalized Federated Learning by Learning to Learn</title><link>http://arxiv.org/abs/2306.05515v3</link><description>We present PeFLL, a new personalized federated learning algorithm thatimproves over the state-of-the-art in three aspects: 1) it produces moreaccurate models, especially in the low-data regime, and not only for clientspresent during its training phase, but also for any that may emerge in thefuture; 2) it reduces the amount of on-client computation and client-servercommunication by providing future clients with ready-to-use personalized modelsthat require no additional finetuning or optimization; 3) it comes withtheoretical guarantees that establish generalization from the observed clientsto future ones. At the core of PeFLL lies a learning-to-learn approach thatjointly trains an embedding network and a hypernetwork. The embedding networkis used to represent clients in a latent descriptor space in a way thatreflects their similarity to each other. The hypernetwork takes as input suchdescriptors and outputs the parameters of fully personalized client models. Incombination, both networks constitute a learning algorithm that achievesstate-of-the-art performance in several personalized federated learningbenchmarks.</description><author>Jonathan Scott, Hossein Zakerinia, Christoph H. Lampert</author><pubDate>Mon, 13 May 2024 15:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05515v3</guid></item><item><title>Human-Modeling in Sequential Decision-Making: An Analysis through the Lens of Human-Aware AI</title><link>http://arxiv.org/abs/2405.07773v1</link><description>"Human-aware" has become a popular keyword used to describe a particularclass of AI systems that are designed to work and interact with humans. Whilethere exists a surprising level of consistency among the works that use thelabel human-aware, the term itself mostly remains poorly understood. In thiswork, we retroactively try to provide an account of what constitutes ahuman-aware AI system. We see that human-aware AI is a design-orientedparadigm, one that focuses on the need for modeling the humans it may interactwith. Additionally, we see that this paradigm offers us intuitive dimensions tounderstand and categorize the kinds of interactions these systems might havewith humans. We show the pedagogical value of these dimensions by using them asa tool to understand and review the current landscape of work related tohuman-AI systems that purport some form of human modeling. To fit the scope ofa workshop paper, we specifically narrowed our review to papers that deal withsequential decision-making and were published in a major AI conference in thelast three years. Our analysis helps identify the space of potential researchproblems that are currently being overlooked. We perform additional analysis onthe degree to which these works make explicit reference to results from socialscience and whether they actually perform user-studies to validate theirsystems. We also provide an accounting of the various AI methods used by theseworks.</description><author>Silvia Tulli, Stylianos Loukas Vasileiou, Sarath Sreedharan</author><pubDate>Mon, 13 May 2024 15:17:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07773v1</guid></item><item><title>Scaling Political Texts with Large Language Models: Asking a Chatbot Might Be All You Need</title><link>http://arxiv.org/abs/2311.16639v2</link><description>We use instruction-tuned Large Language Models (LLMs) such as GPT-4, MiXtral,and Llama 3 to position political texts within policy and ideological spaces.We directly ask the LLMs where a text document or its author stand on the focalpolicy dimension. We illustrate and validate the approach by scaling Britishparty manifestos on the economic, social, and immigration policy dimensions;speeches from a European Parliament debate in 10 languages on the anti- topro-subsidy dimension; Senators of the 117th US Congress based on their tweetson the left-right ideological spectrum; and tweets published by USRepresentatives and Senators after the training cutoff date of GPT-4. Thecorrelation between the position estimates obtained with the best LLMs andbenchmarks based on coding by experts, crowdworkers or roll call votes exceeds.90. This training-free approach also outperforms supervised classifierstrained on large amounts of data. Using instruction-tuned LLMs to scale textsin policy and ideological spaces is fast, cost-efficient, reliable, andreproducible (in the case of open LLMs) even if the texts are short and writtenin different languages. We conclude with cautionary notes about the need forempirical validation.</description><author>Gaël Le Mens, Aina Gallego</author><pubDate>Mon, 13 May 2024 15:16:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16639v2</guid></item><item><title>Hype or Heuristic? Quantum Reinforcement Learning for Join Order Optimisation</title><link>http://arxiv.org/abs/2405.07770v1</link><description>Identifying optimal join orders (JOs) stands out as a key challenge indatabase research and engineering. Owing to the large search space, establishedclassical methods rely on approximations and heuristics. Recent efforts havesuccessfully explored reinforcement learning (RL) for JO. Likewise, quantumversions of RL have received considerable scientific attention. Yet, it is anopen question if they can achieve sustainable, overall practical advantageswith improved quantum processors. In this paper, we present a novel approach that uses quantum reinforcementlearning (QRL) for JO based on a hybrid variational quantum ansatz. It is ableto handle general bushy join trees instead of resorting to simpler left-deepvariants as compared to approaches based on quantum(-inspired) optimisation,yet requires multiple orders of magnitudes fewer qubits, which is a scarceresource even for post-NISQ systems. Despite moderate circuit depth, the ansatz exceeds current NISQ capabilities,which requires an evaluation by numerical simulations. While QRL may notsignificantly outperform classical approaches in solving the JO problem withrespect to result quality (albeit we see parity), we find a drastic reductionin required trainable parameters. This benefits practically relevant aspectsranging from shorter training times compared to classical RL, less involvedclassical optimisation passes, or better use of available training data, andfits data-stream and low-latency processing scenarios. Our comprehensiveevaluation and careful discussion delivers a balanced perspective on possiblepractical quantum advantage, provides insights for future systemic approaches,and allows for quantitatively assessing trade-offs of quantum approaches forone of the most crucial problems of database management systems.</description><author>Maja Franz, Tobias Winker, Sven Groppe, Wolfgang Mauerer</author><pubDate>Mon, 13 May 2024 15:14:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07770v1</guid></item><item><title>Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features</title><link>http://arxiv.org/abs/2307.09913v5</link><description>We investigate the impact of non-regular path expressions on the decidabilityof satisfiability checking and querying in description logics extending ALC.Our primary objects of interest are ALCreg and ALCvpl, the extensions of withpath expressions employing, respectively, regular and visibly-pushdownlanguages. The first one, ALCreg, is a notational variant of the well-knownPropositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, wasintroduced and investigated by Loding and Serre in 2007. The logic ALCvplgeneralises many known decidable non-regular extensions of ALCreg. We provide a series of undecidability results. First, we show thatdecidability of the concept satisfiability problem for ALCvpl is lost uponadding the seemingly innocent Self operator. Second, we establishundecidability for the concept satisfiability problem for ALCvpl extended withnominals. Interestingly, our undecidability proof relies only on one singlenon-regular (visibly-pushdown) language, namely on r#s# := { r^n s^n | n in N }for fixed role names r and s. Finally, in contrast to the classical databasesetting, we establish undecidability of query entailment for queries involvingnon-regular atoms from r#s#, already in the case of ALC-TBoxes.</description><author>Bartosz Bednarczyk</author><pubDate>Mon, 13 May 2024 15:13:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09913v5</guid></item><item><title>$α$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning</title><link>http://arxiv.org/abs/2405.07769v1</link><description>Multitask Learning is a Machine Learning paradigm that aims to train a rangeof (usually related) tasks with the help of a shared model. While the goal isoften to improve the joint performance of all training tasks, another approachis to focus on the performance of a specific target task, while treating theremaining ones as auxiliary data from which to possibly leverage positivetransfer towards the target during training. In such settings, it becomesimportant to estimate the positive or negative influence auxiliary tasks willhave on the target. While many ways have been proposed to estimate task weightsbefore or during training they typically rely on heuristics or extensive searchof the weighting space. We propose a novel method called $\alpha$-VariableImportance Learning ($\alpha$VIL) that is able to adjust task weightsdynamically during model training, by making direct use of task-specificupdates of the underlying model's parameters between training epochs.Experiments indicate that $\alpha$VIL is able to outperform other MultitaskLearning approaches in a variety of settings. To our knowledge, this is thefirst attempt at making direct use of model updates for task weight estimation.</description><author>Rafael Kourdis, Gabriel Gordon-Hall, Philip John Gorinski</author><pubDate>Mon, 13 May 2024 15:12:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07769v1</guid></item></channel></rss>