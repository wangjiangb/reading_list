<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 18 Jun 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LLaNA: Large Language and NeRF Assistant</title><link>http://arxiv.org/abs/2406.11840v1</link><description>Multimodal Large Language Models (MLLMs) have demonstrated an excellentunderstanding of images and 3D data. However, both modalities have shortcomingsin holistically capturing the appearance and geometry of objects. Meanwhile,Neural Radiance Fields (NeRFs), which encode information within the weights ofa simple Multi-Layer Perceptron (MLP), have emerged as an increasinglywidespread modality that simultaneously encodes the geometry and photorealisticappearance of objects. This paper investigates the feasibility andeffectiveness of ingesting NeRF into MLLM. We create LLaNA, the firstgeneral-purpose NeRF-language assistant capable of performing new tasks such asNeRF captioning and Q\&amp;A. Notably, our method directly processes the weights ofthe NeRF's MLP to extract information about the represented objects without theneed to render images or materialize 3D data structures. Moreover, we build adataset of NeRFs with text annotations for various NeRF-language tasks with nohuman intervention. Based on this dataset, we develop a benchmark to evaluatethe NeRF understanding capability of our method. Results show that processingNeRF weights performs favourably against extracting 2D or 3D representationsfrom NeRFs.</description><author>Andrea Amaduzzi, Pierluigi Zama Ramirez, Giuseppe Lisanti, Samuele Salti, Luigi Di Stefano</author><pubDate>Mon, 17 Jun 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11840v1</guid></item><item><title>mDPO: Conditional Preference Optimization for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2406.11839v1</link><description>Direct preference optimization (DPO) has shown to be an effective method forlarge language model (LLM) alignment. Recent works have attempted to apply DPOto multimodal scenarios but have found it challenging to achieve consistentimprovement. Through a comparative experiment, we identify the unconditionalpreference problem in multimodal preference optimization, where the modeloverlooks the image condition. To address this problem, we propose mDPO, amultimodal DPO objective that prevents the over-prioritization of language-onlypreferences by also optimizing image preference. Moreover, we introduce areward anchor that forces the reward to be positive for chosen responses,thereby avoiding the decrease in their likelihood -- an intrinsic problem ofrelative preference optimization. Experiments on two multimodal LLMs ofdifferent sizes and three widely used benchmarks demonstrate that mDPOeffectively addresses the unconditional preference problem in multimodalpreference optimization and significantly improves model performance,particularly in reducing hallucination.</description><author>Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen</author><pubDate>Mon, 17 Jun 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11839v1</guid></item><item><title>Autoregressive Image Generation without Vector Quantization</title><link>http://arxiv.org/abs/2406.11838v1</link><description>Conventional wisdom holds that autoregressive models for image generation aretypically accompanied by vector-quantized tokens. We observe that while adiscrete-valued space can facilitate representing a categorical distribution,it is not a necessity for autoregressive modeling. In this work, we propose tomodel the per-token probability distribution using a diffusion procedure, whichallows us to apply autoregressive models in a continuous-valued space. Ratherthan using categorical cross-entropy loss, we define a Diffusion Loss functionto model the per-token probability. This approach eliminates the need fordiscrete-valued tokenizers. We evaluate its effectiveness across a wide rangeof cases, including standard autoregressive models and generalized maskedautoregressive (MAR) variants. By removing vector quantization, our imagegenerator achieves strong results while enjoying the speed advantage ofsequence modeling. We hope this work will motivate the use of autoregressivegeneration in other continuous-valued domains and applications.</description><author>Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, Kaiming He</author><pubDate>Mon, 17 Jun 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11838v1</guid></item><item><title>Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%</title><link>http://arxiv.org/abs/2406.11837v1</link><description>In the realm of image quantization exemplified by VQGAN, the process encodesimages into discrete tokens drawn from a codebook with a predefined size.Recent advancements, particularly with LLAMA 3, reveal that enlarging thecodebook significantly enhances model performance. However, VQGAN and itsderivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue tograpple with challenges related to expanding the codebook size and enhancingcodebook utilization. For instance, VQGAN-FC is restricted to learning acodebook with a maximum size of 16,384, maintaining a typically low utilizationrate of less than 12% on ImageNet. In this work, we propose a novel imagequantization model named VQGAN-LC (Large Codebook), which extends the codebooksize to 100,000, achieving an utilization rate exceeding 99%. Unlike previousmethods that optimize each codebook entry, our approach begins with a codebookinitialized with 100,000 features extracted by a pre-trained vision encoder.Optimization then focuses on training a projector that aligns the entirecodebook with the feature distributions of the encoder in VQGAN-LC. Wedemonstrate the superior performance of our model over its counterparts acrossa variety of tasks, including image reconstruction, image classification,auto-regressive image generation using GPT, and image creation with diffusion-and flow-based generative models. Code and models are available athttps://github.com/zh460045050/VQGAN-LC.</description><author>Lei Zhu, Fangyun Wei, Yanye Lu, Dong Chen</author><pubDate>Mon, 17 Jun 2024 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11837v1</guid></item><item><title>RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians</title><link>http://arxiv.org/abs/2406.11836v1</link><description>In this work, we explore the possibility of training high-parameter 3DGaussian splatting (3DGS) models on large-scale, high-resolution datasets. Wedesign a general model parallel training method for 3DGS, named RetinaGS, whichuses a proper rendering equation and can be applied to any scene and arbitrarydistribution of Gaussian primitives. It enables us to explore the scalingbehavior of 3DGS in terms of primitive numbers and training resolutions thatwere difficult to explore before and surpass previous state-of-the-artreconstruction quality. We observe a clear positive trend of increasing visualquality when increasing primitive numbers with our method. We also demonstratethe first attempt at training a 3DGS model with more than one billionprimitives on the full MatrixCity dataset that attains a promising visualquality.</description><author>Bingling Li, Shengyi Chen, Luchao Wang, Kaimin He, Sijie Yan, Yuanjun Xiong</author><pubDate>Mon, 17 Jun 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11836v1</guid></item><item><title>OoDIS: Anomaly Instance Segmentation Benchmark</title><link>http://arxiv.org/abs/2406.11835v1</link><description>Autonomous vehicles require a precise understanding of their environment tonavigate safely. Reliable identification of unknown objects, especially thosethat are absent during training, such as wild animals, is critical due to theirpotential to cause serious accidents. Significant progress in semanticsegmentation of anomalies has been driven by the availability ofout-of-distribution (OOD) benchmarks. However, a comprehensive understanding ofscene dynamics requires the segmentation of individual objects, and thus thesegmentation of instances is essential. Development in this area has beenlagging, largely due to the lack of dedicated benchmarks. To address this gap,we have extended the most commonly used anomaly segmentation benchmarks toinclude the instance segmentation task. Our evaluation of anomaly instancesegmentation methods shows that this challenge remains an unsolved problem. Thebenchmark website and the competition page can be found at:https://vision.rwth-aachen.de/oodis .</description><author>Alexey Nekrasov, Rui Zhou, Miriam Ackermann, Alexander Hermans, Bastian Leibe, Matthias Rottmann</author><pubDate>Mon, 17 Jun 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11835v1</guid></item><item><title>MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs</title><link>http://arxiv.org/abs/2406.11833v1</link><description>Generating natural and meaningful responses to communicate with multi-modalhuman inputs is a fundamental capability of Large Vision-LanguageModels(LVLMs). While current open-source LVLMs demonstrate promisingperformance in simplified scenarios such as single-turn single-image input,they fall short in real-world conversation scenarios such as followinginstructions in a long context history with multi-turn and multi-images.Existing LVLM benchmarks primarily focus on single-choice questions orshort-form responses, which do not adequately assess the capabilities of LVLMsin real-world human-AI interaction applications. Therefore, we introduce MMDU,a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuningdataset, designed to evaluate and improve LVLMs' abilities in multi-turn andmulti-image conversations. We employ the clustering algorithm to ffnd therelevant images and textual descriptions from the open-source Wikipedia andconstruct the question-answer pairs by human annotators with the assistance ofthe GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and27 turns, which is at least 5x longer than previous benchmarks and poseschallenges to current LVLMs. Our in-depth analysis of 15 representative LVLMsusing MMDU reveals that open-source LVLMs lag behind closed-source counterpartsdue to limited conversational instruction tuning data. We demonstrate thatffne-tuning open-source LVLMs on MMDU-45k signiffcantly address this gap,generating longer and more accurate conversations, and improving scores on MMDUand existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA:+1.2%). Ourcontributions pave the way for bridging the gap between current LVLM models andreal-world application demands. This project is available athttps://github.com/Liuziyu77/MMDU.</description><author>Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, Jiaqi Wang</author><pubDate>Mon, 17 Jun 2024 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11833v1</guid></item><item><title>Unveiling Encoder-Free Vision-Language Models</title><link>http://arxiv.org/abs/2406.11832v1</link><description>Existing vision-language models (VLMs) mostly rely on vision encoders toextract visual features followed by large language models (LLMs) forvisual-language tasks. However, the vision encoders set a strong inductive biasin abstracting visual representation, e.g., resolution, aspect ratio, andsemantic priors, which could impede the flexibility and efficiency of the VLMs.Training pure VLMs that accept the seamless vision and language inputs, i.e.,without vision encoders, remains challenging and rarely explored. Empiricalobservations reveal that direct training without encoders results in slowconvergence and large performance gaps. In this work, we bridge the gap betweenencoder-based and encoder-free models, and present a simple yet effectivetraining recipe towards pure VLMs. Specifically, we unveil the key aspects oftraining encoder-free VLMs efficiently via thorough experiments: (1) Bridgingvision-language representation inside one unified decoder; (2) Enhancing visualrecognition capability via extra supervision. With these strategies, we launchEVE, an encoder-free vision-language model that can be trained and forwardedefficiently. Notably, solely utilizing 35M publicly accessible data, EVE canimpressively rival the encoder-based VLMs of similar capacities across multiplevision-language benchmarks. It significantly outperforms the counterpartFuyu-8B with mysterious training procedures and undisclosed training data. Webelieve that EVE provides a transparent and efficient route for developing apure decoder-only architecture across modalities. Our code and models arepublicly available at: https://github.com/baaivision/EVE.</description><author>Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, Xinlong Wang</author><pubDate>Mon, 17 Jun 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11832v1</guid></item><item><title>Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models</title><link>http://arxiv.org/abs/2406.11831v1</link><description>Large language models (LLMs) based on decoder-only transformers havedemonstrated superior text understanding capabilities compared to CLIP andT5-series models. However, the paradigm for utilizing current advanced LLMs intext-to-image diffusion models remains to be explored. We observed an unusualphenomenon: directly using a large language model as the prompt encodersignificantly degrades the prompt-following ability in image generation. Weidentified two main obstacles behind this issue. One is the misalignmentbetween the next token prediction training in LLM and the requirement fordiscriminative prompt features in diffusion models. The other is the intrinsicpositional bias introduced by the decoder-only architecture. To deal with thisissue, we propose a novel framework to fully harness the capabilities of LLMs.Through the carefully designed usage guidance, we effectively enhance the textrepresentation capability for prompt encoding and eliminate its inherentpositional bias. This allows us to integrate state-of-the-art LLMs into thetext-to-image generation model flexibly. Furthermore, we also provide aneffective manner to fuse multiple LLMs into our framework. Considering theexcellent performance and scaling capabilities demonstrated by the transformerarchitecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT)based on the framework. We conduct extensive experiments to validate LI-DiTacross model size and data size. Benefiting from the inherent ability of theLLMs and our innovative designs, the prompt understanding performance of LI-DiTeasily surpasses state-of-the-art open-source models as well as mainstreamclosed-source commercial models including Stable Diffusion 3, DALL-E 3, andMidjourney V6. The powerful LI-DiT-10B will be available after furtheroptimization and security checks.</description><author>Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, Yu Liu</author><pubDate>Mon, 17 Jun 2024 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11831v1</guid></item><item><title>Language Modeling with Editable External Knowledge</title><link>http://arxiv.org/abs/2406.11830v1</link><description>When the world changes, so does the text that humans write about it. How dowe build language models that can be easily updated to reflect these changes?One popular approach is retrieval-augmented generation, in which new documentsare inserted into a knowledge base and retrieved during prediction fordownstream tasks. Most prior work on these systems have focused on improvingbehavior during prediction through better retrieval or reasoning. This paperintroduces ERASE, which instead improves model behavior when new documents areacquired, by incrementally deleting or rewriting other entries in the knowledgebase each time a document is added. In two new benchmark datasets evaluatingmodels' ability to answer questions about a stream of news articles orconversations, ERASE improves accuracy relative to conventionalretrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B)absolute. Code and data are available at https://github.com/belindal/ERASE</description><author>Belinda Z. Li, Emmy Liu, Alexis Ross, Abbas Zeitoun, Graham Neubig, Jacob Andreas</author><pubDate>Mon, 17 Jun 2024 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11830v1</guid></item><item><title>RE-GAINS &amp; EnCHANT: Intelligent Tool Manipulation Systems For Enhanced Query Responses</title><link>http://arxiv.org/abs/2401.15724v2</link><description>Despite the remarkable success of LLMs, they still suffer from toolinvocation and tool chaining due to inadequate input queries and/or toolargument descriptions. We propose two novel frameworks, RE-GAINS and EnCHANT,enabling LLMs to tackle tool manipulation for solving complex user queries bymaking API calls. EnCHANT is an open-source solution that makes use of an LLMformat enforcer, an LLM(OpenChat 3.5) and a retriever(ToolBench's APIRetriever). RE-GAINS is based on OpenAI models and embeddings using a specialprompt based on the RAP paper. Both solutions cost less than $0.01 per querywith minimal latency, therefore showcasing the usefulness of the frameworks.</description><author>Sahil Girhepuje, Siva Sankar Sajeev, Purvam Jain, Arya Sikder, Adithya Rama Varma, Ryan George, Akshay Govind Srinivasan, Mahendra Kurup, Ashmit Sinha, Sudip Mondal</author><pubDate>Mon, 17 Jun 2024 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15724v2</guid></item><item><title>Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations</title><link>http://arxiv.org/abs/2406.11828v1</link><description>We study the computational and sample complexity of learning a targetfunction $f_*:\mathbb{R}^d\to\mathbb{R}$ with additive structure, that is,$f_*(x) = \frac{1}{\sqrt{M}}\sum_{m=1}^M f_m(\langle x, v_m\rangle)$, where$f_1,f_2,...,f_M:\mathbb{R}\to\mathbb{R}$ are nonlinear link functions ofsingle-index models (ridge functions) with diverse and near-orthogonal indexfeatures $\{v_m\}_{m=1}^M$, and the number of additive tasks $M$ grows with thedimensionality $M\asymp d^\gamma$ for $\gamma\ge 0$. This problem setting ismotivated by the classical additive model literature, the recent representationlearning theory of two-layer neural network, and large-scale pretraining wherethe model simultaneously acquires a large number of "skills" that are oftenlocalized in distinct parts of the trained network. We prove that a largesubset of polynomial $f_*$ can be efficiently learned by gradient descenttraining of a two-layer neural network, with a polynomial statistical andcomputational complexity that depends on the number of tasks $M$ and theinformation exponent of $f_m$, despite the unknown link function and $M$growing with the dimensionality. We complement this learnability guarantee withcomputational hardness result by establishing statistical query (SQ) lowerbounds for both the correlational SQ and full SQ algorithms.</description><author>Kazusato Oko, Yujin Song, Taiji Suzuki, Denny Wu</author><pubDate>Mon, 17 Jun 2024 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11828v1</guid></item><item><title>WPO: Enhancing RLHF with Weighted Preference Optimization</title><link>http://arxiv.org/abs/2406.11827v1</link><description>Reinforcement learning from human feedback (RLHF) is a promising solution toalign large language models (LLMs) more closely with human values. Off-policypreference optimization, where the preference data is obtained from othermodels, is widely adopted due to its cost efficiency and scalability. However,off-policy preference optimization often suffers from a distributional gapbetween the policy used for data collection and the target policy, leading tosuboptimal optimization. In this paper, we propose a novel strategy to mitigatethis problem by simulating on-policy learning with off-policy preference data.Our Weighted Preference Optimization (WPO) method adapts off-policy data toresemble on-policy data more closely by reweighting preference pairs accordingto their probability under the current policy. This method not only addressesthe distributional gap problem but also enhances the optimization processwithout incurring additional costs. We validate our method on instructionfollowing benchmarks including Alpaca Eval 2 and MT-bench. WPO not onlyoutperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2but also establishes a remarkable length-controlled winning rate againstGPT-4-turbo of 48.6% based on Llama-3-8B-Instruct, making it the strongest 8Bmodel on the leaderboard. We will release the code and models athttps://github.com/wzhouad/WPO.</description><author>Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, Chenguang Zhu</author><pubDate>Mon, 17 Jun 2024 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11827v1</guid></item><item><title>Spectral Introspection Identifies Group Training Dynamics in Deep Neural Networks for Neuroimaging</title><link>http://arxiv.org/abs/2406.11825v1</link><description>Neural networks, whice have had a profound effect on how researchers studycomplex phenomena, do so through a complex, nonlinear mathematical structurewhich can be difficult for human researchers to interpret. This obstacle can beespecially salient when researchers want to better understand the emergence ofparticular model behaviors such as bias, overfitting, overparametrization, andmore. In Neuroimaging, the understanding of how such phenomena emerge isfundamental to preventing and informing users of the potential risks involvedin practice. In this work, we present a novel introspection framework for DeepLearning on Neuroimaging data, which exploits the natural structure of gradientcomputations via the singular value decomposition of gradient components duringreverse-mode auto-differentiation. Unlike post-hoc introspection techniques,which require fully-trained models for evaluation, our method allows for thestudy of training dynamics on the fly, and even more interestingly, allow forthe decomposition of gradients based on which samples belong to particulargroups of interest. We demonstrate how the gradient spectra for several commondeep learning models differ between schizophrenia and control participants fromthe COBRE study, and illustrate how these trajectories may reveal specifictraining dynamics helpful for further analysis.</description><author>Bradley T. Baker, Vince D. Calhoun, Sergey M. Plis</author><pubDate>Mon, 17 Jun 2024 18:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11825v1</guid></item><item><title>Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation</title><link>http://arxiv.org/abs/2406.11824v1</link><description>We introduce Infinigen Indoors, a Blender-based procedural generator ofphotorealistic indoor scenes. It builds upon the existing Infinigen system,which focuses on natural scenes, but expands its coverage to indoor scenes byintroducing a diverse library of procedural indoor assets, including furniture,architecture elements, appliances, and other day-to-day objects. It alsointroduces a constraint-based arrangement system, which consists of adomain-specific language for expressing diverse constraints on scenecomposition, and a solver that generates scene compositions that maximallysatisfy the constraints. We provide an export tool that allows the generated 3Dobjects and scenes to be directly used for training embodied agents inreal-time simulators such as Omniverse and Unreal. Infinigen Indoors isopen-sourced under the BSD license. Please visit https://infinigen.org for codeand videos.</description><author>Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, Zeyu Ma, Jia Deng</author><pubDate>Mon, 17 Jun 2024 18:57:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11824v1</guid></item><item><title>On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning</title><link>http://arxiv.org/abs/2406.11823v1</link><description>Recent advancements in language and vision assistants have showcasedimpressive capabilities but suffer from a lack of transparency, limitingbroader research and reproducibility. While open-source models handle generalimage tasks effectively, they face challenges with the high computationaldemands of complex visually-situated text understanding. Such tasks oftenrequire increased token inputs and large vision modules to harnesshigh-resolution information. Striking a balance between model size and dataimportance remains an open question. This study aims to redefine the design ofvision-language models by identifying key components and creating efficientmodels with constrained inference costs. By strategically formulating datasets,optimizing vision modules, and enhancing supervision techniques, we achievesignificant improvements in inference throughput while maintaining highperformance. Extensive experiments across models ranging from 160M to 13Bparameters offer insights into model optimization. We will fully open-sourceour codebase, models, and datasets at https://github.com/naver-ai/elva .</description><author>Geewook Kim, Minjoon Seo</author><pubDate>Mon, 17 Jun 2024 18:57:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11823v1</guid></item><item><title>Composing Object Relations and Attributes for Image-Text Matching</title><link>http://arxiv.org/abs/2406.11820v1</link><description>We study the visual semantic embedding problem for image-text matching. Mostexisting work utilizes a tailored cross-attention mechanism to perform localalignment across the two image and text modalities. This is computationallyexpensive, even though it is more powerful than the unimodal dual-encoderapproach. This work introduces a dual-encoder image-text matching model,leveraging a scene graph to represent captions with nodes for objects andattributes interconnected by relational edges. Utilizing a graph attentionnetwork, our model efficiently encodes object-attribute and object-objectsemantic relations, resulting in a robust and fast-performing system.Representing caption as a scene graph offers the ability to utilize the strongrelational inductive bias of graph neural networks to learn object-attributeand object-object relations effectively. To train the model, we propose lossesthat align the image and caption both at the holistic level (image-caption) andthe local level (image-object entity), which we show is key to the success ofthe model. Our model is termed Composition model for Object Relations andAttributes, CORA. Experimental results on two prominent image-text retrievalbenchmarks, Flickr30K and MSCOCO, demonstrate that CORA outperforms existingstate-of-the-art computationally expensive cross-attention methods regardingrecall score while achieving fast computation speed of the dual encoder.</description><author>Khoi Pham, Chuong Huynh, Ser-Nam Lim, Abhinav Shrivastava</author><pubDate>Mon, 17 Jun 2024 18:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11820v1</guid></item><item><title>MegaScenes: Scene-Level View Synthesis at Scale</title><link>http://arxiv.org/abs/2406.11819v1</link><description>Scene-level novel view synthesis (NVS) is fundamental to many vision andgraphics applications. Recently, pose-conditioned diffusion models have led tosignificant progress by extracting 3D information from 2D foundation models,but these methods are limited by the lack of scene-level training data. Commondataset choices either consist of isolated objects (Objaverse), or ofobject-centric scenes with limited pose distributions (DTU, CO3D). In thispaper, we create a large-scale scene-level dataset from Internet photocollections, called MegaScenes, which contains over 100K structure from motion(SfM) reconstructions from around the world. Internet photos represent ascalable data source but come with challenges such as lighting and transientobjects. We address these issues to further create a subset suitable for thetask of NVS. Additionally, we analyze failure cases of state-of-the-art NVSmethods and significantly improve generation consistency. Through extensiveexperiments, we validate the effectiveness of both our dataset and method ongenerating in-the-wild scenes. For details on the dataset and code, see ourproject page at https://megascenes.github.io .</description><author>Joseph Tung, Gene Chou, Ruojin Cai, Guandao Yang, Kai Zhang, Gordon Wetzstein, Bharath Hariharan, Noah Snavely</author><pubDate>Mon, 17 Jun 2024 18:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11819v1</guid></item><item><title>Embodied Instruction Following in Unknown Environments</title><link>http://arxiv.org/abs/2406.11818v1</link><description>Enabling embodied agents to complete complex human instructions from naturallanguage is crucial to autonomous systems in household services. Conventionalmethods can only accomplish human instructions in the known environment whereall interactive objects are provided to the embodied agent, and directlydeploying the existing approaches for the unknown environment usually generatesinfeasible plans that manipulate non-existing objects. On the contrary, wepropose an embodied instruction following (EIF) method for complex tasks in theunknown environment, where the agent efficiently explores the unknownenvironment to generate feasible plans with existing objects to accomplishabstract instructions. Specifically, we build a hierarchical embodiedinstruction following framework including the high-level task planner and thelow-level exploration controller with multimodal large language models. We thenconstruct a semantic representation map of the scene with dynamic regionattention to demonstrate the known visual clues, where the goal of taskplanning and scene exploration is aligned for human instruction. For the taskplanner, we generate the feasible step-by-step plans for human goalaccomplishment according to the task completion process and the known visualclues. For the exploration controller, the optimal navigation or objectinteraction policy is predicted based on the generated step-wise plans and theknown visual clues. The experimental results demonstrate that our method canachieve 45.09% success rate in 204 complex human instructions such as makingbreakfast and tidying rooms in large house-level scenes.</description><author>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan</author><pubDate>Mon, 17 Jun 2024 18:55:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11818v1</guid></item><item><title>Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level</title><link>http://arxiv.org/abs/2406.11817v1</link><description>Direct Preference Optimization (DPO), a standard method for aligning languagemodels with human preferences, is traditionally applied to offline preferences.Recent studies show that DPO benefits from iterative training with onlinepreferences labeled by a trained reward model. In this work, we identify apitfall of vanilla iterative DPO - improved response quality can lead toincreased verbosity. To address this, we introduce iterative length-regularizedDPO (iLR-DPO) to penalize response length. Our empirical results show thatiLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasingverbosity. Specifically, our 7B model achieves a $50.5\%$ length-controlled winrate against $\texttt{GPT-4 Preview}$ on AlpacaEval 2.0, and excels acrossstandard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.These results demonstrate the effectiveness of iterative DPO in aligninglanguage models with human feedback.</description><author>Jie Liu, Zhanhui Zhou, Jiaheng Liu, Xingyuan Bu, Chao Yang, Han-Sen Zhong, Wanli Ouyang</author><pubDate>Mon, 17 Jun 2024 18:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11817v1</guid></item><item><title>VideoLLM-online: Online Video Large Language Model for Streaming Video</title><link>http://arxiv.org/abs/2406.11816v1</link><description>Recent Large Language Models have been enhanced with vision capabilities,enabling them to comprehend images, videos, and interleaved vision-languagecontent. However, the learning methods of these large multimodal modelstypically treat videos as predetermined clips, making them less effective andefficient at handling streaming video inputs. In this paper, we propose a novelLearning-In-Video-Stream (LIVE) framework, which enables temporally aligned,long-context, and real-time conversation within a continuous video stream. OurLIVE framework comprises comprehensive approaches to achieve video streamingdialogue, encompassing: (1) a training objective designed to perform languagemodeling for continuous streaming inputs, (2) a data generation scheme thatconverts offline temporal annotations into a streaming dialogue format, and (3)an optimized inference pipeline to speed up the model responses in real-worldvideo streams. With our LIVE framework, we built VideoLLM-online model uponLlama-2/Llama-3 and demonstrate its significant advantages in processingstreaming videos. For instance, on average, our model can support streamingdialogue in a 5-minute video clip at over 10 FPS on an A100 GPU. Moreover, italso showcases state-of-the-art performance on public offline video benchmarks,such as recognition, captioning, and forecasting. The code, model, data, anddemo have been made available at https://showlab.github.io/videollm-online.</description><author>Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Zheng Shou</author><pubDate>Mon, 17 Jun 2024 18:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11816v1</guid></item><item><title>LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning</title><link>http://arxiv.org/abs/2406.11815v1</link><description>In recent years, instruction-tuned Large Multimodal Models (LMMs) have beensuccessful at several tasks, including image captioning and visual questionanswering; yet leveraging these models remains an open question for robotics.Prior LMMs for robotics applications have been extensively trained on languageand action data, but their ability to generalize in different settings hasoften been less than desired. To address this, we introduce LLARVA, a modeltrained with a novel instruction tuning method that leverages structuredprompts to unify a range of robotic learning tasks, scenarios, andenvironments. Additionally, we show that predicting intermediate 2-Drepresentations, which we refer to as "visual traces", can help further alignvision and action spaces for robot learning. We generate 8.5M image-visualtrace pairs from the Open X-Embodiment dataset in order to pre-train our model,and we evaluate on 12 different tasks in the RLBench simulator as well as aphysical Franka Emika Panda 7-DoF robot. Our experiments yield strongperformance, demonstrating that LLARVA - using 2-D and language representations- performs well compared to several contemporary baselines, and can generalizeacross various robot environments and configurations.</description><author>Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, Roei Herzig</author><pubDate>Mon, 17 Jun 2024 18:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11815v1</guid></item><item><title>Stochastic Neural Network Symmetrisation in Markov Categories</title><link>http://arxiv.org/abs/2406.11814v1</link><description>We consider the problem of symmetrising a neural network along a grouphomomorphism: given a homomorphism $\varphi : H \to G$, we would like aprocedure that converts $H$-equivariant neural networks into $G$-equivariantones. We formulate this in terms of Markov categories, which allows us toconsider neural networks whose outputs may be stochastic, but withmeasure-theoretic details abstracted away. We obtain a flexible, compositional,and generic framework for symmetrisation that relies on minimal assumptionsabout the structure of the group and the underlying neural networkarchitecture. Our approach recovers existing methods for deterministicsymmetrisation as special cases, and extends directly to provide a novelmethodology for stochastic symmetrisation also. Beyond this, we believe ourfindings also demonstrate the utility of Markov categories for addressingproblems in machine learning in a conceptual yet mathematically rigorous way.</description><author>Rob Cornish</author><pubDate>Mon, 17 Jun 2024 18:54:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11814v1</guid></item><item><title>How Do Large Language Models Acquire Factual Knowledge During Pretraining?</title><link>http://arxiv.org/abs/2406.11813v1</link><description>Despite the recent observation that large language models (LLMs) can storesubstantial factual knowledge, there is a limited understanding of themechanisms of how they acquire factual knowledge through pretraining. This workaddresses this gap by studying how LLMs acquire factual knowledge duringpretraining. The findings reveal several important insights into the dynamicsof factual knowledge acquisition during pretraining. First, counterintuitively,we observe that pretraining on more data shows no significant improvement inthe model's capability to acquire and maintain factual knowledge. Next, thereis a power-law relationship between training steps and forgetting ofmemorization and generalization of factual knowledge, and LLMs trained withduplicated training data exhibit faster forgetting. Third, training LLMs withlarger batch sizes can enhance the models' robustness to forgetting. Overall,our observations suggest that factual knowledge acquisition in LLM pretrainingoccurs by progressively increasing the probability of factual knowledgepresented in the pretraining data at each step. However, this increase isdiluted by subsequent forgetting. Based on this interpretation, we demonstratethat we can provide plausible explanations for recently observed behaviors ofLLMs, such as the poor performance of LLMs on long-tail knowledge and thebenefits of deduplicating the pretraining corpus.</description><author>Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo</author><pubDate>Mon, 17 Jun 2024 18:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11813v1</guid></item><item><title>RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content</title><link>http://arxiv.org/abs/2406.11811v1</link><description>Large Language Models (LLMs) are trained on vast amounts of data, most ofwhich is automatically scraped from the internet. This data includesencyclopedic documents that harbor a vast amount of general knowledge (e.g.,Wikipedia) but also potentially overlap with benchmark datasets used forevaluating LLMs. Consequently, evaluating models on test splits that might haveleaked into the training set is prone to misleading conclusions. To fostersound evaluation of language models, we introduce a new test dataset namedRepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is acollection of five splits of test sets, four of which have not been released tothe internet or exposed to LLM APIs prior to this publication. Each sample inRepLiQA comprises (1) a reference document crafted by a human annotator anddepicting an imaginary scenario (e.g., a news article) absent from theinternet; (2) a question about the document's topic; (3) a ground-truth answerderived directly from the information in the document; and (4) the paragraphextracted from the reference document containing the answer. As such, accurateanswers can only be generated if a model can find relevant content within theprovided document. We run a large-scale benchmark comprising severalstate-of-the-art LLMs to uncover differences in performance across models ofvarious types and sizes in a context-conditional language modeling setting.Released splits of RepLiQA can be found here:https://huggingface.co/datasets/ServiceNow/repliqa.</description><author>Joao Monteiro, Pierre-Andre Noel, Etienne Marcotte, Sai Rajeswar, Valentina Zantedeschi, David Vazquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian</author><pubDate>Mon, 17 Jun 2024 18:52:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11811v1</guid></item><item><title>ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer</title><link>http://arxiv.org/abs/2309.08583v2</link><description>While state-of-the-art large language models (LLMs) can excel at adaptingtext from one style to another, current work does not address theexplainability of style transfer models. Recent work has explored generatingtextual explanations from larger teacher models and distilling them intosmaller student models. One challenge with such approach is that LLM outputsmay contain errors that require expertise to correct, but gathering andincorporating expert feedback is difficult due to cost and availability. Toaddress this challenge, we propose ICLEF, a novel human-AI collaborationapproach to model distillation that incorporates scarce expert human feedbackby combining in-context learning and model self-critique. We show that ourmethod leads to generation of high-quality synthetic explainable style transferdatasets for formality (e-GYAFC) and subjective bias (e-WNC). Via automatic andhuman evaluation, we show that specialized student models fine-tuned on ourdatasets outperform generalist teacher models on the explainable style transfertask in one-shot settings, and perform competitively compared to few-shotteacher models, highlighting the quality of the data and the role of expertfeedback. In an extrinsic task of authorship attribution, we show thatexplanations generated by smaller models fine-tuned on e-GYAFC are morepredictive of authorship than explanations generated by few-shot teachermodels.</description><author>Arkadiy Saakyan, Smaranda Muresan</author><pubDate>Mon, 17 Jun 2024 18:52:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08583v2</guid></item><item><title>Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics</title><link>http://arxiv.org/abs/2406.11810v1</link><description>We study computationally and statistically efficient Reinforcement Learningalgorithms for the linear Bellman Complete setting, a setting that uses linearfunction approximation to capture value functions and unifies existing modelslike linear Markov Decision Processes (MDP) and Linear Quadratic Regulators(LQR). While it is known from the prior works that this setting isstatistically tractable, it remained open whether a computationally efficientalgorithm exists. Our work provides a computationally efficient algorithm forthe linear Bellman complete setting that works for MDPs with large actionspaces, random initial states, and random rewards but relies on the underlyingdynamics to be deterministic. Our approach is based on randomization: we injectrandom noise into least square regression problems to perform optimistic valueiteration. Our key technical contribution is to carefully design the noise toonly act in the null space of the training data to ensure optimism whilecircumventing a subtle error amplification issue.</description><author>Runzhe Wu, Ayush Sekhari, Akshay Krishnamurthy, Wen Sun</author><pubDate>Mon, 17 Jun 2024 18:52:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11810v1</guid></item><item><title>Physics-Constrained Learning for PDE Systems with Uncertainty Quantified Port-Hamiltonian Models</title><link>http://arxiv.org/abs/2406.11809v1</link><description>Modeling the dynamics of flexible objects has become an emerging topic in thecommunity as these objects become more present in many applications, e.g., softrobotics. Due to the properties of flexible materials, the movements of softobjects are often highly nonlinear and, thus, complex to predict. Data-drivenapproaches seem promising for modeling those complex dynamics but often neglectbasic physical principles, which consequently makes them untrustworthy andlimits generalization. To address this problem, we propose aphysics-constrained learning method that combines powerful learning tools andreliable physical models. Our method leverages the data collected fromobservations by sending them into a Gaussian process that is physicallyconstrained by a distributed Port-Hamiltonian model. Based on the Bayesiannature of the Gaussian process, we not only learn the dynamics of the system,but also enable uncertainty quantification. Furthermore, the proposed approachpreserves the compositional nature of Port-Hamiltonian systems.</description><author>Kaiyuan Tan, Peilun Li, Thomas Beckers</author><pubDate>Mon, 17 Jun 2024 18:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11809v1</guid></item><item><title>Faces of Experimental Pain: Transferability of Deep Learned Heat Pain Features to Electrical Pain</title><link>http://arxiv.org/abs/2406.11808v1</link><description>The limited size of pain datasets are a challenge in developing robust deeplearning models for pain recognition. Transfer learning approaches are oftenemployed in these scenarios. In this study, we investigate whether deep learnedfeature representation for one type of experimentally induced pain can betransferred to another. Participating in the AI4Pain challenge, our goal is toclassify three levels of pain (No-Pain, Low-Pain, High-Pain). The challengedataset contains data collected from 65 participants undergoing varyingintensities of electrical pain. We utilize the video recording from the datasetto investigate the transferability of deep learned heat pain model toelectrical pain. In our proposed approach, we leverage an existing heat painconvolutional neural network (CNN) - trained on BioVid dataset - as a featureextractor. The images from the challenge dataset are inputted to thepre-trained heat pain CNN to obtain feature vectors. These feature vectors areused to train two machine learning models: a simple feed-forward neural networkand a long short-term memory (LSTM) network. Our approach was tested using thedataset's predefined training, validation, and testing splits. Our modelsoutperformed the baseline of the challenge on both the validation and testssets, highlighting the potential of models trained on other pain datasets forreliable feature extraction.</description><author>Pooja Prajod, Dominik Schiller, Daksitha Withanage Don, Elisabeth André</author><pubDate>Mon, 17 Jun 2024 18:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11808v1</guid></item><item><title>Ovis: Structural Embedding Alignment for Multimodal Large Language Model</title><link>http://arxiv.org/abs/2405.20797v2</link><description>Current Multimodal Large Language Models (MLLMs) typically integrate apre-trained LLM with another pre-trained vision transformer through aconnector, such as an MLP, endowing the LLM with visual capabilities. However,the misalignment between two embedding strategies in MLLMs -- the structuraltextual embeddings based on an embedding look-up table and the continuousembeddings generated directly by the vision encoder -- makes challenges for amore seamless fusion of visual and textual information. We propose Ovis, anovel MLLM architecture designed to structurally align visual and textualembeddings. Ovis integrates an additional learnable visual embedding table intothe visual encoder's process. To capture rich visual semantics, each imagepatch indexes the visual embedding table multiple times, resulting in a finalvisual embedding that is a probabilistic combination of the indexed embeddings.This structural approach mirrors the method used for generating textualembeddings. Empirical evaluations on various multimodal benchmarks show thatOvis outperforms open-source MLLMs of similar parameter scales and evensurpasses the proprietary model Qwen-VL-Plus overall. These results highlightthe potential of Ovis' structured visual representation for advancing MLLMarchitectural design and promoting more effective multimodal learning. Code,datasets, and models are available at https://github.com/AIDC-AI/Ovis.</description><author>Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Han-Jia Ye</author><pubDate>Mon, 17 Jun 2024 18:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20797v2</guid></item><item><title>Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA</title><link>http://arxiv.org/abs/2406.09396v2</link><description>Long-form videos that span across wide temporal intervals are highlyinformation redundant and contain multiple distinct events or entities that areoften loosely-related. Therefore, when performing long-form video questionanswering (LVQA),all information necessary to generate a correct response canoften be contained within a small subset of frames. Recent literature explorethe use of large language models (LLMs) in LVQA benchmarks, achievingexceptional performance, while relying on vision language models (VLMs) toconvert all visual content within videos into natural language. Such VLMs oftenindependently caption a large number of frames uniformly sampled from longvideos, which is not efficient and can mostly be redundant. Questioning thesedecision choices, we explore optimal strategies for key-frame selection andsequence-aware captioning, that can significantly reduce these redundancies. Wepropose two novel approaches that improve each of aspects, namely HierarchicalKeyframe Selector and Sequential Visual LLM. Our resulting framework termedLVNet achieves state-of-the-art performance across three benchmark LVQAdatasets. Our code will be released publicly.</description><author>Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryoo, Donghyun Kim, Michael S. Ryoo</author><pubDate>Mon, 17 Jun 2024 18:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09396v2</guid></item><item><title>Efficient Discovery of Significant Patterns with Few-Shot Resampling</title><link>http://arxiv.org/abs/2406.11803v1</link><description>Significant pattern mining is a fundamental task in mining transactionaldata, requiring to identify patterns significantly associated with the value ofa given feature, the target. In several applications, such as biomedicine,basket market analysis, and social networks, the goal is to discover patternswhose association with the target is defined with respect to an underlyingpopulation, or process, of which the dataset represents only a collection ofobservations, or samples. A natural way to capture the association of a patternwith the target is to consider its statistical significance, assessing itsdeviation from the (null) hypothesis of independence between the pattern andthe target. While several algorithms have been proposed to find statisticallysignificant patterns, it remains a computationally demanding task, and forcomplex patterns such as subgroups, no efficient solution exists. We present FSR, an efficient algorithm to identify statistically significantpatterns with rigorous guarantees on the probability of false discoveries. FSRbuilds on a novel general framework for mining significant patterns thatcaptures some of the most commonly considered patterns, including itemsets,sequential patterns, and subgroups. FSR uses a small number of resampleddatasets, obtained by assigning i.i.d. labels to each transaction, torigorously bound the supremum deviation of a quality statistic measuring thesignificance of patterns. FSR builds on novel tight bounds on the supremumdeviation that require to mine a small number of resampled datasets, whileproviding a high effectiveness in discovering significant patterns. As a testcase, we consider significant subgroup mining, and our evaluation on severalreal datasets shows that FSR is effective in discovering significant subgroups,while requiring a small number of resampled datasets.</description><author>Leonardo Pellegrina, Fabio Vandin</author><pubDate>Mon, 17 Jun 2024 18:49:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11803v1</guid></item><item><title>PhyBench: A Physical Commonsense Benchmark for Evaluating Text-to-Image Models</title><link>http://arxiv.org/abs/2406.11802v1</link><description>Text-to-image (T2I) models have made substantial progress in generatingimages from textual prompts. However, they frequently fail to produce imagesconsistent with physical commonsense, a vital capability for applications inworld simulation and everyday tasks. Current T2I evaluation benchmarks focus onmetrics such as accuracy, bias, and safety, neglecting the evaluation ofmodels' internal knowledge, particularly physical commonsense. To address thisissue, we introduce PhyBench, a comprehensive T2I evaluation dataset comprising700 prompts across 4 primary categories: mechanics, optics, thermodynamics, andmaterial properties, encompassing 31 distinct physical scenarios. We assess 6prominent T2I models, including proprietary models DALLE3 and Gemini, anddemonstrate that incorporating physical principles into prompts enhances themodels' ability to generate physically accurate images. Our findings revealthat: (1) even advanced models frequently err in various physical scenarios,except for optics; (2) GPT-4o, with item-specific scoring instructions,effectively evaluates the models' understanding of physical commonsense,closely aligning with human assessments; and (3) current T2I models areprimarily focused on text-to-image translation, lacking profound reasoningregarding physical commonsense. We advocate for increased attention to theinherent knowledge within T2I models, beyond their utility as mere imagegeneration tools. The code and data are available athttps://github.com/OpenGVLab/PhyBench.</description><author>Fanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, Ping Luo</author><pubDate>Mon, 17 Jun 2024 18:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11802v1</guid></item><item><title>Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations</title><link>http://arxiv.org/abs/2406.11801v1</link><description>Ensuring the safe alignment of large language models (LLMs) with human valuesis critical as they become integral to applications like translation andquestion answering. Current alignment methods struggle with dynamic userintentions and complex objectives, making models vulnerable to generatingharmful content. We propose Safety Arithmetic, a training-free frameworkenhancing LLM safety across different scenarios: Base models, Supervisedfine-tuned models (SFT), and Edited models. Safety Arithmetic involves HarmDirection Removal to avoid harmful content and Safety Alignment to promote saferesponses. Additionally, we present NoIntentEdit, a dataset highlighting editinstances that could compromise model safety if used unintentionally. Ourexperiments show that Safety Arithmetic significantly improves safety measures,reduces over-safety, and maintains model utility, outperforming existingmethods in ensuring safe content generation.</description><author>Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria</author><pubDate>Mon, 17 Jun 2024 18:48:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11801v1</guid></item><item><title>Measurement Simplification in ρ-POMDP with Performance Guarantees</title><link>http://arxiv.org/abs/2309.10701v2</link><description>Decision making under uncertainty is at the heart of any autonomous systemacting with imperfect information. The cost of solving the decision makingproblem is exponential in the action and observation spaces, thus rendering itunfeasible for many online systems. This paper introduces a novel approach toefficient decision-making, by partitioning the high-dimensional observationspace. Using the partitioned observation space, we formulate analytical boundson the expected information-theoretic reward, for general belief distributions.These bounds are then used to plan efficiently while keeping performanceguarantees. We show that the bounds are adaptive, computationally efficient,and that they converge to the original solution. We extend the partitioningparadigm and present a hierarchy of partitioned spaces that allows greaterefficiency in planning. We then propose a specific variant of these bounds forGaussian beliefs and show a theoretical performance improvement of at least afactor of 4. Finally, we compare our novel method to other state of the artalgorithms in active SLAM scenarios, in simulation and in real experiments. Inboth cases we show a significant speed-up in planning with performanceguarantees.</description><author>Tom Yotam, Vadim Indelman</author><pubDate>Mon, 17 Jun 2024 18:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10701v2</guid></item><item><title>Mix-Domain Contrastive Learning for Unpaired H&amp;E-to-IHC Stain Translation</title><link>http://arxiv.org/abs/2406.11799v1</link><description>H&amp;E-to-IHC stain translation techniques offer a promising solution forprecise cancer diagnosis, especially in low-resource regions where there is ashortage of health professionals and limited access to expensive equipment.Considering the pixel-level misalignment of H&amp;E-IHC image pairs, currentresearch explores the pathological consistency between patches from the samepositions of the image pair. However, most of them overemphasize thecorrespondence between domains or patches, overlooking the side informationprovided by the non-corresponding objects. In this paper, we propose aMix-Domain Contrastive Learning (MDCL) method to leverage the supervisioninformation in unpaired H&amp;E-to-IHC stain translation. Specifically, theproposed MDCL method aggregates the inter-domain and intra-domain pathologyinformation by estimating the correlation between the anchor patch and all thepatches from the matching images, encouraging the network to learn additionalcontrastive knowledge from mixed domains. With the mix-domain pathologyinformation aggregation, MDCL enhances the pathological consistency between thecorresponding patches and the component discrepancy of the patches from thedifferent positions of the generated IHC image. Extensive experiments on twoH&amp;E-to-IHC stain translation datasets, namely MIST and BCI, demonstrate thatthe proposed method achieves state-of-the-art performance across multiplemetrics.</description><author>Song Wang, Zhong Zhang, Huan Yan, Ming Xu, Guanghui Wang</author><pubDate>Mon, 17 Jun 2024 18:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11799v1</guid></item><item><title>InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?</title><link>http://arxiv.org/abs/2402.10567v4</link><description>Recent advancements in language technology and Artificial Intelligence haveresulted in numerous Language Models being proposed to perform various tasks inthe legal domain ranging from predicting judgments to generating summaries.Despite their immense potential, these models have been proven to learn andexhibit societal biases and make unfair predictions. In this study, we explorethe ability of Large Language Models (LLMs) to perform legal tasks in theIndian landscape when social factors are involved. We present a novel metric,$\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, whichencapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs'safety by considering its performance in the $\textit{Binary StatutoryReasoning}$ task and its fairness exhibition with respect to various axes ofdisparities in the Indian society. Task performance and fairness scores ofLLaMA and LLaMA--2 models indicate that the proposed $LSS_{\beta}$ metric caneffectively determine the readiness of a model for safe usage in the legalsector. We also propose finetuning pipelines, utilising specialised legaldatasets, as a potential method to mitigate bias and improve model safety. Thefinetuning procedures on LLaMA and LLaMA--2 models increase the $LSS_{\beta}$,improving their usability in the Indian legal domain. Our code is publiclyreleased.</description><author>Yogesh Tripathi, Raghav Donakanti, Sahil Girhepuje, Ishan Kavathekar, Bhaskara Hanuma Vedula, Gokul S Krishnan, Shreya Goyal, Anmol Goel, Balaraman Ravindran, Ponnurangam Kumaraguru</author><pubDate>Mon, 17 Jun 2024 18:46:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10567v4</guid></item><item><title>Grokking Group Multiplication with Cosets</title><link>http://arxiv.org/abs/2312.06581v2</link><description>The complex and unpredictable nature of deep neural networks prevents theirsafe use in many high-stakes applications. There have been many techniquesdeveloped to interpret deep neural networks, but all have substantiallimitations. Algorithmic tasks have proven to be a fruitful test ground forinterpreting a neural network end-to-end. Building on previous work, wecompletely reverse engineer fully connected one-hidden layer networks that have``grokked'' the arithmetic of the permutation groups $S_5$ and $S_6$. Themodels discover the true subgroup structure of the full group and converge onneural circuits that decompose the group arithmetic using the permutationgroup's subgroups. We relate how we reverse engineered the model's mechanismsand confirmed our theory was a faithful description of the circuit'sfunctionality. We also draw attention to current challenges in conductinginterpretability research by comparing our work to Chughtai et al. [4] whichalleges to find a different algorithm for this same problem.</description><author>Dashiell Stander, Qinan Yu, Honglu Fan, Stella Biderman</author><pubDate>Mon, 17 Jun 2024 18:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06581v2</guid></item><item><title>DataComp-LM: In search of the next generation of training sets for language models</title><link>http://arxiv.org/abs/2406.11794v1</link><description>We introduce DataComp for Language Models (DCLM), a testbed for controlleddataset experiments with the goal of improving language models. As part ofDCLM, we provide a standardized corpus of 240T tokens extracted from CommonCrawl, effective pretraining recipes based on the OpenLM framework, and a broadsuite of 53 downstream evaluations. Participants in the DCLM benchmark canexperiment with data curation strategies such as deduplication, filtering, anddata mixing at model scales ranging from 412M to 7B parameters. As a baselinefor DCLM, we conduct extensive experiments and find that model-based filteringis key to assembling a high-quality training set. The resulting dataset,DCLM-Baseline enables training a 7B parameter language model from scratch to64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, theprevious state-of-the-art in open-data language models, DCLM-Baselinerepresents a 6.6 percentage point improvement on MMLU while being trained with40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 andLlama 3 8B on MMLU (63% &amp; 66%), and performs similarly on an average of 53natural language understanding tasks while being trained with 6.6x less computethan Llama 3 8B. Our results highlight the importance of dataset design fortraining language models and offer a starting point for further research ondata curation.</description><author>Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muenninghoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldani, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alex</author><pubDate>Mon, 17 Jun 2024 18:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11794v1</guid></item><item><title>A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping</title><link>http://arxiv.org/abs/2406.11786v1</link><description>Robotic grasping presents a difficult motor task in real-world scenarios,constituting a major hurdle to the deployment of capable robots across variousindustries. Notably, the scarcity of data makes grasping particularlychallenging for learned models. Recent advancements in computer vision havewitnessed a growth of successful unsupervised training mechanisms predicated onmassive amounts of data sourced from the Internet, and now nearly all prominentmodels leverage pretrained backbone networks. Against this backdrop, we beginto investigate the potential benefits of large-scale visual pretraining inenhancing robot grasping performance. This preliminary literature review shedslight on critical challenges and delineates prospective directions for futureresearch in visual pretraining for robotic manipulation.</description><author>Abhi Kamboj, Katherine Driggs-Campbell</author><pubDate>Mon, 17 Jun 2024 18:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11786v1</guid></item><item><title>CELL your Model: Contrastive Explanation Methods for Large Language Models</title><link>http://arxiv.org/abs/2406.11785v1</link><description>The advent of black-box deep neural network classification models has sparkedthe need to explain their decisions. However, in the case of generative AI suchas large language models (LLMs), there is no class prediction to explain.Rather, one can ask why an LLM output a particular response to a given prompt.In this paper, we answer this question by proposing, to the best of ourknowledge, the first contrastive explanation methods requiring simplyblack-box/query access. Our explanations suggest that an LLM outputs a reply toa given prompt because if the prompt was slightly modified, the LLM would havegiven a different response that is either less preferable or contradicts theoriginal response. The key insight is that contrastive explanations simplyrequire a distance function that has meaning to the user and not necessarily areal valued representation of a specific response (viz. class label). We offertwo algorithms for finding contrastive explanations: i) A myopic algorithm,which although effective in creating contrasts, requires many model calls andii) A budgeted algorithm, our main algorithmic contribution, whichintelligently creates contrasts adhering to a query budget, necessary forlonger contexts. We show the efficacy of these methods on diverse naturallanguage tasks such as open-text generation, automated red teaming, andexplaining conversational degradation.</description><author>Ronny Luss, Erik Miehling, Amit Dhurandhar</author><pubDate>Mon, 17 Jun 2024 18:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11785v1</guid></item><item><title>MDCR: A Dataset for Multi-Document Conditional Reasoning</title><link>http://arxiv.org/abs/2406.11784v1</link><description>The same real-life questions posed to different individuals may lead todifferent answers based on their unique situations. For instance, whether astudent is eligible for a scholarship depends on eligibility conditions, suchas major or degree required. ConditionalQA was proposed to evaluate models'capability of reading a document and answering eligibility questions,considering unmentioned conditions. However, it is limited to questions onsingle documents, neglecting harder cases that may require cross-documentreasoning and optimization, for example, "What is the maximum number ofscholarships attainable?" Such questions over multiple documents are not onlymore challenging due to more context having to understand, but also because themodel has to (1) explore all possible combinations of unmentioned conditionsand (2) understand the relationship between conditions across documents, toreason about the optimal outcome. To evaluate models' capability of answeringsuch questions, we propose a new dataset MDCR, which can reflect real-worldchallenges and serve as a new test bed for complex conditional reasoning thatrequires optimization. We evaluate this dataset using the most recent LLMs anddemonstrate their limitations in solving this task. We believe this datasetwill facilitate future research in answering optimization questions withunknown conditions.</description><author>Peter Baile Chen, Yi Zhang, Chunwei Liu, Sejal Gupta, Yoon Kim, Michael Cafarella</author><pubDate>Mon, 17 Jun 2024 18:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11784v1</guid></item><item><title>Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs</title><link>http://arxiv.org/abs/2406.11780v1</link><description>Large language models (LLMs) have shown to pose social and ethical risks suchas generating toxic language or facilitating malicious use of hazardousknowledge. Machine unlearning is a promising approach to improve LLM safety bydirectly removing harmful behaviors and knowledge. In this paper, we propose"SPlit, UNlearn, MerGE" (SPUNGE), a framework that can be used with anyunlearning method to amplify its effectiveness. SPUNGE leverages dataattributes during unlearning by splitting unlearning data into subsets based onspecific attribute values, unlearning each subset separately, and merging theunlearned models. We empirically demonstrate that SPUNGE significantly improvesthe performance of two recent unlearning methods on state-of-the-art LLMs whilemaintaining their general capabilities on standard academic benchmarks.</description><author>Swanand Ravindra Kadhe, Farhan Ahmed, Dennis Wei, Nathalie Baracaldo, Inkit Padhi</author><pubDate>Mon, 17 Jun 2024 18:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11780v1</guid></item><item><title>Shaping Up SHAP: Enhancing Stability through Layer-Wise Neighbor Selection</title><link>http://arxiv.org/abs/2312.12115v2</link><description>Machine learning techniques, such as deep learning and ensemble methods, arewidely used in various domains due to their ability to handle complexreal-world tasks. However, their black-box nature has raised multiple concernsabout the fairness, trustworthiness, and transparency of computer-assisteddecision-making. This has led to the emergence of local post-hoc explainabilitymethods, which offer explanations for individual decisions made by black-boxalgorithms. Among these methods, Kernel SHAP is widely used due to itsmodel-agnostic nature and its well-founded theoretical framework. Despite thesestrengths, Kernel SHAP suffers from high instability: different executions ofthe method with the same inputs can lead to significantly differentexplanations, which diminishes the relevance of the explanations. Thecontribution of this paper is two-fold. On the one hand, we show that KernelSHAP's instability is caused by its stochastic neighbor selection procedure,which we adapt to achieve full stability without compromising explanationfidelity. On the other hand, we show that by restricting the neighborsgeneration to perturbations of size 1 -- which we call the coalitions of Layer1 -- we obtain a novel feature-attribution method that is fully stable,computationally efficient, and still meaningful.</description><author>Gwladys Kelodjou, Laurence Rozé, Véronique Masson, Luis Galárraga, Romaric Gaudel, Maurice Tchuente, Alexandre Termier</author><pubDate>Mon, 17 Jun 2024 18:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12115v2</guid></item><item><title>Provable Guarantees for Model Performance via Mechanistic Interpretability</title><link>http://arxiv.org/abs/2406.11779v1</link><description>In this work, we propose using mechanistic interpretability -- techniques forreverse engineering model weights into human-interpretable algorithms -- toderive and compactly prove formal guarantees on model performance. We prototypethis approach by formally lower bounding the accuracy of 151 small transformerstrained on a Max-of-$k$ task. We create 102 different computer-assisted proofstrategies and assess their length and tightness of bound on each of ourmodels. Using quantitative metrics, we show that shorter proofs seem to requireand provide more mechanistic understanding, and that more faithful mechanisticunderstanding leads to tighter performance bounds. We confirm these connectionsby qualitatively examining a subset of our proofs. Finally, we identifycompounding structureless noise as a key challenge for using mechanisticinterpretability to generate compact proofs on model performance.</description><author>Jason Gross, Rajashree Agrawal, Thomas Kwa, Euan Ong, Chun Hei Yip, Alex Gibson, Soufiane Noubir, Lawrence Chan</author><pubDate>Mon, 17 Jun 2024 18:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11779v1</guid></item><item><title>Brain-inspired Computational Modeling of Action Recognition with Recurrent Spiking Neural Networks Equipped with Reinforcement Delay Learning</title><link>http://arxiv.org/abs/2406.11778v1</link><description>The growing interest in brain-inspired computational models arises from theremarkable problem-solving efficiency of the human brain. Action recognition, acomplex task in computational neuroscience, has received significant attentiondue to both its intricate nature and the brain's exceptional performance inthis area. Nevertheless, current solutions for action recognition eitherexhibit limitations in effectively addressing the problem or lack the necessarybiological plausibility. Deep neural networks, for instance, demonstrateacceptable performance but deviate from biological evidence, therebyrestricting their suitability for brain-inspired computational studies. On theother hand, the majority of brain-inspired models proposed for actionrecognition exhibit significantly lower effectiveness compared to deep modelsand fail to achieve human-level performance. This deficiency can be attributedto their disregard for the underlying mechanisms of the brain. In this article,we present an effective brain-inspired computational model for actionrecognition. We equip our model with novel biologically plausible mechanismsfor spiking neural networks that are crucial for learning spatio-temporalpatterns. The key idea behind these new mechanisms is to bridge the gap betweenthe brain's capabilities and action recognition tasks by integrating keybiological principles into our computational framework. Furthermore, weevaluate the performance of our model against other models using a benchmarkdataset for action recognition, DVS-128 Gesture. The results show that ourmodel outperforms previous biologically plausible models and competes with deepsupervised models.</description><author>Alireza Nadafian, Milad Mozafari, Timothée Masquelier, Mohammad Ganjtabesh</author><pubDate>Mon, 17 Jun 2024 18:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11778v1</guid></item><item><title>Improving Multi-Agent Debate with Sparse Communication Topology</title><link>http://arxiv.org/abs/2406.11776v1</link><description>Multi-agent debate has proven effective in improving large language modelsquality for reasoning and factuality tasks. While various role-playingstrategies in multi-agent debates have been explored, in terms of thecommunication among agents, existing approaches adopt a brute force algorithm-- each agent can communicate with all other agents. In this paper, wesystematically investigate the effect of communication connectivity inmulti-agent systems. Our experiments on GPT and Mistral models reveal thatmulti-agent debates leveraging sparse communication topology can achievecomparable or superior performance while significantly reducing computationalcosts. Furthermore, we extend the multi-agent debate framework to multimodalreasoning and alignment labeling tasks, showcasing its broad applicability andeffectiveness. Our findings underscore the importance of communicationconnectivity on enhancing the efficiency and effectiveness of the "society ofminds" approach.</description><author>Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, Eugene Ie</author><pubDate>Mon, 17 Jun 2024 18:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11776v1</guid></item><item><title>Task Me Anything</title><link>http://arxiv.org/abs/2406.11775v1</link><description>Benchmarks for large multimodal language models (MLMs) now serve tosimultaneously assess the general capabilities of models instead of evaluatingfor a specific capability. As a result, when a developer wants to identifywhich models to use for their application, they are overwhelmed by the numberof benchmarks and remain uncertain about which benchmark's results are mostreflective of their specific use case. This paper introduces Task-Me-Anything,a benchmark generation engine which produces a benchmark tailored to a user'sneeds. Task-Me-Anything maintains an extendable taxonomy of visual assets andcan programmatically generate a vast number of task instances. Additionally, italgorithmically addresses user queries regarding MLM performance efficientlywithin a computational budget. It contains 113K images, 10K videos, 2K 3Dobject assets, over 365 object categories, 655 attributes, and 335relationships. It can generate 750M image/video question-answering pairs, whichfocus on evaluating MLM perceptual capabilities. Task-Me-Anything revealscritical insights: open-source MLMs excel in object and attribute recognitionbut lack spatial and temporal understanding; each model exhibits uniquestrengths and weaknesses; larger models generally perform better, thoughexceptions exist; and GPT4o demonstrates challenges in recognizingrotating/moving objects and distinguishing colors.</description><author>Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, Ranjay Krishna</author><pubDate>Mon, 17 Jun 2024 18:32:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11775v1</guid></item><item><title>Optimal Transport-Assisted Risk-Sensitive Q-Learning</title><link>http://arxiv.org/abs/2406.11774v1</link><description>The primary goal of reinforcement learning is to develop decision-makingpolicies that prioritize optimal performance without considering risk orsafety. In contrast, safe reinforcement learning aims to mitigate or avoidunsafe states. This paper presents a risk-sensitive Q-learning algorithm thatleverages optimal transport theory to enhance the agent safety. By integratingoptimal transport into the Q-learning framework, our approach seeks to optimizethe policy's expected return while minimizing the Wasserstein distance betweenthe policy's stationary distribution and a predefined risk distribution, whichencapsulates safety preferences from domain experts. We validate the proposedalgorithm in a Gridworld environment. The results indicate that our methodsignificantly reduces the frequency of visits to risky states and achievesfaster convergence to a stable policy compared to the traditional Q-learningalgorithm.</description><author>Zahra Shahrooei, Ali Baheri</author><pubDate>Mon, 17 Jun 2024 18:32:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11774v1</guid></item><item><title>Deep Learning methodology for the identification of wood species using high-resolution macroscopic images</title><link>http://arxiv.org/abs/2406.11772v1</link><description>Significant advancements in the field of wood species identification areneeded worldwide to support sustainable timber trade. In this work wecontribute to automate the identification of wood species via high-resolutionmacroscopic images of timber. The main challenge of this problem is thatfine-grained patterns in timber are crucial in order to accurately identifywood species, and these patterns are not properly learned by traditionalconvolutional neural networks (CNNs) trained on low/medium resolution images. We propose a Timber Deep Learning Identification with Patch-based InferenceVoting methodology, abbreviated TDLI-PIV methodology. Our proposal exploits theconcept of patching and the availability of high-resolution macroscopic imagesof timber in order to overcome the inherent challenges that CNNs face in timberidentification. The TDLI-PIV methodology is able to capture fine-grainedpatterns in timber and, moreover, boosts robustness and prediction accuracy viaa collaborative voting inference process. In this work we also introduce a new data set of marcroscopic images oftimber, called GOIMAI-Phase-I, which has been obtained using opticalmagnification in order to capture fine-grained details, which contrasts to theother datasets that are publicly available. More concretely, images inGOIMAI-Phase-I are taken with a smartphone with a 24x magnifying lens attachedto the camera. Our data set contains 2120 images of timber and covers 37legally protected wood species. Our experiments have assessed the performance of the TDLI-PIV methodology,involving the comparison with other methodologies available in the literature,exploration of data augmentation methods and the effect that the dataset sizehas on the accuracy of TDLI-PIV.</description><author>David Herrera-Poyatos, Andrés Herrera-Poyatos, Rosana Montes, Paloma de Palacios, Luis G. Esteban, Alberto García Iruela, Francisco García Fernández, Francisco Herrera</author><pubDate>Mon, 17 Jun 2024 18:31:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11772v1</guid></item><item><title>Solving Vision Tasks with Simple Photoreceptors Instead of Cameras</title><link>http://arxiv.org/abs/2406.11769v1</link><description>A de facto standard in solving computer vision problems is to use a commonhigh-resolution camera and choose its placement on an agent (i.e., position andorientation) based on human intuition. On the other hand, extremely simple andwell-designed visual sensors found throughout nature allow many organisms toperform diverse, complex behaviors. In this work, motivated by these examples,we raise the following questions: 1. How effective simple visual sensors are insolving vision tasks? 2. What role does their design play in theireffectiveness? We explore simple sensors with resolutions as low as one-by-onepixel, representing a single photoreceptor First, we demonstrate that just afew photoreceptors can be enough to solve many tasks, such as visual navigationand continuous control, reasonably well, with performance comparable to that ofa high-resolution camera. Second, we show that the design of these simplevisual sensors plays a crucial role in their ability to provide usefulinformation and successfully solve these tasks. To find a well-performingdesign, we present a computational design optimization algorithm and evaluateits effectiveness across different tasks and domains, showing promisingresults. Finally, we perform a human survey to evaluate the effectiveness ofintuitive designs devised manually by humans, showing that the computationallyfound design is among the best designs in most cases.</description><author>Andrei Atanov, Jiawei Fu, Rishubh Singh, Isabella Yu, Andrew Spielberg, Amir Zamir</author><pubDate>Mon, 17 Jun 2024 18:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11769v1</guid></item><item><title>GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities</title><link>http://arxiv.org/abs/2406.11768v1</link><description>Perceiving and understanding non-speech sounds and non-verbal speech isessential to making decisions that help us interact with our surroundings. Inthis paper, we propose GAMA, a novel General-purpose Large Audio-Language Model(LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. Webuild GAMA by integrating an LLM with multiple types of audio representations,including features from a custom Audio Q-Former, a multi-layer aggregator thataggregates features from multiple layers of an audio encoder. We fine-tune GAMAon a large-scale audio-language dataset, which augments it with audiounderstanding capabilities. Next, we propose CompA-R (Instruction-Tuning forComplex Audio Reasoning), a synthetically generated instruction-tuning (IT)dataset with instructions that require the model to perform complex reasoningon the input audio. We instruction-tune GAMA with CompA-R to endow it withcomplex reasoning abilities, where we further add a soft prompt as input withhigh-level semantic evidence by leveraging event tags of the input audio.Finally, we also propose CompA-R-test, a human-labeled evaluation dataset forevaluating the capabilities of LALMs on open-ended audio question-answeringthat requires complex reasoning. Through automated and expert humanevaluations, we show that GAMA outperforms all other LALMs in literature ondiverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed onCompA-R proves to be superior in its complex reasoning and instructionfollowing capabilities.</description><author>Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, S Sakshi, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha</author><pubDate>Mon, 17 Jun 2024 18:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11768v1</guid></item><item><title>Matching Query Image Against Selected NeRF Feature for Efficient and Scalable Localization</title><link>http://arxiv.org/abs/2406.11766v1</link><description>Neural implicit representations such as NeRF have revolutionized 3D scenerepresentation with photo-realistic quality. However, existing methods forvisual localization within NeRF representations suffer from inefficiency andscalability issues, particularly in large-scale environments. This workproposes MatLoc-NeRF, a novel matching-based localization framework usingselected NeRF features. It addresses efficiency by employing a learnablefeature selection mechanism that identifies informative NeRF features formatching with query images. This eliminates the need for all NeRF features oradditional descriptors, leading to faster and more accurate pose estimation. Totackle large-scale scenes, MatLoc-NeRF utilizes a pose-aware scene partitioningstrategy. It ensures that only the most relevant NeRF sub-block generates keyfeatures for a specific pose. Additionally, scene segmentation and a placepredictor provide fast coarse initial pose estimation. Evaluations on publiclarge-scale datasets demonstrate that MatLoc-NeRF achieves superior efficiencyand accuracy compared to existing NeRF-based localization methods.</description><author>Huaiji Zhou, Bing Wang, Changhao Chen</author><pubDate>Mon, 17 Jun 2024 18:29:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11766v1</guid></item><item><title>Leveraging VLM-Based Pipelines to Annotate 3D Objects</title><link>http://arxiv.org/abs/2311.17851v2</link><description>Pretrained vision language models (VLMs) present an opportunity to captionunlabeled 3D objects at scale. The leading approach to summarize VLMdescriptions from different views of an object (Luo et al., 2023) relies on alanguage model (GPT4) to produce the final output. This text-based aggregationis susceptible to hallucinations as it merges potentially contradictorydescriptions. We propose an alternative algorithm to marginalize over factorssuch as the viewpoint that affect the VLM's response. Instead of mergingtext-only responses, we utilize the VLM's joint image-text likelihoods. We showour probabilistic aggregation is not only more reliable and efficient, but setsthe SoTA on inferring object types with respect to human-verified labels. Theaggregated annotations are also useful for conditional inference; they improvedownstream predictions (e.g., of object material) when the object's type isspecified as an auxiliary text-based input. Such auxiliary inputs allowablating the contribution of visual reasoning over visionless reasoning in anunsupervised setting. With these supervised and unsupervised evaluations, weshow how a VLM-based pipeline can be leveraged to produce reliable annotationsfor 764K objects from the Objaverse dataset.</description><author>Rishabh Kabra, Loic Matthey, Alexander Lerchner, Niloy J. Mitra</author><pubDate>Mon, 17 Jun 2024 18:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17851v2</guid></item><item><title>Ultrasound Imaging based on the Variance of a Diffusion Restoration Model</title><link>http://arxiv.org/abs/2403.15316v2</link><description>Despite today's prevalence of ultrasound imaging in medicine, ultrasoundsignal-to-noise ratio is still affected by several sources of noise andartefacts. Moreover, enhancing ultrasound image quality involves balancingconcurrent factors like contrast, resolution, and speckle preservation.Recently, there has been progress in both model-based and learning-basedapproaches addressing the problem of ultrasound image reconstruction. Bringingthe best from both worlds, we propose a hybrid reconstruction method combiningan ultrasound linear direct model with a learning-based prior coming from agenerative Denoising Diffusion model. More specifically, we rely on theunsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model(DDRM). Given the nature of multiplicative noise inherent to ultrasound, thispaper proposes an empirical model to characterize the stochasticity ofdiffusion reconstruction of ultrasound images, and shows the interest of itsvariance as an echogenicity map estimator. We conduct experiments on synthetic,in-vitro, and in-vivo data, demonstrating the efficacy of our variance imagingapproach in achieving high-quality image reconstructions from single plane-waveacquisitions and in comparison to state-of-the-art methods. The code isavailable at: https://github.com/Yuxin-Zhang-Jasmine/DRUSvar</description><author>Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus</author><pubDate>Mon, 17 Jun 2024 18:25:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15316v2</guid></item><item><title>Joint Linked Component Analysis for Multiview Data</title><link>http://arxiv.org/abs/2406.11761v1</link><description>In this work, we propose the joint linked component analysis (joint\_LCA) formultiview data. Unlike classic methods which extract the shared components in asequential manner, the objective of joint\_LCA is to identify the view-specificloading matrices and the rank of the common latent subspace simultaneously. Weformulate a matrix decomposition model where a joint structure and anindividual structure are present in each data view, which enables us to arriveat a clean svd representation for the cross covariance between any pair of dataviews. An objective function with a novel penalty term is then proposed toachieve simultaneous estimation and rank selection. In addition, a refittingprocedure is employed as a remedy to reduce the shrinkage bias caused by thepenalization.</description><author>Lin Xiao, Luo Xiao</author><pubDate>Mon, 17 Jun 2024 18:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11761v1</guid></item><item><title>Quantifying Local Model Validity using Active Learning</title><link>http://arxiv.org/abs/2406.07474v2</link><description>Real-world applications of machine learning models are often subject to legalor policy-based regulations. Some of these regulations require ensuring thevalidity of the model, i.e., the approximation error being smaller than athreshold. A global metric is generally too insensitive to determine thevalidity of a specific prediction, whereas evaluating local validity is costlysince it requires gathering additional data.We propose learning the model errorto acquire a local validity estimate while reducing the amount of required datathrough active learning. Using model validation benchmarks, we provideempirical evidence that the proposed method can lead to an error model withsufficient discriminative properties using a relatively small amount of data.Furthermore, an increased sensitivity to local changes of the validity boundscompared to alternative approaches is demonstrated.</description><author>Sven Lämmle, Can Bogoclu, Robert Voßhall, Anselm Haselhoff, Dirk Roos</author><pubDate>Mon, 17 Jun 2024 18:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07474v2</guid></item><item><title>STAR: SocioTechnical Approach to Red Teaming Language Models</title><link>http://arxiv.org/abs/2406.11757v1</link><description>This research introduces STAR, a sociotechnical framework that improves oncurrent best practices for red teaming safety of large language models. STARmakes two key contributions: it enhances steerability by generatingparameterised instructions for human red teamers, leading to improved coverageof the risk surface. Parameterised instructions also provide more detailedinsights into model failures at no increased cost. Second, STAR improves signalquality by matching demographics to assess harms for specific groups, resultingin more sensitive annotations. STAR further employs a novel step of arbitrationto leverage diverse viewpoints and improve label reliability, treatingdisagreement not as noise but as a valuable contribution to signal quality.</description><author>Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac</author><pubDate>Mon, 17 Jun 2024 18:16:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11757v1</guid></item><item><title>DustNet: skillful neural network predictions of Saharan dust</title><link>http://arxiv.org/abs/2406.11754v1</link><description>Suspended in the atmosphere are millions of tonnes of mineral dust whichinteracts with weather and climate. Accurate representation of mineral dust inweather models is vital, yet remains challenging. Large scale weather modelsuse high power supercomputers and take hours to complete the forecast. Suchcomputational burden allows them to only include monthly climatological meansof mineral dust as input states inhibiting their forecasting accuracy. Here, weintroduce DustNet a simple, accurate and super fast forecasting model for24-hours ahead predictions of aerosol optical depth AOD. DustNet trains in lessthan 8 minutes and creates predictions in 2 seconds on a desktop computer.Created by DustNet predictions outperform the state-of-the-art physics-basedmodel on coarse 1 x 1 degree resolution at 95% of grid locations when comparedto ground truth satellite data. Our results show DustNet has a potential forfast and accurate AOD forecasting which could transform our understanding ofdust impacts on weather patterns.</description><author>Trish E. Nowak, Andy T. Augousti, Benno I. Simmons, Stefan Siegert</author><pubDate>Mon, 17 Jun 2024 18:15:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11754v1</guid></item><item><title>A Semantic-based Layer Freezing Approach to Efficient Fine-Tuning of Language Models</title><link>http://arxiv.org/abs/2406.11753v1</link><description>Finetuning language models (LMs) is crucial for adapting the models todownstream data and tasks. However, full finetuning is usually costly. Existingwork, such as parameter-efficient finetuning (PEFT), often focuses on\textit{how to finetune} but neglects the issue of \textit{where to finetune}.As a pioneering work on answering where to finetune (at the layer level), weconduct a semantic analysis of the LM inference process. We first propose avirtual transition of the latent representation and then trace its factualtransition. Based on the deviation in transitions, we estimate the gain offinetuning each model layer, and further, narrow down the scope for finetuning.We perform extensive experiments across well-known LMs and datasets. Theresults show that our approach is effective and efficient, and outperforms theexisting baselines. Our approach is orthogonal to existing efficienttechniques, such as PEFT methods, offering practical values on LM finetuning.</description><author>Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang</author><pubDate>Mon, 17 Jun 2024 18:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11753v1</guid></item><item><title>Multi-Layer Ranking with Large Language Models for News Source Recommendation</title><link>http://arxiv.org/abs/2406.11745v1</link><description>To seek reliable information sources for news events, we introduce a noveltask of expert recommendation, which aims to identify trustworthy sources basedon their previously quoted statements. To achieve this, we built a noveldataset, called NewsQuote, consisting of 23,571 quote-speaker pairs sourcedfrom a collection of news articles. We formulate the recommendation task as theretrieval of experts based on their likelihood of being associated with a givenquery. We also propose a multi-layer ranking framework employing Large LanguageModels to improve the recommendation performance. Our results show thatemploying an in-context learning based LLM ranker and a multi-layerranking-based filter significantly improve both the predictive quality andbehavioural quality of the recommender system.</description><author>Wenjia Zhang, Lin Gui, Rob Procter, Yulan He</author><pubDate>Mon, 17 Jun 2024 18:02:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11745v1</guid></item><item><title>Domain Generalization for In-Orbit 6D Pose Estimation</title><link>http://arxiv.org/abs/2406.11743v1</link><description>We address the problem of estimating the relative 6D pose, i.e., position andorientation, of a target spacecraft, from a monocular image, a key capabilityfor future autonomous Rendezvous and Proximity Operations. Due to thedifficulty of acquiring large sets of real images, spacecraft pose estimationnetworks are exclusively trained on synthetic ones. However, because thoseimages do not capture the illumination conditions encountered in orbit, poseestimation networks face a domain gap problem, i.e., they do not generalize toreal images. Our work introduces a method that bridges this domain gap. Itrelies on a novel, end-to-end, neural-based architecture as well as a novellearning strategy. This strategy improves the domain generalization abilitiesof the network through multi-task learning and aggressive data augmentationpolicies, thereby enforcing the network to learn domain-invariant features. Wedemonstrate that our method effectively closes the domain gap, achievingstate-of-the-art accuracy on the widespread SPEED+ dataset. Finally, ablationstudies assess the impact of key components of our method on its generalizationabilities.</description><author>Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer</author><pubDate>Mon, 17 Jun 2024 18:01:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11743v1</guid></item><item><title>Transcendence: Generative Models Can Outperform The Experts That Train Them</title><link>http://arxiv.org/abs/2406.11741v1</link><description>Generative models are trained with the simple objective of imitating theconditional probability distribution induced by the data they are trained on.Therefore, when trained on data generated by humans, we may not expect theartificial model to outperform the humans on their original objectives. In thiswork, we study the phenomenon of transcendence: when a generative modelachieves capabilities that surpass the abilities of the experts generating itsdata. We demonstrate transcendence by training an autoregressive transformer toplay chess from game transcripts, and show that the trained model can sometimesachieve better performance than all players in the dataset. We theoreticallyprove that transcendence is enabled by low-temperature sampling, and rigorouslyassess this experimentally. Finally, we discuss other sources of transcendence,laying the groundwork for future investigation of this phenomenon in a broadersetting.</description><author>Edwin Zhang, Vincent Zhu, Naomi Saphra, Anat Kleiman, Benjamin L. Edelman, Milind Tambe, Sham M. Kakade, Eran Malach</author><pubDate>Mon, 17 Jun 2024 18:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11741v1</guid></item><item><title>Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies</title><link>http://arxiv.org/abs/2406.11740v1</link><description>Humans can imagine goal states during planning and perform actions to matchthose goals. In this work, we propose Imagination Policy, a novel multi-taskkey-frame policy network for solving high-precision pick and place tasks.Instead of learning actions directly, Imagination Policy generates point cloudsto imagine desired states which are then translated to actions using rigidaction estimation. This transforms action inference into a local generativetask. We leverage pick and place symmetries underlying the tasks in thegeneration process and achieve extremely high sample efficiency andgeneralizability to unseen configurations. Finally, we demonstratestate-of-the-art performance across various tasks on the RLbench benchmarkcompared with several strong baselines.</description><author>Haojie Huang, Karl Schmeckpeper, Dian Wang, Ondrej Biza, Yaoyao Qian, Haotian Liu, Mingxi Jia, Robert Platt, Robin Walters</author><pubDate>Mon, 17 Jun 2024 18:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11740v1</guid></item><item><title>V3Det Challenge 2024 on Vast Vocabulary and Open Vocabulary Object Detection: Methods and Results</title><link>http://arxiv.org/abs/2406.11739v1</link><description>Detecting objects in real-world scenes is a complex task due to variouschallenges, including the vast range of object categories, and potentialencounters with previously unknown or unseen objects. The challengesnecessitate the development of public benchmarks and challenges to advance thefield of object detection. Inspired by the success of previous COCO and LVISChallenges, we organize the V3Det Challenge 2024 in conjunction with the 4thOpen World Vision Workshop: Visual Perception via Learning in an Open World(VPLOW) at CVPR 2024, Seattle, US. This challenge aims to push the boundariesof object detection research and encourage innovation in this field. The V3DetChallenge 2024 consists of two tracks: 1) Vast Vocabulary Object Detection:This track focuses on detecting objects from a large set of 13204 categories,testing the detection algorithm's ability to recognize and locate diverseobjects. 2) Open Vocabulary Object Detection: This track goes a step further,requiring algorithms to detect objects from an open set of categories,including unknown objects. In the following sections, we will provide acomprehensive summary and analysis of the solutions submitted by participants.By analyzing the methods and solutions presented, we aim to inspire futureresearch directions in vast vocabulary and open-vocabulary object detection,driving progress in this field. Challenge homepage:https://v3det.openxlab.org.cn/challenge</description><author>Jiaqi Wang, Yuhang Zang, Pan Zhang, Tao Chu, Yuhang Cao, Zeyi Sun, Ziyu Liu, Xiaoyi Dong, Tong Wu, Dahua Lin, Zeming Chen, Zhi Wang, Lingchen Meng, Wenhao Yao, Jianwei Yang, Sihong Wu, Zhineng Chen, Zuxuan Wu, Yu-Gang Jiang, Peixi Wu, Bosong Chai, Xuan Nie, Longquan Yan, Zeyu Wang, Qifan Zhou, Boning Wang, Jiaqi Huang, Zunnan Xu, Xiu Li, Kehong Yuan, Yanyan Zu, Jiayao Ha, Qiong Gao, Licheng Jiao</author><pubDate>Mon, 17 Jun 2024 17:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11739v1</guid></item><item><title>Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions</title><link>http://arxiv.org/abs/2406.09264v2</link><description>Recent advancements in general-purpose AI have highlighted the importance ofguiding AI systems towards the intended goals, ethical principles, and valuesof individuals and groups, a concept broadly recognized as alignment. However,the lack of clarified definitions and scopes of human-AI alignment poses asignificant obstacle, hampering collaborative efforts across research domainsto achieve this alignment. In particular, ML- and philosophy-oriented alignmentresearch often views AI alignment as a static, unidirectional process (i.e.,aiming to ensure that AI systems' objectives match humans) rather than anongoing, mutual alignment problem [429]. This perspective largely neglects thelong-term interaction and dynamic changes of alignment. To understand thesegaps, we introduce a systematic review of over 400 papers published between2019 and January 2024, spanning multiple domains such as Human-ComputerInteraction (HCI), Natural Language Processing (NLP), Machine Learning (ML),and others. We characterize, define and scope human-AI alignment. From this, wepresent a conceptual framework of "Bidirectional Human-AI Alignment" toorganize the literature from a human-centered perspective. This frameworkencompasses both 1) conventional studies of aligning AI to humans that ensuresAI produces the intended outcomes determined by humans, and 2) a proposedconcept of aligning humans to AI, which aims to help individuals and societyadjust to AI advancements both cognitively and behaviorally. Additionally, wearticulate the key findings derived from literature analysis, includingdiscussions about human values, interaction techniques, and evaluations. Topave the way for future studies, we envision three key challenges for futuredirections and propose examples of potential future solutions.</description><author>Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li Qiwei, Sushrita Rakshit, Chenglei Si, Yutong Xie, Jeffrey P. Bigham, Frank Bentley, Joyce Chai, Zachary Lipton, Qiaozhu Mei, Rada Mihalcea, Michael Terry, Diyi Yang, Meredith Ringel Morris, Paul Resnick, David Jurgens</author><pubDate>Mon, 17 Jun 2024 17:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09264v2</guid></item><item><title>InterNeRF: Scaling Radiance Fields via Parameter Interpolation</title><link>http://arxiv.org/abs/2406.11737v1</link><description>Neural Radiance Fields (NeRFs) have unmatched fidelity on large, real-worldscenes. A common approach for scaling NeRFs is to partition the scene intoregions, each of which is assigned its own parameters. When implementednaively, such an approach is limited by poor test-time scaling and inconsistentappearance and geometry. We instead propose InterNeRF, a novel architecture forrendering a target view using a subset of the model's parameters. Our approachenables out-of-core training and rendering, increasing total model capacitywith only a modest increase to training time. We demonstrate significantimprovements in multi-room scenes while remaining competitive on standardbenchmarks.</description><author>Clinton Wang, Peter Hedman, Polina Golland, Jonathan T. Barron, Daniel Duckworth</author><pubDate>Mon, 17 Jun 2024 17:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11737v1</guid></item><item><title>MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance</title><link>http://arxiv.org/abs/2401.02906v3</link><description>The deployment of multimodal large language models (MLLMs) has brought fortha unique vulnerability: susceptibility to malicious attacks through visualinputs. This paper investigates the novel challenge of defending MLLMs againstsuch attacks. Compared to large language models (LLMs), MLLMs include anadditional image modality. We discover that images act as a ``foreign language"that is not considered during safety alignment, making MLLMs more prone toproducing harmful responses. Unfortunately, unlike the discrete tokensconsidered in text-based LLMs, the continuous nature of image signals presentssignificant alignment challenges, which poses difficulty to thoroughly coverall possible scenarios. This vulnerability is exacerbated by the fact that moststate-of-the-art MLLMs are fine-tuned on limited image-text pairs that are muchfewer than the extensive text-based pretraining corpus, which makes the MLLMsmore prone to catastrophic forgetting of their original abilities during safetyfine-tuning. To tackle these challenges, we introduce MLLM-Protector, aplug-and-play strategy that solves two subtasks: 1) identifying harmfulresponses via a lightweight harm detector, and 2) transforming harmfulresponses into harmless ones via a detoxifier. This approach effectivelymitigates the risks posed by malicious visual inputs without compromising theoriginal performance of MLLMs. Our results demonstrate that MLLM-Protectoroffers a robust solution to a previously unaddressed aspect of MLLM security.</description><author>Renjie Pi, Tianyang Han, Jianshu Zhang, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang</author><pubDate>Mon, 17 Jun 2024 17:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02906v3</guid></item><item><title>Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models</title><link>http://arxiv.org/abs/2406.11736v1</link><description>One of the primary driving forces contributing to the superior performance ofLarge Language Models (LLMs) is the extensive availability of human-annotatednatural language data, which is used for alignment fine-tuning. This inspiredresearchers to investigate self-training methods to mitigate the extensivereliance on human annotations. However, the current success of self-traininghas been primarily observed in natural language scenarios, rather than in theincreasingly important neural-symbolic scenarios. To this end, we propose anenvironment-guided neural-symbolic self-training framework named ENVISIONS. Itaims to overcome two main challenges: (1) the scarcity of symbolic data, and(2) the limited proficiency of LLMs in processing symbolic language. Extensiveevaluations conducted on three distinct domains demonstrate the effectivenessof our approach. Additionally, we have conducted a comprehensive analysis touncover the factors contributing to ENVISIONS's success, thereby offeringvaluable insights for future research in this area. Code will be available at\url{https://github.com/xufangzhi/ENVISIONS}.</description><author>Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, Zhiyong Wu</author><pubDate>Mon, 17 Jun 2024 17:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11736v1</guid></item><item><title>TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes</title><link>http://arxiv.org/abs/2405.20283v2</link><description>We present TetSphere splatting, an explicit, Lagrangian representation forreconstructing 3D shapes with high-quality geometry. In contrast toconventional object reconstruction methods which predominantly use Eulerianrepresentations, including both neural implicit (e.g., NeRF, NeuS) and explicitrepresentations (e.g., DMTet), and often struggle with high computationaldemands and suboptimal mesh quality, TetSphere splatting utilizes an underusedbut highly effective geometric primitive -- tetrahedral meshes. This approachdirectly yields superior mesh quality without relying on neural networks orpost-processing. It deforms multiple initial tetrahedral spheres to accuratelyreconstruct the 3D shape through a combination of differentiable rendering andgeometric energy optimization, resulting in significant computationalefficiency. Serving as a robust and versatile geometry representation,Tet-Sphere splatting seamlessly integrates into diverse applications, includingsingle-view 3D reconstruction, image-/text-to-3D content generation.Experimental results demonstrate that TetSphere splatting outperforms existingrepresentations, delivering faster optimization speed, enhanced mesh quality,and reliable preservation of thin structures.</description><author>Minghao Guo, Bohan Wang, Kaiming He, Wojciech Matusik</author><pubDate>Mon, 17 Jun 2024 17:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20283v2</guid></item><item><title>A Clipped Trip: the Dynamics of SGD with Gradient Clipping in High-Dimensions</title><link>http://arxiv.org/abs/2406.11733v1</link><description>The success of modern machine learning is due in part to the adaptiveoptimization methods that have been developed to deal with the difficulties oftraining large models over complex datasets. One such method is gradientclipping: a practical procedure with limited theoretical underpinnings. In thiswork, we study clipping in a least squares problem under streaming SGD. Wedevelop a theoretical analysis of the learning dynamics in the limit of largeintrinsic dimension-a model and dataset dependent notion of dimensionality. Inthis limit we find a deterministic equation that describes the evolution of theloss. We show that with Gaussian noise clipping cannot improve SGD performance.Yet, in other noisy settings, clipping can provide benefits with tuning of theclipping threshold. In these cases, clipping biases updates in a way beneficialto training which cannot be recovered by SGD under any schedule. We concludewith a discussion about the links between high-dimensional clipping and neuralnetwork training.</description><author>Noah Marshall, Ke Liang Xiao, Atish Agarwala, Elliot Paquette</author><pubDate>Mon, 17 Jun 2024 17:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11733v1</guid></item><item><title>Correspondence Free Multivector Cloud Registration using Conformal Geometric Algebra</title><link>http://arxiv.org/abs/2406.11732v1</link><description>We present, for the first time, a novel theoretical approach to address theproblem of correspondence free multivector cloud registration in conformalgeometric algebra. Such formalism achieves several favorable properties.Primarily, it forms an orthogonal automorphism that extends beyond the typicalvector space to the entire conformal geometric algebra while respecting themultivector grading. Concretely, the registration can be viewed as anorthogonal transformation (\it i.e., scale, translation, rotation) belonging to$SO(4,1)$ - group of special orthogonal transformations in conformal geometricalgebra. We will show that such formalism is able to: $(i)$ perform theregistration without directly accessing the input multivectors. Instead, we useprimitives or geometric objects provided by the conformal model - themultivectors, $(ii)$ the geometric objects are obtained by solving amultilinear eigenvalue problem to find sets of eigenmultivectors. In this way,we can explicitly avoid solving the correspondences in the registrationprocess. Most importantly, this offers rotation and translation equivariantproperties between the input multivectors and the eigenmultivectors.Experimental evaluation is conducted in datasets commonly used in point cloudregistration, to testify the usefulness of the approach with emphasis toambiguities arising from high levels of noise. The code is available athttps://github.com/Numerical-Geometric-Algebra/RegistrationGA . This work wassubmitted to the International Journal of Computer Vision and is currentlyunder review.</description><author>Francisco Xavier Vasconcelos, Jacinto C. Nascimento</author><pubDate>Mon, 17 Jun 2024 17:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11732v1</guid></item><item><title>CHG Shapley: Efficient Data Valuation and Selection towards Trustworthy Machine Learning</title><link>http://arxiv.org/abs/2406.11730v1</link><description>Understanding the decision-making process of machine learning models iscrucial for ensuring trustworthy machine learning. Data Shapley, a landmarkstudy on data valuation, has significantly advanced this understanding byassessing the contribution of each datum to model accuracy. However, theresource-intensive and time-consuming nature of multiple model retraining posessignificant challenges for applying Data Shapley to large datasets. To addressthis, we propose the CHG (Conduct of Hardness and Gradient) score, whichapproximates the utility of each data subset on model accuracy during a singlemodel training. By deriving the closed-form expression of the Shapley value foreach data point under the CHG score utility function, we reduce thecomputational complexity to the equivalent of a single model retraining, anexponential improvement over existing methods. Additionally, we employ CHGShapley for real-time data selection, demonstrating its effectiveness inidentifying high-value and noisy data. CHG Shapley facilitates trustworthymodel training through efficient data valuation, introducing a noveldata-centric perspective on trustworthy machine learning.</description><author>Huaiguang Cai</author><pubDate>Mon, 17 Jun 2024 17:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11730v1</guid></item><item><title>1000 African Voices: Advancing inclusive multi-speaker multi-accent speech synthesis</title><link>http://arxiv.org/abs/2406.11727v1</link><description>Recent advances in speech synthesis have enabled many useful applicationslike audio directions in Google Maps, screen readers, and automated contentgeneration on platforms like TikTok. However, these systems are mostlydominated by voices sourced from data-rich geographies with personasrepresentative of their source data. Although 3000 of the world's languages aredomiciled in Africa, African voices and personas are under-represented in thesesystems. As speech synthesis becomes increasingly democratized, it is desirableto increase the representation of African English accents. We present Afro-TTS,the first pan-African accented English speech synthesis system able to generatespeech in 86 African accents, with 1000 personas representing the richphonological diversity across the continent for downstream application inEducation, Public Health, and Automated Content Creation. Speaker interpolationretains naturalness and accentedness, enabling the creation of new voices.</description><author>Sewade Ogun, Abraham T. Owodunni, Tobi Olatunji, Eniola Alese, Babatunde Oladimeji, Tejumade Afonja, Kayode Olaleye, Naome A. Etori, Tosin Adewumi</author><pubDate>Mon, 17 Jun 2024 17:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11727v1</guid></item><item><title>Social Environment Design</title><link>http://arxiv.org/abs/2402.14090v3</link><description>Artificial Intelligence (AI) holds promise as a technology that can be usedto improve government and economic policy-making. This paper proposes a newresearch agenda towards this end by introducing Social Environment Design, ageneral framework for the use of AI for automated policy-making that connectswith the Reinforcement Learning, EconCS, and Computational Social Choicecommunities. The framework seeks to capture general economic environments,includes voting on policy objectives, and gives a direction for the systematicanalysis of government and economic policy through AI simulation. We highlightkey open problems for future research in AI-based policy-making. By solvingthese challenges, we hope to achieve various social welfare objectives, therebypromoting more ethical and responsible decision making.</description><author>Edwin Zhang, Sadie Zhao, Tonghan Wang, Safwan Hossain, Henry Gasztowtt, Stephan Zheng, David C. Parkes, Milind Tambe, Yiling Chen</author><pubDate>Mon, 17 Jun 2024 17:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14090v3</guid></item><item><title>TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks</title><link>http://arxiv.org/abs/2403.09207v2</link><description>In this paper, we explore the capabilities of LLMs in capturinglexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b modeland test it on multiple lexical semantic tasks. As the outcome of ourexperiments, we present TaxoLLaMA, the everything-in-one model, lightweight dueto 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 resultsout of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, TaxonomyConstruction, and Lexical Entailment tasks. Moreover, it demonstrates verystrong zero-shot performance on Lexical Entailment and Taxonomy Constructionwith no fine-tuning. We also explore its hidden multilingual and domainadaptation capabilities with a little tuning or few-shot learning. Alldatasets, code, and model are available online athttps://github.com/VityaVitalich/TaxoLLaMA</description><author>Viktor Moskvoretskii, Ekaterina Neminova, Alina Lobanova, Alexander Panchenko, Irina Nikishina</author><pubDate>Mon, 17 Jun 2024 17:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09207v2</guid></item><item><title>Novel Fundus Image Preprocessing for Retcam Images to Improve Deep Learning Classification of Retinopathy of Prematurity</title><link>http://arxiv.org/abs/2302.02524v5</link><description>Retinopathy of Prematurity (ROP) is a potentially blinding eye disorderbecause of damage to the eye's retina which can affect babies born prematurely.Screening of ROP is essential for early detection and treatment. This is alaborious and manual process which requires trained physician performingdilated ophthalmological examination which can be subjective resulting in lowerdiagnosis success for clinically significant disease. Automated diagnosticmethods can assist ophthalmologists increase diagnosis accuracy using deeplearning. Several research groups have highlighted various approaches. CapturedROP Retcam images suffer from poor quality. This paper proposes the use ofimproved novel fundus preprocessing methods using pretrained transfer learningframeworks to create hybrid models to give higher diagnosis accuracy. Oncetrained and validated, the evaluations showed that these novel methods incomparison to traditional imaging processing contribute to better and in manyaspects higher accuracy in classifying Plus disease, Stages of ROP and Zones incomparison to peer papers.</description><author>Sajid Rahim, Kourosh Sabri, Anna Ells, Alan Wassyng, Mark Lawford, Linyang Chu, Wenbo He</author><pubDate>Mon, 17 Jun 2024 17:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02524v5</guid></item><item><title>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</title><link>http://arxiv.org/abs/2406.07476v2</link><description>In this paper, we present the VideoLLaMA 2, a set of Video Large LanguageModels (Video-LLMs) designed to enhance spatial-temporal modeling and audiounderstanding in video and audio-oriented tasks. Building upon its predecessor,VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC)connector, which effectively captures the intricate spatial and temporaldynamics of video data. Additionally, we integrate an Audio Branch into themodel through joint training, thereby enriching the multimodal understandingcapabilities of the model by seamlessly incorporating audio cues. Comprehensiveevaluations on multiple-choice video question answering (MC-VQA), open-endedvideo question answering (OE-VQA), and video captioning (VC) tasks demonstratethat VideoLLaMA 2 consistently achieves competitive results among open-sourcemodels and even gets close to some proprietary models on several benchmarks.Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only andaudio-video question-answering (AQA &amp; OE-AVQA) benchmarks over existing models.These advancements underline VideoLLaMA 2's superior performance in multimodalcomprehension, setting a new standard for intelligent video analysis systems.All models are public to facilitate further research.</description><author>Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing</author><pubDate>Mon, 17 Jun 2024 17:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07476v2</guid></item><item><title>Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity</title><link>http://arxiv.org/abs/2406.11721v1</link><description>Understanding alignment techniques begins with comprehending zero-shotgeneralization brought by instruction tuning, but little of the mechanism hasbeen understood. Existing work has largely been confined to the task level,without considering that tasks are artificially defined and, to LLMs, merelyconsist of tokens and representations. This line of research has been limitedto examining transfer between tasks from a task-pair perspective, with fewstudies focusing on understanding zero-shot generalization from the perspectiveof the data itself. To bridge this gap, we first demonstrate through multiplemetrics that zero-shot generalization during instruction tuning happens veryearly. Next, we investigate the facilitation of zero-shot generalization fromboth data similarity and granularity perspectives, confirming that encounteringhighly similar and fine-grained training data earlier during instructiontuning, without the constraints of defined "tasks", enables bettergeneralization. Finally, we propose a more grounded training data arrangementmethod, Test-centric Multi-turn Arrangement, and show its effectiveness inpromoting continual learning and further loss reduction. For the first time, weshow that zero-shot generalization during instruction tuning is a form ofsimilarity-based generalization between training and test data at the instancelevel. We hope our analysis will advance the understanding of zero-shotgeneralization during instruction tuning and contribute to the development ofmore aligned LLMs. Our code is released athttps://github.com/HBX-hbx/dynamics_of_zero-shot_generalization.</description><author>Bingxiang He, Ning Ding, Cheng Qian, Jia Deng, Ganqu Cui, Lifan Yuan, Huan-ang Gao, Huimin Chen, Zhiyuan Liu, Maosong Sun</author><pubDate>Mon, 17 Jun 2024 17:40:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11721v1</guid></item><item><title>Reward Machines for Deep RL in Noisy and Uncertain Environments</title><link>http://arxiv.org/abs/2406.00120v2</link><description>Reward Machines provide an automata-inspired structure for specifyinginstructions, safety constraints, and other temporally extended reward-worthybehaviour. By exposing complex reward function structure, they enablecounterfactual learning updates that have resulted in impressive sampleefficiency gains. While Reward Machines have been employed in both tabular anddeep RL settings, they have typically relied on a ground-truth interpretationof the domain-specific vocabulary that form the building blocks of the rewardfunction. Such ground-truth interpretations can be elusive in many real-worldsettings, due in part to partial observability or noisy sensing. In this paper,we explore the use of Reward Machines for Deep RL in noisy and uncertainenvironments. We characterize this problem as a POMDP and propose a suite of RLalgorithms that leverage task structure under uncertain interpretation ofdomain-specific vocabulary. Theoretical analysis exposes pitfalls in naiveapproaches to this problem, while experimental results show that our algorithmssuccessfully leverage task structure to improve performance under noisyinterpretations of the vocabulary. Our results provide a general framework forexploiting Reward Machines in partially observable environments.</description><author>Andrew C. Li, Zizhao Chen, Toryn Q. Klassen, Pashootan Vaezipoor, Rodrigo Toro Icarte, Sheila A. McIlraith</author><pubDate>Mon, 17 Jun 2024 17:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00120v2</guid></item><item><title>AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation</title><link>http://arxiv.org/abs/2402.10646v2</link><description>Abstraction ability is crucial in human intelligence, which can also benefitvarious tasks in NLP study. Existing work shows that LLMs are deficient inabstract ability, and how to improve it remains unexplored. In this work, wedesign the framework AbsInstruct to enhance LLMs' abstraction ability throughinstruction tuning. The framework builds instructions with in-depthexplanations to assist LLMs in capturing the underlying rationale ofabstraction. Meanwhile, we introduce a plausibility estimator to selectinstructions that are more consistent with the abstraction knowledge of LLMs tobe aligned. Then, our framework combines abstraction instructions withgeneral-purpose ones to build a hybrid dataset. Extensive experiments andanalyses demonstrate that our framework can considerably enhance LLMs'abstraction ability with strong generalization performance while maintainingtheir general instruction-following abilities.</description><author>Zhaowei Wang, Wei Fan, Qing Zong, Hongming Zhang, Sehyun Choi, Tianqing Fang, Xin Liu, Yangqiu Song, Ginny Y. Wong, Simon See</author><pubDate>Mon, 17 Jun 2024 17:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10646v2</guid></item><item><title>Refusal in Language Models Is Mediated by a Single Direction</title><link>http://arxiv.org/abs/2406.11717v1</link><description>Conversational large language models are fine-tuned for bothinstruction-following and safety, resulting in models that obey benign requestsbut refuse harmful ones. While this refusal behavior is widespread across chatmodels, its underlying mechanisms remain poorly understood. In this work, weshow that refusal is mediated by a one-dimensional subspace, across 13 popularopen-source chat models up to 72B parameters in size. Specifically, for eachmodel, we find a single direction such that erasing this direction from themodel's residual stream activations prevents it from refusing harmfulinstructions, while adding this direction elicits refusal on even harmlessinstructions. Leveraging this insight, we propose a novel white-box jailbreakmethod that surgically disables refusal with minimal effect on othercapabilities. Finally, we mechanistically analyze how adversarial suffixessuppress propagation of the refusal-mediating direction. Our findingsunderscore the brittleness of current safety fine-tuning methods. More broadly,our work showcases how an understanding of model internals can be leveraged todevelop practical methods for controlling model behavior.</description><author>Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, Neel Nanda</author><pubDate>Mon, 17 Jun 2024 17:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11717v1</guid></item><item><title>CIMRL: Combining IMitation and Reinforcement Learning for Safe Autonomous Driving</title><link>http://arxiv.org/abs/2406.08878v2</link><description>Modern approaches to autonomous driving rely heavily on learned componentstrained with large amounts of human driving data via imitation learning.However, these methods require large amounts of expensive data collection andeven then face challenges with safely handling long-tail scenarios andcompounding errors over time. At the same time, pure Reinforcement Learning(RL) methods can fail to learn performant policies in sparse, constrained, andchallenging-to-define reward settings like driving. Both of these challengesmake deploying purely cloned policies in safety critical applications likeautonomous vehicles challenging. In this paper we propose Combining IMitationand Reinforcement Learning (CIMRL) approach - a framework that enables trainingdriving policies in simulation through leveraging imitative motion priors andsafety constraints. CIMRL does not require extensive reward specification andimproves on the closed loop behavior of pure cloning methods. By combining RLand imitation, we demonstrate that our method achieves state-of-the-art resultsin closed loop simulation driving benchmarks.</description><author>Jonathan Booher, Khashayar Rohanimanesh, Junhong Xu, Aleksandr Petiushko</author><pubDate>Mon, 17 Jun 2024 17:34:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08878v2</guid></item><item><title>Rethink Tree Traversal</title><link>http://arxiv.org/abs/2209.04825v5</link><description>We will show how to implement binary decision tree traversal in the languageof matrix computation. Our main contribution is to propose some equivalentalgorithms of binary tree traversal based on a novel matrix representation ofthe hierarchical structure of the decision tree. Our key idea is to travel thebinary decision tree by maximum inner product search. We not only implementdecision tree methods without the recursive traverse but also delve into thepartitioning nature of tree-based methods.</description><author>Jinxiong Zhang</author><pubDate>Mon, 17 Jun 2024 17:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.04825v5</guid></item><item><title>Measuring memorization in RLHF for code completion</title><link>http://arxiv.org/abs/2406.11715v1</link><description>Reinforcement learning with human feedback (RLHF) has become the dominantmethod to align large models to user preferences. Unlike fine-tuning, for whichthere are many studies regarding training data memorization, it is not clearhow memorization is affected by or introduced in the RLHF alignment process.Understanding this relationship is important as real user data may be collectedand used to align large models; if user data is memorized during RLHF and laterregurgitated, this could raise privacy concerns. In this work, we analyze howtraining data memorization can surface and propagate through each phase ofRLHF. We focus our study on code completion models, as code completion is oneof the most popular use cases for large language models. We find that RLHFsignificantly decreases the chance that data used for reward modeling andreinforcement learning is memorized, in comparison to aligning via directlyfine-tuning on this data, but that examples already memorized during thefine-tuning stage of RLHF, will, in the majority of cases, remain memorizedafter RLHF.</description><author>Aneesh Pappu, Billy Porter, Ilia Shumailov, Jamie Hayes</author><pubDate>Mon, 17 Jun 2024 17:33:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11715v1</guid></item><item><title>Scalable Expressiveness through Preprocessed Graph Perturbations</title><link>http://arxiv.org/abs/2406.11714v1</link><description>Graph Neural Networks (GNNs) have emerged as the predominant method foranalyzing graph-structured data. However, canonical GNNs have limitedexpressive power and generalization capability, thus triggering the developmentof more expressive yet computationally intensive methods. One such approach isto create a series of perturbed versions of input graphs and then repeatedlyconduct multiple message-passing operations on all variations during training.Despite their expressive power, this approach does not scale well on largergraphs. To address this scalability issue, we introduce Scalable Expressivenessthrough Preprocessed Graph Perturbation (SE2P). This model offers a flexible,configurable balance between scalability and generalizability with fourdistinct configuration classes. At one extreme, the configuration prioritizesscalability through minimal learnable feature extraction and extensivepreprocessing; at the other extreme, it enhances generalizability with morelearnable feature extractions, though this increases scalability costs. Weconduct extensive experiments on real-world datasets to evaluate thegeneralizability and scalability of SE2P variants compared to variousstate-of-the-art benchmarks. Our results indicate that, depending on the chosenSE2P configuration, the model can enhance generalizability compared tobenchmarks while achieving significant speed improvements of up to 8-fold.</description><author>Danial Saber, Amirali Salehi-Abari</author><pubDate>Mon, 17 Jun 2024 17:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11714v1</guid></item><item><title>Latent Denoising Diffusion GAN: Faster sampling, Higher image quality</title><link>http://arxiv.org/abs/2406.11713v1</link><description>Diffusion models are emerging as powerful solutions for generatinghigh-fidelity and diverse images, often surpassing GANs under manycircumstances. However, their slow inference speed hinders their potential forreal-time applications. To address this, DiffusionGAN leveraged a conditionalGAN to drastically reduce the denoising steps and speed up inference. Itsadvancement, Wavelet Diffusion, further accelerated the process by convertingdata into wavelet space, thus enhancing efficiency. Nonetheless, these modelsstill fall short of GANs in terms of speed and image quality. To bridge thesegaps, this paper introduces the Latent Denoising Diffusion GAN, which employspre-trained autoencoders to compress images into a compact latent space,significantly improving inference speed and image quality. Furthermore, wepropose a Weighted Learning strategy to enhance diversity and image quality.Experimental results on the CIFAR-10, CelebA-HQ, and LSUN-Church datasets provethat our model achieves state-of-the-art running speed among diffusion models.Compared to its predecessors, DiffusionGAN and Wavelet Diffusion, our modelshows remarkable improvements in all evaluation metrics. Code and pre-trainedcheckpoints: \url{https://github.com/thanhluantrinh/LDDGAN.git}</description><author>Luan Thanh Trinh, Tomoki Hamagami</author><pubDate>Mon, 17 Jun 2024 17:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11713v1</guid></item><item><title>OGNI-DC: Robust Depth Completion with Optimization-Guided Neural Iterations</title><link>http://arxiv.org/abs/2406.11711v1</link><description>Depth completion is the task of generating a dense depth map given an imageand a sparse depth map as inputs. It has important applications in variousdownstream tasks. In this paper, we present OGNI-DC, a novel framework fordepth completion. The key to our method is "Optimization-Guided NeuralIterations" (OGNI). It consists of a recurrent unit that refines a depthgradient field and a differentiable depth integrator that integrates the depthgradients into a depth map. OGNI-DC exhibits strong generalization,outperforming baselines by a large margin on unseen datasets and across varioussparsity levels. Moreover, OGNI-DC has high accuracy, achievingstate-of-the-art performance on the NYUv2 and the KITTI benchmarks. Code isavailable at https://github.com/princeton-vl/OGNI-DC.</description><author>Yiming Zuo, Jia Deng</author><pubDate>Mon, 17 Jun 2024 17:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11711v1</guid></item><item><title>Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging</title><link>http://arxiv.org/abs/2406.11709v1</link><description>Socratic questioning is an effective teaching strategy, encouraging criticalthinking and problem-solving. The conversational capabilities of large languagemodels (LLMs) show great potential for providing scalable, real-time studentguidance. However, current LLMs often give away solutions directly, making themineffective instructors. We tackle this issue in the code debugging domain withTreeInstruct, an Instructor agent guided by a novel state space-based planningalgorithm. TreeInstruct asks probing questions to help students independentlyidentify and resolve errors. It estimates a student's conceptual andsyntactical knowledge to dynamically construct a question tree based on theirresponses and current knowledge state, effectively addressing both independentand dependent mistakes concurrently in a multi-turn interaction setting. Inaddition to using an existing single-bug debugging benchmark, we construct amore challenging multi-bug dataset of 150 coding problems, incorrect solutions,and bug fixes -- all carefully constructed and annotated by experts. Extensiveevaluation shows TreeInstruct's state-of-the-art performance on both datasets,proving it to be a more effective instructor than baselines. Furthermore, areal-world case study with five students of varying skill levels furtherdemonstrates TreeInstruct's ability to guide students to debug their codeefficiently with minimal turns and highly Socratic questioning.</description><author>Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han</author><pubDate>Mon, 17 Jun 2024 17:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11709v1</guid></item><item><title>Topology-aware Federated Learning in Edge Computing: A Comprehensive Survey</title><link>http://arxiv.org/abs/2302.02573v2</link><description>The ultra-low latency requirements of 5G/6G applications and privacyconstraints call for distributed machine learning systems to be deployed at theedge. With its simple yet effective approach, federated learning (FL) is anatural solution for massive user-owned devices in edge computing withdistributed and private training data. FL methods based on FedAvg typicallyfollow a naive star topology, ignoring the heterogeneity and hierarchy of thevolatile edge computing architectures and topologies in reality. Several othernetwork topologies exist and can address the limitations and bottlenecks of thestar topology. This motivates us to survey network topology-related FLsolutions. In this paper, we conduct a comprehensive survey of the existing FLworks focusing on network topologies. After a brief overview of FL and edgecomputing networks, we discuss various edge network topologies as well as theiradvantages and disadvantages. Lastly, we discuss the remaining challenges andfuture works for applying FL to topology-specific edge networks.</description><author>Jiajun Wu, Steve Drew, Fan Dong, Zhuangdi Zhu, Jiayu Zhou</author><pubDate>Mon, 17 Jun 2024 17:27:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02573v2</guid></item><item><title>Tackling the Curse of Dimensionality in Fractional and Tempered Fractional PDEs with Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2406.11708v1</link><description>Fractional and tempered fractional partial differential equations (PDEs) areeffective models of long-range interactions, anomalous diffusion, and non-localeffects. Traditional numerical methods for these problems are mesh-based, thusstruggling with the curse of dimensionality (CoD). Physics-informed neuralnetworks (PINNs) offer a promising solution due to their universalapproximation, generalization ability, and mesh-free training. In principle,Monte Carlo fractional PINN (MC-fPINN) estimates fractional derivatives usingMonte Carlo methods and thus could lift CoD. However, this may causesignificant variance and errors, hence affecting convergence; in addition,MC-fPINN is sensitive to hyperparameters. In general, numerical methods andspecifically PINNs for tempered fractional PDEs are under-developed. Herein, weextend MC-fPINN to tempered fractional PDEs to address these issues, resultingin the Monte Carlo tempered fractional PINN (MC-tfPINN). To reduce possiblehigh variance and errors from Monte Carlo sampling, we replace theone-dimensional (1D) Monte Carlo with 1D Gaussian quadrature, applicable toboth MC-fPINN and MC-tfPINN. We validate our methods on various forward andinverse problems of fractional and tempered fractional PDEs, scaling up to100,000 dimensions. Our improved MC-fPINN/MC-tfPINN using quadratureconsistently outperforms the original versions in accuracy and convergencespeed in very high dimensions.</description><author>Zheyuan Hu, Kenji Kawaguchi, Zhongqiang Zhang, George Em Karniadakis</author><pubDate>Mon, 17 Jun 2024 17:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11708v1</guid></item><item><title>A First Physical-World Trajectory Prediction Attack via LiDAR-induced Deceptions in Autonomous Driving</title><link>http://arxiv.org/abs/2406.11707v1</link><description>Trajectory prediction forecasts nearby agents' moves based on theirhistorical trajectories. Accurate trajectory prediction is crucial forautonomous vehicles. Existing attacks compromise the prediction model of avictim AV by directly manipulating the historical trajectory of an attacker AV,which has limited real-world applicability. This paper, for the first time,explores an indirect attack approach that induces prediction errors via attacksagainst the perception module of a victim AV. Although it has been shown thatphysically realizable attacks against LiDAR-based perception are possible byplacing a few objects at strategic locations, it is still an open challenge tofind an object location from the vast search space in order to launch effectiveattacks against prediction under varying victim AV velocities. Through analysis, we observe that a prediction model is prone to an attackfocusing on a single point in the scene. Consequently, we propose a noveltwo-stage attack framework to realize the single-point attack. The first stageof prediction-side attack efficiently identifies, guided by the distribution ofdetection results under object-based attacks against perception, the stateperturbations for the prediction model that are effective andvelocity-insensitive. In the second stage of location matching, we match thefeasible object locations with the found state perturbations. Our evaluationusing a public autonomous driving dataset shows that our attack causes acollision rate of up to 63% and various hazardous responses of the victim AV.The effectiveness of our attack is also demonstrated on a real testbed car. Tothe best of our knowledge, this study is the first security analysis spanningfrom LiDAR-based perception to prediction in autonomous driving, leading to arealistic attack on prediction. To counteract the proposed attack, potentialdefenses are discussed.</description><author>Yang Lou, Yi Zhu, Qun Song, Rui Tan, Chunming Qiao, Wei-Bin Lee, Jianping Wang</author><pubDate>Mon, 17 Jun 2024 17:26:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11707v1</guid></item><item><title>Prompts as Auto-Optimized Training Hyperparameters: Training Best-in-Class IR Models from Scratch with 10 Gold Labels</title><link>http://arxiv.org/abs/2406.11706v1</link><description>We develop a method for training small-scale (under 100M parameter) neuralinformation retrieval models with as few as 10 gold relevance labels. Themethod depends on generating synthetic queries for documents using a languagemodel (LM), and the key step is that we automatically optimize the LM promptthat is used to generate these queries based on training quality. Inexperiments with the BIRCO benchmark, we find that models trained with ourmethod outperform RankZephyr and are competitive with RankLLama, both of whichare 7B parameter models trained on over 100K labels. These findings point tothe power of automatic prompt optimization for synthetic dataset generation.</description><author>Jasper Xian, Saron Samuel, Faraz Khoubsirat, Ronak Pradeep, Md Arafat Sultan, Radu Florian, Salim Roukos, Avirup Sil, Christopher Potts, Omar Khattab</author><pubDate>Mon, 17 Jun 2024 17:25:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11706v1</guid></item><item><title>Nemotron-4 340B Technical Report</title><link>http://arxiv.org/abs/2406.11704v1</link><description>We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base,Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are openaccess under the NVIDIA Open Model License Agreement, a permissive modellicense that allows distribution, modification, and use of the models and itsoutputs. These models perform competitively to open access models on a widerange of evaluation benchmarks, and were sized to fit on a single DGX H100 with8 GPUs when deployed in FP8 precision. We believe that the community canbenefit from these models in various research studies and commercialapplications, especially for generating synthetic data to train smallerlanguage models. Notably, over 98% of data used in our model alignment processis synthetically generated, showcasing the effectiveness of these models ingenerating synthetic data. To further support open research and facilitatemodel development, we are also open-sourcing the synthetic data generationpipeline used in our model alignment process.</description><author>Nvidia, :, Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupi</author><pubDate>Mon, 17 Jun 2024 17:25:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11704v1</guid></item><item><title>Multiple Descents in Unsupervised Learning: The Role of Noise, Domain Shift and Anomalies</title><link>http://arxiv.org/abs/2406.11703v1</link><description>The phenomenon of double descent has recently gained attention in supervisedlearning. It challenges the conventional wisdom of the bias-variance trade-offby showcasing a surprising behavior. As the complexity of the model increases,the test error initially decreases until reaching a certain point where themodel starts to overfit the train set, causing the test error to rise. However,deviating from classical theory, the error exhibits another decline whenexceeding a certain degree of over-parameterization. We study the presence ofdouble descent in unsupervised learning, an area that has received littleattention and is not yet fully understood. We conduct extensive experimentsusing under-complete auto-encoders (AEs) for various applications, such asdealing with noisy data, domain shifts, and anomalies. We use synthetic andreal data and identify model-wise, epoch-wise, and sample-wise double descentfor all the aforementioned applications. Finally, we assessed the usability ofthe AEs for detecting anomalies and mitigating the domain shift betweendatasets. Our findings indicate that over-parameterized models can improveperformance not only in terms of reconstruction, but also in enhancingcapabilities for the downstream task.</description><author>Kobi Rahimi, Tom Tirer, Ofir Lindenbaum</author><pubDate>Mon, 17 Jun 2024 17:24:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11703v1</guid></item><item><title>Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale</title><link>http://arxiv.org/abs/2403.08293v3</link><description>A syntactic language model (SLM) incrementally generates a sentence with itssyntactic tree in a left-to-right manner. We present Generative PretrainedStructured Transformers (GPST), an unsupervised SLM at scale capable of beingpre-trained from scratch on raw texts with high parallelism. GPST circumventsthe limitations of previous SLMs such as relying on gold trees and sequentialtraining. It consists of two components, a usual SLM supervised by auni-directional language modeling loss, and an additional composition model,which induces syntactic parse trees and computes constituent representations,supervised by a bi-directional language modeling loss. We propose arepresentation surrogate to enable joint parallel training of the two models ina hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billiontokens, and demonstrate the superiority of GPST over GPT-2 with a comparablesize in numerous tasks covering both language understanding and languagegeneration. Meanwhile, GPST also significantly outperforms existingunsupervised SLMs on left-to-right grammar induction, while holding asubstantial acceleration on training.</description><author>Xiang Hu, Pengyu Ji, Qingyang Zhu, Wei Wu, Kewei Tu</author><pubDate>Mon, 17 Jun 2024 17:22:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08293v3</guid></item><item><title>Meta Reasoning for Large Language Models</title><link>http://arxiv.org/abs/2406.11698v1</link><description>We introduce Meta-Reasoning Prompting (MRP), a novel and efficient systemprompting method for large language models (LLMs) inspired by humanmeta-reasoning. Traditional in-context learning-based reasoning techniques,such as Tree-of-Thoughts, show promise but lack consistent state-of-the-artperformance across diverse tasks due to their specialized nature. MRP addressesthis limitation by guiding LLMs to dynamically select and apply differentreasoning methods based on the specific requirements of each task, optimizingboth performance and computational efficiency. With MRP, LLM reasoning operatesin two phases. Initially, the LLM identifies the most appropriate reasoningmethod using task input cues and objective descriptions of available methods.Subsequently, it applies the chosen method to complete the task. This dynamicstrategy mirrors human meta-reasoning, allowing the model to excel in a widerange of problem domains. We evaluate the effectiveness of MRP throughcomprehensive benchmarks. The results demonstrate that MRP achieves orapproaches state-of-the-art performance across diverse tasks. MRP represents asignificant advancement in enabling LLMs to identify cognitive challengesacross problems and leverage benefits across different reasoning approaches,enhancing their ability to handle diverse and complex problem domainsefficiently. Every LLM deserves a Meta-Reasoning Prompting to unlock its fullpotential and ensure adaptability in an ever-evolving landscape of challengesand applications.</description><author>Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, Furu Wei</author><pubDate>Mon, 17 Jun 2024 17:14:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11698v1</guid></item><item><title>Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models</title><link>http://arxiv.org/abs/2406.10162v2</link><description>In reinforcement learning, specification gaming occurs when AI systems learnundesired behaviors that are highly rewarded due to misspecified traininggoals. Specification gaming can range from simple behaviors like sycophancy tosophisticated and pernicious behaviors like reward-tampering, where a modeldirectly modifies its own reward mechanism. However, these more perniciousbehaviors may be too complex to be discovered via exploration. In this paper,we study whether Large Language Model (LLM) assistants which find easilydiscovered forms of specification gaming will generalize to perform rarer andmore blatant forms, up to and including reward-tampering. We construct acurriculum of increasingly sophisticated gameable environments and find thattraining on early-curriculum environments leads to more specification gaming onremaining environments. Strikingly, a small but non-negligible proportion ofthe time, LLM assistants trained on the full curriculum generalize zero-shot todirectly rewriting their own reward function. Retraining an LLM not to gameearly-curriculum environments mitigates, but does not eliminate,reward-tampering in later environments. Moreover, adding harmlessness trainingto our gameable environments does not prevent reward-tampering. These resultsdemonstrate that LLMs can generalize from common forms of specification gamingto more pernicious reward tampering and that such behavior may be nontrivial toremove.</description><author>Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez, Evan Hubinger</author><pubDate>Mon, 17 Jun 2024 17:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10162v2</guid></item><item><title>Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs</title><link>http://arxiv.org/abs/2406.11695v1</link><description>Language Model Programs, i.e. sophisticated pipelines of modular languagemodel (LM) calls, are increasingly advancing NLP tasks, but they requirecrafting prompts that are jointly effective for all modules. We study promptoptimization for LM programs, i.e. how to update these prompts to maximize adownstream metric without access to module-level labels or gradients. To makethis tractable, we factorize our problem into optimizing the free-forminstructions and few-shot demonstrations of every module and introduce severalstrategies to craft task-grounded instructions and navigate credit assignmentacross modules. Our strategies include (i) program- and data-aware techniquesfor proposing effective instructions, (ii) a stochastic mini-batch evaluationfunction for learning a surrogate model of our objective, and (iii) ameta-optimization procedure in which we refine how LMs construct proposals overtime. Using these insights we develop MIPRO, a novel optimizer that outperformsbaselines on five of six diverse LM programs using a best-in-class open-sourcemodel (Llama-3-8B), by as high as 12.9% accuracy. We will release our newoptimizers and benchmark in DSPy at https://github.com/stanfordnlp/dspy</description><author>Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, Omar Khattab</author><pubDate>Mon, 17 Jun 2024 17:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11695v1</guid></item><item><title>Lightweight Model Pre-training via Language Guided Knowledge Distillation</title><link>http://arxiv.org/abs/2406.11689v1</link><description>This paper studies the problem of pre-training for small models, which isessential for many mobile devices. Current state-of-the-art methods on thisproblem transfer the representational knowledge of a large network (as aTeacher) into a smaller model (as a Student) using self-superviseddistillation, improving the performance of the small model on downstream tasks.However, existing approaches are insufficient in extracting the crucialknowledge that is useful for discerning categories in downstream tasks duringthe distillation process. In this paper, for the first time, we introducelanguage guidance to the distillation process and propose a new method namedLanguage-Guided Distillation (LGD) system, which uses category names of thetarget downstream task to help refine the knowledge transferred between theteacher and student. To this end, we utilize a pre-trained text encoder toextract semantic embeddings from language and construct a textual semanticspace called Textual Semantics Bank (TSB). Furthermore, we design aLanguage-Guided Knowledge Aggregation (LGKA) module to construct the visualsemantic space, also named Visual Semantics Bank (VSB). The task-relatedknowledge is transferred by driving a student encoder to mimic the similarityscore distribution inferred by a teacher over TSB and VSB. Compared with othersmall models obtained by either ImageNet pre-training or self-superviseddistillation, experiment results show that the distilled lightweight modelusing the proposed LGD method presents state-of-the-art performance and isvalidated on various downstream tasks, including classification, detection, andsegmentation. We have made the code available at https://github.com/mZhenz/LGD.</description><author>Mingsheng Li, Lin Zhang, Mingzhen Zhu, Zilong Huang, Gang Yu, Jiayuan Fan, Tao Chen</author><pubDate>Mon, 17 Jun 2024 17:07:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11689v1</guid></item></channel></rss>