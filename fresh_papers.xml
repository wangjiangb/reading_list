<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 28 Nov 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Video-Guided Foley Sound Generation with Multimodal Controls</title><link>http://arxiv.org/abs/2411.17698v1</link><description>Generating sound effects for videos often requires creating artistic soundeffects that diverge significantly from real-life sources and flexible controlin the sound design. To address this problem, we introduce MultiFoley, a modeldesigned for video-guided sound generation that supports multimodalconditioning through text, audio, and video. Given a silent video and a textprompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheelsspinning without wind noise) or more whimsical sounds (e.g., making a lion'sroar sound like a cat's meow). MultiFoley also allows users to choose referenceaudio from sound effects (SFX) libraries or partial videos for conditioning. Akey novelty of our model lies in its joint training on both internet videodatasets with low-quality audio and professional SFX recordings, enablinghigh-quality, full-bandwidth (48kHz) audio generation. Through automatedevaluations and human studies, we demonstrate that MultiFoley successfullygenerates synchronized high-quality sounds across varied conditional inputs andoutperforms existing methods. Please see our project page for video results:https://ificl.github.io/MultiFoley/</description><author>Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David Bourgin, Andrew Owens, Justin Salamon</author><pubDate>Tue, 26 Nov 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17698v1</guid></item><item><title>LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation</title><link>http://arxiv.org/abs/2411.04997v3</link><description>CLIP is a foundational multimodal model that aligns image and text featuresinto a shared space using contrastive learning on large-scale image-text pairs.Its strength lies in leveraging natural language as a rich supervisory signal.With the rapid progress of large language models (LLMs), we explore theirpotential to further enhance CLIP's multimodal representation learning. Thiswork introduces a fine-tuning approach that integrates LLMs with the pretrainedCLIP visual encoder, leveraging LLMs' advanced text understanding andopen-world knowledge to improve CLIP's ability to process long and complexcaptions. To address the challenge of LLMs' autoregressive nature, we propose acaption-to-caption contrastive learning framework to enhance the discriminativepower of their outputs. Our method achieves substantial performance gains onvarious downstream tasks, demonstrating the effectiveness of combining LLMswith CLIP for enhanced multimodal learning.</description><author>Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu</author><pubDate>Tue, 26 Nov 2024 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04997v3</guid></item><item><title>StableAnimator: High-Quality Identity-Preserving Human Image Animation</title><link>http://arxiv.org/abs/2411.17697v1</link><description>Current diffusion models for human image animation struggle to ensureidentity (ID) consistency. This paper presents StableAnimator, the firstend-to-end ID-preserving video diffusion framework, which synthesizeshigh-quality videos without any post-processing, conditioned on a referenceimage and a sequence of poses. Building upon a video diffusion model,StableAnimator contains carefully designed modules for both training andinference striving for identity consistency. In particular, StableAnimatorbegins by computing image and face embeddings with off-the-shelf extractors,respectively and face embeddings are further refined by interacting with imageembeddings using a global content-aware Face Encoder. Then, StableAnimatorintroduces a novel distribution-aware ID Adapter that prevents interferencecaused by temporal layers while preserving ID via alignment. During inference,we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization tofurther enhance the face quality. We demonstrate that solving the HJB equationcan be integrated into the diffusion denoising process, and the resultingsolution constrains the denoising path and thus benefits ID preservation.Experiments on multiple benchmarks show the effectiveness of StableAnimatorboth qualitatively and quantitatively.</description><author>Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, Zuxuan Wu</author><pubDate>Tue, 26 Nov 2024 18:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17697v1</guid></item><item><title>ScribbleLight: Single Image Indoor Relighting with Scribbles</title><link>http://arxiv.org/abs/2411.17696v1</link><description>Image-based relighting of indoor rooms creates an immersive virtualunderstanding of the space, which is useful for interior design, virtualstaging, and real estate. Relighting indoor rooms from a single image isespecially challenging due to complex illumination interactions betweenmultiple lights and cluttered objects featuring a large variety in geometricaland material complexity. Recently, generative models have been successfullyapplied to image-based relighting conditioned on a target image or a latentcode, albeit without detailed local lighting control. In this paper, weintroduce ScribbleLight, a generative model that supports local fine-grainedcontrol of lighting effects through scribbles that describe changes inlighting. Our key technical novelty is an Albedo-conditioned Stable ImageDiffusion model that preserves the intrinsic color and texture of the originalimage after relighting and an encoder-decoder-based ControlNet architecturethat enables geometry-preserving lighting effects with normal map and scribbleannotations. We demonstrate ScribbleLight's ability to create differentlighting effects (e.g., turning lights on/off, adding highlights, cast shadows,or indirect lighting from unseen lights) from sparse scribble annotations.</description><author>Jun Myeong Choi, Annie Wang, Pieter Peers, Anand Bhattad, Roni Sengupta</author><pubDate>Tue, 26 Nov 2024 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17696v1</guid></item><item><title>Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats</title><link>http://arxiv.org/abs/2411.17693v1</link><description>As large language models (LLMs) become increasingly capable, it is prudent toassess whether safety measures remain effective even if LLMs intentionally tryto bypass them. Previous work introduced control evaluations, an adversarialframework for testing deployment strategies of untrusted models (i.e., modelswhich might be trying to bypass safety measures). While prior work treats asingle failure as unacceptable, we perform control evaluations in a"distributed threat setting" -- a setting where no single action iscatastrophic and no single action provides overwhelming evidence ofmisalignment. We approach this problem with a two-level deployment frameworkthat uses an adaptive macro-protocol to choose between micro-protocols.Micro-protocols operate on a single task, using a less capable, but extensivelytested (trusted) model to harness and monitor the untrusted model. Meanwhile,the macro-protocol maintains an adaptive credence on the untrusted model'salignment based on its past actions, using it to pick between safer and riskiermicro-protocols. We evaluate our method in a code generation testbed where ared team attempts to generate subtly backdoored code with an LLM whosedeployment is safeguarded by a blue team. We plot Pareto frontiers of safety (#of non-backdoored solutions) and usefulness (# of correct solutions). At agiven level of usefulness, our adaptive deployment strategy reduces the numberof backdoors by 80% compared to non-adaptive baselines.</description><author>Jiaxin Wen, Vivek Hebbar, Caleb Larson, Aryan Bhatt, Ansh Radhakrishnan, Mrinank Sharma, Henry Sleight, Shi Feng, He He, Ethan Perez, Buck Shlegeris, Akbir Khan</author><pubDate>Tue, 26 Nov 2024 18:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17693v1</guid></item><item><title>Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens</title><link>http://arxiv.org/abs/2411.17691v1</link><description>We reveal that low-bit quantization favors undertrained large language models(LLMs) by observing that models with larger sizes or fewer training tokensexperience less quantization-induced degradation (QiD) when applying low-bitquantization, whereas smaller models with extensive training tokens suffersignificant QiD. To gain deeper insights into this trend, we study over 1500quantized LLM checkpoints of various sizes and at different training levels(undertrained or fully trained) in a controlled setting, deriving scaling lawsfor understanding the relationship between QiD and factors such as the numberof training tokens, model size and bit width. With the derived scaling laws, we propose a novel perspective that we can useQiD to measure an LLM's training levels and determine the number of trainingtokens required for fully training LLMs of various sizes. Moreover, we use thescaling laws to predict the quantization performance of different-sized LLMstrained with 100 trillion tokens. Our projection shows that the low-bitquantization performance of future models, which are expected to be trainedwith over 100 trillion tokens, may NOT be desirable. This poses a potentialchallenge for low-bit quantization in the future and highlights the need forawareness of a model's training level when evaluating low-bit quantizationresearch. To facilitate future research on this problem, we release all the1500+ quantized checkpoints used in this work athttps://huggingface.co/Xu-Ouyang.</description><author>Xu Ouyang, Tao Ge, Thomas Hartvigsen, Zhisong Zhang, Haitao Mi, Dong Yu</author><pubDate>Tue, 26 Nov 2024 18:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17691v1</guid></item><item><title>Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis</title><link>http://arxiv.org/abs/2411.17690v1</link><description>In this paper, we propose a new task -- generating speech from videos ofpeople and their transcripts (VTTS) -- to motivate new techniques formultimodal speech generation. This task generalizes the task of generatingspeech from cropped lip videos, and is also more complicated than the task ofgenerating generic audio clips (e.g., dog barking) from videos and text.Multilingual versions of the task could lead to new techniques forcross-lingual dubbing. We also present a decoder-only multimodal model for thistask, which we call Visatronic. This model embeds vision, text and speechdirectly into the common subspace of a transformer model and uses anautoregressive loss to learn a generative model of discretized mel-spectrogramsconditioned on speaker videos and transcripts of their speech. By embedding allmodalities into a common subspace, Visatronic can achieve improved results overmodels that use only text or video as input. Further, it presents a muchsimpler approach for multimodal speech generation compared to prevailingapproaches which rely on lip-detectors and complicated architectures to fusemodalities while producing better results. Since the model is flexible enoughto accommodate different ways of ordering inputs as a sequence, we carefullyexplore different strategies to better understand the best way to propagateinformation to the generative steps. To facilitate further research on VTTS, wewill release (i) our code, (ii) clean transcriptions for the large-scaleVoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTSincorporating both objective and subjective metrics.</description><author>Akshita Gupta, Tatiana Likhomanenko, Karren Dai Yang, Richard He Bai, Zakaria Aldeneh, Navdeep Jaitly</author><pubDate>Tue, 26 Nov 2024 18:57:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17690v1</guid></item><item><title>GenDeg: Diffusion-Based Degradation Synthesis for Generalizable All-in-One Image Restoration</title><link>http://arxiv.org/abs/2411.17687v1</link><description>Deep learning-based models for All-In-One Image Restoration (AIOR) haveachieved significant advancements in recent years. However, their practicalapplicability is limited by poor generalization to samples outside the trainingdistribution. This limitation arises primarily from insufficient diversity indegradation variations and scenes within existing datasets, resulting ininadequate representations of real-world scenarios. Additionally, capturinglarge-scale real-world paired data for degradations such as haze, low-light,and raindrops is often cumbersome and sometimes infeasible. In this paper, weleverage the generative capabilities of latent diffusion models to synthesizehigh-quality degraded images from their clean counterparts. Specifically, weintroduce GenDeg, a degradation and intensity-aware conditional diffusion modelcapable of producing diverse degradation patterns on clean images. UsingGenDeg, we synthesize over 550k samples across six degradation types: haze,rain, snow, motion blur, low-light, and raindrops. These generated samples areintegrated with existing datasets to form the GenDS dataset, comprising over750k samples. Our experiments reveal that image restoration models trained onthe GenDS dataset exhibit significant improvements in out-of-distributionperformance compared to those trained solely on existing datasets. Furthermore,we provide comprehensive analyses on the implications of diffusion model-basedsynthetic degradations for AIOR. The code will be made publicly available.</description><author>Sudarshan Rajagopalan, Nithin Gopalakrishnan Nair, Jay N. Paranjape, Vishal M. Patel</author><pubDate>Tue, 26 Nov 2024 18:55:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17687v1</guid></item><item><title>Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration</title><link>http://arxiv.org/abs/2411.17686v1</link><description>To accelerate the inference of heavy Multimodal Large Language Models(MLLMs), this study rethinks the current landscape of training-free tokenreduction research. We regret to find that the critical components of existingmethods are tightly intertwined, with their interconnections and effectsremaining unclear for comparison, transfer, and expansion. Therefore, wepropose a unified ''filter-correlate-compress'' paradigm that decomposes thetoken reduction into three distinct stages within a pipeline, maintainingconsistent design objectives and elements while allowing for uniqueimplementations. We additionally demystify the popular works and subsume theminto our paradigm to showcase its universality. Finally, we offer a suite ofmethods grounded in the paradigm, striking a balance between speed and accuracythroughout different phases of the inference. Experimental results across 10benchmarks indicate that our methods can achieve up to an 82.4% reduction inFLOPs with a minimal impact on performance, simultaneously surpassingstate-of-the-art training-free methods. Our project page is athttps://ficoco-accelerate.github.io/.</description><author>Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, Siteng Huang</author><pubDate>Tue, 26 Nov 2024 18:53:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17686v1</guid></item><item><title>Attamba: Attending To Multi-Token States</title><link>http://arxiv.org/abs/2411.17685v1</link><description>When predicting the next token in a sequence, vanilla transformers computeattention over all previous tokens, resulting in quadratic scaling of computewith sequence length. State-space models compress the entire sequence of tokensinto a fixed-dimensional representation to improve efficiency, while otherarchitectures achieve sub-quadratic complexity via low-rank projections orsparse attention patterns over the sequence. In this paper, we introduceAttamba, a novel architecture that uses state-space models to compress chunksof tokens and applies attention on these compressed key-value representations.We find that replacing key and value projections in a transformer with SSMs canimprove model quality and enable flexible token chunking, resulting in 24%improved perplexity with transformer of similar KV-Cache and attentionfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexitytrade-off. Attamba can perform attention on chunked-sequences of variablelength, enabling a smooth transition between quadratic and linear scaling,offering adaptable efficiency gains.</description><author>Yash Akhauri, Safeen Huda, Mohamed S. Abdelfattah</author><pubDate>Tue, 26 Nov 2024 18:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17685v1</guid></item><item><title>LOLA: LLM-Assisted Online Learning Algorithm for Content Experiments</title><link>http://arxiv.org/abs/2406.02611v3</link><description>Modern media firms require automated and efficient methods to identifycontent that is most engaging and appealing to users. Leveraging a large-scaledataset from Upworthy (a news publisher), which includes 17,681 headline A/Btests, we first investigate the ability of three pure-LLM approaches toidentify the catchiest headline: prompt-based methods, embedding-based methods,and fine-tuned open-source LLMs. Prompt-based approaches perform poorly, whileboth OpenAI-embedding-based models and the fine-tuned Llama-3-8B achievemarginally higher accuracy than random predictions. In sum, none of thepure-LLM-based methods can predict the best-performing headline with highaccuracy. We then introduce the LLM-Assisted Online Learning Algorithm (LOLA),a novel framework that integrates Large Language Models (LLMs) with adaptiveexperimentation to optimize content delivery. LOLA combines the best pure-LLMapproach with the Upper Confidence Bound algorithm to allocate traffic andmaximize clicks adaptively. Our numerical experiments on Upworthy data showthat LOLA outperforms the standard A/B test method (the current status quo atUpworthy), pure bandit algorithms, and pure-LLM approaches, particularly inscenarios with limited experimental traffic. Our approach is scalable andapplicable to content experiments across various settings where firms seek tooptimize user engagement, including digital advertising and social mediarecommendations.</description><author>Zikun Ye, Hema Yoganarasimhan, Yufeng Zheng</author><pubDate>Tue, 26 Nov 2024 18:51:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02611v3</guid></item><item><title>RealSeal: Revolutionizing Media Authentication with Real-Time Realism Scoring</title><link>http://arxiv.org/abs/2411.17684v1</link><description>The growing threat of deepfakes and manipulated media necessitates a radicalrethinking of media authentication. Existing methods for watermarking syntheticdata fall short, as they can be easily removed or altered, and current deepfakedetection algorithms do not achieve perfect accuracy. Provenance techniques,which rely on metadata to verify content origin, fail to address thefundamental problem of staged or fake media. This paper introduces a groundbreaking paradigm shift in media authenticationby advocating for the watermarking of real content at its source, as opposed towatermarking synthetic data. Our innovative approach employs multisensoryinputs and machine learning to assess the realism of content in real-time andacross different contexts. We propose embedding a robust realism score withinthe image metadata, fundamentally transforming how images are trusted andcirculated. By combining established principles of human reasoning aboutreality, rooted in firmware and hardware security, with the sophisticatedreasoning capabilities of contemporary machine learning systems, we develop aholistic approach that analyzes information from multiple perspectives. This ambitious, blue sky approach represents a significant leap forward inthe field, pushing the boundaries of media authenticity and trust. By embracingcutting-edge advancements in technology and interdisciplinary research, we aimto establish a new standard for verifying the authenticity of digital media.</description><author>Bhaktipriya Radharapu, Harish Krishna</author><pubDate>Tue, 26 Nov 2024 18:48:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17684v1</guid></item><item><title>Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning</title><link>http://arxiv.org/abs/2411.17679v1</link><description>Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE(BBPE) have significantly improved the computational efficiency and vocabularyrepresentation stability of large language models (LLMs) by segmenting textinto tokens. However, this segmentation often obscures the internal characterstructures and sequences within tokens, preventing models from fully learningthese intricate details during training. Consequently, LLMs struggle tocomprehend the character compositions and positional relationships withintokens, especially when fine-tuned on downstream tasks with limited data. Inthis paper, we introduce Token Internal Position Awareness (TIPA), a novelapproach that enhances LLMs' understanding of internal token structures bytraining them on reverse character prediction tasks using the tokenizer's ownvocabulary. This method enables models to effectively learn and generalizecharacter positions and internal structures. Experimental results demonstratethat LLMs trained with TIPA outperform baseline models in predicting characterpositions at the token level. Furthermore, when applied to the downstream taskof Chinese Spelling Correction (CSC), TIPA not only accelerates modelconvergence but also significantly improves task performance.</description><author>Zhu Xu, Zhiqiang Zhao, Zihan Zhang, Yuchi Liu, Quanwei Shen, Fei Liu, Yu Kuang</author><pubDate>Tue, 26 Nov 2024 18:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17679v1</guid></item><item><title>From Stars to Insights: Exploration and Implementation of Unified Sentiment Analysis with Distant Supervision</title><link>http://arxiv.org/abs/2305.01710v3</link><description>Sentiment analysis is integral to understanding the voice of the customer andinforming businesses' strategic decisions. Conventional sentiment analysisinvolves three separate tasks: aspect-category detection (ACD), aspect-categorysentiment analysis (ACSA), and rating prediction (RP). However, independentlytackling these tasks can overlook their interdependencies and often requiresexpensive, fine-grained annotations. This paper introduces Unified SentimentAnalysis (Uni-SA), a novel learning paradigm that unifies ACD, ACSA, and RPinto a coherent framework. To achieve this, we propose the Distantly SupervisedPyramid Network (DSPN), which employs a pyramid structure to capture sentimentat word, aspect, and document levels in a hierarchical manner. Evaluations onmulti-aspect review datasets in English and Chinese show that DSPN, using onlystar rating labels for supervision, demonstrates significant efficiencyadvantages while performing comparably well to a variety of benchmark models.Additionally, DSPN's pyramid structure enables the interpretability of itsoutputs. Our findings validate DSPN's effectiveness and efficiency,establishing a robust, resource-efficient, unified framework for sentimentanalysis.</description><author>Wenchang Li, John P. Lalor, Yixing Chen, Vamsi K. Kanuri</author><pubDate>Tue, 26 Nov 2024 18:40:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01710v3</guid></item><item><title>Instance-Aware Graph Prompt Learning</title><link>http://arxiv.org/abs/2411.17676v1</link><description>Graph neural networks stand as the predominant technique for graphrepresentation learning owing to their strong expressive power, yet theperformance highly depends on the availability of high-quality labels in anend-to-end manner. Thus the pretraining and fine-tuning paradigm has beenproposed to mitigate the label cost issue. Subsequently, the gap between thepretext tasks and downstream tasks has spurred the development of graph promptlearning which inserts a set of graph prompts into the original graph data withminimal parameters while preserving competitive performance. However, thecurrent exploratory works are still limited since they all concentrate onlearning fixed task-specific prompts which may not generalize well across thediverse instances that the task comprises. To tackle this challenge, weintroduce Instance-Aware Graph Prompt Learning (IA-GPL) in this paper, aimingto generate distinct prompts tailored to different input instances. The processinvolves generating intermediate prompts for each instance using a lightweightarchitecture, quantizing these prompts through trainable codebook vectors, andemploying the exponential moving average technique to ensure stable training.Extensive experiments conducted on multiple datasets and settings showcase thesuperior performance of IA-GPL compared to state-of-the-art baselines.</description><author>Jiazheng Li, Jundong Li, Chuxu Zhang</author><pubDate>Tue, 26 Nov 2024 18:38:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17676v1</guid></item><item><title>Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting</title><link>http://arxiv.org/abs/2411.17674v1</link><description>Understanding the emotions in a dialogue usually requires external knowledgeto accurately understand the contents. As the LLMs become more and morepowerful, we do not want to settle on the limited ability of the pre-trainedlanguage model. However, the LLMs either can only process text modality or aretoo expensive to process the multimedia information. We aim to utilize both thepower of LLMs and the supplementary features from the multimedia modalities. Inthis paper, we present a framework, Lantern, that can improve the performanceof a certain vanilla model by prompting large language models withreceptive-field-aware attention weighting. This framework trained a multi-taskvanilla model to produce probabilities of emotion classes and dimension scores.These predictions are fed into the LLMs as references to adjust the predictedprobabilities of each emotion class with its external knowledge and contextualunderstanding. We slice the dialogue into different receptive fields, and eachsample is included in exactly t receptive fields. Finally, the predictions ofLLMs are merged with a receptive-field-aware attention-driven weighting module.In the experiments, vanilla models CORECT and SDT are deployed in Lantern withGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-waysettings demonstrated that the Lantern can significantly improve theperformance of current vanilla models by up to 1.23% and 1.80%.</description><author>Liyun Zhang, Dian Ding, Yu Lu, Yi-Chao Chen, Guangtao Xue</author><pubDate>Tue, 26 Nov 2024 18:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17674v1</guid></item><item><title>SketchAgent: Language-Driven Sequential Sketch Generation</title><link>http://arxiv.org/abs/2411.17673v1</link><description>Sketching serves as a versatile tool for externalizing ideas, enabling rapidexploration and visual communication that spans various disciplines. Whileartificial systems have driven substantial advances in content creation andhuman-computer interaction, capturing the dynamic and abstract nature of humansketching remains challenging. In this work, we introduce SketchAgent, alanguage-driven, sequential sketch generation method that enables users tocreate, modify, and refine sketches through dynamic, conversationalinteractions. Our approach requires no training or fine-tuning. Instead, weleverage the sequential nature and rich prior knowledge of off-the-shelfmultimodal large language models (LLMs). We present an intuitive sketchinglanguage, introduced to the model through in-context examples, enabling it to"draw" using string-based actions. These are processed into vector graphics andthen rendered to create a sketch on a pixel canvas, which can be accessed againfor further tasks. By drawing stroke by stroke, our agent captures theevolving, dynamic qualities intrinsic to sketching. We demonstrate thatSketchAgent can generate sketches from diverse prompts, engage indialogue-driven drawing, and collaborate meaningfully with human users.</description><author>Yael Vinker, Tamar Rott Shaham, Kristine Zheng, Alex Zhao, Judith E Fan, Antonio Torralba</author><pubDate>Tue, 26 Nov 2024 18:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17673v1</guid></item><item><title>Synthetic Data Generation with LLM for Improved Depression Prediction</title><link>http://arxiv.org/abs/2411.17672v1</link><description>Automatic detection of depression is a rapidly growing field of research atthe intersection of psychology and machine learning. However, with itsexponential interest comes a growing concern for data privacy and scarcity dueto the sensitivity of such a topic. In this paper, we propose a pipeline forLarge Language Models (LLMs) to generate synthetic data to improve theperformance of depression prediction models. Starting from unstructured,naturalistic text data from recorded transcripts of clinical interviews, weutilize an open-source LLM to generate synthetic data through chain-of-thoughtprompting. This pipeline involves two key steps: the first step is thegeneration of the synopsis and sentiment analysis based on the originaltranscript and depression score, while the second is the generation of thesynthetic synopsis/sentiment analysis based on the summaries generated in thefirst step and a new depression score. Not only was the synthetic datasatisfactory in terms of fidelity and privacy-preserving metrics, it alsobalanced the distribution of severity in the training dataset, therebysignificantly enhancing the model's capability in predicting the intensity ofthe patient's depression. By leveraging LLMs to generate synthetic data thatcan be augmented to limited and imbalanced real-world datasets, we demonstratea novel approach to addressing data scarcity and privacy concerns commonlyfaced in automatic depression detection, all while maintaining the statisticalintegrity of the original dataset. This approach offers a robust framework forfuture mental health research and applications.</description><author>Andrea Kang, Jun Yu Chen, Zoe Lee-Youngzie, Shuhao Fu</author><pubDate>Tue, 26 Nov 2024 18:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17672v1</guid></item><item><title>Linguistic Laws Meet Protein Sequences: A Comparative Analysis of Subword Tokenization Methods</title><link>http://arxiv.org/abs/2411.17669v1</link><description>Tokenization is a crucial step in processing protein sequences for machinelearning models, as proteins are complex sequences of amino acids that requiremeaningful segmentation to capture their functional and structural properties.However, existing subword tokenization methods, developed primarily for humanlanguage, may be inadequate for protein sequences, which have unique patternsand constraints. This study evaluates three prominent tokenization approaches,Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, across varyingvocabulary sizes (400-6400), analyzing their effectiveness in protein sequencerepresentation, domain boundary preservation, and adherence to establishedlinguistic laws. Our comprehensive analysis reveals distinct behavioralpatterns among these tokenizers, with vocabulary size significantly influencingtheir performance. BPE demonstrates better contextual specialization andmarginally better domain boundary preservation at smaller vocabularies, whileSentencePiece achieves better encoding efficiency, leading to lower fertilityscores. WordPiece offers a balanced compromise between these characteristics.However, all tokenizers show limitations in maintaining protein domainintegrity, particularly as vocabulary size increases. Analysis of linguisticlaw adherence shows partial compliance with Zipf's and Brevity laws but notabledeviations from Menzerath's law, suggesting that protein sequences may followdistinct organizational principles from natural languages. These findingshighlight the limitations of applying traditional NLP tokenization methods toprotein sequences and emphasize the need for developing specializedtokenization strategies that better account for the unique characteristics ofproteins.</description><author>Burak Suyunu, Enes Taylan, Arzucan Özgür</author><pubDate>Tue, 26 Nov 2024 18:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17669v1</guid></item><item><title>Anytime Acceleration of Gradient Descent</title><link>http://arxiv.org/abs/2411.17668v1</link><description>This work investigates stepsize-based acceleration of gradient descent with{\em anytime} convergence guarantees. For smooth (non-strongly) convexoptimization, we propose a stepsize schedule that allows gradient descent toachieve convergence guarantees of $O(T^{-1.03})$ for any stopping time $T$,where the stepsize schedule is predetermined without prior knowledge of thestopping time. This result provides an affirmative answer to a COLT openproblem \citep{kornowski2024open} regarding whether stepsize-based accelerationcan yield anytime convergence rates of $o(T^{-1})$. We further extend ourtheory to yield anytime convergence guarantees of$\exp(-\Omega(T/\kappa^{0.97}))$ for smooth and strongly convex optimization,with $\kappa$ being the condition number.</description><author>Zihan Zhang, Jason D. Lee, Simon S. Du, Yuxin Chen</author><pubDate>Tue, 26 Nov 2024 18:29:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17668v1</guid></item><item><title>How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations</title><link>http://arxiv.org/abs/2411.17666v1</link><description>Multimodal foundation models aim to create a unified representation spacethat abstracts away from surface features like language syntax or modalitydifferences. To investigate this, we study the internal representations ofthree recent models, analyzing the model activations from semanticallyequivalent sentences across languages in the text and speech modalities. Ourfindings reveal that: 1) Cross-modal representations converge over modellayers, except in the initial layers specialized at text and speech processing.2) Length adaptation is crucial for reducing the cross-modal gap between textand speech, although current approaches' effectiveness is primarily limited tohigh-resource languages. 3) Speech exhibits larger cross-lingual differencesthan text. 4) For models not explicitly trained for modality-agnosticrepresentations, the modality gap is more prominent than the language gap.</description><author>Hyunji Lee, Danni Liu, Supriti Sinhamahapatra, Jan Niehues</author><pubDate>Tue, 26 Nov 2024 18:29:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17666v1</guid></item><item><title>RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training</title><link>http://arxiv.org/abs/2411.17662v1</link><description>Vision-based pose estimation of articulated robots with unknown joint angleshas applications in collaborative robotics and human-robot interaction tasks.Current frameworks use neural network encoders to extract image features anddownstream layers to predict joint angles and robot pose. While images ofrobots inherently contain rich information about the robot's physicalstructures, existing methods often fail to leverage it fully; therefore,limiting performance under occlusions and truncations. To address this, weintroduce RoboPEPP, a method that fuses information about the robot's physicalmodel into the encoder using a masking-based self-supervisedembedding-predictive architecture. Specifically, we mask the robot's joints andpre-train an encoder-predictor model to infer the joints' embeddings fromsurrounding unmasked regions, enhancing the encoder's understanding of therobot's physical model. The pre-trained encoder-predictor pair, along withjoint angle and keypoint prediction networks, is then fine-tuned for pose andjoint angle estimation. Random masking of input during fine-tuning and keypointfiltering during evaluation further improves robustness. Our method, evaluatedon several datasets, achieves the best results in robot pose and joint angleestimation while being the least sensitive to occlusions and requiring thelowest execution time.</description><author>Raktim Gautam Goswami, Prashanth Krishnamurthy, Yann LeCun, Farshad Khorrami</author><pubDate>Tue, 26 Nov 2024 18:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17662v1</guid></item><item><title>BERT or FastText? A Comparative Analysis of Contextual as well as Non-Contextual Embeddings</title><link>http://arxiv.org/abs/2411.17661v1</link><description>Natural Language Processing (NLP) for low-resource languages presentssignificant challenges, particularly due to the scarcity of high-qualityannotated data and linguistic resources. The choice of embeddings plays acritical role in enhancing the performance of NLP tasks, such as newsclassification, sentiment analysis, and hate speech detection, especially forlow-resource languages like Marathi. In this study, we investigate the impactof various embedding techniques- Contextual BERT-based, Non-ContextualBERT-based, and FastText-based on NLP classification tasks specific to theMarathi language. Our research includes a thorough evaluation of bothcompressed and uncompressed embeddings, providing a comprehensive overview ofhow these embeddings perform across different scenarios. Specifically, wecompare two BERT model embeddings, Muril and MahaBERT, as well as two FastTextmodel embeddings, IndicFT and MahaFT. Our evaluation includes applyingembeddings to a Multiple Logistic Regression (MLR) classifier for taskperformance assessment, as well as TSNE visualizations to observe the spatialdistribution of these embeddings. The results demonstrate that contextualembeddings outperform non-contextual embeddings. Furthermore, BERT-basednon-contextual embeddings extracted from the first BERT embedding layer yieldbetter results than FastText-based embeddings, suggesting a potentialalternative to FastText embeddings.</description><author>Abhay Shanbhag, Suramya Jadhav, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi</author><pubDate>Tue, 26 Nov 2024 18:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17661v1</guid></item><item><title>DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2411.17660v1</link><description>Recent progress in scene synthesis makes standalone SLAM systems purely basedon optimizing hyperprimitives with a Rendering objective possible\cite{monogs}. However, the tracking performance still lacks behind traditional\cite{orbslam} and end-to-end SLAM systems \cite{droid}. An optimal trade-off between robustness, speed and accuracy has not yet beenreached, especially for monocular video. In this paper, we introduce a SLAM system based on an end-to-end Tracker andextend it with a Renderer based on recent 3D Gaussian Splatting techniques. Our framework \textbf{DroidSplat} achieves both SotA tracking and renderingresults on common SLAM benchmarks. We implemented multiple building blocks of modern SLAM systems to run inparallel, allowing for fast inference on common consumer GPU's. Recent progress in monocular depth prediction and camera calibration allowsour system to achieve strong results even on in-the-wild data without knowncamera intrinsics. Code will be available at \url{https://github.com/ChenHoy/DROID-Splat}.</description><author>Christian Homeyer, Leon Begiristain, Christoph Schnörr</author><pubDate>Tue, 26 Nov 2024 18:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17660v1</guid></item><item><title>Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages</title><link>http://arxiv.org/abs/2411.12240v2</link><description>Large Language Models (LLMs) based on transformer architectures haverevolutionized a variety of domains, with tokenization playing a pivotal rolein their pre-processing and fine-tuning stages. In multilingual models,particularly those tailored for Indic languages, effective tokenization iscrucial for optimizing performance. This paper presents a comprehensiveevaluation of tokenizers used by 12 LLMs across all 22 official languages ofIndia, with a focus on comparing the efficiency of their tokenizationprocesses. We employed the Normalized Sequence Length (NSL) as a key metric inour analysis. Our findings reveal that the SUTRA tokenizer outperforms allother models, including several Indic-specific models, excelling in 14languages. Notable insights include the SUTRA tokenizer's superior handling ofIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processingIndian languages, and the limited performance of Project Indus in certainlanguages. This study underscores the critical importance of developingtargeted tokenization strategies for multilingual and Indic-centric models,laying the groundwork for future improvements in tokenizer design to enhancelinguistic coverage and model efficiency.</description><author>S. Tamang, D. J. Bora</author><pubDate>Tue, 26 Nov 2024 18:14:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12240v2</guid></item><item><title>A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics</title><link>http://arxiv.org/abs/2401.15122v3</link><description>In drug discovery, molecular dynamics (MD) simulation for protein-ligandbinding provides a powerful tool for predicting binding affinities, estimatingtransport properties, and exploring pocket sites. There has been a long historyof improving the efficiency of MD simulations through better numerical methodsand, more recently, by utilizing machine learning (ML) methods. Yet, challengesremain, such as accurate modeling of extended-timescale simulations. To addressthis issue, we propose NeuralMD, the first ML surrogate that can facilitatenumerical MD and provide accurate simulations in protein-ligand bindingdynamics. We propose a principled approach that incorporates a novelphysics-informed multi-grained group symmetric framework. Specifically, wepropose (1) the BindingNet model that satisfies group symmetry using vectorframes and captures the multi-level protein-ligand interactions, and (2) anaugmented neural differential equation solver that learns the trajectory underNewtonian mechanics. For the experiment, we design ten single-trajectory andthree multi-trajectory binding simulation tasks. We demonstrate the efficiencyand effectiveness of NeuralMD, achieving over 1K$\times$ speedup compared tostandard numerical MD simulations. NeuralMD also outperforms all other MLapproaches, achieving up to 15$\times$ reduction in reconstruction error and70% increase in validity. Additionally, we qualitatively illustrate that theoscillations in the predicted trajectories align more closely with ground-truthdynamics than those of other machine-learning methods. We believe NeuralMDpaves the foundation for a new research paradigm in simulating protein-liganddynamics.</description><author>Shengchao Liu, Weitao Du, Hannan Xu, Yanjing Li, Zhuoxinran Li, Vignesh Bhethanabotla, Divin Yan, Christian Borgs, Anima Anandkumar, Hongyu Guo, Jennifer Chayes</author><pubDate>Tue, 26 Nov 2024 18:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15122v3</guid></item><item><title>SAMWISE: Infusing wisdom in SAM2 for Text-Driven Video Segmentation</title><link>http://arxiv.org/abs/2411.17646v1</link><description>Referring Video Object Segmentation (RVOS) relies on natural languageexpressions to segment an object in a video clip. Existing methods restrictreasoning either to independent short clips, losing global context, or processthe entire video offline, impairing their application in a streaming fashion.In this work, we aim to surpass these limitations and design an RVOS methodcapable of effectively operating in streaming-like scenarios while retainingcontextual information from past frames. We build upon the Segment-Anything 2(SAM2) model, that provides robust segmentation and tracking capabilities andis naturally suited for streaming processing. We make SAM2 wiser, by empoweringit with natural language understanding and explicit temporal modeling at thefeature extraction stage, without fine-tuning its weights, and withoutoutsourcing modality interaction to external models. To this end, we introducea novel adapter module that injects temporal information and multi-modal cuesin the feature extraction process. We further reveal the phenomenon of trackingbias in SAM2 and propose a learnable module to adjust its tracking focus whenthe current frame features suggest a new object more aligned with the caption.Our proposed method, SAMWISE, achieves state-of-the-art across variousbenchmarks, by adding a negligible overhead of just 4.2 M parameters. The codeis available at https://github.com/ClaudiaCuttano/SAMWISE</description><author>Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, Giuseppe Averta</author><pubDate>Tue, 26 Nov 2024 18:10:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17646v1</guid></item><item><title>Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset</title><link>http://arxiv.org/abs/2411.17645v1</link><description>The use of machine learning and AI on electronic health records (EHRs) holdssubstantial potential for clinical insight. However, this approach facessignificant challenges due to data heterogeneity, sparsity, temporalmisalignment, and limited labeled outcomes. In this context, we leverage alinked EHR dataset of approximately one million de-identified individuals fromBristol, North Somerset, and South Gloucestershire, UK, to characterize urinarytract infections (UTIs) and develop predictive models focused on data quality,fairness and transparency. A comprehensive data pre-processing and curationpipeline transforms the raw EHR data into a structured format suitable for AImodeling. Given the limited availability and biases of ground truth UTIoutcomes, we introduce a UTI risk estimation framework informed by clinicalexpertise to estimate UTI risk across individual patient timelines. Using thisframework, we built pairwise XGBoost models to differentiate UTI riskcategories with explainable AI techniques to identify key predictors whileensuring interpretability. Our findings reveal differences in clinical anddemographic factors across risk groups, offering insights into UTI riskstratification and progression. This study demonstrates the added value ofAI-driven insights into UTI clinical decision-making while prioritizinginterpretability, transparency, and fairness, underscoring the importance ofsound data practices in advancing health outcomes.</description><author>Yujie Dai, Brian Sullivan, Axel Montout, Amy Dillon, Chris Waller, Peter Acs, Rachel Denholm, Philip Williams, Alastair D Hay, Raul Santos-Rodriguez, Andrew Dowsey</author><pubDate>Tue, 26 Nov 2024 18:10:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17645v1</guid></item><item><title>Health AI Developer Foundations</title><link>http://arxiv.org/abs/2411.15128v2</link><description>Robust medical Machine Learning (ML) models have the potential torevolutionize healthcare by accelerating clinical research, improving workflowsand outcomes, and producing novel insights or capabilities. Developing such MLmodels from scratch is cost prohibitive and requires substantial compute, data,and time (e.g., expert labeling). To address these challenges, we introduceHealth AI Developer Foundations (HAI-DEF), a suite of pre-trained,domain-specific foundation models, tools, and recipes to accelerate building MLfor health applications. The models cover various modalities and domains,including radiology (X-rays and computed tomography), histopathology,dermatological imaging, and audio. These models provide domain specificembeddings that facilitate AI development with less labeled data, shortertraining times, and reduced computational costs compared to traditionalapproaches. In addition, we utilize a common interface and style across thesemodels, and prioritize usability to enable developers to integrate HAI-DEFefficiently. We present model evaluations across various tasks and concludewith a discussion of their application and evaluation, covering the importanceof ensuring efficacy, fairness, and equity. Finally, while HAI-DEF andspecifically the foundation models lower the barrier to entry for ML inhealthcare, we emphasize the importance of validation with problem- andpopulation-specific data for each desired usage setting. This technical reportwill be updated over time as more modalities and features are added.</description><author>Atilla P. Kiraly, Sebastien Baur, Kenneth Philbrick, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Nick George, Fayaz Jamil, Jing Tang, Kai Bailey, Faruk Ahmed, Akshay Goel, Abbi Ward, Lin Yang, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Shekoofeh Azizi, David F. Steiner, Yun Liu, Tim Thelin, Rory Pilgrim, Can Kirmizibayrak</author><pubDate>Tue, 26 Nov 2024 18:01:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15128v2</guid></item><item><title>CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs</title><link>http://arxiv.org/abs/2409.08217v2</link><description>Graph neural networks have become the default choice by practitioners forgraph learning tasks such as graph classification and node classification.Nevertheless, popular graph neural network models still struggle to capturehigher-order information, i.e., information that goes \emph{beyond} pairwiseinteractions. Recent work has shown that persistent homology, a tool fromtopological data analysis, can enrich graph neural networks with topologicalinformation that they otherwise could not capture. Calculating such features isefficient for dimension 0 (connected components) and dimension 1 (cycles).However, when it comes to higher-order structures, it does not scale well, witha complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the orderof the structures. In this work, we introduce a novel method that extractsinformation about higher-order structures in the graph while still using theefficient low-dimensional persistent homology algorithm. On standard benchmarkdatasets, we show that our method can lead to up to $31\%$ improvements in testaccuracy.</description><author>Davide Buffelli, Farzin Soleymani, Bastian Rieck</author><pubDate>Tue, 26 Nov 2024 18:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08217v2</guid></item><item><title>On Limitations of LLM as Annotator for Low Resource Languages</title><link>http://arxiv.org/abs/2411.17637v1</link><description>Low-resource languages face significant challenges due to the lack ofsufficient linguistic data, resources, and tools for tasks such as supervisedlearning, annotation, and classification. This shortage hinders the developmentof accurate models and datasets, making it difficult to perform critical NLPtasks like sentiment analysis or hate speech detection. To bridge this gap,Large Language Models (LLMs) present an opportunity for potential annotators,capable of generating datasets and resources for these underrepresentedlanguages. In this paper, we focus on Marathi, a low-resource language, andevaluate the performance of both closed-source and open-source LLMs asannotators. We assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and9B), and Llama 3.1 (8B) on classification tasks including sentiment analysis,news classification, and hate speech detection. Our findings reveal that whileLLMs excel in annotation tasks for high-resource languages like English, theystill fall short when applied to Marathi. Even advanced closed models likeGemini and GPT underperform in comparison to BERT-based baselines, highlightingthe limitations of LLMs as annotators for low-resource languages.</description><author>Suramya Jadhav, Abhay Shanbhag, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi</author><pubDate>Tue, 26 Nov 2024 17:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17637v1</guid></item><item><title>MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation</title><link>http://arxiv.org/abs/2411.17636v1</link><description>Large Language Models (LLMs) have demonstrated remarkable planning abilitiesacross various domains, including robotics manipulation and navigation. Whilerecent efforts in robotics have leveraged LLMs both for high-level andlow-level planning, these approaches often face significant challenges, such ashallucinations in long-horizon tasks and limited adaptability due to thegeneration of plans in a single pass without real-time feedback. To addressthese limitations, we propose a novel multi-agent LLM framework, Multi-AgentLarge Language Model for Manipulation (MALMM) that distributes high-levelplanning and low-level control code generation across specialized LLM agents,supervised by an additional agent that dynamically manages transitions. Byincorporating observations from the environment after each step, our frameworkeffectively handles intermediate failures and enables adaptive re-planning.Unlike existing methods, our approach does not rely on pre-trained skillpolicies or in-context learning examples and generalizes to a variety of newtasks. We evaluate our approach on nine RLBench tasks, including long-horizontasks, and demonstrate its ability to solve robotics manipulation in azero-shot setting, thereby overcoming key limitations of existing LLM-basedmanipulation methods.</description><author>Harsh Singh, Rocktim Jyoti Das, Mingfei Han, Preslav Nakov, Ivan Laptev</author><pubDate>Tue, 26 Nov 2024 17:53:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17636v1</guid></item><item><title>Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies</title><link>http://arxiv.org/abs/2409.07932v2</link><description>Graph path search is a classic computer science problem that has beenrecently approached with Reinforcement Learning (RL) due to its potential tooutperform prior methods. Existing RL techniques typically assume a global viewof the network, which is not suitable for large-scale, dynamic, andprivacy-sensitive settings. An area of particular interest is search in socialnetworks due to its numerous applications. Inspired by seminal work inexperimental sociology, which showed that decentralized yet efficient search ispossible in social networks, we frame the problem as a collaborative taskbetween multiple agents equipped with a limited local view of the network. Wepropose a multi-agent approach for graph path search that successfullyleverages both homophily and structural heterogeneity. Our experiments, carriedout over synthetic and real-world social networks, demonstrate that our modelsignificantly outperforms learned and heuristic baselines. Furthermore, ourresults show that meaningful embeddings for graph navigation can be constructedusing reward-driven learning.</description><author>Alexei Pisacane, Victor-Alexandru Darvariu, Mirco Musolesi</author><pubDate>Tue, 26 Nov 2024 17:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07932v2</guid></item><item><title>Learning Spatially-Aware Language and Audio Embeddings</title><link>http://arxiv.org/abs/2409.11369v2</link><description>Humans can picture a sound scene given an imprecise natural languagedescription. For example, it is easy to imagine an acoustic environment given aphrase like "the lion roar came from right behind me!". For a machine to havethe same degree of comprehension, the machine must know what a lion is(semantic attribute), what the concept of "behind" is (spatial attribute) andhow these pieces of linguistic information align with the semantic and spatialattributes of the sound (what a roar sounds like when its coming from behind).State-of-the-art audio foundation models which learn to map between audioscenes and natural textual descriptions, are trained on non-spatial audio andtext pairs, and hence lack spatial awareness. In contrast, sound eventlocalization and detection models are limited to recognizing sounds from afixed number of classes, and they localize the source to absolute position(e.g., 0.2m) rather than a position described using natural language (e.g.,"next to me"). To address these gaps, we present ELSA a spatially aware-audioand text embedding model trained using multimodal contrastive learning. ELSAsupports non-spatial audio, spatial audio, and open vocabulary text captionsdescribing both the spatial and semantic components of sound. To train ELSA:(a) we spatially augment the audio and captions of three open-source audiodatasets totaling 4,738 hours of audio, and (b) we design an encoder to capturethe semantics of non-spatial audio, and the semantics and spatial attributes ofspatial audio using contrastive learning. ELSA is competitive withstate-of-the-art for both semantic retrieval and 3D source localization. Inparticular, ELSA achieves +2.8% mean audio-to-text and text-to-audio R@1 abovethe baseline, and outperforms by -11.6{\deg} mean-absolute-error in 3D sourcelocalization over the baseline.</description><author>Bhavika Devnani, Skyler Seto, Zakaria Aldeneh, Alessandro Toso, Elena Menyaylenko, Barry-John Theobald, Jonathan Sheaffer, Miguel Sarabia</author><pubDate>Tue, 26 Nov 2024 17:46:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11369v2</guid></item><item><title>Learning Chemical Reaction Representation with Reactant-Product Alignment</title><link>http://arxiv.org/abs/2411.17629v1</link><description>Organic synthesis stands as a cornerstone of chemical industry. Thedevelopment of robust machine learning models to support tasks associated withorganic reactions is of significant interest. However, current methods rely onhand-crafted features or direct adaptations of model architectures from otherdomains, which lacks feasibility as data scales increase or overlook the richchemical information inherent in reactions. To address these issues, this paperintroduces {\modelname}, a novel chemical reaction representation learningmodel tailored for a variety of organic-reaction-related tasks. By integratingatomic correspondence between reactants and products, our model discerns themolecular transformations that occur during the reaction, thereby enhancing thecomprehension of the reaction mechanism. We have designed an adapter structureto incorporate reaction conditions into the chemical reaction representation,allowing the model to handle diverse reaction conditions and adapt to variousdatasets and downstream tasks, e.g., reaction performance prediction.Additionally, we introduce a reaction-center aware attention mechanism thatenables the model to concentrate on key functional groups, thereby generatingpotent representations for chemical reactions. Our model has been evaluated ona range of downstream tasks, including reaction condition prediction, reactionyield prediction, and reaction selectivity prediction. Experimental resultsindicate that our model markedly outperforms existing chemical reactionrepresentation learning architectures across all tasks. Notably, our modelsignificantly outperforms all the baselines with up to 25\% (top-1) and 16\%(top-10) increased accuracy over the strongest baseline on USPTO\_CONDITIONdataset for reaction condition prediction. We plan to open-source the codecontingent upon the acceptance of the paper.</description><author>Kaipeng Zeng, Xianbin Liu, Yu Zhang, Xiaokang Yang, Yaohui Jin, Yanyan Xu</author><pubDate>Tue, 26 Nov 2024 17:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17629v1</guid></item><item><title>BRACTIVE: A Brain Activation Approach to Human Visual Brain Learning</title><link>http://arxiv.org/abs/2405.18808v2</link><description>The human brain is a highly efficient processing unit, and understanding howit works can inspire new algorithms and architectures in machine learning. Inthis work, we introduce a novel framework named Brain Activation Network(BRACTIVE), a transformer-based approach to studying the human visual brain.The main objective of BRACTIVE is to align the visual features of subjects withcorresponding brain representations via fMRI signals. It allows us to identifythe brain's Regions of Interest (ROI) of the subjects. Unlike previous brainresearch methods, which can only identify ROIs for one subject at a time andare limited by the number of subjects, BRACTIVE automatically extends thisidentification to multiple subjects and ROIs. Our experiments demonstrate thatBRACTIVE effectively identifies person-specific regions of interest, such asface and body-selective areas, aligning with neuroscience findings andindicating potential applicability to various object categories. Moreimportantly, we found that leveraging human visual brain activity to guide deepneural networks enhances performance across various benchmarks. It encouragesthe potential of BRACTIVE in both neuroscience and machine intelligencestudies.</description><author>Xuan-Bac Nguyen, Hojin Jang, Xin Li, Samee U. Khan, Pawan Sinha, Khoa Luu</author><pubDate>Tue, 26 Nov 2024 17:40:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18808v2</guid></item><item><title>Brainformer: Mimic Human Visual Brain Functions to Machine Vision Models via fMRI</title><link>http://arxiv.org/abs/2312.00236v4</link><description>Human perception plays a vital role in forming beliefs and understandingreality. A deeper understanding of brain functionality will lead to thedevelopment of novel deep neural networks. In this work, we introduce a novelframework named Brainformer, a straightforward yet effective Transformer-basedframework, to analyze Functional Magnetic Resonance Imaging (fMRI) patterns inthe human perception system from a machine-learning perspective. Specifically,we present the Multi-scale fMRI Transformer to explore brain activity patternsthrough fMRI signals. This architecture includes a simple yet efficient modulefor high-dimensional fMRI signal encoding and incorporates a novel embeddingtechnique called 3D Voxels Embedding. Secondly, drawing inspiration from thefunctionality of the brain's Region of Interest, we introduce a novel lossfunction called Brain fMRI Guidance Loss. This loss function mimics brainactivity patterns from these regions in the deep neural network using fMRIdata. This work introduces a prospective approach to transferring knowledgefrom human perception to neural networks. Our experiments demonstrate thatleveraging fMRI information allows the machine vision model to achieve resultscomparable to State-of-the-Art methods in various image recognition tasks.</description><author>Xuan-Bac Nguyen, Xin Li, Pawan Sinha, Samee U. Khan, Khoa Luu</author><pubDate>Tue, 26 Nov 2024 17:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00236v4</guid></item><item><title>Data-driven development of cycle prediction models for lithium metal batteries using multi modal mining</title><link>http://arxiv.org/abs/2411.17625v1</link><description>Recent advances in data-driven research have shown great potential inunderstanding the intricate relationships between materials and theirperformances. Herein, we introduce a novel multi modal data-driven approachemploying an Automatic Battery data Collector (ABC) that integrates a largelanguage model (LLM) with an automatic graph mining tool, Material GraphDigitizer (MatGD). This platform enables state-of-the-art accurate extractionof battery material data and cyclability performance metrics from diversetextual and graphical data sources. From the database derived through the ABCplatform, we developed machine learning models that can accurately predict thecapacity and stability of lithium metal batteries, which is the first-evermodel developed to achieve such predictions. Our models were alsoexperimentally validated, confirming practical applicability and reliability ofour data-driven approach.</description><author>Jaewoong Lee, Junhee Woo, Sejin Kim, Cinthya Paulina, Hyunmin Park, Hee-Tak Kim, Steve Park, Jihan Kim</author><pubDate>Tue, 26 Nov 2024 17:37:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17625v1</guid></item><item><title>Machine Learning and Multi-source Remote Sensing in Forest Carbon Stock Estimation: A Review</title><link>http://arxiv.org/abs/2411.17624v1</link><description>Quantifying forest carbon is crucial for informing decisions and policiesthat will protect the planet. Machine learning (ML) and remote sensing (RS)techniques have been used to do this task more effectively, yet there lacks asystematic review on the most recent ML methods and RS combinations, especiallywith the consideration of forest characteristics. This study systematicallyanalyzed 25 papers meeting strict inclusion criteria from over 80 relatedstudies, identifying 28 ML methods and key combinations of RS data. RandomForest had the most frequent appearance (88\% of studies), while ExtremeGradient Boosting showed superior performance in 75\% of the studies in whichit was compared with other methods. Sentinel-1 emerged as the most utilizedremote sensing source, with multi-sensor approaches (e.g., Sentinel-1,Sentinel-2, and LiDAR) proving especially effective. Our findings providegrounds for recommending best practices in integrating machine learning andremote sensing for accurate and scalable forest carbon stock estimation.</description><author>Autumn Nguyen, Sulagna Saha</author><pubDate>Tue, 26 Nov 2024 17:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17624v1</guid></item><item><title>IG-CFAT: An Improved GAN-Based Framework for Effectively Exploiting Transformers in Real-World Image Super-Resolution</title><link>http://arxiv.org/abs/2406.13815v3</link><description>In the field of single image super-resolution (SISR), transformer-basedmodels, have demonstrated significant advancements. However, the potential andefficiency of these models in applied fields such as real-world imagesuper-resolution have been less noticed and there are substantial opportunitiesfor improvement. Recently, composite fusion attention transformer (CFAT),outperformed previous state-of-the-art (SOTA) models in classic imagesuper-resolution. In this paper, we propose a novel GAN-based framework byincorporating the CFAT model to effectively exploit the performance oftransformers in real-world image super-resolution. In our proposed approach, weintegrate a semantic-aware discriminator to reconstruct fine details moreaccurately and employ an adaptive degradation model to better simulatereal-world degradations. Moreover, we introduce a new combination of lossfunctions by adding wavelet loss to loss functions of GAN-based models tobetter recover high-frequency details. Empirical results demonstrate thatIG-CFAT significantly outperforms existing SOTA models in both quantitative andqualitative metrics. Our proposed model revolutionizes the field of real-worldimage super-resolution and demonstrates substantially better performance inrecovering fine details and generating realistic textures. The introduction ofIG-CFAT offers a robust and adaptable solution for real-world imagesuper-resolution tasks.</description><author>Alireza Aghelan, Ali Amiryan, Abolfazl Zarghani, Modjtaba Rouhani</author><pubDate>Tue, 26 Nov 2024 17:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13815v3</guid></item><item><title>An Ensemble Approach for Brain Tumor Segmentation and Synthesis</title><link>http://arxiv.org/abs/2411.17617v1</link><description>The integration of machine learning in magnetic resonance imaging (MRI),specifically in neuroimaging, is proving to be incredibly effective, leading tobetter diagnostic accuracy, accelerated image analysis, and data-driveninsights, which can potentially transform patient care. Deep learning modelsutilize multiple layers of processing to capture intricate details of complexdata, which can then be used on a variety of tasks, including brain tumorclassification, segmentation, image synthesis, and registration. Previousresearch demonstrates high accuracy in tumor segmentation using various modelarchitectures, including nn-UNet and Swin-UNet. U-Mamba, which uses state spacemodeling, also achieves high accuracy in medical image segmentation. Toleverage these models, we propose a deep learning framework that ensemblesthese state-of-the-art architectures to achieve accurate segmentation andproduce finely synthesized images.</description><author>Juampablo E. Heras Rivera, Agamdeep S. Chopra, Tianyi Ren, Hitender Oswal, Yutong Pan, Zineb Sordo, Sophie Walters, William Henry, Hooman Mohammadi, Riley Olson, Fargol Rezayaraghi, Tyson Lam, Akshay Jaikanth, Pavan Kancharla, Jacob Ruzevick, Daniela Ushizima, Mehmet Kurt</author><pubDate>Tue, 26 Nov 2024 17:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17617v1</guid></item><item><title>Beyond Grids: Exploring Elastic Input Sampling for Vision Transformers</title><link>http://arxiv.org/abs/2309.13353v2</link><description>Vision transformers have excelled in various computer vision tasks but mostlyrely on rigid input sampling using a fixed-size grid of patches. It limitstheir applicability in real-world problems, such as active visual exploration,where patches have various scales and positions. Our paper addresses thislimitation by formalizing the concept of input elasticity for visiontransformers and introducing an evaluation protocol for measuring thiselasticity. Moreover, we propose modifications to the transformer architectureand training regime, which increase its elasticity. Through extensiveexperimentation, we spotlight opportunities and challenges associated with sucharchitecture.</description><author>Adam Pardyl, Grzegorz Kurzejamski, Jan Olszewski, Tomasz Trzciński, Bartosz Zieliński</author><pubDate>Tue, 26 Nov 2024 17:28:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13353v2</guid></item><item><title>Accelerating Vision Diffusion Transformers with Skip Branches</title><link>http://arxiv.org/abs/2411.17616v1</link><description>Diffusion Transformers (DiT), an emerging image and video generation modelarchitecture, has demonstrated great potential because of its high generationquality and scalability properties. Despite the impressive performance, itspractical deployment is constrained by computational complexity and redundancyin the sequential denoising process. While feature caching across timesteps hasproven effective in accelerating diffusion models, its application to DiT islimited by fundamental architectural differences from U-Net-based approaches.Through empirical analysis of DiT feature dynamics, we identify thatsignificant feature variation between DiT blocks presents a key challenge forfeature reusability. To address this, we convert standard DiT into Skip-DiTwith skip branches to enhance feature smoothness. Further, we introduceSkip-Cache which utilizes the skip branches to cache DiT features acrosstimesteps at the inference time. We validated effectiveness of our proposal ondifferent DiT backbones for video and image generation, showcasing skipbranches to help preserve generation quality and achieve higher speedup.Experimental results indicate that Skip-DiT achieves a 1.5x speedup almost forfree and a 2.2x speedup with only a minor reduction in quantitative metrics.Code is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.</description><author>Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Tianlong Chen, Cheng Yu</author><pubDate>Tue, 26 Nov 2024 17:28:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17616v1</guid></item><item><title>GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering</title><link>http://arxiv.org/abs/2403.15651v3</link><description>In this paper, we present GaNI, a Global and Near-field Illumination-awareneural inverse rendering technique that can reconstruct geometry, albedo, androughness parameters from images of a scene captured with co-located light andcamera. Existing inverse rendering techniques with co-located light-camerafocus on single objects only, without modeling global illumination andnear-field lighting more prominent in scenes with multiple objects. Weintroduce a system that solves this problem in two stages; we first reconstructthe geometry powered by neural volumetric rendering NeuS, followed by inverseneural radiosity that uses the previously predicted geometry to estimate albedoand roughness. However, such a naive combination fails and we propose multipletechnical contributions that enable this two-stage approach. We observe thatNeuS fails to handle near-field illumination and strong specular reflectionsfrom the flashlight in a scene. We propose to implicitly model the effects ofnear-field illumination and introduce a surface angle loss function to handlespecular reflections. Similarly, we observe that invNeRad assumes constantillumination throughout the capture and cannot handle moving flashlights duringcapture. We propose a light position-aware radiance cache network andadditional smoothness priors on roughness to reconstruct reflectance.Experimental evaluation on synthetic and real data shows that our methodoutperforms the existing co-located light-camera-based inverse renderingtechniques. Our approach produces significantly better reflectance and slightlybetter geometry than capture strategies that do not require a dark room.</description><author>Jiaye Wu, Saeed Hadadan, Geng Lin, Matthias Zwicker, David Jacobs, Roni Sengupta</author><pubDate>Tue, 26 Nov 2024 17:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15651v3</guid></item><item><title>Automating Chapter-Level Classification for Electronic Theses and Dissertations</title><link>http://arxiv.org/abs/2411.17614v1</link><description>Traditional archival practices for describing electronic theses anddissertations (ETDs) rely on broad, high-level metadata schemes that fail tocapture the depth, complexity, and interdisciplinary nature of these longscholarly works. The lack of detailed, chapter-level content descriptionsimpedes researchers' ability to locate specific sections or themes, therebyreducing discoverability and overall accessibility. By providing chapter-levelmetadata information, we improve the effectiveness of ETDs as researchresources. This makes it easier for scholars to navigate them efficiently andextract valuable insights. The absence of such metadata further obstructsinterdisciplinary research by obscuring connections across fields, hinderingnew academic discoveries and collaboration. In this paper, we propose a machinelearning and AI-driven solution to automatically categorize ETD chapters. Thissolution is intended to improve discoverability and promote understanding ofchapters. Our approach enriches traditional archival practices by providingcontext-rich descriptions that facilitate targeted navigation and improvedaccess. We aim to support interdisciplinary research and make ETDs moreaccessible. By providing chapter-level classification labels and using them toindex in our developed prototype system, we make content in ETD chapters morediscoverable and usable for a diverse range of scholarly needs. Implementingthis AI-enhanced approach allows archives to serve researchers better, enablingefficient access to relevant information and supporting deeper engagement withETDs. This will increase the impact of ETDs as research tools, fosterinterdisciplinary exploration, and reinforce the role of archives in scholarlycommunication within the data-intensive academic landscape.</description><author>Bipasha Banerjee, William A. Ingram, Edward A. Fox</author><pubDate>Tue, 26 Nov 2024 17:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17614v1</guid></item><item><title>Modality-Incremental Learning with Disjoint Relevance Mapping Networks for Image-based Semantic Segmentation</title><link>http://arxiv.org/abs/2411.17610v1</link><description>In autonomous driving, environment perception has significantly advanced withthe utilization of deep learning techniques for diverse sensors such ascameras, depth sensors, or infrared sensors. The diversity in the sensor stackincreases the safety and contributes to robustness against adverse weather andlighting conditions. However, the variance in data acquired from differentsensors poses challenges. In the context of continual learning (CL),incremental learning is especially challenging for considerably large domainshifts, e.g. different sensor modalities. This amplifies the problem ofcatastrophic forgetting. To address this issue, we formulate the concept ofmodality-incremental learning and examine its necessity, by contrasting it withexisting incremental learning paradigms. We propose the use of a modifiedRelevance Mapping Network (RMN) to incrementally learn new modalities whilepreserving performance on previously learned modalities, in which relevancemaps are disjoint. Experimental results demonstrate that the prevention ofshared connections in this approach helps alleviate the problem of forgettingwithin the constraints of a strict continual learning framework.</description><author>Niharika Hegde, Shishir Muralidhara, René Schuster, Didier Stricker</author><pubDate>Tue, 26 Nov 2024 17:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17610v1</guid></item><item><title>Mixed-State Quantum Denoising Diffusion Probabilistic Model</title><link>http://arxiv.org/abs/2411.17608v1</link><description>Generative quantum machine learning has gained significant attention for itsability to produce quantum states with desired distributions. Among variousquantum generative models, quantum denoising diffusion probabilistic models(QuDDPMs) [Phys. Rev. Lett. 132, 100602 (2024)] provide a promising approachwith stepwise learning that resolves the training issues. However, therequirement of high-fidelity scrambling unitaries in QuDDPM poses a challengein near-term implementation. We propose the \textit{mixed-state quantumdenoising diffusion probabilistic model} (MSQuDDPM) to eliminate the need forscrambling unitaries. Our approach focuses on adapting the quantum noisechannels to the model architecture, which integrates depolarizing noisechannels in the forward diffusion process and parameterized quantum circuitswith projective measurements in the backward denoising steps. We also introduceseveral techniques to improve MSQuDDPM, including a cosine-exponent schedule ofnoise interpolation, the use of single-qubit random ancilla, andsuperfidelity-based cost functions to enhance the convergence. We evaluateMSQuDDPM on quantum ensemble generation tasks, demonstrating its successfulperformance.</description><author>Gino Kwun, Bingzhi Zhang, Quntao Zhuang</author><pubDate>Tue, 26 Nov 2024 17:20:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17608v1</guid></item><item><title>Scaling Speech-Text Pre-training with Synthetic Interleaved Data</title><link>http://arxiv.org/abs/2411.17607v1</link><description>Speech language models (SpeechLMs) accept speech input and produce speechoutput, allowing for more natural human-computer interaction compared totext-based large language models (LLMs). Traditional approaches for developingSpeechLMs are constrained by the limited availability of unsupervised speechdata and parallel speech-text data, which are significantly less abundant thantext pre-training data, thereby limiting their scalability as LLMs. We proposea novel approach to scaling speech-text pre-training by leveraging large-scalesynthetic interleaved data derived from text corpora, eliminating the need forparallel speech-text datasets. Our method efficiently constructs speech-textinterleaved data by sampling text spans from existing text corpora andsynthesizing corresponding speech spans using a text-to-token model, bypassingthe need to generate actual speech. We also employ a supervised speechtokenizer derived from an automatic speech recognition (ASR) model byincorporating a vector-quantized bottleneck into the encoder. This supervisedtraining approach results in discrete speech tokens with strong semanticpreservation even at lower sampling rates (e.g. 12.5Hz), while stillmaintaining speech reconstruction quality. Starting from a pre-trained languagemodel and scaling our pre-training to 1 trillion tokens (with 600B syntheticinterleaved speech-text data), we achieve state-of-the-art performance inspeech language modeling and spoken question answering, improving performanceon spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. Wefurther demonstrate that by fine-tuning the pre-trained model with speechdialogue data, we can develop an end-to-end spoken chatbot that achievescompetitive performance comparable to existing baselines in both conversationalabilities and speech quality, even operating exclusively in the speech domain.</description><author>Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang</author><pubDate>Tue, 26 Nov 2024 17:19:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17607v1</guid></item><item><title>HyperSeg: Towards Universal Visual Segmentation with Large Language Model</title><link>http://arxiv.org/abs/2411.17606v1</link><description>This paper aims to address universal segmentation for image and videoperception with the strong reasoning ability empowered by Visual Large LanguageModels (VLLMs). Despite significant progress in current unified segmentationmethods, limitations in adaptation to both image and video scenarios, as wellas the complex reasoning segmentation, make it difficult for them to handlevarious challenging instructions and achieve an accurate understanding offine-grained vision-language correlations. We propose HyperSeg, the firstVLLM-based universal segmentation model for pixel-level image and videoperception, encompassing generic segmentation tasks and more complex reasoningperception tasks requiring powerful reasoning abilities and world knowledge.Besides, to fully leverage the recognition capabilities of VLLMs and thefine-grained visual information, HyperSeg incorporates hybrid entityrecognition and fine-grained visual perceiver modules for various segmentationtasks. Combined with the temporal adapter, HyperSeg achieves a comprehensiveunderstanding of temporal information. Experimental results validate theeffectiveness of our insights in resolving universal image and videosegmentation tasks, including the more complex reasoning perception tasks. Ourcode is available.</description><author>Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Zheng Zhao, Jie Hu, Yujiu Yang</author><pubDate>Tue, 26 Nov 2024 17:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17606v1</guid></item><item><title>Distractor-free Generalizable 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2411.17605v1</link><description>We present DGGS, a novel framework addressing the previously unexploredchallenge of Distractor-free Generalizable 3D Gaussian Splatting (3DGS). Itaccomplishes two key objectives: fortifying generalizable 3DGS againstdistractor-laden data during both training and inference phases, whilesuccessfully extending cross-scene adaptation capabilities to conventionaldistractor-free approaches. To achieve these objectives, DGGS introduces ascene-agnostic reference-based mask prediction and refinement methodologyduring training phase, coupled with a training view selection strategy,effectively improving distractor prediction accuracy and training stability.Moreover, to address distractor-induced voids and artifacts during inferencestage, we propose a two-stage inference framework for better referenceselection based on the predicted distractor masks, complemented by a distractorpruning module to eliminate residual distractor effects. Extensivegeneralization experiments demonstrate DGGS's advantages under distractor-ladenconditions. Additionally, experimental results show that our scene-agnosticmask inference achieves accuracy comparable to scene-specific trained methods.Homepage is \url{https://github.com/bbbbby-99/DGGS}.</description><author>Yanqi Bao, Jing Liao, Jing Huo, Yang Gao</author><pubDate>Tue, 26 Nov 2024 17:17:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17605v1</guid></item><item><title>Making History Readable</title><link>http://arxiv.org/abs/2411.17600v1</link><description>The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP)hosts digital collections that offer our users access to a wide variety ofdocuments of historical and cultural importance. These collections are not onlyof academic importance but also provide our users with a glance at localhistorical events. Our DLP contains collections comprising digital objectsfeaturing complex layouts, faded imagery, and hard-to-read handwritten text,which makes providing online access to these materials challenging. To addressthese issues, we integrate AI into our DLP workflow and convert the text in thedigital objects into a machine-readable format. To enhance the user experiencewith our historical collections, we use custom AI agents for handwritingrecognition, text extraction, and large language models (LLMs) forsummarization. This poster highlights three collections focusing on handwrittenletters, newspapers, and digitized topographic maps. We discuss the challengeswith each collection and detail our approaches to address them. Our proposedmethods aim to enhance the user experience by making the contents in thesecollections easier to search and navigate.</description><author>Bipasha Banerjee, Jennifer Goyne, William A. Ingram</author><pubDate>Tue, 26 Nov 2024 17:06:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17600v1</guid></item><item><title>Agentic AI for Improving Precision in Identifying Contributions to Sustainable Development Goals</title><link>http://arxiv.org/abs/2411.17598v1</link><description>As research institutions increasingly commit to supporting the UnitedNations' Sustainable Development Goals (SDGs), there is a pressing need toaccurately assess their research output against these goals. Currentapproaches, primarily reliant on keyword-based Boolean search queries, conflateincidental keyword matches with genuine contributions, reducing retrievalprecision and complicating benchmarking efforts. This study investigates theapplication of autoregressive Large Language Models (LLMs) as evaluation agentsto identify relevant scholarly contributions to SDG targets in scholarlypublications. Using a dataset of academic abstracts retrieved via SDG-specifickeyword queries, we demonstrate that small, locally-hosted LLMs candifferentiate semantically relevant contributions to SDG targets from documentsretrieved due to incidental keyword matches, addressing the limitations oftraditional methods. By leveraging the contextual understanding of LLMs, thisapproach provides a scalable framework for improving SDG-related researchmetrics and informing institutional reporting.</description><author>William A. Ingram, Bipasha Banerjee, Edward A. Fox</author><pubDate>Tue, 26 Nov 2024 17:06:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17598v1</guid></item><item><title>Designing the virtual CAT: A digital tool for algorithmic thinking assessment in compulsory education</title><link>http://arxiv.org/abs/2408.01263v3</link><description>Algorithmic thinking (AT) is a critical skill in today's digital society, andit is indispensable not only in computer science-related fields but also ineveryday problem-solving. As a foundational component of digital education andliteracy, fostering AT skills is increasingly relevant for all students andshould become a standard part of compulsory education. However, successfullyintegrating AT into formal education requires effective teaching strategies androbust and scalable assessment procedures. In this paper, we present the designand development process of the virtual Cross Array Task (CAT), a digitaladaptation of an unplugged assessment activity aimed at evaluating algorithmicskills in Swiss compulsory education. The development process followediterative design cycles, incorporating expert evaluations to refine the tool'susability, accessibility and functionality. A participatory design study playeda dual role in shaping the platform. First, it gathered valuable insights fromend users, including students and teachers, to ensure the tool's relevance andpracticality in classroom settings. Second, it facilitated the collection andpreliminary analysis of data related to students' AT skills, providing aninitial evaluation of the tool's assessment capabilities across variousdevelopmental stages. This was achieved through a pilot study involving adiverse group of students aged 4 to 12, spanning preschool to lower secondaryschool levels. The resulting instrument features multilingual support andincludes both gesture-based and visual block-based programming interfaces,making it accessible to a broad range of learners. Findings from the pilotstudy demonstrate the platform's usability and accessibility, as well as itssuitability for assessing AT skills, with preliminary results showing itsability to cater to diverse age groups and educational contexts.</description><author>Giorgia Adorni, Alberto Piatti</author><pubDate>Tue, 26 Nov 2024 17:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01263v3</guid></item><item><title>Can artificial intelligence predict clinical trial outcomes?</title><link>http://arxiv.org/abs/2411.17595v1</link><description>The increasing complexity and cost of clinical trials, particularly in thecontext of oncology and advanced therapies, pose significant challenges fordrug development. This study evaluates the predictive capabilities of largelanguage models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinicaltrial outcomes. By leveraging a curated dataset of trials fromClinicalTrials.gov, we compare the models' performance using metrics includingbalanced accuracy, specificity, recall, and Matthews Correlation Coefficient(MCC). Results indicate that GPT-4o demonstrates robust performance in earlytrial phases, achieving high recall but facing limitations in specificity.Conversely, the HINT model excels in recognizing negative outcomes,particularly in later trial phases, offering a balanced approach across diverseendpoints. Oncology trials, characterized by high complexity, remainchallenging for all models. Additionally, trial duration and disease categoriesinfluence predictive performance, with longer durations and complex diseasessuch as neoplasms reducing accuracy. This study highlights the complementarystrengths of LLMs and HINT, providing insights into optimizing predictive toolsfor clinical trial design and risk management. Future advancements in LLMs areessential to address current gaps in handling negative outcomes and complexdomains.</description><author>Shuyi Jin, Lu Chen, Hongru Ding, Meijie Wang, Lun Yu</author><pubDate>Tue, 26 Nov 2024 17:05:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17595v1</guid></item><item><title>What Differentiates Educational Literature? A Multimodal Fusion Approach of Transformers and Computational Linguistics</title><link>http://arxiv.org/abs/2411.17593v1</link><description>The integration of new literature into the English curriculum remains achallenge since educators often lack scalable tools to rapidly evaluatereadability and adapt texts for diverse classroom needs. This study proposes toaddress this gap through a multimodal approach that combines transformer-basedtext classification with linguistic feature analysis to align texts with UK KeyStages. Eight state-of-the-art Transformers were fine-tuned on segmented textdata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,500 deep neural network topologies were searched for the classification oflinguistic characteristics, achieving an F1 score of 0.392. The fusion of thesemodalities shows a significant improvement, with every multimodal approachoutperforming all unimodal models. In particular, the ELECTRA Transformer fusedwith the neural network achieved an F1 score of 0.996. The proposed approach isfinally encapsulated in a stakeholder-facing web application, providingnon-technical stakeholder access to real-time insights on text complexity,reading difficulty, curriculum alignment, and recommendations for learning agerange. The application empowers data-driven decision making and reduces manualworkload by integrating AI-based recommendations into lesson planning forEnglish literature.</description><author>Jordan J. Bird</author><pubDate>Tue, 26 Nov 2024 17:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17593v1</guid></item><item><title>Learning Instance-Specific Parameters of Black-Box Models Using Differentiable Surrogates</title><link>http://arxiv.org/abs/2407.17530v2</link><description>Tuning parameters of a non-differentiable or black-box compute ischallenging. Existing methods rely mostly on random sampling or grid samplingfrom the parameter space. Further, with all the current methods, it is notpossible to supply any input specific parameters to the black-box. To the bestof our knowledge, for the first time, we are able to learn input-specificparameters for a black box in this work. As a test application, we choose apopular image denoising method BM3D as our black-box compute. Then, we use adifferentiable surrogate model (a neural network) to approximate the black-boxbehaviour. Next, another neural network is used in an end-to-end fashion tolearn input instance-specific parameters for the black-box. Motivated by prioradvances in surrogate-based optimization, we applied our method to theSmartphone Image Denoising Dataset (SIDD) and the Color Berkeley SegmentationDataset (CBSD68) for image denoising. The results are compelling, demonstratinga significant increase in PSNR and a notable improvement in SSIM nearing 0.93.Experimental results underscore the effectiveness of our approach in achievingsubstantial improvements in both model performance and optimization efficiency.For code and implementation details, please refer to our GitHub repository:https://github.com/arnisha-k/instance-specific-param</description><author>Arnisha Khondaker, Nilanjan Ray</author><pubDate>Tue, 26 Nov 2024 16:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17530v2</guid></item><item><title>VideoDirector: Precise Video Editing via Text-to-Video Models</title><link>http://arxiv.org/abs/2411.17592v1</link><description>Despite the typical inversion-then-editing paradigm using text-to-image (T2I)models has demonstrated promising results, directly extending it totext-to-video (T2V) models still suffers severe artifacts such as colorflickering and content distortion. Consequently, current video editing methodsprimarily rely on T2I models, which inherently lack temporal-coherencegenerative ability, often resulting in inferior editing results. In this paper,we attribute the failure of the typical editing paradigm to: 1) TightlySpatial-temporal Coupling. The vanilla pivotal-based inversion strategystruggles to disentangle spatial-temporal information in the video diffusionmodel; 2) Complicated Spatial-temporal Layout. The vanilla cross-attentioncontrol is deficient in preserving the unedited content. To address theselimitations, we propose a spatial-temporal decoupled guidance (STDG) andmulti-frame null-text optimization strategy to provide pivotal temporal cuesfor more precise pivotal inversion. Furthermore, we introduce a self-attentioncontrol strategy to maintain higher fidelity for precise partial contentediting. Experimental results demonstrate that our method (termedVideoDirector) effectively harnesses the powerful temporal generationcapabilities of T2V models, producing edited videos with state-of-the-artperformance in accuracy, motion smoothness, realism, and fidelity to uneditedcontent.</description><author>Yukun Wang, Longguang Wang, Zhiyuan Ma, Qibin Hu, Kai Xu, Yulan Guo</author><pubDate>Tue, 26 Nov 2024 16:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17592v1</guid></item><item><title>LRSAA: Large-scale Remote Sensing Image Target Recognition and Automatic Annotation</title><link>http://arxiv.org/abs/2411.15808v2</link><description>This paper presents a method for object recognition and automatic labeling inlarge-area remote sensing images called LRSAA. The method integrates YOLOv11and MobileNetV3-SSD object detection algorithms through ensemble learning toenhance model performance. Furthermore, it employs Poisson disk samplingsegmentation techniques and the EIOU metric to optimize the training andinference processes of segmented images, followed by the integration ofresults. This approach not only reduces the demand for computational resourcesbut also achieves a good balance between accuracy and speed. The source codefor this project has been made publicly available onhttps://github.com/anaerovane/LRSAA.</description><author>Wuzheng Dong, Yujuan Zhu</author><pubDate>Tue, 26 Nov 2024 16:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15808v2</guid></item><item><title>Pre-training for Action Recognition with Automatically Generated Fractal Datasets</title><link>http://arxiv.org/abs/2411.17584v1</link><description>In recent years, interest in synthetic data has grown, particularly in thecontext of pre-training the image modality to support a range of computervision tasks, including object classification, medical imaging etc. Previouswork has demonstrated that synthetic samples, automatically produced by variousgenerative processes, can replace real counterparts and yield strong visualrepresentations. This approach resolves issues associated with real data suchas collection and labeling costs, copyright and privacy. We extend this trend to the video domain applying it to the task of actionrecognition. Employing fractal geometry, we present methods to automaticallyproduce large-scale datasets of short synthetic video clips, which can beutilized for pre-training neural models. The generated video clips arecharacterized by notable variety, stemmed by the innate ability of fractals togenerate complex multi-scale structures. To narrow the domain gap, we furtheridentify key properties of real videos and carefully emulate them duringpre-training. Through thorough ablations, we determine the attributes thatstrengthen downstream results and offer general guidelines for pre-trainingwith synthetic videos. The proposed approach is evaluated by fine-tuningpre-trained models on established action recognition datasets HMDB51 and UCF101as well as four other video benchmarks related to group action recognition,fine-grained action recognition and dynamic scenes. Compared to standardKinetics pre-training, our reported results come close and are even superior ona portion of downstream datasets. Code and samples of synthetic videos areavailable at https://github.com/davidsvy/fractal_video .</description><author>Davyd Svyezhentsev, George Retsinas, Petros Maragos</author><pubDate>Tue, 26 Nov 2024 16:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17584v1</guid></item><item><title>From Fairness to Infinity: Outcome-Indistinguishable (Omni)Prediction in Evolving Graphs</title><link>http://arxiv.org/abs/2411.17582v1</link><description>Professional networks provide invaluable entree to opportunity throughreferrals and introductions. A rich literature shows they also serve toentrench and even exacerbate a status quo of privilege and disadvantage. Hiringplatforms, equipped with the ability to nudge link formation, provide atantalizing opening for beneficial structural change. We anticipate that key tothis prospect will be the ability to estimate the likelihood of edge formationin an evolving graph. Outcome-indistinguishable prediction algorithms ensurethat the modeled world is indistinguishable from the real world by a family ofstatistical tests. Omnipredictors ensure that predictions can be post-processedto yield loss minimization competitive with respect to a benchmark class ofpredictors for many losses simultaneously, with appropriate post-processing. Webegin by observing that, by combining a slightly modified form of the onlineK29 star algorithm of Vovk (2007) with basic facts from the theory ofreproducing kernel Hilbert spaces, one can derive simple and efficient onlinealgorithms satisfying outcome indistinguishability and omniprediction, withguarantees that improve upon, or are complementary to, those currently known.This is of independent interest. We apply these techniques to evolving graphs,obtaining online outcome-indistinguishable omnipredictors for rich -- possiblyinfinite -- sets of distinguishers that capture properties of pairs of nodes,and their neighborhoods. This yields, inter alia, multicalibrated predictionsof edge formation with respect to pairs of demographic groups, and the abilityto simultaneously optimize loss as measured by a variety of social welfarefunctions.</description><author>Cynthia Dwork, Chris Hays, Nicole Immorlica, Juan C. Perdomo, Pranay Tankala</author><pubDate>Tue, 26 Nov 2024 16:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17582v1</guid></item><item><title>Revisiting Point Cloud Completion: Are We Ready For The Real-World?</title><link>http://arxiv.org/abs/2411.17580v1</link><description>Point clouds acquired in constrained and challenging real-world settings areincomplete, non-uniformly sparse, or both. These obstacles present acutechallenges for a vital task - point cloud completion. Using tools fromAlgebraic Topology and Persistent Homology ($\mathcal{PH}$), we demonstratethat current benchmark synthetic point clouds lack rich topological featuresthat are important constituents of point clouds captured in realistic settings.To facilitate research in this direction, we contribute the first real-worldindustrial point cloud dataset for point cloud completion, RealPC - a diverseset of rich and varied point clouds, consisting of $\sim$ 40,000 pairs across21 categories of industrial structures in railway establishments. Our benchmarkresults on several strong baselines reveal a striking observation - theexisting methods are tailored for synthetic datasets and fail miserably inreal-world settings. Building on our observation that RealPC consists ofseveral 0 and 1-dimensional $\mathcal{PH}$-based topological features, wedemonstrate the potential of integrating Homology-based topological priors intoexisting works. More specifically, we present how 0-dimensional $\mathcal{PH}$priors, which extract the global topology of a complete shape in the form of a3-D skeleton, can assist a model in generating topologically-consistentcomplete shapes.</description><author>Stuti Pathak, Prashant Kumar, Nicholus Mboga, Gunther Steenackers, Rudi Penne</author><pubDate>Tue, 26 Nov 2024 16:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17580v1</guid></item><item><title>PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning</title><link>http://arxiv.org/abs/2402.17589v2</link><description>Recently, the usage of Contrastive Representation Learning (CRL) as apre-training technique improves the performance of learning with noisy labels(LNL) methods. However, instead of pre-training, when trivially combining CRLloss with LNL methods as an end-to-end framework, the empirical experimentsshow severe degeneration of the performance. We verify through experiments thatthis issue is caused by optimization conflicts of losses and propose anend-to-end \textbf{PLReMix} framework by introducing a Pseudo-Label Relaxed(PLR) contrastive loss. This PLR loss constructs a reliable negative set ofeach sample by filtering out its inappropriate negative pairs, alleviating theloss conflicts by trivially combining these losses. The proposed PLR loss ispluggable and we have integrated it into other LNL methods, observing theirimproved performance. Furthermore, a two-dimensional Gaussian Mixture Model isadopted to distinguish clean and noisy samples by leveraging semanticinformation and model outputs simultaneously. Experiments on multiple benchmarkdatasets demonstrate the effectiveness of the proposed method. Code isavailable at \url{https://github.com/lxysl/PLReMix}.</description><author>Xiaoyu Liu, Beitong Zhou, Zuogong Yue, Cheng Cheng</author><pubDate>Tue, 26 Nov 2024 16:45:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17589v2</guid></item><item><title>Functionality understanding and segmentation in 3D scenes</title><link>http://arxiv.org/abs/2411.16310v2</link><description>Understanding functionalities in 3D scenes involves interpreting naturallanguage descriptions to locate functional interactive objects, such as handlesand buttons, in a 3D environment. Functionality understanding is highlychallenging, as it requires both world knowledge to interpret language andspatial perception to identify fine-grained objects. For example, given a tasklike 'turn on the ceiling light', an embodied AI agent must infer that it needsto locate the light switch, even though the switch is not explicitly mentionedin the task description. To date, no dedicated methods have been developed forthis problem. In this paper, we introduce Fun3DU, the first approach designedfor functionality understanding in 3D scenes. Fun3DU uses a language model toparse the task description through Chain-of-Thought reasoning in order toidentify the object of interest. The identified object is segmented acrossmultiple views of the captured scene by using a vision and language model. Thesegmentation results from each view are lifted in 3D and aggregated into thepoint cloud using geometric information. Fun3DU is training-free, relyingentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the mostrecent and only dataset to benchmark this task, which comprises over 3000 taskdescriptions on 230 scenes. Our method significantly outperformsstate-of-the-art open-vocabulary 3D segmentation approaches. Project page:https://jcorsetti.github.io/fun3du</description><author>Jaime Corsetti, Francesco Giuliari, Alice Fasoli, Davide Boscaini, Fabio Poiesi</author><pubDate>Tue, 26 Nov 2024 16:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16310v2</guid></item><item><title>A Distractor-Aware Memory for Visual Object Tracking with SAM2</title><link>http://arxiv.org/abs/2411.17576v1</link><description>Memory-based trackers are video object segmentation methods that form thetarget model by concatenating recently tracked frames into a memory buffer andlocalize the target by attending the current image to the buffered frames.While already achieving top performance on many benchmarks, it was the recentrelease of SAM2 that placed memory-based trackers into focus of the visualobject tracking community. Nevertheless, modern trackers still struggle in thepresence of distractors. We argue that a more sophisticated memory model isrequired, and propose a new distractor-aware memory model for SAM2 and anintrospection-based update strategy that jointly addresses the segmentationaccuracy as well as tracking robustness. The resulting tracker is denoted asSAM2.1++. We also propose a new distractor-distilled DiDi dataset to study thedistractor problem better. SAM2.1++ outperforms SAM2.1 and related SAM memoryextensions on seven benchmarks and sets a solid new state-of-the-art on six ofthem.</description><author>Jovana Videnovic, Alan Lukezic, Matej Kristan</author><pubDate>Tue, 26 Nov 2024 16:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17576v1</guid></item><item><title>Fast and Robust Phase Retrieval via Deep Expectation-Consistent Approximation</title><link>http://arxiv.org/abs/2407.09687v2</link><description>Accurately recovering images from phaseless measurements is a challenging andlong-standing problem. In this work, we present "deepECpr," which combinesexpectation-consistent (EC) approximation with deep denoising networks tosurpass state-of-the-art phase-retrieval methods in both speed and accuracy. Inaddition to applying EC in a non-traditional manner, deepECpr includes a novelstochastic damping scheme that is inspired by recent diffusion methods. Likeexisting phase-retrieval methods based on plug-and-play priors, regularizationby denoising, or diffusion, deepECpr iterates a denoising stage with ameasurement-exploitation stage. But unlike existing methods, deepECpr requiresfar fewer denoiser calls. We compare deepECpr to the state-of-the-art prDeep(Metzler et al., 2018), Deep-ITA (Wang et al., 2020), DOLPH (Shoushtari et al.,2023), and Diffusion Posterior Sampling (Chung et al., 2023) methods for noisyphase-retrieval of color, natural, and unnatural grayscale images onoversampled-Fourier and coded-diffraction-pattern measurements and findimprovements in both PSNR and SSIM with significantly fewer denoiser calls.</description><author>Saurav K. Shastri, Philip Schniter</author><pubDate>Tue, 26 Nov 2024 16:38:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09687v2</guid></item><item><title>Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation</title><link>http://arxiv.org/abs/2411.16638v2</link><description>Modern LLMs can now produce highly readable abstractive summaries, to thepoint where traditional automated metrics for evaluating summary quality, suchas ROUGE, have become saturated. However, LLMs still sometimes introduceunwanted content into summaries, i.e., information inconsistent with orunsupported by their source. Measuring the occurrence of these often subtle``hallucinations'' automatically has proved to be challenging. This in turn hasmotivated development of a variety of metrics intended to measure the factualconsistency of generated summaries against their source. But are theseapproaches measuring what they purport to do? In this work, we stress-testautomatic factuality metrics. Specifically, we investigate whether and to whatdegree superficial attributes of summary texts suffice to predict``factuality'', finding that a (supervised) model using only such shallowfeatures is reasonably competitive with SOTA factuality scoring methods. Wethen evaluate how factuality metrics respond to factual corrections ininconsistent summaries and find that only a few show meaningful improvements.In contrast, some metrics are more sensitive to benign, non-factual edits.Motivated by these insights, we show that one can ``game'' (most) automaticfactuality metrics, i.e., reliably inflate ``factuality'' scores by appendinginnocuous sentences to generated summaries.Taken together, our results raisequestions about the degree to which we should rely on existing automatedfactuality metrics and what exactly we want ``factuality metrics'' to measure.</description><author>Sanjana Ramprasad, Byron C. Wallace</author><pubDate>Tue, 26 Nov 2024 16:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16638v2</guid></item><item><title>Prompting Visual-Language Models for Dynamic Facial Expression Recognition</title><link>http://arxiv.org/abs/2308.13382v3</link><description>This paper presents a novel visual-language model called DFER-CLIP, which isbased on the CLIP model and designed for in-the-wild Dynamic Facial ExpressionRecognition (DFER). Specifically, the proposed DFER-CLIP consists of a visualpart and a textual part. For the visual part, based on the CLIP image encoder,a temporal model consisting of several Transformer encoders is introduced forextracting temporal facial expression features, and the final feature embeddingis obtained as a learnable "class" token. For the textual part, we use asinputs textual descriptions of the facial behaviour that is related to theclasses (facial expressions) that we are interested in recognising -- thosedescriptions are generated using large language models, like ChatGPT. This, incontrast to works that use only the class names and more accurately capturesthe relationship between them. Alongside the textual description, we introducea learnable token which helps the model learn relevant context information foreach expression during training. Extensive experiments demonstrate theeffectiveness of the proposed method and show that our DFER-CLIP also achievesstate-of-the-art results compared with the current supervised DFER methods onthe DFEW, FERV39k, and MAFW benchmarks. Code is publicly available athttps://github.com/zengqunzhao/DFER-CLIP.</description><author>Zengqun Zhao, Ioannis Patras</author><pubDate>Tue, 26 Nov 2024 16:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13382v3</guid></item><item><title>Uncertainty quantification for White Matter Hyperintensity segmentation detects silent failures and improves automated Fazekas quantification</title><link>http://arxiv.org/abs/2411.17571v1</link><description>White Matter Hyperintensities (WMH) are key neuroradiological markers ofsmall vessel disease present in brain MRI. Assessment of WMH is important inresearch and clinics. However, WMH are challenging to segment due to their highvariability in shape, location, size, poorly defined borders, and similarintensity profile to other pathologies (e.g stroke lesions) and artefacts (e.ghead motion). In this work, we apply the most effective techniques foruncertainty quantification (UQ) in segmentation to the WMH segmentation taskacross multiple test-time data distributions. We find a combination ofStochastic Segmentation Networks with Deep Ensembles yields the highest Diceand lowest Absolute Volume Difference % (AVD) score on in-domain andout-of-distribution data. We demonstrate the downstream utility of UQ,proposing a novel method for classification of the clinical Fazekas score usingspatial features extracted for WMH segmentation and UQ maps. We show thatincorporating WMH uncertainty information improves Fazekas classificationperformance and calibration, with median class balanced accuracy forclassification models with (UQ and spatial WMH features)/(spatial WMHfeatures)/(WMH volume only) of 0.71/0.66/0.60 in the Deep WMH and0.82/0.77/0.73 in the Periventricular WMH regions respectively. We demonstratethat stochastic UQ techniques with high sample diversity can improve thedetection of poor quality segmentations. Finally, we qualitatively analyse thesemantic information captured by UQ techniques and demonstrate that uncertaintycan highlight areas where there is ambiguity between WMH and stroke lesions,while identifying clusters of small WMH in deep white matter unsegmented by themodel.</description><author>Ben Philps, Maria del C. Valdes Hernandez, Chen Qin, Una Clancy, Eleni Sakka, Susana Munoz Maniega, Mark E. Bastin, Angela C. C. Jochems, Joanna M. Wardlaw, Miguel O. Bernabeu, Alzheimers Disease Neuroimaging Initiative</author><pubDate>Tue, 26 Nov 2024 16:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17571v1</guid></item><item><title>Learning Explainable Treatment Policies with Clinician-Informed Representations: A Practical Approach</title><link>http://arxiv.org/abs/2411.17570v1</link><description>Digital health interventions (DHIs) and remote patient monitoring (RPM) haveshown great potential in improving chronic disease management throughpersonalized care. However, barriers like limited efficacy and workloadconcerns hinder adoption of existing DHIs; while limited sample sizes and lackof interpretability limit the effectiveness and adoption of purely black-boxalgorithmic DHIs. In this paper, we address these challenges by developing apipeline for learning explainable treatment policies for RPM-enabled DHIs. Weapply our approach in the real-world setting of RPM using a DHI to improveglycemic control of youth with type 1 diabetes. Our main contribution is toreveal the importance of clinical domain knowledge in developing state andaction representations for effective, efficient, and interpretable targetingpolicies. We observe that policies learned from clinician-informedrepresentations are significantly more efficacious and efficient than policieslearned from black-box representations. This work emphasizes the importance ofcollaboration between ML researchers and clinicians for developing effectiveDHIs in the real world.</description><author>Johannes O. Ferstad, Emily B. Fox, David Scheinker, Ramesh Johari</author><pubDate>Tue, 26 Nov 2024 16:32:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17570v1</guid></item><item><title>Improving the Convergence Rates of Forward Gradient Descent with Repeated Sampling</title><link>http://arxiv.org/abs/2411.17567v1</link><description>Forward gradient descent (FGD) has been proposed as a biologically moreplausible alternative of gradient descent as it can be computed withoutbackward pass. Considering the linear model with $d$ parameters, previous workhas found that the prediction error of FGD is, however, by a factor $d$ slowerthan the prediction error of stochastic gradient descent (SGD). In this paperwe show that by computing $\ell$ FGD steps based on each training sample, thissuboptimality factor becomes $d/(\ell \wedge d)$ and thus the suboptimality ofthe rate disappears if $\ell \gtrsim d.$ We also show that FGD with repeatedsampling can adapt to low-dimensional structure in the input distribution. Themain mathematical challenge lies in controlling the dependencies arising fromthe repeated sampling process.</description><author>Niklas Dexheimer, Johannes Schmidt-Hieber</author><pubDate>Tue, 26 Nov 2024 16:28:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17567v1</guid></item><item><title>Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024 SSA and Pediatrics</title><link>http://arxiv.org/abs/2411.15872v2</link><description>Identifying key pathological features in brain MRIs is crucial for thelong-term survival of glioma patients. However, manual segmentation istime-consuming, requiring expert intervention and is susceptible to humanerror. Therefore, significant research has been devoted to developing machinelearning methods that can accurately segment tumors in 3D multimodal brain MRIscans. Despite their progress, state-of-the-art models are often limited by thedata they are trained on, raising concerns about their reliability when appliedto diverse populations that may introduce distribution shifts. Such shifts canstem from lower quality MRI technology (e.g., in sub-Saharan Africa) orvariations in patient demographics (e.g., children). The BraTS-2024 challengeprovides a platform to address these issues. This study presents ourmethodology for segmenting tumors in the BraTS-2024 SSA and Pediatric Tumorstasks using MedNeXt, comprehensive model ensembling, and thoroughpostprocessing. Our approach demonstrated strong performance on the unseenvalidation set, achieving an average Dice Similarity Coefficient (DSC) of 0.896on the BraTS-2024 SSA dataset and an average DSC of 0.830 on the BraTSPediatric Tumor dataset. Additionally, our method achieved an average HausdorffDistance (HD95) of 14.682 on the BraTS-2024 SSA dataset and an average HD95 of37.508 on the BraTS Pediatric dataset. Our GitHub repository can be accessedhere: Project Repository :https://github.com/python-arch/BioMbz-Optimizing-Brain-Tumor-Segmentation-with-MedNeXt-BraTS-2024-SSA-and-Pediatrics</description><author>Sarim Hashmi, Juan Lugo, Abdelrahman Elsayed, Dinesh Saggurthi, Mohammed Elseiagy, Alikhan Nurkamal, Jaskaran Walia, Fadillah Adamsyah Maani, Mohammad Yaqub</author><pubDate>Tue, 26 Nov 2024 16:24:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15872v2</guid></item><item><title>CatNet: Effective FDR Control in LSTM with Gaussian Mirrors and SHAP Feature Importance</title><link>http://arxiv.org/abs/2411.16666v2</link><description>We introduce CatNet, an algorithm that effectively controls False DiscoveryRate (FDR) and selects significant features in LSTM with the Gaussian Mirror(GM) method. To evaluate the feature importance of LSTM in time series, weintroduce a vector of the derivative of the SHapley Additive exPlanations(SHAP) to measure feature importance. We also propose a new kernel-baseddependence measure to avoid multicollinearity in the GM algorithm, to make arobust feature selection with controlled FDR. We use simulated data to evaluateCatNet's performance in both linear models and LSTM models with different linkfunctions. The algorithm effectively controls the FDR while maintaining a highstatistical power in all cases. We also evaluate the algorithm's performance indifferent low-dimensional and high-dimensional cases, demonstrating itsrobustness in various input dimensions. To evaluate CatNet's performance inreal world applications, we construct a multi-factor investment portfolio toforecast the prices of S\&amp;P 500 index components. The results demonstrate thatour model achieves superior predictive accuracy compared to traditional LSTMmodels without feature selection and FDR control. Additionally, CatNeteffectively captures common market-driving features, which helps informeddecision-making in financial markets by enhancing the interpretability ofpredictions. Our study integrates of the Gaussian Mirror algorithm with LSTMmodels for the first time, and introduces SHAP values as a new featureimportance metric for FDR control methods, marking a significant advancement infeature selection and error control for neural networks.</description><author>Jiaan Han, Junxiao Chen, Yanzhe Fu</author><pubDate>Tue, 26 Nov 2024 16:23:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16666v2</guid></item><item><title>Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey</title><link>http://arxiv.org/abs/2411.17558v1</link><description>Visual Question Answering (VQA) is a challenge task that combines naturallanguage processing and computer vision techniques and gradually becomes abenchmark test task in multimodal large language models (MLLMs). The goal ofour survey is to provide an overview of the development of VQA and a detaileddescription of the latest models with high timeliness. This survey gives anup-to-date synthesis of natural language understanding of images and text, aswell as the knowledge reasoning module based on image-question information onthe core VQA tasks. In addition, we elaborate on recent advances in extractingand fusing modal information with vision-language pretraining models andmultimodal large language models in VQA. We also exhaustively review theprogress of knowledge reasoning in VQA by detailing the extraction of internalknowledge and the introduction of external knowledge. Finally, we present thedatasets of VQA and different evaluation metrics and discuss possibledirections for future work.</description><author>Jiayi Kuang, Jingyou Xie, Haohao Luo, Ronghao Li, Zhe Xu, Xianfeng Cheng, Yinghui Li, Xika Lin, Ying Shen</author><pubDate>Tue, 26 Nov 2024 16:21:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17558v1</guid></item><item><title>A Bilayer Segmentation-Recombination Network for Accurate Segmentation of Overlapping C. elegans</title><link>http://arxiv.org/abs/2411.17557v1</link><description>Caenorhabditis elegans (C. elegans) is an excellent model organism because ofits short lifespan and high degree of homology with human genes, and it hasbeen widely used in a variety of human health and disease models. However, thesegmentation of C. elegans remains challenging due to the following reasons: 1)the activity trajectory of C. elegans is uncontrollable, and multiple nematodesoften overlap, resulting in blurred boundaries of C. elegans. This makes itimpossible to clearly study the life trajectory of a certain nematode; and 2)in the microscope images of overlapping C. elegans, the translucent tissues atthe edges obscure each other, leading to inaccurate boundary segmentation. Tosolve these problems, a Bilayer Segmentation-Recombination Network (BR-Net) forthe segmentation of C. elegans instances is proposed. The network consists ofthree parts: A Coarse Mask Segmentation Module (CMSM), a Bilayer SegmentationModule (BSM), and a Semantic Consistency Recombination Module (SCRM). The CMSMis used to extract the coarse mask, and we introduce a Unified Attention Module(UAM) in CMSM to make CMSM better aware of nematode instances. The BilayerSegmentation Module (BSM) segments the aggregated C. elegans into overlappingand non-overlapping regions. This is followed by integration by the SCRM, wheresemantic consistency regularization is introduced to segment nematode instancesmore accurately. Finally, the effectiveness of the method is verified on the C.elegans dataset. The experimental results show that BR-Net exhibits goodcompetitiveness and outperforms other recently proposed instance segmentationmethods in processing C. elegans occlusion images.</description><author>Mengqian Dinga, Jun Liua, Yang Luo, Jinshan Tang</author><pubDate>Tue, 26 Nov 2024 16:18:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17557v1</guid></item><item><title>TAFM-Net: A Novel Approach to Skin Lesion Segmentation Using Transformer Attention and Focal Modulation</title><link>http://arxiv.org/abs/2411.17556v1</link><description>Incorporating modern computer vision techniques into clinical protocols showspromise in improving skin lesion segmentation. The U-Net architecture has beena key model in this area, iteratively improved to address challenges arisingfrom the heterogeneity of dermatologic images due to varying clinical settings,lighting, patient attributes, and hair density. To further improve skin lesionsegmentation, we developed TAFM-Net, an innovative model leveragingself-adaptive transformer attention (TA) coupled with focal modulation (FM).Our model integrates an EfficientNetV2B1 encoder, which employs TA to enhancespatial and channel-related saliency, while a densely connected decoderintegrates FM within skip connections, enhancing feature emphasis, segmentationperformance, and interpretability crucial for medical image analysis. A noveldynamic loss function amalgamates region and boundary information, guidingeffective model training. Our model achieves competitive performance, withJaccard coefficients of 93.64\%, 86.88\% and 92.88\% in the ISIC2016, ISIC2017and ISIC2018 datasets, respectively, demonstrating its potential in real-worldscenarios.</description><author>Tariq M Khan, Dawn Lin, Shahzaib Iqbal, Eirk Meijering</author><pubDate>Tue, 26 Nov 2024 16:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17556v1</guid></item><item><title>Multiscale spatiotemporal heterogeneity analysis of bike-sharing system's self-loop phenomenon: Evidence from Shanghai</title><link>http://arxiv.org/abs/2411.17555v1</link><description>Bike-sharing is an environmentally friendly shared mobility mode, but itsself-loop phenomenon, where bikes are returned to the same station afterseveral time usage, significantly impacts equity in accessing its services.Therefore, this study conducts a multiscale analysis with a spatialautoregressive model and double machine learning framework to assesssocioeconomic features and geospatial location's impact on the self-loopphenomenon at metro stations and street scales. The results reveal thatbike-sharing self-loop intensity exhibits significant spatial lag effect atstreet scale and is positively associated with residential land use. Marginaltreatment effects of residential land use is higher on streets with middle-agedresidents, high fixed employment, and low car ownership. The multimodal publictransit condition reveals significant positive marginal treatment effects atboth scales. To enhance bike-sharing cooperation, we advocate augmentingbicycle availability in areas with high metro usage and low bus coverage,alongside implementing adaptable redistribution strategies.</description><author>Yichen Wang, Qing Yu, Yancun Song, Quan Yuan, Chao Yang, Chengcheng Yu</author><pubDate>Tue, 26 Nov 2024 16:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17555v1</guid></item><item><title>S-CFE: Simple Counterfactual Explanations</title><link>http://arxiv.org/abs/2410.15723v2</link><description>We study the problem of finding optimal sparse, manifold-alignedcounterfactual explanations for classifiers. Canonically, this can beformulated as an optimization problem with multiple non-convex components,including classifier loss functions and manifold alignment (or\emph{plausibility}) metrics. The added complexity of enforcing\emph{sparsity}, or shorter explanations, complicates the problem further.Existing methods often focus on specific models and plausibility measures,relying on convex $\ell_1$ regularizers to enforce sparsity. In this paper, wetackle the canonical formulation using the accelerated proximal gradient (APG)method, a simple yet efficient first-order procedure capable of handling smoothnon-convex objectives and non-smooth $\ell_p$ (where $0 \leq p &lt; 1$)regularizers. This enables our approach to seamlessly incorporate variousclassifiers and plausibility measures while producing sparser solutions. Ouralgorithm only requires differentiable data-manifold regularizers and supportsbox constraints for bounded feature ranges, ensuring the generatedcounterfactuals remain \emph{actionable}. Finally, experiments on real-worlddatasets demonstrate that our approach effectively produces sparse,manifold-aligned counterfactual explanations while maintaining proximity to thefactual data and computational efficiency.</description><author>Shpresim Sadiku, Moritz Wagner, Sai Ganesh Nagarajan, Sebastian Pokutta</author><pubDate>Tue, 26 Nov 2024 16:17:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15723v2</guid></item><item><title>Navigating Spatial Inequities in Freight Truck Crash Severity via Counterfactual Inference in Los Angeles</title><link>http://arxiv.org/abs/2411.17554v1</link><description>Freight truck-related crashes pose significant challenges, leading tosubstantial economic losses, injuries, and fatalities, with pronounced spatialdisparities across different regions. This study adopts a transport geographyperspective to examine spatial justice concerns by employing deepcounterfactual inference models to analyze how socioeconomic disparities, roadinfrastructure, and environmental conditions influence the geographicaldistribution and severity of freight truck crashes. By integrating road networkdatasets, socioeconomic attributes, and crash records from the Los Angelesmetropolitan area, this research provides a nuanced spatial analysis of howdifferent communities are disproportionately impacted. The results revealsignificant spatial disparities in crash severity across areas with varyingpopulation densities, income levels, and minority populations, highlighting thepivotal role of infrastructural and environmental improvements in mitigatingthese disparities. The findings offer insights into targeted, location-specificpolicy interventions, suggesting enhancements in road infrastructure, lighting,and traffic control systems, particularly in low-income andminority-concentrated areas. This research contributes to the literature ontransport geography and spatial equity by providing data-driven insights intoeffective measures for reducing spatial injustices associated with freighttruck-related crashes.</description><author>Yichen Wang, Hao Yin, Yifan Yang, Chenyang Zhao, Siqin Wang</author><pubDate>Tue, 26 Nov 2024 16:15:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17554v1</guid></item><item><title>Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach</title><link>http://arxiv.org/abs/2410.10674v2</link><description>Deep reinforcement learning agents achieve state-of-the-art performance in awide range of simulated control tasks. However, successful applications toreal-world problems remain limited. One reason for this dichotomy is becausethe learnt policies are not robust to observation noise or adversarial attacks.In this paper, we investigate the robustness of deep RL policies to a singlesmall state perturbation in deterministic continuous control tasks. Wedemonstrate that RL policies can be deterministically chaotic, as smallperturbations to the system state have a large impact on subsequent state andreward trajectories. This unstable non-linear behaviour has two consequences:first, inaccuracies in sensor readings, or adversarial attacks, can causesignificant performance degradation; second, even policies that show robustperformance in terms of rewards may have unpredictable behaviour in practice.These two facets of chaos in RL policies drastically restrict the applicationof deep RL to real-world problems. To address this issue, we propose animprovement on the successful Dreamer V3 architecture, implementing MaximalLyapunov Exponent regularisation. This new approach reduces the chaotic statedynamics, rendering the learnt policies more resilient to sensor noise oradversarial attacks and thereby improving the suitability of deep reinforcementlearning for real-world applications.</description><author>Rory Young, Nicolas Pugeault</author><pubDate>Tue, 26 Nov 2024 16:10:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10674v2</guid></item><item><title>Rapid Deployment of Domain-specific Hyperspectral Image Processors with Application to Autonomous Driving</title><link>http://arxiv.org/abs/2411.17543v1</link><description>The article discusses the use of low cost System-On-Module (SOM) platformsfor the implementation of efficient hyperspectral imaging (HSI) processors forapplication in autonomous driving. The work addresses the challenges of shapingand deploying multiple layer fully convolutional networks (FCN) forlow-latency, on-board image semantic segmentation using resource- andpower-constrained processing devices. The paper describes in detail the stepsfollowed to redesign and customize a successfully trained HSI segmentationlightweight FCN that was previously tested on a high-end heterogeneousmultiprocessing system-on-chip (MPSoC) to accommodate it to the constraintsimposed by a low-cost SOM. This SOM features a lower-end but much cheaper MPSoCsuitable for the deployment of automatic driving systems (ADS). In particularthe article reports the data- and hardware-specific quantization techniquesutilized to fit the FCN into a commercial fixed-point programmable AIcoprocessor IP, and proposes a full customized post-training quantizationscheme to reduce computation and storage costs without compromisingsegmentation accuracy.</description><author>Jon Gutiérrez-Zaballa, Koldo Basterretxea, Javier Echanobe, Óscar Mata-Carballeira, M. Victoria Martínez</author><pubDate>Tue, 26 Nov 2024 16:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17543v1</guid></item><item><title>Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression</title><link>http://arxiv.org/abs/2211.07484v8</link><description>We consider contextual bandits with linear constraints (CBwLC), a variant ofcontextual bandits in which the algorithm consumes multiple resources subjectto linear constraints on total consumption. This problem generalizes contextualbandits with knapsacks (CBwK), allowing for packing and covering constraints,as well as positive and negative resource consumption. We provide the firstalgorithm for CBwLC (or CBwK) that is based on regression oracles. Thealgorithm is simple, computationally efficient, and statistically optimal undermild assumptions. Further, we provide the first vanishing-regret guarantees forCBwLC (or CBwK) that extend beyond the stochastic environment. We side-stepstrong impossibility results from prior work by identifying a weaker (and,arguably, fairer) benchmark to compare against. Our algorithm builds onLagrangeBwK (Immorlica et al., FOCS 2019), a Lagrangian-based technique forCBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-basedtechnique for contextual bandits. Our analysis leverages the inherentmodularity of both techniques.</description><author>Aleksandrs Slivkins, Xingyu Zhou, Karthik Abinav Sankararaman, Dylan J. Foster</author><pubDate>Tue, 26 Nov 2024 16:01:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07484v8</guid></item><item><title>Towards safe Bayesian optimization with Wiener kernel regression</title><link>http://arxiv.org/abs/2411.02253v2</link><description>Bayesian Optimization (BO) is a data-driven strategy forminimizing/maximizing black-box functions based on probabilistic surrogatemodels. In the presence of safety constraints, the performance of BO cruciallyrelies on tight probabilistic error bounds related to the uncertaintysurrounding the surrogate model. For the case of Gaussian Process surrogatesand Gaussian measurement noise, we present a novel error bound based on therecently proposed Wiener kernel regression. We prove that under rather mildassumptions, the proposed error bound is tighter than bounds previouslydocumented in the literature which leads to enlarged safety regions. We drawupon a numerical example to demonstrate the efficacy of the proposed errorbound in safe BO.</description><author>Oleksii Molodchyk, Johannes Teutsch, Timm Faulwasser</author><pubDate>Tue, 26 Nov 2024 15:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02253v2</guid></item><item><title>AI-Augmented Ethical Hacking: A Practical Examination of Manual Exploitation and Privilege Escalation in Linux Environments</title><link>http://arxiv.org/abs/2411.17539v1</link><description>This study explores the application of generative AI (GenAI) within manualexploitation and privilege escalation tasks in Linux-based penetration testingenvironments, two areas critical to comprehensive cybersecurity assessments.Building on previous research into the role of GenAI in the ethical hackinglifecycle, this paper presents a hands-on experimental analysis conducted in acontrolled virtual setup to evaluate the utility of GenAI in supporting thesecrucial, often manual, tasks. Our findings demonstrate that GenAI canstreamline processes, such as identifying potential attack vectors and parsingcomplex outputs for sensitive data during privilege escalation. The study alsoidentifies key benefits and challenges associated with GenAI, includingenhanced efficiency and scalability, alongside ethical concerns related to dataprivacy, unintended discovery of vulnerabilities, and potential for misuse.This work contributes to the growing field of AI-assisted cybersecurity byemphasising the importance of human-AI collaboration, especially in contextsrequiring careful decision-making, rather than the complete replacement ofhuman input.</description><author>Haitham S. Al-Sinani, Chris J. Mitchell</author><pubDate>Tue, 26 Nov 2024 15:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17539v1</guid></item><item><title>Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search</title><link>http://arxiv.org/abs/2411.17538v1</link><description>Low isotropy in an embedding space impairs performance on tasks involvingsemantic inference. Our study investigates the impact of isotropy on semanticcode search performance and explores post-processing techniques to mitigatethis issue. We analyze various code language models, examine isotropy in theirembedding spaces, and its influence on search effectiveness. We propose amodified ZCA whitening technique to control isotropy levels in embeddings. Ourresults demonstrate that Soft-ZCA whitening improves the performance ofpre-trained code language models and can complement contrastive fine-tuning.The code for our experiments is available athttps://github.com/drndr/code\_isotropy</description><author>Andor Diera, Lukas Galke, Ansgar Scherp</author><pubDate>Tue, 26 Nov 2024 15:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17538v1</guid></item><item><title>Towards Maximum Likelihood Training for Transducer-based Streaming Speech Recognition</title><link>http://arxiv.org/abs/2411.17537v1</link><description>Transducer neural networks have emerged as the mainstream approach forstreaming automatic speech recognition (ASR), offering state-of-the-artperformance in balancing accuracy and latency. In the conventional framework,streaming transducer models are trained to maximize the likelihood functionbased on non-streaming recursion rules. However, this approach leads to amismatch between training and inference, resulting in the issue of deformedlikelihood and consequently suboptimal ASR accuracy. We introduce amathematical quantification of the gap between the actual likelihood and thedeformed likelihood, namely forward variable causal compensation (FoCC). Wealso present its estimator, FoCCE, as a solution to estimate the exactlikelihood. Through experiments on the LibriSpeech dataset, we show that FoCCEtraining improves the accuracy of the streaming transducers.</description><author>Hyeonseung Lee, Ji Won Yoon, Sungsoo Kim, Nam Soo Kim</author><pubDate>Tue, 26 Nov 2024 15:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17537v1</guid></item><item><title>Box for Mask and Mask for Box: weak losses for multi-task partially supervised learning</title><link>http://arxiv.org/abs/2411.17536v1</link><description>Object detection and semantic segmentation are both scene understanding tasksyet they differ in data structure and information level. Object detectionrequires box coordinates for object instances while semantic segmentationrequires pixel-wise class labels. Making use of one task's information to trainthe other would be beneficial for multi-task partially supervised learningwhere each training example is annotated only for a single task, having thepotential to expand training sets with different-task datasets. This paperstudies various weak losses for partially annotated data in combination withexisting supervised losses. We propose Box-for-Mask and Mask-for-Boxstrategies, and their combination BoMBo, to distil necessary information fromone task annotations to train the other. Ablation studies and experimentalresults on VOC and COCO datasets show favorable results for the proposed idea.Source code and data splits can be found at https://github.com/lhoangan/multas.</description><author>Hoàng-Ân Lê, Paul Berg, Minh-Tan Pham</author><pubDate>Tue, 26 Nov 2024 15:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17536v1</guid></item><item><title>IMPROVE: Improving Medical Plausibility without Reliance on HumanValidation -- An Enhanced Prototype-Guided Diffusion Framework</title><link>http://arxiv.org/abs/2411.17535v1</link><description>Generative models have proven to be very effective in generating syntheticmedical images and find applications in downstream tasks such as enhancing raredisease datasets, long-tailed dataset augmentation, and scaling machinelearning algorithms. For medical applications, the synthetically generatedmedical images by such models are still reasonable in quality when evaluatedbased on traditional metrics such as FID score, precision, and recall. However,these metrics fail to capture the medical/biological plausibility of thegenerated images. Human expert feedback has been used to get biologicalplausibility which demonstrates that these generated images have very lowplausibility. Recently, the research community has further integrated thishuman feedback through Reinforcement Learning from Human Feedback(RLHF), whichgenerates more medically plausible images. However, incorporating humanfeedback is a costly and slow process. In this work, we propose a novelapproach to improve the medical plausibility of generated images without theneed for human feedback. We introduce IMPROVE:Improving Medical Plausibilitywithout Reliance on Human Validation - An Enhanced Prototype-Guided DiffusionFramework, a prototype-guided diffusion process for medical image generationand show that it substantially enhances the biological plausibility of thegenerated medical images without the need for any human feedback. We performexperiments on Bone Marrow and HAM10000 datasets and show that medical accuracycan be substantially increased without human feedback.</description><author>Anurag Shandilya, Swapnil Bhat, Akshat Gautam, Subhash Yadav, Siddharth Bhatt, Deval Mehta, Kshitij Jadhav</author><pubDate>Tue, 26 Nov 2024 15:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17535v1</guid></item><item><title>FTMoMamba: Motion Generation with Frequency and Text State Space Models</title><link>http://arxiv.org/abs/2411.17532v1</link><description>Diffusion models achieve impressive performance in human motion generation.However, current approaches typically ignore the significance offrequency-domain information in capturing fine-grained motions within thelatent space (e.g., low frequencies correlate with static poses, and highfrequencies align with fine-grained motions). Additionally, there is a semanticdiscrepancy between text and motion, leading to inconsistency between thegenerated motions and the text descriptions. In this work, we propose a noveldiffusion-based FTMoMamba framework equipped with a Frequency State Space Model(FreqSSM) and a Text State Space Model (TextSSM). Specifically, to learnfine-grained representation, FreqSSM decomposes sequences into low-frequencyand high-frequency components, guiding the generation of static pose (e.g.,sits, lay) and fine-grained motions (e.g., transition, stumble), respectively.To ensure the consistency between text and motion, TextSSM encodes textfeatures at the sentence level, aligning textual semantics with sequentialfeatures. Extensive experiments show that FTMoMamba achieves superiorperformance on the text-to-motion generation task, especially gaining thelowest FID of 0.181 (rather lower than 0.421 of MLD) on the HumanML3D dataset.</description><author>Chengjian Li, Xiangbo Shu, Qiongjie Cui, Yazhou Yao, Jinhui Tang</author><pubDate>Tue, 26 Nov 2024 15:48:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17532v1</guid></item><item><title>HSI-Drive v2.0: More Data for New Challenges in Scene Understanding for Autonomous Driving</title><link>http://arxiv.org/abs/2411.17530v1</link><description>We present the updated version of the HSI-Drive dataset aimed at developingautomated driving systems (ADS) using hyperspectral imaging (HSI). The v2.0version includes new annotated images from videos recorded during winter andfall in real driving scenarios. Added to the spring and summer images includedin the previous v1.1 version, the new dataset contains 752 images covering thefour seasons. In this paper, we show the improvements achieved over previouslypublished results obtained on the v1.1 dataset, showcasing the enhancedperformance of models trained on the new v2.0 dataset. We also show theprogress made in comprehensive scene understanding by experimenting with morecapable image segmentation models. These models include new segmentationcategories aimed at the identification of essential road safety objects such asthe presence of vehicles and road signs, as well as highly vulnerable groupslike pedestrians and cyclists. In addition, we provide evidence of theperformance and robustness of the models when applied to segmenting HSI videosequences captured in various environments and conditions. Finally, for acorrect assessment of the results described in this work, the constraintsimposed by the processing platforms that can sensibly be deployed in vehiclesfor ADS must be taken into account. Thus, and although implementation detailsare out of the scope of this paper, we focus our research on the development ofcomputationally efficient, lightweight ML models that can eventually operate athigh throughput rates. The dataset and some examples of segmented videos areavailable in https://ipaccess.ehu.eus/HSI-Drive/.</description><author>Jon Gutiérrez-Zaballa, Koldo Basterretxea, Javier Echanobe, M. Victoria Martínez, Unai Martínez-Corral</author><pubDate>Tue, 26 Nov 2024 15:45:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17530v1</guid></item><item><title>MAROON: A Framework for the Joint Characterization of Near-Field High-Resolution Radar and Optical Depth Imaging Techniques</title><link>http://arxiv.org/abs/2411.00527v2</link><description>Utilizing the complementary strengths of wavelength-specific range or depthsensors is crucial for robust computer-assisted tasks such as autonomousdriving. Despite this, there is still little research done at the intersectionof optical depth sensors and radars operating close range, where the target isdecimeters away from the sensors. Together with a growing interest inhigh-resolution imaging radars operating in the near field, the question ariseshow these sensors behave in comparison to their traditional opticalcounterparts. In this work, we take on the unique challenge of jointly characterizing depthimagers from both, the optical and radio-frequency domain using a multimodalspatial calibration. We collect data from four depth imagers, with threeoptical sensors of varying operation principle and an imaging radar. We providea comprehensive evaluation of their depth measurements with respect to distinctobject materials, geometries, and object-to-sensor distances. Specifically, wereveal scattering effects of partially transmissive materials and investigatethe response of radio-frequency signals. All object measurements will be madepublic in form of a multimodal dataset, called MAROON.</description><author>Vanessa Wirth, Johanna Bräunig, Martin Vossiek, Tim Weyrich, Marc Stamminger</author><pubDate>Tue, 26 Nov 2024 15:44:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00527v2</guid></item><item><title>Evolving Markov Chains: Unsupervised Mode Discovery and Recognition from Data Streams</title><link>http://arxiv.org/abs/2411.17528v1</link><description>Markov chains are simple yet powerful mathematical structures to modeltemporally dependent processes. They generally assume stationary data, i.e.,fixed transition probabilities between observations/states. However, live,real-world processes, like in the context of activity tracking, biological timeseries, or industrial monitoring, often switch behavior over time. Suchbehavior switches can be modeled as transitions between higher-level\emph{modes} (e.g., running, walking, etc.). Yet all modes are usually notpreviously known, often exhibit vastly differing transition probabilities, andcan switch unpredictably. Thus, to track behavior changes of live, real-worldprocesses, this study proposes an online and efficient method to constructEvolving Markov chains (EMCs). EMCs adaptively track transition probabilities,automatically discover modes, and detect mode switches in an online manner. Incontrast to previous work, EMCs are of arbitrary order, the proposed updatescheme does not rely on tracking windows, only updates the relevant region ofthe probability tensor, and enjoys geometric convergence of the expectedestimates. Our evaluation of synthetic data and real-world applications onhuman activity recognition, electric motor condition monitoring, and eye-staterecognition from electroencephalography (EEG) measurements illustrates theversatility of the approach and points to the potential of EMCs to efficientlytrack, model, and understand live, real-world processes.</description><author>Kutalmış Coşkun, Borahan Tümer, Bjarne C. Hiller, Martin Becker</author><pubDate>Tue, 26 Nov 2024 15:42:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17528v1</guid></item><item><title>DeltaKWS: A 65nm 36nJ/Decision Bio-inspired Temporal-Sparsity-Aware Digital Keyword Spotting IC with 0.6V Near-Threshold SRAM</title><link>http://arxiv.org/abs/2405.03905v2</link><description>This paper introduces DeltaKWS, to the best of our knowledge, the first$\Delta$RNN-enabled fine-grained temporal sparsity-aware KWS IC forvoice-controlled devices. The 65 nm prototype chip features a number oftechniques to enhance performance, area, and power efficiencies, specifically:1) a bio-inspired delta-gated recurrent neural network ($\Delta$RNN) classifierleveraging temporal similarities between neighboring feature vectors extractedfrom input frames and network hidden states, eliminating unnecessary operationsand memory accesses; 2) an IIR BPF-based FEx that leverages mixed-precisionquantization, low-cost computing structure and channel selection; 3) a 24 kB0.6 V near-$V_\text{TH}$ weight SRAM that achieves 6.6X lower read power thanthe foundry-provided SRAM. From chip measurement results, we show that theDeltaKWS achieves an 11/12-class GSCD accuracy of 90.5%/89.5% respectively andenergy consumption of 36 nJ/decision in 65 nm CMOS process. At 87% temporalsparsity, computing latency and energy/inference are reduced by 2.4X/3.4X,respectively. The IIR BPF-based FEx, $\Delta$RNN accelerator, and 24 kBnear-$V_\text{TH}$ SRAM blocks occupy 0.084 mm$^{2}$, 0.319 mm$^{2}$, and 0.381mm$^{2}$ respectively (0.78 mm$^{2}$ in total).</description><author>Qinyu Chen, Kwantae Kim, Chang Gao, Sheng Zhou, Taekwang Jang, Tobi Delbruck, Shih-Chii Liu</author><pubDate>Tue, 26 Nov 2024 15:37:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03905v2</guid></item><item><title>LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models</title><link>http://arxiv.org/abs/2409.20288v4</link><description>Large language models (LLMs) have made significant progress in naturallanguage processing tasks and demonstrate considerable potential in the legaldomain. However, legal applications demand high standards of accuracy,reliability, and fairness. Applying existing LLMs to legal systems withoutcareful evaluation of their potential and limitations could pose significantrisks in legal practice. To this end, we introduce a standardized comprehensiveChinese legal benchmark LexEval. This benchmark is notable in the followingthree aspects: (1) Ability Modeling: We propose a new taxonomy of legalcognitive abilities to organize different tasks. (2) Scale: To our knowledge,LexEval is currently the largest Chinese legal evaluation dataset, comprising23 tasks and 14,150 questions. (3) Data: we utilize formatted existingdatasets, exam datasets and newly annotated datasets by legal experts tocomprehensively evaluate the various capabilities of LLMs. LexEval not onlyfocuses on the ability of LLMs to apply fundamental legal knowledge but alsodedicates efforts to examining the ethical issues involved in theirapplication. We evaluated 38 open-source and commercial LLMs and obtained someinteresting findings. The experiments and findings offer valuable insights intothe challenges and potential solutions for developing Chinese legal systems andLLM evaluation pipelines. The LexEval dataset and leaderboard are publiclyavailable at \url{https://github.com/CSHaitao/LexEval} and will be continuouslyupdated.</description><author>Haitao Li, You Chen, Qingyao Ai, Yueyue Wu, Ruizhe Zhang, Yiqun Liu</author><pubDate>Tue, 26 Nov 2024 15:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20288v4</guid></item><item><title>Pushing the Limits of Large Language Model Quantization via the Linearity Theorem</title><link>http://arxiv.org/abs/2411.17525v1</link><description>Quantizing large language models has become a standard way to reduce theirmemory and computational costs. Typically, existing methods focus on breakingdown the problem into individual layer-wise sub-problems, and minimizingper-layer error, measured via various metrics. Yet, this approach currentlylacks theoretical justification and the metrics employed may be sub-optimal. Inthis paper, we present a "linearity theorem" establishing a direct relationshipbetween the layer-wise $\ell_2$ reconstruction error and the model perplexityincrease due to quantization. This insight enables two novel applications: (1)a simple data-free LLM quantization method using Hadamard rotations andMSE-optimal grids, dubbed HIGGS, which outperforms all prior data-freeapproaches such as the extremely popular NF4 quantized format, and (2) anoptimal solution to the problem of finding non-uniform per-layer quantizationlevels which match a given compression constraint in the medium-bitwidthregime, obtained by reduction to dynamic programming. On the practical side, wedemonstrate improved accuracy-compression trade-offs on Llama-3.1 and3.2-family models, as well as on Qwen-family models. Further, we show that ourmethod can be efficiently supported in terms of GPU kernels at various batchsizes, advancing both data-free and non-uniform quantization for LLMs.</description><author>Vladimir Malinovskii, Andrei Panferov, Ivan Ilin, Han Guo, Peter Richtárik, Dan Alistarh</author><pubDate>Tue, 26 Nov 2024 15:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17525v1</guid></item><item><title>Refined and Segmented Price Sentiment Indices from Survey Comments</title><link>http://arxiv.org/abs/2411.09937v2</link><description>We aim to enhance a price sentiment index and to more precisely understandprice trends from the perspective of not only consumers but also businesses. Weextract comments related to prices from the Economy Watchers Survey conductedby the Cabinet Office of Japan and classify price trends using a large languagemodel (LLM). We classify whether the survey sample reflects the perspective ofconsumers or businesses, and whether the comments pertain to goods or servicesby utilizing information on the fields of comments and the industries ofrespondents included in the Economy Watchers Survey. From these classifiedprice-related comments, we construct price sentiment indices not only for ageneral purpose but also for more specific objectives by combining perspectiveson consumers and prices, as well as goods and services. It becomes possible toachieve a more accurate classification of price directions by employing a LLMfor classification. Furthermore, integrating the outputs of multiple LLMssuggests the potential for the better performance of the classification. Theuse of more accurately classified comments allows for the construction of anindex with a higher correlation to existing indices than previous studies. Wedemonstrate that the correlation of the price index for consumers, which has alarger sample size, is further enhanced by selecting comments for aggregationbased on the industry of the survey respondents.</description><author>Masahiro Suzuki, Hiroki Sakaji</author><pubDate>Tue, 26 Nov 2024 15:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09937v2</guid></item><item><title>On Statistical Rates of Conditional Diffusion Transformers: Approximation, Estimation and Minimax Optimality</title><link>http://arxiv.org/abs/2411.17522v1</link><description>We investigate the approximation and estimation rates of conditionaldiffusion transformers (DiTs) with classifier-free guidance. We present acomprehensive analysis for ``in-context'' conditional DiTs under four commondata assumptions. We show that both conditional DiTs and their latent variantslead to the minimax optimality of unconditional DiTs under identified settings.Specifically, we discretize the input domains into infinitesimal grids and thenperform a term-by-term Taylor expansion on the conditional diffusion scorefunction under H\"older smooth data assumption. This enables fine-grained useof transformers' universal approximation through a more detailed piecewiseconstant approximation and hence obtains tighter bounds. Additionally, weextend our analysis to the latent setting under the linear latent subspaceassumption. We not only show that latent conditional DiTs achieve lower boundsthan conditional DiTs both in approximation and estimation, but also show theminimax optimality of latent unconditional DiTs. Our findings establishstatistical limits for conditional and unconditional DiTs, and offer practicalguidance toward developing more efficient and accurate DiT models.</description><author>Jerry Yao-Chieh Hu, Weimin Wu, Yi-Chen Lee, Yu-Chao Huang, Minshuo Chen, Han Liu</author><pubDate>Tue, 26 Nov 2024 15:30:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17522v1</guid></item><item><title>SuperMat: Physically Consistent PBR Material Estimation at Interactive Rates</title><link>http://arxiv.org/abs/2411.17515v1</link><description>Decomposing physically-based materials from images into their constituentproperties remains challenging, particularly when maintaining bothcomputational efficiency and physical consistency. While recent diffusion-basedapproaches have shown promise, they face substantial computational overhead dueto multiple denoising steps and separate models for different materialproperties. We present SuperMat, a single-step framework that achieveshigh-quality material decomposition with one-step inference. This enablesend-to-end training with perceptual and re-render losses while decomposingalbedo, metallic, and roughness maps at millisecond-scale speeds. We furtherextend our framework to 3D objects through a UV refinement network, enablingconsistent material estimation across viewpoints while maintaining efficiency.Experiments demonstrate that SuperMat achieves state-of-the-art PBR materialdecomposition quality while reducing inference time from seconds tomilliseconds per image, and completes PBR material estimation for 3D objects inapproximately 3 seconds.</description><author>Yijia Hong, Yuan-Chen Guo, Ran Yi, Yulong Chen, Yan-Pei Cao, Lizhuang Ma</author><pubDate>Tue, 26 Nov 2024 15:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17515v1</guid></item><item><title>Perceptually Optimized Super Resolution</title><link>http://arxiv.org/abs/2411.17513v1</link><description>Modern deep-learning based super-resolution techniques process images andvideos independently of the underlying content and viewing conditions. However,the sensitivity of the human visual system to image details changes dependingon the underlying content characteristics, such as spatial frequency,luminance, color, contrast, or motion. This observation hints thatcomputational resources spent on up-sampling visual content may be wastedwhenever a viewer cannot resolve the results. Motivated by this observation, wepropose a perceptually inspired and architecture-agnostic approach forcontrolling the visual quality and efficiency of super-resolution techniques.The core is a perceptual model that dynamically guides super-resolution methodsaccording to the human's sensitivity to image details. Our technique leveragesthe limitations of the human visual system to improve the efficiency ofsuper-resolution techniques by focusing computational resources on perceptuallyimportant regions; judged on the basis of factors such as adapting luminance,contrast, spatial frequency, motion, and viewing conditions. We demonstrate theapplication of our proposed model in combination with network branching, andnetwork complexity reduction to improve the computational efficiency ofsuper-resolution methods without visible quality loss. Quantitative andqualitative evaluations, including user studies, demonstrate the effectivenessof our approach in reducing FLOPS by factors of 2$\mathbf{x}$ and greater,without sacrificing perceived quality.</description><author>Volodymyr Karpenko, Taimoor Tariq, Jorge Condor, Piotr Didyk</author><pubDate>Tue, 26 Nov 2024 15:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17513v1</guid></item><item><title>Training Hamiltonian neural networks without backpropagation</title><link>http://arxiv.org/abs/2411.17511v1</link><description>Neural networks that synergistically integrate data and physical laws offergreat promise in modeling dynamical systems. However, iterative gradient-basedoptimization of network parameters is often computationally expensive andsuffers from slow convergence. In this work, we present a backpropagation-freealgorithm to accelerate the training of neural networks for approximatingHamiltonian systems through data-agnostic and data-driven algorithms. Weempirically show that data-driven sampling of the network parametersoutperforms data-agnostic sampling or the traditional gradient-based iterativeoptimization of the network parameters when approximating functions with steepgradients or wide input domains. We demonstrate that our approach is more than100 times faster with CPUs than the traditionally trained Hamiltonian NeuralNetworks using gradient-based iterative optimization and is more than fourorders of magnitude accurate in chaotic examples, including the H\'enon-Heilessystem.</description><author>Atamert Rahma, Chinmay Datar, Felix Dietrich</author><pubDate>Tue, 26 Nov 2024 15:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17511v1</guid></item><item><title>Orientation-Aware Graph Neural Networks for Protein Structure Representation Learning</title><link>http://arxiv.org/abs/2201.13299v5</link><description>By folding to particular 3D structures, proteins play a key role in livingbeings. To learn meaningful representation from a protein structure fordownstream tasks, not only the global backbone topology but the localfine-grained orientational relations between amino acids should also beconsidered. In this work, we propose the Orientation-Aware Graph NeuralNetworks (OAGNNs) to better sense the geometric characteristics in proteinstructure (e.g. inner-residue torsion angles, inter-residue orientations).Extending a single weight from a scalar to a 3D vector, we construct a rich setof geometric-meaningful operations to process both the classical and SO(3)representations of a given structure. To plug our designed perceptron unit intoexisting Graph Neural Networks, we further introduce an equivariant messagepassing paradigm, showing superior versatility in maintainingSO(3)-equivariance at the global scale. Experiments have shown that our OAGNNshave a remarkable ability to sense geometric orientational features compared toclassical networks. OAGNNs have also achieved state-of-the-art performance onvarious computational biology applications related to protein 3D structures.</description><author>Jiahan Li, Shitong Luo, Congyue Deng, Chaoran Cheng, Jiaqi Guan, Leonidas Guibas, Jian Peng, Jianzhu Ma</author><pubDate>Tue, 26 Nov 2024 15:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.13299v5</guid></item></channel></rss>