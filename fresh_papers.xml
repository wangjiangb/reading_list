<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 30 Aug 2024 01:00:33 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Awes, Laws, and Flaws From Today's LLM Research</title><link>http://arxiv.org/abs/2408.15409v2</link><description>We perform a critical examination of the scientific methodology behindcontemporary large language model (LLM) research. For this we assess over 2,000research works based on criteria typical of what is considered good research(e.g. presence of statistical tests and reproducibility) and cross-validate itwith arguments that are at the centre of controversy (e.g., claims of emergentbehaviour, the use of LLMs as evaluators). We find multiple trends, such asdeclines in claims of emergent behaviour and ethics disclaimers; the rise ofLLMs as evaluators in spite of a lack of consensus from the community abouttheir useability; and an increase of claims of LLM reasoning abilities,typically without leveraging human evaluation. This paper underscores the needfor more scrutiny and rigour by and from this field to live up to thefundamentals of a responsible scientific method that is ethical, reproducible,systematic, and open to criticism.</description><author>Adrian de Wynter</author><pubDate>Thu, 29 Aug 2024 17:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15409v2</guid></item><item><title>Post-processing fairness with minimal changes</title><link>http://arxiv.org/abs/2408.15096v2</link><description>In this paper, we introduce a novel post-processing algorithm that is bothmodel-agnostic and does not require the sensitive attribute at test time. Inaddition, our algorithm is explicitly designed to enforce minimal changesbetween biased and debiased predictions; a property that, while highlydesirable, is rarely prioritized as an explicit objective in fairnessliterature. Our approach leverages a multiplicative factor applied to the logitvalue of probability scores produced by a black-box classifier. We demonstratethe efficacy of our method through empirical evaluations, comparing itsperformance against other four debiasing algorithms on two widely used datasetsin fairness research.</description><author>Federico Di Gennaro, Thibault Laugel, Vincent Grari, Xavier Renard, Marcin Detyniecki</author><pubDate>Thu, 29 Aug 2024 15:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15096v2</guid></item><item><title>Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2408.15886v2</link><description>In recent years, the evolution of machine learning techniques hassignificantly impacted the field of intrusion detection, particularly withinthe context of the Internet of Things (IoT). As IoT networks expand, the needfor robust security measures to counteract potential threats has becomeincreasingly critical. This paper introduces a hybrid Intrusion DetectionSystem (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs)with the XGBoost algorithm. Our proposed IDS leverages the unique capabilitiesof KANs, which utilize learnable activation functions to model complexrelationships within data, alongside the powerful ensemble learning techniquesof XGBoost, known for its high performance in classification tasks. This hybridapproach not only enhances the detection accuracy but also improves theinterpretability of the model, making it suitable for dynamic and intricate IoTenvironments. Experimental evaluations demonstrate that our hybrid IDS achievesan impressive detection accuracy exceeding 99% in distinguishing between benignand malicious activities. Additionally, we were able to achieve F1 scores,precision, and recall that exceeded 98%. Furthermore, we conduct a comparativeanalysis against traditional Multi-Layer Perceptron (MLP) networks, assessingperformance metrics such as Precision, Recall, and F1-score. The resultsunderscore the efficacy of integrating KANs with XGBoost, highlighting thepotential of this innovative approach to significantly strengthen the securityframework of IoT networks.</description><author>Amar Amouri, Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Ismail Butun, Imad Mahgoub</author><pubDate>Thu, 29 Aug 2024 15:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15886v2</guid></item><item><title>Smart Multi-Modal Search: Contextual Sparse and Dense Embedding Integration in Adobe Express</title><link>http://arxiv.org/abs/2408.14698v2</link><description>As user content and queries become increasingly multi-modal, the need foreffective multi-modal search systems has grown. Traditional search systemsoften rely on textual and metadata annotations for indexed images, whilemulti-modal embeddings like CLIP enable direct search using text and imageembeddings. However, embedding-based approaches face challenges in integratingcontextual features such as user locale and recency. Building a scalablemulti-modal search system requires fine-tuning several components. This paperpresents a multi-modal search architecture and a series of AB tests thatoptimize embeddings and multi-modal technologies in Adobe Express templatesearch. We address considerations such as embedding model selection, the rolesof embeddings in matching and ranking, and the balance between dense and sparseembeddings. Our iterative approach demonstrates how utilizing sparse, dense,and contextual features enhances short and long query search, significantlyreduces null rates (over 70\%), and increases click-through rates (CTR). Ourfindings provide insights into developing robust multi-modal search systems,thereby enhancing relevance for complex queries.</description><author>Cherag Aroraa, Tracy Holloway King, Jayant Kumar, Yi Lu, Sanat Sharma, Arvind Srikantan, David Uvalle, Josep Valls-Vargas, Harsha Vardhan</author><pubDate>Thu, 29 Aug 2024 15:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14698v2</guid></item><item><title>Conan-embedding: General Text Embedding with More and Better Negative Samples</title><link>http://arxiv.org/abs/2408.15710v2</link><description>With the growing popularity of RAG, the capabilities of embedding models aregaining increasing attention. Embedding models are primarily trained throughcontrastive loss learning, with negative examples being a key component.Previous work has proposed various hard negative mining strategies, but thesestrategies are typically employed as preprocessing steps. In this paper, wepropose the conan-embedding model, which maximizes the utilization of more andhigher-quality negative examples. Specifically, since the model's ability tohandle preprocessed negative examples evolves during training, we proposedynamic hard negative mining method to expose the model to more challengingnegative examples throughout the training process. Secondly, contrastivelearning requires as many negative examples as possible but is limited by GPUmemory constraints. Therefore, we use a Cross-GPU balancing Loss to providemore negative examples for embedding training and balance the batch size acrossmultiple tasks. Moreover, we also discovered that the prompt-response pairsfrom LLMs can be used for embedding training. Our approach effectively enhancesthe capabilities of embedding models, currently ranking first on the Chineseleaderboard of Massive text embedding benchmark</description><author>Shiyu Li, Yang Tang, Shizhe Chen, Xi Chen</author><pubDate>Thu, 29 Aug 2024 14:47:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15710v2</guid></item><item><title>Generalist Segmentation Algorithm for Photoreceptors Analysis in Adaptive Optics Imaging</title><link>http://arxiv.org/abs/2408.14810v2</link><description>Analyzing the cone photoreceptor pattern in images obtained from the livinghuman retina using quantitative methods can be crucial for the early detectionand management of various eye conditions. Confocal adaptive optics scanninglight ophthalmoscope (AOSLO) imaging enables visualization of the cones fromreflections of waveguiding cone photoreceptors. While there have beensignificant improvements in automated algorithms for segmenting cones inconfocal AOSLO images, the process of labelling data remains labor-intensiveand manual. This paper introduces a method based on deep learning (DL) fordetecting and segmenting cones in AOSLO images. The models were trained on asemi-automatically labelled dataset of 20 AOSLO batches of images of 18participants for 0$^{\circ}$, 1$^{\circ}$, and 2$^{\circ}$ from the fovealcenter. F1 scores were 0.968, 0.958, and 0.954 for 0$^{\circ}$, 1$^{\circ}$,and 2$^{\circ}$, respectively, which is better than previously reported DLapproaches. Our method minimizes the need for labelled data by onlynecessitating a fraction of labelled cones, which is especially beneficial inthe field of ophthalmology, where labelled data can often be limited.</description><author>Mikhail Kulyabin, Aline Sindel, Hilde Pedersen, Stuart Gilson, Rigmor Baraas, Andreas Maier</author><pubDate>Thu, 29 Aug 2024 14:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14810v2</guid></item><item><title>No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery</title><link>http://arxiv.org/abs/2408.15099v2</link><description>What data or environments to use for training to improve downstreamperformance is a longstanding and very topical question in reinforcementlearning. In particular, Unsupervised Environment Design (UED) methods havegained recent attention as their adaptive curricula enable agents to be robustto in- and out-of-distribution tasks. We ask to what extent these methods arethemselves robust when applied to a novel setting, closely inspired by areal-world robotics problem. Surprisingly, we find that the state-of-the-artUED methods either do not improve upon the na\"{i}ve baseline of DomainRandomisation (DR), or require substantial hyperparameter tuning to do so. Ouranalysis shows that this is due to their underlying scoring functions failingto predict intuitive measures of ``learnability'', i.e., in finding thesettings that the agent sometimes solves, but not always. Based on this, weinstead directly train on levels with high learnability and find that thissimple and intuitive approach outperforms UED methods and DR in severalbinary-outcome environments, including on our domain and the standard UEDdomain of Minigrid. We further introduce a new adversarial evaluation procedurefor directly measuring robustness, closely mirroring the conditional value atrisk (CVaR). We open-source all our code and present visualisations of finalpolicies here: https://github.com/amacrutherford/sampling-for-learnability.</description><author>Alexander Rutherford, Michael Beukman, Timon Willi, Bruno Lacerda, Nick Hawes, Jakob Foerster</author><pubDate>Thu, 29 Aug 2024 14:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15099v2</guid></item><item><title>Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models Without Preference Data</title><link>http://arxiv.org/abs/2408.14874v2</link><description>Reinforcement Learning from Human Feedback (RLHF) has proven effective inaligning large language models with human intentions, yet it often relies oncomplex methodologies like Proximal Policy Optimization (PPO) that requireextensive hyper-parameter tuning and present challenges in sample efficiencyand stability. In this paper, we introduce Inverse-Q*, an innovative frameworkthat transcends traditional RL methods by optimizing token-level reinforcementlearning without the need for additional reward or value models. Inverse-Q*leverages direct preference optimization techniques but extends them byestimating the conditionally optimal policy directly from the model'sresponses, facilitating more granular and flexible policy shaping. Our approachreduces reliance on human annotation and external supervision, making itespecially suitable for low-resource settings. We present extensiveexperimental results demonstrating that Inverse-Q* not only matches butpotentially exceeds the effectiveness of PPO in terms of convergence speed andthe alignment of model responses with human preferences. Our findings suggestthat Inverse-Q* offers a practical and robust alternative to conventional RLHFapproaches, paving the way for more efficient and adaptable model trainingapproaches.</description><author>Han Xia, Songyang Gao, Qiming Ge, Zhiheng Xi, Qi Zhang, Xuanjing Huang</author><pubDate>Thu, 29 Aug 2024 13:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14874v2</guid></item><item><title>GANs Conditioning Methods: A Survey</title><link>http://arxiv.org/abs/2408.15640v2</link><description>In recent years, Generative Adversarial Networks (GANs) have seen significantadvancements, leading to their widespread adoption across various fields. Theoriginal GAN architecture enables the generation of images without any specificcontrol over the content, making it an unconditional generation process.However, many practical applications require precise control over the generatedoutput, which has led to the development of conditional GANs (cGANs) thatincorporate explicit conditioning to guide the generation process. cGANs extendthe original framework by incorporating additional information (conditions),enabling the generation of samples that adhere to that specific criteria.Various conditioning methods have been proposed, each differing in how theyintegrate the conditioning information into both the generator and thediscriminator networks. In this work, we review the conditioning methodsproposed for GANs, exploring the characteristics of each method andhighlighting their unique mechanisms and theoretical foundations. Furthermore,we conduct a comparative analysis of these methods, evaluating theirperformance on various image datasets. Through these analyses, we aim toprovide insights into the strengths and limitations of various conditioningtechniques, guiding future research and application in generative modeling.</description><author>Anis Bourou, Auguste Genovesio, Valérie Mezger</author><pubDate>Thu, 29 Aug 2024 13:47:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15640v2</guid></item><item><title>Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of Peptides</title><link>http://arxiv.org/abs/2408.15126v2</link><description>Molecular Dynamics (MD) simulations are irreplaceable and ubiquitous infields of materials science, chemistry, pharmacology just to name a few.Conventional MD simulations are plagued by numerical stability as well as longequilibration time issues, which limits broader applications of MD simulations.Recently, a surge of deep learning approaches have been devised fortime-coarsened dynamics, which learns the state transition mechanism over muchlarger time scales to overcome these limitations. However, only a few methodstarget the underlying Boltzmann distribution by resampling techniques, whereproposals are rarely accepted as new states with low efficiency. In this work,we propose a force-guided bridge matching model, FBM, a novel framework thatfirst incorporates physical priors into bridge matching for full-atomtime-coarsened dynamics. With the guidance of our well-designed intermediateforce field, FBM is feasible to target the Boltzmann-like distribution bydirect inference without extra steps. Experiments on small peptides verify oursuperiority in terms of comprehensive metrics and demonstrate transferabilityto unseen peptide systems.</description><author>Ziyang Yu, Wenbing Huang, Yang Liu</author><pubDate>Thu, 29 Aug 2024 13:21:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15126v2</guid></item><item><title>Easy, Interpretable, Effective: openSMILE for voice deepfake detection</title><link>http://arxiv.org/abs/2408.15775v2</link><description>In this paper, we demonstrate that attacks in the latest ASVspoof5 dataset --a de facto standard in the field of voice authenticity and deepfake detection-- can be identified with surprising accuracy using a small subset of verysimplistic features. These are derived from the openSMILE library, and arescalar-valued, easy to compute, and human interpretable. For example, attackA10`s unvoiced segments have a mean length of 0.09 +- 0.02, while bona fideinstances have a mean length of 0.18 +- 0.07. Using this feature alone, athreshold classifier achieves an Equal Error Rate (EER) of 10.3% for attackA10. Similarly, across all attacks, we achieve up to 0.8% EER, with an overallEER of 15.7 +- 6.0%. We explore the generalization capabilities of thesefeatures and find that some of them transfer effectively between attacks,primarily when the attacks originate from similar Text-to-Speech (TTS)architectures. This finding may indicate that voice anti-spoofing is, in part,a problem of identifying and remembering signatures or fingerprints ofindividual TTS systems. This allows to better understand anti-spoofing modelsand their challenges in real-world application.</description><author>Octavian Pascu, Dan Oneata, Horia Cucu, Nicolas M. Müller</author><pubDate>Thu, 29 Aug 2024 11:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15775v2</guid></item><item><title>ReMamba: Equip Mamba with Effective Long-Sequence Modeling</title><link>http://arxiv.org/abs/2408.15496v2</link><description>While the Mamba architecture demonstrates superior inference efficiency andcompetitive performance on short-context natural language processing (NLP)tasks, empirical evidence suggests its capacity to comprehend long contexts islimited compared to transformer-based models. In this study, we investigate thelong-context efficiency issues of the Mamba models and propose ReMamba, whichenhances Mamba's ability to comprehend long contexts. ReMamba incorporatesselective compression and adaptation techniques within a two-stage re-forwardprocess, incurring minimal additional inference costs overhead. Experimentalresults on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy,improving over the baselines by 3.2 and 1.6 points, respectively, and attainingperformance almost on par with same-size transformer models.</description><author>Danlong Yuan, Jiahao Liu, Bei Li, Huishuai Zhang, Jingang Wang, Xunliang Cai, Dongyan Zhao</author><pubDate>Thu, 29 Aug 2024 10:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15496v2</guid></item><item><title>Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies</title><link>http://arxiv.org/abs/2408.15294v2</link><description>Developing novel predictive models with complex biomedical information ischallenging due to various idiosyncrasies related to heterogeneity,standardization or sparseness of the data. We previously introduced aperson-centric ontology to organize information about individual patients, anda representation learning framework to extract person-centric knowledge graphs(PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose asystematic approach to examine the results of GNN models trained with bothstructured and unstructured information from the MIMIC-III dataset. Throughablation studies on different clinical, demographic, and social data, we showthe robustness of this approach in identifying predictive features in PKGs forthe task of readmission prediction.</description><author>Christos Theodoropoulos, Natasha Mulligan, Joao Bettencourt-Silva</author><pubDate>Thu, 29 Aug 2024 09:43:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15294v2</guid></item><item><title>Deep Learning Based Speckle Filtering for Polarimetric SAR Images. Application to Sentinel-1</title><link>http://arxiv.org/abs/2408.15678v2</link><description>Speckle suppression in synthetic aperture radar (SAR) images is a keyprocessing step which continues to be a research topic. A wide variety ofmethods, using either spatially-based approaches or transform-based strategies,have been developed and have shown to provide outstanding results. However,recent advances in deep learning techniques and their application to SAR imagedespeckling have been demonstrated to offer state-of-the-art results.Unfortunately, they have been mostly applied to single-polarimetric images. Theextension of a deep learning-based approach for speckle removal to polarimetricSAR (PolSAR) images is complicated because of the complex nature of themeasured covariance matrices for every image pixel, the properties of whichmust be preserved during filtering. In this work, we propose a completeframework to remove speckle in polarimetric SAR images using a convolutionalneural network. The methodology includes a reversible transformation of theoriginal complex covariance matrix to obtain a set of real-valued intensitybands which are fed to the neural network. In addition, the proposed methodincludes a change detection strategy to avoid the neural network to learnerroneous features in areas strongly affected by temporal changes, so that thenetwork only learns the underlying speckle component present in the data. Themethod is implemented and tested with dual-polarimetric images acquired bySentinel-1. Experiments show that the proposed approach offers exceptionalresults in both speckle reduction and resolution preservation. Moreimportantly, it is also shown that the neural network is not generatingartifacts or introducing bias in the filtered images, making them suitable forfurther polarimetric processing and exploitation.</description><author>Alejandro Mestre-Quereda, Juan M. Lopez-Sanchez</author><pubDate>Thu, 29 Aug 2024 09:37:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15678v2</guid></item><item><title>LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation</title><link>http://arxiv.org/abs/2408.15533v2</link><description>Retrieval-Augmented Generation (RAG) has become a primary technique formitigating hallucinations in large language models (LLMs). However, incompleteknowledge extraction and insufficient understanding can still mislead LLMs toproduce irrelevant or even contradictory responses, which means hallucinationspersist in RAG. In this paper, we propose LRP4RAG, a method based on theLayer-wise Relevance Propagation (LRP) algorithm for detecting hallucinationsin RAG. Specifically, we first utilize LRP to compute the relevance between theinput and output of the RAG generator. We then apply further extraction andresampling to the relevance matrix. The processed relevance data are input intomultiple classifiers to determine whether the output contains hallucinations.To the best of our knowledge, this is the first time that LRP has been used fordetecting RAG hallucinations, and extensive experiments demonstrate thatLRP4RAG outperforms existing baselines.</description><author>Haichuan Hu, Yuhan Sun, Quanjun Zhang</author><pubDate>Thu, 29 Aug 2024 08:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15533v2</guid></item><item><title>Shot Segmentation Based on Von Neumann Entropy for Key Frame Extraction</title><link>http://arxiv.org/abs/2408.15844v2</link><description>Video key frame extraction is important in various fields, such as videosummary, retrieval, and compression. Therefore, we suggest a video key frameextraction algorithm based on shot segmentation using Von Neumann entropy. Thesegmentation of shots is achieved through the computation of Von Neumannentropy of the similarity matrix among frames within the video sequence. Theinitial frame of each shot is selected as key frames, which combines thetemporal sequence information of frames. The experimental results show theextracted key frames can fully and accurately represent the original videocontent while minimizing the number of repeated frames.</description><author>Xueqing Zhang, Di Fu, Naihao Liu</author><pubDate>Thu, 29 Aug 2024 07:08:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15844v2</guid></item><item><title>Spatio-Temporal Context Prompting for Zero-Shot Action Detection</title><link>http://arxiv.org/abs/2408.15996v2</link><description>Spatio-temporal action detection encompasses the tasks of localizing andclassifying individual actions within a video. Recent works aim to enhance thisprocess by incorporating interaction modeling, which captures the relationshipbetween people and their surrounding context. However, these approaches haveprimarily focused on fully-supervised learning, and the current limitation liesin the lack of generalization capability to recognize unseen action categories.In this paper, we aim to adapt the pretrained image-language models to detectunseen actions. To this end, we propose a method which can effectively leveragethe rich knowledge of visual-language models to perform Person-ContextInteraction. Meanwhile, our Context Prompting module will utilize contextualinformation to prompt labels, thereby enhancing the generation of morerepresentative text features. Moreover, to address the challenge of recognizingdistinct actions by multiple people at the same timestamp, we design theInterest Token Spotting mechanism which employs pretrained visual knowledge tofind each person's interest context tokens, and then these tokens will be usedfor prompting to generate text features tailored to each individual. Toevaluate the ability to detect unseen actions, we propose a comprehensivebenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that ourmethod achieves superior results compared to previous approaches and can befurther extended to multi-action videos, bringing it closer to real-worldapplications. The code and data can be found inhttps://webber2933.github.io/ST-CLIP-project-page.</description><author>Wei-Jhe Huang, Min-Hung Chen, Shang-Hong Lai</author><pubDate>Thu, 29 Aug 2024 06:54:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15996v2</guid></item><item><title>RIDE: Boosting 3D Object Detection for LiDAR Point Clouds via Rotation-Invariant Analysis</title><link>http://arxiv.org/abs/2408.15643v2</link><description>The rotation robustness property has drawn much attention to point cloudanalysis, whereas it still poses a critical challenge in 3D object detection.When subjected to arbitrary rotation, most existing detectors fail to produceexpected outputs due to the poor rotation robustness. In this paper, we presentRIDE, a pioneering exploration of Rotation-Invariance for the 3DLiDAR-point-based object DEtector, with the key idea of designingrotation-invariant features from LiDAR scenes and then effectivelyincorporating them into existing 3D detectors. Specifically, we design abi-feature extractor that extracts (i) object-aware features though sensitiveto rotation but preserve geometry well, and (ii) rotation-invariant features,which lose geometric information to a certain extent but are robust torotation. These two kinds of features complement each other to decode 3Dproposals that are robust to arbitrary rotations. Particularly, our RIDE iscompatible and easy to plug into the existing one-stage and two-stage 3Ddetectors, and boosts both detection performance and rotation robustness.Extensive experiments on the standard benchmarks showcase that the mean averageprecision (mAP) and rotation robustness can be significantly boosted byintegrating with our RIDE, with +5.6% mAP and 53% rotation robustnessimprovement on KITTI, +5.1% and 28% improvement correspondingly on nuScenes.The code will be available soon.</description><author>Zhaoxuan Wang, Xu Han, Hongxin Liu, Xianzhi Li</author><pubDate>Thu, 29 Aug 2024 03:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15643v2</guid></item><item><title>BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General Role-Playing Language Model</title><link>http://arxiv.org/abs/2408.10903v5</link><description>The rapid advancement of large language models (LLMs) has revolutionizedrole-playing, enabling the development of general role-playing models. However,current role-playing training has two significant issues: (I) Using apredefined role profile to prompt dialogue training for specific scenariosusually leads to inconsistencies and even conflicts between the dialogue andthe profile, resulting in training biases. (II) The model learns to imitate therole based solely on the profile, neglecting profile-dialogue alignment at thesentence level. In this work, we propose a simple yet effective frameworkcalled BEYOND DIALOGUE, designed to overcome these hurdles. This frameworkinnovatively introduces "beyond dialogue" tasks to align dialogue with profiletraits based on each specific scenario, thereby eliminating biases duringtraining. Furthermore, by adopting an innovative prompting mechanism thatgenerates reasoning outcomes for training, the framework allows the model toachieve fine-grained alignment between profile and dialogue at the sentencelevel. The aforementioned methods are fully automated and low-cost.Additionally, the integration of automated dialogue and objective evaluationmethods forms a comprehensive framework, paving the way for generalrole-playing. Experimental results demonstrate that our model excels inadhering to and reflecting various dimensions of role profiles, outperformingmost proprietary general and specialized role-playing baselines. All code anddatasets are available at https://github.com/yuyouyu32/BeyondDialogue.</description><author>Yeyong Yu, Runsheng Yu, Haojie Wei, Zhanqiu Zhang, Quan Qian</author><pubDate>Thu, 29 Aug 2024 02:38:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10903v5</guid></item><item><title>SITransformer: Shared Information-Guided Transformer for Extreme Multimodal Summarization</title><link>http://arxiv.org/abs/2408.15829v2</link><description>Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes anattractive summarization approach by integrating various types of informationto create extremely concise yet informative summaries for individualmodalities. Existing methods overlook the issue that multimodal data oftencontains more topic irrelevant information, which can mislead the model intoproducing inaccurate summaries especially for extremely short ones. In thispaper, we propose SITransformer, a Shared Information-guided Transformer forextreme multimodal summarization. It has a shared information guided pipelinewhich involves a cross-modal shared information extractor and a cross-modalinteraction module. The extractor formulates semantically shared salientinformation from different modalities by devising a novel filtering processconsisting of a differentiable top-k selector and a shared-information guidedgating unit. As a result, the common, salient, and relevant contents acrossmodalities are identified. Next, a transformer with cross-modal attentions isdeveloped for intra- and inter-modality learning with the shared informationguidance to produce the extreme summary. Comprehensive experiments demonstratethat SITransformer significantly enhances the summarization quality for bothvideo and text summaries for XMSMO. Our code will be publicly available athttps://github.com/SichengLeoLiu/MMAsia24-XMSMO.</description><author>Sicheng Liu, Lintao Wang, Xiaogan Zhu, Xuequan Lu, Zhiyong Wang, Kun Hu</author><pubDate>Thu, 29 Aug 2024 02:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15829v2</guid></item><item><title>VFLIP: A Backdoor Defense for Vertical Federated Learning via Identification and Purification</title><link>http://arxiv.org/abs/2408.15591v2</link><description>Vertical Federated Learning (VFL) focuses on handling vertically partitioneddata over FL participants. Recent studies have discovered a significantvulnerability in VFL to backdoor attacks which specifically target the distinctcharacteristics of VFL. Therefore, these attacks may neutralize existingdefense mechanisms designed primarily for Horizontal Federated Learning (HFL)and deep neural networks. In this paper, we present the first backdoor defense,called VFLIP, specialized for VFL. VFLIP employs the identification andpurification techniques that operate at the inference stage, consequentlyimproving the robustness against backdoor attacks to a great extent. VFLIPfirst identifies backdoor-triggered embeddings by adopting a participant-wiseanomaly detection approach. Subsequently, VFLIP conducts purification whichremoves the embeddings identified as malicious and reconstructs all theembeddings based on the remaining embeddings. We conduct extensive experimentson CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstratethat VFLIP can effectively mitigate backdoor attacks in VFL.https://github.com/blingcho/VFLIP-esorics24</description><author>Yungi Cho, Woorim Han, Miseon Yu, Younghan Lee, Ho Bae, Yunheung Paek</author><pubDate>Thu, 29 Aug 2024 02:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15591v2</guid></item><item><title>CGRA4ML: A Framework to Implement Modern Neural Networks for Scientific Edge Computing</title><link>http://arxiv.org/abs/2408.15561v2</link><description>Scientific edge computing increasingly relies on hardware-accelerated neuralnetworks to implement complex, near-sensor processing at extremely highthroughputs and low latencies. Existing frameworks like HLS4ML are effectivefor smaller models, but struggle with larger, modern neural networks due totheir requirement of spatially implementing the neural network layers andstoring all weights in on-chip memory. CGRA4ML is an open-source, modularframework designed to bridge the gap between neural network model complexityand extreme performance requirements. CGRA4ML extends the capabilities ofHLS4ML by allowing off-chip data storage and supporting a broader range ofneural network architectures, including models like ResNet, PointNet, andtransformers. Unlike HLS4ML, CGRA4ML generates SystemVerilog RTL, making itmore suitable for targeting ASIC and FPGA design flows. We demonstrate theeffectiveness of our framework by implementing and scaling larger models thatwere previously unattainable with HLS4ML, showcasing its adaptability andefficiency in handling complex computations. CGRA4ML also introduces anextensive verification framework, with a generated runtime firmware thatenables its integration into different SoC platforms. CGRA4ML's minimal andmodular infrastructure of Python API, SystemVerilog hardware, Tcl toolflows,and C runtime, facilitates easy integration and experimentation, allowingscientists to focus on innovation rather than the intricacies of hardwaredesign and optimization.</description><author>G Abarajithan, Zhenghua Ma, Zepeng Li, Shrideep Koparkar, Ravidu Munasinghe, Francesco Restuccia, Ryan Kastner</author><pubDate>Thu, 29 Aug 2024 01:26:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15561v2</guid></item><item><title>AnomalousPatchCore: Exploring the Use of Anomalous Samples in Industrial Anomaly Detection</title><link>http://arxiv.org/abs/2408.15113v2</link><description>Visual inspection, or industrial anomaly detection, is one of the most commonquality control types in manufacturing. The task is to identify the presence ofan anomaly given an image, e.g., a missing component on an image of a circuitboard, for subsequent manual inspection. While industrial anomaly detection hasseen a surge in recent years, most anomaly detection methods still utilizeknowledge only from normal samples, failing to leverage the information fromthe frequently available anomalous samples. Additionally, they heavily rely onvery general feature extractors pre-trained on common image classificationdatasets. In this paper, we address these shortcomings and propose the newanomaly detection system AnomalousPatchCore~(APC) based on a feature extractorfine-tuned with normal and anomalous in-domain samples and a subsequent memorybank for identifying unusual features. To fine-tune the feature extractor inAPC, we propose three auxiliary tasks that address the different aspects ofanomaly detection~(classification vs. localization) and mitigate the effect ofthe imbalance between normal and anomalous samples. Our extensive evaluation onthe MVTec dataset shows that APC outperforms state-of-the-art systems indetecting anomalies, which is especially important in industrial anomalydetection given the subsequent manual inspection. In detailed ablation studies,we further investigate the properties of our APC.</description><author>Mykhailo Koshil, Tilman Wegener, Detlef Mentrup, Simone Frintrop, Christian Wilms</author><pubDate>Wed, 28 Aug 2024 20:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15113v2</guid></item><item><title>MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of Children with Autism Spectrum Disorder</title><link>http://arxiv.org/abs/2408.15077v2</link><description>Autism spectrum disorder (ASD) is characterized by significant challenges insocial interaction and comprehending communication signals. Recently,therapeutic interventions for ASD have increasingly utilized Deep learningpowered-computer vision techniques to monitor individual progress over time.These models are trained on private, non-public datasets from the autismcommunity, creating challenges in comparing results across different models dueto privacy-preserving data-sharing issues. This work introduces MMASD+, anenhanced version of the novel open-source dataset called Multimodal ASD(MMASD). MMASD+ consists of diverse data modalities, including 3D-Skeleton, 3DBody Mesh, and Optical Flow data. It integrates the capabilities of Yolov8 andDeep SORT algorithms to distinguish between the therapist and children,addressing a significant barrier in the original dataset. Additionally, aMultimodal Transformer framework is proposed to predict 11 action types and thepresence of ASD. This framework achieves an accuracy of 95.03% for predictingaction types and 96.42% for predicting ASD presence, demonstrating over a 10%improvement compared to models trained on single data modalities. Thesefindings highlight the advantages of integrating multiple data modalitieswithin the Multimodal Transformer framework.</description><author>Pavan Uttej Ravva, Behdokht Kiafar, Pinar Kullu, Jicheng Li, Anjana Bhat, Roghayeh Leila Barmaki</author><pubDate>Wed, 28 Aug 2024 20:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15077v2</guid></item><item><title>Q-MRS: A Deep Learning Framework for Quantitative Magnetic Resonance Spectra Analysis</title><link>http://arxiv.org/abs/2408.15999v1</link><description>Magnetic resonance spectroscopy (MRS) is an established technique forstudying tissue metabolism, particularly in central nervous system disorders.While powerful and versatile, MRS is often limited by challenges associatedwith data quality, processing, and quantification. Existing MRS quantificationmethods face difficulties in balancing model complexity and reproducibilityduring spectral modeling, often falling into the trap of eitheroversimplification or over-parameterization. To address these limitations, thisstudy introduces a deep learning (DL) framework that employs transfer learning,in which the model is pre-trained on simulated datasets before it undergoesfine-tuning on in vivo data. The proposed framework showed promisingperformance when applied to the Philips dataset from the BIG GABA repositoryand represents an exciting advancement in MRS data analysis.</description><author>Christopher J. Wu, Lawrence S. Kegeles, Jia Guo</author><pubDate>Wed, 28 Aug 2024 18:05:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15999v1</guid></item><item><title>Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</title><link>http://arxiv.org/abs/2408.15998v1</link><description>The ability to accurately interpret complex visual information is a crucialtopic of multimodal large language models (MLLMs). Recent work indicates thatenhanced visual perception significantly reduces hallucinations and improvesperformance on resolution-sensitive tasks, such as optical characterrecognition and document analysis. A number of recent MLLMs achieve this goalusing a mixture of vision encoders. Despite their success, there is a lack ofsystematic comparisons and detailed ablation studies addressing criticalaspects, such as expert selection and the integration of multiple visionexperts. This study provides an extensive exploration of the design space forMLLMs using a mixture of vision encoders and resolutions. Our findings revealseveral underlying principles common to various existing strategies, leading toa streamlined yet effective design approach. We discover that simplyconcatenating visual tokens from a set of complementary vision encoders is aseffective as more complex mixing architectures or strategies. We additionallyintroduce Pre-Alignment to bridge the gap between vision-focused encoders andlanguage tokens, enhancing model coherence. The resulting family of MLLMs,Eagle, surpasses other leading open-source models on major MLLM benchmarks.Models and code: https://github.com/NVlabs/Eagle</description><author>Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu</author><pubDate>Wed, 28 Aug 2024 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15998v1</guid></item><item><title>Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need</title><link>http://arxiv.org/abs/2408.15997v1</link><description>Time series forecasting requires balancing short-term and long-termdependencies for accurate predictions. Existing methods mainly focus onlong-term dependency modeling, neglecting the complexities of short-termdynamics, which may hinder performance. Transformers are superior in modelinglong-term dependencies but are criticized for their quadratic computationalcost. Mamba provides a near-linear alternative but is reported less effectivein time series longterm forecasting due to potential information loss. Currentarchitectures fall short in offering both high efficiency and strongperformance for long-term dependency modeling. To address these challenges, weintroduce Mixture of Universals (MoU), a versatile model to capture bothshort-term and long-term dependencies for enhancing performance in time seriesforecasting. MoU is composed of two novel designs: Mixture of FeatureExtractors (MoF), an adaptive method designed to improve time series patchrepresentations for short-term dependency, and Mixture of Architectures (MoA),which hierarchically integrates Mamba, FeedForward, Convolution, andSelf-Attention architectures in a specialized order to model long-termdependency from a hybrid perspective. The proposed approach achievesstate-of-the-art performance while maintaining relatively low computationalcosts. Extensive experiments on seven real-world datasets demonstrate thesuperiority of MoU. Code is available at https://github.com/lunaaa95/mou/.</description><author>Sijia Peng, Yun Xiong, Yangyong Zhu, Zhiqiang Shen</author><pubDate>Wed, 28 Aug 2024 17:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15997v1</guid></item><item><title>Spatio-Temporal Context Prompting for Zero-Shot Action Detection</title><link>http://arxiv.org/abs/2408.15996v1</link><description>Spatio-temporal action detection encompasses the tasks of localizing andclassifying individual actions within a video. Recent works aim to enhance thisprocess by incorporating interaction modeling, which captures the relationshipbetween people and their surrounding context. However, these approaches haveprimarily focused on fully-supervised learning, and the current limitation liesin the lack of generalization capability to recognize unseen action categories.In this paper, we aim to adapt the pretrained image-language models to detectunseen actions. To this end, we propose a method which can effectively leveragethe rich knowledge of visual-language models to perform Person-ContextInteraction. Meanwhile, our Context Prompting module will utilize contextualinformation to prompt labels, thereby enhancing the generation of morerepresentative text features. Moreover, to address the challenge of recognizingdistinct actions by multiple people at the same timestamp, we design theInterest Token Spotting mechanism which employs pretrained visual knowledge tofind each person's interest context tokens, and then these tokens will be usedfor prompting to generate text features tailored to each individual. Toevaluate the ability to detect unseen actions, we propose a comprehensivebenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that ourmethod achieves superior results compared to previous approaches and can befurther extended to multi-action videos, bringing it closer to real-worldapplications. The code and data can be found inhttps://webber2933.github.io/ST-CLIP-project-page.</description><author>Wei-Jhe Huang, Min-Hung Chen, Shang-Hong Lai</author><pubDate>Wed, 28 Aug 2024 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15996v1</guid></item><item><title>TEDRA: Text-based Editing of Dynamic and Photoreal Actors</title><link>http://arxiv.org/abs/2408.15995v1</link><description>Over the past years, significant progress has been made in creatingphotorealistic and drivable 3D avatars solely from videos of real humans.However, a core remaining challenge is the fine-grained and user-friendlyediting of clothing styles by means of textual descriptions. To this end, wepresent TEDRA, the first method allowing text-based edits of an avatar, whichmaintains the avatar's high fidelity, space-time coherency, as well asdynamics, and enables skeletal pose and view control. We begin by training amodel to create a controllable and high-fidelity digital replica of the realactor. Next, we personalize a pretrained generative diffusion model byfine-tuning it on various frames of the real character captured from differentcamera angles, ensuring the digital representation faithfully captures thedynamics and movements of the real person. This two-stage process lays thefoundation for our approach to dynamic human avatar editing. Utilizing thispersonalized diffusion model, we modify the dynamic avatar based on a providedtext prompt using our Personalized Normal Aligned Score Distillation Sampling(PNA-SDS) within a model-based guidance framework. Additionally, we propose atime step annealing strategy to ensure high-quality edits. Our resultsdemonstrate a clear improvement over prior work in functionality and visualquality.</description><author>Basavaraj Sunagad, Heming Zhu, Mohit Mendiratta, Adam Kortylewski, Christian Theobalt, Marc Habermann</author><pubDate>Wed, 28 Aug 2024 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15995v1</guid></item><item><title>Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration</title><link>http://arxiv.org/abs/2408.15994v1</link><description>The limitations of task-specific and general image restoration methods forspecific degradation have prompted the development of all-in-one imagerestoration techniques. However, the diversity of patterns among multipledegradation, along with the significant uncertainties in mapping betweendegraded images of different severities and their corresponding undistortedversions, pose significant challenges to the all-in-one restoration tasks. Toaddress these challenges, we propose Perceive-IR, an all-in-one image restorerdesigned to achieve fine-grained quality control that enables restored imagesto more closely resemble their undistorted counterparts, regardless of the typeor severity of degradation. Specifically, Perceive-IR contains two stages: (1)prompt learning stage and (2) restoration stage. In the prompt learning stage,we leverage prompt learning to acquire a fine-grained quality perceiver capableof distinguishing three-tier quality levels by constraining the prompt-imagesimilarity in the CLIP perception space. Subsequently, this quality perceiverand difficulty-adaptive perceptual loss are integrated as a quality-awarelearning strategy to realize fine-grained quality control in restoration stage.For the restoration stage, a semantic guidance module (SGM) and compact featureextraction (CFE) are proposed to further promote the restoration process byutilizing the robust semantic information from the pre-trained large scalevision models and distinguishing degradation-specific features. Extensiveexperiments demonstrate that our Perceive-IR outperforms state-of-the-artmethods in all-in-one image restoration tasks and exhibit superiorgeneralization ability when dealing with unseen tasks.</description><author>Xu Zhang, Jiaqi Ma, Guoli Wang, Qian Zhang, Huan Zhang, Lefei Zhang</author><pubDate>Wed, 28 Aug 2024 17:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15994v1</guid></item><item><title>ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution</title><link>http://arxiv.org/abs/2408.15993v1</link><description>Detecting and attributing temperature increases due to climate change iscrucial for understanding global warming and guiding adaptation strategies. Thecomplexity of distinguishing human-induced climate signals from naturalvariability has challenged traditional detection and attribution (D&amp;A)approaches, which seek to identify specific "fingerprints" in climate responsevariables. Deep learning offers potential for discerning these complex patternsin expansive spatial datasets. However, lack of standard protocols has hinderedconsistent comparisons across studies. We introduce ClimDetect, a standardizeddataset of over 816k daily climate snapshots, designed to enhance modelaccuracy in identifying climate change signals. ClimDetect integrates variousinput and target variables used in past research, ensuring comparability andconsistency. We also explore the application of vision transformers (ViT) toclimate data, a novel and modernizing approach in this context. Our open-accessdata and code serve as a benchmark for advancing climate science throughimproved model evaluations. ClimDetect is publicly accessible via Huggingfacedataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.</description><author>Sungduk Yu, Brian L. White, Anahita Bhiwandiwalla, Musashi Hinck, Matthew Lyle Olson, Tung Nguyen, Vasudev Lal</author><pubDate>Wed, 28 Aug 2024 17:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15993v1</guid></item><item><title>CoGen: Learning from Feedback with Coupled Comprehension and Generation</title><link>http://arxiv.org/abs/2408.15992v1</link><description>Systems with both language comprehension and generation capabilities canbenefit from the tight connection between the two. This work studies couplingcomprehension and generation with focus on continually learning frominteraction with users. We propose techniques to tightly integrate the twocapabilities for both learning and inference. We situate our studies intwo-player reference games, and deploy various models for thousands ofinteractions with human users, while learning from interaction feedbacksignals. We show dramatic improvements in performance over time, withcomprehension-generation coupling leading to performance improvements up to 26%in absolute terms and up to 17% higher accuracies compared to a non-coupledsystem. Our analysis also shows coupling has substantial qualitative impact onthe system's language, making it significantly more human-like.</description><author>Mustafa Omer Gul, Yoav Artzi</author><pubDate>Wed, 28 Aug 2024 17:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15992v1</guid></item><item><title>Distribution Backtracking Builds A Faster Convergence Trajectory for One-step Diffusion Distillation</title><link>http://arxiv.org/abs/2408.15991v1</link><description>Accelerating the sampling speed of diffusion models remains a significantchallenge. Recent score distillation methods distill a heavy teacher model intoan one-step student generator, which is optimized by calculating the differencebetween the two score functions on the samples generated by the student model.However, there is a score mismatch issue in the early stage of the distillationprocess, because existing methods mainly focus on using the endpoint ofpre-trained diffusion models as teacher models, overlooking the importance ofthe convergence trajectory between the student generator and the teacher model.To address this issue, we extend the score distillation process by introducingthe entire convergence trajectory of teacher models and propose DistributionBacktracking Distillation (DisBack) for distilling student generators. DisBaskis composed of two stages: Degradation Recording and Distribution Backtracking.Degradation Recording is designed to obtain the convergence trajectory ofteacher models, which records the degradation path from the trained teachermodel to the untrained initial student generator. The degradation pathimplicitly represents the intermediate distributions of teacher models. ThenDistribution Backtracking trains a student generator to backtrack theintermediate distributions for approximating the convergence trajectory ofteacher models. Extensive experiments show that DisBack achieves faster andbetter convergence than the existing distillation method and accomplishescomparable generation performance. Notably, DisBack is easy to implement andcan be generalized to existing distillation methods to boost performance. Ourcode is publicly available on https://github.com/SYZhang0805/DisBack.</description><author>Shengyuan Zhang, Ling Yang, Zejian Li, An Zhao, Chenye Meng, Changyuan Yang, Guang Yang, Zhiyuan Yang, Lingyun Sun</author><pubDate>Wed, 28 Aug 2024 17:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15991v1</guid></item><item><title>In-Context Imitation Learning via Next-Token Prediction</title><link>http://arxiv.org/abs/2408.15980v1</link><description>We explore how to enhance next-token prediction models to perform in-contextimitation learning on a real robot, where the robot executes new tasks byinterpreting contextual information provided during the input phase, withoutupdating its underlying policy parameters. We propose In-Context RobotTransformer (ICRT), a causal transformer that performs autoregressiveprediction on sensorimotor trajectories without relying on any linguistic dataor reward function. This formulation enables flexible and training-freeexecution of new tasks at test time, achieved by prompting the model withsensorimotor trajectories of the new task composing of image observations,actions and states tuples, collected through human teleoperation. Experimentswith a Franka Emika robot demonstrate that the ICRT can adapt to new tasksspecified by prompts, even in environment configurations that differ from boththe prompt and the training data. In a multitask environment setup, ICRTsignificantly outperforms current state-of-the-art next-token prediction modelsin robotics on generalizing to unseen tasks. Code, checkpoints and data areavailable on https://icrt.dev/</description><author>Letian Fu, Huang Huang, Gaurav Datta, Lawrence Yunliang Chen, William Chung-Ho Panitch, Fangchen Liu, Hui Li, Ken Goldberg</author><pubDate>Wed, 28 Aug 2024 17:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15980v1</guid></item><item><title>WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration</title><link>http://arxiv.org/abs/2408.15978v1</link><description>LLM-based autonomous agents often fail to execute complex web tasks thatrequire dynamic interaction due to the inherent uncertainty and complexity ofthese environments. Existing LLM-based web agents typically rely on rigid,expert-designed policies specific to certain states and actions, which lack theflexibility and generalizability needed to adapt to unseen tasks. In contrast,humans excel by exploring unknowns, continuously adapting strategies, andresolving ambiguities through exploration. To emulate human-like adaptability,web agents need strategic exploration and complex decision-making. Monte CarloTree Search (MCTS) is well-suited for this, but classical MCTS struggles withvast action spaces, unpredictable state transitions, and incomplete informationin web tasks. In light of this, we develop WebPilot, a multi-agent system witha dual optimization strategy that improves MCTS to better handle complex webenvironments. Specifically, the Global Optimization phase involves generating ahigh-level plan by breaking down tasks into manageable subtasks andcontinuously refining this plan, thereby focusing the search process andmitigating the challenges posed by vast action spaces in classical MCTS.Subsequently, the Local Optimization phase executes each subtask using atailored MCTS designed for complex environments, effectively addressinguncertainties and managing incomplete information. Experimental results onWebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, onWebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%relative increase in success rate over the concurrent tree search-based method.WebPilot marks a significant advancement in general autonomous agentcapabilities, paving the way for more advanced and reliable decision-making inpractical environments.</description><author>Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, Volker Tresp</author><pubDate>Wed, 28 Aug 2024 17:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15978v1</guid></item><item><title>Embedded FPGA Developments in 130nm and 28nm CMOS for Machine Learning in Particle Detector Readout</title><link>http://arxiv.org/abs/2404.17701v5</link><description>Embedded field programmable gate array (eFPGA) technology allows theimplementation of reconfigurable logic within the design of anapplication-specific integrated circuit (ASIC). This approach offers the lowpower and efficiency of an ASIC along with the ease of FPGA configuration,particularly beneficial for the use case of machine learning in the datapipeline of next-generation collider experiments. An open-source frameworkcalled "FABulous" was used to design eFPGAs using 130 nm and 28 nm CMOStechnology nodes, which were subsequently fabricated and verified throughtesting. The capability of an eFPGA to act as a front-end readout chip wasassessed using simulation of high energy particles passing through a siliconpixel sensor. A machine learning-based classifier, designed for reduction ofsensor data at the source, was synthesized and configured onto the eFPGA. Asuccessful proof-of-concept was demonstrated through reproduction of theexpected algorithm result on the eFPGA with perfect accuracy. Furtherdevelopment of the eFPGA technology and its application to collider detectorreadout is discussed.</description><author>Julia Gonski, Aseem Gupta, Haoyi Jia, Hyunjoon Kim, Lorenzo Rota, Larry Ruckman, Angelo Dragone, Ryan Herbst</author><pubDate>Wed, 28 Aug 2024 17:47:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17701v5</guid></item><item><title>BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems</title><link>http://arxiv.org/abs/2408.15971v1</link><description>Large Language Models (LLMs) are becoming increasingly powerful and capableof handling complex tasks, e.g., building single agents and multi-agentsystems. Compared to single agents, multi-agent systems have higherrequirements for the collaboration capabilities of language models. Manybenchmarks are proposed to evaluate their collaborative abilities. However,these benchmarks lack fine-grained evaluations of LLM collaborativecapabilities. Additionally, multi-agent collaborative and competitive scenariosare ignored in existing works. To address these two problems, we propose abenchmark, called BattleAgentBench, which defines seven sub-stages of threevarying difficulty levels and conducts a fine-grained evaluation of languagemodels in terms of single-agent scenario navigation capabilities, paired-agenttask execution abilities, and multi-agent collaboration and competitioncapabilities. We conducted extensive evaluations on leading four closed-sourceand seven open-source models. Experimental results indicate that API-basedmodels perform excellently on simple tasks but open-source small modelsstruggle with simple tasks. Regarding difficult tasks that requirecollaborative and competitive abilities, although API-based models havedemonstrated some collaborative capabilities, there is still enormous room forimprovement.</description><author>Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, Jie Tang</author><pubDate>Wed, 28 Aug 2024 17:43:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15971v1</guid></item><item><title>Stability of Primal-Dual Gradient Flow Dynamics for Multi-Block Convex Optimization Problems</title><link>http://arxiv.org/abs/2408.15969v1</link><description>We examine stability properties of primal-dual gradient flow dynamics forcomposite convex optimization problems with multiple, possibly nonsmooth, termsin the objective function under the generalized consensus constraint. Theproposed dynamics are based on the proximal augmented Lagrangian and theyprovide a viable alternative to ADMM which faces significant challenges fromboth analysis and implementation viewpoints in large-scale multi-blockscenarios. In contrast to customized algorithms with individualized convergenceguarantees, we provide a systematic approach for solving a broad class ofchallenging composite optimization problems. We leverage various structuralproperties to establish global (exponential) convergence guarantees for theproposed dynamics. Our assumptions are much weaker than those required to prove(exponential) stability of various primal-dual dynamics as well as (linear)convergence of discrete-time methods, e.g., standard two-block and multi-blockADMM and EXTRA algorithms. Finally, we show necessity of some of our structuralassumptions for exponential stability and provide computational experiments todemonstrate the convenience of the proposed dynamics for parallel anddistributed computing applications.</description><author>Ibrahim K. Ozaslan, Panagiotis Patrinos, Mihailo R. Jovanović</author><pubDate>Wed, 28 Aug 2024 17:43:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15969v1</guid></item><item><title>More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding</title><link>http://arxiv.org/abs/2408.15966v1</link><description>Enabling Large Language Models (LLMs) to comprehend the 3D physical worldremains a significant challenge. Due to the lack of large-scale 3D-text pairdatasets, the success of LLMs has yet to be replicated in 3D understanding. Inthis paper, we rethink this issue and propose a new task: 3D Data-EfficientPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3Dobject understanding with minimal 3D point cloud and text data pairs. Toaddress this task, we introduce GreenPLM, which leverages more text data tocompensate for the lack of 3D data. First, inspired by using CLIP to alignimages and text, we utilize a pre-trained point cloud-text encoder to map the3D point cloud space to the text space. This mapping leaves us to seamlesslyconnect the text space with LLMs. Once the point-text-LLM connection isestablished, we further enhance text-LLM alignment by expanding theintermediate text space, thereby reducing the reliance on 3D point cloud data.Specifically, we generate 6M free-text descriptions of 3D objects, and design athree-stage training strategy to help LLMs better explore the intrinsicconnections between different modalities. To achieve efficient modalityalignment, we design a zero-parameter cross-attention module for token pooling.Extensive experimental results show that GreenPLM requires only 12% of the 3Dtraining data used by existing state-of-the-art models to achieve superior 3Dunderstanding. Remarkably, GreenPLM also achieves competitive performance usingtext-only data. The code and weights are available at:https://github.com/TangYuan96/GreenPLM.</description><author>Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Jinfeng Xu, Yixue Hao, Long Hu, Min Chen</author><pubDate>Wed, 28 Aug 2024 17:38:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15966v1</guid></item><item><title>Flextron: Many-in-One Flexible Large Language Model</title><link>http://arxiv.org/abs/2406.10260v2</link><description>Training modern LLMs is extremely resource intensive, and customizing themfor various deployment scenarios characterized by limited compute and memoryresources through repeated training is impractical. In this paper, we introduceFlextron, a network architecture and post-training model optimization frameworksupporting flexible model deployment. The Flextron architecture utilizes anested elastic structure to rapidly adapt to specific user-defined latency andaccuracy targets during inference with no additional fine-tuning required. Itis also input-adaptive, and can automatically route tokens through itssub-networks for improved performance and efficiency. We present asample-efficient training method and associated routing algorithms forsystematically transforming an existing trained LLM into a Flextron model. Weevaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstratesuperior performance over multiple end-to-end trained variants and otherstate-of-the-art elastic networks, all with a single pretraining run thatconsumes a mere 7.63% tokens compared to original pretraining.</description><author>Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, Pavlo Molchanov</author><pubDate>Wed, 28 Aug 2024 17:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10260v2</guid></item><item><title>Efficient Slice Anomaly Detection Network for 3D Brain MRI Volume</title><link>http://arxiv.org/abs/2408.15958v1</link><description>Current anomaly detection methods excel with benchmark industrial data butstruggle with natural images and medical data due to varying definitions of'normal' and 'abnormal.' This makes accurate identification of deviations inthese fields particularly challenging. Especially for 3D brain MRI data, allthe state-of-the-art models are reconstruction-based with 3D convolutionalneural networks which are memory-intensive, time-consuming and producing noisyoutputs that require further post-processing. We propose a framework calledSimple Slice-based Network (SimpleSliceNet), which utilizes a model pre-trainedon ImageNet and fine-tuned on a separate MRI dataset as a 2D slice featureextractor to reduce computational cost. We aggregate the extracted features toperform anomaly detection tasks on 3D brain MRI volumes. Our model integrates aconditional normalizing flow to calculate log likelihood of features andemploys the Semi-Push-Pull Mechanism to enhance anomaly detection accuracy. Theresults indicate improved performance, showcasing our model's remarkableadaptability and effectiveness when addressing the challenges exists in brainMRI data. In addition, for the large-scale 3D brain volumes, our modelSimpleSliceNet outperforms the state-of-the-art 2D and 3D models in terms ofaccuracy, memory usage and time consumption. Code is available at:https://anonymous.4open.science/r/SimpleSliceNet-8EA3.</description><author>Zeduo Zhang, Yalda Mohsenzadeh</author><pubDate>Wed, 28 Aug 2024 17:20:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15958v1</guid></item><item><title>HER2 and FISH Status Prediction in Breast Biopsy H&amp;E-Stained Images Using Deep Learning</title><link>http://arxiv.org/abs/2408.13818v2</link><description>The current standard for detecting human epidermal growth factor receptor 2(HER2) status in breast cancer patients relies on HER2 amplification,identified through fluorescence in situ hybridization (FISH) orimmunohistochemistry (IHC). However, hematoxylin and eosin (H\&amp;E) tumor stainsare more widely available, and accurately predicting HER2 status using H\&amp;Ecould reduce costs and expedite treatment selection. Deep Learning algorithmsfor H&amp;E have shown effectiveness in predicting various cancer features andclinical outcomes, including moderate success in HER2 status prediction. Inthis work, we employed a customized weak supervision classification techniquecombined with MoCo-v2 contrastive learning to predict HER2 status. We trainedour pipeline on 182 publicly available H&amp;E Whole Slide Images (WSIs) from TheCancer Genome Atlas (TCGA), for which annotations by the pathology team at YaleSchool of Medicine are publicly available. Our pipeline achieved an Area Underthe Curve (AUC) of 0.85 across four different test folds. Additionally, wetested our model on 44 H&amp;E slides from the TCGA-BRCA dataset, which had an HER2score of 2+ and included corresponding HER2 status and FISH test results. Thesecases are considered equivocal for IHC, requiring an expensive FISH test ontheir IHC slides for disambiguation. Our pipeline demonstrated an AUC of 0.81on these challenging H&amp;E slides. Reducing the need for FISH test can havesignificant implications in cancer treatment equity for underservedpopulations.</description><author>Ardhendu Sekhar, Vrinda Goel, Garima Jain, Abhijeet Patil, Ravi Kant Gupta, Amit Sethi</author><pubDate>Wed, 28 Aug 2024 17:19:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13818v2</guid></item><item><title>Halfway Escape Optimization: A Quantum-Inspired Solution for General Optimization Problems</title><link>http://arxiv.org/abs/2405.02850v5</link><description>This paper first proposes the Halfway Escape Optimization (HEO) algorithm, aquantum-inspired metaheuristic designed to address general optimizationproblems characterized by rugged landscapes and high-dimensionality with anefficient convergence rate. The study presents a comprehensive comparativeevaluation of HEO's performance against established optimization algorithms,including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), ArtificialFish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behavedParticle Swarm Optimization (QPSO). The primary analysis encompasses 14benchmark functions with dimension 30, demonstrating HEO's effectiveness andadaptability in navigating general optimization problems and providing valuableinsights into its performance. The test of HEO in Pressure Vessel Design andTubular Column Design infers its feasibility and potential in real-timeapplications. Further validation in Osmancik-97 and Cammeo Rice Classificationproves the effectiveness of HEO and achieves a higher accuracy record.</description><author>Jiawen Li, Anwar PP Abdul Majeed, Pascal Lefevre</author><pubDate>Wed, 28 Aug 2024 17:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02850v5</guid></item><item><title>Generating Binary Species Range Maps</title><link>http://arxiv.org/abs/2408.15956v1</link><description>Accurately predicting the geographic ranges of species is crucial forassisting conservation efforts. Traditionally, range maps were manually createdby experts. However, species distribution models (SDMs) and, more recently,deep learning-based variants offer a potential automated alternative. Deeplearning-based SDMs generate a continuous probability representing thepredicted presence of a species at a given location, which must be binarized bysetting per-species thresholds to obtain binary range maps. However, selectingappropriate per-species thresholds to binarize these predictions is non-trivialas different species can require distinct thresholds. In this work, we evaluatedifferent approaches for automatically identifying the best thresholds forbinarizing range maps using presence-only data. This includes approaches thatrequire the generation of additional pseudo-absence data, along with ones thatonly require presence data. We also propose an extension of an existingpresence-only technique that is more robust to outliers. We perform a detailedevaluation of different thresholding techniques on the tasks of binary rangeestimation and large-scale fine-grained visual classification, and wedemonstrate improved performance over existing pseudo-absence free approachesusing our method.</description><author>Filip Dorm, Christian Lange, Scott Loarie, Oisin Mac Aodha</author><pubDate>Wed, 28 Aug 2024 17:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15956v1</guid></item><item><title>Fall Detection for Smart Living using YOLOv5</title><link>http://arxiv.org/abs/2408.15955v1</link><description>This work introduces a fall detection system using the YOLOv5mu model, whichachieved a mean average precision (mAP) of 0.995, demonstrating exceptionalaccuracy in identifying fall events within smart home environments. Enhanced byadvanced data augmentation techniques, the model demonstrates significantrobustness and adaptability across various conditions. The integration ofYOLOv5mu offers precise, real-time fall detection, which is crucial forimproving safety and emergency response for residents. Future research willfocus on refining the system by incorporating contextual data and exploringmulti-sensor approaches to enhance its performance and practical applicabilityin diverse environments.</description><author>Gracile Astlin Pereira</author><pubDate>Wed, 28 Aug 2024 17:14:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15955v1</guid></item><item><title>InstanSeg: an embedding-based instance segmentation algorithm optimized for accurate, efficient and portable cell segmentation</title><link>http://arxiv.org/abs/2408.15954v1</link><description>Cell and nucleus segmentation are fundamental tasks for quantitative bioimageanalysis. Despite progress in recent years, biologists and other domain expertsstill require novel algorithms to handle increasingly large and complexreal-world datasets. These algorithms must not only achieve state-of-the-artaccuracy, but also be optimized for efficiency, portability anduser-friendliness. Here, we introduce InstanSeg: a novel embedding-basedinstance segmentation pipeline designed to identify cells and nuclei inmicroscopy images. Using six public cell segmentation datasets, we demonstratethat InstanSeg can significantly improve accuracy when compared to the mostwidely used alternative methods, while reducing the processing time by at least60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScriptand supports GPU acceleration on a range of hardware. We provide an open-sourceimplementation of InstanSeg in Python, in addition to a user-friendly,interactive QuPath extension for inference written in Java. Our code andpre-trained models are available at https://github.com/instanseg/instanseg .</description><author>Thibaut Goldsborough, Ben Philps, Alan O'Callaghan, Fiona Inglis, Leo Leplat, Andrew Filby, Hakan Bilen, Peter Bankhead</author><pubDate>Wed, 28 Aug 2024 17:14:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15954v1</guid></item><item><title>Modeling and Analyzing the Influence of Non-Item Pages on Sequential Next-Item Prediction</title><link>http://arxiv.org/abs/2408.15953v1</link><description>Analyzing the sequence of historical interactions between users and items,sequential recommendation models learn user intent and make predictions aboutthe next item of interest. Next to these item interactions, most systems alsohave interactions with pages not related to specific items, for examplenavigation pages, account pages, and pages for a specific category, which mayprovide additional insights into the user's interests. However, while there areseveral approaches to integrate additional information about items and users,the topic of integrating non-item pages has been less explored. We use thehypotheses testing framework HypTrails to show that there is indeed arelationship between these non-item pages and the items of interest and fillthis gap by proposing various approaches of representing non-item pages (e.g,based on their content) to use them as an additional information source for thetask of sequential next-item prediction. We create a synthetic dataset with non-item pages highly related to thesubsequent item to show that the models are generally capable of learning fromthese interactions, and subsequently evaluate the improvements gained byincluding non-item pages in two real-world datasets. We adapt eight popular sequential recommender models, covering CNN-, RNN- andtransformer-based architectures, to integrate non-item pages and investigatethe capabilities of these models to leverage their information for next itemprediction. We also analyze their behavior on noisy data and compare differentitem representation strategies. Our results show that non-item pages are a valuable source of information,but representing such a page well is the key to successfully leverage them. Theinclusion of non-item pages can increase the performance for next-itemprediction in all examined model architectures with a varying degree.</description><author>Elisabeth Fischer, Daniel Schlör, Albin Zehe, Andreas Hotho</author><pubDate>Wed, 28 Aug 2024 17:12:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15953v1</guid></item><item><title>Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games</title><link>http://arxiv.org/abs/2408.15950v1</link><description>Recent advancements in large language models (LLMs) have expanded theircapabilities beyond traditional text-based tasks to multimodal domains,integrating visual, auditory, and textual data. While multimodal LLMs have beenextensively explored for high-level planning in domains like robotics andgames, their potential as low-level controllers remains largely untapped. Thispaper explores the application of multimodal LLMs as low-level controllers inthe domain of Atari video games, introducing Atari game performance as a newbenchmark for evaluating the ability of multimodal LLMs to perform low-levelcontrol tasks. Unlike traditional reinforcement learning (RL) and imitationlearning (IL) methods that require extensive computational resources as well asreward function specification, these LLMs utilize pre-existing multimodalknowledge to directly engage with game environments. Our study assessesmultiple multimodal LLMs performance against traditional RL agents, humanplayers, and random agents, focusing on their ability to understand andinteract with complex visual scenes and formulate strategic responses.Additionally, we examine the impact of In-Context Learning (ICL) byincorporating human-demonstrated game-play trajectories to enhance the modelscontextual understanding. Through this investigation, we aim to determine theextent to which multimodal LLMs can leverage their extensive training toeffectively function as low-level controllers, thereby redefining potentialapplications in dynamic and visually complex environments. Additional resultsand videos are available at our project webpage:https://sites.google.com/view/atari-gpt/.</description><author>Nicholas R. Waytowich, Devin White, MD Sunbeam, Vinicius G. Goecks</author><pubDate>Wed, 28 Aug 2024 17:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15950v1</guid></item><item><title>Proceedings of The second international workshop on eXplainable AI for the Arts (XAIxArts)</title><link>http://arxiv.org/abs/2406.14485v6</link><description>This second international workshop on explainable AI for the Arts (XAIxArts)brought together a community of researchers in HCI, Interaction Design, AI,explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.Workshop held at the 16th ACM Conference on Creativity and Cognition (C&amp;C2024), Chicago, USA.</description><author>Nick Bryan-Kinns, Corey Ford, Shuoyang Zheng, Helen Kennedy, Alan Chamberlain, Makayla Lewis, Drew Hemment, Zijin Li, Qiong Wu, Lanxi Xiao, Gus Xia, Jeba Rezwana, Michael Clemens, Gabriel Vigliensoni</author><pubDate>Wed, 28 Aug 2024 17:08:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14485v6</guid></item><item><title>Auxiliary Input in Training: Incorporating Catheter Features into Deep Learning Models for ECG-Free Dynamic Coronary Roadmapping</title><link>http://arxiv.org/abs/2408.15947v1</link><description>Dynamic coronary roadmapping is a technology that overlays the vessel maps(the "roadmap") extracted from an offline image sequence of X-ray angiographyonto a live stream of X-ray fluoroscopy in real-time. It aims to offernavigational guidance for interventional surgeries without the need forrepeated contrast agent injections, thereby reducing the risks associated withradiation exposure and kidney failure. The precision of the roadmaps iscontingent upon the accurate alignment of angiographic and fluoroscopic imagesbased on their cardiac phases, as well as precise catheter tip tracking. Theformer ensures the selection of a roadmap that closely matches the vessel shapein the current frame, while the latter uses catheter tips as reference pointsto adjust for translational motion between the roadmap and the present vesseltree. Training deep learning models for both tasks is challenging andunderexplored. However, incorporating catheter features into the models couldoffer substantial benefits, given humans heavily rely on catheters to completethe tasks. To this end, we introduce a simple but effective method, auxiliaryinput in training (AIT), and demonstrate that it enhances model performanceacross both tasks, outperforming baseline methods in knowledge incorporationand transfer learning.</description><author>Yikang Liu, Lin Zhao, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun</author><pubDate>Wed, 28 Aug 2024 17:05:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15947v1</guid></item><item><title>Sigma Flows for Image and Data Labeling and Learning Structured Prediction</title><link>http://arxiv.org/abs/2408.15946v1</link><description>This paper introduces the sigma flow model for the prediction of structuredlabelings of data observed on Riemannian manifolds, including Euclidean imagedomains as special case. The approach combines the Laplace-Beltrami frameworkfor image denoising and enhancement, introduced by Sochen, Kimmel and Malladiabout 25 years ago, and the assignment flow approach introduced and studied bythe authors. The sigma flow arises as Riemannian gradient flow of generalized harmonicenergies and thus is governed by a nonlinear geometric PDE which determines aharmonic map from a closed Riemannian domain manifold to a statisticalmanifold, equipped with the Fisher-Rao metric from information geometry. Aspecific ingredient of the sigma flow is the mutual dependency of theRiemannian metric of the domain manifold on the evolving state. This makes theapproach amenable to machine learning in a specific way, by realizing thisdependency through a mapping with compact time-variant parametrization that canbe learned from data. Proof of concept experiments demonstrate the expressivityof the sigma flow model and prediction performance. Structural similarities to transformer network architectures and networksgenerated by the geometric integration of sigma flows are pointed out, whichhighlights the connection to deep learning and, conversely, may stimulate theuse of geometric design principles for structured prediction in other areas ofscientific machine learning.</description><author>Jonas Cassel, Bastian Boll, Stefania Petra, Peter Albers, Christoph Schnörr</author><pubDate>Wed, 28 Aug 2024 17:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15946v1</guid></item><item><title>SCP: Soft Conditional Prompt Learning for Aerial Video Action Recognition</title><link>http://arxiv.org/abs/2305.12437v4</link><description>We present a new learning approach, Soft Conditional Prompt Learning (SCP),which leverages the strengths of prompt learning for aerial video actionrecognition. Our approach is designed to predict the action of each agent byhelping the models focus on the descriptions or instructions associated withactions in the input videos for aerial/robot visual perception. Our formulationsupports various prompts, including learnable prompts, auxiliary visualinformation, and large vision models to improve the recognition performance. Wepresent a soft conditional prompt method that learns to dynamically generateprompts from a pool of prompt experts under different video inputs. By sharingthe same objective with the task, our proposed SCP can optimize prompts thatguide the model's predictions while explicitly learning input-invariant (promptexperts pool) and input-specific (data-dependent) prompt knowledge. Inpractice, we observe a 3.17-10.2% accuracy improvement on the aerial videodatasets (Okutama, NECDrone), which consist of scenes with single-agent andmulti-agent actions. We further evaluate our approach on ground camera videosto verify the effectiveness and generalization and achieve a 1.0-3.6%improvement on dataset SSV2. We integrate our method into the ROS2 as well.</description><author>Xijun Wang, Ruiqi Xian, Tianrui Guan, Fuxiao Liu, Dinesh Manocha</author><pubDate>Wed, 28 Aug 2024 16:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12437v4</guid></item><item><title>Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model</title><link>http://arxiv.org/abs/2402.09786v4</link><description>Generative adversarial networks (GANs) generate photorealistic faces that areoften indistinguishable by humans from real faces. While biases in machinelearning models are often assumed to be due to biases in training data, we findpathological internal color and luminance biases in the discriminator of apre-trained StyleGAN3-r model that are not explicable by the training data. Wealso find that the discriminator systematically stratifies scores by bothimage- and face-level qualities and that this disproportionately affects imagesacross gender, race, and other categories. We examine axes common in researchon stereotyping in social psychology.</description><author>Alvin Grissom II, Ryan F. Lei, Matt Gusdorff, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter</author><pubDate>Wed, 28 Aug 2024 16:48:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09786v4</guid></item><item><title>Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot Learning</title><link>http://arxiv.org/abs/2408.15924v1</link><description>Few-shot image classification is a challenging task in the field of machinelearning, involving the identification of new categories using a limited numberof labeled samples. In recent years, methods based on local descriptors havemade significant progress in this area. However, the key to improvingclassification accuracy lies in effectively filtering background noise andaccurately selecting critical local descriptors highly relevant to imagecategory information. To address this challenge, we propose an innovative weighted adaptivethreshold filtering (WATF) strategy for local descriptors. This strategy candynamically adjust based on the current task and image context, therebyselecting local descriptors most relevant to the image category. This enablesthe model to better focus on category-related information while effectivelymitigating interference from irrelevant background regions. To evaluate the effectiveness of our method, we adopted the N-way K-shotexperimental framework. Experimental results show that our method not onlyimproves the clustering effect of selected local descriptors but alsosignificantly enhances the discriminative ability between image categories.Notably, our method maintains a simple and lightweight design philosophywithout introducing additional learnable parameters. This feature ensuresconsistency in filtering capability during both training and testing phases,further enhancing the reliability and practicality of the method.</description><author>Bingchen Yan</author><pubDate>Wed, 28 Aug 2024 16:36:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15924v1</guid></item><item><title>Generalized Naive Bayes</title><link>http://arxiv.org/abs/2408.15923v1</link><description>In this paper we introduce the so-called Generalized Naive Bayes structure asan extension of the Naive Bayes structure. We give a new greedy algorithm thatfinds a good fitting Generalized Naive Bayes (GNB) probability distribution. Weprove that this fits the data at least as well as the probability distributiondetermined by the classical Naive Bayes (NB). Then, under a not veryrestrictive condition, we give a second algorithm for which we can prove thatit finds the optimal GNB probability distribution, i.e. best fitting structurein the sense of KL divergence. Both algorithms are constructed to maximize theinformation content and aim to minimize redundancy. Based on these algorithms,new methods for feature selection are introduced. We discuss the similaritiesand differences to other related algorithms in terms of structure, methodology,and complexity. Experimental results show, that the algorithms introducedoutperform the related algorithms in many cases.</description><author>Edith Alice Kovács, Anna Ország, Dániel Pfeifer, András Benczúr</author><pubDate>Wed, 28 Aug 2024 16:36:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15923v1</guid></item><item><title>DiffAge3D: Diffusion-based 3D-aware Face Aging</title><link>http://arxiv.org/abs/2408.15922v1</link><description>Face aging is the process of converting an individual's appearance to ayounger or older version of themselves. Existing face aging techniques havebeen limited to 2D settings, which often weaken their applications as there isa growing demand for 3D face modeling. Moreover, existing aging methodsstruggle to perform faithful aging, maintain identity, and retain the finedetails of the input images. Given these limitations and the need for a3D-aware aging method, we propose DiffAge3D, the first 3D-aware aging frameworkthat not only performs faithful aging and identity preservation but alsooperates in a 3D setting. Our aging framework allows to model the aging andcamera pose separately by only taking a single image with a target age. Ourframework includes a robust 3D-aware aging dataset generation pipeline byutilizing a pre-trained 3D GAN and the rich text embedding capabilities withinCLIP model. Notably, we do not employ any inversion bottleneck in datasetgeneration. Instead, we randomly generate training samples from the latentspace of 3D GAN, allowing us to manipulate the rich latent space of GAN togenerate ages even with large gaps. With the generated dataset, we train aviewpoint-aware diffusion-based aging model to control the camera pose andfacial age. Through quantitative and qualitative evaluations, we demonstratethat DiffAge3D outperforms existing methods, particularly inmultiview-consistent aging and fine details preservation.</description><author>Junaid Wahid, Fangneng Zhan, Pramod Rao, Christian Theobalt</author><pubDate>Wed, 28 Aug 2024 16:36:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15922v1</guid></item><item><title>Multi-modal Adversarial Training for Zero-Shot Voice Cloning</title><link>http://arxiv.org/abs/2408.15916v1</link><description>A text-to-speech (TTS) model trained to reconstruct speech given text tendstowards predictions that are close to the average characteristics of a dataset,failing to model the variations that make human speech sound natural. Thisproblem is magnified for zero-shot voice cloning, a task that requires trainingdata with high variance in speaking styles. We build off of recent works whichhave used Generative Advsarial Networks (GAN) by proposing a Transformerencoder-decoder architecture to conditionally discriminates between real andgenerated speech features. The discriminator is used in a training pipelinethat improves both the acoustic and prosodic features of a TTS model. Weintroduce our novel adversarial training technique by applying it to aFastSpeech2 acoustic model and training on Libriheavy, a large multi-speakerdataset, for the task of zero-shot voice cloning. Our model achievesimprovements over the baseline in terms of speech quality and speakersimilarity. Audio examples from our system are available online.</description><author>John Janiczek, Dading Chong, Dongyang Dai, Arlo Faria, Chao Wang, Tao Wang, Yuzong Liu</author><pubDate>Wed, 28 Aug 2024 16:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15916v1</guid></item><item><title>Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models</title><link>http://arxiv.org/abs/2408.15915v1</link><description>The cultivation of expertise for large language models (LLMs) to solve tasksof specific areas often requires special-purpose tuning with calibratedbehaviors on the expected stable outputs. To avoid huge cost brought by manualpreparation of instruction datasets and training resources up to hundreds ofhours, the exploitation of open knowledge including a wealth of low rankadaptation (LoRA) models and instruction datasets serves as a good startingpoint. However, existing methods on model and data selection focus on theperformance of general-purpose capabilities while neglecting the knowledge gapexposed in domain-specific deployment. In the present study, we propose tobridge such gap by introducing few human-annotated samples (i.e., K-shot) foradvancing task expertise of LLMs with open knowledge. Specifically, we developan efficient and scalable pipeline to cost-efficiently produce task expertswhere K-shot data intervene in selecting the most promising expert candidatesand the task-relevant instructions. A mixture-of-expert (MoE) system is builtto make the best use of individual-yet-complementary knowledge between multipleexperts. We unveil the two keys to the success of a MoE system, 1) the abidanceby K-shot, and 2) the insistence on diversity. For the former, we ensure thatmodels that truly possess problem-solving abilities on K-shot are selectedrather than those blind guessers. Besides, during data selection, instructionsthat share task-relevant contexts with K-shot are prioritized. For the latter,we highlight the diversity of constituting experts and that of the fine-tuninginstructions throughout the model and data selection process. Extensiveexperimental results confirm the superiority of our approach over existingmethods on utilization of open knowledge across various tasks. Codes and modelswill be released later.</description><author>Yuncheng Yang, Yulei Qin, Tong Wu, Zihan Xu, Gang Li, Pengcheng Guo, Hang Shao, Yucheng Shi, Ke Li, Xing Sun, Jie Yang, Yun Gu</author><pubDate>Wed, 28 Aug 2024 16:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15915v1</guid></item><item><title>CoRe: Context-Regularized Text Embedding Learning for Text-to-Image Personalization</title><link>http://arxiv.org/abs/2408.15914v1</link><description>Recent advances in text-to-image personalization have enabled high-qualityand controllable image synthesis for user-provided concepts. However, existingmethods still struggle to balance identity preservation with text alignment.Our approach is based on the fact that generating prompt-aligned imagesrequires a precise semantic understanding of the prompt, which involvesaccurately processing the interactions between the new concept and itssurrounding context tokens within the CLIP text encoder. To address this, weaim to embed the new concept properly into the input embedding space of thetext encoder, allowing for seamless integration with existing tokens. Weintroduce Context Regularization (CoRe), which enhances the learning of the newconcept's text embedding by regularizing its context tokens in the prompt. Thisis based on the insight that appropriate output vectors of the text encoder forthe context tokens can only be achieved if the new concept's text embedding iscorrectly learned. CoRe can be applied to arbitrary prompts without requiringthe generation of corresponding images, thus improving the generalization ofthe learned text embedding. Additionally, CoRe can serve as a test-timeoptimization technique to further enhance the generations for specific prompts.Comprehensive experiments demonstrate that our method outperforms severalbaseline methods in both identity preservation and text alignment. Code will bemade publicly available.</description><author>Feize Wu, Yun Pang, Junyi Zhang, Lianyu Pang, Jian Yin, Baoquan Zhao, Qing Li, Xudong Mao</author><pubDate>Wed, 28 Aug 2024 16:27:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15914v1</guid></item><item><title>Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles in Public Policy Documents</title><link>http://arxiv.org/abs/2311.11844v3</link><description>Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4promise automation with better results and less programming, opening up newopportunities for text analysis in political science. In this study, weevaluate LLMs on three original coding tasks involving typical complexitiesencountered in political science settings: a non-English language, legal andpolitical jargon, and complex labels based on abstract constructs. Along thepaper, we propose a practical workflow to optimize the choice of the model andthe prompt. We find that the best prompting strategy consists of providing theLLMs with a detailed codebook, as the one provided to human coders. In thissetting, an LLM can be as good as or possibly better than a human annotatorwhile being much faster, considerably cheaper, and much easier to scale tolarge amounts of text. We also provide a comparison of GPT and popularopen-source LLMs, discussing the trade-offs in the model's choice. Our softwareallows LLMs to be easily used as annotators and is publicly available:https://github.com/lorelupo/pappa.</description><author>Lorenzo Lupo, Oscar Magnusson, Dirk Hovy, Elin Naurin, Lena Wängnerud</author><pubDate>Wed, 28 Aug 2024 16:26:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11844v3</guid></item><item><title>Infusion: internal diffusion for inpainting of dynamic textures and complex motion</title><link>http://arxiv.org/abs/2311.01090v3</link><description>Video inpainting is the task of filling a region in a video in a visuallyconvincing manner. It is very challenging due to the high dimensionality of thedata and the temporal consistency required for obtaining convincing results.Recently, diffusion models have shown impressive results in modeling complexdata distributions, including images and videos. Such models remain nonethelessvery expensive to train and to perform inference with, which strongly reducetheir applicability to videos, and yields unreasonable computational loads. Weshow that in the case of video inpainting, thanks to the highly auto-similarnature of videos, the training data of a diffusion model can be restricted tothe input video and still produce very satisfying results. This leads us toadopt an internal learning approach, which also allows us to greatly reduce theneural network size by about three orders of magnitude less than currentdiffusion models used for image inpainting. We also introduce a new method forefficient training and inference of diffusion models in the context of internallearning, by splitting the diffusion process into different learning intervalscorresponding to different noise levels of the diffusion process. To the bestof our knowledge, this is the first video inpainting method based purely ondiffusion. Other methods require additional components such as optical flowestimation, which limits their performance in the case of dynamic textures andcomplex motions. We show qualitative and quantitative results, demonstratingthat our method reaches state of the art performance in the case of dynamictextures and complex dynamic backgrounds.</description><author>Nicolas Cherel, Andrés Almansa, Yann Gousseau, Alasdair Newson</author><pubDate>Wed, 28 Aug 2024 16:23:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01090v3</guid></item><item><title>MetaGFN: Exploring Distant Modes with Adapted Metadynamics for Continuous GFlowNets</title><link>http://arxiv.org/abs/2408.15905v1</link><description>Generative Flow Networks (GFlowNets) are a class of generative models thatsample objects in proportion to a specified reward function through a learnedpolicy. They can be trained either on-policy or off-policy, needing a balancebetween exploration and exploitation for fast convergence to a targetdistribution. While exploration strategies for discrete GFlowNets have beenstudied, exploration in the continuous case remains to be investigated, despitethe potential for novel exploration algorithms due to the local connectednessof continuous domains. Here, we introduce Adapted Metadynamics, a variant ofmetadynamics that can be applied to arbitrary black-box reward functions oncontinuous domains. We use Adapted Metadynamics as an exploration strategy forcontinuous GFlowNets. We show three continuous domains where the resultingalgorithm, MetaGFN, accelerates convergence to the target distribution anddiscovers more distant reward modes than previous off-policy explorationstrategies used for GFlowNets.</description><author>Dominic Phillips, Flaviu Cipcigan</author><pubDate>Wed, 28 Aug 2024 16:19:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15905v1</guid></item><item><title>LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments</title><link>http://arxiv.org/abs/2408.15903v1</link><description>The rapid obsolescence of information in Large Language Models (LLMs) hasdriven the development of various techniques to incorporate new facts. However,existing methods for knowledge editing still face difficulties with multi-hopquestions that require accurate fact identification and sequential logicalreasoning, particularly among numerous fact updates. To tackle thesechallenges, this paper introduces Graph Memory-based Editing for Large LanguageModels (GMeLLo), a straitforward and effective method that merges the explicitknowledge representation of Knowledge Graphs (KGs) with the linguisticflexibility of LLMs. Beyond merely leveraging LLMs for question answering,GMeLLo employs these models to convert free-form language into structuredqueries and fact triples, facilitating seamless interaction with KGs for rapidupdates and precise multi-hop reasoning. Our results show that GMeLLosignificantly surpasses current state-of-the-art knowledge editing methods inthe multi-hop question answering benchmark, MQuAKE, especially in scenarioswith extensive knowledge edits.</description><author>Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai</author><pubDate>Wed, 28 Aug 2024 16:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15903v1</guid></item><item><title>Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts</title><link>http://arxiv.org/abs/2408.15901v1</link><description>Efficiency, specialization, and adaptability to new data distributions arequalities that are hard to combine in current Large Language Models. TheMixture of Experts (MoE) architecture has been the focus of significantresearch because its inherent conditional computation enables such desirableproperties. In this work, we focus on "upcycling" dense expert models into anMoE, aiming to improve specialization while also adding the ability to adapt tonew tasks easily. We introduce Nexus, an enhanced MoE architecture withadaptive routing where the model learns to project expert embeddings fromdomain representations. This approach allows Nexus to flexibly add new expertsafter the initial upcycling through separately trained dense models, withoutrequiring large-scale MoE training for unseen data domains. Our experimentsshow that Nexus achieves a relative gain of up to 2.1% over the baseline forinitial upcycling, and a 18.8% relative gain for extending the MoE with a newexpert by using limited finetuning data. This flexibility of Nexus is crucialto enable an open-source ecosystem where every user continuously assemblestheir own MoE-mix according to their needs.</description><author>Nikolas Gritsch, Qizhen Zhang, Acyr Locatelli, Sara Hooker, Ahmet Üstün</author><pubDate>Wed, 28 Aug 2024 16:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15901v1</guid></item><item><title>Gen-Swarms: Adapting Deep Generative Models to Swarms of Drones</title><link>http://arxiv.org/abs/2408.15899v1</link><description>Gen-Swarms is an innovative method that leverages and combines thecapabilities of deep generative models with reactive navigation algorithms toautomate the creation of drone shows. Advancements in deep generative models,particularly diffusion models, have demonstrated remarkable effectiveness ingenerating high-quality 2D images. Building on this success, various works haveextended diffusion models to 3D point cloud generation. In contrast,alternative generative models such as flow matching have been proposed,offering a simple and intuitive transition from noise to meaningful outputs.However, the application of flow matching models to 3D point cloud generationremains largely unexplored. Gen-Swarms adapts these models to automaticallygenerate drone shows. Existing 3D point cloud generative models create pointtrajectories which are impractical for drone swarms. In contrast, our methodnot only generates accurate 3D shapes but also guides the swarm motion,producing smooth trajectories and accounting for potential collisions through areactive navigation algorithm incorporated into the sampling process. Forexample, when given a text category like Airplane, Gen-Swarms can rapidly andcontinuously generate numerous variations of 3D airplane shapes. Ourexperiments demonstrate that this approach is particularly well-suited fordrone shows, providing feasible trajectories, creating representative finalshapes, and significantly enhancing the overall performance of drone showgeneration.</description><author>Carlos Plou, Pablo Pueyo, Ruben Martinez-Cantin, Mac Schwager, Ana C. Murillo, Eduardo Montijano</author><pubDate>Wed, 28 Aug 2024 16:12:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15899v1</guid></item><item><title>Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil Generation</title><link>http://arxiv.org/abs/2408.15898v1</link><description>The design of aerodynamic shapes, such as airfoils, has traditionallyrequired significant computational resources and relied on predefined designparameters, which limit the potential for novel shape synthesis. In this work,we introduce a data-driven methodology for airfoil generation using a diffusionmodel. Trained on a dataset of preexisting airfoils, our model can generate anarbitrary number of new airfoils from random vectors, which can be conditionedon specific aerodynamic performance metrics such as lift and drag, or geometriccriteria. Our results demonstrate that the diffusion model effectively producesairfoil shapes with realistic aerodynamic properties, offering substantialimprovements in efficiency, flexibility, and the potential for discoveringinnovative airfoil designs. This approach significantly expands the designspace, facilitating the synthesis of high-performance aerodynamic shapes thattranscend the limitations of traditional methods.</description><author>Reid Graves, Amir Barati Farimani</author><pubDate>Wed, 28 Aug 2024 16:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15898v1</guid></item><item><title>A New Method for Cross-Lingual-based Semantic Role Labeling</title><link>http://arxiv.org/abs/2408.15896v1</link><description>Semantic role labeling is a crucial task in natural language processing,enabling better comprehension of natural language. However, the lack ofannotated data in multiple languages has posed a challenge for researchers. Toaddress this, a deep learning algorithm based on model transfer has beenproposed. The algorithm utilizes a dataset consisting of the English portion ofCoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiencyof training, only ten percent of the educational data from each language isused. The results of the proposed model demonstrate significant improvementscompared to Niksirt et al.'s model. In monolingual mode, the proposed modelachieved a 2.05 percent improvement on F1-score, while in cross-lingual mode,the improvement was even more substantial, reaching 6.23 percent. Worth notingis that the compared model only trained two of the four stages of semantic rolelabeling and employed golden data for the remaining two stages. This suggeststhat the actual superiority of the proposed model surpasses the reportednumbers by a significant margin. The development of cross-lingual methods forsemantic role labeling holds promise, particularly in addressing the scarcityof annotated data for various languages. These advancements pave the way forfurther research in understanding and processing natural language acrossdifferent linguistic contexts.</description><author>Mohammad Ebrahimi, Behrouz Minaei Bidgoli, Nasim Khozouei</author><pubDate>Wed, 28 Aug 2024 16:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15896v1</guid></item><item><title>Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models</title><link>http://arxiv.org/abs/2408.15895v1</link><description>Human coders are biased. We test similar biases in Large Language Models(LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik andMeyer (2018), we find evidence that LLMs use political information, andspecifically party cues, to judge political statements. Not only do LLMs userelevant information to contextualize whether a statement is positive,negative, or neutral based on the party cue, they also reflect the biases ofthe human-generated data upon which they have been trained. We also find thatunlike humans, who are only biased when faced with statements from extremeparties, LLMs exhibit significant bias even when prompted with statements fromcenter-left and center-right parties. The implications of our findings arediscussed in the conclusion.</description><author>Sebastian Vallejo Vera, Hunter Driggers</author><pubDate>Wed, 28 Aug 2024 16:05:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15895v1</guid></item><item><title>The Role of Fibration Symmetries in Geometric Deep Learning</title><link>http://arxiv.org/abs/2408.15894v1</link><description>Geometric Deep Learning (GDL) unifies a broad class of machine learningtechniques from the perspectives of symmetries, offering a framework forintroducing problem-specific inductive biases like Graph Neural Networks(GNNs). However, the current formulation of GDL is limited to global symmetriesthat are not often found in real-world problems. We propose to relax GDL toallow for local symmetries, specifically fibration symmetries in graphs, toleverage regularities of realistic instances. We show that GNNs apply theinductive bias of fibration symmetries and derive a tighter upper bound fortheir expressive power. Additionally, by identifying symmetries in networks, wecollapse network nodes, thereby increasing their computational efficiencyduring both inference and training of deep neural networks. The mathematicalextension introduced here applies beyond graphs to manifolds, bundles, andgrids for the development of models with inductive biases induced by localsymmetries that can lead to better generalization.</description><author>Osvaldo Velarde, Lucas Parra, Paolo Boldi, Hernan Makse</author><pubDate>Wed, 28 Aug 2024 16:04:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15894v1</guid></item><item><title>Disentangled Diffusion Autoencoder for Harmonization of Multi-site Neuroimaging Data</title><link>http://arxiv.org/abs/2408.15890v1</link><description>Combining neuroimaging datasets from multiple sites and scanners can helpincrease statistical power and thus provide greater insight into subtleneuroanatomical effects. However, site-specific effects pose a challenge bypotentially obscuring the biological signal and introducing unwanted variance.Existing harmonization techniques, which use statistical models to remove sucheffects, have been shown to incompletely remove site effects while also failingto preserve biological variability. More recently, generative models using GANsor autoencoder-based approaches, have been proposed for site adjustment.However, such methods are known for instability during training or blurry imagegeneration. In recent years, diffusion models have become increasingly popularfor their ability to generate high-quality synthetic images. In this work, weintroduce the disentangled diffusion autoencoder (DDAE), a novel diffusionmodel designed for controlling specific aspects of an image. We apply the DDAEto the task of harmonizing MR images by generating high-quality site-adjustedimages that preserve biological variability. We use data from 7 different sitesand demonstrate the DDAE's superiority in generating high-resolution,harmonized 2D MR images over previous approaches. As far as we are aware, thiswork marks the first diffusion-based model for site adjustment of neuroimagingdata.</description><author>Ayodeji Ijishakin, Ana Lawry Aguila, Elizabeth Levitis, Ahmed Abdulaal, Andre Altmann, James Cole</author><pubDate>Wed, 28 Aug 2024 16:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15890v1</guid></item><item><title>SpineMamba: Enhancing 3D Spinal Segmentation in Clinical Imaging through Residual Visual Mamba Layers and Shape Priors</title><link>http://arxiv.org/abs/2408.15887v1</link><description>Accurate segmentation of 3D clinical medical images is critical in thediagnosis and treatment of spinal diseases. However, the inherent complexity ofspinal anatomy and uncertainty inherent in current imaging technologies, posessignificant challenges for semantic segmentation of spinal images. Althoughconvolutional neural networks (CNNs) and Transformer-based models have madesome progress in spinal segmentation, their limitations in handling long-rangedependencies hinder further improvements in segmentation accuracy.To addressthese challenges, we introduce a residual visual Mamba layer to effectivelycapture and model the deep semantic features and long-range spatialdependencies of 3D spinal data. To further enhance the structural semanticunderstanding of the vertebrae, we also propose a novel spinal shape priormodule that captures specific anatomical information of the spine from medicalimages, significantly enhancing the model's ability to extract structuralsemantic information of the vertebrae. Comparative and ablation experiments ontwo datasets demonstrate that SpineMamba outperforms existing state-of-the-artmodels. On the CT dataset, the average Dice similarity coefficient forsegmentation reaches as high as 94.40, while on the MR dataset, it reaches86.95. Notably, compared to the renowned nnU-Net, SpineMamba achieves superiorsegmentation performance, exceeding it by up to 2 percentage points. Thisunderscores its accuracy, robustness, and excellent generalizationcapabilities.</description><author>Zhiqing Zhang, Tianyong Liu, Guojia Fan, Bin Li, Qianjin Feng, Shoujun Zhou</author><pubDate>Wed, 28 Aug 2024 15:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15887v1</guid></item><item><title>Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2408.15886v1</link><description>In recent years, the evolution of machine learning techniques hassignificantly impacted the field of intrusion detection, particularly withinthe context of the Internet of Things (IoT). As IoT networks expand, the needfor robust security measures to counteract potential threats has becomeincreasingly critical. This paper introduces a hybrid Intrusion DetectionSystem (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs)with the XGBoost algorithm. Our proposed IDS leverages the unique capabilitiesof KANs, which utilize learnable activation functions to model complexrelationships within data, alongside the powerful ensemble learning techniquesof XGBoost, known for its high performance in classification tasks. This hybridapproach not only enhances the detection accuracy but also improves theinterpretability of the model, making it suitable for dynamic and intricate IoTenvironments. Experimental evaluations demonstrate that our hybrid IDS achievesan impressive detection accuracy exceeding 99% in distinguishing between benignand malicious activities. Additionally, we were able to achieve F1 scores,precision, and recall that exceeded 98%. Furthermore, we conduct a comparativeanalysis against traditional Multi-Layer Perceptron (MLP) networks, assessingperformance metrics such as Precision, Recall, and F1-score. The resultsunderscore the efficacy of integrating KANs with XGBoost, highlighting thepotential of this innovative approach to significantly strengthen the securityframework of IoT networks.</description><author>Amar Amouri, Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Ismail Butun, Imad Mahgoub</author><pubDate>Wed, 28 Aug 2024 15:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15886v1</guid></item><item><title>Secret Collusion among Generative AI Agents</title><link>http://arxiv.org/abs/2402.07510v2</link><description>Recent capability increases in large language models (LLMs) open upapplications in which groups of communicating generative AI agents solve jointtasks. This poses privacy and security challenges concerning the unauthorisedsharing of information, or other unwanted forms of agent coordination. Modernsteganographic techniques could render such dynamics hard to detect. In thispaper, we comprehensively formalise the problem of secret collusion in systemsof generative AI agents by drawing on relevant concepts from both AI andsecurity literature. We study incentives for the use of steganography, andpropose a variety of mitigation measures. Our investigations result in a modelevaluation framework that systematically tests capabilities required forvarious forms of secret collusion. We provide extensive empirical resultsacross a range of contemporary LLMs. While the steganographic capabilities ofcurrent models remain limited, GPT-4 displays a capability jump suggesting theneed for continuous monitoring of steganographic frontier model capabilities.We conclude by laying out a comprehensive research program to mitigate futurerisks of collusion between generative AI models.</description><author>Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H. S. Torr, Lewis Hammond, Christian Schroeder de Witt</author><pubDate>Wed, 28 Aug 2024 15:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07510v2</guid></item><item><title>LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation</title><link>http://arxiv.org/abs/2408.15881v1</link><description>We introduce LLaVA-MoD, a novel framework designed to enable the efficienttraining of small-scale Multimodal Language Models (s-MLLM) by distillingknowledge from large-scale MLLM (l-MLLM). Our approach tackles two fundamentalchallenges in MLLM distillation. First, we optimize the network structure ofs-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into thelanguage model, striking a balance between computational efficiency and modelexpressiveness. Second, we propose a progressive knowledge transfer strategy toensure comprehensive knowledge migration. This strategy begins with mimicdistillation, where we minimize the Kullback-Leibler (KL) divergence betweenoutput distributions to enable the student model to emulate the teachernetwork's understanding. Following this, we introduce preference distillationvia Direct Preference Optimization (DPO), where the key lies in treating l-MLLMas the reference model. During this phase, the s-MLLM's ability to discriminatebetween superior and inferior examples is significantly enhanced beyond l-MLLM,leading to a better student that surpasses its teacher, particularly inhallucination benchmarks. Extensive experiments demonstrate that LLaVA-MoDoutperforms existing models across various multimodal benchmarks whilemaintaining a minimal number of activated parameters and low computationalcosts. Remarkably, LLaVA-MoD, with only 2B activated parameters, surpassesQwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% ofthe training data and 23% trainable parameters. These results underscoreLLaVA-MoD's ability to effectively distill comprehensive knowledge from itsteacher model, paving the way for the development of more efficient MLLMs. Thecode will be available on: https://github.com/shufangxun/LLaVA-MoD.</description><author>Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, Siming Fu, Haoyuan Li, Bolin Li, Zhelun Yu, Si Liu, Hongsheng Li, Hao Jiang</author><pubDate>Wed, 28 Aug 2024 15:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15881v1</guid></item><item><title>Persuasion Games using Large Language Models</title><link>http://arxiv.org/abs/2408.15879v1</link><description>Large Language Models (LLMs) have emerged as formidable instruments capableof comprehending and producing human-like text. This paper explores thepotential of LLMs, to shape human perspectives and subsequently influence theirdecisions on particular tasks. This capability finds applications in diversedomains such as Investment, Credit cards and Insurance, wherein they assistusers in selecting appropriate insurance policies, investment plans, Creditcards, Retail, as well as in Behavioral Change Support Systems (BCSS). We present a sophisticated multi-agent framework wherein a consortium ofagents operate in collaborative manner. The primary agent engages directly withusers through persuasive dialogue, while the auxiliary agents perform taskssuch as information retrieval, response analysis, development of persuasionstrategies, and validation of facts. Empirical evidence from our experimentsdemonstrates that this collaborative methodology significantly enhances thepersuasive efficacy of the LLM. We analyze user resistance to persuasiveefforts continuously and counteract it by employing a combination of rule-basedand LLM-based resistance-persuasion mapping techniques. We employ simulated personas and generate conversations in insurance,banking, and retail domains to evaluate the proficiency of large languagemodels (LLMs) in recognizing, adjusting to, and influencing various personalitytypes. Concurrently, we examine the resistance mechanisms employed by LLMsimulated personas. Persuasion is quantified via measurable surveys before andafter interaction, LLM-generated scores on conversation, and user decisions(purchase or non-purchase).</description><author>Ganesh Prasath Ramani, Shirish Karande, Santhosh V, Yash Bhatia</author><pubDate>Wed, 28 Aug 2024 15:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15879v1</guid></item><item><title>GINN-KAN: Interpretability pipelining with applications in Physics Informed Neural Networks</title><link>http://arxiv.org/abs/2408.14780v2</link><description>Neural networks are powerful function approximators, yet their ``black-box"nature often renders them opaque and difficult to interpret. While manypost-hoc explanation methods exist, they typically fail to capture theunderlying reasoning processes of the networks. A truly interpretable neuralnetwork would be trained similarly to conventional models using techniques suchas backpropagation, but additionally provide insights into the learnedinput-output relationships. In this work, we introduce the concept ofinterpretability pipelineing, to incorporate multiple interpretabilitytechniques to outperform each individual technique. To this end, we firstevaluate several architectures that promise such interpretability, with aparticular focus on two recent models selected for their potential toincorporate interpretability into standard neural network architectures whilestill leveraging backpropagation: the Growing Interpretable Neural Network(GINN) and Kolmogorov Arnold Networks (KAN). We analyze the limitations andstrengths of each and introduce a novel interpretable neural network GINN-KANthat synthesizes the advantages of both models. When tested on the Feynmansymbolic regression benchmark datasets, GINN-KAN outperforms both GINN and KAN.To highlight the capabilities and the generalizability of this approach, weposition GINN-KAN as an alternative to conventional black-box networks inPhysics-Informed Neural Networks (PINNs). We expect this to have far-reachingimplications in the application of deep learning pipelines in the naturalsciences. Our experiments with this interpretable PINN on 15 different partialdifferential equations demonstrate that GINN-KAN augmented PINNs outperformPINNs with black-box networks in solving differential equations and surpass thecapabilities of both GINN and KAN.</description><author>Nisal Ranasinghe, Yu Xia, Sachith Seneviratne, Saman Halgamuge</author><pubDate>Wed, 28 Aug 2024 15:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14780v2</guid></item><item><title>Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation</title><link>http://arxiv.org/abs/2408.15876v1</link><description>In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2)pipeline to explore the training-free paradigm for audio andlanguage-referenced video object segmentation, namely AVS and RVOS tasks. Theintuitive solution leverages GroundingDINO to identify the target object from asingle frame and SAM 2 to segment the identified object throughout the video,which is less robust to spatiotemporal variations due to a lack of videocontext exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novelGPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to performtwo-step temporal-spatial reasoning for sequentially selecting pivot frames andpivot boxes, thereby providing SAM 2 with a high-quality initial object prompt.Within GPT-PS, two task-specific Chain-of-Thought prompts are designed tounleash GPT's temporal-spatial reasoning capacity by guiding GPT to makeselections based on a comprehensive understanding of video and referenceinformation. Furthermore, we propose a Language-Binded Reference Unification(LBRU) module to convert audio signals into language-formatted references,thereby unifying the formats of AVS and RVOS tasks in the same pipeline.Extensive experiments on both tasks show that our training-free AL-Ref-SAM 2pipeline achieves performances comparable to or even better thanfully-supervised fine-tuning methods. The code is available at:https://github.com/appletea233/AL-Ref-SAM2.</description><author>Shaofei Huang, Rui Ling, Hongyu Li, Tianrui Hui, Zongheng Tang, Xiaoming Wei, Jizhong Han, Si Liu</author><pubDate>Wed, 28 Aug 2024 15:47:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15876v1</guid></item><item><title>A Deep Learning Based Resource Allocator for Communication Systems with Dynamic User Utility Demands</title><link>http://arxiv.org/abs/2311.04600v2</link><description>Deep learning (DL) based resource allocation (RA) has recently gainedsignificant attention due to its performance efficiency. However, most relatedstudies assume an ideal case where the number of users and their utilitydemands, e.g., data rate constraints, are fixed, and the designed DL-based RAscheme exploits a policy trained only for these fixed parameters. Consequently,computationally complex policy retraining is required whenever these parameterschange. In this paper, we introduce a DL-based resource allocator (ALCOR) thatallows users to adjust their utility demands freely, such as based on theirapplication layer requirements. ALCOR employs deep neural networks (DNNs) asthe policy in a time-sharing problem. The underlying optimization algorithmiteratively optimizes the on-off status of users to satisfy their utilitydemands in expectation. The policy performs unconstrained RA (URA)--RA withoutconsidering user utility demands--among active users to maximize the sumutility (SU) at each time instant. Depending on the chosen URA scheme, ALCORcan perform RA in either a centralized or distributed scenario. Derivedconvergence analyses provide guarantees for ALCOR's convergence, and numericalexperiments corroborate its effectiveness.</description><author>Pourya Behmandpoor, Mark Eisen, Panagiotis Patrinos, Marc Moonen</author><pubDate>Wed, 28 Aug 2024 15:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04600v2</guid></item><item><title>Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)</title><link>http://arxiv.org/abs/2408.15874v1</link><description>Outlier detection algorithms typically assign an outlier score to eachobservation in a dataset, indicating the degree to which an observation is anoutlier. However, these scores are often not comparable across algorithms andcan be difficult for humans to interpret. Statistical scaling addresses thisproblem by transforming outlier scores into outlier probabilities without usingground-truth labels, thereby improving interpretability and comparabilityacross algorithms. However, the quality of this transformation can be differentfor outliers and inliers. Missing outliers in scenarios where they are ofparticular interest - such as healthcare, finance, or engineering - can becostly or dangerous. Thus, ensuring good probabilities for outliers isessential. This paper argues that statistical scaling, as commonly used in theliterature, does not produce equally good probabilities for outliers as forinliers. Therefore, we propose robust statistical scaling, which uses robustestimators to improve the probabilities for outliers. We evaluate severalvariants of our method against other outlier score transformations forreal-world datasets and outlier detection algorithms, where it can improve theprobabilities for outliers.</description><author>Philipp Röchner, Henrique O. Marques, Ricardo J. G. B. Campello, Arthur Zimek, Franz Rothlauf</author><pubDate>Wed, 28 Aug 2024 15:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15874v1</guid></item><item><title>HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus</title><link>http://arxiv.org/abs/2309.02731v3</link><description>ChatGPT has garnered significant interest due to its impressive performance;however, there is growing concern about its potential risks, particularly inthe detection of AI-generated content (AIGC), which is often challenging foruntrained individuals to identify. Current datasets used for detectingChatGPT-generated text primarily focus on question-answering tasks, oftenoverlooking tasks with semantic-invariant properties, such as summarization,translation, and paraphrasing. In this paper, we demonstrate that detectingmodel-generated text in semantic-invariant tasks is more challenging. Toaddress this gap, we introduce a more extensive and comprehensive dataset thatincorporates a wider range of tasks than previous work, including those withsemantic-invariant properties.</description><author>Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu</author><pubDate>Wed, 28 Aug 2024 15:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02731v3</guid></item><item><title>Geometric Neural Network based on Phase Space for BCI-EEG decoding</title><link>http://arxiv.org/abs/2403.05645v3</link><description>Objective: The integration of Deep Learning (DL) algorithms on brain signalanalysis is still in its nascent stages compared to their success in fieldslike Computer Vision. This is particularly true for BCI, where the brainactivity is decoded to control external devices without requiring musclecontrol. Electroencephalography (EEG) is a widely adopted choice for designingBCI systems due to its non-invasive and cost-effective nature and excellenttemporal resolution. Still, it comes at the expense of limited training data,poor signal-to-noise, and a large variability across and within-subjectrecordings. Finally, setting up a BCI system with many electrodes takes a longtime, hindering the widespread adoption of reliable DL architectures in BCIsoutside research laboratories. To improve adoption, we need to improve usercomfort using, for instance, reliable algorithms that operate with fewelectrodes. Approach: Our research aims to develop a DL algorithm that deliverseffective results with a limited number of electrodes. Taking advantage of theAugmented Covariance Method and the framework of SPDNet, we propose thePhase-SPDNet architecture and analyze its performance and the interpretabilityof the results. The evaluation is conducted on 5-fold cross-validation, usingonly three electrodes positioned above the Motor Cortex. The methodology wastested on nearly 100 subjects from several open-source datasets using theMother Of All BCI Benchmark (MOABB) framework. Main results: The results of ourPhase-SPDNet demonstrate that the augmented approach combined with the SPDNetsignificantly outperforms all the current state-of-the-art DL architecture inMI decoding. Significance: This new architecture is explainable and with a lownumber of trainable parameters.</description><author>Igor Carrara, Bruno Aristimunha, Marie-Constance Corsi, Raphael Y. de Camargo, Sylvain Chevallier, Théodore Papadopoulo</author><pubDate>Wed, 28 Aug 2024 15:39:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05645v3</guid></item><item><title>GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model</title><link>http://arxiv.org/abs/2408.15868v1</link><description>Autonomous driving training requires a diverse range of datasets encompassingvarious traffic conditions, weather scenarios, and road types. Traditional dataaugmentation methods often struggle to generate datasets that represent rareoccurrences. To address this challenge, we propose GenDDS, a novel approach forgenerating driving scenarios generation by leveraging the capabilities ofStable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodologyinvolves the use of descriptive prompts to guide the synthesis process, aimedat producing realistic and diverse driving scenarios. With the power of thelatest computer vision techniques, such as ControlNet and Hotshot-XL, we havebuilt a complete pipeline for video generation together with SDXL. We employthe KITTI dataset, which includes real-world driving videos, to train themodel. Through a series of experiments, we demonstrate that our model cangenerate high-quality driving videos that closely replicate the complexity andvariability of real-world driving scenarios. This research contributes to thedevelopment of sophisticated training data for autonomous driving systems andopens new avenues for creating virtual environments for simulation andvalidation purposes.</description><author>Yongjie Fu, Yunlong Li, Xuan Di</author><pubDate>Wed, 28 Aug 2024 15:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15868v1</guid></item><item><title>On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers</title><link>http://arxiv.org/abs/2407.10734v2</link><description>On-device training of DNNs allows models to adapt and fine-tune to newlycollected data or changing domains while deployed on microcontroller units(MCUs). However, DNN training is a resource-intensive task, making theimplementation and execution of DNN training algorithms on MCUs challenging dueto low processor speeds, constrained throughput, limited floating-pointsupport, and memory constraints. In this work, we explore on-device training ofDNNs for Cortex-M MCUs. We present a method that enables efficient training ofDNNs completely in place on the MCU using fully quantized training (FQT) anddynamic partial gradient updates. We demonstrate the feasibility of ourapproach on multiple vision and time-series datasets and provide insights intothe tradeoff between training accuracy, memory overhead, energy, and latency onreal hardware.</description><author>Mark Deutel, Frank Hannig, Christopher Mutschler, Jürgen Teich</author><pubDate>Wed, 28 Aug 2024 15:36:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10734v2</guid></item><item><title>Retrieval-Augmented Instruction Tuning for Automated Process Engineering Calculations : A Tool-Chaining Problem-Solving Framework with Attributable Reflection</title><link>http://arxiv.org/abs/2408.15866v1</link><description>The current technology landscape lacks a foundational AI model for solvingprocess engineering calculations. In this work, we introduce a novel autonomousagent framework leveraging Retrieval-Augmented Instruction-Tuning (RAIT) toenhance open, customizable small code language models (SLMs) for thesecalculations. By combining instruction tuned code SLMs with Retrieval-AugmentedCode Generation (RACG) using external tools, the agent generates, debugs, andoptimizes code from natural language specifications. Our approach addresses thelimitations of the current lack of a foundational AI model for specializedprocess engineering tasks and offers benefits of explainability, knowledgeediting, and cost-effectiveness. Additionally, we curate custom datasets ofchemical and process engineering problems and solutions to overcome datascarcity. Experimental results show that our framework matches the performanceof large-scale proprietary models on benchmark datasets, proving itseffectiveness and usability.</description><author>Sagar Srinivas Sakhinana, Geethan Sannidhi, Venkataramana Runkana</author><pubDate>Wed, 28 Aug 2024 15:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15866v1</guid></item><item><title>microYOLO: Towards Single-Shot Object Detection on Microcontrollers</title><link>http://arxiv.org/abs/2408.15865v1</link><description>This work-in-progress paper presents results on the feasibility ofsingle-shot object detection on microcontrollers using YOLO. Single-shot objectdetectors like YOLO are widely used, however due to their complexity mainly onlarger GPU-based platforms. We present microYOLO, which can be used on Cortex-Mbased microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS whenclassifying 128x128 RGB images while using less than 800 KB Flash and less than350 KB RAM. Furthermore, we share experimental results for three differentobject detection tasks, analyzing the accuracy of microYOLO on them.</description><author>Mark Deutel, Christopher Mutschler, Jürgen Teich</author><pubDate>Wed, 28 Aug 2024 15:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15865v1</guid></item><item><title>Provable Probabilistic Imaging using Score-Based Generative Priors</title><link>http://arxiv.org/abs/2310.10835v3</link><description>Estimating high-quality images while also quantifying their uncertainty aretwo desired features in an image reconstruction algorithm for solving ill-posedinverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) asa principled framework for characterizing the space of possible solutions to ageneral inverse problem. PMC is able to incorporate expressive score-basedgenerative priors for high-quality image reconstruction while also performinguncertainty quantification via posterior sampling. In particular, we developtwo PMC algorithms that can be viewed as the sampling analogues of thetraditional plug-and-play priors (PnP) and regularization by denoising (RED)algorithms. To improve the sampling efficiency, we introduce weighted annealinginto these PMC algorithms, further developing two additional annealed PMCalgorithms (APMC). We establish a theoretical analysis for characterizing theconvergence behavior of PMC algorithms. Our analysis provides non-asymptoticstationarity guarantees in terms of the Fisher information, fully compatiblewith the joint presence of weighted annealing, potentially non-log-concavelikelihoods, and imperfect score networks. We demonstrate the performance ofthe PMC algorithms on multiple representative inverse problems with both linearand nonlinear forward models. Experimental results show that PMC significantlyimproves reconstruction quality and enables high-fidelity uncertaintyquantification.</description><author>Yu Sun, Zihui Wu, Yifan Chen, Berthy T. Feng, Katherine L. Bouman</author><pubDate>Wed, 28 Aug 2024 15:29:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10835v3</guid></item><item><title>From Complexity to Clarity: How AI Enhances Perceptions of Scientists and the Public's Understanding of Science</title><link>http://arxiv.org/abs/2405.00706v3</link><description>This paper evaluated the effectiveness of using generative AI to simplifyscience communication and enhance the public's understanding of science. Bycomparing lay summaries of journal articles from PNAS, yoked to those generatedby AI, this work first assessed linguistic simplicity differences across suchsummaries and public perceptions in follow-up experiments. Specifically, Study1a analyzed simplicity features of PNAS abstracts (scientific summaries) andsignificance statements (lay summaries), observing that lay summaries wereindeed linguistically simpler, but effect size differences were small. Study 1bused a large language model, GPT-4, to create significance statements based onpaper abstracts and this more than doubled the average effect size withoutfine-tuning. Study 2 experimentally demonstrated that simply-written GPTsummaries facilitated more favorable perceptions of scientists (they wereperceived as more credible and trustworthy, but less intelligent) than morecomplexly-written human PNAS summaries. Crucially, Study 3 experimentallydemonstrated that participants comprehended scientific writing better afterreading simple GPT summaries compared to complex PNAS summaries. In their ownwords, participants also summarized scientific papers in a more detailed andconcrete manner after reading GPT summaries compared to PNAS summaries of thesame article. AI has the potential to engage scientific communities and thepublic via a simple language heuristic, advocating for its integration intoscientific dissemination for a more informed society.</description><author>David M. Markowitz</author><pubDate>Wed, 28 Aug 2024 15:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00706v3</guid></item><item><title>AlphaForge: A Framework to Mine and Dynamically Combine Formulaic Alpha Factors</title><link>http://arxiv.org/abs/2406.18394v4</link><description>The complexity of financial data, characterized by its variability and lowsignal-to-noise ratio, necessitates advanced methods in quantitative investmentthat prioritize both performance and interpretability.Transitioning from earlymanual extraction to genetic programming, the most advanced approach in thealpha factor mining domain currently employs reinforcement learning to mine aset of combination factors with fixed weights. However, the performance ofresultant alpha factors exhibits inconsistency, and the inflexibility of fixedfactor weights proves insufficient in adapting to the dynamic nature offinancial markets. To address this issue, this paper proposes a two-stageformulaic alpha generating framework AlphaForge, for alpha factor mining andfactor combination. This framework employs a generative-predictive neuralnetwork to generate factors, leveraging the robust spatial explorationcapabilities inherent in deep learning while concurrently preserving diversity.The combination model within the framework incorporates the temporalperformance of factors for selection and dynamically adjusts the weightsassigned to each component alpha factor. Experiments conducted on real-worlddatasets demonstrate that our proposed model outperforms contemporarybenchmarks in formulaic alpha factor mining. Furthermore, our model exhibits anotable enhancement in portfolio returns within the realm of quantitativeinvestment and real money investment.</description><author>Hao Shi, Weili Song, Xinting Zhang, Jiahe Shi, Cuicui Luo, Xiang Ao, Hamid Arian, Luis Seco</author><pubDate>Wed, 28 Aug 2024 15:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18394v4</guid></item><item><title>Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free Backdoor Mitigation</title><link>http://arxiv.org/abs/2408.15861v1</link><description>Backdoor attacks present a serious security threat to deep neuron networks(DNNs). Although numerous effective defense techniques have been proposed inrecent years, they inevitably rely on the availability of either clean orpoisoned data. In contrast, data-free defense techniques have evolved slowlyand still lag significantly in performance. To address this issue, differentfrom the traditional approach of pruning followed by fine-tuning, we propose anovel data-free defense method named Optimal Transport-based Backdoor Repairing(OTBR) in this work. This method, based on our findings on neuron weightchanges (NWCs) of random unlearning, uses optimal transport (OT)-based modelfusion to combine the advantages of both pruned and backdoored models.Specifically, we first demonstrate our findings that the NWCs of randomunlearning are positively correlated with those of poison unlearning. Based onthis observation, we propose a random-unlearning NWC pruning technique toeliminate the backdoor effect and obtain a backdoor-free pruned model. Then,motivated by the OT-based model fusion, we propose the pruned-to-backdooredOT-based fusion technique, which fuses pruned and backdoored models to combinethe advantages of both, resulting in a model that demonstrates high cleanaccuracy and a low attack success rate. To our knowledge, this is the firstwork to apply OT and model fusion techniques to backdoor defense. Extensiveexperiments show that our method successfully defends against all sevenbackdoor attacks across three benchmark datasets, outperforming bothstate-of-the-art (SOTA) data-free and data-dependent methods. The codeimplementation and Appendix are provided in the Supplementary Material.</description><author>Weilin Lin, Li Liu, Jianze Li, Hui Xiong</author><pubDate>Wed, 28 Aug 2024 15:21:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15861v1</guid></item><item><title>What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector</title><link>http://arxiv.org/abs/2408.15857v1</link><description>This study presents a detailed analysis of the YOLOv8 object detection model,focusing on its architecture, training techniques, and performance improvementsover previous iterations like YOLOv5. Key innovations, including the CSPNetbackbone for enhanced feature extraction, the FPN+PAN neck for superiormulti-scale object detection, and the transition to an anchor-free approach,are thoroughly examined. The paper reviews YOLOv8's performance acrossbenchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracyand real-time capabilities across diverse hardware platforms. Additionally, thestudy explores YOLOv8's developer-friendly enhancements, such as its unifiedPython package and CLI, which streamline model training and deployment.Overall, this research positions YOLOv8 as a state-of-the-art solution in theevolving object detection field.</description><author>Muhammad Yaseen</author><pubDate>Wed, 28 Aug 2024 15:18:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15857v1</guid></item><item><title>Correlation recurrent units: A novel neural architecture for improving the predictive performance of time-series data</title><link>http://arxiv.org/abs/2211.16653v3</link><description>The time-series forecasting (TSF) problem is a traditional problem in thefield of artificial intelligence. Models such as Recurrent Neural Network(RNN), Long Short Term Memory (LSTM), and GRU (Gate Recurrent Units) havecontributed to improving the predictive accuracy of TSF. Furthermore, modelstructures have been proposed to combine time-series decomposition methods,such as seasonal-trend decomposition using Loess (STL) to ensure improvedpredictive accuracy. However, because this approach is learned in anindependent model for each component, it cannot learn the relationships betweentime-series components. In this study, we propose a new neural architecturecalled a correlation recurrent unit (CRU) that can perform time seriesdecomposition within a neural cell and learn correlations (autocorrelation andcorrelation) between each decomposition component. The proposed neuralarchitecture was evaluated through comparative experiments with previousstudies using five univariate time-series datasets and four multivariatetime-series data. The results showed that long- and short-term predictiveperformance was improved by more than 10%. The experimental results show thatthe proposed CRU is an excellent method for TSF problems compared to otherneural architectures.</description><author>Sunghyun Sim, Dohee Kim, Hyerim Bae</author><pubDate>Wed, 28 Aug 2024 15:17:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.16653v3</guid></item><item><title>chemtrain: Learning Deep Potential Models via Automatic Differentiation and Statistical Physics</title><link>http://arxiv.org/abs/2408.15852v1</link><description>Neural Networks (NNs) are promising models for refining the accuracy ofmolecular dynamics, potentially opening up new fields of application. Typicallytrained bottom-up, atomistic NN potential models can reach first-principleaccuracy, while coarse-grained implicit solvent NN potentials surpass classicalcontinuum solvent models. However, overcoming the limitations of costlygeneration of accurate reference data and data inefficiency of common bottom-uptraining demands efficient incorporation of data from many sources. This paperintroduces the framework chemtrain to learn sophisticated NN potential modelsthrough customizable training routines and advanced training algorithms. Theseroutines can combine multiple top-down and bottom-up algorithms, e.g., toincorporate both experimental and simulation data or pre-train potentials withless costly algorithms. chemtrain provides an object-oriented high-levelinterface to simplify the creation of custom routines. On the lower level,chemtrain relies on JAX to compute gradients and scale the computations to useavailable resources. We demonstrate the simplicity and importance of combiningmultiple algorithms in the examples of parametrizing an all-atomistic model oftitanium and a coarse-grained implicit solvent model of alanine dipeptide.</description><author>Paul Fuchs, Stephan Thaler, Sebastien Röcken, Julija Zavadlav</author><pubDate>Wed, 28 Aug 2024 15:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15852v1</guid></item><item><title>Imperceptible Protection against Style Imitation from Diffusion Models</title><link>http://arxiv.org/abs/2403.19254v2</link><description>Recent progress in diffusion models has profoundly enhanced the fidelity ofimage generation, but it has raised concerns about copyright infringements.While prior methods have introduced adversarial perturbations to prevent styleimitation, most are accompanied by the degradation of artworks' visual quality.Recognizing the importance of maintaining this, we introduce a visuallyimproved protection method while preserving its protection capability. To thisend, we devise a perceptual map to highlight areas sensitive to human eyes,guided by instance-aware refinement, which refines the protection intensityaccordingly. We also introduce a difficulty-aware protection by predicting howdifficult the artwork is to protect and dynamically adjusting the intensitybased on this. Lastly, we integrate a perceptual constraints bank to furtherimprove the imperceptibility. Results show that our method substantiallyelevates the quality of the protected image without compromising on protectionefficacy.</description><author>Namhyuk Ahn, Wonhyuk Ahn, KiYoon Yoo, Daesik Kim, Seung-Hun Nam</author><pubDate>Wed, 28 Aug 2024 15:13:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19254v2</guid></item><item><title>RecurrentGemma: Moving Past Transformers for Efficient Open Language Models</title><link>http://arxiv.org/abs/2404.07839v2</link><description>We introduce RecurrentGemma, a family of open language models which usesGoogle's novel Griffin architecture. Griffin combines linear recurrences withlocal attention to achieve excellent performance on language. It has afixed-sized state, which reduces memory use and enables efficient inference onlong sequences. We provide two sizes of models, containing 2B and 9Bparameters, and provide pre-trained and instruction tuned variants for both.Our models achieve comparable performance to similarly-sized Gemma baselinesdespite being trained on fewer tokens.</description><author>Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz Gustavo Martins, Elisa Bandy, David Huntsperger, Glen</author><pubDate>Wed, 28 Aug 2024 15:05:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07839v2</guid></item><item><title>Shot Segmentation Based on Von Neumann Entropy for Key Frame Extraction</title><link>http://arxiv.org/abs/2408.15844v1</link><description>Video key frame extraction is important in various fields, such as videosummary, retrieval, and compression. Therefore, we suggest a video key frameextraction algorithm based on shot segmentation using Von Neumann entropy. Thesegmentation of shots is achieved through the computation of Von Neumannentropy of the similarity matrix among frames within the video sequence. Theinitial frame of each shot is selected as key frames, which combines thetemporal sequence information of frames. The experimental results show theextracted key frames can fully and accurately represent the original videocontent while minimizing the number of repeated frames.</description><author>Xueqing Zhang. Di Fu, Naihao Liu</author><pubDate>Wed, 28 Aug 2024 15:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15844v1</guid></item><item><title>A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules</title><link>http://arxiv.org/abs/2404.01245v2</link><description>Since ChatGPT was introduced in November 2022, embedding (nearly)unnoticeable statistical signals into text generated by large language models(LLMs), also known as watermarking, has been used as a principled approach toprovable detection of LLM-generated text from its human-written counterpart. Inthis paper, we introduce a general and flexible framework for reasoning aboutthe statistical efficiency of watermarks and designing powerful detectionrules. Inspired by the hypothesis testing formulation of watermark detection,our framework starts by selecting a pivotal statistic of the text and a secretkey -- provided by the LLM to the verifier -- to enable controlling the falsepositive rate (the error of mistakenly detecting human-written text asLLM-generated). Next, this framework allows one to evaluate the power ofwatermark detection rules by obtaining a closed-form expression of theasymptotic false negative rate (the error of incorrectly classifyingLLM-generated text as human-written). Our framework further reduces the problemof determining the optimal detection rule to solving a minimax optimizationprogram. We apply this framework to two representative watermarks -- one ofwhich has been internally implemented at OpenAI -- and obtain several findingsthat can be instrumental in guiding the practice of implementing watermarks. Inparticular, we derive optimal detection rules for these watermarks under ourframework. These theoretically derived detection rules are demonstrated to becompetitive and sometimes enjoy a higher power than existing detectionapproaches through numerical experiments.</description><author>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su</author><pubDate>Wed, 28 Aug 2024 15:01:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01245v2</guid></item><item><title>Guaranteed Coverage Prediction Intervals with Gaussian Process Regression</title><link>http://arxiv.org/abs/2310.15641v2</link><description>Gaussian Process Regression (GPR) is a popular regression method, whichunlike most Machine Learning techniques, provides estimates of uncertainty forits predictions. These uncertainty estimates however, are based on theassumption that the model is well-specified, an assumption that is violated inmost practical applications, since the required knowledge is rarely available.As a result, the produced uncertainty estimates can become very misleading; forexample the prediction intervals (PIs) produced for the 95% confidence levelmay cover much less than 95% of the true labels. To address this issue, thispaper introduces an extension of GPR based on a Machine Learning frameworkcalled, Conformal Prediction (CP). This extension guarantees the production ofPIs with the required coverage even when the model is completely misspecified.The proposed approach combines the advantages of GPR with the valid coverageguarantee of CP, while the performed experimental results demonstrate itssuperiority over existing methods.</description><author>Harris Papadopoulos</author><pubDate>Wed, 28 Aug 2024 15:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15641v2</guid></item><item><title>Downstream bias mitigation is all you need</title><link>http://arxiv.org/abs/2408.00612v2</link><description>The advent of transformer-based architectures and large language models(LLMs) have significantly advanced the performance of natural languageprocessing (NLP) models. Since these LLMs are trained on huge corpuses of datafrom the web and other sources, there has been a major concern about harmfulprejudices that may potentially be transferred from the data. In manyapplications, these pre-trained LLMs are fine-tuned on task specific datasets,which can further contribute to biases. This paper studies the extent of biasesabsorbed by LLMs during pre-training as well as task-specific behaviour afterfine-tuning. We found that controlled interventions on pre-trained LLMs, priorto fine-tuning, have minimal effect on lowering biases in classifiers. However,the biases present in domain-specific datasets play a much bigger role, andhence mitigating them at this stage has a bigger impact. While pre-trainingdoes matter, but after the model has been pre-trained, even slight changes toco-occurrence rates in the fine-tuning dataset has a significant effect on thebias of the model.</description><author>Arkadeep Baksi, Rahul Singh, Tarun Joshi</author><pubDate>Wed, 28 Aug 2024 14:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00612v2</guid></item><item><title>Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models</title><link>http://arxiv.org/abs/2402.16696v3</link><description>Tool-augmented large language models (LLMs) are attracting widespreadattention when accessing up-to-date knowledge and alleviating hallucinationissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstratedsurprising tool-usage capabilities through prompting and in-context learningtechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) inmanipulating tools, current efforts focus on either template-driven ortoken-triggered tool-usage. However, the former hampers LLMs' flexibility toaddress diverse user's queries due to constrained tool interactions, while thelatter limits the generalizability when engaging with new tools, sincetool-usage learning is based on task- and tool-specific datasets. To alleviatethese concerns, in this paper, we propose a decision-aware and generalizabletool-usage framework (DEER). Specifically, we first construct the tool-usagesamples with multiple decision branches via an automatic generation pipeline,thereby inspiring the decision-making awareness of LLMs under diversescenarios. Meanwhile, we propose a novel tool sampling strategy to enhance thegeneralizability of LLMs over unseen tools. Extensive experiments demonstratethat our proposed DEER is effective and significantly outperforms baselinesacross various datasets.</description><author>Anchun Gui, Jian Li, Yong Dai, Nan Du, Han Xiao</author><pubDate>Wed, 28 Aug 2024 14:54:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16696v3</guid></item><item><title>Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature</title><link>http://arxiv.org/abs/2408.15836v1</link><description>The exponential growth of scientific literature necessitates advanced toolsfor effective knowledge exploration. We present Knowledge Navigator, a systemdesigned to enhance exploratory search abilities by organizing and structuringthe retrieved documents from broad topical queries into a navigable, two-levelhierarchy of named and descriptive scientific topics and subtopics. Thisstructured organization provides an overall view of the research themes in adomain, while also enabling iterative search and deeper knowledge discoverywithin specific subtopics by allowing users to refine their focus and retrieveadditional relevant documents. Knowledge Navigator combines LLM capabilitieswith cluster-based methods to enable an effective browsing method. Wedemonstrate our approach's effectiveness through automatic and manualevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,prompts, and benchmarks are made publicly available.</description><author>Uri Katz, Mosh Levy, Yoav Goldberg</author><pubDate>Wed, 28 Aug 2024 14:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15836v1</guid></item></channel></rss>