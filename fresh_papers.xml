<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 18 Feb 2024 18:05:34 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CodeMind: A Framework to Challenge Large Language Models for Code Reasoning</title><link>http://arxiv.org/abs/2402.09664v2</link><description>Solely relying on test passing to evaluate Large Language Models (LLMs) forcode synthesis may result in unfair assessment or promoting models with dataleakage. As an alternative, we introduce CodeMind, a framework designed togauge the code reasoning abilities of LLMs. CodeMind currently supports threecode reasoning tasks: Independent Execution Reasoning (IER), DependentExecution Reasoning (DER), and Specification Reasoning (SR). The first twoevaluate models to predict the execution output of an arbitrary code or codethe model could correctly synthesize. The third one evaluates the extent towhich LLMs implement the specified expected behavior. Our extensive evaluationof nine LLMs across five benchmarks in two different programming languagesusing CodeMind shows that LLMs fairly understand control flow constructs and,in general, are capable of reasoning how inputs evolve to output, specificallyfor simple programs and the ones they can correctly synthesize. However, theirperformance drops for code with higher complexity, non-trivial logical andarithmetic operators, non-primitive types, and API calls. Furthermore, weobserve that, while correlated, specification reasoning (essential for codesynthesis) does not imply execution reasoning (essential for broaderprogramming tasks such as testing and debugging): ranking LLMs based on testpassing can be different compared to code reasoning.</description><author>Changshu Liu, Shizhuo Dylan Zhang, Reyhaneh Jabbarvand</author><pubDate>Fri, 16 Feb 2024 18:35:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09664v2</guid></item><item><title>Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis with Generative AI through Rapid Prototyping, Iteration and Curation</title><link>http://arxiv.org/abs/2402.08812v2</link><description>Complex data analysis inherently seeks unexpected insights throughexploratory \re{visual analysis} methods, transcending logical, step-by-stepprocessing. However, \re{existing interfaces such as notebooks and dashboardshave limitations in exploration and comparison for visual data analysis}.Addressing these limitations, we introduce a "design-like" intelligent canvasenvironment integrating generative AI into data analysis, offering rapidprototyping, iteration, and comparative visualization management. Our dualcontributions include the integration of generative AI components into a canvasinterface, and empirical findings from a user study (N=10) evaluating theeffectiveness of the canvas interface.</description><author>Zijian Ding, Joel Chan</author><pubDate>Fri, 16 Feb 2024 18:04:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08812v2</guid></item><item><title>Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop</title><link>http://arxiv.org/abs/2402.09346v2</link><description>As LLMs become more pervasive across various users and scenarios, identifyingpotential issues when using these models becomes essential. Examples includebias, inconsistencies, and hallucination. Although auditing the LLM for theseproblems is desirable, it is far from being easy or solved. An effective methodis to probe the LLM using different versions of the same question. This couldexpose inconsistencies in its knowledge or operation, indicating potential forbias or hallucination. However, to operationalize this auditing method atscale, we need an approach to create those probes reliably and automatically.In this paper we propose an automatic and scalable solution, where one uses adifferent LLM along with human-in-the-loop. This approach offers verifiabilityand transparency, while avoiding circular reliance on the same LLMs, andincreasing scientific rigor and generalizability. Specifically, we present anovel methodology with two phases of verification using humans: standardizedevaluation criteria to verify responses, and a structured prompt template togenerate desired probes. Experiments on a set of questions from TruthfulQAdataset show that we can generate a reliable set of probes from one LLM thatcan be used to audit inconsistencies in a different LLM. The criteria forgenerating and applying auditing probes is generalizable to various LLMsregardless of the underlying structure or training mechanism.</description><author>Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, Elizabeth Snell Okada, Aman Chadha, Tanya Roosta, Chirag Shah</author><pubDate>Fri, 16 Feb 2024 16:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09346v2</guid></item><item><title>MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models</title><link>http://arxiv.org/abs/2402.09262v2</link><description>We introduce MultiMedEval, an open-source toolkit for fair and reproducibleevaluation of large, medical vision-language models (VLM). MultiMedEvalcomprehensively assesses the models' performance on a broad array of sixmulti-modal tasks, conducted over 23 datasets, and spanning over 11 medicaldomains. The chosen tasks and performance metrics are based on their widespreadadoption in the community and their diversity, ensuring a thorough evaluationof the model's overall generalizability. We open-source a Python toolkit(github.com/corentin-ryr/MultiMedEval) with a simple interface and setupprocess, enabling the evaluation of any VLM in just a few lines of code. Ourgoal is to simplify the intricate landscape of VLM evaluation, thus promotingfair and uniform benchmarking of future models.</description><author>Corentin Royer, Bjoern Menze, Anjany Sekuboyina</author><pubDate>Fri, 16 Feb 2024 16:36:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09262v2</guid></item><item><title>Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for Single Image Test-Time Adaptation</title><link>http://arxiv.org/abs/2402.09604v2</link><description>Test-time adaptation (TTA) refers to adapting a trained model to a new domainduring testing. Existing TTA techniques rely on having multiple test imagesfrom the same domain, yet this may be impractical in real-world applicationssuch as medical imaging, where data acquisition is expensive and imagingconditions vary frequently. Here, we approach such a task, of adapting amedical image segmentation model with only a single unlabeled test image. MostTTA approaches, which directly minimize the entropy of predictions, fail toimprove performance significantly in this setting, in which we also observe thechoice of batch normalization (BN) layer statistics to be a highly importantyet unstable factor due to only having a single test domain example. Toovercome this, we propose to instead integrate over predictions made withvarious estimates of target domain statistics between the training and teststatistics, weighted based on their entropy statistics. Our method, validatedon 24 source/target domain splits across 3 medical image datasets surpasses theleading method by 2.9% Dice coefficient on average.</description><author>Haoyu Dong, Nicholas Konz, Hanxue Gu, Maciej A. Mazurowski</author><pubDate>Fri, 16 Feb 2024 15:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09604v2</guid></item><item><title>PMGDA: A Preference-based Multiple Gradient Descent Algorithm</title><link>http://arxiv.org/abs/2402.09492v2</link><description>It is desirable in many multi-objective machine learning applications, suchas multi-task learning with conflicting objectives and multi-objectivereinforcement learning, to find a Pareto solution that can match a givenpreference of a decision maker. These problems are often large-scale withavailable gradient information but cannot be handled very well by the existingalgorithms. To tackle this critical issue, this paper proposes a novelpredict-and-correct framework for locating a Pareto solution that fits thepreference of a decision maker. In the proposed framework, a constraintfunction is introduced in the search progress to align the solution with auser-specific preference, which can be optimized simultaneously with multipleobjective functions. Experimental results show that our proposed method canefficiently find a particular Pareto solution under the demand of a decisionmaker for standard multiobjective benchmark, multi-task learning, andmulti-objective reinforcement learning problems with more than thousands ofdecision variables. Code is available at: https://github.com/xzhang2523/pmgda. Our code iscurrent provided in the pgmda.rar attached file and will be open-sourced afterpublication.}</description><author>Xiaoyuan Zhang, Xi Lin, Qingfu Zhang</author><pubDate>Fri, 16 Feb 2024 14:01:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09492v2</guid></item><item><title>API Pack: A Massive Multilingual Dataset for API Call Generation</title><link>http://arxiv.org/abs/2402.09615v2</link><description>We introduce API Pack, a multilingual dataset featuring over one millioninstruction-API call pairs aimed at advancing large language models' API callgeneration capabilities. Through experiments, we demonstrate API Pack'sefficacy in enhancing models for this specialized task while maintaining theiroverall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4respectively in generating unseen API calls. Scaling to 100k examples improvesgeneralization to new APIs not seen during training. In addition, cross-lingualAPI call generation is achieved without needing extensive data per language.The dataset, fine-tuned models, and overall code base are publicly available athttps://github.com/zguo0525/API-Pack.</description><author>Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda</author><pubDate>Fri, 16 Feb 2024 13:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09615v2</guid></item><item><title>MiMiC: Minimally Modified Counterfactuals in the Representation Space</title><link>http://arxiv.org/abs/2402.09631v2</link><description>Language models often exhibit undesirable behaviors, such as gender bias ortoxic language. Interventions in the representation space were shown effectivein mitigating such issues by altering the LM behavior. We first show that twoprominent intervention techniques, Linear Erasure and Steering Vectors, do notenable a high degree of control and are limited in expressivity. We then propose a novel intervention methodology for generating expressivecounterfactuals in the representation space, aiming to make representations ofa source class (e.g., "toxic") resemble those of a target class (e.g.,"non-toxic"). This approach, generalizing previous linear interventiontechniques, utilizes a closed-form solution for the Earth Mover's problem underGaussian assumptions and provides theoretical guarantees on the representationspace's geometric organization. We further build on this technique and derive anonlinear intervention that enables controlled generation. We demonstrate theeffectiveness of the proposed approaches in mitigating bias in multiclassclassification and in reducing the generation of toxic language, outperformingstrong baselines.</description><author>Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, Ponnurangam Kumaraguru</author><pubDate>Fri, 16 Feb 2024 12:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09631v2</guid></item><item><title>TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles</title><link>http://arxiv.org/abs/2402.10137v2</link><description>In light of recent advances in large language models (LLMs), the expectationsfor the next generation of virtual assistants include enhanced naturalness andadaptability across diverse usage scenarios. However, the creation ofhigh-quality annotated data for Task-Oriented Dialog (TOD) is recognized to beslow and costly. To address these challenges, we introduce Task-OrientedAutomatic Dialogs (TOAD), a novel and scalable TOD dataset along with itsautomatic generation pipeline. The TOAD dataset simulates realistic app contextinteraction and provide a variety of system response style options. Two aspectsof system response styles are considered, verbosity level and users' expressionmirroring. We benchmark TOAD on two response generation tasks and the resultsshow that modelling more verbose or responses without user expression mirroringis more challenging.</description><author>Yinhong Liu, Yimai Fang, David Vandyke, Nigel Collier</author><pubDate>Fri, 16 Feb 2024 10:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10137v2</guid></item><item><title>Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues</title><link>http://arxiv.org/abs/2402.09091v2</link><description>With the development of LLMs, the security threats of LLMs are getting moreand more attention. Numerous jailbreak attacks have been proposed to assess thesecurity defense of LLMs. Current jailbreak attacks primarily utilize scenariocamouflage techniques. However their explicitly mention of malicious intentwill be easily recognized and defended by LLMs. In this paper, we propose anindirect jailbreak attack approach, Puzzler, which can bypass the LLM's defensestrategy and obtain malicious response by implicitly providing LLMs with someclues about the original malicious query. In addition, inspired by the wisdomof "When unable to attack, defend" from Sun Tzu's Art of War, we adopt adefensive stance to gather clues about the original malicious query throughLLMs. Extensive experimental results show that Puzzler achieves a query successrate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher thanbaselines. Furthermore, when tested against the state-of-the-art jailbreakdetection approaches, Puzzler proves to be more effective at evading detectioncompared to baselines.</description><author>Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, Yang Liu</author><pubDate>Fri, 16 Feb 2024 10:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09091v2</guid></item><item><title>Plausible Extractive Rationalization through Semi-Supervised Entailment Signal</title><link>http://arxiv.org/abs/2402.08479v3</link><description>The increasing use of complex and opaque black box models requires theadoption of interpretable measures, one such option is extractive rationalizingmodels, which serve as a more interpretable alternative. These models, alsoknown as Explain-Then-Predict models, employ an explainer model to extractrationales and subsequently condition the predictor with the extractedinformation. Their primary objective is to provide precise and faithfulexplanations, represented by the extracted rationales. In this paper, we take asemi-supervised approach to optimize for the plausibility of extractedrationales. We adopt a pre-trained natural language inference (NLI) model andfurther fine-tune it on a small set of supervised rationales ($10\%$). The NLIpredictor is leveraged as a source of supervisory signals to the explainer viaentailment alignment. We show that, by enforcing the alignment agreementbetween the explanation and answer in a question-answering task, theperformance can be improved without access to ground truth labels. We evaluateour approach on the ERASER dataset and show that our approach achievescomparable results with supervised extractive models and outperformsunsupervised approaches by $&gt; 100\%$.</description><author>Yeo Wei Jie, Ranjan Satapathy, Erik Cambria</author><pubDate>Fri, 16 Feb 2024 09:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08479v3</guid></item><item><title>Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion</title><link>http://arxiv.org/abs/2402.10009v2</link><description>Editing signals using large pre-trained models, in a zero-shot manner, hasrecently seen rapid advancements in the image domain. However, this wave hasyet to reach the audio domain. In this paper, we explore two zero-shot editingtechniques for audio signals, which use DDPM inversion on pre-trained diffusionmodels. The first, adopted from the image domain, allows text-based editing.The second, is a novel approach for discovering semantically meaningful editingdirections without supervision. When applied to music signals, this methodexposes a range of musically interesting modifications, from controlling theparticipation of specific instruments to improvisations on the melody. Samplesand code can be found on our examples page inhttps://hilamanor.github.io/AudioEditing/ .</description><author>Hila Manor, Tomer Michaeli</author><pubDate>Fri, 16 Feb 2024 09:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10009v2</guid></item><item><title>Reusing Softmax Hardware Unit for GELU Computation in Transformers</title><link>http://arxiv.org/abs/2402.10118v2</link><description>Transformers have improved drastically the performance of natural languageprocessing (NLP) and computer vision applications. The computation oftransformers involves matrix multiplications and non-linear activationfunctions such as softmax and GELU (Gaussion Error Linear Unit) that areaccelerated directly in hardware. Currently, function evaluation is doneseparately for each function and rarely allows for hardware reuse. To mitigatethis problem, in this work, we map the computation of GELU to a softmaxoperator. In this way, the efficient hardware units designed already forsoftmax can be reused for computing GELU as well. Computation of GELU can enjoythe inherent vectorized nature of softmax and produce in parallel multiple GELUoutcomes. Experimental results show that computing GELU via a pre-existing andincrementally modified softmax hardware unit (a) does not reduce the accuracyof representative NLP applications and (b) allows the reduction of the overallhardware area and power by 6.1% and 11.9%, respectively, on average.</description><author>Christodoulos Peltekis, Kosmas Alexandridis, Giorgos Dimitrakopoulos</author><pubDate>Fri, 16 Feb 2024 08:52:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10118v2</guid></item><item><title>Tokenization Preference for Human and Machine Learning Model: An Annotation Study</title><link>http://arxiv.org/abs/2304.10813v3</link><description>Is preferred tokenization for humans also preferred for machine-learning (ML)models? This study examines the relations between preferred tokenization forhumans (appropriateness and readability) and one for ML models (performance onan NLP task). The question texts of the Japanese commonsense question-answeringdataset are tokenized with six different tokenizers, and the performances ofhuman annotators and ML models were compared. Furthermore, we analyze relationsamong performance of answers by human and ML model, the appropriateness oftokenization for human, and response time to questions by human. This studyprovides a quantitative investigation result that shows that preferredtokenizations for humans and ML models are not necessarily always the same. Theresult also implies that existing methods using language models fortokenization could be a good compromise both for human and ML models.</description><author>Tatsuya Hiraoka, Tomoya Iwakura</author><pubDate>Fri, 16 Feb 2024 07:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10813v3</guid></item><item><title>Mitigating Reward Hacking via Information-Theoretic Reward Modeling</title><link>http://arxiv.org/abs/2402.09345v3</link><description>Despite the success of reinforcement learning from human feedback (RLHF) inaligning language models with human values, reward hacking, also termed rewardoveroptimization, remains a critical challenge, which primarily stems fromlimitations in reward modeling, i.e., generalizability of the reward model andinconsistency in the preference dataset. In this work, we tackle this problemfrom an information theoretic-perspective, and propose a generalizable androbust framework for reward modeling, namely InfoRM, by introducing avariational information bottleneck objective to filter out irrelevantinformation and developing a mechanism for model complexity modulation.Notably, we further identify a correlation between overoptimization andoutliers in the latent space, establishing InfoRM as a promising tool fordetecting reward overoptimization. Inspired by this finding, we propose theIntegrated Cluster Deviation Score (ICDS), which quantifies deviations in thelatent space, as an indicator of reward overoptimization to facilitate thedevelopment of online mitigation strategies. Extensive experiments on a widerange of settings and model scales (70M, 440M, 1.4B, and 7B) support theeffectiveness of InfoRM. Further analyses reveal that InfoRM's overoptimizationdetection mechanism is effective, potentially signifying a notable advancementin the field of RLHF. Code will be released upon acceptance.</description><author>Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao</author><pubDate>Fri, 16 Feb 2024 07:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09345v3</guid></item><item><title>Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model</title><link>http://arxiv.org/abs/2402.09786v2</link><description>Generative adversarial networks generate photorealistic faces that are oftenindistinguishable by humans from real faces. We find that the discriminator inthe pre-trained StyleGAN3 model, a popular GAN network, systematicallystratifies scores by both image- and face-level qualities and that thisdisproportionately affects images across gender, race, and other categories. Weexamine the discriminator's bias for color and luminance across axes perceivedrace and gender; we then examine axes common in research on stereotyping insocial psychology.</description><author>Alvin Grissom II, Ryan F. Lei, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter</author><pubDate>Fri, 16 Feb 2024 07:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09786v2</guid></item><item><title>A Stochastic-Geometrical Framework for Object Pose Estimation based on Mixture Models Avoiding the Correspondence Problem</title><link>http://arxiv.org/abs/2311.18107v3</link><description>Background: Pose estimation of rigid objects is a practical challenge inoptical metrology and computer vision. This paper presents a novelstochastic-geometrical modeling framework for object pose estimation based onobserving multiple feature points. Methods: This framework utilizes mixture models for feature point densitiesin object space and for interpreting real measurements. Advantages are theavoidance to resolve individual feature correspondences and to incorporatecorrect stochastic dependencies in multi-view applications. First, the generalmodeling framework is presented, second, a general algorithm for poseestimation is derived, and third, two example models (camera and laterationsetup) are presented. Results: Numerical experiments show the effectiveness of this modeling andgeneral algorithm by presenting four simulation scenarios for three observationsystems, including the dependence on measurement resolution, objectdeformations and measurement noise. Probabilistic modeling utilizing mixturemodels shows the potential for accurate and robust pose estimations whileavoiding the correspondence problem.</description><author>Wolfgang Hoegele</author><pubDate>Fri, 16 Feb 2024 06:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18107v3</guid></item><item><title>LLM Agents can Autonomously Hack Websites</title><link>http://arxiv.org/abs/2402.06664v3</link><description>In recent years, large language models (LLMs) have become increasinglycapable and can now interact with tools (i.e., call functions), read documents,and recursively call themselves. As a result, these LLMs can now functionautonomously as agents. With the rise in capabilities of these agents, recentwork has speculated on how LLM agents would affect cybersecurity. However, notmuch is known about the offensive capabilities of LLM agents. In this work, we show that LLM agents can autonomously hack websites,performing tasks as complex as blind database schema extraction and SQLinjections without human feedback. Importantly, the agent does not need to knowthe vulnerability beforehand. This capability is uniquely enabled by frontiermodels that are highly capable of tool use and leveraging extended context.Namely, we show that GPT-4 is capable of such hacks, but existing open-sourcemodels are not. Finally, we show that GPT-4 is capable of autonomously findingvulnerabilities in websites in the wild. Our findings raise questions about thewidespread deployment of LLMs.</description><author>Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang</author><pubDate>Fri, 16 Feb 2024 04:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06664v3</guid></item><item><title>Towards Versatile and Efficient Visual Knowledge Integration into Pre-trained Language Models with Cross-Modal Adapters</title><link>http://arxiv.org/abs/2305.07358v4</link><description>Humans learn language via multi-modal knowledge. However, due to thetext-only pre-training scheme, most existing pre-trained language models (PLMs)are hindered from the multi-modal information. To inject visual knowledge into PLMs, existing methods incorporate either thetext or image encoder of vision-language models (VLMs) to encode the visualinformation and update all the original parameters of PLMs for knowledgefusion. In this paper, we propose a new plug-and-play module, X-adapter, to flexiblyleverage the aligned visual and textual knowledge learned in pre-trained VLMsand efficiently inject them into PLMs. Specifically, we insert X-adapters into PLMs, and only the added parametersare updated during adaptation. To fully exploit the potential in VLMs, X-adapters consist of twosub-modules, V-expert and T-expert, to fuse VLMs' image and textrepresentations, respectively. We can opt for activating different sub-modules depending on the downstreamtasks. Experimental results show that our method can significantly improve theperformance on object-color reasoning and natural language understanding (NLU)tasks compared with PLM baselines.</description><author>Xinyun Zhang, Haochen Tan, Han Wu, Bei Yu</author><pubDate>Fri, 16 Feb 2024 02:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07358v4</guid></item><item><title>An Analysis of Language Frequency and Error Correction for Esperanto</title><link>http://arxiv.org/abs/2402.09696v2</link><description>Current Grammar Error Correction (GEC) initiatives tend to focus on majorlanguages, with less attention given to low-resource languages like Esperanto.In this article, we begin to bridge this gap by first conducting acomprehensive frequency analysis using the Eo-GP dataset, created explicitlyfor this purpose. We then introduce the Eo-GEC dataset, derived from authenticuser cases and annotated with fine-grained linguistic details for erroridentification. Leveraging GPT-3.5 and GPT-4, our experiments show that GPT-4outperforms GPT-3.5 in both automated and human evaluations, highlighting itsefficacy in addressing Esperanto's grammatical peculiarities and illustratingthe potential of advanced language models to enhance GEC strategies for lesscommonly studied languages.</description><author>Junhong Liang</author><pubDate>Fri, 16 Feb 2024 02:19:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09696v2</guid></item><item><title>U-shaped Vision Mamba for Single Image Dehazing</title><link>http://arxiv.org/abs/2402.04139v4</link><description>Currently, Transformer is the most popular architecture for image dehazing,but due to its large computational complexity, its ability to handle long-rangedependency is limited on resource-constrained devices. To tackle thischallenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficientsingle-image dehazing network. Inspired by the State Space Sequence Models(SSMs), a new deep sequence model known for its power to handle long sequences,we design a Bi-SSM block that integrates the local feature extraction abilityof the convolutional layer with the ability of the SSM to capture long-rangedependencies. Extensive experimental results demonstrate the effectiveness ofour method. Our method provides a more highly efficient idea of long-rangedependency modeling for image dehazing as well as other image restorationtasks. The URL of the code is \url{https://github.com/zzr-idam/UVM-Net}. Ourmethod takes only \textbf{0.009} seconds to infer a $325 \times 325$ resolutionimage (100FPS) without I/O handling time.</description><author>Zhuoran Zheng, Chen Wu</author><pubDate>Fri, 16 Feb 2024 02:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04139v4</guid></item><item><title>Rethinking Machine Unlearning for Large Language Models</title><link>http://arxiv.org/abs/2402.08787v2</link><description>We explore machine unlearning (MU) in the domain of large language models(LLMs), referred to as LLM unlearning. This initiative aims to eliminateundesirable data influence (e.g., sensitive or illegal information) and theassociated model capabilities, while maintaining the integrity of essentialknowledge generation and not affecting causally unrelated information. Weenvision LLM unlearning becoming a pivotal element in the life-cycle managementof LLMs, potentially standing as an essential foundation for developinggenerative AI that is not only safe, secure, and trustworthy, but alsoresource-efficient without the need of full retraining. We navigate theunlearning landscape in LLMs from conceptual formulation, methodologies,metrics, and applications. In particular, we highlight the often-overlookedaspects of existing LLM unlearning research, e.g., unlearning scope, data-modelinteraction, and multifaceted efficacy assessment. We also draw connectionsbetween LLM unlearning and related areas such as model editing, influencefunctions, model explanation, adversarial training, and reinforcement learning.Furthermore, we outline an effective assessment framework for LLM unlearningand explore its applications in copyright and privacy safeguards andsociotechnical harm reduction.</description><author>Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu</author><pubDate>Thu, 15 Feb 2024 21:14:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08787v2</guid></item><item><title>Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling</title><link>http://arxiv.org/abs/2402.10211v1</link><description>Reasoning from sequences of raw sensory data is a ubiquitous problem acrossfields ranging from medical devices to robotics. These problems often involveusing long sequences of raw sensor data (e.g. magnetometers, piezoresistors) topredict sequences of desirable physical quantities (e.g. force, inertialmeasurements). While classical approaches are powerful for locally-linearprediction problems, they often fall short when using real-world sensors. Thesesensors are typically non-linear, are affected by extraneous variables (e.g.vibration), and exhibit data-dependent drift. For many problems, the predictiontask is exacerbated by small labeled datasets since obtaining ground-truthlabels requires expensive equipment. In this work, we present HierarchicalState-Space Models (HiSS), a conceptually simple, new technique for continuoussequential prediction. HiSS stacks structured state-space models on top of eachother to create a temporal hierarchy. Across six real-world sensor datasets,from tactile-based state prediction to accelerometer-based inertialmeasurement, HiSS outperforms state-of-the-art sequence models such as causalTransformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experimentsfurther indicate that HiSS demonstrates efficient scaling to smaller datasetsand is compatible with existing data-filtering techniques. Code, datasets andvideos can be found on https://hiss-csp.github.io.</description><author>Raunaq Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, Tess Hellebrekers, Lerrel Pinto</author><pubDate>Thu, 15 Feb 2024 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10211v1</guid></item><item><title>Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation</title><link>http://arxiv.org/abs/2402.10210v1</link><description>Fine-tuning Diffusion Models remains an underexplored frontier in generativeartificial intelligence (GenAI), especially when compared with the remarkableprogress made in fine-tuning Large Language Models (LLMs). While cutting-edgediffusion models such as Stable Diffusion (SD) and SDXL rely on supervisedfine-tuning, their performance inevitably plateaus after seeing a certainvolume of data. Recently, reinforcement learning (RL) has been employed tofine-tune diffusion models with human preference data, but it requires at leasttwo images ("winner" and "loser" images) for each text prompt. In this paper,we introduce an innovative technique called self-play fine-tuning for diffusionmodels (SPIN-Diffusion), where the diffusion model engages in competition withits earlier versions, facilitating an iterative self-improvement process. Ourapproach offers an alternative to conventional supervised fine-tuning and RLstrategies, significantly improving both model performance and alignment. Ourexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperformsthe existing supervised fine-tuning method in aspects of human preferencealignment and visual appeal right from its first iteration. By the seconditeration, it exceeds the performance of RLHF-based methods across all metrics,achieving these results with less data.</description><author>Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu</author><pubDate>Thu, 15 Feb 2024 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10210v1</guid></item><item><title>Recovering the Pre-Fine-Tuning Weights of Generative Models</title><link>http://arxiv.org/abs/2402.10208v1</link><description>The dominant paradigm in generative modeling consists of two steps: i)pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trainedmodel with human values via fine-tuning. This practice is considered safe, asno current method can recover the unsafe, pre-fine-tuning model weights. Inthis paper, we demonstrate that this assumption is often false. Concretely, wepresent Spectral DeTuning, a method that can recover the weights of thepre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. Incontrast to previous attacks that attempt to recover pre-fine-tuningcapabilities, our method aims to recover the exact pre-fine-tuning weights. Ourapproach exploits this new vulnerability against large-scale models such as apersonalized Stable Diffusion and an aligned Mistral.</description><author>Eliahu Horwitz, Jonathan Kahana, Yedid Hoshen</author><pubDate>Thu, 15 Feb 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10208v1</guid></item><item><title>Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment</title><link>http://arxiv.org/abs/2402.10207v1</link><description>We consider the problem of multi-objective alignment of foundation modelswith human preferences, which is a critical step towards helpful and harmlessAI systems. However, it is generally costly and unstable to fine-tune largefoundation models using reinforcement learning (RL), and themulti-dimensionality, heterogeneity, and conflicting nature of humanpreferences further complicate the alignment process. In this paper, weintroduce Rewards-in-Context (RiC), which conditions the response of afoundation model on multiple rewards in its prompt context and appliessupervised fine-tuning for alignment. The salient features of RiC aresimplicity and adaptivity, as it only requires supervised fine-tuning of asingle foundation model and supports dynamic adjustment for user preferencesduring inference time. Inspired by the analytical solution of an abstractedconvex optimization problem, our dynamic inference-time adjustment methodapproaches the Pareto-optimal solution for multiple objectives. Empiricalevidence demonstrates the efficacy of our method in aligning both LargeLanguage Models (LLMs) and diffusion models to accommodate diverse rewards withonly around $10\%$ GPU hours compared with multi-objective RL baseline.</description><author>Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen</author><pubDate>Thu, 15 Feb 2024 18:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10207v1</guid></item><item><title>Ising on the Graph: Task-specific Graph Subsampling via the Ising Model</title><link>http://arxiv.org/abs/2402.10206v1</link><description>Reducing a graph while preserving its overall structure is an importantproblem with many applications. Typically, the reduction approaches eitherremove edges (sparsification) or merge nodes (coarsening) in an unsupervisedway with no specific downstream task in mind. In this paper, we present anapproach for subsampling graph structures using an Ising model defined oneither the nodes or edges and learning the external magnetic field of the Isingmodel using a graph neural network. Our approach is task-specific as it canlearn how to reduce a graph for a specific downstream task in an end-to-endfashion. The utilized loss function of the task does not even have to bedifferentiable. We showcase the versatility of our approach on three distinctapplications: image segmentation, 3D shape sparsification, and sparseapproximate matrix inverse determination.</description><author>Maria Bånkestad, Jennifer Andersson, Sebastian Mair, Jens Sjölund</author><pubDate>Thu, 15 Feb 2024 18:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10206v1</guid></item><item><title>BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data</title><link>http://arxiv.org/abs/2402.08093v2</link><description>We introduce a text-to-speech (TTS) model called BASE TTS, which stands for$\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with$\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date,trained on 100K hours of public domain speech data, achieving a newstate-of-the-art in speech naturalness. It deploys a 1-billion-parameterautoregressive Transformer that converts raw texts into discrete codes("speechcodes") followed by a convolution-based decoder which converts thesespeechcodes into waveforms in an incremental, streamable manner. Further, ourspeechcodes are built using a novel speech tokenization technique that featuresspeaker ID disentanglement and compression with byte-pair encoding. Echoing thewidely-reported "emergent abilities" of large language models when trained onincreasing volume of data, we show that BASE TTS variants built with 10K+ hoursand 500M+ parameters begin to demonstrate natural prosody on textually complexsentences. We design and share a specialized dataset to measure these emergentabilities for text-to-speech. We showcase state-of-the-art naturalness of BASETTS by evaluating against baselines that include publicly available large-scaletext-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generatedby the model can be heard at https://amazon-ltts-paper.com/.</description><author>Mateusz Łajszczak, Guillermo Cámbara, Yang Li, Fatih Beyhan, Arent van Korlaar, Fan Yang, Arnaud Joly, Álvaro Martín-Cortinas, Ammar Abbas, Adam Michalski, Alexis Moinet, Sri Karlapati, Ewa Muszyńska, Haohan Guo, Bartosz Putrycz, Soledad López Gambino, Kayeon Yoo, Elena Sokolova, Thomas Drugman</author><pubDate>Thu, 15 Feb 2024 18:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08093v2</guid></item><item><title>Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model</title><link>http://arxiv.org/abs/2402.10204v1</link><description>Reconstructing sky models from dirty radio images for accurate sourcelocalization and flux estimation is crucial for studying galaxy evolution athigh redshift, especially in deep fields using instruments like the AtacamaLarge Millimetre Array (ALMA). With new projects like the Square KilometreArray (SKA), there's a growing need for better source extraction methods.Current techniques, such as CLEAN and PyBDSF, often fail to detect faintsources, highlighting the need for more accurate methods. This study proposesusing stochastic neural networks to rebuild sky models directly from dirtyimages. This method can pinpoint radio sources and measure their fluxes withrelated uncertainties, marking a potential improvement in radio sourcecharacterization. We tested this approach on 10164 images simulated with theCASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We appliedconditional Denoising Diffusion Probabilistic Models (DDPMs) for sky modelsreconstruction, then used Photutils to determine source coordinates and fluxes,assessing the model's performance across different water vapor levels. Ourmethod showed excellence in source localization, achieving more than 90%completeness at a signal-to-noise ratio (SNR) as low as 2. It also surpassedPyBDSF in flux estimation, accurately identifying fluxes for 96% of sources inthe test set, a significant improvement over CLEAN+ PyBDSF's 57%. ConditionalDDPMs is a powerful tool for image-to-image translation, yielding accurate androbust characterisation of radio sources, and outperforming existingmethodologies. While this study underscores its significant potential forapplications in radio astronomy, we also acknowledge certain limitations thataccompany its usage, suggesting directions for further refinement and research.</description><author>Mariia Drozdova, Vitaliy Kinakh, Omkar Bait, Olga Taran, Erica Lastufka, Miroslava Dessauges-Zavadsky, Taras Holotyak, Daniel Schaerer, Slava Voloshynovskiy</author><pubDate>Thu, 15 Feb 2024 18:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10204v1</guid></item><item><title>Bridging Associative Memory and Probabilistic Modeling</title><link>http://arxiv.org/abs/2402.10202v1</link><description>Associative memory and probabilistic modeling are two fundamental topics inartificial intelligence. The first studies recurrent neural networks designedto denoise, complete and retrieve data, whereas the second studies learning andsampling from probability distributions. Based on the observation thatassociative memory's energy functions can be seen as probabilistic modeling'snegative log likelihoods, we build a bridge between the two that enables usefulflow of ideas in both directions. We showcase four examples: First, we proposenew energy-based models that flexibly adapt their energy functions to newin-context datasets, an approach we term \textit{in-context learning of energyfunctions}. Second, we propose two new associative memory models: one thatdynamically creates new memories as necessitated by the training data usingBayesian nonparametrics, and another that explicitly computes proportionalmemory assignments using the evidence lower bound. Third, using tools fromassociative memory, we analytically and numerically characterize the memorycapacity of Gaussian kernel density estimators, a widespread tool inprobababilistic modeling. Fourth, we study a widespread implementation choicein transformers -- normalization followed by self attention -- to show itperforms clustering on the hypersphere. Altogether, this work urges furtherexchange of useful ideas between these two continents of artificialintelligence.</description><author>Rylan Schaeffer, Nika Zahedi, Mikail Khona, Dhruv Pai, Sang Truong, Yilun Du, Mitchell Ostrow, Sarthak Chandra, Andres Carranza, Ila Rani Fiete, Andrey Gromov, Sanmi Koyejo</author><pubDate>Thu, 15 Feb 2024 18:56:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10202v1</guid></item><item><title>Real-time Animation Generation and Control on Rigged Models via Large Language Models</title><link>http://arxiv.org/abs/2310.17838v2</link><description>We introduce a novel method for real-time animation control and generation onrigged models using natural language input. First, we embed a large languagemodel (LLM) in Unity to output structured texts that can be parsed into diverseand realistic animations. Second, we illustrate LLM's potential to enableflexible state transition between existing animations. We showcase therobustness of our approach through qualitative results on various rigged modelsand motions.</description><author>Han Huang, Fernanda De La Torre, Cathy Mengying Fang, Andrzej Banburski-Fahey, Judith Amores, Jaron Lanier</author><pubDate>Thu, 15 Feb 2024 18:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17838v2</guid></item><item><title>Chain-of-Thought Reasoning Without Prompting</title><link>http://arxiv.org/abs/2402.10200v1</link><description>In enhancing the reasoning capabilities of large language models (LLMs),prior research primarily focuses on specific prompting techniques such asfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, whileeffective, often involve manually intensive prompt engineering. Our study takesa novel approach by asking: Can LLMs reason effectively without prompting? Ourfindings reveal that, intriguingly, CoT reasoning paths can be elicited frompre-trained LLMs by simply altering the \textit{decoding} process. Rather thanconventional greedy decoding, we investigate the top-$k$ alternative tokens,uncovering that CoT paths are frequently inherent in these sequences. Thisapproach not only bypasses the confounders of prompting but also allows us toassess the LLMs' \textit{intrinsic} reasoning abilities. Moreover, we observethat the presence of a CoT in the decoding path correlates with a higherconfidence in the model's decoded answer. This confidence metric effectivelydifferentiates between CoT and non-CoT paths. Extensive empirical studies onvarious reasoning benchmarks show that the proposed CoT-decoding substantiallyoutperforms the standard greedy decoding.</description><author>Xuezhi Wang, Denny Zhou</author><pubDate>Thu, 15 Feb 2024 18:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10200v1</guid></item><item><title>Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention</title><link>http://arxiv.org/abs/2402.10198v1</link><description>Transformer-based architectures achieved breakthrough performance in naturallanguage processing and computer vision, yet they remain inferior to simplerlinear baselines in multivariate long-term forecasting. To better understandthis phenomenon, we start by studying a toy linear forecasting problem forwhich we show that transformers are incapable of converging to their truesolution despite their high expressive power. We further identify the attentionof transformers as being responsible for this low generalization capacity.Building upon this insight, we propose a shallow lightweight transformer modelthat successfully escapes bad local minima when optimized with sharpness-awareoptimization. We empirically demonstrate that this result extends to allcommonly used real-world multivariate time series datasets. In particular,SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% onaverage, while having ~4 times fewer parameters. The code is available athttps://github.com/romilbert/samformer.</description><author>Romain Ilbert, Ambroise Odonnat, Vasilii Feofanov, Aladin Virmaux, Giuseppe Paolo, Themis Palpanas, Ievgen Redko</author><pubDate>Thu, 15 Feb 2024 18:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10198v1</guid></item><item><title>When Less is More: On the Value of "Co-training" for Semi-Supervised Software Defect Predictors</title><link>http://arxiv.org/abs/2211.05920v2</link><description>Labeling a module defective or non-defective is an expensive task. Hence,there are often limits on how much-labeled data is available for training.Semi-supervised classifiers use far fewer labels for training models. However,there are numerous semi-supervised methods, including self-labeling,co-training, maximal-margin, and graph-based methods, to name a few. Only ahandful of these methods have been tested in SE for (e.g.) predicting defectsand even there, those methods have been tested on just a handful of projects. This paper applies a wide range of 55 semi-supervised learners to over 714projects. We find that semi-supervised "co-training methods" work significantlybetter than other approaches. Specifically, after labeling, just 2.5% of data, then make predictions that are competitive to those using 100%of the data. That said, co-training needs to be used cautiously since the specific choiceof co-training methods needs to be carefully selected based on a user'sspecific goals. Also, we warn that a commonly-used co-training method("multi-view"-- where different learners get different sets of columns) doesnot improve predictions (while adding too much to the run time costs 11 hoursvs. 1.8 hours). It is an open question, worthy of future work, to test if these reductionscan be seen in other areas of software analytics. To assist with exploringother areas, all the codes used are available athttps://github.com/ai-se/Semi-Supervised.</description><author>Suvodeep Majumder, Joymallya Chakraborty, Tim Menzies</author><pubDate>Thu, 15 Feb 2024 18:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05920v2</guid></item><item><title>A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents</title><link>http://arxiv.org/abs/2402.10196v1</link><description>Language agents powered by large language models (LLMs) have seen explodingdevelopment. Their capability of using language as a vehicle for thought andcommunication lends an incredible level of flexibility and versatility. Peoplehave quickly capitalized on this capability to connect LLMs to a wide range ofexternal components and environments: databases, tools, the Internet, roboticembodiment, etc. Many believe an unprecedentedly powerful automation technologyis emerging. However, new automation technologies come with new safety risks,especially for intricate systems like language agents. There is a surprisinglylarge gap between the speed and scale of their development and deployment andour understanding of their safety risks. Are we building a house of cards? Inthis position paper, we present the first systematic effort in mappingadversarial attacks against language agents. We first present a unifiedconceptual framework for agents with three major components: Perception, Brain,and Action. Under this framework, we present a comprehensive discussion andpropose 12 potential attack scenarios against different components of an agent,covering different attack strategies (e.g., input manipulation, adversarialdemonstrations, jailbreaking, backdoors). We also draw connections tosuccessful attack strategies previously applied to LLMs. We emphasize theurgency to gain a thorough understanding of language agent risks before theirwidespread deployment.</description><author>Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun</author><pubDate>Thu, 15 Feb 2024 18:51:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10196v1</guid></item><item><title>Inverse Feasibility in Over-the-Air Federated Learning</title><link>http://arxiv.org/abs/2211.14115v4</link><description>We introduce the concept of inverse feasibility for linear forward models asa tool to enhance OTA FL algorithms. Inverse feasibility is defined as an upperbound on the condition number of the forward operator as a function of itsparameters. We analyze an existing OTA FL model using this definition, identifyareas for improvement, and propose a new OTA FL model. Numerical experimentsillustrate the main implications of the theoretical results. The proposedframework, which is based on inverse problem theory, can potentially complementexisting notions of security and privacy by providing additional desirablecharacteristics to networks.</description><author>Tomasz Piotrowski, Rafail Ismayilov, Matthias Frey, Renato L. G. Cavalcante</author><pubDate>Thu, 15 Feb 2024 18:50:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14115v4</guid></item><item><title>BitDelta: Your Fine-Tune May Only Be Worth One Bit</title><link>http://arxiv.org/abs/2402.10193v1</link><description>Large Language Models (LLMs) are typically trained in two phases:pre-training on large internet-scale datasets, and fine-tuning for downstreamtasks. Given the higher computational demand of pre-training, it's intuitive toassume that fine-tuning adds less new information to the model, and is thusmore compressible. We explore this assumption by decomposing the weights offine-tuned models into their pre-trained components and an additional delta. Weintroduce a simple method, BitDelta, which successfully quantizes this deltadown to 1 bit without compromising performance. This interesting finding notonly highlights the potential redundancy of information added duringfine-tuning, but also has significant implications for the multi-tenant servingand multi-tenant storage of fine-tuned models. By enabling the use of a singlehigh-precision base model accompanied by multiple 1-bit deltas, BitDeltadramatically reduces GPU memory requirements by more than 10x, which can alsobe translated to enhanced generation latency in multi-tenant settings. Wevalidate BitDelta through experiments across Llama-2 and Mistral modelfamilies, and on models up to 70B parameters, showcasing minimal performancedegradation over all tested settings.</description><author>James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle Cai</author><pubDate>Thu, 15 Feb 2024 18:50:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10193v1</guid></item><item><title>Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias</title><link>http://arxiv.org/abs/2402.10192v1</link><description>With the impressive progress of deep learning, applications relying onmachine learning are increasingly being integrated into daily life. However,most deep learning models have an opaque, oracle-like nature making itdifficult to interpret and understand their decisions. This problem led to thedevelopment of the field known as eXplainable Artificial Intelligence (XAI).One method in this field known as Projective Simulation (PS) models achain-of-thought as a random walk of a particle on a graph with vertices thathave concepts attached to them. While this description has various benefits,including the possibility of quantization, it cannot be naturally used to modelthoughts that combine several concepts simultaneously. To overcome thislimitation, we introduce Multi-Excitation Projective Simulation (mePS), ageneralization that considers a chain-of-thought to be a random walk of severalparticles on a hypergraph. A definition for a dynamic hypergraph is put forwardto describe the agent's training history along with applications to AI andhypergraph visualization. An inductive bias inspired by the remarkablysuccessful few-body interaction models used in quantum many-body physics isformalized for our classical mePS framework and employed to tackle theexponential complexity associated with naive implementations of hypergraphs. Weprove that our inductive bias reduces the complexity from exponential topolynomial, with the exponent representing the cutoff on how many particles caninteract. We numerically apply our method to two toy environments and a morecomplex scenario modelling the diagnosis of a broken computer. Theseenvironments demonstrate the resource savings provided by an appropriate choiceof inductive bias, as well as showcasing aspects of interpretability. A quantummodel for mePS is also briefly outlined and some future directions for it arediscussed.</description><author>Philip A. LeMaitre, Marius Krumm, Hans J. Briegel</author><pubDate>Thu, 15 Feb 2024 18:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10192v1</guid></item><item><title>FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss for Unlabeled Clients</title><link>http://arxiv.org/abs/2402.10191v1</link><description>Federated learning (FL) is a distributed learning paradigm that facilitatescollaborative training of a shared global model across devices while keepingdata localized. The deployment of FL in numerous real-world applications facesdelays, primarily due to the prevalent reliance on supervised tasks. Generatingdetailed labels at edge devices, if feasible, is demanding, given resourceconstraints and the imperative for continuous data updates. In addressing thesechallenges, solutions such as federated semi-supervised learning (FSSL), whichrelies on unlabeled clients' data and a limited amount of labeled data on theserver, become pivotal. In this paper, we propose FedAnchor, an innovative FSSLmethod that introduces a unique double-head structure, called anchor head,paired with the classification head trained exclusively on labeled anchor dataon the server. The anchor head is empowered with a newly designed labelcontrastive loss based on the cosine similarity metric. Our approach mitigatesthe confirmation bias and overfitting issues associated with pseudo-labelingtechniques based on high-confidence model prediction samples. Extensiveexperiments on CIFAR10/100 and SVHN datasets demonstrate that our methodoutperforms the state-of-the-art method by a significant margin in terms ofconvergence rate and model accuracy.</description><author>Xinchi Qiu, Yan Gao, Lorenzo Sani, Heng Pan, Wanru Zhao, Pedro P. B. Gusmao, Mina Alibeigi, Alex Iacob, Nicholas D. Lane</author><pubDate>Thu, 15 Feb 2024 18:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10191v1</guid></item><item><title>Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models</title><link>http://arxiv.org/abs/2402.10189v1</link><description>In-context learning has emerged as a groundbreaking ability of Large LanguageModels (LLMs) and revolutionized various fields by providing a fewtask-relevant demonstrations in the prompt. However, trustworthy issues withLLM's response, such as hallucination, have also been actively discussed.Existing works have been devoted to quantifying the uncertainty in LLM'sresponse, but they often overlook the complex nature of LLMs and the uniquenessof in-context learning. In this work, we delve into the predictive uncertaintyof LLMs associated with in-context learning, highlighting that suchuncertainties may stem from both the provided demonstrations (aleatoricuncertainty) and ambiguities tied to the model's configurations (epistemicuncertainty). We propose a novel formulation and corresponding estimationmethod to quantify both types of uncertainties. The proposed method offers anunsupervised way to understand the prediction of in-context learning in aplug-and-play fashion. Extensive experiments are conducted to demonstrate theeffectiveness of the decomposition. The code and data are available at:\url{https://github.com/lingchen0331/UQ_ICL}.</description><author>Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</author><pubDate>Thu, 15 Feb 2024 18:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10189v1</guid></item><item><title>Self-consistent Validation for Machine Learning Electronic Structure</title><link>http://arxiv.org/abs/2402.10186v1</link><description>Machine learning has emerged as a significant approach to efficiently tackleelectronic structure problems. Despite its potential, there is less guaranteefor the model to generalize to unseen data that hinders its application inreal-world scenarios. To address this issue, a technique has been proposed toestimate the accuracy of the predictions. This method integrates machinelearning with self-consistent field methods to achieve both low validation costand interpret-ability. This, in turn, enables exploration of the model'sability with active learning and instills confidence in its integration intoreal-world studies.</description><author>Gengyuan Hu, Gengchen Wei, Zekun Lou, Philip H. S. Torr, Wanli Ouyang, Han-sen Zhong, Chen Lin</author><pubDate>Thu, 15 Feb 2024 18:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10186v1</guid></item><item><title>Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective</title><link>http://arxiv.org/abs/2402.10184v1</link><description>There is a trilemma in reinforcement learning from human feedback (RLHF): theincompatibility between highly diverse contexts, low labeling cost, andreliable alignment performance. Here we aim to mitigate such incompatibilitythrough the design of dataset information structures during reward modeling.Specifically, we first reexamine the RLHF process and propose a theoreticalframework portraying it as an autoencoding process over text distributions. Ourframework formalizes the RLHF objective of ensuring distributional consistencybetween human preference and large language model (LLM) behavior. Building onthis framework, we then systematically investigate the performance impact ofinformation structure in the reward modeling stage of RLHF. To furtherunderstand reward generalization in the reward modeling stage, we introduce anew method based on random graph theory that models generalization in thesemantic space. A key insight of our analysis is the superiority of thetree-based information structure in reward modeling, compared to chain-basedbaselines adopted by conventional RLHF methods. We derive that under highlycomplex contexts with limited data, the tree-based reward model (RM) induces upto $\Theta(\log n/\log\log n)$ times less variance than chain-based RM where$n$ is the dataset size. To validate our theoretical contribution, wedemonstrate that on three different NLP tasks, the tree-based RM achieves 65%win rate on average against chain-based baselines. Looking forward, we hope ourframework can serve as a step towards understanding goal misgeneralization.</description><author>Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou, Han Yang, Josef Dai, Xuehai Pan, Yaodong Yang</author><pubDate>Thu, 15 Feb 2024 18:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10184v1</guid></item><item><title>Generalizing across Temporal Domains with Koopman Operators</title><link>http://arxiv.org/abs/2402.07834v2</link><description>In the field of domain generalization, the task of constructing a predictivemodel capable of generalizing to a target domain without access to target dataremains challenging. This problem becomes further complicated when consideringevolving dynamics between domains. While various approaches have been proposedto address this issue, a comprehensive understanding of the underlyinggeneralization theory is still lacking. In this study, we contribute noveltheoretic results that aligning conditional distribution leads to the reductionof generalization bounds. Our analysis serves as a key motivation for solvingthe Temporal Domain Generalization (TDG) problem through the application ofKoopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). Byemploying Koopman Operators, we effectively address the time-evolvingdistributions encountered in TDG using the principles of Koopman theory, wheremeasurement functions are sought to establish linear transition relationsbetween evolving domains. Through empirical evaluations conducted on syntheticand real-world datasets, we validate the effectiveness of our proposedapproach.</description><author>Qiuhao Zeng, Wei Wang, Fan Zhou, Gezheng Xu, Ruizhi Pu, Changjian Shui, Christian Gagne, Shichun Yang, Boyu Wang, Charles X. Ling</author><pubDate>Thu, 15 Feb 2024 18:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07834v2</guid></item><item><title>TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation</title><link>http://arxiv.org/abs/2402.10178v1</link><description>The emergence of Large Language Models (LLMs) like ChatGPT has inspired thedevelopment of LLM-based agents capable of addressing complex, real-worldtasks. However, these agents often struggle during task execution due tomethodological constraints, such as error propagation and limited adaptability.To address this issue, we propose a multi-agent framework based on dynamic TaskDecomposition and Agent Generation (TDAG). This framework dynamicallydecomposes complex tasks into smaller subtasks and assigns each to aspecifically generated subagent, thereby enhancing adaptability in diverse andunpredictable real-world tasks. Simultaneously, existing benchmarks often lackthe granularity needed to evaluate incremental progress in complex, multi-steptasks. In response, we introduce ItineraryBench in the context of travelplanning, featuring interconnected, progressively complex tasks with afine-grained evaluation system. ItineraryBench is designed to assess agents'abilities in memory, planning, and tool usage across tasks of varyingcomplexity. Our experimental results reveal that TDAG significantly outperformsestablished baselines, showcasing its superior adaptability and contextawareness in complex task scenarios.</description><author>Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, Jinsong Su</author><pubDate>Thu, 15 Feb 2024 18:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10178v1</guid></item><item><title>Large Scale Constrained Clustering With Reinforcement Learning</title><link>http://arxiv.org/abs/2402.10177v1</link><description>Given a network, allocating resources at clusters level, rather than at eachnode, enhances efficiency in resource allocation and usage. In this paper, westudy the problem of finding fully connected disjoint clusters to minimize theintra-cluster distances and maximize the number of nodes assigned to theclusters, while also ensuring that no two nodes within a cluster exceed athreshold distance. While the problem can easily be formulated using a binarylinear model, traditional combinatorial optimization solvers struggle whendealing with large-scale instances. We propose an approach to solve thisconstrained clustering problem via reinforcement learning. Our method involvestraining an agent to generate both feasible and (near) optimal solutions. Theagent learns problem-specific heuristics, tailored to the instances encounteredin this task. In the results section, we show that our algorithm finds nearoptimal solutions, even for large scale instances.</description><author>Benedikt Schesch, Marco Caserta</author><pubDate>Thu, 15 Feb 2024 18:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10177v1</guid></item><item><title>OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset</title><link>http://arxiv.org/abs/2402.10176v1</link><description>Recent work has shown the immense potential of synthetically generateddatasets for training large language models (LLMs), especially for acquiringtargeted skills. Current large-scale math instruction tuning datasets such asMetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructedusing outputs from closed-source LLMs with commercially restrictive licenses. Akey reason limiting the use of open-source LLMs in these data generationpipelines has been the wide gap between the mathematical skills of the bestclosed-source LLMs, such as GPT-4, and the best open-source LLMs. Building onthe recent progress in open-source LLMs, our proposed prompting novelty, andsome brute-force scaling, we construct OpenMathInstruct-1, a math instructiontuning dataset with 1.8M problem-solution pairs. The dataset is constructed bysynthesizing code-interpreter solutions for GSM8K and MATH, two popular mathreasoning benchmarks, using the recently released and permissively licensedMixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset ofOpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, whichis competitive with the best gpt-distilled models. We release our code, models,and the OpenMathInstruct-1 dataset under a commercially permissive license.</description><author>Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman</author><pubDate>Thu, 15 Feb 2024 18:26:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10176v1</guid></item><item><title>Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications</title><link>http://arxiv.org/abs/2402.09015v2</link><description>The rapid development in the field of Large Language Models (LLMs) has led toa surge in applications that facilitate collaboration among multiple agents toassist humans in their daily tasks. However, a significant gap remains inassessing whether LLM-powered applications genuinely enhance user experienceand task execution efficiency. This highlights the pressing need for methods toverify utility of LLM-powered applications, particularly by ensuring alignmentbetween the application's functionality and end-user needs. We introduceAgentEval provides an implementation for the math problems}, a novel frameworkdesigned to simplify the utility verification process by automaticallyproposing a set of criteria tailored to the unique purpose of any givenapplication. This allows for a comprehensive assessment, quantifying theutility of an application against the suggested criteria. We present acomprehensive analysis of the robustness of quantifier's work.</description><author>Negar Arabzadeh, Julia Kiseleva, Qingyun Wu, Chi Wang, Ahmed Awadallah, Victor Dibia, Adam Fourney, Charles Clarke</author><pubDate>Thu, 15 Feb 2024 18:24:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09015v2</guid></item><item><title>Out-Of-Domain Unlabeled Data Improves Generalization</title><link>http://arxiv.org/abs/2310.00027v2</link><description>We propose a novel framework for incorporating unlabeled data intosemi-supervised classification problems, where scenarios involving theminimization of either i) adversarially robust or ii) non-robust loss functionshave been considered. Notably, we allow the unlabeled samples to deviateslightly (in total variation sense) from the in-domain distribution. The coreidea behind our framework is to combine Distributionally Robust Optimization(DRO) with self-supervised training. As a result, we also leverage efficientpolynomial-time algorithms for the training stage. From a theoreticalstandpoint, we apply our framework on the classification problem of a mixtureof two Gaussians in $\mathbb{R}^d$, where in addition to the $m$ independentand labeled samples from the true distribution, a set of $n$ (usually with$n\gg m$) out of domain and unlabeled samples are given as well. Using only thelabeled data, it is known that the generalization error can be bounded by$\propto\left(d/m\right)^{1/2}$. However, using our method on both isotropicand non-isotropic Gaussian mixture models, one can derive a new set ofanalytically explicit and non-asymptotic bounds which show substantialimprovement on the generalization error compared to ERM. Our results underscoretwo significant insights: 1) out-of-domain samples, even when unlabeled, can beharnessed to narrow the generalization gap, provided that the true datadistribution adheres to a form of the ``cluster assumption", and 2) thesemi-supervised learning paradigm can be regarded as a special case of ourframework when there are no distributional shifts. We validate our claimsthrough experiments conducted on a variety of synthetic and real-worlddatasets.</description><author>Amir Hossein Saberi, Amir Najafi, Alireza Heidari, Mohammad Hosein Movasaghinia, Abolfazl Motahari, Babak H. Khalaj</author><pubDate>Thu, 15 Feb 2024 18:23:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00027v2</guid></item><item><title>Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence</title><link>http://arxiv.org/abs/2402.10175v1</link><description>Recent large language models (LLMs) have shown remarkable performance inaligning generated text with user intentions across various tasks. When itcomes to long-form text generation, there has been a growing interest ingeneration from a discourse coherence perspective. However, existing lexical orsemantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture thediscourse coherence. The development of discourse-specific automatic evaluationmethods for assessing the output of LLMs warrants greater focus andexploration. In this paper, we present a novel automatic metric designed toquantify the discourse divergence between two long-form articles. Extensiveexperiments on three datasets from representative domains demonstrate that ourmetric aligns more closely with human preferences and GPT-4 coherenceevaluation, outperforming existing evaluation methods.</description><author>Yinhong Liu, Yixuan Su, Ehsan Shareghi, Nigel Collier</author><pubDate>Thu, 15 Feb 2024 18:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10175v1</guid></item><item><title>OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models</title><link>http://arxiv.org/abs/2402.10172v1</link><description>Optimization problems are pervasive in sectors from manufacturing anddistribution to healthcare. However, most such problems are still solvedheuristically by hand rather than optimally by state-of-the-art solvers becausethe expertise required to formulate and solve these problems limits thewidespread adoption of optimization tools and techniques. This paper introducesOptiMUS, a Large Language Model (LLM)-based agent designed to formulate andsolve (mixed integer) linear programming problems from their natural languagedescriptions. OptiMUS can develop mathematical models, write and debug solvercode, evaluate the generated solutions, and improve its model and code based onthese evaluations. OptiMUS utilizes a modular structure to process problems,allowing it to handle problems with long descriptions and complex data withoutlong prompts. Experiments demonstrate that OptiMUS outperforms existingstate-of-the-art methods on easy datasets by more than $20\%$ and on harddatasets (including a new dataset, NLP4LP, released with this paper thatfeatures long and complex problems) by more than $30\%$.</description><author>Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell</author><pubDate>Thu, 15 Feb 2024 18:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10172v1</guid></item><item><title>Data Engineering for Scaling Language Models to 128K Context</title><link>http://arxiv.org/abs/2402.10171v1</link><description>We study the continual pretraining recipe for scaling language models'context lengths to 128K, with a focus on data engineering. We hypothesize thatlong context modeling, in particular \textit{the ability to utilize informationat arbitrary input locations}, is a capability that is mostly already acquiredthrough large-scale pretraining, and that this capability can be readilyextended to contexts substantially longer than seen during training~(e.g., 4Kto 128K) through lightweight continual pretraining on appropriate data mixture.We investigate the \textit{quantity} and \textit{quality} of the data forcontinual pretraining: (1) for quantity, we show that 500 million to 5 billiontokens are enough to enable the model to retrieve information anywhere withinthe 128K context; (2) for quality, our results equally emphasize \textit{domainbalance} and \textit{length upsampling}. Concretely, we find that naivelyupsampling longer data on certain domains like books, a common practice ofexisting work, gives suboptimal performance, and that a balanced domain mixtureis important. We demonstrate that continual pretraining of the full model on1B-5B tokens of such data is an effective and affordable strategy for scalingthe context length of language models to 128K. Our recipe outperforms strongopen-source long-context models and closes the gap to frontier models likeGPT-4 128K.</description><author>Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, Hao Peng</author><pubDate>Thu, 15 Feb 2024 18:19:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10171v1</guid></item><item><title>Secure Vertical Federated Learning Under Unreliable Connectivity</title><link>http://arxiv.org/abs/2305.16794v2</link><description>Most work in privacy-preserving federated learning (FL) has focused onhorizontally partitioned datasets where clients hold the same features andtrain complete client-level models independently. However, individual datapoints are often scattered across different institutions, known as clients, invertical FL (VFL) settings. Addressing this category of FL necessitates theexchange of intermediate outputs and gradients among participants, resulting inpotential privacy leakage risks and slow convergence rates. Additionally, inmany real-world scenarios, VFL training also faces the acute issue of clientstragglers and drop-outs, a serious challenge that can significantly hinder thetraining process but has been largely overlooked in existing studies. In thiswork, we present vFedSec, a first dropout-tolerant VFL protocol, which cansupport the most generalized vertical framework. It achieves secure andefficient model training by using an innovative Secure Layer alongside anembedding-padding technique. We provide theoretical proof that our designattains enhanced security while maintaining training performance. Empiricalresults from extensive experiments also demonstrate vFedSec is robust to clientdropout and provides secure training with negligible computation andcommunication overhead. Compared to widely adopted homomorphic encryption (HE)methods, our approach achieves a remarkable &gt; 690x speedup and reducescommunication costs significantly by &gt; 9.6x.</description><author>Xinchi Qiu, Heng Pan, Wanru Zhao, Chenyang Ma, William F. Shen, Pedro P. B. Gusmao, Nicholas D. Lane</author><pubDate>Thu, 15 Feb 2024 18:16:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16794v2</guid></item><item><title>Minimally Supervised Learning using Topological Projections in Self-Organizing Maps</title><link>http://arxiv.org/abs/2401.06923v2</link><description>Parameter prediction is essential for many applications, facilitatinginsightful interpretation and decision-making. However, in many real lifedomains, such as power systems, medicine, and engineering, it can be veryexpensive to acquire ground truth labels for certain datasets as they mayrequire extensive and expensive laboratory testing. In this work, we introducea semi-supervised learning approach based on topological projections inself-organizing maps (SOMs), which significantly reduces the required number oflabeled data points to perform parameter prediction, effectively exploitinginformation contained in large unlabeled datasets. Our proposed method firsttrains SOMs on unlabeled data and then a minimal number of available labeleddata points are assigned to key best matching units (BMU). The values estimatedfor newly-encountered data points are computed utilizing the average of the $n$closest labeled data points in the SOM's U-matrix in tandem with a topologicalshortest path distance calculation scheme. Our results indicate that theproposed minimally supervised model significantly outperforms traditionalregression techniques, including linear and polynomial regression, Gaussianprocess regression, K-nearest neighbors, as well as deep neural network modelsand related clustering schemes.</description><author>Zimeng Lyu, Alexander Ororbia, Rui Li, Travis Desell</author><pubDate>Thu, 15 Feb 2024 18:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06923v2</guid></item><item><title>The Emergence of Reproducibility and Consistency in Diffusion Models</title><link>http://arxiv.org/abs/2310.05264v2</link><description>In this work, we investigate an intriguing and prevalent phenomenon ofdiffusion models which we term as "consistent model reproducibility": given thesame starting noise input and a deterministic sampler, different diffusionmodels often yield remarkably similar outputs. We confirm this phenomenonthrough comprehensive experiments, implying that different diffusion modelsconsistently reach the same data distribution and scoring function regardlessof diffusion model frameworks, model architectures, or training procedures.More strikingly, our further investigation implies that diffusion models arelearning distinct distributions affected by the training data size. This issupported by the fact that the model reproducibility manifests in two distincttraining regimes: (i) "memorization regime", where the diffusion model overfitsto the training data distribution, and (ii) "generalization regime", where themodel learns the underlying data distribution. Our study also finds that thisvaluable property generalizes to many variants of diffusion models, includingthose for conditional use, solving inverse problems, and model fine-tuning.Finally, our work raises numerous intriguing theoretical questions for futureinvestigation and highlights practical implications regarding trainingefficiency, model privacy, and the controlled generation of diffusion models.</description><author>Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Peng Wang, Liyue Shen, Qing Qu</author><pubDate>Thu, 15 Feb 2024 18:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05264v2</guid></item><item><title>DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning</title><link>http://arxiv.org/abs/2402.10168v1</link><description>A vital aspect of Indian Classical Music (ICM) is Raga, which serves as amelodic framework for compositions and improvisations alike. Raga Recognitionis an important music information retrieval task in ICM as it can aid numerousdownstream applications ranging from music recommendations to organizing hugemusic collections. In this work, we propose a deep learning based approach toRaga recognition. Our approach employs efficient pre possessing and learnstemporal sequences in music data using Long Short Term Memory based RecurrentNeural Networks (LSTM-RNN). We train and test the network on smaller sequencessampled from the original audio while the final inference is performed on theaudio as a whole. Our method achieves an accuracy of 88.1% and 97 % duringinference on the Comp Music Carnatic dataset and its 10 Raga subsetrespectively making it the state-of-the-art for the Raga recognition task. Ourapproach also enables sequence ranking which aids us in retrieving melodicpatterns from a given music data base that are closely related to the presentedquery sequence.</description><author>Sathwik Tejaswi Madhusudhan, Girish Chowdhary</author><pubDate>Thu, 15 Feb 2024 18:11:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10168v1</guid></item><item><title>Random features and polynomial rules</title><link>http://arxiv.org/abs/2402.10164v1</link><description>Random features models play a distinguished role in the theory of deeplearning, describing the behavior of neural networks close to theirinfinite-width limit. In this work, we present a thorough analysis of thegeneralization performance of random features models for generic supervisedlearning problems with Gaussian data. Our approach, built with tools from thestatistical mechanics of disordered systems, maps the random features model toan equivalent polynomial model, and allows us to plot average generalizationcurves as functions of the two main control parameters of the problem: thenumber of random features $N$ and the size $P$ of the training set, bothassumed to scale as powers in the input dimension $D$. Our results extend thecase of proportional scaling between $N$, $P$ and $D$. They are in accordancewith rigorous bounds known for certain particular learning tasks and are inquantitative agreement with numerical experiments performed over many order ofmagnitudes of $N$ and $P$. We find good agreement also far from the asymptoticlimits where $D\to \infty$ and at least one between $P/D^K$, $N/D^L$ remainsfinite.</description><author>Fabián Aguirre-López, Silvio Franz, Mauro Pastore</author><pubDate>Thu, 15 Feb 2024 18:09:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10164v1</guid></item><item><title>Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks</title><link>http://arxiv.org/abs/2402.10163v1</link><description>Traveling waves are a fundamental phenomenon in the brain, playing a crucialrole in short-term information storage. In this study, we leverage the conceptof traveling wave dynamics within a neural lattice to formulate a theoreticalmodel of neural working memory, study its properties, and its real worldimplications in AI. The proposed model diverges from traditional approaches,which assume information storage in static, register-like locations updated byinterference. Instead, the model stores data as waves that is updated by thewave's boundary conditions. We rigorously examine the model's capabilities inrepresenting and learning state histories, which are vital for learninghistory-dependent dynamical systems. The findings reveal that the modelreliably stores external information and enhances the learning process byaddressing the diminishing gradient problem. To understand the model'sreal-world applicability, we explore two cases: linear boundary condition andnon-linear, self-attention-driven boundary condition. The experiments revealthat the linear scenario is effectively learned by Recurrent Neural Networks(RNNs) through backpropagation when modeling history-dependent dynamicalsystems. Conversely, the non-linear scenario parallels the autoregressive loopof an attention-only transformer. Collectively, our findings suggest thebroader relevance of traveling waves in AI and its potential in advancingneural network architectures.</description><author>Arjun Karuvally, Terrence J. Sejnowski, Hava T. Siegelmann</author><pubDate>Thu, 15 Feb 2024 18:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10163v1</guid></item><item><title>Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients</title><link>http://arxiv.org/abs/2402.10153v1</link><description>Effective diabetes management is crucial for maintaining health in diabeticpatients. Large Language Models (LLMs) have opened new avenues for diabetesmanagement, facilitating their efficacy. However, current LLM-based approachesare limited by their dependence on general sources and lack of integration withdomain-specific knowledge, leading to inaccurate responses. In this paper, wepropose a knowledge-infused LLM-powered conversational health agent (CHA) fordiabetic patients. We customize and leverage the open-source openCHA framework,enhancing our CHA with external knowledge and analytical capabilities. Thisintegration involves two key components: 1) incorporating the American DiabetesAssociation dietary guidelines and the Nutritionix information and 2) deployinganalytical tools that enable nutritional intake calculation and comparison withthe guidelines. We compare the proposed CHA with GPT4. Our evaluation includes100 diabetes-related questions on daily meal choices and assessing thepotential risks associated with the suggested diet. Our findings show that theproposed agent demonstrates superior performance in generating responses tomanage essential nutrients.</description><author>Mahyar Abbasian, Zhongqi Yang, Elahe Khatibi, Pengfei Zhang, Nitish Nagesh, Iman Azimi, Ramesh Jain, Amir M. Rahmani</author><pubDate>Thu, 15 Feb 2024 18:00:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10153v1</guid></item><item><title>ControlLM: Crafting Diverse Personalities for Language Models</title><link>http://arxiv.org/abs/2402.10151v1</link><description>As language models continue to scale in size and capability, they display anarray of emerging behaviors, both beneficial and concerning. This heightens theneed to control model behaviors. We hope to be able to control the personalitytraits of language models at the inference-time so as to have various characterfeatures, on top of which the requirements of different types of tasks can bemet. Personality is a higher-level and more abstract behavioral representationfor language models. We introduce ControlLM, which leverages differentialactivation patterns, derived from contrasting behavioral prompts in the model'slatent space, to influence the model's personality traits at inference. Thisapproach allows for the precise, real-time adjustment of model behavior. First,we demonstrate ControlLM's capacity to elicit diverse persona behaviors withoutany training, while precision control allows personality traits to closelymatch average human values. Subsequently, we showcase improved reasoning andquestion answering through selective amplification of beneficial attributeslike conscientiousness and friendliness. We hope that this work will inspireresearch on controlling human-like behaviors of language models and provideinsights for future research. Our code is publicly available at:https://github.com/wengsyx/ControlLM.</description><author>Yixuan Weng, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao</author><pubDate>Thu, 15 Feb 2024 17:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10151v1</guid></item><item><title>$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning</title><link>http://arxiv.org/abs/2402.10150v1</link><description>In self-supervised contrastive learning, a widely-adopted objective functionis InfoNCE, which uses the heuristic cosine similarity for the representationcomparison, and is closely related to maximizing the Kullback-Leibler(KL)-based mutual information. In this paper, we aim at answering twointriguing questions: (1) Can we go beyond the KL-based objective? (2) Besidesthe popular cosine similarity, can we design a better similarity function? Weprovide answers to both questions by generalizing the KL-based mutualinformation to the $f$-Mutual Information in Contrastive Learning ($f$-MICL)using the $f$-divergences. To answer the first question, we provide a widerange of $f$-MICL objectives which share the nice properties of InfoNCE (e.g.,alignment and uniformity), and meanwhile result in similar or even superiorperformance. For the second question, assuming that the joint featuredistribution is proportional to the Gaussian kernel, we derive an $f$-Gaussiansimilarity with better interpretability and empirical performance. Finally, weidentify close relationships between the $f$-MICL objective and several popularInfoNCE-based objectives. Using benchmark tasks from both vision and naturallanguage, we empirically evaluate $f$-MICL with different $f$-divergences onvarious architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that$f$-MICL generally outperforms the benchmarks and the best-performing$f$-divergence is task and dataset dependent.</description><author>Yiwei Lu, Guojun Zhang, Sun Sun, Hongyu Guo, Yaoliang Yu</author><pubDate>Thu, 15 Feb 2024 17:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10150v1</guid></item><item><title>How to Fix a Broken Confidence Estimator: Evaluating Post-hoc Methods for Selective Classification with Deep Neural Networks</title><link>http://arxiv.org/abs/2305.15508v3</link><description>This paper addresses the problem of selective classification for deep neuralnetworks, where a model is allowed to abstain from low-confidence predictionsto avoid potential errors. We focus on so-called post-hoc methods, whichreplace the confidence estimator of a given classifier without modifying orretraining it, thus being practically appealing. Considering neural networkswith softmax outputs, our goal is to identify the best confidence estimatorthat can be computed directly from the unnormalized logits. This problem ismotivated by the intriguing observation in recent work that many classifiersappear to have a "broken" confidence estimator, in the sense that theirselective classification performance is much worse than what could be expectedby their corresponding accuracies. We perform an extensive experimental studyof many existing and proposed confidence estimators applied to 84 pretrainedImageNet classifiers available from popular repositories. Our results show thata simple $p$-norm normalization of the logits, followed by taking the maximumlogit as the confidence estimator, can lead to considerable gains in selectiveclassification performance, completely fixing the pathological behaviorobserved in many classifiers. As a consequence, the selective classificationperformance of any classifier becomes almost entirely determined by itscorresponding accuracy. Moreover, these results are shown to be consistentunder distribution shift.</description><author>Luís Felipe P. Cattelan, Danilo Silva</author><pubDate>Thu, 15 Feb 2024 17:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15508v3</guid></item><item><title>A chaotic maps-based privacy-preserving distributed deep learning for incomplete and Non-IID datasets</title><link>http://arxiv.org/abs/2402.10145v1</link><description>Federated Learning is a machine learning approach that enables the trainingof a deep learning model among several participants with sensitive data thatwish to share their own knowledge without compromising the privacy of theirdata. In this research, the authors employ a secured Federated Learning methodwith an additional layer of privacy and proposes a method for addressing thenon-IID challenge. Moreover, differential privacy is compared withchaotic-based encryption as layer of privacy. The experimental approachassesses the performance of the federated deep learning model with differentialprivacy using both IID and non-IID data. In each experiment, the FederatedLearning process improves the average performance metrics of the deep neuralnetwork, even in the case of non-IID data.</description><author>Irina Arévalo, Jose L. Salmeron</author><pubDate>Thu, 15 Feb 2024 17:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10145v1</guid></item><item><title>Tracking Changing Probabilities via Dynamic Learners</title><link>http://arxiv.org/abs/2402.10142v1</link><description>Consider a predictor, a learner, whose input is a stream of discrete items.The predictor's task, at every time point, is probabilistic multiclassprediction, i.e., to predict which item may occur next by outputting zero ormore candidate items, each with a probability, after which the actual item isrevealed and the predictor learns from this observation. To outputprobabilities, the predictor keeps track of the proportions of the items it hasseen. The predictor has constant (limited) space and we seek efficientprediction and update techniques: The stream is unbounded, the set of items isunknown to the predictor and their totality can also grow unbounded. Moreover,there is non-stationarity: the underlying frequencies of items may change,substantially, from time to time. For instance, new items may start appearingand a few currently frequent items may cease to occur again. The predictor,being space-bounded, need only provide probabilities for those items with(currently) sufficiently high frequency, i.e., the salient items. This problemis motivated in the setting of prediction games, a self-supervised learningregime where concepts serve as both the predictors and the predictands, and theset of concepts grows over time, resulting in non-stationarities as newconcepts are generated and used. We develop moving average techniques designedto respond to such non-stationarities in a timely manner, and explore theirproperties. One is a simple technique based on queuing of count snapshots, andanother is a combination of queuing together with an extended version of sparseEMA. The latter combination supports predictand-specific dynamic learningrates. We find that this flexibility allows for a more accurate and timelyconvergence.</description><author>Omid Madani</author><pubDate>Thu, 15 Feb 2024 17:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10142v1</guid></item><item><title>Better Fair than Sorry: Adversarial Missing Data Imputation for Fair GNNs</title><link>http://arxiv.org/abs/2311.01591v2</link><description>This paper addresses the problem of learning fair Graph Neural Networks(GNNs) under missing protected attributes. GNNs have achieved state-of-the-artresults in many relevant tasks where decisions might disproportionately impactspecific communities. However, existing work on fair GNNs assumes that eitherprotected attributes are fully-observed or that the missing data imputation isfair. In practice, biases in the imputation will be propagated to the modeloutcomes, leading them to overestimate the fairness of their predictions. Weaddress this challenge by proposing Better Fair than Sorry (BFtS), a fairmissing data imputation model for protected attributes used by fair GNNs. Thekey design principle behind BFtS is that imputations should approximate theworst-case scenario for the fair GNN -- i.e. when optimizing fairness is thehardest. We implement this idea using a 3-player adversarial scheme where twoadversaries collaborate against the fair GNN. Experiments using synthetic andreal datasets show that BFtS often achieves a better fairness $\times$ accuracytrade-off than existing alternatives.</description><author>Debolina Halder Lina, Arlei Silva</author><pubDate>Thu, 15 Feb 2024 17:48:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01591v2</guid></item><item><title>Concentrated Differential Privacy for Bandits</title><link>http://arxiv.org/abs/2309.00557v2</link><description>Bandits serve as the theoretical foundation of sequential learning and analgorithmic foundation of modern recommender systems. However, recommendersystems often rely on user-sensitive data, making privacy a critical concern.This paper contributes to the understanding of Differential Privacy (DP) inbandits with a trusted centralised decision-maker, and especially theimplications of ensuring zero Concentrated Differential Privacy (zCDP). First,we formalise and compare different adaptations of DP to bandits, depending onthe considered input and the interaction protocol. Then, we propose threeprivate algorithms, namely AdaC-UCB, AdaC-GOPE and AdaC-OFUL, for three banditsettings, namely finite-armed bandits, linear bandits, and linear contextualbandits. The three algorithms share a generic algorithmic blueprint, i.e. theGaussian mechanism and adaptive episodes, to ensure a good privacy-utilitytrade-off. We analyse and upper bound the regret of these three algorithms. Ouranalysis shows that in all of these settings, the prices of imposing zCDP are(asymptotically) negligible in comparison with the regrets incurred obliviousto privacy. Next, we complement our regret upper bounds with the first minimaxlower bounds on the regret of bandits with zCDP. To prove the lower bounds, weelaborate a new proof technique based on couplings and optimal transport. Weconclude by experimentally validating our theoretical results for the threedifferent settings of bandits.</description><author>Achraf Azize, Debabrota Basu</author><pubDate>Thu, 15 Feb 2024 17:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00557v2</guid></item><item><title>Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition</title><link>http://arxiv.org/abs/2302.04944v2</link><description>Training a team to complete a complex task via multi-agent reinforcementlearning can be difficult due to challenges such as policy search in a largejoint policy space, and non-stationarity caused by mutually adapting agents. Tofacilitate efficient learning of complex multi-agent tasks, we propose anapproach which uses an expert-provided decomposition of a task into simplermulti-agent sub-tasks. In each sub-task, a subset of the entire team is trainedto acquire sub-task-specific policies. The sub-teams are then merged andtransferred to the target task, where their policies are collectivelyfine-tuned to solve the more complex target task. We show empirically that suchapproaches can greatly reduce the number of timesteps required to solve acomplex target task relative to training from-scratch. However, we alsoidentify and investigate two problems with naive implementations of approachesbased on sub-task decomposition, and propose a simple and scalable method toaddress these problems which augments existing actor-critic algorithms. Wedemonstrate the empirical benefits of our proposed method, enabling sub-taskdecomposition approaches to be deployed in diverse multi-agent tasks.</description><author>Elliot Fosong, Arrasy Rahman, Ignacio Carlucho, Stefano V. Albrecht</author><pubDate>Thu, 15 Feb 2024 17:43:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04944v2</guid></item><item><title>Connectivity Oracles for Predictable Vertex Failures</title><link>http://arxiv.org/abs/2312.08489v2</link><description>The problem of designing connectivity oracles supporting vertex failures isone of the basic data structures problems for undirected graphs. It is alreadywell understood: previous works [Duan--Pettie STOC'10; Long--Saranurak FOCS'22]achieve query time linear in the number of failed vertices, and it isconditionally optimal as long as we require preprocessing time polynomial inthe size of the graph and update time polynomial in the number of failedvertices. We revisit this problem in the paradigm of algorithms with predictions: weask if the query time can be improved if the set of failed vertices can bepredicted beforehand up to a small number of errors. More specifically, wedesign a data structure that, given a graph $G=(V,E)$ and a set of verticespredicted to fail $\widehat{D} \subseteq V$ of size $d=|\widehat{D}|$,preprocesses it in time $\tilde{O}(d|E|)$ and then can receive an update givenas the symmetric difference between the predicted and the actual set of failedvertices $\widehat{D} \triangle D = (\widehat{D} \setminus D) \cup (D \setminus\widehat{D})$ of size $\eta = |\widehat{D} \triangle D|$, process it in time$\tilde{O}(\eta^4)$, and after that answer connectivity queries in $G \setminusD$ in time $O(\eta)$. Viewed from another perspective, our data structureprovides an improvement over the state of the art for the \emph{fully dynamicsubgraph connectivity problem} in the \emph{sensitivity setting}[Henzinger--Neumann ESA'16]. We argue that the preprocessing time and query time of our data structure areconditionally optimal under standard fine-grained complexity assumptions.</description><author>Bingbing Hu, Evangelos Kosinas, Adam Polak</author><pubDate>Thu, 15 Feb 2024 17:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08489v2</guid></item><item><title>ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection</title><link>http://arxiv.org/abs/2307.02591v3</link><description>Opioid related aberrant behaviors (ORABs) present novel risk factors foropioid overdose. This paper introduces a novel biomedical natural languageprocessing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is anexpert-annotated dataset designed to identify ORABs from patients' EHR notesand classify them into nine categories; 1) Confirmed Aberrant Behavior, 2)Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioiddependency, 6) Benzodiazepines, 7) Medication Changes, 8) Central NervousSystem-related, and 9) Social Determinants of Health. We explored twostate-of-the-art natural language processing models (fine-tuning andprompt-tuning approaches) to identify ORAB. Experimental results show that theprompt-tuning models outperformed the fine-tuning models in most cateogoriesand the gains were especially higher among uncommon categories (SuggestedAberrant Behavior, Confirmed Aberrant Behaviors, Diagnosed Opioid Dependence,and Medication Change). Although the best model achieved the highest 88.17\% onmacro average area under precision recall curve, uncommon classes still have alarge room for performance improvement. ODD is publicly available.</description><author>Sunjae Kwon, Xun Wang, Weisong Liu, Emily Druhl, Minhee L. Sung, Joel I. Reisman, Wenjun Li, Robert D. Kerns, William Becker, Hong Yu</author><pubDate>Thu, 15 Feb 2024 17:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02591v3</guid></item><item><title>Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data</title><link>http://arxiv.org/abs/2402.10135v1</link><description>The increasing requirements for data protection and privacy has attracted ahuge research interest on distributed artificial intelligence and specificallyon federated learning, an emerging machine learning approach that allows theconstruction of a model between several participants who hold their own privatedata. In the initial proposal of federated learning the architecture wascentralised and the aggregation was done with federated averaging, meaning thata central server will orchestrate the federation using the most straightforwardaveraging strategy. This research is focused on testing different federatedstrategies in a peer-to-peer environment. The authors propose variousaggregation strategies for federated learning, including weighted averagingaggregation, using different factors and strategies based on participantcontribution. The strategies are tested with varying data sizes to identify themost robust ones. This research tests the strategies with several biomedicaldatasets and the results of the experiments show that the accuracy-basedweighted average outperforms the classical federated averaging method.</description><author>Jose L. Salmeron, Irina Arévalo, Antonio Ruiz-Celma</author><pubDate>Thu, 15 Feb 2024 17:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10135v1</guid></item><item><title>Revisiting LARS for Large Batch Training Generalization of Neural Networks</title><link>http://arxiv.org/abs/2309.14053v4</link><description>This paper explores Large Batch Training techniques using layer-wise adaptivescaling ratio (LARS) across diverse settings, uncovering insights. LARSalgorithms with warm-up tend to be trapped in sharp minimizers early on due toredundant ratio scaling. Additionally, a fixed steep decline in the latterphase restricts deep neural networks from effectively navigating early-phasesharp minimizers. Building on these findings, we propose Time Varying LARS(TVLARS), a novel algorithm that replaces warm-up with a configurablesigmoid-like function for robust training in the initial phase. TVLARS promotesgradient exploration early on, surpassing sharp optimizers and graduallytransitioning to LARS for robustness in later phases. Extensive experimentsdemonstrate that TVLARS consistently outperforms LARS and LAMB in most cases,with up to 2\% improvement in classification scenarios. Notably, in allself-supervised learning cases, TVLARS dominates LARS and LAMB with performanceimprovements of up to 10\%.</description><author>Khoi Do, Duong Nguyen, Hoa Nguyen, Long Tran-Thanh, Nguyen-Hoang Tran, Quoc-Viet Pham</author><pubDate>Thu, 15 Feb 2024 17:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14053v4</guid></item><item><title>Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem</title><link>http://arxiv.org/abs/2402.10133v1</link><description>Procedural content generation uses algorithmic techniques to create largeamounts of new content for games at much lower production costs. In newerapproaches, procedural content generation utilizes machine learning. However,these methods usually require expensive collection of large amounts of data, aswell as the development and training of fairly complex learning models, whichcan be both extremely time-consuming and expensive. The core of our research isto explore whether we can lower the barrier to the use of personalizedprocedural content generation through a more practical and generalizableapproach with large language models. Matching game content with playerpreferences benefits both players, who enjoy the game more, and developers, whoincreasingly depend on players enjoying the game before being able to monetizeit. Therefore, this paper presents a novel approach to achievingpersonalization by using large language models to propose levels based on thegameplay data continuously collected from individual players. We compared thelevels generated using our approach with levels generated with more traditionalprocedural generation techniques. Our easily reproducible method has provenviable in a production setting and outperformed levels generated by traditionalmethods in the probability that a player will not quit the game mid-level.</description><author>Davor Hafnar, Jure Demšar</author><pubDate>Thu, 15 Feb 2024 17:37:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10133v1</guid></item><item><title>Is Continual Learning Ready for Real-world Challenges?</title><link>http://arxiv.org/abs/2402.10130v1</link><description>Despite continual learning's long and well-established academic history, itsapplication in real-world scenarios remains rather limited. This paper contendsthat this gap is attributable to a misalignment between the actual challengesof continual learning and the evaluation protocols in use, rendering proposedsolutions ineffective for addressing the complexities of real-world setups. Wevalidate our hypothesis and assess progress to date, using a new 3D semanticsegmentation benchmark, OCL-3DSS. We investigate various continual learningschemes from the literature by utilizing more realistic protocols thatnecessitate online and continual learning for dynamic, real-world scenarios(eg., in robotics and 3D vision applications). The outcomes are sobering: allconsidered methods perform poorly, significantly deviating from the upper boundof joint offline training. This raises questions about the applicability ofexisting methods in realistic settings. Our paper aims to initiate a paradigmshift, advocating for the adoption of continual learning methods through newexperimental protocols that better emulate real-world conditions to facilitatebreakthroughs in the field.</description><author>Theodora Kontogianni, Yuanwen Yue, Siyu Tang, Konrad Schindler</author><pubDate>Thu, 15 Feb 2024 17:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10130v1</guid></item><item><title>GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering</title><link>http://arxiv.org/abs/2402.10128v1</link><description>Advancements in 3D Gaussian Splatting have significantly accelerated 3Dreconstruction and generation. However, it may require a large number ofGaussians, which creates a substantial memory footprint. This paper introducesGES (Generalized Exponential Splatting), a novel representation that employsGeneralized Exponential Function (GEF) to model 3D scenes, requiring far fewerparticles to represent a scene and thus significantly outperforming GaussianSplatting methods in efficiency with a plug-and-play replacement ability forGaussian-based utilities. GES is validated theoretically and empirically inboth principled 1D setup and realistic 3D scenes. It is shown to represent signals with sharp edges more accurately, which aretypically challenging for Gaussians due to their inherent low-passcharacteristics. Our empirical analysis demonstrates that GEF outperformsGaussians in fitting natural-occurring signals (e.g. squares, triangles, andparabolic signals), thereby reducing the need for extensive splittingoperations that increase the memory footprint of Gaussian Splatting. With theaid of a frequency-modulated loss, GES achieves competitive performance innovel-view synthesis benchmarks while requiring less than half the memorystorage of Gaussian Splatting and increasing the rendering speed by up to 39%.The code is available on the project website https://abdullahamdi.com/ges .</description><author>Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi</author><pubDate>Thu, 15 Feb 2024 17:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10128v1</guid></item><item><title>Explain Variance of Prediction in Variational Time Series Models for Clinical Deterioration Prediction</title><link>http://arxiv.org/abs/2402.06808v2</link><description>Missingness and measurement frequency are two sides of the same coin. Howfrequent should we measure clinical variables and conduct laboratory tests? Itdepends on many factors such as the stability of patient conditions, diagnosticprocess, treatment plan and measurement costs. The utility of measurementsvaries disease by disease, patient by patient. In this study we propose a novelview of clinical variable measurement frequency from a predictive modelingperspective, namely the measurements of clinical variables reduce uncertaintyin model predictions. To achieve this goal, we propose variance SHAP withvariational time series models, an application of Shapley AdditiveExpanation(SHAP) algorithm to attribute epistemic prediction uncertainty. Theprediction variance is estimated by sampling the conditional hidden space invariational models and can be approximated deterministically by delta's method.This approach works with variational time series models such as variationalrecurrent neural networks and variational transformers. Since SHAP values areadditive, the variance SHAP of binary data imputation masks can be directlyinterpreted as the contribution to prediction variance by measurements. Wetested our ideas on a public ICU dataset with deterioration prediction task andstudy the relation between variance SHAP and measurement time intervals.</description><author>Jiacheng Liu, Jaideep Srivastava</author><pubDate>Thu, 15 Feb 2024 17:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06808v2</guid></item><item><title>Nonlinear spiked covariance matrices and signal propagation in deep neural networks</title><link>http://arxiv.org/abs/2402.10127v1</link><description>Many recent works have studied the eigenvalue spectrum of the ConjugateKernel (CK) defined by the nonlinear feature map of a feedforward neuralnetwork. However, existing results only establish weak convergence of theempirical eigenvalue distribution, and fall short of providing precisequantitative characterizations of the ''spike'' eigenvalues and eigenvectorsthat often capture the low-dimensional signal structure of the learningproblem. In this work, we characterize these signal eigenvalues andeigenvectors for a nonlinear version of the spiked covariance model, includingthe CK as a special case. Using this general result, we give a quantitativedescription of how spiked eigenstructure in the input data propagates throughthe hidden layers of a neural network with random weights. As a secondapplication, we study a simple regime of representation learning where theweight matrix develops a rank-one signal component over training andcharacterize the alignment of the target function with the spike eigenvector ofthe CK on test data.</description><author>Zhichao Wang, Denny Wu, Zhou Fan</author><pubDate>Thu, 15 Feb 2024 17:31:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10127v1</guid></item><item><title>Policy Improvement using Language Feedback Models</title><link>http://arxiv.org/abs/2402.07876v2</link><description>We introduce Language Feedback Models (LFMs) that identify desirablebehaviour - actions that help achieve tasks specified in the instruction - forimitation learning in instruction following. To train LFMs, we obtain feedbackfrom Large Language Models (LLMs) on visual trajectories verbalized to languagedescriptions. First, by using LFMs to identify desirable behaviour to imitate,we improve in task-completion rate over strong behavioural cloning baselines onthree distinct language grounding environments (Touchdown, ScienceWorld, andALFWorld). Second, LFMs outperform using LLMs as experts to directly predictactions, when controlling for the number of LLM output tokens. Third, LFMsgeneralize to unseen environments, improving task-completion rate by 3.5-12.0%through one round of adaptation. Finally, LFM can be modified to providehuman-interpretable feedback without performance loss, allowing humanverification of desirable behaviour for imitation learning.</description><author>Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre Côté</author><pubDate>Thu, 15 Feb 2024 17:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07876v2</guid></item><item><title>Are self-explanations from Large Language Models faithful?</title><link>http://arxiv.org/abs/2401.07927v3</link><description>Instruction-tuned Large Language Models (LLMs) excel at many tasks and willeven explain their reasoning, so-called self-explanations. However, convincingand wrong self-explanations can lead to unsupported confidence in LLMs, thusincreasing risk. Therefore, it's important to measure if self-explanationstruly reflect the model's behavior. Such a measure is calledinterpretability-faithfulness and is challenging to perform since the groundtruth is inaccessible, and many LLMs only have an inference API. To addressthis, we propose employing self-consistency checks to measure faithfulness. Forexample, if an LLM says a set of words is important for making a prediction,then it should not be able to make its prediction without these words. Whileself-consistency checks are a common approach to faithfulness, they have notpreviously been successfully applied to LLM self-explanations forcounterfactual, importance measure, and redaction explanations. Our resultsdemonstrate that faithfulness is explanation, model, and task-dependent,showing self-explanations should not be trusted in general. For example, withsentiment classification, counterfactuals are more faithful for Llama2,importance measures for Mistral, and redaction for Falcon 40B.</description><author>Andreas Madsen, Sarath Chandar, Siva Reddy</author><pubDate>Thu, 15 Feb 2024 17:19:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07927v3</guid></item><item><title>Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN</title><link>http://arxiv.org/abs/2402.10115v1</link><description>In this study, we tackle a modern research challenge within the field ofperceptual brain decoding, which revolves around synthesizing images from EEGsignals using an adversarial deep learning framework. The specific objective isto recreate images belonging to various object categories by leveraging EEGrecordings obtained while subjects view those images. To achieve this, weemploy a Transformer-encoder based EEG encoder to produce EEG encodings, whichserve as inputs to the generator component of the GAN network. Alongside theadversarial loss, we also incorporate perceptual loss to enhance the quality ofthe generated images.</description><author>Rahul Mishra, Arnav Bhavsar</author><pubDate>Thu, 15 Feb 2024 17:10:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10115v1</guid></item><item><title>Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning</title><link>http://arxiv.org/abs/2402.10110v1</link><description>Instruction tuning is critical to large language models (LLMs) for achievingbetter instruction following and task adaptation capabilities but its successheavily relies on the training data quality. Many recent methods focus onimproving the data quality but often overlook the compatibility of the datawith the student model being finetuned. This paper introduces SelectiveReflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflectionand introspection for improving existing data quality with the data selectioncapability of the student LLM, to automatically refine existinginstruction-tuning data. This teacher-student collaboration produceshigh-quality and student-compatible instruction-response pairs, resulting insample-efficient instruction tuning and LLMs of superior performance. SelectiveReflection-Tuning is a data augmentation and synthesis that generally improvesLLM finetuning and self-improvement without collecting brand-new data. We applyour method to Alpaca and WizardLM data and achieve much stronger and top-tier7B and 13B LLMs. Our codes, models, and data will be released athttps://github.com/tianyi-lab/Reflection_Tuning.</description><author>Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, Tianyi Zhou</author><pubDate>Thu, 15 Feb 2024 17:06:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10110v1</guid></item><item><title>Towards Reducing Diagnostic Errors with Interpretable Risk Prediction</title><link>http://arxiv.org/abs/2402.10109v1</link><description>Many diagnostic errors occur because clinicians cannot easily access relevantinformation in patient Electronic Health Records (EHRs). In this work wepropose a method to use LLMs to identify pieces of evidence in patient EHR datathat indicate increased or decreased risk of specific diagnoses; our ultimateaim is to increase access to evidence and reduce diagnostic errors. Inparticular, we propose a Neural Additive Model to make predictions backed byevidence with individualized risk estimates at time-points where clinicians arestill uncertain, aiming to specifically mitigate delays in diagnosis and errorsstemming from an incomplete differential. To train such a model, it isnecessary to infer temporally fine-grained retrospective labels of eventual"true" diagnoses. We do so with LLMs, to ensure that the input text is frombefore a confident diagnosis can be made. We use an LLM to retrieve an initialpool of evidence, but then refine this set of evidence according tocorrelations learned by the model. We conduct an in-depth evaluation of theusefulness of our approach by simulating how it might be used by a clinician todecide between a pre-defined list of differential diagnoses.</description><author>Denis Jered McInerney, William Dickinson, Lucy Flynn, Andrea Young, Geoffrey Young, Jan-Willem van de Meent, Byron C. Wallace</author><pubDate>Thu, 15 Feb 2024 17:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10109v1</guid></item><item><title>Dual input stream transformer for vertical drift correction in eye-tracking reading data</title><link>http://arxiv.org/abs/2311.06095v2</link><description>We introduce a novel Dual Input Stream Transformer (DIST) for the challengingproblem of assigning fixation points from eye-tracking data collected duringpassage reading to the line of text that the reader was actually focused on.This post-processing step is crucial for analysis of the reading data due tothe presence of noise in the form of vertical drift. We evaluate DIST againsteleven classical approaches on a comprehensive suite of nine diverse datasets.We demonstrate that combining multiple instances of the DIST model in anensemble achieves high accuracy across all datasets. Further combining the DISTensemble with the best classical approach yields an average accuracy of 98.17%. Our approach presents a significant step towards addressing the bottleneckof manual line assignment in reading research. Through extensive analysis andablation studies, we identify key factors that contribute to DIST's success,including the incorporation of line overlap features and the use of a secondinput stream. Via rigorous evaluation, we demonstrate that DIST is robust tovarious experimental setups, making it a safe first choice for practitioners inthe field.</description><author>Thomas M. Mercier, Marcin Budka, Martin R. Vasilev, Julie A. Kirkby, Bernhard Angele, Timothy J. Slattery</author><pubDate>Thu, 15 Feb 2024 17:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06095v2</guid></item><item><title>Fast and explainable clustering based on sorting</title><link>http://arxiv.org/abs/2202.01456v2</link><description>We introduce a fast and explainable clustering method called CLASSIX. Itconsists of two phases, namely a greedy aggregation phase of the sorted datainto groups of nearby data points, followed by the merging of groups intoclusters. The algorithm is controlled by two scalar parameters, namely adistance parameter for the aggregation and another parameter controlling theminimal cluster size. Extensive experiments are conducted to give acomprehensive evaluation of the clustering performance on synthetic andreal-world datasets, with various cluster shapes and low to high featuredimensionality. Our experiments demonstrate that CLASSIX competes withstate-of-the-art clustering algorithms. The algorithm has linear spacecomplexity and achieves near linear time complexity on a wide range ofproblems. Its inherent simplicity allows for the generation of intuitiveexplanations of the computed clusters.</description><author>Xinye Chen, Stefan Güttel</author><pubDate>Thu, 15 Feb 2024 17:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.01456v2</guid></item><item><title>Quantized Embedding Vectors for Controllable Diffusion Language Models</title><link>http://arxiv.org/abs/2402.10107v1</link><description>Improving the controllability, portability, and inference speed of diffusionlanguage models (DLMs) is a key challenge in natural language generation. Whilerecent research has shown significant success in complex text generation withlanguage models, the memory and computational power are still very demandingand fall short of expectations, which naturally results in low portability andinstability for the models. To mitigate these issues, numerous well-establishedmethods were proposed for neural network quantization. To further enhance theirportability of independent deployment as well as improve their stabilityevaluated by language perplexity, we propose a novel approach called theQuantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLMbuilds upon the recent successful controllable DLMs by remodeling thetask-specific embedding space via quantization. This leads to a gradient-basedcontroller for the generation tasks, and more stable intermediate latentvariables are obtained, which naturally brings in an accelerated convergence aswell as better controllability. Additionally, the adaption fine-tuning methodis employed to reduce tunable weights. Experimental results on five challengingfine-grained control tasks demonstrate that QE-CDLM compares favorably toexisting methods in terms of quality and feasibility, achieving betterperplexity and lightweight fine-tuning.</description><author>Cheng Kang, Xinye Chen, Yong Hu, Daniel Novak</author><pubDate>Thu, 15 Feb 2024 17:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10107v1</guid></item><item><title>Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling</title><link>http://arxiv.org/abs/2306.03117v2</link><description>The dynamic nature of proteins is crucial for determining their biologicalfunctions and properties, for which Monte Carlo (MC) and molecular dynamics(MD) simulations stand as predominant tools to study such phenomena. Byutilizing empirically derived force fields, MC or MD simulations explore theconformational space through numerically evolving the system via Markov chainor Newtonian mechanics. However, the high-energy barrier of the force fieldscan hamper the exploration of both methods by the rare event, resulting ininadequately sampled ensemble without exhaustive running. Existinglearning-based approaches perform direct sampling yet heavily rely ontarget-specific simulation data for training, which suffers from high dataacquisition cost and poor generalizability. Inspired by simulated annealing, wepropose Str2Str, a novel structure-to-structure translation framework capableof zero-shot conformation sampling with roto-translation equivariant property.Our method leverages an amortized denoising score matching objective trained ongeneral crystal structures and has no reliance on simulation data during bothtraining and inference. Experimental results across several benchmarkingprotein systems demonstrate that Str2Str outperforms previous state-of-the-artgenerative structure prediction models and can be orders of magnitude fastercompared to long MD simulations. Our open-source implementation is available athttps://github.com/lujiarui/Str2Str</description><author>Jiarui Lu, Bozitao Zhong, Zuobai Zhang, Jian Tang</author><pubDate>Thu, 15 Feb 2024 16:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03117v2</guid></item><item><title>GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving</title><link>http://arxiv.org/abs/2402.10104v1</link><description>Recent advancements in Large Language Models (LLMs) and Multi-Modal Models(MMs) have demonstrated their remarkable capabilities in problem-solving. Yet,their proficiency in tackling geometry math problems, which necessitates anintegrated understanding of both textual and visual information, has not beenthoroughly evaluated. To address this gap, we introduce the GeoEval benchmark,a comprehensive collection that includes a main subset of 2000 problems, a 750problem subset focusing on backward reasoning, an augmented subset of 2000problems, and a hard subset of 300 problems. This benchmark facilitates adeeper investigation into the performance of LLMs and MMs on solving geometrymath problems. Our evaluation of ten LLMs and MMs across these varied subsetsreveals that the WizardMath model excels, achieving a 55.67\% accuracy rate onthe main subset but only a 6.00\% accuracy on the challenging subset. Thishighlights the critical need for testing models against datasets on which theyhave not been pre-trained. Additionally, our findings indicate that GPT-seriesmodels perform more effectively on problems they have rephrased, suggesting apromising method for enhancing model capabilities.</description><author>Jiaxin Zhang, Zhongzhi Li, Mingliang Zhang, Fei Yin, Chenglin Liu, Yashar Moshfeghi</author><pubDate>Thu, 15 Feb 2024 16:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10104v1</guid></item><item><title>FedMT: Federated Learning with Mixed-type Labels</title><link>http://arxiv.org/abs/2210.02042v4</link><description>In federated learning (FL), classifiers (e.g., deep networks) are trained ondatasets from multiple data centers without exchanging data across them, whichimproves the sample efficiency. However, the conventional FL setting assumesthe same labeling criterion in all data centers involved, thus limiting itspractical utility. This limitation becomes particularly notable in domains likedisease diagnosis, where different clinical centers may adhere to differentstandards, making traditional FL methods unsuitable. This paper addresses thisimportant yet under-explored setting of FL, namely FL with mixed-type labels,where the allowance of different labeling criteria introduces inter-centerlabel space differences. To address this challenge effectively and efficiently,we introduce a model-agnostic approach called FedMT, which estimates labelspace correspondences and projects classification scores to construct lossfunctions. The proposed FedMT is versatile and integrates seamlessly withvarious FL methods, such as FedAvg. Experimental results on benchmark andmedical datasets highlight the substantial improvement in classificationaccuracy achieved by FedMT in the presence of mixed-type labels.</description><author>Qiong Zhang, Jing Peng, Xin Zhang, Aline Talhouk, Gang Niu, Xiaoxiao Li</author><pubDate>Thu, 15 Feb 2024 16:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.02042v4</guid></item><item><title>Indiscriminate Data Poisoning Attacks on Neural Networks</title><link>http://arxiv.org/abs/2204.09092v2</link><description>Data poisoning attacks, in which a malicious adversary aims to influence amodel by injecting "poisoned" data into the training process, have attractedsignificant recent attention. In this work, we take a closer look at existingpoisoning attacks and connect them with old and new algorithms for solvingsequential Stackelberg games. By choosing an appropriate loss function for theattacker and optimizing with algorithms that exploit second-order information,we design poisoning attacks that are effective on neural networks. We presentefficient implementations that exploit modern auto-differentiation packages andallow simultaneous and coordinated generation of tens of thousands of poisonedpoints, in contrast to existing methods that generate poisoned points one byone. We further perform extensive experiments that empirically explore theeffect of data poisoning attacks on deep neural networks.</description><author>Yiwei Lu, Gautam Kamath, Yaoliang Yu</author><pubDate>Thu, 15 Feb 2024 16:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.09092v2</guid></item><item><title>A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research</title><link>http://arxiv.org/abs/2402.10102v1</link><description>Distributed Artificial Intelligence is attracting interest day by day. Inthis paper, the authors introduce an innovative methodology for distributedlearning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in aprivacy-preserving way. The authors design a training scheme for collaborativeFCM learning that offers data privacy compliant with the current regulation.This method is applied to a cancer detection problem, proving that theperformance of the model is improved by the Federated Learning process, andobtaining similar results to the ones that can be found in the literature.</description><author>Jose L. Salmeron, Irina Arévalo</author><pubDate>Thu, 15 Feb 2024 16:56:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10102v1</guid></item><item><title>Any-Shift Prompting for Generalization over Distributions</title><link>http://arxiv.org/abs/2402.10099v1</link><description>Image-language models with prompt learning have shown remarkable advances innumerous downstream vision tasks. Nevertheless, conventional prompt learningmethods overfit their training distribution and lose the generalization abilityon test distributions. To improve generalization across various distributionshifts, we propose any-shift prompting: a general probabilistic inferenceframework that considers the relationship between training and testdistributions during prompt learning. We explicitly connect training and testdistributions in the latent space by constructing training and test prompts ina hierarchical architecture. Within this framework, the test prompt exploitsthe distribution relationships to guide the generalization of the CLIPimage-language model from training to any test distribution. To effectivelyencode the distribution information and their relationships, we furtherintroduce a transformer inference network with a pseudo-shift trainingmechanism. The network generates the tailored test prompt with both trainingand test information in a feedforward pass, avoiding extra training costs attest time. Extensive experiments on twenty-three datasets demonstrate theeffectiveness of any-shift prompting on the generalization over variousdistribution shifts.</description><author>Zehao Xiao, Jiayi Shen, Mohammad Mahdi Derakhshani, Shengcai Liao, Cees G. M. Snoek</author><pubDate>Thu, 15 Feb 2024 16:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10099v1</guid></item><item><title>Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling</title><link>http://arxiv.org/abs/2402.10097v1</link><description>Federated Learning (FL) algorithms commonly sample a random subset of clientsto address the straggler issue and improve communication efficiency. Whilerecent works have proposed various client sampling methods, they havelimitations in joint system and data heterogeneity design, which may not alignwith practical heterogeneous wireless networks. In this work, we advocate a newindependent client sampling strategy to minimize the wall-clock training timeof FL, while considering data heterogeneity and system heterogeneity in bothcommunication and computation. We first derive a new convergence bound fornon-convex loss functions with independent client sampling and then propose anadaptive bandwidth allocation scheme. Furthermore, we propose an efficientindependent client sampling algorithm based on the upper bounds on theconvergence rounds and the expected per-round training time, to minimize thewall-clock time of FL, while considering both the data and systemheterogeneity. Experimental results under practical wireless network settingswith real-world prototype demonstrate that the proposed independent samplingscheme substantially outperforms the current best sampling schemes undervarious training models and datasets.</description><author>Jiaxiang Geng, Yanzhao Hou, Xiaofeng Tao, Juncheng Wang, Bing Luo</author><pubDate>Thu, 15 Feb 2024 16:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10097v1</guid></item><item><title>Decision Theoretic Foundations for Experiments Evaluating Human Decisions</title><link>http://arxiv.org/abs/2401.15106v2</link><description>Decision-making with information displays is a key focus of research in areaslike explainable AI, human-AI teaming, and data visualization. However, whatconstitutes a decision problem, and what is required for an experiment to becapable of concluding that human decisions are flawed in some way, remain opento speculation. We present a widely applicable definition of a decision problemsynthesized from statistical decision theory and information economics. Weargue that to attribute loss in human performance to forms of bias, anexperiment must provide participants with the information that a rational agentwould need to identify the normative decision. We evaluate the extent to whichrecent evaluations of decision-making from the literature on AI-assisteddecisions achieve this criteria. We find that only 10 (26\%) of 39 studies thatclaim to identify biased behavior present participants with sufficientinformation to characterize their behavior as deviating from gooddecision-making in at least one treatment condition. We motivate the value ofstudying well-defined decision problems by describing a characterization ofperformance losses they allow us to conceive. In contrast, the ambiguities of apoorly communicated decision problem preclude normative interpretation. Weconclude with recommendations for practice.</description><author>Jessica Hullman, Alex Kale, Jason Hartline</author><pubDate>Thu, 15 Feb 2024 16:51:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15106v2</guid></item><item><title>Classification Diffusion Models</title><link>http://arxiv.org/abs/2402.10095v1</link><description>A prominent family of methods for learning data distributions relies ondensity ratio estimation (DRE), where a model is trained to $\textit{classify}$between data samples and samples from some reference distribution. Thesetechniques are successful in simple low-dimensional settings but fail toachieve good results on complex high-dimensional data, like images. A differentfamily of methods for learning distributions is that of denoising diffusionmodels (DDMs), in which a model is trained to $\textit{denoise}$ data samples.These approaches achieve state-of-the-art results in image, video, and audiogeneration. In this work, we present $\textit{Classification Diffusion Models}$(CDMs), a generative technique that adopts the denoising-based formalism ofDDMs while making use of a classifier that predicts the amount of noise addedto a clean signal, similarly to DRE methods. Our approach is based on theobservation that an MSE-optimal denoiser for white Gaussian noise can beexpressed in terms of the gradient of a cross-entropy-optimal classifier forpredicting the noise level. As we illustrate, CDM achieves better denoisingresults compared to DDM, and leads to at least comparable FID in imagegeneration. CDM is also capable of highly efficient one-step exact likelihoodestimation, achieving state-of-the-art results among methods that use a singlestep. Code is available on the project's webpage inhttps://shaharYadin.github.io/CDM/ .</description><author>Shahar Yadin, Noam Elata, Tomer Michaeli</author><pubDate>Thu, 15 Feb 2024 16:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10095v1</guid></item><item><title>Stabilized Neural Differential Equations for Learning Dynamics with Explicit Constraints</title><link>http://arxiv.org/abs/2306.09739v3</link><description>Many successful methods to learn dynamical systems from data have recentlybeen introduced. However, ensuring that the inferred dynamics preserve knownconstraints, such as conservation laws or restrictions on the allowed systemstates, remains challenging. We propose stabilized neural differentialequations (SNDEs), a method to enforce arbitrary manifold constraints forneural differential equations. Our approach is based on a stabilization termthat, when added to the original dynamics, renders the constraint manifoldprovably asymptotically stable. Due to its simplicity, our method is compatiblewith all common neural differential equation (NDE) models and broadlyapplicable. In extensive empirical evaluations, we demonstrate that SNDEsoutperform existing methods while broadening the types of constraints that canbe incorporated into NDE training.</description><author>Alistair White, Niki Kilbertus, Maximilian Gelbrecht, Niklas Boers</author><pubDate>Thu, 15 Feb 2024 16:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09739v3</guid></item><item><title>MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations</title><link>http://arxiv.org/abs/2402.10093v1</link><description>We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learningboost for pre-trained MIM models. The motivation behind MIM-Refiner is rootedin the insight that optimal representations within MIM models generally residein intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastiveheads that are connected to diverse intermediate layers. In each head, amodified nearest neighbor objective helps to construct respective semanticclusters. The refinement process is short but effective. Within a few epochs, we refinethe features of MIM models from subpar to state-of-the-art, off-the-shelffeatures. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K,achieves new state-of-the-art results in linear probing (84.7%) and low-shotclassification among models that are pre-trained on ImageNet-1K. In ImageNet-1K1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%,outperforming larger models that were trained on up to 2000x more data such asDINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page:https://ml-jku.github.io/MIM-Refiner</description><author>Benedikt Alkin, Lukas Miklautz, Sepp Hochreiter, Johannes Brandstetter</author><pubDate>Thu, 15 Feb 2024 16:46:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10093v1</guid></item><item><title>Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4</title><link>http://arxiv.org/abs/2402.10083v1</link><description>Purpose: To assess the alignment of GPT-4-based evaluation to human clinicianexperts, for the evaluation of responses to ophthalmology-related patientqueries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmologyquestions and paired answers were created by ophthalmologists to representcommonly asked patient questions, divided into fine-tuning (368; 92%), andtesting (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,additional 8 glaucoma QnA pairs were included. 200 responses to the testingdataset were generated by 5 fine-tuned LLMs for evaluation. A customizedclinical evaluation rubric was used to guide GPT-4 evaluation, grounded onclinical accuracy, relevance, patient safety, and ease of understanding. GPT-4evaluation was then compared against ranking by 5 clinicians for clinicalalignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4evaluation demonstrated significant agreement with human clinician rankings,with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80respectively; while correlation based on Cohen Kappa was more modest at 0.50.Notably, qualitative analysis and the glaucoma sub-analysis revealed clinicalinaccuracies in the LLM-generated responses, which were appropriatelyidentified by the GPT-4 evaluation. Conclusion: The notable clinical alignmentof GPT-4 evaluation highlighted its potential to streamline the clinicalevaluation of LLM chatbot responses to healthcare-related queries. Bycomplementing the existing clinician-dependent manual grading, this efficientand automated evaluation could assist the validation of future developments inLLM applications for healthcare.</description><author>Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting</author><pubDate>Thu, 15 Feb 2024 16:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10083v1</guid></item><item><title>FedRDF: A Robust and Dynamic Aggregation Function against Poisoning Attacks in Federated Learning</title><link>http://arxiv.org/abs/2402.10082v1</link><description>Federated Learning (FL) represents a promising approach to typical privacyconcerns associated with centralized Machine Learning (ML) deployments. Despiteits well-known advantages, FL is vulnerable to security attacks such asByzantine behaviors and poisoning attacks, which can significantly degrademodel performance and hinder convergence. The effectiveness of existingapproaches to mitigate complex attacks, such as median, trimmed mean, or Krumaggregation functions, has been only partially demonstrated in the case ofspecific attacks. Our study introduces a novel robust aggregation mechanismutilizing the Fourier Transform (FT), which is able to effectively handlingsophisticated attacks without prior knowledge of the number of attackers.Employing this data technique, weights generated by FL clients are projectedinto the frequency domain to ascertain their density function, selecting theone exhibiting the highest frequency. Consequently, malicious clients' weightsare excluded. Our proposed approach was tested against various model poisoningattacks, demonstrating superior performance over state-of-the-art aggregationmethods.</description><author>Enrique Mármol Campos, Aurora González Vidal, José Luis Hernández Ramos, Antonio Skarmeta</author><pubDate>Thu, 15 Feb 2024 16:42:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10082v1</guid></item><item><title>Identifiability of Direct Effects from Summary Causal Graphs</title><link>http://arxiv.org/abs/2306.16958v4</link><description>Dynamic structural causal models (SCMs) are a powerful framework forreasoning in dynamic systems about direct effects which measure how a change inone variable affects another variable while holding all other variablesconstant. The causal relations in a dynamic structural causal model can bequalitatively represented with an acyclic full-time causal graph. Assuminglinearity and no hidden confounding and given the full-time causal graph, thedirect causal effect is always identifiable. However, in many application sucha graph is not available for various reasons but nevertheless experts haveaccess to the summary causal graph of the full-time causal graph whichrepresents causal relations between time series while omitting temporalinformation and allowing cycles. This paper presents a complete identifiabilityresult which characterizes all cases for which the direct effect is graphicallyidentifiable from a summary causal graph and gives two sound finite adjustmentsets that can be used to estimate the direct effect whenever it isidentifiable.</description><author>Simon Ferreira, Charles K. Assaad</author><pubDate>Thu, 15 Feb 2024 16:42:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16958v4</guid></item><item><title>Forecasting Response to Treatment with Global Deep Learning and Patient-Specific Pharmacokinetic Priors</title><link>http://arxiv.org/abs/2309.13135v6</link><description>Forecasting healthcare time series is crucial for early detection of adverseoutcomes and for patient monitoring. Forecasting, however, can be difficult inpractice due to noisy and intermittent data. The challenges are oftenexacerbated by change points induced via extrinsic factors, such as theadministration of medication. To address these challenges, we propose a novelhybrid global-local architecture and a pharmacokinetic encoder that informsdeep learning models of patient-specific treatment effects. We showcase theefficacy of our approach in achieving significant accuracy gains for a bloodglucose forecasting task using both realistically simulated and real-worlddata. Our global-local architecture improves over patient-specific models by9.2-14.6%. Additionally, our pharmacokinetic encoder improves over alternativeencoding techniques by 4.4% on simulated data and 2.1% on real-world data. Theproposed approach can have multiple beneficial applications in clinicalpractice, such as issuing early warnings about unexpected treatment responses,or helping to characterize patient-specific treatment effects in terms of drugabsorption and elimination characteristics.</description><author>Willa Potosnak, Cristian Challu, Kin G. Olivares, Artur Dubrawski</author><pubDate>Thu, 15 Feb 2024 16:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13135v6</guid></item><item><title>InstructBooth: Instruction-following Personalized Text-to-Image Generation</title><link>http://arxiv.org/abs/2312.03011v2</link><description>Personalizing text-to-image models using a limited set of images for aspecific object has been explored in subject-specific image generation.However, existing methods often face challenges in aligning with text promptsdue to overfitting to the limited training images. In this work, we introduceInstructBooth, a novel method designed to enhance image-text alignment inpersonalized text-to-image models without sacrificing the personalizationability. Our approach first personalizes text-to-image models with a smallnumber of subject-specific images using a unique identifier. Afterpersonalization, we fine-tune personalized text-to-image models usingreinforcement learning to maximize a reward that quantifies image-textalignment. Additionally, we propose complementary techniques to increase thesynergy between these two processes. Our method demonstrates superiorimage-text alignment compared to existing baselines, while maintaining highpersonalization ability. In human evaluations, InstructBooth outperforms themwhen considering all comprehensive factors. Our project page is athttps://sites.google.com/view/instructbooth.</description><author>Daewon Chae, Nokyung Park, Jinkyu Kim, Kimin Lee</author><pubDate>Thu, 15 Feb 2024 16:38:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03011v2</guid></item><item><title>QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference</title><link>http://arxiv.org/abs/2402.10076v1</link><description>We introduce QUICK, a group of novel optimized CUDA kernels for the efficientinference of quantized Large Language Models (LLMs). QUICK addresses the sharedmemory bank-conflict problem of state-of-the-art mixed precision matrixmultiplication kernels. Our method interleaves the quantized weight matrices ofLLMs offline to skip the shared memory write-back after the dequantization. Wedemonstrate up to 1.91x speedup over existing kernels of AutoAWQ on largerbatches and up to 1.94x throughput gain on representative LLM models on variousNVIDIA GPU devices.</description><author>Taesu Kim, Jongho Lee, Daehyun Ahn, Sarang Kim, Jiwoong Choi, Minkyu Kim, Hyungjun Kim</author><pubDate>Thu, 15 Feb 2024 16:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10076v1</guid></item></channel></rss>